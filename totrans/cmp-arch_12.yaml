- en: '**10**'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**10**'
- en: '**MEMORY**'
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**内存**'
- en: '![Image](../images/f0215-01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0215-01.jpg)'
- en: So far we’ve constructed registers and a small, Baby-sized RAM to use as memory.
    We made these from flip-flops. Larger memories can’t usually afford to use flip-flops,
    however, so they’re typically made using other technologies, like DRAM and hard
    disks. These other technologies are slower, creating a trade-off between speed
    and size. In this chapter, we’ll look at the details of larger memories. We’ll
    discuss primary memory, caches, and secondary and offline memory, and begin by
    looking at the memory hierarchy.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经构建了寄存器和一个小型的、婴儿级别的RAM来作为内存。我们使用触发器（flip-flops）制造了这些内存。然而，更大的内存通常不能使用触发器，所以它们通常使用其他技术，如DRAM和硬盘。这些其他技术较慢，因此在速度和容量之间形成了一个权衡。在本章中，我们将深入了解更大内存的细节。我们将讨论主内存、缓存、以及二级和离线内存，并从内存层次结构开始。
- en: The Memory Hierarchy
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存层次结构
- en: At any point in time, usually only some of our data is important and in frequent,
    current use. Other data is used occasionally, and some is out of use entirely.
    We usually want to arrange our data so that the parts in working use are kept
    in fast, easily available memory, while the other parts are kept in slower, cheaper
    memories. This arrangement is known as a *memory hierarchy*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何时刻，通常只有一部分数据是重要的，并且在频繁使用。其他数据偶尔使用，有些数据完全不再使用。我们通常希望将数据安排在快速且易于访问的内存中，以便工作中的数据能够高效访问，而其他数据则保存在较慢且便宜的内存中。这种安排被称为*内存层次结构*。
- en: Memory hierarchies played out in pre-digital life, too. For example, people
    used to carry around shopping lists and important phone numbers written on scraps
    of paper for immediate, regular use. On their desks would be larger paper documents
    used only when at work. Beyond the desk were shelves and cabinets containing books
    and files with data used less often. Still further removed were storage boxes
    in attics, then local and national libraries and archives that required increasing
    time to visit. Data could be promoted and demoted between these different stores
    at different times. For example, a book might sit in the library unused for years,
    then be promoted to your desk for a few weeks when you needed it. Unused documents
    on your desk could be demoted to a filing cabinet then to the attic.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 内存层次结构在数字化之前的生活中也存在。例如，人们过去会将购物清单和重要的电话号码写在纸片上，方便即时和经常使用。在他们的办公桌上，通常会有较大的纸质文件，只有在工作时才会用到。更远的地方是架子和柜子，里面存放着不常用的书籍和文件。更远的地方是阁楼里的存储箱，然后是本地和国家的图书馆和档案馆，访问这些地方需要更多时间。数据可以在这些不同的存储之间进行提升和降级。例如，一本书可能多年未被使用，静静地放在图书馆里，之后在需要时被提升到桌面上使用几周。桌面上不再使用的文件可以降级到文件柜，再到阁楼中。
- en: The same concepts apply to computer memory. When fast and slow versions of the
    same technology are available, the fast one is better, so it can command a higher
    price, meaning you can buy less of it compared to the slower one. Given a budget,
    you can thus trade off speed for capacity. Since most people want some data to
    be more readily accessible than other data, it makes economic sense to buy and
    use a mixture of memory types, ranging from small and fast for working data to
    large and slow for rarely used data. [Figure 10-1](ch10.xhtml#ch10fig1) shows
    the approximate speeds and capacities for each of the levels of memory hierarchy
    that we’ll discuss in this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的概念也适用于计算机内存。当同一技术有快速和慢速版本时，快速版本更好，因此它可以要求更高的价格，这意味着与较慢版本相比，你能购买的量较少。在有预算的情况下，你可以在速度和容量之间进行权衡。由于大多数人希望某些数据比其他数据更容易访问，因此购买和使用不同类型的内存（从小而快速的工作数据到大而慢的很少使用的数据）是有经济意义的。[图
    10-1](ch10.xhtml#ch10fig1)展示了本章中将讨论的每个内存层次的大致速度和容量。
- en: '![Image](../images/f0216-01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0216-01.jpg)'
- en: '*Figure 10-1: The memory hierarchy*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-1：内存层次结构*'
- en: 'These levels can be defined as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层次可以定义如下：
- en: '**Registers** Memory inside the CPU, as described in [Chapter 7](ch07.xhtml).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**寄存器** 位于CPU内部的内存，如[第7章](ch07.xhtml)所描述。'
- en: '**Cache** Memory outside but close to the CPU, which contains fast copies of
    primary memory.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**缓存** 位于CPU外部但靠近CPU的内存，包含主内存的快速副本。'
- en: '**Primary memory** Memory stored in an address space that is directly accessible
    by the CPU’s load and store instructions.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**主内存** 存储在地址空间中，可以通过CPU的加载和存储指令直接访问的内存。'
- en: '**Secondary memory** Memory not directly accessible to the CPU via its registers
    and address space, but that can be moved into primary memory by I/O to enable
    such access.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**次级内存** 是CPU无法通过其寄存器和地址空间直接访问的内存，但可以通过I/O传输到主内存中以实现访问。'
- en: '**Tertiary memory** Memory that isn’t directly connected to the address space
    or to I/O, but that can be mechanically connected to I/O without human intervention.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**三级内存** 是不直接连接到地址空间或I/O的内存，但可以通过机械方式连接到I/O，无需人工干预。'
- en: '**Offline memory** Memory that can be connected only to the computer with human
    intervention.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**离线内存** 是只能通过人工干预连接到计算机的内存。'
- en: According to Church’s definition of a computer, any machine that relies on fixed-length
    addresses—such as the Manchester Baby we built in [Chapter 7](ch07.xhtml)—isn’t
    quite a computer. A Church computer needs to be able to simulate any other machine,
    and to do this it needs to be able to ask for and get more storage as needed.
    Machines based on a CPU and bus with fixed-sized addresses can’t easily extend
    their memory beyond that fixed size, however. To get around this problem, and
    to allow for unlimited memory, we need to use memory levels below primary memory,
    such as the secondary and tertiary levels shown in [Figure 10-1](ch10.xhtml#ch10fig1).
    These lower levels aren’t addressed directly from the CPU, but instead are devices
    that connect to it through I/O modules.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 根据丘奇对计算机的定义，任何依赖固定长度地址的机器——比如我们在[第7章](ch07.xhtml)中构建的曼彻斯特宝宝——都不完全算是计算机。丘奇计算机需要能够模拟任何其他机器，并且为了做到这一点，它必须能够按需请求并获取更多的存储空间。然而，基于固定大小地址的CPU和总线构建的机器，无法轻松扩展超出该固定大小的内存。为了绕过这个问题，并支持无限的内存，我们需要使用主内存以下的内存级别，例如在[图10-1](ch10.xhtml#ch10fig1)中显示的次级和三级内存。这些较低级别的内存不能直接从CPU寻址，而是通过I/O模块与CPU连接的设备。
- en: Primary Memory
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主内存
- en: '*Primary memory* (aka *system memory*) is memory stored in an address space
    that’s directly accessible by the CPU’s load and store instructions. This includes
    RAM and ROM. Most modern machines use von Neumann architectures; remember, this
    means that the program and data are stored together in the same primary memory.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*主内存*（也称为*系统内存*）是存储在地址空间中的内存，这些地址空间可以被CPU的加载和存储指令直接访问。这包括RAM和ROM。大多数现代计算机使用冯·诺依曼架构；记住，这意味着程序和数据存储在同一主内存中。'
- en: In primary memory, each memory location is given a unique address. For example
    a 16-bit address space has 2^(16) = 65,536[10] unique addresses, numbered from
    0000[16] to FFFF[16]. Each address stores a fixed-size array of bits called a
    *word*. Often, but not always, the word length is chosen to be the same as the
    address length, such as storing 64-bit words in a 64-bit address space on a modern
    laptop. You saw a simple way to implement this structure using flip-flops in [Chapter
    6](ch06.xhtml); you saw how to attach it to a CPU directly in [Chapter 7](ch07.xhtml)
    and indirectly via a bus in [Chapter 9](ch09.xhtml).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在主内存中，每个内存位置都有一个唯一的地址。例如，一个16位的地址空间有2^(16) = 65,536[10]个唯一地址，编号从0000[16]到FFFF[16]。每个地址存储一个固定大小的位数组，称为*字*。通常，虽然不总是如此，字长被选择为与地址长度相同，例如在现代笔记本电脑的64位地址空间中存储64位字。你在[第6章](ch06.xhtml)中看到了实现这种结构的简单方法；在[第7章](ch07.xhtml)中你看到如何将其直接连接到CPU，在[第9章](ch09.xhtml)中则是通过总线间接连接。
- en: '*Bytes and Endianness*'
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*字节与字节序*'
- en: Related to the SI versus binary prefix debate is the question of whether to
    measure memory in bits (b), bytes (B), or words (W). Bits are the most basic unit,
    and they work well with SI units.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 关于国际单位制（SI）与二进制前缀的争论，涉及到是否应以比特（b）、字节（B）或字（W）来衡量内存。比特是最基本的单位，它与国际单位制单位配合使用效果很好。
- en: In modern use, a byte means 8 bits, and the term comes from the 8-bit era, when
    what is now known as a word was by definition 8 bits. One byte was what was stored
    at one memory address, and what was brought into one register of the CPU for processing.
    The term *byte* is supposed to suggest the CPU taking the smallest “bite” of memory
    to process. It was deliberately misspelled to avoid confusion with the term *bit*.
    “Byte” originally meant *any* such natural CPU size, ranging between 1 and 6 bits
    in early processors of the 1950s. It only later came to be standardized to mean
    8 bits.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代使用中，一个字节表示8位，这一术语来源于8位时代，当时现在所称的字是按定义为8位的。一个字节是存储在一个内存地址中的数据，也是被带入CPU的一个寄存器进行处理的数据。术语*字节*应该意味着CPU从内存中“咬取”最小的一块进行处理。为了避免与*比特*这一术语混淆，它故意拼写错了。“字节”最初是指任何这种自然的CPU大小，早期1950年代的处理器中，字节的位数范围从1到6位。直到后来才标准化为8位。
- en: In the 8-bit era, it was very natural to measure primary memory in bytes and
    what are now called kibibytes. You would compute the number of addresses, such
    as 2^(16) for addresses that are 16 bits long, then append the word *bytes* to
    this number to get the total addressable memory size. For example, a “64 kibibyte”
    machine such as the Commodore 64 had 2^(16) addresses containing 1 byte each.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在8位时代，用字节和现在所谓的千字节（kibibyte）来衡量主存储器是非常自然的。你会计算地址的数量，比如2^(16)表示16位长的地址，然后在这个数字后加上字“bytes”来得到总的可寻址内存大小。例如，一台“64千字节”的机器，如Commodore
    64，具有2^(16)个地址，每个地址包含1个字节。
- en: The byte really should have little or no relevance in the modern 64-bit age,
    in which words are 64 bits rather than 8 bits. If we were to store 64-bit words
    at each of 2^(32) = 4 gibi addresses, we would talk about having primary memory
    sizes such as “4 gibiwords.”
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代64位时代，字节的意义应该几乎没有，尤其是在现在字长是64位而不是8位的情况下。如果我们在2^(32) = 4 gibi地址处存储64位字，我们将讨论像“4
    gibiwords”这样的主存储器大小。
- en: However, most actual current machines *don’t* address memory per word. For historical
    reasons, they usually continue to address memory per byte, just as they did in
    the 8-bit era. This is called *byte addressing* and it means that a word on, say,
    a 32-bit architecture is stored across 4 bytes with separate addresses. Suppose
    we want to store a 32-bit word such as 12B4A85C[16]. We do this using 4 bytes
    containing 12[16], B4[16], A8[16], and 5C[16].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数实际的现代计算机*并不*按字存取内存。出于历史原因，它们通常仍然按字节存取内存，就像在8位时代那样。这被称为*字节寻址*，意思是，在例如32位架构中，一个字被分散存储在4个字节中，每个字节有独立的地址。假设我们要存储一个32位字，如12B4A85C[16]。我们可以使用4个字节，分别包含12[16]，B4[16]，A8[16]，和5C[16]。
- en: A standards war raged for decades over the order in which these bytes should
    be stored in memory addresses. The ordering is referred to as *endianness*. *Big
    endians* believe the bytes should be stored in the order (12[16], B4[16], A8[16],
    5C[16]) because this looks like the human-readable number 12B4A85C[16]. Big endians
    say this makes life easier and nicer for the humans who see architecture, including
    architects themselves and assembly programmers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些字节应该以何种顺序存储在内存地址中，经历了几十年的标准战争。这个顺序被称为*字节序*。*大端序*认为字节应该按照顺序（12[16]，B4[16]，A8[16]，5C[16]）存储，因为这看起来像人类可读的数字12B4A85C[16]。大端序认为，这种方式能让看到架构的人，包括架构师和汇编程序员，感到更轻松和更好。
- en: '*Little endians*, on the other hand, believe the number should be stored as
    (5C[16], A8[16], B4[16], 12[16]). This initially seems crazy to most Western people.
    In particular, if you string the bytes together in this order, you have the nonsensical
    number 5CA8B412[16] rather than the desired 12B4A85C[16]. However, little endians
    point out that such stringing is based on certain cultural prejudices.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，*小端序*认为数字应该按（5C[16]，A8[16]，B4[16]，12[16]）的顺序存储。这最初让大多数西方人觉得很疯狂。特别是，如果你按这个顺序将字节串联起来，你得到的将是没有意义的数字5CA8B412[16]，而不是期望的12B4A85C[16]。然而，小端序指出，这样的排序是基于某些文化偏见的。
- en: The West uses the Arabic decimal number system, which writes numbers with the
    highest power on the left and the lowest on the right. It imported this system
    unchanged from the original Arabic. But Arabic *text* is written and read from
    right to left, the opposite of Western text. In Arabic, a number string such as
    “24” is written the same, and has the same value, 24, as in the West, but it’s
    *read* from right to left as “four and twenty.” The zeroth column is the units,
    and the first column is the tens. This makes sense when arithmetic is performed
    using the number, because almost all arithmetic algorithms begin by operating
    on the zeroth column and move progressively up the higher-numbered columns. The
    numbers of these columns match the powers that the base is raised to—for example,
    the zeroth column is the units, or zeroth power.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 西方使用的是阿拉伯十进制数字系统，数字从左到右按从高到低的幂次排列。这个系统是从原始的阿拉伯数字中引入的，保持不变。但阿拉伯*文本*是从右到左书写和阅读的，正好与西方的文本相反。在阿拉伯语中，像“24”这样的数字字符串写法是相同的，数值也和西方一样是24，但它是从右到左读作“four
    and twenty”。零位列是单位，第一列是十位。这样做在进行算术运算时是有意义的，因为几乎所有的算术算法都是从零位列开始操作，并逐步处理较高的位列。这些列的数字与基础被提升的幂次相对应——例如，零位列是单位，或者说是零次幂。
- en: 'The little-endian system assigns numerical addresses so that the zeroth byte
    is at zero offset from the address of the word, and the *n*th byte is at an *n*
    byte offset. This can make arithmetic easier and faster for the machine in some
    cases. For example, if the machine is adding two words of different byte lengths
    (say, a short int plus a long int), it’s easy and quick to find the *n*th byte
    of each. Similar issues can also arise for words containing instructions of variable
    lengths: with little endianness, you can always be sure that the opcode is at
    zero offset rather than having to look for it. Little endianness is now dominant
    in commercial architectures, so it has effectively won the war.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 小端系统分配数值地址，使得第零字节位于字的地址零偏移处，第 *n* 个字节位于 *n* 字节的偏移位置。这在某些情况下可以使机器的算术运算更简便、更快速。例如，如果机器在加法运算中处理两个不同字节长度的字（比如，一个短整型和一个长整型），它就能快速且轻松地找到每个字节的
    *n* 位。同样的问题也可能出现在包含可变长度指令的字中：采用小端格式时，你总能确保操作码位于零偏移位置，而无需去寻找它。小端格式在商业架构中现在占主导地位，因此它实际上已经赢得了这场“战争”。
- en: '*Memory Modules*'
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*内存模块*'
- en: RAM and ROM often come in discrete modules that can be added and removed to
    change the amount of available memory. With a bus architecture, these modules
    can easily be attached and detached. For example, [Figure 10-2](ch10.xhtml#ch10fig2)
    shows one ROM module and two RAM modules on the same bus as a CPU.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: RAM 和 ROM 通常以离散模块的形式出现，可以通过添加和移除来更改可用内存的数量。采用总线架构时，这些模块可以很方便地连接或断开。例如，[图 10-2](ch10.xhtml#ch10fig2)
    显示了一个 ROM 模块和两个 RAM 模块与 CPU 共享同一总线的情况。
- en: '![Image](../images/f0219-01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0219-01.jpg)'
- en: '*Figure 10-2: A bus architecture including a CPU, two RAM modules, and a ROM
    module*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-2：包括 CPU、两个 RAM 模块和一个 ROM 模块的总线架构*'
- en: In general, there could be many modules of both RAM and ROM. All the RAM modules
    can see the same signals passing along the bus, but each module is configured
    with a different part of the address space, so only the single module that hosts
    the specified address will actually respond.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，RAM 和 ROM 都可以有多个模块。所有的 RAM 模块都可以看到沿总线传输的相同信号，但每个模块被配置为与地址空间的不同部分对应，因此只有托管指定地址的单个模块会作出响应。
- en: All bus modules—including memory and I/O modules—are usually manufactured to
    respond to some default address space, such as starting at address 0\. However,
    when they’re mounted onto a bus, these addresses need to be remapped to be unique
    when compared to the other modules. This remapping is done by digital logic components
    called *memory controllers*, which listen to the bus for global addresses and
    route them to the appropriate module, converting to the module’s own local addresses.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所有总线模块——包括内存和 I/O 模块——通常都会制造为响应某些默认的地址空间，比如从地址 0 开始。然而，当它们被安装到总线上时，这些地址需要重新映射，以便与其他模块进行比较时具有唯一性。这一重新映射是由称为
    *内存控制器* 的数字逻辑组件完成的，它们监听总线上的全局地址，并将其路由到相应的模块，转换为该模块的本地地址。
- en: '*Random-Access Memory*'
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*随机访问存储器*'
- en: '*Random access* means that any random location in memory can be chosen and
    accessed quickly, without some regions being faster to access than others. By
    contrast, something like a cassette tape or punch-card deck isn’t random access
    because it’s faster to access data in sequence than to fast-forward or rewind
    to a far-away location. While RAM stands for “random-access memory,” it’s a historical
    misnomer that doesn’t paint a full picture. By modern convention, RAM refers to
    memory that’s not only random access but also both readable and writable, as well
    as *volatile*, meaning its data is lost when the machine is powered off. Many
    ROMs are also random access, but they aren’t considered RAM under the conventional
    use of the term because they don’t fit the other parts of the definition.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机访问* 意味着可以快速选择并访问内存中的任何位置，而不需要某些区域比其他区域访问更快。相比之下，像磁带或打孔卡片那样的存储方式就不是随机访问，因为在顺序访问数据时，跳到远距离位置的速度通常比快进或倒带要慢。虽然
    RAM 代表“随机访问存储器”，但它是一个历史上的误称，并未全面描述其特点。根据现代约定，RAM 指的是不仅具有随机访问能力，还可以读写，并且是 *易失性*
    的内存，这意味着机器断电后数据会丢失。许多 ROM 也是随机访问的，但由于它们不符合该术语的其他定义，因此通常不被视为 RAM。'
- en: '**HISTORICAL RAMS**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**历史上的 RAM**'
- en: We’ve already discussed Babbage’s Analytical Engine RAM, which is still the
    foundation for RAM architecture today, in [Chapter 3](ch03.xhtml). In the Analytical
    Engine, each memory address corresponds to a stack of gears whose rotations represent
    a word. One address at a time can be physically connected to the bus. Once connected,
    any rotation of the gears will be transferred first to the linear motion of the
    bus, and then to rotation of a register in the CPU, and vice versa. Now let’s
    consider a few other historical examples of RAM.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第3章](ch03.xhtml)中讨论了巴贝奇的解析机内存，这也是今天RAM架构的基础。在解析机中，每个内存地址对应一堆齿轮的堆叠，其旋转代表一个字。一时间只能有一个地址与总线物理连接。一旦连接，任何齿轮的旋转都会首先传递到总线的线性运动，再传递到CPU中寄存器的旋转，反之亦然。现在让我们来看一些其他历史上的RAM实例。
- en: '**Acoustic Mercury Delay Line RAM**'
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**声学汞延迟线RAM**'
- en: In “From Combinatorial to Sequential Logic” on [page 144](ch06.xhtml#lev129),
    we discussed how the presence and absence of the audio feedback created by an
    electric guitar and amplifier feedback loop could be used to store 1 bit of information.
    This was, in fact, exactly how computer memory was implemented in the UNIVAC era,
    using mercury delay lines, as shown in the following figure.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在《从组合逻辑到顺序逻辑》一节中，[第144页](ch06.xhtml#lev129)我们讨论了由电吉他和功放反馈回路产生的音频反馈的有无如何用于存储1位信息。事实上，这正是UNIVAC时代计算机内存的实现方式，采用了汞延迟线，如下图所示。
- en: '![Image](../images/f0220-01.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0220-01.jpg)'
- en: A delay line was literally a microphone and speaker placed some distance apart
    and used to store a bit of information through feedback. By placing them at two
    ends of a tube and filling the tube with mercury, the speed of sound is delayed,
    so the tube can be made shorter than earlier versions using air.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟线实际上是将麦克风和扬声器放置在一定距离处，通过反馈来存储一位信息。通过将它们放置在管道的两端，并将管道填充汞，可以延迟声音的传播速度，从而使管道比使用空气的早期版本更短。
- en: In machines of this era, delay lines could be organized into an address space,
    as in the Analytical Engine. When the CPU executed a load or store, this would
    be implemented by making and breaking the electric circuits to connect the required
    delay line to the bus, disconnecting the others and placing a copy of the data
    onto the bus for transmission.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个时代的机器中，延迟线可以像在解析机中那样组织成地址空间。当CPU执行加载或存储操作时，会通过断开和连接电路来实现，将所需的延迟线连接到总线，断开其他线，并将数据的副本放到总线上进行传输。
- en: '**Williams Tube RAM**'
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**威廉姆斯管RAM**'
- en: The Manchester Baby was built to research a new type of RAM, known as the Williams
    tube. The technology, shown below, was conceived in 1946, based on the cathode
    ray tube (CRT), as found in old TV screens.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 曼彻斯特宝贝机是为研究一种新型RAM——威廉姆斯管而建造的。该技术如下面所示，诞生于1946年，基于旧电视屏幕中使用的阴极射线管（CRT）。
- en: '![Image](../images/f0221-01.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0221-01.jpg)'
- en: 'As with CRT screens, the Williams tube fires a stream of electrons in a beam,
    and uses adjustable magnets to deflect the beam to land on one pixel at a time,
    in a scanning pattern covering a screen. The screen is made from a fluorescent
    material, meaning that each pixel glows when absorbing the electron beam. Unlike
    CRT televisions and monitors, the Williams tube’s purpose was not as a human-readable
    display but as actual RAM storage. Pixels retain their charge and color for a
    short period of time after they’re hit by the beam. This means they can be used
    in a feedback system: we write a screen-full of pixels using the scanning beam,
    quickly read the screen’s state, and pass the data read off the screen back to
    the scanning beam to be written to the screen again. This refreshes the data on
    the screen, keeping it alive for as long as we like, rather than allowing the
    pixels to fade away.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与CRT屏幕一样，威廉姆斯管通过电子束发射一束电子流，并利用可调磁铁来偏转电子束，使其一次落在一个像素上，扫描模式覆盖整个屏幕。屏幕由荧光材料制成，这意味着每个像素在吸收电子束时会发光。与CRT电视和显示器不同，威廉姆斯管的目的是作为实际的RAM存储，而不是供人类读取的显示器。像素在被电子束击中后，会在短时间内保持其电荷和颜色。这意味着它们可以用作反馈系统：我们使用扫描电子束写入满屏像素，快速读取屏幕状态，然后将读取的数据传回扫描电子束，再次写入屏幕。这种方式刷新了屏幕上的数据，使其保持活跃，直到我们希望它保持为止，而不是让像素逐渐消失。
- en: The original Williams tube’s screen contained 32 words of 32 bits each, with
    each row of the screen being one word and each column of the screen being a bit
    within a word. Thus, the whole system stored 32×32 = 1,024 bits. Phosphor was
    used as the fluorescent material, which glows green when stuck by the electron
    beam.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的威廉姆斯管屏幕包含 32 个字，每个字为 32 位，每行屏幕为一个字，每列屏幕为字中的一个比特。因此，整个系统存储了 32×32 = 1,024
    位。荧光材料采用磷光体，当电子束撞击时，它会发出绿色光。
- en: '**Static RAM**'
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**静态 RAM**'
- en: The kind of RAM we saw previously in [Figure 6-22](ch06.xhtml#ch06fig22), made
    from flip-flops, is known as *static RAM* or *SRAM* (pronounced “es-ram”). Because
    SRAM is made from flip-flops (the same structures that are used to make CPU registers),
    it’s fast and expensive. The flip-flops are typically built from around four to
    six transistors each (depending on the flip-flop type and on how the logic gates
    are implemented). They have stable memory states, meaning they don’t have to be
    actively refreshed. They’re available for reading almost immediately after being
    written to. What sets SRAM apart from CPU registers is that SRAM is addressed,
    and CPU registers aren’t.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前在[图 6-22](ch06.xhtml#ch06fig22)中看到的由触发器构成的 RAM 被称为 *静态 RAM* 或 *SRAM*（发音为“es-ram”）。由于
    SRAM 是由触发器（与 CPU 寄存器相同的结构）构成的，它既快速又昂贵。触发器通常由大约四到六个晶体管组成（具体取决于触发器类型以及逻辑门的实现方式）。它们具有稳定的内存状态，这意味着它们不需要主动刷新。写入后几乎可以立即读取它们。SRAM
    与 CPU 寄存器的不同之处在于，SRAM 是寻址的，而 CPU 寄存器则不是。
- en: SRAM is typically used to implement caches, as we’ll discuss later in the chapter.
    It isn’t usually used for main memory, except in some specialized and expensive
    machines, such as high-end routers, where main memory access speed is critical.
    [Figure 10-3](ch10.xhtml#ch10fig3) shows an SRAM chip.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: SRAM 通常用于实现缓存，如本章后续所讨论的那样。它通常不用于主内存，除非在一些特殊且昂贵的机器中，例如高速路由器，在这些机器中，主内存访问速度至关重要。[图
    10-3](ch10.xhtml#ch10fig3) 显示了一个 SRAM 芯片。
- en: '![Image](../images/f0222-01.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0222-01.jpg)'
- en: '*Figure 10-3: An SRAM chip*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-3：一个 SRAM 芯片*'
- en: Cache chips like this may be placed between the CPU and RAM. Alternatively,
    a similar SRAM cache might be found on the same silicon as the CPU.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 类似这样的缓存芯片可以放置在 CPU 和 RAM 之间。或者，类似的 SRAM 缓存也可能与 CPU 在同一硅片上。
- en: '**Dynamic RAM**'
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**动态 RAM**'
- en: '*Dynamic RAM (DRAM)* is cheaper and more compact than SRAM, but slower. Instead
    of being made from flip-flops, it’s made using cheaper and slower capacitors.
    A *capacitor* is a component for storing electric charge. It consists of two metal
    plates separated by an insulator. Current can’t flow across the plates, but placing
    a current on them causes them to accumulate charge until they’re full of it. Capacitors
    don’t usually appear in CPU design; they’re a different kind of electronic component.
    One bit of DRAM storage is made from just one transistor plus one capacitor. Capacitors
    can be manufactured on silicon using similar masking processes to transistor manufacture.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*动态 RAM (DRAM)* 比 SRAM 更便宜且更紧凑，但速度较慢。它不是由触发器构成，而是使用更便宜且较慢的电容器。*电容器*是用于储存电荷的组件。它由两块金属板和一层绝缘物质隔开。电流不能穿过这两块板，但在其上施加电流会使它们积累电荷，直到它们充满电。电容器通常不会出现在
    CPU 设计中，它们是另一种电子元件。一个 DRAM 存储位由一个晶体管和一个电容器组成。电容器可以使用与晶体管制造相似的掩膜工艺在硅上制造。'
- en: As RAM, DRAM features the same addressing system as SRAM, and its circuit diagram
    has the same overall structure as SRAM, based on words stored at addresses. The
    difference is that the words are implemented with capacitors instead of flip-flops
    ([Figure 10-4](ch10.xhtml#ch10fig4)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 RAM，DRAM 采用与 SRAM 相同的寻址系统，其电路图与 SRAM 基于存储在地址中的字的总体结构相同。不同之处在于，字是由电容器而非触发器实现的（[图
    10-4](ch10.xhtml#ch10fig4)）。
- en: DRAM is structured as a 2D array of words or bytes, with each located at a “row”
    and “column.” The requested address is converted (by a memory controller chip)
    into two smaller addresses per row and per column, which are AND gated together
    using a single transistor at the combined address. This saves a huge amount of
    digital logic, but the work needed to split the address into two parts makes DRAM
    addressing slower than SRAM addressing.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: DRAM 结构为一个 2D 数组，由字或字节组成，每个字或字节位于一个“行”和“列”中。请求的地址由内存控制芯片转换为每行和每列的两个较小的地址，这些地址通过一个单一的晶体管在组合地址上进行与门操作。这节省了大量的数字逻辑，但将地址分成两部分所需的工作使得
    DRAM 的寻址速度比 SRAM 慢。
- en: Due to the nature of capacitors, reading the DRAM discharges it and destroys
    the stored information (as in the Analytical Engine’s RAM). Reading and writing
    the capacitor state is an analog process, which takes time to complete. The charge
    can also leak away over time, as capacitors are analog devices. To handle these
    related problems, DRAM must be periodically refreshed, for example, around every
    64 milliseconds on a 2018 DRAM. (The need to constantly refresh is the source
    of the “dynamic” in DRAM.) Like mercury lines and Williams tubes, a refresh reads
    the current state and then rewrites it a short time later. Refreshing must be
    timed carefully and may sometimes conflict with and stall a CPU read or write,
    which then has to wait until the refresh completes before trying again.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于电容器的特性，读取DRAM会使其放电并销毁存储的信息（就像分析引擎的RAM一样）。读取和写入电容器状态是一个模拟过程，需要一定的时间来完成。由于电容器是模拟设备，电荷也可能随时间泄漏。为了处理这些相关问题，DRAM必须定期刷新，例如，在2018年版的DRAM上，大约每64毫秒刷新一次。（不断刷新是“动态”DRAM的根源。）像水银线路和威廉姆斯管一样，刷新过程会读取当前状态，然后在短时间内重写它。刷新必须小心计时，有时可能与CPU的读取或写入发生冲突，导致CPU需要等待刷新完成后才能重新尝试。
- en: '![Image](../images/f0223-01.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0223-01.jpg)'
- en: '*Figure 10-4: A DRAM circuit, showing capacitors and addressing*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10-4：一个DRAM电路，展示了电容器和寻址*'
- en: DRAM benefits from *pre-charging*, roughly a way to “warm it up” just before
    it’s used; this avoids recharging conflicts with access. Hence, modern CPUs and
    memory controllers work together to try to predict—several instructions in advance—which
    memory should be “warmed up” before use.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: DRAM受益于*预充电*，这大致是一种在使用之前“预热”的方式；这样可以避免与访问发生冲突的重新充电。因此，现代的CPU和内存控制器会协作，尽量预测——提前几个指令——哪些内存在使用前应该被“预热”。
- en: Modern DRAM chips are usually packaged together on printed circuit board modules
    of around eight chips, each sharing part of an address space, as shown in [Figure
    10-5](ch10.xhtml#ch10fig5). These modules attach to a motherboard via a standard
    interface, as seen previously in the introduction ([Figure 2](fm03.xhtml#fig2)).
    Extra memory can be added to a desktop PC by adding more DRAM modules to its memory
    slots.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现代的DRAM芯片通常被打包在大约八个芯片的印刷电路板模块上，每个芯片共享一部分地址空间，如[图10-5](ch10.xhtml#ch10fig5)所示。这些模块通过标准接口连接到主板，如之前在介绍中所见（[图2](fm03.xhtml#fig2)）。可以通过将更多DRAM模块添加到台式机的内存插槽中，来增加额外的内存。
- en: '![Image](../images/f0223-02.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0223-02.jpg)'
- en: '*Figure 10-5: A DRAM module*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10-5：一个DRAM模块*'
- en: '*Single in-line memory modules (SIMMs)* have a 32-bit bus width, and they were
    standard in 1990s PCs. Double in-line memory modules (DIMMs) replaced SIMMs in
    the 2000s. They have a 64-bit bus width, and each stores many gigabytes. Double
    data rate (DDR) DRAM doubled the speed of DRAM through technology that enables
    data to transfer on both the rising and falling edges of the clock. This doubles
    the bandwidth (as *bandwidth = bus width × clock speed* × *data rate*). SIMMs
    and DIMMs have gone through several improved standards that can be visually distinguished
    by the different notch positions, designed so they can be inserted only into the
    right type of sockets.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*单列内存模块(SIMMs)*具有32位总线宽度，它们曾是1990年代PC的标准。双列内存模块(DIMMs)在2000年代取代了SIMMs。它们具有64位总线宽度，每个DIMM存储多达几千兆字节。双倍数据速率(DDR)DRAM通过一种使数据能够在时钟的上升沿和下降沿同时传输的技术，使DRAM的速度翻倍。这使得带宽翻倍（因为*带宽
    = 总线宽度 × 时钟速度 × 数据速率*）。SIMMs和DIMMs经历了几个改进的标准，可以通过不同的缺口位置直观区分，设计目的是使它们只能插入到正确类型的插槽中。'
- en: '**Error Correction Code RAM**'
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**错误更正码RAM**'
- en: RAM, like other chips, has become so miniaturized that the component size is
    getting close to atomic scales. At these scales, quantum effects and particle
    physics come into play. Quantum effects can include various types of inherent
    noise and uncertainty about the location of particles used in memory. Cosmic rays
    are random particles most commonly including electrons, alpha particles, and muons,
    hurtling at high speed through space from either the sun or elsewhere in the galaxy.
    If a cosmic ray collides with a sensitive component of RAM, then it can corrupt
    it and flip its Boolean state.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: RAM像其他芯片一样，已经被微型化到接近原子尺度。在这些尺度下，量子效应和粒子物理学开始发挥作用。量子效应可能包括各种类型的固有噪声和关于用于内存的粒子位置的不确定性。宇宙射线是最常见的随机粒子，通常包括电子、α粒子和μ子，它们以高速穿越太空，来源可能是太阳或银河系的其他地方。如果宇宙射线与RAM的敏感组件发生碰撞，它可以破坏该组件并翻转其布尔状态。
- en: '*Error correction code RAM (ECC-RAM)* has extra chips on the DIMM that store
    extra copies or checksums of the data and use them to automatically correct such
    flips at the hardware level. ECC-RAM is primarily used in space applications where
    computers are located outside the protection of Earth’s atmosphere and so are
    more exposed to cosmic rays. As its price falls, it may also be found in other
    high-value, safety-critical systems on the ground.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*错误校正码内存 (ECC-RAM)* 在 DIMM 上有额外的芯片，这些芯片存储数据的额外副本或校验和，并利用它们在硬件级别自动修正类似的错误。ECC-RAM
    主要应用于太空领域，那里计算机位于地球大气层的保护之外，因此更容易受到宇宙射线的影响。随着价格的下降，ECC-RAM 也可能出现在其他高价值、关键安全系统中。'
- en: '**THE ROWHAMMER VULNERABILITIES**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**ROWHAMMER 漏洞**'
- en: '*Rowhammer* refers to a set of memory hardware vulnerabilities currently affecting
    computer security. DRAM capacitors are now so small and tightly packed that their
    electric fields may affect neighboring rows of memory. Security researchers have
    begun to exploit this effect to read and write memory belonging to target programs.
    The researchers write new programs and arrange for them to be stored in a region
    of memory physically next to, for example, the addresses containing your online
    banking password, owned by the target program. They then load and store data in
    their own program’s locations, in ways that are likely to trigger physical interactions
    between the capacitors in their own and the target’s memory. For example, this
    could include putting their own addresses into states likely to cause cosmic ray–style
    errors in the target memory. Or they might be able to infer the state of the target
    memory by observing similar errors or small time delays in their own reads and
    writes caused by the target’s capacitor states.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*Rowhammer* 指的是当前影响计算机安全的一组内存硬件漏洞。DRAM 电容器现在非常小且密集，以至于它们的电场可能会影响邻近的内存行。安全研究人员已经开始利用这一效应来读取和写入目标程序的内存。研究人员编写新程序，并安排将它们存储在内存中物理上紧邻目标程序的区域，例如存储在线银行密码的地址。然后，他们在自己程序的位置加载并存储数据，方式上可能会触发自己与目标内存中电容器的物理交互。例如，这可能包括将自己的地址置于可能引起目标内存中类似宇宙射线风格错误的状态。或者，他们可能通过观察自己读写中的相似错误或由于目标电容器状态引起的微小时间延迟来推断目标内存的状态。'
- en: Research is currently ongoing into defenses against rowhammer attacks. Approaches
    include use of ECC-RAM to correct any maliciously induced cosmic ray–style errors,
    use of higher memory refresh rates, and software-level solutions such as operating
    system code to randomize the locations of programs in memory and prevent deliverable
    co-location of code next to targets.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的研究正在进行，以防御 rowhammer 攻击。方法包括使用 ECC-RAM 来修正任何恶意引发的宇宙射线风格错误，使用更高的内存刷新率，以及通过操作系统代码等软件层面的解决方案来随机化程序在内存中的位置，从而防止代码与目标程序的交付性共同定位。
- en: '*Read-Only Memory*'
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*只读存储器*'
- en: '*Read-only memory (ROM)* traditionally refers to memory chips that can only
    be read from, not written to, and that are pre-programmed with permanent collections
    of subroutines by their manufacturer, then mounted at fixed addresses in primary
    memory. ROMs have since evolved to include other types of memory that don’t fit
    this traditional definition or name very well or at all.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*只读存储器 (ROM)* 传统上指的是只能读取而不能写入的内存芯片，这些芯片由制造商预先编程，内含永久性的子程序集合，然后被安装在主内存中的固定地址处。ROM
    随着时间的推移，已经发展出包括其他类型的内存，这些内存并不完全符合这一传统定义或名称。'
- en: 'First, the ROM versus RAM distinction has never been a true partition because,
    as noted earlier, ROM chips are random access, just like RAM: they’re mounted
    in the main address space and accessing any address within them takes the same
    amount of time. The difference between ROM and RAM is that RAM is readable and
    writable, while ROM is traditionally only readable.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，ROM 与 RAM 的区别从未真正存在，因为如前所述，ROM 芯片和 RAM 一样是随机访问的：它们被安装在主地址空间中，访问其中的任何地址所需的时间都是一样的。ROM
    和 RAM 的区别在于，RAM 是可读写的，而 ROM 传统上只是可读的。
- en: Second, ROMs have evolved over time to allow increasing ease of rewriting, with
    programs stored in ROM that are able to be rewritten in some way now known as
    *firmware*. The following sections describe the main steps of this evolution,
    as illustrated in [Figure 10-6](ch10.xhtml#ch10fig6).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，ROM 随着时间的推移发展出了越来越容易重写的特性，存储在 ROM 中的程序现在可以以某种方式被重写，这些程序通常被称为 *固件*。以下部分描述了这一演变的主要步骤，如[图
    10-6](ch10.xhtml#ch10fig6)所示。
- en: '![Image](../images/f0225-01.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0225-01.jpg)'
- en: '*Figure 10-6: Evolution of ROMs: MROM, PROM, EPROM, EEPROM, and SD card–mounted
    flash. Note that, unusually, the actual silicon is visible in the EPROM package,
    through a transparent window, which is needed to expose it to light.*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10-6：只读存储器的演变：MROM、PROM、EPROM、EEPROM和SD卡挂载的闪存。请注意，不同寻常的是，EPROM封装中实际的硅片是可见的，通过一个透明窗口，可以将其暴露在光下。*'
- en: Let’s go through a few of these types of ROM.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来了解一下这些不同类型的只读存储器。
- en: '**Mask ROM**'
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**掩膜只读存储器（Mask ROM）**'
- en: '*Mask ROM (MROM)* is ROM whose contents are programmed using photo-lithography
    by the manufacturer. It remains read-only forever and can’t be overwritten. If
    you want to update an MROM chip, you have to remove it, throw it away, and insert
    a brand new chip containing the new content. Photolithography is very expensive,
    so MROMs are difficult to produce and to upgrade.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*掩膜只读存储器（MROM）*是一种由制造商通过光刻技术编程的只读存储器。它永远是只读的，无法覆盖。如果你想更新MROM芯片，你必须将其取出、丢弃，然后插入一个包含新内容的全新芯片。光刻技术非常昂贵，因此MROM的生产和升级都很困难。'
- en: '**Programmable ROM**'
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**可编程只读存储器（Programmable ROM）**'
- en: '*Programmable ROM (PROM)* was a great advance over MROM. Similar to the programmable
    logic arrays (PLAs) discussed in [Chapter 5](ch05.xhtml), PROMs are chips manufactured
    by photolithography to include a generic circuit with many fuses. The programmer
    can then selectively blow the fuses to create different structures. While PLAs
    enable arbitrary digital logic networks to be burned in this way, PROMs instead
    contain a fixed structure of addresses and words, and allow only the bits composing
    the words to be burned, to make a ROM. Usually each bit contains 1 when its fuse
    is intact and changes to 0 if its fuse is blown. Like PLAs, PROMs can never be
    erased once they’re programmed.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*可编程只读存储器（PROM）*是对MROM的巨大改进。与[第5章](ch05.xhtml)中讨论的可编程逻辑阵列（PLAs）类似，PROM是通过光刻技术制造的芯片，包含一个通用电路和多个保险丝。程序员可以选择性地熔断保险丝来创建不同的结构。虽然PLAs使得可以以这种方式烧录任意数字逻辑网络，但PROM则包含一个固定的地址和字结构，只允许烧录组成字的位来制作只读存储器。通常，每个位在其保险丝完好时包含1，若其保险丝熔断则变为0。像PLAs一样，PROM一旦编程后就不能被擦除。'
- en: '**Erasable Programmable ROM**'
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**可擦除可编程只读存储器（Erasable Programmable ROM）**'
- en: '*Erasable programmable ROM (EPROM)* is like PROM, but the chip’s data can be
    erased using ultraviolet light. Then new data can be burned on. This cycle can
    be repeated many times. Although the erasing process was quite complex, requiring
    that you take the chip out of the computer and put it in a light box, it was still
    something you, a skilled end-user customer, could do without needing the computer
    manufacturer.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*可擦除可编程只读存储器（EPROM）*类似于PROM，但该芯片的数据可以通过紫外线照射来擦除。然后可以重新写入新的数据。这个过程可以反复进行很多次。尽管擦除过程相当复杂，需要将芯片从计算机中取出并放入光照盒中，但这仍然是你作为一名熟练的最终用户客户可以做的，无需计算机制造商的帮助。'
- en: '**Electrically Erasable Programmable ROM**'
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**电可擦可编程只读存储器（Electrically Erasable Programmable ROM）**'
- en: '*Electrically erasable programmable ROM (EEPROM)* is like EPROM in that you
    can wipe the entire chip and rewrite it, but here you only need to use electricity
    to erase and reprogram. This removes the need to physically manipulate the ROM;
    it can remain inside the computer. EEPROM is used today in ROMs that allow their
    firmware to be upgraded. If you’ve ever done a firmware update, you’ll have seen
    that it can be done entirely in software, without having to physically touch anything.
    You wouldn’t want to be updating firmware every day, but maybe once per year or
    whenever a bug fix has been found.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*电可擦可编程只读存储器（EEPROM）*类似于EPROM，你可以擦除整个芯片并重新写入它，但在这里你只需要使用电流来擦除和重新编程。这消除了物理操作ROM的需要；它可以保持在计算机内部。今天，EEPROM被用于那些可以升级固件的ROM。如果你曾经进行过固件更新，你会看到它完全可以通过软件完成，而无需物理接触任何东西。你不希望每天都更新固件，但可能每年更新一次，或者当发现有
    bug 修复时。'
- en: '**Flash Memory**'
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**闪存（Flash Memory）**'
- en: '*Flash memory* is EEPROM that can be erased and rewritten block-wise, meaning
    you can selectively wipe and rewrite just one small part, or block, of the memory
    at a time. This way you can leave most of the ROM intact, unlike with regular
    EEPROM, where you have to wipe and rewrite an entire chip of ROM at a time, as
    in a firmware update. Flash memory makes it much easier to rewrite portions of
    ROM frequently, while the chip is online, making it more feasible for day-to-day
    storage, functioning almost like RAM in some cases.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Caches
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A *cache* is an extra layer in the memory pyramid between the fast registers
    of the CPU and the slower RAM. It stores copies of the most heavily used memory
    contents, making them available for quick retrieval. (*Cache* is an archaic word
    for a store of items such as food, weapons, or pirate treasure.) Without a cache,
    RAM would connect straight to the CPU, either directly, as discussed in [Chapter
    7](ch07.xhtml), or using a bus with control (C), address (A), and data (D) lines,
    as discussed in [Chapter 9](ch09.xhtml) and summarized in [Figure 10-7](ch10.xhtml#ch10fig7).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0226-01.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-7: A basic CPU, bus, and RAM architecture*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this kind of cacheless architecture is that most programs need
    to access RAM frequently, but the capacitors that implement DRAM are slower than
    the flip-flops that implement the CPU’s registers. RAM thus becomes a major bottleneck
    for system speed. It’s no use having a fast, gigahertz CPU if the RAM is running
    orders of magnitude slower and the CPU has to wait around for each load and store
    to complete. Adding an SRAM-based cache made from flip-flops between the CPU and
    RAM, as shown in [Figure 10-8](ch10.xhtml#ch10fig8), helps avoid these bottlenecks.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0227-01.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-8: A basic CPU, bus, and RAM architecture with a cache in between*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: When the CPU needs to load some data, the cache checks if it has it, and returns
    it quickly if so. If not, the cache refers to the next memory level down (in [Figure
    10-8](ch10.xhtml#ch10fig8), RAM) and fetches the data from that level. Caching
    can also occur at *all* levels of the memory hierarchy, from registers to hard
    disks and jukeboxes (more on the latter in the “Tertiary Memory” section). However,
    it’s most commonly considered at the primary memory level, as we’re discussing
    here, between the registers and the main DRAM memory.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Initial designs began with a single cache, made from SRAM. More recent machines
    have made use of Moore’s law for transistor density to fill silicon with larger
    caches and more levels of cache. It’s common today to have at least three cache
    levels, called L1, L2, and L3, as in [Figure 10-9](ch10.xhtml#ch10fig9).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0227-02.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-9: A basic CPU, bus, and RAM architecture with L1, L2, and L3 caches*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: All these cache layers between CPU and DRAM memory are typically made in SRAM,
    but they have different operations policies that trade off size and speed in their
    different digital logic implementations. Historically, caches lived on dedicated
    chips outside the CPU. While lower levels still do this, a major trend is to move
    bigger and higher cache levels onto the CPU silicon itself.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the caches of your machines helps you write faster programs. Typically,
    each level of cache is 10 times faster than the one below it, so when you fill
    a level you’ll see a sudden slowdown in memory access. If you know the cache sizes,
    you can redesign your code to keep data in use within known cache-level limits
    to benefit from their speed.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '*Cache Concepts*'
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Caches are based on the *principle of locality*, which states that only a small
    amount of memory space is being accessed at any given time, and values in that
    space are being accessed repeatedly. It’s therefore useful to copy recently accessed
    values and their neighbors from larger, slower memory to smaller, faster memory.
    There are several different ways to think about “neighbors” and “locality.” *Temporal
    locality* is the property that values tend to be accessed repeatedly at nearby
    times. *Sequential locality* is the property that some sequences tend to be re-accessed
    in the same order multiple times. *Spatial locality* is the property that values
    nearby in memory tend to be accessed together. These concepts apply to both instructions
    and data, often arising due to loops and subroutines.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Cache memory is made of many *cache lines*. Each line contains a *block* with
    copies of several contiguous words from memory, as well as a *tag*, an address
    or other identifier describing which memory location has been copied into the
    block. Each line also has a *dirty bit* that tracks whether the CPU has changed
    the value in the cache, making it different from the equivalent value in memory.
    [Table 10-1](ch10.xhtml#ch10tab1) shows a few example cache lines.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 10-1:** Cache Lines'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '| **Tag** | **Block** | **Dirty bit** |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
- en: '| `$08F4` | `01101100 01101100 10011010` | `1` |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
- en: '| `$2AD5` | `10010101 11100110 00110110` | `0` |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: Each cache line shown in the table has a block of three 8-bit words, a tag consisting
    of the full address from a 16-bit address space, and a dirty bit. The 1 dirty
    bit for the first line indicates it’s been updated, while the 0 dirty bit for
    the second line indicates it hasn’t.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: We don’t cache individual addresses, but rather lines because it’s very cheap
    to move around larger chunks of memory rather than individual words. By bringing
    in whole lines around a target word, we exploit spatial locality—data and programs
    in neighboring locations are likely to be used next. The line prepares for this.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'Some cache systems use “hash functions” to choose a location in the cache for
    storing a piece of data, usually based on the data’s address in lower-level memory.
    A *hash function* is a many-to-one function that maps a big input number to a
    smaller output number, the *hash value*. It’s not usually possible to recover
    the original value from the hash value. For example, a function that takes the
    last two hex digits of a hex number is a simple hash function: *hash*(9A8E[16])
    = 8E[16]. The function that performs a Boolean AND of all binary digits in a number
    is another hash function: *hash*(01101001[2]) = 0&1&1&0&1&0&0 = 0\. A commonly
    used hash function for caches is to compute the value of an address modulo the
    number of available lines in the cache.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Finding an item in a cache is known as a *hit*. Not finding an item in a cache
    is known as a *miss*. When a miss occurs, we have to go back to the underlying
    memory and find the item there instead, usually making a new copy in the cache
    for future use. The *hit rate* is the ratio of hits to attempts (hits and misses
    together). This measures the proportion of cache lookups that are successful.
    The *miss rate* is the ratio of misses to attempts. This measures the proportion
    of cache lookups that are unsuccessful. The *hit time* is the time required to
    access requested data if a hit has occurred, and the *miss penalty* is the time
    required to process a miss.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: A cache has only a limited number of lines, and they quickly fill up as we store
    cached copies of everything that we access from the underlying memory. Once the
    cache is full, we’ll continue to request new addresses. These will initially miss,
    but temporal locality suggests that these new addresses are more likely to be
    reused than the older ones in the cache. We should therefore choose lines in the
    cache to overwrite, discarding their previously cached addresses and replacing
    them with the new ones. The contents of the overwritten lines are called *victims*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a cache structure, we need algorithms, implemented in fast digital
    logic, to manage it. We need to decide how to best make use of the available lines,
    and how to create and look up tags. As with most digital logic design, there will
    be trade-offs between methods that are simple and methods that are fast. The latter
    tend to require more silicon, making them more complex, error-prone, and expensive.
    Let’s take a look at a few options for using caches.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '*Cache Read Policies*'
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reading from a cache is a simpler task than writing to it, so we’ll first study
    some options for cache read algorithms.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '**Direct Mapped**'
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Direct mapping* is the simplest, easiest, and cheapest cache read policy to
    implement and understand. It’s sketched out in [Figure 10-10](ch10.xhtml#ch10fig10).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0229-01.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-10: A direct mapping cache read policy (showing lookup and caching)*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: In essence, the line where we store or look for a tag is addressed using a fixed
    hash of the tag. A line with this tag will only ever be stored at a single location.
    If multiple lines compete for the location, the new one will replace the older
    one. For example, suppose we load from address 67AB[16]. We might compute *hash*(67AB[16])
    = 4[16], which means that this address and its contents will be cached in line
    4[16], victimizing anything that was previously on this line.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: The drawback is that direct mapping can’t keep multiple in-use addresses in
    cache if they share the same hash. Suppose our program has a tight loop that reads
    and writes the two alternating addresses 67AB[16] and 12C9[16] many times. The
    problem here is that *hash*(67AB[16]) = *hash*(12C0[16]) = 4[16]. Both addresses
    will continually fight and victimize one another, overwriting line 4[16], even
    if no other addresses or cache lines are being used in the loop at all. In such
    a case, the cache will give no benefit at all, as every attempt will miss.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '**Fully Associative**'
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To fix the problem with direct mapping, we’d like to have addresses use different
    cache lines depending on how in-use our lines are, so that we victimize lines
    that are the least used, as sketched out in the *fully associative cache* of [Figure
    10-11](ch10.xhtml#ch10fig11).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0230-01.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-11: A fully associative cache sketch*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Here, each line of cache RAM is given its own digital logic block, including
    a comparator, multiplexer, and OR arrays. Only three such blocks are shown for
    illustration purposes, but for a 256-line cache, for example, there would be 256
    such blocks, all running in parallel.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to be able to store a tag, block, and dirty bit on *any* available
    line and be able to find it quickly. Caching is the easy part here: we just create
    some digital logic to count how much use each line is getting and to pick out
    the line with the lowest count.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The cache lookup is the harder part. In direct mapping, we just computed the
    same hash function as we used for caching, to tell us at which line to find a
    desired address. Now it could be anywhere in the cache, so we need to add lots
    of extra digital logic to check each of the lines’ tags for a match with the desired
    one and activate the matching line if it exists. Doing this in parallel (which
    is the only realistic way to make this fast enough to be useful) requires *N*
    copies of this matching digital logic, one for each of the *N* lines of cache,
    making it a much larger and more energy-consuming beast.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '**Set Associative**'
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Set associative* cache reading is an attempt to get the best of both of the
    above methods. Here we partition the *N*-line cache into several smaller sets
    of lines. We use hashing on addresses to hash to a set number, rather than a line
    number. During caching we find the set number from this hash, similar to the direct
    mapping approach, then choose as the victim the line within this set that has
    the least usage, similar to the fully associative approach. During lookup we again
    find the set number from the hash, then we use parallel matching checks on all
    items in just the one set to quickly find the matching line.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*集合关联*缓存读取是一种试图同时兼得上述两种方法优点的做法。在这里，我们将*N*行缓存分成几个较小的行集合。我们使用地址哈希来计算一个集合编号，而不是计算行号。在缓存过程中，我们通过这个哈希找到集合编号，类似于直接映射方法，然后选择该集合中使用最少的行作为替换行，类似于完全关联方法。在查找时，我们再次通过哈希找到集合编号，然后在该集合中并行匹配所有项，以快速找到匹配的行。'
- en: This approach means we only have to activate the comparators within a single
    set, rather than the entire cache, but we still avoid the direct-mapped problem
    of tight loops sharing hash values. In practice, this is often found to be a nice
    balance.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法意味着我们只需要激活单个集合中的比较器，而不是整个缓存，但我们仍然避免了紧密循环共享哈希值的直接映射问题。实际上，这通常被认为是一种良好的平衡。
- en: '*Cache Write Policies*'
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*缓存写入策略*'
- en: Caches become a bit more complicated when we do stores because a store changes
    the state of the memory. Suppose we’ve recently loaded an integer 17 from address
    540A[16] and cached a copy during the load. We want to increment this integer
    to 18 and store the result back at 540A[16]. Due to the locality principles, it’s
    likely that we’ll continue to both load and store from 540A[16] in the near future,
    so rather than store 18 directly in 540A[16], it may be faster to store it only
    in the cache line that’s currently caching 540A[16]. This means that all the future
    loads and stores can just hit the cache and don’t need to go to main memory.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进行存储操作时，缓存会变得更加复杂，因为存储操作会改变内存的状态。假设我们最近从地址540A[16]加载了一个整数17，并在加载时缓存了一个副本。我们想将这个整数递增到18，并将结果存回到540A[16]。由于局部性原则，我们很可能在不久的将来继续从540A[16]加载和存储，因此与其直接将18存储到540A[16]，不如只将它存储在当前缓存540A[16]的缓存行中。这样，所有未来的加载和存储操作都可以直接命中缓存，而不需要访问主存。
- en: 'The problem is that eventually this line will be victimized and we’ll lose
    all the changes we’ve made to the value; the main memory still contains the old
    value of 17\. To avoid this, at some point we need to copy the modified value
    back to main memory. The dirty bit shown earlier in [Table 10-1](ch10.xhtml#ch10tab1)
    tracks whether this needs doing. It’s set to 0 if the value in the line is the
    same as the value in memory, or to 1 if the value in the line has been updated
    but the value in memory hasn’t. Algorithms called *cache write policies* use this
    dirty bit to manage the copying back to memory. Let’s look at two different approaches:
    write-back and write-through.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，最终这行数据会被替换，我们会丢失对值所做的所有更改；主存中仍然包含旧值17。为了避免这种情况，在某个时刻我们需要将修改后的值复制回主存。前面在[表10-1](ch10.xhtml#ch10tab1)中显示的脏位会跟踪是否需要进行此操作。如果缓存行中的值与内存中的值相同，脏位被设置为0；如果缓存行中的值已更新，但内存中的值没有变化，脏位则设置为1。名为*缓存写入策略*的算法利用这个脏位来管理回写到内存。我们来看两种不同的方式：写回和写穿透。
- en: '**Write-Back**'
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**写回**'
- en: '*Write-back* is the simpler cache writing method: it copies the contents of
    the cache block back to RAM only when the line is victimized. This is relatively
    slow, however, because victimization occurs only when an instruction is in a rush
    to get executed. We get told to start writing back only once the victimization
    has been announced, and the victimizing instruction will now have to wait for
    us to do a slow RAM access before it can overwrite our victim line.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*写回*是更简单的缓存写入方法：只有当缓存行被替换时，它才会将缓存块的内容复制回RAM。然而，这相对较慢，因为替换只有在指令急于执行时才会发生。我们被要求在替换被宣布后开始写回，而替换的指令将不得不等待我们进行一次缓慢的RAM访问，才能覆盖我们的替换行。'
- en: '**Write-Through**'
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**写穿透**'
- en: '*Write-through* is a potentially faster alternative to write-back, although
    it uses more resources. In write-through, we don’t wait until our line is victimized
    to copy our line’s block back to RAM; rather, we do it multiple times, continually,
    in the background, using digital logic attached to the cache line and bus. This
    logic acts similarly to an application like SyncThing or Dropbox, continually
    looking out for any changes in the cached version and copying them back to the
    main version in RAM. This doesn’t create extra work for the CPU, as the extra
    digital logic is located on the cache itself. It does, however, lead to more traffic
    on the bus, as we’re sending these updates many more times than with the write-back
    approach.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '*Advanced Cache Architectures*'
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consider how caches should interact with the advanced CPU developments of [Chapter
    8](ch08.xhtml). Pipelined CPUs need to care a lot about cache misses, as they
    form another possible hazard. An efficient pipeline may be timed to assume that
    memory accesses will be cached, and if there’s a miss they’ll need to stall or
    otherwise handle this hazard.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: You saw in [Chapter 8](ch08.xhtml) how branch prediction attempts to guess the
    flow of a program to enable pipelines and out-of-order execution to go more smoothly.
    This can be used in conjunction with caching to *preemptively* fetch and store
    data—that is, before the actual load and store instructions are reached. These
    instructions take much longer to execute than in-CPU operations, so it’s useful
    to initiate them early. CPUs can look ahead in the program to try to guess which
    parts of main memory are likely to be needed many instructions down the line,
    and start caching them in advance so the CPU fetches will be faster.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned, each layer of the cache—L1, L2, and L3—provides roughly a tenfold
    speedup over the layer below it, so the potential gain from preemptively moving
    data higher up in the memory hierarchy isn’t trivial. The caches can always be
    rolled back and the CPU stalled if preemption gets it wrong. It’s not the end
    of the world if we bring the wrong data into the cache: the cache is a big place,
    and it’s okay to change what’s in it.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Due to the row-column structure of DRAM addressing, it’s faster to read multiple
    items in a single DRAM row all at once rather than individually. (Once a row is
    activated, it’s almost free to read many columns versus a single one.) Hence,
    modern DRAM controllers will typically work in harmony with the cache to move
    large DRAM rows into cache lines.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Cache writes can unnecessarily slow down a system if we know in advance that
    the data won’t need to be read again soon. In this case, writing to the cache
    and then transferring to main memory can be slower than just writing directly
    to main memory. Modern CPUs may provide special instructions for cacheless writing,
    which canny programmers and compiler writers can use to make programs faster.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: It’s been found empirically that L1 caches work more smoothly if they’re split
    into two separate, parallel caches, one for instructions and one for data. This
    can occur in Harvard architectures, where instructions and data are already separated
    in RAM, but also in von Neumann architectures, where instructions and data can
    be distinguished by which part of the CU is requesting them (instructions are
    requested during the fetch stage, while data is requested during the execute stage).
    This separation occurs only at L1, with lower cache levels sharing instructions
    and data, as in [Figure 10-12](ch10.xhtml#ch10fig12).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0233-01.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-12: A basic CPU, bus, and RAM architecture with separate L1 caches
    for instructions and data, and shared L2 and L3 caches*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Separating the instructions and data at the L1 level appears to be effective
    because both data and programs exhibit spatial locality individually, but with
    little locality between them. Also, instructions aren’t usually overwritten, while
    data often is, so separating out the instructions can simplify the cache write
    process.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Secondary and Offline Memory
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Secondary memory* is memory that can quickly be brought into addressed memory
    space via I/O. Data items in secondary memory don’t have addresses in the primary
    memory address space. Rather, they’re accessed via I/O, usually via an I/O module
    that *does* sit in the primary address space and relays requests to the secondary
    storage. Secondary storage is sometimes called *online storage* to emphasize that
    it’s powered, active, and available whenever the computer is on.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '*Offline memory* is that which can’t automatically be loaded into primary memory
    without *manual* human interventions. Often this includes secondary memory media
    that are physically ejectable and replaceable, such as tapes, discs, and USB devices.
    These media are secondary memory when connected to the computer, and offline memory
    when disconnected. Offline memory is typically used for backup and archival purposes,
    as well as for transportation. The fastest way to move petabytes of data around
    the world is still to put it on a truck as offline memory and drive it to its
    destination.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Secondary and offline memory should really nowadays be measured in bits and
    SI units—for example, describing an “8.8 terabit hard disk” instead of a “1 tebibyte
    hard disk.” This is because they aren’t part of primary memory address space and
    so aren’t addressed using primary memory’s word or byte addresses. The concept
    of bytes is even less relevant here than in modern primary memory. However, as
    primary memory is still often byte-addressed and measured in bytes, most people
    still have a better feel for sizes in bytes rather than bits, so they choose to
    measure secondary memory in the same units.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Secondary (and offline) memory is usually characterized by requiring some mechanical
    motion to look up data, rather than being random access. This includes scrolling
    through tape or spinning discs made from various materials. We’ll look at some
    details of these technologies next.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '*Tapes*'
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Tapes* are one-dimensional data stores that must be scrolled left or right
    to locate a required datum. You can think of human-written paper scrolls, like
    the Torah, as the original tapes. Tapes aren’t random access because a reading
    device has a position at one point in the tape, and it takes longer to move the
    tape (or the reader) to access a far-away location than a nearby location. Fast
    algorithms using tape storage need to take this structure into account and optimize
    memory access to reduce large address jumps.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '**Punch Cards**'
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Punch cards* are the original computational secondary storage, as used in
    the Jacquard loom and Analytical Engine (seen in [Figure 1-11](ch01.xhtml#ch01fig11)).
    They continued to be used in IBM Hollerith machines, and were used to store and
    read programs for early electronic machines of the 1960s. Occasional industrial
    use continued even into the 1980s, and allegedly at least one UK council may still
    be using them today. In punch cards, binary digits of data are represented by
    the presence or absence of holes punched or not punched at a series of physical
    locations on a card or piece of paper. The holes are usually about the size made
    by the desktop hole punchers you buy to file your paper documents into ring binders.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Cards are 2D, having rows and columns. Typically each row stores one word, with
    their row numbers acting as addresses (in a secondary address space, not primary
    RAM addresses). Conceptually, and sometimes physically, decks of cards are *chained*
    together to make what is really a 2D tape.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '**Punched Tape**'
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Punched tape* is an alternative to punch cards. Such tapes were used by the
    British Post Office, formed the inspiration for the Turing Machine, and were also
    used in the Colossus, as seen in [Figure 1-22](ch01.xhtml#ch01fig22). Depending
    on your point of view, tape is conceptually simpler than cards because it’s just
    a single 1D row of bits; or it’s more complex than cards because you have to worry
    more about aligning and reading words, which on cards are easily presented as
    rows.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '**Magnetic Tape**'
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Magnetic tape* was developed in the 1920s for analog audio recording in studios,
    commercialized for home use as 8-track systems in the 1960s, then used widely
    in 4-track compact cassettes during the 1980s. Analog magnetic tape was also widely
    used in the 1980s for home video recordings, following one of the first modern
    data standards wars between competing VHS and Betamax formats.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: In these systems, a magnetizable material such as iron oxide is formed into
    a tape structure, and the level of magnetization at each point along the tape
    is used to store data. Unlike punched paper, magnetic tape is easy to remagnetize
    and can be rewritten many times.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The same magnetic tapes can be used to store digital information, in various
    ways. For example, 0s and 1s can be encoded as single cycles of two different
    audible frequencies—a method that’s resilient to the heavy noise added by most
    tape devices. Algorithms developed for optimal access of punched tape carried
    over directly to magnetic tapes, as in the 1980s machine of [Figure 10-13](ch10.xhtml#ch10fig13).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0235-01.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-13: A 1980s compact cassette and player/recorder, used for both
    analog music and digital file storage*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Magnetic tape is still in use today for offline storage, specifically for weekly
    or daily backups of company systems. Tape is cheap and cost-effective for large-scale
    storage, where access time is less important. Tapes are thus useful for the daily
    backup task because you want to have lots of old backups kept around for as long
    as possible. In particular, if someone attacks your company in a more subtle way
    than just deleting everything—for example, by making a series of small changes
    to your database—it’s useful to have a long series of backups so you can recover
    the state of the system from different days, weeks, months, years, and even decades.
    You can buy a new tape for a few dollars every day to get this assurance. Having
    many tapes around also means they can be kept at many more locations than can
    hard drives—for example, with a different employee taking one tape home each day
    so that even if half the staff’s houses burn down on the same day, you still have
    many recent backups around.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: The most popular current standard for magnetic tape storage is *Linear Tape
    Open (LTO)*, shown in [Figure 10-14](ch10.xhtml#ch10fig14).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0236-01.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-14: An IBM Ultrium Linear Tape Open cartridge and drive*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: LTO is an open source standard that, as of 2020, stored around 36TB on about
    1 km of tape in one cartridge that fits in your pocket and takes around 12 hours
    to write. This is a good size and time for most small businesses; they can back
    up the whole system overnight onto a single cartridge.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '*Disks*'
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Audio recording began in the 1870s with wax cylinders, as shown in [Figure 10-15](ch10.xhtml#ch10fig15).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0236-02.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-15: A wax cylinder audio storage device*'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Here, sound waves enter the acoustic horn and are concentrated to vibrate a
    needle, etching the sound wave into a spiral around a hot wax cylinder as it rotates
    and is slowly moved left to right. When the wax cylinder is cool it can then be
    spun past the needle again to make it vibrate in the same ways, and have its motions
    amplified by the horn, replaying the sound.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Wax cylinders were used commercially until 1898, when they were replaced by
    gramophones with discs, rotating at 78 revolutions per minute ([Figure 10-16](ch10.xhtml#ch10fig16),
    left). These “78” disks used the same idea of etching the analog sound wave directly
    into their spiral grooves, and their vinyl descendants—now with electrical amplification—are
    still in use by DJs today ([Figure 10-16](ch10.xhtml#ch10fig16), right).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0237-01.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-16: A gramophone (left) and a modern Technics SL-1200 turntable
    (right)*'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Unlike audio discs, which have a single track spiraling in from the edge to
    the center, most data disks are truly 2D, as they have many independent *tracks*,
    each at a fixed radius, as shown in [Figure 10-17](ch10.xhtml#ch10fig17).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0237-02.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-17: The single track of an audio disc (left) and the 2D track of
    a data disk (right). The latter shows a track (A), sector (B), geometric sector
    (C), and cluster (D).*'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Tracks near the edge are larger than those in the center, so they store more
    data. Tracks are divided into fixed-data-size *sectors* around their circumference.
    Each sector has an address composed of its track ID and location within the track.
    In most systems, sectors store their own location in some of their bits so that
    we can figure out which part of the disk we’re looking at. They may also store
    redundant bits, which compensate for physical damage to the disk, using Shannon’s
    theory of communication. Sectors may be grouped into contiguous *clusters*, which
    are the smallest unit that can be read or written together.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Data on disks can be accessed in an *almost* random-access manner: individual
    sectors can be stored or retrieved in any order, not only sequentially, but reads
    and writes to nearby sectors and tracks will be faster due to the motion of the
    disk and head. It’s easy and fast to read from a series of sectors in order around
    the same track as they spin past the head. If you want data on the same track
    but at a different angle from the current sector, you have to wait for the disk
    to spin around to bring that sector under your head. If you want data from a different
    track, you have to move your head along the radius, which is very slow, as it’s
    a physical device. I/O modules controlling spinning disks thus need to consider
    the *access time*—the time it takes to read or write one sector. Access time is
    composed of two main factors: *seek time* is the time it takes for the arm to
    position itself over the track, and *rotational delay* is the time it takes for
    the desired sector to position itself under the head.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '**Floppy Disks**'
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Magnetic disks use the same technology as magnetic tape to represent data, but
    they arrange the magnetizable material into a 2D disk rather than a 1D tape. The
    disk is read and written by a magnetic head on an arm, like a gramophone needle.
    *Floppy disks* ([Figure 10-18](ch10.xhtml#ch10fig18)) first appeared in the 1960s.
    They’re so-called because they physically flex.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0238-01.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-18: Three generations of floppy disks: 8 inch (1970s), 5 1/4 inch
    (1980s), and 3 1/2 inch (1990s)*'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Floppy disks are vulnerable to damage, so they’re usually encased in a plastic
    sheath, as in the figure.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '**Hard Disks**'
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Hard disks* are made of nonflexible materials. They can store higher information
    densities and spin faster than floppies. These devices usually require sealing
    the head into a package with the disk, as in [Figure 10-19](ch10.xhtml#ch10fig19),
    rather than allowing removable disks, as with floppies.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0238-02.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-19: The inside of a magnetic hard drive*'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Hard *drives* usually contain multiple hard disks packaged together, each with
    its own head, with a single address space spanning all of them. This can help
    reduce access times, because the heads can all read and write together. The disks
    spin at speeds such as 90 to 250 Hz, which causes a layer of air to lift the head
    off the surface, so the head doesn’t physically contact the platter. This means
    there’s no physical wear to the head or the disk. Designers have invested heavily
    in technology to automatically and rapidly park the head if the unit is in physical
    danger, such as being struck or pushed. Without this, the head would crash into
    the disk and destroy it during such an incident.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '**Optical Discs**'
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Optical discs are modern-day version of the Babylonian clay tablets seen in
    [Figure 1-5](ch01.xhtml#ch01fig5). Like those tablets, they’re solid objects with
    small cavities—known as pits—made in them to represent data, as shown in [Figure
    10-20](ch10.xhtml#ch10fig20). Like punch cards, they use binary encoding, so each
    location either contains a pit or doesn’t contain a pit. The pits are read using
    a laser, and their nanometer scales are comparable with the wavelengths of this
    laser light.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0239-01.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-20: Four generations of optical storage*'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '*LaserDisc* (1978) was the first optical disc, having a 12-inch diameter like
    a vinyl album and marketed for home video. *Compact discs*, or *CDs* (1982), used
    roughly 800 nm pits, read by a laser head, to store up to 700 MB of audio data.
    CDs started seeing use for general rather than audio data storage in 1988 with
    the *CD-ROM* specification. Like CDs, these became read-only after initially creating
    the pits on their surfaces. *CD-R* was a version that simplified the recording
    process, allowing home users to “burn” their own CD-ROMs, again only once. These
    were used in the late 1990s for copying audio music collections, first using CD
    audio representations and then using bulk MP3 storage. They were usually blue
    on the burnable side and gold on top. Their “burning” was a physical process involving
    lasers and heat; this is the origin of modern slang “burning” now used for writing
    to other types of ROM, such as flash or FPGA. *CD-RW* was an improved CD-ROM that
    could be rewritten several times.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '*Digital Versatile Disc (DVD)* (1995), was an order of magnitude improvement,
    reducing pit size to 400 nm to achieve disc capacity of up to 4.7GB using the
    same size physical disc as CDs. DVDs were initially used for video but soon also
    for general data. As with CDs, write-once DVD-R and rewritable DVD-RW were also
    developed. *Blu-ray* (like its short-lived competitor, HD-DVD) reduced the pit
    size again, this time to 150 nm, allowing storage up to 25GB on the same size
    disc. As these pits are smaller, they require shorter-wavelength blue rather than
    infrared or red laser light to read them, hence the name.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '*Solid-State Drives*'
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For secondary storage, most current computers have moved from hard drives to
    *solid-state drives (SSDs)*. These are manufactured to have the same form factors
    and I/O interfaces, and similar capacities, as hard drives, but with no moving
    parts. This makes them faster, more reliable, lower power, quieter, smaller, and
    less prone to breakage when dropped. As there are no moving parts, they can be
    truly random access. SSDs are flash memory, as we’ve previously reviewed.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: The same flash memory technology is also used as offline storage, where SSD
    drives are easily removable, such as when connected to I/O via USB (known as USB
    sticks) or SD (known as SD cards).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Tertiary Memory
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Tertiary memory* is a recently proposed level in the memory hierarchy. It
    lies below secondary memory but above offline memory, and has been created to
    describe memories that used to be offline—requiring humans to physically load
    and eject media such as discs and tapes—but is now automated by mechanical processes.
    For example, automated Blu-ray and LTO tape jukeboxes as in [Figure 10-21](ch10.xhtml#ch10fig21)
    form tertiary memory.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0240-01.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10-21: A robotic tape jukebox in a data center*'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: In the figure, a robot arm is used—as in 1950s vinyl record jukeboxes—to pick
    up tapes and place them into the reader and storage containers. Similar robotic
    systems can be built around Blu-ray discs. Mobile robots driving baskets of hard
    disks around can now also be considered tertiary memory.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Data Centers
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you put thousands, or tens or hundreds of thousands, of secondary and tertiary
    memories together in a warehouse-sized building, you get a *data center*. Search
    engines, social networks, online retailers, media streamers, and governments all
    now need to store and access data at this scale. A typical data center will contain
    many different layers of the lower levels of the memory hierarchy. For example,
    tapes take longer to fast-forward and rewind than disks, so these are more likely
    to be found as long-term backup systems than serving the latest social media posts.
    Once you access something from a slower backup system, it will then be cached
    somewhere higher up the memory hierarchy, such as on an SSD drive, making for
    faster retrieval next time.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Data centers may be built with extreme security and resilience in mind. For
    example, HSBC’s literal “data mine” is widely believed to store backups of all
    its global financial data in a former UK coal mine. You can tell it’s a data center
    because there are huge air ducts rising out of the ground to disperse all the
    heat from the computers. The mine is thought to be robust to nuclear, chemical,
    and biological attack. In the event of a nuclear war, the rest of humanity may
    be bombed back to computing with Ishango bones, but the bank will still be able
    to come after your mortgage repayments.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Memory architecture is driven by economics: you can buy big, slow, cheap memory;
    small, fast, expensive memory; or some mixture of both. Empirically, most programs
    show spatial, sequential, and temporal locality, in which different small parts
    of memory tend to be in heavy, repeated use at different times. Memory architectures
    are thus designed in hierarchies that fit both the economics and usage patterns,
    including caches between layers to promote currently in-use memory to higher levels.
    Primary memory is that which is addressed directly by the CPU, using the bus,
    while secondary memory is connected via I/O. Secondary memory often takes the
    form of spinning disks, which can be disconnected and replaced, becoming offline
    memory if humans are involved or tertiary memory if the process is automated by
    robotics.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Your Computer’s Memory**'
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Try to find the sizes and speeds for each type of memory in your own computer,
    including caches, RAM, and secondary storage. If you can open up your computer,
    look inside, locate them, and find their makes and model numbers, then look up
    their datasheets online. Most operating systems have utilities that will display
    useful information about their memory; for example, Linux will show caches with
    `lscpu` `or cat /proc/cpuinfo`, RAM with `free -h`, and secondary memory with
    `lsblk`.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Building a Static RAM in LogiSim**'
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Build the static random-access memory (SRAM) presented in [Figure 6-22](ch06.xhtml#ch06fig22)
    in LogiSim. It should be able to store and read 2-bit words at the four memory
    locations.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extend your LogiSim SRAM to have longer words and more addresses.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Challenging**'
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Make four copies of your SRAM, representing multiple RAM chips. Each one will
    have the same address space, starting from address zero. Design a memory controller
    module that converts addresses from a larger global address space—having two extra
    bits—to sections of particular RAM chips and these local addresses within them.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try attaching this system to the Manchester Baby model in place of its previous
    LogiSim RAM.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**More Challenging**'
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Design and build a direct-mapped cache in LogiSim and link it to your LogiSim
    RAM from the previous task. (This won’t speed up that RAM, as it’s already fast
    SRAM, but it could then enable that SRAM to be replaced by a larger and cheaper,
    but slower, DRAM.)
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to build the other types of cache too, if you’re feeling brave. Use the
    sketches provided in this chapter as starting points.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Reading
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a definitive recent classic on memory, see U. Drepper, “What Every Programmer
    Should Know About Memory,” November 21, 2007, *[https://people.freebsd.org/~lstewart/articles/cpumemory.pdf](https://people.freebsd.org/~lstewart/articles/cpumemory.pdf)*.
    In fact, this resource contains far more than any normal human should know about
    memory.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
