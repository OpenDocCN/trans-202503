- en: '**21**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**MULTIPLE LINEAR REGRESSION**'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Multiple linear regression is a straightforward generalization of the single-predictor
    models discussed in the previous chapter. It allows you to model your continuous
    response variable in terms of more than one predictor so you can measure the joint
    effect of several explanatory variables on the response variable. In this chapter,
    you’ll see how to model your response variable in this way, and you’ll use R to
    fit the model using least-squares. You’ll also explore other key statistical aspects
    of linear modeling in the R environment, such as transforming variables and including
    interactive effects.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple linear regression represents an important part of the practice of statistics.
    It lets you control or adjust for multiple sources of influence on the value of
    the response, rather than just measuring the effect of one explanatory variable
    (in most situations, there is more than one contributor to the outcome measurements).
    At the heart of this class of methods is the intention to uncover potentially
    causal relationships between your response variable and the (joint) effect of
    any explanatory variables. In reality, causality itself is extremely difficult
    to establish, but you can strengthen any evidence of causality by using a well-designed
    study supported by sound data collection and by fitting models that might realistically
    gauge the relationships present in your data.
  prefs: []
  type: TYPE_NORMAL
- en: '**21.1 Terminology**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before you look at the theory behind multiple regression models, it’s important
    to have a clear understanding of some terminology associated with variables.
  prefs: []
  type: TYPE_NORMAL
- en: • A *lurking variable* influences the response, another predictor, or both,
    but goes unmeasured (or is not included) in a predictive model. For example, say
    a researcher establishes a link between the volume of trash thrown out by a household
    and whether the household owns a trampoline. The potential lurking variable here
    would be the number of children in the household—this variable is more likely
    to be positively associated with an increase in trash and chances of owning a
    trampoline. An interpretation that suggests owning a trampoline is a cause of
    increased waste would be erroneous.
  prefs: []
  type: TYPE_NORMAL
- en: • The presence of a lurking variable can lead to spurious conclusions about
    causal relationships between the response and the other predictors, or it can
    mask a true cause-and-effect association; this kind of error is referred to as
    *confounding*. To put it another way, you can think of confounding as the entanglement
    of the effects of one or more predictors on the response.
  prefs: []
  type: TYPE_NORMAL
- en: • A *nuisance* or *extraneous variable* is a predictor of secondary or no interest
    that has the potential to confound relationships between other variables and so
    affect your estimates of the other regression coefficients. Extraneous variables
    are included in the modeling as a matter of necessity, but the specific nature
    of their influence on the response is not the primary interest of the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: These definitions will become clearer once you begin fitting and interpreting
    the regression models in [Section 21.3](ch21.xhtml#ch21lev1sec69). The main message
    I want to emphasize here, once more, is that correlation does not imply causation.
    If a fitted model finds a statistically significant association between a predictor
    (or predictors) and a response, it’s important to consider the possibility that
    lurking variables are contributing to the results and to attempt to control any
    confounding before you draw conclusions. Multiple regression models allow you
    to do this.
  prefs: []
  type: TYPE_NORMAL
- en: '**21.2 Theory**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before you start using R to fit regression models, you’ll examine the technical
    definitions of a linear regression model with multiple predictors. Here, you’ll
    look at how the models work in a mathematical sense and get a glimpse of the calculations
    that happen “behind the scenes” when estimating the model parameters in R.
  prefs: []
  type: TYPE_NORMAL
- en: '***21.2.1 Extending the Simple Model to a Multiple Model***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Rather than having just one predictor, you want to determine the value of a
    continuous response variable *Y* given the values of *p* > 1 independent explanatory
    variables *X*[1], *X*[2], . . ., *X*[p]. The overarching model is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e21-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *β*[0], . . . , *β*[p] are the regression coefficients and, as before,
    you assume independent, normally distributed residuals є ~ N(0, ˙) around the
    mean.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, you have *n* data records; each record provides values for each
    of the predictors *X*[*j*] ; *j* = {1, . . ., *p*}. The model to be fitted is
    given in terms of the mean response, conditional upon a particular realization
    of the set of explanatory variables
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0487-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where the ![image](../images/bj.jpg) represent estimates of the regression coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: In simple linear regression, where you have only one predictor variable, recall
    that the goal is to find the “line of best fit.” The idea of least-squares estimation
    for linear models with multiple independent predictors follows much the same motivation.
    Now, however, in an abstract sense you can think of the relationship between response
    and predictors as a multidimensional plane or surface. You want to find the surface
    that best fits your multivariate data in terms of minimizing the overall squared
    distance between itself and the raw response data.
  prefs: []
  type: TYPE_NORMAL
- en: More formally, for your *n* data records, the ![image](../images/bj.jpg) are
    found as the values that minimize the sum
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e21-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *x* [*j*],[*i*] is the observed value of individual *i* for explanatory
    variable *X*[*j*] and *y[i]* is their response value.
  prefs: []
  type: TYPE_NORMAL
- en: '***21.2.2 Estimating in Matrix Form***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The computations involved in minimizing this squared distance (21.2) are made
    much easier by a *matrix representation* of the data. When dealing with *n* multivariate
    observations, you can write [Equation (21.1)](ch21.xhtml#ch21eq1) as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '***Y*** = ***X*** · + є,'
  prefs: []
  type: TYPE_NORMAL
- en: where ***Y*** and є denote *n* × 1 column matrices such that
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0487-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *y[i]* and є[*i*] refer to the response observation and random error
    term for the *i*th individual. The quantity ***β*** is a (*p* + 1) × 1 column
    matrix of the regression coefficients, and then the observed predictor data for
    all individuals and explanatory variables are stored in an *n* × (*p* + 1) matrix
    ***X***, called the *design matrix*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0488-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The minimization of (21.2) providing the estimated regression coefficient values
    is then found with the following calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e21-3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It’s important to note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: • The symbol · represents matrix multiplication, the superscript ^⊤ represents
    the transpose, and ^−¹ represents the inverse when applied to matrices (as per
    [Section 3.3](ch03.xhtml#ch03lev1sec14)).
  prefs: []
  type: TYPE_NORMAL
- en: • Extending the size of *β* and ***X*** (note the leading column of 1s in ***X***)
    to create structures of size *p* + 1 (as opposed to just the number of predictors
    *p*) allows for the estimation of the overall intercept *β*[0].
  prefs: []
  type: TYPE_NORMAL
- en: • As well as (21.3), the design matrix plays a crucial role in the estimation
    of other quantities, such as the standard errors of the coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: '***21.2.3 A Basic Example***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can manually estimate the *β[j]* (*j* = 0, 1, . . ., *p*) in R using the
    functions covered in [Chapter 3](ch03.xhtml#ch03): `%*%` (matrix multiplication),
    `t` (matrix transposition), and `solve` (matrix inversion). As a quick demonstration,
    let’s say you have two predictor variables: *X[1]* as continuous and *X[2]* as
    binary. Your target regression equation is therefore ![image](../images/f0488-01a.jpg).
    Suppose you collect the following data, where the response data, data for *X*[1],
    and data for *X*[2], for *n* = 8 individuals, are given in the columns `y`, `x1`,
    and `x2`, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To get your point estimates in ***β*** = [*β*[0], *β*[1], *β*[2]]^┬ for the
    linear model, you first have to construct ***X*** and ***Y*** as required by (21.3).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now all you have to do is execute the line corresponding to (21.3).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You’ve just used least-squares to fit your model based on the observed data
    in `demo.data`, which results in the estimates ![image](../images/f0489-01.jpg),
    ![image](../images/f0489-02.jpg), and ![image](../images/f0489-03.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: '**21.3 Implementing in R and Interpreting**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ever helpful, R automatically builds the matrices and carries out all the necessary
    calculations when you instruct it to fit a multiple linear regression model. As
    in simple regression models, you use `lm` and just include any additional predictors
    when you specify the formula in the first argument. So that you can focus on the
    R syntax and on interpretation, I’ll focus only on *main effects* for the moment,
    and then you’ll explore more complex relationships later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to output and interpretation, working with multiple explanatory
    variables follows the same rules as you’ve seen in [Chapter 20](ch20.xhtml#ch20).
    Any numeric-continuous variables (or a categorical variable being treated as such)
    have a slope coefficient that provides a “per-unit-change” quantity. Any *k*-group
    categorical variables (factors, formally unordered) are dummy coded and provide
    *k* − 1 intercepts.
  prefs: []
  type: TYPE_NORMAL
- en: '***21.3.1 Additional Predictors***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s first confirm the manual matrix calculations from a moment ago. Using
    the `demo.data` object, fit the multiple linear model and examine the coefficients
    from that object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You’ll see that you obtain exactly the point estimates stored earlier in `BETA.HAT`.
  prefs: []
  type: TYPE_NORMAL
- en: With the response variable on the left as usual, you specify the multiple predictors
    on the right side of the `~` symbol; altogether this represents the formula argument.
    To fit a model with several main effects, use `+` to separate any variables you
    want to include. In fact, you’ve already seen this notation in [Section 19.2.2](ch19.xhtml#ch19lev2sec172),
    when investigating two-way ANOVA.
  prefs: []
  type: TYPE_NORMAL
- en: To study the interpretation of the parameter estimates of a multiple linear
    regression model, let’s return to the `survey` data set in the `MASS` package.
    In [Chapter 20](ch20.xhtml#ch20), you explored several simple linear regression
    models based on a response variable of student height, as well as stand-alone
    predictors of handspan (continuous) and sex (categorical, *k* = 2). You found
    that handspan was highly statistically significant, with the estimated coefficient
    suggesting an average increase of about 3.12 cm for each 1 cm increase in handspan.
    When you looked at the same *t*-test using sex as the explanatory variable, the
    model also suggested evidence against the null hypothesis, with “being male” adding
    around 13.14 cm to the mean height when compared to the mean for females (the
    category used as the reference level).
  prefs: []
  type: TYPE_NORMAL
- en: What those models can’t tell you is the *joint effect* of sex and handspan on
    predicting height. If you include both predictors in a multiple linear model,
    you can (to some extent) reduce any confounding that might otherwise occur in
    the isolated fits of the effect of either single predictor on height.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The coefficient for handspan is now only about 1.59, almost half of its corresponding
    value (3.12 cm) in the stand-alone simple linear regression for height. Despite
    this, it’s still highly statistically significant in the presence of sex. The
    coefficient for sex has also reduced in magnitude when compared with its simple
    linear model and is also still significant in the presence of handspan. You’ll
    interpret these new figures in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: As for the rest of the output, the `Residual standard error` still provides
    you with an estimate of the standard error of the random noise term є, and you’re
    also provided with an `R-squared` value. When associated with more than one predictor,
    the latter is formally referred to as the coefficient of *multiple* determination.
    The calculation of this coefficient, as in the single predictor setting, comes
    from the correlations between the variables in the model. I’ll leave the theoretical
    intricacies to more advanced texts, but it’s important to note that `R-squared`
    still represents the proportion of variability in the response that’s explained
    by the regression; in this example, it sits at around 0.51.
  prefs: []
  type: TYPE_NORMAL
- en: You can continue to add explanatory variables in the same way if you need to
    do so. In [Section 20.5.2](ch20.xhtml#ch20lev2sec186), you examined smoking frequency
    as a stand-alone categorical predictor for height and found that this explanatory
    variable provided no statistical evidence of an impact on the mean response. But
    could the smoking variable contribute in a statistically significant way if you
    control for handspan and sex?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Since it’s a categorical variable with *k* > 2 levels, `Smoke` is dummy coded
    (with heavy smokers as the default reference level), giving you three extra intercepts
    for the three nonreference levels of the variable; the fourth is incorporated
    into the overall intercept.
  prefs: []
  type: TYPE_NORMAL
- en: In the `summary` of the latest fit, you can see that while handspan and sex
    continue to yield very small *p*-values, smoking frequency suggests no such evidence
    against the hypotheses of zero coefficients. The smoking variable has had little
    effect on the values of the other coefficients compared with the previous model
    in `survmult`, and the `R-squared` coefficient of multiple determination has barely
    increased.
  prefs: []
  type: TYPE_NORMAL
- en: 'One question you might now ask is, if smoking frequency doesn’t benefit your
    ability to predict mean height in any substantial way, should you remove that
    variable from the model altogether? This is the primary goal of *model selection*:
    to find the “best” model for predicting the outcome, without fitting one that
    is unnecessarily complex (by including more explanatory variables than is required).
    You’ll look at some common ways researchers attempt to achieve this in [Section
    22.2](ch22.xhtml#ch22lev1sec73).'
  prefs: []
  type: TYPE_NORMAL
- en: '***21.3.2 Interpreting Marginal Effects***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In multiple regression, the estimation of each predictor takes into account
    the effect of all other predictors present in the model. A coefficient for a specific
    predictor *Z* should therefore be interpreted as the change in the mean response
    for a one-unit increase in *Z*, while holding all other predictors constant.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you’ve determined that smoking frequency still appears to have no discernible
    impact on mean height when taking sex and handspan into consideration, return
    your focus to `survmult`, the model that includes only the explanatory variables
    of sex and handspan. Note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: • For students of the same sex (that is, focusing on either just males or just
    females), a 1 cm increase in handspan leads to an estimated increase of 1.5944
    cm in mean height.
  prefs: []
  type: TYPE_NORMAL
- en: • For students of similar handspan, males on average will be 9.4898 cm taller
    than females.
  prefs: []
  type: TYPE_NORMAL
- en: • The difference in the values of the two estimated predictor coefficients when
    compared with their respective simple linear model fits, plus the fact that both
    continue to indicate evidence against the null hypothesis of “being zero” in the
    multivariate fit, suggests that confounding (in terms of the effect of both handspan
    and sex on the response variable of height) is present in the single-predictor
    models.
  prefs: []
  type: TYPE_NORMAL
- en: The final point highlights the general usefulness of multiple regression. It
    shows that, in this example, if you use only single predictor models, the determination
    of the “true” impact that each explanatory variable has in predicting the mean
    response is misleading since some of the change in height is determined by sex,
    but some is also attributed to handspan. It’s worth noting that the coefficient
    of determination (refer to [Section 20.3.3](ch20.xhtml#ch20lev2sec179)) for the
    `survmult` model is noticeably higher than the same quantity in either of the
    single-variate models, so you’re actually accounting for more of the variation
    in the response by using multiple regression.
  prefs: []
  type: TYPE_NORMAL
- en: The fitted model itself can be thought of as
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e21-4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where “handspan” is the writing handspan supplied in centimeters and “sex” is
    supplied as either 1 (if male) or 0 (if female).
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The baseline (overall) intercept of around 137.687 cm represents the mean
    height of a female with a handspan of 0 cm—again, this is clearly not directly
    interpretable in the context of the application. For this kind of situation, some
    researchers center the offending continuous predictor (or predictors) on zero
    by subtracting the sample mean of all the observations on that predictor from
    each observation prior to fitting the model. The centered predictor data are then
    used in place of the original (untranslated) data. The resulting fitted model
    allows you to use the mean value of the untranslated predictor (in this case handspan)
    rather than a zero value in order to directly interpret the intercept estimate*
    ![image](../images/b0.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: '***21.3.3 Visualizing the Multiple Linear Model***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As shown here, “being male” simply changes the overall intercept by around
    9.49 cm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Because of this, you could also write (21.4) as two equations. Here’s the equation
    for female students:'
  prefs: []
  type: TYPE_NORMAL
- en: “Mean height” = 137.687 + 1.594 × “handspan”
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the equation for male students:'
  prefs: []
  type: TYPE_NORMAL
- en: '| “Mean height” | = | (137.687 + 9.4898) + 1.594 × “handspan” |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | 147.177 + 1.594 × “handspan” |'
  prefs: []
  type: TYPE_TB
- en: 'This is handy because it allows you to visualize the multivariate model in
    much the same way as you can the simple linear models. This code produces [Figure
    21-1](ch21.xhtml#ch21fig1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: First, a scatterplot of the height and handspan observations, split by sex,
    is drawn. Then, `abline` adds the line corresponding to females and adds a second
    one corresponding to males, based on those two equations.
  prefs: []
  type: TYPE_NORMAL
- en: Although this plot might look like two separate simple linear model fits, one
    for each level of sex, it’s important to recognize that isn’t the case. You’re
    effectively looking at a representation of a multivariate model on a two-dimensional
    canvas, where the statistics that determine the fit of the two visible lines have
    been estimated “jointly,” in other words, when considering both predictors.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f21-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 21-1: Visualizing the observed data and fitted multiple linear model
    of student height modeled by handspan and sex*'
  prefs: []
  type: TYPE_NORMAL
- en: '***21.3.4 Finding Confidence Intervals***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As in [Chapter 20](ch20.xhtml#ch20), you can easily find confidence intervals
    for any of the regression parameters in multiple regression models with `confint`.
    Using `survmult2`, the object of the fitted model for student height including
    the smoking frequency predictor, the output of a call to `confint` looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `Wr.Hnd` and `SexMale` variables were shown to be statistically
    significant at the 5 percent level in the earlier model summary and that their
    95 percent confidence levels do not include the null value of zero. On the other
    hand, all the coefficients for the dummy variables associated with the smoking
    frequency predictor are all nonsignificant, and their confidence intervals clearly
    include zero. This reflects the fact that the smoking variable isn’t, as a whole,
    considered statistically significant in this particular model.
  prefs: []
  type: TYPE_NORMAL
- en: '***21.3.5 Omnibus F-Test***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First encountered in [Section 20.5.2](ch20.xhtml#ch20lev2sec186) in the context
    of multilevel predictors, you can think of the omnibus *F*-test more generally
    for multiple regression models as a test with the following hypotheses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e21-5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The test is effectively comparing the amount of error attributed to the “null”
    model (in other words, one with an intercept only) with the amount of error attributed
    to the predictors when all the predictors are present. In other words, the more
    the predictors are able to model the response, the more error they explain, giving
    you a more extreme *F* statistic and therefore a smaller *p*-value. The single
    result makes the test especially useful when you have many explanatory variables.
    The test works the same regardless of the mix of predictors you have in a given
    model: one or more might be continuous, discrete, binary, and/or categorical with
    *k* > 2 levels. When multiple regression models are fitted, the amount of output
    alone can take time to digest and interpret, and care must be taken to avoid Type
    I errors (incorrect rejection of a true null hypothesis—refer to [Section 18.5](ch18.xhtml#ch18lev1sec58)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *F*-test helps boil all that down, allowing you to conclude either of the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Evidence against H[0] if the associated *p*-value is smaller than your chosen
    significance level *α*, which suggests that your regression—your combination of
    the explanatory variables—does a significantly better job of predicting the response
    than if you removed *all* those predictors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No evidence against H[0] if the associated *p*-value is larger than *α*, which
    suggests that using the predictors has no tangible benefit over having an intercept
    alone.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The downside is that the test doesn’t tell you which of the predictors (or which
    subset thereof) is having a beneficial impact on the fit of the model, nor does
    it tell you anything about their coefficients or respective standard errors.
  prefs: []
  type: TYPE_NORMAL
- en: You can compute the *F*-test statistic using the coefficient of determination,
    *R*², from the fitted regression model. Let *p* be the number of regression parameters
    requiring estimation, excluding the intercept *β*[0]. Then,
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e21-6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *n* is the number of observations used in fitting the model (after records
    with missing values have been deleted). Then, under H[0] in (21.5), ![image](../images/f.jpg)
    follows an *F* distribution (see [Section 16.2.5](ch16.xhtml#ch16lev2sec145) and
    also [Section 19.1.2](ch19.xhtml#ch19lev2sec169)) with df[1] = *p*, df[2] = *n*−
    *p*−1 degrees of freedom. The *p*-value associated with (21.6) is yielded as the
    upper-tail area of that *F* distribution.
  prefs: []
  type: TYPE_NORMAL
- en: As a quick exercise to confirm this, turn your attention back to the fitted
    multiple regression model `survmult2` in [Section 21.3.1](ch21.xhtml#ch21lev2sec193),
    which is the model for student height by handspan, sex, and smoking status from
    `survey`. You can extract the coefficient of multiple determination from the `summary`
    report (using the technique noted in [Section 20.3.4](ch20.xhtml#ch20lev2sec180)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This matches the multiple R-squared value from [Section 21.3.1](ch21.xhtml#ch21lev2sec193).
    Then, you can get *n* as the original size of the data set in `survey` minus any
    missing values (reported as 30 in the earlier `summary` output).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You get *p* as the number of estimated regression parameters (minus 1 for the
    intercept).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then confirm the value of *n* − *p* − 1, which matches the `summary`
    output (`201 degrees of freedom`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, you find the test statistic ![image](../images/f.jpg) as dictated
    by (21.6), and you can use the `pf` function as follows to obtain the corresponding
    *p*-value for the test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the omnibus *F*-test for this example gives a *p*-value that’s
    so small, it’s effectively zero. These calculations match the relevant results
    reported in the output of `summary(survmult2)` completely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking back at the student height multiple regression fit based on handspan,
    sex, and smoking in `survmult2` in [Section 21.3.1](ch21.xhtml#ch21lev2sec193),
    it’s little surprise that with two of the predictors yielding small *p*-values,
    the omnibus *F*-test suggests strong evidence against H[0] based on (21.5). This
    highlights the “umbrella” nature of the omnibus test: although the smoking frequency
    variable itself doesn’t appear to contribute anything statistically important,
    the *F*-test for that model still suggests `survmult2` should be preferred over
    a “no-predictor” model, because both handspan and sex are important.'
  prefs: []
  type: TYPE_NORMAL
- en: '***21.3.6 Predicting from a Multiple Linear Model***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Prediction (or *forecasting*) for multiple regression follows the same rules
    as for simple regression. It’s important to remember that point predictions found
    for a particular *covariate profile*—the collection of predictor values for a
    given individual—are associated with the mean (or *expected value*) of the response;
    that confidence intervals provide measures for mean responses; and that prediction
    intervals provide measures for raw observations. You also have to consider the
    issue of interpolation (predictions based on *x* values that fall within the range
    of the originally observed covariate data) versus extrapolation (prediction from
    *x* values that fall outside the range of said data). Other than that, the R syntax
    for `predict` is identical to that used in [Section 20.4](ch20.xhtml#ch20lev1sec65).
  prefs: []
  type: TYPE_NORMAL
- en: As an example, using the model fitted on student height as a linear function
    of handspan and sex (in `survmult`), you can estimate the mean height of a male
    student with a writing handspan of 16.5 cm, together with a confidence interval.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The result indicates that you have an expected value of about 173.48 cm and
    that you can be 95 percent confident the true value lies somewhere between 170.94
    and 176.03 (rounded to 2 d.p.). In the same way, the mean height of a female with
    a handspan of 13 cm is estimated at 158.42 cm, with a 99 percent prediction interval
    of 139.76 to 177.07.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: There are in fact two female students in the data set with writing handspans
    of 13 cm, as you can see in [Figure 21-1](ch21.xhtml#ch21fig1). Using your knowledge
    of subsetting data frames, you can inspect these two records and select the three
    variables of interest.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now, the second female’s height falls well inside the prediction interval, but
    the first female’s height is significantly higher than the upper limit. It’s important
    to realize that, technically, nothing has gone wrong here in terms of the model
    fitting and interpretation—it’s still possible that an observation can fall outside
    a prediction interval, even a wide 99 percent interval, though it’s perhaps improbable.
    There could be any number of reasons for this occurring. First, the model could
    be inadequate. For example, you might be excluding important predictors in the
    fitted model and therefore have less predictive power. Second, although the prediction
    is within the range of the observed data, it has occurred at one extreme end of
    the range, where it’s less reliable because your data are relatively sparse. Third,
    the observation itself may be tainted in some way—perhaps the individual recorded
    her handspan incorrectly, in which case her invalid observation should be removed
    prior to model fitting. It’s with this critical eye that a good statistician will
    appraise data and models; this is a skill that I’ll emphasize further as this
    chapter unfolds.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 21.1**'
  prefs: []
  type: TYPE_NORMAL
- en: In the `MASS` package, you’ll find the data frame `cats`, which provides data
    on sex, body weight (in kilograms), and heart weight (in grams) for 144 household
    cats (see [Venables and Ripley, 2002](ref.xhtml#ref69), for further details);
    you can read the documentation with a call to `?cats`. Load the `MASS` package
    with a call to `library("MASS")`, and access the object directly by entering `cats`
    at the console prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Plot heart weight on the vertical axis and body weight on the horizontal axis,
    using different colors or point characters to distinguish between male and female
    cats. Annotate your plot with a legend and appropriate axis labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a least-squares multiple linear regression model using heart weight as the
    response variable and the other two variables as predictors, and view a model
    summary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write down the equation for the fitted model and interpret the estimated regression
    coefficients for body weight and sex. Are both statistically significant? What
    does this say about the relationship between the response and predictors?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Report and interpret the coefficient of determination and the outcome of the
    omnibus *F*-test.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Tilman’s cat, Sigma, is a 3.4 kg female. Use your model to estimate her mean
    heart weight and provide a 95 percent prediction interval.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `predict` to superimpose continuous lines based on the fitted linear model
    on your plot from (a), one for male cats and one for female. What do you notice?
    Does this reflect the statistical significance (or lack thereof) of the parameter
    estimates?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `boot` package ([Davison and Hinkley, 1997](ref.xhtml#ref16); [Canty and
    Ripley, 2015](ref.xhtml#ref09)) is another library of R code that’s included with
    the standard installation but isn’t automatically loaded. Load `boot` with a call
    to `library("boot")`. You’ll find a data frame called `nuclear`, which contains
    data on the construction of nuclear power plants in the United States in the late
    1960s ([Cox and Snell, 1981](ref.xhtml#ref14)).
  prefs: []
  type: TYPE_NORMAL
- en: Access the documentation by entering `?nuclear` at the prompt and examine the
    details of the variables. (Note there is a mistake for `date`, which provides
    the date that the construction permits were issued—it should read “measured in
    years since January 1 **1900** to the nearest month.”) Use `pairs` to produce
    a quick scatterplot matrix of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One of the original objectives was to predict the cost of further construction
    of these power plants. Create a fit and summary of a linear regression model that
    aims to model `cost` by `t1` and `t2`, two variables that describe different elapsed
    times associated with the application for and issue of various permits. Take note
    of the estimated regression coefficients and their significance in the fitted
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refit the model, but this time also include an effect for the date the construction
    permit was issued. Contrast the output for this new model against the previous
    one. What do you notice, and what does this information suggest about the relationships
    in the data with respect to these predictors?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a third model for power plant cost, using the predictors for “date of permit
    issue,” “power plant capacity,” and the binary variable describing whether the
    plant was sited in the northeastern United States. Write down the fitted model
    equation and provide 95 percent confidence intervals for each estimated coefficient.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following table gives an excerpt of a historical data set compiled between
    1961 and 1973\. It concerns the annual murder rate in Detroit, Michigan; the data
    were originally presented and analyzed by Fisher ([1976](ref.xhtml#ref23)) and
    are reproduced here from Harraway ([1995](ref.xhtml#ref30)). In the data set you’ll
    find the number of murders, police officers, and gun licenses issued per 100,000
    population, as well as the overall unemployment rate as a percentage of the overall
    population.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Murders** | **Police** | **Unemployment** | **Guns** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 8.60 | 260.35 | 11.0 | 178.15 |'
  prefs: []
  type: TYPE_TB
- en: '| 8.90 | 269.80 | 7.0 | 156.41 |'
  prefs: []
  type: TYPE_TB
- en: '| 8.52 | 272.04 | 5.2 | 198.02 |'
  prefs: []
  type: TYPE_TB
- en: '| 8.89 | 272.96 | 4.3 | 222.10 |'
  prefs: []
  type: TYPE_TB
- en: '| 13.07 | 272.51 | 3.5 | 301.92 |'
  prefs: []
  type: TYPE_TB
- en: '| 14.57 | 261.34 | 3.2 | 391.22 |'
  prefs: []
  type: TYPE_TB
- en: '| 21.36 | 268.89 | 4.1 | 665.56 |'
  prefs: []
  type: TYPE_TB
- en: '| 28.03 | 295.99 | 3.9 | 1131.21 |'
  prefs: []
  type: TYPE_TB
- en: '| 31.49 | 319.87 | 3.6 | 837.60 |'
  prefs: []
  type: TYPE_TB
- en: '| 37.39 | 341.43 | 7.1 | 794.90 |'
  prefs: []
  type: TYPE_TB
- en: '| 46.26 | 356.59 | 8.4 | 817.74 |'
  prefs: []
  type: TYPE_TB
- en: '| 47.24 | 376.69 | 7.7 | 583.17 |'
  prefs: []
  type: TYPE_TB
- en: '| 52.33 | 390.19 | 6.3 | 709.59 |'
  prefs: []
  type: TYPE_TB
- en: Create your own data frame in your R workspace and produce a scatterplot matrix.
    Which of the variables appears to be most strongly related to the murder rate?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a multiple linear regression model using the number of murders as the response
    and all other variables as predictors. Write down the model equation and interpret
    the coefficients. Is it reasonable to state that all relationships between the
    response and the predictors are causal?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify the amount of variation in the response attributed to the joint effect
    of the three explanatory variables. Then refit the model excluding the predictor
    associated with the largest (in other words, “most nonsignificant”) *p*-value.
    Compare the new coefficient of determination with that of the previous model.
    Is there much difference?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your model from (k) to predict the mean number of murders per 100,000 residents,
    with 300 police officers and 500 issued gun licenses. Compare this to the mean
    response if there were no gun licenses issued and provide 99 percent confidence
    intervals for both predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**21.4 Transforming Numeric Variables**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, the linear function as strictly defined by the standard regression
    equation, (21.1), can be inadequate when it comes to capturing relationships between
    a response and selected covariates. You might, for example, observe curvature
    in a scatterplot between two numeric variables to which a perfectly straight line
    isn’t necessarily best suited. To a certain extent, the requirement that your
    data exhibit such linear behavior in order for a linear regression model to be
    appropriate can be relaxed by simply transforming (typically in a nonlinear fashion)
    certain variables before any estimation or model fitting takes place.
  prefs: []
  type: TYPE_NORMAL
- en: '*Numeric transformation* refers to the application of a mathematical function
    to your numeric observations in order to rescale them. Finding the square root
    of a number and converting a temperature from Fahrenheit to Celsius are both examples
    of a numeric transformation. In the context of regression, transformation is generally
    applied only to continuous variables and can be done in any number of ways. In
    this section, you’ll limit your attention to examples using the two most common
    approaches: *polynomial* and *logarithmic* transformations. However, note that
    the appropriateness of the methods used to transform variables, and any modeling
    benefits that might occur, can only really be considered on a case-by-case basis.'
  prefs: []
  type: TYPE_NORMAL
- en: Transformation in general doesn’t represent a universal solution to solving
    problems of nonlinearity in the trends in your data, but it can at least improve
    how faithfully a linear model is able to represent those trends.
  prefs: []
  type: TYPE_NORMAL
- en: '***21.4.1 Polynomial***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Following on from a comment made earlier, let’s say you observe a curved relationship
    in your data such that a straight line isn’t a sensible choice for modeling it.
    In an effort to fit your data more closely, a polynomial or *power* transformation
    can be applied to a specific predictor variable in your regression model. This
    is a straightforward technique that, by allowing *polynomial curvature* in the
    relationships, allows changes in that predictor to influence the response in more
    complex ways than otherwise possible. You achieve this by including additional
    terms in the model definition that represent the impact of progressively higher
    powers of the variable of interest on the response.
  prefs: []
  type: TYPE_NORMAL
- en: 'To clarify the concept of polynomial curvature, consider the following sequence
    between −4 and 4, as well as the simple vectors computed from it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, you’re taking the original value of `x` and calculating specific functionals
    of it. The vector `y`, as a copy of `x`, is clearly linear (in technical terms,
    this is a “polynomial of order 1”). You assign `y2` to take on an additionally
    squared valued of `x`, providing *quadratic* behavior—a polynomial of order 2\.
    Lastly, the vector `y3` represents the results of a *cubic* function of the values
    of `x`, with the inclusion of `x` raised to the power of 3—a polynomial of order
    3.
  prefs: []
  type: TYPE_NORMAL
- en: The following three lines of code produce, separately, the plots from left to
    right in [Figure 21-2](ch21.xhtml#ch21fig2).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/f21-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 21-2: Illustrating linear (left), quadratic (middle), and cubic functions
    (right) of* `x`'
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps a bit more generally, let’s say you have data for a continuous predictor,
    *X*, that you want to use to model your response, *Y* . Following estimation in
    the usual way, linearly, the simple model is ![image](../images/f0503-01.jpg);
    a quadratic trend in *X* can be modeled via the multiple regression ![image](../images/f0503-02.jpg);
    a cubic relationship can be captured by ![image](../images/f0503-03.jpg); and
    so on. From the plots in [Figure 21-2](ch21.xhtml#ch21fig2), a good way to interpret
    the effects of including these extra terms is in the complexity of the curves
    that can be captured. At order 1, the linear relationship allows no curvature.
    At order 2, a quadratic function of any given variable allows one “bend.” At order
    3, the model can cope with two bends in the relationship, and this continues if
    you keep adding terms corresponding to increasing powers of the covariate. The
    regression coefficients associated with these terms (all implied to be 1 in the
    code that produced the previous plots) are able to control the specific appearance
    (in other words, the *strength* and *direction*) of the curvature.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fitting a Polynomial Transformation**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Return your attention to the built-in `mtcars` data set. Consider the `disp`
    variable, which describes engine displacement volume in cubic inches, against
    a response variable of miles per gallon. If you examine a plot of the data in
    [Figure 21-3](ch21.xhtml#ch21fig3), you can see that there does appear to be a
    slight yet noticeable curve in the relationship between displacement and mileage.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/f21-03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 21-3: Scatterplot of miles per gallon and engine displacement, for
    the* `mtcars` *data*'
  prefs: []
  type: TYPE_NORMAL
- en: Is the straight line that a simple linear regression model would provide really
    the best way to represent this relationship? To investigate this, start by fitting
    that basic linear setup.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This clearly indicates statistical evidence of a negative linear impact of displacement
    on mileage—for each additional cubic inch of displacement, the mean response decreases
    by about 0.041 miles per gallon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, try to capture the apparent curve in the data by adding a quadratic term
    in `disp` to the model. You can do this in two ways. First, you could create a
    new vector in the workspace by simply squaring the `mtcars$disp` vector and then
    supplying the result to the formula in `lm`. Second, you could specify `disp^2`
    directly as an additive term in the formula. If you do it this way, it’s essential
    to wrap that particular expression in a call to `I` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Use of the `I` function around a given term in the formula is necessary when
    said term requires an arithmetic calculation—in this case, `disp^2`—before the
    model itself is actually fitted.
  prefs: []
  type: TYPE_NORMAL
- en: Turning to the fitted multiple regression model itself, you can see that the
    contribution of the squared component is statistically significant—the output
    corresponding to `I(disp^2)` shows a *p*-value of 0.0031\. This implies that even
    if a linear trend is taken into account, the model that includes a quadratic component
    (which introduces a curve) is a better-fitting model. This conclusion is supported
    by a noticeably higher coefficient of determination compared to the first fit
    (0.7927 against 0.7183). You can see the fit of this quadratic curve in [Figure
    21-4](ch21.xhtml#ch21fig4) (code for which follows shortly).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here you might reasonably wonder whether you can improve the ability of the
    model to capture the relationship further by adding yet another higher-order term
    in the covariate of interest. To that end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that a cubic component also offers a statistically significant
    contribution. However, if you were to continue adding higher-order terms, you’d
    find that fitting a polynomial of order 4 to these data isn’t able to improve
    the fit at all, with several coefficients being rendered nonsignificant (the order
    4 fit isn’t shown).
  prefs: []
  type: TYPE_NORMAL
- en: So, letting ŷ be miles per gallon and *x* be displacement in cubic inches, and
    expanding the e-notation from the previous output, the fitted multiple regression
    model is
  prefs: []
  type: TYPE_NORMAL
- en: ŷ = 50.7 − 0.3372*x* + 0.0011*x*² − 0.000001*x*³,
  prefs: []
  type: TYPE_NORMAL
- en: which is precisely what the order 3 line in the left panel of [Figure 21-4](ch21.xhtml#ch21fig4)
    reflects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Plotting the Polynomial Fit**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To address the plot itself, you visualize the data and the first (simple linear)
    model in `car.order1` in the usual way. To begin [Figure 21-4](ch21.xhtml#ch21fig4),
    execute the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/f21-04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 21-4: Three different models, polynomials of orders 1, 2, and 3, fitted
    to the “mileage per displacement” relationship from the* `mtcars` *data set. Left:
    Visible plot limits constrained to the data. Right: Visible plot limits widened
    considerably to illustrate unreliability in extrapolation.*'
  prefs: []
  type: TYPE_NORMAL
- en: It’s a little more difficult to add the line corresponding to either of the
    polynomial-termed models since `abline` is equipped to handle only straight-line
    trends. One way to do this is to make use of `predict` for each value in a sequence
    that represents the desired values of the explanatory variable. (I favor this
    approach because it also allows you to simultaneously calculate confidence and
    prediction bands if you want.) To add the line for the order 2 model only, first
    create the required sequence over the observed range of `disp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here, the sequence has been widened a little by minus and plus 50 to predict
    a small amount on either side of the scope of the original covariate data, so
    the curve meets the edges of the graph. Then you make the prediction itself and
    superimpose the fitted line.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: You use the same technique, followed by the final addition of the legend, for
    the order 3 polynomial.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The result of all this is on the left panel of [Figure 21-4](ch21.xhtml#ch21fig4).
    Even though you’ve used raw data from only one covariate, `disp`, the example
    illustrated here is considered multiple regression because more than one parameter
    (in addition to the universal intercept *β*[0]) required estimation in the order
    2 and 3 models.
  prefs: []
  type: TYPE_NORMAL
- en: The different types of trend lines fitted to the mileage and displacement data
    clearly show different interpretations of the relationship. Visually, you could
    reasonably argue that the simple linear fit is inadequate at modeling the relationship
    between response and predictor, but it’s harder to come to a clear conclusion
    when choosing between the order 2 and order 3 versions. The order 2 fit captures
    the curve that tapers off as `disp` increases; the order 3 fit additionally allows
    for a bump (in technical terms a *saddle* or *inflection*), followed by a steeper
    downward trend in the same domain.
  prefs: []
  type: TYPE_NORMAL
- en: So, which model is “best”? In this case, the statistical significance of the
    parameters suggests that the order 3 model should be preferred. Having said that,
    there are other things to consider when choosing between different models, which
    you’ll think about more carefully in [Section 22.2](ch22.xhtml#ch22lev1sec73).
  prefs: []
  type: TYPE_NORMAL
- en: '**Pitfalls of Polynomials**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: One particular drawback associated with polynomial terms in linear regression
    models is the instability of the fitted trend when trying to perform any kind
    of extrapolation. The right plot in [Figure 21-4](ch21.xhtml#ch21fig4) shows the
    same three fitted models (MPG by displacement), but this time with a much wider
    scale for displacement. As you can see, the validity of these models is questionable.
    Though the order 2 and 3 models fit MPG acceptably within the range of the observed
    data, if you move even slightly outside the maximum threshold of observed displacement
    values, the predictions of the mean mileage go wildly off course. The order 2
    model in particular becomes completely nonsensical, suggesting a rapid improvement
    in MPG once the engine displacement rises over 500 cubic inches. You must keep
    this natural mathematical behavior of polynomial functions in mind if you’re considering
    using higher-order terms in your regression models.
  prefs: []
  type: TYPE_NORMAL
- en: To create this plot, the same code that created the left plot can be used; you
    simply use `xlim` to widen the *x*-axis range and define the `disp.seq` object
    to a correspondingly wider sequence (in this case, I just set `xlim=c(10,1000)`
    with matching `from` and `to` limits in the creation of `disp.seq`).
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Models like this are still referred to as* linear *regression models, which
    might seem a bit confusing since the fitted trends for higher-order polynomials
    are clearly nonlinear. This is because* linear regression *refers to the fact
    that the function defining the mean response is linear in terms of the regression
    parameters* *β*[0], *β*[1], . . ., [*β*p]*. As such, any transformation applied
    to individual variables doesn’t affect the linearity of the function with respect
    to the coefficients themselves.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***21.4.2 Logarithmic***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In statistical modeling situations where you have positive numeric observations,
    it’s common to perform a log transformation of the data to dramatically reduce
    the overall range of the data and bring extreme observations closer to a measure
    of centrality. In that sense, transforming to a logarithmic scale can help reduce
    the severity of heavily skewed data (see [Section 15.2.4](ch15.xhtml#ch15lev2sec136)).
    In the context of regression modeling, log transformations can be used to capture
    trends where apparent curves “flatten off,” without the same kind of instability
    outside the range of the observed data that you saw with some of the polynomials.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to refresh your memory on logarithms, turn back to [Section 2.1.2](ch02.xhtml#ch02lev2sec18);
    it suffices here to note that the logarithm is the power to which you must raise
    a base value in order to obtain an *x* value. For example, in 3⁵ = 243, the logarithm
    is 5 and 3 is the base, expressed as log[3] 243 = 5\. Because of the ubiquity
    of the exponential function in common probability distributions, statisticians
    almost exclusively work with the natural log (logarithm to the base *e*). From
    here, assume all mentions of the log transformation refer to the natural log.
  prefs: []
  type: TYPE_NORMAL
- en: 'To briefly illustrate the typical behavior of the log transformation, take
    a look at [Figure 21-5](ch21.xhtml#ch21fig5), achieved with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This plots the log of the integers 1 to 1000 against the raw values, as well
    as plotting the negative log. You can see the way in which the log-transformed
    values taper off and flatten out as the raw values increase.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f21-05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 21-5: The log function applied to integers 1 to 1000*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fitting the Log Transformation**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As noted, one use of the log transformation in regression is to allow this kind
    of curvature in situations when a perfectly straight line doesn’t suit the observed
    relationship. For an illustration, return to the `mtcars` examples and consider
    mileage as a function of both horsepower and transmission type (variables `hp`
    and `am`, respectively). Create a scatterplot of MPG against horsepower, with
    different colors distinguishing between automatic and manual cars.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The plotted points shown in [Figure 21-6](ch21.xhtml#ch21fig6) suggest that
    curved trends in horsepower may be more appropriate than straight-line relationships.
    Note that you have to explicitly coerce the binary numeric `mtcars$am` vector
    to a factor here in order to use it as a selector for the vector of two colors.
    You’ll add the lines in after fitting the linear model.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f21-06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 21-6: Scatterplot of MPG on horsepower, split by transmission type,
    with lines corresponding to a multiple linear regression using a log-scaled effect
    of horsepower superimposed*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s do so using the log transformation of horsepower to try to capture the
    curved relationship. Since, in this example, you also want to account for the
    potential of transmission type to affect the response, this is included as an
    additional predictor variable as usual.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The output indicates jointly statistically significant effects of both log-horsepower
    and transmission type on mileage. Keeping transmission constant, the mean MPG
    drops by around 9.24 for each additional unit of log-horsepower. Having a manual
    transmission increases the mean MPG by roughly 4.2 (estimated in this order owing
    to the coding of `am`—`0` for automatic, `1` for manual; see `?mtcars`). The coefficient
    of determination shows 82.7 percent of the variation in the response is explained
    by this regression, suggesting a satisfactory fit.
  prefs: []
  type: TYPE_NORMAL
- en: '**Plotting the Log Transformation Fit**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To visualize the fitted model, you first need to calculate the fitted values
    for all desired predictor values. The following code creates a sequence of horsepower
    values (minus and plus 20 horsepower) and performs the required prediction for
    both transmission types.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the above code, since you want to plot predictions for both possible values
    of `am`, when using `newdata` you need to replicate `hp.seq` twice. Then, when
    you provide values for `am` to `newdata`, one series of `hp.seq` is paired with
    an appropriately replicated `am` value of `0`, the other with `1`. The result
    of this is a vector of predictions of length twice that of `hp.seq`, `car.log.pred`,
    with the first `n` elements corresponding to automatic cars and the latter `n`
    to manuals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can add these lines to [Figure 21-6](ch21.xhtml#ch21fig6) with the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: By examining the scatterplot, you can see that the fitted model appears to do
    a good job of estimating the joint relationship between horsepower/transmission
    and MPG. The statistical significance of transmission type in this model directly
    affects the difference between the two added lines. If `am` weren’t significant,
    the lines would be closer together; in that case, the model would be suggesting
    that one curve would be sufficient to capture the relationship. As usual, extrapolation
    too far outside the range of the observed predictor data isn’t a great idea, though
    it’s less unstable for log-transformed trends than for polynomial functions.
  prefs: []
  type: TYPE_NORMAL
- en: '***21.4.3 Other Transformations***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Transformation can involve more than one variable of the data set and isn’t
    limited to just predictor variables either. In their original investigation into
    the `mtcars` data, Henderson and Velleman ([1981](ref.xhtml#ref32)) also noted
    the presence of the same curved relationships you’ve uncovered between the response
    and variables such as horsepower and displacement. They argued that it’s preferable
    to use gallons per mile (GPM) instead of MPG as the response variable to improve
    linearity. This would involve modeling a transformation of MPG, namely, that GPM
    = 1/MPG.
  prefs: []
  type: TYPE_NORMAL
- en: The authors also commented on the limited influence that both horsepower and
    displacement have on GPM if the weight of the car is included in a fitted model,
    because of the relatively high correlations present among these three predictors
    (known as *multicollinearity*). To address this, the authors created a new predictor
    variable calculated as horsepower divided by weight. This measures, in their words,
    how “overpowered” a car is—and they proceeded to use that new predictor instead
    of horsepower or displacement alone. This is just some of the experimentation
    that took place in the search for an appropriate way to model these data.
  prefs: []
  type: TYPE_NORMAL
- en: To this end, however you choose to model your own data, the objective of transforming
    numeric variables should always be to fit a valid model that represents the data
    and the relationships more realistically and accurately. When reaching for this
    goal, there’s plenty of freedom in how you can transform numeric observations
    in applications of regression methods. For a further discussion on transformations
    in linear regression, [Chapter 7](ch07.xhtml#ch07) of Faraway ([2005](ref.xhtml#ref21))
    provides an informative introduction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 21.2**'
  prefs: []
  type: TYPE_NORMAL
- en: The following table presents data collected in one of Galileo’s famous “ball”
    experiments, in which he rolled a ball down a ramp of different heights and measured
    how far it traveled from the base of the ramp. For more on this and other interesting
    examples, look at “Teaching Statistics with Data of Historic Significance” by
    Dickey and Arnold ([1995](ref.xhtml#ref17)).
  prefs: []
  type: TYPE_NORMAL
- en: '| **Initial height** | **Distance** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1000 | 573 |'
  prefs: []
  type: TYPE_TB
- en: '| 800 | 534 |'
  prefs: []
  type: TYPE_TB
- en: '| 600 | 495 |'
  prefs: []
  type: TYPE_TB
- en: '| 450 | 451 |'
  prefs: []
  type: TYPE_TB
- en: '| 300 | 395 |'
  prefs: []
  type: TYPE_TB
- en: '| 200 | 337 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 253 |'
  prefs: []
  type: TYPE_TB
- en: Create a data frame in R based on this table and plot the data points with distance
    on the *y*-axis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Galileo believed there was a quadratic relationship between initial height and
    the distance traveled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit an order 2 polynomial in height, with distance as the response.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a cubic (order 3) and a quartic (order 4) model for these data. What do
    they tell you about the nature of the relationship?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on your models from (b), choose the one that you think best represents
    the data and plot the fitted line on the raw data. Add 90 percent confidence bands
    for mean distance traveled to the plot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The contributed R package `faraway` contains a large number of data sets that
    accompany a linear regression textbook by Faraway ([2005](ref.xhtml#ref21)). Install
    the package and then call `library("faraway")` to load it. One of the data sets
    is `trees`, which provides data on the dimensions of felled trees of a certain
    type (see, for example, [Atkinson, 1985](ref.xhtml#ref04)).
  prefs: []
  type: TYPE_NORMAL
- en: Access the data object at the prompt and plot volume against girth (the latter
    along the *x*-axis).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fit two models with `Volume` as the response: one quadratic model in `Girth`
    and the other based on log transformations of both `Volume` and `Girth`. Write
    down the model equations for each and comment on the similarity (or difference)
    of the fits in terms of the coefficient of determination and the omnibus *F*-test.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `predict` to add lines to the plot from (d) for each of the two models from
    (e). Use different line types; add a corresponding legend. Also include 95 percent
    prediction intervals, with line types matching those of the fitted values (note
    that for the model that involves log transformation of the response and the predictor,
    any returned values from `predict` will themselves be on the log scale; you have
    to back-transform these to the original scale using `exp` before the lines for
    that model can be superimposed). Comment on the respective fits and their estimated
    prediction intervals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, turn your attention back to the `mtcars` data frame.
  prefs: []
  type: TYPE_NORMAL
- en: Fit and summarize a multiple linear regression model to determine mean MPG from
    horsepower, weight, and displacement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the spirit of Henderson and Velleman ([1981](ref.xhtml#ref32)), use `I` to
    refit the model in (g) in terms of GPM = 1/MPG. Which model explains a greater
    amount of variation in the response?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**21.5 Interactive Terms**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, you’ve looked only at the joint main effects of how predictors affect
    the outcome variable (and one-to-one transformations thereof). Now you’ll look
    at interactions between covariates. An *interactive effect* between predictors
    is an additional change to the response that occurs at particular combinations
    of the predictors. In other words, an interactive effect is present if, for a
    given covariate profile, the values of the predictors are such that they produce
    an effect that augments the stand-alone main effects associated with those predictors.
  prefs: []
  type: TYPE_NORMAL
- en: '***21.5.1 Concept and Motivation***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Diagrams such as those found in [Figure 21-7](ch21.xhtml#ch21fig7) are often
    used to help explain the concept of interactive effects. These diagrams show your
    mean response value, ŷ, on the vertical axis, as usual, and a predictor value
    for the variable *x[1]* on the horizontal axis. They also show a binary categorical
    variable *x*[2], which can be either zero or one. These hypothetical variables
    are labeled as such in the images.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f21-07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 21-7: Concept of an interactive effect between two predictors x*[1]
    *and x*[2]*, on the mean response value* ŷ*. Left: Only main effects of x*[1]
    *and x[2]* *influence* ŷ*. Right: An interaction between x*[1] *and x[2]* *is
    needed in addition to their main effects in order to model* ŷ.'
  prefs: []
  type: TYPE_NORMAL
- en: The left diagram shows the limit of the models you’ve considered so far in this
    chapter—that both *x[1]* and *x[2]* affect ŷ independently of each other. The
    right diagram, however, clearly shows that the effect of *x[1]* on ŷ changes completely
    depending on the value of *x*[2]. On the left, only main effects of *x[1]* and
    *x[2]* are needed to determine ŷ; on the right, main effects *and* an interactive
    effect between *x[1]* and *x[2]* are present.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*When estimating regression models, you always have to accompany interactions
    with the main effects of the relevant predictors, for reasons of interpretability.
    Since interactions are themselves best understood as an augmentation of the main
    effects, it makes no sense to remove the latter and leave in the former.*'
  prefs: []
  type: TYPE_NORMAL
- en: For a good example of an interaction, think about pharmacology. Interactive
    effects between medicines are relatively common, which is why health care professionals
    often ask about other medicines you might be taking. Consider statins—drugs commonly
    used to reduce cholesterol. Users of statins are told to avoid grapefruit juice
    because it contains natural chemical compounds that inhibit the efficacy of the
    enzyme responsible for the correct metabolization of the drug. If an individual
    is taking statins and not consuming grapefruit, you would expect a negative relationship
    between cholesterol level and statin use (think about “statin use” either as a
    continuous or as a categorical dosage variable)—as statin use increases or is
    affirmative, the cholesterol level decreases. On the other hand, for an individual
    on statins who *is* consuming grapefruit, the nature of the relationship between
    cholesterol level and statin use could easily be different—weakened negative,
    neutral, or even positive. If so, since the effect of the statins on cholesterol
    changes according to the value of another variable—whether or not grapefruit is
    consumed—this would be considered an interaction between those two predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Interactions can occur between categorical variables, numeric variables, or
    both. It’s most common to find *two-way* interactions—interactions between exactly
    two predictors—which is what you’ll focus on in [Sections 21.5.2](ch21.xhtml#ch21lev2sec203)
    to [21.5.4](ch21.xhtml#ch21lev2sec205). Three-way and higher-order interactive
    effects are technically possible but less common, partly because they are difficult
    to interpret in a real-world context. You’ll consider an example of these in [Section
    21.5.5](ch21.xhtml#ch21lev2sec206).
  prefs: []
  type: TYPE_NORMAL
- en: '***21.5.2 One Categorical, One Continuous***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generally, a two-way interaction between a categorical and a continuous predictor
    should be understood as effecting a change in the slope of the continuous predictor
    with respect to the nonreference levels of the categorical predictor. In the presence
    of a term for the continuous variable, a categorical variable with *k* levels
    will have *k* − 1 main effect terms, so there will be a further *k* − 1 interactive
    terms between all the alternative levels of the categorical variable and the continuous
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: The different slopes for *x[1]* by category of *x[2]* for ŷ can be seen clearly
    on the right of [Figure 21-7](ch21.xhtml#ch21fig7). In such a situation, in addition
    to the main effects for *x[1]* and *x*[2], there would be one interactive term
    in the fitted model corresponding to *x[2]* = 1\. This defines the additive term
    needed to change the slope in *x[1]* for *x[2]* = 0 to the new slope in *x[1]*
    for *x[2]* = 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'For an example, let’s access a new data set. In [Exercise 21.2](ch21.xhtml#ch21exc2),
    you looked at the `faraway` package ([Faraway, 2005](ref.xhtml#ref21)) to access
    the `trees` data. In this package, you’ll also find the `diabetes` object—a cardiovascular
    disease data set detailing characteristics of 403 African Americans (originally
    investigated and reported in [Schorling et al., 1997](ref.xhtml#ref59); [Willems
    et al., 1997](ref.xhtml#ref76)). Install `faraway` if you haven’t already and
    load it with `library("faraway")`. Restrict your attention to the total cholesterol
    level (`chol`—continuous), age of the individual (`age`—continuous), and body
    frame type (`frame`—categorical with *k* = 3 levels: `"small"` as the reference
    level, `"medium"`, and `"large"`). You can see the data in [Figure 21-8](ch21.xhtml#ch21fig8),
    which will be created momentarily.'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll look at modeling total cholesterol by age and body frame. It seems logical
    to expect that cholesterol is related to both age and body type, so it makes sense
    to also consider the possibility that the effect of age on cholesterol is different
    for individuals of different body frames. To investigate, let’s fit the multiple
    linear regression and include a two-way interaction between the two variables.
    In the call to `lm`, you specify the main effects first, using `+` as usual, and
    then specify an interactive effect of two predictors by using a colon (`:`) between
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Inspecting the estimated model parameters in the output, you can see a main
    effect coefficient for `age`, main effect coefficients for the two levels of `frame`
    (that aren’t the reference level), and two further terms for the interactive effect
    of `age` with those same nonreference levels.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*There’s actually a shortcut to doing this in R—the* cross-factor *notation.
    The same model shown previously could have been fitted by using* `chol~age*frame`
    *in* `lm`*; the symbol* `*` *between two variables in a formula should be interpreted
    as “include an intercept, all main effects, and the interaction.” I’ll use this
    shortcut from now on.*'
  prefs: []
  type: TYPE_NORMAL
- en: The output shows the significance of `age` and some evidence to support the
    presence of a main effect of `frame`. There’s also slight indication of significance
    of the interaction, though it’s weak. Assessing significance in this case, where
    one predictor is categorical with *k* > 2 levels, follows the same rule as noted
    in the discussion of multilevel variables in [Section 20.5.2](ch20.xhtml#ch20lev2sec186)—if
    at least one of the coefficients is significant, the entire effect should be deemed
    significant.
  prefs: []
  type: TYPE_NORMAL
- en: The general equation for the fitted model can be written down directly from
    the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e21-7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: I’ve used a colon (:) to denote the interactive terms to mirror the R output.
  prefs: []
  type: TYPE_NORMAL
- en: For the reference level of the categorical predictor, body type “small,” the
    fitted model can be written down straight from the output.
  prefs: []
  type: TYPE_NORMAL
- en: “Mean total cholesterol” = 155.9636 + 0.9852 × “age”
  prefs: []
  type: TYPE_NORMAL
- en: For a model with the main effects only, changing body type to “medium” or “large”
    would affect only the intercept—you know from [Section 20.5](ch20.xhtml#ch20lev1sec66)
    that the relevant effect is simply added to the outcome. The presence of the interaction,
    however, means that *in addition* to the change in the intercept, the main effect
    slope of `age` must now also be changed according to the relevant interactive
    term. For an individual with a “medium” frame, the model is
  prefs: []
  type: TYPE_NORMAL
- en: '| “Mean total cholesterol” | = | 155.9636 + 0.9852 × “age” + 28.6051 − 0.3514
    × “age” |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | 184.5687 + (0.9852 − 0.3514) × “age” |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | 184.5687 + 0.6338 × “age” |'
  prefs: []
  type: TYPE_TB
- en: and for an individual with a “large” frame, the model is
  prefs: []
  type: TYPE_NORMAL
- en: '| “Mean total cholesterol” | = | 155.9636 + 0.9852 × “age” + 44.9474 − 0.8511
    × “age” |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | 200.911 + (0.9852 − 0.8511) × “age” |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | 200.911 + 0.1341 × “age” |'
  prefs: []
  type: TYPE_TB
- en: 'You can easily calculate these in R by accessing the coefficients of the fitted
    model object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s sum the relevant components of this vector. Once you have the sums,
    you’ll be able to plot the fitted model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The three lines are stored as numeric vectors of length 2, with the intercept
    first and the slope second. This is the form required by the optional `coef` argument
    of `abline`, which allows you to superimpose these straight lines on a plot of
    the raw data. The following code produces [Figure 21-8](ch21.xhtml#ch21fig8).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/f21-08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 21-8: Fitted linear model, main effects, and interaction for mean total
    cholesterol by age and body frame*'
  prefs: []
  type: TYPE_NORMAL
- en: If you examine the fitted model in [Figure 21-8](ch21.xhtml#ch21fig8), it’s
    clear that inclusion of an interaction between age and body frame has allowed
    more flexibility in the way mean total cholesterol relates to the two predictors.
    The nonparallel nature of the three plotted lines reflects the concept illustrated
    in [Figure 21-7](ch21.xhtml#ch21fig7).
  prefs: []
  type: TYPE_NORMAL
- en: I walked through this to illustrate how the concept works, but in practice you
    don’t need to go through all of these steps to find the point estimates (and any
    associated confidence intervals). You can predict from a fitted linear model with
    interactions in the same way as for main-effect-only models through the use of
    `predict`.
  prefs: []
  type: TYPE_NORMAL
- en: '***21.5.3 Two Categorical***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You met the concept of interactions between two categorical explanatory variables
    in the introduction to two-way ANOVA in [Section 19.2](ch19.xhtml#ch19lev1sec60).
    There, you uncovered evidence of an interactive effect of wool type and tension
    on the mean number of warp breaks in lengths of yarn (based on the ready-to-use
    `warpbreaks` data frame). You then visualized the interaction with an interaction
    plot ([Figure 19-2](ch19.xhtml#ch19fig2) on [page 447](ch19.xhtml#page_447)),
    not unlike the diagrams in [Figure 21-7](ch21.xhtml#ch21fig7).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement the same model as the last `warpbreaks` example in [Section
    19.2.2](ch19.xhtml#ch19lev2sec172) in an explicit linear regression format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Here I’ve used the cross-factor symbol `*`, rather than `wool + tension + wool:tension`.
    When both predictors in a two-way interaction are categorical, there will be a
    term for each nonreference level of the first predictor combined with all nonreference
    levels of the second predictor. In this example, `wool` is binary with only *k*
    = 2 levels and `tension` has *k* = 3; therefore, the only interaction terms present
    are the “medium” (`M`) and “high” (`H`) tension levels (“low”, `L`, is the reference
    level) with wool type `B` (`A` is the reference level). Therefore, altogether
    in the fitted model, there are terms for `B`, `M`, `H`, `B:M`, and `B:H`.
  prefs: []
  type: TYPE_NORMAL
- en: These results provide the same conclusion as the ANOVA analysis—there is indeed
    statistical evidence of an interactive effect between wool type and tension on
    mean breaks, on top of the contributing main effects of those predictors.
  prefs: []
  type: TYPE_NORMAL
- en: The general fitted model can be understood as
  prefs: []
  type: TYPE_NORMAL
- en: '| “Mean warp breaks” | = | 44.556 − 16.333 × “wool type B” |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | − 20.556 × “medium tension” |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | − 20.000 × “high tension” |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | + 21.111 × “wool type B : medium tension” |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | + 10.556 × “wool type B : high tension” |'
  prefs: []
  type: TYPE_TB
- en: The additional interaction terms work the same way as the main effects—when
    only categorical predictors are involved, the model can be seen as a series of
    additive terms to the overall intercept. Exactly which ones you use in any given
    prediction depends on the covariate profile of a given individual.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a quick series of examples: for wool A at low tension, the mean
    number of warp breaks is predicted as simply the overall intercept; for wool A
    at high tension, you have the overall intercept and the main effect term for high
    tension; for wool B at low tension, you have the overall intercept and the main
    effect for wool type B only; and for wool B at medium tension, you have the overall
    intercept, the main effect for wool type B, the main effect for medium tension,
    *and* the interactive term for wool B with medium tension.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use `predict` to estimate the mean warp breaks for these four scenarios;
    they’re accompanied here with 90 percent confidence intervals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '***21.5.4 Two Continuous***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Finally, you’ll look at the situation when the two predictors are continuous.
    In this case, an interaction term operates as a modifier on the continuous plane
    that’s fitted using the main effects only. In a similar way to an interaction
    between a continuous and a categorical predictor, an interaction between two continuous
    explanatory variables allows the slope associated with one variable to be affected,
    but this time, that modification is made in a continuous way (that is, according
    to the value of the other continuous variable).
  prefs: []
  type: TYPE_NORMAL
- en: Returning to the `mtcars` data frame, consider MPG once more as a function of
    horsepower and weight. The fitted model, shown next, includes the interaction
    in addition to the main effects of the two continuous predictors. As you can see,
    there is a single estimated interactive term, and it is deemed significantly different
    from zero.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The model is written as
  prefs: []
  type: TYPE_NORMAL
- en: '| “Mean MPG” | = | 49.80842 − 0.12010 × “horsepower” |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | − 8.21662 × “weight” |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | + 0.02785 × “horsepower : weight” |'
  prefs: []
  type: TYPE_TB
- en: '|  | = | 49.80842 − 0.12010 × “horsepower” |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | − 8.21662 × “weight” |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | + 0.02785 × “horsepower” × “weight” |'
  prefs: []
  type: TYPE_TB
- en: The second version of the model equation provided here reveals for the first
    time an interaction expressed as the *product* of the values of the two predictors,
    which is exactly how the fitted model is used to predict the response. (Technically,
    this is the same as when at least one of the predictors is categorical—but the
    dummy coding simply results in zeros and ones for the respective terms, so multiplication
    just amounts to the presence or absence of a given term, as you’ve seen.)
  prefs: []
  type: TYPE_NORMAL
- en: You can interpret an interaction between two continuous predictors by considering
    the sign (+ or −) of the coefficient. Negativity suggests that as the values of
    the predictors increase, the response is reduced after computing the result of
    the main effects. Positivity, as is the case here, suggests that as the values
    of the predictors increase, the effect is an additional increase, an amplification,
    on the mean response.
  prefs: []
  type: TYPE_NORMAL
- en: Contextually, the negative main effects of `hp` and `wt` indicate that mileage
    is naturally reduced for heavier, more powerful cars. However, positivity of the
    interactive effect suggests that this impact on the response is “softened” as
    horsepower or weight is increased. To put it another way, the negative relationship
    imparted by the main effects is rendered “less extreme” as the values of the predictors
    get bigger and bigger.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 21-9](ch21.xhtml#ch21fig9) contrasts the main-effects-only version
    of the model (obtained using `lm` with the formula `mpg~hp+wt`; not explicitly
    fitted in this section) with the interaction version of the model fitted just
    above as the object `car.fit`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f21-09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 21-9: Response surfaces for mean MPG by horsepower and weight, for
    a main-effects-only model (left), and one that includes the two-way interaction
    between the continuous predictors (right)*'
  prefs: []
  type: TYPE_NORMAL
- en: The plotted *response surfaces* show the mean MPG on the vertical *z*-axis and
    the two predictor variables on the horizontal axes as marked. You can interpret
    the predicted mean MPG, based on a given horsepower and weight value, as a point
    on the surface. Note that both surfaces decrease in MPG (vertically along the
    *z*-axis) as you move to larger values of either predictor along the respective
    horizontal axes.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll show how these plots are created in [Chapter 25](ch25.xhtml#ch25). For
    now, they serve simply to highlight the aforementioned “softening” impact of the
    interaction in `car.fit`. On the left, the main-effects-only model shows a flat
    plane decreasing according to the negative linear slopes in each predictor. On
    the right, however, the presence of the positive interactive term flattens this
    plane out, meaning the rate of decrease is slowed as the values of the predictor
    variables increase.
  prefs: []
  type: TYPE_NORMAL
- en: '***21.5.5 Higher-Order Interactions***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned, two-way interactions are the most common kind of interactions
    you’ll encounter in applications of regression methods. This is because for three-way
    or higher-order terms, you need a lot more data for a reliable estimation of interactive
    effects, and there are a number of interpretative complexities to overcome. Three-way
    interactions are far rarer than two-way effects, and four-way and above are rarer
    still.
  prefs: []
  type: TYPE_NORMAL
- en: In [Exercise 21.1](ch21.xhtml#ch21exc1), you used the `nuclear` data set found
    in the `boot` package (provided with the standard R installation), which includes
    data on the constructions of nuclear power plants in the United States. In the
    exercises, you focused mainly on date and time predictors related to construction
    permits to model the mean cost of construction for the nuclear power plants. For
    the sake of this example, assume you don’t have the data on these predictors.
    Can the cost of construction be adequately modeled using only the variables that
    describe characteristics of the plant itself?
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `boot` package and access the `?nuclear` help page to find details
    on the variables: `cap` (continuous variable describing the capacity of the plant);
    `cum.n` (treated as continuous, describing the number of similar constructions
    the engineers had previously worked on); `ne` (binary, describing whether the
    plant was in the northeastern United States); and `ct` (binary, describing whether
    the plant had a cooling tower).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following model is fitted with the final construction cost of the plant
    as the response; a main effect for capacity; and main effects of, and all two-way
    interactions and the three-way interaction among, `cum.n`, `ne`, and `ct`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In this code, you specify the higher-order interactions by extending the number
    of variables connected with a `*` (using `*` instead of `:` since you want to
    include all the lower-order effects of those three predictors as well).
  prefs: []
  type: TYPE_NORMAL
- en: In the estimated results, the main effect for `cap` is positive, showing that
    an increased power capacity is tied to an increased construction cost. All other
    main effects are negative, which at face value seems to imply that a reduced construction
    cost is associated with more experienced engineers, plants constructed in the
    Northeast, and plants with a cooling tower. However, this isn’t an accurate statement
    since you haven’t yet considered the interactive terms in those predictors. All
    estimated two-way interactive effects are positive—having more experienced engineers
    means a higher construction cost in the Northeast regardless of whether there’s
    a cooling tower, and having more experienced engineers also means higher costs
    for plants *with* a cooling tower, regardless of region.
  prefs: []
  type: TYPE_NORMAL
- en: Cost is also dramatically increased for plants in the Northeast with a cooling
    tower, regardless of the experience of the engineer. All that being said, the
    negative three-way interaction suggests that the increased cost associated with
    more experienced engineers working in the Northeast *and* on a plant with a cooling
    tower is lessened somewhat after the main effects and two-way interactive effects
    are calculated.
  prefs: []
  type: TYPE_NORMAL
- en: At the least, this example highlights the complexities associated with interpreting
    model coefficients for higher-order interactions. It’s also possible that statistically
    significant high-order interactions crop up due to lurking variables that have
    gone unaccounted for, that is, that the significant interactions are a spurious
    manifestation of patterns in the data that simpler terms involving those missing
    predictors could explain just as well (if not better). In part, this motivates
    the importance of adequate *model selection*, which is up next in the discussion.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 21.3**'
  prefs: []
  type: TYPE_NORMAL
- en: Return your attention to the `cats` data frame in package `MASS`. In the first
    few problems in [Exercise 21.1](ch21.xhtml#ch21exc1), you fitted the main-effect-only
    model to predict the heart weights of domestic cats by total body weight and sex.
  prefs: []
  type: TYPE_NORMAL
- en: Fit the model again, and this time include an interaction between the two predictors.
    Inspect the model summary. What do you notice in terms of the parameter estimates
    and their significance when compared to the earlier main-effect-only version?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Produce a scatterplot of heart weight on body weight, using different point
    characters or colors to distinguish the observations according to sex. Use `abline`
    to add two lines denoting the fitted model. How does this plot differ from the
    one in [Exercise 21.1](ch21.xhtml#ch21exc1) (d)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predict the heart weight of Tilman’s cat using the new model (remember that
    Sigma is a 3.4 kg female) accompanied by a 95 percent prediction interval. Compare
    it to the main-effects-only model from the earlier exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In [Exercise 21.2](ch21.xhtml#ch21exc2), you accessed the `trees` data frame
    in the contributed `faraway` package. After loading the package, access the `?trees`
    help file; you’ll find the volume and girth measurements you used earlier, as
    well as data on the height of each tree.
  prefs: []
  type: TYPE_NORMAL
- en: Without using any transformations of the data, fit and inspect a main-effects-only
    model for predicting volume from girth and height. Then, fit and inspect a second
    version of this model including an interaction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat (d), but this time use the log transformation of all variables. What
    do you notice about the significance of the interaction between the untransformed
    and transformed models? What does this suggest about the relationships in the
    data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Turn back to the `mtcars` data set and remind yourself of the variables in the
    help file `?mtcars`.
  prefs: []
  type: TYPE_NORMAL
- en: Fit a linear model for `mpg` based on a two-way interaction between `hp` and
    `factor(cyl)` and their main effects, as well as a main effect for `wt`. Produce
    a summary of the fit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interpret the estimated coefficients for the interaction between horsepower
    and the (categorical) number of cylinders.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Suppose you’re keen on purchasing a 1970s performance car. Your mother advises
    you to purchase a “practical and economical” car that’s capable of an average
    MPG value of at least 25\. You see three vehicles advertised: car 1 is a four-cylinder,
    100 horsepower car that weighs 2100 lbs; car 2 is an eight-cylinder, 210 horsepower
    car that weighs 3900 lbs; and car 3 is a six-cylinder, 200 horsepower car that
    weighs 2900 lbs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your model to predict the mean MPG for each of the three cars; provide 95
    percent confidence intervals. Based on your point estimates only, which car would
    you propose to your mother?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: You still want the most gas-guzzling car you can own with your mother’s blessing,
    so you decide to be sneaky and base your decision on what the confidence intervals
    tell you instead. Does this change your choice of vehicle?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Important Code in This Chapter**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| **Function/operator** | **Brief description** | **First occurrence** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `I` | Include arithmetic term | [Section 21.4.1](ch21.xhtml#ch21lev2sec199),
    [p. 505](ch21.xhtml#page_505) |'
  prefs: []
  type: TYPE_TB
- en: '| `:` | Interaction term | [Section 21.5.2](ch21.xhtml#ch21lev2sec203), [p.
    516](ch21.xhtml#page_516) |'
  prefs: []
  type: TYPE_TB
- en: '| `*` | Cross-factor operator | [Section 21.5.3](ch21.xhtml#ch21lev2sec204),
    [p. 519](ch21.xhtml#page_519) |'
  prefs: []
  type: TYPE_TB
