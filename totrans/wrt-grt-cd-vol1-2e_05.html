<html><head></head><body>
<h2 class="h2" id="ch06"><span epub:type="pagebreak" id="page_131"/><strong><span class="big">6</span><br/>MEMORY ORGANIZATION AND ACCESS</strong></h2>&#13;
<div class="image1"><img alt="Image" src="../images/comm1.jpg"/></div>&#13;
<p class="noindents">This chapter describes the basic components of a computer system: the CPU, memory, I/O, and the bus that connects them. We’ll begin by discussing bus organization and memory organization. These two hardware components may have as large a performance impact on your software as the CPU’s speed. Understanding memory performance characteristics, data locality, and cache operation can help you design software that runs as fast as possible.</p>&#13;
<h3 class="h3" id="sec6_1"><strong>6.1 The Basic System Components</strong></h3>&#13;
<p class="noindent">The basic operational design of a computer system is called its <em>architecture</em>. John von Neumann, a pioneer in computer design, is credited with the principal architecture in use today. For example, the 80x86 family uses the <em><a href="gloss01.xhtml#gloss01_260">von Neumann architecture (VNA)</a></em>. A typical VNA has three major components: the <em><a href="gloss01.xhtml#gloss01_46">central processing unit (CPU)</a></em>, <em>memory</em>, and <em>input/output (I/O)</em>, as shown in <a href="ch06.xhtml#ch06fig01">Figure 6-1</a>.</p>&#13;
<div class="image"><img alt="image" src="../images/06fig01.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_132"/><a id="ch06fig01"/><em>Figure 6-1: Typical von Neumann machine</em></p>&#13;
<p class="indent">In VNA machines, like the 80x86 systems, all computations occur within the CPU. Data and machine instructions reside in memory until the CPU requires them, at which point the system transfers the data into the CPU. To the CPU, most I/O devices look like memory; the major difference between them is that I/O devices are generally located in the outside world, whereas memory is located within the same machine.</p>&#13;
<h4 class="h4" id="sec6_1_1"><strong><em>6.1.1 The System Bus</em></strong></h4>&#13;
<p class="noindent">The <em><a href="gloss01.xhtml#gloss01_243">system bus</a></em> connects the various components of a VNA machine. A <em>bus</em> is a collection of wires on which electrical signals pass between system components. Most CPUs have three major buses: the <em>data</em> bus, the <em>address</em> bus, and the <em>control</em> bus. These buses vary from processor to processor, but each bus carries comparable information on most CPUs. For example, the data buses on the Pentium and 80386 have different implementations, but both variants carry data between the processor, I/O, and memory.</p>&#13;
<h5 class="h5" id="sec6_1_1_1"><strong>6.1.1.1 The Data Bus</strong></h5>&#13;
<p class="noindent">CPUs use the data bus to shuttle data between the various components in a computer system. The size of this bus varies widely among CPUs. Indeed, bus size (or <em>width</em>) is one of the main attributes that defines the “size” of the processor.</p>&#13;
<p class="indent">Most modern, general-purpose CPUs (such as those in PCs) employ a 32-bit-wide or, more commonly, 64-bit-wide data bus. Some processors use 8-bit or 16-bit data buses, and there may well be some CPUs with 128-bit data buses by the time you read this.</p>&#13;
<p class="indent">You’ll often hear the terms <em>8-</em>, <em>16-</em>, <em>32-</em>, or <em>64-bit processor</em>. Processor size is determined by whichever value is smaller: the number of data lines on <span epub:type="pagebreak" id="page_133"/>the processor or the size of the largest general-purpose integer register. For example, older Intel 80x86 CPUs all have 64-bit buses but only 32-bit general-purpose integer registers, so they’re classified as 32-bit processors. The AMD (and newer Intel) x86-64 processors support 64-bit integer registers and a 64-bit bus, so they’re 64-bit processors.</p>&#13;
<p class="indent">Although the 80x86 family members with 8-, 16-, 32-, and 64-bit data buses can process data blocks up to the bit width of the bus, they can also access smaller memory units of 8, 16, or 32 bits. Therefore, anything you can do with a small data bus can be done with a larger data bus as well; the larger data bus, however, may access memory faster and can access larger chunks of data in one memory operation. You’ll read about the exact nature of these memory accesses a little later in this chapter.</p>&#13;
<h5 class="h5" id="sec6_1_1_2"><strong>6.1.1.2 The Address Bus</strong></h5>&#13;
<p class="noindent">The data bus on an 80x86 family processor transfers information between a particular memory location or I/O device and the CPU. <em>Which</em> memory location or I/O device is where the address bus comes in. The system designer assigns each memory location and I/O device a unique memory address. When the software wants to access a particular memory location or I/O device, it places the corresponding address on the address bus. Circuitry within the device checks the address and, if it matches, transfers data. All other memory locations ignore the request on the address bus.</p>&#13;
<p class="indent">With a single address bus line, a processor can access exactly two unique addresses: 0 and 1. With <em>n</em> address lines, the processor can access 2<sup><em>n</em></sup> unique addresses (because there are 2<sup><em>n</em></sup> unique values in an <em>n</em>-bit binary number). The number of bits on the address bus determines the <em>maximum</em> number of addressable memory and I/O locations. Early 80x86 processors, for example, provided only 20 lines on the address bus. Therefore, they could access only up to 1,048,576 (or 2<sup>20</sup>) memory locations. Larger address buses can access more memory (see <a href="ch06.xhtml#ch06tab01">Table 6-1</a>).</p>&#13;
<p class="tabcap"><a id="ch06tab01"/><strong>Table 6-1:</strong> 80x86 Addressing Capabilities</p>&#13;
<table class="topbot-d">&#13;
<colgroup>&#13;
<col style="width:30%"/>&#13;
<col style="width:35%"/>&#13;
<col style="width:35%"/>&#13;
</colgroup>&#13;
<tbody>&#13;
<tr>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Processor</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Address bus size</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Maximum addressable memory</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">8088, 8086, 80186, 80188</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">20</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">1,048,576 (1MB)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">80286, 80386sx</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">24</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">16,777,216 (16MB)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">80386dx</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">32</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">4,294,976,296 (4GB)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">80486, Pentium</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">32</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">4,294,976,296 (4GB)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">Pentium Pro, II, III, IV</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">36</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">68,719,476,736 (64GB)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">Core, i3, i5, i7, i9</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">≥ 40</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">≥1,099,511,627,776 (≥1TB)</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Newer processors will support larger address buses. Many other processors (such as ARM and IA-64) already provide much larger addresses buses and, in fact, support addresses up to 64 bits in the software.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_134"/>A 64-bit address range is truly infinite as far as memory is concerned. No one will ever put 2<sup>64</sup> bytes of memory into a computer system and feel that they need more. Of course, people have made claims like this in the past. A few years ago, no one ever thought a computer would need 1GB of memory, yet computers with 64GB of memory (or more) are very common today. However, 2<sup>64</sup> is effectively infinity for one simple reason—it’s physically impossible to build that much memory based on estimates of the current size (about 2<sup>86</sup> different elementary particles) of the universe. Unless you can attach 1 byte of memory to every elementary particle on the planet, you won’t even come close to approaching 2<sup>64</sup> bytes of memory on a given computer system. Then again, maybe we really will use whole planets as computer systems one day, as Douglas Adams predicted in <em>The Hitchhiker’s Guide to the Galaxy</em>. Who knows?</p>&#13;
<p class="indent">While the newer 64-bit processors have an internal 64-bit address space, they rarely bring out 64 address lines on the chip. This is because pins are a precious commodity on large CPUs, and it doesn’t make sense to bring out extra address pins that will never be used. Currently, 40- to 52-bit address buses are the upper limit. In the distant future, this may expand a bit, but it’s hard to imagine the need for, or even possibility of, a physical 64-bit address bus.</p>&#13;
<p class="indent">On modern processors, CPU manufacturers are building memory controllers directly onto the CPU. Instead of having a traditional address and data bus to which you connect arbitrary memory devices, newer CPUs contain specialized buses intended to talk to very specific <em>dynamic random-access memory (DRAM)</em> modules. A typical CPU’s memory controller connects to only a certain number of DRAM modules; thus, the maximum DRAM you can easily connect to a CPU is a function of the memory control built into the CPU rather than the size of the external address bus. This is why some older laptops have a 16MB or 32MB maximum memory limitation even though they have 64-bit CPUs.<sup><a href="footnotes.xhtml#fn6_1a" id="fn6_1">1</a></sup></p>&#13;
<h5 class="h5" id="sec6_1_1_3"><strong>6.1.1.3 The Control Bus</strong></h5>&#13;
<p class="noindent">The control bus is an eclectic collection of signals that control how the processor communicates with the rest of the system. To understand its importance, consider the data bus for a moment. The CPU uses the data bus to move data between itself and memory. The system uses two lines on the control bus, <em>read</em> and <em>write</em>, to determine the data flow direction (CPU to memory, or memory to CPU). So, when the CPU wants to write data to memory, it <em>asserts</em> (places a signal on) the write control line. When the CPU wants to read data from memory, it asserts the read control line.</p>&#13;
<p class="indent">Although the exact composition of the control bus varies among processors, some control lines—like the system clock lines, interrupt lines, status lines, and byte enable lines—are common to all processors. The byte <span epub:type="pagebreak" id="page_135"/>enable lines appear on the control bus of some CPUs that support byte-addressable memory. These control lines allow 16-, 32-, and 64-bit processors to deal with smaller chunks of data by communicating the size of the accompanying data. Additional details appear in the sections “16-Bit Data Buses” on page <a href="#page_138">138</a> and “32-Bit Data Buses” on <a href="#page_140">page 140</a>.</p>&#13;
<p class="indent">On the 80x86 family of processors, the control bus also contains a signal that helps distinguish between address spaces. The 80x86 family, unlike many other processors, provides two distinct address spaces: one for memory and one for I/O. However, it has only one physical address bus, shared between I/O and memory, so additional control lines decide which component the address is intended for. When these signals are active, the I/O devices use the address on the LO 16 bits of the address bus. When they’re inactive, the I/O devices ignore them, and the memory subsystem takes over at that point.</p>&#13;
<h3 class="h3" id="sec6_2"><strong>6.2 Physical Organization of Memory</strong></h3>&#13;
<p class="noindent">A typical CPU addresses a maximum of 2<sup><em>n</em></sup> different memory locations, where <em>n</em> is the number of bits on the address bus (most computer systems built around 80x86 family CPUs do not include the maximum addressable amount of memory). But what exactly is a memory location? The 80x86, as an example, supports <em><a href="gloss01.xhtml#gloss01_40">byte-addressable memory</a></em>. Therefore, the basic memory unit is a byte. With address buses containing 20, 24, 32, 36, or 40 address lines, the 80x86 processors can address 1MB, 16MB, 4GB, 64GB, or 1TB of memory, respectively. Some CPU families do not provide byte-addressable memory; instead, they commonly address memory only in double-word or even quad-word chunks. However, because of the vast amount of software that <em>assumes</em> memory is byte-addressable (such as all those C/C++ programs out there), even CPUs that don’t support byte-addressable memory in hardware still use byte addresses and simulate byte addressing in software. We’ll return to this topic shortly.</p>&#13;
<p class="indent">Think of memory as an array of bytes. The address of the first byte is 0 and the address of the last byte is 2<sup><em>n</em></sup> – 1. For a CPU with a 20-bit address bus, the following pseudo-Pascal array declaration is a good approximation of memory:</p>&#13;
<p class="programs">Memory: array [0..1048575] of byte; // 1MB address space (20 bits)</p>&#13;
<p class="indent">To execute the equivalent of the Pascal statement <span class="literal">Memory [125] := 0;</span> the CPU places the value <span class="literal">0</span> on the data bus, places the address <span class="literal">125</span> on the address bus, and asserts the write line on the control bus, as shown in <a href="ch06.xhtml#ch06fig02">Figure 6-2</a>.</p>&#13;
<div class="image"><img alt="image" src="../images/06fig02.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_136"/><a id="ch06fig02"/><em>Figure 6-2: Memory write operation</em></p>&#13;
<p class="indent">To execute the equivalent of <span class="literal">CPU := Memory [125];</span> the CPU places the address <span class="literal">125</span> on the address bus, asserts the read line on the control bus, and then reads the resulting data from the data bus (see <a href="ch06.xhtml#ch06fig03">Figure 6-3</a>).</p>&#13;
<div class="image"><img alt="image" src="../images/06fig03.jpg"/></div>&#13;
<p class="figcap"><a id="ch06fig03"/><em>Figure 6-3: Memory read operation</em></p>&#13;
<p class="indent">This discussion applies <em>only</em> when the processor is accessing a single byte in memory. What happens when it accesses a word or a double word? Because memory consists of an array of bytes, how can we possibly deal with values larger than 8 bits?</p>&#13;
<p class="indent">Different computer systems have different solutions to this problem. The 80x86 family stores the LO byte of a word at the address specified and the HO byte at the next location. Therefore, a word consumes two consecutive memory addresses (as you would expect, because a word consists of 2 bytes). Similarly, a double word consumes four consecutive memory locations.</p>&#13;
<p class="indent">The address for a word or a double word is the address of its LO byte. The remaining bytes follow this LO byte, with the HO byte appearing at the address of the word plus 1 or the address of the double word plus 3 (see <a href="ch06.xhtml#ch06fig04">Figure 6-4</a>).</p>&#13;
<p class="indent">It is quite possible for byte, word, and double-word values to overlap in memory. For example, in <a href="ch06.xhtml#ch06fig04">Figure 6-4</a>, you could have a word variable <span epub:type="pagebreak" id="page_137"/>beginning at address 193, a byte variable at address 194, and a double-word value beginning at address 192. Bytes, words, and double words may begin at <em>any</em> valid address in memory. We’ll soon see, however, that starting larger objects at an arbitrary address is not a good idea.</p>&#13;
<div class="image"><img alt="image" src="../images/06fig04.jpg"/></div>&#13;
<p class="figcap"><a id="ch06fig04"/><em>Figure 6-4: Byte, word, and double-word storage in memory (on an 80x86)</em></p>&#13;
<h4 class="h4" id="sec6_2_1"><strong><em>6.2.1 8-Bit Data Buses</em></strong></h4>&#13;
<p class="noindent">A processor with an 8-bit bus (like the old 8088 CPU) can transfer 8 bits of data at a time. Because each memory address corresponds to an 8-bit byte, an 8-bit bus turns out to be the most convenient architecture (from the hardware perspective), as <a href="ch06.xhtml#ch06fig05">Figure 6-5</a> shows.</p>&#13;
<div class="image"><img alt="image" src="../images/06fig05.jpg"/></div>&#13;
<p class="figcap"><a id="ch06fig05"/><em>Figure 6-5: An 8-bit CPU &lt;–&gt; memory interface</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_138"/>The term <em>byte-addressable memory array</em> means that the CPU can address memory in chunks as small as a single byte. It also means that this is the <em>smallest</em> unit of memory you can access at once with the processor. That is, if the processor wants to access a 4-bit value, it must read 8 bits and then ignore the extra 4 bits.</p>&#13;
<p class="indent">Byte addressability <em>does not</em> imply that the CPU can access 8 bits starting at any arbitrary bit boundary. When you specify address 125 in memory, you get the entire 8 bits at that address—nothing less, nothing more. Addresses are integers; you cannot specify, for example, address 125.5 to fetch fewer than 8 bits or to fetch a byte straddling two byte addresses.</p>&#13;
<p class="indent">Although CPUs with an 8-bit data bus conveniently manipulate byte values, they can also manipulate word and double-word values. However, this requires multiple memory operations, because these processors can move only 8 bits of data at once. Loading a word requires two memory operations; loading a double word requires four memory operations.</p>&#13;
<h4 class="h4" id="sec6_2_2"><strong><em>6.2.2 16-Bit Data Buses</em></strong></h4>&#13;
<p class="noindent">Some CPUs (such as the 8086, the 80286, and variants of the ARM processor family) have a 16-bit data bus. This allows these processors to access twice as much memory in the same amount of time as their 8-bit counterparts. These processors organize memory into two <em>banks</em>: an “even” bank and an “odd” bank (see <a href="ch06.xhtml#ch06fig06">Figure 6-6</a>).</p>&#13;
<div class="image"><img alt="image" src="../images/06fig06.jpg"/></div>&#13;
<p class="figcap"><a id="ch06fig06"/><em>Figure 6-6: Byte addressing in word memory</em></p>&#13;
<p class="indent"><a href="ch06.xhtml#ch06fig07">Figure 6-7</a> illustrates the data bus connection to the CPU. In this figure, the data bus lines D0 through D7 transfer the LO byte of the word, while bus lines D8 through D15 transfer the HO byte of the word.</p>&#13;
<p class="indent">The 16-bit members of the 80x86 family can load a word from any arbitrary address. As mentioned earlier, the processor fetches the LO byte of the value from the address specified and the HO byte from the next consecutive address. However, this creates a subtle problem. What happens when you access a word that begins on an odd address? Suppose you want to read a word from location 125. The LO byte of the word comes from location 125 and the HO byte of the word comes from location 126. It turns out that there are actually <em>two</em> problems with this approach.</p>&#13;
<div class="image"><img alt="image" src="../images/06fig07.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_139"/><a id="ch06fig07"/><em>Figure 6-7: A 16-bit processor memory organization</em></p>&#13;
<p class="indent">As you can see in <a href="ch06.xhtml#ch06fig07">Figure 6-7</a>, data bus lines 8 through 15 (the HO byte) connect to the odd bank, and data bus lines 0 through 7 (the LO byte) connect to the even bank. Accessing memory location 125 will transfer data to the CPU on lines D8 through D15 of the data bus, placing the data in the HO byte, yet we need this in the LO byte! Fortunately, the 80x86 CPUs automatically recognize and handle this situation.</p>&#13;
<p class="indent">The second problem is even more obscure. When accessing words, we’re really accessing two separate bytes, each of which has its own byte address. So, what address appears on the address bus? The 16-bit 80x86 CPUs always place even addresses on the bus. Bytes at even addresses always appear on data lines D0 through D7, and bytes at odd addresses always appear on data lines D8 through D15. If you access a word at an even address, the CPU can bring in the entire 16-bit chunk in one memory operation. Likewise, if you access a single byte, the CPU activates the appropriate bank (using a byte-enable control line) and transfers that byte on the appropriate data lines for its address.</p>&#13;
<p class="indent">But what happens when the CPU accesses a word at an odd address, like the example given earlier? The CPU can’t place address 125 on the address bus and read the 16 bits from memory. There are no odd addresses coming out of a 16-bit 80x86 CPU—they’re always even. Therefore, if you try to put 125 on the address bus, 124 is what will actually appear there. Were you to read the 16 bits at this address, you would get the word at addresses 124 (LO byte) and 125 (HO byte)—not what you’d expect. Accessing a word at an odd address requires two memory operations (just as with the 8-bit bus on the 8088/80188). First, the CPU must read the byte at address 125, and then the byte at address 126. Second, it needs to swap the positions of these bytes internally because both entered the CPU on the wrong half of the data bus.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_140"/>Fortunately, the 16-bit 80x86 CPUs hide these details from you. Your programs can access words at <em>any</em> address and the CPU will properly access and swap (if necessary) the data in memory. However, because of the two operations it requires, accessing words at odd addresses on a 16-bit processor is slower than accessing words at even addresses. By carefully arranging how you use memory, you can improve the speed of your programs on these CPUs.</p>&#13;
<h4 class="h4" id="sec6_2_3"><strong><em>6.2.3 32-Bit Data Buses</em></strong></h4>&#13;
<p class="noindent">Accessing 32-bit quantities always takes at least two memory operations on the 16-bit processors. To access a 32-bit quantity at an odd address, a 16-bit processor may require three memory operations.</p>&#13;
<p class="indent">The 80x86 processors with a 32-bit data bus, such as the Pentium and Core processors, use four banks of memory connected to the 32-bit data bus (see <a href="ch06.xhtml#ch06fig08">Figure 6-8</a>).</p>&#13;
<div class="image"><img alt="image" src="../images/06fig08.jpg"/></div>&#13;
<p class="figcap"><a id="ch06fig08"/><em>Figure 6-8: 32-bit processor memory interface</em></p>&#13;
<p class="indent">With a 32-bit memory interface, the 80x86 CPU can access any single byte with one memory operation. With a 16-bit memory interface, the address placed on the address bus is always an even number; and with a 32-bit memory interface, it’s always some multiple of 4. Using various byte-enable control lines, the CPU can select which of the 4 bytes at that address the software wants to access. As with the 16-bit processor, the CPU will automatically rearrange bytes as necessary.</p>&#13;
<p class="indent">A 32-bit CPU can also access a word at most memory addresses using a single memory operation, though word accesses at certain addresses will take two memory operations (see <a href="ch06.xhtml#ch06fig09">Figure 6-9</a>). This is the same problem we encountered with the 16-bit processor attempting to retrieve a word with an <span epub:type="pagebreak" id="page_141"/>odd address, except it occurs half as often—only when the address divided by 4 leaves a remainder of 3.</p>&#13;
<div class="image"><img alt="image" src="../images/06fig09.jpg"/></div>&#13;
<p class="figcap"><a id="ch06fig09"/><em>Figure 6-9: Accessing a word on a 32-bit processor at (address mod 4) = 3</em></p>&#13;
<p class="indent">A 32-bit CPU can access a double word in a single memory operation <em>only if</em> the address of that value is evenly divisible by 4. If not, the CPU may require two memory operations.</p>&#13;
<p class="indent">Once again, the 80x86 CPU handles all this automatically. However, there’s a performance benefit to proper data alignment. Generally, the LO byte of word values should always be placed at even addresses, and the LO byte of double-word values should always be placed at addresses that are evenly divisible by 4.</p>&#13;
<h4 class="h4" id="sec6_2_4"><strong><em>6.2.4 64-Bit Data Buses</em></strong></h4>&#13;
<p class="noindent">The Pentium and later processors, like Intel i-Series, provide a 64-bit data bus and special cache memory that reduces the impact of nonaligned data access. Although there may still be a penalty for accessing data at an inappropriate address, modern x86 CPUs suffer from the problem less frequently than the earlier CPUs. We’ll look at the details in “Cache Memory” on page <a href="#page_151">151</a>.</p>&#13;
<h4 class="h4" id="sec6_2_5"><strong><em>6.2.5 Small Accesses on Non-80x86 Processors</em></strong></h4>&#13;
<p class="noindent">Although the 80x86 processor is not the only processor that will let you access a byte, word, or double-word object at an arbitrary byte address, most processors created in the past 30 years do <em>not</em> allow it. For example, the 68000 processor found in the original Apple Macintosh system would allow you to access a byte at any address, but raised an exception if you attempted to access a word at an odd address.<sup><a href="footnotes.xhtml#fn6_2a" id="fn6_2">2</a></sup> Many processors require that you access an object at an address that is a multiple of the object’s size, or they’ll raise an exception.</p>&#13;
<p class="indent">Most RISC processors, including those found in modern smartphones and tablets (typically ARM processors), do not allow you to access byte and word objects at all. Most RISC CPUs require that all data accesses be the same size as the data bus (or general-purpose integer register size, whichever is smaller). This is generally a double-word (32-bit) or quad-word (64-bit) access. If you want to access bytes or words on such a machine, you have to treat them as packed fields and use the shift and mask techniques to <span epub:type="pagebreak" id="page_142"/>extract or insert byte and word data in a double word. Although it’s nearly impossible to avoid byte accesses in software that does any character and string processing, if you expect your software to run efficiently on various modern RISC CPUs, you should avoid word data types (and the performance penalty for accessing them) in favor of double words.</p>&#13;
<h3 class="h3" id="sec6_3"><strong>6.3 Big-Endian vs. Little-Endian Organization</strong></h3>&#13;
<p class="noindent">Earlier, you read that the 80x86 CPU family stores the LO byte of a word or double-word value at a particular address in memory and the successive HO bytes at successively higher addresses. Now we’ll look in more depth at how different processors store multibyte objects in byte-addressable memory.</p>&#13;
<p class="indent">Almost every CPU whose “bit size” is some power of 2 (8, 16, 32, 64, and so on) numbers the bits and nibbles as shown in the previous chapters. There are some exceptions, but they are rare, and most of the time they represent a notational change, not a functional change (meaning you can safely ignore the difference). Once you start dealing with objects larger than 8 bits, however, things become more complicated. Different CPUs organize the bytes in a multibyte object differently.</p>&#13;
<p class="indent">Consider the layout of the bytes in a double word on an 80x86 CPU (see <a href="ch06.xhtml#ch06fig10">Figure 6-10</a> ). The LO byte, which contributes the smallest component of a binary number, sits in bit positions 0 through 7 and appears at the lowest address in memory. It seems reasonable that the bits that contribute the least would be located at the lowest address in memory.</p>&#13;
<div class="image"><img alt="image" src="../images/06fig10.jpg"/></div>&#13;
<p class="figcap"><a id="ch06fig10"/><em>Figure 6-10: Byte layout in a double word on the 80x86 processor</em></p>&#13;
<p class="indent">This is not the only possible organization, however. Some CPUs reverse the memory addresses of all the bytes in a double word, using the organization shown in <a href="ch06.xhtml#ch06fig11">Figure 6-11</a>.</p>&#13;
<div class="image"><img alt="image" src="../images/06fig11.jpg"/></div>&#13;
<p class="figcap"><a id="ch06fig11"/><em>Figure 6-11: Alternate byte layout in a double word</em></p>&#13;
<p class="indent">The original Apple Macintosh (68000 and PowerPC) and most non-80x86 Unix boxes use the data organization shown in <a href="ch06.xhtml#ch06fig11">Figure 6-11</a>. Even on 80x86 systems, certain protocols (such as network transmissions) specify this data organization. Therefore, this isn’t some rare and esoteric <span epub:type="pagebreak" id="page_143"/>convention; it’s quite common, and not something you can ignore if you work on PCs.</p>&#13;
<p class="indent">The byte organization that Intel uses is whimsically known as the <em><a href="gloss01.xhtml#gloss01_137">little-endian byte organization</a></em>. The alternate form is known as <em><a href="gloss01.xhtml#gloss01_27">big-endian byte organization</a></em>.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>These terms come from Jonathan Swift’s</em> Gulliver’s Travels; <em>the Lilliputians were arguing over whether one should open an egg by cracking it on the little end or the big end—a parody of the arguments the Catholics and Protestants were having over their respective doctrines when Swift was writing.</em></p>&#13;
</div>&#13;
<p class="indent">The time for arguing over which format is superior was back before there were several different CPUs created using different <em><a href="gloss01.xhtml#gloss01_88">endianness</a></em>. Today, that argument is irrelevant. Regardless of which format is better or worse, we have to deal with the fact that different CPUs sport different endianness, and we have to take care when writing software if we want our programs to run on both types of processors.</p>&#13;
<p class="indent">We encounter the big-endian versus little-endian problem when we try to pass binary data between two computers. For example, the double-word binary representation of 256 on a little-endian machine has the following byte values:</p>&#13;
<p class="programs">LO byte:     0<br/>&#13;
Byte #1:     1<br/>&#13;
Byte #2:     0<br/>&#13;
HO byte:     0</p>&#13;
<p class="indent">If you assemble these 4 bytes on a little-endian machine, their layout takes this form:</p>&#13;
<p class="programs">Byte:        3    2    1    0<br/>&#13;
256:         0    0    1    0    (each digit represents an 8-bit value)</p>&#13;
<p class="indent">On a big-endian machine, however, the layout takes the following form:</p>&#13;
<p class="programs">Byte:        3    2    1    0<br/>&#13;
256:         0    1    0    0    (each digit represents an 8-bit value)</p>&#13;
<p class="indent">This means that if you take a 32-bit value from one of these machines and attempt to use it on the other machine (with a different endianness), you won’t get correct results. For example, if you take a big-endian version of the value 256 and interpret it as little-endian, you’ll discover that it has a <span class="literal">1</span> in bit position 16, and a little-endian machine will think that the value is actually 65,536 (that is, <span class="literal">%1_0000_0000_0000_0000</span>).</p>&#13;
<p class="indent">When you’re exchanging data between two different machines, the best solution is to convert your values to some canonical form and then convert the canonical form back to the local format if the local and canonical formats are not the same. Exactly what constitutes a “canonical” format depends, usually, on the transmission medium. For example, when you are <span epub:type="pagebreak" id="page_144"/>transmitting data across networks, the canonical form is usually big-endian because TCP/IP and some other network protocols use the big-endian format. When you’re transmitting data across the Universal Serial Bus (USB), the canonical format is little-endian. Of course, if you control the software on both ends, the choice of canonical form is arbitrary; still, you should attempt to use the appropriate form for the transmission medium to avoid confusion down the road.</p>&#13;
<p class="indent">To convert between the endian forms, you must do a <em>mirror-image swap</em> of the bytes in the object: first swap the bytes at opposite ends of the binary number, and then work your way toward the middle of the object, swapping pairs of bytes as you go along. For example, to convert between the big-endian and little-endian format within a double word, you’d first swap bytes 0 and 3, then you’d swap bytes 1 and 2 (see <a href="ch06.xhtml#ch06fig12">Figure 6-12</a>).</p>&#13;
<div class="image"><img alt="image" src="../images/06fig12.jpg"/></div>&#13;
<p class="figcap"><a id="ch06fig12"/><em>Figure 6-12: Endian conversion in a double word</em></p>&#13;
<p class="indent">For word values, all you need to do is swap the HO and LO bytes to change the endianness. For quad-word values, you need to swap bytes 0 and 7, 1 and 6, 2 and 5, and 3 and 4. Because very little software deals with 128-bit integers, you probably won’t need to worry about long-word endianness conversion, but the concept is the same if you do.</p>&#13;
<p class="indent">Note that the endianness conversion process is <em>reflexive</em>; that is, the same algorithm that converts big-endian to little-endian also converts little-endian to big-endian. If you run the algorithm twice, you wind up with the data in the original format.</p>&#13;
<p class="indent">Even if you’re not writing software that exchanges data between two computers, the issue of endianness may arise. Some programs assemble larger objects from discrete bytes by assigning those bytes to specific positions within the larger value. If the software puts the LO byte into bit positions 0 through 7 (little-endian format) on a big-endian machine, the program will not produce correct results. Therefore, if the software needs to run on different CPUs that have different byte organizations, it will have to determine the endianness of the machine it’s running on and adjust how it assembles larger objects from bytes accordingly.</p>&#13;
<p class="indent">To illustrate how to build larger objects from discrete bytes, we’ll start with a short example that demonstrates how you could assemble a 32-bit object from 4 individual bytes. The most common way to do this is <span epub:type="pagebreak" id="page_145"/>to create a <em><a href="gloss01.xhtml#gloss01_76">discriminant union</a></em> structure that contains a 32-bit object and a 4-byte array.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>Many languages, but not all, support the discriminant union data type. For example, in Pascal, you would instead use a case variant record. See your language reference manual for details.</em></p>&#13;
</div>&#13;
<p class="indent">Unions are similar to records or structures except the compiler allocates the storage for each field of the union at the same address in memory. Consider the following two declarations from the C programming language:</p>&#13;
<p class="programs">struct<br/>&#13;
{<br/>&#13;
    short unsigned i;   // Assume shorts require 16 bits.<br/>&#13;
    short unsigned u;<br/>&#13;
    long unsigned r;    // Assume longs require 32 bits.<br/>&#13;
} RECORDvar;<br/><br/>&#13;
union<br/>&#13;
{<br/>&#13;
    short unsigned i;<br/>&#13;
    short unsigned u;<br/>&#13;
    long unsigned r;<br/>&#13;
} UNIONvar;</p>&#13;
<p class="indent">As <a href="ch06.xhtml#ch06fig13">Figure 6-13</a> shows, the <span class="literal">RECORDvar</span> object consumes 8 bytes in memory, and the fields do not share their memory with any other fields (that is, each field starts at a different offset from the base address of the record). The <span class="literal">UNIONvar</span> object, on the other hand, overlays all the fields in the union in the same memory locations. Therefore, writing a value to the <span class="literal">i</span> field of the union also overwrites the value of the <span class="literal">u</span> field as well as 2 bytes of the <span class="literal">r</span> field (whether they are the LO or HO bytes depends entirely on the endianness of the CPU).</p>&#13;
<div class="image"><img alt="image" src="../images/06fig13.jpg"/></div>&#13;
<p class="figcap"><a id="ch06fig13"/><em>Figure 6-13: Layout of a union versus a record (struct) in memory</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_146"/>In the C programming language, you can use this behavior to access the individual bytes of a 32-bit object. Consider the following union declaration in C:</p>&#13;
<p class="programs">union<br/>&#13;
{<br/>&#13;
    unsigned long bits32; /* This assumes that C uses 32 bits for <br/>&#13;
                             unsigned long */<br/>&#13;
    unsigned char bytes[4];<br/>&#13;
} theValue;</p>&#13;
<p class="indent">This creates the data type shown in <a href="ch06.xhtml#ch06fig14">Figure 6-14</a> on a little-endian machine, and the structure shown in <a href="ch06.xhtml#ch06fig15">Figure 6-15</a> on a big-endian machine.</p>&#13;
<div class="image"><img alt="image" src="../images/06fig14.jpg"/></div>&#13;
<p class="figcap"><a id="ch06fig14"/><em>Figure 6-14: A C union on a little-endian machine</em></p>&#13;
<div class="image"><img alt="image" src="../images/06fig15.jpg"/></div>&#13;
<p class="figcap"><a id="ch06fig15"/><em>Figure 6-15: A C union on a big-endian machine</em></p>&#13;
<p class="indent">To assemble a 32-bit object from 4 discrete bytes on a little-endian machine, you’d use code like the following:</p>&#13;
<p class="programs">theValue.bytes[0] = byte0;<br/>&#13;
theValue.bytes[1] = byte1;<br/>&#13;
theValue.bytes[2] = byte2;<br/>&#13;
theValue.bytes[3] = byte3;</p>&#13;
<p class="indent">This code functions properly because C allocates the first byte of an array at the lowest address in memory (corresponding to bits 0..7 in the <span class="literal">theValue.bits32</span> object on a little-endian machine); the second byte of the array follows (bits 8..15), then the third (bits 16..23), and finally the HO byte (occupying the highest address in memory, corresponding to bits 24..31).</p>&#13;
<p class="indent">However, on a big-endian machine, this code won’t work properly because <span class="literal">theValue.bytes[0]</span> corresponds to bits 24 through 31 of the 32-bit value rather than bits 0 through 7. To assemble this 32-bit value properly on a big-endian system, you’d need to use code like the following:</p>&#13;
<p class="programs">theValue.bytes[0] = byte3;<br/>&#13;
theValue.bytes[1] = byte2;<br/>&#13;
theValue.bytes[2] = byte1;<br/>&#13;
theValue.bytes[3] = byte0;</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_147"/>But how do you determine if your code is running on a little-endian or big-endian machine? This is actually a simple task. Consider the following C code:</p>&#13;
<p class="programs">theValue.bytes[0] = 0;<br/>&#13;
theValue.bytes[1] = 1;<br/>&#13;
theValue.bytes[2] = 0;<br/>&#13;
theValue.bytes[3] = 0;<br/>&#13;
isLittleEndian = theValue.bits32 == 256;</p>&#13;
<p class="indent">On a big-endian machine, this code sequence will store the value <span class="literal">1</span> into bit 16, producing a 32-bit value that is definitely not equal to 256, whereas on a little-endian machine this code will store the value <span class="literal">1</span> into bit 8, producing a 32-bit value equal to 256. Therefore, you can test the <span class="literal">isLittleEndian</span> variable to determine whether the current machine is little-endian (<span class="literal">true</span>) or big-endian (<span class="literal">false</span>).</p>&#13;
<h3 class="h3" id="sec6_4"><strong>6.4 The System Clock</strong></h3>&#13;
<p class="noindent">Although modern computers are quite fast and getting faster all the time, they still require time to accomplish even the smallest tasks. On von Neumann machines, most operations are <em>serialized</em>, which means that the computer executes commands in a prescribed order.<sup><a href="footnotes.xhtml#fn6_3a" id="fn6_3">3</a></sup> It wouldn’t do, in the following code sequence, to execute the Pascal statement <span class="literal">I := I * 5 + 2;</span> before the statement <span class="literal">I := J;</span> finishes:</p>&#13;
<p class="programs">I := J;<br/>&#13;
I := I * 5 + 2;</p>&#13;
<p class="indent">These operations do not occur instantaneously. Moving a copy of <span class="literal">J</span> into <span class="literal">I</span> takes a certain amount of time. Likewise, multiplying <span class="literal">I</span> by 5 and then adding 2 and storing the result back into <span class="literal">I</span> takes time.</p>&#13;
<p class="indent">To execute statements in the proper order, the processor relies on the <em>system clock</em>, which serves as the timing standard within the system. To understand why certain operations take longer than others, you must first understand how the system clock functions.</p>&#13;
<p class="indent">The system clock is an electrical signal on the control bus that alternates between 0 and 1 periodically (see <a href="ch06.xhtml#ch06fig16">Figure 6-16</a>). All activity within the CPU is synchronized with the edges (rising or falling) of this clock signal.</p>&#13;
<div class="image"><img alt="image" src="../images/06fig16.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_148"/><a id="ch06fig16"/><em>Figure 6-16: The system clock</em></p>&#13;
<p class="indent">The rate at which the system clock alternates between 0 and 1 is the <em>system clock frequency</em>, and the time it takes for the system clock to switch from 0 to 1 and back to 0 is the <em>clock period</em> or <em>clock cycle</em>. On most modern systems, the system clock frequency exceeds several billion cycles per second. A typical Pentium IV chip, circa 2004, runs at speeds of three billion cycles per second or faster. <em>Hertz (Hz)</em> is the unit corresponding to one cycle per second, so the aforementioned Pentium chip runs at between 3,000 and 4,000 million hertz, or 3,000 to 4,000 megahertz (MHz), or 3 to 4 gigahertz (GHz, or one billion cycles per second). Typical frequencies for 80x86 parts range from 5 MHz up to several gigahertz and beyond.</p>&#13;
<p class="indent">The clock period is the reciprocal of the clock frequency. For example, a 1 MHz (MHz or one million cycles per second) clock would have a clock period of 1 microsecond (one millionth of a second, µs<sup><a href="footnotes.xhtml#fn6_4a" id="fn6_4">4</a></sup>). A CPU running at 1 GHz would have a clock period of one nanosecond (ns), or one billionth of a second. Clock periods are usually expressed in microseconds or nanoseconds.</p>&#13;
<p class="indent">To ensure synchronization, most CPUs start an operation on either the <em><a href="gloss01.xhtml#gloss01_93">falling edge</a></em> (when the clock goes from 1 to 0) or the <em><a href="gloss01.xhtml#gloss01_217">rising edge</a></em> (when the clock goes from 0 to 1). The system clock spends most of its time at either 0 or 1 and very little time switching between the two. Therefore, a clock edge is the perfect synchronization point.</p>&#13;
<p class="indent">Because all CPU operations are synchronized with the clock, the CPU cannot perform tasks any faster than the clock runs. However, just because a CPU is running at some clock frequency doesn’t mean that it executes that many operations each second. Many operations take multiple clock cycles to complete, so the CPU often performs operations at a significantly slower rate.</p>&#13;
<h4 class="h4" id="sec6_4_1"><strong><em>6.4.1 Memory Access and the System Clock</em></strong></h4>&#13;
<p class="noindent">Memory access is an operation that is synchronized with the system clock; that is, memory access occurs no more than once every clock cycle. On some older processors, it takes several clock cycles to access a memory location. The <em><a href="gloss01.xhtml#gloss01_149">memory access time</a></em> is the number of clock cycles between a <span epub:type="pagebreak" id="page_149"/>memory request (read or write) and when the memory operation completes. This is an important value, because longer memory access times result in lower performance.</p>&#13;
<p class="indent">Modern CPUs are much faster than memory devices, so systems built around these CPUs often use a second clock, the <em>bus clock</em>, which is some fraction of the CPU speed. For example, typical processors in the 100 MHz to 4 GHz range can use 1600 MHz, 800 MHz, 500 MHz, 400 MHz, 133 MHz, 100 MHz, or 66 MHz bus clocks (a given CPU generally supports several different bus speeds, and the exact range it supports depends upon that CPU).</p>&#13;
<p class="indent">When reading from memory, the memory access time is the time between when the CPU places an address on the address bus and the time when the CPU takes the data off the data bus. On typical 80x86 CPUs with a one-cycle memory access time, the timing of a read operation looks something like <a href="ch06.xhtml#ch06fig17">Figure 6-17</a>. The timing of writing data to memory is similar (see <a href="ch06.xhtml#ch06fig18">Figure 6-18</a>).</p>&#13;
<div class="image"><img alt="image" src="../images/06fig17.jpg"/></div>&#13;
<p class="figcap"><a id="ch06fig17"/><em>Figure 6-17: A typical memory read cycle</em></p>&#13;
<div class="image"><img alt="image" src="../images/06fig18.jpg"/></div>&#13;
<p class="figcap"><a id="ch06fig18"/><em>Figure 6-18: A typical memory write cycle</em></p>&#13;
<p class="indent">The CPU doesn’t wait for memory. The access time is specified by the bus clock frequency. If the memory subsystem doesn’t work fast enough to keep up with the CPU’s expected access time, the CPU will read garbage data on a memory read operation and will not properly store the data on a memory write. This will surely cause the system to fail.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_150"/>Memory devices have various ratings, but the two major ones are capacity and speed. Typical dynamic RAM (random access memory) devices have capacities of 16GB (or more) and speeds of 0.1 to 100 ns. A typical 4 GHz Intel system uses 1600 MHz (1.6 GHz, or 0.625 ns) memory devices.</p>&#13;
<p class="indent">Now, I just said that the memory speed must match the bus speed or the system will fail. At 4 GHz the clock period is roughly 0.25 ns. So how can a system designer get away with using 0.625 ns memory? The answer is <em><a href="gloss01.xhtml#gloss01_261">wait states</a></em>.</p>&#13;
<h4 class="h4" id="sec6_4_2"><strong><em>6.4.2 Wait States</em></strong></h4>&#13;
<p class="noindent">A wait state is an extra clock cycle that gives a device additional time to respond to the CPU. For example, a 100 MHz Pentium system has a 10 ns clock period, implying that you need 10 ns memory. In fact, you need even faster memory devices because in many computer systems there’s additional decoding and buffering logic between the CPU and memory, and this circuitry introduces its own delays. In <a href="ch06.xhtml#ch06fig19">Figure 6-19</a>, you can see that buffering and decoding costs the system an additional 10 ns. If the CPU needs the data back in 10 ns, the memory must respond in 0 ns (which is impossible).</p>&#13;
<div class="image"><img alt="image" src="../images/06fig19.jpg"/></div>&#13;
<p class="figcap"><a id="ch06fig19"/><em>Figure 6-19: Decoding and buffer delays</em></p>&#13;
<p class="indent">If cost-effective memory won’t work with a fast processor, how do companies manage to sell fast PCs? One part of the answer is the wait state. For example, if you have a 100 MHz processor with a memory cycle time of 10 ns and you lose 2 ns to buffering and decoding, you’ll need 8 ns memory. What if your system can only support 20 ns memory, though? By adding wait states to extend the memory cycle to 20 ns, you can solve this problem.</p>&#13;
<p class="indent">Almost every general-purpose CPU in existence provides a pin (whose signal appears on the control bus) that allows you to insert wait states. If necessary, the memory address decoding circuitry asserts this signal to give the memory sufficient access time (see <a href="ch06.xhtml#ch06fig20">Figure 6-20</a>).</p>&#13;
<div class="image"><img alt="image" src="../images/06fig20.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_151"/><a id="ch06fig20"/><em>Figure 6-20: Inserting a wait state into a memory read operation</em></p>&#13;
<p class="indent">From the system performance point of view, wait states are <em>not</em> a good thing. As long as the CPU is waiting for data from memory, it can’t operate on that data. Adding a wait state typically <em>doubles</em> (or worse, on some systems) the amount of time required to access memory. Running with a wait state on every memory access is almost like cutting the processor clock frequency in half. You’ll get less work done in the same amount of time.</p>&#13;
<p class="indent">However, we’re not doomed to slow execution because of added wait states. There are several tricks hardware designers can employ to achieve zero wait states <em>most</em> of the time. The most common is the use of <em>cache</em> (pronounced “cash”) memory.</p>&#13;
<h4 class="h4" id="sec6_4_3"><strong><em>6.4.3 Cache Memory</em></strong></h4>&#13;
<p class="noindent">A typical program tends to access the same memory locations repeatedly (known as <em><a href="gloss01.xhtml#gloss01_245">temporal locality of reference</a></em>), and to access adjacent memory locations (<em>spatial locality of reference</em>). Both forms of locality occur in the following Pascal code segment:</p>&#13;
<p class="programs">for i := 0 to 10 do<br/>&#13;
         A [i] := 0;</p>&#13;
<p class="indent">There are two occurrences each of spatial and temporal locality of reference within this loop. Let’s consider the obvious ones first.</p>&#13;
<p class="indent">In this Pascal code, the program references the variable <span class="literal">i</span> several times. The <span class="literal">for</span> loop compares <span class="literal">i</span> against <span class="literal">10</span> to see if the loop is complete. It also increments <span class="literal">i</span> by 1 at the bottom of the loop. The assignment statement also uses <span class="literal">i</span> as an array index. This shows temporal locality of reference in action.</p>&#13;
<p class="indent">The loop itself zeros out the elements of array <span class="literal">A</span> by writing a <span class="literal">0</span> to the first location in <span class="literal">A</span>, then to the second location in <span class="literal">A</span>, and so on. Because Pascal stores the elements of <span class="literal">A</span> in consecutive memory locations, each loop iteration accesses adjacent memory locations. This shows spatial locality of reference.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_152"/>What about the second occurrences of temporal and spatial locality? Machine instructions also reside in memory, and the CPU fetches these instructions sequentially from memory and executes them repeatedly, once for each loop iteration.</p>&#13;
<p class="indent">If you look at the execution profile of a typical program, you’ll probably discover that the program executes less than half the statements. Generally, a program might use only 10 to 20 percent of the memory allotted to it. At any given time, a 1MB program might access only 4KB to 8KB of data and code. So, if you paid an outrageous sum of money for expensive zero-wait-state RAM, you’d be using only a tiny fraction of it at any given time. Wouldn’t it be nice if you could buy a small amount of fast RAM and dynamically reassign its addresses as the program executes? This is exactly what cache memory does for you.</p>&#13;
<p class="indent">Cache memory is a small amount of very fast memory that sits between the CPU and main memory. Unlike in normal memory, the bytes within a cache do not have fixed addresses. Cache memory can dynamically reassign addresses, which allows the system to keep recently accessed values in the cache. Addresses that the CPU has never accessed, or hasn’t accessed in some time, remain in main (slow) memory. Because most memory accesses are to recently accessed variables (or to locations near a recently accessed location), the data generally appears in cache memory.</p>&#13;
<p class="indent">A <em><a href="gloss01.xhtml#gloss01_41">cache hit</a></em> occurs whenever the CPU accesses memory and finds the data in the cache. In such a case, the CPU can usually access data with zero wait states. A <em><a href="gloss01.xhtml#gloss01_43">cache miss</a></em> occurs if the data cannot be found in the cache. In that case, the CPU has to read the data from main memory, incurring a performance loss. To take advantage of temporal locality of reference, the CPU copies data into the cache whenever it accesses an address that’s not present in the cache. Because the system will likely access that address shortly, it can save wait states on future accesses by having that data in the cache.</p>&#13;
<p class="indent">Cache memory does not eliminate the need for wait states. Although a program may spend considerable time executing code in one area of memory, eventually it will call a procedure or wander off to some section of code outside cache memory. When that happens, the CPU has to go to main memory to fetch the data. Because main memory is slow, this will require the insertion of wait states. However, once the CPU accesses the data, it will be available in the cache for future use.</p>&#13;
<p class="indent">We’ve discussed how cache memory handles the temporal aspects of memory access, but not the spatial aspects. Caching memory locations <em>when you access them</em> won’t speed up the program if you constantly access consecutive locations that you’ve never accessed before. To solve this problem, when a cache miss occurs, most caching systems will read several consecutive bytes of main memory (which engineers call a <em>cache line</em>). For example, 80x86 CPUs read between 16 and 64 bytes upon a cache miss. Most memory chips available today have special modes that let you quickly access several <span epub:type="pagebreak" id="page_153"/>consecutive memory locations on the chip. The cache exploits this capability to reduce the average number of wait states needed to access sequential memory locations. Although reading 16 bytes on each cache miss is expensive if you access only a few bytes in the corresponding cache line, cache memory systems work quite well in the average case.</p>&#13;
<p class="indent">The ratio of cache hits to misses increases with the size (in bytes) of the cache memory subsystem. The 80486 CPU, for example, has 8,192 bytes of on-chip cache. Intel claims to get an 80 to 95 percent hit rate with this cache (meaning 80 to 95 percent of the time the CPU finds the data in the cache). This sounds very impressive, but let’s play around with the numbers a little bit. Suppose we pick the 80 percent figure. This means that one out of every five memory accesses, on average, will not be in the cache. If you have a 50 MHz processor (20 ns period) and a 90 ns memory access time, four out of five memory accesses require only 20 ns (one clock cycle) because they are in the cache, and the fifth will require about four wait states (20 ns for a normal memory access plus 80 additional ns, or four wait states, to get at least 90 ns). However, the cache always reads 16 consecutive bytes (4 double words) from memory. Most 80486-era memory subsystems let you read consecutive addresses in about 40 ns after accessing the first location. Therefore, the 80486 will require an additional six clock cycles to read the remaining 3 double words, for a total of 220 ns. This corresponds to 11 clock cycles (at 20 ns each), which is one normal memory cycle plus 10 wait states.</p>&#13;
<p class="indent">Altogether, the system will require 15 clock cycles to access five memory locations, or 3 clock cycles per access, on average. That’s equivalent to two wait states added to every memory access. Doesn’t sound so impressive, does it? It gets even worse as you move up to faster processors and the difference in speed between the CPU and memory increases.</p>&#13;
<p class="indent">To improve the hit ratio, you can add more cache memory. Alas, you can’t pull an Intel i9 chip apart and solder more cache onto the chip. However, modern Intel CPUs have a significantly larger cache than the 80486 and operate with fewer average wait states. This improves the cache hit ratio. For example, increasing the hit ratio from 80 percent to 90 percent lets you access 10 memory locations in 20 cycles. This reduces the average number of wait states per memory access to one wait state—a substantial improvement.</p>&#13;
<p class="indent">Another way to improve performance is to build a <em>two-level</em> (L2) caching system. Many Intel CPUs work in this fashion. The first level is the on-chip 8,192-byte cache. The next level, between the on-chip cache and main memory, is a secondary cache (see <a href="ch06.xhtml#ch06fig21">Figure 6-21</a>). On newer processors, the first- and second-level caches generally appear in the same packaging as the CPU. This allows the CPU designers to build a higher-performance CPU/memory interface, allowing the CPU to move data between caches and the CPU (as well as main memory) much more rapidly.</p>&#13;
<div class="image"><img alt="image" src="../images/06fig21.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_154"/><a id="ch06fig21"/><em>Figure 6-21: A two-level caching system</em></p>&#13;
<p class="indent">A typical on-CPU secondary cache contains anywhere from 32,768 bytes to over 2MB of memory.</p>&#13;
<p class="indent">Secondary cache generally does not operate at zero wait states. The circuitry to support that much fast memory would be <em>very</em> expensive, so most system designers use slower memory, which requires one or two wait states. This is still much faster than main memory. Combined with the existing on-chip L1 cache, you can get better performance from the system with a L2 caching system.</p>&#13;
<p class="indent">Today, many CPUs incorporate a <em>three-level (L3) cache</em>. Though the performance improvement afforded by an L3 cache is nowhere near what you get with an L1 or L2 cache subsystem, L3 cache subsystems can be quite large (usually several megabytes<sup><a href="footnotes.xhtml#fn6_5a" id="fn6_5">5</a></sup>) and work well for large systems with gigabytes of main memory. For programs that manipulate considerable data yet exhibit locality of reference, an L3 caching subsystem can be very effective.</p>&#13;
<h3 class="h3" id="sec6_5"><strong>6.5 CPU Memory Access</strong></h3>&#13;
<p class="noindent">Most CPUs have two or three different ways to access memory. The most common <em><a href="gloss01.xhtml#gloss01_150">memory addressing modes</a></em> modern CPUs support are <em>direct</em>, <em>indirect</em>, and <em>indexed</em>. A few CPUs (like the 80x86) support additional addressing modes like <em>scaled-index</em>, while some RISC CPUs support only indirect access to memory. Having additional memory addressing modes makes memory access more flexible. Sometimes a particular addressing mode will allow you to access data in a complex data structure with a single instruction, where otherwise two or more instructions would be required.</p>&#13;
<p class="indent">RISC processors can often take three to five instructions to do what a single 80x86 instruction does. However, this does not mean that an 80x86 program will run three to five times faster. Don’t forget that access to memory is very slow, usually requiring wait states. Whereas the 80x86 frequently accesses memory, RISC processors rarely do. Therefore, that RISC <span epub:type="pagebreak" id="page_155"/>processor can probably execute the first four instructions, which do not access memory at all, while the single 80x86 instruction, which does access memory, is spinning on some wait states. In the fifth instruction the RISC CPU might access memory and incur wait states of its own. If both processors execute an average of one instruction per clock cycle and have to insert 30 wait states for a main memory access, we’re talking about 31 clock cycles (80x86) versus 35 clock cycles (RISC), only about a 12 percent difference.</p>&#13;
<p class="indent">Choosing an appropriate addressing mode often enables an application to compute the same result with fewer instructions and with fewer memory accesses, thus improving performance. Therefore, if you want to write fast and compact code, it’s important to understand how an application can use the different addressing modes a CPU provides.</p>&#13;
<h4 class="h4" id="sec6_5_1"><strong><em>6.5.1 The Direct Memory Addressing Mode</em></strong></h4>&#13;
<p class="noindent">The direct addressing mode encodes a variable’s memory address as part of the actual machine instruction that accesses the variable. On the 80x86, direct addresses are 32-bit values appended to the instruction’s encoding. Generally, a program uses the direct addressing mode to access global static variables. Here’s an example in HLA assembly language:</p>&#13;
<p class="programs">static<br/>&#13;
    i:dword;<br/>&#13;
         . . .<br/>&#13;
    mov( eax, i ); // Store EAX's value into the i variable.</p>&#13;
<p class="indent">When you’re accessing variables whose memory address is known prior to the program’s execution, the direct addressing mode is ideal. With a single instruction, you can reference the memory location associated with the variable. On those CPUs that don’t support a direct addressing mode, you may need an extra instruction (or more) to load a register with the variable’s memory address prior to accessing that variable.</p>&#13;
<h4 class="h4" id="sec6_5_2"><strong><em>6.5.2 The Indirect Addressing Mode</em></strong></h4>&#13;
<p class="noindent">The indirect addressing mode typically uses a register to hold a memory address (there are a few CPUs that use memory locations to hold the indirect address, but this form of indirect addressing is rare in modern CPUs).</p>&#13;
<p class="indent">There are a couple of advantages of the indirect addressing mode over the direct addressing mode. First, you can modify the value of an indirect address (the value being held in a register) at runtime. Second, encoding which register specifies the indirect address takes far fewer bits than encoding a 32-bit (or 64-bit) direct address, so the instructions are smaller. One disadvantage is that it may take one or more instructions to load a register with an address before you can access that address.</p>&#13;
<p class="indent">The following HLA sequence uses an 80x86 indirect addressing mode (brackets around the register name denote the use of indirect addressing):</p>&#13;
<p class="programs">static<br/>&#13;
    byteArray: byte[16];<br/>&#13;
<span epub:type="pagebreak" id="page_156"/>         . . .<br/>&#13;
    lea( ebx, byteArray );  // Loads EBX register with the address <br/>&#13;
                            // of byteArray.<br/>&#13;
    mov( [ebx], al );       // Loads byteArray[0] into AL.<br/>&#13;
    inc( ebx );             // Point EBX at the next byte in memory<br/>&#13;
                            // (byteArray[1]).<br/>&#13;
    mov( [ebx], ah );       // Loads byteArray[1] into AH.</p>&#13;
<p class="indent">The indirect addressing mode is useful for many operations, such as accessing objects referenced by a pointer variable.</p>&#13;
<h4 class="h4" id="sec6_5_3"><strong><em>6.5.3 The Indexed Addressing Mode</em></strong></h4>&#13;
<p class="noindent">The indexed addressing mode combines the direct and indirect addressing modes. Specifically, the machine instructions using this addressing mode encode both an offset (direct address) and a register in the bits that make up the instruction. At runtime, the CPU computes the sum of these two address components to create an <em><a href="gloss01.xhtml#gloss01_87">effective address</a></em>. This addressing mode is great for accessing array elements and for indirect access to objects like structures and records. Though the instruction encoding is usually larger than for the indirect addressing mode, the indexed addressing mode has the advantage that you can specify an address directly within an instruction without having to use a separate instruction to load the address into a register.</p>&#13;
<p class="indent">Here’s a typical example of an HLA sequence that uses an 80x86 indexed addressing mode:</p>&#13;
<p class="programs">static<br/>&#13;
    byteArray: byte[16];<br/>&#13;
        . . .<br/>&#13;
    mov( 0, ebx );                    // Initialize an index into the array.<br/>&#13;
    while( ebx &lt; 16 ) do<br/><br/>&#13;
        mov( 0, byteArray[ebx] );     // Zeros out byteArray[ebx].<br/>&#13;
        inc( ebx );                   // EBX := EBX +1, move on to the<br/>&#13;
                                      // next array element.<br/><br/>&#13;
    endwhile;</p>&#13;
<p class="indent">The <span class="literal">byteArray[ebx]</span> instruction in this short program demonstrates the indexed addressing mode. The effective address is the address of the <span class="literal">byteArray</span> variable plus the current value in the EBX register.</p>&#13;
<p class="indent">To avoid wasting space encoding a 32-bit or 64-bit address into every instruction that uses an indexed addressing mode, many CPUs provide a shorter form that encodes an 8-bit or 16-bit offset as part of the instruction. When using this smaller form, the register provides the base address of the object in memory, and the offset provides a fixed displacement into that data structure in memory. This is useful, for example, for accessing fields of a record or structure in memory via a pointer to that structure. The earlier <span epub:type="pagebreak" id="page_157"/>HLA example encodes the address of <span class="literal">byteArray</span> using a 4-byte address. Compare that with the following use of the indexed addressing mode:</p>&#13;
<p class="programs">lea( ebx, byteArray ); // Loads the address of byteArray into EBX.<br/>&#13;
    . . .<br/>&#13;
mov( al, [ebx+2] );    // Stores al into byteArray[2]</p>&#13;
<p class="indent">This last instruction encodes the displacement value using a single byte (rather than 4 bytes); hence, the instruction is shorter and more efficient.</p>&#13;
<h4 class="h4" id="sec6_5_4"><strong><em>6.5.4 The Scaled-Index Addressing Modes</em></strong></h4>&#13;
<p class="noindent">The scaled-index addressing mode, available on several CPUs, provides two facilities above and beyond the indexed addressing mode:</p>&#13;
<ul>&#13;
<li class="noindent">The ability to use two registers (plus an offset) to compute the effective address</li>&#13;
<li class="noindent">The ability to multiply one of those two registers’ values by a constant (typically 1, 2, 4, or 8) prior to computing the effective address.</li>&#13;
</ul>&#13;
<p class="indent">This addressing mode is especially useful for accessing elements of arrays whose element sizes match one of the scaling constants (see the discussion of arrays in <a href="ch07.xhtml#ch07">Chapter 7</a> for the reasons).</p>&#13;
<p class="indent">The 80x86 provides a scaled-index addressing mode that takes one of several forms, as shown in the following HLA statements:</p>&#13;
<p class="programs">mov( [ebx+ecx*1], al );             // EBX is base address, ecx is index.<br/>&#13;
mov( wordArray[ecx*2], ax );        // wordArray is base address, ecx is index.<br/>&#13;
mov( dwordArray[ebx+ecx*4], eax );  // Effective address is combination <br/>&#13;
                                    // of offset(dwordArray)+ebx+(ecx*4).</p>&#13;
<h3 class="h3" id="sec6_6"><strong>6.6 For More Information</strong></h3>&#13;
<p class="ref">Hennessy, John L., and David A. Patterson. <em>Computer Architecture: A Quantitative Approach</em>. 5th ed. Waltham, MA: Elsevier, 2012.</p>&#13;
<p class="ref">Hyde, Randall. <em>The Art of Assembly Language</em>. 2nd ed. San Francisco: No Starch Press, 2010.</p>&#13;
<p class="ref">Patterson, David A., and John L. Hennessy. <em>Computer Organization and Design: The Hardware/Software Interface</em>. 5th ed. Waltham, MA: Elsevier, 2014.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em><a href="ch11.xhtml#ch11">Chapter 11</a> in this book provides additional information about cache memory and memory architecture.</em><span epub:type="pagebreak" id="page_158"/></p>&#13;
</div>&#13;
</body></html>