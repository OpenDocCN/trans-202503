- en: '**12'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SAMPLING**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We’ve powered the majority of our experiments by extracting samples from the
    uniform distribution. While we’ve also worked with the normal distribution ([Chapter
    1](ch01.xhtml)), beta distribution ([Chapter 3](ch03.xhtml)), and binomial distribution
    ([Chapter 9](ch09.xhtml)), the tried-and-true uniform distribution is our oldest
    and dearest friend.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll sample from arbitrary probability distributions, be they
    discrete or continuous. This ability is critical to simulation and fundamental
    to Bayesian inference.
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll discuss terminology and unpack the term *Bayesian inference*. Following
    that, we’ll dive into sampling from arbitrary discrete probability distributions,
    first in one dimension, then in two.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete distributions dealt with, we’ll move on to sampling from continuous
    distributions via inverse transform sampling, rejection sampling, and Markov Chain
    Monte Carlo sampling using the Metropolis-Hastings algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: This is perhaps our most mathematical chapter, but don’t let that throw you.
    If you want to continue exploring the algorithms or apply them to different situations,
    the code is what matters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction to Sampling**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before diving into sampling, let’s agree on terminology and the concepts that
    terminology entails. We’ll also introduce notions related to Bayesian statistics
    and inference, with the latter being a prime motivator for, and beneficiary of,
    the development of sampling algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '***Terminology***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If we roll a standard die, we’ll get one of six possible outputs, each occurring
    with equal probability. We express this distribution as a bar graph with bars
    labeled 1 through 6, each of equal height corresponding to the fraction 1/6\.
    The sum of the fractions represented by each bar is 1 because the graph is a discrete
    probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *probability mass function (PMF)* tells us the probability of any discrete
    outcome. For a standard die, the PMF is 1/6 for each outcome. For a binomial distribution,
    the PMF depends on the number of trials (*n*) and the probability of an event
    happening (*p*) per trial according to the formula ![Image](../images/f0324-01.jpg).
    Here *k*, *k* = 0, . . . , *n*, is the number of events occurring during the *n*
    trials. Think of the continuous case as moving the discrete case to more and more
    possible outcomes; for example, a bar graph where the bars become increasingly
    narrow until they have an infinitesimal width. Talk of infinitesimals often implies
    calculus, as is the case here. The discrete distribution morphs into a continuous
    one so that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0324-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *f*(*x*) is a *probability density function (PDF)*, the continuous analog
    of a probability mass function. The notation *P*(*X* = *i*) stands for the probability
    of a random variable, *X*, taking on the value *i*.
  prefs: []
  type: TYPE_NORMAL
- en: The *∫* symbol is simply an old-fashioned script “S” for “sum.” The thing summed
    is an infinite number of areas formed by rectangles of width *dx* (a single entity,
    not the product of *d* and *x*) and height *f*(*x*), that is, the PDF function
    value at *x*. If limits are given, ![Image](../images/f0324-01a.jpg), the sum
    is for *x* = *a* through *x* = *b*. No limits given implies “sum over all *x*
    that matter, even if from –*∞* to +*∞*.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Sample a discrete distribution, and the probability of returning *x* is *P*(*X*
    = *x*), or the probability associated with the bar labeled *x*. The probability
    of sampling a continuous distribution and getting a specific *x* is, counterintuitively,
    *P*(*x*) = 0 for any real number *x*. We can’t talk of the probability of returning
    *x* as a sample, but instead, the probability of the sample lying in some range,
    [*a*, *b*]. That probability is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0324-04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is nothing more than the area under the PDF from *a* to *b*. The variables
    in the integral are dummy variables. I switched from *x* to *t* to avoid confusion
    with the *x* we’re asking about on the left-hand side of the equation.
  prefs: []
  type: TYPE_NORMAL
- en: We must talk about the probability over a range in the continuous case because
    not all infinities are created equal, a fact first realized by Georg Cantor in
    the 19th century. Because there are so many more real numbers than integers, the
    probability of selecting any one from a continuous distribution becomes identically
    zero. While the algorithms of this chapter return samples that appear to come
    from the desired continuous distribution, don’t be fooled. As with all computation,
    we never use real numbers, but rational numbers in one form or another. Ultimately,
    our samples approximate the desired continuous probability distribution. However,
    as they say, if it walks like a duck and quacks like a duck, it’s a duck—or a
    reasonably useful facsimile of one.
  prefs: []
  type: TYPE_NORMAL
- en: The PMF and PDF relate to the probability of sampling a particular value from
    a distribution. A related concept is the *cumulative distribution function (CDF)*,
    which we use for both discrete and continuous distributions. The CDF at *x* is
    the sum of the area under the PMF or PDF from its lowest value to *x*. If we sum
    over all the bars of the discrete distribution or integrate over all of the PDF,
    we get an area of 1, so the CDF is a function running from 0 on the left to 1
    on the right as *x* increases.
  prefs: []
  type: TYPE_NORMAL
- en: For example, [Figure 12-1](ch012.xhtml#ch012fig01) shows the PMF (top left)
    and CDF (top right) for a binomial distribution with *n* = 10 trials, each with
    probability *p* = 0.7\. On the bottom are the PDF and CDF for a standard normal
    distribution. In both cases, the CDF approaches 1 from the left.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-1: The PMF and CDF for a binomial distribution (top) and a standard
    normal distribution (bottom)*'
  prefs: []
  type: TYPE_NORMAL
- en: The code to generate [Figure 12-1](ch012.xhtml#ch012fig01) is in *cdf.py*, which
    demonstrates how to realize pure math as code, at least with regard to probability
    functions. I’ll skip the plotting portion of the code; see [Listing 12-1](ch012.xhtml#ch012list01)
    for the remaining relevant bits.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 12-1: Generating CDFs from PMFs and PDFs*'
  prefs: []
  type: TYPE_NORMAL
- en: The first code paragraph generates samples from the binomial distribution. To
    save effort, I’m using NumPy’s function to draw 10,000 random samples, meaning
    `z` is a vector of 10,000 values, each a randomly selected sample from the binomial
    distribution. To get the distribution itself, we need a histogram. The samples,
    in `z`, are integers, so the most efficient way to get the counts is with the
    `bincount` method. There were *n* = 10 trials, but there are 11 possible outcomes
    from 0 events up to 10, hence `minlength=11` in the call to `bincount`.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s turn to the `h = h / h.sum()` line. The `bincount` method returns the
    per-outcome counts. We want a discrete probability distribution, which must sum
    to 1, so we divide each count by the total to transform them into fractions adding
    to 1\. Therefore, `h` is now an *estimate* of the discrete binomial probability
    distribution for *n* = 10 and *p* = 0.7\. For a better estimate of the true binomial
    distribution, increase the number of samples to 20,000 or more.
  prefs: []
  type: TYPE_NORMAL
- en: A discrete distribution’s CDF is the running sum of the per-outcome probabilities.
    In other words
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0326-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: NumPy’s `cumsum` calculates this cumulative sum for us to generate the entire
    CDF in a single function call.
  prefs: []
  type: TYPE_NORMAL
- en: 'The continuous case is similar, though we don’t need to draw samples from it
    because the PDF of the normal distribution has a closed-form representation. For
    the standard normal (*µ* = 0, *σ* = 1), the PDF is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0326-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In code, we estimate this function (`y`) using 10,000 values of *x* equally
    spaced from –7 to 7 (`linspace`).
  prefs: []
  type: TYPE_NORMAL
- en: To estimate the CDF, however, we can’t simply sum the values in `y` as in the
    discrete case. The area under the PDF must sum to 1, but because it’s an area,
    we multiply each *y* value by the width of the rectangle it makes with the *x*-axis.
    The rectangle’s width is the difference between successive `x` values, so we multiply
    `y` by `x[1] - x[0]` before summing. There’s no need to scale `y`, as the values
    in `y` are the actual PDF values, not counts.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter’s goal is to sample from arbitrary distributions, where “sampling”
    means that we ask a distribution to give us a number according to the probabilities
    assigned to possible outputs. The higher the *y*-axis value of a distribution
    in a PMF or PDF plot, the higher the oracle’s probability of selecting that number
    (discrete) or a number in a very narrow range about that position (continuous).
  prefs: []
  type: TYPE_NORMAL
- en: For example, the code in *nselect.py* generates the plot in [Figure 12-2](ch012.xhtml#ch012fig02),
    in which the black dots on the *x*-axis signify 30 samples from the normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-2: Thirty samples from a standard normal distribution*'
  prefs: []
  type: TYPE_NORMAL
- en: The samples are concentrated near the center of the PDF—the most likely region
    to be sampled. As we draw more samples, their density along the *x*-axis increases
    in proportion to the probability of being selected. Transforming the density into
    counts via a histogram approximates the PDF itself.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s discuss Bayesian inference, as its use depends on efficient sampling
    from complicated probability distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '***Bayesian Inference***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: English minister Thomas Bayes (1701–1761) originated a seemingly simple equation
    that has recently turned the world of statistics on its head. Deriving the equation
    is an exercise in basic probability theory.
  prefs: []
  type: TYPE_NORMAL
- en: We write the probability of event *B* happening, given that event *A* has already
    happened, as *P*(*B*|*A*). This is the *conditional probability* of *B* given
    *A*. The probability of *A* happening is *P*(*A*), and the probability of both
    *B* and *A* happening is their *joint probability*, *P*(*A*, *B*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Probability theory states that *P*(*B*, *A*) = *P*(*B*|*A*)*P*(*A*), which,
    switching the order of events, gives us *P*(*A*, *B*) = *P*(*A*|*B*)*P*(*B*).
    Joint probabilities represent the probability of all combinations of the events,
    meaning *P*(*B*, *A*) = *P*(*A*, *B*). Putting these observations together tells
    us that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0328-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The final equation is *Bayes’ theorem*, which relates the *posterior probability*,
    *P*(*B*|*A*), to the product of the *likelihood*, *P*(*A*|*B*), and the *prior
    probability*, *P*(*B*). The denominator on the right, *P*(*A*), is the *evidence*.
    It’s a normalizing value to ensure that the posterior probability is a probability—that
    the sum over the PDF of the posterior is 1\. In practice, *P*(*A*) becomes an
    integral that typically can’t be expressed in closed form. In Bayesian modeling,
    the likelihood and prior probabilities are selected and known functional forms,
    but the evidence becomes intractable. This is where the sampling methods we’ll
    discuss soon come into play. Drawing samples from the posterior is *Bayesian inference*.
    Without advanced sampling methods, Bayesian inference is all but impossible; with
    them, Bayesian inference becomes a paradigm shift, as Sharon Bertsch McGrayne
    notes in her book *The Theory That Would Not Die* (Yale University Press, 2011):'
  prefs: []
  type: TYPE_NORMAL
- en: The combination of Bayes and Markov Chain Monte Carlo has been called “arguably
    the most powerful mechanism ever created for processing data and knowledge.”
  prefs: []
  type: TYPE_NORMAL
- en: Markov Chain Monte Carlo is one of the sampling algorithms we’ll explore in
    this chapter. While we recognize Monte Carlo from [Chapter 11](ch011.xhtml), we’ll
    discuss the Markov chain aspect in time.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start sampling from arbitrary distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Discrete Distributions**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An arbitrary, one-dimensional discrete probability mass function might look
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p[X]*(*x*) = [1, 1, 3, 4, 5, 1, 7, 4, 3]'
  prefs: []
  type: TYPE_NORMAL
- en: While perhaps unexpected, as far as we’re concerned this is a perfectly valid
    PMF represented as a Python list. It illustrates a distribution returning samples
    in the range 0 through 8 (the list has nine elements), and each sample returns
    an index into the PMF. As it stands, the PMF isn’t *normalized*, so the sum of
    the “probabilities” isn’t 1; it’s 1 + 1 + 3 + 4 + 5 + 1 + 7 + 4 + 3 = 29\. To
    get probabilities, we divide each number by this sum.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PMF tells us the proportion with which each value—0 through 8—appears,
    on average, after a large number of samples. For example, 6 appears seven times
    as often as 0 because the ratio between 6 and 0 is 7 : 1\. Likewise, the ratio
    between 6 and 7 is 7 : 4, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s calculate the ratio between the probability of sampling a 6 (7/29) and
    the probability of getting a 0 (1/29):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0329-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The result is as expected.
  prefs: []
  type: TYPE_NORMAL
- en: This section explores three approaches to sampling from arbitrary discrete distributions.
    Two approaches expect the PMF to be normalized (sums to 1), while the third uses
    a PMF expressed as integer ratios between the elements. This may seem to be a
    drawback, but we often sample from distributions approximated by histograms, and
    the bins of a histogram are integer counts.
  prefs: []
  type: TYPE_NORMAL
- en: '***Sequential Search***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Chapter 7](ch07.xhtml), we generated fractals using IFS by applying maps
    selected according to a given probability. In other words, we sampled from a distribution
    over maps. The code we used is in [Listing 12-2](ch012.xhtml#ch012list02).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 12-2: Choosing a map*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 12-2](ch012.xhtml#ch012list02) implements a version of *inversion
    sampling by sequential search*; at least, that’s what Luc Devroye calls it in
    his book *Non-Uniform Random Variate Generation* (Springer, 1986). You’ll find
    Devroye’s book on his website, *[http://luc.devroye.org/books-luc.html](http://luc.devroye.org/books-luc.html)*.
    He’s giving it away. I recommend grabbing a copy.'
  prefs: []
  type: TYPE_NORMAL
- en: The implementation in [Listing 12-2](ch012.xhtml#ch012list02) isn’t exactly
    terse. We can do better, as in [Listing 12-3](ch012.xhtml#ch012list03).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 12-3: A more parsimonious implementation of inversion by sequential
    search*'
  prefs: []
  type: TYPE_NORMAL
- en: First, we hand the function the PMF as `probs`, which we expect to be normalized.
    The second argument is our old friend, an instance of `RE` configured to return
    floats in [0, 1).
  prefs: []
  type: TYPE_NORMAL
- en: The code picks a uniformly distributed value, `u`, and subtracts the probabilities
    in `probs`, in order, until the result is zero or negative. We then return the
    number of times we subtract, `k`, as the sampled index into `probs` after adjusting
    for indexing from zero.
  prefs: []
  type: TYPE_NORMAL
- en: We can visualize the sampling process as in [Figure 12-3](ch012.xhtml#ch012fig03)
    using the unnormalized PMF we discussed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '*p[X]*(*x*) = [1, 1, 3, 4, 5, 1, 7, 4, 3]'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-3: Sequential sampling of a discrete distribution*'
  prefs: []
  type: TYPE_NORMAL
- en: The Probability row reflects this PMF where the length of each box is in proportion
    to the other boxes so that the 7 box is seven times longer than the 1 box. To
    get the probability passed in `probs`, divide each box label by the sum, 29.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-3](ch012.xhtml#ch012fig03) shows a selected *u* as a double arrow.
    This *u* is about 0.4 because it’s a bit less than half the distance across the
    entire PMF, which must sum to 1\. The vertical line after the box labeled 5 marks
    the full set of probabilities subtracted from *u* until *u* < 0\. We subtract
    five boxes in this order: 1, 1, 3, 4, and 5\. Adjust for indexing from 0, and
    the returned sample is 4—the label on the matched box below *u* in the Value row.
    Ignore the Reorder row for the time being.'
  prefs: []
  type: TYPE_NORMAL
- en: This process maps [0, 1) to [0, 8] using the width of the boxes to transform
    the uniform input to a nonuniform output that will match the desired PMF if we
    draw enough samples. Try selecting other samples by closing your eyes and placing
    your finger somewhere on [Figure 12-3](ch012.xhtml#ch012fig03)’s Probability row.
    Then, slide your finger down to the Value row and read off the output, which is
    the number of boxes covered from the left. After repeating this a few times, you
    should see that 6 will be chosen most often, followed by 4\. Inversion sampling
    by sequential search is our first discrete sampling algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make one minor improvement. The top row of [Figure 12-3](ch012.xhtml#ch012fig03)
    presents the PMF in order, meaning the probability assigned to 0 is 1/29, while
    the probability assigned to 6 is 7/29\. This makes sense if we use [Listing 12-3](ch012.xhtml#ch012list03)
    to sample from the distribution. However, to get 6 (the most frequent value) as
    a sample, we need to search from the beginning of the probability vector each
    time. If we list the probabilities in descending order, we’ll select the most
    likely outcome after one pass through the `while` loop of [Listing 12-3](ch012.xhtml#ch012list03).
    The next most likely outcome requires only two passes, and so on. This notion
    leads to the Reorder row of [Figure 12-3](ch012.xhtml#ch012fig03).
  prefs: []
  type: TYPE_NORMAL
- en: The bars of the Reorder row run from left to right in decreasing size order,
    with the label on each bar listing the value to return should that bar be selected.
    Notice, the algorithm in [Listing 12-3](ch012.xhtml#ch012list03) doesn’t change;
    it’s still subtracting probabilities from *u*, but they’re now ordered from greatest
    to least. Therefore, we must map the index returned by [Listing 12-3](ch012.xhtml#ch012list03)
    to identify the true value selected. For example, if [Listing 12-3](ch012.xhtml#ch012list03)
    returns index 1, the mapping knows that 1 → 4 to return 4 as the selected value.
    This adjustment should speed things up, depending on the arrangement of probabilities
    in `probs`. The reordering tweak is our second discrete sampling algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '***Fast-Loaded Dice Roller***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our final discrete sampling algorithm is relatively new: the *Fast Loaded Dice
    Roller (FLDR)*, presented by Saad et al. in their 2020 paper, “The Fast Loaded
    Dice Roller: A Near-Optimal Exact Sampler for Discrete Probability Distributions.”
    You’ll find the code and paper on their GitHub site at *[https://github.com/probcomp/fast-loaded-dice-roller](https://github.com/probcomp/fast-loaded-dice-roller)*.
    We need only the *fldr.py* file from the *src/python* directory. Either copy that
    file from the GitHub repo via your browser or install the full package with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If you copy *fldr.py* from GitHub, place it in the folder for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The FLDR paper describes the algorithm and its genesis. It also refers to the
    Devroye book mentioned earlier, which motivated the algorithm’s design. We won’t
    discuss the details, as they’re rather involved and mathematical. However, it’s
    interesting to learn that there are more sophisticated ways of thinking about
    sampling from a discrete distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The version of FLDR we’ll use wants PMFs as vectors of integers, precisely as
    I’ve presented them so far. Using the FLDR requires two steps; the first conditions
    the algorithm based on the PMF, and the second draws individual samples on demand.
    We need the `fldr_preprocess_int` function to configure the sampler and the function
    `fldr_sample` to draw a sample. The FLDR code is not NumPy aware, but we can live
    with that.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our algorithms, we’ll pit them against each other.
  prefs: []
  type: TYPE_NORMAL
- en: '***Runtime Performance***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s find out whether our algorithms work, and how they compare to each other
    in terms of runtime performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, run *discrete_test.py* without arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The command line’s form is familiar. The only required argument is the number
    of samples to draw from the nine-element distribution presented at start of “Discrete
    Distributions” on [page 328](ch012.xhtml#ch00lev1_79): [1, 1, 3, 4, 5, 1, 7, 4,
    3].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s select some samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The command line requests 5,000 samples and displays the number of times each
    possible output was selected by algorithm type. For example, 1 was selected 187
    times by the sequential algorithm. As we expect, 6 is the most frequent output.
    The final line contains the expected number of samples, found by multiplying the
    probability by the number of samples, rounded to the nearest integer.
  prefs: []
  type: TYPE_NORMAL
- en: The three algorithms appear to work as expected, with the results close to the
    expected output. Looking at the runtime on the right, the sequential algorithm
    is the slowest, the reordered algorithm is slightly faster, and FLDR is nearly
    an order of magnitude faster still.
  prefs: []
  type: TYPE_NORMAL
- en: If we ask for 50 samples instead of 5,000
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: the output is noisy, as the expected frequency and the sampled frequencies are
    farther apart; for example, the sequential algorithm picked 2 in 10 instances
    while the expected frequency is only 5.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-4](ch012.xhtml#ch012fig04) demonstrates this effect visually using
    FLDR to select 50 versus 5,000 samples.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-4: Sampling from a discrete distribution using 50 samples (left)
    and 5,000 (right)*'
  prefs: []
  type: TYPE_NORMAL
- en: The bars are the true distribution and the dots are the samples, both expressed
    as probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-5](ch012.xhtml#ch012fig05) displays the runtime performance of our
    samplers for ⌊*N^α*⌋ samples with *α* running from 1 to 6 in 25 steps. Note that
    the *x*-axis is in units of 1 million.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-5: Sample time as a function of the number of samples*'
  prefs: []
  type: TYPE_NORMAL
- en: We can safely say that all three sampling algorithms run in ![Image](../images/c0301-01.jpg)(*n*)
    time. However, [Figure 12-5](ch012.xhtml#ch012fig05) is a good practical example
    for us. Big O notation ignores multiplicative factors, so while all three algorithms
    run in linear time, in practice we’ll want to use FLDR.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through *discrete_test.py*, starting with the setup ([Listing 12-4](ch012.xhtml#ch012list04)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 12-4: Setting up* discrete_test.py'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll focus on the beginning, where we introduce functions from `fldr`, and
    on the end, where we define `probabilities`.
  prefs: []
  type: TYPE_NORMAL
- en: FLDR wants integer counts, so we use `probabilities` in that case. When working
    with sequential sampling algorithms, which need true probabilities, we use `prob`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 12-5](ch012.xhtml#ch012list05) uses each algorithm to draw the requested
    number of samples. Each case creates a vector of the samples, `z`, timing how
    long it takes. A list comprehension draws the samples.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 12-5: Sampling with each algorithm*'
  prefs: []
  type: TYPE_NORMAL
- en: The first code paragraph calls `Sequential`, which we saw in [Listing 12-3](ch012.xhtml#ch012list03),
    and then creates the histogram with `bincount` before displaying the counts and
    generation time.
  prefs: []
  type: TYPE_NORMAL
- en: The second paragraph ultimately calls `Sequential`, but first rearranges `prob`
    to be in decreasing sort order. NumPy’s `argsort` returns the indices that sort
    `prob` in ascending order. The `[::-1]` idiom reverses the list to put `idx` in
    decreasing order.
  prefs: []
  type: TYPE_NORMAL
- en: We then call `Sequential` with `p` instead of `prob`. This means that the values
    in `z` are not the proper indices, but indices into `idx`, which holds the proper
    indices. In other words, `idx` is the mapping to get the proper sample values.
    A call to `bincount` using `idx` indexed by `z` generates the correct sample frequencies.
    Consider taking a moment to convince yourself that `idx[z]` makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: Symmetry tells us that the final code paragraph in [Listing 12-5](ch012.xhtml#ch012list05)
    draws samples using FLDR by repeated calls to `fldr_sample`. But first, we have
    to pass the probabilities to `fldr_preprocess_int` to create the structure `fldr_sample`
    uses.
  prefs: []
  type: TYPE_NORMAL
- en: The final line of [Listing 12-5](ch012.xhtml#ch012list05) displays the expected
    per-value counts by rounding the product of the probability and the number of
    samples, `N`.
  prefs: []
  type: TYPE_NORMAL
- en: What if the distribution we want to sample from is two dimensional? What does
    that even mean? Let’s find out.
  prefs: []
  type: TYPE_NORMAL
- en: '***Two Dimensions***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can store a one-dimensional discrete distribution in a vector. By extension,
    we might imagine storing a two-dimensional distribution in a matrix. But how do
    we interpret the distribution?
  prefs: []
  type: TYPE_NORMAL
- en: Since a one-dimensional distribution tells us how often we should expect to
    sample each value, then a two-dimensional distribution refers to a *pair* of values,
    namely the indices of each dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this two-dimensional distribution, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0335-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The sum of all elements is 1, so *p[X]*(***x***) is a PMF. Note that I have
    replaced *x* with ***x***, a vector. We can also write *p[X]*(*x*, *y*) to emphasize
    that we have two dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The distribution says that *P*(*X* = 0, *Y* = 0) = 0.1 while *P*(*X* = 2, *Y*
    = 3) = 0.2, that is, the value of the variables are the indices of the rows and
    columns of the matrix representing the distribution. Two-dimensional probability
    distributions show up when considering joint distributions—how often a pair of
    random variables appear together with some combination of values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use our existing sampling techniques to draw samples from a two-dimensional
    distribution by unraveling the distribution, sampling, and converting the samples
    back to two-dimensional pairs. For example, unraveling the previous distribution
    gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p[X]*(*x*) = [0.1, 0.0, 0.1, 0.2, 0.0, 0.0, 0.1, 0.1, 0.2, 0.0, 0.0, 0.2]'
  prefs: []
  type: TYPE_NORMAL
- en: If we sample from this distribution using one of the aforementioned algorithms,
    we’ll get samples in the range [0, 11]. To convert the samples back to pairs,
    we must undo the raveling, meaning we need to know the number of rows and columns
    in the original two-dimensional distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s walk through an example. Run *discrete_ravel.py* with this command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The output consists of four sections. The code itself samples from the previous
    *p[X]*(***x***) by unraveling it, sampling, then mapping the samples back to (*x*,
    *y*) pairs that represent the frequency with which combinations of *x* and *y*
    appear. If all goes well, one- and two-dimensional histograms of these frequencies
    should approximate *p[X]*(***x***).
  prefs: []
  type: TYPE_NORMAL
- en: 'The first output line gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is a PMF generated from the samples drawn using the unraveled histogram.
    The values are all around 0.1 and 0.2, which is encouraging.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second output block is the same estimated PMF remapped to two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This looks very much like *p[X]*(***x***).
  prefs: []
  type: TYPE_NORMAL
- en: 'The estimated PMF looks right. As for the sampled values, here are the first
    eight drawn from the unraveled PMF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Mapped back to pairs, these samples become:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The conversion from one-dimensional sample to two-dimensional pair is
  prefs: []
  type: TYPE_NORMAL
- en: (*x*, *y*) = (*z* ÷ 4, *z* mod 4)
  prefs: []
  type: TYPE_NORMAL
- en: where *z* is the 1D sample value and ÷ means integer division. The 4 comes from
    the number of columns in the two-dimensional PMF.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 12-6](ch012.xhtml#ch012list06) shows the unravel, sample, remap process.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 12-6: Sampling a two-dimensional PMF by unraveling*'
  prefs: []
  type: TYPE_NORMAL
- en: The two-dimensional PMF is in `prob2`, which unravels to become the one-dimensional
    PMF `prob`. The second paragraph samples from `prob` as we did earlier. Notice
    `rng`, an instance of our `RE` class. I’m ignoring some of the code head of *discrete_ravel.py*,
    so make sure to review the file itself. As before, the sample counts come from
    `bincout`, which we then turn back into a one-dimensional PMF by dividing `h`
    by the sum of the counts.
  prefs: []
  type: TYPE_NORMAL
- en: Three of the four outputs come next as `h`, `h` reshaped as a 3×4 matrix, and
    the first eight samples from `z`.
  prefs: []
  type: TYPE_NORMAL
- en: When mapping samples to pairs, we save time by calling `unravel_index`, which
    needs the one-dimensional indices along with the shape of the source array—here
    3×4 from `prob2`. NumPy returns the *x*- and *y*-coordinates, so a pair is (*x*[0],
    *y*[0]) and so on, as given by the list comprehension using `zip`.
  prefs: []
  type: TYPE_NORMAL
- en: We can also use this unraveling approach for distributions of more than two
    dimensions. If we have three random variables—*X*, *Y*, and *Z*—then samples from
    a three-dimensional PMF, *p[XYZ]*(*x*, *y*, *z*), are triplets, (*x*, *y*, *z*),
    according to the probability with which a particular combination of values appears.
    We unravel, sample in one dimension, and use `unravel_index` to map back to triplets.
    Bear in mind that as the dimensionality increases, the number of samples necessary
    to reasonably approximate the distribution goes up dramatically.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose every random variable in our system takes on one of 10 possible values.
    If we have only one variable, we must sample from a probability distribution representable
    as a vector of 10 elements. With two random variables, we need a matrix to represent
    the joint distribution, a 10 × 10 = 100-element vector when unraveled. If we have
    three random variables, we unravel to a vector of 10 × 10 × 10 = 1,000 elements;
    for four random variables, we need 10,000 elements.
  prefs: []
  type: TYPE_NORMAL
- en: Fixing the number of values at 10, an *n*-dimensional PMF unravels into a vector
    of 10*^n* elements—the distribution size scales exponentially with dimensionality.
    Therefore, this trick works best for only two or three dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '***Images***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now for a bit of fun. The code in *discrete_2d.py* knows how to use grayscale
    versions of images as discrete two-dimensional probability distributions, so we
    can sample from them. A grayscale image is a matrix of integer values from 0 to
    255, making an unraveled grayscale image immediately useful to FLDR as a distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Samples become pixel locations. The higher the image intensity at a pixel, the
    more likely it is to be sampled. Therefore, if we draw enough samples, scale them
    to [0, 1], and multiply by 255, we can transform the estimated distribution back
    into an image and compare it with the original. That’s a lot of words; let’s look
    at some code.
  prefs: []
  type: TYPE_NORMAL
- en: The four paragraphs in [Listing 12-7](ch012.xhtml#ch012list07) present the essential
    code, minus imports and command line processing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 12-7: Treating images as two-dimensional distributions and sampling
    from them*'
  prefs: []
  type: TYPE_NORMAL
- en: To turn the input image into a one-dimensional distribution, we first load the
    image, resize it to half its original dimensionality, and then unravel it into
    a list of pixel intensities, [0, 255]. Using a list comprehension with `int` is
    necessary because FLDR doesn’t work with NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: The next paragraph configures FLDR (`x`) and then uses it to draw `N` samples
    (`z`), with `N` given on the command line.
  prefs: []
  type: TYPE_NORMAL
- en: Samples in hand, `unravel_index` turns the one-dimensional samples into (*x*,
    *y*) pairs, or pixel locations. We then use the pixel locations to populate `im`,
    a two-dimensional histogram counting the number of times FLDR selected each pixel.
    To convert `im` into an image, we must scale it so that the most often sampled
    pixel has a value of 1, which we get by dividing by the maximum.
  prefs: []
  type: TYPE_NORMAL
- en: The final few lines of code create an output directory and dump the original
    and sampled images into it. We must multiply `im`, now [0, 1], by 255 and make
    it an unsigned int before writing it to disk as an image.
  prefs: []
  type: TYPE_NORMAL
- en: Run *discrete_2d.py* without arguments to learn the command line options. Try
    experimenting with the images in *test_images* and those in *images*. The latter
    contains high-contrast images that might make it easier to see where samples are
    coming from, especially the inverted images (those with “_inv” in their filename).
    The ramp images present a linear, quadratic, and cubic ramp, from left to right.
    We’ll sample brighter regions first as they’re more probable.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-6](ch012.xhtml#ch012fig06) shows one of the high-contrast images
    where white areas are most likely to be sampled. This version of the image prints
    well. The inverse version requires far fewer samples in general.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-6: Original hawk image (top left) and sampled images with an increasing
    number of samples*'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 12-6](ch012.xhtml#ch012fig06), the original image is in the upper
    left, followed by sampled images using 60,000, 120,000, 240,000, 480,000, and
    960,000 samples, left to right and top to bottom. I used this command line
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: varying the number of samples as needed.
  prefs: []
  type: TYPE_NORMAL
- en: This experiment concludes our investigation of sampling from discrete distributions.
    Let’s move on to the more mathematically relevant case of sampling from continuous
    distributions and learn some new techniques, culminating in our introduction to
    the world of Markov Chain Monte Carlo.
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous Distributions**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s change focus to consider continuous distributions, represented by PDFs
    that admit any real number input over their range. The techniques of the previous
    section no longer work in this case—at least not without alteration—but other
    methods exist, three of which we’ll explore: inverse transform sampling, rejection
    sampling, and Markov Chain Monte Carlo.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Inverse Transform***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We represent continuous distributions by PDFs. Note the word *function*, which
    tells us there’s a mathematical relationship describing the shape of the PDF.
    The CDF for a given PDF is an integral:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0340-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The integral is the continuous version of summing discrete probabilities. It
    represents the area under the PDF from –*∞* to some *x*. Replace –*∞* with any
    value below which the PDF is always zero.
  prefs: []
  type: TYPE_NORMAL
- en: The CDF runs from 0 up to 1, meaning that a plot of the CDF produces *y*-axis
    values that begin with 0 and end with 1; review [Figure 12-1](ch012.xhtml#ch012fig01)’s
    CDFs. If we pick a random value on the *y*-axis of the CDF plot, slide horizontally
    from there to the curve, and move down to the *x*-axis, we’ll have a sample from
    the PDF. Pick another *y*-axis starting point and repeat the process to get a
    new *x* and yet another sample from the PDF. Uniformly sampling *y*-axis values
    in [0, 1] produces *x* values that, when histogrammed, follow the form of the
    PDF.
  prefs: []
  type: TYPE_NORMAL
- en: To express this process mathematically, flip the graph of a function, *F*(*x*),
    along the line *y* = *x* (which runs 45 degrees up from the *x*-axis in the first
    quadrant), and you have a graph of the inverse of the function, *F*^(–1)(*x*),
    if it exists. The inverse flips *x* and *y* values, meaning inputs to the inverse
    function act like *y* values for the function, and the output of the inverse function
    is the *x* producing that input for the function itself.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, if we know the functional form of the inverse of a function representing
    a CDF, we can sample from the PDF by selecting random values in [0, 1] as inputs
    and keeping the outputs of the inverse CDF as the desired samples. This process
    is *inverse transform sampling*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s work through an example. Suppose we want to draw samples from an *exponential
    distribution*, whose PDF is
  prefs: []
  type: TYPE_NORMAL
- en: '*f*(*x*) = λ*e*^(–λ*x*)'
  prefs: []
  type: TYPE_NORMAL
- en: with λ (lambda) being a constant that decides how quickly the PDF decays from
    a maximum of λ at *x* = 0\. The most probable samples from this PDF are close
    to zero, with samples farther from zero less likely.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CDF for this PDF is an integral:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0340-03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, *F*(*x*) = 1 – *e*^(–λ*x*). If we find the inverse of this function,
    we can generate exponentially distributed samples from uniformly distributed inputs.
    To find the inverse, set the CDF equal to *u* (for *uniform sample*) and solve
    for *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0341-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We now have *F*^(–1)(*u*), a mapping from uniform inputs *u* in [0, 1], the
    range of the CDF, to *x*, a value selected based on the shape of the exponential
    distribution PDF. Before putting the inverse function to work, let’s make one
    more observation.
  prefs: []
  type: TYPE_NORMAL
- en: We don’t care, specifically, about the exact pairing of this *u* to that *x*
    in terms of a sequence of *u*’s. We plan on picking *u* values at random. Because
    *u* is in [0, 1], 1 – *u* is also in [0, 1], but “flipped” along the *u*-axis
    because it’s the complement of *u*, giving us two values that sum to 1\. So, we
    can replace 1 – *u* in *F*^(–1)(*u*) with *u*, and our samples will still be from
    the exponential distribution. This step isn’t necessary, but it makes the plot
    of the inverse function look less strange to us, as we’re used to curves that
    decay from a high point as *x* increases to the right.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-7](ch012.xhtml#ch012fig07) shows a plot of *F*^(–1)(*u*) = (–log
    *u*)/λ, where specific *u* values have been mapped to their respective *x* outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-7: Inverse transform sampling from* e^(–λ*x*) *using – log(*u*)/λ*'
  prefs: []
  type: TYPE_NORMAL
- en: The distribution of the *x* values—or a properly scaled histogram of many *x*
    values from many *u* inputs—will become a better approximation of λ*e*^(–λ*x*)
    as the number of samples increases.
  prefs: []
  type: TYPE_NORMAL
- en: The file *inverse.py* samples from functions supplied as the inverse of their
    CDF. In other words, we give the code *F*^(–1)(*u*) and the corresponding PDF,
    *f*(*x*), along with the desired number of samples, and it gives us the samples,
    along with a plot of the PDF and the histogram of the samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the code to draw samples from *f*(*x*) = 2*e*^(–2*x*). The inverse
    CDF for this PDF is *F*^(–1)(*u*) = (–log *u*)/2\. To draw 1,000 samples, use
    a command line like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The first argument is the desired number of samples. The second, enclosed in
    double quotation marks, is a NumPy-aware version of the code implementing *F*^(–1)(*u*)
    that uses NumPy functions and `u` as the independent variable. The next argument,
    also enclosed in double quotes, is *f*(*x*), the PDF. Note that it’s a function
    of `x`, not `u`. The remaining arguments are the output directory and the usual
    randomness source with an optional seed.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12 shows the output of *inverse.py* for 1,000 (left) and 10,000 (right)
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-8: 1,000 (left) and 10,000 (right) samples from 2*e^(*–2*x)'
  prefs: []
  type: TYPE_NORMAL
- en: The code scales the output so the curve and histogram match. The samples follow
    the desired distribution, with more samples better representing the PDF. The 10,000
    samples are beginning to select *x* values farther from zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s walk through another example. The Kumaraswamy distribution is like the
    beta distribution, but the functional forms of the PDF and CDF are conducive to
    inverse sampling. Specifically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0342-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *a* and *b* are constants that define the shape of the distribution, much
    like the *a* and *b* of the beta distribution. I leave it as an exercise for you
    to show that *F*^(–1)(*u*) comes from *F*(*x*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s draw samples from this distribution for *a* = 2 and *b* = 5\. The command
    line we need is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The resulting plot is in [Figure 12-9](ch012.xhtml#ch012fig09). As expected,
    the samples follow the shape of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-9: Sampling Kumaraswamy (2,5)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary code in *inverse.py* is straightforward because we supply the PDF
    and inverse CDF on the command line in a form that lets us use Python’s `eval`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: That’s all there is. We create `samples` to hold the `N` requested samples,
    and then loop to generate `samples[i]` from `u` by evaluating the inverse CDF
    function passed from the command line as the `ifunc` string. The remainder of
    *inverse.py* creates the output plot.
  prefs: []
  type: TYPE_NORMAL
- en: Inverse transform sampling is direct and works from closed-form functions, but
    it’s of limited applicability because of the two conditions that must be met to
    allow its use. The PDF must produce a closed-form CDF, and that CDF must be invertible
    to get *F*^(–1)(*u*). This doesn’t happen too often, especially for arbitrary
    continuous PDFs. In “Exercises” on [page 358](ch012.xhtml#ch00lev1_81), I suggest
    another PDF/CDF combination that works with *inverse.py*, but only if you restrict
    *u* to something other than [0, 1).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore the next continuous PDF sampling algorithm, rejection sampling.
    Unlike inverse transform sampling, rejection sampling works with arbitrary PDFs.
  prefs: []
  type: TYPE_NORMAL
- en: '***Rejection***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We want to draw samples from a function *q*(*x*) so that a histogram of many
    samples from the function converges on the shape of the function itself. While
    we don’t know how to sample directly from *q*(*x*), we can sample from a *proposal
    function* that we’ll call *p*(*x*). If we find a constant, *c*, such that
  prefs: []
  type: TYPE_NORMAL
- en: '*q*(*x*) *≤ cp*(*x*), ∀*x*'
  prefs: []
  type: TYPE_NORMAL
- en: then we can use samples from *p*(*x*) to draw samples from *q*(*x*). Recall
    that ∀ means “for all.”
  prefs: []
  type: TYPE_NORMAL
- en: First, we draw a sample from the proposal function, *x ∼ p*(*x*), where *∼*
    means “draw a sample from.” This gives us a candidate *x* position.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we pick a *y* value that’s some fraction of the way up from *x* but still
    less than *cp*(*x*). In other words, we pick a uniform value in the range [0,
    *cp*(*x*)], or *y* = *ucp*(*x*) for some *u* in [0, 1].
  prefs: []
  type: TYPE_NORMAL
- en: If *y ≤ q*(*x*), we keep *x* as a sample from *q*(*x*); otherwise, we reject
    *x* and repeat with another sample from *p*(*x*). We stop when the desired number
    of samples from *q*(*x*) have been kept.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-10](ch012.xhtml#ch012fig10) shows the situation for two candidate
    *x* positions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-10: Rejection sampling with two candidate* x *positions*'
  prefs: []
  type: TYPE_NORMAL
- en: The solid curve is *q*(*x*), the function we want to sample from. The dashed
    curve, here a uniform value over the range of *q*(*x*), is *cp*(*x*).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at *x* = *x*[0] first. The algorithm says to sample from *p*(*x*),
    which gives us *x*[0]. Next, we pick a *y* some fraction of the way up from *x*[0]
    to *cp*(*x*[0]). We can write this as *y*[0] = *u*[0]*cp*(*x*[0]). While we know
    that *y*[0] will always be less than or equal to *cp*(*x*[0]), we’re wondering
    whether *y*[0] is less than *q*(*x*[0]). For *x*[0], this is the case, so we accept
    *x*[0] as a valid sample from *q*(*x*).
  prefs: []
  type: TYPE_NORMAL
- en: In *x*[1], *y*[1] = *u*[1]*cp*(*x*[1]) is greater than *q*(*x*[1]), so we reject
    *x*[1] as a valid sample from *q*(*x*) and the process repeats.
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithmically, the process boils down to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x ∼ p*(*x*).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*u ∼ U*[0, 1).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If *ucp*(*x*) *≤ q*(*x*), accept *x* as a sample; otherwise, reject *x*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat from step 1 until we’ve accepted *N* samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You might see the condition of step 3 written as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0345-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which we get by dividing by *cp*(*x*). In that form, we’re asking whether *u*
    is less than the fraction of the way to *cp*(*x*) covered by *q*(*x*) for the
    selected *x*. If not, reject *x* and try again.
  prefs: []
  type: TYPE_NORMAL
- en: Think of rejection sampling as randomly throwing darts at the *xy*-plane. If
    the *y*-coordinate of the dart is less than both *cp*(*x*) and *q*(*x*), we accept
    the dart’s *x*-coordinate as a sample from *q*(*x*). In effect, we’re keeping
    all the *x*-coordinate values for darts that land under the *q*(*x*) curve. We
    did the same in [Chapter 3](ch03.xhtml) to estimate *π*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s put this process into practice with *rejection.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Rejection sampling works for any proposal function, *p*(*x*), so long as we
    can draw samples from it, but *rejection.py* restricts us to two options: a uniform
    distribution, represented by the dashed line in [Figure 12-10](ch012.xhtml#ch012fig10),
    and a normal distribution with a given mean (*µ*) and standard deviation (*σ*).
    The Box-Muller transform lets us sample from a normal distribution (see [Chapter
    1](ch01.xhtml)).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s reproduce the example in [Figure 12-10](ch012.xhtml#ch012fig10). The proposal
    function is a uniform distribution multiplied by 4.1, as this puts the proposal
    function just above the highest part of the sampling function, *q*(*x*)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0345-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which is the sum of two normal curves centered at ±5, with one being four times
    higher than the other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The output tells us we need over 830,000 candidate samples to get the requested
    100,000\. That’s a conversion rate of 12 percent, meaning we rejected 88 percent
    of the candidates. The efficiency of rejection sampling depends critically on
    the closeness between the proposal function, *cp*(*x*), and the sampling function,
    *q*(*x*). The closer the proposal function is to the sampling function, the better.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-11](ch012.xhtml#ch012fig11) shows a histogram of the samples from
    *q*(*x*). The proposal function is in [Figure 12-10](ch012.xhtml#ch012fig10).
    Note that rejection sampling doesn’t care whether *p*(*x*) and *q*(*x*) are normalized
    (in which the area under the curves is 1). So long as *cp*(*x*) is above *q*(*x*),
    all will be well (if perhaps slow).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-11: Sampling with a uniform proposal function*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use a normal curve for the proposal function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The proposal function is now a normal curve with a mean of 0 and a standard
    deviation of 1\. The multiplier is 4.
  prefs: []
  type: TYPE_NORMAL
- en: The output is in *reject1*. We’re told that we need 1.5 million candidates to
    get 100,000 samples for a conversion rate of 6.6 percent. [Figure 12-12](ch012.xhtml#ch012fig12)
    shows the histogram (left) and a plot of the proposal and sampling functions (right).
    The plots might seem strange to you, for good reason.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-12: Using N(0,1) as the proposal function*'
  prefs: []
  type: TYPE_NORMAL
- en: The proposal function is the dashed curve on the right in [Figure 12-12](ch012.xhtml#ch012fig12).
    It’s centered between the normal curves making up *q*(*x*), and is larger than
    *q*(*x*) over only a small region around zero. The algorithm can’t select samples
    in areas where *p*(*x*) < *q*(*x*); therefore, it selects only in the region covering
    the overlap between the right part of the leftmost normal curve and the left part
    of the rightmost normal curve.
  prefs: []
  type: TYPE_NORMAL
- en: In the histogram on the left, the code scales *q*(*x*) and the histogram of
    the samples, so both have 1 as their maximum *y* value. The histogram peaks at
    two locations, the left and right maxima of the overlap region. While not what
    we’re after, the output from this run is correct, given the constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try a few more examples. The file *run_rejection_examples* contains [Listing
    12-8](ch012.xhtml#ch012list08).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 12-8: Additional examples*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-13](ch012.xhtml#ch012fig13) shows the generated plots for *reject2*
    through *reject5* from top to bottom.'
  prefs: []
  type: TYPE_NORMAL
- en: The topmost plot covers the entire *q*(*x*) range, with samples drawn from each
    peak. The next row shows samples from only the left peak, as the proposal function
    covers it and a tiny part of the right peak. The third row limits selection to
    *x ∈* [–11, 4], restricting the range of samples while still mirroring the shape
    of *q*(*x*). The final row of [Listing 12-8](ch012.xhtml#ch012list08) switches
    to a new function, *q*(*x*) = 2*x*² + 3, and a uniform proposal function.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-13: Histograms and proposal functions for the command lines in [Listing
    12-8](ch012.xhtml#ch012list08)*'
  prefs: []
  type: TYPE_NORMAL
- en: Experiment with *rejection.py* with *q*(*x*) functions and proposal functions
    that either completely or partially cover *q*(*x*). Remember to pick a *c* such
    that *q*(*x*) < *cp*(*x*) for the regions from which you want to sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rejection sampling isn’t restricted to one dimension. For example, if we have
    *q*(*x*, *y*), we can draw samples as long as we can sample from *p*(*x*, *y*).
    The uniform function, which is 1 for all (*x*, *y*) points, is an easy *p*(*x*,
    *y*) to use. A multivariate normal distribution will also work, though it’s harder
    to code and visualize. The algorithm remains the same, but instead of drawing
    *x* from *p*(*x*), we draw (*x*, *y*) from *p*(*x*, *y*)—a random point in 2D
    space. The test is still *ucp*(*x*, *y*) *≤ q*(*x*, *y*), or:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0349-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If the condition holds, the point (*x*, *y*) is a sample from *q*(*x*, *y*).
  prefs: []
  type: TYPE_NORMAL
- en: The extension to arbitrary dimensions, ***x*** = (*x*[0], *x*[1], *x*[2], .
    . . ), follows as long as we can sample from *p*(***x***). However, as the dimensionality
    increases, the number of samples rejected tends to increase exponentially (for
    each new dimension) unless *p*(***x***) follows *q*(***x***) very closely, which
    is quite difficult to do while maintaining easy sampling from *p*(***x***). This
    effect, where the utility of rejection sampling decreases with the problem’s dimensionality,
    is a form of the *curse of dimensionality* that frequently plagues machine learning
    models as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The inefficiency of rejection sampling as the dimensionality of the problem
    increases leads to our final sampling algorithm, which handles high-dimensional
    problems: Markov Chain Monte Carlo.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Markov Chain Monte Carlo***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our final sampling algorithm is the most powerful: *Markov Chain Monte Carlo
    (MCMC)*. We learned about Monte Carlo algorithms in [Chapter 10](ch010.xhtml).
    The phrase *Markov chain*, named after Russian mathematician Andrey Markov (1856–1922),
    refers to a process where the *transition probability* of moving from a current
    state to a new state depends only on the current state and nothing that came before.
    Markov chains are helpful in simulations because what happens next depends solely
    on what the system is currently doing, regardless of its history.'
  prefs: []
  type: TYPE_NORMAL
- en: MCMC uses Markov chains to approximate sampling from a complex probability distribution.
    A *stationary distribution* in Markov chains, typically denoted as ***π***, is
    a vector in the discrete case or a PDF in the continuous case. Regardless of the
    initial distribution, walking the Markov chain eventually reaches the stationary
    distribution governed solely by the transition probabilities if specific criteria
    are met.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll first delve into stationary distributions; then, we’ll explore the Metropolis-Hastings
    algorithm and use it to walk a continuous Markov chain—only to realize that its
    stationary distribution is the very PDF we want to sample from.
  prefs: []
  type: TYPE_NORMAL
- en: '**Walking a Markov Chain**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s work through an example. Fezzes are all the rage this year. Everyone’s
    wearing them, and there are three colors: red, green, or blue. The probability
    of a person changing their fez color next year depends on the color they wear
    this year. The probabilities are fixed from year to year. What happens to an initial
    distribution of fez colors as time passes? Will the distribution of colors change
    continuously or eventually settle down to a specific, perhaps stationary, distribution?'
  prefs: []
  type: TYPE_NORMAL
- en: We encode transition probabilities in a matrix where the row shows a current
    state (a fez color) and the columns of that row represent the probability of a
    transition from the current state to a new state—a new fez color, which may be
    the same as the current.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0350-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The rows are red, green, and blue fezzes, as are the columns. Therefore, if
    someone is wearing a red fez this year, they have a 53 percent chance of wearing
    a red fez again next year, a 5 percent chance of changing to a green fez, and
    a 42 percent chance of donning a blue fez. The row sums to 1, as it must. Similarly,
    a green fez aficionado has an 83 percent likelihood of continuing to wear a green
    fez next year, though 13 percent will switch to a red fez, and a rogue 4 percent
    will go all in on a blue fez.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the transition matrix, we need an initial distribution of fezzes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0350-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The vector tells us 70 percent of the population owns a red fez, 24 percent
    a green one, and only 6 percent a blue one.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find out what the distribution looks like next year, we need to see what
    happens to the proportion of the population wearing each fez color when acted
    on by the transition matrix. Those wearing a red fez transition to new colors
    according to the transition matrix’s first row, [0.53, 0.05, 0.42]. We multiply
    the red-fez wearers by the transition probabilities to get the fraction of next
    year’s fez colors from those currently wearing a red fez:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0350-03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For the greens, we multiply the second row of the transition matrix by 0.24,
    and for the blues we multiply the last row by 0.06\. Finally, we sum across to
    get the new distribution of fez colors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, these steps involve nothing more than multiplying the current distribution
    as a row vector by the transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0350-04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Next year, 41 percent of the population will wear red fezzes, 25 percent will
    wear green, and nearly 34 percent blue. The Markov property tells us that the
    following distribution is this distribution multiplied again on the right by the
    transition matrix, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The file *markov_chain.py* accepts an initial distribution of fez colors (***π***)
    and a transition matrix (***P***) and generates the Markov chain until the distribution
    becomes stationary. To make things more interesting, the distribution of red,
    green, and blue fezzes is treated as an RGB color so that the output file, *markov_chain.png*,
    shows the transition from initial to stationary distribution as a color bar running
    from left to right.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the code with the previous values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The first three values are the initial distribution: red, green, and blue fezzes.
    The final argument, which must not contain spaces, is a Python list representing
    the transition matrix. The values are not precisely the same as before, but the
    inputs are scaled by their respective sums so that'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0351-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and likewise for the individual rows of the transition matrix.
  prefs: []
  type: TYPE_NORMAL
- en: We print the Markov chain
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: which tells us that the stationary distribution is 22 percent red, 51.5 percent
    green, and 26.4 percent blue fezzes.
  prefs: []
  type: TYPE_NORMAL
- en: Try experimenting with the code, changing the input distribution to `1 0 0`
    (100 percent red) or `0 0 1` (100 percent blue). You’ll always end up at the stationary
    distribution, though the number of links in the chain might differ. Then, alter
    the transition matrix and see what happens.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only portion of the code worth discussing builds the chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `while` loop runs until the difference between the new distribution and
    the `last` distribution is less than or equal to `eps`. The `chain` list holds
    the sequence of distributions. We use `@` to perform the matrix multiplication,
    ***π** ← **πP***.
  prefs: []
  type: TYPE_NORMAL
- en: The power behind MCMC comes from the fact that the Metropolis-Hastings algorithm,
    to which we now turn, runs the Markov chain without directly generating it, but
    as a proxy returns samples from ***π*** once the chain is long enough to reach
    the stationary distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploring Metropolis-Hastings**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A paper titled “Equation of State Calculations by Fast Computing Machines” appeared
    in the June 1953 edition of *The Journal of Chemical Physics*. The authors were
    Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H.
    Teller, and Edward Teller. The paper introduced the *Metropolis algorithm*, named
    as such because Metropolis’s name is first on the paper. However, as so often
    happens in the thoroughly human enterprise of science, the process that led to
    the algorithm is disputed. It appears more likely now that the actual inventors
    were Marshall and Arianna Rosenbluth, not Metropolis. All five authors have since
    passed away, so it’s doubtful we’ll ever know the whole story. We’ll refer to
    the algorithm by its modern name, the Metropolis algorithm, knowing full well
    that credit may belong elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: In 1970, Wilfred Hastings extended the algorithm to the case where the proposal
    distribution is not symmetric; hence the algorithm is now known as *Metropolis-Hastings
    (MH)*. We’ll restrict ourselves to a symmetric normal distribution as the proposal
    distribution, so, technically, we’re using only the Metropolis part.
  prefs: []
  type: TYPE_NORMAL
- en: MH generates samples using a proposal distribution in much the same way as rejection
    sampling; however, in this case, the proposal distribution walks around randomly
    (we’ll learn what that means soon) and, as a consequence, alters a Markov chain
    distribution. Run the random walk with proper rejection and acceptance of moves
    for long enough, and eventually, we’ll reach the Markov chain’s stationary distribution.
    At that point, the samples MH returns are from the stationary distribution, which
    is the distribution we want to draw samples from in the first place. How convenient!
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*A full mathematical treatment of MH and what it’s doing under the hood is
    beyond what we tackle here. Those interested will find a good summary on Gregory
    Gundersen’s blog:* [https://gregorygundersen.com/blog/2019/11/02/metropolis-hastings](https://gregorygundersen.com/blog/2019/11/02/metropolis-hastings)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: For our purposes, we’ll accept MH’s claims and instead look at the random walk
    version of the algorithm. MH requires a function to sample from, the functional
    form we want our samples to follow when we make a histogram of their distribution.
    This is the stationary distribution for a Markov chain, so we’ll call this function
    *π*(*x*) (not to be confused with the number, *π*). MH works well in the multidimensional
    case, but we’ll limit ourselves to one dimension, so it’s *π*(*x*) and not ***π***(*x*).
  prefs: []
  type: TYPE_NORMAL
- en: MH also requires a proposal distribution, *Q*(*x*). We’ll use a normal distribution
    because it’s symmetric, and we know how to sample from it efficiently, *x′ ∼ N*(*x*,
    *σ*) for a user-supplied *σ* and mean *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'With *π*(*x*) and *Q*(*x*) on hand, random walk MH is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: Pick an initial sample, *x*; for example, *x* = 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Propose a new sample based on the current: *x′ ∼ N*(*x*, *σ*).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define ![Image](../images/f0353-01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define *ρ* = min(1, *A*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define *u ∼* uniform(0, 1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If *u* < *ρ*, accept *x′*, *x ← x′*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Otherwise, keep *x*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Output *x* as a sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2–8 until all desired samples are collected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The acceptance value, *A*, evaluates the function we want to sample from using
    the current sample position, *x*, and the proposed sample position, *x′*. If this
    value, passed through *ρ* (rho) to limit it to a maximum of 1, is less than a
    random uniform sample, accept the proposal (*x′*) as a new sample; otherwise,
    stick with the current sample, *x*. Before looping, output whatever *x* is as
    a sample from the distribution, *π*(*x*).
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm is the simplest way to implement MH. In practice, we can make
    it even simpler because there’s no need to define *ρ*; we can use *A* as it is
    because *u* is always in [0, 1).
  prefs: []
  type: TYPE_NORMAL
- en: In step 2, the new proposal position, *x′*, comes from the proposal function,
    a normal distribution centered on *x*, the current sample. This is the random
    walk part. In step 6, if *x′* is ultimately accepted, it becomes the new *x* we
    use to pick the next proposal position. In other words, the normal curve jumps
    to a new position on the *x*-axis when a proposal is accepted. We’ll soon generate
    animations showing this behavior.
  prefs: []
  type: TYPE_NORMAL
- en: We base acceptance or rejection on the value of *π*(*x′*) and *π*(*x*), that
    is, the ratio of *π*’s *y* value at the proposed new sample position and the current.
    If *π* has a high value at the current position, the fraction, *A*, will be small,
    meaning the comparison in step 6 is less likely to succeed. If the proposal is
    rejected, *x* is output again as a sample from *π*. We want this because *π* has
    a high *y* value at that *x*. If *π*(*x*) is tiny, *π*(*x′*) is more likely greater,
    meaning *A* is greater than 1\. If *A* > 1, *ρ* = 1 and the proposal position
    will always be selected because *u* < 1\. Therefore, the parts of *π* less likely
    to be selected when viewing *π* as a PDF will be less often sampled.
  prefs: []
  type: TYPE_NORMAL
- en: Given this behavior, we can imagine that, in time, the random walk based on
    samples from the normal distribution will wander over *π* in proportion to *π*’s
    value at each position, thereby generating samples in the desired proportions.
    I haven’t commented on *σ*, the user-supplied parameter to MH, yet. We’ll experiment
    with it shortly and understand its effect then.
  prefs: []
  type: TYPE_NORMAL
- en: As for the definition of *A*, I’ve written it as the ratio of *π*(*x*) evaluated
    at the current and proposed *x* positions. This is the Metropolis version of the
    algorithm, which works with symmetric proposal distributions. If the proposal
    distribution isn’t symmetric, the numerator and denominator each have an additional
    multiplicative factor. In the symmetric case, this factor is the same for the
    numerator and denominator, so it cancels.
  prefs: []
  type: TYPE_NORMAL
- en: '*A* is a ratio, and Bayes’ theorem writes the posterior as the product of the
    likelihood and the prior, all divided by a normalizing factor that, in practice,
    is usually an intractable integral. Since it works with the ratio, MH cancels
    this integral, so we don’t need to compute it in the first place. MH makes it
    possible to sample from posterior distributions using only the likelihoods and
    priors. This makes Bayesians very happy and leads to the dramatic quote earlier
    in the chapter about Bayes and MCMC.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the algorithm to see that your samples *don’t* follow *π*(*x*). We’ve neglected
    a key statement about the MH algorithm: it doesn’t claim to immediately generate
    samples from *π*(*x*), but only in the limit, after some period of time. How long
    a time, and how many samples do we generate before we trust that the samples are
    coming from *π*(*x*)? There is no foolproof answer to that question. Our experiment
    with fezzes generally converged to the stationary distribution after a dozen or
    fewer iterations. That might be the case with MH, but it’s generally accepted
    that complex *π*(*x*) functions require many thousands of samples or more before
    they come from *π*(*x*). Therefore, when we implement MH in code, we’ll specify
    a number of *burn-in* samples, which we’ll throw away, and keep only those that
    come after. We did something similar in [Chapter 7](ch07.xhtml) when playing the
    chaos game to generate points on the attractor of an iterated function system.'
  prefs: []
  type: TYPE_NORMAL
- en: This is a random walk algorithm and a Markov algorithm because we randomly draw
    the next candidate sample, *x′*, from a distribution with a mean value based on
    the current sample, *x*. In a random walk, the next position is relative to the
    current position. It’s a Markov algorithm because history doesn’t matter; only
    the current sample position, *x*, influences any possible new position. Finally,
    it’s a Monte Carlo algorithm because it depends on randomness and isn’t guaranteed
    to generate accurate samples from *π*(*x*), at least initially.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into some code and contemplate the *mcmc.py* file. You’ll find code
    to parse the command line, sample from a normal distribution, and generate a series
    of plots using the samples—all of which we’ve seen several times before.
  prefs: []
  type: TYPE_NORMAL
- en: The heart of *mcmc.py* is the `MH` function ([Listing 12-9](ch012.xhtml#ch012list09)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 12-9: A random walk Metropolis-Hastings sampler*'
  prefs: []
  type: TYPE_NORMAL
- en: As with rejection sampling, `func` is a string defining *π*(*x*). The rules
    for its composition are the same as with *rejection.py*. We ultimately want `nsamples`’
    worth of samples, excluding the first `burn`’s worth, which we discard. This explains
    the `while` loop condition knowing that the list `samples` holds all the generated
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: The body of the `while` loop is a direct implementation of the MH algorithm,
    ignoring the explicit definition of *A* and *ρ*. First, we sample a proposal position,
    `p`, from a normal curve centered on the current sample position, `q`. Then, if
    we’ve given `MH limits`, they restrict the portion of *π*(*x*) we sample from
    in the end. We did the same with rejection sampling.
  prefs: []
  type: TYPE_NORMAL
- en: We define `func` with `x` as the independent variable, so we need to call `eval`
    and assign to `x` to get the numerator (`num`) and denominator (`den`). Finally,
    if *u* is less than `num`/`den`, accept `p` as the new `q` before appending `q`
    to `samples`.
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve acquired all samples—including those marked as burn-in, for plotting
    purposes—return `samples` as a NumPy vector after excluding the burn-in samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run *mcmc.py* without arguments to see the command line arguments it expects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: There are a lot of arguments here, but we know what most of them do. We want
    `N` samples after ignoring the first `burn` samples. We know that `func` is a
    string defining *π*(*x*). If `limits` isn’t `none`, it restricts the *x*-axis
    range sampled.
  prefs: []
  type: TYPE_NORMAL
- en: We use `q` to supply an initial sample position. Finally, the shape of the proposal
    function, the normal distribution from which *x′* is drawn, depends on `sigma`.
    If `sigma` is too small, the normal distribution is narrow, and it’s harder to
    jump to other parts of *π*(*x*). On the other hand, if `sigma` is larger, it’s
    easier to sample from all of *π*(*x*), to a point.
  prefs: []
  type: TYPE_NORMAL
- en: We understand `outdir`, `kind`, and `seed`. The final argument is the required
    string, either `yes` or `no`. If `yes`, the output plot showing *π*(*x*) and the
    histogram of samples will also show the normal distribution centered on the initial
    `q` with standard deviation `sigma`. Read through *mcmc.py* to understand how
    the output plots are made. Let’s run the code to understand what it produces.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with this command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We’re asking for 100,000 samples after 10,000 were thrown away as burn-in. We
    use the same sum-of-two-normal-curves function for *π*(*x*) as with rejection
    sampling. The `none` option opens all of the *x*-axis to sampling, though we’ll
    end up sampling only where *π*(*x*) is nonzero. The initial sample position is
    0 and `sigma` is 3\. Finally, we want to see the initial distribution function
    in the output plot; we’re fixing the pseudorandom generator and seed and dumping
    all output in `tmp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-14](ch012.xhtml#ch012fig14) shows the plots *mcmc.py* creates.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/12fig14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-14: Using Metropolis-Hastings to sample from π(*x*) (top) along
    with the trace (bottom left) and burn-in plots (bottom right)*'
  prefs: []
  type: TYPE_NORMAL
- en: The top plot is *π*(*x*) along with the histogram of the samples MH produced.
    Also included, because we said `yes` on the command line, is the initial proposal
    distribution, a normal curve centered at *x* = 0 with *σ* = 3\. Note that the
    curves are scaled to have 1.0 as their maximum value. As with rejection sampling,
    we’re looking for the shape of *π*(*x*) and the histogram to match.
  prefs: []
  type: TYPE_NORMAL
- en: The graphs on the bottom of [Figure 12-14](ch012.xhtml#ch012fig14) are *trace
    plots* that show the sampled *x* as a function of the sample number. Think of
    “sample number” as time, so the graphs show how *x* changes over time. The plot
    on the left follows samples generated after the burn-in period, while the plot
    on the right shows the burn-in samples.
  prefs: []
  type: TYPE_NORMAL
- en: The plots were created by the same command line as the top plot, but the total
    number of samples was set to 10,000, with 1,000 for burn-in. On the left, most
    samples are near *x* = –5, the peak of the larger normal curve from which *π*(*x*)
    is made. The remaining samples center on *x* = 5, the smaller peak. The burn-in
    plot on the right, however, jumps around near the respective peaks.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many things to explore with *mcmc.py*. I’ll offer two suggestions
    as starting points. I recommend running these command lines, then contemplating
    the output to see if you fully understand it. Remember to look at the trace plots
    as well, especially for *π*(*x*) = 2*x*² + 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Earlier, I promised that we’d create a movie showing the random walk inherent
    in our implementation of MH; I’ll make good on that promise now. Run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The file *mcmc_movie.py* is very similar to *mcmc.py*. The output directory,
    *tmp*, contains a new directory, *frames*. This directory contains files running
    from *frame_0000.png* to *frame_0899.png* showing each proposed sample (thin vertical
    line) along with each accepted sample (thick vertical line) as MH moves through
    its random walk. Use an image viewer that can page through a directory of files
    in alphabetical order to view the walk as a movie, or download *mcmc_movie.mp4*
    from the book’s GitHub page.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are some things you may wish to try:'
  prefs: []
  type: TYPE_NORMAL
- en: Update `ChooseMap` in *ifs.py* to use `Sequential` ([Listing 12-3](ch012.xhtml#ch012list03)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use *inverse.py* with ![Image](../images/f0358-01.jpg) instead of just *u*.
    Does anything change about the samples? What does this *F*^(–1)(*u*) look like?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Cauchy distribution is characterized by *µ* and *γ*. The PDF is![Image](../images/f0358-02.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'with corresponding CDF:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Image](../images/f0358-03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Try to sample from this function with *inverse.py*. Set *µ* = –2 and *γ* =
    1\. For example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What do you see? Now, replace *inverse.py* with *inverse_cauchy.py*. What’s
    the difference between the two programs? There are times when algorithms need
    to be tweaked to succeed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Execute the shell script *run_rejection_c* and explain the output in terms
    of the rejection test for cases when *cp*(*x*) ≫ *q*(*x*) versus just barely exceeding
    *q*(*x*). Hint: consider the likelihood of picking a *y* value for a given *x*
    when *q*(*x*) *≈ cp*(*x*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Experiment with *mcmc.py* using functions that are always positive over some
    given range. What happens as you make the burn-in larger or smaller? Try changing
    *σ*. Do large or small *σ* values work better? Here’s a function to try:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p[X]*(*x*) = sin³(*x*) + 1'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'I suggest a command line like this one:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Can you explain the plot and histogram? The limits are, roughly, –3*π* to 3*π*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we sampled from arbitrary distributions. First, we introduced
    terminology and concepts from Bayesian inference, a primary user of sampling techniques.
    After that, we sampled from discrete distributions, which often appear when working
    with histograms. We learned about sequential sampling and the FLDR, both of which
    run in ![Image](../images/c0301-01.jpg)(*n*) time—though, practically, the dice
    roller is some five to seven times faster.
  prefs: []
  type: TYPE_NORMAL
- en: We then experimented with sampling from a two-dimensional discrete distribution
    by unraveling the two-dimensional distribution to manipulate it as a one-dimensional
    distribution. As grayscale images are, in effect, two-dimensional discrete distributions,
    we sampled from them and observed that more intense pixels were sampled most often.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuous distributions, characterized by PDFs, came next. In certain cases,
    sampling becomes a simple process if the cumulative distribution function is invertible.
    When such is not the case, we explored two approaches: rejection sampling and
    MCMC with the MH algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Rejection sampling works well in one dimension, but suffers as the dimensionality
    of the samples increases. We explored how the algorithm behaves for two proposal
    distributions, the uniform and the normal, to realize that the closer the proposal
    function is to the actual PDF, the fewer samples are rejected and the more efficient
    the algorithm becomes.
  prefs: []
  type: TYPE_NORMAL
- en: When the distribution we want to sample becomes complex or is of high dimensionality,
    rejection sampling is best replaced by MCMC. We learned about MH in one dimension
    using a symmetric normal distribution as the proposal distribution. Animated plots
    showed the progress of the sampling algorithm over time.
  prefs: []
  type: TYPE_NORMAL
