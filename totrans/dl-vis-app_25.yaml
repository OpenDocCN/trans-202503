- en: '21'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '21'
- en: Reinforcement Learning
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: There are many ways to train a machine learning system. When we have a set of
    labeled samples, we can use supervised learning to teach the computer to predict
    the right label for each sample. When we can’t offer any feedback, we can use
    unsupervised learning and let the computer do its best. But sometimes we’re somewhere
    in between these two extremes. Perhaps we know *something* about what we want
    the system to learn, but it’s not as clear-cut as having labels for samples. Perhaps
    all we know is how to tell a better solution from a worse one.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 训练机器学习系统有很多方法。当我们有一组带标签的样本时，可以使用监督学习来教计算机预测每个样本的正确标签。当我们无法提供任何反馈时，可以使用无监督学习，让计算机尽力而为。但有时候我们介于这两者之间。也许我们对希望系统学习的内容有*一些*了解，但并不像有标签的样本那样明确。也许我们所知道的只是如何区分一个更好的解决方案和一个更差的解决方案。
- en: For example, we might be trying to teach a new kind of humanoid robot how to
    walk on two legs. We don’t know exactly how it ought to balance and how it should
    move, but we know we want it to be upright and not falling over. If the robot
    tries to slither on its belly, or hop on one leg, we can tell it that’s not the
    right way to proceed. If it starts with both legs on the ground and then uses
    them to make some forward progress, we can tell it that it’s on the right track
    and keep exploring those kinds of behaviors. This strategy of rewarding what we
    recognize as progress is called *reinforcement learning* *(RL)* (Sutton and Baro
    2018). The term describes a general approach to learning, rather than a specific
    algorithm.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能试图教一种新型的人形机器人如何用两条腿走路。我们并不确切知道它应该如何保持平衡和移动，但我们知道我们希望它站立起来而不是摔倒。如果机器人尝试趴在地上滑行，或者单腿跳跃，我们可以告诉它那不是正确的做法。如果它两条腿都放在地上，然后用它们前进，我们可以告诉它这条路是对的，并鼓励它继续探索这类行为。奖励我们认定为进步的策略被称为*强化学习*（*RL*）（Sutton
    和 Baro 2018）。这个术语描述了一种通用的学习方法，而非具体的算法。
- en: In this chapter we cover some of the basic ideas of this vast field. The key
    idea is that RL breaks up the simulated world into one entity that takes action
    and the rest of the world that responds to that action. To make this concrete,
    we will use RL to learn how to play a simple one-player game, and then dig into
    the details of the technique. We’ll begin with a simple algorithm that has some
    flaws and upgrade it into something that learns efficiently and well.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将介绍这一广阔领域的一些基本概念。核心思想是，强化学习将模拟的世界分解为一个采取行动的实体和响应这些行动的其他世界。为了更具体地说明，我们将使用强化学习来学习如何玩一个简单的单人游戏，然后深入探讨这种技术的细节。我们将从一个有缺陷的简单算法开始，并将其升级为能够高效学习的更好版本。
- en: Basic Ideas
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本概念
- en: Suppose that you’re playing a game of checkers with a friend, and it’s your
    turn. At this moment, you can move one of your pieces, and your friend has to
    wait. In reinforcement learning, we say that you’re the *actor* or *agent* since
    you have the choice of action. Everything else in the universe—the board, the
    pieces, the rules, and even your friend—are lumped together as the *environment*.
    These roles aren’t fixed. When it’s your friend’s turn to move, then they’re the
    agent, and everything else—the board, the pieces, the rules, and even you—are
    now part of the environment.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在和朋友下跳棋，轮到你了。此时，你可以移动你的一枚棋子，朋友需要等待。在强化学习中，我们说你是*行动者*或*智能体*，因为你有选择行动的权利。宇宙中的其他一切——棋盘、棋子、规则，甚至你的朋友——都被归为*环境*。这些角色并非固定不变。当轮到你朋友走棋时，他们就是智能体，而其他的一切——棋盘、棋子、规则，甚至你——现在都成了环境的一部分。
- en: When the actor or agent chooses an action, they change the environment. In our
    checkers game, you’re the agent, so you move one of your pieces, and maybe you
    remove some of your opponent’s pieces. The result is that the world has changed.
    In reinforcement learning, after an agent’s action, they’re given a piece of *feedback,*
    also called a *reward,* that tells them how “good” their action was, using whatever
    criteria we like. The feedback or reward is usually just a single number. Since
    we’re creating this world, the feedback can mean anything we want. In a game of
    checkers, for instance, a move that wins the game would be assigned a huge positive
    reward, whereas a move that loses the game would be assigned a huge negative reward.
    In between, the more a move seems to lead to victory, the bigger the reward.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当演员或代理选择一个行动时，他们会改变环境。在我们的跳棋游戏中，你是代理，所以你会移动你的棋子，也许还会移除对手的一些棋子。结果是，世界发生了变化。在强化学习中，代理的每一个行动之后，都会得到一条*反馈*，也叫*奖励*，它告诉他们“行动”有多“好”，这个评估可以依据我们喜欢的任何标准。反馈或奖励通常只是一个单一的数字。由于我们在创建这个世界，反馈可以代表任何我们想要的意义。例如，在一局跳棋游戏中，一步能赢得比赛的走法会被分配一个巨大的正向奖励，而一部导致失败的走法会被分配一个巨大的负向奖励。在两者之间，越是能够带来胜利的走法，奖励越大。
- en: Through trial and error, an agent can discover which actions are better than
    others in different situations, and can thus gradually make better and better
    choices as it gains experience. This approach works particularly well for situations
    in which we don’t already know the best thing to do at all times. For example,
    consider the problem of scheduling the elevators in a tall and busy office building.
    Even just figuring out where elevator cars ought to go when they’re empty is hard.
    Should the cars always return to the ground floor? Should some wait at the top?
    Should they wait at floors evenly distributed between the top and bottom? Maybe
    these policies should change over time so in the early morning and just after
    lunch, the cars should be on the ground floor, waiting for people arriving off
    the street, but in the late afternoon, they should be higher up, ready to help
    people descend and head home. There’s no obvious answer to how we should schedule
    a particular building. It all depends on the average traffic pattern for that
    building (and that pattern itself might depend on the time, season, or weather).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反复试验，代理能够发现不同情境下哪些行动比其他行动更好，从而随着经验的积累，逐渐做出更好的选择。这种方法对于我们并不总是知道最佳行动的情况特别有效。例如，考虑一下在一座高楼繁忙的办公大楼中安排电梯的问题。即便只是弄清楚空车应该去哪里也是个难题。电梯车厢应该总是返回到一楼吗？是否应该让一些车停在楼顶？还是应该停在楼层之间，分布均匀？也许这些策略应该随时间变化，在清晨和午饭后，电梯车应停在一楼，等待从街上来的人员，但在下午晚些时候，车应停得更高一点，准备帮助人们下楼，回家。如何安排一座特定大楼的电梯并没有明显的答案。一切都取决于大楼的平均交通模式（而这个模式本身可能依赖于时间、季节或天气）。
- en: This is an ideal problem for reinforcement learning. The elevator’s control
    system can try out a policy for directing the empty cars, and then use feedback
    from the environment (such as the number of people waiting for elevators, their
    average waiting time, the density of the elevator cars, etc.) to help it adjust
    that policy to perform as well as it can on the metrics we’re measuring.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是强化学习的理想问题。电梯的控制系统可以尝试一种指引空车的策略，然后利用来自环境的反馈（例如等待电梯的人数、他们的平均等待时间、电梯车厢的密度等）来帮助调整该策略，以便在我们衡量的指标上尽可能表现得更好。
- en: 'Reinforcement learning can help us with problems for which we don’t know the
    best result. We may not have a measurement as clear as the winning conditions
    of a game, but only better and worse outcomes. This is a key point: we may not
    be able to find any objective, consistent “right” or “best” answer. Instead, we’re
    trying to find the best answer we can with the information we have according to
    whatever metrics we’re measuring by. In some situations, we may not even have
    any idea of how well we’re doing along the way. For example, in a complex game,
    we might not be able to tell if we’re ahead or behind until the surprising moment
    when we win or lose. In those cases, we can only evaluate our actions in light
    of how things finally work out when the task is done.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习可以帮助我们解决那些我们不知道最佳结果的问题。我们可能没有像游戏的胜利条件那样明确的衡量标准，而只有更好或更差的结果。这是一个关键点：我们可能无法找到任何客观一致的“正确”或“最佳”答案。相反，我们是在根据我们所拥有的信息，并通过我们所衡量的任何标准，尝试找到我们能得到的最佳答案。在某些情况下，我们甚至可能不知道自己在过程中做得如何。例如，在一场复杂的游戏中，我们可能无法判断自己是领先还是落后，直到在我们赢或输的惊讶时刻。对于这些情况，我们只能根据最终任务完成时的结果来评估我们的行为。
- en: Reinforcement learning offers a nice way to model uncertainty. In simple rule-based
    games, we can, in principle, evaluate any board and select the best move, assuming
    that the other player always does the same. But in the real world, other players
    make moves that surprise us. And when we deal with the real world, where on some
    days more people need an elevator than on other days, we need to have strategies
    that can continue to perform well in the face of surprises. Reinforcement learning
    can be a good choice for these kinds of situations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习提供了一种很好的方式来建模不确定性。在简单的基于规则的游戏中，我们原则上可以评估任何棋盘并选择最佳的移动，假设另一个玩家总是做出相同的动作。但在现实世界中，其他玩家会做出让我们感到惊讶的举动。而且，当我们处理现实世界时，比如有些天比其他天更多人需要电梯，我们需要有能够在面对惊讶时仍然表现良好的策略。强化学习可能是应对这类情况的好选择。
- en: Let’s look at reinforcement learning in more detail with a specific example.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个具体的例子更详细地了解强化学习。
- en: Learning a New Game
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习一款新游戏
- en: Let’s see the steps involved in using reinforcement learning to teach a program
    how to play *tic-tac-toe*(also called *naughts and crosses*, or *Xs and Os*).
    To play, the players alternate placing an X or O in the cells of a three by three
    grid, and the first to get three of their symbols in a row (in any direction)
    is the winner. In the examples of [Figure 21-1](#figure21-1), we play O and our
    computer learner plays X.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用强化学习来教程序如何玩*井字游戏*（也叫*圈叉*，或*X与O*）的步骤来了解。游戏中，玩家交替在一个三乘三的网格中放置X或O，第一个在一行（任何方向）中放置三个相同符号的人获胜。在[图21-1](#figure21-1)的示例中，我们玩O，我们的计算机学习者玩X。
- en: '![F21001](Images/F21001.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![F21001](Images/F21001.png)'
- en: 'Figure 21-1: A game of tic-tac-toe, reading left to right. X moved first.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-1：一局井字游戏，从左到右阅读。X先走。
- en: 'In this scenario, the program we’re training is the agent. It’s playing against
    the environment, which would probably be simulated by another program that knows
    all about the game and how to play. The agent doesn’t know the rules of the game,
    how to win or lose, or even how to make moves. But our agent won’t be completely
    in the dark. At the start of each of the agent’s turns, the environment gives
    it two important pieces of information: the current board, and the list of available
    moves. This is shown in steps 1 and 2 of [Figure 21-2](#figure21-2).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个场景中，我们正在训练的程序是代理。它正在与环境进行对抗，环境可能由另一个了解游戏及如何玩的程序模拟。代理不知道游戏规则、如何获胜或失败，甚至不知道如何进行移动。但我们的代理并不会完全一无所知。在每次代理轮到行动时，环境会给它两个重要的信息：当前的棋盘和可用的动作列表。这个过程在[图21-2](#figure21-2)的步骤1和步骤2中有所展示。
- en: '![F21002](Images/F21002.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![F21002](Images/F21002.png)'
- en: 'Figure 21-2: The basic information exchange loop between a player and the environment
    in a game of tic-tac-toe'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-2：井字游戏中玩家与环境之间的基本信息交换循环
- en: In step 3, the agent picks a move, based on any methodology it likes. For example,
    it can pick at random, or consult an online resource, or use its own memory of
    previous games. Part of the challenge in reinforcement learning is designing an
    agent that can do a good job with the resources we have available for it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3步中，代理基于任何喜欢的方法选择一个动作。例如，它可以随机选择，或者查询在线资源，或者使用它对之前游戏的记忆。在强化学习中，挑战的一部分是设计一个能够充分利用我们为其提供的资源的代理。
- en: Once the agent picks a move, it communicates that to the environment in step
    4\. The environment then follows step 5, starting with actually making the move
    by placing an X in the chosen cell. The environment then checks to see if the
    agent has won. If so, it sets the reward to something big. Otherwise, it computes
    a reward based on how good the move seems to be for the agent. Now the environment,
    simulating the other player, makes its own move. If it’s won, then it changes
    the reward to something very low. If the game ended as a result of the environment’s
    or agent’s move, we call the reward an *ultimate reward* or *final reward.* In
    step 6, the environment sends the reward (sometimes this is called the *reward
    signal)* to the agent, so the agent can learn how good its selected move was.
    If nobody’s won, we return to the start of the loop and the agent gets to take
    another turn.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦代理选择了一个动作，它将在第 4 步中将其传达给环境。然后，环境按照第 5 步进行，首先通过在选择的单元格中放置一个 X 来实际执行该动作。接着，环境检查代理是否赢得了比赛。如果是，它会给出一个较大的奖励。否则，它会根据该动作对代理的好处来计算奖励。现在，环境模拟另一个玩家，进行自己的回合。如果环境赢了，它会将奖励设置为一个非常低的数值。如果游戏因环境或代理的动作结束，我们称该奖励为*最终奖励*或*终极奖励*。在第
    6 步中，环境将奖励（有时称为*奖励信号*）发送给代理，这样代理就可以学习它选择的动作有多好。如果没有人获胜，我们将返回到循环的开始，代理将再次进行一次回合。
- en: In some cases, we don’t give the agent the list of available moves. This might
    be because there are too many to list, or they have too many variations. Then
    we might give the agent some guidelines, or even no guidance at all.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们并不直接给代理提供可用的动作列表。这可能是因为可供选择的动作太多，或者它们有太多的变化。此时我们可能会给代理一些指导，或者甚至不提供任何指导。
- en: Following this procedure, the agent will probably make useless or terrible actions
    when it starts learning, but using the techniques below we’d hope that the agent
    will gradually learn to find good actions. For our discussions, we’ll keep things
    simple and assume that the agent is given a list of possible actions to choose
    from.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这个程序，代理在开始学习时可能会做出无用或糟糕的动作，但通过下面的技术，我们希望代理能够逐渐学会找到好的动作。为了讨论的方便，我们将简化问题，假设代理提供了一个可供选择的动作列表。
- en: The Structure of Reinforcement Learning
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习的结构
- en: Let’s reorganize and generalize our tic-tac-toe example into a more abstract
    description. This will let us embrace situations that go beyond turn-taking games.
    We’ll organize things into three steps, which we discuss in turn.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将井字游戏的例子重新组织和概括成一个更抽象的描述。这将使我们能够涵盖超出轮流游戏的情形。我们将事情组织成三个步骤，接下来我们将逐一讨论这些步骤。
- en: Before we begin, a bit of terminology. At the start of training, we place the
    environment into an *initial state*. In a board game, this is the setup for the
    start of a new game. In our elevator example, this might be placing all elevator
    cars on the ground floor. A full training cycle (such as a game from start to
    finish) is called an *episode*. We generally expect to teach the agent over a
    great many episodes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，先介绍一些术语。在训练开始时，我们将环境置于*初始状态*。在棋盘游戏中，这相当于为新游戏的开始进行布置。在我们的电梯示例中，这可能是将所有电梯轿厢放置在底楼。一个完整的训练周期（例如从开始到结束的游戏）称为*一个回合*。我们通常期望在许多回合中训练代理。
- en: 'Step 1: The Agent Selects an Action'
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤 1：代理选择一个动作
- en: We begin with [Figure 21-3](#figure21-3).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从[图 21-3](#figure21-3)开始。
- en: '![F21003](Images/F21003.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![F21003](Images/F21003.png)'
- en: 'Figure 21-3: The environment provides the agent with the current world state
    and a choice of actions. The agent chooses an action and communicates that to
    the environment.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-3：环境向代理提供当前的世界状态以及一系列可选择的动作。代理选择一个动作并将其传达给环境。
- en: Recall that the environment is the world in which all of our agent’s actions
    take place. The environment is completely described by a set of numbers that are
    collectively called the *environmental state*, the *state variables*, or simply
    the *state*. This might be a short list, or a very long one, depending on the
    complexity of the environment. In the case of a board game, the state is commonly
    made up of the position of all the markers on the board, plus any game assets
    (such as game money, power-ups, hidden cards, etc.) held by each player.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，环境是所有代理动作发生的世界。环境通过一组数字来完全描述，这些数字统称为*环境状态*，*状态变量*，或简单地称为*状态*。这可能是一个简短的列表，也可能是一个非常长的列表，具体取决于环境的复杂性。在棋盘游戏的情况下，状态通常由所有棋子在棋盘上的位置以及每个玩家持有的任何游戏资产（如游戏币、能量增强、隐藏卡牌等）组成。
- en: The agent then chooses one of the available actions. We often anthropomorphize
    the agent and talk about how the agent “wants” to achieve some result, such as
    winning a game or scheduling the elevators so nobody has to wait too long. In
    basic reinforcement learning, the agent is idle until the environment tells it
    that it’s time to take an action. The agent then chooses an action from the list
    of actions by using an algorithm called its *policy*, along with whatever *private
    information* the agent may have access to (including what it’s learned from previous
    episodes).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，智能体从可用的动作中选择一个。我们常常将智能体拟人化，谈论它如何“希望”达成某个结果，比如赢得一场游戏或调度电梯使得没人需要等太长时间。在基本的强化学习中，智能体处于空闲状态，直到环境告诉它该采取行动。然后，智能体使用一种叫做*策略*的算法，以及智能体可能访问的任何*私有信息*（包括它从先前的回合中学到的知识）来选择一个动作。
- en: We usually think of the agent’s private information as a database. It might
    contain descriptions of possible strategies or some kind of history of the actions
    that were taken in previous states and the rewards that were returned. The policy,
    by contrast, is an algorithm that is usually controlled by a set of parameters.
    The parameters usually change over time as the agent plays and searches for improved
    action-choosing policies.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将智能体的私有信息视为一个数据库。它可能包含可能策略的描述，或者记录在先前状态中采取的动作以及得到的奖励的某种历史记录。与此相对，策略是一个通常由一组参数控制的算法。这些参数通常会随着智能体的游戏进行而变化，在此过程中智能体会搜索更好的动作选择策略。
- en: We usually don’t think of the agent implementing its action. Instead, the chosen
    action is reported to the environment, and the environment takes care of performing
    the action. This is because the environment is in charge of the state. Returning
    to our elevator example, if the agent directs a car to move from the 13th floor
    to the 8th floor, the agent doesn’t update the state to place the car at the 8th
    floor. Something might go wrong along the way, such as a mechanical failure causing
    the car to get stuck. The agent simply tells the environment what it wants to
    do, and the environment tries to make that happen, maintaining the state so it’s
    always a correct picture of the current situation. In our tic-tac-toe game, the
    state contains the current distribution of X and O markers on the board.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常不认为智能体执行其动作。相反，选择的动作会被报告给环境，由环境负责执行该动作。这是因为环境负责状态的管理。回到我们的电梯示例，如果智能体指示一辆电梯从13楼移动到8楼，智能体并不会更新状态来将电梯置于8楼。途中可能会出问题，比如机械故障导致电梯卡住。智能体只是告诉环境它想做什么，环境会尽力去实现这一目标，同时保持状态，使其始终反映当前的情况。在我们的井字游戏中，状态包含了棋盘上当前X和O标记的分布情况。
- en: 'Step 2: The Environment Responds'
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 2 步：环境响应
- en: '[Figure 21-4](#figure21-4) shows step 2 of our reinforcement learning overview.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 21-4](#figure21-4) 显示了我们强化学习概述的第 2 步。'
- en: '![F21004](Images/F21004.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![F21004](Images/F21004.png)'
- en: 'Figure 21-4: Step 2 of our reinforcement learning process. This step starts
    with the computation of the new state (far right).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-4：我们强化学习过程的第 2 步。此步骤从计算新状态（最右侧）开始。
- en: In this step, the environment processes the agent’s action to produce a new
    state and processes the information that follows from this change. The environment
    saves its new state in the state variables, so that they reflect the new environment
    when the agent next gets to choose an action. The environment also uses its new
    state to determine what actions will be available to the agent on its next move.
    The previous state and the available actions are entirely replaced by their new
    versions. Lastly, the environment provides a reward signal that tells the agent
    how “good” its last chosen action was. The meaning of “good” is completely dependent
    on what this whole system is doing. In a game, good actions are moves that lead
    to stronger positions or even victory. In an elevator scheduling system, good
    actions might be those that minimize wait times.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一过程中，环境处理智能体的动作，生成一个新状态，并处理这一变化所带来的信息。环境将其新状态保存在状态变量中，以便在智能体下一次选择动作时，状态能够反映新的环境。环境还使用其新状态来确定下一步可以供智能体选择的动作。前一个状态和可用的动作完全被它们的新版本所替代。最后，环境提供一个奖励信号，告诉智能体它上次选择的动作有多“好”。“好”的含义完全取决于整个系统在做什么。在游戏中，好的动作是那些能带来更强位置或甚至胜利的动作。在电梯调度系统中，好的动作可能是那些能最小化等待时间的动作。
- en: 'Step 3: The Agent Updates Itself'
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 3 步：智能体更新自身
- en: '[Figure 21-5](#figure21-5) shows step 3 of our reinforcement learning overview.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 21-5](#figure21-5)展示了强化学习概述中的步骤 3。'
- en: '![F21005](Images/F21005.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![F21005](Images/F21005.png)'
- en: 'Figure 21-5: Step 3 of our reinforcement learning process, where the agent
    updates itself in response to the reward'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-5：强化学习过程中步骤 3，智能体根据奖励更新自身
- en: In this step, the agent uses the reward value to update its private information
    and policy parameters so that the next time this situation comes around, it’s
    able to build on what it has learned from this choice. After step 3, the agent
    might wait quietly until the environment tells it that it’s time to take action
    again. Alternatively, it can immediately start planning for its next move. This
    is particularly useful for some real-time systems where the reward precedes the
    full calculation of the new state.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步骤中，智能体利用奖励值更新其私有信息和策略参数，这样下次遇到相同情境时，能够建立在从此次选择中学到的东西上。步骤 3 完成后，智能体可能会安静地等待，直到环境告诉它再次行动的时机。或者它可以立即开始为下一步行动进行规划。这对于某些实时系统特别有用，因为奖励通常在新状态的完整计算之前就已经产生。
- en: Rather than simply stashing each reward into its private information, an agent
    usually processes that reward in some way to extract as much value from it as
    possible. This might even involve changing the values for other actions. For example,
    if we have just won a game and received an ultimate reward, we probably want to
    add a little bit of that reward to each of the moves that led us to victory.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体通常不会只是将每一个奖励存入其私有信息中，而是会以某种方式处理这些奖励，从中提取尽可能多的价值。这甚至可能涉及改变其他行动的值。例如，如果我们刚刚赢得了一场游戏并获得了终极奖励，我们可能希望将一些奖励分配到导致我们胜利的每一步行动上。
- en: The goal of reinforcement learning is to discover ways to help the agent in
    this scenario learn from the feedback to choose actions that bring it the best
    possible rewards. Whether it’s winning a game, scheduling elevators, designing
    vaccines, or moving a robot, we want to create an agent that can *learn from experience*
    to become as good as possible at manipulating its environment to bring about positive
    rewards.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是在这种情境下帮助智能体从反馈中学习，选择带来最佳奖励的行动。无论是赢得游戏、调度电梯、设计疫苗还是操作机器人，我们都希望创建一个能够*从经验中学习*的智能体，使其能够尽可能有效地操控环境，带来积极的奖励。
- en: Back to the Big Picture
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回到大局观
- en: Now that we’ve seen the overall approach, let’s look at some big-picture issues.
    When the agent updates its policy, it might have access to all the parameters
    of the state, or only some of them. If an agent gets to see the entire state,
    we say it has *full observability*, otherwise it has only *limited observability*
    (or *partial observability*). One reason we may give an agent only limited observability
    is that some parameters may be very expensive to compute, and we’re not sure if
    they’re relevant or not. So, we block the agent’s access to those parameters to
    see if doing so hurts the agent’s performance. If leaving them out does no harm,
    we can leave them off entirely from then on and save effort. Or we can only compute
    them and make them visible when they seem necessary. Another example of partial
    observability is if we’re teaching the system to play a card game such as poker.
    We don’t reveal to the system we’re teaching what cards are in its opponent’s
    hand.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了整体方法，让我们来看一些大局观的问题。当智能体更新其策略时，它可能能够访问整个状态的所有参数，或者只访问其中的一部分。如果智能体能够看到整个状态，我们称它为具有*完全可观察性*，否则它只有*有限可观察性*（或*部分可观察性*）。我们可能给智能体仅限的可观察性，其中一个原因是某些参数可能计算成本非常高，而我们不确定它们是否相关。因此，我们限制智能体访问这些参数，以观察这样做是否会影响智能体的表现。如果不包含这些参数不会造成任何伤害，我们可以从此以后完全忽略它们，节省计算精力。或者我们可以仅在看似必要时计算这些参数并使其可见。部分可观察性的另一个例子是我们正在教系统玩一款扑克牌游戏。我们不会向正在学习的系统揭示对手手中有哪些牌。
- en: As soon as we start thinking about using feedback to train agents in the way
    we’ve been discussing, we find ourselves facing two interesting problems. First,
    when we receive an ultimate reward (perhaps for winning or losing a game), we
    want to share some of that reward with every move we made along the way. Suppose
    we’re playing a game and make a winning move. That final move gets great feedback,
    but the intermediate steps were essential, and we should remember that they led
    to victory. That way if we see those intermediate boards again, we are more likely
    to select the move that leads to winning. Finding a way to share the ultimate
    reward this way is called the *credit assignment problem*. By the same token,
    if we lose, we’d want to let the moves that led us there take some of the blame,
    so we are less likely to select them again.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们开始考虑如何利用反馈来训练代理，就会发现自己面临两个有趣的问题。首先，当我们获得最终奖励时（例如赢得或输掉一场游戏），我们希望将这一奖励分配给我们沿途做出的每一个决策。假设我们正在玩一场游戏，并做出了一个获胜的决策。这个最终的决策会得到很好的反馈，但中间的步骤同样至关重要，我们应该记住它们也促成了胜利。这样，如果我们再次看到这些中间的棋盘，我们更有可能选择那个能够带来胜利的决策。以这种方式分配最终奖励的问题叫做*信用分配问题*。同样地，如果我们输了，我们也希望让那些导致失败的步骤承担一些责任，这样我们就不太可能再次选择它们。
- en: Second, suppose at some point the agent sees a situation (such as a game board)
    that it has seen before, and that at some earlier point, it tried a move that
    got a reasonably good score. But it hasn’t yet tried some of the other possible
    moves. Should it select the safe move with known returns, or risk something new
    that might either lead to failure or to even greater success? Somehow we need
    to decide, each time we pick an action, whether we want to take a risk with a
    new action and *explore* where it might lead, or play it safe with an action we’ve
    tried before and *exploit* what we’ve already learned. This is called the *explore
    or exploit dilemma*. Part of the task of designing a reinforcement learning system
    is thinking about how we want to balance these issues of the known and unknown,
    or guarantee and risk.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，假设在某一时刻，代理看到了一种它曾经见过的情境（比如一个游戏棋盘），并且在之前尝试过一个获得相对较好分数的动作。但它还没有尝试过其他可能的动作。它应该选择那个已知回报的安全动作，还是冒险尝试一个可能会失败或者带来更大成功的新动作？我们需要在每次选择动作时做出决定，是要冒险探索未知的行动，看看它能带我们去哪里，还是选择我们已经尝试过的安全行动，*利用*我们已经学到的东西。这就是*探索还是利用困境*。设计强化学习系统的一部分任务就是思考我们如何平衡已知与未知、保证与风险之间的问题。
- en: Understanding Rewards
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解奖励
- en: For the agent to perform as well as possible, it should be guided by a policy
    that leads the agent to pick the actions that deliver the highest rewards. Understanding
    the nature of rewards, and how to use them wisely, is time well spent. Let’s dig
    in.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使代理表现得尽可能好，它应该受到一个策略的引导，使代理选择那些能带来最高奖励的行为。理解奖励的性质，并学会明智地使用奖励，是非常值得投入的时间。让我们深入探讨一下。
- en: 'We can distinguish between two categories of rewards: *immediate* and *long
    term*. Immediate rewards are the ones we’ve focused on so far. The environment
    delivers these back to the agent right after executing an action, as we saw in
    [Figure 21-2](#figure21-2). Long-term rewards are more general and refer to our
    overall objective, like winning a game.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以区分两类奖励：*即时奖励*和*长期奖励*。即时奖励是我们迄今为止关注的重点。环境在执行一个动作后，立即将这些奖励反馈给代理，就像我们在[图21-2](#figure21-2)中看到的那样。长期奖励则更为广泛，指的是我们的整体目标，比如赢得一场游戏。
- en: We’d like to understand each immediate reward in the context of all the other
    rewards we get during a given game, or episode. There are lots of ways to interpret
    rewards and what they should mean to us. Let’s look at one popular approach called
    the *discounted future reward (DFR)*. This is a way to address the credit assignment
    problem, or make sure that all the actions that led us to success share in that
    ultimate victory.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在考虑每次游戏或回合中的所有其他奖励的背景下，理解每个即时奖励。奖励的解释方式有很多种，不同的方式会让它们对我们产生不同的意义。让我们来看一种流行的方法，称为*折扣未来奖励（DFR）*。这是一种解决信用分配问题的方法，或者说，确保所有带领我们走向成功的行为都能共享最终的胜利。
- en: To see how DFR works, we need to unwind the reward process a little bit. Let’s
    imagine that we’re an agent playing a game. When the game is done, we can line
    up the rewards we’ve collected for that game in a list, one after the other in
    the order they were received, along with the moves that earned those rewards.
    Adding up all the rewards gets us the *total reward* for that game, as in [Figure
    21-6](#figure21-6).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解 DFR（动态未来奖励）是如何工作的，我们需要稍微拆解一下奖励过程。假设我们是一个在玩游戏的代理。游戏结束后，我们可以将我们在这局游戏中获得的所有奖励按顺序列出，并记录下为这些奖励所做的操作。将所有奖励加起来，我们就得到了这局游戏的*总奖励*，如[图
    21-6](#figure21-6)所示。
- en: '![F21006](Images/F21006.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![F21006](Images/F21006.png)'
- en: 'Figure 21-6: The total reward associated with any episode is the sum of all
    the rewards that arrive from the first to the last move of the episode.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-6：与任何回合相关的总奖励是从回合的第一步到最后一步所获得的所有奖励的总和。
- en: We can add up any piece of this list, such as the first five entries, or the
    last eight. Let’s start at move 5 and add up all the rewards from there up to
    the game’s end, as in [Figure 21-7](#figure21-7).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个列表中的任意一部分加起来，例如前五个条目，或最后八个条目。让我们从第五个操作开始，将从此处到游戏结束的所有奖励加起来，如[图 21-7](#figure21-7)所示。
- en: '[Figure 21-7](#figure21-7) shows us the *total future reward*, or *TFR*, associated
    with the fifth move of the game. It’s that part of the total reward that comes
    from the fifth move and all the moves that followed it.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 21-7](#figure21-7)展示了与游戏第五个操作相关的*总未来奖励*（TFR）。它是第五个操作及其之后所有操作所带来的总奖励的一部分。'
- en: The very first move of a game is special, because its total future reward is
    the same as the game’s total reward. Since our rewards so far are always zero
    or positive, each subsequent move’s TFR is equal to or less than the TFR of the
    move before it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏的第一次操作是特别的，因为它的总未来奖励与游戏的总奖励相同。由于我们至今所获得的奖励总是零或正数，每个后续操作的 TFR 都等于或小于前一个操作的
    TFR。
- en: '![F21007](Images/F21007.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![F21007](Images/F21007.png)'
- en: 'Figure 21-7: The total future reward for any move is the sum of the reward
    for that move and all other moves to the end of the episode.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-7：任何操作的总未来奖励是该操作奖励与该操作之后所有其他操作奖励的总和，直到本局结束。
- en: The total future reward is a good description of how well a given move contributed
    to a game we just finished, but it’s not as good at predicting how useful that
    move might be in future games, even if they start with the exact same sequence
    of moves. This is because real environments are unpredictable. If we’re playing
    a multiplayer game, we can’t be sure that the other player (or players) will act
    the same way in the next game as they did in the previous game. If they make a
    different move, then that can change the trajectory of the game, and thus it can
    also change the rewards we earn. It can even change whether we win or lose. Even
    if we’re playing solitaire, we might be playing with a shuffled deck of cards,
    or a computer game with pseudorandom numbers, so we can’t be sure what’s going
    to come our way in the future, even if we play the exact same way we did in the
    past.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 总未来奖励是一个很好的描述，说明了某一特定操作对我们刚完成的游戏的贡献，但它并不是预测该操作在未来游戏中可能有多大作用的最佳方式，即使它们从完全相同的操作序列开始。这是因为现实环境是不可预测的。如果我们在玩一款多人游戏，我们不能确定其他玩家（或玩家们）在下一局中是否会像上一局那样行动。如果他们做出了不同的操作，那么这可能会改变游戏的轨迹，从而也会改变我们所获得的奖励，甚至可能改变我们是赢还是输。即使我们在玩单人游戏，我们也可能是在用一副洗过的扑克牌玩，或者是在玩带有伪随机数的电脑游戏，因此即使我们按以前相同的方式玩，我们也不能确定未来会发生什么。
- en: Immediate rewards are more reliable. We can imagine two types of immediate rewards.
    The first tells us the quality of the move we just made, *before* the environment
    responds. For example, in our game of tic-tac-toe, if the agent places an X in
    some cell, they can receive an *instant reward* describing how well the player
    is set up to win later on before the environment makes its move in return. This
    kind of reward is completely predictable. If we face the identical environment
    again later and make the same move, we get the same reward.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 立即奖励更加可靠。我们可以想象两种类型的立即奖励。第一种奖励告诉我们刚才所做操作的质量，*在*环境做出回应之前。例如，在我们的井字棋游戏中，如果代理在某个格子里放置了一个
    X，他们可以在环境回应之前，得到一个*即时奖励*，这个奖励描述了玩家如何为未来的胜利做准备。这种奖励是完全可预测的。如果我们再次面对相同的环境并做出相同的操作，我们会得到相同的奖励。
- en: The second kind of reward tells us the quality of the move we just made *after*
    the environment responds, so the reward can be influenced by the environment’s
    move. This type of reward, which we might call the *resulting reward*, isn’t as
    predictable as the instant reward because the environment might respond in a different
    way each time we make the move.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第二类奖励告诉我们在环境响应之后我们刚刚做出的动作的质量，因此奖励可能会受到环境动作的影响。这种类型的奖励，我们可以称之为*结果奖励*，它不像即时奖励那样可预测，因为每次我们做出动作时，环境的响应可能会不同。
- en: Let’s compare the two. Suppose we’re training an agent-powered robot how to
    use a remote control to turn on a device. It might pick up the remote control,
    press the power button, and put the remote back down, doing the same thing 100
    times in a row, earning high rewards. But all this time, the battery is draining,
    so the 101st time the agent repeats the process, the device won’t turn on. If
    the agent receives the instant reward for pressing the button, that is, the one
    that is computed and returned *before* the environment responds, the agent gets
    a large reward, because it did the right thing. On the other hand, the resulting
    reward, which is computed and returned *after* the environment responds, will
    be low or even 0, because the device failed to turn on.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来比较这两者。假设我们正在训练一个由智能体驱动的机器人如何使用遥控器打开设备。它可能会拿起遥控器，按下电源按钮，然后将遥控器放回，重复做同样的事情100次，获得高奖励。但在这个过程中，电池一直在消耗，所以第101次机器人重复这个过程时，设备无法打开。如果智能体得到的是按下按钮时的即时奖励，也就是在环境响应*之前*计算并返回的奖励，智能体将获得大量奖励，因为它做对了事情。另一方面，结果奖励是在环境响应*之后*计算并返回的，将会很低，甚至为0，因为设备未能打开。
- en: From here on, we’ll be using the resulting reward when we refer to the immediate
    reward.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，当我们提到即时奖励时，我们将使用结果奖励。
- en: When something works 100 times in a row but then fails the 101st time, that’s
    a *surprise*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当某件事连续100次有效，但第101次失败时，那就是一个*惊讶*。
- en: It’s important to deal gracefully with surprises, because most environments
    are unpredictable. Generally speaking, each action we take is intended to bring
    about a result. So waiting to see that result, even if we can’t be certain about
    what will happen, is a big part of understanding if our action represented a good
    choice.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 处理惊讶的情况非常重要，因为大多数环境都是不可预测的。一般来说，我们采取的每个行动都是为了带来一个结果。因此，即使我们不能确定会发生什么，等待看到这个结果也是理解我们行动是否是一个好选择的重要部分。
- en: We say that real environments, with their unpredictable elements, are *stochastic*.
    By contrast, a perfectly predictable environment (such as a game based purely
    on logic) is *deterministic*. The amount of unpredictability (or *stochasticity*)
    can vary in amount. If the unpredictability is low (that is, the environment is
    largely deterministic), then we may feel pretty confident about saying that the
    rewards we just received are likely to be repeated, or very nearly so, in future
    games. With very high unpredictability (that is, in a largely stochastic environment),
    we have to assume that if we repeat the same actions, any predictions we make
    about future rewards should be considered little more than estimates.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说真实的环境因为其中的不确定因素是*随机的*。相比之下，一个完全可预测的环境（比如完全基于逻辑的游戏）是*确定性的*。不确定性（或者*随机性*）的程度可以有所不同。如果不确定性低（即环境大致上是确定性的），那么我们可能会相当有信心地认为我们刚刚获得的奖励很可能在未来的游戏中重复，或者几乎如此。在不确定性非常高的情况下（也就是说，在一个主要是随机的环境中），我们必须假设，如果我们重复相同的动作，任何关于未来奖励的预测都应被视为仅仅是估算。
- en: We quantify our estimate of the stochasticity, or uncertainty, of the environment
    with a *discount factor*. This is a number between 0 and 1, usually written with
    the lowercase Greek letter *γ* (gamma). The value of *γ* that we select represents
    our confidence in the repeatability of the environment. If we think the environment
    is close to being deterministic and that we’ll get about the same reward for a
    given action every time, we set *γ* to a value near 1\. If we think the environment
    is chaotic and unpredictable, we set *γ* to a value nearer to 0.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过*折扣因子*来量化对环境随机性或不确定性的估计。这个数值介于0和1之间，通常用小写的希腊字母*γ*（gamma）表示。我们选择的*γ*值代表我们对环境可重复性的信心。如果我们认为环境接近于确定性的，并且每次给定动作都能获得相似的奖励，我们会将*γ*设置为接近1的值。如果我们认为环境是混乱且不可预测的，我们会将*γ*设置为接近0的值。
- en: We need to somehow accommodate unexpected surprises into our learned rewards
    in a principled way. One way to do that is to create a modified version of the
    total future reward that accounts for how confident we are that the game will
    proceed in just the same way again. We generally attach high values of this modified
    TFR to moves we feel confident about, and lower values to the others.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要以一种有原则的方式将意外的惊喜考虑到我们已经学习到的奖励中。实现这一点的一种方法是创建一个修改版的总未来奖励，考虑到我们对游戏是否会以相同方式继续进行的信心。我们通常会将高的修改版TFR值附加在我们有信心的动作上，而将较低的值附加在其他动作上。
- en: We can use the discount factor to create a version of the total future reward
    called the discounted future reward (DFR). Rather than adding up all the rewards
    that come after an action, as we do for the TFR, we start with the immediate reward,
    and then we reduce the values of the subsequent rewards by multiplying them by
    *γ* one time for each step they are in our future. The reward for one step in
    the future is multiplied by *γ* once, the reward after that is multiplied by *γ*
    twice, and so on. This accounts for the fact that we consider future rewards increasingly
    less reliable. The technique is illustrated graphically in [Figure 21-8](#figure21-8).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用折扣因子来创建一个称为折扣未来奖励（DFR）的总未来奖励版本。与为TFR加总所有动作之后的奖励不同，我们从即时奖励开始，然后通过将后续奖励分别乘以*γ*，将其逐步折扣。未来一步的奖励乘以*γ*一次，接下来的奖励乘以*γ*两次，依此类推。这考虑了我们对未来奖励的可靠性逐渐降低的事实。此技术在[图
    21-8](#figure21-8)中有图示说明。
- en: Notice that in [Figure 21-8](#figure21-8) each successive value gets multiplied
    by *γ* one more time than the one before. These increased multiplications can
    have a significant effect on the amount by which each reward contributes to the
    sum.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在[图 21-8](#figure21-8)中，每个后续的值都比前一个值多乘以一次*γ*。这些增加的乘法可能会对每个奖励对总和的贡献量产生显著影响。
- en: '![F21008](Images/F21008.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![F21008](Images/F21008.png)'
- en: 'Figure 21-8: The DFR is found by adding together the immediate reward, the
    next reward after multiplying it by gamma, the reward after that multiplied by
    gamma twice, and so on.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-8：通过将即时奖励、下一个奖励乘以*γ*、下一个奖励再乘以*γ*两次，以此类推，来得到DFR。
- en: Let’s see this in action. We can consider the reward and the discounted future
    reward we’d get from our opening move in a game, using several values for *γ*.
    [Figure 21-9](#figure21-9) shows a set of immediate rewards for an imaginary game
    with 10 moves.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个实际应用。我们可以考虑在一局游戏中的开局动作所获得的奖励和折扣后的未来奖励，使用不同的*γ*值。[图 21-9](#figure21-9)展示了一个假想游戏中10步的即时奖励。
- en: '![F21009](Images/F21009.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![F21009](Images/F21009.png)'
- en: 'Figure 21-9: Immediate rewards for a game with 10 moves. The game ended without
    a clear winner.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-9：一个有10步的游戏的即时奖励。游戏结束时没有明确的赢家。
- en: Applying different future discounts to these rewards following [Figure 21-8](#figure21-8)
    gives us the curves of [Figure 21-10](#figure21-10). Notice how quickly the rewards
    drop to 0 as the discount factor *γ* decreases. This means that we’re less sure
    of our predictions of the future.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些奖励应用不同的未来折扣，参照[图 21-8](#figure21-8)，我们得到了[图 21-10](#figure21-10)中的曲线。请注意，随着折扣因子*γ*的减小，奖励很快就会降到0。这意味着我们对未来的预测不再那么确定。
- en: '![F21010](Images/F21010.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![F21010](Images/F21010.png)'
- en: 'Figure 21-10: The rewards of [Figure 21-9](#figure21-9) as discounted by different
    values of *γ*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-10：在不同的*γ*值下，折扣后的[图 21-9](#figure21-9)奖励。
- en: If we add up the values of each curve in [Figure 21-10](#figure21-10), we get
    the discounted future reward for the first move for different values of *γ*. These
    DFRs are shown in [Figure 21-11](#figure21-11). Notice that as we think of the
    future as being increasingly unpredictable (that is, *γ* gets smaller), the DFR
    also becomes smaller because we’re less confident of getting those future rewards.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将[图 21-10](#figure21-10)中每条曲线的值加起来，我们就得到了在不同*γ*值下，第一步动作的折扣未来奖励。这些DFR在[图
    21-11](#figure21-11)中显示了出来。请注意，随着我们认为未来变得越来越不可预测（也就是*γ*变小），DFR也变得更小，因为我们对获得这些未来奖励的信心降低了。
- en: When *γ* has a value near 1, the future rewards aren’t diminished much, so the
    DFR is close to the TFR. In other words, we’re saying that the total rewards we
    got for making this move are likely to be similar to the total rewards we’ll get
    if we play this move again.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当*γ*接近1时，未来的奖励几乎没有被削减，所以折扣未来奖励（DFR）接近总未来奖励（TFR）。换句话说，我们是在说，做出这个动作所获得的总奖励很可能与我们如果再次做这个动作所获得的总奖励相似。
- en: But when *γ* has a value near 0, then the future rewards are scaled way down
    to the point where they practically don’t matter, and we’re left with just the
    immediate reward. In other words, we’re saying that we have little confidence
    that the game will continue again as it did this time, so the only reward we can
    be sure of is the immediate reward.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当*γ*的值接近0时，未来的奖励被大幅缩小到几乎不重要的地步，最终我们只剩下即时奖励。换句话说，我们表示对游戏是否会继续像这次一样进行信心很小，因此我们能确定的唯一奖励就是即时奖励。
- en: In many reinforcement learning scenarios, we often pick a value of *γ* around
    0.8 or 0.9 to get started, and then adjust the value as we discover more about
    how stochastic our system is and how well our agent is learning.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多强化学习场景中，我们通常会选择一个大约为0.8或0.9的*γ*值来开始，然后随着对系统的随机性和代理学习效果的了解，逐步调整该值。
- en: '![F21011](Images/F21011.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![F21011](Images/F21011.png)'
- en: 'Figure 21-11: The DFR from [Figure 21-10](#figure21-10) for different values
    of *γ*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-11：来自[图21-10](#figure21-10)的DFR，适用于不同的*γ*值
- en: So far, we’ve been discussing principles and ideas, but we still don’t have
    a specific algorithm for the agent to use when it picks an action. To develop
    such an algorithm, let’s start with a description of an environment.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的都是一些原则和思路，但我们仍然没有一个具体的算法来指导代理在选择行动时的决策。为了制定这样的算法，我们从描述环境开始。
- en: Flippers
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Flippers
- en: In the following sections, we’re going to look at actual algorithms for learning
    a game. To keep our focus on the algorithms and not the game, let’s pare down
    tic-tac-toe into a new single-player game that we’ll call *Flippers*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将探讨学习游戏的实际算法。为了让我们将重点放在算法上而非游戏本身，让我们将井字游戏简化为一个新的单人游戏，我们称之为*Flippers*。
- en: We play Flippers on a square grid of size three by three. Each cell holds a
    little tile that pivots around a bar, as in [Figure 21-12](#figure21-12).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一个三乘三的方格网格上玩Flippers游戏。每个格子中都有一个围绕杠杆转动的小瓦片，正如[图21-12](#figure21-12)所示。
- en: One side of each tile is blank, while the other side holds a dot. On each move,
    the player pushes one tile to flip it over. If it was showing a dot, the dot disappears,
    and vice versa.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 每个瓦片的一面是空白的，而另一面则有一个点。在每一步操作中，玩家推动一个瓦片使其翻转。如果该面显示的是点，点会消失，反之亦然。
- en: The game begins with the tiles in a random state. Victory comes from having
    exactly three blue dots showing, arranged in either a vertical column or horizontal
    row, with all the other tiles showing blanks. This may not be the most intellectually
    demanding game ever invented, but it’ll help make our algorithms clear.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏开始时，瓦片处于随机状态。胜利的条件是恰好有三个蓝点显示，并且这些点排列成竖直列或水平行，其他所有瓦片显示为空白。这可能不是有史以来最具智力挑战的游戏，但它将帮助我们澄清算法的逻辑。
- en: '![F21012](Images/F21012.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![F21012](Images/F21012.png)'
- en: 'Figure 21-12: The board for the game of Flippers. Each tile is blank on one
    side and has a dot on the other. A move in the game consists of flipping (or rotating)
    one tile.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-12：Flippers游戏的棋盘。每个瓦片一面是空白的，另一面有一个点。游戏中的一步操作包括翻转（或旋转）一个瓦片。
- en: 'Starting from a random board, we want to get to victory in the smallest number
    of flips. Since diagonal lines don’t count as victory, there are six different
    boards that satisfy our conditions for winning: three with horizontal rows and
    three with vertical columns.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从随机棋盘开始，我们希望在最少的翻转次数内获得胜利。由于对角线不算作胜利条件，因此有六种不同的棋盘配置满足我们胜利的条件：三种水平行和三种竖直列。
- en: '[Figure 21-13](#figure21-13) shows an example game, along with our notation
    for showing the moves. We read the game left to right. Each board but the last
    shows the starting configuration for that move, with one cell highlighted in red.
    That’s the cell that is going to be flipped over.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[图21-13](#figure21-13)展示了一个示例游戏，并附上了表示操作的符号。我们从左到右读取游戏。每个棋盘（除了最后一个）都展示了该步的起始配置，并且有一个格子被用红色高亮标出。这个格子就是将要被翻转的格子。'
- en: '![F21013](Images/F21013.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![F21013](Images/F21013.png)'
- en: 'Figure 21-13: Playing a game of Flippers. (a) The initial board, showing three
    dots. The red square shows the tile we intend to flip for this move. (b) The resulting
    board is like part (a), but the tile in the upper-right has gone from blank to
    dot. Our move for this board is to flip the center tile. (c) through (e) show
    subsequent steps in game play. Board (e) is a winning board.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-13：玩Flippers游戏。（a）初始棋盘，显示三个点。红色方块表示我们这一步要翻转的瓦片。（b）结果棋盘与（a）相似，但右上角的瓦片已经从空白变为点。我们这一步的操作是翻转中央的瓦片。（c）到（e）展示了游戏进行中的后续步骤。棋盘（e）是一个获胜的棋盘。
- en: Now that we have a game to play, we can look at how to use reinforcement learning
    to win it.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个可以玩的游戏，我们可以看看如何利用强化学习来赢得它。
- en: L-Learning
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L-学习
- en: Let’s build a complete system for learning how to play Flippers. Although we
    will make this algorithm much better in the next section, this starting version
    is going to perform so badly that we call it *L-learning*, where L stands for
    “lousy.” Note that L-learning is a stepping-stone that we invented to help us
    get to something better and not a practical algorithm that appears in the literature.
    It is, after all, lousy.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个完整的Flippers学习系统。虽然我们将在下一节中大大改进这个算法，但这个初步版本的表现会差到我们称它为*L-学习*，其中L代表“糟糕的”。请注意，L-学习是我们发明的一个垫脚石，目的是帮助我们达到更好的结果，而不是文献中出现的实用算法。毕竟，它很糟糕。
- en: The Basics
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础知识
- en: To make things easy, we’re going to use a very simple reward system. Every move
    we make in Flippers gets an immediate reward of 0, except for the final move that
    wins the game. Because Flippers is such an easy game, every game can be won. To
    prove this, we can take any starting board and flip over all the tiles that are
    showing a dot, so that there are no dots showing. Then we can flip over three
    tiles in any row or column, and we’ve won. Thus, no game should take more than
    12 moves at most.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，我们将使用一个非常简单的奖励系统。在Flippers中，我们每走一步都会得到一个即时奖励0，除了最后一步赢得游戏的那一步。因为Flippers是一个非常简单的游戏，每一局都能获胜。为了证明这一点，我们可以从任何一个起始棋盘开始，将所有显示点的瓷砖翻转，这样就没有点显示出来。然后，我们可以翻转任意一行或一列中的三块瓷砖，就算获胜了。因此，任何游戏最多不应超过12步。
- en: Our goal is not simply to win, however, but to win in the smallest number of
    moves. The final, winning move gets a reward that depends on the length of the
    game. If it takes one move to win the game, the reward is 1\. If it takes more
    moves, this final reward drops off quickly with the number of moves that were
    taken. The specific formula for this curve is less important than the fact that
    it drops off fast and is always getting smaller. [Figure 21-14](#figure21-14)
    shows a graph of our final reward versus game length curve.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标不仅仅是获胜，而是在最少的步数内获胜。最终的胜利动作会获得一个奖励，这个奖励取决于游戏的时长。如果只需要一步就能获胜，奖励是1。如果需要更多的步骤，这个最终奖励会随着所用步数的增加而迅速下降。这个曲线的具体公式并不重要，重要的是它下降得很快，并且一直在变小。[图
    21-14](#figure21-14)展示了最终奖励与游戏时长的曲线图。
- en: '![F21014](Images/F21014.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![F21014](Images/F21014.png)'
- en: 'Figure 21-14: The reward for victory in Flippers starts at 1 for an immediate
    win but drops quickly with the number of moves required to win the game.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-14：Flippers中胜利的奖励从一步获胜的1开始，但随着获胜所需步数的增加，奖励迅速下降。
- en: At the heart of our system is a grid of numbers that we call the *L-table*.
    Each row of the L-table represents one state of the board. Each column represents
    one of the nine actions we can make in response to that board. The content of
    each cell in the table is a single number, which we call an *L-value*. [Figure
    21-15](#figure21-15) shows this schematically.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们系统的核心是一个数字网格，我们称之为*L表*。L表的每一行代表棋盘的一个状态。每一列代表我们在该棋盘状态下可以采取的九个动作之一。表中每个单元格的内容是一个数字，我们称之为*L值*。[图
    21-15](#figure21-15)展示了这一点的示意图。
- en: '![F21015](Images/F21015.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![F21015](Images/F21015.png)'
- en: 'Figure 21-15: The L-table contains one row for each of the 512 possible patterns
    of blanks and dots on a Flipper board and one column for each of the 9 possible
    actions.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-15：L表包含512种可能的Flipper棋盘上空白和点的模式，每一行代表一种模式，每一列代表9种可能的动作之一。
- en: 'This table is big, but not too big. The board has only 512 possible configurations,
    so we need 512 rows. Each row has 9 columns, for a total of 512 × 9 = 4,608 cells.
    We’re going to use the L-table to help us choose the highest-rewarding action
    in response to each board. To make that happen, we’re going to fill each cell
    in the table with a score: a number, based on experience, that tells us how good
    the corresponding move is.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表很大，但并不算太大。棋盘只有512种可能的配置，因此我们需要512行。每行有9列，总共有512 × 9 = 4,608个单元格。我们将使用L表来帮助我们选择在每种棋盘状态下最有奖励的行动。为了实现这一点，我们将在表中的每个单元格里填写一个分数：一个基于经验的数字，告诉我们对应的动作有多好。
- en: We save values into the L-table as we learn how good moves are, and we read
    those values back to guide our choice of moves as we play. Before we start assigning
    values to the L-table, we initialize it with a 0 in every cell. As we play a game,
    we will keep a record of all the moves we’ve played. When the game is over, we
    will look back through our moves for the whole game, and determine a value for
    each. Then we will combine this value with the number already in that move’s cell
    to produce a new value for that move (we’ll get to the mechanics for this in a
    moment). The way we combine the old and new values is called the *update rule*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将值保存在L表中，以便在学习如何评估动作时使用，并在游戏进行时通过读取这些值来指导我们的动作选择。在开始为L表分配值之前，我们首先将每个单元格初始化为0。随着游戏的进行，我们会记录所有我们所采取的动作。当游戏结束时，我们将回顾整个游戏中我们所做的所有动作，并为每个动作确定一个值。然后，我们将这个值与该动作所在单元格中已有的值结合，产生该动作的新值（我们稍后会详细介绍这一过程）。我们将旧值和新值结合的方式称为*更新规则*。
- en: As we play a game (either during the learning phase, or later for real), we
    pick an action by looking at the corresponding row for the board at the start
    of that move. We use a policyto tell us which of the actions in that row we want
    to select.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏进行过程中（无论是在学习阶段还是后来的真实游戏中），我们通过查看该步开始时棋盘的对应行来选择一个动作。我们使用策略来告诉我们在该行中选择哪个动作。
- en: Let’s make these steps concrete. First, after each game (or episode), we need
    to determine the score we want to assign to each action we played. Let’s use the
    total future reward, or TFR, that we discussed earlier. Recall that the TFR comes
    from lining up all our actions and their rewards, and then summing up all the
    rewards that came after that action.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把这些步骤具体化。首先，在每局游戏（或情节）结束后，我们需要确定分配给每个我们所采取的动作的分数。我们使用之前讨论过的总未来奖励，或者TFR。回想一下，TFR来源于将所有动作及其奖励排列起来，然后将该动作之后的所有奖励加总起来。
- en: 'While we play the game, every move along the way gets an immediate reward of
    0, but the final move gets a positive reward based on the game’s length: the shorter
    the game, the larger the reward. This means that the TFR for each action we took
    along the way is the same as this final reward.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏进行过程中，每一步都获得一个即时的奖励0，但最终一步会根据游戏的长度获得正向奖励：游戏越短，奖励越大。这意味着我们沿途所采取的每个动作的TFR与最终的奖励相同。
- en: Second, let’s pick a simple update rule that says that after each game, the
    TFR we compute for each cell merely replaces whatever was in there before. In
    other words, the TFR for each action in this game becomes the new value in the
    cell at the intersection of the row of the board we were looking at when we took
    that action, and the column of the action we chose to take.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们选择一个简单的更新规则，即在每局游戏结束后，我们为每个单元格计算的TFR会直接替换该单元格之前的内容。换句话说，这局游戏中每个动作的TFR将成为该单元格的新值，该单元格位于我们采取该动作时所看的棋盘行和我们选择的动作所在的列的交点。
- en: This simple update rule is good for getting familiar with how the L-learning
    system works. But because it doesn’t combine our new experience with what we’ve
    learned before, this rule is a big reason that this algorithm isn’t going to perform
    well.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的更新规则有助于让我们熟悉L学习系统的工作方式。但由于它没有将我们的新经验与之前学到的内容结合起来，因此这个规则是该算法表现不佳的一个重要原因。
- en: Now that we have values in our L-table, we need a policy that tells us which
    move to play in response to a given configuration of the board. Let’s say that
    we choose the action corresponding to the largest L-value in the row. If multiple
    cells have the same maximum value, we pick one at random. [Figure 21-16](#figure21-16)
    shows this graphically.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在L表中有了值，我们需要一个策略来告诉我们在面对某个棋盘配置时应选择哪个动作。假设我们选择对应于行中最大L值的动作。如果多个单元格有相同的最大值，我们将随机选择一个。[图21-16](#figure21-16)以图形化方式展示了这一过程。
- en: '![F21016](Images/F21016.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![F21016](Images/F21016.png)'
- en: 'Figure 21-16: The policy step involves choosing one action in response to a
    board.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-16：策略步骤包括选择一个动作以应对棋盘上的局面。
- en: In [Figure 21-16](#figure21-16), we see a row of the L-table that lists the
    possible moves we can take in response to the board state shown at the far left.
    Each column holds the most recently computed TFR that resulted when that action
    was taken at that board state. Note that two columns hold 0, because we haven’t
    tried those actions yet. In L-learning, we choose the largest value. Here that
    means we flip the center-right tile.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图21-16](#figure21-16)中，我们看到L表格的一行，列出了我们可以对最左边的棋盘状态做出的可能动作。每列都保存了在该棋盘状态下采取该动作时最近计算出的TFR（即时奖励）。请注意，有两列的值为0，因为我们还没有尝试这些动作。在L学习中，我们选择最大的值。在这里，这意味着我们翻转中心右侧的棋盘格。
- en: The L-Learning Algorithm
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: L学习算法
- en: We now have all the steps required for L-learning. Let’s combine them into a
    functional, but lousy, reinforcement learning algorithm. We start our agent with
    a private memory that contains a 512 by 9 table filled with zeros, representing
    the L-table.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了进行L学习所需的所有步骤。让我们将它们组合成一个功能性但不太完善的强化学习算法。我们从一个包含512×9表格并填充零的私有记忆开始，这个表格代表L表格。
- en: In the first move of the first game, the agent sees a board. It finds the row
    for that board in its L-table and scans the nine entries there for the move with
    the largest score. They’re all zero, so it picks one at random. This will happen
    frequently for quite a while because the agent will see lots of boards it has
    never seen before. The tile flips, the agent considers the new board, picks a
    new action, and so on, until it finally wins the game (the computer will produce
    a winning board eventually, even if the actions are selected entirely at random).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一局游戏的第一步中，代理看到一个棋盘。它在L表格中找到该棋盘对应的行，并扫描该行中的九个条目，选择得分最高的动作。因为这些值都是零，所以它会随机选择一个。这种情况会持续一段时间，因为代理会看到很多它以前从未见过的棋盘。棋盘翻转后，代理会考虑新的棋盘，选择新的动作，依此类推，直到它最终赢得游戏（即使所有动作完全随机选择，计算机最终也会产生一个获胜的棋盘）。
- en: When the game ends, the agent wants to distribute the final reward among all
    the moves that got it to victory. To do so, while the game is played, the system
    will need to maintain a list of each move it plays, in order.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当游戏结束时，代理希望将最终奖励分配给所有帮助它赢得胜利的动作。为此，在游戏进行过程中，系统需要保持一个按顺序排列的列表，记录每一步它所执行的动作。
- en: We’ll later find that list more useful if each entry saves more than just the
    selected move. Anticipating that need, let’s say that after each move, the agent
    retains a small bundle consisting of the starting board, the action the agent
    took, the immediate reward it received, and the board that resulted from that
    move. [Figure 21-17](#figure21-17) shows this visually. The agent saves these
    bundles in a list that starts empty at the start of the game and grows by one
    bundle after every move.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会发现，如果每个条目不仅保存选择的动作，还保存更多信息，这个列表会更有用。预见到这一需求，我们假设在每一步之后，代理会保留一个小的组合，包含起始棋盘、代理所采取的动作、收到的即时奖励以及该动作所导致的结果棋盘。[图21-17](#figure21-17)展示了这一过程。代理将这些组合保存在一个列表中，游戏开始时该列表为空，每一步后列表都会增加一个组合。
- en: '![F21017](Images/F21017.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![F21017](Images/F21017.png)'
- en: 'Figure 21-17: Each time we make a move, we append a bundle of four values to
    the end of a growing list of bundles: the starting state, our chosen action, the
    reward we received, and the final state the environment returned to us after taking
    that action.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-17：每次我们采取一个动作时，我们都会将一个包含四个值的组合追加到不断增长的组合列表的末尾：起始状态、我们选择的动作、我们收到的奖励以及环境在采取该动作后返回给我们的最终状态。
- en: 'As [Figure 21-17](#figure21-17) shows, we can save this bundle as a list of
    four numbers: the row number of the starting state, the column number of the action,
    the value of the reward, and the row number of the resulting state.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图21-17](#figure21-17)所示，我们可以将这个组合保存为一个包含四个数字的列表：起始状态的行号、动作的列号、奖励的值和结果状态的行号。
- en: To make our first move, we look at the row of the L-table corresponding to our
    starting board, and the nine numbers we find along that row. Our policy will be
    to usually pick the largest value in the row, but sometimes pick one of the others
    for the sake of exploration. If all values are the same, as they are when we start
    out, we pick one at random.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行第一次移动，我们查看L表格中对应起始棋盘的行，并查看该行中的九个数字。我们的策略通常是选择该行中最大的值，但有时也会为了探索而选择其他值。如果所有值都相同（就像我们刚开始时的情况），我们会随机选择一个。
- en: 'The environment flips that tile for us, either making a dot appear or disappear.
    The environment then gives us back a reward and the new board. We make a little
    bundle to represent this move: the board we started with, the action we just took,
    the reward we got back, and the new state that resulted. We stick that bundle
    onto the end of our list of moves.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 环境为我们翻转那个方块，可能让一个点出现或消失。然后，环境给我们回馈一个奖励以及新的棋盘。我们会把这个动作打包成一个小的集合：我们开始时的棋盘、我们刚刚采取的动作、我们获得的奖励以及因此产生的新状态。我们把这个集合加入到我们动作列表的末尾。
- en: Because we’re playing solo, the environment isn’t going to make any moves on
    its own. As soon as it has sent us our feedback, the environment tells us to take
    a new action. So again, we look at the current board, find its row in the L-table,
    select the largest cell in that row, and report that as our action. We get back
    a reward and a new state, and we add a new bundle of the four items describing
    this move to our list.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们是单人游戏，环境不会主动采取任何动作。一旦它给我们反馈了，它就会告诉我们采取新动作。因此，我们再次查看当前棋盘，在L-table中找到它对应的行，选择该行中最大的单元格，并将其报告为我们的动作。我们会得到一个奖励和一个新状态，并将描述这个动作的四个项目作为一个新集合添加到我们的列表中。
- en: This goes on until the game is over. In the final piece of feedback, we get
    our only nonzero reward. It’s the final reward based on the number of moves we
    played in the game, which drops off quickly, as we saw in [Figure 21-14](#figure21-14).
    With that final, nonzero reward, we know the game is over, so it’s time to learn
    from our experience.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程会一直持续到游戏结束。在最后的反馈中，我们会得到唯一的非零奖励。它是基于我们在游戏中所进行的移动次数的最终奖励，正如我们在[图21-14](#figure21-14)中看到的那样，这个奖励会迅速递减。通过这个最终的非零奖励，我们知道游戏结束了，接下来是时候从我们的经验中学习了。
- en: We start by looking at our bundles from our list of moves. Conceptually, we
    line up our board states and resulting moves, along with their rewards, as in
    [Figure 21-18](#figure21-18). One by one, we look at each move and find its TFR
    by adding up all the rewards that came after that move. In [Figure 21-18](#figure21-18)
    the calculations aren’t very interesting, since all the immediate rewards except
    the last are zero. But it’s worth seeing the steps here, as we’ll have nonzero
    immediate rewards later on.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从查看我们的动作列表中的集合开始。从概念上讲，我们排列出我们的棋盘状态和相应的动作，以及它们的奖励，如[图21-18](#figure21-18)所示。我们逐一查看每个动作，并通过将所有后续奖励相加来找到它的TFR。在[图21-18](#figure21-18)中，计算并不特别有趣，因为除了最后一个奖励，所有即时奖励都是零。但值得注意的是，随着后续步骤的发展，我们将会遇到非零的即时奖励。
- en: '![F21018](Images/F21018.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![F21018](Images/F21018.png)'
- en: 'Figure 21-18: Finding the TFR for each move. We add up the immediate reward
    for each move (shown directly underneath it) with the immediate rewards for all
    following moves. In our game where every immediate reward is zero except for the
    final reward, these sums are all the same.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-18：为每个动作找到TFR。我们将每个动作的即时奖励（直接显示在其下方）与所有后续动作的即时奖励相加。在我们的游戏中，除了最后的奖励，所有即时奖励都是零，因此这些和都是相同的。
- en: We then use our simple update rule and the list of moves we made, and plunk
    each action’s TFR into the cell of the L-table corresponding to that action for
    that board, as shown in [Figure 21-19](#figure21-19).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用简单的更新规则以及我们所做的动作列表，并将每个动作的TFR值放入L-table中对应该棋盘的单元格，如[图21-19](#figure21-19)所示。
- en: '![F21019](Images/F21019.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![F21019](Images/F21019.png)'
- en: 'Figure 21-19: Updating our L-table with the new TFR for each action we took
    in the game. We find the row corresponding to the board we were looking at when
    we took each action and the column corresponding to the action we made. The new
    TFR replaces whatever was in that cell before.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-19：使用每个动作的最新TFR值来更新我们的L-table。我们找到对应我们在执行每个动作时所看到棋盘的行，以及对应我们所采取动作的列。新的TFR值会替换掉之前单元格中的任何值。
- en: If we want to learn some more, we go back up to the start of the process and
    play a new game. When we’re done, we compute a TFR value for each action we played
    and store that in its corresponding cell (overwriting whatever was there before).
    Note that we don’t reset the L-table after each game, though, so it gradually
    fills up with TFRs as we play more episodes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要学习更多，我们会回到流程的起始点并开始一个新游戏。当我们完成后，我们为每个我们执行的动作计算一个TFR值，并将其存储在相应的单元格中（覆盖之前的值）。请注意，我们在每次游戏后不会重置L-table，因此，随着我们玩更多的回合，TFR会逐渐填满整个表格。
- en: When it’s time to stop training and start playing, we use the L-table to pick
    our moves. That is, at each move, we’re presented with a board, so we find that
    row of the table, pick the largest L-value in that row, and select the action
    corresponding to that column.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练结束，开始实际游戏时，我们使用L表来选择动作。也就是说，在每次行动时，我们会呈现一个棋盘，我们查找该行，选择该行中L值最大的列，并选择对应的动作。
- en: Testing Our Algorithm
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试我们的算法
- en: 'Let’s see how well our system works. Let’s start out by playing 3,000 episodes
    of Flippers from start to finish so the L-table can get filled up pretty well.
    [Figure 21-20](#figure21-20) shows a game of Flippers played from start to finish
    after these 3,000 episodes of training. It’s not a very nice result. There’s a
    simple two-move solution that any human would spot: flip the left-middle cell,
    and then the upper-left cell (or do it in the other order). Instead, our algorithm
    seems to meander randomly until it finally stumbles on a solution after six moves.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的系统表现如何。我们先进行3,000局Flippers游戏，从头到尾进行，这样L表就可以得到充分填充。[图 21-20](#figure21-20)显示了在3,000局训练后从头到尾玩的Flippers游戏。这不是一个很理想的结果。其实有一个简单的两步解法，任何人都能轻松发现：翻转左中间的单元格，然后是左上角的单元格（或者反过来）。然而，我们的算法似乎在随机地游走，直到最终在六步之后偶然找到了解决方案。
- en: '![F21020](Images/F21020.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![F21020](Images/F21020.png)'
- en: 'Figure 21-20: Playing a game of Flippers after 3,000 episodes of training with
    the L-table algorithm. Read the game left to right.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-20：使用L表算法经过3,000局训练后玩Flippers游戏。游戏从左到右阅读。
- en: The arrangement shown in [Figure 21-20](#figure21-20) shows rows of the L-table
    arranged as columns to better fit the page. Each column represents one board configuration
    (or state). The nine possible actions are shown in each row, highlighted in red.
    The thick black outline shows the action that the agent selected from that list,
    leading to the new board in the column to its right. The shaded cell shows the
    action taken. If the move causes a dot to appear, the move is shown as a solid
    red dot. If the move causes the dot to go away, it’s shown as an outlined red
    dot. The colored bar below each board shows its L-value from the table. Larger
    and greener bars correspond to larger L-values calculated with discounted future
    rewards.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 21-20](#figure21-20)所示的排列将L表的行排列为列，以便更好地适应页面。每一列代表一个棋盘配置（或状态）。每行显示了九个可能的动作，红色高亮显示。粗黑边框表示代理从该列表中选择的动作，导致其右侧列的新棋盘。阴影单元格表示已执行的动作。如果该动作导致出现一个点，则该动作显示为实心红点。如果该动作导致点消失，则显示为红色轮廓点。每个棋盘下方的彩条显示了该棋盘的L值，值越大，条形越绿，表示通过折扣未来奖励计算出的L值越大。'
- en: Boards near the right have larger L-values than those near the left. That’s
    because those boards were sometimes the randomly chosen starting board for a game.
    If we picked a good move and won immediately, or in just a few moves, the final
    reward was large.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的棋盘比左侧的棋盘具有更大的L值。这是因为这些棋盘有时是游戏随机选择的起始棋盘。如果我们选择了一个好的动作并立即获胜，或者在少数几步内获胜，最终的奖励会很大。
- en: Returning to this game, starting from the position on the far left, the algorithm’s
    first move was to flip the cell in the lower left, introducing a new dot. From
    that result, it then flipped the square in the middle of the leftmost column,
    again introducing a dot. From that position it then flipped the upper-left square,
    removing the dot that was there. The game continued in this way until it found
    a solution.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 回到这个游戏，从最左边的位置开始，算法的第一步是翻转左下角的单元格，引入一个新点。从这个结果开始，算法接着翻转最左列中间的方块，再次引入一个点。从这个位置，它又翻转了左上角的方块，移除了原本存在的点。游戏以这种方式继续，直到找到解决方案。
- en: We’d expect our algorithm to improve with more training, and it does. [Figure
    21-21](#figure21-21) shows the same game as [Figure 21-20](#figure21-20) after
    doubling the length of the training run to 6,000 episodes.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预计算法会随着更多的训练而改进，结果确实如此。[图 21-21](#figure21-21)显示了将训练时间延长到6,000局后，与[图 21-20](#figure21-20)相同的游戏。
- en: '![F21021](Images/F21021.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![F21021](Images/F21021.png)'
- en: 'Figure 21-21: The same game as [Figure 21-20](#figure21-20) after 6,000 episodes
    of training'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-21：在6,000局训练后与[图 21-20](#figure21-20)相同的游戏
- en: This is very nice. The algorithm found the easy answer and went right for it.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常好。算法找到了简单的答案，并直接采取了这个行动。
- en: We seem to have created a great algorithm for learning and playing. So why did
    we label everything with “L” for lousy? It seems to be working just fine.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们创建了一个很棒的学习和游戏算法。那么为什么我们要给它贴上“L”（差劲）的标签呢？它似乎运作得相当不错。
- en: It *is* just fine, as long as the environment remains completely predictable.
    Remember that earlier in this chapter we discussed unpredictable environments.
    In reality, most environments are unpredictable. Logic-based single-player games,
    such as the Flippers game we’ve been looking at, are one of the few activities
    that are completely deterministic. If our goal is to play only single-player games
    in completely deterministic environments where we are able to execute every intended
    move perfectly and the environment responds identically every time, then this
    algorithm isn’t so lousy. But such deterministic games and environments are rare.
    For example, as soon as there’s a second player, there’s uncertainty, and the
    game becomes unpredictable. In any situation in which the environment is not perfectly
    deterministic, the L-learning algorithm flounders.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 只要环境保持完全可预测，其实是*没问题*的。记得在本章之前，我们讨论过不可预测的环境。实际上，大多数环境都是不可预测的。基于逻辑的单人游戏，例如我们一直在研究的Flippers游戏，是少数完全确定性的活动之一。如果我们的目标是只玩单人游戏，并且这些游戏在完全确定的环境中进行，我们能够完美地执行每一个预定的动作，并且环境每次都作出相同的反应，那么这个算法其实并不糟糕。但是这种确定性的游戏和环境是很少的。例如，一旦有第二个玩家，便会产生不确定性，游戏变得不可预测。在任何环境不完全确定的情况下，L学习算法都会陷入困境。
- en: Let’s see why, and then we will see how to fix it.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看原因，然后我们将看到如何解决这个问题。
- en: Handling Unpredictability
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理不可预测性
- en: Because we don’t have an opponent when playing Flippers on the computer, we
    have a completely deterministic system. Every time we make a move, we are guaranteed
    to get back the same result. But in the real world, even single-player activities
    can have unpredictable events. Video games throw random surprises at us, a lawnmower
    can hit a rock and jump to the side, or an internet connection can stutter and
    cause us to miss making the winning bid in an auction.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在电脑上玩Flippers时我们没有对手，所以我们拥有一个完全确定性的系统。每次我们做出一个动作，我们都能保证得到相同的结果。但在现实世界中，即便是单人活动也可能发生不可预测的事件。视频游戏会给我们带来随机的惊喜，割草机可能撞到一块石头并跳到一边，或者互联网连接可能会卡顿，导致我们错过在拍卖中做出获胜的出价。
- en: Since handling unpredictability is so important, let’s introduce some artificial
    randomness into Flippers and see how our L-learning algorithm responds. Our model
    of randomness takes the form of a big truck that drives by our playing area every
    now and then, shaking our board. Sometimes it’s enough to cause one or more random
    tiles to spontaneously flip over. Of course, we still want to play good games
    and win, even in the face of such surprises, but our L-learning system is helpless
    in the face of this kind of event.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 由于处理不可预测性非常重要，让我们在Flippers中引入一些人工随机性，看看我们的L学习算法如何响应。我们对随机性的模型表现为一辆大卡车，它不时经过我们的游戏区域，震动我们的棋盘。有时，这足以导致一个或多个随机的棋盘格自发翻转。当然，我们仍然希望能够在面对这样的惊讶时玩得愉快并赢得比赛，但我们的L学习系统在这种事件面前束手无策。
- en: It’s the combination of our policy and update rule that causes trouble. Remember
    that before we start learning, each row starts out with all zeros. When a training
    game is finally won, every action gets the same score, based on the length of
    the game, as we saw in [Figure 21-19](#figure21-19). As we continue to play our
    training games, the next time we come to that board, we pick the cell with the
    largest value.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 正是我们的策略和更新规则的结合引发了问题。记得在我们开始学习之前，每一行都是从零开始的。当一场训练游戏最终获胜时，每个动作都根据游戏的时长获得相同的分数，正如我们在[图21-19](#figure21-19)中看到的那样。当我们继续进行训练游戏时，下次当我们遇到那个棋盘时，我们会选择具有最大值的单元格。
- en: Suppose we’re in the midst of a training game. We’re looking at a board that
    we once received as a starting board and we won it in two moves. The L-table values
    for each of those moves have large scores, so we select the high-scoring move,
    preparing to win on the next flip. But just after our first move, the big truck
    comes rumbling by, shaking our board and flipping a tile. Playing from this board
    forward, we end up requiring lots of moves before winning. This means that the
    TFR that ultimately comes from playing that action is less than if the truck hadn’t
    come by.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在进行一场训练游戏。我们看到一个曾经作为起始棋盘获得的棋盘，并且我们在两步之内就赢得了它。那两步的L表格值得到了很高的分数，所以我们选择得分较高的动作，准备在下一次翻转时获胜。但是就在我们第一次行动后，那辆大卡车咆哮着驶过，震动了我们的棋盘并翻转了一个棋盘格。从这个棋盘开始向前玩，我们最终需要更多的步骤才能获胜。这意味着，如果卡车没有经过，最终从这个动作中得到的TFR会更高。
- en: 'And here’s the problem: that smaller value overwrites the previous value in
    every cell that led to this long game. In other words, because of that event,
    every action we played sees its L-value lowered. In particular, that great starting
    move that led to victory in just one more move now has a low score. When we encounter
    this board again in a later game, we might find that one of the other cells has
    a larger value than the cell that formerly held the great move. The result is
    that this one-time random event causes us to stop making the best move we’ve found
    up to that point. We have “forgotten” that this was a great move, because a random
    event turned it into a bad move once. That low score makes it unlikely that we’ll
    ever choose that move again.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这是问题所在：那个较小的值覆盖了每个导致这场长时间游戏的格子的先前值。换句话说，正因为那次事件，我们所采取的每个行动的L值都被降低了。特别是，那一开始就导致胜利的绝佳起手，现在得分很低。当我们在以后的游戏中再次遇到这个棋盘时，可能会发现另一个格子的值比原来那个进行绝佳走法的格子还要大。结果是，这个偶然事件导致我们不再执行到目前为止找到的最佳行动。我们“忘记”了这一步曾经是绝佳走法，因为一次随机事件让它变成了一个糟糕的选择。这个低分使得我们不太可能再选择这一步。
- en: Let’s see this problem in action. [Figure 21-22](#figure21-22) shows an example
    where there are no unpredictable events. We start at the top with a board with
    three dots, and we find that the largest value in that row of the table is 0.6,
    corresponding to a flip of the center square. We make that move, and supposing
    the next move is also well-chosen, we have a victory in two moves, as shown in
    the center row. The reward of 0.7 replaces the 0.6 that was there for our first
    move, cementing that move’s status as the one to make. Everything went right.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个问题是如何发生的。[图21-22](#figure21-22)显示了一个没有不可预测事件的例子。我们从顶部开始，棋盘上有三个点，我们发现该行中最大的值是0.6，对应于翻转中心格子。我们执行这个操作，假设下一步也选得不错，我们将在两步内获胜，如中间行所示。0.7的奖励替代了我们第一步时的0.6，巩固了这一走法作为最好的选择。一切都很顺利。
- en: '![F21022](Images/F21022.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![F21022](Images/F21022.png)'
- en: 'Figure 21-22: When there are no surprises, our algorithm works well. (a) The
    row of the L-table for the starting board. (b) The game plays out and is won in
    just a total of two moves. (c) The value of 0.7 overwrites the previous value
    for all table entries that led to this success.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-22：当没有意外事件时，我们的算法表现良好。（a）起始棋盘的L表格行。（b）游戏顺利进行，并在两步内获胜。（c）0.7的值覆盖了所有导致这一成功的表格条目的先前值。
- en: In [Figure 21-23](#figure21-23) we introduce our rumbling truck. Just after
    we flip the center tile, the truck shakes the board and the bottom-right tile
    flips. This puts us on a whole new path. Let’s suppose the algorithm finally finds
    victory after four more moves. The total is five moves, and the reward of 0.44
    is placed in every cell that led to this success.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图21-23](#figure21-23)中，我们介绍了我们的隆隆卡车。就在我们翻转中心瓷砖后，卡车震动了棋盘，右下角的瓷砖被翻转了。这将我们带上了一条全新的道路。假设算法在再进行四步之后最终获得了胜利。总共五步，奖励0.44被放置在每个导致成功的格子中。
- en: This is terrible. In one quick stroke, we have “forgotten” our best move. In
    this example, two other actions now have better scores. The next time we come
    to this board, the cell with score 0.55 will be picked, which will not place us
    one move away from victory as before. In other words, our best move is now forgotten,
    and we’re going to always play a worse move.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这真糟糕。只需一笔，我们就“忘记”了我们的最佳走法。在这个例子中，另外两个行动现在得分更高。下次我们遇到这个棋盘时，会选择得分为0.55的格子，这样就不能像之前那样让我们一步之内获胜。换句话说，我们的最佳走法现在被遗忘了，我们将永远做出更差的选择。
- en: '![F21023](Images/F21023.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![F21023](Images/F21023.png)'
- en: 'Figure 21-23: (a) When a truck rumbles by, it flips the lower-right square,
    causing the game to take five moves to win. (b) The new reward of 0.44 overwrites
    the old value of 0.6\. This cell is no longer the highest-scoring cell in the
    row.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-23：（a）当卡车经过时，它翻转了右下角的格子，导致游戏需要五步才能获胜。（b）新的奖励0.44覆盖了之前的0.6值。这个格子不再是该行中得分最高的格子。
- en: Recall that we said that occasionally during training we’ll pick one of the
    cells in the row at random, just to explore what might happen. So someday we might
    make a new choice, or the truck might rumble by again and help us remember this
    cell, but that might not happen for a long time. And by the time the truck does
    come by and sets this move right again, others will have gone wrong. The L-table
    is almost always inferior to what it ought to be, and thus, on average, games
    powered by L-learning are longer and we get lower rewards. One surprise caused
    us to forget how to play this board well.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们曾说过，在训练过程中，我们会偶尔随机选择行中的一个单元格，以探索可能发生的情况。因此，某天我们可能会做出新的选择，或者卡车再次轰隆而过，帮助我们记住这个单元格，但可能很久才会发生一次。直到卡车经过并再次设置正确这个动作时，其他地方可能已经出错。L-table
    几乎总是比应有的状态差，因此，平均而言，使用 L-学习的游戏会更长，奖励也较低。一个意外使我们忘记了如何好好玩这个棋盘。
- en: That’s why we called this algorithm lousy.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们称这个算法为糟糕的原因。
- en: But all is not lost. We looked at this algorithm because the lousy version can
    be improved. Most of the algorithm is fine. We only need to fix how it fails in
    the face of unpredictability. From now on, we assume that when we play Flippers,
    that big truck may come thundering along, creating unpredictability in the form
    of occasionally flipping a random tile. In the next section, we’ll see how to
    handle this kind of unpredictable event gracefully and produce an improved learning
    algorithm that works well.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 但并非一切都丧失了。我们之所以查看这个算法，是因为其较差的版本可以被改进。大部分算法本身没有问题。我们只需要修复它在面对不可预测性时的缺陷。从现在开始，我们假设在玩
    Flippers 时，可能会有一辆大卡车突然驶过，造成不确定性，偶尔会翻动一个随机的方块。在接下来的章节中，我们将看到如何优雅地处理这种不可预测事件，并改进学习算法，使其更有效。
- en: Q-Learning
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q-学习
- en: Without too much effort, we can upgrade L-learning to a much more effective
    algorithm that is commonly used today, called *Q-learning*(the Q is for quality)
    (Watkins 1989; Eden, Knittel, and van Uffelen 2020). Q-learning looks a lot like
    L-learning, but, naturally, it instead fills up Q-tables with Q-values. The big
    improvement is that Q-learning performs well in stochastic, or unpredictable,
    environments.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 不费太大力气，我们可以将 L-学习升级为一个如今广泛使用的更有效的算法，称为*Q-学习*（Q代表质量）（Watkins 1989；Eden、Knittel
    和 van Uffelen 2020）。Q-学习看起来和 L-学习很像，但自然地，它用 Q-值填充 Q-table。最大的改进是，Q-学习在随机或不可预测的环境中表现良好。
- en: 'To get from L-learning to Q-learning we make three upgrades: we improve how
    we compute new values for Q-table cells, how we update existing values, and the
    policy we use for choosing an action.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从 L-学习转向 Q-学习，我们进行三个升级：我们改进了 Q-table 单元格中新值的计算方式，如何更新现有值，以及我们选择动作的策略。
- en: The Q-table algorithm starts with two important principles. First, we *expect*
    uncertainty in our results, so we build it in from the start. Second, we work
    out new Q-table values as we go, rather than waiting for the final reward. This
    second idea lets us work with games (or processes) that go on for a very long
    time, or perhaps never reach a conclusion (like scheduling elevators). By updating
    as we go, we’re able to develop our table of useful values even if we never get
    a final reward.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Q-table 算法基于两个重要原则。首先，我们*期望*结果中存在不确定性，因此从一开始就将其考虑在内。第二，我们在过程中逐步计算新的 Q-table
    值，而不是等到最终的奖励。这第二个想法使我们能够处理那些持续很长时间的游戏（或过程），甚至可能永远无法得出结论的情况（如电梯调度）。通过逐步更新，我们能够在即使没有最终奖励的情况下，开发出有用的
    Q-table 值。
- en: To make this work, we need to also upgrade the environment’s super simple rewarding
    process from the last section. Rather than always rewarding zero except for the
    final move, the environment will instead return immediate rewards that estimate
    the quality of each action as soon as it’s taken.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让这个工作有效，我们还需要升级上一节中环境的超简单奖励过程。环境不再总是除最后一步外奖励零，而是会根据每次采取的行动立即返回估计每个动作质量的奖励。
- en: Q-Values and Updates
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q-值与更新
- en: Q-values are a way to approximate the total future reward even when we don’t
    know how things are going to end up. To find a Q-value, we add together the immediate
    reward, plus all the other rewards that are yet to come. So far, that’s nothing
    more than the definition of the total future reward. The change is that now we
    find the future rewards by using the reward from the next state.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Q-值是一种近似未来总奖励的方法，即使我们不知道事情最终会如何发展。要找出 Q-值，我们将即时奖励与所有未来可能的奖励加在一起。到目前为止，这不过是总未来奖励的定义。变化在于，现在我们通过使用下一个状态的奖励来寻找未来的奖励。
- en: 'In [Figure 21-17](#figure21-17), we saved four pieces of information for every
    move: the starting state, the action we chose, the reward we got, and the new
    state that action landed us in. We saved that new state so that we could use it
    now, where we will use it to compute the rest of the future rewards.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 21-17](#figure21-17)中，我们为每个动作保存了四个信息：起始状态、我们选择的动作、我们获得的奖励和该动作带来的新状态。我们保存了这个新状态，以便现在使用它，并用它来计算其余的未来奖励。
- en: The key insight is to notice that our next move begins with that new state,
    and by following our policy we will always select the action whose cell has the
    greatest Q-value. If that cell’s Q-value is the total future reward for *that*
    action, then adding together that cell’s value with our immediate reward gives
    us the current cell’s total future reward. This works because our policy guarantees
    us that we always pick the cell with the largest Q-value for any given board state.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的见解是注意到我们的下一步从那个新状态开始，并且通过遵循我们的策略，我们总是会选择Q值最大的动作。如果那个单元格的Q值是*该*动作的总未来奖励，那么将那个单元格的值与我们即时的奖励相加就能得到当前单元格的总未来奖励。这是可行的，因为我们的策略保证了我们始终为任意给定的棋盘状态选择Q值最大的单元格。
- en: If multiple cells in the next state share the maximum value, then it doesn’t
    matter which one we pick when we get there. All we care about now is the total
    future reward that comes from the next action.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果下一个状态中的多个单元格共享最大值，那么当我们到达那里时，选择哪个单元格并不重要。现在我们关心的只是来自下一个动作的总未来奖励。
- en: '[Figure 21-24](#figure21-24) shows this idea visually. Note that the value
    we compute in this step isn’t the final Q-value, but it’s almost there.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 21-24](#figure21-24)直观地展示了这个概念。请注意，我们在此步骤中计算的值并不是最终的Q值，但它几乎接近。'
- en: '![F21024](Images/F21024.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![F21024](Images/F21024.png)'
- en: 'Figure 21-24: Part of the process for computing a new Q-value for a cell. The
    new value is the sum of two others. The first value is the immediate reward for
    taking the action that cell corresponds to, here 0.2\. The second value is the
    largest Q-value of all the actions belonging to the new state, here 0.6.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-24：计算单元格新Q值的过程的一部分。新值是两个其他值的和。第一个值是采取与该单元格对应的动作所得到的即时奖励，这里是0.2。第二个值是属于新状态的所有动作中Q值最大的，这里是0.6。
- en: The step that’s missing is where Q-learning accounts for randomness. Rather
    than use the value in the cell for our next action, we use the discounted value
    of that cell. Recall that this means we multiply it by our discount factor, a
    number from 0 to 1, often written as *γ* (gamma). As we discussed earlier, the
    smaller the value of *γ*, the less certain we are that unpredictable events in
    the future won’t change this value. [Figure 21-25](#figure21-25) shows the idea.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失的步骤是Q学习如何考虑随机性。我们不会使用下一个动作单元格的值，而是使用该单元格的折扣值。请回忆，这意味着我们将其乘以我们的折扣因子，一个介于0到1之间的数值，通常写作*γ*（伽马）。如前所述，*γ*的值越小，我们就越不确定未来不可预测的事件不会改变这个值。[图
    21-25](#figure21-25)展示了这一概念。
- en: '![F21025](Images/F21025.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![F21025](Images/F21025.png)'
- en: 'Figure 21-25: To find the Q-value, we modify [Figure 21-24](#figure21-24) to
    include the discount factor γ, which reduces the future rewards based on how confident
    we are that they won’t be changed by future, unpredictable events.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-25：为了找到Q值，我们修改了[图 21-24](#figure21-24)，加入了折扣因子γ，该因子根据我们对未来不可预测事件的信心来减少未来奖励。
- en: Note that the many multiplications in the discounted future reward shown in
    [Figure 21-8](#figure21-8) are automatically handled by this scheme. The first
    multiplication is included here explicitly. The multiplication for the states
    beyond that are accounted for when the Q-values in the cells for the next state
    are evaluated.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[图 21-8](#figure21-8)中显示的折扣未来奖励中的多个乘法操作是由这个方案自动处理的。第一次乘法在此显式地包含在内。对于之后的状态的乘法，会在评估下一个状态的Q值时考虑。
- en: Now that we’ve calculated a new value, how do we update the current value? We
    saw during L-learning that simply replacing the current value with the new one
    is a poor choice in the face of uncertainty. But we want to update the cell’s
    Q-value in some way, or we’ll never improve.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算出了一个新值，如何更新当前的值呢？在L-learning中我们看到，面对不确定性时，单纯地用新值替换当前值并不是一个好选择。但我们仍然需要以某种方式更新单元格的Q值，否则我们永远无法改进。
- en: The Q-learning solution to this puzzle is to update the new cell’s value as
    a blend of the old and new values. The amount of blending is left up to us as
    a parameter that we specify. That is, the blend is controlled by a single number
    between 0 and 1, usually written as the lowercase Greek letter *α* (alpha). At
    the extreme value of *α* = 0, the value in the cell doesn’t change at all. At
    the other extreme value of *α* = 1, the new value replaces the old one, as in
    L-learning. Values of *α* between 0 and 1 blend, or mix, the two values, as shown
    in [Figure 21-26](#figure21-26).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个难题的Q-learning方法是将新单元格的值更新为旧值和新值的混合。混合的程度由我们指定的一个参数决定。也就是说，混合由一个介于0和1之间的数字控制，通常写作小写希腊字母*α*（alpha）。在*α*
    = 0的极端值下，单元格中的值完全不变。在另一个极端值*α* = 1时，新的值完全替代旧的值，就像在L-learning中一样。介于0和1之间的*α*值会混合这两个值，如[图21-26](#figure21-26)所示。
- en: '![F21026](Images/F21026.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![F21026](Images/F21026.png)'
- en: 'Figure 21-26: The value of α lets us blend smoothly from the old value (when
    α = 0) to the new value (when α = 1), or any value in between.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-26：*α*的值让我们能够平滑地从旧值（当*α* = 0时）过渡到新值（当*α* = 1时），或者是介于两者之间的任何值。
- en: The parameter *α* is called the *learning rate*, and it’s left up to us to set
    it. It’s unfortunate that this is the same term that’s used by the update step
    of backpropagation, but usually context makes it clear which type of “learning
    rate” we’re referring to.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 参数*α*被称为*学习率*，它由我们来设置。遗憾的是，这是反向传播更新步骤中也使用的同一个术语，但通常上下文会使我们明确指的是哪种“学习率”。
- en: In practice, we usually set α to a value close to 1, such as 0.9 or even 0.99\.
    These values near 1 cause the new values to dominate the value stored in the cell.
    For instance, when *α* = 0.9, the new value stored in the cell is 10 percent of
    the old value, and 90 percent of the new value. But even a value of 0.99 is very
    different than 1, because remembering even 1 percent of the old value is often
    enough to make a difference.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，我们通常会将*α*设置为接近1的值，例如0.9甚至0.99。接近1的这些值使得新值在单元格中占主导地位。例如，当*α* = 0.9时，存储在单元格中的新值为旧值的10%，和新值的90%。但是即使是0.99的值与1也有很大不同，因为即使记住旧值的1%，也足以带来差异。
- en: Using our value for *α*, we run our system through some training and see how
    it does. Then we can adjust the value based on what we see and try again, repeating
    the process until we’ve found the value of *α* that seems to work best. We usually
    automate this search so we don’t have to do it ourselves.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的*α*值，我们让系统通过一些训练，看看它的表现如何。然后我们可以根据观察到的结果调整这个值，再次尝试，重复这个过程，直到找到看起来最有效的*α*值。我们通常会自动化这个搜索过程，这样就不需要我们自己手动调整了。
- en: The elephant in the room is that this whole argument has been based on having
    the correct Q-values in the next state, even before we get there. But where did
    they come from? And if we have the correct Q-values already, then why do any of
    this in the first place?
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这里面有一个显而易见的问题，那就是整个论证是基于在下一状态中就拥有正确的Q值，尽管我们还没有到达那个状态。那么，这些Q值是从哪里来的呢？如果我们已经有了正确的Q值，为什么还要做这些呢？
- en: These are fair questions, and we will return to them after we look at the new
    policy rule.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题很有道理，我们将在了解了新的策略规则后再来讨论。
- en: Q-Learning Policy
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q-Learning 策略
- en: Recall that the policy rule tells us which action to select when we’re given
    a state of the environment. We use this policy while learning, and later, when
    playing actual games. The policy we used in L-learning was to usually select the
    action with the highest L-value in the row of the table corresponding to the current
    board. That makes sense, since we’ve learned that this is the action that brings
    us the highest rewards. But this policy doesn’t explicitly address the explore
    or exploit dilemma. In an unpredictable environment, the move that brings us the
    best rewards sometimes may not bring us the best reward at other times. And completely
    untried moves can be far better, if only we give them a chance.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，策略规则告诉我们在给定环境状态时选择哪个动作。在学习时，我们使用这个策略，之后在实际游戏中也使用它。我们在L-learning中使用的策略是通常选择当前棋盘对应的表格行中具有最高L值的动作。这是有道理的，因为我们已经学到这是能够带来最高奖励的动作。但这个策略并没有明确解决探索与利用的困境。在一个不可预测的环境中，某个动作带来的最佳奖励有时可能并不是最好的奖励，而完全没有尝试过的动作，如果我们给它们一个机会，可能会带来更好的结果。
- en: Still, we don’t want to pick moves at random, because we do want to favor the
    ones that we know lead to high rewards. We just don’t want to do that every time.
    Q-learning picks a middle road. Instead of always picking the action with the
    highest Q-value, we *almost* always pick the action with the highest Q-value.
    The rest of the time we pick one of the other values. Let’s look at two popular
    policies for doing this.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们并不希望每次都随机选择动作，因为我们确实希望偏向那些我们知道会带来高回报的动作。我们只是希望不是每次都这样做。Q学习选择了一条中间道路。我们不是总是选择得分最高的动作，而是*几乎*总是选择得分最高的动作。其余的时间我们会选择其他的动作之一。让我们来看两种常见的策略来实现这一点。
- en: The first approach we’ll look at is called *epsilon-greedy* or *epsilon-soft*
    (these refer to the Greek lowercase letter *ε*, epsilon, so they sometimes appear
    as *ε*-greedy and *ε*-soft). The algorithms are almost the same. We pick some
    number *ε* between 0 and 1, but usually it’s a small number quite close to 0,
    such as 0.01 or less.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要查看的第一个方法叫做*epsilon-greedy*（或*epsilon-soft*）（这些名称来源于希腊小写字母*ε*，epsilon，因此有时也会出现*ε*-greedy和*ε*-soft）。这两种算法几乎相同。我们从0到1之间选择一个*ε*值，通常这个值很小，接近0，比如0.01或更小。
- en: Each time we’re at a row and ready to choose an action, we ask the system for
    a random number between 0 and 1, chosen from a uniform distribution. If the random
    number is greater than *ε*, then we proceed as usual and pick the action with
    the greatest Q-value in the row. But in that occasional case when the random number
    is less than *ε*, we select an action at random out of all the other actions in
    the row. In this way, we usually pick the most promising choice, but infrequently,
    we select one of the other actions and see where it leads us. [Figure 21-27](#figure21-27)
    shows this idea graphically.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们处于某一行并准备选择一个动作时，我们从系统请求一个0到1之间的随机数，这个数来自均匀分布。如果随机数大于*ε*，那么我们像平常一样选择得分最高的Q值对应的动作。但在那种偶尔的情况下，如果随机数小于*ε*，我们会从该行的所有其他动作中随机选择一个。在这种方式下，我们通常会选择最有前景的动作，但偶尔也会选择其他动作，看看它会带我们走向哪里。[图
    21-27](#figure21-27)以图形方式展示了这一思想。
- en: The other policy we’ll look at is called *softmax*. This works in a way similar
    to the softmax layer that we discussed in Chapter 13\. When we apply softmax to
    the Q-values in a row, they are transformed in a complex way so that they add
    up to 1\. This lets us treat the resulting values as a discrete probability distribution,
    and then we randomly select one of entries according to those probabilities.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要查看的另一种策略叫做*softmax*。这与我们在第13章中讨论的softmax层的工作方式相似。当我们对一行的Q值应用softmax时，它们会以一种复杂的方式进行转换，使得它们的总和为1。这让我们能够将结果值视为一个离散的概率分布，然后根据这些概率随机选择其中一个值。
- en: '![F21027](Images/F21027.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![F21027](Images/F21027.png)'
- en: 'Figure 21-27: The epsilon-greedy policy'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-27：epsilon-greedy策略
- en: In this way, we usually get the action with the largest score. Infrequently,
    we get the value with the second-highest score. Even less frequently, we get the
    value with the third-highest score, and so on. [Figure 21-28](#figure21-28) illustrates
    the idea.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们通常会选择得分最高的动作。偶尔，我们会选择得分第二高的动作。更少的时候，我们会选择得分第三高的动作，依此类推。[图 21-28](#figure21-28)展示了这一思想。
- en: '![F21028](Images/F21028.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![F21028](Images/F21028.png)'
- en: 'Figure 21-28: The softmax policy for picking an action temporarily scales all
    the actions in the row so that they add up to 1.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-28：softmax策略暂时缩放行中的所有动作，使它们的总和为1。
- en: An attractive quality of this scheme is that the probabilities of choosing each
    action always reflect the most current Q-values of all the actions associated
    with a given state. So as the values change over time, so too do the probabilities
    of picking the actions.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方案的一个吸引人的特点是，选择每个动作的概率始终反映了与给定状态相关的所有动作的最新Q值。因此，随着值的变化，选择动作的概率也会变化。
- en: The particular calculations carried out by softmax can sometimes lead to the
    system not settling down on a good set of Q-values. An alternative is the *mellowmax*
    policy, which uses slightly different math (Asadi and Littman 2017).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: softmax进行的特定计算有时会导致系统未能稳定在一组良好的Q值上。一个替代方法是*mellowmax*策略，它使用稍有不同的数学（Asadi和Littman
    2017年）。
- en: Putting It All Together
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有内容结合起来
- en: We can summarize the Q-learning policy and update rule in a few words and a
    diagram. In words, when it’s time for a move, we use the current state to find
    the appropriate row of the Q-table. We then select an action from that row according
    to our policy (either epsilon-greedy or softmax). We take that action and get
    back a reward and a new state. Now we want to update our Q-value to reflect what
    we’ve learned from the reward. We look at the Q-values in that new state and select
    the largest one. We discount that by how much we think the environment is unpredictable,
    add it to the immediate reward we just got, and blend that new value with the
    current Q-value, producing a new Q-value for the action we just took, which we
    save.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用几句话和一个图示来总结Q学习的策略和更新规则。用语言描述时，当需要执行一步时，我们使用当前的状态来找到Q表中的相应行。然后我们根据策略（epsilon-greedy或softmax）从该行中选择一个动作。我们执行这个动作，得到一个奖励和一个新状态。现在，我们希望更新我们的Q值，以反映从奖励中学到的东西。我们查看新状态下的Q值，并选择其中最大的一个。我们根据环境的不可预测性对其进行折扣，然后将其加到我们刚得到的即时奖励上，并将该新值与当前的Q值进行混合，生成我们刚采取的动作的新Q值，并将其保存。
- en: '[Figure 21-29](#figure21-29) summarizes the process.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[图21-29](#figure21-29)总结了这个过程。'
- en: '![F21029](Images/F21029.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![F21029](Images/F21029.png)'
- en: 'Figure 21-29: The Q-learning policy and update procedure. (a) Choosing an action.
    (b) Finding a new Q-value for that action.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-29：Q学习策略和更新过程。（a）选择一个动作。（b）为该动作找到新的Q值。
- en: When we start a move, shown in [Figure 21-29](#figure21-29)(a), we look at the
    Q-table row for the current state, and use our policy to pick an action, here
    shown in red. That action is communicated to the environment. The environment
    responds with a reward, and a new state. As in [Figure 21-29](#figure21-29)(b),
    we find the row of the Q-table corresponding to the new state and select the largest
    reward there (this assumes that we’re going to pick the largest action when we
    get to that new state, which we know won’t always be the case. We will return
    to this issue soon). We discount this reward by multiplying it by *γ*, and then
    we add it to the immediate reward for this move, giving us a new value for the
    action we originally chose. We blend the old value and new value using *α*, and
    that new value is placed into the original action’s cell in the Q-table.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始执行一个动作时，如[图21-29](#figure21-29)(a)所示，我们查看当前状态对应的Q表行，并用我们的策略来选择一个动作，这里用红色表示。这个动作会传达给环境，环境会做出回应，给出奖励和新状态。如[图21-29](#figure21-29)(b)所示，我们找到新状态对应的Q表行，并选择其中最大的奖励（假设我们到达新状态时会选择最大的动作，虽然我们知道这并不总是如此。我们很快会回到这个问题）。我们通过将该奖励乘以*γ*来对其进行折扣，然后将其加到我们此时的即时奖励中，从而得到原来选择的动作的新值。我们用*α*将旧值和新值混合，这个新值就被放入Q表中原来动作的单元格里。
- en: The best values for the policy parameter *ε*, the learning rate *α*, and the
    discount factor *γ* have to be found by trial and error. These factors depend
    intimately on the specific nature of the task we’re performing, the nature of
    the environment, and the data we’re working with. Experience and intuition often
    give us good starting points, but nothing beats traditional trial-and-error to
    find the best values for any particular learning system.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 策略参数*ε*、学习率*α*和折扣因子*γ*的最佳值必须通过反复试验来找到。这些因素与我们执行的任务的具体性质、环境的特性以及我们使用的数据密切相关。经验和直觉通常能为我们提供良好的起点，但没有什么能比传统的试错法更能找到任何特定学习系统的最佳值。
- en: The Elephant in the Room
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 居于眼前的大象
- en: Earlier we promised to return to the problem that we needed to have accurate
    Q-values in order to evaluate the update rule, but those values themselves were
    computed by the update rule using the values that came after them, and so on.
    Each step seems to depend on the data from the following step. How can we use
    data that we haven’t created yet?
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们承诺会回到一个问题，那就是我们需要准确的Q值来评估更新规则，但这些值本身是通过更新规则计算出来的，而更新规则使用的是后续的值，依此类推。每一步似乎都依赖于下一步的数据。我们怎么能使用那些尚未创建的数据呢？
- en: 'Here’s the beautiful, simple answer to that problem: we ignore it. Incredibly
    enough, we can initialize the Q-table with all zeros, and then start learning.
    In the beginning, the system makes moves erratically because there’s nothing in
    the Q-table to help it pick one cell over another. It picks one of the cells at
    random and plays that move. All of the actions in the resulting state are also
    zero, so the update rule, no matter what values we use for *α* and *γ*, keeps
    the cell’s score at zero.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这是解决该问题的简单美丽答案：我们忽略它。令人难以置信的是，我们可以将Q表初始化为全零，然后开始学习。刚开始时，由于Q表中没有任何信息帮助系统选择一个单元格，它的行动会显得非常混乱。系统会随机选择一个单元格并执行该操作。结果状态下的所有动作也都是零，因此更新规则无论我们使用什么值的*α*和*γ*，都会使该单元格的得分保持为零。
- en: Our system plays games that look chaotic and foolish, making terrible choices
    and missing obvious good moves. But eventually, the system stumbles onto a victory.
    That victory gets a reward of a positive number, and that reward updates the Q-value
    of the action that led to it. Sometime later, an action that led us to that action
    incorporates some of that great reward, because of the step in Q-learning that
    looks ahead to the next state. That ripple effect continues to slowly work backward
    through the system, as new games fall into the states that lead to states that
    previously led to victory.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系统进行的游戏看起来混乱且愚蠢，做出糟糕的选择，错失明显的好动作。但最终，系统偶然发现了胜利。这个胜利会得到一个正数的奖励，这个奖励会更新导致胜利的动作的Q值。稍后，某个导致我们执行该动作的动作会融入一些巨大的奖励，因为Q学习的步骤会预见到下一个状态。这个波动效应会继续缓慢地向后影响系统，因为新的游戏进入到那些曾经导致胜利的状态。
- en: Note that the information isn’t actually moving backward. Every game is played
    from beginning to end, and every update is made immediately after each move. The
    information seems to move backward because Q-learning involves the step of looking
    forward one move when evaluating the update rule. The score from the nextmove
    is able to influence the score for this one.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，信息并没有真正地向后移动。每场游戏从头到尾进行，每次更新都在每一步后立即进行。信息看似向后移动，是因为Q学习涉及在评估更新规则时向前看一步。下一步的得分能够影响当前步骤的得分。
- en: At some point, thanks to our policy that sometimes tries out new actions, every
    move eventually leads to a path to victory, and those values also influence earlier
    and earlier actions. Eventually the Q-table fills up with values that accurately
    predict the rewards of each action. Further playing serves to only improve the
    accuracy of those values. This process of settling into a consistent solution
    is called *convergence*. We say that the Q-learning algorithm *converges*.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个时刻，感谢我们的策略，有时尝试新的动作，每个动作最终都会通向胜利的路径，这些值也会影响到越来越早的动作。最终，Q表会填充上准确预测每个动作奖励的值。进一步的游戏只会提高这些值的准确性。这个过程被称为*收敛*。我们说Q学习算法*收敛*了。
- en: We can prove mathematically that Q-learning converges (Melo 2020). This kind
    of proof guarantees that the Q-table gradually gets better. What we can’t say
    is how long that will take. The larger the table, and the more unpredictable the
    environment, the longer the training process requires. The speed of convergence
    also depends on the nature of the task the system is trying to learn, the feedback
    provided, and, of course, our chosen values for the policy variable *ε*, the learning
    rate *α,* and the discount factor *γ*. As always, there’s no substitute for trial-and-error
    experimentation to learn the specific idiosyncrasies of any particular system.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从数学上证明Q学习是收敛的（Melo 2020）。这种证明保证了Q表会逐渐变得更好。但我们无法说出这个过程需要多长时间。表格越大，环境越不可预测，训练过程所需的时间也越长。收敛的速度还取决于系统试图学习的任务的性质、提供的反馈，当然，还有我们为策略变量*ε*、学习率*α*和折扣因子*γ*选择的值。像往常一样，没有什么能代替通过反复试验来学习任何特定系统的独特性。
- en: Note that the Q-learning algorithm very nicely addresses two of the problems
    we discussed earlier. The credit assignment problem asks us to make sure that
    the moves that lead up to a victory are rewarded, even when the environment isn’t
    providing that reward. The nature of the update rule takes care of this, propagating
    the rewards for successful moves backward from the final step that led to victory
    all the way back to the very first move. The algorithm also addresses the explore
    or exploit dilemma by using epsilon-greedy or softmax policies. They both favor
    choosing actions that have proven to be successful (exploitation), but they also
    sometimes try the other actions just to see what might come of them (exploration).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Q学习算法很好地解决了我们之前讨论的两个问题。奖励分配问题要求我们确保引导胜利的动作得到奖励，即使环境没有提供这个奖励。更新规则的性质解决了这个问题，将成功动作的奖励从导致胜利的最终步骤反向传播，直到第一步。算法还通过使用epsilon-greedy或softmax策略解决了探索或利用的困境。它们都偏好选择那些已被证明成功的动作（利用），但有时也会尝试其他动作，看看可能的结果是什么（探索）。
- en: Q-learning in Action
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q学习法在行动中
- en: Let’s put Q-learning to work and see if it can learn how to play Flippers in
    an unpredictable environment. One way to measure the algorithm’s performance is
    to have the trained model play a large number of random games and see how long
    they take. The better the algorithm has gotten at finding good moves and eliminating
    bad ones, the fewer moves each game should require before reaching victory.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们让Q学习法发挥作用，看看它是否能在不可预测的环境中学会如何玩《翻转者》游戏。衡量算法表现的一种方式是让训练过的模型进行大量随机游戏，并看看它们需要多长时间。算法在找到好的动作并消除坏动作方面做得越好，每局游戏在胜利前所需的步数就越少。
- en: The longest well-played game is the one that starts out with all nine cells
    showing a dot. Then we have to flip six cells to get to a victory. So, we’d like
    to see our algorithm win every game in six moves or less.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 最长的良好游戏是从九个格子都显示点开始的。然后，我们需要翻转六个格子才能获胜。所以，我们希望看到我们的算法每局游戏都能在六步或更少的步骤内获胜。
- en: To see the effect of training on the algorithm, let’s look at plots of the lengths
    of a large number of games for different amounts of training. Our plots show the
    results of playing games that start with each of the 512 possible patterns of
    dots and blanks, in an environment with a considerable degree of unpredictability.
    We played 10 games for each starting board, for a total of 5,120 games. We cut
    off any game that ran for more than 100 steps.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看训练对算法的影响，我们来看一下不同训练量下大量游戏长度的图表。我们的图表显示了在一个具有相当程度不可预测性的环境中，从每种可能的512种点和空格的起始模式开始的游戏结果。我们为每个起始棋盘玩了10局游戏，总共进行了5120局游戏。我们会中断任何超过100步的游戏。
- en: We set *α* to 0.95, so each cell retained just 5 percent of its old value when
    it was updated. This way we don’t completely lose what we’ve learned before, but
    we are expecting new values to be better than old ones, since they’ll be based
    on improved Q-table values when they pick the next move. To select moves, we used
    an epsilon-greedy policy with a relatively high *ε* of 0.1, encouraging the algorithm
    to seek out new moves 1 time out 10.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将*α*设置为0.95，因此每个格子在更新时只保留5%的旧值。这样，我们不会完全失去之前学到的东西，但我们期望新的值比旧的值更好，因为它们将基于改进后的Q表值来选择下一步行动。为了选择动作，我们采用了一个相对较高*ε*值为0.1的epsilon-greedy策略，鼓励算法每10次中有1次去尝试新的动作。
- en: We introduced a lot of unpredictability by simulating our random truck coming
    by after each move with a probability of 1 in 10, flipping over a single random
    tile each time. To account for this, we set the discount factor *γ* to 0.2\. This
    low value says we’re only 20 percent sure that the future will play out the same
    way each time because of the influence of those random events. We set this higher
    than the noise level we know the truck introduces (10 percent), because we expect
    that most well-played games will only be three or four moves long, so they are
    less likely to see a random event than a game of 10 or more moves.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过模拟每次移动后有1/10的概率随机卡车经过，每次翻转一个随机的瓦片，引入了很多不可预测性。为了解决这个问题，我们将折扣因子*γ*设置为0.2。这个较低的值意味着我们只对未来的走势有20%的确定性，因为这些随机事件的影响。我们将这个值设置得比我们知道卡车引入的噪声水平（10%）还要高，因为我们预计大多数经过良好操作的游戏只有三到四步长，因此它们出现随机事件的概率低于十步或更多步的游戏。
- en: These values of *α*, *γ*, and *ε* are all basically informed guesses. In particular,
    *γ* was chosen based on our knowledge of how often random events would occur,
    which we rarely know ahead of time. In a real situation we’d experiment with our
    parameters to find what works best for this game and this amount of noise.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 *α*、*γ* 和 *ε* 的值基本上是基于经验的猜测。特别是，*γ* 的选择是基于我们对随机事件发生频率的了解，而这种情况我们通常无法提前知道。在实际情况中，我们会通过实验调整参数，以找到最适合此游戏和噪音量的设置。
- en: '[Figure 21-30](#figure21-30) shows the game lengths after training for just
    300 games. The algorithm already found a lot of quick wins.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 21-30](#figure21-30) 显示了在仅训练 300 场游戏后的游戏时长。算法已经找到了很多快速获胜的方法。'
- en: '![F21030](Images/F21030.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![F21030](Images/F21030.png)'
- en: 'Figure 21-30: The number of games that required from 0 to 40 moves to win (we
    played each of the 512 starting boards 10 times) using a Q-table that had been
    trained for 300 games'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-30：使用经过 300 场游戏训练的 Q 表，在 0 到 40 步之间获胜所需的游戏数量（我们对每个 512 种起始局面进行了 10 次游戏）
- en: The “instant wins” are in the first column, corresponding to zero moves. These
    are games whose starting boards already have just three dots, arranged in a vertical
    column or horizontal row. Since there are six possible winning game configurations,
    and we ran through all the possible board configurations 10 times each, we started
    with a winning board 60 times.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: “瞬间获胜”位于第一列，对应于零步。这些游戏的起始局面已经有三个点，排列成垂直列或水平行。由于有六种可能的获胜局面，我们对所有可能的局面进行了 10 次游戏，因此我们从一个获胜局面开始了
    60 次游戏。
- en: Since no game in [Figure 21-30](#figure21-30) hit our 100-move cutoff, we can
    see that the algorithm never fell into a long-lived loop. A loop might just be
    two states alternating forever, or a long string of them that wraps back around
    on itself. Loops are possible in Flippers, and there’s nothing in the basic Q-learning
    algorithm that explicitly prevents the system from getting into a loop.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 [图 21-30](#figure21-30) 中没有一场游戏达到了我们的 100 步限制，我们可以看到算法从未陷入长期循环。循环可能只是两个状态永远交替，或者是一串长的状态，它最终会回到自身。Flippers
    中是有可能出现循环的，而且基本的 Q-learning 算法并没有明确防止系统进入循环。
- en: We might say that the system “discovered” that loops don’t get to victory and
    thus don’t bring any rewards, so it learned to avoid them. If at some point it
    did return to a previously visited state, either by making that move or as the
    result of a randomly introduced flip, the relatively high value of *ε* meant it
    had a good chance of eventually picking a new action and thereby going off in
    a new direction.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说系统“发现”了循环无法获胜，因此不会带来任何奖励，所以它学会了避免循环。如果某个时候它确实回到了先前访问过的状态，无论是通过做出那个动作，还是通过随机引入的翻转，*ε*
    的相对较高值意味着它有很大的机会最终选择一个新的动作，从而进入一个新的方向。
- en: Let’s raise the number of training games to 3,000, as in [Figure 21-31](#figure21-31).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将训练游戏的数量提高到 3,000，如 [图 21-31](#figure21-31) 所示。
- en: '![F21031](Images/F21031.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![F21031](Images/F21031.png)'
- en: 'Figure 21-31: The number of games of different lengths resulting from playing
    5,120 games, based on a Q-table trained by playing 3,000 games'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-31：根据训练 3,000 场游戏的 Q 表，进行 5,120 场游戏后，不同长度的游戏数量
- en: The algorithm has learned a lot. The longest game is now just 20 moves, with
    most games being won in 10 moves or less. It’s nice to see the denser clustering
    around four and five moves.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法已经学到了很多东西。现在最久的游戏仅需 20 步，大多数游戏在 10 步内就能获胜。可以看到，四步和五步周围的集群更密集，效果相当不错。
- en: Let’s look at a typical game played after these 3,000 episodes of training.
    [Figure 21-32](#figure21-32) shows the game, played left to right. The algorithm
    took eight moves to win.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在这 3,000 轮训练后进行的一场典型游戏。[图 21-32](#figure21-32) 显示了这场从左到右进行的游戏。该算法用了八步才赢得了比赛。
- en: '[Figure 21-32](#figure21-32) is not an encouraging result. Just by looking
    at the starting board, we can see at least four different ways to win this game
    in four moves. For example, flip the lower-left square and then flip the three
    dots in the middle and rightmost columns. But our algorithm seems to be flipping
    over tiles at random. It eventually stumbles onto a solution, but it’s definitely
    not an elegant result.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 21-32](#figure21-32) 的结果并不令人鼓舞。仅从起始局面看，我们就能看到至少四种不同的方式在四步内赢得游戏。例如，可以先翻转左下角的方块，然后翻转中间和最右列的三个点。但我们的算法似乎在随机翻动方块。它最终偶然找到了解法，但这绝对不是一个优雅的结果。'
- en: '![F21032](Images/F21032.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![F21032](Images/F21032.png)'
- en: 'Figure 21-32: Playing a game of Flippers after training Q-learning for 3,000
    episodes'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-32：在训练了 3,000 轮 Q-learning 后，进行 Flippers 游戏
- en: If we train the algorithm for more episodes, we expect its performance to improve.
    After 3,000 more training episodes (for a total of 6,000), and looking at the
    number of games that required different numbers of moves, we get the results of
    [Figure 21-33](#figure21-33).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对算法进行更多的训练回合，我们预计其表现会有所改善。经过额外 3,000 次训练回合（总共 6,000 次），并查看需要不同步数的游戏数量，我们得到了[图
    21-33](#figure21-33)的结果。
- en: Compared to our results in [Figure 21-31](#figure21-31), after 3,000 games of
    training, the longest game has decreased from 20 moves to 18, and the shorter
    games of just 3 and 4 steps have become more frequent.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在[图 21-31](#figure21-31)中的结果相比，经过 3,000 场训练后，最久的游戏从 20 步减少到 18 步，且只有 3 步和
    4 步的短游戏变得更加频繁。
- en: This chart suggests that the algorithm is learning, but how is it actually performing
    when it plays a game? In fact, the algorithm has taken a huge jump in ability.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图表表明算法正在学习，但它在实际玩游戏时表现如何？事实上，算法的能力已经有了巨大的飞跃。
- en: '[Figure 21-34](#figure21-34) shows the very same game as [Figure 21-32](#figure21-32),
    which required eight moves to win. Now it takes just four moves, which is the
    minimum number for this board (though there’s more than one way to achieve it).'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 21-34](#figure21-34)显示了与[图 21-32](#figure21-32)相同的游戏，这个游戏最初需要八步才能获胜。现在它只需要四步，这是这个棋盘上最少的步数（虽然有不止一种方法可以实现）。'
- en: '![F21033](Images/F21033.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![F21033](Images/F21033.png)'
- en: 'Figure 21-33: The number of games that required a given number of moves to
    win our 5,120 games after training the Q-table with 6,000 games'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-33：在训练了 6,000 次游戏的 Q 表之后，我们的 5,120 场游戏中，获胜所需的步数分布。
- en: Q-learning has done remarkably well even in this highly unpredictable learning
    environment, where a tile is flipped over at random after 10 percent of the moves.
    It weathered that unpredictability and managed to find ideal solutions for most
    games, even with only 6,000 training runs.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 在这种高度不可预测的学习环境中表现出色，在每进行 10% 的动作后，瓦片会随机翻转。它成功应对了这种不可预测性，并且在仅进行 6,000
    次训练的情况下，为大多数游戏找到了理想的解决方案。
- en: SARSA
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SARSA
- en: Q-learning does a great job, but it has a flaw that can reduce the accuracy
    of the Q-values that it relies on. It’s the problem we referred to when discussing
    [Figure 21-29](#figure21-29), when we noted that we were basing our future reward
    on the score of the most likely next action, even though that’s not necessarily
    the action that would be taken. In other words, the update rule *assumes* we’re
    going to pick the highest-scoring action on our next move, and its calculations
    of the new Q-value are based on that assumption. This isn’t an unreasonable assumption,
    because both our epsilon-greedy and softmax policies usually pick the most rewarding
    action. But the assumption is wrong when one of those policies chooses one of
    the other actions.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 表现出色，但它存在一个缺陷，可能会降低其所依赖的 Q 值的准确性。这就是我们在讨论[图 21-29](#figure21-29)时提到的问题，我们注意到，尽管并不一定是采取的行动，仍然根据最可能的下一步行动的得分来预测未来的奖励。换句话说，更新规则*假设*我们在下一步会选择得分最高的动作，它基于这个假设计算新的
    Q 值。这一假设并非不合理，因为我们的 epsilon-greedy 和 softmax 策略通常会选择最有奖励的动作。但当这些策略选择其他动作时，这个假设就不成立了。
- en: When our policy picks any action other than the one we used in the update rule,
    the calculation will have used the wrong data, and we end up with reduced accuracy
    in the new value that we compute for that action. Happily, we can fix that problem.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的策略选择了更新规则中没有使用的其他动作时，计算将使用错误的数据，最终导致我们计算该动作的新值时准确性降低。幸运的是，我们可以解决这个问题。
- en: '![F21034](Images/F21034.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![F21034](Images/F21034.png)'
- en: 'Figure 21-34: The game of [Figure 21-32](#figure21-32), solved more efficiently
    by Q-learning thanks to more training episodes'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-34：通过更多训练回合，Q-learning 更高效地解决了[图 21-32](#figure21-32)中的游戏
- en: The Algorithm
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法
- en: It would be nice to keep all the virtues of Q-learning, but avoid making the
    mistake of calculating a move’s Q-value by using the Q-value of the highest-scoring
    next action when there’s a chance we won’t actually select that action when we
    make our next move. We can do that by modifying Q-learning just a little, creating
    a new algorithm known as *SARSA* (Rummery and Niranjan 1994). This is an acronym
    for “state-action-reward-state-action.” The “SARS” part we’ve had covered ever
    since [Figure 21-17](#figure21-17), when we saved the starting state (S), action
    (A), reward (R), and resulting state (S). What’s new here is the extra action
    “A” at the end.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如果能保留Q学习的所有优点，同时避免犯错，通过使用最高得分的下一个动作的Q值来计算某个动作的Q值，而实际上我们可能并不会在下一个动作中选择那个动作，那该多好啊。我们可以通过稍微修改Q学习，创建一个新算法，称为*SARSA*（Rummery
    和 Niranjan 1994）。这是“状态-动作-奖励-状态-动作”的缩写。我们从[图21-17](#figure21-17)开始，已经涵盖了“SARS”部分，即我们保存了初始状态（S）、动作（A）、奖励（R）和结果状态（S）。这里新增的是末尾的额外动作“A”。
- en: SARSA fixes the problem of choosing the wrong cell from the next state by choosing
    that next cell *with our policy* (rather than just selecting the biggest one),
    and *remembering* the choice of action (that’s the extra “A” at the end). Then
    when it’s time to make our new move, we select the action that we computed previously
    and saved.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA通过使用我们的策略（而不是仅仅选择得分最高的那个）来选择下一个状态中的正确单元，并*记住*我们选择的动作（这就是末尾的额外“A”）。然后，当我们需要做出新的动作时，我们选择之前计算并保存的动作。
- en: In other words, we’ve moved the time when we apply our action-choosing policy.
    Instead of choosing our action at the start of a move, we choose it during the
    previous move and remember our choice. That lets us use the value of the action
    we really will use when building the new Q-value.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们已经将应用我们选择动作策略的时间推迟了。我们不是在动作开始时选择我们的动作，而是在前一个动作中选择，并记住我们的选择。这使得我们在构建新的Q值时，能够使用我们实际将会使用的动作的值。
- en: Those two changes (moving the action-choosing step and remembering the action
    we chose) are all that differentiate SARSA from Q-learning, but they can make
    a big difference in learning speed.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这两项改变（推迟动作选择步骤和记住我们选择的动作）是SARSA与Q学习的区别所在，但它们可以显著提高学习速度。
- en: Let’s look at three successive moves using SARSA. The first move is shown in
    [Figure 21-35](#figure21-35). Because this is the first move, we use our policy
    to pick an action for this move in [Figure 21-35](#figure21-35)(a). This is the
    only time we do this. Once we have our chosen action, we use our policy to pick
    the action for move two. We get a reward from the environment, and update the
    Q-value for the action we just chose, in [Figure 21-35](#figure21-35)(b).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下使用SARSA的三个连续动作。第一个动作如[图21-35](#figure21-35)所示。由于这是第一次动作，我们使用我们的策略为此动作选择一个动作，如[图21-35](#figure21-35)(a)所示。这是唯一一次这么做。选择完我们的动作后，我们使用策略为第二步选择动作。我们从环境中获得奖励，并更新我们刚刚选择的动作的Q值，如[图21-35](#figure21-35)(b)所示。
- en: '![F21035](Images/F21035.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![F21035](Images/F21035.png)'
- en: 'Figure 21-35: Using SARSA in the first move of our game. (a) We use our policy
    to pick the current action. (b) We also use our policy to pick our next action
    and update our current Q-value with the Q-value for that next action.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-35：在游戏的第一步中使用SARSA。（a）我们使用策略选择当前动作。（b）我们也使用策略选择下一个动作，并使用该下一个动作的Q值来更新当前Q值。
- en: The second move is shown in [Figure 21-36](#figure21-36). Now we use the action
    we picked for ourselves last time and then pick the action we’ll use in the third
    move once we get there.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个动作如[图21-36](#figure21-36)所示。现在我们使用上次为自己选择的动作，然后选择在第三个动作中使用的动作。
- en: '![F21036](Images/F21036.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![F21036](Images/F21036.png)'
- en: 'Figure 21-36: The second move using SARSA. (a) We make the action we picked
    for ourselves last time. (b) We pick the next action, and use its Q-value to update
    the current action’s Q-value.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-36：使用SARSA的第二步动作。（a）我们执行上次为自己选择的动作。（b）我们选择下一个动作，并利用其Q值来更新当前动作的Q值。
- en: The third move is shown in [Figure 21-37](#figure21-37). Here again we take
    the previously determined action and work out the action for the next, fourth
    move.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个动作如[图21-37](#figure21-37)所示。在这里，我们再次选择之前确定的动作，并为下一个第四个动作计算出相应的动作。
- en: '![F21037](Images/F21037.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![F21037](Images/F21037.png)'
- en: 'Figure 21-37: The third move using SARSA. (a) We take the action we determined
    during the second move. (b) We choose an action for the fourth move, and use its
    Q-value to improve the current action’s Q-value.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-37：使用SARSA的第三步。（a）我们采取了第二步时确定的动作。（b）我们为第四步选择一个动作，并利用其Q值来改善当前动作的Q值。
- en: Happily, we can prove that SARSA will also converge. As before, we can’t guarantee
    how long it will take, but it usually starts producing good results sooner than
    Q-learning and improves them quickly after that.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 值得高兴的是，我们可以证明SARSA也会收敛。像以前一样，我们不能保证它需要多久，但通常它比Q学习更早产生良好的结果，并且很快改善这些结果。
- en: SARSA in Action
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SARSA的应用
- en: 'Let’s see how well SARSA plays Flippers, using the same approach we took to
    Q-learning. [Figure 21-38](#figure21-38) shows the number of moves required by
    our 5,120 games after 3,000 training episodes using SARSA. For this plot and those
    following, we continue to use the same parameters as for the Q-learning plots:
    the learning rate *α* is 0.95, we introduce a random flip with a probability of
    0.1 after every move, the discount factor *γ* is 0.2, and we pick moves with an
    epsilon-greedy policy with *ε* set to 0.1.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看SARSA在Flippers游戏中的表现，采用与Q学习相同的方法。[图 21-38](#figure21-38)显示了我们在使用SARSA进行3000次训练后，5120局游戏所需的步数。在此图和接下来的图中，我们继续使用与Q学习图中相同的参数：学习率*α*为0.95，每步后引入0.1的随机翻转，折扣因子*γ*为0.2，并采用epsilon贪心策略，*ε*设为0.1。
- en: '![F21038](Images/F21038.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![F21038](Images/F21038.png)'
- en: 'Figure 21-38: The lengths of 5,120 games using SARSA after training with 3,000
    games. Note that only a few games required more than the maximum of six moves.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-38：使用SARSA在训练3000局后，5120局游戏的步数。注意只有少数游戏需要超过6步。
- en: This is looking great, with most values clustered around 4\. The longest game
    is only 15 steps, with very few longer than 8.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来不错，大多数值聚集在4左右。最长的游戏仅为15步，很少有超过8步的游戏。
- en: Let’s look at a typical game. [Figure 21-39](#figure21-39) shows the game, played
    left to right. The algorithm needed seven moves to win. That’s not terrible, but
    we know it can be solved more quickly.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个典型的游戏。[图 21-39](#figure21-39)显示了从左到右的游戏过程。该算法需要七步才能获胜。虽然不算差，但我们知道它可以更快地解决。
- en: As always, more training should result in better performance. As before, let’s
    double our training to 6,000 episodes.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，更多的训练应当带来更好的表现。像之前一样，让我们将训练次数增加到6000次。
- en: '[Figure 21-40](#figure21-40) shows the lengths of our 5,120 games after 6,000
    training episodes.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 21-40](#figure21-40)显示了我们在进行6000次训练后，5120局游戏的步数。'
- en: '![F21039](Images/F21039.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![F21039](Images/F21039.png)'
- en: 'Figure 21-39: Playing a game of Flippers after 3,000 episodes of training to
    SARSA'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-39：在SARSA进行3000次训练后玩Flippers游戏
- en: '![F21040](Images/F21040.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![F21040](Images/F21040.png)'
- en: 'Figure 21-40: The lengths of our 5,120 games using SARSA after training for
    6,000 games. Note how much shorter most of the games have become, and that none
    of the games got caught in a loop.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-40：使用SARSA在训练6000局后，我们的5120局游戏的步数。注意大多数游戏的步数变得更短，且没有游戏陷入循环。
- en: The longest game has gone down from 15 to 14, which isn’t much to shout about,
    but the number of short games of lengths 3 and 4 is now even more pronounced.
    There weren’t many games that required more than 6 moves.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 最长的游戏从15步减少到14步，这个变化不算大，但长度为3和4步的短游戏数量现在更加明显。需要超过6步的游戏并不多。
- en: '[Figure 21-41](#figure21-41) shows the same game as [Figure 21-39](#figure21-39),
    which required 7 moves to win. Now it takes just 3 moves, which is the minimum
    for this board (though again, there’s more than one way to win with just 3 moves).'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 21-41](#figure21-41)显示了与[图 21-39](#figure21-39)相同的游戏，该游戏需要7步获胜。现在只需3步，这是该棋盘的最小步数（尽管同样的，3步也有不止一种获胜方法）。'
- en: '![F21041](Images/F21041.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![F21041](Images/F21041.png)'
- en: 'Figure 21-41: The same game as [Figure 21-39](#figure21-39), after 3,000 more
    training episodes'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21-41：与[图 21-39](#figure21-39)相同的游戏，经过3000次训练后
- en: Comparing Q-Learning and SARSA
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较Q学习与SARSA
- en: Let’s compare the Q-learning and SARSA algorithms. [Figure 21-42](#figure21-42)
    shows the lengths of all 5,120 possible games, after 6,000 games of training by
    Q-learning and SARSA. These results are slightly different from the previous plots
    because they were generated by new runs of the algorithm, so the random events
    were different.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来比较Q学习与SARSA算法。[图 21-42](#figure21-42)显示了经过6000局训练后，Q学习和SARSA的5120种可能游戏的步数。这些结果与之前的图略有不同，因为它们是通过新一轮的算法运行生成的，因此随机事件有所不同。
- en: They’re roughly comparable, but Q-learning produces a few games that are longer
    than SARSA’s maximum of 12.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 它们大致相当，但Q学习产生了一些游戏，其时长超过了SARSA的最大时长12步。
- en: '![F21042](Images/F21042.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![F21042](Images/F21042.png)'
- en: 'Figure 21-42: Comparing game lengths after 6,000 training games for both Q
    and SARSA. SARSA’s longest game was 11 steps, while Q-learning went as high as
    18.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-42：在6,000场训练游戏后比较Q学习和SARSA的游戏时长。SARSA的最长游戏是11步，而Q学习则有高达18步的情况。
- en: More training helps. We’ve increased the training length by a factor of 10,
    for 60,000 games each. The results are shown in [Figure 21-43](#figure21-43).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的训练是有帮助的。我们将训练长度增加了10倍，每个训练进行了60,000场游戏。结果如[图21-43](#figure21-43)所示。
- en: '![f21043](Images/f21043.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![f21043](Images/f21043.png)'
- en: 'Figure 21-43: The same training scenario as in [Figure 21-42](#figure21-42),
    but now we’ve trained for 60,000 games'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-43：与[图21-42](#figure21-42)相同的训练场景，但这次我们进行了60,000场训练游戏
- en: At this level of training, SARSA is doing an excellent job on Flippers, with
    almost all games coming in at 6 moves or less (very few games required 7 moves).
    Q-learning is faring slightly worse overall, needing up to 16 steps to solve some
    of its games, but it too is greatly concentrated in the region of 4 moves and
    under.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个训练阶段，SARSA在Flippers上表现非常出色，几乎所有的游戏都在6步或更少内完成（很少有游戏需要7步）。Q学习总体上稍微差一些，某些游戏需要多达16步才能解决，但它也大部分集中在4步以内。
- en: Another way to compare Q-learning and SARSA for this simple game is to plot
    the average game length after increasingly long training sessions. This gives
    us an idea of how effectively they’re learning to win the game. [Figure 21-44](#figure21-44)
    shows this for our Flippers game.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种比较Q学习和SARSA在这个简单游戏中的方法是绘制经过逐渐延长的训练后的平均游戏时长。这可以让我们了解它们在学习如何赢得游戏方面的有效性。[图21-44](#figure21-44)展示了我们Flippers游戏的结果。
- en: '![f21044](Images/f21044.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![f21044](Images/f21044.png)'
- en: 'Figure 21-44: The length of the average game for training sessions from 1 to
    100,000 episodes (in increments of 1,000)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图21-44：从1到100,000场训练游戏（以1,000场为增量）的平均游戏时长
- en: The trend here is easy to see. Both algorithms drop quickly and then level off,
    but after a noisy start, SARSA always performs better, ultimately saving almost
    a half move on every game (that is, in general, it plays one less move for every
    two games). By the time we reach 100,000 training games, both algorithms appear
    to have stopped improving. It seems likely that the Q-tables of each algorithm
    have settled down into stable states, changing a little bit over time due to the
    random flips introduced by the environment.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的趋势很容易看出来。两种算法都迅速下降，然后趋于平稳，但在经历了一段噪声较大的起步后，SARSA的表现始终更好，最终每场游戏节省了近半步（也就是说，一般每两场游戏少走一步）。当我们达到100,000场训练游戏时，似乎两种算法的表现都停止了改善。看起来每个算法的Q表已经稳定下来，随着时间的推移，因环境的随机波动而略微变化。
- en: So, although Q-learning and SARSA can both do a great job of learning to play
    Flippers, SARSA’s games will generally be shorter.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，尽管Q学习和SARSA都能在学习玩Flippers时表现得很出色，但SARSA的游戏通常会更短。
- en: The Big Picture
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全貌
- en: Let’s step back and review the big picture of reinforcement learning.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们退后一步，回顾一下强化学习的全貌。
- en: There’s an environment and an agent. The environment provides the agent with
    two lists of numbers (the state variables and the available actions). Using its
    policy, the agent considers these two lists, along with whatever private information
    it has saved internally, to select one of the values from the list of actions,
    which it returns to the environment. In response, the environment gives the agent
    back a number (the reward) and two new lists.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个环境和一个智能体。环境向智能体提供两组数字（状态变量和可用动作）。利用其策略，智能体会考虑这两组列表，以及它保存的任何私人信息，从中选择一个动作，并将其返回给环境。作为回应，环境会返回一个数字（奖励）和两组新的列表。
- en: Interpreting the lists as boards and moves was great because it lets us think
    of Q-learning in terms of learning to play a game. But the agent doesn’t know
    it’s in a game, or that there are rules, or really much of anything. It just knows
    that two lists of numbers come in, it picks a value from one of the lists, and
    then a reward value arrives in response. It’s remarkable that this little process
    can do much that’s interesting at all, but if we can find a way to describe our
    environment, and actions on that environment, using sets of numbers, and we can
    find even a crude way to distinguish a good action from a bad one, this algorithm
    can learn how to perform high-quality actions.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 将列表解释为棋盘和动作是很棒的，因为这让我们能够将 Q-learning 看作是学习如何玩游戏。但代理并不知道它正在玩一个游戏，也不知道有规则，或者实际上对任何事情知之甚少。它只知道两个数字列表传入，它从其中一个列表中选一个值，然后返回一个奖励值。这个小过程能做出许多有趣的事情，令人惊讶。但如果我们能够找到一种方法来描述我们的环境，以及在这个环境中采取的行动，使用数字集合表示，并且我们能找到一种粗略的方式来区分好的行动和坏的行动，那么这个算法就能学习如何执行高质量的行动。
- en: 'This worked for our simple game of Flippers, but how practical is all of this
    Q-table stuff in practice? In Flippers, there are nine squares and each can have
    a dot or not, so the game needs a Q-table with 512 rows and 9 columns, or 4,608
    cells. In a game of tic-tac-toe, there are nine squares, and each can have one
    of three symbols: blank, X, or O. The Q-table for this game would need 20,000
    rows and 9 columns, or 180,000 cells.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我们简单的 Flippers 游戏中是有效的，但所有这些 Q-table 内容在实际操作中有多实用呢？在 Flippers 中，有九个方格，每个方格可以有一个点或者没有，因此游戏需要一个包含
    512 行和 9 列的 Q-table，即 4,608 个单元格。在井字游戏中，有九个方格，每个方格可以有三种符号之一：空白、X 或 O。这个游戏的 Q-table
    需要 20,000 行和 9 列，即 180,000 个单元格。
- en: That’s big, but not ridiculously big for a modern computer. But what if we want
    a slightly more challenging game? Rather than play tic-tac-toe on a 3 by 3 board,
    suppose we played on a 4 by 4 board. There are a bit more than 43 million such
    boards, so our table would have 43 million rows and 9 columns, or a bit under
    390 million cells. That’s getting pretty big, even for modern computers. Let’s
    increase it just one more modest step, and play tic-tac-toe on a 5 by 5 board.
    That hardly seems outrageous. Yet that board has almost 850 *billion* states.
    If we get a little ambitious and play on a 13 by 13 board, we find that the number
    of states is more than the number of atoms in the visible universe (Villanueva
    2009). In fact, it’s roughly the number of atoms in one *billion* visible universes.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这很大，但对于现代计算机来说并不算荒谬大。但如果我们想玩一个稍微有挑战性的游戏呢？假设我们不是在 3×3 的棋盘上玩井字游戏，而是在 4×4 的棋盘上玩。这样的棋盘数量略超过
    4300 万个，所以我们的表格将需要 4300 万行和 9 列，或者稍低于 3.9 亿个单元格。即使是对于现代计算机来说，这也开始变得非常庞大。我们再稍微增加一点，假设我们在
    5×5 的棋盘上玩井字游戏。这个似乎并不荒唐。然而，这个棋盘有接近 850 *十亿* 种状态。如果我们再稍微大胆一点，在 13×13 的棋盘上玩，我们会发现状态数量超过了可见宇宙中原子数量（Villanueva
    2009）。实际上，这大约是一个*十亿*个可见宇宙中的原子数量。
- en: Storing the table for this game is not remotely practical, but it’s an entirely
    reasonable thing to want to do. More reasonably, we might want to play Go. The
    standard board for the game of Go is a grid of 19 by 19 intersections, and each
    intersection can be empty, have a black stone, or a white stone. This is like
    our tic-tac-toe board, but unfathomably bigger. We’d need a table whose rows would
    have labels requiring 173 digits. Such numbers are not just wildly impractical,
    they’re incomprehensible.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 存储这个游戏的表格并不是现实可行的，但想要这么做是完全合理的。更为合理的情况是，我们可能想玩围棋。围棋的标准棋盘是一个 19×19 的交叉点网格，每个交叉点可以为空、黑子或白子。这就像我们的井字游戏棋盘，但大得无法想象。我们需要一个行标签需要
    173 位数字的表格。这样的数字不仅完全不实际，简直是无法理解的。
- en: Yet this is the basic strategy that was used by the Deep Mind team to build
    AlphaGo, which famously beat a world champion human player (DeepMind 2020). They
    did it by combining reinforcement learning with deep learning. One of the key
    insights in this *deep reinforcement learning* approach was to eliminate explicit
    storage of the Q-table. We can think of the table as a function that takes a board
    state as input and returns a move number and Q-value as output. As we’ve seen,
    neural networks are great at learning how to predict things like this.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这正是 Deep Mind 团队用来构建 AlphaGo 的基本策略，AlphaGo famously 击败了世界冠军人类玩家（DeepMind
    2020）。他们通过将强化学习与深度学习相结合来实现这一目标。这种*深度强化学习*方法的一个关键见解是消除 Q-table 的显式存储。我们可以将这个表格看作是一个函数，它以棋盘状态为输入，返回一个动作编号和
    Q 值作为输出。正如我们所见，神经网络在预测这种事情上非常擅长。
- en: We can build a deep learning system that takes the board input and predicts
    the Q-value we’d get for each move if we really did keep the table around. With
    enough training, this network can become accurate enough that we can abandon the
    Q-table and use just the network. Training a system like this can be challenging,
    but it can be done, with excellent results (Mnih et al. 2013; Matiisen 2015).
    Deep reinforcement learning has been applied to fields as diverse as video games,
    robotics, and even healthcare (François-Lavet et al. 2018). It’s also the central
    algorithm behind AlphaZero, arguably the best player of the game Go that has ever
    existed (Silver et al. 2017; Hassabis and Silver 2017; Craven and Page 2018).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以构建一个深度学习系统，接受棋盘输入，并预测如果我们继续保持棋盘存在，每一步的Q值。通过足够的训练，这个网络可以变得足够准确，以至于我们可以放弃Q表格，单纯使用网络。训练这样的系统可能是具有挑战性的，但是可以做到的，且效果非常好（Mnih
    et al. 2013；Matiisen 2015）。深度强化学习已被应用于视频游戏、机器人学，甚至医疗保健等领域（François-Lavet et al.
    2018）。它还是AlphaZero背后的核心算法，AlphaZero无疑是有史以来围棋的最佳玩家（Silver et al. 2017；Hassabis
    and Silver 2017；Craven and Page 2018）。
- en: Reinforcement learning has an advantage over supervised learning because it
    does not require a database that has been manually labeled, which is often a time-consuming
    and expensive process. On the other hand, it requires us to design an algorithm
    for creating rewards that guide an agent toward the desired behavior. In complex
    situations, this can be a difficult problem to solve.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习相较于监督学习的一个优势在于，它不需要手动标注的数据库，而人工标注通常是一个既费时又昂贵的过程。另一方面，它要求我们设计一个奖励生成算法，引导代理朝着期望的行为前进。在复杂的情况下，这可能是一个难以解决的问题。
- en: This has necessarily been a high-level overview of a big topic. Much more information
    on reinforcement learning can be found in dedicated references (François-Lavet
    et al. 2018; Sutton and Baro 2018).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这必然是一个关于一个大主题的高层次概述。更多关于强化学习的信息可以在专门的参考资料中找到（François-Lavet et al. 2018；Sutton
    and Baro 2018）。
- en: Summary
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we took a look at some of the basic ideas in reinforcement learning,
    or RL. We saw that the basic idea is to break the world into an agent who acts,
    and an environment that encompasses everything else. The agent is given a list
    of options, and using a policy, it selects one. The environment executes that
    action, along with follow-on effects (which can include making a return move in
    a game, or carrying out a simulation or real-world action), and then returns to
    the agent a reward describing the quality of its chosen action. Typically the
    reward describes how well the agent has succeeded in improving the environment
    in some way.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了强化学习（RL）的一些基本概念。我们看到，强化学习的基本思想是将世界分为一个行动的**代理**和一个包含其他一切的**环境**。代理被赋予一系列选项，并根据一个策略选择其中一个。环境执行该动作，并产生后续效果（这可能包括在游戏中做出回合动作，或进行模拟或现实世界中的操作），然后返回一个奖励，描述代理所选动作的质量。通常，奖励描述了代理在某种程度上改善环境的成功程度。
- en: We applied these ideas to the one-player game of Flippers with a simple algorithm
    that recorded the rewards in a table, and used a simple policy to select the move
    with the highest reward when possible. We saw that this didn’t handle the unpredictability
    of the real world very well, so we improved the method into the Q-learning algorithm
    with a better update rule and learning policy.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些思想应用于单人游戏《Flippers》，使用一种简单的算法将奖励记录在表格中，并使用一个简单的策略在可能的情况下选择具有最高奖励的动作。我们看到，这种方法并不能很好地应对现实世界的不可预测性，因此我们将其改进为具有更好更新规则和学习策略的Q学习算法。
- en: Then we improved that method again by prechoosing our next move, resulting in
    the SARSA algorithm. This learned to play Flippers even better.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们通过预先选择下一个动作再次改进了该方法，得出了SARSA算法。这个算法学会了更好地玩《Flippers》。
- en: In practice, a vast number of algorithms fall under the category of reinforcement
    learning, with more arriving all the time. It’s a vibrant field of research and
    development.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，很多算法都属于强化学习范畴，并且新的算法不断涌现。它是一个充满活力的研究与开发领域。
- en: In the next chapter, we’ll look at a powerful method for training generators
    that can produce images, video, audio, text, and other kinds of data so well that
    we can’t reliably distinguish generated data from data in the training set.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨一种强大的方法，用于训练生成器，可以生成图像、视频、音频、文本及其他类型的数据，生成的数据与训练集中的数据难以区分。
