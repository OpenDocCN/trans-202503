<html><head></head><body>
<h2 class="h2" id="ch14"><span epub:type="pagebreak" id="page_381"/><strong><span class="big">14</span><br/>MASS STORAGE DEVICES AND FILESYSTEMS</strong></h2>&#13;
<div class="image1"><img alt="Image" src="../images/comm1.jpg"/></div>&#13;
<p class="noindents">The most prevalent I/O device on modern computers is probably the mass storage device. Whereas some PCs don’t have a display (they’re operated <em>headlessly</em>), or even a keyboard or mouse (they’re accessed remotely), almost every computer system recognizable as a PC has a mass storage device of some sort. This chapter will focus on the types of mass storage devices—hard drives, floppy disks, tape drives, flash drives, solid state drives, and more—as well as the special filesystem format they use to organize the data they store.</p>&#13;
<h3 class="h3" id="sec14_1"><strong>14.1 Disk Drives</strong></h3>&#13;
<p class="noindent">Almost all modern computer systems include some sort of disk drive unit to provide online mass storage. At one time, certain workstation vendors produced <em>diskless workstations</em>, but the relentless drop in price and increasing storage space of fixed (aka “hard”) disk and solid-state drive (SSD) <span epub:type="pagebreak" id="page_382"/>units have all but obliterated the diskless computer system. Disk drives are so ubiquitous in modern systems that most people take them for granted. However, it’s dangerous for a programmer to take a disk drive for granted. Software constantly interacts with the disk drive as a medium for application file storage, so it’s very important to understand how disk drives operate if you want to write efficient code.</p>&#13;
<h4 class="h4" id="sec14_1_1"><strong><em>14.1.1 Floppy Disk Drives</em></strong></h4>&#13;
<p class="noindent">Floppy disks have all but disappeared from today’s PCs. Their limited storage capacity (typically 1.44MB) is far too small for modern applications and the data they produce. It’s hard to believe that at the beginning of the PC revolution a 143KB (that’s <em>kilo</em>bytes, not megabytes or gigabytes) floppy drive was considered a high-ticket item. However, floppy disk drives have failed to keep up with technological advances in the computer industry. Therefore, we won’t consider them further in this chapter.</p>&#13;
<h4 class="h4" id="sec14_1_2"><strong><em>14.1.2 Hard Drives</em></strong></h4>&#13;
<p class="noindent">The fixed disk drive, more commonly known as the hard drive, is the most common mass storage device in use today (though, as of 2020, SSDs are rapidly replacing hard drives). The modern hard drive is truly an engineering marvel. Between 1982 and 2020, the capacity of a single drive unit has increased over 2,400,000-fold, from 5MB to over 16TB (terabytes). At the same time, the minimum price for a new unit has dropped from $2,500 (US) to below $50. No other component in the computer system has enjoyed such a radical increase in capacity and performance along with a comparable drop in price. (Semiconductor RAM probably comes in second: paying the 1982 price today would get you about 40,000 times the capacity.)</p>&#13;
<p class="indent">While hard drives were decreasing in price and increasing in capacity, they were also becoming faster. In the early 1980s, a hard-drive subsystem was doing well to transfer 1MBps between the drive and the CPU’s memory; modern hard drives can transfer more than 2,500MBps.<sup><a href="footnotes.xhtml#fn14_1a" id="fn14_1">1</a></sup> While this increase in performance isn’t as great as that of memory or CPUs, keep in mind that disk drives are mechanical units on which the laws of physics place greater limitations. In some cases, the dropping cost of hard drives has allowed system designers to improve their performance by using disk arrays (see “<a href="#sec14_1_3">RAID Systems</a>” on page <a href="#sec14_1_3">388</a> for details). By using certain hard-disk subsystems like disk arrays, you could achieve 2500MBps (or better) transfer rates, though it’s not especially cheap to do so.</p>&#13;
<p class="indent">Hard drives are so named because their data is stored on a small, rigid disk that is usually made out of aluminum or glass and is coated with a magnetic material. Floppy disks, in contrast, store their information on a thin piece of flexible Mylar plastic.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_383"/>In disk-drive terminology, the small aluminum or glass disk is known as a <em>platter</em>. Each platter has two surfaces, front and back (or top and bottom), both of which have the magnetic coating. During operation, the hard-drive unit spins this platter at a particular speed, which these days is usually 3,600; 5,400; 7,200; 10,000; or 15,000 revolutions per minute (RPM). Generally, though not always, the faster the platter spins, the faster the data is read from the disk and the higher the data transfer rate between the disk and the system. The smaller disk drives in laptop computers typically spin at much slower speeds, like 2,000 or 4,000 RPM, to conserve battery life and generate less heat.</p>&#13;
<p class="indent">A hard-disk subsystem contains two main active components: the disk platter(s) and the read/write head. The read/write head, when held stationary, floats above concentric circles, or <em>tracks</em>, on the disk surface. Each track is broken up into a sequence of sections known as <em>sectors</em> or <em>blocks</em>. The actual number of sectors varies by drive design, but a typical hard drive has between 32 and 128 sectors per track (see <a href="ch14.xhtml#ch14fig01">Figure 14-1</a>). Each sector typically holds between 256 and 4,096 bytes of data. Many disk-drive units let the OS choose between several different sector sizes, the most common being 512 bytes and 4,096 bytes.</p>&#13;
<div class="image"><img alt="image" src="../images/14fig01.jpg"/></div>&#13;
<p class="figcap"><a id="ch14fig01"/><em>Figure 14-1: Tracks and sectors on a hard-disk platter</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_384"/>The disk drive records data when the read/write head sends a series of electrical pulses to the platter, which translates them into magnetic pulses that the platter’s magnetic surface retains. The frequency at which the disk controller can record these pulses is limited by the quality of the electronics, the read/write head design, and the quality of the magnetic surface.</p>&#13;
<p class="indent">The magnetic medium is capable of recording two adjacent bits on its disk surface and then differentiating between them during a later read operation. However, as you record bits closer and closer together, it becomes increasingly difficult to differentiate between them in the magnetic domain. <em>Bit density</em> is a measure of how closely a particular hard disk can pack data into its tracks—the higher the bit density, the more data you can squeeze onto a single track. However, recovering densely packed data requires faster and more expensive electronics.</p>&#13;
<p class="indent">The bit density has a big impact on the performance of the drive. If the drive’s platters are rotating at a fixed number of RPM, then the higher bit density, the more bits will rotate underneath the read/write head over a certain duration. Larger disk drives tend to be faster than smaller disk drives because they employ a higher bit density.</p>&#13;
<p class="indent">By moving the disk’s read/write head in a roughly linear path from the center of the disk platter to the outside edge, the system can position a single read/write head over any one of several thousand tracks. Yet the use of only one read/write head means that it will take a fair amount of time to move the head among the disk’s many tracks. Indeed, two of the most cited hard-disk performance parameters are the read/write head’s average seek time and track-to-track seek time.</p>&#13;
<p class="indent">The <em>average seek time</em> is half the amount of time it takes to move the read/write head from the edge of the disk to the center, or vice versa. A typical high-performance disk drive has an average seek time between 5 and 10 milliseconds. On the other hand, its <em>track-to-track seek time</em>—that is, the amount of time it takes to move the disk head from one track to the next—is on the order of 1 or 2 milliseconds. From these numbers, you can see that the acceleration and deceleration of the read/write head consumes a much greater percentage of the track-to-track seek time than of the average seek time. It takes only 20 times longer to traverse 1,000 tracks than it does to move to the next track. And because moving the read/write heads from one track to the next is usually the most common operation, the track-to-track seek time is probably a better indication of the disk’s performance. Regardless of which metric you use, however, keep in mind that moving the disk’s read/write head is one of the most expensive operations you can do on a disk drive, so it’s something you want to minimize.</p>&#13;
<p class="indent">Because most hard-drive subsystems record data on both sides of a disk platter, there are two read/write heads associated with each platter—one for the top and one for the bottom. And because most hard drives incorporate multiple platters in their disk assembly in order to increase storage capacity (see <a href="ch14.xhtml#ch14fig02">Figure 14-2</a>), a typical drive has multiple pairs of read/write heads.</p>&#13;
<div class="image"><img alt="image" src="../images/14fig02.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_385"/><a id="ch14fig02"/><em>Figure 14-2: Multiple-platter hard-disk assembly</em></p>&#13;
<p class="indent">The various read/write heads are physically connected to the same actuator. Therefore, each head sits above the same track on its respective platter, and all the heads move across the disk surfaces as a unit. The set of all tracks over which the read/write heads are currently sitting is known as a <em>cylinder</em> (see <a href="ch14.xhtml#ch14fig03">Figure 14-3</a>).</p>&#13;
<div class="image"><img alt="image" src="../images/14fig03.jpg"/></div>&#13;
<p class="figcap"><a id="ch14fig03"/><em>Figure 14-3: A hard-disk cylinder</em></p>&#13;
<p class="indent">Although using multiple heads and platters increases the cost of a hard-disk drive, it also improves the performance. The performance boost occurs when data the system needs isn’t located on the current track. In a hard-disk subsystem with only one platter, the read/write head would need to move to another track to locate the data. But in a subsystem with multiple platters, the next block of data to read is usually located within the same cylinder. And because the hard-disk controller can quickly switch between read/write heads electronically, doubling the number of platters in a disk <span epub:type="pagebreak" id="page_386"/>subsystem nearly doubles the disk unit’s track-to-track seek performance because it winds up doing half the number of seek operations. Of course, increasing the number of platters also increases the unit’s capacity, which is another reason why high-capacity drives are often higher-performance drives as well.</p>&#13;
<p class="indent">With older disk drives, when the system wants to read a particular sector from a particular track on one of the platters, it commands the disk to position the read/write head over the appropriate track, and the disk drive then waits for the desired sector to rotate underneath. But by the time the head settles down, there’s a chance that the desired sector has just passed under the head, in which case the disk has to wait for almost one complete rotation before it can read the data. On average, the desired sector appears halfway across the disk. If the disk is rotating at 7,200 RPM (120 revolutions per second), it requires 8.333 milliseconds for one complete rotation of the platter. Typically, 4.2 milliseconds will pass before the sector rotates underneath the head. This delay is known as the <em><a href="gloss01.xhtml#gloss01_22">average rotational latency</a></em> of the drive, and it is usually equal to the time needed for one rotation, divided by 2.</p>&#13;
<p class="indent">To see how average rotational latency can be a problem, consider that an OS usually manipulates disk data in sector-sized chunks. For example, when reading data from a disk file, the OS typically requests that the disk subsystem read a sector of data and return that data. Upon receiving the data, the OS processes it and then very likely makes a request for additional data from the disk. But what happens when this second request is for data located on the next sector of the current track? Unfortunately, while the OS is processing the first sector’s data, the disk platters are still moving underneath the read/write heads. If the OS wants to read the next sector on the disk’s surface but doesn’t notify the drive immediately after reading the first sector, the second sector will rotate underneath the read/write head. When this happens, the OS will have to wait for almost a complete disk rotation before it can read the second sector. This is known as <em>blowing revs</em> (revolutions). If the OS (or application) is constantly blowing revs when reading data from a file, filesystem performance suffers dramatically. In early “single-tasking” OSes running on slower machines, blowing revs was an unpleasant fact of life. If a track had 64 sectors, it would often take 64 revolutions of the disk in order to read all the data on a single track.</p>&#13;
<p class="indent">To combat this problem, the disk-formatting routines for older drives allow the user to interleave sectors. <em>Interleaving</em> is the process of spreading out sectors within a track so that logically adjacent sectors are not physically adjacent on the disk surface (see <a href="ch14.xhtml#ch14fig04">Figure 14-4</a>).</p>&#13;
<p class="indent">The advantage of interleaving sectors is that once the OS reads a sector, it will take a full sector’s rotation time before the logically adjacent sector moves under the read/write head. This gives the OS time to do some processing and to issue a new disk I/O request before the desired sector moves underneath the head. However, in modern multitasking OSes, it’s difficult to guarantee that an application will gain control of the CPU so that it can respond before the next logical sector moves under the head, so interleaving isn’t very effective.</p>&#13;
<div class="image"><img alt="image" src="../images/14fig04.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_387"/><a id="ch14fig04"/><em>Figure 14-4: Interleaving sectors</em></p>&#13;
<p class="indent">To solve this problem, as well as improve disk performance in general, most modern disk drives include memory on the disk controller that allows it to read data from an entire track in one disk revolution. Once it caches the track data in memory, the controller can communicate disk read/write operations at RAM speed rather than at disk rotation speeds, which can dramatically improve performance. Reading the first sector from a track still exhibits rotational latency, but once the disk controller reads the entire track, the latency is all but eliminated for that track.</p>&#13;
<p class="indent">A typical track may have 64 sectors of 512 bytes each, for a total of 32KB per track. Because newer disks usually have between 8MB and 512MB of on-controller memory, the controller can buffer hundreds of tracks in its memory. Therefore, the disk controller cache improves not only the performance of disk read/write operations on a single track, but also overall disk performance. Note that the disk controller cache speeds up read operations <em>and</em> write operations. For example, the CPU can often write data to the disk controller’s cache memory within a few microseconds and then return to normal data processing while the disk controller moves the disk read/write heads into position. When the disk heads are finally in position at the appropriate track, the controller can write the data from the cache to the disk surface.</p>&#13;
<p class="indent">From an application designer’s perspective, advances in disk subsystem design have reduced the need to understand how disk-drive geometries (track and sector layouts) and disk-controller hardware affect the application’s performance. Despite these attempts to make the hardware transparent to the application, though, software engineers wanting to write great <span epub:type="pagebreak" id="page_388"/>code must always remain cognizant of the disk drive’s underlying operation. For example, it’s valuable to know that sequential file operations are usually much faster than random-access operations because sequential operations require fewer head seeks. Also, if you know that a disk controller has an on-board cache, you can write file data in smaller blocks, doing other processing between the block operations, to give the hardware time to write the data to the disk surface. Though the techniques early programmers used to maximize disk performance don’t apply to modern hardware, by understanding how disks operate and how they store their data, you can avoid various pitfalls that produce slow code.</p>&#13;
<h4 class="h4" id="sec14_1_3"><strong><em>14.1.3 RAID Systems</em></strong></h4>&#13;
<p class="noindent">Because a modern disk drive typically has between 8 and 16 heads, you might wonder if you could improve performance by simultaneously reading or writing data on multiple heads. While this is certainly possible, it really didn’t happen until SATA and larger disk caches came along. But there’s yet another way to improve disk drive performance using parallel read and write operations—the <em>redundant array of inexpensive disks (RAID)</em> configuration.</p>&#13;
<p class="indent">The RAID concept is quite simple: you connect multiple hard-disk drives to a special host controller card (sometimes known as an <em>adapter</em>), which simultaneously reads and writes the various disk drives. By hooking up two disk drives to a RAID controller card, you can read and write data about twice as fast as you could with a single disk drive. By hooking up four disk drives, you can improve average performance by almost a factor of 4.</p>&#13;
<p class="indent">RAID controllers support different configurations depending on the purpose of the disk subsystem. So-called <em><a href="gloss01.xhtml#gloss01_209">RAID 0</a></em> subsystems use multiple disk drives simply to increase the data transfer rate. If you connect two 150GB disk drives to a RAID controller, you’ll produce the equivalent of a 300GB disk subsystem with double the data transfer rate. This is a typical configuration for personal RAID systems—those systems that are not installed on a file server.</p>&#13;
<p class="indent">Many high-end file-server systems are <em>RAID 1</em> (and higher) subsystems that store multiple copies of the data across the multiple disk drives, rather than increasing the data transfer rate between the system and the disk drive. In such configurations, should one disk fail, a copy of the data is still available on another disk drive. Some even higher-level RAID subsystems combine four or more disk drives to increase the data transfer rate and provide redundant data storage. This type of configuration usually appears on high-end, high-availability file server systems.</p>&#13;
<p class="indent">Modern RAID system configurations can be categorized as follows:</p>&#13;
<p class="uln-indent"><strong>RAID 0</strong> Interleaves data across all disks to increase performance (at the expense of reliability). This is known as <em>striping</em>. Requires a minimum of two disks.</p>&#13;
<p class="uln-indent"><strong>RAID 1</strong> Replicates data on pairs of drives to increase reliability (at the cost of performance; also cuts in half the total amount of storage available). Allows failure of at least one drive without data loss (depending on the drives that fail, could support two or more drive failures). <span epub:type="pagebreak" id="page_389"/>Requires an even number of drives, with a minimum of two disks. This is known as <em>mirroring</em>.</p>&#13;
<p class="uln-indent"><strong>RAID 5</strong> Stores parity information on the drives. Faster than RAID 1, slower than RAID 0. Allows failure of one drive without data loss. Requires a minimum of three drives. At three drives, 66 percent of the total storage is available for data; any drives you add beyond three increase data storage by the size of the added drive.</p>&#13;
<p class="uln-indent"><strong>RAID 6</strong> Stores duplicate parity information across the drives. Faster than RAID 1, slower than RAID 0 and 5. Allows failure of two drives without data loss. Requires a minimum of four drives. At four drives, half the total storage is available for data, but any drives you add beyond four increase system storage by the size of the added drive.</p>&#13;
<p class="uln-indent"><strong>RAID 10</strong> Combination of RAID 1 + RAID 0. Minimum four drives; expansion has to be in pairs of drives. Interleaved (striped) data across drives to speed up performance, plus redundant storage on pairs of drives for reliability. Faster than RAID 1 (but slower than RAID 0).</p>&#13;
<p class="uln-indent"><strong>RAID 50</strong><strong>, 60</strong> Combination of RAID 5 + RAID 0 or RAID 6 + RAID 0.</p>&#13;
<p class="indent">There are other RAID combinations (like 2, 3, and 4), but most are obsolete and you won’t find them in use in modern systems.</p>&#13;
<p class="indent">RAID systems enable you to dramatically increase disk subsystem performance without having to purchase exotic and expensive mass storage solutions. Though a software engineer can’t assume that every computer system in the world has a fast RAID subsystem available, for those applications that demand the absolute highest-performance storage subsystem, RAID (possibly using SSDs) could be a solution.</p>&#13;
<h4 class="h4" id="sec14_1_4"><strong><em>14.1.4 Optical Drives</em></strong></h4>&#13;
<p class="noindent">An optical drive uses a laser beam and a special photosensitive medium to record and play back digital data. Optical drives have a few advantages over hard-disk subsystems that use magnetic media:</p>&#13;
<ul>&#13;
<li class="noindent">They are more shock resistant, so banging the disk drive around during operation won’t destroy the drive unit as easily as it would a hard disk.</li>&#13;
<li class="noindent">The medium is usually removable, allowing you to maintain an almost unlimited amount of offline or near-line storage.</li>&#13;
<li class="noindent">They’re fairly high-capacity (though modern USB memory sticks and SD cards have greater capacities).</li>&#13;
</ul>&#13;
<p class="indent">At one time, optical storage systems appeared to be the wave of the future because they offered very high storage capacity in a small space. Unfortunately, they have fallen out of favor in all but a few niche markets because they also have several drawbacks:</p>&#13;
<ul>&#13;
<li class="noindent">While their read performance is okay, their write speed is very slow—an order of magnitude slower than a hard drive and only a few times faster than a <em>floptical</em> (older combined magnetic/optical floppy) drive.</li>&#13;
<li class="noindent"><span epub:type="pagebreak" id="page_390"/>Although the optical medium is far more robust than the magnetic medium, the magnetic medium in a hard drive is usually sealed away from dirt, humidity, and abrasion. In contrast, optical media is easily accessible to someone who really wants to do damage to the disk’s surface.</li>&#13;
<li class="noindent">Seek times for optical-disk subsystems are much slower than for magnetic disks.</li>&#13;
<li class="noindent">Optical disks have limited storage capacity, currently less than about 128GB (Blu-ray).</li>&#13;
</ul>&#13;
<p class="indent">Ultimately, the low price and increasing capacity of USB flash drives killed off optical drives for personal computer use.</p>&#13;
<p class="indent">One area where optical-disk subsystems are still in use, however, is in <em>near-line storage subsystems</em>, which typically use a robotic jukebox to manage hundreds or thousands of optical disks. Although you could argue that a rack of high-capacity hard-disk drives would provide a more space-efficient storage solution, it would consume far more power, generate far more heat, and require a more sophisticated interface than an optical jukebox, which usually has only a single optical-drive unit and a robotic disk-selection mechanism. For archival storage, where the server system rarely needs access to any particular piece of data in the storage subsystem, a jukebox system is a very cost-effective solution.</p>&#13;
<p class="indent">If you wind up writing software that manipulates files on an optical-drive subsystem, the most important thing to remember is that read access is much faster than write access. You should try to use the optical system as a “read-mostly” device and avoid writing data as much as possible to the device. You should also avoid random access on an optical disk’s surface, as seek times are very slow.</p>&#13;
<p class="indent">CD, DVD, and Blu-ray drives are also optical drives. However, because of their widespread use, and their sufficiently different organization and performance when compared with standard optical drives, they warrant a separate discussion.</p>&#13;
<h4 class="h4" id="sec14_1_5"><strong><em>14.1.5 CD, DVD, and Blu-ray Drives</em></strong></h4>&#13;
<p class="noindent">CD-ROM was the first optical drive subsystem to gain wide acceptance in the personal computer market. CD-ROM disks were based on the audio CD digital recording standard, and they provided a large amount of storage (650MB) when compared to hard-disk-drive storage capacities at the time (typically 100MB). As time passed, of course, this relationship reversed. Still, CD-ROMs became the preferred distribution vehicle for most commercial applications, completely replacing the floppy-disk medium for this purpose.</p>&#13;
<p class="indent">Although the CD-ROM format is a very inexpensive distribution medium in large quantities, often costing only a few cents per disk, it’s not appropriate for small production runs. The problem is that it typically costs several hundreds or thousands of dollars to produce a disk master (from which the run of CD-ROMs are made), meaning that CD-ROM is usually cost-effective only when the quantity of disks being produced is at least in the thousands.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_391"/>The solution was a new CD medium, CD-Recordable (CD-R), which allowed the production of one-off CD-ROMs. CD-R uses a write-once optical disk technology, known euphemistically as <em>WORM</em> (write-once, read-many). When first introduced, CD-R disks cost about $10 to $15. However, once the drives reached critical mass and media manufacturers began producing blank CD-R disks in huge quantities, their bulk retail price fell to about $0.25. As a result, CD-R made it possible to distribute a fair amount of data in small quantities.</p>&#13;
<p class="indent">One obvious drawback to CD-R is the “write-once” limitation. To overcome it, the CD-Rewriteable (CD-RW) drive and medium were created. CD-RW, as its name suggests, supports both reading and writing. Unlike with optical disks, however, you can’t simply rewrite a single sector on CD-RW. Instead, to rewrite the data on a CD-RW disk, you must first erase the whole disk.</p>&#13;
<p class="indent">Although the 650MB of storage on a CD seemed like a gargantuan amount when CDs were first introduced, the old maxim that data and programs expand to fill up all available space certainly held true. Though CDs were ultimately expanded to 700MB, various games (with embedded video), large databases, developer documentation, programmer development systems, clip art, stock photographs, and even regular applications reached the point where a single CD was woefully inadequate. The DVD-ROM (and later, DVD-R, DVD-RW, DVD+RW, and DVD-RAM) disk reduced this problem by offering between 3GB and 17GB of storage on a single disk. Except for the DVD-RAM format, you can view the DVD formats as faster, higher-capacity versions of the CD formats. There are some clear technical differences between the two, but most of them are transparent to the software. Today, Blu-ray optical discs deliver up to 128GB of storage (Blu-ray BDXL). However, electronic distribution via the internet has largely replaced physical media, so Blu-ray discs have never become as popular as distribution or storage media.</p>&#13;
<p class="indent">The CD and DVD formats were created for reading data in a continuous stream—<em>streaming</em> data—from the storage medium. The track-to-track head movement time required to read data stored on a hard disk creates a big gap in the streaming sequence, which is unacceptable for audio and video applications. CDs and DVDs record information on a single, very long track that forms a spiral across the surface of the whole disk. Thus, the CD or DVD player can continuously read the data simply by moving the laser beam along the disk’s single spiral track at a constant rate.</p>&#13;
<p class="indent">Although having a single track is great for streaming data, it does make it a bit more difficult to locate a specific sector on the disk. The CD or DVD drive can only approximate a sector’s position by mechanically positioning the laser beam to some point on the disk. Next, it must actually read data from the disk surface to determine where the laser is positioned, and then do some fine-tuning to locate the desired sector. As a result, searching for a specific sector on a CD or DVD disk can take an order of magnitude longer than searching for a specific sector on a hard disk.</p>&#13;
<p class="indent">The most important thing to remember for a programmer writing code that interacts with CD or DVD media is that random access is verboten. <span epub:type="pagebreak" id="page_392"/>These media were designed for sequential streaming access, and seeking data on such media will hinder your application performance. If you’re using these disks to deliver your application and its data to the end user, you should have the user copy the data to a hard disk before use if high-performance random access is necessary.</p>&#13;
<h3 class="h3" id="sec14_2"><strong>14.2 Tape Drives</strong></h3>&#13;
<p class="noindent">Tape drives were also popular mass storage devices. Traditionally, PC owners used tape drives to back up data stored on hard-disk drives back in the days when hard drives were much smaller. For many years, tape storage was far more cost-effective than hard-disk storage on a cost-per-megabyte basis. Indeed, at one time there was an order of magnitude difference in cost per megabyte between tape storage and magnetic disk storage. And because tape drives held more data than most hard-disk drives, they were more space-efficient too.</p>&#13;
<p class="indent">However, because of competition and technological advances in the hard-disk-drive marketplace, tapes have lost these advantages. Hard-disk drives now exceed 16TB in storage, and the optimum price point for hard disks is about $0.25 per gigabyte. Tape storage today costs far more per megabyte than hard-disk storage. Plus, only a few tape technologies allow you to store 250GB on a single tape, and those that do (such as Digital Linear Tape, or DLT) are extremely expensive. It’s not surprising that tape drives are seeing less and less use these days in home PCs and are typically found only in larger file server machines. Linear Tape-Open (LTO) drives extend the capacity to around 12TB (expected to increase to around 200TB in the future). Nevertheless, today a typical LTO-8 tape costs almost $130 (US), about half the price per megabyte of a hard drive.</p>&#13;
<p class="indent">Back in the days of mainframes, application programs interacted with tape drives in much the same way that today’s applications interact with hard-disk drives. A tape drive, however, is not an efficient random-access device. That is, although software can read a random set of blocks from a tape, it cannot do so with acceptable performance. Of course, in the days when most applications ran on mainframes, applications generally were not interactive, and CPUs were much slower; thus, the standard for “acceptable performance” was different.</p>&#13;
<p class="indent">In a tape drive, the read/write head is fixed, and the tape transport mechanism moves the tape past the head linearly, from the beginning of the tape to the end, or vice versa. If the beginning of the tape is currently positioned over the read/write head and you want to read data at the end of the tape, you have to move the entire tape past the head to get to the desired data. This can be very slow, requiring tens or even hundreds of seconds, depending on the length and format of the tape. Compare this with the tens of milliseconds it takes to reposition a hard disk’s read/write head (or the negligible time it takes to get data from an SSD). Therefore, to perform well on a tape drive, software must be written to account for the <span epub:type="pagebreak" id="page_393"/>limitations of a sequential access device. In particular, data should be read or written sequentially on a tape.</p>&#13;
<p class="indent">Originally, data was written to tapes in blocks (much like sectors on a hard disk), and the drives were designed to allow quasi-random access to the tape’s blocks. If you’ve ever watched old movies that used the reel-to-reel drives, with the reels constantly stopping, starting, stopping, reversing, stopping, and continuing, you’ve seen “random access” in action. Such tape drives were very expensive because they required powerful motors, finely tooled tape-path mechanisms, and so on. As hard drives became larger and less expensive, applications stopped using tape as a data manipulation medium and used it only for offline storage (to back up data from hard disks).</p>&#13;
<p class="indent">Because sequential data access on tape does not require the heavy-duty mechanics of the original tape drives, tape-drive manufacturers sought to make a lower-cost product suitable for sequential access only. Their solution was the <em>streaming tape drive</em>, which was designed to keep the data constantly moving from the CPU to the tape, or vice versa. For example, while backing up the data from a hard disk to tape, a streaming tape drive treats the data like a video or audio recording and just lets the tape run, constantly writing the data from the hard disk to the tape. Because of the way streaming tape drives work, very few applications deal directly with the tape unit. Today, it’s very rare for anything other than a tape backup utility program, run by the system administrator, to access the tape hardware.</p>&#13;
<h3 class="h3" id="sec14_3"><strong>14.3 Flash Storage</strong></h3>&#13;
<p class="noindent">An interesting storage medium that has become popular because of its compact form factor<sup><a href="footnotes.xhtml#fn14_2a" id="fn14_2">2</a></sup> is flash storage. The flash medium is actually a semiconductor device, based on <em>electrically erasable programmable read-only memory (EEPROM)</em> technology, which, despite its name, is both readable and writable. Unlike regular semiconductor memory, flash storage is <em>nonvolatile</em>, meaning it maintains its data even when disconnected from power. Like other semiconductor technologies, flash storage is purely electronic and doesn’t require any motors or other electromechanical devices for proper operation. Therefore, flash storage devices are more reliable and shock resistant, and they use far less power than mechanical storage solutions such as disk drives. This makes flash storage especially valuable in portable battery-powered devices like cell phones, tablets, laptop computers, electronic cameras, MP3 playback devices, and recorders.</p>&#13;
<p class="indent">Flash storage modules now provide in excess of 1TB of storage, and their optimal price point is about $0.15 (US) per gigabyte. This makes them comparable, per bit, to hard-disk storage.</p>&#13;
<p class="indent">Flash devices are sold in many different form factors. OEMs (original equipment manufacturers) can buy flash storage devices that look like other semiconductor chips and mount them directly on their circuit boards. However, most flash memory devices sold today are built into one <span epub:type="pagebreak" id="page_394"/>of several standard forms, including SDHC cards, CompactFlash cards, smart-memory modules, memory sticks, USB/flash modules, or SSDs. For example, you might remove a CompactFlash card from your camera, insert it into a special CompactFlash card reader on your PC, and access your photographs just as you would files on a disk drive.</p>&#13;
<p class="indent">Memory in a flash storage module is organized in blocks of bytes, not unlike sectors on a hard disk. In contrast to regular semiconductor memory or RAM, however, you can’t write individual bytes in a flash storage module. Although you can generally <em>read</em> an individual byte from a flash storage device, to write to a particular byte you must first erase the entire block on which it resides. The block size varies by device, but most OSes treat these flash blocks like a disk sector for the purposes of reading and writing. Although the basic flash storage device itself could connect directly to the CPU’s memory bus, most common flash storage packages (such as CompactFlash cards and memory sticks) contain electronics that simulate a hard-disk interface, and you access the flash device just as you would a hard-disk drive.</p>&#13;
<p class="indent">One interesting aspect to flash memory devices, and EEPROM devices in general, is that they have a limited write lifetime. That is, you can write to a particular memory cell in a flash memory module only a certain number of times before that cell begins to have problems retaining the information. This was a big concern in early EEPROM/flash devices, because the average number of write cycles before failures began occurring was around 10,000. That is, if some software wrote to the same memory block 10,000 times in a row, the EEPROM/flash device would probably develop a bad memory cell in that block, effectively rendering the entire chip useless. On the other hand, if the software wrote just once to 10,000 separate blocks, the device could still take 9,999 more writes to each memory cell. Therefore, the OSes of these early devices would try to spread out write operations across the entire device to minimize damage. Although modern flash devices still exhibit this problem, technological advances have reduced it almost to the point where we can ignore it. A modern flash memory cell supports an average of about a million write cycles before it will go bad. Furthermore, today’s OSes simply mark bad flash blocks, the same way they mark bad sectors on a disk, and will skip a block once they determine that it has gone bad.</p>&#13;
<p class="indent">Being electronic, flash devices do not exhibit rotational latency times at all, and they don’t exhibit much in the way of seek times either. There’s a tiny amount of time needed to write an address to a flash memory module, but it’s nothing compared to the head seek times on a hard disk. Despite this, flash memory is generally nowhere near as fast as typical RAM. Reading data from a flash device itself usually takes microseconds (rather than nanoseconds), and the interface between the flash memory device and the system may require additional time to set up a data transfer. In addition, it’s common to interface a flash storage module to a PC using a USB flash reader device, and this can further reduce the average read time per byte to hundreds of microseconds.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_395"/>Write performance is even worse. To write a block of data to flash, you must write the data, read it back, compare it to the original data, and rewrite it if they don’t match. This process can take several tens or even hundreds of milliseconds.</p>&#13;
<p class="indent">As a result, flash memory modules are generally quite a bit slower than high-performance hard-disk subsystems. However, thanks mainly to demand from high-end digital camera users who want to be able to snap as many pictures as possible in a short time, technological advances are boosting their performance. Though flash memory performance probably won’t catch up with hard-disk performance any time soon, you can expect it to continue improving over time.</p>&#13;
<h3 class="h3" id="sec14_4"><strong>14.4 RAM Disks</strong></h3>&#13;
<p class="noindent">Another interesting mass storage device is the RAM disk, a semiconductor solution that treats a large block of the computer system’s memory as though it were a disk drive, simulating blocks and sectors using memory arrays. The advantage of memory-based disks is that they are very high performance. RAM disks don’t suffer from the time delays associated with head seek time and rotational latency that you find on hard, optical, and floppy drives. Their interface to the CPU is also much faster, so data transfer times are very short, often running at the maximum bus speed. It’s hard to imagine a faster storage technology than a RAM disk.</p>&#13;
<p class="indent">RAM disks, however, have two disadvantages: cost and volatility. The cost per byte of storage in a RAM disk system is very high. Indeed, byte for byte, semiconductor storage is as much as 10,000 times more expensive than magnetic hard-disk storage. As a result, RAM disks usually have low storage capacities, typically no more than several gigabytes. And RAM disks are volatile—they lose their memory unless they are powered at all times. This generally means that semiconductor disks are great for storing temporary files and files you’ll copy back to some permanent storage device before shutting down the system. They are not particularly well suited for maintaining important information over long periods of time.</p>&#13;
<h3 class="h3" id="sec14_5"><strong>14.5 Solid-State Drives</strong></h3>&#13;
<p class="noindent">Modern high-performance PCs use solid-state drives (SSDs). SSDs use flash memory (like USB sticks) with a high-performance interface to the system. But SSDs aren’t simply USB flash drives in different clothing. USB flash drives are designed for low cost per bit—except for certain camera applications (particularly 4K and 8K camcorders), speed is secondary to cost and capacity. A typical USB flash drive, for example, is quite a bit slower than a mediocre hard drive. SSDs, on the other hand, must be fast. Because of their solid-state design, they’re typically an order of magnitude faster than rotating magnetic media. With a RAID configuration, SSDs can actually achieve the performance limits of SATA interfaces.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_396"/>As this was being written, SSDs cost between 4 and 16 times as much as high-capacity hard drives (8TB drives and 1TB SSDs both cost about $100 US). However, the price-per-gigabyte gap has been closing. SSDs are rapidly replacing rotating magnetic drives, and rotating magnetic media will likely be relegated to the trash bin of history (much like tape drives). Before that point, why would anyone pay more for an SSD?</p>&#13;
<p class="indent">SSDs typically use a different underlying technology to store data and provide a much faster electronic interface to the PC. This is why an SSD tends to be much more expensive than a USB flash drive. That’s also why SSDs can achieve 2,500MBps data transfer rates, while high-quality memory cards are capable of only around 100MBps (and USB flash/thumb drives are even worse).</p>&#13;
<p class="indent">From a programmer’s perspective, one of the big advantages of SSDs is that you no longer have to worry about seek times and other latency issues. SSDs tend to be true(r) random-access devices (at least when compared with hard drives). Accessing data at the beginning of the drive and then at the end takes only a little longer than accessing any pair of data elements elsewhere on the SSD.</p>&#13;
<p class="indent">There are a couple of disadvantages to SSDs, though. First of all, their write performance is usually much slower than their read performance (though writing to an SSD is still much faster than writing to a hard drive). Fortunately, data is read far more often than it is written, but this is something to consider when you’re working on software that writes data to a SSD. The second drawback is that SSDs wear out after a while. Writing to the same location over and over again will eventually cause the associated memory cell(s) to fail. Fortunately, modern OSes work around these failures. However, when you write applications that continuously overwrite file data, keep this issue in mind.</p>&#13;
<h3 class="h3" id="sec14_6"><strong>14.6 Hybrid Drives</strong></h3>&#13;
<p class="noindent">Most modern hard drives contain an on-board RAM cache (to hold entire tracks of data to eliminate rotational latency, for example). Hybrid drives, such as Apple’s older Fusion Drive, combine a small SSD with a large hard drive—typically a 32GB to 128GB SSD and a 2TB magnetic disk, in Apple’s case. Frequently accessed data stays in the SSD cache, and is swapped out to the hard drive when space is needed for new data. This works the same way as caching in main memory, boosting the system performance to near-SSD speeds for data that is accessed regularly.</p>&#13;
<h3 class="h3" id="sec14_7"><strong>14.7 Filesystems on Mass Storage Devices</strong></h3>&#13;
<p class="noindent">Very few applications access mass storage devices directly. That is, applications do not generally read and write tracks, sectors, or blocks on a mass storage device; instead, they open, read, write, and otherwise manipulate <em>files</em> on it. The OS’s <em>file manager</em> abstracts away the physical <span epub:type="pagebreak" id="page_397"/>configuration of the underlying storage device and provides a convenient storage facility for multiple independent files on a single device.</p>&#13;
<p class="indent">On the earliest computer systems, applications were responsible for tracking the physical location of data on a mass storage device, because there was no file manager available to do so. They were able to maximize their performance by carefully considering the layout of data on the disk. For example, they could manually interleave data across various sectors on a track to give the CPU time to process it between reading and writing those sectors on the track. Such software was often many times faster than comparable software using a generic file manager. Later, when file managers were commonly available, some application authors still managed their files on a storage device for performance reasons. This was especially true back in the days of floppy disks, when low-level software written to manipulate data at the track and sector level often ran 10 times faster than the same application using a file manager system.</p>&#13;
<p class="indent">In theory, today’s software could benefit from this approach as well, but in practice you rarely see this kind of low-level disk access in modern applications, for several reasons. First, writing software that manipulates a mass storage device at such a low level locks you into using that particular device. That is, if your software manipulates a disk with 48 sectors per track, 12 tracks per cylinder, and 768 cylinders per drive, that software will not work optimally (if at all) on a drive with a different sector, track, and cylinder layout. Second, accessing the drive at a low level makes it difficult to share the device among different applications, something that can be especially costly on a multitasking system that may have several applications sharing the device at once. For example, if you’ve laid out your data on various sectors on a track to coordinate computation time with sector access, your work is lost when the OS interrupts your program and gives some other application its time slice—time you were counting on to do any necessary computations prior to the next data sector rotating under the read/write head. Third, some of the features of modern mass storage devices, such as on-board caching controllers and SCSI interfaces that present a storage device as a sequence of blocks rather than as something with a given track and sector geometry, eliminate any advantage that low-level software might have had. Fourth, modern OSes typically contain file buffering and block caching algorithms that provide good filesystem performance, obviating the need to operate at such a low level. Finally, low-level disk access is very complex, and writing such software is difficult.</p>&#13;
<h4 class="h4" id="sec14_7_1"><strong><em>14.7.1 Sequential Filesystems</em></strong></h4>&#13;
<p class="noindent">The earliest file manager systems stored files sequentially on the disk’s surface. That is, if each sector/block on the disk held 512 bytes and a file was 32KB long, that file would consume 64 consecutive sectors/blocks on the disk’s surface. To access that file at some future time, the file manager only needed to know the file’s starting block number and the number of blocks it occupied. The filesystem had to maintain these two pieces of information <span epub:type="pagebreak" id="page_398"/>somewhere in nonvolatile storage. The obvious place was on the storage media itself, in a data structure known as the <em>directory</em>—an array of values starting at a specific disk location that the OS can reference when an application requests a particular file. The file manager can search through the directory for the file’s name and extract its starting block and length. With this information, the filesystem can provide the application with access to the file’s data.</p>&#13;
<p class="indent">One advantage of the sequential filesystem is that it is very fast. The OS can read or write a single file’s data very rapidly if the file is stored in sequential blocks on the disk’s surface. But a sequential file organization has some serious problems, too. The biggest and most obvious drawback is that you can’t extend the size of a file once the file manager places another file at the next block on the disk. Disk fragmentation is another big problem. As applications create and delete many small and medium files, the disk fills up with short sequences of unused sectors that, individually, are too small for most files. On sequential filesystems, disks often had free space sufficient to hold some data, but they couldn’t use it because it was scattered in small pieces all over the disk’s surface. To solve this problem, users had to run disk compaction programs to coalesce all the free sectors and move them to the end of the disk by physically rearranging files on its surface. Another solution was to copy files from one full disk to another empty disk, collecting the many small, unused sectors together. Obviously, this was extra work that the user had to do—work that the OS should have been doing.</p>&#13;
<p class="indent">The sequential file storage scheme really falls apart when used with multitasking OSes. If two applications attempt to write file data to the disk concurrently, the filesystem must place the starting block of the second application’s file beyond the last block required by the first application’s file. As the OS has no way of determining how large the files can grow, each application has to tell the OS the maximum length of the file when the application first opens it. Unfortunately, many applications cannot determine in advance how much space they’ll need for their files, so they have to guess the size of the file when opening it. If the estimated file size is too small, either the program has to abort with a “file full” error, or the application has to create a larger file, copy the old data from the “full” file to the new file, and then delete the old file. As you can imagine, this is horribly inefficient and definitely not great code.</p>&#13;
<p class="indent">To avoid such performance problems, many applications grossly overestimate the amount of space they need for their files. As a result, they wind up wasting disk space when the files don’t actually use all the data allocated to them, a form of <em>internal</em> fragmentation. Furthermore, if applications truncate their files when closing them, the resulting free sections returned to the OS tend to fragment the disk into the small, unusable blocks of free space described previously, a problem known as <em>external</em> fragmentation. For these reasons, sequential storage on the disk has been replaced by more sophisticated storage management schemes in modern OSes.</p>&#13;
<h4 class="h4" id="sec14_7_2"><span epub:type="pagebreak" id="page_399"/><strong><em>14.7.2 Efficient File Allocation Strategies</em></strong></h4>&#13;
<p class="noindent">Most modern file allocation strategies allow files to be stored across arbitrary blocks on the disk. Because the filesystem can now place bytes of the file in any free block on the disk, the problems of external fragmentation and the limitation on file size are all but eliminated. As long as there’s at least one free block on the disk, you can expand the size of any file. However, with this flexibility comes some added complexity. In a sequential filesystem, it was easy to locate free space on the disk; by simply noting the starting block numbers and sizes of the files in a directory, the filesystem could locate a free block large enough to satisfy the current disk allocation request, if one was available. But with files stored across arbitrary blocks, scanning the directory and noting which blocks a file uses is far too expensive to compute, so the filesystem has to keep track of the free and used blocks. Most modern OSes use one of three data structures—a set, a table (array), or a list—to keep track of which sectors are free and which are not. Each scheme has its advantages and disadvantages.</p>&#13;
<h5 class="h5" id="sec14_7_2_1"><strong>14.7.2.1 Free-Space Bitmaps</strong></h5>&#13;
<p class="noindent">The free-space bitmap scheme uses a set data structure to maintain a set of free blocks on the disk drive. If a block is a member of that set, the file manager can remove it whenever it needs another block for a file. Because set membership is a Boolean relationship (a block is either in the set or it’s not), it takes exactly 1 bit to specify the set membership of each block.</p>&#13;
<p class="indent">Typically, a file manager reserves a certain section of the disk to hold a bitmap that specifies which blocks on the disk are free. The bitmap consumes some integral number of blocks on the disk, with each block consumed representing a specific number of other blocks on the disk, which we can calculate by multiplying the block size (in bytes) by 8 (bits per byte). For example, if the OS uses 4,096-byte blocks on the disk, a bitmap consisting of a single block can track up to 32,768 other blocks on the disk.</p>&#13;
<p class="indent">The disadvantage of the bitmap scheme is that as disks get large, so does the bitmap. For example, on a 120GB drive with 4,096-byte blocks, the bitmap will be almost 4MB long. While this is a small percentage of the total disk capacity, accessing a single bit in a bitmap this large can be clumsy. To find a free block, the OS has to do a linear search through this 4MB bitmap. Even if you keep the bitmap in system memory (which is a bit expensive, considering that you have to do it for each drive), searching through it every time you need a free sector is an expensive proposition. As a result, you don’t see this scheme used much on larger disk drives.</p>&#13;
<p class="indent">One advantage (and also a disadvantage) of the bitmap scheme is that the file manager uses it only to keep track of the free space on the disk, not which sectors belong to a given file. As a result, if the free-space bitmap is damaged somehow, nothing is permanently lost; you can easily reconstruct it by searching through all the disk directories and computing which sectors are being used by the files in those directories (the remaining sectors, obviously, are the free ones). Although this process is somewhat time-consuming, it’s nice to have the option if disaster strikes.</p>&#13;
<h5 class="h5" id="sec14_7_2_2"><span epub:type="pagebreak" id="page_400"/><strong>14.7.2.2 File Allocation Tables</strong></h5>&#13;
<p class="noindent">Another way to track disk-sector usage is with a table of sector pointers, or a <em>file allocation table (FAT)</em>. This scheme is widely used. Cementing its popularity, this is also the default file allocation scheme used on most USB flash drives. An interesting facet of the FAT structure is that it combines both free-space management and file-sector allocation management into the same data structure, ultimately saving space when compared to the bitmap scheme, which uses separate data structures for each. Furthermore, unlike the bitmap scheme, FAT doesn’t require an inefficient linear search to find the next available free sector.</p>&#13;
<p class="indent">The FAT is really nothing more than an array of self-relative pointers (that is, indexes into itself), setting aside one pointer for each sector/block on the storage device. When a disk is initialized, the first several blocks on its surface are reserved for objects like the root directory and the FAT itself, and the remaining blocks on the disk are free. Somewhere in the root directory is a free-space pointer that specifies the next available free block on the disk. Assuming the free-space pointer initially contains the value <span class="literal">64</span>, implying that the next free block is block 64, the FAT entries at indexes 64, 65, 66, and so on, would contain the following values, assuming there are <em>n</em> blocks on the disk, numbered from 0 to <em>n</em> – 1:</p>&#13;
<table class="topbot-d">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<tbody>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="tab_th"><strong>FAT index</strong></p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="tab_th"><strong>FAT entry value</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">. . .</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">. . .</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">64</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">65</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">65</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">66</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">66</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">67</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">67</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">68</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">. . .</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">. . .</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><em>n</em> – 2</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><em>n</em> – 1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><em>n</em> – 1</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">0</span></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">The entry at block 64 tells you the next available free block on the disk, 65. Moving on to entry 65, you’ll find the value of the next available free block on the disk, <span class="literal">66</span>. The last entry in the FAT contains a <span class="literal">0</span> (block 0 contains meta-information for the entire disk partition and is never available).</p>&#13;
<p class="indent">Whenever an application needs one or more blocks to hold some new data on the disk’s surface, the file manager grabs the free-space pointer value and then continues going through the FAT entries for however many blocks are required to store the new data. For example, if each block is 4,096 bytes long and the current application is attempting to write 8,000 bytes to a file, the file manager will need to remove two blocks from the free-block list, following these steps:</p>&#13;
<ol>&#13;
<li class="noindent">Get the value of the free-space pointer.</li>&#13;
<li class="noindent">Save the value of the free-space pointer in order to determine the first free sector.</li>&#13;
<li class="noindent"><span epub:type="pagebreak" id="page_401"/>Continue going through the FAT entries for the number of blocks required to store the application’s data.</li>&#13;
<li class="noindent">Extract the FAT entry value of the last block where the application needs to store its data, and set the free-space pointer to this value.</li>&#13;
<li class="noindent">Store a <span class="literal">0</span> over the FAT entry value of the last block that the application uses, marking the end to the list of blocks that the application needs.</li>&#13;
<li class="noindent">Return the original (as it was prior to these steps) value of the free-space pointer into the FAT as the pointer to the list of blocks that are now allocated for the application.</li></ol>&#13;
<p class="indent">After the block allocation in our earlier example, the application has blocks 64 and 65 at its disposal, the free-space pointer contains <span class="literal">66</span>, and the FAT looks like this:</p>&#13;
<table class="topbot-d">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<tbody>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="tab_th"><strong>FAT index</strong></p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="tab_th"><strong>FAT entry value</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">. . .</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">. . .</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">64</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">65</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">65</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">0</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">66</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">67</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">67</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">68</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">. . .</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">. . .</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><em>n</em> – 2</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><em>n</em> – 1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><em>n</em> – 1</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">0</span></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">This is not to imply that entries in the FAT <em>always</em> contain the index of the next entry in the table. As the file manager allocates and deallocates storage for files on the disk, these numbers tend to become scrambled. For example, if an application returns block 64 to the free list but holds on to block 65, the free-space pointer would contain the value <span class="literal">64</span>, and the FAT would have the following values:</p>&#13;
<table class="topbot-d">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<tbody>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="tab_th"><strong>FAT index</strong></p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="tab_th"><strong>FAT entry value</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">. . .</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">. . .</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">64</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">66</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">65</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">0</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">66</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">67</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">67</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><span class="literal">68</span></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">. . .</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">. . .</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><em>n</em> – 2</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba"><em>n</em> – 1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><em>n</em> – 1</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba"><span class="literal">0</span></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">As noted earlier, one advantage of the FAT data structure is that it combines both free-space management and file allocation management <span epub:type="pagebreak" id="page_402"/>into a single data structure. This means that each file doesn’t have to carry around a list of the blocks its data occupies. Instead, it needs only a single pointer value specifying an index into the FAT where the first block of the file’s data can be found. You can find the remaining blocks containing the file’s data by stepping through the FAT.</p>&#13;
<p class="indent">One important advantage of the FAT scheme over the set (bitmap) scheme is that once the disk using a FAT filesystem is full, it doesn’t maintain information about which blocks are free. In contrast, the bitmap scheme consumes space on the disk to track free blocks even when there are none available. The FAT scheme replaces the entries originally used to track free blocks with the file-block pointers. When the disk is full, the values that originally maintained the free-block list are no longer consuming disk space because they’re all now tracking blocks in files. In this case, the free-space pointer contains <span class="literal">0</span> (to denote an empty free-space list) and all the FAT entries contain chains of block indexes for file data.</p>&#13;
<p class="indent">However, the FAT scheme does have a couple of disadvantages. First, unlike the bitmap in a set scheme filesystem, the table in a FAT filesystem represents a single point of failure. If the FAT is somehow destroyed, it can be very difficult to repair the disk and recover files; losing some free space on a disk is a problem, but losing track of where your files are on the disk is a <em>major</em> problem. Furthermore, because the disk head tends to spend more time in the FAT area of a storage device than in any other single area on the disk, the FAT is the most likely part of a hard disk to be damaged by a head crash, and the most likely part of a floppy or optical drive to exhibit excessive wear. This is a sufficiently big concern that some FAT filesystems provide an option to maintain an extra copy of the FAT on the disk.</p>&#13;
<p class="indent">Another problem with the FAT is that it’s usually located at a fixed place on the disk, typically at some low block number. In order to determine which block or blocks to read for a particular file, the disk heads must move to the FAT, so if the FAT is at the beginning of the disk, they’ll constantly be traveling to and from the FAT across large distances. This massive head movement not only is slow but tends to wear out the mechanical parts of the disk drive sooner. In newer versions of Microsoft OSes, the FAT-32 scheme eliminates part of this problem by allowing the FAT to be positioned somewhere other than the beginning of the disk, though still at a fixed location. Application file I/O performance can be quite low with a FAT filesystem unless the OS caches the FAT in main memory, which can be dangerous if the system crashes, because you could lose track of all file data whose FAT entries have not been written to disk.</p>&#13;
<p class="indent">The FAT scheme is also inefficient for doing random access on a file. To read from offset <em>m</em> to offset <em>n</em> in a file, the file manager must divide <em>n</em> by the block size to obtain the block offset into the file containing the byte at offset <em>n</em>, divide <em>m</em> by the block size to obtain its block offset, and then sequentially search through the FAT chain between these two blocks to find the sector(s) containing the desired data. This linear search can be expensive if the file is a large database with many thousands of blocks between the current block position and the desired block position.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_403"/>Yet another problem with the FAT filesystem, though this one is rather esoteric, is that it doesn’t support sparse files. That is, you cannot write to byte 0 and byte 1,000,000 of a file without also allocating every byte of data between those two points on the disk surface. Some non-FAT file managers allocate only the blocks where an application has written data. For example, if an application writes data only to bytes 0 and 1,000,000 of a file, the file manager allocates only two blocks for the file. If the application attempts to read a block that hasn’t been previously allocated (for example, if the application in the current example attempts to read the byte at offset 500,000 without first writing to that location), the file manager simply returns <span class="literal">0</span>s for the read operation without actually using any space on the disk. But because of how a FAT is organized, you can’t create sparse files on the disk.</p>&#13;
<h5 class="h5" id="sec14_7_2_3"><strong>14.7.2.3 Lists of Blocks</strong></h5>&#13;
<p class="noindent">To overcome the limitations of the FAT filesystem, advanced OSes—such as Windows NT/2000/XP/7/8/10, macOS (APFS), and various flavors of Unix—use a list-of-blocks scheme instead. Indeed, the list scheme enjoys all the advantages of a FAT system (such as efficient, nonlinear free-block location, and efficient storage of the free-block list), and it solves many of FAT’s problems.</p>&#13;
<p class="indent">The list scheme begins by setting aside several blocks on the disk for the purpose of keeping (generally) 32- or 64-bit pointers to each free block on the disk. If each block on the disk holds 4,096 bytes, a block can hold 1,024 (or 512) pointers. Dividing the number of blocks on the disk by 1,024 (512) determines the number of blocks the free-block list will initially consume. As you’ll soon see, the system uses these blocks to store data once the disk fills up, so there’s no storage overhead associated with the blocks consumed by the free-block list.</p>&#13;
<p class="indent">If a block in the free-block list contains 1,024 pointers (the following examples all assume 32-bit pointers), then the first 1,023 pointers contain the block numbers of free blocks on the disk. The file manager maintains two pointers on the disk: one that holds the block number of the current block containing free-block pointers, and one that holds an index into that current block. Whenever the filesystem needs a free block, it obtains the index for one from the free-block list by using these two pointers. Then the file manager increments the index into the free-block list to the next available entry in the list. When the index increments to 1,023 (the 1,024th item in the free-block list), instead of using the pointer entry value at that index to locate a free block, the file manager uses it as the address of the next block containing a list of free-block pointers on the disk, and it uses the current block, containing a now-empty list of block pointers, as the free block. This is how the file manager reuses the blocks originally designated to hold the free-block list, rather than reusing the pointers in the free-block list to keep track of the blocks belonging to a given file, as FAT does. Once the file manager uses up all the free-block pointers in a given block, it uses that block for actual file data.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_404"/>Unlike the FAT, the list scheme does not merge the free-block list and the file list into the same data structure. Instead, a separate data structure for each file holds the list of blocks associated with that file. Under typical Unix and Linux filesystems, the directory entry for the file actually holds the first 8 to 16 entries in the list (see <a href="ch14.xhtml#ch14fig05">Figure 14-5</a>). This allows the OS to track small files (up to 32KB or 64KB) without having to allocate any extra space on the disk.</p>&#13;
<div class="image"><img alt="image" src="../images/14fig05.jpg"/></div>&#13;
<p class="figcap"><a id="ch14fig05"/><em>Figure 14-5: Block list for small files</em></p>&#13;
<p class="indent">Research on various flavors of Unix suggests that the vast majority of files are small, and embedding several pointers into the directory entry provides an efficient way to access small files. Of course, as time passes, the average file size seems to increase. But as it turns out, block sizes tend to increase as well. When the research was first done, the typical block size was 512 bytes, but today it’s 4,096 bytes. During that time, then, average file sizes could have increased by a factor of 8 without, on average, requiring any extra space in the directory entries.</p>&#13;
<p class="indent">For medium files, up to about 4MB, the OS will allocate a single block with 1,024 pointers to the blocks that store the file’s data. The OS continues to use the pointers found in the directory entry for the first few blocks of the file, and then it uses a block on the disk to hold the next group of block pointers. Generally, the last pointer in the directory entry holds the location of this block (see <a href="ch14.xhtml#ch14fig06">Figure 14-6</a>).</p>&#13;
<div class="image"><img alt="image" src="../images/14fig06.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_405"/><a id="ch14fig06"/><em>Figure 14-6: Block list for medium files</em></p>&#13;
<p class="indent">For files larger than about 4MB, the filesystem switches to a three-tiered block scheme, which works for file sizes up to 4GB. In this scheme, the last pointer in the directory entry stores the location of a block of 1,024 pointers, and each pointer in this block holds the location of an additional block of 1,024 pointers, with each pointer in <em>this</em> block storing the location of a block that contains actual file data. See <a href="ch14.xhtml#ch14fig07">Figure 14-7</a> for the details.</p>&#13;
<div class="image"><img alt="image" src="../images/14fig07.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_406"/><a id="ch14fig07"/><em>Figure 14-7: Three-level block list for large files (up to 4GB)</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_407"/>One advantage to this tree structure is that it readily supports sparse files: an application can write to block 0 and block 100 of a file without having to allocate data blocks for every block between those two points. By placing a special block pointer value (typically <span class="literal">0</span>) in the intervening entries in the block list, the OS can determine whether a block isn’t present in the file. Should an application attempt to read a missing block in the file, the OS can simply return all <span class="literal">0</span>s for the empty block. Of course, once the application writes data to a block that hadn’t been previously allocated, the OS must copy the data to the disk and fill in the appropriate block pointer in the block list.</p>&#13;
<p class="indent">As disks became larger, the 4GB file limit imposed by this scheme began to create some problems for certain applications, such as video editors, large database applications, and web servers. One could easily extend this scheme 1,000 times—to 4TB—by adding another level to the block-list tree. The only problem with this approach is that the more levels of indirection you have, the slower random file access becomes, because the OS may have to read several blocks from the disk in order to get a single block of data. (When it has one level, it makes sense to cache the block-pointer list in memory, but with two and three levels, it’s impractical to do this for every file). Another way to extend the maximum size 4GB at a time is to use multiple pointers to second-tier file blocks (for example, have all or most of the original 8 to 16 pointers in the directory point at second-tier block-list entries rather than directly at file data blocks). Although there’s no current convention for extending beyond three levels, rest assured that as the need arises, OS designers will develop schemes for accessing large files efficiently. For example, 64-bit OSes can use 64-bit pointers, rather than 32-bit pointers, and eliminate the 4GB limitation.</p>&#13;
<h3 class="h3" id="sec14_8"><strong>14.8 Writing Software That Manipulates Data on a Mass Storage Device</strong></h3>&#13;
<p class="noindent">Understanding how different mass storage devices behave is important if you want to write high-performance software that manipulates files on these devices. Although modern OSes attempt to isolate applications from the physical realities of mass storage, an OS can only do so much for you. Furthermore, because an OS can’t predict how your particular application will access files on a mass storage device, it can’t tailor file access optimizations to your application; instead, its optimizations are geared toward applications exhibiting <em>typical</em> file access patterns. The less typical your application’s file I/O is, then, the less likely you’ll get the best performance out of the system. In this section, we’ll look at how you can coordinate your file access activities with the OS to achieve the best performance.</p>&#13;
<h4 class="h4" id="sec14_8_1"><strong><em>14.8.1 File Access Performance</em></strong></h4>&#13;
<p class="noindent">Although disk drives and most other mass storage devices are often thought of as “random access” devices, the fact is that mass storage access is usually more efficient when done sequentially. Sequential access on a disk drive is relatively efficient because the OS can move the read/write head one track <span epub:type="pagebreak" id="page_408"/>at a time (assuming the file appears in sequential blocks on the disk). This is much faster than accessing one block on the disk, moving the read/write head to some other track, accessing another block, moving the head again, and so on. Therefore, you should avoid random file access in an application if at all possible.</p>&#13;
<p class="indent">You should also try to read or write large blocks of data on each file access rather than reading or writing small amounts more frequently. There are two reasons for this. First, OS calls are not fast, so if you make half as many calls by reading or writing twice as much data on each access, the application will often run twice as fast. Second, the OS must read or write whole disk blocks. If your block size is 4,096 bytes, but you just write 2,000 bytes to some block and then seek to some other position in the file outside that block, the OS will actually have to read the entire 4,096-byte block from the disk, merge in the 2,000 bytes, and then finally write the entire 4,096 bytes back to the disk. Contrast this with a write operation that writes a full 4,096 bytes—in this case, the OS wouldn’t have to read the data from the disk first; it would only have to write the block. Writing full blocks improves disk access performance by a factor of 2, because writing partial blocks requires the OS to first read the block, merge the data, and then write the block; writing whole blocks renders the read operation unnecessary. Even if your application doesn’t write data in increments that are even multiples of the disk’s block size, writing large blocks improves performance. If you write 16,000 bytes to a file in one write operation, the OS will still have to write the last block of those 16,000 bytes using a read-merge-write operation, but it will write the first three blocks using only write operations.</p>&#13;
<p class="indent">If you start with a relatively empty disk, the OS generally attempts to write the data for new files in sequential blocks. This organization is probably most efficient for future file access. However, as the system’s users create and delete files on the disk, the blocks of data for individual files may be distributed nonsequentially. In a very bad case, the OS may wind up allocating a few blocks here and a few blocks there all across the disk’s surface. As a result, even sequential file access can behave like slow random file access. As discussed previously, this kind of file fragmentation can dramatically decrease filesystem performance. Unfortunately, there’s no way for an application to determine if its file data is fragmented across the disk surface and, even if it could, there’s little it could do about the situation. Although there are utilities available to <em>defragment</em> the blocks on the disk’s surface, an application generally can’t request their execution (and “defragger” utilities are quite slow anyway).</p>&#13;
<p class="indent">Although applications rarely get the opportunity to defragment their data files during normal program execution, there are some things you can do to reduce the probability of your data files becoming fragmented. The best advice you can follow is to always write file data in large chunks. Indeed, if you can write the whole file in a single write operation, do so. In addition to speeding up access to the OS, writing large amounts of data tends to result in the allocation of sequential blocks. When you write small blocks of data to the disk, other applications in a multitasking environment <span epub:type="pagebreak" id="page_409"/>could also be writing to the disk concurrently. In this case, the OS may interleave the block allocation requests for the files being written by several different applications, making it unlikely that a particular file’s data will be written sequentially. It is important to try to write a file’s data in sequential blocks, even if you plan to access portions of that data randomly, since searching for random records in a file written sequentially generally requires far less head movement than searching for random records in a file whose blocks are scattered all over.</p>&#13;
<p class="indent">If you’re going to create a file and then access its blocks of data repeatedly, whether randomly or sequentially, try to preallocate the blocks on the disk. If you know, for example, that your file’s data will not exceed 1MB, you could write a block of one million <span class="literal">0</span>s to the disk before your application starts manipulating the file. By doing so, you help ensure that the OS will write your file to sequential blocks on the disk. Though you pay an initial price to write all those <span class="literal">0</span>s (an operation you wouldn’t normally do, presumably), the savings in read/write head-seek times could easily make up for it. This scheme is especially useful if an application is reading or writing two or more files concurrently (which would almost guarantee the interleaving of the blocks for the various files).</p>&#13;
<h4 class="h4" id="sec14_8_2"><strong><em>14.8.2 Synchronous and Asynchronous I/O</em></strong></h4>&#13;
<p class="noindent">Because most mass storage devices are mechanical, and, therefore, subject to mechanical delays, applications that use them extensively have to wait for them to complete read/write operations. Most disk I/O operations are <em>synchronous</em>, meaning that an application that makes a call to the OS must wait until that I/O request is complete before continuing subsequent operations.</p>&#13;
<p class="indent">This is why most modern OSes also provide an <em><a href="gloss01.xhtml#gloss01_19">asynchronous I/O</a></em> capability, in which the OS begins the application’s request and then returns control to the application without waiting for the I/O operation to complete. While the I/O operation proceeds, the application promises not to do anything with the data buffer specified for it. However, the application can do computation and schedule additional I/O operations, because the OS will notify it when the original request completes. This is especially useful when you’re accessing files on multiple disk drives in the system, which is usually possible only with SCSI and other high-end drives.</p>&#13;
<h4 class="h4" id="sec14_8_3"><strong><em>14.8.3 The Implications of I/O Type</em></strong></h4>&#13;
<p class="noindent">Another important consideration for writing software that manipulates mass storage devices is the type of I/O you’re performing. <em>Binary I/O</em> is usually faster than <em>formatted text I/O</em>, because of the format of the data written to disk. For example, suppose you have an array of 16 integer values that you want to write to a file. To achieve this, you could use either of the following two C/C++ code sequences:</p>&#13;
<p class="programs">FILE *f;<br/>&#13;
int array[16];<br/>&#13;
    . . .<br/>&#13;
<span epub:type="pagebreak" id="page_410"/>// Sequence #1:<br/><br/>&#13;
fwrite( f, array, 16 * sizeof( int ));<br/>&#13;
    . . .<br/>&#13;
// Sequence #2:<br/><br/>&#13;
for( i=0; i &lt; 16; ++i )<br/>&#13;
    fprintf( f, "%d ", array[i] );</p>&#13;
<p class="indent">The second sequence looks like it would run slower than the first because it uses a loop, rather than a single call, to step through each element of the array. But although the extra execution overhead of the loop does have a small negative impact on the execution time of the write operation, this efficiency loss is minor compared to the real problem with the second sequence. Whereas the first code sequence writes out a 64-byte memory image consisting of 16 32-bit integers to the disk, the second code sequence converts each of the 16 integers to a string of characters and then writes each string to the disk. This integer-to-string conversion process is relatively slow. Furthermore, the <span class="literal">fprintf()</span> function has to interpret the format string (<span class="literal">"%d"</span>) at runtime, which incurs an additional delay.</p>&#13;
<p class="indent">The advantage of formatted I/O is that the resulting file is both human-readable and easily read by other applications. However, if you’re using a file to hold data that is of interest only to your application, a more efficient approach might be to write the data as a memory image.</p>&#13;
<h4 class="h4" id="sec14_8_4"><strong><em>14.8.4 Memory-Mapped Files</em></strong></h4>&#13;
<p class="noindent">Memory-mapped files use the OS’s virtual memory capabilities to map memory addresses in the application space directly to blocks on the disk. Modern OSes have highly optimized virtual memory subsystems, so piggy-backing file I/O on top of the virtual memory subsystem results in very efficient file access. Furthermore, memory-mapped file access is easy. When you open a memory-mapped file, the OS returns a memory pointer to some block of memory. By simply accessing the memory locations referenced by this pointer, just as you would any other in-memory data structure, you can access the file’s data. This makes file access almost trivial, while often improving file manipulation performance, especially when file access is random.</p>&#13;
<p class="indent">One of the reasons that memory-mapped files are so much more efficient than regular files is that the OS reads the list of blocks belonging to memory-mapped files only once. It then sets up the system’s memory management tables to point at each block belonging to the file. After opening the file, the OS rarely has to read any file metadata from the disk, which greatly reduces superfluous disk access during random file access. It also improves sequential file access, though to a lesser degree. The OS doesn’t constantly have to copy data between the disk, internal OS buffers, and application data buffers.</p>&#13;
<p class="indent">Memory-mapped file access does have some disadvantages. First, you can’t map gigantic files entirely into memory, at least on older PCs and OSes that have a 32-bit address bus and set aside a maximum of 4GB per <span epub:type="pagebreak" id="page_411"/>application. Generally, it isn’t practical to use a memory-mapped access scheme for files larger than 256MB, though this has changed as more CPUs with 64-bit addressing capabilities have become available. It’s also not a good idea to use memory-mapped files when an application is already using most of the RAM physically present in the system. Fortunately, these two situations are not typical, so they don’t limit the use of memory-mapped files much.</p>&#13;
<p class="indent">A more common and significant issue is that when you first create a memory-mapped file, you have to tell the OS the file’s maximum size. If it’s impossible to determine the file’s final size, you’ll have to overestimate it and then truncate the file when you close it. Unfortunately, this wastes system memory while the file is open. Memory-mapped files work well when you’re manipulating files in read-only mode or simply reading and writing data in an existing file without extending the file’s size. Fortunately, you can always create a file using traditional file access mechanisms and then use memory-mapped file I/O to access the file later.</p>&#13;
<p class="indent">Finally, almost every OS does memory-mapped file access differently, so it’s unlikely that memory-mapped file I/O code will be portable between OSes. Nevertheless, the code to open and close memory-mapped files is quite short, and it’s easy enough to provide multiple copies of the code for the various OSes you need to support. Of course, actually accessing the file’s data consists of simple memory accesses, and that’s independent of the OS. For more information on memory-mapped files, consult your OS’s API reference. Given their convenience and performance, you should seriously consider using memory-mapped files whenever possible in your applications.</p>&#13;
<h3 class="h3" id="sec14_9"><strong>14.9 For More Information</strong></h3>&#13;
<p class="ref">Silberschatz, Abraham, Peter Baer Galvin, and Greg Gagne. <em>Operating System Concepts</em>. 8th ed. Hoboken, NJ: John Wiley &amp; Sons, 2009.<span epub:type="pagebreak" id="page_412"/></p>&#13;
</body></html>