<html><head></head><body>
<h2 class="h2" id="ch16"><span epub:type="pagebreak" id="page_331"/><span class="big"><strong>16</strong></span><br/><strong>COMMON PROBABILITY DISTRIBUTIONS</strong></h2>&#13;
<div class="image"><img src="../images/common-01.jpg" alt="image"/></div>&#13;
<p class="noindent">In this chapter, you’ll look at a number of standard probability distributions that exist for dealing with commonly occurring random phenomena in statistical modeling. These distributions follow the same natural rules as the examples presented in <a href="ch15.xhtml#ch15">Chapter 15</a>, and they’re useful because their properties are well understood and documented. In fact, they are so ubiquitous that most statistical software packages have corresponding built-in functionality for their evaluation, and R is no exception. Several of these distributions represent an essential ingredient in traditional statistical hypothesis testing, which is explored in <a href="ch17.xhtml#ch17">Chapters 17</a> and <a href="ch18.xhtml#ch18">18</a>.</p>&#13;
<p class="indent">Just like the random variables they model, the common distributions you’ll examine here are broadly categorized as either discrete or continuous. Each distribution has four core R functions tied to it—a <code>d</code>-function, providing specific mass or density function values; a <code>p</code>-function, providing cumulative distribution probabilities; a <code>q</code>-function, providing quantiles; and an <code>r</code>-function, providing random variate generation.</p>&#13;
<h3 class="h3" id="ch16lev1sec50"><span epub:type="pagebreak" id="page_332"/><strong>16.1 Common Probability Mass Functions</strong></h3>&#13;
<p class="noindent">You’ll start here by looking at definitions and examples of some common probability mass functions for discrete random variables. Continuous distributions will be explored in in <a href="ch16.xhtml#ch16lev1sec51">Section 16.2</a>.</p>&#13;
<h4 class="h4" id="ch16lev2sec137"><strong><em>16.1.1 Bernoulli Distribution</em></strong></h4>&#13;
<p class="noindent">The <em>Bernoulli</em> distribution is the probability distribution of a discrete random variable that has only two possible outcomes, such as success or failure. This type of variable can be referred to as <em>binary</em> or <em>dichotomous</em>.</p>&#13;
<p class="indent">Let’s say you’ve defined a binary random variable <em>X</em> for the success or failure of an event, where <em>X</em> = 0 is failure, <em>X</em> = 1 is success, and <em>p</em> is the known probability of success. <a href="ch16.xhtml#ch16tab1">Table 16-1</a> shows the probability mass function for <em>X</em>.</p>&#13;
<p class="tabt"><strong><a id="ch16tab1"/>Table 16-1:</strong> The Bernoulli Probability Mass Function</p>&#13;
<table class="topbotr">&#13;
<thead>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table1r"><p class="table"><strong><em>x</em></strong></p></td>&#13;
<td style="vertical-align: top;" class="table2r"><p class="table"><strong>0</strong></p></td>&#13;
<td style="vertical-align: top;" class="table2r"><p class="table"><strong>1</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table3r"><p class="table">Pr(<em>X</em> = <em>x</em>)</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">1 – <em>p</em></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><em>p</em></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">From <a href="ch15.xhtml#ch15lev2sec134">Section 15.2.2</a> you know that the probabilities associated with all possible outcomes must sum to 1. Therefore, if the probability of success is <em>p</em> for a binary random variable, the only other alternative outcome, failure, must occur with probability 1 − <em>p</em>.</p>&#13;
<p class="indent">In mathematical terms, for a discrete random variable <em>X</em> = <em>x</em>, the Bernoulli mass function <em>f</em> is</p>&#13;
<div class="imagec"><a id="ch16eq1"/><img src="../images/e16-1.jpg" alt="image"/></div>&#13;
<p class="noindent">where <em>p</em> is a parameter of the distribution. The notation</p>&#13;
<p class="center"><em>X</em> ∼ BERN(<em>p</em>)</p>&#13;
<p class="noindent">is often used to indicate that “<em>X</em> follows a Bernoulli distribution with parameter <em>p</em>.”</p>&#13;
<p class="indentb">The following are the key points to remember:</p>&#13;
<p class="bull">• <em>X</em> is dichotomous and can take only the values 1 (“success”) or 0 (“failure”).</p>&#13;
<p class="bull">• <em>p</em> should be interpreted as “the probability of success,” and therefore 0 ≤ <em>p</em> ≤ 1.</p>&#13;
<p class="indentt">The mean and variance are defined as follows, respectively:</p>&#13;
<div class="imagec"><img src="../images/f0332-01.jpg" alt="image"/></div>&#13;
<p class="indent">Say you use the common example of rolling a die, with success defined as getting a 4, and you roll once. You therefore have a binary random <span epub:type="pagebreak" id="page_333"/>variable <em>X</em> that can be modeled using the Bernoulli distribution, with the probability of success <img class="middle" src="../images/f0333-01.jpg" alt="image"/>. For this example, <img class="middle" src="../images/f0333-02.jpg" alt="image"/>. You can easily determine, using (16.1), that</p>&#13;
<div class="imagec"><img src="../images/f0333-03.jpg" alt="image"/></div>&#13;
<p class="noindent">and, in much the same way, that <img class="middle" src="../images/f0333-04.jpg" alt="image"/>. Furthermore, you’d have <img class="middle" src="../images/f0333-05.jpg" alt="image"/> and <img class="middle" src="../images/f0333-06.jpg" alt="image"/>.</p>&#13;
<h4 class="h4" id="ch16lev2sec138"><strong><em>16.1.2 Binomial Distribution</em></strong></h4>&#13;
<p class="noindent">The <em>binomial distribution</em> is the distribution of successes in <em>n</em> number of trials involving binary discrete random variables. The role of the Bernoulli distribution is typically one of a “building block” for more complicated distributions, like the binomial, that give you more interesting results.</p>&#13;
<p class="indent">For example, suppose you define a random variable <img class="middle" src="../images/f0333-07.jpg" alt="image"/>, where <em>Y</em><sub>1</sub>, <em>Y</em><sub>2</sub>, ..., <em>Y<sub>n</sub></em> are each Bernoulli random variables corresponding to the same event, in other words, the die roll with success defined as rolling a 4. The new random variable <em>X</em>, a sum of Bernoulli random variables, now describes <em>the number of successes in n trials</em> of the defined action. Providing that certain reasonable assumptions are satisfied, the probability distribution that describes this success count is the binomial distribution.</p>&#13;
<p class="indent">In mathematical terms, for a discrete random variable and a realization <em>X</em> = <em>x</em>, the binomial mass function <em>f</em> is</p>&#13;
<div class="imagec"><a id="ch16eq2"/><img src="../images/e16-2.jpg" alt="image"/></div>&#13;
<p class="noindent">where</p>&#13;
<div class="imagec"><a id="ch16eq3"/><img src="../images/e16-3.jpg" alt="image"/></div>&#13;
<p class="noindent">is known as the <em>binomial coefficient</em>. (Recall the use of the integer factorial operator !, as first discussed in <a href="ch10.xhtml#ch10exc4">Exercise 10.4</a> on <a href="ch10.xhtml#page_203">page 203</a>.) This coefficient, also referred to as a <em>combination</em>, accounts for all different orders in which you might observe <em>x</em> successes throughout <em>n</em> trials.</p>&#13;
<p class="indent">The parameters of the binomial distribution are <em>n</em> and <em>p</em>, and the notation</p>&#13;
<p class="center"><em>X</em> ∼ BIN(<em>n</em>, <em>p</em>)</p>&#13;
<p class="noindent">is often used to indicate that <em>X</em> follows a binomial distribution for <em>n</em> trials with parameter <em>p</em>.</p>&#13;
<p class="indentb">The following are the key points to remember:</p>&#13;
<p class="bull">• <em>X</em> can take only the values 0, 1, ..., <em>n</em> and represents the total number of successes.</p>&#13;
<p class="bull">• <em>p</em> should be interpreted as “the probability of success at each trial.” Therefore, 0 ≤ <em>p</em> ≤ 1, and <em>n</em> &gt; 0 is an integer interpreted as “the number of trials.”</p>&#13;
<p class="bull"><span epub:type="pagebreak" id="page_334"/>• Each of the <em>n</em> trials is a Bernoulli success and failure event, the trials are independent (in other words, the outcome of one doesn’t affect the outcome of any other), and <em>p</em> is constant.</p>&#13;
<p class="indentt">The mean and variance are defined as follows:</p>&#13;
<div class="imagec"><img src="../images/f0334-01.jpg" alt="image"/></div>&#13;
<p class="indent">Counting the number of successes of repeated trials of a binary-valued test is one of the common random phenomena mentioned at the start of this section. Consider the specific situation in which there’s only one “trial,” that is, <em>n</em> = 1. Examining <a href="ch16.xhtml#ch16eq2">Equations (16.2)</a> and <a href="ch16.xhtml#ch16eq3">(16.3)</a>, it should become clear that (16.2) simplifies to (16.1). In other words, the Bernoulli distribution is just a special case of the binomial. Clearly, this makes sense with respect to the definition of a binomial random variable as a sum of <em>n</em> Bernoulli random variables. In turn, R provides functionality for the binomial distribution though not explicitly for the Bernoulli.</p>&#13;
<p class="indent">To illustrate this, I’ll return to the example of rolling a die with success defined as getting a 4. If you roll the die independently eight times, what is the probability of observing exactly five successes (five 4s) in total? Well, you’d have <img class="middle" src="../images/f0334-03.jpg" alt="image"/>, and this probability can be worked through mathematically using (16.2).</p>&#13;
<div class="imagec"><img src="../images/f0334-02.jpg" alt="image"/></div>&#13;
<p class="indent">The result tells you there is approximately a 0.4 percent chance that you’ll observe exactly five 4s in eight rolls of the die. This is small and makes sense—it’s far more probable that you might observe zero to two 4s in eight rolls of a die.</p>&#13;
<p class="indentb">Fortunately, R functions will handle the arithmetic in these situations. The built-in functions <code>dbinom</code>, <code>pbinom</code>, <code>qbinom</code>, and <code>rbinom</code> are all relevant to the binomial and Bernoulli distributions and are summarized in one help file indexed by each of these function names.</p>&#13;
<p class="bull">• <code>dbinom</code> directly provides the mass function probabilities Pr(<em>X</em> = <em>x</em>) for any valid <em>x</em>—that is, 0 ≤ <em>x</em> ≤ <em>n</em>.</p>&#13;
<p class="bull">• <code>pbinom</code> provides the cumulative probability distribution—given a valid <em>x</em>, it yields Pr(<em>X</em> ≤ <em>x</em>).</p>&#13;
<p class="bull">• <code>qbinom</code> provides the <em>inverse</em> cumulative probability distribution (also known as the <em>quantile function</em> of the distribution)—given a valid probability 0 ≤ <em>p</em> ≤ 1, it yields the value of <em>x</em> that satisfies Pr(<em>X</em> ≤ <em>x</em>) = <em>p</em>.</p>&#13;
<p class="bull">• <code>rbinom</code> is used to generate any number of realizations of <em>X</em> given a specific binomial distribution.</p>&#13;
<h5 class="h5" id="ch16lev3sec44"><span epub:type="pagebreak" id="page_335"/><strong>The dbinom Function</strong></h5>&#13;
<p class="noindent">With this knowledge, you can use R to confirm the result of Pr(<em>X</em> = 5) for the die-roll example described a moment ago.</p>&#13;
<pre>R&gt; dbinom(x=5,size=8,prob=1/6)<br/>[1] 0.004167619</pre>&#13;
<p class="indent">To the <code>dbinom</code> function, you provide the specific value of interest as <code>x</code>; the total number of trials, <em>n</em>, as <code>size</code>; and the probability of success at each trial, <em>p</em>, as <code>prob</code>. True to R, a vector argument is possible for <code>x</code>. If you want the full probability mass function table for <em>X</em> for this example, you can supply the vector <code>0:8</code> to <code>x</code>.</p>&#13;
<pre>R&gt; X.prob &lt;- dbinom(x=0:8,size=8,prob=1/6)<br/>R&gt; X.prob<br/>[1] 2.325680e-01 3.721089e-01 2.604762e-01 1.041905e-01 2.604762e-02<br/>[6] 4.167619e-03 4.167619e-04 2.381497e-05 5.953742e-07</pre>&#13;
<p class="indent">These can be confirmed to sum to 1.</p>&#13;
<pre>R&gt; sum(X.prob)<br/>[1] 1</pre>&#13;
<p class="indent">The resulting vector of probabilities, which corresponds to the specific outcomes <em>x</em> = {0, 1, ..., 8}, is returned using e-notation (refer to <a href="ch02.xhtml#ch02lev2sec19">Section 2.1.3</a>). You can tidy this up by rounding the results using the <code>round</code> function introduced in <a href="ch13.xhtml#ch13lev2sec117">Section 13.2.2</a>. Rounding to three decimal places, the results are easier to read.</p>&#13;
<pre>R&gt; round(X.prob,3)<br/>[1] 0.233 0.372 0.260 0.104 0.026 0.004 0.000 0.000 0.000</pre>&#13;
<p class="indent">The achievement of one success in eight trials has the highest probability, at approximately 0.372. Furthermore, the mean (expected value) and variance of <em>X</em> in this example are <img class="middle" src="../images/f0335-01.jpg" alt="image"/> and <img class="middle" src="../images/f0335-02.jpg" alt="image"/>.</p>&#13;
<pre>R&gt; 8/6<br/>[1] 1.333333<br/>R&gt; 8*(1/6)*(5/6)<br/>[1] 1.111111</pre>&#13;
<p class="indent">You can plot the corresponding probability mass function in the same way as for the example in <a href="ch15.xhtml#ch15lev2sec134">Section 15.2.2</a>; the following line produces <a href="ch16.xhtml#ch16fig1">Figure 16-1</a>:</p>&#13;
<pre>R&gt; barplot(X.prob,names.arg=0:8,space=0,xlab="x",ylab="Pr(X = x)")</pre>&#13;
<div class="image"><span epub:type="pagebreak" id="page_336"/><img src="../images/f16-01.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch16fig1"/>Figure 16-1: The probability mass function associated with the binomial distribution of the die-rolling example</em></p>&#13;
<h5 class="h5" id="ch16lev3sec45"><strong>The pbinom Function</strong></h5>&#13;
<p class="noindent">The other R functions for the binomial distribution work in much the same way. The first argument is always the value (or values) of interest; <em>n</em> is supplied as <code>size</code> and <em>p</em> as <code>prob</code>. To find, for example, the probability that you observe three or fewer 4s, Pr(<em>X</em> ≤ 3), you either sum the relevant individual entries from <code>dbinom</code> as earlier or use <code>pbinom</code>.</p>&#13;
<pre>R&gt; sum(dbinom(x=0:3,size=8,prob=1/6))<br/>[1] 0.9693436<br/>R&gt; pbinom(q=3,size=8,prob=1/6)<br/>[1] 0.9693436</pre>&#13;
<p class="indent">Note that the pivotal argument to <code>pbinom</code> is tagged <code>q</code>, not <code>x</code>; this is because, in a cumulative sense, you are searching for a probability based on a quantile. The cumulative distribution results from <code>pbinom</code> can be used in the same way to search for “upper-tail” probabilities (probabilities to the right of a given value) since you know that the total probability mass is always 1. To find the probability that you observe <em>at least</em> three 4s in eight rolls of the die, Pr(<em>X</em> ≥ 3) (which is equivalent to Pr(<em>X</em> &gt; 2) in the context of this discrete random variable), note that the following finds the correct result because it’s the complement of Pr(<em>X</em> ≤ 2) that you’re looking for:</p>&#13;
<pre>R&gt; 1-pbinom(q=2,size=8,prob=1/6)<br/>[1] 0.1348469</pre>&#13;
<h5 class="h5" id="ch16lev3sec46"><span epub:type="pagebreak" id="page_337"/><strong>The qbinom Function</strong></h5>&#13;
<p class="noindent">Less frequently used is the <code>qbinom</code> function, which is the inverse of <code>pbinom</code>. Where <code>pbinom</code> provides a cumulative probability when given a quantile value <code>q</code>, the function <code>qbinom</code> provides a quantile value when given a cumulative probability <code>p</code>. The discrete nature of a binomial random variable means <code>qbinom</code> will return the nearest value of <em>x</em> below which <code>p</code> lies. For example, note that</p>&#13;
<pre>R&gt; qbinom(p=0.95,size=8,prob=1/6)<br/>[1] 3</pre>&#13;
<p class="noindent">provides 3 as the quantile value, even though you know from earlier that the exact probability that lies at or below 3, Pr(<em>X</em> ≤ 3), is 0.9693436. You’ll look at <code>p</code>- and <code>q</code>-functions more when dealing with continuous probability distributions; see <a href="ch16.xhtml#ch16lev1sec51">Section 16.2</a>.</p>&#13;
<h5 class="h5" id="ch16lev3sec47"><strong>The rbinom Function</strong></h5>&#13;
<p class="noindent">Lastly, the random generation of realizations of a binomially distributed variable is retrieved using the <code>rbinom</code> function. Again, going with the <img class="middle" src="../images/f0337-01.jpg" alt="image"/> distribution, note the following:</p>&#13;
<pre>R&gt; rbinom(n=1,size=8,prob=1/6)<br/>[1] 0<br/>R&gt; rbinom(n=1,size=8,prob=1/6)<br/>[1] 2<br/>R&gt; rbinom(n=1,size=8,prob=1/6)<br/>[1] 2<br/>R&gt; rbinom(n=3,size=8,prob=1/6)<br/>[1] 2 1 1</pre>&#13;
<p class="indent">The initial argument <code>n</code> doesn’t refer to the number of trials. The number of trials is still provided to <code>size</code> with <em>p</em> given to <code>prob</code>. Here, <code>n</code> requests the number of realizations you want to generate for the random variable <img class="middle" src="../images/f0337-02.jpg" alt="image"/>. The first three lines each request a single realization—in the first eight rolls, you observe zero successes (4s), and in the second and third sets of eight rolls, you observe two and two 4s, respectively. The fourth line highlights the fact that multiple realizations of <em>X</em> are easily obtained and stored as a vector by increasing <code>n</code>. As these are <em>randomly generated realizations</em>, if you run these lines now, you’ll probably observe some different values.</p>&#13;
<p class="indent">Though not used often in standard statistical testing methods, the <code>r-</code>functions for probability distributions, either discrete or continuous, play an important role when it comes to simulation and various advanced numeric algorithms in computational statistics.</p>&#13;
<div class="ex">&#13;
<p class="ext"><span epub:type="pagebreak" id="page_338"/><a id="ch16exc1"/><strong>Exercise 16.1</strong></p>&#13;
<p class="noindentz">A forested nature reserve has 13 bird-viewing platforms scattered throughout a large block of land. The naturalists claim that at any point in time, there is a 75 percent chance of seeing birds at each platform. Suppose you walk through the reserve and visit every platform. If you assume that all relevant conditions are satisfied, let <em>X</em> be a binomial random variable representing the total number of platforms at which you see birds.</p>&#13;
<ol type="a">&#13;
<li><p class="noindents">Visualize the probability mass function of the binomial distribution of interest.</p></li>&#13;
<li><p class="noindents">What is the probability you see birds at all sites?</p></li>&#13;
<li><p class="noindents">What is the probability you see birds at more than 9 platforms?</p></li>&#13;
<li><p class="noindents">What is the probability of seeing birds at between 8 and 11 platforms (inclusive)? Confirm your answer by using only the <code>d</code>-function and then again using only the <code>p</code>-function.</p></li>&#13;
<li><p class="noindents">Say that, before your visit, you decide that if you see birds at fewer than 9 sites, you’ll make a scene and demand your entry fee back. What’s the probability of your embarrassing yourself in this way?</p></li>&#13;
<li><p class="noindents">Simulate realizations of <em>X</em> that represent 10 different visits to the reserve; store your resulting vector as an object.</p></li>&#13;
<li><p class="noindents">Compute the mean and standard deviation of the distribution of interest.</p></li>&#13;
</ol>&#13;
</div>&#13;
<h4 class="h4" id="ch16lev2sec139"><strong><em>16.1.3 Poisson Distribution</em></strong></h4>&#13;
<p class="noindent">In this section, you’ll use the <em>Poisson</em> distribution to model a slightly more general, but just as important, discrete random variable—a <em>count</em>. For example, the variable of interest might be the number of seismic tremors detected by a certain station in a given year or the number of imperfections found on square-foot pieces of sheet metal coming off a factory production line.</p>&#13;
<p class="indent">Importantly, the events or items being counted are assumed to manifest independently of one another. In mathematical terms, for a discrete random variable and a realization <em>X</em> = <em>x</em>, the Poisson mass function <em>f</em> is given as follows, where <em>λ</em><sub>p</sub> is a parameter of the distribution (this will be explained further momentarily):</p>&#13;
<div class="imagec"><a id="ch16eq4"/><img src="../images/e16-4.jpg" alt="image"/></div>&#13;
<p class="indent">The notation</p>&#13;
<p class="center"><em>X</em> ∼ POIS(<em>λ</em><sub>p</sub>)</p>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_339"/>is often used to indicate that “<em>X</em> follows a Poisson distribution with parameter <em>λ</em><sub>p</sub>.”</p>&#13;
<p class="indentb">The following are the keys points to remember:</p>&#13;
<p class="bull">• The entities, features, or events being counted occur independently in a well-defined interval at a constant rate.</p>&#13;
<p class="bull">• <em>X</em> can take only non-negative integers: 0,1,. . ..</p>&#13;
<p class="bull">• <em>λ</em><sub>p</sub> should be interpreted as the “mean number of occurrences” and must therefore be finite and strictly positive; that is, 0 &lt; <em>λ</em><sub>p</sub> &lt; ∞.</p>&#13;
<p class="indentt">The mean and variance are as follows:</p>&#13;
<div class="imagec"><img src="../images/f0339-01.jpg" alt="image"/></div>&#13;
<p class="indent">Like the binomial random variable, the values taken by a Poisson random variable are discrete, non-negative integers. Unlike the binomial, however, there’s typically no upper limit on a Poisson count. While this implies that an “infinite count” is allowed to occur, it’s a distinct feature of the Poisson distribution that the probability mass associated with some value <em>x</em> goes to zero as <em>x</em> itself goes to infinity.</p>&#13;
<p class="indent">As noted in <a href="ch16.xhtml#ch16eq4">Equation (16.4)</a>, any Poisson distribution depends upon the specification of a single parameter, denoted here with <em>λ</em><sub>p</sub>. This parameter describes the mean number of occurrences, which impacts the overall shape of the mass function, as shown in <a href="ch16.xhtml#ch16fig2">Figure 16-2</a>.</p>&#13;
<div class="image"><img src="../images/f16-02.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch16fig2"/>Figure 16-2: Three examples of the Poisson probability mass function, plotted for 0</em> ≤ x ≤ <em>30. The “expected count” parameter</em> <em>λ</em><sub>p</sub> <em>is altered from 3.00 (left) to 6.89 (middle) and to 17.20 (right).</em></p>&#13;
<p class="indent">Again, it’s worth noting that the total probability mass over all possible outcomes is 1, no matter what the value of <em>λ</em><sub>p</sub> is and regardless of the fact that possible outcomes can, technically, range from 0 to infinity.</p>&#13;
<p class="indent">By definition, it’s easy to understand why the mean of <em>X</em>, <em>μ<sub>X</sub></em>, is equal to <em>λ</em><sub>p</sub>; in fact, it turns out that the variance of a Poisson distributed random variable is also equal to <em>λ</em><sub>p</sub>.</p>&#13;
<p class="indent">Consider the example of blemishes on 1-foot-square sheets of metal coming off a production line, mentioned in the opening of this section. Suppose you’re told that the number of blemishes found, <em>X</em>, is thought to follow a Poisson distribution with <em>λ</em><sub>p</sub> = 3.22, as in <em>X</em> ∼ POIS(3.22). In other words, you’d expect to see an average of 3.22 blemishes on your 1-foot sheets.</p>&#13;
<h5 class="h5" id="ch16lev3sec48"><span epub:type="pagebreak" id="page_340"/><strong>The dpois and ppois Functions</strong></h5>&#13;
<p class="noindent">The R <code>dpois</code> function provides the individual Poisson mass function probabilities Pr(<em>X</em> = <em>x</em>) for the Poisson distribution. The <code>ppois</code> function provides the left cumulative probabilities, as in Pr(<em>X</em> ≤ <em>x</em>). Consider the following lines of code:</p>&#13;
<pre>R&gt; dpois(x=3,lambda=3.22)<br/>[1] 0.2223249<br/>R&gt; dpois(x=0,lambda=3.22)<br/>[1] 0.03995506<br/>R&gt; round(dpois(0:10,3.22),3)<br/>[1] 0.040 0.129 0.207 0.222 0.179 0.115 0.062 0.028 0.011 0.004 0.001</pre>&#13;
<p class="indent">The first call finds that Pr(<em>X</em> = 3) = 0.22 (to 2 d.p.); in other words, the probability that you observe exactly three blemishes on a randomly selected piece of sheet metal is equal to about 0.22. The second call indicates a less than 4 percent chance that the piece is flawless. The third line returns a rounded version of the relevant mass function for the values 0 ≤ <em>x</em> ≤ 10. By hand you can confirm the first result like this:</p>&#13;
<pre>R&gt; (3.22^3*exp(-3.22))/prod(3:1)<br/>[1] 0.2223249</pre>&#13;
<p class="indent">You create a visualization of the mass function with the following:</p>&#13;
<pre>R&gt; barplot(dpois(x=0:10,lambda=3.22),ylim=c(0,0.25),space=0,<br/>           names.arg=0:10,ylab="Pr(X=x)",xlab="x")</pre>&#13;
<p class="indent">This is shown on the left of <a href="ch16.xhtml#ch16fig3">Figure 16-3</a>.</p>&#13;
<div class="image"><img src="../images/f16-03.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch16fig3"/>Figure 16-3: The Poisson probability mass function (left) and cumulative distribution function (right) for</em> <em>λ<sub>p</sub> = 3.22 plotted for the integers 0</em> ≤ x ≤ <em>10, with reference to the sheet metal example</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_341"/>To calculate cumulative results, you use <code>ppois</code>.</p>&#13;
<pre>R&gt; ppois(q=2,lambda=3.22)<br/>[1] 0.3757454<br/>R&gt; 1-ppois(q=5,lambda=3.22)<br/>[1] 0.1077005</pre>&#13;
<p class="indent">These lines find that the probability you observe at most two imperfections, Pr(<em>X</em> ≤ 2), is about 0.38, and the probability you observe strictly more than five blemishes, Pr(<em>X</em> ≥ 6), is roughly 0.11.</p>&#13;
<p class="indent">A visualization of the cumulative mass function is given on the right of <a href="ch16.xhtml#ch16fig3">Figure 16-3</a>, created with the following:</p>&#13;
<pre>R&gt; barplot(ppois(q=0:10,lambda=3.22),ylim=0:1,space=0,<br/>           names.arg=0:10,ylab="Pr(X&lt;=x)",xlab="x")</pre>&#13;
<h5 class="h5" id="ch16lev3sec49"><strong>The qpois Function</strong></h5>&#13;
<p class="noindent">The <code>q</code>-function for the Poisson distribution, <code>qpois</code>, provides the inverse of <code>ppois</code>, in the same way as <code>qbinom</code> in <a href="ch16.xhtml#ch16lev2sec138">Section 16.1.2</a> provides the inverse of <code>pbinom</code>.</p>&#13;
<h5 class="h5" id="ch16lev3sec50"><strong>The rpois Function</strong></h5>&#13;
<p class="noindent">To produce random variates, you use <code>rpois</code>; you supply the number of variates you want as <code>n</code> and supply the all-important parameter as <code>lambda</code>. You can imagine</p>&#13;
<pre>R&gt; rpois(n=15,lambda=3.22)<br/>[1] 0 2 9 1 3 1 9 3 4 3 2 2 6 3 5</pre>&#13;
<p class="noindent">as selecting fifteen 1-foot-square metal sheets from the production line at random and counting the number of blemishes on each. Note again that this is random generation; your specific results are likely to vary.</p>&#13;
<div class="ex">&#13;
<p class="ext"><a id="ch16exc2"/><strong>Exercise 16.2</strong></p>&#13;
<p class="noindentz">Every Saturday, at the same time, an individual stands by the side of a road and tallies the number of cars going by within a 120-minute window. Based on previous knowledge, she believes that the mean number of cars going by during this time is exactly 107. Let <em>X</em> represent the appropriate Poisson random variable of the number of cars passing her position in each Saturday session.</p>&#13;
<ol type="a">&#13;
<li><p class="noindents">What is the probability that more than 100 cars pass her on any given Saturday?</p></li>&#13;
<li><p class="noindents">Determine the probability that no cars pass.</p></li>&#13;
<li><p class="noindents"><span epub:type="pagebreak" id="page_342"/>Plot the relevant Poisson mass function over the values in 60 ≤ <em>x</em> ≤ 150.</p></li>&#13;
<li><p class="noindents">Simulate 260 results from this distribution (about five years of weekly Saturday monitoring sessions). Plot the simulated results using <code>hist</code>; use <code>xlim</code> to set the horizontal limits from 60 to 150. Compare your histogram to the shape of your mass function from (c).</p></li>&#13;
</ol>&#13;
</div>&#13;
<h4 class="h4" id="ch16lev2sec140"><strong><em>16.1.4 Other Mass Functions</em></strong></h4>&#13;
<p class="noindentb">There are many other well-defined probability mass functions in R’s built-in suite of statistical calculations. All model a discrete random variable in a certain way under certain conditions and are defined with at least one parameter, and most are represented by their own set of <code>d</code>-, <code>p</code>-, <code>q</code>-, and <code>r</code>-functions. Here I summarize a few more:</p>&#13;
<p class="bull">• The <em>geometric</em> distribution counts the number of failures before a success is recorded and is dependent on a “probability of success parameter” <code>prob</code>. Its functions are <code>dgeom</code>, <code>pgeom</code>, <code>qgeom</code>, and <code>rgeom</code>.</p>&#13;
<p class="bull">• The <em>negative binomial</em> distribution is a generalization of the geometric distribution, dependent upon parameters <code>size</code> (number of trials) and <code>prob</code>. Its functions are <code>dnbinom</code>, <code>pnbinom</code>, <code>qnbinom</code>, and <code>rnbinom</code>.</p>&#13;
<p class="bull">• The <em>hypergeometric</em> distribution is used to model sampling without replacement (in other words, a “success” can change the probabilities associated with further successes), dependent upon parameters <code>m</code>, <code>n</code>, and <code>k</code> describing the nature of sampled items. Its functions are <code>dhyper</code>, <code>phyper</code>, <code>qhyper</code>, and <code>rhyper</code>.</p>&#13;
<p class="bull">• The <em>multinomial</em> distribution is a generalization of the binomial, where a success can occur in one of multiple categories at each trial, with parameters <code>size</code> and <code>prob</code> (this time, <code>prob</code> must be a vector of probabilities corresponding to the multiple categories). Its built-in functions are limited to <code>dmultinom</code> and <code>rmultinom</code>.</p>&#13;
<p class="indentt">As noted earlier, some familiar probability distributions are just simplifications or special cases of functions that describe a more general class of distributions.</p>&#13;
<h3 class="h3" id="ch16lev1sec51"><strong>16.2 Common Probability Density Functions</strong></h3>&#13;
<p class="noindent">When considering continuous random variables, you need to deal with probability density functions. There are a number of common continuous probability distributions frequently used over many different types of problems. In this section, you’ll be familiarized with some of these and R’s accompanying <code>d</code>-, <code>p</code>-, <code>q</code>-, and <code>r</code>-functions.</p>&#13;
<h4 class="h4" id="ch16lev2sec141"><span epub:type="pagebreak" id="page_343"/><strong><em>16.2.1 Uniform</em></strong></h4>&#13;
<p class="noindent">The <em>uniform</em> distribution is a simple density function that describes a continuous random variable whose interval of possible values offers no fluctuations in probability. This will become clear in a moment when you plot <a href="ch16.xhtml#ch16fig4">Figure 16-4</a>.</p>&#13;
<div class="image"><img src="../images/f16-04.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch16fig4"/>Figure 16-4: Two uniform distributions plotted on the same scale for comparability. Left:</em> X ∼ <em>UNIF(</em>−<em>0.4,1.1); right:</em> X ∼ <em>UNIF(0.223,0.410). The total area underneath each density function is, as always, 1.</em></p>&#13;
<p class="indent">For a continuous random variable <em>a</em> ≤ <em>X</em> ≤ <em>b</em>, the uniform density function <em>f</em> is</p>&#13;
<div class="imagec"><a id="ch16eq5"/><img src="../images/e16-5.jpg" alt="image"/></div>&#13;
<p class="noindent">where <em>a</em> and <em>b</em> are parameters of the distribution defining the limits of the possible values <em>X</em> can take. The notation</p>&#13;
<p class="center"><em>X</em> ∼ UNIF(<em>a</em>, <em>b</em>)</p>&#13;
<p class="noindent">is often used to indicate that “<em>X</em> follows a uniform distribution with limits <em>a</em> and <em>b</em>.”</p>&#13;
<p class="indentb">The following are the key points to remember:</p>&#13;
<p class="bull">• <em>X</em> can take any value in the interval bounded by <em>a</em> and <em>b</em>.</p>&#13;
<p class="bull">• <em>a</em> and <em>b</em> can be any values, provided that <em>a</em> &lt; <em>b</em>, and they represent the lower and upper limits, respectively, of the interval of possible values.</p>&#13;
<p class="indentt">The mean and variance are as follows:</p>&#13;
<div class="imagec"><img src="../images/f0343-01.jpg" alt="image"/></div>&#13;
<p class="indent">For the more complicated densities in this section, it’s especially useful to visualize the functions in order to understand the probabilistic structure associated with a continuous random variable. For the uniform distribution, <span epub:type="pagebreak" id="page_344"/>given <a href="ch16.xhtml#ch16eq5">Equation (16.5)</a>, you can recognize the two different uniform distributions shown in <a href="ch16.xhtml#ch16fig4">Figure 16-4</a>. I’ll provide the code to produce these types of plots shortly.</p>&#13;
<p class="indent">For the left plot in <a href="ch16.xhtml#ch16fig4">Figure 16-4</a>, you can confirm the exact height of the <em>X</em> ∼ UNIF(−0.4,1.1) density by hand: <img class="middle" src="../images/f0344-01.jpg" alt="image"/>. For the plot on the right, based on <em>X</em> ∼ UNIF(0.223,0.410), you can use R to find that its height is roughly 5.35.</p>&#13;
<pre>R&gt; 1/(0.41-0.223)<br/>[1] 5.347594</pre>&#13;
<h5 class="h5" id="ch16lev3sec51"><strong>The dunif Function</strong></h5>&#13;
<p class="noindent">You can use the built-in <code>d</code>-function for the uniform distribution, <code>dunif</code>, to return these heights for any value within the defined interval. The <code>dunif</code> command returns zero for values outside of the interval. The parameters of the distribution, <em>a</em> and <em>b</em>, are provided as the arguments <code>min</code> and <code>max</code>, respectively. For example, the line</p>&#13;
<pre>R&gt; dunif(x=c(-2,-0.33,0,0.5,1.05,1.2),min=-0.4,max=1.1)<br/>[1] 0.0000000 0.6666667 0.6666667 0.6666667 0.6666667 0.0000000</pre>&#13;
<p class="noindent">evaluates the uniform density function of <em>X</em> ∼ UNIF(−0.4,1.1) at the values given in the vector passed to <code>x</code>. You’ll notice that the first and last values fall outside the bounds defined by <code>min</code> and <code>max</code>, and so they are zero. All others evaluate to the height value of <img class="middle" src="../images/2by3.jpg" alt="image"/>, as previously calculated.</p>&#13;
<p class="indent">As a second example, the line</p>&#13;
<pre>R&gt; dunif(x=c(0.3,0,0.41),min=0.223,max=0.41)<br/>[1] 5.347594 0.000000 5.347594</pre>&#13;
<p class="noindent">confirms the correct density values for the <em>X</em> ∼ UNIF(0.223,0.410) distribution, with the second value, zero, falling outside the defined interval.</p>&#13;
<p class="indent">This most recent example in particular should remind you that probability density functions for continuous random variables, unlike mass functions for discrete variables, <em>do not</em> directly provide probabilities, as mentioned in <a href="ch15.xhtml#ch15lev2sec135">Section 15.2.3</a>. In other words, the results just returned by <code>dunif</code> represent the respective density functions themselves and not any notion of chance attached to the specific values of <em>x</em> at which they were evaluated.</p>&#13;
<p class="indent">To calculate some probabilities based on the uniform density function, use the example of a faulty drill press. In a woodworker’s shop, imagine there’s a drill press that cannot keep to a constant alignment when in use; instead, it randomly hits the intended target at up to 0.4 cm to the left or 1.1 cm to the right. Let the random variable <em>X</em> ∼ UNIF(−0.4,1.1) represent where the drill hits the material relative to the target at 0. <a href="ch16.xhtml#ch16fig5">Figure 16-5</a> replots the left image of <a href="ch16.xhtml#ch16fig4">Figure 16-4</a> on a more detailed scale. You have three versions, each marking off a different area under the density function: Pr(<em>X</em> ≤ −0.21), Pr(−0.21 ≤ <em>X</em> ≤ 0.6), and Pr(<em>X</em> ≥ 0.6).</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_345"/><img src="../images/f16-05.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch16fig5"/>Figure 16-5: Three areas underneath the</em> X ∼ <em>UNIF(</em>−<em>0.4,1.1) density function for the drill press example. Left: Pr(</em>X ≤ −<em>0.21); middle: Pr(</em>−<em>0.21</em> ≤ X ≤ <em>0.6); right: Pr(</em>X ≥ <em>0.6).</em></p>&#13;
<p class="indent">These plots are created using the coordinate-based plotting skills covered in <a href="ch07.xhtml#ch07">Chapter 7</a>. The density itself is presented with the following:</p>&#13;
<pre>R&gt; a1 &lt;- -4/10<br/>R&gt; b1 &lt;- 11/10<br/>R&gt; unif1 &lt;- 1/(b1-a1)<br/>R&gt; plot(c(a1,b1),rep(unif1,2),type="o",pch=19,xlim=c(a1-1/10,b1+1/10),<br/>        ylim=c(0,0.75),ylab="f(x)",xlab="x")<br/>R&gt; abline(h=0,lty=2)<br/>R&gt; segments(c(a1-2,b1+2,a1,b1),rep(0,4),rep(c(a1,b1),2),rep(c(0,unif1),each=2),<br/>            lty=rep(1:2,each=2))<br/>R&gt; points(c(a1,b1),c(0,0))</pre>&#13;
<p class="indent">You can use much of the same code to produce the plots in <a href="ch16.xhtml#ch16fig4">Figure 16-4</a> by modifying the <code>xlim</code> and <code>ylim</code> arguments to adjust the scale of the axes.</p>&#13;
<p class="indent">You add the vertical lines denoting <em>f</em> (−0.21) and <em>f</em> (0.6) in <a href="ch16.xhtml#ch16fig5">Figure 16-5</a> with another call to <code>segments</code>.</p>&#13;
<pre>R&gt; segments(c(-0.21,0.6),c(0,0),c(-0.21,0.6),rep(unif1,2),lty=3)</pre>&#13;
<p class="indent">Finally, you can shade the areas using the <code>polygon</code> function, which was first explored in <a href="ch15.xhtml#ch15lev2sec135">Section 15.2.3</a>. For example, in the leftmost plot in <a href="ch16.xhtml#ch16fig5">Figure 16-5</a>, use the previous plotting code followed by this:</p>&#13;
<pre>R&gt; polygon(rbind(c(a1,0),c(a1,unif1),c(-0.21,unif1),c(-0.21,0)),col="gray",<br/>           border=NA)</pre>&#13;
<p class="indent">As mentioned earlier, the three shaded areas in <a href="ch16.xhtml#ch16fig5">Figure 16-5</a> represent, from left to right, Pr(<em>X</em> &lt; −0.21), Pr(−0.21 &lt; <em>X</em> &lt; 0.6), and Pr(<em>X</em> &gt; 0.6), respectively. In terms of the drill press example, you can interpret these as the probability that the drill hits the target 0.21 cm to the left or more, the probability that the drill hits the target between 0.21 cm to the left and 0.6 cm to the right, and the probability that the drill hits the target 0.6 cm to the right or more, respectively. (Remember from <a href="ch15.xhtml#ch15lev2sec135">Section 15.2.3</a> that it makes no difference if you use ≤ or &lt; (or ≥ or &gt;) for probabilities <span epub:type="pagebreak" id="page_346"/>associated with continuous random variables.) Though you could evaluate these probabilities geometrically for such a simple density function, it’s still faster to use R.</p>&#13;
<h5 class="h5" id="ch16lev3sec52"><strong>The punif Function</strong></h5>&#13;
<p class="noindent">Remember that probabilities associated with continuous random variables are defined as <em>areas under the function</em>, and therefore your study focuses on the appropriate intervals of <em>X</em> rather than any specific value. The <code>p</code>-function for densities, just like the <code>p</code>-function for discrete random variables, provides the cumulative probability distribution Pr(<em>X</em> ≤ <em>x</em>). In the context of the uniform density, this means that given a specific value of <em>x</em> (supplied as a “quantile” argument <code>q</code>), <code>punif</code> will provide the left-directional area underneath the function from that specific value.</p>&#13;
<p class="indent">Accessing <code>punif</code>, the line</p>&#13;
<pre>R&gt; punif(-0.21,min=a1,max=b1)<br/>[1] 0.1266667</pre>&#13;
<p class="noindent">tells you that the leftmost area in <a href="ch16.xhtml#ch16fig5">Figure 16-5</a> represents a probability of about 0.127. The line</p>&#13;
<pre>R&gt; 1-punif(q=0.6,min=a1,max=b1)<br/>[1] 0.3333333</pre>&#13;
<p class="indent">tells you that <img class="middle" src="../images/f0346-01.jpg" alt="image"/>. The final result for Pr(−0.21 &lt; <em>X</em> &lt; 0.6), giving a 54 percent chance, is found with</p>&#13;
<pre>R&gt; punif(q=0.6,min=a1,max=b1) - punif(q=-0.21,min=a1,max=b1)<br/>[1] 0.54</pre>&#13;
<p class="noindent">since the first call provides the area under the density from 0.6 all the way left and the second call provides the area from −0.21 all the way left. Therefore, this difference is the middle area as defined.</p>&#13;
<p class="indent">It’s essential to be able to manipulate cumulative probability results like this when working with probability distributions in R, and the beginner might find it useful to sketch out the desired area before using <code>p</code>-functions, especially with respect to density functions.</p>&#13;
<h5 class="h5" id="ch16lev3sec53"><strong>The qunif Function</strong></h5>&#13;
<p class="noindent">The <code>q</code>-functions for densities are used more often than they are for mass functions because the continuous nature of the variable means that a unique quantile value can be found for any valid probability <code>p</code>.</p>&#13;
<p class="indent">The <code>qunif</code> function is the inverse of <code>punif</code>:</p>&#13;
<pre>R&gt; qunif(p=0.1266667,min=a1,max=b1)<br/>[1] -0.21<br/>R&gt; qunif(p=1-1/3,min=a1,max=b1)<br/>[1] 0.6</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_347"/>These lines confirm the values of <em>X</em> used earlier to get the lower- and upper-tail probabilities Pr(<em>X</em> &lt; −0.21) and Pr(<em>X</em> &gt; 0.6), respectively. Any <code>q</code>-function expects a <em>cumulative</em> (in other words, left-hand) probability as its first argument, which is why you need to supply <code>1-1/3</code> in the second line to recover <code>0.6</code>. (The total area is 1. You know that you want the area to the <em>right</em> of 0.6 to be <img class="middle" src="../images/1by3.jpg" alt="image"/>; thus, the area on the left must be <img class="middle" src="../images/f0347-01.jpg" alt="image"/>.)</p>&#13;
<h5 class="h5" id="ch16lev3sec54"><strong>The runif Function</strong></h5>&#13;
<p class="noindent">Lastly, to generate random realizations of a specific uniform distribution, you use <code>runif</code>. Let’s say the woodworker drills 10 separate holes using the faulty press; you can simulate one instance of the position of each of these holes relative to their target with the following call.</p>&#13;
<pre>R&gt; runif(n=10,min=a1,max=b1)<br/> [1] -0.2429272 -0.1226586  0.9318365 0.4829028 0.5963365<br/> [6]  0.2009347  0.3073956 -0.1416678 0.5551469 0.4033372</pre>&#13;
<p class="indent">Again, note that the specific values of <code>r</code>-function calls like <code>runif</code> will be different each time they are run.</p>&#13;
<div class="ex">&#13;
<p class="ext"><a id="ch16exc3"/><strong>Exercise 16.3</strong></p>&#13;
<p class="noindentz">You visit a national park and are informed that the height of a certain species of tree found in the forest is uniformly distributed between 3 and 70 feet.</p>&#13;
<ol type="a">&#13;
<li><p class="noindents">What is the probability you encounter a tree shorter than <img class="middle" src="../images/f0347-02.jpg" alt="image"/> feet?</p></li>&#13;
<li><p class="noindents">For this probability density function, what is the height that marks the cutoff point of the tallest 15 percent of trees?</p></li>&#13;
<li><p class="noindents">Evaluate the mean and standard deviation of the tree height distribution.</p></li>&#13;
<li><p class="noindents">Using (c), confirm that the chance that you encounter a tree with a height that is within half a standard deviation (that is, below or above) of the mean height is roughly 28.9 percent.</p></li>&#13;
<li><p class="noindents">At what height is the density function itself? Show it in a plot.</p></li>&#13;
<li><p class="noindents">Simulate 10 observed tree heights. Based on these data, use <code>quantile</code> (refer to <a href="ch13.xhtml#ch13lev2sec118">Section 13.2.3</a>) to estimate the answer you arrived at in (b). Repeat your simulation, this time generating 1,000 variates, and estimate (b) again. Do this a handful of times, taking a mental note of your two estimates each time. Overall, what do you notice of your two estimates (one based on 10 variates at a time and the other based on 1,000) with respect to the “true” value in (b)?</p></li>&#13;
</ol>&#13;
</div>&#13;
<h4 class="h4" id="ch16lev2sec142"><span epub:type="pagebreak" id="page_348"/><strong><em>16.2.2 Normal</em></strong></h4>&#13;
<p class="noindent">The <em>normal distribution</em> is one of the most well-known and commonly applied probability distributions in modeling continuous random variables. Characterized by a distinctive “bell-shaped” curve, it’s also referred to as the <em>Gaussian</em> distribution.</p>&#13;
<p class="indent">For a continuous random variable −∞ &lt; <em>X</em> &lt; ∞, the normal density function <em>f</em> is</p>&#13;
<div class="imagec"><a id="ch16eq6"/><img src="../images/e16-6.jpg" alt="image"/></div>&#13;
<p class="noindent">where <em>μ</em> and <em>σ</em> are parameters of the distribution, <em>π</em> is the familiar geometric value 3.1415 ..., and exp{·} is the exponential function (refer to <a href="ch02.xhtml#ch02lev2sec18">Section 2.1.2</a>). The notation</p>&#13;
<p class="center"><em>X</em> ∼ N(<em>μ</em>, <em>σ</em>)</p>&#13;
<p class="noindent">is often used to indicate that “<em>X</em> follows a normal distribution with mean <em>μ</em> and standard deviation <em>σ</em>.”</p>&#13;
<p class="indentb">The following are the key points to remember:</p>&#13;
<p class="bull">• Theoretically, <em>X</em> can take any value from −∞ to ∞.</p>&#13;
<p class="bull">• As hinted at earlier, the parameters <em>μ</em> and <em>σ</em> directly describe the mean and the standard deviation of the distribution, with the square of the latter, <em>σ</em><sup>2</sup>, being the variance.</p>&#13;
<p class="bull">• In practice, the mean parameter is finite −∞ &lt; <em>μ</em> &lt; ∞, and the standard deviation parameter is strictly positive and finite 0 &lt; <em>σ</em> &lt; ∞.</p>&#13;
<p class="bull">• If you have a random variable <em>X</em> ∼ N(<em>μ</em>, <em>σ</em>), then you can create a new random variable <em>Z</em> = (<em>X</em> − <em>μ</em>)/σ, which means <em>Z</em> ∼ N(0,1). This is known as <em>standardization</em> of <em>X</em>.</p>&#13;
<p class="indentt">The two parameters noted earlier fully define a particular normal distribution. These are always perfectly symmetric, unimodal, and centered on the mean <em>μ</em>, and they have a degree of “spread” defined using the standard deviation <em>σ</em>.</p>&#13;
<p class="indent">The top image of <a href="ch16.xhtml#ch16fig6">Figure 16-6</a> provides the density functions for four specific normal distributions. You can see that altering the mean results in a translation, where the center of the distribution is simply shifted to sit on the specific value of <em>μ</em>. The effect of a smaller standard deviation is to reduce the spread, resulting in a taller, skinnier appearance of the density. Increasing <em>σ</em> flattens the density out around the mean.</p>&#13;
<p class="indent">The bottom image zooms in on the N(0,1) distribution when you have a normal density centered on <em>μ</em> = 0 and with a standard deviation of <em>σ</em> = 1. This distribution, known as the <em>standard normal</em>, is frequently used as a standard reference to compare different normally distributed random variables with one another on the same scale of probabilities. It’s common practice to rescale, or <em>standardize</em>, some variable <em>X</em> ∼ N(<em>μ<sub>X</sub></em> ,σ<sub><em>X</em></sub>) to a new variable <em>Z</em> such that <em>Z</em> ∼ N(0,1) (you’ll see this practiced in <a href="ch18.xhtml#ch18">Chapter 18</a>). Vertical lines in the plot show plus or minus one, two, and three times the standard deviation away from the mean of zero. This serves to highlight the fact that for <em>any</em> <span epub:type="pagebreak" id="page_349"/>normal distribution, a probability of exactly 0.5 lies either above or below the mean. Furthermore, note that there’s a probability of approximately 0.683 of a value falling within one standard deviation of the mean, approximately 0.954 probability under the curve from −2<em>σ</em> to +2σ, and approximately 0.997 probability between −3<em>σ</em> and +3σ.</p>&#13;
<div class="image"><img src="../images/f16-06.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch16fig6"/>Figure 16-6: Illustrating the normal distribution. Top: Four different instances of the density achieved by varying the mean</em> <em>μ</em> <em>and standard deviation</em> <em>σ. Bottom: The “standard normal” distribution, N(0,1), marking off the mean</em> ±<em>1</em>σ, ±<em>2</em>σ<em>, and</em> ±<em>3</em>σ.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The mathematical definition of the normal density means that as you move further away from the mean, the value of the density function itself will approach zero. In actual fact, any normal density function never actually touches the horizontal line at zero; it just gets closer and closer as you move to positive or negative infinity. This behavior is formally referred to as</em> asymptotic<em>; in this case, you’d say that the normal distribution f</em> (<em>x</em>) <em>has a</em> horizontal asymptote <em>at f</em> (<em>x</em>) <em>= 0. In discussing probabilities as areas under the curve, you’d refer to the fact that “the total area under the curve from negative to positive infinity” is 1, in other words, <img class="middle" src="../images/f0349-01.jpg" alt="image"/>.</em></p>&#13;
</div>&#13;
<h5 class="h5" id="ch16lev3sec55"><span epub:type="pagebreak" id="page_350"/><strong>The dnorm Function</strong></h5>&#13;
<p class="noindent">Being a probability density function, the <code>dnorm</code> function itself doesn’t provide probabilities—merely the value of the desired normal function curve <em>f</em> (<em>x</em>) at any <em>x</em>. To plot a normal density, you’d therefore be able to use <code>seq</code> (refer to <a href="ch02.xhtml#ch02lev2sec21">Section 2.3.2</a>) to create a fine sequence of values for <em>x</em>, evaluate the density at these values with <code>dnorm</code>, and then plot the result as a line. For example, to produce an image of the standard normal distribution curve similar to that in the bottom image of <a href="ch16.xhtml#ch16fig6">Figure 16-6</a>, the following code will create the desired <em>x</em> values as <code>xvals</code>.</p>&#13;
<pre>R&gt; xvals &lt;- seq(-4,4,length=50)<br/>R&gt; fx &lt;- dnorm(xvals,mean=0,sd=1)<br/>R&gt; fx<br/> [1] 0.0001338302 0.0002537388 0.0004684284 0.0008420216 0.0014737603<br/> [6] 0.0025116210 0.0041677820 0.0067340995 0.0105944324 0.0162292891<br/>[11] 0.0242072211 0.0351571786 0.0497172078 0.0684578227 0.0917831740<br/>[16] 0.1198192782 0.1523049307 0.1885058641 0.2271744074 0.2665738719<br/>[21] 0.3045786052 0.3388479358 0.3670573564 0.3871565916 0.3976152387<br/>[26] 0.3976152387 0.3871565916 0.3670573564 0.3388479358 0.3045786052<br/>[31] 0.2665738719 0.2271744074 0.1885058641 0.1523049307 0.1198192782<br/>[36] 0.0917831740 0.0684578227 0.0497172078 0.0351571786 0.0242072211<br/>[41] 0.0162292891 0.0105944324 0.0067340995 0.0041677820 0.0025116210<br/>[46] 0.0014737603 0.0008420216 0.0004684284 0.0002537388 0.0001338302</pre>&#13;
<p class="indent">Then <code>dnorm</code>, which includes specification of <em>μ</em> as <code>mean</code> and <em>σ</em> as <code>sigma</code>, produces the precise values of <em>f</em>(<em>x</em>) at those <code>xvals</code>. Finally, a call such as <code>plot(xvals,fx,type="l")</code> achieves the bare bones of a density plot, which you can easily enhance by adding titles and using commands such as <code>abline</code> and <code>segments</code> to mark locations off (I’ll produce another plot in a moment, so this basic one isn’t shown here).</p>&#13;
<p class="indent">Note that if you don’t supply any values to <code>mean</code> and <code>sd</code>, the default behavior of R is to implement the standard normal distribution; the object <code>fx</code> shown earlier could have been created with an even shorter call using just <code>dnorm(xvals)</code>.</p>&#13;
<h5 class="h5" id="ch16lev3sec56"><strong>The pnorm Function</strong></h5>&#13;
<p class="noindent">The <code>pnorm</code> function obtains left-side probabilities under the specified normal density. As with <code>dnorm</code>, if no parameter values are supplied, R automatically sets <code>mean=0</code> and <code>sd=1</code>. In the same way you used <code>punif</code> in <a href="ch16.xhtml#ch16lev2sec141">Section 16.2.1</a>, you can find differences of results from <code>pnorm</code> to find any areas you want when you provide the function with the desired values in the argument <code>q</code>.</p>&#13;
<p class="indent">For example, it was mentioned earlier that a probability of approximately 0.683 lies within one standard deviation of the mean. You can confirm this using <code>pnorm</code> for the standard normal.</p>&#13;
<pre>R&gt; pnorm(q=1)-pnorm(q=-1)<br/>[1] 0.6826895</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_351"/>The first call to <code>pnorm</code> evaluates the area under the curve from positive 1 left (in other words, all the way to −∞) and then finds the difference between that and the area from −1 left. The result reflects the proportion between the two dashed lines in the bottom of <a href="ch16.xhtml#ch16fig6">Figure 16-6</a>. These kinds of probabilities will be the same for <em>any</em> normal distribution. Consider the distribution where <em>μ</em> = −3.42 and <em>σ</em> = 0.2. Then the following provides the same value:</p>&#13;
<pre>R&gt; mu &lt;- -3.42<br/>R&gt; sigma &lt;- 0.2<br/>R&gt; mu.minus.1sig &lt;- mu-sigma<br/>R&gt; mu.minus.1sig<br/>[1] -3.62<br/>R&gt; mu.plus.1sig &lt;- mu+sigma<br/>R&gt; mu.plus.1sig<br/>[1] -3.22<br/>R&gt; pnorm(q=mu.plus.1sig,mean=mu,sd=sigma) -<br/>   pnorm(q=mu.minus.1sig,mean=mu,sd=sigma)<br/>[1] 0.6826895</pre>&#13;
<p class="indent">It takes a little more work to specify the distribution of interest since it’s not standard, but the principle is the same: plus and minus one standard deviation away from the mean.</p>&#13;
<p class="indent">The symmetry of the normal distribution is also useful when it comes to calculating probabilities. Sticking with the N(3.42,0.2) distribution, you can see that the probability you make an observation greater than <em>μ</em> + <em>σ</em> = −3.42 + 0.2 = −3.22 (an <em>upper-tail</em> probability) is identical to the probability of making an observation less than <em>μ</em> − <em>σ</em> = −3.42 − 0.2 = −3.62 (a <em>lower-tail</em> probability).</p>&#13;
<pre>R&gt; 1-pnorm(mu.plus.1sig,mu,sigma)<br/>[1] 0.1586553<br/>R&gt; pnorm(mu.minus.1sig,mu,sigma)<br/>[1] 0.1586553</pre>&#13;
<p class="indent">You can also evaluate these values by hand, given the result you computed earlier that says Pr(μ − <em>σ</em> &lt; <em>X</em> &lt; <em>μ</em> + <em>σ</em>) = 0.6826895. The remaining probability <em>outside</em> of this middle area must be as follows:</p>&#13;
<pre>R&gt; 1-0.6826895<br/>[1] 0.3173105</pre>&#13;
<p class="indent">So, in each of the lower- and upper-tail areas marked off by <em>μ</em> − <em>σ</em> and <em>μ</em> + <em>σ</em>, respectively, there must be a probability of the following:</p>&#13;
<pre>R&gt; 0.3173105/2<br/>[1] 0.1586552</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_352"/>This is what was just found using <code>pnorm</code> (note that there can be some minor rounding errors in these types of calculations). You can see this in <a href="ch16.xhtml#ch16fig7">Figure 16-7</a>, which is, initially, plotted with the following:</p>&#13;
<pre>R&gt; xvals &lt;- seq(-5,-2,length=300)<br/>R&gt; fx &lt;- dnorm(xvals,mean=mu,sd=sigma)<br/>R&gt; plot(xvals,fx,type="l",xlim=c(-4.4,-2.5),main="N(-3.42,0.2) distribution",<br/>        xlab="x",ylab="f(x)")<br/>R&gt; abline(h=0,col="gray")<br/>R&gt; abline(v=c(mu.plus.1sig,mu.minus.1sig),lty=3:2)<br/>R&gt; legend("topleft",legend=c("-3.62\n(mean - 1 sd)","\n-3.22\n(mean + 1 sd)"),<br/>          lty=2:3,bty="n")</pre>&#13;
<div class="image"><img src="../images/f16-07.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch16fig7"/>Figure 16-7: Illustrating the example in the text, where the symmetry of the normal distribution is used to point out features of probabilities under the curve. Note that the total area under the density is 1, which in conjunction with symmetry is useful for making calculations.</em></p>&#13;
<p class="indent">To add the shaded area between <em>μ</em> ± <em>σ</em>, you can use <code>polygon</code>, for which you need the vertices of the shape of interest. To get a smooth curve, make use of the fine sequence <code>xvals</code> and corresponding <code>fx</code> as defined in the code, and use logical vector subsetting to restrict attention to those locations of <em>x</em> such that −3.62 ≤ <em>x</em> ≤ −3.22.</p>&#13;
<pre><span epub:type="pagebreak" id="page_353"/>R&gt; xvals.sub &lt;- xvals[xvals&gt;=mu.minus.1sig &amp; xvals&lt;=mu.plus.1sig]<br/>R&gt; fx.sub &lt;- fx[xvals&gt;=mu.minus.1sig &amp; xvals&lt;=mu.plus.1sig]</pre>&#13;
<p class="indent">You can then sandwich these between the two corners at the bottom of the shaded polygon using the matrix structure that the <code>polygon</code> function expects.</p>&#13;
<pre>R&gt; polygon(rbind(c(mu.minus.1sig,0),cbind(xvals.sub,fx.sub),c(mu.plus.1sig,0)),<br/>           border=NA,col="gray")</pre>&#13;
<p class="indent">Finally, <code>arrows</code> and <code>text</code> indicate the areas discussed in the text.</p>&#13;
<pre>R&gt; arrows(c(-4.2,-2.7,-2.9),c(0.5,0.5,1.2),c(-3.7,-3.15,-3.4),c(0.2,0.2,1))<br/>R&gt; text(c(-4.2,-2.7,-2.9),c(0.5,0.5,1.2)+0.05,<br/>        labels=c("0.159","0.159","0.682"))</pre>&#13;
<h5 class="h5" id="ch16lev3sec57"><strong>The qnorm Function</strong></h5>&#13;
<p class="noindent">Let’s turn to <code>qnorm</code>. To find the quantile value that will give you a lower-tail probability of 0.159, you use the following:</p>&#13;
<pre>R&gt; qnorm(p=0.159,mean=mu,sd=sigma)<br/>[1] -3.619715</pre>&#13;
<p class="indent">Given the earlier results and what you already know about previous <code>q-</code>functions, it should be clear why the result is a value of approximately −3.62. You find the upper quartile (the value <em>above which</em> you’d find a probability of 0.25) with this:</p>&#13;
<pre>R&gt; qnorm(p=1-0.25,mean=mu,sd=sigma)<br/>[1] -3.285102</pre>&#13;
<p class="indent">Remember that the <code>q</code>-function will operate based on the (left) lower-tail probability, so to find a quantile based on an upper-tail probability, you must first subtract it from the total probability of 1.</p>&#13;
<p class="indent">In some methods and models used in frequentist statistics, it’s common to assume that your observed data are normal. You can test the validity of this assumption by using your knowledge of the theoretical quantiles of the normal distribution, found in the results of <code>qnorm</code>: calculate a range of sample quantile values for your observed data and plot these against the same quantiles for a correspondingly standardized normal distribution. This visual tool is referred to as a normal <em>quantile-quantile</em> or <em>QQ</em> plot and is useful when viewed alongside a histogram. If the plotted points don’t lie on a straight line, then the quantiles from your data do not match the appearance of those from a normal curve, and the assumption that your data are normal may not be valid.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_354"/>The built-in <code>qqnorm</code> function takes in your raw data and produces the corresponding plot. Go back once more to the ready-to-use <code>chickwts</code> data set. Let’s say you want to find out whether it’s reasonable to assume the weights are normally distributed. To that end, you use</p>&#13;
<pre>R&gt; hist(chickwts$weight,main="",xlab="weight")<br/>R&gt; qqnorm(chickwts$weight,main="Normal QQ plot of weights")<br/>R&gt; qqline(chickwts$weight,col="gray")</pre>&#13;
<p class="noindent">to produce the histogram of the 71 weights and the normal QQ plot given in <a href="ch16.xhtml#ch16fig8">Figure 16-8</a>. The additional <code>qqline</code> command adds the “optimal” line that the coordinates would lie along if the data were perfectly normal.</p>&#13;
<div class="image"><img src="../images/f16-08.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch16fig8"/>Figure 16-8: Histogram (left) and normal QQ plot (right) of the weights of chicks in the</em> <code>chickwts</code> <em>data set. Are the data normally distributed?</em></p>&#13;
<p class="indent">If you inspect the histogram of the weights, you can see that the data match the general appearance of a normal distribution, with a roughly symmetric unimodal appearance. That said, it doesn’t quite achieve the smoothness and naturally decaying height that produces the familiar normal bell shape. This is reflected in the QQ plot on the right; the central quantile values appear to lie on the line relatively well, except for some relatively minor “wiggles.” There are some clear discrepancies in the outer tails, but note that it is typical to observe discrepancies in these extreme quantiles in any QQ plot because fewer data points naturally occur there. Taking all of this into consideration, for this example the assumption of normality isn’t completely unreasonable.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>It’s important to consider the sample size when assessing the validity of these kinds of assumptions; the larger the sample size, the less random variability will creep into the histogram and QQ plot, and you can more confidently reach a conclusion about whether your data are normal. For instance, the assumption of normality in this example may be complicated by the fact there’s a relatively small sample size of only 71.</em></p>&#13;
</div>&#13;
<h5 class="h5" id="ch16lev3sec58"><span epub:type="pagebreak" id="page_355"/><strong>The rnorm Function</strong></h5>&#13;
<p class="noindent">Random variates of any given normal distribution are generated with <code>rnorm</code>; for example, the line</p>&#13;
<pre>R&gt; rnorm(n=7,mu,sigma)<br/>[1] -3.764532 -3.231154 -3.124965 -3.490482 -3.884633 -3.192205 -3.475835</pre>&#13;
<p class="noindent">produces seven normally distributed values arising from N(−3.42,0.2). In contrast to the QQ plot produced for the chick weights in <a href="ch16.xhtml#ch16fig8">Figure 16-8</a>, you can use <code>rnorm</code>, <code>qqnorm</code>, and <code>qqline</code> to examine the degree to which hypothetically observed data sets that are truly normal vary in the context of a QQ plot.</p>&#13;
<p class="indent">The following code generates 71 standard normal values and produces a corresponding normal QQ plot and then does the same for a separate data set of <em>n</em> = 710; these are displayed in <a href="ch16.xhtml#ch16fig9">Figure 16-9</a>.</p>&#13;
<pre>R&gt; fakedata1 &lt;- rnorm(n=71)<br/>R&gt; fakedata2 &lt;- rnorm(n=710)<br/>R&gt; qqnorm(fakedata1,main="Normal QQ plot of generated N(0,1) data; n=71")<br/>R&gt; qqline(fakedata1,col="gray")<br/>R&gt; qqnorm(fakedata2,main="Normal QQ plot of generated N(0,1) data; n=710")<br/>R&gt; qqline(fakedata2,col="gray")</pre>&#13;
<div class="image"><img src="../images/f16-09.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch16fig9"/>Figure 16-9: Normal QQ plots of 71 (left) and 710 (right) observations randomly generated from the standard normal distribution</em></p>&#13;
<p class="indent">You can see that the QQ plot for the simulated data set of size <em>n</em> = 71 shows similar deviation from the optimal line as does the chick weights data set. Bumping the sample size up by a factor of 10 shows that the QQ plot for the <em>n</em> = 710 normal observations offers up far less random variation, although visible discrepancies in the tails do still occur. A good way to get used to assessing these effects is to rerun these lines of code several times <span epub:type="pagebreak" id="page_356"/>(in other words, generating new data sets each time) and examine how each new QQ plot varies.</p>&#13;
<h5 class="h5" id="ch16lev3sec59"><strong>Normal Functions in Action: A Quick Example</strong></h5>&#13;
<p class="noindent">Let’s finish this section with one more working problem. Assume the manufacturer of a certain type of snack knows that the total net weight of the snacks in its 80-gram advertised package, <em>X</em>, is normally distributed with a mean of 80.2 grams and a standard deviation of 1.1 grams. The manufacturer weighs the contents of randomly selected individual packets. The probability a randomly selected packet is less than 78 grams (that is, Pr(<em>X</em> &lt; 78)) is as follows:</p>&#13;
<pre>R&gt; pnorm(78,80.2,1.1)<br/>[1] 0.02275013</pre>&#13;
<p class="indent">The probability a packet is found to weigh between 80.5 and 81.5 grams is as follows:</p>&#13;
<pre>R&gt; pnorm(81.5,80.2,1.1)-pnorm(80.5,80.2,1.1)<br/>[1] 0.2738925</pre>&#13;
<p class="indent">The weight below which the lightest 20 percent of packets lie is as follows:</p>&#13;
<pre>R&gt; qnorm(0.2,80.2,1.1)<br/>[1] 79.27422</pre>&#13;
<p class="indent">A simulation of five randomly selected packets can be found with the following:</p>&#13;
<pre>R&gt; round(rnorm(5,80.2,1.1),1)<br/>[1] 78.6 77.9 78.6 80.2 80.8</pre>&#13;
<div class="ex">&#13;
<p class="ext"><a id="ch16exc4"/><strong>Exercise 16.4</strong></p>&#13;
<ol type="a">&#13;
<li><p class="noindents">A tutor knows that the length of time taken to complete a certain statistics question by first-year undergraduate students, <em>X</em>, is normally distributed with a mean of 17 minutes and a standard deviation of 4.5 minutes.</p>&#13;
<ol type="i">&#13;
<li><p class="noindents">What is the probability a randomly selected undergraduate takes more than 20 minutes to complete the question?</p></li>&#13;
<li><p class="noindents">What’s the chance that a student takes between 5 and 10 minutes to finish the question?</p></li>&#13;
<li><p class="noindents">Find the time that marks off the slowest 10 percent of students.</p></li>&#13;
<li><p class="noindents"><span epub:type="pagebreak" id="page_357"/>Plot the normal distribution of interest between ±4<em>σ</em> and shade in the probability area of (iii), the slowest 10 percent of students.</p></li>&#13;
<li><p class="noindents">Generate a realization of times based on a class of 10 students completing the question.</p></li>&#13;
</ol></li>&#13;
<li><p class="noindents">A meticulous gardener is interested in the length of blades of grass on his lawn. He believes that blade length <em>X</em> follows a normal distribution centered on 10 mm with a variance of 2 mm.</p>&#13;
<ol type="i">&#13;
<li><p class="noindents">Find the probability that a blade of grass is between 9.5 and 11 mm long.</p></li>&#13;
<li><p class="noindents">What are the standardized values of 9.5 and 11 in the context of this distribution? Using the standardized values, confirm that you can obtain the same probability you found in (i) with the standard normal density.</p></li>&#13;
<li><p class="noindents">Below which value are the shortest 2.5 percent of blade lengths found?</p></li>&#13;
<li><p class="noindents">Standardize your answer from (iii).</p></li>&#13;
</ol></li>&#13;
</ol>&#13;
</div>&#13;
<h4 class="h4" id="ch16lev2sec143"><strong><em>16.2.3 Student’s t-distribution</em></strong></h4>&#13;
<p class="noindent">The <em>Student’s t-distribution</em> is a continuous probability distribution generally used when dealing with statistics estimated from a sample of data. It will become particularly relevant in the next two chapters, so I’ll briefly explain it here first.</p>&#13;
<p class="indent">Any particular <em>t</em>-distribution looks a lot like the standard normal distribution—it’s bell-shaped, symmetric, and unimodal, and it’s centered on zero. The difference is that while a normal distribution is typically used to deal with a population, the <em>t</em>-distribution deals with <em>sample</em> from a population.</p>&#13;
<p class="indent">For the <em>t</em>-distribution you don’t have to define any parameters per se, but you must choose the appropriate <em>t</em>-distribution by way of a strictly positive integer <em>ν</em> &gt; 0; this is referred to as the <em>degrees of freedom</em> (df), called so because it represents the number of individual components in the calculation of a given statistic that are “free to change.” You’ll see in the upcoming chapters that this quantity is usually directly related to sample sizes.</p>&#13;
<p class="indent">For the moment, though, you should just loosely think of the <em>t-</em>distribution as the representation of a family of curves and think of the degrees of freedom as the “selector” you use to tell you which particular version of the density to use. The precise equation for the density of the <em>t-</em>distribution is also not especially useful in an introductory setting, though it is useful to remember that the total probability underneath any <em>t</em> curve is naturally 1.</p>&#13;
<p class="indent">For a <em>t</em>-distribution, the <code>dt</code>, <code>pt</code>, <code>qt</code>, and <code>rt</code> functions represent the R implementation of the density, the cumulative distribution (left probabilities), the quantile, and the random variate generation functions, <span epub:type="pagebreak" id="page_358"/>respectively. The first arguments, <code>x</code>, <code>q</code>, <code>p</code>, and <code>n</code>, respectively, provide the relevant value (or values) of interest to these functions; the second argument in all of these is <code>df</code>, to which you must specify the degrees of freedom <em>ν</em>.</p>&#13;
<p class="indent">The best way to get an impression of the <em>t</em> family is through a visualization. <a href="ch16.xhtml#ch16fig10">Figure 16-10</a> plots the standard normal distribution, as well as the <em>t-</em>distribution curve with <em>ν</em> = 1, <em>ν</em> = 6, and <em>ν</em> = 20 df.</p>&#13;
<div class="image"><img src="../images/f16-10.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch16fig10"/>Figure 16-10: Comparing the standard normal distribution with three instances of the</em> t<em>-distribution. Note that the higher the degrees of freedom, the closer the</em> t<em>-distribution approximation becomes to the normal.</em></p>&#13;
<p class="indent">The one important note to take away from <a href="ch16.xhtml#ch16fig10">Figure 16-10</a>, and indeed from this section, is the way in which the <em>t</em> density function changes with respect to the N(0,1) distribution as you increase the df. For small values of <em>ν</em> close to 1, the <em>t</em>-distribution is shorter, in terms of its mode, with more probability occurring in noticeably fatter tails. It turns out that the <em>t</em> density approaches the standard normal density as <em>ν</em> → ∞. As a case in point, note that the upper 5 percent tail of the standard normal distribution is delineated by the following value:</p>&#13;
<pre>R&gt; qnorm(1-0.05)<br/>[1] 1.644854</pre>&#13;
<p class="indent">The same upper tail of the <em>t</em>-distribution is provided with df values of <em>ν</em> = 1, <em>ν</em> = 6, and <em>ν</em> = 20, respectively.</p>&#13;
<pre>R&gt; qt(1-0.05,df=1)<br/>[1] 6.313752<br/>R&gt; qt(1-0.05,df=6)<br/>[1] 1.94318<br/>R&gt; qt(1-0.05,df=20)<br/>[1] 1.724718</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_359"/>In direct comparison with the standard normal, the heavier weight in the tails of the <em>t</em> density leads naturally to more extreme quantile values given a specific probability. Notice that this extremity, however, is reduced as the df is increased—fitting in with the aforementioned fact that the <em>t-</em>distribution continues to improve in terms of its approximation to the standard normal as you raise the df.</p>&#13;
<h4 class="h4" id="ch16lev2sec144"><strong><em>16.2.4 Exponential</em></strong></h4>&#13;
<p class="noindent">Of course, probability density functions don’t have to be symmetrical like those you’ve encountered so far, nor do they need to allow for the random variable to be able to take values from negative infinity to positive infinity (like the normal or <em>t</em>-distributions). A good example of this is the <em>exponential distribution</em>, for which realizations of a random variable <em>X</em> are valid only on a 0 ≤ <em>X</em> &lt; ∞ domain.</p>&#13;
<p class="indent">For a continuous random variable 0 ≤ <em>X</em> &lt; ∞, the exponential density function <em>f</em> is</p>&#13;
<div class="imagec"><a id="ch16eq7"/><img src="../images/e16-7.jpg" alt="image"/></div>&#13;
<p class="noindent">where <em>λ</em><sub>e</sub> is a parameter of the distribution and exp{·} is the exponential function. The notation</p>&#13;
<p class="center"><em>X</em> ∼ EXP(<em>λ</em><sub>e</sub>)</p>&#13;
<p class="noindent">is often used to indicate that “<em>X</em> follows an exponential distribution with rate <em>λ</em><sub>e</sub>.”</p>&#13;
<p class="indentb">The following are the key points to note:</p>&#13;
<p class="bull">• Theoretically, <em>X</em> can take any value in the range 0 to ∞, and <em>f</em> (<em>x</em>) decreases as <em>x</em> increases.</p>&#13;
<p class="bull">• The “rate” parameter must be strictly positive; in other words, <em>λ</em><sub>e</sub> &gt; 0. It defines <em>f</em> (0) and the rate of decay of the function to the horizontal asymptote at zero.</p>&#13;
<p class="indentt">The mean and variance are as follows, respectively:</p>&#13;
<div class="imagec"><img src="../images/f0359-01.jpg" alt="image"/></div>&#13;
<h5 class="h5" id="ch16lev3sec60"><strong>The dexp Function</strong></h5>&#13;
<p class="noindent">The density function for the exponential distribution is a steadily decreasing line beginning at <em>f</em> (0) = <em>λ</em>; the rate of this decay ensures a total area of 1 underneath the curve. You create <a href="ch16.xhtml#ch16fig11">Figure 16-11</a> with the relevant <code>d</code>-function in the following code.</p>&#13;
<pre>R&gt; xvals &lt;- seq(0,10,length=200)<br/>R&gt; plot(xvals,dexp(x=xvals,rate=1.65),xlim=c(0,8),ylim=c(0,1.65),type="l",<br/>        xlab="x",ylab="f(x)")<br/>R&gt; lines(xvals,dexp(x=xvals,rate=1),lty=2)<br/>R&gt; lines(xvals,dexp(x=xvals,rate=0.4),lty=3)<br/>R&gt; abline(v=0,col="gray")<br/>R&gt; abline(h=0,col="gray")<br/>R&gt; legend("topright",legend=c("EXP(1.65)","EXP(1)","EXP(0.4)"),lty=1:3)</pre>&#13;
<div class="image"><span epub:type="pagebreak" id="page_360"/><img src="../images/f16-11.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch16fig11"/>Figure 16-11: Three different exponential density functions. Decreasing</em> <em>λ</em><sub>e</sub> <em>lowers the mode and extends the tail.</em></p>&#13;
<p class="indent">The parameter <em>λ</em><sub>e</sub> is provided to <code>rate</code> in <code>dexp</code>, which is evaluated at <em>x</em>, provided to the first argument <code>x</code> (via the <code>xvals</code> object in this example). You can see that a distinct feature of the exponential density function is that aforementioned decay to zero, with larger values of <em>λ</em><sub>e</sub> translating to a taller (yet sharper and more rapid) drop.</p>&#13;
<p class="indent">This naturally decreasing behavior helps identify the role the exponential distribution often plays in applications—one of a “time-until-event” nature. In fact, there’s a special relationship between the exponential distribution and the Poisson distribution introduced in <a href="ch16.xhtml#ch16lev2sec139">Section 16.1.3</a>. When the Poisson distribution is used to model the count of a certain event through time, you use the exponential distribution to model the time between these events. In such a setting, the exponential parameter <em>λ</em><sub>e</sub> defines the mean <em>rate</em> at which the events occur over time.</p>&#13;
<h5 class="h5" id="ch16lev3sec61"><strong>The pexp Function</strong></h5>&#13;
<p class="noindent">Let’s revisit the example from <a href="ch16.xhtml#ch16exc2">Exercise 16.2</a>, where the average number of cars passing an individual within a 120-minute window was said to be 107. Define the random variable <em>X</em> to be the waiting time between two consecutive cars passing and, using an exponential distribution for <em>X</em> on a minute time scale, set <em>λ</em><sub>e</sub> = 107/120 ≈ 0.89 (rounded to 2 d.p.). If 107 cars are typically observed in a two-hour window, then you see cars at an average rate of 0.89 per minute.</p>&#13;
<p class="indent">Thus, you interpret <em>λ</em><sub>e</sub> as the “per-unit-time” measure of the <em>λ</em><sub>p</sub> parameter from the Poisson mass function. The interpretation of the mean as the <span epub:type="pagebreak" id="page_361"/>reciprocal of the rate, <em>μ<sub>X</sub></em> = 1/λ<sub>e</sub>, is also intuitive. For example, when observing cars at a rate of about 0.89 per minute, note that the average waiting time between cars is roughly 1/0.89 ≈ 1.12 minutes.</p>&#13;
<p class="indent">So, in the current example, you want to examine the density <img class="middle" src="../images/f0361-01.jpg" alt="image"/>.</p>&#13;
<pre>R&gt; lambda.e &lt;- 107/120<br/>R&gt; lambda.e<br/>[1] 0.8916667</pre>&#13;
<p class="indent">Say a car has just passed the individual’s location and you want to find the probability that they must wait more than two and a half minutes before seeing another car, in other words, Pr(<em>X</em> &gt; 2.5). You can do so using <code>pexp</code>.</p>&#13;
<pre>R&gt; 1-pexp(q=2.5,rate=lambda.e)<br/>[1] 0.1076181</pre>&#13;
<p class="indent">This indicates that you have just over a 10 percent chance of observing at least a 2-minute 30-second gap before the next car appears. Remember that the default behavior of the <code>p</code>-function is to find the cumulative, left-hand probability from the provided value, so you need to subtract the result from 1 to find an upper-tail probability. You find the probability of waiting less than 25 seconds with the following, which gives a result of roughly 0.31:</p>&#13;
<pre>R&gt; pexp(25/60,lambda.e)<br/>[1] 0.3103202</pre>&#13;
<p class="indent">Note the need to first convert the value of interest from seconds to minutes since you’ve defined <em>f</em> (<em>x</em>) via <em>λ</em><sub>e</sub> ≈ 0.89 on the scale of the latter.</p>&#13;
<h5 class="h5" id="ch16lev3sec62"><strong>The qexp Function</strong></h5>&#13;
<p class="noindent">Use the appropriate quantile function <code>qexp</code> to find, say, the cutoff point for the shortest 15 percent of waits.</p>&#13;
<pre>R&gt; qexp(p=0.15,lambda.e)<br/>[1] 0.1822642</pre>&#13;
<p class="indent">This indicates that the value of interest is about 0.182 minutes, in other words, roughly 0.182 × 60 = 10.9 seconds.</p>&#13;
<p class="indent">As usual, you can use <code>rexp</code> to generate random variates of any specific exponential distribution.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>It is important to distinguish between the “exponential distribution,” the “exponential</em> family <em>of distributions,” and the “exponential</em> function<em>.” The first refers to the density function that’s just been studied, whereas the second refers to a general class of probability distributions, including the Poisson, the normal, and the exponential itself. The third is just the standard mathematical exponential function upon which the exponential family members depend and is directly accessible in R via</em> <code><span class="codeitalic">exp</code></span>.</p>&#13;
</div>&#13;
<div class="ex">&#13;
<p class="ext"><span epub:type="pagebreak" id="page_362"/><a id="ch16exc5"/><strong>Exercise 16.5</strong></p>&#13;
<ol type="a">&#13;
<li><p class="noindents">Situated in the central north island of New Zealand, the Pohutu geyser is said to be the largest active geyser in the southern hemisphere. Suppose that it erupts an average of 3,500 times every year.</p>&#13;
<ol type="i">&#13;
<li><p class="noindents">With the intention of modeling a random variable <em>X</em> as the time between consecutive eruptions, evaluate the parameter value <em>λ</em><sub>e</sub> with respect to a time scale in days (assume 365.25 days per year to account for leap years).</p></li>&#13;
<li><p class="noindents">Plot the density function of interest. What’s the mean wait in days between eruptions?</p></li>&#13;
<li><p class="noindents">What’s the probability of waiting less than 30 minutes for the next eruption?</p></li>&#13;
<li><p class="noindents">What waiting time defines the longest 10 percent of waits? Convert your answer to hours.</p></li>&#13;
</ol></li>&#13;
<li><p class="noindents">You can also use the exponential distribution to model certain product survival times, or “time-to-failure” type of variables. Say a manufacturer of a particular air conditioning unit knows that the product has an average life of 11 years before it needs any type of repair callout. Let the random variable <em>X</em> represent the time until the necessary repair of one of these units and assume <em>X</em> follows an exponential distribution with <em>λ</em><sub>e</sub> = 1/11.</p>&#13;
<ol type="i">&#13;
<li><p class="noindents">The company offers a five-year full repair warranty on this unit. What’s the probability that a randomly selected air conditioner owner makes use of the warranty?</p></li>&#13;
<li><p class="noindents">A rival company offers a six-year guarantee on its competing air conditioning unit but knows that its units last, on average, only nine years before requiring some kind of repair. What are the chances of making use of that warranty?</p></li>&#13;
<li><p class="noindents">Determine the probabilities that the units in (i) and the units in (ii) last more than 15 years.</p></li>&#13;
</ol></li>&#13;
</ol>&#13;
</div>&#13;
<h4 class="h4" id="ch16lev2sec145"><strong><em>16.2.5 Other Density Functions</em></strong></h4>&#13;
<p class="noindentb">There are a number of other common probability density functions used for a wide variety of tasks involving continuous random variables. I’ll summarize a few here:</p>&#13;
<p class="bull">• The <em>chi-squared distribution</em> models sums of squared normal variates and is thus often related to operations concerning sample variances of normally distributed data. Its functions are <code>dchisq</code>, <code>pchisq</code>, <code>qchisq</code>, and <code>rchisq</code>, and like the <em>t</em>-distribution (<a href="ch16.xhtml#ch16lev2sec143">Section 16.2.3</a>), it’s dependent upon specification of a degrees of freedom provided as the argument <code>df</code>.</p>&#13;
<p class="bull"><span epub:type="pagebreak" id="page_363"/>• The <em>F-distribution</em> is used to model ratios of two chi-squared random variables and is useful in, for example, regression problems (see <a href="ch20.xhtml#ch20">Chapter 20</a>). Its functions are <code>df</code>, <code>pf</code>, <code>qf</code>, and <code>rf</code>, and as it involves two chi-squared values, it’s dependent upon the specification of a <em>pair</em> of degrees of freedom values supplied as the arguments <code>df1</code> and <code>df2</code>.</p>&#13;
<p class="bull">• The <em>gamma distribution</em> is a generalization of both the exponential and chi-squared distributions. Its functions are <code>dgamma</code>, <code>pgamma</code>, <code>qgamma</code>, and <code>rgamma</code>, and it’s dependent upon “shape” and “scale” parameters provided as the arguments <code>shape</code> and <code>scale</code>, respectively.</p>&#13;
<p class="bull">• The <em>beta distribution</em> is often used in Bayesian modeling, and it has implemented functions <code>dbeta</code>, <code>pbeta</code>, <code>qbeta</code>, and <code>rbeta</code>. It’s defined by two “shape” parameters <em>α</em> and <em>β</em>, supplied as <code>shape1</code> and <code>shape2</code>, respectively.</p>&#13;
<p class="indentt">In particular, you’ll encounter the chi-squared and <em>F</em>-distributions over the next couple of chapters.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>In all of the common probability distributions you’ve examined over the past couple of sections, I’ve emphasized the need to perform “one-minus” operations to find probabilities or quantiles with respect to an upper- or right-tailed area. This is because of the cumulative nature of the</em> <code><span class="codeitalic">p</code></span><em>- and</em> <code><span class="codeitalic">q</code></span><em>-functions—by definition, it’s the lower tail that is dealt with. However, most</em> <code><span class="codeitalic">p</code></span><em>- and</em> <code><span class="codeitalic">q</code></span><em>-functions in R include an optional logical argument,</em> <code><span class="codeitalic">lower.tail</code></span><em>, which defaults to</em> <code><span class="codeitalic">FALSE</code></span><em>. Therefore, an alternative is to set</em> <code><span class="codeitalic">lower.tail=TRUE</code></span> <em>in any relevant function call, in which case R will expect or return upper-tail areas specifically.</em></p>&#13;
</div>&#13;
<h5 class="h5" id="ch16lev3sec63"><strong>Important Code in This Chapter</strong></h5>&#13;
<table class="topbot">&#13;
<thead>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table_th"><p class="table"><strong>Function/operator</strong></p></td>&#13;
<td style="vertical-align: top;" class="table_th"><p class="table"><strong>Brief description</strong></p></td>&#13;
<td style="vertical-align: top;" class="table_th"><p class="table"><strong>First occurrence</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>dbinom</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Binomial mass function</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec138">Section 16.1.2</a>, <a href="ch16.xhtml#page_335">p. 335</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>pbinom</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Binomial cumulative problems</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec138">Section 16.1.2</a>, <a href="ch16.xhtml#page_336">p. 336</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>qbinom</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Binomial quantiles function</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec138">Section 16.1.2</a>, <a href="ch16.xhtml#page_337">p. 337</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>rbinom</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Binomial random realizations</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec138">Section 16.1.2</a>, <a href="ch16.xhtml#page_337">p. 337</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>dpois</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Poisson mass function</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec139">Section 16.1.3</a>, <a href="ch16.xhtml#page_340">p. 340</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>ppois</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Poisson cumulative problems</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec139">Section 16.1.3</a>, <a href="ch16.xhtml#page_341">p. 341</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>rpois</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Poisson random realizations</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec139">Section 16.1.3</a>, <a href="ch16.xhtml#page_341">p. 341</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>dunif</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Uniform density function</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec141">Section 16.2.1</a>, <a href="ch16.xhtml#page_344">p. 344</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>punif</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Uniform cumulative problems</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec141">Section 16.2.1</a>, <a href="ch16.xhtml#page_346">p. 346</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>qunif</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Uniform quantiles</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec141">Section 16.2.1</a>, <a href="ch16.xhtml#page_346">p. 346</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>runif</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Uniform random realizations</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec141">Section 16.2.1</a>, <a href="ch16.xhtml#page_347">p. 347</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>dnorm</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Normal density function</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec142">Section 16.2.2</a>, <a href="ch16.xhtml#page_350">p. 350</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>pnorm</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Normal cumulative problems</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec142">Section 16.2.2</a>, <a href="ch16.xhtml#page_350">p. 350</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>qnorm</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Normal quantiles</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec142">Section 16.2.2</a>, <a href="ch16.xhtml#page_353">p. 353</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>rnorm</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Normal random realizations</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec142">Section 16.2.2</a>, <a href="ch16.xhtml#page_355">p. 355</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>qt</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Student’s <em>t</em> quantiles</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec143">Section 16.2.3</a>, <a href="ch16.xhtml#page_358">p. 358</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>dexp</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Exponential density function</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec144">Section 16.2.4</a>, <a href="ch16.xhtml#page_359">p. 359</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>pexp</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Exponential cumulative problems</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec144">Section 16.2.4</a>, <a href="ch16.xhtml#page_361">p. 361</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>qexp</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Exponential quantiles</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch16.xhtml#ch16lev2sec144">Section 16.2.4</a>, <a href="ch16.xhtml#page_361">p. 361</a><span epub:type="pagebreak" id="page_364"/></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
</body></html>