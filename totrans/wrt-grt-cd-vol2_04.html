<html><head></head><body>
		<h2 class="h2" id="ch04"><span epub:type="pagebreak" id="page_47"/><strong><span class="big">4</span></strong><br/><strong>COMPILER OPERATION AND CODE GENERATION</strong></h2>&#13;
		<div class="image1">&#13;
			<img alt="image" src="../images/common01.jpg"/>&#13;
		</div>&#13;
		<p class="noindent">In order to write HLL code that produces efficient machine code, you must first understand how compilers and linkers translate high-level source statements into executable machine code. Complete coverage of compiler theory is beyond the scope of this book; however, this chapter explains the basics of the translation process so you can understand and work within the limitations of HLL compilers. We’ll cover the following topics:</p>&#13;
		<ul>&#13;
			<li>&#13;
				<p class="noindent">The different types of input files programming languages use</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Differences between compilers and interpreters</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">How typical compilers process source files to produce executable programs</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">The process of optimization and why compilers cannot produce the best possible code for a given source file</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Different types of output files that compilers produce</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Common object file formats, such as COFF and ELF</p>&#13;
				</li>&#13;
			<li><span epub:type="pagebreak" id="page_48"/>&#13;
			<p class="noindent">Memory organization and alignment issues that affect the size and efficiency of executable files a compiler produces</p>&#13;
			</li>&#13;
			<li>&#13;
				<p class="noindent">How linker options can affect the efficiency of your code</p>&#13;
				</li>&#13;
		</ul>&#13;
		<p class="indent">This material provides the foundation for all the chapters that follow, and is crucial to helping a compiler produce the best possible code. We’ll begin with a discussion of file formats used by programming languages.</p>&#13;
		<h3 class="h3" id="ch00lev1sec31"><strong>4.1 File Types That Programming Languages Use</strong></h3>&#13;
		<p class="noindent">A typical program can take many forms. A <em>source file</em> is a human-readable form that a programmer creates and supplies to a language translator (such as a compiler). A typical compiler translates the source file or files into an <em>object code</em> file. A <em>linker program</em> combines separate object modules to produce a relocatable or executable file. Finally, a <em>loader</em> (usually the operating system) loads the executable file into memory and makes the final modifications to the object code prior to execution. Note that the modifications are made to the object code that is now in memory; the actual file on the disk does not get modified. These are not the only types of files that language processing systems manipulate, but they are typical. To fully understand compiler limitations, it’s important to know how the language processor deals with each of these file types. We’ll look at source files first.</p>&#13;
		<h3 class="h3" id="ch00lev1sec32"><strong>4.2 Source Files</strong></h3>&#13;
		<p class="noindent">Traditionally, source files contain pure ASCII or Unicode text (or some other character set) that a programmer has created with a text editor. One advantage to using pure text files is that a programmer can manipulate a source file using any program that processes text files. For example, a program that counts the number of lines in an arbitrary text file will also count the number of source lines in a program. Because there are hundreds of little filter programs that manipulate text files, maintaining source files in a pure text format is a good approach. This format is sometimes called <em>plain vanilla text</em>.</p>&#13;
		<h4 class="h4" id="ch00lev2sec29"><strong>4.2.1 Tokenized Source Files</strong></h4>&#13;
		<p class="noindent">Some language processing systems (especially interpreters) maintain their source files in a <em>tokenized</em> form. Tokenized source files generally use special single-byte <em>token</em> values to compress reserved words and other lexical elements in the source language, and thus they are often smaller than text source files. Furthermore, interpreters that operate on tokenized code are generally an order of magnitude faster than interpreters that operate on pure text, because processing strings of single-byte tokens is far more efficient than recognizing reserved word strings.</p>&#13;
		<p class="indent">Generally, the tokenized file from the interpreter consists of a sequence of bytes that map directly to strings such as <code>if</code> and <code>print</code> in the source file. <span epub:type="pagebreak" id="page_49"/>So, by using a table of strings and a little extra logic, you can decipher a tokenized program to produce the original source code. (Usually, you lose any extra whitespace you inserted into the source file, but that’s about the only difference.) Many of the original BASIC interpreters found on early PC systems worked this way. You’d type a line of BASIC source code into the interpreter, and the interpreter would immediately tokenize that line and store the tokenized form in memory. Later, when you executed the <code>LIST</code> command, the interpreter would <em>detokenize</em> the source code in memory to produce the listing.</p>&#13;
		<p class="indent">On the flip side, tokenized source files often use a proprietary format. This means they can’t take advantage of general-purpose text-manipulation tools like <code>wc</code> (word count), <code>entab</code>, and <code>detab</code> (which count the number of lines, words, and characters in a text file; replace spaces with tabs; and replace tabs with spaces, respectively).</p>&#13;
		<p class="indent">To overcome this limitation, most languages that operate on tokenized files enable you to detokenize a source file to produce a standard text file. (They also allow you to retokenize a source file, given an input text file.) You then run the resulting text file through some filter program, and retokenize the output of the filter program to produce a new tokenized source file. Although this takes considerable work, it allows language translators that work with tokenized files to take advantage of various text-based utility programs.</p>&#13;
		<h4 class="h4" id="ch00lev2sec30"><strong>4.2.2 Specialized Source Files</strong></h4>&#13;
		<p class="noindent">Some programming languages, such as Embarcadero’s Delphi and Free Pascal’s comparable Lazarus program, do not use a traditional text-based file format at all. Instead, they often use graphical elements like flowcharts and forms to represent the instructions the program is to perform. Other examples are the Scratch programming language, which allows you to write simple programs using graphical elements on a bitmapped display, and the Microsoft Visual Studio and Apple Xcode integrated development environments (IDEs), which both allow you to specify a screen layout using graphical operations rather than a text-based source file.</p>&#13;
		<h3 class="h3" id="ch00lev1sec33"><strong>4.3 Types of Computer Language Processors</strong></h3>&#13;
		<p class="noindent">Computer language processing systems generally fall into one of four categories: pure interpreters, interpreters, compilers, and incremental compilers. These systems differ in how they process the source program and execute the result, which affects their respective efficiency.</p>&#13;
		<h4 class="h4" id="ch00lev2sec31"><strong>4.3.1 Pure Interpreters</strong></h4>&#13;
		<p class="noindent"><em>Pure interpreters</em> operate directly on a text source file and tend to be very inefficient. They continuously scan the source file (usually an ASCII text file), processing it as string data. Recognizing <em>lexemes</em> (language components such as reserved words, literal constants, and the like) consumes <span epub:type="pagebreak" id="page_50"/>time. Indeed, many pure interpreters spend more time processing the lexemes (that is, performing <em>lexical analysis</em>) than they do actually executing the program. Because the actual on-the-fly execution of the lexeme takes only a little additional effort beyond the lexical analysis, pure interpreters tend to be the smallest of the computer language processing programs. For this reason, pure interpreters are popular when you need a very compact language processor. They are also popular for scripting languages and very high-level languages that let you manipulate the language’s source code as string data during program execution.</p>&#13;
		<h4 class="h4" id="ch00lev2sec32"><strong>4.3.2 Interpreters</strong></h4>&#13;
		<p class="noindent">An <em>interpreter</em> executes some representation of a program’s source file at runtime. This representation isn’t necessarily a text file in human-readable form. As noted in the previous section, many interpreters operate on tokenized source files in order to avoid lexical analysis during execution. Some interpreters read a text source file as input and translate the input file to a tokenized form prior to execution. This allows programmers to work with text files in their favorite editor while enjoying the fast execution of a tokenized format. The only costs are an initial delay to tokenize the source file (which is unnoticeable on most modern machines) and the fact that it may not be possible to execute strings containing program statements.</p>&#13;
		<h4 class="h4" id="ch00lev2sec33"><strong>4.3.3 Compilers</strong></h4>&#13;
		<p class="noindent">A <em>compiler</em> translates a source program in text form into executable machine code. This is a complex process, particularly in optimizing compilers. There are a couple of things to note about the code a compiler produces. First, a compiler produces machine instructions that the underlying CPU can execute directly. Therefore, the CPU doesn’t waste any cycles decoding the source file while executing the program—all of the CPU’s resources are dedicated to executing the machine code. Thus, the resulting program generally runs many times faster than an interpreted version does. Of course, some compilers do a better job of translating HLL source code into machine code than other compilers, but even low-quality compilers do a better job than most interpreters.</p>&#13;
		<p class="indent">A compiler’s translation from source code to machine code is a one-way function. In contrast to interpreters, it is very difficult, if not impossible, to reconstruct the original source file if you’re given only the machine code output from a program.</p>&#13;
		<h4 class="h4" id="ch00lev2sec34"><strong>4.3.4 Incremental Compilers</strong></h4>&#13;
		<p class="noindent">An <em>incremental compiler</em> is a cross between a compiler and an interpreter. There are many different types of incremental compilers, but in general, they operate like an interpreter in that they do not compile the source file directly into machine code; instead, they translate the source code into an intermediate form. Unlike the tokenized code from interpreters, however, this intermediate form usually is not strongly correlated to the original <span epub:type="pagebreak" id="page_51"/>source file. The intermediate form is generally the machine code for a <em>virtual machine language</em>—“virtual” in that there is no real CPU that can execute this code. However, it is easy to write an interpreter that can execute it. Because interpreters for virtual machines (VMs) are usually much more efficient than interpreters for tokenized code, executing VM code is usually much faster than executing a list of tokens in an interpreter. Languages like Java use this compilation technique, along with a <em>Java bytecode engine</em> (an interpreter program), to interpretively execute the Java “machine code” (see <a href="ch04.xhtml#ch4fig1">Figure 4-1</a>). The big advantage to VM execution is that the VM code is portable; that is, programs running on the virtual machine can execute anywhere an interpreter is available. True machine code, by contrast, executes only on the CPU (family) for which it was written. Generally, interpreted VM code runs about 2 to 10 times faster than interpreted code (tokenized), and pure machine code runs about 2 to 10 times faster than interpreted VM code.</p>&#13;
		<div class="image" id="ch4fig1">&#13;
			<img alt="Image" src="../images/04fig01.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 4-1: The JBC interpreter</em></p>&#13;
		<p class="indent">In an attempt to improve the performance of programs compiled via an incremental compiler, many vendors (particularly Java systems vendors) have turned to a technique known as <em>just-in-time (JIT) compilation</em>. The concept is based on the fact that the time spent in interpretation is largely consumed by fetching and deciphering the VM code at runtime. This interpretation occurs repeatedly as the program executes. JIT compilation translates the VM code to actual machine code whenever it encounters a VM instruction for the first time. This spares the interpreter from repeating the interpretation process the next time it encounters the same statement in the program (for example, in a loop). Although JIT compilation is nowhere near as good as a true compiler, it can typically improve the performance of a program by a factor of two to five.</p>&#13;
		<div class="note">&#13;
			<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
			<p class="notep"><em>Older compilers and some freely available compilers compile the source code to assembly language, and then a separate compiler, known as an</em> assembler, <em>assembles this output to the desired machine code. Most modern and highly efficient compilers skip this step altogether. See “Compiler Output” on <a href="ch04.xhtml#page_67">page 67</a> for more on this subject.</em></p>&#13;
		</div>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_52"/>Of the four categories of computer language processors just described, this chapter will focus on compilers. By understanding how a compiler generates machine code, you can choose appropriate HLL statements to generate better, more efficient machine code. If you want to improve the performance of programs written with an interpreter or incremental compiler instead, the best approach is to use an optimizing compiler to process your application. For example, GNU provides a compiler for Java that produces optimized machine code rather than interpreted Java bytecode (JBC); the resulting executable files run much faster than interpreted JBC or even JIT-compiled bytecode.</p>&#13;
		<h3 class="h3" id="ch00lev1sec34"><strong>4.4 The Translation Process</strong></h3>&#13;
		<p class="noindent">A typical compiler is broken down into several logical components called <em>phases</em>. Although their exact number and names may vary somewhat among different compilers, the five most common phases are <em>lexical analysis</em>, <em>syntax analysis</em>, <em>intermediate code generation</em>, <em>native code generation</em>, and, for compilers that support it, <em>optimization</em>.</p>&#13;
		<p class="indent"><a href="ch04.xhtml#ch4fig2">Figure 4-2</a> shows how the compiler logically arranges these phases to translate source code in the HLL into machine (object) code.</p>&#13;
		<div class="image" id="ch4fig2">&#13;
			<img alt="Image" src="../images/04fig02.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 4-2: Phases of compilation</em></p>&#13;
		<p class="indent">Although <a href="ch04.xhtml#ch4fig2">Figure 4-2</a> suggests that the compiler executes these phases sequentially, most compilers do not. Instead, the phases tend to execute in <span epub:type="pagebreak" id="page_53"/>parallel, with each phase doing a small amount of work, passing its output to the next phase, and then waiting for input from the previous phase. In a typical compiler, the <em>parser</em> (the syntax analysis phase) is probably the closest thing you’ll find to the main program or the master process. The parser usually drives the compilation process in that it calls the <em>scanner</em> (lexical analysis phase) to obtain input and calls the intermediate code generator to process its own output. The intermediate code generator may (optionally) call the optimizer and then call the native code generator. The native code generator may (optionally) call the optimizer as well. The output from the native code generation phase is the executable code. After the native code generator/optimizer emits some code, execution returns to the intermediate code generator, then to the parser, which requests more input from the scanner, starting the whole process over.</p>&#13;
		<div class="note">&#13;
			<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
			<p class="notep"><em>Other compiler organizations are possible. Some compilers, for example, allow the user to choose whether the compiler runs the optimization phase, while others don’t have an optimization phase at all. Similarly, some compilers dispense with intermediate code generation and directly call a native code generator. Some compilers include additional phases that process object modules compiled at different times.</em></p>&#13;
		</div>&#13;
		<p class="indent">Thus, although <a href="ch04.xhtml#ch4fig2">Figure 4-2</a> doesn’t accurately depict the typical (parallel) execution path, the <em>data flow</em> it shows is correct. The scanner reads the source file, translates it to a different form, and then passes this translated data on to the parser. The parser accepts its input from the scanner, translates that input to a different form, and then passes this new data to the intermediate code generator. Similarly, the remaining phases read their input from the previous phase, translate the input to a (possibly) different form, and then pass that input on to the next phase. The compiler writes the output of its last phase to the executable object file.</p>&#13;
		<p class="indent">Let’s take a closer look at each phase of the code translation process.</p>&#13;
		<h4 class="h4" id="ch00lev2sec35"><strong>4.4.1 Scanning (Lexical Analysis)</strong></h4>&#13;
		<p class="noindent">The scanner (aka the <em>lexical analyzer</em>, or <em>lexer</em>) is responsible for reading the character/string data found in the source file and breaking up this data into tokens that represent the lexical items, or lexemes, in the source file. As mentioned previously, lexemes are the character sequences in the source file that we would recognize as atomic components of the language. For example, a scanner for the C language would recognize substrings like <code>if</code> and <code>while</code> as C reserved words. The scanner would not, however, pick out the “if ” within the identifier <code>ifReady</code> and treat it as a reserved word. Instead, the scanner considers the context in which a reserved word is used so that it can differentiate between reserved words and identifiers. For each lexeme, the scanner creates a small data package—a token—and passes it on to the parser. A token typically contains several values:</p>&#13;
		<ul>&#13;
			<li>&#13;
				<p class="noindent">A small integer that uniquely identifies the token’s class (whether it’s a reserved word, identifier, integer constant, operator, or character string literal)</p>&#13;
				</li>&#13;
			<li><span epub:type="pagebreak" id="page_54"/>&#13;
			<p class="noindent">Another value that differentiates the token within a class (for example, this value would indicate which reserved word the scanner has processed)</p>&#13;
			</li>&#13;
			<li>&#13;
				<p class="noindent">Any other attributes the scanner might associate with the lexeme</p>&#13;
				</li>&#13;
		</ul>&#13;
		<div class="note">&#13;
			<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
			<p class="notep"><em>Do not confuse this reference to a token with the compressed-style tokens in an interpreter discussed previously. In this context, tokens are simply a variable-sized data structure that holds information associated with a lexeme for the interpreter/compiler.</em></p>&#13;
		</div>&#13;
		<p class="indent">When the scanner sees the character string <code>12345</code> in the source file, for example, it might identify the token’s class as a literal constant, the token’s second value as an integer typed constant, and the token’s attribute as the numeric equivalent of the string (that is, twelve thousand, three hundred, forty-five). <a href="ch04.xhtml#ch4fig3">Figure 4-3</a> demonstrates what this token might look like in memory.</p>&#13;
		<div class="image" id="ch4fig3">&#13;
			<img alt="Image" src="../images/04fig03.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 4-3: A token for the lexeme <span class="codeitalic1">"12345"</span></em></p>&#13;
		<p class="indent">The token’s enumerated value is <code>345</code> (indicating an integer constant), the token class’s value is <code>5</code> (indicating a literal constant), the token’s attribute value is <code>12345</code> (the numeric form of the lexeme), and the lexeme string is <code>"12345"</code> as returned by the scanner. Different code sequences in the compiler can refer to this token data structure as appropriate.</p>&#13;
		<p class="indent">Strictly speaking, the lexical analysis phase is optional. A parser could work directly with the source file. However, tokenization makes the compilation process more efficient, because it allows the parser to deal with tokens as integer values rather than as string data. Because most CPUs can handle small integer values much more efficiently than string data, and because the parser has to refer to the token data multiple times during processing, lexical analysis saves considerable time during compilation. Generally, pure interpreters are the only language processors that rescan each token during parsing, and this is one major reason why they are so slow (compared to, say, an interpreter that stores the source file in a tokenized form to avoid constantly processing a pure-text source file).</p>&#13;
		<h4 class="h4" id="ch00lev2sec36"><strong>4.4.2 Parsing (Syntax Analysis)</strong></h4>&#13;
		<p class="noindent">The parser is the part of the compiler that is responsible for checking whether the source program is syntactically (and semantically) correct. If there’s an error in the source file, it’s usually the parser that discovers and reports it. The parser is also responsible for reorganizing the token stream (that is, the source code) into a more complex data structure that <span epub:type="pagebreak" id="page_55"/>represents the meaning or semantics of the program. The scanner and parser generally process the source file in a linear fashion from the beginning to the end of the file, and the compiler usually reads the source file only once. Later phases, however, need to refer to the body of the source program in a more ad hoc way. By building up a data structure representation of the source code (often called an <em>abstract syntax tree</em>, or <em>AST</em>), the parser enables the code generation and optimization phases to easily reference different parts of the program.</p>&#13;
		<p class="indent"><a href="ch04.xhtml#ch4fig4">Figure 4-4</a> shows how a compiler might represent the expression <code>12345+6</code> using three nodes in an AST (<code>43</code> is the value for the addition operator and <code>7</code> is the subclass representing arithmetic operators).</p>&#13;
		<div class="image" id="ch4fig4">&#13;
			<img alt="Image" src="../images/04fig04.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 4-4: A portion of an abstract syntax tree</em></p>&#13;
		<h4 class="h4" id="ch00lev2sec37"><strong>4.4.3 Intermediate Code Generation</strong></h4>&#13;
		<p class="noindent">The intermediate code generation phase is responsible for translating the AST representation of the source file into a quasi–machine code form. There are two reasons compilers typically translate a program into an intermediate form rather than converting it directly to native machine code.</p>&#13;
		<p class="indent">First, the compiler’s optimization phase can do certain types of optimizations, such as common subexpression elimination, much more easily on this intermediate form.</p>&#13;
		<p class="indent">Second, many compilers, known as <em>cross-compilers</em>, generate executable machine code for several different CPUs. By breaking the code generation phase into two pieces—the intermediate code generator and the native code generator—the compiler writer can move all the CPU-independent activities into the intermediate code generation phase and write this code only once. This simplifies the native code generation phase. That is, <span epub:type="pagebreak" id="page_56"/>because the compiler needs only one intermediate code generation phase but may need separate native code generation phases for each CPU the compiler supports, moving as much of the CPU-independent code as possible into the intermediate code generator will reduce the size of the native code generators. For the same reason, the optimization phase is often broken into two components (refer back to <a href="ch04.xhtml#ch4fig2">Figure 4-2</a>): a CPU-independent component (the part following the intermediate code generator) and a CPU-dependent component.</p>&#13;
		<p class="indent">Some language systems, such as Microsoft’s VB.NET and C#, actually emit the intermediate code as the output of the compiler (in the .NET system, Microsoft calls this code <em>Common Intermediate Language</em>, or <em>CIL</em>). Native code generation and optimization are actually handled by the Microsoft <em>Common Language Runtime (CLR)</em> system, which performs JIT compilation on the CIL code the .NET compilers produce.</p>&#13;
		<h4 class="h4" id="ch00lev2sec38"><strong>4.4.4 Optimization</strong></h4>&#13;
		<p class="noindent">The optimization phase, which follows intermediate code generation, translates the intermediate code into a more efficient form. This generally involves eliminating unnecessary entries from the AST. For example, the compiler’s optimizer might transform the following intermediate code:</p>&#13;
		<pre class="programs">&#13;
			move the constant 5 into the variable i<br/>move a copy of i into j<br/>move a copy of j into k<br/>add k to m</pre>&#13;
		<p class="noindent">to something like:</p>&#13;
		<pre class="programs">&#13;
			move the constant 5 into k<br/>add k to m</pre>&#13;
		<p class="indent">If there are no more references to <code>i</code> and <code>j</code>, the optimizer can eliminate all references to them. Indeed, if <code>k</code> is never used again, the optimizer can replace these two instructions with the single instruction <code>add 5 to m</code>. Note that this type of transformation is valid on nearly all CPUs. Therefore, this type of transformation/optimization is perfect for the first optimization phase.</p>&#13;
		<h5 class="h5" id="ch00lev3sec25"><strong>4.4.4.1 The Problem with Optimization</strong></h5>&#13;
		<p class="noindent">Transforming intermediate code “into a more efficient form” is not a well-defined process—what makes one form of a program more efficient than another? The primary definition of efficiency is that the program minimizes the use of some system resource, usually memory (space) or CPU cycles (speed). A compiler’s optimizer could manage other resources, but space and speed are the principal considerations for programmers. But even if we consider only these two facets of optimization, describing the “optimal” result is difficult. The problem is that optimizing for one goal (say, better performance) may create conflicts with another optimization <span epub:type="pagebreak" id="page_57"/>goal (such as reduced memory usage). For this reason, the optimization process is usually a matter of compromise, where you make trade-offs and sacrifice certain subgoals (for example, running certain sections of the code a little slower) in order to create a reasonable result (like creating a program that doesn’t consume too much memory).</p>&#13;
		<h5 class="h5" id="ch00lev3sec26"><strong>4.4.4.2 Optimization’s Effect on Compile Time</strong></h5>&#13;
		<p class="noindent">You might think that it’s possible to set a single goal (for example, highest possible performance) and optimize strictly for that. However, the compiler must also be capable of producing an executable result in a reasonable amount of time. The optimization process is an example of what complexity theory calls an <em>NP-complete problem</em>. These are problems that are, as far as we know, intractable; that is, you cannot produce a guaranteed correct result (for example, an optimal version of a program) without first computing all possibilities and choosing from among them. Unfortunately, the time generally required to solve an NP-complete problem increases exponentially with the size of the input, which in the case of compiler optimization means roughly the number of lines of source code.</p>&#13;
		<p class="indent">This means that in the worst case, producing a truly optimal program would take longer than it was worth. Adding one line of source code could approximately <em>double</em> the amount of time it takes to compile and optimize the code. Adding two lines could <em>quadruple</em> the amount of time. In fact, a full guaranteed optimization of a modern application could take longer than the known lifetime of the universe.</p>&#13;
		<p class="indent">For all but the smallest source files (a few dozen lines), a perfect optimizer would take far too long to be of any practical value (and such optimizers have been written; search online for “superoptimizers” for details). For this reason, compiler optimizers rarely produce a truly optimal program. They simply produce the best result they can, given the limited amount of CPU time the user is willing to allow for the process.</p>&#13;
		<div class="note">&#13;
			<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
			<p class="notep"><em>Languages that rely on JIT compilation (such as Java, C#, and VB.Net) move part of the optimization phase to runtime. Therefore, the optimizer’s performance has a direct impact on the application’s runtime. Because the JIT compiler system is running concurrently with the application, it cannot spend considerable time optimizing the code without having a huge impact on runtime. This is why languages such as Java and C#, even when ultimately compiled to low-level machine code, rarely perform as well as highly optimized code compiled by traditional languages such as C/C++ and Pascal.</em></p>&#13;
		</div>&#13;
		<p class="indent">Rather than trying all possibilities and choosing the best result, modern optimizers use heuristics and case-based algorithms to determine the transformations they will apply to the machine code they produce. You need to be aware of these techniques so you can write your HLL code in a manner that allows an optimizer to easily process it and produce better machine code.</p>&#13;
		<h5 class="h5" id="ch00lev3sec27"><span epub:type="pagebreak" id="page_58"/><strong>4.4.4.3 Basic Blocks, Reducible Code, and Optimization</strong></h5>&#13;
		<p class="noindent">Understanding how the compiler organizes the intermediate code (to output better machine code in later phases) is very important if you want to be able to help the optimizer do its job more efficiently. As control flows through the program, the optimizer keeps track of variable values in a process known as <em>data flow analysis (DFA)</em>. After careful DFA, a compiler can determine where a variable is uninitialized, when the variable contains certain values, when the program no longer uses the variable, and (just as importantly) when the compiler simply doesn’t know anything about the variable’s value. For example, consider the following Pascal code:</p>&#13;
		<pre class="programs">&#13;
			    path := 5;<br/>    if( i = 2 ) then begin<br/><br/>        writeln( 'Path = ', path );<br/><br/>    end;<br/>    i := path + 1;<br/>    if( i &lt; 20 ) then begin<br/><br/>        path := path + 1;<br/>        i := 0;<br/><br/>    end;</pre>&#13;
		<p class="indent">A good optimizer will replace this code with something like the following:</p>&#13;
		<pre class="programs">&#13;
			    if( i = 2 ) then begin<br/><br/>        (* Because the compiler knows that path = 5 *)<br/><br/>        writeln( 'path = ', 5 );<br/><br/>    end;<br/>    i := 0;     (* Because the compiler knows that path &lt; 20 *)<br/>    path := 6;  (* Because the compiler knows that path &lt; 20 *)</pre>&#13;
		<p class="indent">In fact, the compiler probably would not generate code for the last two statements; instead, it would substitute the value <code>0</code> for <code>i</code> and <code>6</code> for <code>path</code> in later references. If this seems impressive to you, note that some compilers can even track constant assignments and expressions through nested function calls and complex expressions.</p>&#13;
		<p class="indent">Although a complete description of how a compiler analyzes data flow is beyond the scope of this book, you should have a basic understanding of the process, because a sloppily written program can thwart the compiler’s optimization abilities. Great code works synergistically with the compiler, not against it.</p>&#13;
		<p class="indent">Some compilers can do some truly amazing things when it comes to optimizing high-level code. However, optimization is an inherently slow process. As noted earlier, it is an intractable problem. Fortunately, most <span epub:type="pagebreak" id="page_59"/>programs don’t require full optimization. Even if it runs a little slower than the optimal program, a good approximation is an acceptable compromise when compared to intractable compilation times.</p>&#13;
		<p class="indent">The major concession to compilation time that compilers make during optimization is that they search for only so many possible improvements to a section of code before they move on. Therefore, if your programming style confuses the compiler, it may not be able to generate an optimal (or even close to optimal) executable because it has too many possibilities to consider. The trick, then, is to learn how compilers optimize the source file so you can accommodate them.</p>&#13;
		<p class="indent">To analyze data flow, compilers divide the source code into sequences known as <em>basic blocks</em>—machine instructions into and out of which there are no branches except at the beginning and end. For example, consider the following C code:</p>&#13;
		<pre class="programs">&#13;
			    x = 2;              // Basic block 1<br/>    j = 5;<br/>    i = f( &amp;x, j );     // End of basic block 1<br/>    j = i * 2 + j;      // Basic block 2<br/>    if( j &lt; 10 )        // End of basic block 2<br/>    {<br/>        j = 0;          // Basic block 3<br/>        i = i + 10;<br/>        x = x + i;      // End of basic block 3<br/>    }<br/>    else<br/>    {<br/>        temp = i;       // Basic block 4<br/>        i = j;<br/>        j = j + x;<br/>        x = temp;       // End of basic block 4<br/>    }<br/>    x = x * 2;          // Basic block 5<br/>    ++i;<br/>    --j;<br/><br/><br/><br/>    printf( "i=%d, j=%d, x=%d\n", i, j, x ); // End basic block 5<br/><br/>    // Basic block 6 begins here</pre>&#13;
		<p class="indent">This code snippet contains five basic blocks. Basic block 1 starts with the beginning of the source code. A basic block ends at the point where there is a jump into or out of the sequence of instructions. Basic block 1 ends at the call to the <code>f()</code> function. Basic block 2 starts with the statement following the call to the <code>f()</code> function, then ends at the beginning of the <code>if</code> statement because the <code>if</code> can transfer control to either of two locations. The <code>else</code> clause terminates basic block 3. It also marks the beginning of basic block 4 because there is a jump (from the <code>if</code>’s <code>then</code> clause) to the first statement following the <code>else</code> clause. Basic block 4 ends not because the code <span epub:type="pagebreak" id="page_60"/>transfers control somewhere else, but because there is a jump from basic block 2 to the first statement that begins basic block 5 (from the <code>if</code>’s <code>then</code> section). Basic block 5 ends with a call to the C <code>printf()</code> function.</p>&#13;
		<p class="indent">The easiest way to determine where the basic blocks begin and end is to consider the assembly code that the compiler will generate. Wherever there is a conditional branch/jump, unconditional jump, or call instruction, a basic block will end. Note, however, that the basic block includes the instruction that transfers control to a new location. A new basic block begins immediately after the instruction that transfers control to a new location. Also note that the target label of any conditional branch, unconditional jump, or call instruction begins a basic block.</p>&#13;
		<p class="indent">Basic blocks make it easy for the compiler to track what’s happening to variables and other program objects. As the compiler processes each statement, it can (symbolically) track the values that a variable will hold based upon their initial values and the computations on them within the basic block.</p>&#13;
		<p class="indent">A problem occurs when the paths from two basic blocks join into a single code stream. For example, at the end of basic block 3 in the current example, the compiler could easily determine that the variable <code>j</code> contains zero because code in the basic block assigns the value <code>0</code> to <code>j</code> and then makes no other assignments to <code>j</code>. Similarly, at the end of basic block 3, the program knows that <code>j</code> contains the value <code>j0 + x0</code> (assuming <code>j0</code> represents the initial value of <code>j</code> upon entry into the basic block and <code>x0</code> represents the initial value of <code>x</code> upon entry into the block). But when the paths merge at the beginning of basic block 4, the compiler probably can’t determine whether <code>j</code> will contain zero or the value <code>j0 + x0</code>. This means the compiler has to note that <code>j</code>’s value could be either of two different values at this point.</p>&#13;
		<p class="indent">While keeping track of two possible values that a variable might contain at a given point is easy for a decent optimizer, it’s not hard to imagine a situation where the compiler would have to keep track of many different possible values. In fact, if you have several <code>if</code> statements that the code executes sequentially, and each path through these <code>if</code> statements modifies a given variable, then the number of possible values for each variable doubles with each <code>if</code> statement. In other words, the number of possibilities increases exponentially with the number of <code>if</code> statements in a code sequence. At some point, the compiler cannot keep track of all the possible values a variable might contain, so it has to stop monitoring that information for the given variable. When this happens, there are fewer optimization possibilities that the compiler can consider.</p>&#13;
		<p class="indent">Fortunately, although loops, conditional statements, <code>switch/case</code> statements, and procedure/function calls can increase the number of possible paths through the code exponentially, in practice compilers have few problems with typical well-written programs. This is because as paths from basic blocks converge, programs often make new assignments to their variables (thereby eliminating the old values the compiler was tracking). Compilers generally assume that programs rarely assign a different value to a variable along every distinct path in the program, and their internal data structures <span epub:type="pagebreak" id="page_61"/>reflect this. Keep in mind that if you violate this assumption, the compiler may lose track of variable values and generate inferior code as a result.</p>&#13;
		<p class="indent">Poorly structured programs can create control flow paths that confuse the compiler, reducing the opportunities for optimization. Good programs produce <em>reducible flow graphs</em>, pictorial depictions of the control flow path. <a href="ch04.xhtml#ch4fig5">Figure 4-5</a> is a flow graph for the previous code fragment.</p>&#13;
		<div class="image" id="ch4fig5">&#13;
			<img alt="Image" src="../images/04fig05.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 4-5: An example flow graph</em></p>&#13;
		<p class="indent">As you can see, arrows connect the end of each basic block with the beginning of the basic block into which they transfer control. In this particular example, all of the arrows flow downward, but this isn’t always the case. Loops, for example, transfer control backward in the flow graph. As another example, consider the following Pascal code:</p>&#13;
		<pre class="programs">&#13;
			    write( 'Input a value for i:' );<br/>    readln( i );<br/>    j := 0;<br/>    while( ( j &lt; i ) and ( i &gt; 0 ) ) do begin<br/><br/>        a[j] := i;<br/>        b[i] := 0;<br/>        j := j + 1;<br/>        i := i - 1;<br/><span epub:type="pagebreak" id="page_62"/>    end; (* while *)<br/>    k := i + j;<br/>    writeln( 'i = ', i, 'j = ', j, 'k = ', k );</pre>&#13;
		<p class="indent"><a href="ch04.xhtml#ch4fig6">Figure 4-6</a> shows the flow graph for this simple code fragment.</p>&#13;
		<div class="image" id="ch4fig6">&#13;
			<img alt="Image" src="../images/04fig06.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 4-6: Flow graph for a <span class="codeitalic">while</span> loop</em></p>&#13;
		<p class="indent">As mentioned, flow graphs in well-structured programs are <em>reducible</em>. Although a complete description of what constitutes a reducible flow graph is beyond the scope of this book, any program that consists only of structured control statements (<code>if</code>, <code>while</code>, <code>repeat..until</code>, and so on) and avoids <code>goto</code> statements will be reducible. This is an important point because compiler optimizers generally do a much better job when working on reducible programs. In contrast, programs that are not reducible tend to confuse them.</p>&#13;
		<p class="indent">What makes reducible programs easier for optimizers to deal with is that their basic blocks can be collapsed in an outline fashion, with enclosing blocks inheriting properties (for example, which variables the block modifies) from the enclosed blocks. By processing the source file this way, the optimizer can deal with a small number of basic blocks rather than a large number of statements. This hierarchical approach to optimization is more efficient and allows the optimizer to maintain more information about the program’s state. Furthermore, the exponential time complexity of the optimization problem works for us in this case. By reducing the number of blocks the code has to deal with, you dramatically decrease <span epub:type="pagebreak" id="page_63"/>the amount of work the optimizer must do. Again, the exact details of how the compiler achieves this are not important here. The takeaway is that making your programs reducible enables the optimizer to do its job more effectively. Attempts to “optimize” your code by sticking in lots of <code>goto</code> statements—to avoid duplicating code and executing unnecessary tests—may actually work against you. While you may save a few bytes or a few cycles in the immediate area you’re working on, the end result might confuse the compiler enough that it cannot optimize as well, causing an overall loss of efficiency.</p>&#13;
		<h5 class="h5" id="ch00lev3sec28"><strong>4.4.4.4 Common Compiler Optimizations</strong></h5>&#13;
		<p class="noindent"><a href="ch12.xhtml#ch12">Chapter 12</a> will provide complete definitions and examples of common compiler optimizations in programming contexts where compilers typically use them. But for now, here’s a quick preview of the basic types:</p>&#13;
		<p class="noindent1"><strong>Constant folding</strong></p>&#13;
		<p class="noindent2">Constant folding computes the value of constant expressions or subexpressions at compile time rather than at runtime. See “Constant Folding” on <a href="ch12.xhtml#page_397">page 397</a> for more information.</p>&#13;
		<p class="noindent1"><strong>Constant propagation</strong></p>&#13;
		<p class="noindent2">Constant propagation replaces a variable with a constant value if the compiler determines that the program assigned that constant to the variable earlier in the code. See “Constant Propagation” on <a href="ch12.xhtml#page_400">page 400</a> for more information.</p>&#13;
		<p class="noindent1"><strong>Dead code elimination</strong></p>&#13;
		<p class="noindent2">Dead code elimination removes the object code associated with a particular source code statement when the program will never use the result of that statement, or when a conditional block will never be <code>true</code>. See “Dead Code Elimination” on <a href="ch12.xhtml#page_404">page 404</a> for more information.</p>&#13;
		<p class="noindent1"><strong>Common subexpression elimination</strong></p>&#13;
		<p class="noindent2">Frequently, part of an expression will appear elsewhere in the current function; this is known as a <em>subexpression</em>. If the values of the variables in a subexpression haven’t changed, the program does not need to recompute them everywhere the subexpression appears. The program can simply save the subexpression’s value on the first evaluation and then use it for every other occurrence of the subexpression. See “Common Subexpression Elimination” on <a href="ch12.xhtml#page_410">page 410</a> for more information.</p>&#13;
		<p class="noindent1"><strong>Strength reduction</strong></p>&#13;
		<p class="noindent2">Often, the CPU can directly compute a value using a different operator than the source code specifies. For example, a <code>shift</code> instruction can implement multiplication or division by a constant that is a power of 2, and a bitwise <code>and</code> instruction can compute certain modulo (remainder) operations (the <code>shift</code> and <code>and</code> instructions generally execute much faster than the multiply and divide instructions). Most compiler optimizers <span epub:type="pagebreak" id="page_64"/>are good at recognizing such operations and replacing the more expensive computation with a less expensive sequence of machine instructions. See “Strength Reduction” on <a href="ch12.xhtml#page_417">page 417</a> for more information.</p>&#13;
		<p class="noindent1"><strong>Induction</strong></p>&#13;
		<p class="noindent2">In many expressions, particularly those appearing within a loop, the value of one variable in the expression is completely dependent upon some other variable. Frequently, the compiler can eliminate the computation of the new value or merge the two computations into one for the duration of that loop. See “Induction” on <a href="ch12.xhtml#page_422">page 422</a> for more information.</p>&#13;
		<p class="noindent1"><strong>Loop invariants</strong></p>&#13;
		<p class="noindent2">The optimizations so far have all been techniques a compiler can use to improve code that is already well written. Handling loop invariants, by contrast, is a compiler optimization for fixing bad code. A <em>loop invariant</em> is an expression that does not change on each iteration of some loop. An optimizer can compute the result of such a calculation just once, outside the loop, and then use the computed value within the loop’s body. Many optimizers are smart enough to discover loop invariant calculations and can use <em>code motion</em> to move the invariant calculation outside the loop. See “Loop Invariants” on <a href="ch12.xhtml#page_427">page 427</a> for more information.</p>&#13;
		<p class="indentt">Good compilers can perform many other optimizations, but these are the standard optimizations that any decent compiler should be able to do.</p>&#13;
		<h5 class="h5" id="ch00lev3sec29"><strong>4.4.4.5 Compiler Optimization Control</strong></h5>&#13;
		<p class="noindent">By default, most compilers do very little or no optimization unless you explicitly tell them to. This might seem counterintuitive; after all, we generally want compilers to produce the best possible code for us. However, there are many definitions of “optimal,” and no single compiler output is going to satisfy every possible one.</p>&#13;
		<p class="indent">You might argue that some sort of optimization, even if it’s not the particular type you’re interested in, is better than none at all. However, there are a few reasons why no optimization is a compiler’s default state:</p>&#13;
		<ul>&#13;
			<li>&#13;
				<p class="noindent">Optimization is a slow process. You get quicker turnaround times on compiles when you have the optimizer turned off. This can be a big help during rapid edit-compile-test cycles.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Many debuggers don’t work properly with optimized code, and you have to turn off optimization in order to use a debugger on your application (this also makes analyzing the compiler output much easier).</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Most compiler defects occur in the optimizer. By emitting unoptimized code, you’re less likely to encounter defects in the compiler (then again, the compiler’s author is less likely to be notified about defects in the compiler, too).</p>&#13;
				</li>&#13;
		</ul>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_65"/>Most compilers provide command-line options that let you control the types of optimization the compiler performs. Early C compilers under Unix used command-line arguments like <code>-O</code>, <code>-O1</code>, and <code>-O2</code>. Many later compilers (C and otherwise) have adopted this strategy, if not exactly the same command-line options.</p>&#13;
		<p class="indent">If you’re wondering why a compiler might offer multiple options to control optimization rather than just a single option (optimization or no optimization), remember that “optimal” means different things to different people. Some people might want code that is optimized for space; others might want code that is optimized for speed (and those two optimizations could be mutually exclusive in a given situation). Some people might want to optimize their files but don’t want the compiler to take forever to process them, so they’d be willing to compromise with a small set of fast optimizations. Others might want to control optimization for a specific member of a CPU family (such as the Core i9 processor in the 80x86 family). Furthermore, some optimizations are “safe” (that is, they always produce correct code) only if the program is written in a certain way. You certainly don’t want to enable such optimizations unless the programmer guarantees that they’ve written their code accordingly. Finally, for programmers who are writing their HLL code carefully, some optimizations the compiler performs may actually produce <em>inferior</em> code, in which case the ability to specify optimizations is very handy. For these reasons and more, most modern compilers provide considerable flexibility over the types of optimizations they perform.</p>&#13;
		<p class="indent">Consider the Microsoft Visual C++ compiler. It provides the following command-line options to control optimization:</p>&#13;
		<pre class="programs">&#13;
			                              -OPTIMIZATION-<br/><br/>/O1 minimize space<br/>/O2 maximize speed<br/>/Ob&lt;n&gt; inline expansion (default n=0)<br/>/Od disable optimizations (default)<br/>/Og enable global optimization<br/>/Oi[-] enable intrinsic functions<br/>/Os favor code space<br/>/Ot favor code speed<br/>/Ox maximum optimizations<br/>/favor:&lt;blend|AMD64|INTEL64|ATOM&gt; select processor to optimize for, one of:<br/>    blend - a combination of optimizations for several different x64 processors<br/>    AMD64 - 64-bit AMD processors<br/>    INTEL64 - Intel(R)64 architecture processors<br/>    ATOM - Intel(R) Atom(TM) processors<br/><br/>                             -CODE GENERATION-<br/><br/>/Gw[-] separate global variables for linker<br/>/GF enable read-only string pooling<br/>/Gm[-] enable minimal rebuild<br/>/Gy[-] separate functions for linker<br/>/GS[-] enable security checks<br/><span epub:type="pagebreak" id="page_66"/>/GR[-] enable C++ RTTI<br/>/GX[-] enable C++ EH (same as /EHsc)<br/>/guard:cf[-] enable CFG (control flow guard)<br/>/EHs enable C++ EH (no SEH exceptions)<br/>/EHa enable C++ EH (w/ SEH exceptions)<br/>/EHc extern "C" defaults to nothrow<br/>/EHr always generate noexcept runtime termination checks<br/>/fp:&lt;except[-]|fast|precise|strict&gt; choose floating-point model:<br/>    except[-] - consider floating-point exceptions when generating code<br/>    fast - "fast" floating-point model; results are less predictable<br/>    precise - "precise" floating-point model; results are predictable<br/>    strict - "strict" floating-point model (implies /fp:except)<br/>/Qfast_transcendentals generate inline FP intrinsics even with /fp:except<br/>/Qspectre[-] enable mitigations for CVE 2017-5753<br/>/Qpar[-] enable parallel code generation<br/>/Qpar-report:1 auto-parallelizer diagnostic; indicate parallelized loops<br/>/Qpar-report:2 auto-parallelizer diagnostic; indicate loops not parallelized<br/>/Qvec-report:1 auto-vectorizer diagnostic; indicate vectorized loops<br/>/Qvec-report:2 auto-vectorizer diagnostic; indicate loops not vectorized<br/>/GL[-] enable link-time code generation<br/>/volatile:&lt;iso|ms&gt; choose volatile model:<br/>    iso - Acquire/release semantics not guaranteed on volatile accesses<br/>    ms  - Acquire/release semantics guaranteed on volatile accesses<br/>/GA optimize for Windows Application<br/>/Ge force stack checking for all funcs<br/>/Gs[num] control stack checking calls<br/>/Gh enable _penter function call<br/>/GH enable _pexit function call<br/>/GT generate fiber-safe TLS accesses<br/>/RTC1 Enable fast checks (/RTCsu)<br/>/RTCc Convert to smaller type checks<br/>/RTCs Stack Frame runtime checking<br/>/RTCu Uninitialized local usage checks<br/>/clr[:option] compile for common language runtime, where option is:<br/>    pure - produce IL-only output file (no native executable code)<br/>    safe - produce IL-only verifiable output file<br/>    initialAppDomain - enable initial AppDomain behavior of Visual C++ 2002<br/>    noAssembly - do not produce an assembly<br/>    nostdlib - ignore the default \clr directory<br/>/homeparams Force parameters passed in registers to be written to the stack<br/>/GZ Enable stack checks (/RTCs)<br/>/arch:AVX enable use of instructions available with AVX-enabled CPUs<br/>/arch:AVX2 enable use of instructions available with AVX2-enabled CPUs<br/>/Gv __vectorcall calling convention</pre>&#13;
		<p class="indent">GCC has a comparable—though much longer—list, which you can view by specifying <code>-v --help</code> on the GCC command line. Most of the individual optimization flags begin with <code>-f</code>. You can also use <code>-O<span class="codeitalic1">n</code></span>, where <span class="codeitalic">n</span> is a single digit integer value, to specify different levels of optimization. Take care when using <code>-O3</code> (or higher), as doing so may perform some unsafe optimizations in certain cases.</p>&#13;
		<h4 class="h4" id="ch00lev2sec39"><span epub:type="pagebreak" id="page_67"/><strong>4.4.5 Compiler Benchmarking</strong></h4>&#13;
		<p class="noindent">One real-world constraint on our ability to produce great code is that different compilers provide a wildly varying set of optimizations. Even the same optimizations performed by two different compilers can differ greatly in effectiveness.</p>&#13;
		<p class="indent">Fortunately, several websites have benchmarked various compilers. (One good example is Willus.com.) Simply search online for a topic like “compiler benchmarks” or “compiler comparisons” and have fun.</p>&#13;
		<h4 class="h4" id="ch00lev2sec40"><strong>4.4.6 Native Code Generation</strong></h4>&#13;
		<p class="noindent">The native code generation phase is responsible for translating the intermediate code into machine code for the target CPU. An 80x86 native code generator, for example, might translate the intermediate code sequence given previously into something like the following:</p>&#13;
		<pre class="programs">&#13;
			mov( 5, eax ); // move the constant 5 into the EAX register.<br/>mov( eax, k ); // Store the value in EAX (5) into k.<br/>add( eax, m ); // Add the value in EAX to variable m.</pre>&#13;
		<p class="indent">The second optimization phase, which takes place after native code generation, handles machine idiosyncrasies that don’t exist on all machines. For example, an optimizer for a Pentium II processor might replace an instruction of the form <code>add(1, eax);</code> with the instruction <code>inc(eax);</code>. Optimizers for later CPUs might do just the opposite. Optimizers for certain 80x86 processors might arrange the sequence of instructions one way to maximize parallel execution of the instructions in a superscalar CPU, while an optimizer targeting a different (80x86) CPU might arrange the instructions differently.</p>&#13;
		<h3 class="h3" id="ch00lev1sec35"><strong>4.5 Compiler Output</strong></h3>&#13;
		<p class="noindent">The previous section stated that compilers typically produce machine code as their output. Strictly speaking, this is neither necessary nor even that common. Most compiler output is not code that a given CPU can directly execute. Some compilers emit assembly language source code, which requires further processing by an assembler prior to execution. Other compilers produce an object file, which is similar to executable code but is not directly executable. Still other compilers actually produce source code output that requires further processing by a different HLL compiler. I’ll discuss these different output formats and their advantages and disadvantages in this section.</p>&#13;
		<h4 class="h4" id="ch00lev2sec41"><strong>4.5.1 Emitting HLL Code as Compiler Output</strong></h4>&#13;
		<p class="noindent">Some compilers actually emit output that is source code for a different high-level programming language (see <a href="ch04.xhtml#ch4fig7">Figure 4-7</a>). For example, many compilers (including the original C++ compiler) emit C code as their output. Indeed, compiler writers who emit some high-level source code from their compiler frequently choose the C programming language.</p>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_68"/>Emitting HLL code as compiler output offers several advantages. The output is human-readable and generally easy to verify. The HLL code emitted is often portable across various platforms; for example, if a compiler emits C code, you can usually compile that output on several different machines because C compilers exist for most platforms. Finally, by emitting HLL code, a translator can rely on the optimizer of the target language’s compiler, thereby saving the effort of writing its own optimizer. In other words, emitting HLL code allows a compiler writer to create a less complex code generator module and rely on the robustness of some other compiler for the most complex part of the compilation process.</p>&#13;
		<div class="image" id="ch4fig7">&#13;
			<img alt="Image" src="../images/04fig07.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 4-7: Emission of HLL code by a compiler</em></p>&#13;
		<p class="indent">Emitting HLL code has several disadvantages, too. First and foremost, this approach usually takes more processing time than directly generating executable code. To produce an executable file, a second, otherwise unnecessary, compiler might be needed. Worse, the output of that second compiler might need to be processed further by another compiler or assembler, exacerbating the problem. Another disadvantage is that in HLL code it’s difficult to embed debugging information that a debugger program can use. Perhaps the most fundamental problem with this approach, however, is that HLLs are usually an abstraction of the underlying machine. Therefore, it could be quite difficult for a compiler to emit statements in an HLL that efficiently map to low-level machine code.</p>&#13;
		<p class="indent">Generally, compilers that emit HLL statements as their output are translating a <em>very high-level language (VHLL)</em> into a lower-level language. For example, C is often considered to be a fairly low-level HLL, which is one reason why it’s a popular output format for many compilers. Projects that have attempted to create a special, portable, low-level language specifically for this purpose have never been enormously popular. Check out any of the “C- -” projects on the internet for examples of such systems.</p>&#13;
		<p class="indent">If you want to write efficient code by analyzing compiler output, you’ll probably find it more difficult to work with compilers that output HLL code. With a standard compiler, all you have to learn is the particular machine code statements that your compiler produces. However, when a compiler emits HLL statements as its output, learning to write great code <span epub:type="pagebreak" id="page_69"/>with that compiler is more difficult. You need to understand both how the main language emits the HLL statements and how the second compiler translates the code into machine code.</p>&#13;
		<p class="indent">Generally, compilers that produce HLL code as their output are either experimental compilers for VHLLs, or compilers that attempt to translate legacy code in an older language to a more modern language (for example, FORTRAN to C). As a result, expecting those compilers to emit efficient code is generally asking too much. Thus, you’d probably be wise to avoid a compiler that emits HLL statements. A compiler that directly generates machine code (or assembly language code) is more likely to produce smaller and faster-running executables.</p>&#13;
		<h4 class="h4" id="ch00lev2sec42"><strong>4.5.2 Emitting Assembly Language as Compiler Output</strong></h4>&#13;
		<p class="noindent">Many compilers will emit human-readable assembly language source files rather than binary machine code files (see <a href="ch04.xhtml#ch4fig8">Figure 4-8</a>). Probably the most famous example is the FSF/GNU GCC compiler suite, which emits assembly language output for the FSF/GNU assembler Gas. Like compilers that emit HLL source code, emitting assembly language has some advantages and disadvantages.</p>&#13;
		<div class="image" id="ch4fig8">&#13;
			<img alt="Image" src="../images/04fig08.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 4-8: Emission of assembly code by a compiler</em></p>&#13;
		<p class="indent">The principal disadvantages to emitting assembly code are similar to the downsides of emitting HLL source output. First, you have to run a second language translator (namely the assembler) to produce the actual object code for execution. Second, some assemblers may not allow the embedding of debugging metadata that allows a debugger to work with the original source code (though many assemblers do support this capability). These two disadvantages turn out to be minimal if a compiler emits code for an appropriate assembler. For example, Gas is very fast and supports the insertion of debug information for use by source-level debuggers. Therefore, the FSF/GNU compilers don’t suffer as a result of emitting Gas output.</p>&#13;
		<p class="indent">The advantage of assembly language output, particularly for our purposes, is that it’s easy to read the compiler’s output and determine which machine instructions the compiler emits. Indeed, this book uses this <span epub:type="pagebreak" id="page_70"/>compiler facility to analyze compiler output. Emitting assembly code frees the compiler writer from having to worry about several different object code output formats—the underlying assembler handles that—which allows the compiler writer to create a more portable compiler. True, the assembler has to be capable of generating code for different operating systems, but you only need to repeat this exercise once for each object file format, rather than once for each format multiplied by the number of compilers you write. The FSF/GNU compiler suite has taken good advantage of the Unix philosophy of using small tools that chain together to accomplish larger, more complicated tasks—that is, minimize redundancy.</p>&#13;
		<p class="indent">Another advantage of compilers that can emit assembly language output is that they generally allow you to embed <em>inline assembly language</em> statements in the HLL code. This lets you insert a few machine instructions directly into time-critical sections of your code without the hassle of having to create a separate assembly language program and link its output to your HLL program.</p>&#13;
		<h4 class="h4" id="ch00lev2sec43"><strong>4.5.3 Emitting Object Files as Compiler Output</strong></h4>&#13;
		<p class="noindent">Most compilers translate the source language into an object file format, an intermediate file format that contains machine instructions and binary runtime data along with certain metadata. This metadata allows a linker/loader program to combine various object modules to produce a complete executable. This in turn allows programmers to link <em>library modules</em> and other object modules that they’ve written and compiled separately from their main application module.</p>&#13;
		<p class="indent">The advantage of object file output is that you don’t need a separate compiler or assembler to convert the compiler’s output to object code form, which saves a little time during compilation. Note, however, that a linker program must still process the object file output, which consumes a little time after compilation. Nevertheless, linkers are generally quite fast, so it’s usually more cost-effective to compile a single module and link it with several previously compiled modules than it is to compile all the modules together to form an executable file.</p>&#13;
		<p class="indent">Object modules are binary files and do not contain human-readable data, so it’s a bit more difficult to analyze compiler output in this format than in the others we’ve discussed. Fortunately, there are utility programs that will disassemble the output of an object module into a human-readable form. The result isn’t as easy to read as straight assembly compiler output, but you can still do a reasonably good job.</p>&#13;
		<p class="indent">Because object files are challenging to analyze, many compiler writers provide an option to emit assembly code instead of object code. This handy feature makes analysis much easier, so we’ll use it with various compilers throughout this book.</p>&#13;
		<div class="note">&#13;
			<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
			<p class="notep"><em>The section “Object File Formats” on <a href="ch04.xhtml#page_71">page 71</a> provides a detailed look at the elements of an object file, focusing on COFF (Common Object File Format).</em></p>&#13;
		</div>&#13;
		<h4 class="h4" id="ch00lev2sec44"><span epub:type="pagebreak" id="page_71"/><strong>4.5.4 Emitting Executable Files as Compiler Output</strong></h4>&#13;
		<p class="noindent">Some compilers directly emit an executable output file. These compilers are often very fast, producing almost instantaneous turnaround during the edit-compile-run-test-debug cycle. Unfortunately, their output is often the most difficult to read and analyze, requiring the use of a debugger or disassembler program and a lot of manual work. Nevertheless, the fast turnaround makes these compilers popular, so later in this book, we’ll look at how to analyze the executable files they produce.</p>&#13;
		<h3 class="h3" id="ch00lev1sec36"><strong>4.6 Object File Formats</strong></h3>&#13;
		<p class="noindent">As previously noted, object files are among the most popular forms of compiler output. Even though it is possible to create a proprietary object file format—one that only a single compiler and its associated tools can use—most compilers emit code using one or more standardized object file formats. This allows different compilers to share the same set of object file utilities, including linkers, librarians, dump utilities, and disassemblers. Examples of common object file formats include: OMF (Object Module Format), COFF (Common Object File Format), PE/COFF (Microsoft’s Portable Executable variant on COFF), and ELF (Executable and Linkable Format). There are several variants of these file formats, as well as many altogether different object file formats.</p>&#13;
		<p class="indent">Most programmers understand that object files represent the machine code that an application executes, but they often don’t realize the impact of the object file’s organization on their application’s performance and size. Although you don’t need to have detailed knowledge of an object file’s internal representation to write great code, having a basic understanding will help you organize your source files to better take advantage of the way compilers and assemblers generate code for your applications.</p>&#13;
		<p class="indent">An object file usually begins with a header that comprises the first few bytes of the file. This header contains certain <em>signature information</em> that identifies the file as a valid object file, along with several other values that define the location of various data structures in the file. Beyond the header, an object file is usually divided into several sections, each containing application data, machine instructions, symbol table entries, relocation data, and other metadata (data about the program). In some cases, the actual code and data represent only a small part of the entire object code file.</p>&#13;
		<p class="indent">To get a feeling for how object files are structured, it’s worthwhile to look at a specific object file format in detail. I’ll use COFF in the following discussion because most object file formats (for example, ELF and PE/COFF) are based on, or very similar to, COFF. The basic layout of a COFF file is shown in <a href="ch04.xhtml#ch4fig9">Figure 4-9</a>, after which I’ll describe each section in turn.</p>&#13;
		<div class="image" id="ch4fig9">&#13;
			<span epub:type="pagebreak" id="page_72"/>&#13;
			<img alt="Image" src="../images/04fig09.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 4-9: Layout of a COFF file</em></p>&#13;
		<h4 class="h4" id="ch00lev2sec45"><strong>4.6.1 The COFF File Header</strong></h4>&#13;
		<p class="noindent">At the beginning of every COFF file is a <em>COFF file header</em>. Here are the definitions that Microsoft Windows and Linux use for the COFF header structure:</p>&#13;
		<pre class="programs">&#13;
			// Microsoft Windows winnt.h version:<br/><br/>typedef struct _IMAGE_FILE_HEADER {<br/>    WORD    Machine;<br/>    WORD    NumberOfSections;<br/>    DWORD   TimeDateStamp;<br/>    DWORD   PointerToSymbolTable;<br/>    DWORD   NumberOfSymbols;<br/>    WORD    SizeOfOptionalHeader;<br/>    WORD    Characteristics;<br/>} IMAGE_FILE_HEADER, *PIMAGE_FILE_HEADER;<br/><br/>// Linux coff.h version:<br/><br/>struct COFF_filehdr {<br/>        char f_magic[2];        /* magic number */<br/>        char f_nscns[2];        /* number of sections */<br/>        char f_timdat[4];       /* time &amp; date stamp */<br/>        char f_symptr[4];       /* file pointer to symtab */<br/>        char f_nsyms[4];        /* number of symtab entries */<br/>        char f_opthdr[2];       /* sizeof(optional hdr) */<br/>        char f_flags[2];        /* flags */<br/>};</pre>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_73"/>The Linux <em>coff.h</em> header file uses traditional Unix names for these fields; the Microsoft <em>winnt.h</em> header file uses (arguably) more readable names. Here’s a summary of each field in the header, with Unix names to the left of the slash and Microsoft equivalents to the right:</p>&#13;
		<p class="noindent1"><span class="codestrong">f_magic/Machine</span></p>&#13;
		<p class="noindent2">Identifies the system for which this COFF file was created. In the original Unix definition, this value identified the particular Unix port for which the code was created. Today’s operating systems define this value somewhat differently, but the bottom line is that this value is a signature that specifies whether the COFF file contains data or machine instructions that are appropriate for the current operating system and CPU.</p>&#13;
		<p class="indentt"><a href="ch04.xhtml#ch4tab1">Table 4-1</a> provides the encodings for the <code>f_magic/Machine</code> field.</p>&#13;
		<p class="tabcap" id="ch4tab1"><strong>Table 4-1:</strong> <code>f_magic/Machine</code> Field Encoding</p>&#13;
		<table class="all">&#13;
			<colgroup>&#13;
				<col style="width:30%"/>&#13;
				<col style="width:70%"/>&#13;
			</colgroup>&#13;
			<tbody>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table"><strong>Value</strong></p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table"><strong>Description</strong></p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">0x14c</p>&#13;
					</td>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">Intel 386</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">0x8664</p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">x86-64</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">0x162</p>&#13;
					</td>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">MIPS R3000</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">0x168</p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">MIPS R10000</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">0x169</p>&#13;
					</td>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">MIPS little endian WCI v2</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">0x183</p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">old Alpha AXP</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">0x184</p>&#13;
					</td>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">Alpha AXP</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">0x1a2</p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">Hitachi SH3</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">0x1a3</p>&#13;
					</td>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">Hitachi SH3 DSP</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">0x1a6</p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">Hitachi SH4</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">0x1a8</p>&#13;
					</td>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">Hitachi SH5</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">0x1c0</p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">ARM little endian</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">0x1c2</p>&#13;
					</td>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">Thumb</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">0x1c4</p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">ARMv7</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">0x1d3</p>&#13;
					</td>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">Matsushita AM33</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">0x1f0</p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">PowerPC little endian</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">0x1f1</p>&#13;
					</td>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">PowerPC with floating-point support</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">0x200</p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">Intel IA64</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">0x266</p>&#13;
					</td>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">MIPS16</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">0x268</p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">Motorola 68000 series</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">0x284</p>&#13;
					</td>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">Alpha AXP 64-bit</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">0x366</p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">MIPS with FPU</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">0x466</p>&#13;
					</td>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">MIPS16 with FPU</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">0xebc</p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">EFI bytecode</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">0x8664</p>&#13;
					</td>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">AMD AMD64</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">0x9041</p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">Mitsubishi M32R little endian</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">0xaa64</p>&#13;
					</td>&#13;
					<td class="table-1b" style="vertical-align: top;">&#13;
						<p class="table">ARM64 little endian</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">0xc0ee</p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">CLR pure MSIL</p>&#13;
					</td>&#13;
				</tr>&#13;
			</tbody>&#13;
		</table>&#13;
		<p class="noindent1"><span epub:type="pagebreak" id="page_74"/><span class="codestrong">f_nscns/NumberOfSections</span></p>&#13;
		<p class="noindent2">Specifies how many segments (sections) are present in the COFF file. A linker program can iterate through a set of section headers (described a little later) using this value.</p>&#13;
		<p class="noindent1"><span class="codestrong">f_timdat/TimeDateStamp</span></p>&#13;
		<p class="noindent2">Contains a Unix-style timestamp (number of seconds since January 1, 1970) value specifying the file’s creation date and time.</p>&#13;
		<p class="noindent1"><span class="codestrong">f_symptr/PointerToSymbolTable</span></p>&#13;
		<p class="noindent2">Contains a file offset value (that is, the number of bytes from the beginning of the file) that specifies where the <em>symbol table</em> begins in the file. The symbol table is a data structure that specifies the names and other information about all external, global, and other symbols used by the code in the COFF file. Linkers use the symbol table to resolve external references. This symbol table information may also appear in the final executable file for use by a symbolic debugger.</p>&#13;
		<p class="noindent1"><span class="codestrong">f_nsyms/NumberOfSymbols</span></p>&#13;
		<p class="noindent2">The number of entries in the symbol table.</p>&#13;
		<p class="noindent1"><span class="codestrong">f_opthdr/SizeOfOptionalHeader</span></p>&#13;
		<p class="noindent2">Specifies the size of the optional header that immediately follows the file header (that is, the first byte of the optional header immediately follows the <code>f_flags/Characteristics</code> field in the file header structure). A linker or other object code manipulation program would use the value in this field to determine where the optional header ends and the section headers begin in the file. The section headers immediately follow the optional header, but the optional header’s size isn’t fixed. Different implementations of a COFF file can have different optional header structures. If the optional header is not present in a COFF file, the <code>f_opthdr/SizeOfOptionalHeader</code> field will contain zero, and the first section header will immediately follow the file header.</p>&#13;
		<p class="noindent1"><span class="codestrong">f_flags/Characteristics</span></p>&#13;
		<p class="noindent2">A small bitmap that specifies certain Boolean flags, such as whether the file is executable, whether it contains symbol information, and whether it contains line number information (for use by debuggers).</p>&#13;
		<h4 class="h4" id="ch00lev2sec46"><strong>4.6.2 The COFF Optional Header</strong></h4>&#13;
		<p class="noindent">The COFF optional header contains information pertinent to executable files. This header may not be present if the file contains object code that is not executable (because of unresolved references). Note, however, that this optional header is always present in Linux COFF and Microsoft PE/COFF files, even when the file is not executable. The Windows and Linux structures for this optional file header take the following forms in C.</p>&#13;
		<pre class="programs"><span epub:type="pagebreak" id="page_75"/>// Microsoft PE/COFF Optional Header (from winnt.h)<br/><br/>typedef struct _IMAGE_OPTIONAL_HEADER {<br/>    //<br/>    // Standard fields.<br/>    //<br/><br/>    WORD    Magic;<br/>    BYTE    MajorLinkerVersion;<br/>    BYTE    MinorLinkerVersion;<br/>    DWORD   SizeOfCode;<br/>    DWORD   SizeOfInitializedData;<br/>    DWORD   SizeOfUninitializedData;<br/>    DWORD   AddressOfEntryPoint;<br/>    DWORD   BaseOfCode;<br/>    DWORD   BaseOfData;<br/><br/>    //<br/>    // NT additional fields.<br/>    //<br/><br/>    DWORD   ImageBase;<br/>    DWORD   SectionAlignment;<br/>    DWORD   FileAlignment;<br/>    WORD    MajorOperatingSystemVersion;<br/>    WORD    MinorOperatingSystemVersion;<br/>    WORD    MajorImageVersion;<br/>    WORD    MinorImageVersion;<br/>    WORD    MajorSubsystemVersion;<br/>    WORD    MinorSubsystemVersion;<br/>    DWORD   Win32VersionValue;<br/>    DWORD   SizeOfImage;<br/>    DWORD   SizeOfHeaders;<br/>    DWORD   CheckSum;<br/>    WORD    Subsystem;<br/>    WORD    DllCharacteristics;<br/>    DWORD   SizeOfStackReserve;<br/>    DWORD   SizeOfStackCommit;<br/>    DWORD   SizeOfHeapReserve;<br/>    DWORD   SizeOfHeapCommit;<br/>    DWORD   LoaderFlags;<br/>    DWORD   NumberOfRvaAndSizes;<br/>    IMAGE_DATA_DIRECTORY DataDirectory[IMAGE_NUMBEROF_DIRECTORY_ENTRIES];<br/>} IMAGE_OPTIONAL_HEADER32, *PIMAGE_OPTIONAL_HEADER32;<br/><br/><br/>// Linux/COFF Optional Header format (from coff.h)<br/><br/>typedef struct<br/>{<br/>  char  magic[2];  /* type of file */<br/>  char  vstamp[2]; /* version stamp */<br/>  char  tsize[4];  /* text size in bytes, padded to<br/>                      FW bdry */<br/><span epub:type="pagebreak" id="page_76"/>  char  dsize[4]; /* initialized   data "   " */<br/>  char  bsize[4]; /* uninitialized data "   " */<br/>  char  entry[4]; /* entry pt. */<br/>  char  text_start[4]; /* base of text used for this file */<br/>  char  data_start[4]; /* base of data used for this file */<br/>} COFF_AOUTHDR;</pre>&#13;
		<p class="indent">The first thing to notice is that these structures are not identical. The Microsoft version has considerably more information than the Linux version. The <code>f_opthdr/SizeOfOptionalHeader</code> field exists in the file header to determine the actual size of the optional header.</p>&#13;
		<p class="noindent1"><span class="codestrong">magic/Magic</span></p>&#13;
		<p class="noindent2">Provides yet another signature value for the COFF file. This signature value identifies the file type (that is, COFF) rather than the system under which it was created. Linkers use the value of this field to determine if they are truly operating on a COFF file (instead of some arbitrary file that would confuse the linker).</p>&#13;
		<p class="noindent1"><span class="codestrong">vstamp/MajorLinkerVersion/MinorLinkerVersion</span></p>&#13;
		<p class="noindent2">Specifies the version number of the COFF format so that a linker written for an older version of the file format won’t try to process files intended for newer linkers.</p>&#13;
		<p class="noindent1"><span class="codestrong">tsize/SizeOfCode</span></p>&#13;
		<p class="noindent2">Attempts to specify the size of the code section found in the file. If the COFF file contains more than one code section, the value of this field is undefined, although it usually specifies the size of the first code/text section in the COFF file.</p>&#13;
		<p class="noindent1"><span class="codestrong">dsize/SizeOfInitializedData</span></p>&#13;
		<p class="noindent2">Specifies the size of the data segment appearing in this COFF file. Once again, this field is undefined if there are two or more data sections in the file. Usually, this field specifies the size of the first data section if there are multiple data sections.</p>&#13;
		<p class="noindent1"><span class="codestrong">bsize/SizeOfUninitializedData</span></p>&#13;
		<p class="noindent2">Specifies the size of the <em>block started by symbol (BSS)</em> section—the uninitialized data section—in the COFF file. As for the text and data sections, this field is undefined if there are two or more BSS sections; in such cases this field usually specifies the size of the first BSS section in the file.</p>&#13;
		<div class="note">&#13;
			<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
			<p class="notep"><em>See “Pages, Segments, and File Size” on <a href="ch04.xhtml#page_81">page 81</a> for more on BSS sections.</em></p>&#13;
		</div>&#13;
		<p class="noindent1"><span class="codestrong">entry/AddressOfEntryPoint</span></p>&#13;
		<p class="noindent2">Contains the starting address of the executable program. Like other pointers in the COFF file header, this field is actually an offset into the file; it is not an actual memory address.</p>&#13;
		<p class="noindent1"><span epub:type="pagebreak" id="page_77"/><span class="codestrong">text_start/BaseOfCode</span></p>&#13;
		<p class="noindent2">Specifies a file offset into the COFF file where the code section begins. If there are two or more code sections, this field is undefined, but it generally specifies the offset to the first code section in the COFF file.</p>&#13;
		<p class="noindent1"><span class="codestrong">data_start/BaseOfData</span></p>&#13;
		<p class="noindent2">Specifies a file offset into the COFF file where the data section begins. If there are two or more data sections, this field is undefined, but it generally specifies the offset to the first data section in the COFF file.</p>&#13;
		<p class="indentt">There is no need for a <code>bss_start/StartOfUninitializedData</code> field. The COFF file format assumes that the operating system’s program loader will automatically allocate storage for a BSS section when the program loads into memory. There is no need to consume space in the COFF file for uninitialized data (however, “Executable File Formats” on <a href="ch04.xhtml#page_80">page 80</a> describes how some compilers actually merge BSS and DATA sections together for performance reasons).</p>&#13;
		<p class="indent">The optional file header structure is actually a throwback to the <em>a.out</em> format, an older object file format used in Unix systems. This is why it doesn’t handle multiple text/code and data sections, even though COFF allows them.</p>&#13;
		<p class="indent">The remaining fields in the Windows variant of the optional header hold values that Windows linkers allow programmers to specify. While their purposes will likely be clear to anyone who has manually run Microsoft’s linker from a command line, those are not important here. What is important is that COFF does not require a specific data structure for the optional header. Different implementations of COFF (such as Microsoft’s) may freely extend the definition of the optional header.</p>&#13;
		<h4 class="h4" id="ch00lev2sec47"><strong>4.6.3 COFF Section Headers</strong></h4>&#13;
		<p class="noindent">The section headers follow the optional header in a COFF file. Unlike the file and optional headers, a COFF file may contain multiple section headers. The <code>f_nscns/NumberOfSections</code> field in the file header specifies the exact number of section headers (and, therefore, sections) found in the COFF file. Keep in mind that the first section header does not begin at a fixed offset in the file. Because the optional header’s size is variable (and, in fact, could even be 0 if it isn’t present), you have to add the <code>f_opthdr/SizeOfOptionalHeader</code> field in the file header to the size of the file header to get the starting offset of the first section header. Section headers are a fixed size, so once you have the address of the first section header you can easily compute the address of any other by multiplying the desired section header number by the section header size and adding the result to the base offset of the first section header.</p>&#13;
		<p class="indent">Here are the C struct definitions for Windows and Linux section headers:</p>&#13;
		<pre class="programs">&#13;
			// Windows section header structure (from winnt.h)<br/><br/>typedef struct _IMAGE_SECTION_HEADER {<br/>    BYTE    Name[IMAGE_SIZEOF_SHORT_NAME];<br/>    union {<br/>            DWORD   PhysicalAddress;<br/><span epub:type="pagebreak" id="page_78"/>            DWORD   VirtualSize;<br/>    } Misc;<br/>    DWORD   VirtualAddress;<br/>    DWORD   SizeOfRawData;<br/>    DWORD   PointerToRawData;<br/>    DWORD   PointerToRelocations;<br/>    DWORD   PointerToLinenumbers;<br/>    WORD    NumberOfRelocations;<br/>    WORD    NumberOfLinenumbers;<br/>    DWORD   Characteristics;<br/>} IMAGE_SECTION_HEADER, *PIMAGE_SECTION_HEADER;<br/><br/><br/>// Linux section header definition (from coff.h)<br/><br/>struct COFF_scnhdr<br/>{<br/>  char s_name[8]; /* section name */<br/>  char s_paddr[4]; /* physical address, aliased s_nlib */<br/>  char s_vaddr[4]; /* virtual address */<br/>  char s_size[4]; /* section size */<br/>  char s_scnptr[4]; /* file ptr to raw data */<br/>  char s_relptr[4]; /* file ptr to relocation */<br/>  char s_lnnoptr[4]; /* file ptr to line numbers */<br/>  char s_nreloc[2]; /* number of relocation entries */<br/>  char s_nlnno[2]; /* number of line number entries */<br/>  char s_flags[4]; /* flags */<br/>};</pre>&#13;
		<p class="indent">If you inspect these two structures closely, you’ll find that they are roughly equivalent (the only structural difference is that Windows overloads the physical address field, which in Linux is always equivalent to the <code>VirtualAddress</code> field, to hold a <code>VirtualSize</code> field).</p>&#13;
		<p class="indent">Here’s a summary of each field:</p>&#13;
		<p class="noindent1"><span class="codestrong">s_name/Name</span></p>&#13;
		<p class="noindent2">Specifies the name of the section. As is apparent in the Linux definition, this field is limited to eight characters and, accordingly, section names will be a maximum of eight characters long. (Usually, if a source file specifies a longer name, the compiler/assembler will truncate it to 8 characters when creating the COFF file.) If the section name is exactly eight characters long, those eight characters will consume all 8 bytes of this field and there will be no zero-terminating byte. If the section name is shorter than eight characters, a zero-terminating byte will follow the name. The value of this field is often something like <code>.text</code>, <code>CODE</code>, <code>.data</code>, or <code>DATA</code>. Note, however, that the name does not define the segment’s type. You could create a code/text section and name it <code>DATA</code>; you could also create a data section and name it <code>.text</code> or <code>CODE</code>. The <code>s_flags/Characteristics</code> field determines the actual type of this section.</p>&#13;
		<p class="noindent1"><span epub:type="pagebreak" id="page_79"/><span class="codestrong">s_paddr/PhysicalAddress/VirtualSize</span></p>&#13;
		<p class="noindent2">Not used by most tools. Under Unix-like operating systems (such as Linux), this field is usually set to the same value as the <code>VirtualAddress</code> field. Different Windows tools set this field to different values (including zero); the linker/loader seems to ignore whatever value appears here.</p>&#13;
		<p class="noindent1"><span class="codestrong">s_vaddr/VirtualAddress</span></p>&#13;
		<p class="noindent2">Specifies the section’s loading address in memory (that is, its virtual memory address). Note that this is a runtime memory address, not an offset into the file. The program loader uses this value to determine where to load the section into memory.</p>&#13;
		<p class="noindent1"><span class="codestrong">s_size/SizeOfRawData</span></p>&#13;
		<p class="noindent2">Specifies the size, in bytes, of the section.</p>&#13;
		<p class="noindent1"><span class="codestrong">s_scnptr/PointerToRawData</span></p>&#13;
		<p class="noindent2">Provides the file offset to the start of the section’s data in the COFF file.</p>&#13;
		<p class="noindent1"><span class="codestrong">s_relptr/PointerToRelocations</span></p>&#13;
		<p class="noindent2">Provides a file offset to the relocation list for this particular section.</p>&#13;
		<p class="noindent1"><span class="codestrong">s_lnnoptr/PointerToLinenumbers</span></p>&#13;
		<p class="noindent2">Contains a file offset to the line number records for the current section.</p>&#13;
		<p class="noindent1"><span class="codestrong">s_nreloc/NumberOfRelocations</span></p>&#13;
		<p class="noindent2">Specifies the number of <em>relocation entries</em> found at that file offset. Relocation entries are small structures that provide file offsets to address data in the section’s data area that must be patched when the file is loaded into memory. We won’t discuss these relocation entries in this book, but if you’re interested in more details, see the references at the end of this chapter.</p>&#13;
		<p class="noindent1"><span class="codestrong">s_nlnno/NumberOfLinenumbers</span></p>&#13;
		<p class="noindent2">Specifies how many line number records can be found at that offset. Line number information is used by debuggers and is beyond the scope of this chapter. Again, see the references at the end of this chapter if you’re interested in more information about the line number entries.</p>&#13;
		<p class="noindent1"><span class="codestrong">s_flags/Characteristics</span></p>&#13;
		<p class="noindent2">A bitmap that specifies the characteristics of this section. In particular, this field will tell you whether the section requires relocation, whether it contains code, whether it is read-only, and so on.</p>&#13;
		<h4 class="h4" id="ch00lev2sec48"><strong>4.6.4 COFF Sections</strong></h4>&#13;
		<p class="noindent">The section headers provide a directory that describes the actual data and code found in the object file. The <code>s_scnptr/PointerToRawData</code> field contains a file offset to where the raw binary data or code is sitting in the file, and the <code>s_size/SizeOfRawData</code> field specifies the length of the section’s data. Due <span epub:type="pagebreak" id="page_80"/>to relocation requirements, the data actually appearing in the section block may not be an exact representation of the data that the operating system loads into memory. This is because many instruction operand addresses and pointer values appearing in the section may need to be <em>patched</em> to relocate the file based on where the OS loads it into memory. The relocation list (which is separate from the section’s data) contains offsets into the section where the OS must patch the relocatable addresses. The OS performs this patching when loading the section’s data from disk.</p>&#13;
		<p class="indent">Although the bytes in a COFF section may not be an exact representation of the data in memory at runtime, the COFF format requires that all of the bytes in the section <em>map</em> to the corresponding address in memory. This allows the loader to copy the section’s data directly from the file into sequential memory locations. The relocation operation never inserts or deletes bytes in a section; it only changes the values of certain bytes in the section. This requirement helps simplify the system loader and improves application performance because the operating system doesn’t have to move large blocks of memory around when loading the application into memory. The drawback to this scheme is that the COFF format misses the opportunity to compress redundant data appearing in the section’s data area. The COFF designers felt it was more important to emphasize performance over space in their design.</p>&#13;
		<h4 class="h4" id="ch00lev2sec49"><strong>4.6.5 The Relocation Section</strong></h4>&#13;
		<p class="noindent">The relocation section in the COFF file contains the offsets to the pointers in the COFF sections that must be relocated when the system loads those sections’ code or data into memory.</p>&#13;
		<h4 class="h4" id="ch00lev2sec50"><strong>4.6.6 Debugging and Symbolic Information</strong></h4>&#13;
		<p class="noindent">The last three sections shown in <a href="ch04.xhtml#ch4fig9">Figure 4-9</a> contain information that debuggers and linkers use. One section contains line number information that a debugger uses to correlate lines of source code with the executable machine code instructions. The symbol table and string table sections hold the public and external symbols for the COFF file. Linkers use this information to resolve external references between object modules; debuggers use this information to display symbolic variable and function names during debugging.</p>&#13;
		<div class="note">&#13;
			<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
			<p class="notep"><em>This book doesn’t provide a complete description of the COFF file format, but you’ll definitely want to dig deeper into it and other object code formats (ELF, MACH-O, OMF, and so on) if you’re interested in writing applications such as assemblers, compilers, and linkers. To study this area further, see the references at the end of this chapter.</em></p>&#13;
		</div>&#13;
		<h3 class="h3" id="ch00lev1sec37"><strong>4.7 Executable File Formats</strong></h3>&#13;
		<p class="noindent">Most operating systems use a special file format for executable files. Often, the executable file format is similar to the object file format, the principal difference being that there are usually no unresolved external references in the executable file.</p>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_81"/>In addition to machine code and binary data, executable files contain other metadata, including debugging information, linkage information for dynamically linked libraries, and details about how the operating system should load different sections of the file into memory. Depending on the CPU and OS, the executable files may also contain relocation information so that the OS can patch absolute addresses when it loads the file into memory. Object code files contain the same information, so it’s no surprise that the executable file formats used by many operating systems are similar to their object file formats.</p>&#13;
		<p class="indent">The Executable and Linkable Format (ELF), employed by Linux, QNX, and other Unix-like operating systems, is very typical of a combined object file format and executable format. Indeed, the name of the format suggests its dual nature. As another example, Microsoft’s PE file format is a straightforward variant of the COFF format. The similarity between the object and executable file formats allows the OS designer to share code between the loader (responsible for executing the program) and linker applications. Given this similarity, there’s little reason to discuss the specific data structures found in an executable file, as doing so would largely repeat the information from the previous sections.</p>&#13;
		<p class="indent">However, there’s one very practical difference in the layout of these two types of files worth mentioning. Object files are usually designed to be as small as possible, while executable files are usually designed to load into memory as fast as possible, even if this means that they’re larger than absolutely necessary. It may seem paradoxical that a larger file could load into memory faster than a smaller file; however, the OS might load only a small part of the executable file into memory at one time if it supports virtual memory. As we’ll discuss next, a well-designed executable file format can take advantage of this fact by laying out the data and machine instructions in the file to reduce virtual memory overhead.</p>&#13;
		<h4 class="h4" id="ch00lev2sec51"><strong>4.7.1 Pages, Segments, and File Size</strong></h4>&#13;
		<p class="noindent">Virtual memory subsystems and memory protection schemes generally operate on <em>pages</em> in memory. A page on a typical processor is usually between 1KB and 64KB in size. Whatever the size, a page is the smallest unit of memory to which you can apply discrete protection features (such as whether the data in that page is read-only, read/write, or executable). In particular, you cannot mix read-only/executable code with read/write data in the same page—the two must appear in separate pages in memory. Using the 80x86 CPU family as an example, pages in memory are 4KB each. Therefore, the minimum amount of code space and the minimum amount of data space we can allocate to a process is 8KB if we have read/write data and we want to place the machine instructions in read-only memory. In fact, most programs contain several segments or sections (as you saw previously with object files) to which we can apply individual protection rights, and each section will require a unique set of one or more pages in memory that are not shared with any of the other sections. A typical program has four or more sections in memory: code or text, static data, uninitialized <span epub:type="pagebreak" id="page_82"/>data, and stack are the most common. In addition, many compilers also generate heap segments, linkage segments, read-only segments, constant data segments, and application-named data segments (see <a href="ch04.xhtml#ch4fig10">Figure 4-10</a>).</p>&#13;
		<div class="image" id="ch4fig10">&#13;
			<img alt="Image" src="../images/04fig10.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 4-10: Typical segments found in memory</em></p>&#13;
		<p class="indent">Because operating systems map segments to pages, a segment will always require some number of bytes that is a multiple of the page size. For example, if a program has a segment that contains only a single byte of data, that segment will still consume 4,096 bytes on an 80x86 processor. Similarly, if an 80x86 application consists of six different segments, that application will consume at least 24KB in memory, regardless of the number of machine instructions and data bytes the program uses and regardless of the executable file’s size.</p>&#13;
		<p class="indent">Many executable file formats (such as ELF and PE/COFF) provide an option in memory for a BSS section where a programmer can place uninitialized static variables. Because the values are uninitialized, there is no need to clutter the executable file with random data values for each of these variables. Therefore, the BSS section in some executable file formats is just a small stub that tells the OS loader the size of the BSS section. This way, you can add new uninitialized static variables to your application without affecting the executable file’s size. When you increase the amount of BSS data, the compiler simply adjusts a value to tell the loader how many bytes to reserve for the uninitialized variables. Were you to add those same variables to an initialized data section, the size of the executable file would grow with each byte of data that you added. Obviously, saving space on your mass storage device is a good thing to do, so using BSS sections to reduce your executable file sizes is a useful optimization.</p>&#13;
		<p class="indent">The one thing that many people tend to forget, however, is that a BSS section still requires main memory at runtime. Even though the executable file size may be smaller, each byte of data you declare in your program translates to 1 byte of data in memory. Some programmers have the mistaken impression that the executable’s file size is indicative of the amount of memory that the program consumes. This, however, isn’t necessarily <span epub:type="pagebreak" id="page_83"/>true, as our BSS example shows. A given application’s executable file might consist of only 600 bytes, but if that program uses four different sections, with each section consuming a 4KB page in memory, the program will require 16,384 bytes of memory when the OS loads it into memory. This is because the underlying memory protection hardware requires the OS to allocate whole pages of memory to a given process.</p>&#13;
		<h4 class="h4" id="ch00lev2sec52"><strong>4.7.2 Internal Fragmentation</strong></h4>&#13;
		<p class="noindent">Another reason an executable file might be smaller than an application’s <em>execution memory footprint</em> (the amount of memory the application consumes at runtime) is <em>internal fragmentation</em>. Internal fragmentation occurs when you must allocate sections of memory in fixed-sized chunks even though you might need only a portion of each chunk (see <a href="ch04.xhtml#ch4fig11">Figure 4-11</a>).</p>&#13;
		<div class="image" id="ch4fig11">&#13;
			<img alt="Image" src="../images/04fig11.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 4-11: Internal fragmentation</em></p>&#13;
		<p class="indent">Remember that each section in memory consumes an integral number of pages, even if that section’s data size is not a multiple of the page size. All bytes from the last data/code byte in a section to the end of the page holding that byte are wasted; this is internal fragmentation. Some executable file formats allow you to pack each section without padding it to some multiple of the page size. However, as you’ll soon see, there may be a performance penalty for packing sections together in this fashion, so some executable formats don’t do it.</p>&#13;
		<p class="indent">Finally, don’t forget that an executable file’s size does not include any data (including data objects on the heap and values placed on the CPU’s stack) allocated dynamically at runtime. As you can see, an application can actually consume much more memory than the executable file’s size.</p>&#13;
		<p class="indent">Programmers commonly compete to see who can write the smallest “Hello World” program using their favorite language. Assembly language programmers are especially guilty of bragging about how much smaller they can write this program in assembly than they can in C or some other HLL. This is a fun mental challenge, but whether the program’s executable file is 600 or 16,000 bytes long, the chances are pretty good that the program will consume exactly the same amount of memory at runtime once the operating system allocates four or five pages for the program’s different sections. While writing the world’s shortest “Hello World” application might win bragging rights, in real-world terms such an application saves almost nothing at runtime due to internal fragmentation.</p>&#13;
		<h4 class="h4" id="ch00lev2sec53"><span epub:type="pagebreak" id="page_84"/><strong>4.7.3 Reasons to Optimize for Space</strong></h4>&#13;
		<p class="noindent">This is not to suggest that optimizing for space isn’t worthwhile. Programmers who write great code consider all the machine resources their application uses, and they avoid wasting those resources. However, attempting to take this process to an extreme is a waste of effort. Once you’ve gotten a given section below 4,096 bytes (on an 80x86 or other CPU with a 4KB page size), additional optimizations save you nothing. Remember, the <em>allocation granularity</em>—that is, the minimum allocation block size—is 4,096 bytes. If you have a section with 4,097 bytes of data, it’s going to consume 8,192 bytes at runtime. In that case, it would behoove you to reduce that section by 1 byte (thereby saving 4,096 bytes at runtime). However, if you have a data section that consumes 16,380 bytes, attempting to reduce its size by 4,092 bytes in order to reduce the file size is going to be difficult unless the data organization was very bad to begin with.</p>&#13;
		<p class="indent">Note that most operating systems allocate disk space in clusters (or blocks) that are often comparable to (or even larger than) the page size for the memory management unit in the CPU. Therefore, if you shrink an executable’s file size down to 700 bytes in an attempt to save disk space (an admirable goal, even given the gargantuan size of modern disk drive subsystems), the savings won’t be as great as you’d expect. That 700-byte application, for example, is still going to consume a minimum of one block on the disk’s surface. All you achieve by reducing your application’s code or data size is to waste that much more space in the disk file—subject, of course, to section/block allocation granularity.</p>&#13;
		<p class="indent">For larger executable files, those larger than the disk block size, internal fragmentation has less impact with respect to wasted space. If an executable file packs the data and code sections without any wasted space between the sections, then internal fragmentation occurs only at the end of the file, in the very last disk block. Assuming that file sizes are random (even distribution), then internal fragmentation will waste approximately one-half of a disk block per file (that is, an average of 2KB per file when the disk block size is 4KB). For a very small file, one that is less than 4KB in size, this might represent a significant amount of the file’s space. For larger applications, however, the wasted space becomes insignificant. So it would seem that as long as an executable file packs all the sections of the program sequentially in the file, the file will be as small as possible. But is this really desirable?</p>&#13;
		<p class="indent">Assuming all things are equal, having smaller executable files is a good thing. However, all things aren’t always equal, so sometimes creating the smallest possible executable file isn’t really best. To understand why, recall the earlier discussion of the operating system’s virtual memory subsystem. When an OS loads an application into memory for execution, it doesn’t actually have to read the entire file. Instead, the operating system’s paging system can load only those pages needed to start the application. This usually consists of the first page of executable code, a page of memory to hold stack-based data, and, possibly, some data pages. In theory, an application could begin execution with as few as two or three pages of memory and bring in the remaining pages of code and data <em>on demand</em> (as the <span epub:type="pagebreak" id="page_85"/>application requests the data or code found in those pages). This is known as <em>demand-paged memory management</em>. In practice, most operating systems actually preload pages for efficiency reasons (maintaining a working set of pages in memory). However, operating systems generally don’t load the entire executable file into memory; instead, they load various blocks as the application requires them. As a result, the effort needed to load a page of memory from a file can dramatically affect a program’s performance. Is there some way, then, to organize the executable file to improve performance when the OS uses demand-paged memory management? Yes—if you make the file a little larger.</p>&#13;
		<p class="indent">The trick to improving performance is to organize the executable file’s blocks to match the memory page layout. This means that sections in memory should be aligned on page-sized boundaries in the executable file. It also means that disk blocks should be the size of, or a multiple of the size of, a disk sector or block. This being the case, the virtual memory management system can rapidly copy a single block on the disk into a single page of memory, update any necessary relocation values, and continue program execution. On the other hand, if a page of data is spread across two blocks on the disk and is not aligned on a disk block boundary, the OS has to read two blocks (rather than one) from disk into an internal buffer and then copy the page of data from that buffer to the destination page where it belongs. This extra work can be very time-consuming and hamper application performance.</p>&#13;
		<p class="indent">For this reason, some compilers will actually pad the executable file to ensure that each section in the executable file begins on a block boundary that the virtual memory management subsystem can map directly to a page in memory. Compilers that employ this technique often produce much larger executable file sizes than those that don’t. This is especially true if the executable file contains a large amount of BSS (uninitialized) data that a packed file format can represent very compactly.</p>&#13;
		<p class="indent">Because some compilers produce packed files at the expense of execution time, while others produce expanded files that load and run faster, it’s dangerous to compare the quality of compilers based on the size of the executable files they produce. The best way to determine the quality of a compiler’s output is by directly analyzing that output, not by using a weak metric such as output file size.</p>&#13;
		<div class="note">&#13;
			<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
			<p class="notep"><em>Analyzing compiler output is the subject of the very next chapter, so if you’re interested in the topic, keep reading.</em></p>&#13;
		</div>&#13;
		<h3 class="h3" id="ch00lev1sec38"><strong>4.8 Data and Code Alignment in an Object File</strong></h3>&#13;
		<p class="noindent">As I pointed out in <em>WGC1</em>, aligning data objects on an address boundary that is “natural” for that object’s size can improve performance. It’s also true that aligning the start of a procedure’s code or the starting instruction of a loop on some nice boundary can improve performance. Compiler writers are well aware of this fact and will often emit <em>padding bytes</em> in the data or <span epub:type="pagebreak" id="page_86"/>code stream to align data or code sequences on an appropriate boundary. However, note that the linker is free to move sections of code around when linking two object files to produce a single executable result.</p>&#13;
		<p class="indent">Sections are generally aligned to a page boundary in memory. For a typical application, the text/code section will begin on a page boundary, the data section will begin on a different page boundary, the BSS section (if it exists) will begin on its own page boundary, and so on. However, this doesn’t imply that each and every section associated with a section header in the object files starts on its own page in memory. The linker program will combine sections that have the same name into a single section in the executable file. So, for example, if two different object files both contain a <code>.text</code> segment, the linker will combine them into a single <code>.text</code> section in the final executable file. By combining sections that have the same name, the linker avoids wasting a large amount of memory to internal fragmentation.</p>&#13;
		<p class="indent">How does the linker respect the alignment requirements of each of the sections it combines? The answer, of course, depends on exactly what object file format and OS you’re using, but it’s usually found in the object file format itself. For example, in a Windows PE/COFF file the <code>IMAGE_OPTIONAL_HEADER32</code> structure contains a field named <code>SectionAlignment</code>. This field specifies the address boundary that the linker and OS must respect when combining sections and loading the section into memory. Under Windows, the <code>SectionAlignment</code> field in the PE/COFF optional header will usually contain 32 or 4,096 bytes. The 4KB value, of course, will align a section to a 4KB page boundary in memory. The alignment value of 32 was probably chosen because this is a reasonable cache line value (see <em>WGC1</em> for a discussion of cache lines). Other values are certainly possible—an application programmer can usually specify section alignment values by using linker (or compiler) command-line parameters.</p>&#13;
		<h4 class="h4" id="ch00lev2sec54"><strong>4.8.1 Choosing a Section Alignment Size</strong></h4>&#13;
		<p class="noindent">Within each section, a compiler, assembler, or other code-generation tool can guarantee any alignment that is a submultiple of the section’s alignment. For example, if the section’s alignment value is 32, then alignments of 1, 2, 4, 8, 16, and 32 are possible within that section. Larger alignment values are not possible. If a section’s alignment value is 32 bytes, you cannot guarantee alignment within that section on a 64-byte boundary, because the OS or linker will respect only the section’s alignment value and it can place that section on any boundary that is a multiple of 32 bytes. And about half of those won’t be 64-byte boundaries.</p>&#13;
		<p class="indent">Perhaps less obvious, but just as true, is the fact that you cannot align an object within a section on a boundary that is not a submultiple of the section’s alignment. For example, a section with a 32-byte alignment value will not allow an alignment of 5 bytes. True, you could guarantee that the offset of some object within the section would be a multiple of 5; however, if the starting memory address of the section is not a multiple of 5, then the address of the object you attempted to align might not fall on a multiple of 5 bytes. The only solution is to pick a section alignment value that is some multiple of 5.</p>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_87"/>Because memory addresses are binary values, most language translators and linkers limit alignment values to a power of 2 that is less than or equal to some maximum value, usually the memory management unit’s page size. Many languages restrict the alignment value to a small power of 2 (such as 32, 64, or 256).</p>&#13;
		<h4 class="h4" id="ch00lev2sec55"><strong>4.8.2 Combining Sections</strong></h4>&#13;
		<p class="noindent">When a linker combines two sections, it has to respect the alignment values associated with each section because the application may depend on that alignment for correct operation. Therefore, a linker or other program that combines sections in object files can’t simply concatenate the data for the two sections when building the combined section.</p>&#13;
		<p class="indent">When combining two sections, a linker might have to add padding bytes between the sections if one or both of the lengths is not a multiple of the sections’ alignment. For example, if two sections have an alignment value of 32, and one section is 37 bytes long and the other section is 50 bytes long, the linker will have to add 27 bytes of padding between the first and second sections, or it will have to add 14 bytes of padding between the second section and the first (the linker usually gets to choose in which order it places the sections in the combined file).</p>&#13;
		<p class="indent">The situation gets a bit more complicated if the alignment values are not the same for the two sections. When a linker combines two sections, it has to ensure that the alignment requests are met for the data in both sections. If the alignment value of one section is a multiple of the other section’s alignment value, then the linker can simply choose the larger of the two alignment values. For example, if the alignment values are always powers of 2 (as most linkers require), then the linker can simply choose the larger of the two alignment values for the combined section.</p>&#13;
		<p class="indent">If one section’s alignment value is not a multiple of the other’s, then the only way to guarantee the alignment requirements of both sections when combining them is to use an alignment value that is a product of the two values (or, better yet, the <em>least common multiple</em> of the two values). For example, combining a section aligned on a 32-byte boundary with one aligned on a 5-byte boundary requires an alignment value of 160 bytes (5 × 32). Because of the complexities of combining two such sections, most linkers require section sizes to be small powers of 2, which guarantees that the larger segment alignment value is always a multiple of the smaller alignment value.</p>&#13;
		<h4 class="h4" id="ch00lev2sec56"><strong>4.8.3 Controlling the Section Alignment</strong></h4>&#13;
		<p class="noindent">You typically use linker options to control the section alignment within your programs. For example, with the Microsoft <em>link.exe</em> program, the <code>/ALIGN:value</code> command-line parameter tells the linker to align all sections in the output file to the specified boundary (which must be a power of 2). GNU’s <em>ld</em> linker program lets you specify a section alignment by using the <code>BLOCK(value)</code> option in a linker script file. The macOS linker (<code>ld</code>) provides a <code>-segalign value</code> command-line option you can use to specify section alignment. The exact command and possible values are specific to the linker; <span epub:type="pagebreak" id="page_88"/>however, almost every modern linker allows you to specify the section alignment properties. See your linker’s documentation for details.</p>&#13;
		<p class="indent">One word of caution about setting the section alignment: more often than not, a linker will require that all sections in a given file be aligned on the same boundary (a power of 2). Therefore, if you have different alignment requirements for all your sections, then you’ll need to choose the largest alignment value for all the sections in your object file.</p>&#13;
		<h4 class="h4" id="ch00lev2sec57"><strong>4.8.4 Aligning Sections Within Library Modules</strong></h4>&#13;
		<p class="noindent">Section alignment can have a very big impact on the size of your executable files if you use a lot of short library routines. Suppose, for example, that you’ve specified an alignment size of 16 bytes for the sections associated with the object files appearing in a library. Each library function that the linker processes will be placed on a 16-byte boundary. If the functions are small (fewer than 16 bytes in length), the space between the functions will be unused when the linker creates the final executable. This is another form of internal fragmentation.</p>&#13;
		<p class="indent">To understand why you would want to align the code (or data) in a section on a given boundary, think about how cache lines work (see <em>WGC1</em> for a refresher). By aligning the start of a function on a cache line, you may be able to slightly increase the execution speed of that function, as it may generate fewer cache misses during execution. For this reason, many programmers like to align all their functions at the start of a cache line. Although the size of a cache line varies from CPU to CPU, a typical cache line is 16 to 64 bytes long, so many compilers, assemblers, and linkers will attempt to align code and data to one of these boundaries. On the 80x86 processor, there are some other benefits to 16-byte alignment, so many 80x86-based tools default to a 16-byte section alignment for object files.</p>&#13;
		<p class="indent">Consider, for example, the following short HLA (High-Level Assembly) program, processed by Microsoft tools, that calls two relatively small library routines:</p>&#13;
		<pre class="programs">&#13;
			program t;<br/>#include( "bits.hhf" )<br/><br/>begin t;<br/><br/>bits.cnt( 5 );<br/>bits.reverse32( 10 );<br/><br/>end t;<br/><br/>Here is the source code to the bits.cnt library module:<br/><br/>unit bitsUnit;<br/><br/>#includeonce( "bits.hhf" );<br/><br/><br/>    // bitCount-<br/><span epub:type="pagebreak" id="page_89"/>    //<br/>    //  Counts the number of "1" bits in a dword value.<br/>    //  This function returns the dword count value in EAX.<br/><br/>    procedure bits.cnt( BitsToCnt:dword ); @nodisplay;<br/><br/>    const<br/>        EveryOtherBit       := $5555_5555;<br/>        EveryAlternatePair  := $3333_3333;<br/>        EvenNibbles         := $0f0f_0f0f;<br/><br/>    begin cnt;<br/><br/>        push( edx );<br/>        mov( BitsToCnt, eax );<br/>        mov( eax, edx );<br/><br/>        // Compute sum of each pair of bits<br/>        // in EAX. The algorithm treats<br/>        // each pair of bits in EAX as a<br/>        // 2-bit number and calculates the<br/>        // number of bits as follows (description<br/>        // is for bits 0 and 1, but it generalizes<br/>        // to each pair):<br/>        //<br/>        //  EDX =   BIT1  BIT0<br/>        //  EAX =      0  BIT1<br/>        //<br/>        //  EDX-EAX =   00 if both bits were 0.<br/>        //              01 if Bit0 = 1 and Bit1 = 0.<br/>        //              01 if Bit0 = 0 and Bit1 = 1.<br/>        //              10 if Bit0 = 1 and Bit1 = 1.<br/>        //<br/>        // Note that the result is left in EDX.<br/><br/>        shr( 1, eax );<br/>        and( EveryOtherBit, eax );<br/>        sub( eax, edx );<br/><br/>        // Now sum up the groups of 2 bits to<br/>        // produces sums of 4 bits. This works<br/>        // as follows:<br/>        //<br/>        //  EDX = bits 2,3, 6,7, 10,11, 14,15, ..., 30,31<br/>        //        in bit positions 0,1, 4,5, ..., 28,29 with<br/>        //        0s in the other positions.<br/>        //<br/>        //  EAX = bits 0,1, 4,5, 8,9, ... 28,29 with 0s<br/>        //        in the other positions.<br/>        //<br/>        //  EDX + EAX produces the sums of these pairs of bits.<br/>        //  The sums consume bits 0,1,2, 4,5,6, 8,9,10, ...<br/>        //                                            28,29,30<br/>        //  in EAX with the remaining bits all containing 0.<br/><span epub:type="pagebreak" id="page_90"/>        mov( edx, eax );<br/>        shr( 2, edx );<br/>        and( EveryAlternatePair, eax );<br/>        and( EveryAlternatePair, edx );<br/>        add( edx, eax );<br/><br/>        // Now compute the sums of the even and odd nibbles in<br/>        // the number. Since bits 3, 7, 11, etc. in EAX all<br/>        // contain 0 from the above calculation, we don't need<br/>        // to AND anything first, just shift and add the two<br/>        // values.<br/>        // This computes the sum of the bits in the 4 bytes<br/>        // as four separate values in EAX (AL contains number of<br/>        // bits in original AL, AH contains number of bits in<br/>        // original AH, etc.)<br/><br/>        mov( eax, edx );<br/>        shr( 4, eax );<br/>        add( edx, eax );<br/>        and( EvenNibbles, eax );<br/><br/>        // Now for the tricky part.<br/>        // We want to compute the sum of the 4 bytes<br/>        // and return the result in EAX. The following<br/>        // multiplication achieves this. It works<br/>        // as follows:<br/>        //  (1) the $01 component leaves bits 24..31<br/>        //      in bits 24..31.<br/>        //<br/>        //  (2) the $100 component adds bits 17..23<br/>        //      into bits 24..31.<br/>        //<br/>        //  (3) the $1_0000 component adds bits 8..15<br/>        //      into bits 24..31.<br/>        //<br/>        //  (4) the $1000_0000 component adds bits 0..7<br/>        //      into bits 24..31.<br/>        //<br/>        //  Bits 0..23 are filled with garbage, but bits<br/>        //  24..31 contain the actual sum of the bits<br/>        //  in EAX's original value. The SHR instruction<br/>        //  moves this value into bits 0..7 and zeros<br/>        //  out the HO bits of EAX.<br/><br/>        intmul( $0101_0101, eax );<br/>        shr( 24, eax );<br/><br/>        pop( edx );<br/><br/>    end cnt;<br/><br/>end bitsUnit;</pre>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_91"/>Here is the source code for the <code>bits.reverse32()</code> library function. Note that this source file also includes the <code>bits.reverse16()</code> and <code>bits.reverse8()</code> functions (to conserve space, the bodies of these functions do not appear below). Although their operation is not pertinent to our discussion, note that these functions swap the values in the HO (high-order) and LO (low-order) bit positions. Because these three functions appear in a single source file, any program that includes one of these functions will automatically include all three (because of the way compilers, assemblers, and linkers work).</p>&#13;
		<pre class="programs">&#13;
			unit bitsUnit;<br/><br/>#include( "bits.hhf" );<br/><br/><br/>    procedure bits.reverse32( BitsToReverse:dword ); @nodisplay; @noframe;<br/>    begin reverse32;<br/><br/>        push( ebx );<br/>        mov( [esp+8], eax );<br/><br/>        // Swap the bytes in the numbers:<br/><br/>        bswap( eax );<br/><br/>        // Swap the nibbles in the numbers<br/><br/>        mov( $f0f0_f0f0, ebx );<br/>        and( eax, ebx );<br/>        and( $0f0f_0f0f, eax );<br/>        shr( 4, ebx );<br/>        shl( 4, eax );<br/>        or( ebx, eax );<br/><br/>        // Swap each pair of 2 bits in the numbers:<br/><br/>        mov( eax, ebx );<br/>        shr( 2, eax );<br/>        shl( 2, ebx );<br/>        and( $3333_3333, eax );<br/>        and( $cccc_cccc, ebx );<br/>        or( ebx, eax );<br/><br/>        // Swap every other bit in the number:<br/><br/>        lea( ebx, [eax + eax] );<br/>        shr( 1, eax );<br/>        and( $5555_5555, eax );<br/>        and( $aaaa_aaaa, ebx );<br/>        or( ebx, eax );<br/>        pop( ebx );<br/>        ret( 4 );<br/><span epub:type="pagebreak" id="page_92"/>    end reverse32;<br/><br/><br/>    procedure bits.reverse16( BitsToReverse:word );<br/>        @nodisplay; @noframe;<br/>    begin reverse16;<br/><br/>        // Uninteresting code that is very similar to<br/>        // that appearing in reverse32 has been snipped...<br/><br/>    end reverse16;<br/><br/><br/><br/>    procedure bits.reverse8( BitsToReverse:byte );<br/>        @nodisplay; @noframe;<br/>    begin reverse8;<br/><br/>        // Uninteresting code snipped...<br/><br/>    end reverse8;<br/><br/><br/>end bitsUnit;</pre>&#13;
		<p class="indent">The Microsoft <em>dumpbin.exe</em> tool allows you to examine the various fields of an <em>.obj</em> or <em>.exe</em> file. Running <code>dumpbin</code> with the <code>/headers</code> command-line option on the <em>bitcnt.obj</em> and <em>reverse.obj</em> files (produced for the HLA standard library) tells us that each of the sections is aligned to a 16-byte boundary. Therefore, when the linker combines the <em>bitcnt.obj</em> and <em>reverse.obj</em> data with the sample program given earlier, it will align the <code>bits.cnt()</code> function in the <em>bitcnt.obj</em> file on a 16-byte boundary, and the three functions in the <em>reverse.obj</em> file on a 16-byte boundary. (Note that it will not align each function in the file on a 16-byte boundary. That task is the responsibility of the tool that created the object file, if such alignment is desired.) By using the <em>dumpbin.exe</em> program with the <code>/disasm</code> command-line option on the executable file, you can see that the linker has honored these alignment requests (note that an address that is aligned on a 16-byte boundary will have a <code>0</code> in the LO hexadecimal digit):</p>&#13;
		<pre class="programs">&#13;
			  Address   opcodes            Assembly Instructions<br/>  --------- ------------------ -----------------------------<br/>  04001000: E9 EB 00 00 00     jmp         040010F0<br/>  04001005: E9 57 01 00 00     jmp         04001161<br/>  0400100A: E8 F1 00 00 00     call        04001100<br/><br/>; Here's where the main program starts.<br/><br/>  0400100F: 6A 00              push        0<br/>  04001011: 8B EC              mov         ebp,esp<br/>  04001013: 55                 push        ebp<br/>  04001014: 6A 05              push        5<br/>  04001016: E8 65 01 00 00     call        04001180<br/>  0400101B: 6A 0A              push        0Ah<br/><span epub:type="pagebreak" id="page_93"/>  0400101D: E8 0E 00 00 00     call        04001030<br/>  04001022: 6A 00              push        0<br/>  04001024: FF 15 00 20 00 04  call        dword ptr ds:[04002000h]<br/><br/>;The following INT3 instructions are used as padding in order<br/>;to align the bits.reverse32 function (which immediately follows)<br/>;to a 16-byte boundary:<br/><br/>  0400102A: CC                 int         3<br/>  0400102B: CC                 int         3<br/>  0400102C: CC                 int         3<br/>  0400102D: CC                 int         3<br/>  0400102E: CC                 int         3<br/>  0400102F: CC                 int         3<br/><br/>; Here's where bits.reverse32 starts. Note that this address<br/>; is rounded up to a 16-byte boundary.<br/><br/>  04001030: 53                 push        ebx<br/>  04001031: 8B 44 24 08        mov         eax,dword ptr [esp+8]<br/>  04001035: 0F C8              bswap       eax<br/>  04001037: BB F0 F0 F0 F0     mov         ebx,0F0F0F0F0h<br/>  0400103C: 23 D8              and         ebx,eax<br/>  0400103E: 25 0F 0F 0F 0F     and         eax,0F0F0F0Fh<br/>  04001043: C1 EB 04           shr         ebx,4<br/>  04001046: C1 E0 04           shl         eax,4<br/>  04001049: 0B C3              or          eax,ebx<br/>  0400104B: 8B D8              mov         ebx,eax<br/>  0400104D: C1 E8 02           shr         eax,2<br/>  04001050: C1 E3 02           shl         ebx,2<br/>  04001053: 25 33 33 33 33     and         eax,33333333h<br/>  04001058: 81 E3 CC CC CC CC  and         ebx,0CCCCCCCCh<br/>  0400105E: 0B C3              or          eax,ebx<br/>  04001060: 8D 1C 00           lea         ebx,[eax+eax]<br/>  04001063: D1 E8              shr         eax,1<br/>  04001065: 25 55 55 55 55     and         eax,55555555h<br/>  0400106A: 81 E3 AA AA AA AA  and         ebx,0AAAAAAAAh<br/>  04001070: 0B C3              or          eax,ebx<br/>  04001072: 5B                 pop         ebx<br/>  04001073: C2 04 00           ret         4<br/><br/>; Here's where bits.reverse16 begins. As this function appeared<br/>; in the same file as bits.reverse32, and no alignment option<br/>; was specified in the source file, HLA and the linker won't<br/>; bother aligning this to any particular boundary. Instead, the<br/>; code immediately follows the bits.reverse32 function<br/>; in memory.<br/><br/>  04001076: 53                 push        ebx<br/>  04001077: 50                 push        eax<br/>  04001078: 8B 44 24 0C        mov         eax,dword ptr [esp+0Ch]<br/><br/>        .<br/>        .    ; uninteresting code for bits.reverse16 and<br/><span epub:type="pagebreak" id="page_94"/>        .    ; bits.reverse8 was snipped<br/>; end of bits.reverse8 code<br/><br/>  040010E6: 88 04 24           mov         byte ptr [esp],al<br/>  040010E9: 58                 pop         eax<br/>  040010EA: C2 04 00           ret         4<br/><br/>; More padding bytes to align the following function (used by<br/>; HLA exception handling) to a 16-byte boundary:<br/><br/>  040010ED: CC                 int         3<br/>  040010EE: CC                 int         3<br/>  040010EF: CC                 int         3<br/><br/>; Default exception return function (automatically generated<br/>; by HLA):<br/><br/>  040010F0: B8 01 00 00 00     mov         eax,1<br/>  040010F5: C3                 ret<br/><br/>; More padding bytes to align the internal HLA BuildExcepts<br/>; function to a 16-byte boundary:<br/><br/>  040010F6: CC                 int         3<br/>  040010F7: CC                 int         3<br/>  040010F8: CC                 int         3<br/>  040010F9: CC                 int         3<br/>  040010FA: CC                 int         3<br/>  040010FB: CC                 int         3<br/>  040010FC: CC                 int         3<br/>  040010FD: CC                 int         3<br/>  040010FE: CC                 int         3<br/>  040010FF: CC                 int         3<br/><br/>; HLA BuildExcepts code (automatically generated by the<br/>; compiler):<br/><br/>  04001100: 58                 pop         eax<br/>  04001101: 68 05 10 00 04     push        4001005h<br/>  04001106: 55                 push        ebp<br/><br/>        .<br/>        .    ; Remainder of BuildExcepts code goes here<br/>        .    ; along with some other code and data<br/>        .<br/><br/>; Padding bytes to ensure that bits.cnt is aligned<br/>; on a 16-byte boundary:<br/><br/>  0400117D: CC                 int         3<br/>  0400117E: CC                 int         3<br/><span epub:type="pagebreak" id="page_95"/>  0400117F: CC                 int         3<br/><br/>; Here's the low-level machine code for the bits.cnt function:<br/><br/>  04001180: 55                 push        ebp<br/>  04001181: 8B EC              mov         ebp,esp<br/>  04001183: 83 E4 FC           and         esp,0FFFFFFFCh<br/>  04001186: 52                 push        edx<br/>  04001187: 8B 45 08           mov         eax,dword ptr [ebp+8]<br/>  0400118A: 8B D0              mov         edx,eax<br/>  0400118C: D1 E8              shr         eax,1<br/>  0400118E: 25 55 55 55 55     and         eax,55555555h<br/>  04001193: 2B D0              sub         edx,eax<br/>  04001195: 8B C2              mov         eax,edx<br/>  04001197: C1 EA 02           shr         edx,2<br/>  0400119A: 25 33 33 33 33     and         eax,33333333h<br/>  0400119F: 81 E2 33 33 33 33  and         edx,33333333h<br/>  040011A5: 03 C2              add         eax,edx<br/>  040011A7: 8B D0              mov         edx,eax<br/>  040011A9: C1 E8 04           shr         eax,4<br/>  040011AC: 03 C2              add         eax,edx<br/>  040011AE: 25 0F 0F 0F 0F     and         eax,0F0F0F0Fh<br/>  040011B3: 69 C0 01 01 01 01  imul        eax,eax,1010101h<br/>  040011B9: C1 E8 18           shr         eax,18h<br/>  040011BC: 5A                 pop         edx<br/>  040011BD: 8B E5              mov         esp,ebp<br/>  040011BF: 5D                 pop         ebp<br/>  040011C0: C2 04 00           ret         4</pre>&#13;
		<p class="indent">The exact operation of this program really isn’t important (after all, it doesn’t actually do anything useful). The takeaway is how the linker inserts extra bytes (<code>$cc</code>, the <code>int 3</code> instruction) before a group of one or more functions appearing in a source file to ensure that they are aligned on the specified boundary.</p>&#13;
		<p class="indent">In this particular example, the <code>bits.cnt()</code> function is actually 64 bytes long, and the linker inserted only 3 bytes in order to align it to a 16-byte boundary. This percentage of waste—the number of padding bytes compared to the size of the function—is quite low. However, if you have a large number of small functions, the wasted space can become significant (as with the default exception handler in this example that has only two instructions). When creating your own library modules, you’ll need to weigh the inefficiencies of extra space for padding against the small performance gains you’ll obtain by using aligned code.</p>&#13;
		<p class="indent">Object code dump utilities (like <em>dumpbin.exe</em>) are quite useful for analyzing object code and executable files in order to determine attributes such as section size and alignment. Linux (and most Unix-like systems) provides the comparable <code>objdump</code> utility. I’ll discuss these tools in the next chapter, as they are great for analyzing compiler output.</p>&#13;
		<h3 class="h3" id="ch00lev1sec39"><span epub:type="pagebreak" id="page_96"/><strong>4.9 How Linkers Affect Code</strong></h3>&#13;
		<p class="noindent">The limitations of object file formats such as COFF and ELF have a big impact on the quality of code that compilers can generate. Because of how object file formats are designed, linkers and compilers often have to inject extra code into an executable file that wouldn’t be necessary otherwise. In this section we’ll explore some of the problems that generic object code formats like COFF and ELF inflict on the executable code.</p>&#13;
		<p class="indent">One problem with generic object file formats like COFF and ELF is that they were not designed to produce efficient executable files for specific CPUs. Instead, they were created to support a wide variety of CPUs and to make it easy to link together object modules. Unfortunately, their versatility often prevents them from creating the best possible object files.</p>&#13;
		<p class="indent">Perhaps the biggest problem with the COFF and ELF formats is that relocation values in the object file must apply to 32- and 64-bit pointers in the object code. This creates problems, for example, when an instruction encodes a displacement or address value with less than 32 (64) bits. On some processors, such as the 80x86, displacements smaller than 32 bits are so small (for example, the 80x86’s 8-bit displacement) that you would never use them to refer to code outside the current object module. However, on some RISC processors, such as the PowerPC or ARM, displacements are much larger (26 bits in the case of the PowerPC branch instruction). This can lead to code kludges like the function stub generation that GCC produces for external function calls. Consider the following C program and the PowerPC code that GCC emits for it:</p>&#13;
		<pre class="programs">&#13;
			#include &lt;stdio.h&gt;<br/>int main( int argc )<br/>{<br/>      .<br/>      .<br/>      .<br/>    printf<br/>    (<br/>        "%d %d %d %d %d ",<br/>        .<br/>        .<br/>        .<br/>    );<br/>    return( 0 );<br/>}<br/><br/>; PowerPC assembly output from GCC:<br/><br/>            .<br/>            .<br/>            .<br/>        ;The following sets up the<br/>        ; call to printf and calls printf:<br/><br/>        addis r3,r31,ha16(LC0-L1$pb)<br/>        la r3,lo16(LC0-L1$pb)(r3)<br/><span epub:type="pagebreak" id="page_97"/>        lwz r4,64(r30)<br/>        lwz r5,80(r30)<br/>        lwz r6,1104(r30)<br/>        lwz r7,1120(r30)<br/>        lis r0,0x400<br/>        ori r0,r0,1120<br/>        lwzx r8,r30,r0<br/>        bl L_printf$stub ; Call to printf "stub" routine.<br/><br/>        ;Return from main program:<br/><br/>        li r0,0<br/>        mr r3,r0<br/>        lwz r1,0(r1)<br/>        lwz r0,8(r1)<br/>        mtlr r0<br/>        lmw r30,-8(r1)<br/>        blr<br/><br/>; Stub, to call the external printf function.<br/>; This code does an indirect jump to the printf<br/>; function using the 32-bit L_printf$lazy_ptr<br/>; pointer that the linker can modify.<br/><br/>        .data<br/>        .picsymbol_stub<br/>L_printf$stub:<br/>        .indirect_symbol _printf<br/>        mflr r0<br/>        bcl 20,31,L0$_printf<br/>L0$_printf:<br/>        mflr r11<br/>        addis r11,r11,ha16(L_printf$lazy_ptr-L0$_printf)<br/>        mtlr r0<br/>        lwz r12,lo16(L_printf$lazy_ptr-L0$_printf)(r11)<br/>        mtctr r12<br/>        addi r11,r11,lo16(L_printf$lazy_ptr-L0$_printf)<br/>        bctr<br/>.data<br/>.lazy_symbol_pointer<br/>L_printf$lazy_ptr:<br/>        .indirect_symbol _printf<br/><br/>; The following is where the compiler places a 32-bit<br/>; pointer that the linker can fill in with the address<br/>; of the actual printf function:<br/><br/>        .long dyld_stub_binding_helper</pre>&#13;
		<p class="indent">The compiler must generate the <code>L_printf$stub</code> stub because it doesn’t know how far away the actual <code>printf()</code> routine will be when the linker adds it to the final executable file. It’s unlikely that <code>printf()</code> would be sitting outside the ±32MB range that the PowerPC’s 24-bit branch displacement supports (extended to 26 bits), but it’s not guaranteed. If <code>printf()</code> is part of <span epub:type="pagebreak" id="page_98"/>a shared library that is dynamically linked in at runtime, it very well could be outside this range. Therefore, the compiler has to make the safe choice and use a 32-bit displacement for the address of the <code>printf()</code> function. Unfortunately, PowerPC instructions don’t support a 32-bit displacement, because all PowerPC instructions are 32 bits long. A 32-bit displacement would leave no room for the instruction’s opcode. Therefore, the compiler has to store a 32-bit pointer to the <code>printf()</code> routine in a variable and jump indirect through that variable. Accessing a 32-bit memory pointer on the PowerPC takes quite a bit of code if you don’t already have the pointer’s address in a register, hence all the extra code following the <code>L_printf$stub</code> label.</p>&#13;
		<p class="indent">If the linker were able to adjust 26-bit displacements rather than just 32-bit values, there would be no need for the <code>L_printf$stub</code> routine or the <code>L_printf$lazy_ptr</code> pointer variable. Instead, the <code>bl L_printf$stub</code> instruction would be able to branch directly to the <code>printf()</code> routine (assuming it’s not more than ±32MB away). Because single program files generally don’t contain more than 32MB of machine instructions, there would rarely be the need to go through the gymnastics this code does in order to call an external routine.</p>&#13;
		<p class="indent">Unfortunately, there is nothing you can do about the object file format; you’re stuck with whatever format the OS specifies (which is usually a variant of COFF or ELF on modern 32-bit and 64-bit machines). However, you can work within those limitations.</p>&#13;
		<p class="indent">If you expect your code to run on a CPU like the PowerPC or ARM (or some other RISC processor) that cannot encode 32-bit displacements directly within instructions, you can optimize by avoiding cross-module calls as much as possible. While it’s not good programming practice to create monolithic applications, where all the source code appears in one source file (or is processed by a single compilation), there’s really no need to place all of your own functions in separate source modules and compile each one separately from the others—particularly if these routines make calls to one another. By placing a set of common routines your code uses into a single compilation unit (source file), you allow the compiler to optimize the calls among these functions and avoid all the stub generation on processors like the PowerPC. This is not a suggestion to simply move all of your external functions into a single source file. The code is better only if the functions in a module call one another or share other global objects. If the functions are completely independent of one another and are called only by code external to the compilation unit, then you’ve saved nothing because the compiler may still need to generate stub routines in the external code.</p>&#13;
		<h3 class="h3" id="ch00lev1sec40"><strong>4.10 For More Information</strong></h3>&#13;
		<p class="bib">Aho, Alfred V., Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman. <em>Compilers: Principles, Techniques, and Tools</em>. 2nd ed. Essex, UK: Pearson Education Limited, 1986.</p>&#13;
		<p class="bib">Gircys, Gintaras. <em>Understanding and Using COFF</em>. Sebastopol, CA: O’Reilly Media, 1988.</p>&#13;
		<p class="bib">Levine, John R. <em>Linkers and Loaders</em>. San Diego: Academic Press, 2000.</p>&#13;
	</body></html>