- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Preparation
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Machine learning algorithms can only work as well as the data they’re trained
    on. In the real world, our data can come from noisy sensors, computer programs
    with bugs, or even incomplete or inaccurate transcriptions of paper records. We
    always need to look at our data and fix any problems before we use it.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: A rich body of methods has been developed for just this job. They’re referred
    to as techniques for *data preparation*, or *data cleaning*. The idea is to process
    our data beforelearning from it so that our learning systems can use the data
    most efficiently.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: We also want to make sure that the data itself is well suited to machine learning,
    which may mean adjusting it, for example, by scaling numbers, or combining categories.
    This work is essential because the particular way the data is structured, and
    the numerical ranges it spans, can have a strong effect on the information an
    algorithm can extract from it.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Our goal in this chapter is to see how we can adjust the data we’re given, without
    changing its meaning, to get the most efficient and effective learning process.
    We begin with techniques to confirm that our data is clean and ready for training.
    We then consider methods for examining the data itself, and for making sure that
    we’ve got it in the best form for machine learning. This can involve doing simple
    things like replacing strings with numbers or taking more interesting actions
    like scaling the data. Finally, we look at ways to reduce the size of our training
    data. This lets our algorithms run and learn more quickly.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Basic Data Cleaning
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start by considering some simple ways to ensure that our data is well
    cleaned. The idea is to make sure that we’re starting out with data that has no
    blanks, incorrect entries, or other errors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: If our data is in textual form, then we want to make sure there are no typographical
    errors, misspellings, embedded unprintable characters, or other such corruptions.
    For example, if we have a collection of animal photos along with a text file describing
    them and our system is case-sensitive, then we want to make sure that every giraffe
    is labeled as giraffe and not girafe or Giraffe, and we want to avoid other typos
    or variants like beautiful giraffe or giraffe-extra tall. Every reference to a
    giraffe needs to use the identical string.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: We should also look for other common-sense things. We want to remove any accidental
    duplicates in our training data, because they will skew our idea of what data
    we’re working with. If we accidentally include a single piece of data multiple
    times, our learner will interpret it as multiple, different samples that just
    happen to have the same value, and thus that sample may have more influence than
    it should.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: We also want to make sure we don’t have any typographical errors, like missing
    a decimal point so we specify a value of 1,000 rather than 1.000, or putting two
    minus signs in front of a number rather than just one. It’s not uncommon to find
    some hand-composed databases with blanks or question marks in them, signifying
    that people didn’t have any data to enter. Some computer-generated databases can
    include a code like `NaN` (not a number), which is a placeholder indicating that
    the computer wanted to print a number but didn’t have a valid number to show.
    More troubling, sometimes when people are missing data for a numerical field,
    they enter something like 0 or –1\. We have to find and fix all such issues before
    we start learning from the data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要确保没有任何拼写错误，比如漏掉小数点导致我们指定了1,000而不是1.000，或者在数字前放了两个减号而不是一个。在一些手工编写的数据库中，发现空白或问号的情况并不罕见，表示人们没有数据可输入。一些计算机生成的数据库可能会包含像`NaN`（不是一个数字）这样的代码，这是一个占位符，表示计算机本来想输出一个数字，但没有有效的数字可显示。更麻烦的是，有时候当人们在填写数值字段时缺少数据时，他们会输入类似0或–1的值。我们必须在开始从数据中学习之前找到并修复所有这些问题。
- en: We also need to make sure that the data is in a format that will be properly
    interpreted by the software we’re giving it to. For example, we can use a format
    known as *scientific notation* to write very large and very small numbers. The
    problem is that such notation has no official format. Different programs use slightly
    different forms for this type of output, and other programs that read that data
    (like the library functions we often use in deep learning) can misinterpret forms
    they’re not expecting. For example, in scientific notation, the value 0.007 is
    commonly printed out as `7e-3` or `7E-3`. When we provide the sequence `7e-3`
    as an input, a program may interpret it as (7 × *e*) – 3, where *e* is Euler’s
    constant, which has a value of about 2.7\. The result is that the computer thinks
    that `7e-3` means that we’re asking it to first multiply the values of 7 and *e*
    together, and then subtract 3, giving us about 16 rather than 0.007\. We need
    to catch these sorts of things so that our programs properly interpret their inputs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要确保数据的格式能够被我们提供的计算软件正确解读。例如，我们可以使用一种称为*科学计数法*的格式来表示非常大或非常小的数字。问题在于，这种表示法没有一个正式的格式。不同的程序使用略有不同的方式来输出这种格式，而其他读取这些数据的程序（比如我们在深度学习中经常使用的库函数）可能会误解它们无法预料的格式。例如，在科学计数法中，值0.007通常会被表示为`7e-3`或`7E-3`。当我们提供`7e-3`作为输入时，程序可能会将其解释为（7
    × *e*）– 3，其中*e*是欧拉常数，值约为2.7。结果是计算机认为`7e-3`意味着我们要求它先将7和*e*的值相乘，然后减去3，得出大约16，而不是0.007。我们需要捕捉到这些问题，以确保我们的程序能正确解读它们的输入。
- en: We also want to look for missing data. If a sample is missing data for one or
    more features, we might be able to patch the holes manually or algorithmically,
    but it might be better to simply remove the sample altogether. This is a subjective
    call that we usually make on a case-by-case basis.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要查找缺失的数据。如果一个样本缺失了一个或多个特征的数据，我们可能能够手动或通过算法填补这些空缺，但有时候直接删除这个样本可能更好。这是一个主观的决策，通常需要根据具体情况逐个判断。
- en: Lastly, we want to identify any pieces of data that seem dramatically different
    from all the others. Some of these *outliers* might be mere typos, like a forgotten
    decimal point. Others might be the result of human error, like a misdirected copy
    and paste, or when someone forgot to delete an entry from a spreadsheet. When
    we don’t know if an outlier is a real piece of data or an error of some kind,
    we have to use our judgment to decide whether to leave it in or remove it manually.
    This is a subjective decision that depends entirely on what our data represents,
    how well we understand it, and what we want to do with it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要识别出任何与其他数据明显不同的部分。这些*异常值*可能只是一些拼写错误，比如漏掉了小数点。也有可能是人为错误，比如错误的复制粘贴，或者某人忘记从电子表格中删除一条记录。当我们不确定某个异常值是有效数据还是某种错误时，我们必须运用我们的判断力来决定是否保留它，或者手动将其删除。这是一个主观的决策，完全依赖于我们的数据代表什么，我们对其的理解程度，以及我们想如何处理它。
- en: Though these steps may seem straightforward, in practice, carrying them out
    can be a major effort depending on the size and complexity of our data and how
    messed up it is when we first get it. Many tools are available to help us clean
    data. Some are stand-alone, and others are built into machine-learning libraries.
    Commercial services will also clean data for a fee.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些步骤看起来可能很简单，但在实践中，根据我们的数据的大小和复杂性以及我们首次获取时的混乱程度，执行起来可能是一项重大工作。有许多工具可帮助我们清理数据。有些是独立的，而其他的内置在机器学习库中。商业服务也会收费清理数据。
- en: 'It’s useful to keep in mind this classic computing motto: *garbage in, garbage
    out*. In other words, our results are only as good as our starting data, so it’s
    vital that we start with the best data available, which means working hard to
    make it as clean as we possibly can.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这句经典的计算格言非常有用：*垃圾进，垃圾出*。换句话说，我们的结果只能和我们的起始数据一样好，所以从最好的可用数据开始非常重要，这意味着我们必须努力使其尽可能清洁。
- en: Now that we’ve taken care of the essential small stuff, let’s turn our attention
    to making the data well suited for learning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经处理了基本的细枝末节，让我们把注意力转向使数据适合学习。
- en: The Importance of Consistency
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一致性的重要性
- en: 'Preparing numbers for learning means applying *transformations* to them, without
    changing the relationships among them that we care about. We cover several such
    transformations later in this chapter, where we might scale all the numbers to
    a given range or eliminate some superfluous data so that the learner has less
    work to do. When we do these things, we must always obey a vital principle: any
    time we modify our training data in some way, *we must also modify all future
    data the same way*.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 准备学习的数字意味着对它们应用*变换*，而不改变我们关心的它们之间的关系。我们在本章后面涵盖了几种这样的变换，例如，我们可能将所有数字缩放到给定范围内或者消除一些多余的数据，这样学习者的工作就少了。当我们做这些事情时，我们必须始终遵守一个重要原则：任何时候我们以某种方式修改我们的训练数据，*我们也必须以同样的方式修改所有未来的数据*。
- en: Let’s look at why this is so important. When we make any changes to our training
    data, we typically modify or combine the values in ways that are designed to improve
    the computer’s learning efficiency or accuracy. [Figure 10-1](#figure10-1) shows
    the idea visually.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看为什么这么重要。当我们对训练数据进行任何更改时，我们通常修改或组合值的方式，旨在提高计算机的学习效率或准确性。[图 10-1](#figure10-1)
    用视觉方式展示了这个想法。
- en: '![F10001](Images/F10001.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![F10001](Images/F10001.png)'
- en: 'Figure 10-1: The flow of preprocessing for training and evaluating'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-1：用于训练和评估预处理流程的流程图
- en: As the figure shows, we typically determine any needed transformations by looking
    at the entirety of the training set. We transform that data to train our learner,
    and we also use the same transformation for all new data that comes after we’ve
    released our system to the world. The key point is that we must apply the *identical*
    modificationsto all new data before we give it to our algorithm for as long as
    our system is in use. This step must not be skipped.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图所示，我们通常通过查看整个训练集来确定所需的所有变换。我们将数据转换为训练我们的学习者，并且在我们释放系统到世界之后，我们也使用相同的转换来处理所有新数据。关键点在于，我们必须为我们的算法提供的所有新数据应用*相同的*修改，只要我们的系统在使用中。这一步绝不能被忽略。
- en: The fact that we need to reuse the same transformation on all data we evaluate
    pops up again and again in machine learning, often in subtle ways. Let’s first
    look at the problem in a general way with a visual example.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在机器学习中再次强调，每当我们评估数据时，我们都需要重新使用相同的转换方式。让我们首先用一个视觉示例来以一种一般的方式看待这个问题。
- en: Suppose we want to teach a classifier how to distinguish pictures of cows from
    pictures of zebras. We can collect a huge number of photos of both animals to
    use as training data. What most obviously distinguishes pictures of these two
    animals are their different black-and-white markings. To make sure our learner
    pays attention to these elements, we may decide to crop each photo to isolate
    the animal’s hide, and then we train with those isolated patches of texture. These
    cropped photos are all that the learner sees. [Figure 10-2](#figure10-2) shows
    a couple of samples.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想教一个分类器如何区分牛的图片和斑马的图片。我们可以收集大量两种动物的照片作为训练数据。最明显区分这两种动物图片的是它们不同的黑白斑纹。为了确保我们的学习者关注这些元素，我们可能决定裁剪每张照片以隔离动物的皮毛，然后用这些孤立的纹理块来训练。学习者只看到这些裁剪过的照片。[图
    10-2](#figure10-2) 展示了几个样本。
- en: '![F10002](Images/F10002.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![F10002](Images/F10002.png)'
- en: 'Figure 10-2: Left: A patch of texture from a cow. Right: A patch of texture
    from a zebra.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-2：左：一块牛的纹理。右：一块斑马的纹理。
- en: Suppose that we’ve trained our system and deployed it, but we forget to tell
    people about this preprocessing step of cropping each image to just the texture.
    Without knowing this vital information, a typical user might give our system complete
    pictures of cows and zebras, like those in [Figure 10-3](#figure10-3), and ask
    the system to identify the animal in each one.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经训练并部署了我们的系统，但我们忘记告知用户这一图像预处理步骤——将每张图片裁剪到仅保留纹理的部分。如果没有了解这个关键信息，典型的用户可能会提供完整的牛和斑马照片，像[图10-3](#figure10-3)中那样，并要求系统识别每张图片中的动物。
- en: '![F10003](Images/F10003.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![F10003](Images/F10003.png)'
- en: 'Figure 10-3: Left: A photo of a cow. Right: A photo of a zebra. If we trained
    our system on the isolated patterns of [Figure 10-2](#figure10-2), it could be
    misled by all the extra details in the photos.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-3：左：一张牛的照片。右：一张斑马的照片。如果我们在[图10-2](#figure10-2)中训练我们的系统，系统可能会被照片中的额外细节误导。
- en: Humans can pick out the hide patterns from these photos. A computer, on the
    other hand, can be misled by the legs, the heads, the ground, and other details,
    thus reducing its ability to give us good results. The difference between the
    prepared data of [Figure 10-2](#figure10-2) and the unprepared data of [Figure
    10-3](#figure10-3) can result in a system that performs beautifully on our training
    data but gives lousy results in the real world. To avoid this, all new data, like
    that in [Figure 10-3](#figure10-3), must be cropped to produce inputs that are
    just like the training data in [Figure 10-2](#figure10-2).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 人类可以从这些照片中识别出皮肤纹理模式。另一方面，计算机可能会被腿部、头部、地面和其他细节误导，从而降低其提供良好结果的能力。[图10-2](#figure10-2)中的准备数据和[图10-3](#figure10-3)中的未准备数据之间的差异可能会导致系统在训练数据上表现出色，但在实际应用中给出糟糕的结果。为了避免这种情况，所有新数据（如[图10-3](#figure10-3)中的数据）必须裁剪成与[图10-2](#figure10-2)中的训练数据完全相同的输入。
- en: 'Forgetting to transform new data in the same way as we transformed the training
    data is an easy mistake to make but usually causes our algorithms to underperform,
    sometimes to the point of becoming useless. The rule to remember is this: we determine
    how to modify our training data, then we modify it, and then we *remember how
    we modified it*. Any time we deal with more data, we must first *modify that data
    in the identical way* that the training data was modified. We’ll come back to
    this idea later and see how it’s used in practice.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 忘记以与我们转换训练数据相同的方式转换新数据是一个容易犯的错误，但通常会导致我们的算法表现不佳，有时甚至变得毫无用处。记住这个规则：我们首先确定如何修改训练数据，然后修改它，接着我们*记住我们是如何修改它的*。每当我们处理更多数据时，必须首先*以与训练数据相同的方式修改这些数据*。我们稍后会回到这个想法，看看它是如何在实践中使用的。
- en: Types of Data
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据类型
- en: 'Typical databases contain different types of data: floating-point numbers,
    strings, integers that refer to categories, and so on. We’ll treat each of these
    data typesin its own way, so it’s useful to distinguish them and give each unique
    type its own name. The most common naming system is based on whether a kind of
    data can be sorted. Though we rarely use explicit sorting when we do deep learning,
    this naming system is still convenient and widely used.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的数据库包含不同类型的数据：浮动小数、字符串、整数（用于类别标识）等。我们会根据不同的数据类型采用不同的处理方式，因此区分这些数据并为每种类型命名是很有用的。最常见的命名系统是基于数据是否可以排序的标准。尽管我们在深度学习中很少显式进行排序，但这个命名系统仍然很方便并被广泛使用。
- en: 'Recall that each sample is a list of values, each of which is called a *feature*.
    Each feature in a sample can be either of two general varieties: *numerical* or
    *categorical*.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，每个样本是一组值的列表，每个值被称为*特征*。每个样本中的特征可以是两种基本类型之一：*数值型*或*类别型*。
- en: Numerical data is simply a number, either floating-point or integer. We also
    call this *quantitative* data. Numerical, or quantitative, data can be sorted
    just by using its values.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 数值数据就是一个数字，可以是浮动小数或整数。我们也称之为*定量*数据。数值数据或定量数据可以通过其值进行排序。
- en: Categorical data is just about anything else, though often it’s a string that
    describes a label such as cow or zebra. The two types of categorical data correspond
    to data that can be naturally sorted and that which can’t.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 类别数据指的是其他类型的数据，通常它是描述标签的字符串，比如牛或斑马。这两种类别数据分别对应可自然排序的数据和不可排序的数据。
- en: '*Ordinal* data is categorical data that has a known order (hence the name),
    so we can sort it. Strings can be sorted alphabetically, but we can also sort
    them by meaning. For example, we can think of the rainbow colors as ordinal, because
    they have a natural ordering in which they appear in a rainbow, from red to orange
    on to violet. To sort the names of colors by rainbow order, we need to use a program
    that understands the orders of colors in a rainbow. Another example of ordinal
    data are strings that describe a person at different ages, such as infant, teenager,
    and elderly. These strings also have a natural order, so we can sort them as well,
    again by some kind of custom routine.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*有序*数据是具有已知顺序的分类数据（因此得名），因此我们可以对其进行排序。字符串可以按字母顺序排序，也可以按含义排序。例如，我们可以将彩虹的颜色视为有序数据，因为它们在彩虹中有一个自然的排序，从红色到橙色，再到紫色。为了按彩虹顺序排序颜色名称，我们需要使用一个理解彩虹颜色顺序的程序。另一个有序数据的例子是描述一个人不同年龄段的字符串，如婴儿、青少年和老年人。这些字符串也有一个自然的顺序，因此我们也可以对它们进行排序，再次通过某种自定义的程序。'
- en: '*Nominal* data is categorical data without a natural ordering. For example,
    a list of desktop items such as paper clip, stapler, and pencil sharpener has
    no natural or built-in ordering, nor does a collection of pictures of clothing,
    like socks, shirts, gloves, and bowler hats. We can turn nominal data into ordinal
    data just by defining an order and sticking to it. For example, we can assert
    that the order of clothing should be from head to toes, so our previous example
    would have the order bowler hats, shirts, gloves, and socks, thereby turning our
    pictures into ordinal data. The order we create for nominal data doesn’t have
    to make any particular kind of sense, it just has to be defined and then used
    consistently.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*名义*数据是没有自然顺序的分类数据。例如，桌面物品列表，如回形针、订书机和削笔刀，并没有自然的排序，衣物图片的集合，比如袜子、衬衫、手套和圆顶礼帽，也没有自然顺序。我们可以通过定义一个顺序并遵循它，将名义数据转化为有序数据。例如，我们可以声明衣物的顺序应该从头到脚，因此我们之前的例子中的顺序应为圆顶礼帽、衬衫、手套和袜子，从而将这些图片转化为有序数据。我们为名义数据创建的顺序不必有特别的逻辑，只要它被定义并且始终如一地使用即可。'
- en: Machine learning algorithms require numbers as input, so we convert string data
    (and any other nonnumerical data) into numbers before we learn from it. Taking
    strings as an example, we could make a list of all the strings in the training
    data, and assign each one a unique number starting with 0\. Many libraries provide
    built-in routines to create and apply this transformation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法需要数字作为输入，因此我们在学习之前将字符串数据（和任何其他非数字数据）转化为数字。以字符串为例，我们可以列出所有训练数据中的字符串，并为每个字符串分配一个唯一的数字，从0开始。许多库提供了内置的程序来创建并应用这种转换。
- en: One-Hot Encoding
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一位编码（One-Hot Encoding）
- en: Sometimes it’s useful to turn integers into lists. For instance, we might have
    a classifier with 10 classes where class 3 might be toaster and class 7 might
    be ball-point pen, and so on. When we manually assign a label to a photo of one
    of these objects, we consult this list and give it the correct number. When the
    system makes a prediction, it gives us back a list of 10 numbers. Each number
    represents the system’s confidence that the input belongs to the corresponding
    class.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，将整数转化为列表是有用的。例如，我们可能有一个有10个类别的分类器，其中类别3可能是烤面包机，类别7可能是圆珠笔，依此类推。当我们手动为这些物体之一的照片分配标签时，我们查阅这个列表并给它正确的数字。当系统做出预测时，它会返回一个包含10个数字的列表。每个数字代表系统认为输入属于相应类别的置信度。
- en: This means we are comparing our label (an integer) with the classifier’s output
    (a list). When we build classifiers, it makes sense to compare lists to lists,
    so we need a way to turn our label into a list.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们将标签（一个整数）与分类器的输出（一个列表）进行比较。当我们构建分类器时，将列表与列表进行比较是有意义的，因此我们需要一种方法将标签转化为列表。
- en: 'That’s easily done. Our list form of the label is just the list we want from
    the output. Let’s suppose that we’re labeling a picture of a toaster. We want
    the system’s output to be a list of ten values, with a 1 in the slot for class
    3, corresponding to complete certainty that this is a toaster, and a 0 in every
    other slot, indicating complete certainty that the image is none of those other
    things. So, the list form of our label is the very same thing: ten numbers, all
    0, except for a 1 in slot 3.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这很容易做到。我们标签的列表形式就是我们从输出中想要的列表。假设我们正在标注一个烤面包机的图片。我们希望系统的输出是一个包含十个值的列表，其中第 3 类的槽位为
    1，表示我们完全确定这是一个烤面包机，而其他所有槽位都是 0，表示我们完全确定图像不是其他任何东西。所以，我们的标签的列表形式就是这个：十个数字，全部为 0，除了第
    3 个槽位为 1。
- en: Converting a label like 3 or 7 into this kind of list is called *one-hot encoding*,
    referring to only one entry in the list being “hot,” or marked. The list itself
    is sometimes called a *dummy variable*. When we provide class labels to the system
    during training, we usually provide this one-hot encoded list, or dummy variable,
    rather than a single integer.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 将像 3 或 7 这样的标签转换为这种列表的过程称为*独热编码*，即列表中只有一个条目是“热的”或已标记的。这个列表本身有时被称为*虚拟变量*。当我们在训练过程中向系统提供类别标签时，通常提供的是这个独热编码列表或虚拟变量，而不是单一的整数。
- en: Let’s see this in action. [Figure 10-4](#figure10-4)(a) shows the eight colors
    in the original 1903 box of Crayola Crayons (Crayola 2016). Let’s suppose these
    colors appear as strings in our data. The one-hot labels that we provide to the
    system as our labels are shown in the rightmost column.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看实际应用中的情况。[图 10-4](#figure10-4)(a) 显示了 1903 年原版 Crayola Crayons 盒子中的八种颜色（Crayola
    2016）。假设这些颜色在我们的数据中以字符串形式出现。我们提供给系统的独热标签显示在最右列。
- en: '![F10004](Images/F10004.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![F10004](Images/F10004.png)'
- en: 'Figure 10-4: One-hot encoding for the original eight Crayola colors in 1903\.
    (a) The original eight strings. (b) Each string is assigned a value from 0 to
    7\. (c) Each time the string appears in our data, we replace it with a list of
    eight numbers, all of which are 0 except for a 1 in the position corresponding
    to that string’s value.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-4：1903年原始八种 Crayola 颜色的独热编码。（a）原始的八个字符串。（b）每个字符串都分配一个从 0 到 7 的值。（c）每次该字符串出现在我们的数据中时，我们将其替换为一个包含八个数字的列表，所有数字为
    0，除了对应该字符串值的位置为 1。
- en: So far, we’ve converted data in one form to another form. Now let’s look at
    some transformations that actually change the values in our data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将数据从一种形式转换成了另一种形式。现在，让我们来看一些实际上改变我们数据值的变换。
- en: Normalizing and Standardizing
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准化和规范化
- en: 'We often work with samples whose features span different numerical ranges.
    For instance, suppose we collected data on a herd of African bush elephants. Our
    data describes each elephant with four values:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常处理特征跨越不同数值范围的样本。例如，假设我们收集了关于一群非洲草原象的数据。我们的数据用四个值描述每只大象：
- en: Age in hours (0, 420,000)
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 年龄（小时）（0, 420,000）
- en: Weight in tons (0, 7)
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重量（吨）（0, 7）
- en: Tail length in centimeters (120, 155)
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尾巴长度（厘米）（120, 155）
- en: Age relative to the historical mean age, in hours (−210,000, 210,000)
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相对于历史平均年龄的年龄，单位小时（−210,000, 210,000）
- en: These are significantly different ranges of numbers. Generally speaking, because
    of the numerical nature of the algorithms we use, larger numbers may influence
    a learning program more than smaller ones. The values in feature 4 are not only
    large, but they also can be negative.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是显著不同的数字范围。一般来说，由于我们使用的算法是数值型的，较大的数字可能比较小的数字对学习程序的影响更大。特征 4 中的值不仅很大，而且还可能是负数。
- en: For the best learning behavior, we want all of our data to be roughly comparable,
    or to fit in roughly the same range of numbers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最佳的学习效果，我们希望所有数据在某种程度上具有可比性，或者大致适合相同的数字范围。
- en: Normalization
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 规范化
- en: A common first step in transforming our data is to *normalize* each feature.
    The word *normal* is used in everyday life to mean “typical,” but it also has
    specialized technical meanings in different fields. In this context, we use the
    word in its statistical sense. We say that when we scale data into some specific
    range, the data has been *normalized.* The most popular choice of ranges for normalization
    are [−1,1] and [0,1], depending on the data and what it means (it doesn’t make
    sense to speak of negative apples or ages, for instance). Every machine learning
    library offers a routine to do this, but we have to remember to call it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 转换数据的一个常见第一步是对每个特征进行*归一化*。在日常生活中，*normal*这个词通常意味着“典型的”，但在不同领域它也有专门的技术含义。在这个上下文中，我们使用的是统计学上的含义。我们说，当我们将数据缩放到某个特定范围时，数据就被*归一化*了。归一化的最常见范围是[−1,1]和[0,1]，具体取决于数据及其含义（例如，谈论负的苹果或年龄就没有意义）。每个机器学习库都提供了执行此操作的常规方法，但我们必须记得调用它。
- en: '[Figure 10-5](#figure10-5) shows a 2D dataset that we’ll use for demonstration.
    We’ve chosen a guitar because the shape helps us see what happens to the points
    as we move them around. We also added colors strictly as a visual aid, only to
    help us see how the points move. The colors have no other meaning.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-5](#figure10-5)展示了我们将用于演示的二维数据集。我们选择了吉他形状，因为它有助于我们看到数据点在移动时发生的变化。我们还添加了颜色，仅仅作为视觉辅助工具，帮助我们看到数据点的移动。颜色没有其他意义。'
- en: '![F10005](Images/F10005.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![F10005](Images/F10005.png)'
- en: 'Figure 10-5: A guitar shape made of 232 points'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-5：由232个数据点构成的吉他形状
- en: Typically, these points are the results of measurements, say the age and weights
    of some people, or the tempo and volume of a song. To keep things generic, let’s
    call the two features x and y.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些数据点是测量结果，例如某些人的年龄和体重，或者一首歌的节奏和音量。为了保持通用性，我们将这两个特征称为x和y。
- en: '[Figure 10-6](#figure10-6) shows the results of normalizing each feature in
    our guitar-shaped data to the range [−1,1] on each axis.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-6](#figure10-6)显示了将我们吉他形状数据中的每个特征归一化到范围[−1,1]后的结果。'
- en: '![F10006](Images/F10006.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![F10006](Images/F10006.png)'
- en: 'Figure 10-6: The data of [Figure 10-5](#figure10-5) after normalization to
    the range [−1,1] on each axis. The skewing of the shape is due to it being stretched
    more along the Y axis than the X.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-6：对[图 10-5](#figure10-5)中的数据进行归一化处理后，数据范围被调整到[−1,1]。形状的倾斜是由于在y轴方向上拉伸较多，而x轴方向的拉伸较少。
- en: In [Figure 10-6](#figure10-6), the x values are scaled from −1 to 1, and the
    y values are independently scaled from −1 to 1\. The guitar shape resulting from
    this operation has skewed a little bit because it’s been stretched more vertically
    than horizontally. This happens any time the different dimensions of the starting
    data span different ranges. In our case, the x data originally spanned the range
    of about [–1, 0] and the y data spanned about [–0.5, 0.2]. When we adjusted the
    values, we had to stretch the y values apart more than the x values, causing the
    skewing we see in [Figure 10-6](#figure10-6).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 10-6](#figure10-6)中，x值的范围从−1到1进行缩放，而y值则独立地从−1到1进行缩放。通过这种操作得到的吉他形状稍微发生了倾斜，因为它在垂直方向上被拉伸得比水平方向更长。每当起始数据的不同维度跨越不同范围时，就会发生这种情况。在我们的例子中，x数据最初的范围大约是[–1,
    0]，而y数据的范围大约是[–0.5, 0.2]。当我们调整这些值时，我们必须比x值更大地拉伸y值，导致了在[图 10-6](#figure10-6)中看到的倾斜。
- en: Standardization
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准化
- en: Another common operation involves *standardizing* each feature. This is a two-step
    process. First, we add (or subtract) a fixed value to all the data for each feature
    so that the mean value of every feature is 0 (this step is also called *mean normalization*
    or *mean subtraction*). In our 2D data, this moves the entire dataset left-right
    and up-down so that the mean value is sitting right on (0,0). Then, instead of
    normalizing or scaling each feature to lie between −1 and 1, we scale it so that
    it has a standard deviation of 1 (this step is also called *variance normalization*).
    Recall from Chapter 2 that this means about 68 percent of the values in that feature
    lie in the range of −1 to 1\.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见操作是对每个特征进行*标准化*。这是一个两步过程。首先，我们为每个特征的所有数据添加（或减去）一个固定值，使得每个特征的均值为0（这个步骤也叫做*均值归一化*或*均值减法*）。在我们的二维数据中，这会将整个数据集左右和上下移动，使得均值恰好位于(0,0)的位置。然后，我们不是将每个特征归一化或缩放到−1到1之间，而是将其缩放到标准差为1（这个步骤也叫做*方差归一化*）。回想第二章，这意味着该特征中大约68%的值位于−1到1的范围内。
- en: In our 2D example, the x values are stretched or compressed horizontally until
    about 68 percent of the data is between −1 and 1 on the X axis, and then the y
    values are stretched or compressed vertically until the same thing is true on
    the Y axis. This necessarily means that points will land outside of the range
    [−1, 1] on each axis, so our results are different than what we get from normalization.
    [Figure 10-7](#figure10-7) shows the application of standardization to our starting
    data in [Figure 10-5](#figure10-5).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的二维示例中，x值会被水平拉伸或压缩，直到大约68%的数据在X轴的−1和1之间，然后y值会被垂直拉伸或压缩，直到Y轴上也满足相同的条件。这必然意味着点会落在每个轴的[−1,1]范围之外，因此我们的结果与归一化所得到的不同。[图10-7](#figure10-7)展示了标准化应用于我们在[图10-5](#figure10-5)中的原始数据。
- en: '![F10007](Images/F10007.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![F10007](Images/F10007.png)'
- en: 'Figure 10-7: The data of [Figure 10-5](#figure10-5) after standardization'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-7：标准化后[图10-5](#figure10-5)的数据
- en: Here again we see that when the original shape doesn’t fit a normal distribution,
    a transformation like standardization can skew or otherwise distort the shape
    of the original data. Most libraries offer routines to normalize or standardize
    any or all of our features in one call. This makes it convenient to satisfy some
    algorithms that require their input to be normalized or standardized.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们再次看到，当原始形状不符合正态分布时，像标准化这样的转换可能会扭曲或以其他方式改变原始数据的形状。大多数库提供了一个函数，可以一次性对我们的所有特征进行归一化或标准化处理。这使得满足某些要求输入数据归一化或标准化的算法变得更加方便。
- en: Remembering the Transformation
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 记住转换
- en: Both normalization and standardization routines are controlled by parameters
    that tell them how to do their jobs. Most library routines analyze the data to
    find these parameters and then use them to apply the transformation. Because it’s
    so important to transform future data with the same operations, these library
    calls always give us a way to hang onto these parameters so we can apply the same
    transformations again later.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化和标准化过程都由参数控制，这些参数告诉它们如何执行任务。大多数库函数会分析数据以找到这些参数，然后利用它们来应用转换。由于使用相同操作转换未来数据至关重要，这些库调用总是提供一种方法来保存这些参数，以便我们以后可以再次应用相同的转换。
- en: In other words, when we later receive a new batch of data to evaluate, either
    to evaluate our system’s accuracy or to make real predictions out in the field,
    we do *not* analyze that data to find new normalizing or standardizing transformations.
    Instead, we apply the same normalizing or standardizing steps that we determined
    for the training data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，当我们稍后收到一批新数据以进行评估时，无论是为了评估系统的准确性，还是为了在实际应用中做出预测，我们*不会*分析这些数据以寻找新的归一化或标准化转换。相反，我们会应用为训练数据确定的相同归一化或标准化步骤。
- en: A consequence of this step is that the newly transformed data is almost never
    itself normalized or standardized. That is, it won’t be in the range [−1,1] on
    both axes, or it won’t have its average at (0,0) and contain 68 percent of its
    data in the range [−1,1] on each axis. That’s fine. What’s important is that we’re
    using the same transformation. If the new data isn’t quite normalized or standardized,
    so be it.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程的结果是，新转换后的数据几乎从未真正归一化或标准化。也就是说，数据的范围不会在两个轴上都在[−1,1]之间，或者它的平均值不会在(0,0)处，并且其68%的数据不会在每个轴上的[−1,1]范围内。这没关系。重要的是我们使用了相同的转换。如果新数据没有完全归一化或标准化，那也无妨。
- en: Types of Transformations
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换类型
- en: Some transformations are *univariate*, which means they work on just one feature
    at a time, each independent of the others (the name comes from combining *uni*,
    for one, with *variate*, which means the same as variable or feature). Others
    are *multivariate*, meaning they work on many features simultaneously.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一些转换是*单变量*的，这意味着它们一次只处理一个特征，每个特征彼此独立（该名称来源于将*uni*（表示“一个”）与*variate*（表示“变量”或“特征”）结合）。其他的则是*多变量*的，意味着它们同时处理多个特征。
- en: Let’s consider normalization. This is usually implemented as a univariate transformer
    that treats each feature as a separate set of data to be manipulated. That is,
    if it’s scaling 2D points to the range [0,1], it would scale all the x values
    to that range, and then independently scale all the y values. The two sets of
    features don’t interact in any way, so how the X axis gets scaled does not depend
    at all on the y values, and vice versa. [Figure 10-8](#figure10-8) shows this
    ideal visually for a normalizer applied to data with three features.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一下标准化。这通常作为一个单变量变换器来实现，它将每个特征当作一个独立的数据集来处理。也就是说，如果它是将二维点缩放到[0,1]范围，它会首先将所有的x值缩放到这个范围，然后独立地将所有的y值缩放到这个范围。两个特征集之间没有任何交互，因此X轴如何被缩放与y值无关，反之亦然。[图10-8](#figure10-8)直观地展示了这一理想情况，展示了对具有三个特征的数据应用的标准化过程。
- en: '![F10008](Images/F10008.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![F10008](Images/F10008.png)'
- en: 'Figure 10-8: When we apply a univariate transformation, each feature is transformed
    independently of the others. Here we are normalizing three features to the range
    [0,1]. (a) The starting ranges of three features. (b) Each of the three ranges
    is independently shifted and stretched to the range [0,1].'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-8：当我们应用单变量变换时，每个特征是独立地变换的。在这里，我们将三个特征标准化到[0,1]的范围内。(a) 三个特征的起始范围。(b) 每个特征的范围独立地被移动和拉伸到[0,1]的范围内。
- en: By contrast, a multivariate algorithm looks at multiple features at a time and
    treats them as a group. The most extreme (and most common) version of this process
    is to handle all of the features simultaneously. If we scale our three colored
    bars in a multivariate way, we move and stretch them all as a group until they
    collectivelyfill the range [0,1], as illustrated in [Figure 10-9](#figure10-9).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，多变量算法同时考虑多个特征，并将它们作为一个整体来处理。这个过程的最极端（也是最常见）版本是同时处理所有特征。如果我们以多变量方式缩放这三根彩条，我们将它们作为一个整体进行移动和拉伸，直到它们共同覆盖[0,1]的范围，正如[图10-9](#figure10-9)所示。
- en: We can apply many transformations in either a univariate or multivariate way.
    We choose based on our data and application. For instance, the univariate version
    made sense in [Figure 10-6](#figure10-6) when we scaled our x and y samples because
    they’re essentially independent. But suppose our features are temperature measurements
    made at different times over the course of different days? We probably want to
    scale all the features together so that, as a collection, they span the range
    of temperatures we’re working with.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采用单变量或多变量方式进行多种变换。我们根据数据和应用场景来选择。例如，当我们缩放x和y样本时，单变量版本在[图10-6](#figure10-6)中是合理的，因为它们本质上是独立的。但假设我们的特征是不同时间、不同日期的温度测量值呢？我们可能希望将所有特征一起缩放，这样作为一个整体，它们就覆盖了我们所使用的温度范围。
- en: '![F10009](Images/F10009.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![F10009](Images/F10009.png)'
- en: 'Figure 10-9: When we apply a multivariate transformation, we treat multiple
    features simultaneously. Here we are again normalizing to the range [0,1]. (a)
    The starting ranges of three features. (b) The bars are shifted and stretched
    as a group so that their collective minimum and maximum values span the range
    [0,1].'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-9：当我们应用多变量变换时，我们同时处理多个特征。在这里，我们再次将范围标准化到[0,1]。(a) 三个特征的起始范围。(b) 条形图作为一个整体被移动和拉伸，使得它们的最小值和最大值共同覆盖[0,1]的范围。
- en: Slice Processing
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 切片处理
- en: Given a dataset, we need to think about how we select the data we want to transform.
    There are three approaches, depending on whether we think of *slicing*, or extracting,
    our data by sample, by feature, or by element. These approaches are respectively
    called *samplewise*, *featurewise*, and *elementwise* processing.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个数据集，我们需要思考如何选择我们要转换的数据。根据我们是否按样本、按特征或按元素提取数据，通常有三种方法。这些方法分别称为*样本级*、*特征级*和*元素级*处理。
- en: Let’s look at them in that order. For this discussion, let’s assume that each
    sample in our dataset is a list of numbers. We can arrange the whole dataset in
    a 2D grid, where each row holds a sample and each element in that row is a feature.
    [Figure 10-10](#figure10-10) shows the setup.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按这个顺序来看看它们。为了讨论的方便，假设我们数据集中的每个样本都是一组数字列表。我们可以将整个数据集安排成一个二维网格，其中每一行包含一个样本，每一行中的每个元素是一个特征。[图10-10](#figure10-10)展示了这种设置。
- en: '![F10010](Images/F10010.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![F10010](Images/F10010.png)'
- en: 'Figure 10-10: Our database for the coming discussion is a 2D grid. Each row
    is a sample that contains multiple features, which make up the columns.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-10：我们接下来讨论的数据集是一个二维网格。每一行是一个样本，包含多个特征，这些特征构成了列。
- en: Samplewise Processing
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 样本级处理
- en: The samplewise approach is appropriate when all of our features are aspects
    of the same thing. For example, suppose our input data contains little snippets
    of audio, such as a person speaking into a cell phone. Then the features in each
    sample are the amplitude of the audio at successive moments, as in [Figure 10-11](#figure10-11).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 样本逐一处理方法适用于所有特征都是同一事物的不同方面的情况。例如，假设我们的输入数据包含一些音频片段，例如某人通过手机讲话。那么每个样本中的特征就是音频在连续时刻的振幅，如[图10-11](#figure10-11)所示。
- en: If we want to scale this data to the range [0,1], it makes sense to scale all
    the features in a single sample so the loudest parts are set to 1 and the quietest
    parts to 0\. Thus, we process each sample, one at a time, independent of the other
    samples.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想将这些数据缩放到[0,1]范围内，那么按单个样本缩放所有特征是有意义的，这样最响亮的部分就设为1，最安静的部分设为0。因此，我们一次处理一个样本，独立于其他样本。
- en: '![F10011](Images/F10011.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![F10011](Images/F10011.png)'
- en: 'Figure 10-11: Each sample consists of a series of measurements of a short audio
    waveform. Each feature gives us an instantaneous measurement of the volume of
    the sound at that moment.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-11：每个样本由一段短音频波形的测量数据组成。每个特征给出了该时刻声音的音量的瞬时测量值。
- en: Featurewise Processing
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 按特征处理
- en: The featurewise approach is appropriate when our samples represent essentially
    different things.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 按特征处理方法适用于我们的样本代表本质上不同的事物。
- en: Suppose we’ve taken a variety of weather measurements each evening, recording
    the temperature, rainfall, wind speed, and humidity. This gives us four features
    per sample, as in [Figure 10-12](#figure10-12).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们每天晚上都进行各种天气测量，记录温度、降水量、风速和湿度。这为每个样本提供了四个特征，如[图10-12](#figure10-12)所示。
- en: '![F10012](Images/F10012.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![F10012](Images/F10012.png)'
- en: 'Figure 10-12: When we process our data featurewise, we analyze each column
    independently. Top three lines: The original data. Middle line: The range. Bottom
    line: The scaled data.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-12：当我们按特征处理数据时，我们独立分析每一列。前三行：原始数据。中间一行：范围。底部一行：缩放后的数据。
- en: It doesn’t make sense to scale this data on a samplewise basis, because the
    units and measurements are incompatible. We can’t compare the wind speed and the
    humidity on an equal footing. But we can analyze all of the humidity values together,
    and the same is true for all the values for temperature, rainfall, and humidity.
    In other words, we modify each feature in turn.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 按样本逐一缩放这些数据没有意义，因为单位和测量方式不兼容。我们不能在同等条件下比较风速和湿度。但我们可以将所有湿度值一起分析，温度、降水量和湿度的所有值也可以以同样的方式处理。换句话说，我们依次修改每个特征。
- en: When we process data featurewise, each column of feature values is sometimes
    called a *fibre*.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们按特征处理数据时，每一列特征值有时被称为*纤维*。
- en: Elementwise Processing
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 元素逐一处理
- en: The elementwise approach treats each element in the grid of [Figure 10-10](#figure10-10)
    as an independent entity and applies the same transformation to every element
    in the grid independently. This is useful, for example, when all of our data represents
    the same kind of thing, but we want to change its units. For instance, suppose
    that each sample corresponds to a family with eight members and contains the heights
    of each of the eight people. Our measurement team reported their heights in inches,
    but we want the heights in millimeters.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 元素逐一处理方法将[图10-10](#figure10-10)网格中的每个元素视为独立实体，并对网格中的每个元素独立应用相同的变换。例如，当我们的所有数据代表相同类型的事物，但我们想改变其单位时，这种方法很有用。例如，假设每个样本对应一个有八个成员的家庭，并包含这八个人的身高。我们的测量团队报告了他们的身高（单位为英寸），但我们想要将其转换为毫米。
- en: We need only multiply every entry in the grid by 25.4 to convert inches to millimeters.
    It doesn’t matter if we think of this as working across rows or along columns,
    since every element is handled the same way.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要将网格中的每个条目乘以25.4，将英寸转换为毫米。无论我们是沿着行还是列进行操作都没有关系，因为每个元素都以相同的方式处理。
- en: We do this frequently when we work with images. Image data often arrives with
    each pixel in the range [0,255]. We often apply an elementwise scaling operation
    to divide every pixel value in the entire input by 255, giving us data from 0
    to 1.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在处理图像时经常这样做。图像数据通常每个像素的值在[0,255]范围内。我们通常应用元素逐一缩放操作，将整个输入图像的每个像素值除以255，得到的结果是一个0到1之间的数据。
- en: Most libraries allow us to apply transformations using any of these interpretations.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数库允许我们使用这些解释中的任何一种应用变换。
- en: Inverse Transformations
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逆变换
- en: We’ve been looking at different transformations that we can apply to our data.
    However, sometimes we want to undo, or *invert*, those steps so we can more easily
    compare our results to our original data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we work for the traffic department of a city that has one
    major highway. Our city is far north, so the temperature often drops below freezing.
    The city managers have noticed that the traffic density seems to vary with temperature,
    with more people staying home on the coldest days. In order to plan for roadwork
    and other construction, the managers want to know how many cars they can predict
    for each morning’s rush-hour commute based on the temperature. Because it takes
    some time to measure and process the data, we decide to measure the temperature
    at midnight each evening and then predict how many cars will be on the road between
    7 and 8 am the next morning. We’re going to start using our system in the middle
    of winter, so we expect temperatures both above and below freezing (0° Celsius).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: For a few months, we measure the temperature at every midnight, and we count
    the total number of cars passing a particular marker on the road between 7 and
    8 am the next morning. The raw data is shown in [Figure 10-13](#figure10-13).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: We want to give this data to a machine-learning system that will learn the connection
    between temperature and traffic density. After deployment, we feed in a sample
    consisting of one feature, describing the temperature in degrees, and we get back
    a real number telling us the predicted number of cars on the road.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![F10013](Images/F10013.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-13: Each midnight we measure the temperature, and then the following
    morning, we measure the number of cars on the road between 7 and 8 am.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Let’s suppose that the regression algorithm we’re using works best when its
    input data is scaled to the range [0,1]. We can normalize the data to [0,1] on
    both axes, as in [Figure 10-14](#figure10-14).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![F10014](Images/F10014.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-14: Normalizing both ranges to [0,1] makes the data more amenable
    for training.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: This looks just like [Figure 10-13](#figure10-13), only now both our scales
    (and data) run from 0 to 1\.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: We’ve stressed the importance of remembering this transformation so we can apply
    it to future data. Let’s look at those mechanics in three steps. For convenience,
    let’s use an object-oriented philosophy, where our transformations are carried
    out by objects that remember their own parameters.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: The first of our three steps is to create a transformer object for each axis.
    This object is capable of performing this transformation (also called a *mapping*).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Second, let’s give that object our input data to analyze. It finds the smallest
    and largest values and uses them to create the transformation that shifts and
    scales our input data to the range [0,1]. We’ll give the temperature data to the
    first transformer and the vehicle count data to the second transformer.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve only created the transformers, but we haven’t applied them. Nothing
    has changed in any of our data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只创建了转换器，但还没有应用它们。我们的数据没有发生任何变化。
- en: '[Figure 10-15](#figure10-15) shows the idea.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-15](#figure10-15)展示了这个过程。'
- en: '![f10015](Images/f10015.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![f10015](Images/f10015.png)'
- en: 'Figure 10-15: Building transformation objects. Left: The temperature data is
    fed to a transformation object, represented by a blue rectangle. Right: We also
    build a yellow transformer for the car counts.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-15：构建转换对象。左侧：温度数据输入到一个转换对象中，表示为一个蓝色矩形。右侧：我们还为汽车计数构建了一个黄色的转换器。
- en: The third step is to give our data to the transform objects again, but this
    time, we tell them to apply the transformation they have already computed. The
    result is a new set of data that has been transformed to the range [0,1]. [Figure
    10-16](#figure10-16) shows the idea.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步是将我们的数据再次交给转换对象，这次我们告诉它们应用它们已经计算出的转换。结果是一个新的数据集，已经转换到[0,1]的范围内。[图 10-16](#figure10-16)展示了这个过程。
- en: Now we’re ready to learn. We give our transformed data to our learning algorithm
    and let it figure out the relationship between the inputs and the outputs, as
    shown schematically in [Figure 10-17](#figure10-17).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好进行学习了。我们将转换后的数据提供给学习算法，让它分析输入与输出之间的关系，如[图 10-17](#figure10-17)所示。
- en: Let’s assume that we’ve trained our system, and it’s doing a good job of predicting
    car counts from temperature data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经训练好了系统，并且它能很好地根据温度数据预测汽车数量。
- en: The next day, we deploy our system on a web page for the city managers. On the
    first night, the manager on duty measures a midnight temperature of −10° Celsius.
    She opens up our application, finds the input box for the temperature, types in
    −10, and hits the big “Predict Traffic” button.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 第二天，我们将系统部署到城市管理者的网页上。在第一个晚上，值班经理测得午夜时温度为−10°摄氏度。她打开我们的应用程序，找到温度输入框，输入−10并点击“大型预测交通”按钮。
- en: '![F10016](Images/F10016.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![F10016](Images/F10016.png)'
- en: 'Figure 10-16: Each feature is modified by the transformation we previously
    computed for it. The output of the transformations goes into our learning system.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-16：每个特征都被我们之前计算出的转换所修改。转换后的输出进入我们的学习系统。
- en: '![F10017](Images/F10017.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![F10017](Images/F10017.png)'
- en: 'Figure 10-17: A schematic view of learning from our transformed features and
    targets'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-17：从我们转换后的特征和目标中学习的示意图
- en: Uh-oh, something’s gone wrong. We can’t just feed −10 into our trained system,
    because as [Figure 10-17](#figure10-17) shows, it’s expecting a number in the
    range of 0 to 1\. We need to transform the data somehow. The only way that makes
    sense is to apply the same transformation that we applied to the temperatures
    when we trained our system. For example, if in our original dataset −10 became
    the value 0.29, then if the temperature is −10 tonight, we should enter 0.29,
    not –10\.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀，出现了问题。我们不能直接将−10输入到我们训练过的系统中，因为正如[图 10-17](#figure10-17)所示，它期望输入的是一个范围在0到1之间的数字。我们需要以某种方式对数据进行转换。唯一合理的做法是应用我们在训练系统时对温度所做的相同转换。例如，如果在原始数据集中−10变成了0.29，那么如果今晚的温度是−10，我们应该输入0.29，而不是−10\。
- en: Here’s where we see the value of saving our transformation as an object. We
    can simply tell that object to take the same transformation that it applied to
    our training data and now apply it to this new piece of data. If −10 turned into
    0.29 during training, any new input of −10 turns into 0.29 during deployment as
    well.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这时我们就能看到将转换保存为对象的价值。我们可以简单地告诉这个对象，使用它在训练数据上应用的相同转换，现在将其应用到这组新的数据上。如果−10在训练中变成了0.29，那么任何新的−10输入，在部署时也会变成0.29。
- en: Let’s suppose that we correctly give the temperature 0.29 to the system, and
    it produces a traffic density of 0.32\. This corresponds to the value of some
    number of cars transformed by our car transformation. But that value is between
    0 and 1, because that was the range of the data we trained on representing car
    counts. How do we undo that transformation and turn it into a number of cars?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正确地将温度0.29提供给系统，它输出的交通密度是0.32\。这对应于通过我们对汽车转换所得到的某个汽车数量的值。但是这个值是在0和1之间，因为这是我们训练时代表汽车数量的数据的范围。我们如何撤销这个转换，把它转换回汽车数量呢？
- en: In any machine learning library, every transformation object comes with a routine
    to *invert*, or undo, its transformation, providing us with an *inverse transformation*.
    In this case, it inverts the normalizing transformation it’s been applying so
    far. If the object transformed 39 cars into the normalized value 0.32, then the
    inverse transformation turns the normalized value 0.32 back into 39 cars. This
    is the value we print out to the city manager. [Figure 10-18](#figure10-18) shows
    these steps.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何机器学习库中，每个转换对象都有一个*逆向*例程，用来撤销其转换，提供一个*逆向转换*。在这种情况下，它会反转之前应用的标准化转换。如果该对象将39辆车转换为标准化值0.32，那么逆向转换会将标准化值0.32转回为39辆车。这就是我们打印给城市经理的值。[图10-18](#figure10-18)展示了这些步骤。
- en: '![F10018](Images/F10018.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![F10018](Images/F10018.png)'
- en: 'Figure 10-18: When we feed a new temperature to our system, we transform it
    using the transformation we computed for our temperature data, turning it into
    a number from 0 to 1\. The value that comes out is then run through the inverse
    of the transformation we computed for the car data, turning it from a scaled number
    into a number of cars.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-18：当我们将新的温度输入系统时，我们使用我们为温度数据计算的转换，将其转换为一个0到1之间的数字。出来的值接着会通过我们为汽车数据计算的逆向转换，将其从缩放的数字转换为汽车的数量。
- en: One thing that can seemingly go wrong here is if we get new samples outside
    of the original input range. Suppose we get a surprisingly cold temperature reading
    one night of −50° Celsius, which is far below the minimum value in our original
    data. The result is that the transformed value is a negative number, outside of
    our [0,1] range. The same thing can happen if we get a very hot night, giving
    us a positive temperature that transforms to a value greater than 1, which is
    again outside of [0,1].
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这里可能出现的一个问题是，如果我们获得了超出原始输入范围的新样本。假设我们在某个夜晚测得了一个令人吃惊的低温 −50° 摄氏度，这个温度远低于我们原始数据中的最小值。结果是，经过转换后的值是一个负数，超出了我们的[0,1]范围。如果我们遇到一个非常热的夜晚，测得一个正温度，经过转换后得到的值将大于1，这同样超出了[0,1]范围。
- en: Both situations are fine. Our desire for scaling inputs to [0,1] is to make
    training go as efficiently as possible, and also to keep numerical issues in check.
    Once the system is trained, we can give it any values we want as input, and it
    calculates a corresponding output. Of course, we still have to pay attention to
    our data. If the system predicts a negative number of cars for tomorrow, we don’t
    want to make plans based on that number.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 两种情况都是可以的。我们希望将输入缩放到[0,1]范围内，是为了让训练尽可能高效，并且控制数值问题。一旦系统训练完毕，我们可以给它任何我们想要的输入，它会计算出相应的输出。当然，我们仍然需要注意我们的数据。如果系统预测明天的车数为负数，我们是不能基于这个数字来制定计划的。
- en: Information Leakage in Cross-Validation
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证中的信息泄漏
- en: We’ve seen how to build the transformation from the training set and then retain
    that transformation and apply it, unchanged, to all additional data. If we don’t
    follow this policy carefully, we can get *information leakage*, where information
    that doesn’t belong in our transformation accidentally sneaks (or leaks) into
    it, affecting the transformation. This means that we don’t transform the data
    the way we intend. Worse, this leakage can lead to the system getting an unfair
    advantage when it evaluates our test data, giving us an overinflated measure of
    accuracy. We may conclude that our system is performing well enough to be deployed,
    only to be disappointed when it has much worse performance when used for real.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何从训练集构建转换，然后保持这个转换，并将其不变地应用于所有附加数据。如果我们不仔细遵循这个策略，就可能会发生*信息泄漏*，即本不应包含在转换中的信息意外地渗入（或泄漏）到转换中，从而影响转换。这意味着我们没有按照预期的方式转换数据。更糟糕的是，这种泄漏可能导致系统在评估我们的测试数据时获得不公平的优势，从而给我们一个虚高的准确度衡量值。我们可能会得出系统足够好，可以部署的结论，结果却会在实际使用时表现得更差。
- en: Information leakage is a challenging problem because many of its causes can
    be subtle. As an example, let’s see how information leakage can affect the process
    of cross-validation, which we discussed in Chapter 8\. Modern libraries give us
    convenient routines that provide fast and correct implementations of cross-validation,
    so we don’t have to write our own code to do it. But let’s look under the hood
    anyway. We’ll see how a seemingly reasonable approach leads to information leakage,
    and then how we can fix it. Seeing this in action will help us get better at preventing,
    spotting, and fixing information leakage in our own systems and algorithms.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Recall that in cross-validation, we set aside one fold (or section) of the starting
    training set to serve as a temporary validation set. Then we build a new learner
    and train it with the remaining data. When we’re done training, we evaluate the
    learner using the saved fold as the validation set. This means that each time
    through the loop, we have a new training set (the starting data without the samples
    in the selected fold). If we’re going to apply a transformation to our data, we
    need to build it from just the data that’s being used as the training set for
    that learner. We then apply that transformation to the current training set, and
    we apply *the same transformation* to the current validation set. The key thing
    to remember is that because in cross-validation we create a new training set and
    validation set each time through the loop, we need to build a new transformation
    each time through the loop as well.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see what happens if we do it incorrectly. [Figure 10-19](#figure10-19)
    shows our starting set of samples at the left. They’re analyzed to produce a transformation
    (shown by the red circle), which is then applied by a transformation routine (marked
    T). To show the change, we colored the transformed samples red, like the transformation.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![f10019](Images/f10019.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-19: A *wrong* way to do cross-validation: building one transform
    based on all the original training data'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Then we enter the cross-validation process. Here the loop is “unrolled,” so
    we’re showing several instances of the training sessions, each associated with
    a different fold. Each time through the loop, we remove a fold, train on the remaining
    samples, and then test with the validation fold and create a score.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: The problem here is that when we analyzed the input data to build the transformation,
    we included the data in every fold in our analysis. To see why this is a problem,
    let’s look more closely at what’s going on with a simpler and more specific scenario.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that the transformation we want to apply is scaling all the features
    in the training set, as a group, to the range 0 to 1\. That is, we’ll do samplewise
    multivariate normalization. Let’s say that in the very first fold, the smallest
    and largest feature values are 0.01 and 0.99\. In the other folds, the largest
    and smallest values occupy smaller ranges. [Figure 10-20](#figure10-20) shows
    the range of data contained in each of the five folds. We’re going to analyze
    data in all the folds and build our transformation from that.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 10-20](#figure10-20), our dataset is shown at the left, split into
    five folds. Inside each box, we show the range of values in that fold, with 0
    at the left and 1 at the right. The top fold has features running from 0.01 to
    0.99\. The other folds have values that are well within this range. When we analyze
    all the folds as a group, the range of the first fold dominates, so we only stretch
    the whole dataset by a little bit.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![F10020](Images/F10020.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-20: A *wrong*way to transform our data for cross-validation is to
    transform everything at once before the loop.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s proceed with the cross-validation loop. Our input data is the stack
    of five transformed folds at the far right of [Figure 10-20](#figure10-20). Let’s
    start by extracting the first fold and setting it aside; then we can train with
    the rest of the data, and validate. But we’ve done something bad here, because
    *our training data’s transformation was influenced by the validation data*. This
    is a violation of our basic principle that we create the transform using only
    the values in the training data. But here we used what is now the validation data
    when we computed our transform. We say that information has *leaked* from this
    step’s validation data into the transformation parameters, where it doesn’t belong.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The right way to build the transformation for the training data is to remove
    the validation data from the samples, thenbuild the transformation from the remaining
    data, and then apply that transformation to both the training and validation data.
    [Figure 10-21](#figure10-21) shows this visually.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![f10021](Images/f10021.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-21: The proper way to transform our data for cross-validation is
    to first remove the fold samples and then compute the transformation from the
    data that remains.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Now we can apply that transformation to both the training set and the validation
    data. Note that here the validation data ends up way outside the range [0,1],
    which is no problem, because that data really is more extreme than the training
    set.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: To fix our cross-validation process, we need to use this scheme in the loop
    and compute a new transformation for every training set. [Figure 10-22](#figure10-22)
    shows the right way to proceed.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![F10022](Images/F10022.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-22: The proper way to do cross-validation'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: For each fold we want to use as a validation set, we analyze the starting samples
    with that fold removed and then apply the resulting transform to both the training
    set and the validation set in the fold. The different colors show that each time
    through the loop, we build and apply a different transformation.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们想要作为验证集的每一个折叠，我们会分析移除该折叠的起始样本，然后将得到的变换应用到训练集和验证集。不同的颜色表明每次循环时，我们构建并应用的是不同的变换。
- en: We’ve discussed information leakage in the context of cross-validation because
    it’s a great example of this tricky topic. Happily, the cross-validation routines
    in modern libraries all do the right thing, so we don’t have to worry about this
    problem ourselves when we use library routines. But this doesn’t take the responsibility
    off of us when we write our own code. Information leakage is often subtle, and
    it can creep into our programs in unexpected ways. It’s important that we always
    think carefully about possible sources of information leakage when we build and
    apply transformations.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在交叉验证的背景下讨论了信息泄漏，因为它是这个棘手话题的一个很好的例子。幸运的是，现代库中的交叉验证程序都能正确处理，所以我们在使用库的例程时不必担心这个问题。但这并不意味着我们在编写自己代码时就可以不管信息泄漏。信息泄漏往往很微妙，它可能以意想不到的方式潜入我们的程序。我们在构建和应用变换时，始终要仔细考虑可能的信息泄漏源，这一点非常重要。
- en: Shrinking the Dataset
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩小数据集
- en: We’ve been looking at ways to adjust the numbers in our data, and how to select
    the numbers that go into each transformation. Now let’s look at a different kind
    of transformation, designed not just to manipulate the data, but to actually compress
    it. We will literally create a new dataset that’s smaller than the original training
    set, typically by removing or combining features in each sample.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在研究如何调整数据中的数字，以及如何选择进入每个变换的数字。现在，让我们来看一种不同的变换方式，这种方式不仅仅是操作数据，而是实际压缩数据。我们将字面上创建一个比原始训练集更小的新数据集，通常是通过删除或合并每个样本中的特征。
- en: 'This has two advantages: improved speed and accuracy when training. It stands
    to reason that the less data we need to process during training, the faster our
    training goes. By going faster, we mean we can pack in more learning in a given
    amount of time, resulting in a more accurate system.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这有两个好处：提高训练时的速度和准确性。可以推理出，训练时我们处理的数据越少，训练的速度就越快。提到速度，我们是指在给定的时间内，我们能够完成更多的学习，从而使得系统更为准确。
- en: Let’s look at a few ways to shrink our dataset.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看几种缩小数据集的方法。
- en: Feature Selection
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征选择
- en: If we’ve collected features in our data that are redundant, irrelevant, or otherwise
    not helpful, then we should eliminate them so that we don’t waste time on them.
    This process is called *feature selection*, or sometimes, *feature filtering*.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在数据中收集了冗余、不相关或其他无用的特征，那么我们应该将它们剔除，以免浪费时间。这一过程叫做*特征选择*，有时也叫做*特征筛选*。
- en: Let’s consider an example where some data is actually superfluous. Suppose we’re
    hand-labeling images of elephants by entering their size, species, and other characteristics
    into a database. For some reason nobody can quite remember, we also have a field
    for the number of heads. Elephants have only one head, so that field’s going to
    be nothing but 1’s. So that data is not just useless, it also slows us down. We
    ought to remove that field from our data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个例子，其中一些数据实际上是多余的。假设我们正在手动标注大象的图像，输入它们的大小、物种以及其他特征到数据库中。不知为何，大家记不清楚，我们还设有一个字段用于记录头的数量。大象只有一个头，所以这个字段的值将始终是1。因此，这个数据不仅没用，还会拖慢我们的速度。我们应该从数据中删除这个字段。
- en: We can generalize this idea to removing features that are *almost* useless,
    that contribute very little, or that simply make the least contribution to getting
    the right answer. Let’s continue with our collection of elephant images. We have
    values for each animal’s height, weight, last known latitude and longitude, trunk
    length, ear size, and so on. But for this (imaginary) species, the trunk length
    and ear size may be closely correlated. If so, we can remove (or *filter out*)
    either one and still get the benefit of the information they each represent.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个想法推广到去除那些*几乎*无用的特征，这些特征贡献很小，或者根本没有对得到正确答案做出太大贡献。继续以我们收集的大象图像为例。我们为每只大象记录了身高、体重、最后已知的纬度和经度、象鼻长度、耳朵大小等等。但对于这个（假设的）物种，象鼻长度和耳朵大小可能是高度相关的。如果是这样，我们可以删除（或*筛选掉*）其中一个特征，依然可以获得它们各自代表的信息的好处。
- en: Many libraries offer tools that can estimate the impact of removing each field
    from a database. We can then use this information to guide us in simplifying our
    database and speeding our learning without sacrificing more accuracy than we’re
    willing to give up. Because removing a feature is a transformation, any features
    we remove from our training set must be removed from all future data as well.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality Reduction
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another approach to reducing the size of our dataset is combining features,
    so one feature can do the work of two or more. This is called *dimensionality
    reduction*, where *dimensionality* refers to the number of features.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: The intuition here is that some of the features in our data might be closely
    related without being entirely redundant. If the relationship is strong, we might
    be able to combine those two features into just one new one. An everyday example
    of this is the *body mass index (BMI)*. This is a single number that combines
    a person’s height and weight. Some measurements of a person’s health can be computed
    with just the BMI. For example, charts that help people decide if they need to
    lose weight can be conveniently indexed by age and BMI (CDC 2017).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a tool that automatically determines how to select and combine
    features to make the smallest impact on our results.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Principal component analysis (PCA)*, is a mathematical technique for reducing
    the dimensionality of data. Let’s get a visual feel for PCA by looking at what
    it does to our guitar data.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-23](#figure10-23) shows our starting guitar data again. As before,
    the colors of the dots are just to make it easier to track them in the following
    figures as the data is manipulated, and don’t have any other meaning.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![F10023](Images/F10023.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-23: The starting data for our discussion of PCA'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to crunch this two-dimensional data down to one-dimensional data.
    That is, we combine each set of paired x and y values to create a single new number
    based on both of them, just as BMI is a single number that combines a person’s
    height and weight.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by standardizing the data. [Figure 10-24](#figure10-24) shows this
    combination of setting each dimension to have a mean of 0 and a standard deviation
    of 1, as we saw before.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: We already know that we’re going to try to reduce this 2D data to just 1D. To
    get a feel for the idea before we actually apply it, let’s go through the process
    with one key step missing, and then we’ll put that step back in.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: To get started, let’s draw a horizontal line on the X axis. We’ll call this
    the *projection line*. Then we’ll *project*, or move, each data point to its closest
    spot on the projection line. Because our line is horizontal, we only need to move
    our points up or down to find their closest point on the projection line.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![F10024](Images/F10024.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-24: Our input data after standardizing'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: The results of projecting the data in [Figure 10-24](#figure10-24) onto a horizontal
    projection line are shown in [Figure 10-25](#figure10-25).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![F10025](Images/F10025.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-25: We project each data point of the guitar by moving it to its
    nearest point on the projection line. For clarity, we’re showing the path taken
    by only about 25 percent of the points.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: The results after all the points are processed is shown in [Figure 10-26](#figure10-26).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![F10026](Images/F10026.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-26: The result of [Figure 10-25](#figure10-25) after all the points
    have been moved to the projection line. Each point now is described only by its
    x coordinate, resulting in a one-dimensional dataset.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: This is the one-dimensional dataset we were after, because the points only differ
    by their x value (the y value is always 0, so it’s irrelevant). But this would
    be a lousy way to combine our features, because all we did was throw away the
    y values. It’s like computing BMI by simply using the weight and ignoring the
    height.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: To improve the situation, let’s include the step we skipped. Instead of using
    a horizontal projection line, we rotate the line around until it’s passing through
    the direction of maximum variance. Think of this as the line that, after projection,
    has the largest range of points. Any library routine that implements PCA finds
    this line for us automatically. [Figure 10-27](#figure10-27) shows this line for
    our guitar data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![F10027](Images/F10027.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-27: The thick black line is the line of maximum variance through
    our original data. This is our projection line.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Now we continue just like before. We project each point onto this projection
    line by moving it to its closest point on the line. As before, we do this by moving
    perpendicular to the line until we intersect it. [Figure 10-28](#figure10-28)
    shows this process.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![F10028](Images/F10028.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-28: We project the guitar data onto the projection line by moving
    each point to its closest point on the line. For clarity, we’re showing the path
    taken by only about 25 percent of the points.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: The projected points are shown in [Figure 10-29](#figure10-29). Note that they
    all lie on the line of maximum variance that we found in [Figure 10-27](#figure10-27).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![f10029](Images/f10029.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-29: The points of the guitar dataset projected onto the line of maximum
    variance'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: For convenience, we can rotate this line of points to lie on the X axis, as
    shown in [Figure 10-30](#figure10-30). Now the y coordinate is irrelevant again,
    and we have 1D data that includes information about each point from both its original
    x and y values.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![F10030](Images/F10030.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-30: Rotating the points of [Figure 10-29](#figure10-29) into a horizontal
    position'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Although the straight line of points in [Figure 10-30](#figure10-30) looks roughly
    like the line of points in [Figure 10-26](#figure10-26), they’re different, because
    the points are distributed differently along the X axis. In other words, they
    have different values, because they were computed by projecting onto a tilted
    line, rather than a horizontal one. [Figure 10-31](#figure10-31) shows the two
    projections together. The PCA result is not just longer, but the points are also
    distributed differently.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![F10031](Images/F10031.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-31: Comparing the points created by projection to y = 0 in [Figure
    10-26](#figure10-26) (top) and the PCA algorithm in [Figure 10-30](#figure10-30)
    (bottom)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: All of the steps we just discussed are carried out automatically by a machine
    learning library when we call its PCA routine.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of the 1D data created by this projection step is that every point’s
    single value (its x value) is a combination of the 2D data it started with. We
    reduced the dimensionality of our dataset by one dimension, but we did so while
    retaining as much information as we could. Our learning algorithms now only have
    to process one feature rather than two, so they’ll run faster. Of course, we’ve
    thrown some information away, so the accuracy may suffer. The trick to using PCA
    effectively is to choose dimensions that can be combined while still staying within
    the performance goals of our project.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: If we have 3D data, we can imagine placing a plane in the middle of the cloud
    of samples and projecting our data down onto the plane. The library’s job is to
    find the best orientation of that plane. This takes our data from 3D to 2D. If
    we want to go all the way down to 1D, we can imagine projecting the data onto
    a line through the volume of points. In practice, we can use this technique in
    problems with any number of dimensions, where we may reduce the dimensionality
    of the data by tens or more.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'The critical questions for this kind of algorithm include: How many dimensions
    should we try to compress? Which dimensions should be combined? How they should
    get combined? We usually use the letter *k* to stand for the number of dimensions
    remaining in our data after PCA has done its work. So, in our guitar example,
    *k* was 1\. We can call *k* a parameter of the algorithm, though usually we call
    it a hyperparameterof the entire learning system. As we’ve seen, the letter *k*
    is used for lots of different algorithms in machine learning, which is unfortunate;
    it’s important to pay attention to the context when we see references to *k*.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Compressing too little means our training and evaluation steps are going to
    be inefficient, but compressing too much means we risk eliminating important information
    that we should have kept. To pick the best value for the hyperparameter *k*, we
    usually try out a few different values to see how they do and then pick the one
    that seems to work best. We can automate this search using the techniques of *hyperparameter
    searching* provided by many libraries.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: As always, whatever PCA transformations we use to compress our training data
    must also be used in the same way for all future data.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: PCA for Simple Images
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Images are an important and special kind of data. Let’s apply PCA to a simple
    set of images.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-32](#figure10-32) shows a set of six images, perhaps drawn from
    a huge dataset of tens of thousands of such pictures. If these grayscale images
    are 1,000 pixels on a side, each contains 1,000 × 1,000, or 1 million, pixels.
    Is there any better way to represent them than with a million numbers each?'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![F10032](Images/F10032.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-32: Six images we’d like to represent'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by observing that each image in [Figure 10-32](#figure10-32) can
    be re-created from the three images in [Figure 10-33](#figure10-33), each scaled
    by a different amount and then added together.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![F10033](Images/F10033.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-33: We can create all six images in [Figure 10-32](#figure10-32)
    by scaling these three by different amounts and adding the results.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can reconstruct the first image in [Figure 10-32](#figure10-32)
    by adding 20 percent of the circle, 70 percent of the vertical box, and 40 percent
    of the horizontal box. We often call these scaling factors the *weights*. The
    weights for each of the six starting images are shown in [Figure 10-34](#figure10-34).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![F10034](Images/F10034.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-34: The weights to use when scaling the three images in [Figure 10-33](#figure10-33)
    to recover the images in [Figure 10-32](#figure10-32)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-35](#figure10-35) shows the process of scaling and combining the
    components to recover the first original image.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: In general, we can represent any image of this type with the three weights of
    the component images that make the best match. To reconstruct any of the input
    images, we need the three simpler pictures (1 million values each) plus the three
    numbers for that specific image. If we had 1,000 images, storing each one would
    take a total of 1,000 megabytes. But using this compressed form, we need a total
    of only 3.001 megabytes.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![F10035](Images/F10035.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-35: Recovering the first image in [Figure 10-32](#figure10-32) by
    scaling the images in [Figure 10-33](#figure10-33)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: For these simple images, it’s easy to find the components as three geometric
    shapes. But when we have more realistic pictures, this won’t generally be possible.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that we can use the projection technique we discussed earlier.
    Instead of projecting a collection of points to create a new set of points on
    a line, we can project a collection of images to create a new image. This is a
    more abstract process than the one we followed with the guitar points, but the
    concept is the same. Let’s get a feeling for how PCA handles images by skipping
    the mechanics and focusing on the results.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Consider again the six starting images of [Figure 10-32](#figure10-32). Remember
    that these are grayscale images, not vector drawings. Let’s ask PCA to find a
    grayscale image that comes closest to representing all the images, in the same
    way that our diagonal line came closest to representing all the points in our
    guitar. Then we can represent each starting image as a sum of this image, scaled
    by an appropriate amount, plus whatever is left over. [Figure 10-36](#figure10-36)
    shows this process.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![F10036](Images/F10036.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-36: Running PCA on the images from [Figure 10-32](#figure10-32)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: The starting images from 10-32 are shown at the top of [Figure 10-36](#figure10-36).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Now we ask PCA to find an image that corresponds to the line in [Figure 10-28](#figure10-28).
    That is, an image that, in some sense, captures something from all of the inputs.
    Let’s say it found the picture at the left in [Figure 10-36](#figure10-36), showing
    the two bars and the circle all in black and overlaid. We’ll call this the *shared
    image*, which isn’t a formal term but is useful here.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now represent each image at the top of [Figure 10-36](#figure10-36) as
    a combination of a scaled version of the shared image, and some other image. To
    do this, we find the lightest pixel in each input image, and scale the shared
    image to that intensity. That scaling factor is shown at the top of the copies
    of the common image in the middle row, where the copies have been scaled in intensity
    by that amount. If we subtract each scaled common image from the source image
    above it in the figure, we get their difference. We could write this as “source
    – common = difference,” or equivalently, “source = common + difference,” which
    is shown in the figure.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: We can then run PCA again, this time on the bottom row of [Figure 10-36](#figure10-36).
    Again, it projects these six images to create a new image that is the best match
    for all of them. As before, we can represent each picture as the sum of a scaled
    version of what’s common, plus whatever is left. [Figure 10-37](#figure10-37)
    shows the idea. In this demonstration, we’re supposing that PCA created an image
    of the two overlapping boxes as the best matching image.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![F10037](Images/F10037.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-37: Running PCA on the bottom row of [Figure 10-36](#figure10-36)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Something interesting happened in two images on the bottom row. Let’s look at
    the second column from the left. The image we want to match on the top row is
    a circle and horizontal box, but we’re trying to match it with a scaled pair of
    crossed boxes. To match the top image, we need to add in some of the circle, but
    subtract the vertical box that we just introduced. That just means setting the
    corresponding data in the bottom image to negative values. These are perfectly
    valid numbers to place into our data, though we have to be careful if we try to
    display this image directly. If we reconstruct the top image by adding up the
    two images beneath it, the negative values in the red region in the bottom row
    cancel out the positive values in the vertical box in the middle row, so the sum
    of these images matches the circle and horizontal box at the top. The same reasoning
    applies to the horizontal box in the rightmost column.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-38](#figure10-38) summarizes the two steps we’ve seen so far.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: We took only two steps here, but we can repeat this process dozens or hundreds
    of times.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: To represent each starting image, we only need the collection of common images
    and the weight we assigned to each. Since the common images are shared by all
    the images, we can consider them a shared resource. Then each image can be completely
    described by a reference to this shared resource, and the list of weights to be
    applied to the common images.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![F10038](Images/F10038.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-38: Representing each starting image (top row) as the sum of two
    scaled component images (second and third rows), plus whatever’s left (bottom
    row)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Each of the common images is called a *component*, and the one we create at
    every step is the *principal* component (since it’s the best of all possible components,
    in the same way that the line in [Figure 10-28](#figure10-28) was the best line).
    We find these principal components by analyzing the input images. Hence, the name
    *principal component analysis*.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: The more components we include, the more accurately each reconstructed image
    will match its original. We usually aim to produce enough components so that each
    reconstructed image has all the qualities we care about in its original.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: In this discussion, most of the weights were positive, but we also saw some
    component images that should be subtracted, not added, and thus they produce weights
    with negative values. This is so that the final pixels, when all the components
    are summed together, have the desired values.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: How well did we do with just two component images? [Figure 10-39](#figure10-39)
    shows the original six images and our reconstructed images using the sum of the
    middle two rows of [Figure 10-38](#figure10-38).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: The matches aren’t perfect, but they’re a good start, particularly for just
    two components. So, we have a common pool of two images (requiring 1 million numbers
    each), and then each image itself can be described with just two numbers. The
    beauty of this scheme is that our algorithms never need to see the common images.
    We just need those for finding the weights that describe each input image (and
    to reconstruct the images, if we want). As far as the learning algorithm is concerned,
    each image is described by only two numbers. This means our algorithms consume
    less memory and run more quickly.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![F10039](Images/F10039.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-39: Our starting six images (top), and the reconstructed images from
    [Figure 10-35](#figure10-35)c (bottom)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'We skipped a step in this example: normally when we use PCA, we standardize
    all the images first. That’s included in our next example.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: PCA for Real Images
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The images in the last section were contrived for simplicity. Now we’ll apply
    PCA to real pictures.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin with the six pictures of huskies shown in [Figure 10-40](#figure10-40).
    To make our processing easier to see, these images are only 45 × 64 pixels. These
    were aligned by hand so that the eyes and nose are in about the same location
    in each image. This way each pixel in each image has a good chance of representing
    the same part of a dog as the corresponding pixel in the other images. For instance,
    a pixel just below the center is likely to part of a nose, one near the upper
    corners is likely to be an ear, and so on.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![f10040](Images/f10040.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-40: Our starting set of huskies'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: A database of six dogs isn’t a lot of training data, so let’s enlarge our database
    using the idea of *data augmentation,* a common strategy for *amplifying* or *enlarging*
    a dataset. In this case, let’s run through our set of six images in random order
    over and over. Each time through, we’ll pick an image, make a copy, randomly shift
    it horizontally and vertically up to 10 percent on each axis, rotate it up to
    five degrees clockwise or counterclockwise, and maybe flip it left to right. Then
    we append that transformed image to our training set. [Figure 10-41](#figure10-41)
    shows the results of the first two passes through our six dogs. We used this technique
    of creating variations to build a training set of 4,000 dog images.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '![F10041](Images/F10041.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-41: Each row shows a set of new images created by shifting, rotating,
    and perhaps horizontally flipping each of our input images.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Since we’d like to run PCA on these images, our first step is to standardize
    them. This means we analyze the same pixel in each of our 4,000 images and adjust
    the collection to have zero mean and unit variance (Turk and Pentland 1991). The
    standardized versions of our first six generated dogs are shown in [Figure 10-42](#figure10-42).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![F10042](Images/F10042.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-42: Our first six huskies after standardization'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Since 12 images fit nicely into a figure, let’s begin by arbitrarily asking
    PCA to find the 12 images that, when added together with appropriate weights,
    best reconstruct the input images. Each of the projections (or components) found
    by PCA is technically known as an *eigenvector*, from the German *eigen* meaning
    “own” (or roughly “self”), and *vector* from the mathematical name for this kind
    of object. When we create eigenvectors of particular types of things, it’s common
    to create a playful name by combining the prefix eigen with the object we’re processing.
    Hence, [Figure 10-43](#figure10-43) shows our 12 *eigendogs*.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '![F10043](Images/F10043.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-43: The 12 eigendogs produced by PCA'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the eigendogs tells us a lot about how PCA is analyzing our images.
    The first eigendog is a big smudge that is darker roughly where most of the dogs
    appear in the image. This is the single image that comes closest to approximating
    every input image. The second eigendog gives us refinements to the first, capturing
    some of the left-right shading differences.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: The next eigendog provides additional detail, and so it goes through all 12
    eigendogs. So, the first eigendog captures the broadest, most common features,
    and each additional eigendog lets us recover a little more detail.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: PCA is able not only to create the eigendogs of [Figure 10-43](#figure10-43),
    but also to take as input any picture, and tell us the weight to apply to each
    eigendog image so that, when the weighted images are added together, we get the
    best approximation to the input image.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how well we can recover our original images by combining these 12
    eigendogs with their corresponding weights. [Figure 10-44](#figure10-44) shows
    the weights that PCA found for each input image. We create the reconstructed dogs
    by scaling each eigendog image from [Figure 10-43](#figure10-43) with its corresponding
    weight, and then adding the results together.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![f10044](Images/f10044.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-44: Reconstructing our original inputs from a set of 12 eigendogs.
    Top row: The reconstructed dogs. Bottom row: The weights applied to the eigendogs
    of [Figure 10-43](#figure10-43) to build the image directly above. Notice that
    the vertical scales on the bottom row are not all the same.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: The recovered dogs in [Figure 10-44](#figure10-44) are not great. We’ve asked
    PCA to represent all 4,000 images in our training set with just 12 pictures. It
    did its best, but these results are pretty blurry. They do seem to be on the right
    track, though.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try using 100 eigendogs. The first 12 eigendog images look just like those
    in [Figure 10-43](#figure10-43), but then they get more complicated and detailed.
    The results of reconstructing our first set of 6 dogs are shown in [Figure 10-45](#figure10-45).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![F10045](Images/F10045.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-45: Reconstructing our original inputs from a set of 100 eigendogs'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: That’s better! They’re starting to look like dogs. But it seems that 100 eigendogs
    is still not enough.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Let’s crank up our number of eigendogs to 500 and try again. [Figure 10-46](#figure10-46)
    shows the results.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![F10046](Images/F10046.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-46: Reconstructing our original inputs from a set of 500 eigendogs'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: These are looking pretty great. They are all easily recognized as the 6 standardized
    dogs in [Figure 10-42](#figure10-42). They’re not perfect, but considering that
    we’re adding together different amounts of 500 shared images, we’ve done a fine
    job of matching the original images. There’s nothing special about these first
    6 images. If we look at any of the 4,000 images in our database, they all look
    this good. We could keep increasing the number of eigendogs, and the results would
    continue to improve, with the images getting increasingly sharper and less noisy.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: In each plot of the weights, the eigendog images that get the most weight are
    the ones at the start, which capture the big structures. As we work our way down
    the list, each new eigendog is generally weighted a little less than the one before,
    so it contributes less to the overall result.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: The value of PCA here is not that we can make images that look just like the
    starting set, but rather, that we can use the eigendogs’ representation to reduce
    the amount of data our deep learning system has to process. This is illustrated
    in [Figure 10-47](#figure10-47). Our set of input dogs goes into PCA, which generates
    a set of eigendogs. Then each dog we’d like to classify goes into PCA again, which
    gives us the weights for that image. Those are the values that go into the classifier.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, rather than training our categorizer on all of the
    pixels from each image, we can train it on just that image’s 100 or 500 weights.
    The categorizer never sees a full image of a million pixels. It never even sees
    the eigendogs. It just gets a list of the weights for each image, and that’s the
    data it uses for analysis and prediction during training. When we want to classify
    a new image, we provide just its weights, and the computer gives us back a class.
    This can save a lot of computation, which translates to a savings in time, and
    perhaps increased quality of final results.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '![F10047](Images/F10047.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-47: In the top row we first we use PCA to build a set of eigendogs,
    and then in the bottom row we find the weights for each input to our classifier,
    which only uses those weights to find the input’s class.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the data we hand the classifier is not each input image, but its
    weights. The classifier then proceeds to work out which breed of dog it’s looking
    at based just on those weights. Often we need only a few hundred weights to represent
    input samples with many thousands, or even millions, of features.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we looked at ways to prepare data. We saw that it’s important
    to inspect our data before we do anything with it and make sure that it’s clean.
    Once our data is clean, we can transform it to better fit our learning algorithms
    in a number of ways. These transformations are built from the training data only.
    It’s important to remember that any transforms we apply to the training data must
    then be applied to every additional sample we give to our algorithm, from validation
    and test data to deployment data provided by real-world users.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll dig into classifiers and survey some of the most
    important algorithms for the job.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
