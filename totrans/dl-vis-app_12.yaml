- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Preparation
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: Machine learning algorithms can only work as well as the data they’re trained
    on. In the real world, our data can come from noisy sensors, computer programs
    with bugs, or even incomplete or inaccurate transcriptions of paper records. We
    always need to look at our data and fix any problems before we use it.
  prefs: []
  type: TYPE_NORMAL
- en: A rich body of methods has been developed for just this job. They’re referred
    to as techniques for *data preparation*, or *data cleaning*. The idea is to process
    our data beforelearning from it so that our learning systems can use the data
    most efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: We also want to make sure that the data itself is well suited to machine learning,
    which may mean adjusting it, for example, by scaling numbers, or combining categories.
    This work is essential because the particular way the data is structured, and
    the numerical ranges it spans, can have a strong effect on the information an
    algorithm can extract from it.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal in this chapter is to see how we can adjust the data we’re given, without
    changing its meaning, to get the most efficient and effective learning process.
    We begin with techniques to confirm that our data is clean and ready for training.
    We then consider methods for examining the data itself, and for making sure that
    we’ve got it in the best form for machine learning. This can involve doing simple
    things like replacing strings with numbers or taking more interesting actions
    like scaling the data. Finally, we look at ways to reduce the size of our training
    data. This lets our algorithms run and learn more quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Data Cleaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start by considering some simple ways to ensure that our data is well
    cleaned. The idea is to make sure that we’re starting out with data that has no
    blanks, incorrect entries, or other errors.
  prefs: []
  type: TYPE_NORMAL
- en: If our data is in textual form, then we want to make sure there are no typographical
    errors, misspellings, embedded unprintable characters, or other such corruptions.
    For example, if we have a collection of animal photos along with a text file describing
    them and our system is case-sensitive, then we want to make sure that every giraffe
    is labeled as giraffe and not girafe or Giraffe, and we want to avoid other typos
    or variants like beautiful giraffe or giraffe-extra tall. Every reference to a
    giraffe needs to use the identical string.
  prefs: []
  type: TYPE_NORMAL
- en: We should also look for other common-sense things. We want to remove any accidental
    duplicates in our training data, because they will skew our idea of what data
    we’re working with. If we accidentally include a single piece of data multiple
    times, our learner will interpret it as multiple, different samples that just
    happen to have the same value, and thus that sample may have more influence than
    it should.
  prefs: []
  type: TYPE_NORMAL
- en: We also want to make sure we don’t have any typographical errors, like missing
    a decimal point so we specify a value of 1,000 rather than 1.000, or putting two
    minus signs in front of a number rather than just one. It’s not uncommon to find
    some hand-composed databases with blanks or question marks in them, signifying
    that people didn’t have any data to enter. Some computer-generated databases can
    include a code like `NaN` (not a number), which is a placeholder indicating that
    the computer wanted to print a number but didn’t have a valid number to show.
    More troubling, sometimes when people are missing data for a numerical field,
    they enter something like 0 or –1\. We have to find and fix all such issues before
    we start learning from the data.
  prefs: []
  type: TYPE_NORMAL
- en: We also need to make sure that the data is in a format that will be properly
    interpreted by the software we’re giving it to. For example, we can use a format
    known as *scientific notation* to write very large and very small numbers. The
    problem is that such notation has no official format. Different programs use slightly
    different forms for this type of output, and other programs that read that data
    (like the library functions we often use in deep learning) can misinterpret forms
    they’re not expecting. For example, in scientific notation, the value 0.007 is
    commonly printed out as `7e-3` or `7E-3`. When we provide the sequence `7e-3`
    as an input, a program may interpret it as (7 × *e*) – 3, where *e* is Euler’s
    constant, which has a value of about 2.7\. The result is that the computer thinks
    that `7e-3` means that we’re asking it to first multiply the values of 7 and *e*
    together, and then subtract 3, giving us about 16 rather than 0.007\. We need
    to catch these sorts of things so that our programs properly interpret their inputs.
  prefs: []
  type: TYPE_NORMAL
- en: We also want to look for missing data. If a sample is missing data for one or
    more features, we might be able to patch the holes manually or algorithmically,
    but it might be better to simply remove the sample altogether. This is a subjective
    call that we usually make on a case-by-case basis.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we want to identify any pieces of data that seem dramatically different
    from all the others. Some of these *outliers* might be mere typos, like a forgotten
    decimal point. Others might be the result of human error, like a misdirected copy
    and paste, or when someone forgot to delete an entry from a spreadsheet. When
    we don’t know if an outlier is a real piece of data or an error of some kind,
    we have to use our judgment to decide whether to leave it in or remove it manually.
    This is a subjective decision that depends entirely on what our data represents,
    how well we understand it, and what we want to do with it.
  prefs: []
  type: TYPE_NORMAL
- en: Though these steps may seem straightforward, in practice, carrying them out
    can be a major effort depending on the size and complexity of our data and how
    messed up it is when we first get it. Many tools are available to help us clean
    data. Some are stand-alone, and others are built into machine-learning libraries.
    Commercial services will also clean data for a fee.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s useful to keep in mind this classic computing motto: *garbage in, garbage
    out*. In other words, our results are only as good as our starting data, so it’s
    vital that we start with the best data available, which means working hard to
    make it as clean as we possibly can.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve taken care of the essential small stuff, let’s turn our attention
    to making the data well suited for learning.
  prefs: []
  type: TYPE_NORMAL
- en: The Importance of Consistency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Preparing numbers for learning means applying *transformations* to them, without
    changing the relationships among them that we care about. We cover several such
    transformations later in this chapter, where we might scale all the numbers to
    a given range or eliminate some superfluous data so that the learner has less
    work to do. When we do these things, we must always obey a vital principle: any
    time we modify our training data in some way, *we must also modify all future
    data the same way*.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at why this is so important. When we make any changes to our training
    data, we typically modify or combine the values in ways that are designed to improve
    the computer’s learning efficiency or accuracy. [Figure 10-1](#figure10-1) shows
    the idea visually.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10001](Images/F10001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-1: The flow of preprocessing for training and evaluating'
  prefs: []
  type: TYPE_NORMAL
- en: As the figure shows, we typically determine any needed transformations by looking
    at the entirety of the training set. We transform that data to train our learner,
    and we also use the same transformation for all new data that comes after we’ve
    released our system to the world. The key point is that we must apply the *identical*
    modificationsto all new data before we give it to our algorithm for as long as
    our system is in use. This step must not be skipped.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that we need to reuse the same transformation on all data we evaluate
    pops up again and again in machine learning, often in subtle ways. Let’s first
    look at the problem in a general way with a visual example.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we want to teach a classifier how to distinguish pictures of cows from
    pictures of zebras. We can collect a huge number of photos of both animals to
    use as training data. What most obviously distinguishes pictures of these two
    animals are their different black-and-white markings. To make sure our learner
    pays attention to these elements, we may decide to crop each photo to isolate
    the animal’s hide, and then we train with those isolated patches of texture. These
    cropped photos are all that the learner sees. [Figure 10-2](#figure10-2) shows
    a couple of samples.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10002](Images/F10002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-2: Left: A patch of texture from a cow. Right: A patch of texture
    from a zebra.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we’ve trained our system and deployed it, but we forget to tell
    people about this preprocessing step of cropping each image to just the texture.
    Without knowing this vital information, a typical user might give our system complete
    pictures of cows and zebras, like those in [Figure 10-3](#figure10-3), and ask
    the system to identify the animal in each one.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10003](Images/F10003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-3: Left: A photo of a cow. Right: A photo of a zebra. If we trained
    our system on the isolated patterns of [Figure 10-2](#figure10-2), it could be
    misled by all the extra details in the photos.'
  prefs: []
  type: TYPE_NORMAL
- en: Humans can pick out the hide patterns from these photos. A computer, on the
    other hand, can be misled by the legs, the heads, the ground, and other details,
    thus reducing its ability to give us good results. The difference between the
    prepared data of [Figure 10-2](#figure10-2) and the unprepared data of [Figure
    10-3](#figure10-3) can result in a system that performs beautifully on our training
    data but gives lousy results in the real world. To avoid this, all new data, like
    that in [Figure 10-3](#figure10-3), must be cropped to produce inputs that are
    just like the training data in [Figure 10-2](#figure10-2).
  prefs: []
  type: TYPE_NORMAL
- en: 'Forgetting to transform new data in the same way as we transformed the training
    data is an easy mistake to make but usually causes our algorithms to underperform,
    sometimes to the point of becoming useless. The rule to remember is this: we determine
    how to modify our training data, then we modify it, and then we *remember how
    we modified it*. Any time we deal with more data, we must first *modify that data
    in the identical way* that the training data was modified. We’ll come back to
    this idea later and see how it’s used in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: Types of Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Typical databases contain different types of data: floating-point numbers,
    strings, integers that refer to categories, and so on. We’ll treat each of these
    data typesin its own way, so it’s useful to distinguish them and give each unique
    type its own name. The most common naming system is based on whether a kind of
    data can be sorted. Though we rarely use explicit sorting when we do deep learning,
    this naming system is still convenient and widely used.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that each sample is a list of values, each of which is called a *feature*.
    Each feature in a sample can be either of two general varieties: *numerical* or
    *categorical*.'
  prefs: []
  type: TYPE_NORMAL
- en: Numerical data is simply a number, either floating-point or integer. We also
    call this *quantitative* data. Numerical, or quantitative, data can be sorted
    just by using its values.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical data is just about anything else, though often it’s a string that
    describes a label such as cow or zebra. The two types of categorical data correspond
    to data that can be naturally sorted and that which can’t.
  prefs: []
  type: TYPE_NORMAL
- en: '*Ordinal* data is categorical data that has a known order (hence the name),
    so we can sort it. Strings can be sorted alphabetically, but we can also sort
    them by meaning. For example, we can think of the rainbow colors as ordinal, because
    they have a natural ordering in which they appear in a rainbow, from red to orange
    on to violet. To sort the names of colors by rainbow order, we need to use a program
    that understands the orders of colors in a rainbow. Another example of ordinal
    data are strings that describe a person at different ages, such as infant, teenager,
    and elderly. These strings also have a natural order, so we can sort them as well,
    again by some kind of custom routine.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Nominal* data is categorical data without a natural ordering. For example,
    a list of desktop items such as paper clip, stapler, and pencil sharpener has
    no natural or built-in ordering, nor does a collection of pictures of clothing,
    like socks, shirts, gloves, and bowler hats. We can turn nominal data into ordinal
    data just by defining an order and sticking to it. For example, we can assert
    that the order of clothing should be from head to toes, so our previous example
    would have the order bowler hats, shirts, gloves, and socks, thereby turning our
    pictures into ordinal data. The order we create for nominal data doesn’t have
    to make any particular kind of sense, it just has to be defined and then used
    consistently.'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning algorithms require numbers as input, so we convert string data
    (and any other nonnumerical data) into numbers before we learn from it. Taking
    strings as an example, we could make a list of all the strings in the training
    data, and assign each one a unique number starting with 0\. Many libraries provide
    built-in routines to create and apply this transformation.
  prefs: []
  type: TYPE_NORMAL
- en: One-Hot Encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes it’s useful to turn integers into lists. For instance, we might have
    a classifier with 10 classes where class 3 might be toaster and class 7 might
    be ball-point pen, and so on. When we manually assign a label to a photo of one
    of these objects, we consult this list and give it the correct number. When the
    system makes a prediction, it gives us back a list of 10 numbers. Each number
    represents the system’s confidence that the input belongs to the corresponding
    class.
  prefs: []
  type: TYPE_NORMAL
- en: This means we are comparing our label (an integer) with the classifier’s output
    (a list). When we build classifiers, it makes sense to compare lists to lists,
    so we need a way to turn our label into a list.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s easily done. Our list form of the label is just the list we want from
    the output. Let’s suppose that we’re labeling a picture of a toaster. We want
    the system’s output to be a list of ten values, with a 1 in the slot for class
    3, corresponding to complete certainty that this is a toaster, and a 0 in every
    other slot, indicating complete certainty that the image is none of those other
    things. So, the list form of our label is the very same thing: ten numbers, all
    0, except for a 1 in slot 3.'
  prefs: []
  type: TYPE_NORMAL
- en: Converting a label like 3 or 7 into this kind of list is called *one-hot encoding*,
    referring to only one entry in the list being “hot,” or marked. The list itself
    is sometimes called a *dummy variable*. When we provide class labels to the system
    during training, we usually provide this one-hot encoded list, or dummy variable,
    rather than a single integer.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see this in action. [Figure 10-4](#figure10-4)(a) shows the eight colors
    in the original 1903 box of Crayola Crayons (Crayola 2016). Let’s suppose these
    colors appear as strings in our data. The one-hot labels that we provide to the
    system as our labels are shown in the rightmost column.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10004](Images/F10004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-4: One-hot encoding for the original eight Crayola colors in 1903\.
    (a) The original eight strings. (b) Each string is assigned a value from 0 to
    7\. (c) Each time the string appears in our data, we replace it with a list of
    eight numbers, all of which are 0 except for a 1 in the position corresponding
    to that string’s value.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve converted data in one form to another form. Now let’s look at
    some transformations that actually change the values in our data.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing and Standardizing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We often work with samples whose features span different numerical ranges.
    For instance, suppose we collected data on a herd of African bush elephants. Our
    data describes each elephant with four values:'
  prefs: []
  type: TYPE_NORMAL
- en: Age in hours (0, 420,000)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Weight in tons (0, 7)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tail length in centimeters (120, 155)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Age relative to the historical mean age, in hours (−210,000, 210,000)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These are significantly different ranges of numbers. Generally speaking, because
    of the numerical nature of the algorithms we use, larger numbers may influence
    a learning program more than smaller ones. The values in feature 4 are not only
    large, but they also can be negative.
  prefs: []
  type: TYPE_NORMAL
- en: For the best learning behavior, we want all of our data to be roughly comparable,
    or to fit in roughly the same range of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A common first step in transforming our data is to *normalize* each feature.
    The word *normal* is used in everyday life to mean “typical,” but it also has
    specialized technical meanings in different fields. In this context, we use the
    word in its statistical sense. We say that when we scale data into some specific
    range, the data has been *normalized.* The most popular choice of ranges for normalization
    are [−1,1] and [0,1], depending on the data and what it means (it doesn’t make
    sense to speak of negative apples or ages, for instance). Every machine learning
    library offers a routine to do this, but we have to remember to call it.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-5](#figure10-5) shows a 2D dataset that we’ll use for demonstration.
    We’ve chosen a guitar because the shape helps us see what happens to the points
    as we move them around. We also added colors strictly as a visual aid, only to
    help us see how the points move. The colors have no other meaning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F10005](Images/F10005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-5: A guitar shape made of 232 points'
  prefs: []
  type: TYPE_NORMAL
- en: Typically, these points are the results of measurements, say the age and weights
    of some people, or the tempo and volume of a song. To keep things generic, let’s
    call the two features x and y.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-6](#figure10-6) shows the results of normalizing each feature in
    our guitar-shaped data to the range [−1,1] on each axis.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F10006](Images/F10006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-6: The data of [Figure 10-5](#figure10-5) after normalization to
    the range [−1,1] on each axis. The skewing of the shape is due to it being stretched
    more along the Y axis than the X.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 10-6](#figure10-6), the x values are scaled from −1 to 1, and the
    y values are independently scaled from −1 to 1\. The guitar shape resulting from
    this operation has skewed a little bit because it’s been stretched more vertically
    than horizontally. This happens any time the different dimensions of the starting
    data span different ranges. In our case, the x data originally spanned the range
    of about [–1, 0] and the y data spanned about [–0.5, 0.2]. When we adjusted the
    values, we had to stretch the y values apart more than the x values, causing the
    skewing we see in [Figure 10-6](#figure10-6).
  prefs: []
  type: TYPE_NORMAL
- en: Standardization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another common operation involves *standardizing* each feature. This is a two-step
    process. First, we add (or subtract) a fixed value to all the data for each feature
    so that the mean value of every feature is 0 (this step is also called *mean normalization*
    or *mean subtraction*). In our 2D data, this moves the entire dataset left-right
    and up-down so that the mean value is sitting right on (0,0). Then, instead of
    normalizing or scaling each feature to lie between −1 and 1, we scale it so that
    it has a standard deviation of 1 (this step is also called *variance normalization*).
    Recall from Chapter 2 that this means about 68 percent of the values in that feature
    lie in the range of −1 to 1\.
  prefs: []
  type: TYPE_NORMAL
- en: In our 2D example, the x values are stretched or compressed horizontally until
    about 68 percent of the data is between −1 and 1 on the X axis, and then the y
    values are stretched or compressed vertically until the same thing is true on
    the Y axis. This necessarily means that points will land outside of the range
    [−1, 1] on each axis, so our results are different than what we get from normalization.
    [Figure 10-7](#figure10-7) shows the application of standardization to our starting
    data in [Figure 10-5](#figure10-5).
  prefs: []
  type: TYPE_NORMAL
- en: '![F10007](Images/F10007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-7: The data of [Figure 10-5](#figure10-5) after standardization'
  prefs: []
  type: TYPE_NORMAL
- en: Here again we see that when the original shape doesn’t fit a normal distribution,
    a transformation like standardization can skew or otherwise distort the shape
    of the original data. Most libraries offer routines to normalize or standardize
    any or all of our features in one call. This makes it convenient to satisfy some
    algorithms that require their input to be normalized or standardized.
  prefs: []
  type: TYPE_NORMAL
- en: Remembering the Transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Both normalization and standardization routines are controlled by parameters
    that tell them how to do their jobs. Most library routines analyze the data to
    find these parameters and then use them to apply the transformation. Because it’s
    so important to transform future data with the same operations, these library
    calls always give us a way to hang onto these parameters so we can apply the same
    transformations again later.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, when we later receive a new batch of data to evaluate, either
    to evaluate our system’s accuracy or to make real predictions out in the field,
    we do *not* analyze that data to find new normalizing or standardizing transformations.
    Instead, we apply the same normalizing or standardizing steps that we determined
    for the training data.
  prefs: []
  type: TYPE_NORMAL
- en: A consequence of this step is that the newly transformed data is almost never
    itself normalized or standardized. That is, it won’t be in the range [−1,1] on
    both axes, or it won’t have its average at (0,0) and contain 68 percent of its
    data in the range [−1,1] on each axis. That’s fine. What’s important is that we’re
    using the same transformation. If the new data isn’t quite normalized or standardized,
    so be it.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some transformations are *univariate*, which means they work on just one feature
    at a time, each independent of the others (the name comes from combining *uni*,
    for one, with *variate*, which means the same as variable or feature). Others
    are *multivariate*, meaning they work on many features simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider normalization. This is usually implemented as a univariate transformer
    that treats each feature as a separate set of data to be manipulated. That is,
    if it’s scaling 2D points to the range [0,1], it would scale all the x values
    to that range, and then independently scale all the y values. The two sets of
    features don’t interact in any way, so how the X axis gets scaled does not depend
    at all on the y values, and vice versa. [Figure 10-8](#figure10-8) shows this
    ideal visually for a normalizer applied to data with three features.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10008](Images/F10008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-8: When we apply a univariate transformation, each feature is transformed
    independently of the others. Here we are normalizing three features to the range
    [0,1]. (a) The starting ranges of three features. (b) Each of the three ranges
    is independently shifted and stretched to the range [0,1].'
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, a multivariate algorithm looks at multiple features at a time and
    treats them as a group. The most extreme (and most common) version of this process
    is to handle all of the features simultaneously. If we scale our three colored
    bars in a multivariate way, we move and stretch them all as a group until they
    collectivelyfill the range [0,1], as illustrated in [Figure 10-9](#figure10-9).
  prefs: []
  type: TYPE_NORMAL
- en: We can apply many transformations in either a univariate or multivariate way.
    We choose based on our data and application. For instance, the univariate version
    made sense in [Figure 10-6](#figure10-6) when we scaled our x and y samples because
    they’re essentially independent. But suppose our features are temperature measurements
    made at different times over the course of different days? We probably want to
    scale all the features together so that, as a collection, they span the range
    of temperatures we’re working with.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10009](Images/F10009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-9: When we apply a multivariate transformation, we treat multiple
    features simultaneously. Here we are again normalizing to the range [0,1]. (a)
    The starting ranges of three features. (b) The bars are shifted and stretched
    as a group so that their collective minimum and maximum values span the range
    [0,1].'
  prefs: []
  type: TYPE_NORMAL
- en: Slice Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a dataset, we need to think about how we select the data we want to transform.
    There are three approaches, depending on whether we think of *slicing*, or extracting,
    our data by sample, by feature, or by element. These approaches are respectively
    called *samplewise*, *featurewise*, and *elementwise* processing.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at them in that order. For this discussion, let’s assume that each
    sample in our dataset is a list of numbers. We can arrange the whole dataset in
    a 2D grid, where each row holds a sample and each element in that row is a feature.
    [Figure 10-10](#figure10-10) shows the setup.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10010](Images/F10010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-10: Our database for the coming discussion is a 2D grid. Each row
    is a sample that contains multiple features, which make up the columns.'
  prefs: []
  type: TYPE_NORMAL
- en: Samplewise Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The samplewise approach is appropriate when all of our features are aspects
    of the same thing. For example, suppose our input data contains little snippets
    of audio, such as a person speaking into a cell phone. Then the features in each
    sample are the amplitude of the audio at successive moments, as in [Figure 10-11](#figure10-11).
  prefs: []
  type: TYPE_NORMAL
- en: If we want to scale this data to the range [0,1], it makes sense to scale all
    the features in a single sample so the loudest parts are set to 1 and the quietest
    parts to 0\. Thus, we process each sample, one at a time, independent of the other
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10011](Images/F10011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-11: Each sample consists of a series of measurements of a short audio
    waveform. Each feature gives us an instantaneous measurement of the volume of
    the sound at that moment.'
  prefs: []
  type: TYPE_NORMAL
- en: Featurewise Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The featurewise approach is appropriate when our samples represent essentially
    different things.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we’ve taken a variety of weather measurements each evening, recording
    the temperature, rainfall, wind speed, and humidity. This gives us four features
    per sample, as in [Figure 10-12](#figure10-12).
  prefs: []
  type: TYPE_NORMAL
- en: '![F10012](Images/F10012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-12: When we process our data featurewise, we analyze each column
    independently. Top three lines: The original data. Middle line: The range. Bottom
    line: The scaled data.'
  prefs: []
  type: TYPE_NORMAL
- en: It doesn’t make sense to scale this data on a samplewise basis, because the
    units and measurements are incompatible. We can’t compare the wind speed and the
    humidity on an equal footing. But we can analyze all of the humidity values together,
    and the same is true for all the values for temperature, rainfall, and humidity.
    In other words, we modify each feature in turn.
  prefs: []
  type: TYPE_NORMAL
- en: When we process data featurewise, each column of feature values is sometimes
    called a *fibre*.
  prefs: []
  type: TYPE_NORMAL
- en: Elementwise Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The elementwise approach treats each element in the grid of [Figure 10-10](#figure10-10)
    as an independent entity and applies the same transformation to every element
    in the grid independently. This is useful, for example, when all of our data represents
    the same kind of thing, but we want to change its units. For instance, suppose
    that each sample corresponds to a family with eight members and contains the heights
    of each of the eight people. Our measurement team reported their heights in inches,
    but we want the heights in millimeters.
  prefs: []
  type: TYPE_NORMAL
- en: We need only multiply every entry in the grid by 25.4 to convert inches to millimeters.
    It doesn’t matter if we think of this as working across rows or along columns,
    since every element is handled the same way.
  prefs: []
  type: TYPE_NORMAL
- en: We do this frequently when we work with images. Image data often arrives with
    each pixel in the range [0,255]. We often apply an elementwise scaling operation
    to divide every pixel value in the entire input by 255, giving us data from 0
    to 1.
  prefs: []
  type: TYPE_NORMAL
- en: Most libraries allow us to apply transformations using any of these interpretations.
  prefs: []
  type: TYPE_NORMAL
- en: Inverse Transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve been looking at different transformations that we can apply to our data.
    However, sometimes we want to undo, or *invert*, those steps so we can more easily
    compare our results to our original data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we work for the traffic department of a city that has one
    major highway. Our city is far north, so the temperature often drops below freezing.
    The city managers have noticed that the traffic density seems to vary with temperature,
    with more people staying home on the coldest days. In order to plan for roadwork
    and other construction, the managers want to know how many cars they can predict
    for each morning’s rush-hour commute based on the temperature. Because it takes
    some time to measure and process the data, we decide to measure the temperature
    at midnight each evening and then predict how many cars will be on the road between
    7 and 8 am the next morning. We’re going to start using our system in the middle
    of winter, so we expect temperatures both above and below freezing (0° Celsius).
  prefs: []
  type: TYPE_NORMAL
- en: For a few months, we measure the temperature at every midnight, and we count
    the total number of cars passing a particular marker on the road between 7 and
    8 am the next morning. The raw data is shown in [Figure 10-13](#figure10-13).
  prefs: []
  type: TYPE_NORMAL
- en: We want to give this data to a machine-learning system that will learn the connection
    between temperature and traffic density. After deployment, we feed in a sample
    consisting of one feature, describing the temperature in degrees, and we get back
    a real number telling us the predicted number of cars on the road.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10013](Images/F10013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-13: Each midnight we measure the temperature, and then the following
    morning, we measure the number of cars on the road between 7 and 8 am.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s suppose that the regression algorithm we’re using works best when its
    input data is scaled to the range [0,1]. We can normalize the data to [0,1] on
    both axes, as in [Figure 10-14](#figure10-14).
  prefs: []
  type: TYPE_NORMAL
- en: '![F10014](Images/F10014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-14: Normalizing both ranges to [0,1] makes the data more amenable
    for training.'
  prefs: []
  type: TYPE_NORMAL
- en: This looks just like [Figure 10-13](#figure10-13), only now both our scales
    (and data) run from 0 to 1\.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve stressed the importance of remembering this transformation so we can apply
    it to future data. Let’s look at those mechanics in three steps. For convenience,
    let’s use an object-oriented philosophy, where our transformations are carried
    out by objects that remember their own parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The first of our three steps is to create a transformer object for each axis.
    This object is capable of performing this transformation (also called a *mapping*).
  prefs: []
  type: TYPE_NORMAL
- en: Second, let’s give that object our input data to analyze. It finds the smallest
    and largest values and uses them to create the transformation that shifts and
    scales our input data to the range [0,1]. We’ll give the temperature data to the
    first transformer and the vehicle count data to the second transformer.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve only created the transformers, but we haven’t applied them. Nothing
    has changed in any of our data.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-15](#figure10-15) shows the idea.'
  prefs: []
  type: TYPE_NORMAL
- en: '![f10015](Images/f10015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-15: Building transformation objects. Left: The temperature data is
    fed to a transformation object, represented by a blue rectangle. Right: We also
    build a yellow transformer for the car counts.'
  prefs: []
  type: TYPE_NORMAL
- en: The third step is to give our data to the transform objects again, but this
    time, we tell them to apply the transformation they have already computed. The
    result is a new set of data that has been transformed to the range [0,1]. [Figure
    10-16](#figure10-16) shows the idea.
  prefs: []
  type: TYPE_NORMAL
- en: Now we’re ready to learn. We give our transformed data to our learning algorithm
    and let it figure out the relationship between the inputs and the outputs, as
    shown schematically in [Figure 10-17](#figure10-17).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume that we’ve trained our system, and it’s doing a good job of predicting
    car counts from temperature data.
  prefs: []
  type: TYPE_NORMAL
- en: The next day, we deploy our system on a web page for the city managers. On the
    first night, the manager on duty measures a midnight temperature of −10° Celsius.
    She opens up our application, finds the input box for the temperature, types in
    −10, and hits the big “Predict Traffic” button.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10016](Images/F10016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-16: Each feature is modified by the transformation we previously
    computed for it. The output of the transformations goes into our learning system.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F10017](Images/F10017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-17: A schematic view of learning from our transformed features and
    targets'
  prefs: []
  type: TYPE_NORMAL
- en: Uh-oh, something’s gone wrong. We can’t just feed −10 into our trained system,
    because as [Figure 10-17](#figure10-17) shows, it’s expecting a number in the
    range of 0 to 1\. We need to transform the data somehow. The only way that makes
    sense is to apply the same transformation that we applied to the temperatures
    when we trained our system. For example, if in our original dataset −10 became
    the value 0.29, then if the temperature is −10 tonight, we should enter 0.29,
    not –10\.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s where we see the value of saving our transformation as an object. We
    can simply tell that object to take the same transformation that it applied to
    our training data and now apply it to this new piece of data. If −10 turned into
    0.29 during training, any new input of −10 turns into 0.29 during deployment as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s suppose that we correctly give the temperature 0.29 to the system, and
    it produces a traffic density of 0.32\. This corresponds to the value of some
    number of cars transformed by our car transformation. But that value is between
    0 and 1, because that was the range of the data we trained on representing car
    counts. How do we undo that transformation and turn it into a number of cars?
  prefs: []
  type: TYPE_NORMAL
- en: In any machine learning library, every transformation object comes with a routine
    to *invert*, or undo, its transformation, providing us with an *inverse transformation*.
    In this case, it inverts the normalizing transformation it’s been applying so
    far. If the object transformed 39 cars into the normalized value 0.32, then the
    inverse transformation turns the normalized value 0.32 back into 39 cars. This
    is the value we print out to the city manager. [Figure 10-18](#figure10-18) shows
    these steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10018](Images/F10018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-18: When we feed a new temperature to our system, we transform it
    using the transformation we computed for our temperature data, turning it into
    a number from 0 to 1\. The value that comes out is then run through the inverse
    of the transformation we computed for the car data, turning it from a scaled number
    into a number of cars.'
  prefs: []
  type: TYPE_NORMAL
- en: One thing that can seemingly go wrong here is if we get new samples outside
    of the original input range. Suppose we get a surprisingly cold temperature reading
    one night of −50° Celsius, which is far below the minimum value in our original
    data. The result is that the transformed value is a negative number, outside of
    our [0,1] range. The same thing can happen if we get a very hot night, giving
    us a positive temperature that transforms to a value greater than 1, which is
    again outside of [0,1].
  prefs: []
  type: TYPE_NORMAL
- en: Both situations are fine. Our desire for scaling inputs to [0,1] is to make
    training go as efficiently as possible, and also to keep numerical issues in check.
    Once the system is trained, we can give it any values we want as input, and it
    calculates a corresponding output. Of course, we still have to pay attention to
    our data. If the system predicts a negative number of cars for tomorrow, we don’t
    want to make plans based on that number.
  prefs: []
  type: TYPE_NORMAL
- en: Information Leakage in Cross-Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve seen how to build the transformation from the training set and then retain
    that transformation and apply it, unchanged, to all additional data. If we don’t
    follow this policy carefully, we can get *information leakage*, where information
    that doesn’t belong in our transformation accidentally sneaks (or leaks) into
    it, affecting the transformation. This means that we don’t transform the data
    the way we intend. Worse, this leakage can lead to the system getting an unfair
    advantage when it evaluates our test data, giving us an overinflated measure of
    accuracy. We may conclude that our system is performing well enough to be deployed,
    only to be disappointed when it has much worse performance when used for real.
  prefs: []
  type: TYPE_NORMAL
- en: Information leakage is a challenging problem because many of its causes can
    be subtle. As an example, let’s see how information leakage can affect the process
    of cross-validation, which we discussed in Chapter 8\. Modern libraries give us
    convenient routines that provide fast and correct implementations of cross-validation,
    so we don’t have to write our own code to do it. But let’s look under the hood
    anyway. We’ll see how a seemingly reasonable approach leads to information leakage,
    and then how we can fix it. Seeing this in action will help us get better at preventing,
    spotting, and fixing information leakage in our own systems and algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that in cross-validation, we set aside one fold (or section) of the starting
    training set to serve as a temporary validation set. Then we build a new learner
    and train it with the remaining data. When we’re done training, we evaluate the
    learner using the saved fold as the validation set. This means that each time
    through the loop, we have a new training set (the starting data without the samples
    in the selected fold). If we’re going to apply a transformation to our data, we
    need to build it from just the data that’s being used as the training set for
    that learner. We then apply that transformation to the current training set, and
    we apply *the same transformation* to the current validation set. The key thing
    to remember is that because in cross-validation we create a new training set and
    validation set each time through the loop, we need to build a new transformation
    each time through the loop as well.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see what happens if we do it incorrectly. [Figure 10-19](#figure10-19)
    shows our starting set of samples at the left. They’re analyzed to produce a transformation
    (shown by the red circle), which is then applied by a transformation routine (marked
    T). To show the change, we colored the transformed samples red, like the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: '![f10019](Images/f10019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-19: A *wrong* way to do cross-validation: building one transform
    based on all the original training data'
  prefs: []
  type: TYPE_NORMAL
- en: Then we enter the cross-validation process. Here the loop is “unrolled,” so
    we’re showing several instances of the training sessions, each associated with
    a different fold. Each time through the loop, we remove a fold, train on the remaining
    samples, and then test with the validation fold and create a score.
  prefs: []
  type: TYPE_NORMAL
- en: The problem here is that when we analyzed the input data to build the transformation,
    we included the data in every fold in our analysis. To see why this is a problem,
    let’s look more closely at what’s going on with a simpler and more specific scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that the transformation we want to apply is scaling all the features
    in the training set, as a group, to the range 0 to 1\. That is, we’ll do samplewise
    multivariate normalization. Let’s say that in the very first fold, the smallest
    and largest feature values are 0.01 and 0.99\. In the other folds, the largest
    and smallest values occupy smaller ranges. [Figure 10-20](#figure10-20) shows
    the range of data contained in each of the five folds. We’re going to analyze
    data in all the folds and build our transformation from that.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 10-20](#figure10-20), our dataset is shown at the left, split into
    five folds. Inside each box, we show the range of values in that fold, with 0
    at the left and 1 at the right. The top fold has features running from 0.01 to
    0.99\. The other folds have values that are well within this range. When we analyze
    all the folds as a group, the range of the first fold dominates, so we only stretch
    the whole dataset by a little bit.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10020](Images/F10020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-20: A *wrong*way to transform our data for cross-validation is to
    transform everything at once before the loop.'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s proceed with the cross-validation loop. Our input data is the stack
    of five transformed folds at the far right of [Figure 10-20](#figure10-20). Let’s
    start by extracting the first fold and setting it aside; then we can train with
    the rest of the data, and validate. But we’ve done something bad here, because
    *our training data’s transformation was influenced by the validation data*. This
    is a violation of our basic principle that we create the transform using only
    the values in the training data. But here we used what is now the validation data
    when we computed our transform. We say that information has *leaked* from this
    step’s validation data into the transformation parameters, where it doesn’t belong.
  prefs: []
  type: TYPE_NORMAL
- en: The right way to build the transformation for the training data is to remove
    the validation data from the samples, thenbuild the transformation from the remaining
    data, and then apply that transformation to both the training and validation data.
    [Figure 10-21](#figure10-21) shows this visually.
  prefs: []
  type: TYPE_NORMAL
- en: '![f10021](Images/f10021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-21: The proper way to transform our data for cross-validation is
    to first remove the fold samples and then compute the transformation from the
    data that remains.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we can apply that transformation to both the training set and the validation
    data. Note that here the validation data ends up way outside the range [0,1],
    which is no problem, because that data really is more extreme than the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: To fix our cross-validation process, we need to use this scheme in the loop
    and compute a new transformation for every training set. [Figure 10-22](#figure10-22)
    shows the right way to proceed.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10022](Images/F10022.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-22: The proper way to do cross-validation'
  prefs: []
  type: TYPE_NORMAL
- en: For each fold we want to use as a validation set, we analyze the starting samples
    with that fold removed and then apply the resulting transform to both the training
    set and the validation set in the fold. The different colors show that each time
    through the loop, we build and apply a different transformation.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve discussed information leakage in the context of cross-validation because
    it’s a great example of this tricky topic. Happily, the cross-validation routines
    in modern libraries all do the right thing, so we don’t have to worry about this
    problem ourselves when we use library routines. But this doesn’t take the responsibility
    off of us when we write our own code. Information leakage is often subtle, and
    it can creep into our programs in unexpected ways. It’s important that we always
    think carefully about possible sources of information leakage when we build and
    apply transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Shrinking the Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve been looking at ways to adjust the numbers in our data, and how to select
    the numbers that go into each transformation. Now let’s look at a different kind
    of transformation, designed not just to manipulate the data, but to actually compress
    it. We will literally create a new dataset that’s smaller than the original training
    set, typically by removing or combining features in each sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'This has two advantages: improved speed and accuracy when training. It stands
    to reason that the less data we need to process during training, the faster our
    training goes. By going faster, we mean we can pack in more learning in a given
    amount of time, resulting in a more accurate system.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a few ways to shrink our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we’ve collected features in our data that are redundant, irrelevant, or otherwise
    not helpful, then we should eliminate them so that we don’t waste time on them.
    This process is called *feature selection*, or sometimes, *feature filtering*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider an example where some data is actually superfluous. Suppose we’re
    hand-labeling images of elephants by entering their size, species, and other characteristics
    into a database. For some reason nobody can quite remember, we also have a field
    for the number of heads. Elephants have only one head, so that field’s going to
    be nothing but 1’s. So that data is not just useless, it also slows us down. We
    ought to remove that field from our data.
  prefs: []
  type: TYPE_NORMAL
- en: We can generalize this idea to removing features that are *almost* useless,
    that contribute very little, or that simply make the least contribution to getting
    the right answer. Let’s continue with our collection of elephant images. We have
    values for each animal’s height, weight, last known latitude and longitude, trunk
    length, ear size, and so on. But for this (imaginary) species, the trunk length
    and ear size may be closely correlated. If so, we can remove (or *filter out*)
    either one and still get the benefit of the information they each represent.
  prefs: []
  type: TYPE_NORMAL
- en: Many libraries offer tools that can estimate the impact of removing each field
    from a database. We can then use this information to guide us in simplifying our
    database and speeding our learning without sacrificing more accuracy than we’re
    willing to give up. Because removing a feature is a transformation, any features
    we remove from our training set must be removed from all future data as well.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another approach to reducing the size of our dataset is combining features,
    so one feature can do the work of two or more. This is called *dimensionality
    reduction*, where *dimensionality* refers to the number of features.
  prefs: []
  type: TYPE_NORMAL
- en: The intuition here is that some of the features in our data might be closely
    related without being entirely redundant. If the relationship is strong, we might
    be able to combine those two features into just one new one. An everyday example
    of this is the *body mass index (BMI)*. This is a single number that combines
    a person’s height and weight. Some measurements of a person’s health can be computed
    with just the BMI. For example, charts that help people decide if they need to
    lose weight can be conveniently indexed by age and BMI (CDC 2017).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a tool that automatically determines how to select and combine
    features to make the smallest impact on our results.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Principal component analysis (PCA)*, is a mathematical technique for reducing
    the dimensionality of data. Let’s get a visual feel for PCA by looking at what
    it does to our guitar data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-23](#figure10-23) shows our starting guitar data again. As before,
    the colors of the dots are just to make it easier to track them in the following
    figures as the data is manipulated, and don’t have any other meaning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F10023](Images/F10023.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-23: The starting data for our discussion of PCA'
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to crunch this two-dimensional data down to one-dimensional data.
    That is, we combine each set of paired x and y values to create a single new number
    based on both of them, just as BMI is a single number that combines a person’s
    height and weight.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by standardizing the data. [Figure 10-24](#figure10-24) shows this
    combination of setting each dimension to have a mean of 0 and a standard deviation
    of 1, as we saw before.
  prefs: []
  type: TYPE_NORMAL
- en: We already know that we’re going to try to reduce this 2D data to just 1D. To
    get a feel for the idea before we actually apply it, let’s go through the process
    with one key step missing, and then we’ll put that step back in.
  prefs: []
  type: TYPE_NORMAL
- en: To get started, let’s draw a horizontal line on the X axis. We’ll call this
    the *projection line*. Then we’ll *project*, or move, each data point to its closest
    spot on the projection line. Because our line is horizontal, we only need to move
    our points up or down to find their closest point on the projection line.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10024](Images/F10024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-24: Our input data after standardizing'
  prefs: []
  type: TYPE_NORMAL
- en: The results of projecting the data in [Figure 10-24](#figure10-24) onto a horizontal
    projection line are shown in [Figure 10-25](#figure10-25).
  prefs: []
  type: TYPE_NORMAL
- en: '![F10025](Images/F10025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-25: We project each data point of the guitar by moving it to its
    nearest point on the projection line. For clarity, we’re showing the path taken
    by only about 25 percent of the points.'
  prefs: []
  type: TYPE_NORMAL
- en: The results after all the points are processed is shown in [Figure 10-26](#figure10-26).
  prefs: []
  type: TYPE_NORMAL
- en: '![F10026](Images/F10026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-26: The result of [Figure 10-25](#figure10-25) after all the points
    have been moved to the projection line. Each point now is described only by its
    x coordinate, resulting in a one-dimensional dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: This is the one-dimensional dataset we were after, because the points only differ
    by their x value (the y value is always 0, so it’s irrelevant). But this would
    be a lousy way to combine our features, because all we did was throw away the
    y values. It’s like computing BMI by simply using the weight and ignoring the
    height.
  prefs: []
  type: TYPE_NORMAL
- en: To improve the situation, let’s include the step we skipped. Instead of using
    a horizontal projection line, we rotate the line around until it’s passing through
    the direction of maximum variance. Think of this as the line that, after projection,
    has the largest range of points. Any library routine that implements PCA finds
    this line for us automatically. [Figure 10-27](#figure10-27) shows this line for
    our guitar data.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10027](Images/F10027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-27: The thick black line is the line of maximum variance through
    our original data. This is our projection line.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we continue just like before. We project each point onto this projection
    line by moving it to its closest point on the line. As before, we do this by moving
    perpendicular to the line until we intersect it. [Figure 10-28](#figure10-28)
    shows this process.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10028](Images/F10028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-28: We project the guitar data onto the projection line by moving
    each point to its closest point on the line. For clarity, we’re showing the path
    taken by only about 25 percent of the points.'
  prefs: []
  type: TYPE_NORMAL
- en: The projected points are shown in [Figure 10-29](#figure10-29). Note that they
    all lie on the line of maximum variance that we found in [Figure 10-27](#figure10-27).
  prefs: []
  type: TYPE_NORMAL
- en: '![f10029](Images/f10029.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-29: The points of the guitar dataset projected onto the line of maximum
    variance'
  prefs: []
  type: TYPE_NORMAL
- en: For convenience, we can rotate this line of points to lie on the X axis, as
    shown in [Figure 10-30](#figure10-30). Now the y coordinate is irrelevant again,
    and we have 1D data that includes information about each point from both its original
    x and y values.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10030](Images/F10030.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-30: Rotating the points of [Figure 10-29](#figure10-29) into a horizontal
    position'
  prefs: []
  type: TYPE_NORMAL
- en: Although the straight line of points in [Figure 10-30](#figure10-30) looks roughly
    like the line of points in [Figure 10-26](#figure10-26), they’re different, because
    the points are distributed differently along the X axis. In other words, they
    have different values, because they were computed by projecting onto a tilted
    line, rather than a horizontal one. [Figure 10-31](#figure10-31) shows the two
    projections together. The PCA result is not just longer, but the points are also
    distributed differently.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10031](Images/F10031.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-31: Comparing the points created by projection to y = 0 in [Figure
    10-26](#figure10-26) (top) and the PCA algorithm in [Figure 10-30](#figure10-30)
    (bottom)'
  prefs: []
  type: TYPE_NORMAL
- en: All of the steps we just discussed are carried out automatically by a machine
    learning library when we call its PCA routine.
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of the 1D data created by this projection step is that every point’s
    single value (its x value) is a combination of the 2D data it started with. We
    reduced the dimensionality of our dataset by one dimension, but we did so while
    retaining as much information as we could. Our learning algorithms now only have
    to process one feature rather than two, so they’ll run faster. Of course, we’ve
    thrown some information away, so the accuracy may suffer. The trick to using PCA
    effectively is to choose dimensions that can be combined while still staying within
    the performance goals of our project.
  prefs: []
  type: TYPE_NORMAL
- en: If we have 3D data, we can imagine placing a plane in the middle of the cloud
    of samples and projecting our data down onto the plane. The library’s job is to
    find the best orientation of that plane. This takes our data from 3D to 2D. If
    we want to go all the way down to 1D, we can imagine projecting the data onto
    a line through the volume of points. In practice, we can use this technique in
    problems with any number of dimensions, where we may reduce the dimensionality
    of the data by tens or more.
  prefs: []
  type: TYPE_NORMAL
- en: 'The critical questions for this kind of algorithm include: How many dimensions
    should we try to compress? Which dimensions should be combined? How they should
    get combined? We usually use the letter *k* to stand for the number of dimensions
    remaining in our data after PCA has done its work. So, in our guitar example,
    *k* was 1\. We can call *k* a parameter of the algorithm, though usually we call
    it a hyperparameterof the entire learning system. As we’ve seen, the letter *k*
    is used for lots of different algorithms in machine learning, which is unfortunate;
    it’s important to pay attention to the context when we see references to *k*.'
  prefs: []
  type: TYPE_NORMAL
- en: Compressing too little means our training and evaluation steps are going to
    be inefficient, but compressing too much means we risk eliminating important information
    that we should have kept. To pick the best value for the hyperparameter *k*, we
    usually try out a few different values to see how they do and then pick the one
    that seems to work best. We can automate this search using the techniques of *hyperparameter
    searching* provided by many libraries.
  prefs: []
  type: TYPE_NORMAL
- en: As always, whatever PCA transformations we use to compress our training data
    must also be used in the same way for all future data.
  prefs: []
  type: TYPE_NORMAL
- en: PCA for Simple Images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Images are an important and special kind of data. Let’s apply PCA to a simple
    set of images.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-32](#figure10-32) shows a set of six images, perhaps drawn from
    a huge dataset of tens of thousands of such pictures. If these grayscale images
    are 1,000 pixels on a side, each contains 1,000 × 1,000, or 1 million, pixels.
    Is there any better way to represent them than with a million numbers each?'
  prefs: []
  type: TYPE_NORMAL
- en: '![F10032](Images/F10032.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-32: Six images we’d like to represent'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by observing that each image in [Figure 10-32](#figure10-32) can
    be re-created from the three images in [Figure 10-33](#figure10-33), each scaled
    by a different amount and then added together.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10033](Images/F10033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-33: We can create all six images in [Figure 10-32](#figure10-32)
    by scaling these three by different amounts and adding the results.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can reconstruct the first image in [Figure 10-32](#figure10-32)
    by adding 20 percent of the circle, 70 percent of the vertical box, and 40 percent
    of the horizontal box. We often call these scaling factors the *weights*. The
    weights for each of the six starting images are shown in [Figure 10-34](#figure10-34).
  prefs: []
  type: TYPE_NORMAL
- en: '![F10034](Images/F10034.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-34: The weights to use when scaling the three images in [Figure 10-33](#figure10-33)
    to recover the images in [Figure 10-32](#figure10-32)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-35](#figure10-35) shows the process of scaling and combining the
    components to recover the first original image.'
  prefs: []
  type: TYPE_NORMAL
- en: In general, we can represent any image of this type with the three weights of
    the component images that make the best match. To reconstruct any of the input
    images, we need the three simpler pictures (1 million values each) plus the three
    numbers for that specific image. If we had 1,000 images, storing each one would
    take a total of 1,000 megabytes. But using this compressed form, we need a total
    of only 3.001 megabytes.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10035](Images/F10035.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-35: Recovering the first image in [Figure 10-32](#figure10-32) by
    scaling the images in [Figure 10-33](#figure10-33)'
  prefs: []
  type: TYPE_NORMAL
- en: For these simple images, it’s easy to find the components as three geometric
    shapes. But when we have more realistic pictures, this won’t generally be possible.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that we can use the projection technique we discussed earlier.
    Instead of projecting a collection of points to create a new set of points on
    a line, we can project a collection of images to create a new image. This is a
    more abstract process than the one we followed with the guitar points, but the
    concept is the same. Let’s get a feeling for how PCA handles images by skipping
    the mechanics and focusing on the results.
  prefs: []
  type: TYPE_NORMAL
- en: Consider again the six starting images of [Figure 10-32](#figure10-32). Remember
    that these are grayscale images, not vector drawings. Let’s ask PCA to find a
    grayscale image that comes closest to representing all the images, in the same
    way that our diagonal line came closest to representing all the points in our
    guitar. Then we can represent each starting image as a sum of this image, scaled
    by an appropriate amount, plus whatever is left over. [Figure 10-36](#figure10-36)
    shows this process.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10036](Images/F10036.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-36: Running PCA on the images from [Figure 10-32](#figure10-32)'
  prefs: []
  type: TYPE_NORMAL
- en: The starting images from 10-32 are shown at the top of [Figure 10-36](#figure10-36).
  prefs: []
  type: TYPE_NORMAL
- en: Now we ask PCA to find an image that corresponds to the line in [Figure 10-28](#figure10-28).
    That is, an image that, in some sense, captures something from all of the inputs.
    Let’s say it found the picture at the left in [Figure 10-36](#figure10-36), showing
    the two bars and the circle all in black and overlaid. We’ll call this the *shared
    image*, which isn’t a formal term but is useful here.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now represent each image at the top of [Figure 10-36](#figure10-36) as
    a combination of a scaled version of the shared image, and some other image. To
    do this, we find the lightest pixel in each input image, and scale the shared
    image to that intensity. That scaling factor is shown at the top of the copies
    of the common image in the middle row, where the copies have been scaled in intensity
    by that amount. If we subtract each scaled common image from the source image
    above it in the figure, we get their difference. We could write this as “source
    – common = difference,” or equivalently, “source = common + difference,” which
    is shown in the figure.
  prefs: []
  type: TYPE_NORMAL
- en: We can then run PCA again, this time on the bottom row of [Figure 10-36](#figure10-36).
    Again, it projects these six images to create a new image that is the best match
    for all of them. As before, we can represent each picture as the sum of a scaled
    version of what’s common, plus whatever is left. [Figure 10-37](#figure10-37)
    shows the idea. In this demonstration, we’re supposing that PCA created an image
    of the two overlapping boxes as the best matching image.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10037](Images/F10037.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-37: Running PCA on the bottom row of [Figure 10-36](#figure10-36)'
  prefs: []
  type: TYPE_NORMAL
- en: Something interesting happened in two images on the bottom row. Let’s look at
    the second column from the left. The image we want to match on the top row is
    a circle and horizontal box, but we’re trying to match it with a scaled pair of
    crossed boxes. To match the top image, we need to add in some of the circle, but
    subtract the vertical box that we just introduced. That just means setting the
    corresponding data in the bottom image to negative values. These are perfectly
    valid numbers to place into our data, though we have to be careful if we try to
    display this image directly. If we reconstruct the top image by adding up the
    two images beneath it, the negative values in the red region in the bottom row
    cancel out the positive values in the vertical box in the middle row, so the sum
    of these images matches the circle and horizontal box at the top. The same reasoning
    applies to the horizontal box in the rightmost column.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-38](#figure10-38) summarizes the two steps we’ve seen so far.'
  prefs: []
  type: TYPE_NORMAL
- en: We took only two steps here, but we can repeat this process dozens or hundreds
    of times.
  prefs: []
  type: TYPE_NORMAL
- en: To represent each starting image, we only need the collection of common images
    and the weight we assigned to each. Since the common images are shared by all
    the images, we can consider them a shared resource. Then each image can be completely
    described by a reference to this shared resource, and the list of weights to be
    applied to the common images.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10038](Images/F10038.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-38: Representing each starting image (top row) as the sum of two
    scaled component images (second and third rows), plus whatever’s left (bottom
    row)'
  prefs: []
  type: TYPE_NORMAL
- en: Each of the common images is called a *component*, and the one we create at
    every step is the *principal* component (since it’s the best of all possible components,
    in the same way that the line in [Figure 10-28](#figure10-28) was the best line).
    We find these principal components by analyzing the input images. Hence, the name
    *principal component analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: The more components we include, the more accurately each reconstructed image
    will match its original. We usually aim to produce enough components so that each
    reconstructed image has all the qualities we care about in its original.
  prefs: []
  type: TYPE_NORMAL
- en: In this discussion, most of the weights were positive, but we also saw some
    component images that should be subtracted, not added, and thus they produce weights
    with negative values. This is so that the final pixels, when all the components
    are summed together, have the desired values.
  prefs: []
  type: TYPE_NORMAL
- en: How well did we do with just two component images? [Figure 10-39](#figure10-39)
    shows the original six images and our reconstructed images using the sum of the
    middle two rows of [Figure 10-38](#figure10-38).
  prefs: []
  type: TYPE_NORMAL
- en: The matches aren’t perfect, but they’re a good start, particularly for just
    two components. So, we have a common pool of two images (requiring 1 million numbers
    each), and then each image itself can be described with just two numbers. The
    beauty of this scheme is that our algorithms never need to see the common images.
    We just need those for finding the weights that describe each input image (and
    to reconstruct the images, if we want). As far as the learning algorithm is concerned,
    each image is described by only two numbers. This means our algorithms consume
    less memory and run more quickly.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10039](Images/F10039.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-39: Our starting six images (top), and the reconstructed images from
    [Figure 10-35](#figure10-35)c (bottom)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We skipped a step in this example: normally when we use PCA, we standardize
    all the images first. That’s included in our next example.'
  prefs: []
  type: TYPE_NORMAL
- en: PCA for Real Images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The images in the last section were contrived for simplicity. Now we’ll apply
    PCA to real pictures.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin with the six pictures of huskies shown in [Figure 10-40](#figure10-40).
    To make our processing easier to see, these images are only 45 × 64 pixels. These
    were aligned by hand so that the eyes and nose are in about the same location
    in each image. This way each pixel in each image has a good chance of representing
    the same part of a dog as the corresponding pixel in the other images. For instance,
    a pixel just below the center is likely to part of a nose, one near the upper
    corners is likely to be an ear, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![f10040](Images/f10040.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-40: Our starting set of huskies'
  prefs: []
  type: TYPE_NORMAL
- en: A database of six dogs isn’t a lot of training data, so let’s enlarge our database
    using the idea of *data augmentation,* a common strategy for *amplifying* or *enlarging*
    a dataset. In this case, let’s run through our set of six images in random order
    over and over. Each time through, we’ll pick an image, make a copy, randomly shift
    it horizontally and vertically up to 10 percent on each axis, rotate it up to
    five degrees clockwise or counterclockwise, and maybe flip it left to right. Then
    we append that transformed image to our training set. [Figure 10-41](#figure10-41)
    shows the results of the first two passes through our six dogs. We used this technique
    of creating variations to build a training set of 4,000 dog images.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10041](Images/F10041.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-41: Each row shows a set of new images created by shifting, rotating,
    and perhaps horizontally flipping each of our input images.'
  prefs: []
  type: TYPE_NORMAL
- en: Since we’d like to run PCA on these images, our first step is to standardize
    them. This means we analyze the same pixel in each of our 4,000 images and adjust
    the collection to have zero mean and unit variance (Turk and Pentland 1991). The
    standardized versions of our first six generated dogs are shown in [Figure 10-42](#figure10-42).
  prefs: []
  type: TYPE_NORMAL
- en: '![F10042](Images/F10042.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-42: Our first six huskies after standardization'
  prefs: []
  type: TYPE_NORMAL
- en: Since 12 images fit nicely into a figure, let’s begin by arbitrarily asking
    PCA to find the 12 images that, when added together with appropriate weights,
    best reconstruct the input images. Each of the projections (or components) found
    by PCA is technically known as an *eigenvector*, from the German *eigen* meaning
    “own” (or roughly “self”), and *vector* from the mathematical name for this kind
    of object. When we create eigenvectors of particular types of things, it’s common
    to create a playful name by combining the prefix eigen with the object we’re processing.
    Hence, [Figure 10-43](#figure10-43) shows our 12 *eigendogs*.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10043](Images/F10043.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-43: The 12 eigendogs produced by PCA'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the eigendogs tells us a lot about how PCA is analyzing our images.
    The first eigendog is a big smudge that is darker roughly where most of the dogs
    appear in the image. This is the single image that comes closest to approximating
    every input image. The second eigendog gives us refinements to the first, capturing
    some of the left-right shading differences.
  prefs: []
  type: TYPE_NORMAL
- en: The next eigendog provides additional detail, and so it goes through all 12
    eigendogs. So, the first eigendog captures the broadest, most common features,
    and each additional eigendog lets us recover a little more detail.
  prefs: []
  type: TYPE_NORMAL
- en: PCA is able not only to create the eigendogs of [Figure 10-43](#figure10-43),
    but also to take as input any picture, and tell us the weight to apply to each
    eigendog image so that, when the weighted images are added together, we get the
    best approximation to the input image.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how well we can recover our original images by combining these 12
    eigendogs with their corresponding weights. [Figure 10-44](#figure10-44) shows
    the weights that PCA found for each input image. We create the reconstructed dogs
    by scaling each eigendog image from [Figure 10-43](#figure10-43) with its corresponding
    weight, and then adding the results together.
  prefs: []
  type: TYPE_NORMAL
- en: '![f10044](Images/f10044.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-44: Reconstructing our original inputs from a set of 12 eigendogs.
    Top row: The reconstructed dogs. Bottom row: The weights applied to the eigendogs
    of [Figure 10-43](#figure10-43) to build the image directly above. Notice that
    the vertical scales on the bottom row are not all the same.'
  prefs: []
  type: TYPE_NORMAL
- en: The recovered dogs in [Figure 10-44](#figure10-44) are not great. We’ve asked
    PCA to represent all 4,000 images in our training set with just 12 pictures. It
    did its best, but these results are pretty blurry. They do seem to be on the right
    track, though.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try using 100 eigendogs. The first 12 eigendog images look just like those
    in [Figure 10-43](#figure10-43), but then they get more complicated and detailed.
    The results of reconstructing our first set of 6 dogs are shown in [Figure 10-45](#figure10-45).
  prefs: []
  type: TYPE_NORMAL
- en: '![F10045](Images/F10045.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-45: Reconstructing our original inputs from a set of 100 eigendogs'
  prefs: []
  type: TYPE_NORMAL
- en: That’s better! They’re starting to look like dogs. But it seems that 100 eigendogs
    is still not enough.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s crank up our number of eigendogs to 500 and try again. [Figure 10-46](#figure10-46)
    shows the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10046](Images/F10046.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-46: Reconstructing our original inputs from a set of 500 eigendogs'
  prefs: []
  type: TYPE_NORMAL
- en: These are looking pretty great. They are all easily recognized as the 6 standardized
    dogs in [Figure 10-42](#figure10-42). They’re not perfect, but considering that
    we’re adding together different amounts of 500 shared images, we’ve done a fine
    job of matching the original images. There’s nothing special about these first
    6 images. If we look at any of the 4,000 images in our database, they all look
    this good. We could keep increasing the number of eigendogs, and the results would
    continue to improve, with the images getting increasingly sharper and less noisy.
  prefs: []
  type: TYPE_NORMAL
- en: In each plot of the weights, the eigendog images that get the most weight are
    the ones at the start, which capture the big structures. As we work our way down
    the list, each new eigendog is generally weighted a little less than the one before,
    so it contributes less to the overall result.
  prefs: []
  type: TYPE_NORMAL
- en: The value of PCA here is not that we can make images that look just like the
    starting set, but rather, that we can use the eigendogs’ representation to reduce
    the amount of data our deep learning system has to process. This is illustrated
    in [Figure 10-47](#figure10-47). Our set of input dogs goes into PCA, which generates
    a set of eigendogs. Then each dog we’d like to classify goes into PCA again, which
    gives us the weights for that image. Those are the values that go into the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, rather than training our categorizer on all of the
    pixels from each image, we can train it on just that image’s 100 or 500 weights.
    The categorizer never sees a full image of a million pixels. It never even sees
    the eigendogs. It just gets a list of the weights for each image, and that’s the
    data it uses for analysis and prediction during training. When we want to classify
    a new image, we provide just its weights, and the computer gives us back a class.
    This can save a lot of computation, which translates to a savings in time, and
    perhaps increased quality of final results.
  prefs: []
  type: TYPE_NORMAL
- en: '![F10047](Images/F10047.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-47: In the top row we first we use PCA to build a set of eigendogs,
    and then in the bottom row we find the weights for each input to our classifier,
    which only uses those weights to find the input’s class.'
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the data we hand the classifier is not each input image, but its
    weights. The classifier then proceeds to work out which breed of dog it’s looking
    at based just on those weights. Often we need only a few hundred weights to represent
    input samples with many thousands, or even millions, of features.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we looked at ways to prepare data. We saw that it’s important
    to inspect our data before we do anything with it and make sure that it’s clean.
    Once our data is clean, we can transform it to better fit our learning algorithms
    in a number of ways. These transformations are built from the training data only.
    It’s important to remember that any transforms we apply to the training data must
    then be applied to every additional sample we give to our algorithm, from validation
    and test data to deployment data provided by real-world users.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll dig into classifiers and survey some of the most
    important algorithms for the job.
  prefs: []
  type: TYPE_NORMAL
