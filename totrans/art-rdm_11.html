<html><head></head><body>
<h2 class="h2" id="ch11"><span epub:type="pagebreak" id="page_295"/><strong><span class="big">11</span><br/>COMPUTER SCIENCE ALGORITHMS</strong></h2>
<div class="image1"><img alt="Image" src="../images/common.jpg"/></div>
<p class="noindent">While everything we’ve looked at so far can be called a “randomized algorithm,” in computer science the phrase refers to two broad categories of algorithms—the subject of this chapter.</p>
<p class="indent">A <em>randomized algorithm</em> employs randomness as part of its operation. The algorithm succeeds in accomplishing its goal, either by producing the correct answer quickly, but sometimes not, or by running rapidly with some probability of returning a false or nonoptimal result.</p>
<p class="indent">We’ll begin by defining the two broad categories of randomized algorithms with examples. Next, we’ll learn about estimating the number of animals in a population. Following that, we’ll learn how to demonstrate that a number is a prime to any desired level of certainty while avoiding the brute force approach of searching for all possible divisors. We’ll end with randomized Quicksort, the textbook example of a randomized algorithm.</p>
<h3 class="h3" id="ch00lev1_67"><strong>Las Vegas and Monte Carlo</strong></h3>
<p class="noindent">Las Vegas and Monte Carlo are locations famously associated with gambling, that is, with games of chance dependent on randomness and probability. However, when computer scientists refer to Las Vegas and Monte Carlo, they are (usually) referring to the two main types of randomized algorithms.</p>
<p class="indent"><span epub:type="pagebreak" id="page_296"/>A <em>Las Vegas algorithm</em> always returns a correct result for its input in a random amount of time; that is, how long the algorithm takes to execute isn’t deterministic, but the output is correct.</p>
<p class="indent">On the other hand, a <em>Monte Carlo algorithm</em> offers no assurance that its output is correct, but the runtime is deterministic. There is a nonzero probability that the output isn’t correct, but for a practical Monte Carlo algorithm, this probability is small. Most algorithms we’ve encountered, including swarm intelligence and evolutionary algorithms, are Monte Carlo algorithms. Las Vegas algorithms can be transformed into Monte Carlo algorithms by allowing them to exit before locating the correct output.</p>
<p class="indent">The first example we’ll investigate is a sorting algorithm that is, at our discretion, a Las Vegas or Monte Carlo algorithm. The second is a Monte Carlo algorithm for verifying matrix multiplication.</p>
<h4 class="h4" id="ch00lev2_88"><em><strong>Permutation Sort</strong></em></h4>
<p class="noindent">A <em>permutation</em> is a possible arrangement of a set of items. If there are <em>n</em> items in the set, there are <em>n</em>! = <em>n</em>(<em>n</em> – 1)(<em>n</em> – 2) . . . 1 possible permutations. For example, if the set consists of three things, say <em>A</em> = {1, 2, 3}, then there are six possible permutations:</p>
<p class="center">{1, 2, 3}, {1, 3, 2}, {2, 1, 3}, {2, 3, 1}, {3, 1, 2}, {3, 2, 1}</p>
<p class="noindent">Notice that one permutation sorts the items from smallest to largest. Therefore, if given a vector of unsorted numbers, we might use a sort algorithm that generates permutations until finding the one that sorts the items. While we can implement this deterministically, we can also use random permutations with the hope that we might stumble across the correct ordering before testing too many candidate permutations. The <em>permutation sort</em> algorithm (also known as <em>bogosort</em> or <em>stupid sort</em>) implements this idea.</p>
<p class="indent">Run the file <em>permutation_sort.py</em> with no arguments:</p>
<pre class="pre">permutation_sort &lt;items&gt; &lt;limit&gt; [&lt;kind&gt; | &lt;kind&gt; &lt;seed&gt;]

  &lt;items&gt; - number of items in the list
  &lt;limit&gt; - number of passes maximum (0=Las Vegas else Monte Carlo)
  &lt;kind&gt;  - randomness source
  &lt;seed&gt;  - seed value</pre>
<p class="indent">The code generates a random vector of integers in [0, 99] and sorts it by trying random permutations up to <span class="literal">limit</span>. To score each permutation, the code returns the fraction of pairs of elements that are out of order, where <em>a</em> &gt; <em>b</em> for <em>a</em> at index <em>i</em> and <em>b</em> at index <em>i</em> + 1. If the array is sorted, the score is zero.</p>
<p class="indent">If <span class="literal">limit</span> is zero, the algorithm runs until it finds the correct permutation, which depends on the number of possible permutations. As the number of permutations increases (<em>n</em>!), the runtime rapidly increases if we insist on trying until we succeed. In this way, a <span class="literal">limit</span> of 0 turns <em>permutation_sort.py</em> into a Las Vegas algorithm.</p>
<p class="indent"><span epub:type="pagebreak" id="page_297"/>For example, to run <em>permutation_sort.py</em> as a Las Vegas algorithm, use:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 permutation_sort.py 6 0 minstd 42</span>
sorted: 0  25  44  57  65  96  (268 iterations)</pre>
<p class="noindent">The code found the proper order of elements after testing 268 of the possible 6! = 720 permutations. Changing the randomness source from <span class="literal">minstd</span> to <span class="literal">pcg64</span> sorts in 59 iterations while <span class="literal">mt19937</span> uses 20. We set the limit to 0 to make the code run until success, but the number of permutations tested was often far less than the maximum.</p>
<p class="indent">If we switch to Monte Carlo:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 permutation_sort.py 6 100 minstd 42</span>
partially: 0  57  25  44  65  96  (score = 0.16667, 100 iterations)</pre>
<p class="noindent">we get a partially sorted array with a score &gt; 0. Because of the fixed randomness source and seed, we know we need 268 iterations to sort the array:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 permutation_sort.py 6 268 minstd 42</span>
sorted: 0  25  44  57  65  96  (268 iterations)</pre>
<p class="indent"><a href="ch011.xhtml#ch011list01">Listing 11-1</a> shows the main loop in <em>permutation_sort.py</em>.</p>
<pre class="pre">v = np.array([int(rng.random()*100) for i in range(N)], dtype="uint8")
k = 0
score = Score(v)
while (score != 0) and (k &lt; limit):
    k += 1
    i = np.argsort(rng.random(len(v)))
    s = Score(v[i])
    if (s &lt; score):
        score = s
        v = v[i]</pre>
<p class="list" id="ch011list01"><em>Listing 11-1: The main loop in</em> permutation_sort.py</p>
<p class="indent">We create the vector (<span class="literal">v</span>), along with an initial <span class="literal">score</span>. The main loop, <span class="literal">while</span>, runs until the score is zero or the <span class="literal">limit</span> is exceeded. If Las Vegas, we set <span class="literal">limit</span> to a huge number to play the odds that we’ll find the true ordering long before that many trials.</p>
<p class="indent">The body of the <span class="literal">while</span> loop creates a random ordering of <span class="literal">v</span> and calculates the score. Whenever it finds a lower score, the code reorders <span class="literal">v</span> to return at least a partially ordered vector should the limit be reached; however, this is not strictly necessary.</p>
<p class="indent">The remainder of the file displays the results or calculates the score (<a href="ch011.xhtml#ch011list02">Listing 11-2</a>).</p>
<pre class="pre">def Score(arg):
    n = 0
    for i in range(len(arg)-1):
        if (arg[i] &gt; arg[i+1]):
            <span epub:type="pagebreak" id="page_298"/>n += 1
    return n / len(arg)</pre>
<p class="list" id="ch011list02"><em>Listing 11-2: Scoring a permutation</em></p>
<p class="indent">Let’s plot the mean number of permutations tested as a function of the number of items to sort, <em>permutation_sort_plot.py</em>, which plots the mean and standard error for 10 calls to <em>permutation_sort.py</em> for <em>n</em> in [2, 10]. The result is <a href="ch011.xhtml#ch011fig01">Figure 11-1</a>.</p>
<div class="image"><img alt="Image" id="ch011fig01" src="../images/11fig01.jpg"/></div>
<p class="figcap"><em>Figure 11-1: The mean number of permutations (in millions) tested as a function of the number of items</em></p>
<p class="indent"><a href="ch011.xhtml#ch011fig01">Figure 11-1</a> illustrates <em>combinatorial explosion</em>, which is the rapid growth in a problem’s runtime or resource use as a function of the size of its input. Permutation sort works decently when sorting lists of up to nine items; any more and the complexity explodes.</p>
<p class="indent">We also see this effect in <em>permutation_sort_plot.py</em>’s output:</p>
<pre class="pre"> 2:  0.127855 +/- 0.002026
 3:  0.128128 +/- 0.001737
 4:  0.129859 +/- 0.002469
 5:  0.131369 +/- 0.002483
 6:  0.136637 +/- 0.003704
 7:  0.172775 +/- 0.008236
 8:  0.534369 +/- 0.081601
 9:  1.987567 +/- 0.488691
10: 44.133984 +/- 10.929158</pre>
<p class="indent">The output shows, as a function of <em>n</em>, the mean (± SE) time in seconds to sort a vector of that size. After seven elements, runtimes quickly increase.</p>
<p class="indent">Combinatorial explosion is the bane of many otherwise useful algorithms, often reaching a point where many lifetimes of the universe are insufficient to find the correct output.</p>
<span epub:type="pagebreak" id="page_299"/>
<p class="indent">Permutation sort is tied closely to the factorial, which is why we’re getting these results:</p>
<div class="tablec1">
<table class="table">
<colgroup>
<col style="width:30%"/>
<col style="width:30%"/>
<col style="width:40%"/>
</colgroup>
<tbody>
<tr>
<td>2! = 2</td>
<td class="borderl">5! = 120</td>
<td class="borderl">8! = 40,320</td>
</tr>
<tr>
<td>3! = 6</td>
<td class="borderl">6! = 720</td>
<td class="borderl">9! = 362,880</td>
</tr>
<tr>
<td>4! = 24</td>
<td class="borderl">7! = 5,040</td>
<td class="borderl">10! = 3,628,800</td>
</tr>
</tbody>
</table>
</div>
<p class="noindent">The factorial grows at a tremendous rate. If we want to sort 20 items, we have</p>
<p class="center">20! = 2,432,902,008,176,640,000</p>
<p class="noindent">permutations to check. At 1 millisecond per permutation, we’d need over 77 million years of computing time to check them all. This doesn’t mean permutation sort couldn’t, by pure chance, sort 20 items in less than a second, but the probability is exceedingly low. This is the paradox of randomized algorithms.</p>
<h4 class="h4" id="ch00lev2_89"><em><strong>Matrix Multiplication</strong></em></h4>
<p class="noindent">I have three matrices, <em><strong>A</strong></em>, <em><strong>B</strong></em>, and <em><strong>C</strong></em>. We’ll use bold, uppercase letters to represent matrices. Does <em><strong>AB</strong></em> = <em><strong>C</strong></em>? How can we know?</p>
<h5 class="h5"><strong>Introducing Matrix Multiplication</strong></h5>
<p class="noindent">First, let’s ensure we’re on the same page regarding matrix multiplication. A <em>matrix</em> is a 2D array of numbers, so the matrices here might be:</p>
<div class="image1"><img alt="Image" src="../images/f0299-03.jpg"/></div>
<p class="noindent">These are 2×2 matrices with two rows and two columns. If the number of rows equals the number of columns, we’re working with <em>square matrices</em>. However, the number of rows and columns need not match; for example, we might have a matrix of 3×5 or 1,000×13. The latter case is typical in machine learning, where rows represent observations and columns represent features associated with those observations. An <em>n</em>×1 matrix is a <em>column vector</em>, while a 1×<em>n</em> matrix is a <em>row vector</em>.</p>
<p class="indent">Asking whether <em><strong>AB</strong></em> = <em><strong>C</strong></em> implies that we know how to find <em><strong>AB</strong></em>. In NumPy, in order to multiply two 2D arrays, we multiply each corresponding element (<a href="ch011.xhtml#ch011list03">Listing 11-3</a>).</p>
<pre class="pre">&gt;&gt;&gt; <span class="codestrong1">A = np.array([[1,2],[3,4]])</span>
&gt;&gt;&gt; <span class="codestrong1">B = np.array([[1,0],[2,3]])</span>
&gt;&gt;&gt; <span class="codestrong1">A*B</span>
array([[ 1,  0],
       [ 6, 12]])</pre>
<p class="list" id="ch011list03"><em>Listing 11-3: Multiplying element-wise in NumPy</em></p>
<p class="noindent">Unfortunately, multiplying matrices is more involved. We begin by checking that the number of columns of the first matrix equals the number of rows of <span epub:type="pagebreak" id="page_300"/>the second. If not, then we can’t multiply the matrices. Therefore, to multiply an <em>n</em>×<em>m</em> matrix by a <em>u</em>×<em>v</em> matrix requires <em>m</em> = <em>u</em>. If this is true, we can multiply to produce an <em>n</em>×<em>v</em> result. The square matrices in this section automatically satisfy this requirement.</p>
<p class="indent">The matrix multiplication process requires multiplying each column of the second matrix by the rows of the first matrix, where the elements of the column multiply the corresponding elements of the row. We then sum these products to produce a single output value. For example, in symbols, multiplying two 2×2 matrices returns a new 2×2 matrix:</p>
<div class="image1"><img alt="Image" src="../images/f0300-01.jpg"/></div>
<p class="noindent">We’re indexing matrices from 0, as we would NumPy arrays. However, many math books index from 1 so that the first element of the first row of matrix <em><strong>A</strong></em> is denoted <em>a</em><sub>11</sub>, not <em>a</em><sub>00</sub>.</p>
<p class="indent">Mathematically, if <em><strong>A</strong></em> is an <em>n</em> × <em>m</em> matrix and <em><strong>B</strong></em> is an <em>m</em> × <em>p</em> matrix, then the elements of <em><strong>C</strong></em> = <em><strong>AB</strong></em>, an <em>n</em> × <em>p</em> matrix, are:</p>
<div class="image1" id="ch11equ01"><img alt="Image" src="../images/f0300-02.jpg"/></div>
<p class="noindent">Remember that matrix multiplication does not commute; in general, <em><strong>AB</strong></em> ≠ <em><strong>BA</strong></em>. The sum over <em>k</em> in <a href="ch011.xhtml#ch11equ01">Equation 11.1</a> illustrates why: the single index accesses by row for <em><strong>A</strong></em> and by column for <em><strong>B</strong></em> so that swapping the order of <em><strong>A</strong></em> and <em><strong>B</strong></em> means different elements of the matrices are mixed.</p>
<p class="indent">The sum in <a href="ch011.xhtml#ch11equ01">Equation 11.1</a> uses index variable <em>k</em> with two more implied sums over all values of <em>i</em> and <em>j</em> to fill in the output matrix, <em><strong>C</strong></em>. These observations point toward an implementation: matrix multiplication becomes a triple loop indexing elements of 2D arrays.</p>
<p class="indent"><a href="ch011.xhtml#ch011list04">Listing 11-4</a> translates the loops of <a href="ch011.xhtml#ch11equ01">Equation 11.1</a> into code.</p>
<pre class="pre">def mmult(A,B):
    n,m = A.shape
    p = B.shape[1]
    C = np.zeros((n,p), dtype=A.dtype)
    for i in range(n):
        for j in range(p):
            for k in range(m):
                C[i,j] += A[i,k]*B[k,j]
    return C</pre>
<p class="list" id="ch011list04"><em>Listing 11-4: Naive matrix multiplication</em></p>
<p class="indent">We’ll use this implementation even though NumPy supports matrix multiplication natively in several ways, for example, via the <span class="literal">@</span> operator. To understand why, we’ll learn how computer scientists measure algorithm performance.</p>
<h5 class="h5"><span epub:type="pagebreak" id="page_301"/><strong>Introducing Big O Notation</strong></h5>
<p class="noindent">Computer scientists characterize the resource use of an algorithm by comparing the algorithm’s performance as input size increases to a similar function that captures how the algorithm’s resource use changes as the input grows. Here, resource refers to either memory or time. For example, an <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em>) algorithm linearly increases the memory used as the size of the input, <em>n</em>, increases. A linear function can be written as <em>y</em> = <em>mx</em> + <em>b</em> for input <em>x</em>, but in big O notation, we ignore multiplicative and constant factors so that <em>y</em> = <em>x</em> is functionally the same as <em>x</em> gets very large.</p>
<p class="indent">The matrix multiplication code in <a href="ch011.xhtml#ch011list04">Listing 11-4</a> contains a triply nested loop. If the input matrices are square (<em>n</em>×<em>n</em>), then <span class="literal">I</span> = <span class="literal">J</span> = <span class="literal">K</span> = <em>n</em>. Each loop runs <em>n</em> times, making the innermost loop run <em>n</em> times for every increment of the next outer loop, which must run <em>n</em> times to increment the outermost loop. Therefore, the total number of operations required to multiply two <em>n</em>×<em>n</em> matrices scales as <em>n</em><sup>3</sup>. The time needed to create the output matrix, <span class="literal">C</span>, and evaluate the first two lines of the function doesn’t alter the essential character of the function—namely that it takes <em>n</em><sup>3</sup> passes through the three loops.</p>
<p class="indent">A computer scientist would therefore write that <a href="ch011.xhtml#ch011list04">Listing 11-4</a> is an <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>3</sup>) algorithm and an “<em>n</em> cubed” implementation. In general, we want algorithms that scale as <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em>) or better. As <em>n</em> increases, the work required by the algorithm scales linearly or, better still, sublinearly like <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(log <em>n</em>) or <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em> log <em>n</em>). In other words, a plot of the work as a function of <em>n</em> is a straight line. Ideally, we want <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(1) algorithms that run in constant time regardless of the size of their input, but that isn’t always possible. An algorithm that runs in <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2</sup>) is often tolerable, but <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>3</sup>) is suitable only for small values of <em>n</em>.</p>
<p class="indent">Note that <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em>), <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2</sup>), and <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>3</sup>) are all powers of <em>n</em>. Such algorithms are known as <em>polynomial time</em> algorithms. We never want algorithms that run in <em>superpolynomial time</em>, with a runtime (or resource use) such that no polynomial tracks it. For example, an algorithm running in <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(2<em><sup>n</sup></em>) time is an <em>exponential time</em> algorithm, and its resource use grows dramatically with the size of the input at a rate no polynomial can match. Worse still is the permutation sort we experimented with previously; it’s an <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em>!) algorithm that runs in <em>factorial time</em>. To see how factorial time is worse than exponential time, make a plot comparing 2<em><sup>n</sup></em> and <em>n</em>! for <em>n</em> = [1, 8].</p>
<p class="indent">Matrix multiplication as in <a href="ch011.xhtml#ch011list04">Listing 11-4</a> is <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>3</sup>). Our goal is to quickly check whether <em><strong>AB</strong></em> = <em><strong>C</strong></em> when given three matrices. We first multiply <em><strong>A</strong></em> and <em><strong>B</strong></em> and then check whether each element of the result matches the corresponding element in <em><strong>C</strong></em>. The multiplication is <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>3</sup>) and the check runs in <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2</sup>) time because we have to examine each element. As the cube grows much faster than the square, the overall naive algorithm runs in essentially <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>3</sup>) time. Let’s see if we can do better.</p>
<h5 class="h5"><strong>Introducing Freivalds’ Algorithm</strong></h5>
<p class="noindent">In 1977, Latvian computer scientist Rūsiņš Freivalds invented a randomized algorithm that correctly answers the question of whether <em><strong>AB</strong></em> = <em><strong>C</strong></em> with high probability, yet runs in <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2</sup>) time.</p>
<p class="indent"><span epub:type="pagebreak" id="page_302"/>For the following, we’ll assume that <em><strong>A</strong></em>, <em><strong>B</strong></em>, and <em><strong>C</strong></em> are <em>n</em>×<em>n</em> matrices. The algorithm works for non-square matrices, but this restriction makes things easier to follow.</p>
<p class="indent">The algorithm itself is straightforward:</p>
<ol>
<li class="noindent">Pick a random <em>n</em>-element vector, <em><strong>r</strong></em> = {0, 1}<em><sup>n</sup></em>, that is, a random vector of 0s and 1s.</li>
<li class="noindent">Calculate <em><strong>D</strong></em> = <em><strong>A</strong></em>(<em><strong>Br</strong></em>) – <em><strong>Cr</strong></em>. (Note: the parentheses matter.)</li>
<li class="noindent">If all elements of <em><strong>D</strong></em> are zero, claim “yes,” <em><strong>AB</strong></em> = <em><strong>C</strong></em>; otherwise, claim “no.”</li>
</ol>
<p class="indent">At first glance, Freivalds’ algorithm doesn’t look like it will help. However, recall how matrix multiplication works. The expression <em><strong>Br</strong></em> is multiplying an <em>n</em>×<em>n</em> matrix by an <em>n</em>×1 vector, which returns an <em>n</em>×1 vector. The next multiplication by <em><strong>A</strong></em> returns another <em>n</em>×1 vector. Likewise, <em><strong>Cr</strong></em> is also an <em>n</em>×1 vector. At no point is a full <em>n</em>×<em>n</em> matrix multiplication happening. As <em>n</em> grows, the savings in the number of calculations grows all the faster. Freivalds’ algorithm runs in <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2</sup>) time—a considerable improvement over the <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>3</sup>) runtime of the naive algorithm.</p>
<p class="indent">Multiplying <em><strong>B</strong></em> by <em><strong>r</strong></em> is the equivalent of selecting a random subset of <em><strong>B</strong></em>’s columns and adding their value across the rows. For example:</p>
<div class="image1"><img alt="Image" src="../images/f0302-01.jpg"/></div>
<p class="indent">The algorithm is betting that examining random elements of the three matrices will, if they are equal, result in <em><strong>D</strong></em> being a vector of all zeros more often than <em><strong>D</strong></em> being all zeros by chance. An analysis of the probabilities involved, which we won’t cover, demonstrates that the probability of <em><strong>D</strong></em> being all zeros given <em><strong>AB</strong></em> ≠ <em><strong>C</strong></em> is less than or equal to 1/2.</p>
<p class="indent">If the probability of one calculation involving a randomly selected <em><strong>r</strong></em> returning the wrong answer is at most 1/2, then two random vectors (if we run the algorithm twice) have a probability of returning the wrong answer of at most (1/2)(1/2) = 1/4. Here the wrong answer is an output of “yes” when in fact <em><strong>AB</strong></em> ≠ <em><strong>C</strong></em>.</p>
<p class="indent">Each application of the algorithm is independent of any previous application. For independent events, like the flip of a fair coin, probabilities multiply, so <em>k</em> runs of Freivalds’ algorithm implies that the probability of a false “yes” result is 1/2<em><sup>k</sup></em> or less. This means we can be as confident of the result as we like by running the algorithm multiple times.</p>
<p class="indent">The algorithm will always return “yes” when <em><strong>AB</strong></em> = <em><strong>C</strong></em>, meaning it is <em>one sided</em>—an error in the output happens only if <em><strong>AB</strong></em> ≠ <em><strong>C</strong></em>. In a <em>two-sided</em> error, the algorithm could be wrong in either case, with some probability.</p>
<h5 class="h5"><span epub:type="pagebreak" id="page_303"/><strong>Testing Freivalds’ Algorithm</strong></h5>
<p class="noindent">Let’s give the algorithm a try using <em>freivalds.py</em>, which generates 1,000 random triplets of <em>n</em>×<em>n</em> matrices, with <em>n</em> given on the command line. In all cases, <em><strong>AB</strong></em> ≠ <em><strong>C</strong></em>, so we report failures as a fraction of 1,000.</p>
<p class="indent">Run <em>freivalds.py</em> like so:</p>
<pre class="pre">freivalds &lt;N&gt; &lt;mode&gt; &lt;reps&gt; [&lt;kind&gt; | &lt;kind&gt; &lt;seed&gt;]

  &lt;N&gt;     - matrix size (always square)
  &lt;mode&gt;  - 0=Freivalds', 1=naive
  &lt;reps&gt;  - reps of Freivalds' (ignored for others)
  &lt;kind&gt;  - randomness source
  &lt;seed&gt;  - seed</pre>
<p class="noindent">The first argument is the dimensionality of the matrices. The second decides whether to use the naive algorithm that calculates <em><strong>AB</strong></em> – <em><strong>C</strong></em> or Freivalds’. The third is the number of times to repeat the test with random <em><strong>r</strong></em> vectors. We’ll use this option shortly to track the error rate. As usual, the other arguments enable any randomness source and a seed to repeat the same sequence of random matrices.</p>
<p class="indent">For example:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 freivalds.py 3 0 1 mt19937 19937</span>
0.08598161 0.132</pre>
<p class="noindent">tells us that testing 1,000 3×3 matrices using Freivalds’ algorithm once each took some 0.09 seconds and failed 13.2 percent of the 1,000 tests.</p>
<p class="indent">To use the naive algorithm, change the 0 to 1 on the command line:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 freivalds.py 3 1 1 mt19937 19937</span>
0.05456829 0.000</pre>
<p class="indent">As expected, there are no failures because the complete calculation always catches when <em><strong>AB</strong></em> ≠ <em><strong>C</strong></em>. While the naive algorithm seems to run faster than Freivalds’, this is an illusion; as <em>n</em> increases, the two diverge.</p>
<p class="indent">Failing 13 percent of the time when checking 3×3 matrices isn’t too inspiring. Let’s repeat the test, but check twice instead of once:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 freivalds.py 3 0 2 mt19937 19937</span>
0.14664984 0.016</pre>
<p class="noindent">Now we fail only 1.6 percent of the tests at the expense of nearly doubling the running time. Let’s try four tests instead of two:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 freivalds.py 3 0 4 mt19937 19937</span>
0.26030445 0.000</pre>
<p class="noindent">With four tests, Freivalds’ algorithm is 1,000 out of 1,000.</p>
<p class="indent">Freivalds’ algorithm is probabilistic. The likelihood of error decreases quickly as matrix size increases. To see this effect, alter the matrix size while <span epub:type="pagebreak" id="page_304"/>fixing the repetitions at 1. By the time <em>n</em> = 11, the error is generally below 0.1 percent.</p>
<p class="indent">It makes sense that the error rate goes down with matrix size. The probability that a random selection of values sum by accident to two equal values (<em><strong>A</strong></em>(<em><strong>Br</strong></em>) and <em><strong>Cr</strong></em>) should decrease as the number of values summed increases.</p>
<p class="indent">Let’s explore running time as a function of <em>n</em>. Run <em>freivalds_plots.py</em> to produce the graphs in <a href="ch011.xhtml#ch011fig02">Figure 11-2</a>.</p>
<div class="image"><img alt="Image" id="ch011fig02" src="../images/11fig02.jpg"/></div>
<p class="figcap"><em>Figure 11-2: Comparing Freivalds’ running time to the naive algorithm as a function of matrix size (left) and plotting Freivalds’ running time alone to show the</em> <img alt="Image" class="inline" src="../images/c0301-01.jpg"/><em>(</em>n<sup><em>2</em></sup><em>) complexity—note the</em> y<em>-axis range (right)</em></p>
<p class="indent">On the left of <a href="ch011.xhtml#ch011fig02">Figure 11-2</a>, we see the growth in running time between Freivalds’ and the naive algorithm as the size of the matrices increases. The naive algorithm, <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>3</sup>), grows substantially faster than Freivalds’ <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2</sup>), shown by itself on the right.</p>
<p class="indent">The combination of performance gain and decreasing likelihood of error as <em>n</em> increases makes Freivalds’ algorithm particularly nice. Yes, it’s probabilistic, but in the places where it’s most desirable (large <em>n</em>), it’s also most likely to be correct.</p>
<p class="indent">Before examining <em>freivalds.py</em>, I have a confession. We can do matrix multiplication better than <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>3</sup>), especially for matrices with <em>n</em> &gt; 100. Volker Strassen’s 1969 matrix multiplication algorithm has a runtime of about <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>log<sub>2</sub> 7</sup>) <em>≈</em> <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2.807</sup>), which is slightly better than the naive algorithm. NumPy, based on the BLAS library, makes use of Strassen’s algorithm, which is why we didn’t use NumPy in this section. However, <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2</sup>) is better than <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2.807</sup>), so Freivalds’ algorithm is still useful, even with Strassen matrix multiplication.</p>
<div class="box">
<p class="box-title"><strong>GALACTIC ALGORITHMS</strong></p>
<p class="box-para">There are matrix multiplication algorithms with even better asymptotic behavior than Strassen’s algorithm. The current best have complexity <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2.373</sup>) or so. However, these algorithms are, in practice, completely useless. The seeming contradiction has to do with Big O notation, which shows the overall behavior <span epub:type="pagebreak" id="page_305"/>but ignores multiplicative factors and constants. This means that an algorithm running in 10<em>n</em><sup>3</sup> time is the same as one running in 10,000<em>n</em><sup>3</sup> + 10,000 time. Both scale as <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>3</sup>), but in practice, the first is more likely to be helpful.</p>
<p class="box-para">Matrix multiplication algorithms that beat Strassen’s algorithm in overall complexity, like the <em>Coppersmith–Winograd</em> algorithm, have constants so large that the algorithm becomes practical only once <em>n</em> is some number far larger than anything computers can currently handle, if ever.</p>
<p class="box-para">Such algorithms have been christened <em>galactic algorithms</em> by Kenneth W. Regan. We cannot effectively use galactic algorithms in practice even if they are “the best” in terms of asymptotic behavior. While these algorithms are of theoretical importance, they won’t show up in our toolkits any time soon.</p>
</div>
<h5 class="h5"><strong>Exploring the Code</strong></h5>
<p class="noindent"><a href="ch011.xhtml#ch011list05">Listing 11-5</a> contains the code implementing Freivalds’ algorithm. The <span class="literal">mmult</span> function is in <a href="ch011.xhtml#ch011list04">Listing 11-4</a>. The <span class="literal">array_equal</span> function asks whether the absolute maximum of the difference between <em><strong>A</strong></em>(<em><strong>Br</strong></em>) and <em><strong>Cr</strong></em> is below <span class="literal">eps</span>, and returns <span class="literal">True</span> if so.</p>
<pre class="pre">def array_equal(a,b, eps=1e-7):
    return np.abs(a-b).max() &lt;= eps

k = 0
m = 1000
s = time.time()
for i in range(m):
    A = 100*rng.random(N*N).reshape((N,N))
    B = 100*rng.random(N*N).reshape((N,N))
    C = A@B + 0.1*rng.random(N*N).reshape((N,N))
    if (mode == 0):
        t = True
        for j in range(reps):
            r = (2*rng.random(N)).astype("uint8").reshape((N,1))
            t &amp;= array_equal(mmult(A,mmult(B,r)), mmult(C,r))
    else:
        t = array_equal(mmult(A,B), C)
    k += 1 if t else 0
print("%0.8f %0.3f" % (time.time()-s, k/m))</pre>
<p class="list" id="ch011list05"><em>Listing 11-5: Freivalds’ algorithm</em></p>
<p class="indent">The outer <span class="literal">for</span> loop executes 1,000 trials using a randomly selected set of matrices each time. <em><strong>C</strong></em> is such that it never equals <em><strong>AB</strong></em>, so every call to <span class="literal">array_equal</span> should return <span class="literal">False</span>.</p>
<p class="indent">The body of the outer <span class="literal">for</span> loop either multiplies <em><strong>A</strong></em> and <em><strong>B</strong></em> directly (<span class="literal">mode==1</span>), or uses Freivalds’ algorithm by generating a random binary <span epub:type="pagebreak" id="page_306"/>vector, <span class="literal">r</span>. Note that <span class="literal">r</span> is reshaped to be an <em>n</em>×1 column vector, as required for matrix multiplication.</p>
<p class="indent">The inner <span class="literal">for</span> loop applies Freivalds’ repeatedly (<span class="literal">reps</span>) each time, AND-ing the result with <span class="literal">t</span>. The AND operation means that after <span class="literal">reps</span> tests with different <span class="literal">r</span> vectors each time, the only way <span class="literal">t</span> is still true is if all tests give a wrong result. Each test should see <span class="literal">array_equal</span> return <span class="literal">False</span> because <em><strong>AB</strong></em> ≠ <em><strong>C</strong></em> by design. Once <span class="literal">t</span> becomes <span class="literal">False</span>, it remains <span class="literal">False</span> for all remaining tests, so even one correct output from <span class="literal">array_equal</span> causes <span class="literal">t</span> to have the expected value.</p>
<p class="indent">If <span class="literal">t</span> is still <span class="literal">True</span> after the inner loop, then the trial failed and we increment <span class="literal">k</span>. After all trials, we print the total runtime and the fraction of the 1,000 trials that failed.</p>
<p class="indent">Freivalds’ algorithm is a Monte Carlo algorithm because it might, with a probability we can minimize, produce a false output and claim <em><strong>AB</strong></em> = <em><strong>C</strong></em> when it isn’t true.</p>
<p class="indent">Let’s turn to a different type of question for the next section: to get an estimate of the number of things in a collection, is it necessary to count them individually?</p>
<h3 class="h3" id="ch00lev1_68"><strong>Counting Animals</strong></h3>
<p class="noindent">Ecologists often want to know how many animals of a particular species live in an area, though counting each one is often impossible. Enter <em>mark and recapture</em>, a strategy for estimating population size from a small sample.</p>
<p class="indent">In mark and recapture, the ecologist first goes into the field and captures <em>n</em> specimens, which they then mark and release. A short while later, they revisit the field and capture animals again until they get at least one that is marked. If they capture <em>K</em> animals to get <em>k</em> that are marked, they now have everything necessary to estimate the full population size, <em>N</em>. They do this by using ratios.</p>
<p class="indent">Initially, the ecologist marked <em>n</em> of the <em>N</em> animals, meaning the fraction of the total population marked is <em>n</em>/<em>N</em>. The recapture phase netted <em>k</em> marked animals out of <em>K</em>. Assuming no births, deaths, or migrations, the two ratios should be approximately equal, so solving for <em>N</em> gives:</p>
<div class="image1"><img alt="Image" src="../images/f0306-01.jpg"/></div>
<p class="noindent">This equation results in the <em>Lincoln-Petersen population estimate</em>, hence <em>N<sub>L</sub></em>.</p>
<p class="indent">A slightly less biased estimate of the population (or so it’s claimed) comes from the <em>Chapman population estimate</em>:</p>
<div class="image1"><img alt="Image" src="../images/f0306-02.jpg"/></div>
<p class="noindent">Finally, we have a Bayesian approach to mark and recapture:</p>
<div class="image1"><img alt="Image" src="../images/f0306-03.jpg"/></div>
<p class="noindent">This approach requires at least three marked animals in the recapture group to avoid dividing by zero.</p>
<p class="indent"><span epub:type="pagebreak" id="page_307"/>Let’s compare these three different estimates for the same population size with <em>mark_recapture.py</em>:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 mark_recapture.py</span>
mark_recapture &lt;pop&gt; &lt;mark&gt; &lt;reps&gt; [&lt;kind&gt; | &lt;kind&gt; &lt;seed&gt;]
  &lt;pop&gt;  - population size (e.g. 1000)
  &lt;mark&gt; - number to mark (&lt; pop)
  &lt;reps&gt; - number of repetitions 
  &lt;kind&gt; - randomness source
  &lt;seed&gt; - seed</pre>
<p class="indent">The code simulates marking and recapturing by randomly marking a specified number of animals before recapturing a fraction of the population to count how many are marked. Let’s run the code a few times to get a feel for the output. We’ll fix the true population size at 1,000 and initially mark 100, or 10 percent. Setting the repetitions to 1 takes a single sampling, which is similar to what an ecologist might do in practice. We get:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 mark_recapture.py 1000 100 1 mt19937 11</span>
Lincoln-Petersen population estimate = 1250
Chapman population estimate          = 1132
Bayesian population estimate         = 1633

&gt; <span class="codestrong1">python3 mark_recapture.py 1000 100 1 mt19937 12</span>
Lincoln-Petersen population estimate = 666
Chapman population estimate          = 636
Bayesian population estimate         = 753

&gt; <span class="codestrong1">python3 mark_recapture.py 1000 100 1 mt19937 13</span>
Lincoln-Petersen population estimate = 833
Chapman population estimate          = 783
Bayesian population estimate         = 980</pre>
<p class="indent">The estimates vary widely from run to run, as we might expect from a randomized algorithm. While the Lincoln-Petersen and Chapman estimates are generally low, the Bayesian estimates are closer to or even exceed the population size.</p>
<p class="indent">Using a single repetition is akin to attempting to generalize from a single collected data point, so let’s increase the repetitions:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 mark_recapture.py 1000 100 25 mt19937 11</span>
Lincoln-Petersen population estimate = 1028.4713 +/-  78.4623
Chapman population estimate          =  940.0367 +/-  61.6982
Bayesian population estimate         = 1345.2015 +/- 166.3078

&gt; <span class="codestrong1">python3 mark_recapture.py 1000 100 25 mt19937 12</span>
Lincoln-Petersen population estimate = 1052.0985 +/-  61.3198
Chapman population estimate          =  963.4317 +/-  49.9620
Bayesian population estimate         = 1345.9986 +/- 108.4192

<span epub:type="pagebreak" id="page_308"/>&gt; <span class="codestrong1">python3 mark_recapture.py 1000 100 25 mt19937 13</span>
Lincoln-Petersen population estimate = 1112.8340 +/-  80.5759
Chapman population estimate          = 1009.5146 +/-  63.3742
Bayesian population estimate         = 1485.2546 +/- 169.0492</pre>
<p class="indent">The output now reflects the mean and standard errors for 25 repetitions, providing a better idea of how the estimates behave. The Lincoln-Petersen and Chapman results are closer to the actual population size, while the Bayesian estimate is consistently too high. The standard errors are illustrative as well, with the Bayesian standard error being larger than the others, indicating more trial-to-trial variation.</p>
<p class="indent">Try experimenting with different combinations of population size and number of animals initially marked.</p>
<p class="indent"><a href="ch011.xhtml#ch011fig03">Figure 11-3</a> presents three somewhat crowded graphs.</p>
<div class="image"><img alt="Image" id="ch011fig03" src="../images/11fig03.jpg"/></div>
<p class="figcap"><em>Figure 11-3: The three mark and recapture estimators as a function of the true population size and the fraction of that size initially marked. The plots show the signed deviation from the true population size: Lincoln-Peterson (top left), Chapman (top right), and Bayesian (bottom).</em></p>
<p class="indent">In the top-left graph in <a href="ch011.xhtml#ch011fig03">Figure 11-3</a>, each of the seven plotted lines represents a different true population size from 100 to 10,000. The <em>x</em>-axis indicates the fraction of the true population marked by the ecologist on their first trip to the field. The value plotted is the median signed difference between the Lincoln-Petersen estimate for that combination of population size <span epub:type="pagebreak" id="page_309"/>and fraction initially marked and the true population size. If the curve is above zero, the estimate is too low; below zero, it’s too high. In other words, the graph shows <em>N</em><sub>true</sub> – <em>N</em><sub>est</sub>, so underestimating is a positive difference and overestimating is negative. The remaining two graphs show the same information for the Chapman (top-right) and Bayesian (bottom) estimators.</p>
<p class="indent">For populations above 1,000, the Lincoln-Petersen estimator is generally useful when initially marking more than 10 percent of the population, which may not be feasible in practice. However, for small populations, the estimator requires some 20 percent of the population to be marked to achieve reliability. One might use a simulation to generate a correction function for the Lincoln-Petersen estimator based on the suspected population size and the number of animals initially marked.</p>
<p class="indent">The Chapman estimator consistently underestimates the true population size to the point where one questions its utility compared to the Lincoln-Petersen estimate. However, the underestimate is relatively consistent for populations above 1,000, so again, a fudge factor might be derived from a simulation.</p>
<p class="indent">The Bayesian estimator’s performance is quite different. It consistently overestimates the actual population, converging to the true population value only when the population becomes large and the percent initially marked is also significant (at least 15 percent). In practice, these conditions are unlikely to be met.</p>
<p class="indent"><a href="ch011.xhtml#ch011fig03">Figure 11-3</a> is the output of <em>mark_recapture_range.py</em>, which can be understood by examining the relevant parts of <em>mark_recapture.py</em> in <a href="ch011.xhtml#ch011list06">Listing 11-6</a>.</p>
<pre class="pre">lincoln = []
chapman = []
bayes = []

for j in range(nreps):
    pop = np.zeros(npop, dtype="uint8")
    idx = np.argsort(rng.random(npop))[:nmark]
    pop[idx] = 1

    K = nmark
    while (True):
        idx = np.argsort(rng.random(npop))[:K]
        k = pop[idx].sum()
        if (k &gt; 2):
            break
        K += 5

    lincoln.append(nmark*K/k)
    chapman.append((nmark+1)*(K+1)/(k+1) - 1)
    bayes.append((nmark-1)*(K-1)/(k-2))</pre>
<p class="list" id="ch011list06"><em>Listing 11-6: Simulating mark and recapture estimates</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_310"/>The outer <span class="literal">for</span> loop over <span class="literal">nreps</span> handles the trials. For each trial, we create a population (<span class="literal">pop</span>) vector where <span class="literal">npop</span> is the population size from the command line. The vector is initially zero as we haven’t marked any animals yet.</p>
<p class="indent">The next two lines represent the ecologist’s first field trip. The <span class="literal">argsort</span> trick, coupled with keeping only the first <span class="literal">nmark</span> elements of the sort order, sets <span class="literal">idx</span> to the indices of <span class="literal">pop</span> that the ecologist has initially captured and marked (<span class="literal">pop[idx]=1</span>).</p>
<p class="indent">The second code paragraph represents the recapture phase in which the ecologist returns to the field and captures as many animals as were initially marked (<span class="literal">K</span>). We represent the captured animals by the <span class="literal">K</span> indices in <span class="literal">idx</span> as assigned in the inner <span class="literal">while</span> loop.</p>
<p class="indent">Marks are binary, so the sum of the selected elements of <span class="literal">pop</span> is the number of marked animals, <span class="literal">k</span>. If <span class="literal">k</span> is 3 or greater, break out of the <span class="literal">while</span> loop. Otherwise, increase <span class="literal">K</span> by five and try again. The final code paragraph calculates the three estimates of the true population size for this trial.</p>
<p class="indent">When the outer <span class="literal">for</span> loop exits, we have three vectors of estimates for the given population size and number initially marked. The remainder of <em>mark_recapture.py</em> displays the results. Given the simulation results, my money’s on the Lincoln-Petersen estimator.</p>
<p class="indent">Let’s move on from counting to the mathematically important task of primality testing.</p>
<h3 class="h3" id="ch00lev1_69"><strong>Testing Primality</strong></h3>
<p class="noindent">Prime numbers—integers evenly divisible by only themselves and one—are greatly beloved by number theorists. Primes have fascinated humanity since antiquity, and significant computing power is currently devoted to locating <em>Mersenne primes</em> of the form 2<em><sup>p</sup></em> – 1, where <em>p</em> is a prime number.</p>
<p class="indent">The largest known primes are Mersenne primes. As of this writing, the largest known Mersenne prime, discovered in 2018, is:</p>
<p class="center"><em>M</em><sub>82,589,933</sub> = 2<sup>82,589,933</sup> − 1</p>
<p class="noindent"><em>M</em><sub>82,589,933</sub> is a 24,862,048-digit number. Mersenne primes are sometimes denoted by their number and not their exponent. Therefore, <em>M</em><sub>82,589,933</sub>, the 51st Mersenne prime, might be given as <em>M</em><sub>51</sub>.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>To contribute in locating Mersenne primes, visit</em> <a href="https://www.mersenne.org">https://www.mersenne.org</a> <em>and sign up for the Great Internet Mersenne Prime Search.</em></p>
</div>
<p class="indent">How do we know if <em>n</em> is a prime number? The definition gives us a natural starting point for a primality testing algorithm: if the only numbers that evenly divide <em>n</em> (resulting in no remainder) are 1 and <em>n</em>, then <em>n</em> is a prime.</p>
<p class="indent">Let’s turn this definition into an algorithm. The brute force approach is to test every number that could be a factor of <em>n</em>. In practice, this means testing every integer up to <img alt="Image" class="inline" src="../images/f0311-01.jpg"/> because any factor of <em>n</em> larger than <img alt="Image" class="inline" src="../images/f0311-01.jpg"/> will necessarily be multiplied by some number less than <img alt="Image" class="inline" src="../images/f0311-01.jpg"/>, and will be caught before reaching <img alt="Image" class="inline" src="../images/f0311-01.jpg"/>.</p>
<p class="indent"><span epub:type="pagebreak" id="page_311"/>When contemplating numbers comprising nearly 25 million digits, the amount of work involved increases dramatically. And if <em>n</em> is prime, must we test <em>every</em> integer up to <img alt="Image" class="inline" src="../images/f0311-01.jpg"/>?</p>
<p class="indent">The <em>Miller-Rabin test</em> is a fast, randomized algorithm to decide whether a positive integer, <em>n</em>, is prime. However, to understand the Miller-Rabin test, we need to know a bit about modular arithmetic.</p>
<h4 class="h4" id="ch00lev2_90"><em><strong>Modular Arithmetic</strong></em></h4>
<p class="noindent">We’re used to the set of integers, denoted ℤ from the German for number. Integers are unbounded and extend infinitely in both directions from zero. If we restrict the range to the set {0, 1, 2, 3}, we can define arithmetic operations over this set by wrapping around as needed. Adding works as expected if the sum is less than 4: 1 + 1 = 2 and 2 + 1 = 3. However, if the sum exceeds 4, we wrap around. For example, 2+3 = 1 because 2+3 = 5, and we subtract 4 from 5 to get 1. Another way to view these operations is to apply the modulo operator after each addition to return the remainder after dividing by 4. For example, 5 mod 4 = 1.</p>
<p class="indent">Pierre Fermat, a 17th-century French mathematician, realized that if <em>n</em> is a prime number, then</p>
<p class="center"><em>a</em><sup><em>n</em> – 1</sup> ≡ 1 (mod <em>n</em>), 0 &lt; <em>a</em> &lt; <em>n</em></p>
<p class="noindent">where ≡ means:</p>
<p class="center"><em>a</em><sup><em>n</em> – 1</sup> mod <em>n</em> = 1 mod <em>n</em> = 1</p>
<p class="noindent">Great! We have a primality test: pick an integer 0 &lt; <em>a</em> &lt; <em>n</em>, raise it to the <em>n</em> – 1 power, divide by <em>n</em>, and see if the remainder is 1. If it is, then <em>n</em> is a prime number, so the algorithm works and identifies <em>n</em> as a prime. However, some composite numbers also pass the test for many values of <em>a</em>, meaning this alone isn’t sufficient to prove <em>n</em> is a prime. If this test fails, then <em>n</em> is definitely not a prime.</p>
<p class="indent">The Miller-Rabin test combines Fermat’s test with another fact—if <em>n</em> is prime, the following is likely also true</p>
<div class="image1"><img alt="Image" src="../images/f0311-03.jpg"/></div>
<p class="noindent">for some <em>r</em> in [0, <em>s</em>) where <em>n</em> = 2<em><sup>s</sup>d</em> + 1 and <em>d</em> is odd. It’s likely true because there are sometimes <em>a</em> values satisfying the congruence even if <em>n</em> is composite. We’ll discuss these non-witness numbers shortly.</p>
<p class="indent">The first condition, Fermat’s test, is straightforward enough, but let’s unpack this second condition. We need to express <em>n</em> as 2<em><sup>s</sup>d</em> + 1 or, equivalently, as <em>n</em>–1 = 2<em><sup>s</sup>d</em>. For suitable choices of <em>s</em> and <em>d</em>, 2<em><sup>s</sup>d</em> is another way of writing the exponent in the Fermat condition.</p>
<p class="indent">All of the math in ≡ –1 (mod <em>n</em>) is modulo <em>n</em>, meaning the numbers are in the set {0, 1, 2, . . . , <em>n</em> – 1}, usually denoted as <span class="ent">ℤ</span><em><sub>n</sub></em>. We view a negative number as counting backward, so –1 ≡ <em>n</em> – 1.</p>
<p class="indent">The second condition checks to see if <em>x</em><sup>2</sup> ≡ –1 (mod <em>n</em>) for some <em>x</em>. The Miller-Rabin test uses a sequence of such values of <em>x</em>, looking for one that, when squared modulo <em>n</em>, gives –1 (that is, <em>n</em> – 1). The sequence begins with <span epub:type="pagebreak" id="page_312"/><em>r</em> = 0 and <em>d</em> as the exponent. The next check uses the square, which is the same as <em>r</em> = 1:</p>
<div class="image1"><img alt="Image" src="../images/f0312-01.jpg"/></div>
<p class="noindent">This is all modulo <em>n</em>. The next squaring returns <em>r</em> = 2, and so on.</p>
<p class="indent">If any of the sequence of such expressions is equivalent to <em>n</em> – 1, then <em>n</em> has a reasonably high probability of being a prime number. Otherwise, <em>n</em> is definitely <em>not</em> prime, and <em>a</em> is a <em>witness</em> to this fact.</p>
<h4 class="h4" id="ch00lev2_91"><em><strong>The Miller-Rabin Test</strong></em></h4>
<p class="noindent">Let’s put Miller-Rabin into code, as in <a href="ch011.xhtml#ch011list07">Listing 11-7</a>.</p>
<pre class="pre">def MillerRabin(n, rounds=5):
    if (n==2):
        return True
    if (n%2 == 0):
        return False

    s = 0
    d = n-1
    while (d%2 == 0):
        s += 1
        d //= 2

    for k in range(rounds):
        a = int(rng.random()) # [1,n-1]
        x = pow(a,d,n)
        if (x==1) or (x == n-1):
            continue
        b = False
        for j in range(s-1):
            x = pow(x,2,n)
            if (x == n-1):
                b = True
                break
        if (b):
            continue
        return False
    return True</pre>
<p class="list" id="ch011list07"><em>Listing 11-7: Miller-Rabin in code</em></p>
<p class="indent">The function <span class="literal">MillerRabin</span> accepts <em>n</em> and <span class="literal">rounds</span> with a default value of 5. The first code paragraph captures trivial cases. As half of all numbers are even, testing directly for 2 and evenness saves time.</p>
<p class="indent">The second code paragraph locates <em>s</em> and <em>d</em> so that <em>n</em> = 2<em><sup>s</sup>d</em> + 1. It’s always possible to find an <em>s</em> and <em>d</em> decomposition for any <em>n</em> (positive integer).</p>
<p class="indent"><span epub:type="pagebreak" id="page_313"/>For now, we’ll focus on the body of the outer <span class="literal">for</span> loop in the third paragraph, which implements a pass through the Miller-Rabin test for a randomly selected <em>a</em> and initial <em>x</em> value, <em>a<sup>d</sup></em> (mod <em>n</em>).</p>
<p class="indent">The built-in Python function, <span class="literal">pow</span>, computes exponents and accepts an optional third argument so that <span class="literal">pow(a,d,n)</span> efficiently implements <em>a<sup>d</sup></em> (mod <em>n</em>).</p>
<p class="indent">The following <span class="literal">if</span> checks for 1 or –1. If that’s the case, the Fermat test has passed, so this pass through the outer <span class="literal">for</span> loop is over.</p>
<p class="indent">Otherwise, the inner <span class="literal">for</span> loop initiates the sequence of successive squarings of <em>x</em> = <em>a<sup>d</sup></em> while looking for one equivalent to –1. If such a squaring is found, the inner loop breaks and the outer loop cycles; otherwise, <em>n</em> is composite and <span class="literal">MillerRabin</span> returns <span class="literal">False</span>.</p>
<p class="indent">When all rounds (the loop over <span class="literal">k</span>) are complete, and every test supports the notion that <em>n</em> is a prime, <span class="literal">MillerRabin</span> returns <span class="literal">True</span>.</p>
<p class="indent">The outer <span class="literal">for</span> loop applies the Miller-Rabin test repeated for randomly selected <em>a</em> values. Since an <em>a</em> value demonstrating <em>n</em> to be a composite number is a witness number, an <em>a</em> value that leads to a claim of prime when <em>n</em> is not prime is a <em>non-witness</em> number. It is never the case that all possible <em>a</em> values for a given <em>n</em> are non-witness numbers, so repeated applications of the outer loop body minimize the probability that a non-prime input will return <span class="literal">True</span>.</p>
<p class="indent">You’ll find <span class="literal">MillerRabin</span> in the file <em>miller_rabin.py</em>. It expects a number to test, the number of rounds (<em>a</em> values to try), and the randomness source:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 miller_rabin.py</span>
miller_rabin &lt;n&gt; &lt;rounds&gt; [&lt;kind&gt; | &lt;kind&gt; &lt;seed&gt;]
  &lt;n&gt;      - number to test
  &lt;rounds&gt; - number of rounds
  &lt;kind&gt;   - randomness source
  &lt;seed&gt;   - seed</pre>
<p class="noindent">For example:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 miller_rabin.py 73939133 1</span>
73939133 is probably prime
&gt; <span class="codestrong1">python3 miller_rabin.py 73939134 1</span>
73939134 is composite</pre>
<p class="indent">The output must be correct for these cases as 73,939,133 is a prime, and the closest two primes can be to each other is 2 away:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 miller_rabin.py 8675309 1</span>
8675309 is probably prime
&gt; <span class="codestrong1">python3 miller_rabin.py 8675311 1</span>
8675311 is probably prime</pre>
<p class="noindent">Both 8,675,309 and 8,675,311 are twin primes, so the test is correct.</p>
<p class="indent">Miller-Rabin always labels a prime a prime. Let’s explore when Miller-Rabin fails.</p>
<span epub:type="pagebreak" id="page_314"/>
<h4 class="h4" id="ch00lev2_92"><em><strong>Non-witness Numbers</strong></em></h4>
<p class="noindent">As mentioned, a witness number, <em>a</em>, testifies to the fact that <em>n</em> isn’t prime. Also, there are composite numbers for which the Miller-Rabin test fails if it selects a non-witness number as <em>a</em>.</p>
<p class="indent">We’ll force the Miller-Rabin algorithm to fail, probabilistically, using a composite number with a known set of non-witness numbers to see if we can detect the expected number of failures.</p>
<p class="indent">Our target is <em>n</em> = 65. As a multiple of 5, 65 is composite. There are 64 potential witness numbers, from 1 through 64. Of these potential witness numbers, it’s known that 8, 18, 47, 57, and 64 are non-witness numbers. If the Miller-Rabin test runs for one round and selects a non-witness number for <em>a</em>, it fails and claims that 65 is prime.</p>
<p class="indent">Because there are five non-witness numbers out of 64 possible, the Miller-Rabin test for a single round should fail about 5/64 = 7.8 percent of the time. I checked this by running <em>miller_rabin.py</em> 1,000 times and counting the number of times the output indicated a prime, which it did precisely 78 times, implying a failure rate of 78/1, 000 = 7.8 percent.</p>
<p class="indent">At worst, the Miller-Rabin single-round failure probability for arbitrary <em>n</em> is 1/4. Since each round is independent of the previous, running the test for <em>k</em> rounds means the worst possible failure probability is (1/4)<em><sup>k</sup></em> = 4<sup>–<em>k</em></sup>. However, for most <em>n</em> values, the actual failure probability is far less than this.</p>
<p class="indent">Let’s stick with 65. Knowing that its single-round failure rate is about 7.8 percent, running two rounds should fail (5/64)<sup>2</sup> ≈ 0.61 percent of the time. Running <em>miller_rabin.py</em> 20,000 times produced 131 failures, giving a failure rate of 131/20, 000 = 0.655 percent. Three rounds puts the failure rate at about 0.05 percent. We can achieve any desired precision by setting <em>k</em> high enough.</p>
<h4 class="h4" id="ch00lev2_93"><em><strong>Miller-Rabin Performance</strong></em></h4>
<p class="noindent">Let’s compare Miller-Rabin’s runtime performance to the brute force approach implemented in <em>brute_primes.py</em>. The code in <em>prime_tests.py</em> runs both Miller-Rabin and the brute force algorithm for the largest 1, 2, 3, . . . , 15-digit prime numbers. Recall, the brute force algorithm runs the longest when the input is a prime.</p>
<p class="indent">The largest single-digit prime is 7, while the largest 15-digit prime is 999,999,999,999,989. In <a href="ch011.xhtml#ch011fig04">Figure 11-4</a>, we plot the mean of five runs of <em>miller_rabin.py</em> and <em>brute_primes.py</em> for each prime to show how the runtime changed as the inputs grew.</p>
<span epub:type="pagebreak" id="page_315"/>
<div class="image"><img alt="Image" id="ch011fig04" src="../images/11fig04.jpg"/></div>
<p class="figcap"><em>Figure 11-4: Comparing Miller-Rabin to the brute force primality test</em></p>
<p class="indent">The runtime complexity of the brute force algorithm is <img alt="Image" class="inline" src="../images/f0315-01.jpg"/> while that of Miller-Rabin is <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(log<sup>3</sup> <em>n</em>). The brute force algorithm quickly becomes unmanageable, even though it’s sublinear, because <img alt="Image" class="inline" src="../images/f0315-02.jpg"/> and 1/2 &lt; 1.</p>
<p class="indent">Miller-Rabin is a Monte Carlo algorithm because it claims <em>n</em> is a prime when there’s a nonzero probability that it isn’t. If <em>n</em> truly is a prime, Miller-Rabin always correctly labels it, but it also calls some composite numbers prime regardless of the number of rounds. Therefore, Miller-Rabin’s false positive rate is nonzero, but its false negative rate is identically zero. In practice, however, increasing the number of rounds can make the false-positive rate as low as desired.</p>
<p class="indent">We have one more randomized algorithm to contemplate.</p>
<h3 class="h3" id="ch00lev1_70"><strong>Working with Quicksort</strong></h3>
<p class="noindent">Quicksort was developed by British computer scientist Tony Hoare in 1959 and is probably still the most widely used sorting algorithm. It’s NumPy’s default, for example.</p>
<p class="indent">If you take an undergraduate course in algorithms, you’ll almost assuredly run across Quicksort, as it’s easy to implement and understand, even if it’s recursive. While most courses focus on characterizing its runtime complexity, we’ll discuss the algorithm at a high level instead, and then run experiments on two versions: the standard nonrandom version and a randomized version.</p>
<p class="indent">Quicksort is a recursive, <em>divide-and-conquer</em> algorithm, meaning it calls itself on smaller and smaller versions of the problem until it encounters a base condition, at which point the implementation pieces everything back together to produce a sorted output.</p>
<p class="indent"><span epub:type="pagebreak" id="page_316"/>The algorithm is as follows:</p>
<ol>
<li class="noindent">If the input array is empty or has only one element, return it.</li>
<li class="noindent">Pick a <em>pivot</em> element, either the first in the array or at random.</li>
<li class="noindent">Separate the array into three subsets: those elements less than the pivot, those equal to the pivot, and those greater than the pivot.</li>
<li class="noindent">Return the concatenation of Quicksort called on the lower elements, the elements matching the pivot, and Quicksort called on the higher elements.</li>
</ol>
<p class="indent">Step 1 is the base condition. If the array is empty or contains a single element, it’s sorted. step 2 picks a pivot value, an array element we use in Step 3 to split the array into three parts: those less than, equal to, and greater than the pivot. Step 2 is where randomness comes into play. Non-random Quicksort always picks a specific element of the array, as it’s already assumed to be in random order. Randomized Quicksort, however, selects its pivot element at random. We’ll experiment with the subtle difference between nonrandom and random Quicksort.</p>
<p class="indent">Step 4 is the recursive part. The array is sorted if we merge the sorted lower partition with the same partition followed by the sorted higher partition. We sort the lower and higher partitions by using the sorting routine, that is, by calling Quicksort again. Each call on a portion of the array will, we assume, work with a smaller number of elements until we have single elements, the base condition of step 1.</p>
<p class="indent">Naive sorting methods, like bubble sort or gnome sort, run in <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2</sup>) time where <em>n</em> is the number of elements to sort. As we’ve learned, <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2</sup>) algorithms are acceptable for small <em>n</em> values, but quickly become unmanageable as <em>n</em> grows. Quicksort’s average runtime complexity is <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em> log <em>n</em>), which grows at a much slower rate. This is why Quicksort is still widely used over 50 years after its introduction.</p>
<p class="indent">While Quicksort’s <em>average</em> complexity is <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em> log <em>n</em>), if the array passed to Quicksort is already mostly or completely sorted, the complexity becomes <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2</sup>), which is no better than bubble sort. This happens if the array is in order or reverse order. Let’s find out whether randomized Quicksort can help us here.</p>
<h4 class="h4" id="ch00lev2_94"><em><strong>Running Quicksort in Python</strong></em></h4>
<p class="noindent">The file <em>Quicksort.py</em> implements Quicksort twice. The first implementation uses a random pivot (<span class="literal">QuicksortRandom</span>), and the second implementation always uses the first element of the array as the pivot (<span class="literal">Quicksort</span>). The functions are in <a href="ch011.xhtml#ch011list08">Listing 11-8</a>.</p>
<pre class="pre">def QuicksortRandom(arr):
    if (len(arr) &lt; 2):
        return arr
    pivot = arr[np.random.randint(0, len(arr))]
    low  = arr[np.where(arr &lt; pivot)]
    <span epub:type="pagebreak" id="page_317"/>same = arr[np.where(arr == pivot)]
    high = arr[np.where(arr &gt; pivot)]
    return np.hstack((QuicksortRandom(low), same, QuicksortRandom(high)))

def Quicksort (arr):
    if (len(arr) &lt; 2):
        return arr
    pivot = arr[0]
    low  = arr[np.where(arr &lt; pivot)]
    same = arr[np.where(arr == pivot)]
    high = arr[np.where(arr &gt; pivot)]
    return np.hstack((Quicksort(low), same, Quicksort(high)))</pre>
<p class="list" id="ch011list08"><em>Listing 11-8: Randomized and nonrandom Quicksort</em></p>
<p class="indent">For this example, we use NumPy instead of our <span class="literal">RE</span> class because it’s already loaded, which minimizes the overhead when calling <span class="literal">QuicksortRandom</span>. The implementations differ only in how they assign <span class="literal">pivot</span>.</p>
<p class="indent">Both implementations follow the Quicksort algorithm step-by-step. First, we check for the base condition where <span class="literal">arr</span> is already sorted. We then split into <span class="literal">low</span>, <span class="literal">same</span>, and <span class="literal">high</span> based on the selected <span class="literal">pivot</span>. Finally, NumPy’s <span class="literal">hstack</span> function concatenates the vectors returned by the recursive calls to <span class="literal">Quicksort</span>.</p>
<p class="indent">A high-performance implementation wouldn’t call <span class="literal">where</span> three times, as each makes a full pass over <span class="literal">arr</span>, but we’re interested only in relative performance differences as the input size changes.</p>
<h4 class="h4" id="ch00lev2_95"><em><strong>Experimenting with Quicksort</strong></em></h4>
<p class="noindent">The file <em>quicksort_tests.py</em> generates two graphs. The first, on the left in <a href="ch011.xhtml#ch011fig05">Figure 11-5</a>, compares randomized Quicksort and nonrandom Quicksort as the input array size increases. In all cases, the input arrays are in random order. Therefore, the left side of <a href="ch011.xhtml#ch011fig05">Figure 11-5</a> represents the average case runtime. The points plotted are the mean over five runs. The dashed line represents <em>y</em> = <em>n</em> log <em>n</em>.</p>
<div class="image"><img alt="Image" id="ch011fig05" src="../images/11fig05.jpg"/></div>
<p class="figcap"><em>Figure 11-5: Randomized and nonrandom Quicksort on random inputs (left) and the same algorithms on pathological inputs (right)</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_318"/>The rightmost graph in <a href="ch011.xhtml#ch011fig05">Figure 11-5</a> shows the runtime for the case with already sorted input. This situation forces deterministic Quicksort into becoming an <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2</sup>) algorithm, which is why it tracks the curved plot, <em>y</em> = <em>n</em><sup>2</sup>. Randomized Quicksort, on the other hand, is unaffected by the order of the input and runs as before.</p>
<p class="indent">Correctly interpreting <a href="ch011.xhtml#ch011fig05">Figure 11-5</a> requires an explanation. Asymptotic runtime performance of algorithms ignores multiplicative factors and constants as they don’t alter the overall form of the function as <em>n</em> increases. The randomized Quicksort function takes slightly longer to run than the non-random Quicksort because of the extra step of selecting a random index into the array. Therefore, plotting both runtimes together would make it somewhat difficult to see that the overall functional form is the same between <span class="literal">QuicksortRandom</span> and <span class="literal">Quicksort</span>. Moreover, plotting <em>y</em> = <em>n</em> log <em>n</em> follows an entirely different scale in terms of <em>y</em>-axis values, but again, the form of the function is the same. Therefore, to plot all three together, <a href="ch011.xhtml#ch011fig05">Figure 11-5</a> divides each <em>y</em> value by the maximum <em>y</em> value to map the output to [0, 1] regardless of the actual range. This clarifies that randomized Quicksort and nonrandom Quicksort scale in the same way, and are following <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em> log <em>n</em>)—all curves lie essentially on top of each other.</p>
<p class="indent">Now reconsider the right side of <a href="ch011.xhtml#ch011fig05">Figure 11-5</a> showing the case where the input array is already sorted. Again, the dashed line shows <em>y</em> = <em>n</em> log <em>n</em>, and randomized Quicksort still follows that form. However, nonrandom Quicksort, which selects the first element of the array as its pivot, does not. Instead, it follows the dotted line, <em>y</em> = <em>n</em><sup>2</sup>, meaning the pathological input case alters nonrandom Quicksort, turning it into an <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2</sup>) algorithm.</p>
<p class="indent">Randomized Quicksort is a Las Vegas algorithm because it always returns the proper output—a sorted array. While the randomness involved doesn’t make the implementation easier, it protects against a pathological case that’s more frequent in practice than we might initially suspect. Therefore, I recommend always using randomized Quicksort.</p>
<p class="indent">To understand why nonrandom Quicksort behaves so poorly with sorted input, consider what happens during a pass when the pivot is the smallest value in the array; for example, when picking the first element as the pivot and the input array is already sorted. When this happens, the low vector is empty and, ignoring duplicates of the pivot, all the remaining values in the array end up in the high vector. This happens every time the function recurses, turning the recursion into a list of function calls <em>n</em> deep. Add the <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em>) pass through the array on each recursive call (implicit in our implementation via <span class="literal">where</span>), and we arrive at an <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2</sup>) algorithm, which is no better than a bubble sort.</p>
<p class="indent">Selecting a random pivot at each level ensures that this situation won’t happen in the long run, as it would amount to a string of <em>n</em> rolls of an <em>n-</em>sided fair die each landing on 1—an increasingly unlikely event as <em>n</em> grows.</p>
<span epub:type="pagebreak" id="page_319"/>
<h3 class="h3" id="ch00lev1_71"><strong>Exercises</strong></h3>
<p class="noindent">Consider the following exercises to further explore randomized algorithms:</p>
<ul>
<li class="noindent">Write a Las Vegas algorithm to locate positive integers, <em>a</em>, <em>b</em>, and <em>c</em>, that satisfy <em>a</em><sup>2</sup> + <em>b</em><sup>2</sup> = <em>c</em><sup>2</sup>. Your code will be a Las Vegas algorithm because there are an infinite number of solutions, namely all the Pythagorean triples.</li>
<li class="noindent">Can you write a successful Las Vegas program to find positive integers <em>a</em>, <em>b</em>, and <em>c</em> such that <em>a<sup>n</sup></em> + <em>b<sup>n</sup></em> = <em>c<sup>n</sup></em> for some <em>n</em> &gt; 2? If not, how about a Monte Carlo algorithm? What might your stopping criteria be? I recommend searching for “Fermat’s last theorem.”</li>
<li class="noindent">Extend the permutation sort runtime plot for <em>n</em> = 11, 12, or even 13. How long do you have to wait?</li>
<li class="noindent">Make a plot of the mean number of trials of Freivalds’ algorithm to get a failure case as a function of <em>n</em>, the size of the square matrices.</li>
<li class="noindent">The file <em>test_mmult.py</em> generates output suitable for <em>curves.py</em> from <a href="ch04.xhtml">Chapter 4</a>. Use that output and <em>curves.py</em> to generate fits. Is the fit exponent what you expect for the naive algorithm? What about NumPy, with the knowledge that it uses Strassen’s algorithm?</li>
<li class="noindent">I have a bag full of marbles. I want to estimate how many are in the bag. Therefore, I pick one randomly, mark it, and put it back in the bag. I then pick another marble randomly, mark it, and put it back in the bag. I repeat this process, counting the number of marbles selected, until I pick a marble I’ve already marked. If the number of marbles picked and marked is <em>k</em>, then the number of marbles in the bag is approximately
<div class="image1"><img alt="Image" src="../images/f0319-01.jpg"/></div>
<p class="noindent">where the combination of floor (⌊) on the left and ceiling (⌉) on the right means “round to the nearest integer.” I encountered this algorithm via a brief description of the process, but the description had no derivation for the formula and no references. Nonetheless, it sort of works. After experimenting some, I realized that the estimate is better if the formula is tweaked slightly to become:</p>
<div class="image1"><img alt="Image" src="../images/f0319-04.jpg"/></div>
<p class="noindent">Implement this algorithm and explore how well it works on average. Then examine <em>count.py</em>, which runs the algorithm for many iterations, averages the results, and produces plots. For example</p>
<span epub:type="pagebreak" id="page_320"/>
<pre class="pre">&gt; <span class="codestrong1">python3 count.py 1_000_000_000 40 pcg64 6502</span>
N = 1023827699, iterations 41414, total 1656576</pre>
<p class="noindent">estimates slightly more than 1 billion marbles in the bag. The correct number is exactly 1 billion. It uses 40 iterations of the algorithm for a total of 1.7 million marbles marked. <a href="ch011.xhtml#ch011fig06">Figure 11-6</a> is the resulting plot, <em>count_plot.png</em>, which shows each of the 40 estimates, the true value (solid line), and overall estimate (dashed).</p>
<div class="image"><img alt="Image" id="ch011fig06" src="../images/11fig06.jpg"/></div>
<p class="figcap"><em>Figure 11-6: Forty estimates of marbles</em></p>
<p class="noindent">Please contact me if you know a reference for this algorithm or how to derive the estimate formula.</p></li>
<li class="noindent">Can you think of a “fudge factor” for the Lincoln-Petersen population estimate for the case where the population is believed to be small?</li>
<li class="noindent">How does the runtime performance of nonrandom Quicksort vary as the array becomes more disordered? To figure this out, fix the array size (<em>n</em>) but change the degree of disorder in the array. For example, begin with a sorted array, then swap two elements, then three, and so on. Is the transition from <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em><sup>2</sup>) to <img alt="Image" class="inline" src="../images/c0301-01.jpg"/>(<em>n</em> log <em>n</em>) linear with the number of elements swapped? Or does it seem more rapid?</li>
</ul>
<span epub:type="pagebreak" id="page_321"/>
<h3 class="h3" id="ch00lev1_72"><strong>Summary</strong></h3>
<p class="noindent">In this chapter, we explored randomized algorithms, differentiating between Las Vegas and Monte Carlo. The former always produced correct output, eventually, while the latter may produce incorrect output. We considered permutation sort and Freivalds’ algorithm for testing matrix multiplication. We learned that we can turn permutation sort from a Las Vegas algorithm into a Monte Carlo algorithm by imposing a limit on the number of candidate permutations considered. In general, we can transform Las Vegas algorithms into Monte Carlo algorithms, but not vice versa.</p>
<p class="indent">We then discussed the mark and recapture algorithm that ecologists use to estimate animal populations. We estimate the number of animals in a population by marking a known number and then recapturing animals and looking at the number marked. With sufficient numbers, the ratio of marked animals to animals recaptured should match the ratio of animals originally marked to the population size. We explored three estimators associated with this process and saw how they behave.</p>
<p class="indent">The Miller-Rabin algorithm quickly decides whether a positive integer is a prime. However, as a randomized algorithm, there’s a certain probability that it’ll falsely claim a composite number is prime. We learned how to decrease the likelihood of a false positive by repeated applications.</p>
<p class="indent">We concluded the chapter by comparing nonrandom and randomized Quicksort implementations. Randomized Quicksort adds little to the runtime while protecting against pathological inputs that are already (or mostly) sorted.</p>
<p class="indent">In our final chapter, we’ll consider randomness as it relates to sampling from probability distributions.<span epub:type="pagebreak" id="page_322"/></p>
</body></html>