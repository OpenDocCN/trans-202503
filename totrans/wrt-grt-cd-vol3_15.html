<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch12"><span epub:type="pagebreak" id="page_261"/><strong><span class="big">12</span><br/>SOFTWARE TEST DOCUMENTATION</strong></h2>
<div class="image1"><img src="Images/com.jpg" alt="Image" width="204" height="204"/></div>
<p class="noindents">This chapter covers software test documentation, focusing primarily on the Software Test Case (STC) and Software Test Procedure (STP) documents. As has been the case for the previous chapters, this discussion is based on IEEE Standards, specifically the IEEE Standard for Software and System Test Documentation (IEEE Std 829-2008, hereafter <em>Std 829</em><sup><a href="ch19_footnote.xhtml#ch12fn1" id="ch12fn1a">1</a></sup>).</p>
<h4 class="h4" id="lev-12.1"><span epub:type="pagebreak" id="page_262"/>12.1 The Software Test Documents in Std 829</h4>
<p class="noindent">Std 829 actually describes many additional documents above and beyond the STC and STP, including:</p>
<ul>
<li class="noindent">Master Test Plan (MTP)</li>
<li class="noindent"><em>Level</em> Test Plan (LTP)</li>
<li class="noindent"><em>Level</em> Test Design (LTD)</li>
<li class="noindent"><em>Level</em> Test Case (LTC)</li>
<li class="noindent"><em>Level</em> Test Procedure (LTPr)</li>
<li class="noindent"><em>Level</em> Test Log (LTL)</li>
<li class="noindent">Anomaly Report (AR)</li>
<li class="noindent"><em>Level</em> Interim Test Status Report (LITSR)</li>
<li class="noindent"><em>Level</em> Test Report (LTR)</li>
<li class="noindent">Master Test Report (MTR)</li>
</ul>
<p class="indent">Note that these are not actual document names—the word <em>level</em> is a placeholder for the scope or extent of software testing being documented. The scope could be at the level of <em>components</em> or <em>component integration</em>, apply to the entire <em>system</em>, or focus on <em>acceptance</em>. For example, <em>Level</em> Test Plan could refer to a Component (or Unit) Test Plan, Component Integration (or simply Integration) Test Plan, System (or System Integration) Test Plan, or an Acceptance Test Plan.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>Test levels are explained further in “Software Development Testing Levels” on <a href="ch12.xhtml#page_265">page 265</a>.</em></p>
</div>
<p class="indent">In all, Std 829 defines 31 different document types, but these are the main ones. The majority of these documents exist to support software management activities. Because this is a book on personal software engineering rather than software project management, this chapter won’t go into detail on most of them. Instead, we’ll concentrate on those <em>level</em> test documents that pertain to actual software testing—specifically, the <em>Level</em> Test Case, <em>Level</em> Test Procedure, <em>Level</em> Test Log, and Anomaly Report document types. We will cover all four <em>level</em> classifications—component, component integration, system, and acceptance—though the latter two are the main test documents used in this chapter. The differences between the <em>level</em> test documents are relatively minor, so this chapter applies the umbrella names mentioned earlier: Software Test Cases and Software Test Procedures. Keep in mind, however, that while these are common software engineering terms, Std 829 refers only to the <em>level</em> test documents.</p>
<h4 class="h4" id="lev-12.1.1"><strong><em>12.1.1 Process Support</em></strong></h4>
<p class="noindent">Although this chapter focuses on software testing, Std 829 describes the testing process in far more general terms. In particular, the testing process also handles the verification and validation of each document step in the <span epub:type="pagebreak" id="page_263"/>development process. Specifically, this means that the testing process tests the documentation as well as the actual software.</p>
<p class="indent">For the SyRS and SRS, the verification step ensures that the requirements actually satisfy customer needs (and <em>only</em> satisfy customer needs, without gold plating). For the SDD, the verification step ensures that the SDD covers all the requirements. For the STC, the verification step ensures that each requirement has one or more test cases that test the requirement. For the STP, the verification ensures that the set of test procedures fully covers all the test cases.</p>
<p class="indent">In addition to documentation, Std 829 discusses test procedures for verifying acquisitions (such as purchases of third-party libraries and computing hardware), administering RFPs (Requests for Proposals), and many other activities. These testing activities are very important. As noted previously, though, these are largely management activities rather than software development activities, so they’re mentioned only briefly here.</p>
<p class="indent">Std 829 states that testing needs to support the processes of management, acquisition, supply, development, operation, and maintenance. This chapter will concentrate on the development and operation processes (and, to a limited extent, the maintenance processes, which are largely an iteration of the development and operation processes). For more details on the other processes, see Std 829, IEEE/EIA Std 12207.0-1996 [B21], and ISO-IEC-IEEE-29148-2011.</p>
<p class="indent">Note that Std 829 allows you to combine and omit some of the testing documents. This means that you could have only a single document and still conform to Std 829. In reality, the final number of documents you create depends on the size of the project (large projects will require more documentation) and the turnaround you expect (fast projects will have fewer documents).</p>
<h4 class="h4" id="lev-12.1.2"><strong><em>12.1.2 Integrity Levels and Risk Assessment</em></strong></h4>
<p class="noindent">Std 829 defines four <em>integrity levels</em> that describe the importance or sensitivity to risk for a piece of software:</p>
<p class="uln-indent"><strong>Catastrophic (level 4)</strong> This level means that the software must execute properly, or something disastrous could occur (such as death, irreparable harm to the system, environmental damage, or a huge financial loss). There are no workarounds for catastrophic system failures. An example is a braking failure in a software-controlled self-driving vehicle.</p>
<p class="uln-indent"><strong>Critical (level 3)</strong> This level means that software must execute properly, or there could be serious problems including permanent injury, major performance degradation, environmental damage, or financial loss. A partial workaround may be possible for a critical system failure. An example is the transmission-controlling software in the self-driving vehicle being unable to shift out of second gear.</p>
<p class="uln-indent"><strong>Marginal (level 2)</strong> This level means that the software must execute properly, or there may be (minor) incorrect results produced and some <span epub:type="pagebreak" id="page_264"/>functionality lost. Workarounds to solve the problem are possible. Continuing with the self-driving-vehicle example, a software failure that prevents the infotainment center from operating is a marginal problem.</p>
<p class="uln-indent"><strong>Negligible (level 1)</strong> This level means that the software must execute properly, or else some minor functionality might not exist in the system (or the software might not be as “polished” as it should be). Negligible issues generally don’t require a workaround and can be safely ignored until an update comes along. An example is a spelling mistake on the touchscreen of the infotainment center in the self-driving vehicle.</p>
<p class="indenta">The higher the level, the greater the importance of the testing process; that is, level 4 (catastrophic) items demand higher-quality and more intensive testing than level 1 (negligible) items. Integrity levels, then, become the basis for determining the number, quality, and depth of test cases you create. For a feature in the program that could have catastrophic results in the event of a failure, you want a fair number of test cases that exercise that feature with considerable depth. For features that have negligible potential consequences, you might not have any test cases or only very shallow tests (such as a cursory review).<sup><a href="ch19_footnote.xhtml#ch12fn2" id="ch12fn2a">2</a></sup></p>
<p class="indent"><em>Risk assessment</em> is an attempt to determine where in your system failures are likely to occur, their expected frequency, and the associated costs. While risk assessment is predictive by its very nature (which means it won’t be perfect), you can often identify those parts of the program that are more likely to exhibit problems (such as complex sections of code, code produced by less experienced engineers, code from questionable sources like open source libraries found on the internet, and code using poorly understood algorithms). If you can categorize the likelihood of a problem as <em>likely</em>, <em>probable</em>, <em>occasional</em>, or <em>unlikely</em>, you can help identify the code that warrants more stringent testing (and, conversely, code that requires minimal testing).</p>
<p class="indentb">You can combine the integrity level and risk assessment levels in a matrix to produce a risk assessment scheme, as shown in <a href="ch12.xhtml#ch12tab1">Table 12-1</a>. In this example, a value of 4 denotes extreme importance, and a value of 1 indicates little importance.</p>
<p class="tabcap" id="ch12tab1"><strong>Table 12-1:</strong> Risk Assessment Scheme</p>
<table class="topbot-d">
<colgroup>
<col style="width:20%"/>
<col style="width:20%"/>
<col style="width:20%"/>
<col style="width:20%"/>
<col style="width:20%"/>
</colgroup>
<thead>
<tr>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"><strong>Consequence</strong></p></td>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"><strong>Likelihood</strong></p></td>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"> </p></td>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"> </p></td>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"> </p></td>
</tr>
</thead>
<tbody>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba"/></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">Likely</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">Probable</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">Occasional</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">Unlikely</p></td>
</tr>
<tr>
<td style="vertical-align: top;"><p class="taba">Catastrophic</p></td>
<td style="vertical-align: top;"><p class="taba">4</p></td>
<td style="vertical-align: top;"><p class="taba">4</p></td>
<td style="vertical-align: top;"><p class="taba">3.5</p></td>
<td style="vertical-align: top;"><p class="taba">3</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">Critical</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">4</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">3.5</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">3</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">2.5</p></td>
</tr>
<tr>
<td style="vertical-align: top;"><p class="taba">Marginal</p></td>
<td style="vertical-align: top;"><p class="taba">3</p></td>
<td style="vertical-align: top;"><p class="taba">2.5</p></td>
<td style="vertical-align: top;"><p class="taba">1.5</p></td>
<td style="vertical-align: top;"><p class="taba">1</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">Negligible</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">2</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">1.5</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">1</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">1</p></td>
</tr>
</tbody>
</table>
<p class="indent"><span epub:type="pagebreak" id="page_265"/>Std 829 does not mandate using an integrity level or risk assessment scheme in your test documentation, though it does consider this to be best practice. If you do use an integrity level, Std 829 does not require that you use the IEEE-recommended scheme (you could, for example, use a finer-grained integrity level with values from 1 to 10). However, if you “roll your own” integrity level, the IEEE recommends that you document a mapping from your integrity levels to those suggested by the IEEE so that readers can easily compare them.</p>
<h4 class="h4" id="lev-12.1.3"><strong><em>12.1.3 Software Development Testing Levels</em></strong></h4>
<p class="noindent">In addition—and in contrast—to the integrity levels just described, the IEEE defines four testing levels, each of which generally describes the scope or extent of software testing being documented:</p>
<p class="uln-indent"><strong>Component (also known as</strong> <strong><em>unit</em></strong><strong>)</strong><strong><sup><a href="ch19_footnote.xhtml#ch12fn3" id="ch12fn3a">3</a></sup></strong> This level deals with subroutines, functions, modules, and subprograms at the lowest code level. <em>Unit testing</em>, for example, consists of testing individual functions and other small program units independent of the rest of the program.</p>
<p class="uln-indent"><strong>Component integration (also known as simply</strong> <strong><em>integration</em></strong><strong>)</strong> This level is the point at which you begin combining individual units together to form a larger portion of the system, though not necessarily the whole system. Integration testing, for example, occurs when you combine (pretested) units to see if they play well together (that is, pass appropriate parameters, return appropriate function results, and so on).</p>
<p class="uln-indent"><strong>System (also known as</strong> <strong><em>system integration</em></strong><strong>)</strong> This level of testing is the ultimate form of integration testing—you’ve integrated all the program units together and formed the complete system. Unit testing, integration testing, and system integration testing are typically tests the developers perform before releasing a complete system outside the development group.</p>
<p class="uln-indent"><strong>Acceptance (variants include</strong> <strong><em>factory acceptance</em></strong> <strong>and</strong> <strong><em>site acceptance</em></strong><strong>)</strong> <em>Acceptance testing</em> <em>(AT)</em> is post-development. As its name implies, it refers to how the customer determines whether the system is acceptable. Depending on the system, there may be a couple of acceptance testing variants. <em>Factory acceptance testing (FAT)</em> occurs on systems prior to leaving the manufacturer (typically on the factory floor, hence the name). Even if a product is pure software, it can have a factory acceptance test where the customer’s representatives come to test the software under the watchful eye of the software development team. This allows the team to make quick changes to the system if the customer discovers minor errors during the FAT.</p>
<p class="uln-indenti">A <em>site acceptance test</em> <em>(SAT)</em> is performed at the customer’s site after the system is installed. For hardware-based systems, this ensures that <span epub:type="pagebreak" id="page_266"/>the hardware is installed properly and the software is functioning as intended. For pure software systems, the SAT provides a final check (after a possible AT or FAT) that the software is usable by the system’s end users.</p>
<h3 class="h3" id="lev-12.2"><strong>12.2 Test Plans</strong></h3>
<p class="noindent">A software test plan is a document that describes the scope, organization, and activities associated with the testing process. This is largely a managerial overview of how the testing will take place, the resources testing will require, schedules, necessary tools, and objectives. This chapter won’t consider test plans in detail, as they are beyond the scope of this book; however, the following sections will present outlines provided in IEEE Std 829-2008 as a reference. For more details on these test plans, consult Std 829.</p>
<h4 class="h4" id="lev-12.2.1"><strong><em>12.2.1 Master Test Plan</em></strong></h4>
<p class="noindent">The <em>Master Test Plan (MTP)</em> is an organization-wide top-level management document that tracks the testing process across a whole project (or set of projects). Software engineers are rarely involved directly with the MTP, which is largely an umbrella document that the QA (Quality Assurance) department uses to track quality aspects of a project. A project manager or project lead might be aware of the MTP—and might contribute to it during schedule and resource development—but the development team rarely sees the MTP except in passing.</p>
<p class="indentb">The following outline comes from Section 8 of IEEE Std 829-2008 (and uses the IEEE section numbers):</p>
<p class="number1">1 Introduction</p>
<p class="number2">1.1 Document Identifier</p>
<p class="number2">1.2 Scope</p>
<p class="number2">1.3 References</p>
<p class="number2">1.4 System Overview and Key Features</p>
<p class="number2">1.5 Test Overview</p>
<p class="number3">1.5.1 Organization</p>
<p class="number3">1.5.2 Master Test Schedule</p>
<p class="number3">1.5.3 Integrity Level Schema</p>
<p class="number3">1.5.4 Resources Summary</p>
<p class="number3">1.5.5 Responsibilities</p>
<p class="number3">1.5.6 Tools, Techniques, Methods, and Metrics</p>
<p class="number1">2 Details of the Master Test Plan</p>
<p class="number2">2.1 Test Processes Including Definition of Test Levels</p>
<p class="number3">2.1.1 Process: Management</p>
<p class="number4">2.1.1.1 Activity: Management of Test Effort</p>
<p class="number3"><span epub:type="pagebreak" id="page_267"/>2.1.2 Process: Acquisition</p>
<p class="number4">2.1.2.1 Activity: Acquisition Support Test</p>
<p class="number3">2.1.3 Process: Supply</p>
<p class="number4">2.1.3.1 Activity: Planning Test</p>
<p class="number3">2.1.4 Process: Development</p>
<p class="number4">2.1.4.1 Activity: Concept</p>
<p class="number4">2.1.4.2 Activity: Requirements</p>
<p class="number4">2.1.4.3 Activity: Design</p>
<p class="number4">2.1.4.4 Activity: Implementation</p>
<p class="number4">2.1.4.5 Activity: Test</p>
<p class="number4">2.1.4.6 Activity: Install/Checkout</p>
<p class="number3">2.1.5 Process: Operation</p>
<p class="number4">2.1.5.1 Activity: Operational Test</p>
<p class="number3">2.1.6 Process: Maintenance</p>
<p class="number4">2.1.6.1 Activity: Maintenance Test</p>
<p class="number2">2.2 Test Documentation Requirements</p>
<p class="number2">2.3 Test Administration Requirements</p>
<p class="number2">2.4 Test Reporting Requirements</p>
<p class="number1">3 General</p>
<p class="number2">3.1 Glossary</p>
<p class="number2">3.2 Document Change Procedures and History</p>
<p class="indenta">Many of these sections contain information common to IEEE documents (for example, see the SRS and SDD samples in previous chapters). As the MTP is beyond the scope of this chapter, please consult Std 829 for specific descriptions of each section in this outline.</p>
<h4 class="h4" id="lev-12.2.2"><strong><em>12.2.2</em> Level <em>Test Plan</em></strong></h4>
<p class="noindent">A Level <em>Test Plan (LTP)</em> refers to a set of test plans based on the development state. As this chapter noted earlier, each document in the set generally describes the scope or extent of software test being documented: Component Test Plan (aka Unit Test Plan, or UTP), Component Integration Test Plan (aka Integration Test Plan, or ITP), System Test Plan (aka System Integration Test Plan, or SITP), and Acceptance Test Plan (ATP; may include a Factory Acceptance Test Plan [FATP] or Site Acceptance Test Plan [SATP]).<sup><a href="ch19_footnote.xhtml#ch12fn4" id="ch12fn4a">4</a></sup></p>
<p class="indent">LTPs are also managerial/QA documents, but the development team (even to the level of individual software engineers) often has input on their creation and use, because these documents reference detailed features of the software design. These test plans are not guiding documents—that is, <span epub:type="pagebreak" id="page_268"/>a software engineer wouldn’t necessarily reference these documents while actually testing the software—but they can’t be created without development team feedback. Like the MTP, LTPs provide a road map for the creation of the test case and test procedure documents (of primary interest to the development and testing teams) and outline how to perform the tests. LTPs provide a good high-level view of the testing process, especially for external organizations interested in its quality.<sup><a href="ch19_footnote.xhtml#ch12fn5" id="ch12fn5a">5</a></sup></p>
<p class="indentb">Here is the LTP outline from Std 829:</p>
<p class="number1">1 Introduction</p>
<p class="number2">1.1 Document Identifier</p>
<p class="number2">1.2 Scope</p>
<p class="number2">1.3 References</p>
<p class="number2">1.4 Level in the Overall Sequence</p>
<p class="number2">1.5 Test Classes and Overall Test Conditions</p>
<p class="number1">2 Details for This Level of Test Plan</p>
<p class="number2">2.1 Test Items and Their Identifiers</p>
<p class="number2">2.2 Test Traceability Matrix</p>
<p class="number2">2.3 Features to Be Tested</p>
<p class="number2">2.4 Features Not to Be Tested</p>
<p class="number2">2.5 Approach</p>
<p class="number2">2.6 Item Pass/Fail Criteria</p>
<p class="number2">2.7 Suspension Criteria and Resumption Requirements</p>
<p class="number2">2.8 Test Deliverables</p>
<p class="number1">3 Test Management</p>
<p class="number2">3.1 Planned Activities and Tasks; Test Progression</p>
<p class="number2">3.2 Environmental/Infrastructure</p>
<p class="number2">3.3 Responsibilities and Authority</p>
<p class="number2">3.4 Interfaces Among the Parties Involved</p>
<p class="number2">3.5 Resources and Their Allocation</p>
<p class="number2">3.6 Training</p>
<p class="number2">3.7 Schedules, Estimates, and Costs</p>
<p class="number2">3.8 Risk(s) and Contingency(s)</p>
<p class="number1">4 General</p>
<p class="number2">4.1 Quality Assurance Procedures</p>
<p class="number2">4.2 Metrics</p>
<p class="number2">4.3 Test Coverage</p>
<p class="number2"><span epub:type="pagebreak" id="page_269"/>4.4 Glossary</p>
<p class="number2">4.5 Document Change Procedures and History</p>
<p class="indenta">You might notice that there is considerable overlap between the LTPs and the MTP. Std 829 states that if you are replicating information in a test plan that exists elsewhere, you can simply provide a reference to the containing document rather than duplicating the information in your LTP (or MTP). For example, you’re likely to have an overall Reverse Traceability Matrix (RTM) that includes traceability for all the tests. Rather than replicating that traceability information in section 2.2 of an LTP, you would simply reference the RTM document that contains this information.</p>
<h4 class="h4" id="lev-12.2.3"><strong><em>12.2.3</em> Level <em>Test Design Documentation</em></strong></h4>
<p class="noindent">The Level <em>Test Design (LTD)</em> documentation, as its name suggests, describes the design of the tests. Once again, there are four types of LTD documents, each generally describing the scope or extent of software testing being documented: Component Test Design (aka Unit Test Design, or UTD), Component Integration Test Design (aka Integration Test Design, or ITD), System Test Design (aka System Integration Test Design, or SITD), and Acceptance Test Design (ATD; this may include a Factory Acceptance Test Design [FATD] or a Site Acceptance Test Design [SATD]).</p>
<p class="indentb">The main purpose of the LTD is to collect common information in one place that would be replicated throughout the test procedures. That means that this document could very easily be merged with your test procedures document (at the cost of some repetition in that document). This book will take that approach, merging pertinent items from the test design directly into the test cases and test procedures documents.<sup><a href="ch19_footnote.xhtml#ch12fn6" id="ch12fn6a">6</a></sup> For that reason this section will present the IEEE recommended outline without additional commentary and save the details for the STC and STP documents.</p>
<p class="number1">1 Introduction</p>
<p class="number2">1.1 Document Identifier</p>
<p class="number2">1.2 Scope</p>
<p class="number2">1.3 References</p>
<p class="number1">2 Details of the Level Test Design</p>
<p class="number2">2.1 Features to Be Tested</p>
<p class="number2">2.2 Approach Refinements</p>
<p class="number2">2.3 Test Identification</p>
<p class="number2">2.4 Feature Pass/Fail Criteria</p>
<p class="number2">2.5 Test Deliverables</p>
<p class="number1"><span epub:type="pagebreak" id="page_270"/>3 General</p>
<p class="number2">3.1 Glossary</p>
<p class="number2">3.2 Document Change Procedures and History</p>
<h3 class="h3" id="lev-12.3"><strong>12.3 Software Review List Documentation</strong></h3>
<p class="noindent">When you build the RTM starting with your requirements, one of the columns you usually create is the test/verification type column. Typically, a software requirement will have one of two associated verification types: <em>T</em> (for <em>test</em>) and <em>R</em> (for <em>review</em>).<sup><a href="ch19_footnote.xhtml#ch12fn7" id="ch12fn7a">7</a></sup> Requirements marked <em>T</em> will have associated test cases and test procedures (see “<a href="ch10.xhtml#lev-10.9">Updating the Traceability Matrix with Requirement Information</a>” on <a href="ch10.xhtml#page_222">page 222</a> for details on creating test cases). Items marked <em>R</em> will need to be reviewed. This section describes how to create a Software Review List (SRL) document to track the review of the system (usually the source code) to verify those requirements.</p>
<p class="indent">The SRL is relatively straightforward. The core of the document is simply a list of items, each of which you check off after you review it and are confident that the software properly supports the associated requirement.</p>
<p class="indent">In theory, you could create <em>level</em> review list documentation at four separate levels: component, component integration, system, and acceptance (as is the case for other Std 829 <em>level</em> documents). In reality, however, a single SRL that is suitable for both system (integration) and acceptance use will suffice.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>The SRL document is not a part of Std 829 (or any other IEEE standards document, for that matter). Std 829 certainly allows you to use this document as part of your verification package, but the format presented in this section is not from the IEEE.</em></p>
</div>
<h4 class="h4" id="lev-12.3.1"><strong><em>12.3.1 Sample SRL Outline</em></strong></h4>
<p class="noindentb">Although the SRL is not a standard IEEE document, the following outline for it is somewhat similar to the SRS, STC, and STP recommended formats from the IEEE:</p>
<p class="number1">1 Introduction (once per document)</p>
<p class="number2">1.1 Document Identifier</p>
<p class="number2">1.2 Document Change Procedures and History</p>
<p class="number2">1.3 Scope</p>
<p class="number2">1.4 Intended Audience</p>
<p class="number2">1.5 Definitions, Acronyms, and Abbreviations</p>
<p class="number2"><span epub:type="pagebreak" id="page_271"/>1.6 References</p>
<p class="number2">1.7 Notation for Description</p>
<p class="number1">2 General System Description</p>
<p class="number1">3 Checklist (one per review item)</p>
<p class="number2">3.1 Review Identifier (Tag)</p>
<p class="number2">3.2 Discussion of Item to Review</p>
<h4 class="h4" id="lev-12.3.2"><strong><em>12.3.2 Sample SRL</em></strong></h4>
<p class="noindentb">This sample SRL continues to use the DAQ DIP switch project from the previous chapters. Specifically, this SRL is based on the requirements from “(Selected) DAQ Software Requirements (from SRS)” on <a href="ch10.xhtml#page_219">page 219</a> and the verification types detailed in “Requirements to Be Verified by Review” on <a href="ch10.xhtml#page_223">page 223</a>.</p>
<p class="number1"><strong>1 Introduction</strong></p>
<p class="numberp">This Software Review List provides a software review checklist for those DAQ system requirements that are to be verified by review.</p>
<p class="number2"><strong>1.1 Document Identifier</strong></p>
<p class="numberp">DAQ_SRL v1.0</p>
<p class="number2"><strong>1.2 Document Change Procedures and History</strong></p>
<p class="numberp">All revisions should be noted here, by date and version number.</p>
<p class="numberp">Mar 23, 2018—Version 1.0</p>
<p class="number2"><strong>1.3 Scope</strong></p>
<p class="numberp">This SRL deals with those requirements in the DAQ DIP switch initialization project for which creating a formal test procedure would be difficult (or otherwise economically unviable) but whose correctness can be easily verified by reviewing the source code and the build system for the source code.</p>
<p class="number2"><strong>1.4 Intended Audience</strong></p>
<p class="numberp">The <em>normal</em> audience for an SRL:</p>
<p class="numberp">This document is intended primarily for those individuals who will be testing/reviewing the DAQ DIP switch project. Project management and the development team may also wish to review this document.</p>
<p class="numberp">The <em>real</em> audience for this SRL:</p>
<p class="numberp">This SRL is intended for readers of <em>Write Great Code, Volume 3</em>. It provides an example SRL that can serve as a template for SRLs they may need to create.</p>
<p class="number2"><span epub:type="pagebreak" id="page_272"/><strong>1.5 Definitions, Acronyms, and Abbreviations</strong></p>
<p class="numberp">DAQ: Data acquisition system</p>
<p class="numberp">DIP: Dual inline package</p>
<p class="numberp">SDD: Software Design Document</p>
<p class="numberp">SRL: Software Review List</p>
<p class="numberp">SRS: Software Requirements Specification</p>
<p class="number2"><strong>1.6 References</strong></p>
<p class="numberp">SDD: IEEE Std 1016-2009</p>
<p class="numberp">SRS: IEEE Std 830-1998</p>
<p class="numberp">STC/STP: IEEE Std 829-2008</p>
<p class="number2"><strong>1.7 Notation for Description</strong></p>
<p class="numberp">Review identifiers (<em>tags</em>) in this document shall take the form:</p>
<p class="numberp">DAQ_SR_<em>xxx_yyy_zzz</em></p>
<p class="numberp">where <em>xxx_yyy</em> is a string of (possibly decimal) numbers taken from the corresponding requirement (for example, DAQ_SRS_<em>xxx_yyy</em>) and <em>zzz</em> is a (possibly decimal) numeric sequence that creates a unique identifier out of the whole sequence. Note that <em>zzz</em> values in SRL tags are usually numbered from 000 or 001 and usually increment by 1 for each additional review item sharing the same <em>xxx_yyy</em> string.</p>
<p class="number1"><strong>2 General System Description</strong></p>
<p class="numberp1">The purpose behind the DAQ DIP switch system is to initialize the DAQ system upon power-up. The DAQ DIP switch system is a small subset of the larger Plantation Productions DAQ system that is useful as an example within this book.</p>
<p class="number1"><strong>3 Checklist</strong></p>
<p class="numberp1">Check off each of the following items as it is verified during the review process.</p>
<p class="number2"><strong>3.1 DAQ_SR_700_000_000</strong></p>
<p class="numberp">Verify code is written for a Netburner MOD54415 evaluation board.</p>
<p class="number2"><strong>3.2 DAQ_SR_700_000.01_000.1</strong></p>
<p class="numberp">Verify code is written for μC/OS.</p>
<p class="number2"><strong>3.3 DAQ_SR_702_001_000</strong></p>
<p class="numberp">Verify that software creates a separate task to handle serial port command processing.</p>
<p class="number2"><span epub:type="pagebreak" id="page_273"/><strong>3.4 DAQ_SR_702_002_000</strong></p>
<p class="numberp">Verify that serial task priority is lower than USB and Ethernet task priorities (note that the higher the priority number, the lower the priority).</p>
<p class="number2"><strong>3.5 DAQ_SR_703_001_000</strong></p>
<p class="numberp">Same as DAQ_SRS_702_001, but doesn’t start an RS-232 task if DIP switch 1 is in the OFF position.</p>
<p class="number2"><strong>3.6 DAQ_SR_705_001_000</strong></p>
<p class="numberp">Verify that software creates a separate task to handle USB port command processing.</p>
<p class="number2"><strong>3.7 DAQ_SR_705_002_000</strong></p>
<p class="numberp">Verify that a USB task has a higher priority than the Ethernet and serial protocol tasks.</p>
<p class="number2"><strong>3.8 DAQ_SR_706_001_000</strong></p>
<p class="numberp">Verify that software does not start the USB task if DIP switch 2 is in the OFF position.</p>
<p class="number2"><strong>3.9 DAQ_SR_716_001_000</strong></p>
<p class="numberp">Verify that the Ethernet listening task is started only if Ethernet communications are enabled.</p>
<p class="number2"><strong>3.10 DAQ_SR_716_002_000</strong></p>
<p class="number2p">Verify that the Ethernet listening task has a priority lower than the USB task but higher than the serial task.</p>
<p class="number2"><strong>3.11 DAQ_SR_719_000_000</strong></p>
<p class="number2p">Verify that software sets the unit test mode value to ON based on the DIP switch 7 setting.</p>
<p class="number2"><strong>3.12 DAQ_SR_720_000_000</strong></p>
<p class="number2p">Verify that software sets the unit test mode value to OFF based on the DIP switch 7 setting.</p>
<p class="number2"><strong>3.13 DAQ_SR_723_000_000</strong></p>
<p class="number2p">Verify that the software provides a function to read the DIP switches.</p>
<p class="number2"><strong>3.14 DAQ_SR_723_000.01_000</strong></p>
<p class="number2p">Verify that the system uses the DIP switch reading to initialize RS-232 (serial), USB, Ethernet, unit test mode, and debug mode on startup.</p>
<p class="number2"><strong>3.15 DAQ_SR_723_000.02_000</strong></p>
<p class="number2p">Verify that the startup code stores the DIP switch reading for later use by the software.</p>
<p class="number2"><span epub:type="pagebreak" id="page_274"/><strong>3.16 DAQ_SR_725_000_000</strong></p>
<p class="number2p">Verify that the command processor responds to a command when a complete line of text is received from the USB, RS-232, and Ethernet ports.</p>
<p class="number2"><strong>3.17 DAQ_SR_738_001_000</strong></p>
<p class="number2p">Verify that the system starts a new process (task) to handle command processing for each new Ethernet connection.</p>
<p class="number2"><strong>3.18 DAQ_SR_738_002_000</strong></p>
<p class="number2p">Verify that the Ethernet command processing tasks have a priority between the Ethernet listening task and the USB command task.</p>
<h4 class="h4" id="lev-12.3.3"><strong><em>12.3.3 Adding SRL Items to the Traceability Matrix</em></strong></h4>
<p class="noindent">Once you’ve created an SRL, you’ll want to add all the <em>SR</em> tags to the RTM so you can trace the reviewed items back to the requirements, as well as to everything else in the RTM. To do so, just locate the requirement associated with each review item tag (this is trivial if you’re using the tag numbering this chapter recommends; the SRS tag number is incorporated into the SRL tag number) and add the SRL tag to the appropriate column in the same row of the RTM containing the requirement.</p>
<p class="indent">When you’ve got both SRL and STC documents, there’s really no need to create separate columns in the RTM for both types, as they are mutually exclusive and the tag will differentiate them. (See “<a href="ch10.xhtml#lev-10.4.5">A Sample Software Requirements Specification</a>” on <a href="ch10.xhtml#page_203">page 203</a> for some additional commentary on this.)</p>
<h3 class="h3" id="lev-12.4"><strong>12.4 Software Test Case Documentation</strong></h3>
<p class="noindent">For each item in the RTM whose requirement verification type is <em>T</em>, you’ll need to create a software test case. The <em>Software Test Case (STC)</em> document is where you’ll put the actual test cases.</p>
<p class="indent">As with all the 829 Std <em>level</em> documents, there are four levels in the <em>Level</em> Test Case documentation. The term <em>Software Test Case</em> generically refers to any one of these. As this chapter noted earlier, this is actually a set of test cases, where each document in the set type generally describes the scope or extent of software testing being documented: Component Test Cases (aka Unit Test Cases, or UTC), Component Integration Test Cases (aka Integration Test Cases, or ITC), System Test Cases (aka System Integration Test Cases, or SITC), and Acceptance Test Cases (ATC; may include Factory Acceptance Test Cases [FATC] and Site Acceptance Test Cases [SATC]).<sup><a href="ch19_footnote.xhtml#ch12fn8" id="ch12fn8a">8</a></sup></p>
<p class="indentb"><span epub:type="pagebreak" id="page_275"/>The STC document lists all the individual test cases (tests) for a project. Here is the Std 829 outline for the <em>Level</em> Test Case documentation:</p>
<p class="number1">1 Introduction (once per document)</p>
<p class="number2">1.1 Document Identifier</p>
<p class="number2">1.2 Scope</p>
<p class="number2">1.3 References</p>
<p class="number2">1.4 Context</p>
<p class="number2">1.5 Notation for Description</p>
<p class="number1">2 Details (once per test case)</p>
<p class="number2">2.1 Test Case Identifier</p>
<p class="number2">2.2 Objective</p>
<p class="number2">2.3 Inputs</p>
<p class="number2">2.4 Outcome(s)</p>
<p class="number2">2.5 Environmental Needs</p>
<p class="number2">2.6 Special Procedural Requirements</p>
<p class="number2">2.7 Intercase Dependencies</p>
<p class="number1">3 Global (once per document)</p>
<p class="number2">3.1 Glossary</p>
<p class="number2">3.2 Document Change Procedures and History</p>
<p class="indenta">In common practice, the Unit Test Cases and the Integration Test Cases are often combined into the same document (the differentiation between the two usually occurs at the level of test procedures). You will typically develop UTCs and ITCs from your source code and from the SDD (see <a href="ch12.xhtml#ch12fig1">Figure 12-1</a>, which is an extension of <a href="ch09.xhtml#ch9fig1">Figure 9-1</a>).</p>
<div class="image"><a id="ch12fig1"/><img src="Images/fig12-1.jpg" alt="image" width="321" height="178"/></div>
<p class="figcap"><em>Figure 12-1: Unit and Integration Test Case sources</em></p>
<p class="indent">Often, the UTC and ITC (and test procedure) documents exist as software rather than as natural-language documents. Using an <em>automated test procedure</em>, a piece of software that runs all the unit and integration tests, is a software engineering best practice. By doing so, you can dramatically <span epub:type="pagebreak" id="page_276"/>reduce the time it takes to run tests as well as the errors introduced in manually performed test procedures.<sup><a href="ch19_footnote.xhtml#ch12fn9" id="ch12fn9a">9</a></sup></p>
<p class="indent">Unfortunately, it isn’t possible to create automated tests for every test case, so you’ll usually have a UTC/ITC document covering (at least) the test cases you must perform manually.</p>
<p class="indent">Many organizations—particularly those that embrace Agile development models and test-driven development (TDD)—forgo formal UTC and ITC documents. Informally written procedures and automated test procedures are far more common in these situations because the cost of creating and (especially) maintaining the documentation quickly gets out of hand. As long as the development team can provide <em>some</em> documentation that they are performing a fixed set of unit/integration tests (that is, they’re not doing ad hoc, “by the seat of the pants” tests that could differ on every test run), larger organizations tend to leave them be.</p>
<p class="indent">Regardless of whether it’s formal, informal, or automated, having a repeatable test procedure is key. <em>Regression tests</em>, which check to see if anything has broken, or regressed, since you’ve made changes to the code, require a repeatable testing process. Therefore, you need some kind of test case to ensure repeatability.</p>
<p class="indent">For unit/integration testing, the test data you generate will be a combination of black-box-generated test data and white-box-generated test data. <em>Black-box test data</em> generally comes from the system requirements (SyRS and SRS); you consider only the functionality of the system (which the requirements provide) when you create its input test data. When you generate <em>white-box test data</em>, on the other hand, you analyze the software’s source code. For example, ensuring that you execute every statement in the program at least once during testing—that is, achieving complete code coverage—requires careful analysis of the source code and, therefore, is a white-box test-data-generation technique.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep">Write Great Code, Volume 6: Testing, Debugging, and Quality Assurance <em>will consider the techniques for generating white-box and black-box test data in greater detail.</em></p>
</div>
<p class="indent">Once you get to the level of a system integration test or (even more importantly) an acceptance test, formal documentation for your test cases becomes mandatory. If you’re creating a custom system for a customer, or your software is subject to regulatory or legal restrictions (such as life-threatening environments in an autonomous vehicle), you’ll likely have to convince some overseer organization that you’ve put in your best effort during testing and prove that the system meets its requirements. This is where it’s essential to have formal documentation like that recommended <span epub:type="pagebreak" id="page_277"/>by Std 829.<sup><a href="ch19_footnote.xhtml#ch12fn10" id="ch12fn10a">10</a></sup> For this reason, most SITC and (most certainly) ATC documents derive their cases directly from the requirements (see <a href="ch12.xhtml#ch12fig2">Figure 12-2</a>). So, with this motivation in hand, let’s return to the discussion of the <em>Level</em> Test Case document (see the outline at the beginning of this section).</p>
<div class="image"><a id="ch12fig2"/><img src="Images/fig12-2.jpg" alt="image" width="508" height="148"/></div>
<p class="figcap"><em>Figure 12-2: SITC and ATC derivation</em></p>
<p class="indent">More often than not, the (F)ATC document is simply a subset of the SITC document. (If you have FATC documentation and SATC documentation, the site variant is often a subset of the FATC document.) The SITC document will contain test cases for every requirement. In the ATC documents, system architects may merge or eliminate test cases that are nearly or entirely redundant, or are of little interest to customers and end users.</p>
<h4 class="h4" id="lev-12.4.1"><strong><em>12.4.1 Introduction in the STC Document</em></strong></h4>
<p class="noindent">The introductory section of an STC (or any <em>Level</em> Test Case) document should include the following information.</p>
<h5 class="h5" id="lev-12.4.1.1"><strong>12.4.1.1 Document Identifier</strong></h5>
<p class="noindent">The document identifier should be some unique name/number and should include the issuing date, author identification, status (for example, draft or final), approval signatures, and possibly a version number. A single ID name/number is imperative so you can reference the test case documentation in other documents (such as the STP and RTM).</p>
<h5 class="h5" id="lev-12.4.1.2"><strong>12.4.1.2 Scope</strong></h5>
<p class="noindent">This section summarizes the software system and features to test.</p>
<h5 class="h5" id="lev-12.4.1.3"><strong>12.4.1.3 References</strong></h5>
<p class="noindent">This section should provide a list of all reference documents, internal and external, associated with the STC. Internal references would normally include documents such as the SyRS, SRS, SDD, RTM, and (if it exists) the MTP. External references would include standards like IEEE Std 829-2008 and any regulatory or legal documents that might apply.</p>
<h5 class="h5" id="lev-12.4.1.4"><span epub:type="pagebreak" id="page_278"/><strong>12.4.1.4 Context</strong></h5>
<p class="noindent">In this section you provide any context for the test cases that doesn’t appear in any other documentation. Examples might include naming automated test-generation software or internet-based tools used to generate or evaluate test cases.</p>
<h5 class="h5" id="lev-12.4.1.5"><strong>12.4.1.5 Notation for Description</strong></h5>
<p class="noindent">This section should describe the tags (identifiers) you’ll apply to the test cases. For example, this chapter uses tags of the form <em>proj</em>_STC_<em>xxx</em>_<em>yyy</em>_<em>zzz</em>, so this section of the STC would explain what this means and how to generate STC tags.</p>
<h4 class="h4" id="lev-12.4.2"><strong><em>12.4.2 Details</em></strong></h4>
<p class="noindent">You repeat the subsections contained herein for each test case in the STC.</p>
<h5 class="h5" id="lev-12.4.2.1"><strong>12.4.2.1 Test Case Identifier</strong></h5>
<p class="noindent">The test case identifier is the tag associated with this particular test case. For example, this book uses tags of the form <em>DAQ_STC_002_000_001</em> where <em>DAQ</em> is the project ID (for the DAQ DIP switch project), <em>002_000</em> is from the SRS requirement tag, and <em>001</em> is a test-case-specific value to make this tag unique among all the others. The Swimming Pool Monitor (SPM) project from previous chapters might use tags like <em>POOL_STC_002_001</em> within the STC. Std 829 doesn’t require the use of this tag format, only that all test case tags be unique.</p>
<h5 class="h5" id="lev-12.4.2.2"><strong>12.4.2.2 Objective</strong></h5>
<p class="noindent">This is a brief description of the focus or goal of this particular test case. (Note that a set of test cases can have the same objective, in which case this field could simply reference the objectives in a different test case.) This field is a good place to put risk assessment and integrity level information, if relevant.</p>
<h5 class="h5" id="lev-12.4.2.3"><strong>12.4.2.3 Input(s)</strong></h5>
<p class="noindent">This section lists all inputs and their relationships (in terms of timing, ordering, and the like) that a tester needs in order to perform this test case. Some inputs might be exact, and some may be approximate, in which case you must provide tolerances for the input data. If the input set is large, this section might simply reference an input file, database, or some other input stream that will provide the test data.<sup><a href="ch19_footnote.xhtml#ch12fn11" id="ch12fn11a">11</a></sup></p>
<h5 class="h5" id="lev-12.4.2.4"><span epub:type="pagebreak" id="page_279"/><strong>12.4.2.4 Outcome(s)</strong></h5>
<p class="noindent">This section lists all expected output data values and behaviors such as response time, timing relationships, and order of output data. The test case should provide exact output data values if possible; if you can provide only approximate data values, the test case must also supply tolerances. If an output stream is large, then this section can reference externally supplied files or databases.</p>
<p class="indent">If the test is successful by virtue of the fact that it runs without crashing—that is, self-validating—then this section is unnecessary in the test case.</p>
<h5 class="h5" id="lev-12.4.2.5"><strong>12.4.2.5 Environmental Needs</strong></h5>
<p class="noindent">This section describes any preexisting software or data such as a known database that is needed for the test. It could also describe any internet sites referenced by their URLs that must be active in order to execute the test case. This could also include any special power requirements, such as requiring a UPS to be fully charged before testing power failures, or it could include other conditions such as the swimming pool being filled with water before running tests on the SPM system.</p>
<h6 class="h6" id="lev-12.4.2.5.1"><strong>12.4.2.5.1 Hardware Environmental Needs</strong></h6>
<p class="numberp1">This section lists any hardware needed to run the test and specifies its configuration settings. It could also specify any special hardware such as a test fixture for the test operation. For example, a test fixture for the SPM might be a five-gallon bucket filled with water and a hose connected to the water feed valve that is part of the SPM.</p>
<h6 class="h6" id="lev-12.4.2.5.2"><strong>12.4.2.5.2 Software Environmental Needs</strong></h6>
<p class="numberp1">This section lists all software (and its versions/configurations) that would be needed to run the test. This could include operating systems/device drivers, dynamically linked libraries, simulators, code scaffolding (as in code drivers),<sup><a href="ch19_footnote.xhtml#ch12fn12" id="ch12fn12a">12</a></sup> and test tools.</p>
<h6 class="h6" id="lev-12.4.2.5.3"><strong>12.4.2.5.3 Other Environmental Needs</strong></h6>
<p class="numberp1">This is a catch-all section that lets you add information such as configuration specifics or anything else you feel the need to document. For example, for tests at a specific date or time, you’d need to consider Daylight Saving Time changes where a daily report may have 23 or 25 hours to report on, and so on.</p>
<h5 class="h5" id="lev-12.4.2.6"><strong>12.4.2.6 Special Procedural Requirements</strong></h5>
<p class="noindent">This section lists any exceptional conditions or constraints on the test case. This could also include any special preconditions or postconditions. For example, one precondition on the SPM when testing to see if the software properly responds to a low pool condition is that the water level is <span epub:type="pagebreak" id="page_280"/>below all three low-pool sensors. This should also list any postconditions, such as the bucket must not have overfilled. If you’re using an automated test procedure, this is a good place to specify the particular tool to use and how to employ it for the test.</p>
<p class="indent">Note that this section should not duplicate steps that appear in the test procedure. Instead, it should provide guidance for properly writing the steps in the test procedure that will perform this test case.</p>
<h5 class="h5" id="lev-12.4.2.7"><strong>12.4.2.7 Intercase Dependencies</strong></h5>
<p class="noindent">This section should list (by tag identifier) any test cases that must be executed immediately prior to the current one, so that appropriate system state conditions are in place before the current test is executed. Std 829 suggests that by sequencing the test cases in the order in which they must execute, you can reduce the need to state intercase dependencies. (Obviously, such dependencies should be clearly documented.) In general, however, you shouldn’t rely on such implicit dependency organization and should explicitly document any dependencies. In the STP, though, you <em>can</em> rely on the ordering of test steps. Having already clearly delineated the execution order in the STC will help reduce errors when you create the STP.</p>
<h5 class="h5" id="lev-12.4.2.8"><strong>12.4.2.8 Pass/Fail Criteria</strong></h5>
<p class="noindent">In Std 829, the IEEE recommends putting the pass/fail criteria in the <em>Level</em> Test Design documentation; they are not part of the Std 829 STC. However, it’s not a bad idea, especially in cases where you don’t have an LTD in your documentation set, to include pass/fail criteria for each test case.</p>
<p class="indent">Note that if the pass/fail criterion is simply “All system outputs must match that specified by the Outcome(s) section,” then you can probably dispense with this section, but it wouldn’t hurt to explicitly state this default condition in the introduction section.</p>
<h4 class="h4" id="lev-12.4.3"><strong><em>12.4.3 General</em></strong></h4>
<p class="noindent">This section provides a brief introduction and discussion of the Glossary and Document Change Procedures and History sections.</p>
<h5 class="h5" id="lev-12.4.3.1"><strong>12.4.3.1 Glossary</strong></h5>
<p class="noindent">The Glossary section provides an alphabetical list of all terms used in the STC. It should include all acronyms along with their definitions. Although Std 829 lists the glossary at the end of the outline, it usually appears near the beginning of the document, close to the References section.</p>
<h5 class="h5" id="lev-12.4.3.2"><strong>12.4.3.2 Document Change Procedures and History</strong></h5>
<p class="noindent">This section describes the process for creating, implementing, and approving changes to the STC. This could be nothing more than a reference to a Configuration Management Plan document that describes the document change procedures for all project documents or for all documents within an <span epub:type="pagebreak" id="page_281"/>organization. The change history should contain a chronological list of the following information:</p>
<ul>
<li class="noindent">Document ID (each revision should have a unique ID, which can simply be a date affixed to the document ID)</li>
<li class="noindent">Version number (which you should number sequentially, starting with the first approved version of the STC)</li>
<li class="noindent">A description of the changes made to the STC for the current version</li>
<li class="noindent">Authorship and role</li>
</ul>
<p class="indent">Often, the change history appears in the STC near the beginning of the document, or just after the cover page and near the document identifier.</p>
<h4 class="h4" id="lev-12.4.4"><strong><em>12.4.4 A Sample Software Test Case Document</em></strong></h4>
<p class="noindent">Continuing with the theme of the past couple of chapters, this chapter will provide a sample STC for the Plantation Productions DAQ system DIP switch initialization design. This STC will serve as an acceptance test (pure functional test cases) built exclusively from the project SRS (see “(Selected) DAQ Software Requirements (from SRS)” on <a href="ch10.xhtml#page_219">page 219</a>). The test cases appearing in this sample STC are all the requirements from this project SRS that have not been included in “Requirements to Be Verified by Review” on <a href="ch10.xhtml#page_223">page 223</a> where the “verify by review” requirements are listed. Note, however, that for editorial/space reasons, this example will not provide test cases for every “verify by review” test requirement in that project SRS.<sup><a href="ch19_footnote.xhtml#ch12fn13" id="ch12fn13a">13</a></sup></p>
<table class="topbot-d">
<colgroup>
<col style="width:30%"/>
<col style="width:70%"/>
</colgroup>
<thead>
<tr>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"><strong>Term</strong></p></td>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"><strong>Definition</strong></p></td>
</tr>
</thead>
<tbody>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">DAQ</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">Data acquisition system</p></td>
</tr>
<tr>
<td style="vertical-align: top;"><p class="taba">SBC</p></td>
<td style="vertical-align: top;"><p class="taba">Single-board computer</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">Software Design Description (SDD)</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">Documentation of the design of the software system (IEEE Std 1016-2009)—that is, this document.</p></td>
</tr>
<tr>
<td style="vertical-align: top;"><p class="taba">Software Requirements Specification (SRS)</p></td>
<td style="vertical-align: top;"><p class="taba">Documentation of the essential requirements (functions, performance, design constraints, and attributes) of the software and its external interfaces (IEEE Std 610.12-1990).</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">System Requirements Specification (SyRS)</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">A structured collection of information that embodies the requirements of the system (IEEE Std 1233-1998). A specification that documents the requirements to establish a design basis and the conceptual design for a system or subsystem.</p></td>
</tr>
<tr>
<td style="vertical-align: top;"><p class="taba">Software Test Cases (STC)</p></td>
<td style="vertical-align: top;"><p class="taba">Documentation that describes test cases (inputs and outcomes) to verify correct operation of the software based on various design concerns/requirements (IEEE Std 829-2009).</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">Software Test Procedures (STP)</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">Documentation that describes the step-by-step procedure to execute a set of test cases to verify correct operation of the software based on various design concerns/requirements (IEEE Std 829-2009).</p></td>
</tr>
</tbody>
</table>
<p class="number1"><span epub:type="pagebreak" id="page_282"/><strong>1 Introduction</strong></p>
<p class="numberp1">Software Test Cases for DAQ DIP Switch Project</p>
<p class="number2"><strong>1.1 Document Identifier (and Change History)</strong></p>
<p class="numberp">Mar 22, 2018: DAQ_STC v1.0; Author: Randall Hyde</p>
<p class="number2"><strong>1.2 Scope</strong></p>
<p class="numberp">This document describes only the DIP switch test cases in the DAQ system (for space/editorial reasons). For the full software design description, please see <em><a href="http://www.plantation-productions.com/Electronics/DAQ/DAQ.html">http://www.plantation-productions.com/Electronics/DAQ/DAQ.html</a></em>.</p>
<p class="number2"><strong>1.3 Glossary, Acronyms, and Abbreviations</strong></p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>This is a very simple and short example to keep the book’s page count down. Please don’t use this as boilerplate; you should diligently pick out terms and abbreviations that your document uses and list them in this section.</em></p>
</div>
<p class="number2"><strong>1.4 References</strong></p>
<table class="topbot-d">
<colgroup>
<col style="width:30%"/>
<col style="width:70%"/>
</colgroup>
<thead>
<tr>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"><strong>Reference</strong></p></td>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"><strong>Discussion</strong></p></td>
</tr>
</thead>
<tbody>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">DAQ STC</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">An example of a full STC for the Plantation Productions DAQ system can be found at <em><a href="http://www.plantation-productions.com/Electronics/DAQ/DAQ.html">http://www.plantation-productions.com/Electronics/DAQ/DAQ.html</a></em>.</p></td>
</tr>
<tr>
<td style="vertical-align: top;"><p class="taba">IEEE Std 830-1998</p></td>
<td style="vertical-align: top;"><p class="taba">SRS documentation standard</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">IEEE Std 829-2008</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">STP documentation standard</p></td>
</tr>
<tr>
<td style="vertical-align: top;"><p class="taba">IEEE Std 1012-1998</p></td>
<td style="vertical-align: top;"><p class="taba">Software verification and validation standard</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">IEEE Std 1016-2009</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">SDD documentation standard</p></td>
</tr>
<tr>
<td style="vertical-align: top;"><p class="taba">IEEE Std 1233-1998</p></td>
<td style="vertical-align: top;"><p class="taba">SyRS documentation standard</p></td>
</tr>
</tbody>
</table>
<p class="number2"><strong>1.5 Context</strong></p>
<p class="numberp">The DAQ system of Plantation Productions, Inc., fulfilled a need for a well-documented digital data acquisition and control system that engineers could design into safety-critical systems such as nuclear research reactors. Although there are many COTS systems<sup><a href="ch19_footnote.xhtml#ch12fn14" id="ch12fn14a">14</a></sup> that could be used, they suffer from a couple of major drawbacks, including: they are usually proprietary, thus being difficult to modify or repair after purchase; they are often obsolete within 5 to 10 years without a way to repair or replace them; and they rarely have full support documentation (for example, SRS, SDD, STC, and STP) that an engineer can use to validate and verify the system.</p>
<p class="numberp"><span epub:type="pagebreak" id="page_283"/>The DAQ system overcomes this problem by providing an open hardware and open source set of designs with full design documentation that is validated and verified for safety systems.</p>
<p class="numberp">Although originally designed for a nuclear research reactor, the DAQ system is useful anywhere you need an Ethernet-based control system supporting digital (TTL-level) I/O, optically isolated digital inputs, mechanical or solid-state relay digital outputs, (isolated and conditioned) analog inputs (for example, ±10v and 4-20mA), and (conditioned) analog outputs (±10v).</p>
<p class="number2"><strong>1.6 Notation for Description</strong></p>
<p class="numberp">Test case identifiers (<em>tags</em>) in this document shall take the form:</p>
<p class="numberp">DAQ_STC_<em>xxx</em>_<em>yyy</em>_<em>zzz</em></p>
<p class="numberp">where <em>xxx_yyy</em> is a string of (possibly decimal) numbers taken from the corresponding requirement (for example, DAQ_SRS_<em>xxx</em>_<em>yyy</em>) and <em>zzz</em> is a (possibly decimal) numeric sequence that creates a unique identifier out of the whole sequence. Note that <em>zzz</em> values in STC tags are usually numbered from 000 or 001 and usually increment by 1 for each additional test case item sharing the same <em>xxx_yyy</em> string.</p>
<p class="number1"><strong>2 Details (Test Cases)</strong></p>
<p class="number2"><strong>2.1 DAQ_STC_701_000_000</strong></p>
<p class="numberp">Objective: Test command acceptance across RS-232.</p>
<p class="numberp">Inputs:</p>
<p class="numberpa">1. DIP switch 1 set to ON position.</p>
<p class="numberp">2. Type <code><span class="codestrong1">help</code></span> command on serial terminal.</p>
<p class="numberp">Outcome:</p>
<p class="numberp">1. Screen displays <code>help</code> message.</p>
<p class="numberp">Environmental Needs:</p>
<p class="numberpb1"><strong>Hardware</strong> Functioning (booted) DAQ system, PC with RS-232 port connected to DAQ</p>
<p class="numberpb1"><strong>Software</strong> Latest version of DAQ firmware installed</p>
<p class="numberpb1"><strong>External</strong> Serial terminal emulator program running on PC</p>
<p class="numberp">Special Procedural Requirements:</p>
<p class="numberpb">[None]</p>
<p class="numberp">Intercase Dependencies:</p>
<p class="numberpb">[None]</p>
<p class="number2"><span epub:type="pagebreak" id="page_284"/><strong>2.2 DAQ_STC_702_000_000</strong></p>
<p class="numberp">Objective: Test command acceptance with DIP switch 1 ON.</p>
<p class="numberp">Inputs:</p>
<p class="numberpa">1. DIP switch 1 set to ON position.</p>
<p class="numberp">2. Type <code><span class="codestrong1">help</code></span> command on serial terminal.</p>
<p class="numberp">Outcome:</p>
<p class="numberp">1. Screen displays <code>help</code> message.</p>
<p class="numberp">Environmental Needs:</p>
<p class="numberpb1"><strong>Hardware</strong> Functioning (booted) DAQ system, PC with RS-232 port connected to DAQ</p>
<p class="numberpb1"><strong>Software</strong> Latest version of DAQ firmware installed</p>
<p class="numberpb1"><strong>External</strong> Serial terminal emulator program running on PC</p>
<p class="numberp">Special Procedural Requirements:</p>
<p class="numberpb">[None]</p>
<p class="numberp">Intercase Dependencies:</p>
<p class="numberpb">Same test as DAQ_STC_701_000_000</p>
<p class="number2"><strong>2.3 DAQ_STC_703_000_000</strong></p>
<p class="numberp">Objective: Test command rejection with DIP switch 1 OFF.</p>
<p class="numberp">Inputs:</p>
<p class="numberpa">1. DIP switch 1 set to OFF position.</p>
<p class="numberp">2. Type <code><span class="codestrong1">help</code></span> command on serial terminal.</p>
<p class="numberp">Outcome:</p>
<p class="numberp">1. System ignores command, no response on terminal program.</p>
<p class="numberp">Environmental Needs:</p>
<p class="numberpb1"><strong>Hardware</strong> Functioning (booted) DAQ system, PC with RS-232 port connected to DAQ</p>
<p class="numberpb1"><strong>Software</strong> Latest version of DAQ firmware installed</p>
<p class="numberpb1"><strong>External</strong> Serial terminal emulator program running on PC</p>
<p class="numberp">Special Procedural Requirements:</p>
<p class="numberpb">[None]</p>
<p class="numberp">Intercase Dependencies:</p>
<p class="numberpb">[None]</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>For space/editorial reasons, this sample has deleted several test cases at this point because they are very similar in content to the previous test cases.</em></p>
</div>
<p class="number2"><span epub:type="pagebreak" id="page_285"/><strong>2.4 DAQ_STC_709_000_000</strong></p>
<p class="numberp">Objective: Test Ethernet address with both DIP switches 5 and 6 OFF.</p>
<p class="numberp">Inputs:</p>
<p class="numberp">1. DIP switch 3 set to ON position (4 = don’t care).</p>
<p class="numberp">2. DIP switch 5 set to OFF position.</p>
<p class="numberp">3. DIP switch 6 set to OFF position</p>
<p class="numberp">4. Using an Ethernet terminal program, attempt connection to IP address 192.168.2.70, port 20560 (0x5050).</p>
<p class="numberp">5. Issue <code><span class="codestrong1">help</code></span> command.</p>
<p class="numberp">Outcome:</p>
<p class="numberp">1. Ethernet terminal connects to DAQ system.</p>
<p class="numberp">2. Terminal program display DAQ <code>help</code> message.</p>
<p class="numberp">Environmental Needs:</p>
<p class="numberpb1"><strong>Hardware</strong> Functioning (booted) DAQ system, PC with Ethernet port connected to DAQ</p>
<p class="numberpb1"><strong>Software</strong> Latest version of DAQ firmware installed</p>
<p class="numberpb1"><strong>External</strong> Ethernet terminal emulator program running on PC</p>
<p class="numberp">Special Procedural Requirements:</p>
<p class="numberpb">[None]</p>
<p class="numberp">Intercase Dependencies:</p>
<p class="numberpb">Cases DAQ_STC_708_000_000 to DAQ_STC_718_001_000 are closely related and should be performed together.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>For space/editorial reasons, this sample has deleted several test cases at this point because they are very similar in content to the previous test cases.</em></p>
</div>
<p class="number2"><strong>2.6 DAQ_STC_710_000_000</strong></p>
<p class="numberp">Objective: Test Ethernet address with DIP switches 5 ON and 6 OFF.</p>
<p class="numberp">Inputs:</p>
<p class="numberp">1. DIP switch 3 set to ON position (4 = don’t care).</p>
<p class="numberp">2. DIP switch 5 set to ON position.</p>
<p class="numberp">3. DIP switch 6 set to OFF position.</p>
<p class="numberp">4. Using an Ethernet terminal program, attempt connection to IP address 192.168.2.71, port 20560 (0x5050).</p>
<p class="numberp">5. Issue <code><span class="codestrong1">help</code></span> command.</p>
<p class="numberp">Outcome:</p>
<p class="numberp">1. Ethernet terminal connects to DAQ system.</p>
<p class="numberp">2. Terminal program displays DAQ <code>help</code> message.</p>
<p class="numberp"><span epub:type="pagebreak" id="page_286"/>Environmental Needs:</p>
<p class="numberpb1"><strong>Hardware</strong> Functioning (booted) DAQ system, PC with Ethernet port connected to DAQ</p>
<p class="numberpb1"><strong>Software</strong> Latest version of DAQ firmware installed</p>
<p class="numberpb1"><strong>External</strong> Ethernet terminal emulator program running on PC</p>
<p class="numberp">Special Procedural Requirements:</p>
<p class="numberpb">[None]</p>
<p class="numberp">Intercase Dependencies:</p>
<p class="numberpb">Cases DAQ_STC_708_000_000 to DAQ_STC_718_001_000 are closely related and should be performed together.</p>
<p class="number2"><strong>2.7 DAQ_STC_711_000_000</strong></p>
<p class="numberp">Objective: Test Ethernet address with DIP switch 5 OFF and 6 ON.</p>
<p class="numberp">Inputs:</p>
<p class="numberp">1. DIP switch 3 set to ON position (4 = don’t care).</p>
<p class="numberp">2. DIP switch 5 set to OFF position.</p>
<p class="numberp">3. DIP switch 6 set to ON position.</p>
<p class="numberp">4. Using an Ethernet terminal program, attempt connection to IP address 192.168.2.72, port 20560 (0x5050).</p>
<p class="numberp">5. Issue <code><span class="codestrong1">help</code></span> command.</p>
<p class="numberp">Outcome:</p>
<p class="numberp">1. Ethernet terminal connects to DAQ system.</p>
<p class="numberp">2. Terminal program displays DAQ <code>help</code> message.</p>
<p class="numberp">Environmental Needs:</p>
<p class="numberpb1"><strong>Hardware</strong> Functioning (booted) DAQ system, PC with Ethernet port connected to DAQ</p>
<p class="numberpb1"><strong>Software</strong> Latest version of DAQ firmware installed</p>
<p class="numberpb1"><strong>External</strong> Ethernet terminal emulator program running on PC</p>
<p class="numberp">Special Procedural Requirements:</p>
<p class="numberpb">[None]</p>
<p class="numberp">Intercase Dependencies:</p>
<p class="numberpb">Cases DAQ_STC_708_000_000 to DAQ_STC_718_001_000 are closely related and should be performed together.</p>
<p class="number2"><strong>2.8 DAQ_STC_712_000_000</strong></p>
<p class="numberp">Objective: Test Ethernet address with both DIP switches 5 and 6 ON.</p>
<p class="numberp">Inputs:</p>
<p class="numberp">1. DIP switch 3 set to ON position (4 = don’t care).</p>
<p class="numberp">2. DIP switch 5 set to ON position.</p>
<p class="numberp"><span epub:type="pagebreak" id="page_287"/>3. DIP switch 6 set to ON position.</p>
<p class="numberp">4. Using an Ethernet terminal program, attempt connection to IP address 192.168.2.73, port 20560 (0x5050).</p>
<p class="numberp">5. Issue <code><span class="codestrong1">help</code></span> command.</p>
<p class="numberp">Outcome:</p>
<p class="numberp">1. Ethernet terminal connects to DAQ system.</p>
<p class="numberp">2. Terminal program displays DAQ <code>help</code> message.</p>
<p class="numberp">Environmental Needs:</p>
<p class="numberpb1"><strong>Hardware</strong> Functioning (booted) DAQ system, PC with Ethernet port connected to DAQ</p>
<p class="numberpb1"><strong>Software</strong> Latest version of DAQ firmware installed</p>
<p class="numberpb1"><strong>External</strong> Ethernet terminal emulator program running on PC</p>
<p class="numberp">Special Procedural Requirements:</p>
<p class="numberpb">[None]</p>
<p class="numberp">Intercase Dependencies:</p>
<p class="numberpb">Cases DAQ_STC_708_000_000 to DAQ_STC_718_001_000 are closely related and should be performed together.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>For space/editorial reasons, this sample has deleted several test cases at this point because they are very similar in content to the previous test cases.</em></p>
</div>
<p class="number2"><strong>2.9 DAQ_STC_726_000_000</strong></p>
<p class="numberp">Objective: Test command acceptance from RS-232 port.</p>
<p class="numberp">Inputs:</p>
<p class="numberpa">1. DIP switch 1 set to ON position.</p>
<p class="numberp">2. Type <code><span class="codestrong1">help</code></span> command on serial terminal.</p>
<p class="numberp">Outcome:</p>
<p class="numberpa">1. Screen displays <code>help</code> message.</p>
<p class="numberp">Environmental Needs:</p>
<p class="numberpb1"><strong>Hardware</strong> Functioning (booted) DAQ system, PC with RS-232 port connected to DAQ</p>
<p class="numberpb1"><strong>Software</strong> Latest version of DAQ firmware installed</p>
<p class="numberpb1"><strong>External</strong> Serial terminal emulator program running on PC</p>
<p class="numberp">Special Procedural Requirements:</p>
<p class="numberpb">[None]</p>
<p class="numberp">Intercase Dependencies:</p>
<p class="numberpb">Same test as DAQ_STC_701_000_000</p>
<p class="number1"><span epub:type="pagebreak" id="page_288"/><strong>3 Test Case Document Change Procedure</strong></p>
<p class="numberp1">When making any modifications to this STC, the author of the change must make a new entry in section 1.1 of this STC document, listing (at a minimum) the date, document ID (DAQ_STC), version number, and authorship.</p>
<h4 class="h4" id="lev-12.4.5"><strong><em>12.4.5 Updating the RTM with STC Information</em></strong></h4>
<p class="noindent">Due to software review and software test case (and analysis/other) verification methods being mutually exclusive, you need only a single column in the RTM to associate the tags for these objects with other items in the RTM. In the RTM of the official DAQ system (which has only test cases and software review items), the label for this column is simply <em>Software Test/Review Cases</em>. When you add both DAQ_SR_<em>xxx_yyy_zzz</em> and DAQ_STC_<em>xxx_yyy_zzz</em> items to this column, there is never any ambiguity as the tag clearly identifies which verification type you’re using. Of course, this assumes that you’re using the tag identifier format that this chapter suggests. You could use your own tag format that also differentiates review and test case items in the tag name.</p>
<p class="indent">If you’re using this chapter’s STC tag format, locating the row in the RTM where you want to place the test case tag is very easy. Just locate the requirement with the tag DAQ_SRS_<em>xxx_yyy</em> and add the STC tag to the appropriate column in the same row. If you’re using a different tag format that doesn’t include requirement traceability directly in the tag name, you’ll have to determine the association manually (hopefully it’s contained within the test case).</p>
<h3 class="h3" id="lev-12.5"><strong>12.5 Software Test Procedure Documentation</strong></h3>
<p class="noindent">The <em>Software Test Procedure (STP)</em> specifies the steps for executing a collection of test cases, which, in turn, evaluate the quality of the software system. In one respect, the STP is an optional document; after all, if you execute all the test cases (in an appropriate order), you will fully test all the test cases. The purpose behind an STP is to streamline the testing process. More often than not, test cases overlap. Although they test different requirements, it may turn out that the inputs for multiple test cases are identical. In some cases, even the outcomes are identical. By merging such test cases into a single procedure, you can run a single test sequence that handles all test cases.</p>
<p class="indent">Another reason for merging test cases into a single STP is the convenience of a common setup. Many test cases require (possibly elaborate) setup to ensure certain environmental conditions prior to execution. More often than not, multiple test cases require the same setup prior to their execution. By merging those test cases into a single procedure, you can perform the setup once for the entire set rather than repeating it for each and every test case.</p>
<p class="indent">Finally, some test cases may have dependencies that require other test cases to execute prior to their execution. By putting these test cases in a test procedure, you can ensure that the test operation satisfies the dependencies.</p>
<p class="indent"><span epub:type="pagebreak" id="page_289"/>Std 829 defines a set of Level <em>Test Procedures (LTPr)</em>. As with all of the <em>level</em> test documents in Std 829 there are four variants of the LTPr, each variant being a document generally describing the scope or extent of software testing being documented: Component Test Procedures (aka Unit Test Procedures, or UTP), Component Integration Test Procedures (aka Integration Test Procedures, or ITP), System Test Procedures (aka System Integration Test Procedures, or SITP), and Acceptance Test Procedures (ATP; may include Factory Acceptance Test Procedures [FATP] or Site Acceptance Test Procedures [SATP]).<sup><a href="ch19_footnote.xhtml#ch12fn15" id="ch12fn15a">15</a></sup></p>
<p class="indent">UTPs and ITPs are often automated test procedures or less formal documents, similar to their test case document counterparts; see “<a href="ch12.xhtml#lev-12.4">Software Test Case Documentation</a>” on <a href="ch12.xhtml#page_274">page 274</a> for an in-depth discussion.</p>
<p class="indent">If you look back at <a href="ch12.xhtml#ch12fig1">Figures 12-1</a> and <a href="ch12.xhtml#ch12fig2">12-2</a>, you can see that the STP (and all LTPrs) are derived directly from the STC (LTC) documentation. <a href="ch12.xhtml#ch12fig1">Figure 12-1</a> applies to UTPs and ITPs. <a href="ch12.xhtml#ch12fig2">Figure 12-2</a> applies to SITPs and ATPs (noting that ATPs derive from test cases that come strictly from SyRS/SRS requirements, not from SDD elements).</p>
<p class="indent">As is true for test case documentation, ATPs are usually a subset of the SITPs to the customer or end user. Likewise, if there are FATP and SATP documents, the SATP is often a subset of the FATP, with further refinement to end-user requirements.<sup><a href="ch19_footnote.xhtml#ch12fn16" id="ch12fn16a">16</a></sup></p>
<h4 class="h4" id="lev-12.5.1"><strong><em>12.5.1 The IEEE Std 829-2009 Software Test Procedure</em></strong></h4>
<p class="noindentb">The outline for the Std 829 STP is as follows:</p>
<p class="number1">1 Introduction</p>
<p class="number2">1.1 Document Identifier</p>
<p class="number2">1.2 Scope</p>
<p class="number2">1.3 References</p>
<p class="number2">1.4 Relationship to Other Documents</p>
<p class="number1">2 Details</p>
<p class="number2">2.1 Inputs, Outputs, and Special Requirements</p>
<p class="number2">2.2 Ordered Description of the Steps to Be Taken to Execute the Test Cases</p>
<p class="number1">3 General</p>
<p class="number2">3.1 Glossary</p>
<p class="number2">3.2 Document Change Procedures and History</p>
<h4 class="h4" id="lev-12.5.2"><span epub:type="pagebreak" id="page_290"/><em>12.5.2 Extended Outline for Software Test Procedure</em></h4>
<p class="noindent">As is typical for IEEE standards, you’re allowed to augment this outline (adding, deleting, moving, and editing items, with appropriate justification). This flexibility is important in this particular case because there are a couple of things missing from this outline.</p>
<p class="indent">First of all, the introduction is missing Notation for Descriptions, which appears in the STC outline (“Software Test Case Documentation” on <a href="ch12.xhtml#page_274">page 274</a>).<sup><a href="ch19_footnote.xhtml#ch12fn17" id="ch12fn17a">17</a></sup> Perhaps the authors of Std 829 were expecting very few test procedures to appear in Section 2 (“Details”) of the document. In practice, however, it’s common to have a large number of test procedures. There are some very good reasons for breaking a single large test procedure into a series of smaller ones:</p>
<ul>
<li class="noindent">Testing can take place in parallel. By assigning (independent) test procedures to multiple test teams, you can complete the testing faster.</li>
<li class="noindent">Certain tests may tie up resources (for example, test equipment such as oscilloscopes, logic analyzers, test fixtures, and signal generators). By breaking up a large test procedure into smaller test procedures, you may be able to limit the time a testing team needs access to certain resources.</li>
<li class="noindent">It’s nice to be able to complete a test procedure within a single working day (or even between breaks in the day) so testers don’t lose focus when performing tests.</li>
<li class="noindent">Organizing test procedures by their related activities (and by required setup prior to those activities) can streamline test procedures, reducing steps and making them more efficient to run.</li>
<li class="noindent">Many organizations require a testing team to rerun a test procedure from the beginning (a regression test) if any part of that test fails. Breaking a test procedure into smaller pieces makes rerunning test procedures far less expensive.</li>
</ul>
<p class="indent">To be able to trace these test procedures back to the STC, to the SRS, and to other documentation in the RTM, you’re going to need test procedure identifiers (tags). Therefore, you should have a section to describe the notation you’re using for these tags.</p>
<p class="indent">Of course, the second thing missing from the IEEE outline is an entry for the test procedure identification in the <em>Details</em> section. To make traceability easier, it would also be nice to have a section in each test procedure where you list the associated test cases it covers. Finally, for my own purposes, I like to include the following information with each test procedure:</p>
<ul>
<li class="noindent">Brief description</li>
<li class="noindent">Tag/identification</li>
<li class="noindent">Purpose</li>
<li class="noindent"><span epub:type="pagebreak" id="page_291"/>Traceability (test cases covered)</li>
<li class="noindent">Pass/fail criteria (as this may change with each procedure)</li>
<li class="noindent">Any special requirements (for example, environmental) required to run this test procedure; this could include input/output files that must exist, among other things</li>
<li class="noindent">All setup required prior to running the test procedure</li>
<li class="noindent">Software version number when executing the test procedure</li>
<li class="noindent">Procedure steps to execute the test procedure</li>
</ul>
<p class="indentb">Incorporating these items produces the following extended outline for an arbitrary STP suitable for an SIT, AT, FAT, or SAT:</p>
<p class="number1">1 Table of Contents</p>
<p class="number1">2 Introduction</p>
<p class="number2">2.1 Document Identifier and Change History (moved)</p>
<p class="number2">2.2 Scope</p>
<p class="number2">2.3 Glossary, Acronyms, and Abbreviations (moved)</p>
<p class="number2">2.4 References</p>
<p class="number2">2.5 Notation for Descriptions</p>
<p class="number2">2.6 Relationship to Other Documents (removed)</p>
<p class="number2">2.7 Instructions for Running the Tests (added)</p>
<p class="number1">3 Test Procedures (name changed from <em>Details</em>)</p>
<p class="number2">3.1 Brief Description (simple phrase), Procedure #1</p>
<p class="number3">3.1.1 Procedure Identification (Tag)</p>
<p class="number3">3.1.2 Purpose</p>
<p class="number3">3.1.3 List of Test Cases Covered by This Procedure</p>
<p class="number3">3.1.4 Special Requirements</p>
<p class="number3">3.1.5 Setup Required Prior to Running Procedure</p>
<p class="number3">3.1.6 Software Version Number for This Execution</p>
<p class="number3">3.1.7 Detailed Steps to Run the Procedure</p>
<p class="number3">3.1.8 Sign-off on Test Procedure</p>
<p class="number2">3.2 Brief Description (simple phrase), Procedure #2</p>
<ul class="bullet-1">
<li class="noindent">(Same subsections as previous section)</li>
<li class="noindent">. . .</li>
</ul>
<p class="number2">3.<em>n</em>	Brief Description (simple phrase), Procedure #<em>n</em></p>
<ul class="bullet-1">
<li class="noindent">(Same subsections as previous sections)</li>
</ul>
<p class="number1">4 General</p>
<p class="number2">4.1 Document Change Procedures</p>
<p class="number2">4.2 Attachments and Appendixes</p>
<p class="number1">5 Index</p>
<h4 class="h4" id="lev-12.5.3"><span epub:type="pagebreak" id="page_292"/><strong><em>12.5.3 Introduction in the STP Document</em></strong></h4>
<p class="noindent">The following subsections describe the components of the STP introduction.</p>
<h5 class="h5" id="lev-12.5.3.1"><strong>12.5.3.1 Document Identifier and Change History</strong></h5>
<p class="noindent">The document identifier should be some (organization-wide) unique name; this will typically include some project designation such as <em>DAQ_STP</em>, a creation/modification date, a version number, and authorship. A list of these identifiers (one for each revision to the document) would form the change history.</p>
<h5 class="h5" id="lev-12.5.3.2"><strong>12.5.3.2 Scope</strong></h5>
<p class="noindent">The scope here has largely the same definition as that used for the STC (see “<a href="ch12.xhtml#lev-12.4">Software Test Case Documentation</a>” on <a href="ch12.xhtml#page_274">page 274</a>). Std 829 suggests describing the scope of the STP based on its focus and relationship to the STC and other test documentation. More often than not, you can get away with a simple reference to the Scope section in the STC.</p>
<h5 class="h5" id="lev-12.5.3.3"><strong>12.5.3.3 References</strong></h5>
<p class="noindent">As usual, provide a link to any external documents (such as the STC) that are relevant to the STP. Std 829 also suggests including links to the individual test cases covered by this procedure. That, however, would be meaningful only if the STP contained just a few test procedures. In this revised format, the STP will attach the test case links to the individual test procedures in Section 3 (“Test Procedures”). If you have a very large system consisting of multiple, independent applications, you will probably have separate STPs for each of those applications. You would want to provide links to those other STPs in this section of the STP document.</p>
<h5 class="h5" id="lev-12.5.3.4"><strong>12.5.3.4 Notation for Descriptions</strong></h5>
<p class="noindent">As in the STC, you would describe your STP tag format here. This book recommends using STP tags of the form <em>proj</em>_STP_<em>xxx</em>, where <em>proj</em> is some project-specific ID (such as <em>DAQ</em> or <em>POOL</em>) and <em>xxx</em> is some unique (possibly decimal) numeric sequence.</p>
<p class="indent">Note that there is a many-to-one relationship from STC test cases to STP test procedures. Therefore, you cannot easily embed traceability information into the STP tags (there’s a similar situation with SDD tags; see “<a href="ch11.xhtml#lev-11.4">SDD Traceability and Tags</a>” on <a href="ch11.xhtml#page_245">page 245</a>). This is why it’s important to include the related STC tags with each test procedure, to facilitate traceability back to the corresponding test cases.</p>
<h5 class="h5" id="lev-12.5.3.5"><strong>12.5.3.5 Relationship to Other Documents</strong></h5>
<p class="noindent">In the modified variant of the STP, I’ve removed this section. Std 829 suggests using it to describe the relationship of this STP to other test procedure <span epub:type="pagebreak" id="page_293"/>documents—specifically, which test procedures must be performed before or after other test procedures. However, in the modified form all test procedures appear in the same document. Therefore, a description of the relationship between tests should accompany each individual test procedure. (This information appears in the “Special Requirements” section.)</p>
<p class="indent">This is one reason for including this section in the modified form of the STP: very large systems may contain multiple (and relatively independent) software applications. There would probably be separate STP documents for each of these applications. This section of the modified STP could describe the relationship of this STP to those others, including the order in which tests must execute these STPs.</p>
<h5 class="h5" id="lev-12.5.3.6"><strong>12.5.3.6 Instructions for Running Tests</strong></h5>
<p class="noindent">This section should contain generic instructions to whomever will be running the tests. Usually the people running the tests are not the software developers.<sup><a href="ch19_footnote.xhtml#ch12fn18" id="ch12fn18a">18</a></sup> This section can provide insights into the software to be tested for those who have not lived with it on a daily basis from its inception.</p>
<p class="indent">One important piece of information that should appear here is what to do if a test procedure fails. Should the tester attempt to continue that test procedure (if possible) in hopes of finding additional problems? Should the tester immediately suspend testing operations until the development team resolves the issue? If a test has been suspended, what is the process for resuming the test? For example, most QA teams require, at the very least, rerunning the test procedure from the beginning.<sup><a href="ch19_footnote.xhtml#ch12fn19" id="ch12fn19a">19</a></sup> Some QA teams may also require a meeting with development to determine a set of regression tests to run before resuming the test procedure from the point of failure.</p>
<p class="indent">This section should also discuss how to log any problems/anomalies that occur during testing and to describe how to bring the system back into a stable state or shut it down should a critical or catastrophic event occur.</p>
<p class="indent">This is also where you’ll describe how to log successful runs of a test procedure. A tester will usually log the date and time they begin a test, provide the name of the test engineer, and specify which test procedure they are executing. At the successful conclusion of a test, most test procedures require signatures by the test engineer, a possible QA or customer representative, and possibly other managerial or project-related personnel. This section should describe the process for obtaining these signatures and signing off on successful runs of a test procedure.</p>
<h4 class="h4" id="lev-12.5.4"><span epub:type="pagebreak" id="page_294"/><strong><em>12.5.4 Test Procedures</em></strong></h4>
<p class="noindent">This section of the document repeats for each individual test procedure for the system under test. This is a modification of the Std 829 STP, which describes only a single (or maybe a few) test procedures in the document. Presumably, there would be multiple STP documents if your system requires a large number of test procedures.</p>
<h5 class="h5" id="lev-12.5.4.1"><strong>12.5.4.1 Brief Description (for Test Procedure #1)</strong></h5>
<p class="noindent">This is the title of the test procedure. It should be a short phrase, such as <em>DIP Switch #1 Test</em>, that provides a quick and perhaps informal procedure identification.</p>
<h6 class="h6"><strong>Procedure Identification</strong></h6>
<p class="numberp1">This is the unique identifier (tag) for this test procedure. Other documentation (such as the RTM) will reference this test procedure using its tag.</p>
<h6 class="h6"><strong>Purpose</strong></h6>
<p class="numberp1">This is an expanded description of this test procedure: why it exists, what it tests, and where it sits in the big picture.</p>
<h6 class="h6"><strong>List of Test Cases Covered by This Procedure</strong></h6>
<p class="numberp1">This section provides reverse traceability back to the STC document. It is simply a list of all the test cases that this test procedure covers. Note that this set of test cases should be mutually exclusive of the sets found in other test procedures—no test case tag should ever appear in more than one test procedure. You want to preserve the many-to-one relationship from test cases to test procedures. This will help keep the RTM clean, meaning that you won’t have to attach multiple test procedures to the same row in the RTM.</p>
<p class="uln-indenti">Now, it is quite possible that multiple test procedures will provide inputs (and verify corresponding outcomes) that test the same test case. This isn’t a problem; just pick one procedure that will take credit for covering that test case and assign the test case to that procedure. When someone is tracing through the requirements and verifying that the test procedures test a particular requirement, they’re not going to care if the test procedures test that requirement multiple times; they’ll be interested only in determining that the requirement has been tested at least once somewhere in the test procedures.</p>
<p class="uln-indenti">If you have a choice of test procedures with which to associate a given test case, it’s best to include that test case in a test procedure that also handles related test cases. Of course, in general, this type of association, whereby related test cases are put into the same test procedure, happens automatically. That’s because you don’t arbitrarily create test procedures and then assign test cases to them. Instead, you pick a set of (related) test cases and use them to generate a test procedure.</p>
<h6 class="h6"><span epub:type="pagebreak" id="page_295"/><strong>Special Requirements</strong></h6>
<p class="numberp1">This section identifies anything external you’ll need for the test procedure in order to successfully execute the test. This includes databases, input files, existing directory paths, online resources (such as web pages), dynamically linked libraries and other third-party tools, and automated test procedures.</p>
<h6 class="h6"><strong>Setup Required Prior to Running Procedure</strong></h6>
<p class="numberp1">This section describes any processes or procedures to execute before you can run the test procedure. For example, a test procedure for autonomous vehicle software might require an operator to drive the vehicle to a specified starting point on a test track before starting the test. Other examples might be ensuring an internet or server connection is available. With the SPM, an example of setup could include ensuring that the test fixture (five-gallon bucket of water) is filled to some specified level.</p>
<h6 class="h6"><strong>Software Version Number for This Execution</strong></h6>
<p class="numberp1">This is a “fill in the blank” field for the test procedure. It does not mandate a software version for running the test; rather, the tester enters the current software version number prior to the test’s execution. Note that this field has to be filled in for each test procedure. You cannot simply write this value down once for the whole STP. The reason is quite simple: during testing you may encounter defects that require you to suspend the test. Once the development team corrects those defects, the testing can resume, usually from the beginning of the test procedure. Because different procedures in an STP could have been run on different versions of the software, you need to identify which version of the software you’re using when running each procedure.<sup><a href="ch19_footnote.xhtml#ch12fn20" id="ch12fn20a">20</a></sup></p>
<h6 class="h6"><strong>Detailed Steps Required to Run This Procedure</strong></h6>
<p class="numberp1b">This section contains steps that are necessary to execute the test procedure. There are two types of steps in a test procedure: actions and verifications. An <em>action</em> is a statement of work to be done, such as providing some input to the system. A <em>verification</em> involves checking some outcome/output and confirming that the system is operating correctly.</p>
<p class="numberp1i">You must number all procedure steps sequentially—typically starting from 1, though you could also use section numbers like 3.2.1 through 3.2.40 for a test procedure that has 40 steps. At the very least, each verification step should be preceded by three or so underline characters (___) or a box symbol (see <a href="ch12.xhtml#ch12fig3">Figure 12-3</a>) so that the tester can physically check off the step once they have successfully completed <span epub:type="pagebreak" id="page_296"/>it. Some people prefer putting the checkbox on every item (that is, both actions and verifications) in the test procedure to ensure that the tester marks off each step as they complete it. Perhaps there should be lines on the actions and checkboxes on the verifications. However, this adds considerable menial work to the process, so consider carefully whether it’s important enough to do.</p>
<div class="image"><a id="ch12fig3"/><img src="Images/fig12-3.jpg" alt="image" width="195" height="33"/></div>
<p class="figcap"><em>Figure 12-3: Using a checkbox on a verify statement</em></p>
<p class="numberp1i">Note that the detailed steps should include information (in appropriate positions) such as the following:</p>
<ul class="bullet1zz">
<li class="noindent">Any actions needed to start the procedure (obviously, these should appear in the first few steps of the procedure)</li>
<li class="noindent">A discussion of how to make measurements or observe outputs (don’t assume the tester is as familiar with the software as the developers are)</li>
<li class="noindent">How to shut down the system at the conclusion of the test procedure to leave the system in a stable state (if this is necessary, it will obviously appear in the last steps of the procedure)</li>
<li class="noindent">Sign-off</li>
</ul>
<p class="numberp1i">At the end of the test procedure there should be blank lines for the tester, observers, customer representatives, and possibly management personnel to sign off on the successful conclusion of the test procedure. A signature and date are the minimum information that should appear here. Each organization may mandate which signatures are necessary. At the very least (such as in a one-person shop), whoever executes the test procedure should sign and date it to affirm that it was run.</p>
<h4 class="h4" id="lev-12.5.5"><strong><em>12.5.5 General</em></strong></h4>
<p class="noindent">The last section of an STP is a generic catch-all section where you can place information that doesn’t fit anywhere else.</p>
<h5 class="h5" id="lev-12.5.5.1"><strong>12.5.5.1 Document Change Procedures</strong></h5>
<p class="noindent">Many organizations have set policies for changing test procedure documents. They could, for example, require customer approval before making official changes to an ATP. This section outlines the rules and necessary approval procedures and processes for making changes to the STP.</p>
<h5 class="h5" id="lev-12.5.5.2"><strong>12.5.5.2 Attachments and Appendixes</strong></h5>
<p class="noindent">It’s often useful to attach large tables, images, and other documentation directly to the LTP so that it is always available to a reader, as opposed to providing a link to a document that the reader cannot access.</p>
<h4 class="h4" id="lev-12.5.6"><span epub:type="pagebreak" id="page_297"/><strong><em>12.5.6 Index</em></strong></h4>
<p class="noindent">If desired, you can add an index at the end of the STP.</p>
<h4 class="h4" id="lev-12.5.7"><strong><em>12.5.7 A Sample STP</em></strong></h4>
<p class="noindentb">This section presents a shortened (for space/editorial purposes) example of an STP for the DAQ DIP switch project.</p>
<p class="number1"><strong>1 Table of Contents</strong></p>
<p class="numberp1">[Omitted for space reasons]</p>
<p class="number1"><strong>2 Introduction</strong></p>
<p class="number2"><strong>2.1 Document Identifier</strong></p>
<p class="numberp">Mar 22, 2018: DAQ_LTP, Version 1.0 Randall Hyde</p>
<p class="number2"><strong>2.2 Scope</strong></p>
<p class="numberp">This document describes some of the DIP switch test procedures in the DAQ system (shortened for space/editorial reasons).</p>
<p class="number2"><strong>2.3 Glossary, Acronyms, and Abbreviations</strong></p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>This is a very simple and short example to keep this book smaller. Please don’t use this as boilerplate; you should diligently pick out terms and abbreviations your document uses and list them in this section.</em></p>
</div>
<table class="topbot-d">
<colgroup>
<col style="width:30%"/>
<col style="width:70%"/>
</colgroup>
<thead>
<tr>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"><strong>Term</strong></p></td>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"><strong>Definition</strong></p></td>
</tr>
</thead>
<tbody>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">DAQ</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">Data acquisition system</p></td>
</tr>
<tr>
<td style="vertical-align: top;"><p class="taba">SBC</p></td>
<td style="vertical-align: top;"><p class="taba">Single-board computer</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">Software Design Description (SDD)</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">Documentation of the design of the software system (IEEE Std 1016-2009)—that is, this document.</p></td>
</tr>
<tr>
<td style="vertical-align: top;"><p class="taba">Software Requirements Specification (SRS)</p></td>
<td style="vertical-align: top;"><p class="taba">Documentation of the essential requirements (functions, performance, design constraints, and attributes) of the software and its external interfaces (IEEE Std 610.12-1990).</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">System Requirements Specification (SyRS)</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">A structured collection of information that embodies the requirements of the system (IEEE Std 1233-1998). A specification that documents the requirements to establish a design basis and the conceptual design for a system or subsystem.</p></td>
</tr>
<tr>
<td style="vertical-align: top;"><p class="taba">Software Test Cases (STC)</p></td>
<td style="vertical-align: top;"><p class="taba">Documentation that describes test cases (inputs and outcomes) to verify correct operation of the software based on various design concerns/requirements (IEEE Std 829-2009).</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">Software Test Procedures (STP)</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">Documentation that describes the step-by-step procedure to execute a set of test cases to verify correct operation of the software based on various design concerns/requirements (IEEE Std 829-2009).</p></td>
</tr>
</tbody>
</table>
<p class="number2"><span epub:type="pagebreak" id="page_298"/><strong>2.4 References</strong></p>
<table class="topbot-d">
<colgroup>
<col style="width:30%"/>
<col style="width:70%"/>
</colgroup>
<thead>
<tr>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"><strong>Reference</strong></p></td>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"><strong>Discussion</strong></p></td>
</tr>
</thead>
<tbody>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">DAQ STC</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">See “<a href="ch12.xhtml#lev-12.4.4">A Sample Software Test Case Document</a>” on <a href="ch12.xhtml#page_281">page 281</a>.</p></td>
</tr>
<tr>
<td style="vertical-align: top;"><p class="taba">DAQ STP</p></td>
<td style="vertical-align: top;"><p class="taba">An example of a full STP for the Plantation Productions DAQ system can be found at <em><a href="http://www.plantation-productions.com/Electronics/DAQ/DAQ.html">http://www.plantation-productions.com/Electronics/DAQ/DAQ.html</a></em>.</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">IEEE Std 830-1998</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">SRS documentation standard</p></td>
</tr>
<tr>
<td style="vertical-align: top;"><p class="taba">IEEE Std 829-2008</p></td>
<td style="vertical-align: top;"><p class="taba">STP documentation standard</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">IEEE Std 1012-1998</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">Software verification and validation standard</p></td>
</tr>
<tr>
<td style="vertical-align: top;"><p class="taba">IEEE Std 1016-2009</p></td>
<td style="vertical-align: top;"><p class="taba">SDD documentation standard</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">IEEE Std 1233-1998</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">SyRS documentation standard</p></td>
</tr>
</tbody>
</table>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>An additional reference that might make sense (not included here because it doesn’t exist for this simple project) is a link to any associated documentation for the DAQ system, such as programming manuals or schematics.</em></p>
</div>
<p class="number2"><strong>2.5 Notation for Descriptions</strong></p>
<p class="numberp">Test procedure identifiers (<em>tags</em>) in this document shall take the form:</p>
<p class="numberp">DAQ_STP_<em>xxx</em></p>
<p class="numberp">where <em>xxx</em> is a (possibly dotted decimal) numeric sequence that creates a unique identifier out of the whole sequence. Note that <em>xxx</em> values for STP tags are usually numbered from 000 or 001 and usually increment by 1 for each additional test case item sharing the same <em>xxx</em> string.</p>
<p class="number2"><strong>2.6 Instructions for Running the Tests</strong></p>
<p class="numberp">Execute each test procedure exactly as stated. If tester encounters an error or omission in the procedure, tester should redline (with red ink, which tester should use only for redlines) the procedure with the correct information and justify the redline in the test log (with date/timestamp and signature). All redlines within the test procedure(s) must be initialized by all signatories at the end of the test procedure.</p>
<p class="numberp">If tester discovers a defect in the software itself (that is, not simply a defect in the test procedure), the tester shall note the anomaly in a test log and create an Anomaly Report for the defect. If the defect is marginal or negligible in nature, the tester may continue with the test procedure, if possible, and attempt to find any other defects in the system on the same test procedure run. If the defect is critical or catastrophic in nature, or the defect is such that it is impossible to continue the test procedure, the tester shall <span epub:type="pagebreak" id="page_299"/>immediately suspend the test and shut off power to the system. Once the defect is corrected, tester must restart the test procedure from the beginning of the procedure.</p>
<p class="numberp">A test procedure succeeds if and only if the tester completes all steps without any failures.</p>
<p class="number1"><strong>3 Test Procedures</strong></p>
<p class="number2"><strong>3.1 RS-232 (Serial Port) Operation</strong></p>
<p class="number3">3.1.1 DAQ_STP_001</p>
<p class="number3">3.1.2 Purpose</p>
<p class="numberp2">This test procedure tests the proper operation of DAQ commands sourced from the RS-232 port.</p>
<p class="number3">3.1.3 Test Cases</p>
<p class="numberp2">DAQ_STC_701_000_000</p>
<p class="numberp2">DAQ_STC_702_000_000</p>
<p class="numberp2">DAQ_STC_703_000_000</p>
<p class="numberp2">DAQ_STC_726_000_000</p>
<p class="number3">3.1.4 Special Requirements</p>
<p class="numberp2">This test procedure requires a serial terminal emulator program running on a PC (for example, the <em>MTTY.exe</em> program that comes as part of the Netburner SDK; you could even use Hyperterm if you are masochistic). There should be a NULL modem cable between the PC’s serial port and the COM1 port on the Netburner.</p>
<p class="number3">3.1.5 Setup Required Prior to Running</p>
<p class="numberp2">Netburner powered up and running application software. Serial terminal program should be properly connected to the serial port on the PC that is wired to the Netburner.</p>
<p class="number3">3.1.6 Software Version Number</p>
<p class="numberp2">Version number: ____________</p>
<p class="numberp2">Date: ____________</p>
<p class="number3">3.1.7 Detailed Steps</p>
<p class="numberp2s1">1. <span class="space"/>Set DIP switch 1 to the ON position.</p>
<p class="numberp2s1">2. <span class="space"/>Reset the Netburner and wait several seconds for it to finish rebooting. Note: Rebooting Netburner may produce information on the serial terminal. You can ignore this.</p>
<p class="numberp2s1">3. <span class="space"/>Press <small>ENTER</small> on the line by itself into the terminal emulator.</p>
<p class="numberp2s1"><span epub:type="pagebreak" id="page_300"/>4. ______ Verify that the DAQ system responds with a newline without any other output</p>
<p class="numberp2s1">5. <span class="space"/>Type <code><span class="codestrong1">help</code></span>, then press <span class="small">ENTER</span> on a line by itself.</p>
<p class="numberp2s1">6. ______ Verify that the DAQ software responds with a help message (contents unimportant as long as it is obviously a help response).</p>
<p class="numberp2s1">7. <span class="space"/>Set DIP switch 1 to the OFF position.</p>
<p class="numberp2s1">8. <span class="space"/>Reset the Netburner and wait several seconds for it to finish rebooting. Note: Rebooting Netburner may produce information on the serial terminal. You can ignore this.</p>
<p class="numberp2s1">9. <span class="space"/>Type the help command into the serial terminal.</p>
<p class="numberp2sr">10. ______ Verify that the DAQ system ignores the help command.</p>
<p class="number3">3.1.8 Sign-off on Test Procedure</p>
<p class="numberp2">Tester: _________________ Date: _________</p>
<p class="numberp2">QA: _________________ Date: _________</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>In a full STP document, there would probably be additional test procedures here; the following test procedure ignores that possibility and continues tag numbering with DAQ_STP_002.</em></p>
</div>
<p class="number2"><strong>3.2 Ethernet Address Selection</strong></p>
<p class="number3">3.2.1 DAQ_STP_002</p>
<p class="number3">3.2.2 Purpose</p>
<p class="numberp2">This test procedure tests the initialization of the Ethernet IP address based on DIP switches 5 and 6.</p>
<p class="number3">3.2.3 Test Cases</p>
<p class="numberp2">DAQ_STC_709_000_000</p>
<p class="numberp2">DAQ_STC_710_000_000</p>
<p class="numberp2">DAQ_STC_711_000_000</p>
<p class="numberp2">DAQ_STC_712_000_000</p>
<p class="number3">3.2.4 Special Requirements</p>
<p class="numberp2">This test procedure requires an Ethernet terminal emulator program running on a PC (<em>Hercules.exe</em> has been a good choice in the past). There should be an Ethernet (crossover or through a hub) cable between the PC’s Ethernet port and the Ethernet port on the Netburner.</p>
<p class="number3"><span epub:type="pagebreak" id="page_301"/>3.2.5 Setup Required Prior to Running</p>
<p class="numberp2">Netburner powered up and running application software. DIP switch 3 in the ON position. DIP switch 4 in the OFF position.</p>
<p class="number3">3.2.6 Software Version Number</p>
<p class="numberp2">Version number: _________</p>
<p class="numberp2">Date: _________</p>
<p class="number3">3.2.7 Detailed Steps</p>
<p class="numberp2">1. <span class="space"/>Set DIP switches 5 and 6 to the OFF position.</p>
<p class="numberp2">2. <span class="space"/>Reset the Netburner and wait several seconds for it to finish rebooting.</p>
<p class="numberp2">3. <span class="space"/>From the Ethernet terminal program, attempt to connect to the Netburner at IP address 192.168.2.70, port 20560 (0x5050).</p>
<p class="numberp2">4. <span class="space"/>Verify that the connection was successful.</p>
<p class="numberp2">5. <span class="space"/>Enter a <code><span class="codestrong1">help</code></span> command and press the <span class="small">ENTER</span> key.</p>
<p class="numberp2">6. ______ Verify that the DAQ system responds with an appropriate help message.</p>
<p class="numberp2">7. <span class="space"/>Set DIP switch 5 to the ON position and 6 to the OFF position.</p>
<p class="numberp2">8. <span class="space"/>Reset the Netburner and wait several seconds for it to finish rebooting.</p>
<p class="numberp2">9. <span class="space"/>From the Ethernet terminal program, attempt to connect to the Netburner at IP address 192.168.2.71, port 20560 (0x5050).</p>
<p class="numberp2sr">10. ______ Verify that the connection was successful.</p>
<p class="numberp2sr">11. <span class="space"/>Enter a <code><span class="codestrong1">help</code></span> command and press the <span class="small">ENTER</span> key.</p>
<p class="numberp2sr">12. ______ Verify that the DAQ system responds with an appropriate help message.</p>
<p class="numberp2sr">13. <span class="space"/>Set DIP switch 5 to the OFF position and 6 to the ON position.</p>
<p class="numberp2sr">14. <span class="space"/>Reset the Netburner and wait several seconds for it to finish rebooting.</p>
<p class="numberp2sr">15. <span class="space"/>From the Ethernet terminal program, attempt to connect to the Netburner at IP address 192.168.2.72, port 20560 (0x5050).</p>
<p class="numberp2sr">16. ______ Verify that the connection was successful.</p>
<p class="numberp2sr">17. <span class="space"/>Enter a <code><span class="codestrong1">help</code></span> command and press the <span class="small">ENTER</span> key.</p>
<p class="numberp2sr"><span epub:type="pagebreak" id="page_302"/>18. ______ Verify that the DAQ system responds with an appropriate help message.</p>
<p class="numberp2sr">19. <span class="space"/> Set DIP switches 5 and 6 to the ON position.</p>
<p class="numberp2sr">20. <span class="space"/> Reset the Netburner and wait several seconds for it to finish rebooting.</p>
<p class="numberp2sr">21. <span class="space"/> From the Ethernet terminal program, attempt to connect to the Netburner at IP address 192.168.2.73, port 20560 (0x5050).</p>
<p class="numberp2sr">22. ______ Verify that the connection was successful.</p>
<p class="numberp2sr">23. <span class="space"/> Enter a <code><span class="codestrong1">help</code></span> command and press the <span class="small">ENTER</span> key.</p>
<p class="numberp2sr">24. ______ Verify that the DAQ system responds with an appropriate help message.</p>
<p class="number3">3.2.8 Sign-off on Test Procedure</p>
<p class="numberp2">Tester: _________________ Date: _________</p>
<p class="numberp2">QA: _________________ Date: _________</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>In a full STP document, there would probably be additional test procedures here.</em></p>
</div>
<p class="number1"><strong>4 General</strong></p>
<p class="number2"><strong>4.1 Document Change Procedures</strong></p>
<p class="numberp">Whenever making changes to this document, add a new line to Section 2.1 listing, at a minimum, the date, project name (<em>DAQ_STP</em>), version number, and authorship.</p>
<p class="number2"><strong>4.2 Attachments and Appendixes</strong></p>
<p class="numberp">[In the interests of space, none are provided here; in a real STP, putting the schematic of the DAQ system would be a good idea.]</p>
<p class="number1"><strong>5 Index</strong></p>
<p class="numberp1">[Omitted for space reasons.]</p>
<h4 class="h4" id="lev-12.5.8"><strong><em>12.5.8 Updating the RTM with STP Information</em></strong></h4>
<p class="noindent">Because STP tags are very similar in nature to SDD tags, it should come as no surprise that the process for adding STP tags to the RTM is quite similar to that for adding SDD tags (see “<a href="ch11.xhtml#lev-11.7">Updating the Traceability Matrix with Design Information</a>” on <a href="ch11.xhtml#page_259">page 259</a>).</p>
<p class="indent">The STP adds a single column to the RTM: the STP tag column. Unfortunately, the STP tag does not directly embed any traceability information, so you’ll have to extract that information from the STP to determine where to place STP tags in the RTM.</p>
<p class="indent">As you may recall from “List of Test Cases Covered by This Procedure” on <a href="ch12.xhtml#page_294">page 294</a>, each test procedure in an STP must include the list of test cases it covers. Though Std 829 does not require this, I strongly suggest <span epub:type="pagebreak" id="page_303"/>that you include this section. If you’ve done that, you’ve already created the reverse traceability back to the requirements, which makes it easy to fill in the STP tags in the RTM. To do so, just locate each test case tag (listed in the current test procedure) and copy the test procedure’s STP tag into the STP tag column in the RTM (on the same row as the corresponding test case). Of course, because there are multiple test cases associated with a single test procedure, you’ll also have several copies of the same STP tag spread throughout the RTM (one per associated test case).</p>
<p class="indent">Should you ever want to easily trace your STP tags back to all the requirements in the RTM, particularly without having to look up the list in the STP, simply sort the RTM by the STP tag column. This will collect all the requirements (and everything else linked to that STP tag) into a contiguous group in the matrix and make it easy to identify everything associated with that tag.</p>
<p class="indent">If you choose some other method of specifying test cases in the test procedure that doesn’t involve incorporating the STC tags within the test procedures, then determining the placement of the STP tags in the RTM becomes a manual—and often laborious—process. That’s why I strongly recommend including STC tag numbers in a test procedure when you first create it.</p>
<h3 class="h3" id="lev-12.6">12.6 <em>Level</em> Test Logs</h3>
<p class="noindent">Although each test procedure contains a signature section where the tester (and any other desired personnel) can sign off on a successful test completion, a separate test log is needed to handle anomalies that occur during testing or to simply hold comments and concerns that the tester may have while running the test procedure.</p>
<p class="indent">Perhaps the most important job of this Level <em>Test Log (LTL)</em> is to present a chronological view of the testing process. Whenever a tester begins running a test procedure, they should first log an entry stating the date, time, test procedure they are executing, and their name. Throughout the test’s execution, the tester can add entries to the test log (as necessary) indicating:</p>
<ul>
<li class="noindent">Start of a test procedure (date/time)</li>
<li class="noindent">End of a test procedure (date/time)</li>
<li class="noindent">Anomalies/defects found (and whether the test was continued or suspended)</li>
<li class="noindent">Redlines/changes needed to the test procedure because of errors found in the procedure itself (for example, the test procedure could list an incorrect outcome; if the tester can show that the program output was correct even if it differs from the test procedure, they would redline the test procedure and add an appropriate justification to the test log)</li>
<li class="noindent">Concerns about outcomes the program produces that the tester finds questionable (perhaps the test procedure doesn’t list any outcome, or the test procedure’s outcomes are questionable)</li>
<li class="noindent"><span epub:type="pagebreak" id="page_304"/>Personnel changes (for example, if a tester changes in the middle of a test due to a break, shift change, or different experience needed)</li>
<li class="noindent">Any break period during the test procedure (for example, lunch break or end of the workday)</li>
</ul>
<p class="indentb">Technically, all you need for a test log is a sheet of (preferably lined) paper. More often than not, STP creators add several sheets of lined paper to the end of the STP specifically for this test log. Some organizations simply maintain the test log electronically using a word processor or text editor (or even a specially written application). Of course, Std 829 outlines a formal recommendation for test logs:</p>
<p class="number1">1 Introduction</p>
<p class="number2">1.1 Document Identifier</p>
<p class="number2">1.2 Scope</p>
<p class="number2">1.3 References</p>
<p class="number1">2 Details</p>
<p class="number2">2.1 Description</p>
<p class="number2">2.2 Activity and Event Entries</p>
<p class="number1">3 General</p>
<p class="number2">3.1 Glossary</p>
<h4 class="h4" id="lev-12.6.1"><em>12.6.1 Introduction in the</em> Level <em>Test Logs Document</em></h4>
<p class="noindent">In addition to introducing the subsections that follow, this section might also identify the organization that created the document and the current status.</p>
<h5 class="h5" id="lev-12.6.1.1"><strong>12.6.1.1 Document Identifier</strong></h5>
<p class="noindent">A unique identifier for this document; as with all Std 829 documents this should include, at the very least, the date, some descriptive name, a version number, and authorship. A change history (of the outline/format, not the specific log) might appear here as well.</p>
<h5 class="h5" id="lev-12.6.1.2"><strong>12.6.1.2 Scope</strong></h5>
<p class="noindent">The Scope section summarizes the system and features that the associated test procedure tested. Generally, this would be a reference to the test procedure’s Scope section unless there was something special about this particular test run.</p>
<h5 class="h5" id="lev-12.6.1.3"><strong>12.6.1.3 References</strong></h5>
<p class="noindent">At the very least, this section should refer to the STP (and in particular, the specific test) document for which this test log was created.</p>
<h4 class="h4" id="lev-12.6.2"><span epub:type="pagebreak" id="page_305"/><strong><em>12.6.2 Details</em></strong></h4>
<p class="noindent">This section introduces the following subsections and is what most people would consider the actual “test log.”</p>
<h5 class="h5" id="lev-12.6.2.1"><strong>12.6.2.1 Description</strong></h5>
<p class="noindent">This section (only one occurrence per test log) describes items that will apply to all test log entries. This could include the following:</p>
<ul>
<li class="noindent">Identification of the test subject (for example, by version number)</li>
<li class="noindent">Identification of any changes made to the test procedure (for example, redlines) prior to this test</li>
<li class="noindent">Date and time of the start of the test</li>
<li class="noindent">Date and time of the stop of the test</li>
<li class="noindent">Name of the tester running the test</li>
<li class="noindent">Explanation for why testing was halted (if this should happen)</li>
</ul>
<h5 class="h5" id="lev-12.6.2.2"><strong>12.6.2.2 Activities and Event Entries</strong></h5>
<p class="noindent">This section of the test log records each event during the execution of the test procedure. This section (containing multiple entries) typically documents the following:</p>
<ul>
<li class="noindent">Description of the test procedure execution (procedure ID/tag)</li>
<li class="noindent">All personnel observing/involved in the test run—including testers, support personnel, and observers—and the role of each participant</li>
<li class="noindent">The result of each test procedure execution (pass, fail, commentary)</li>
<li class="noindent">A record of any deviations from the test procedure (for example, redlines)</li>
<li class="noindent">A record of any defects or anomalies discovered during the test procedure (along with a reference to an associated Anomaly Report if one is generated)</li>
</ul>
<h4 class="h4" id="lev-12.6.3"><strong><em>12.6.3 Glossary</em></strong></h4>
<p class="noindent">This section of the LTL documentation contains the usual glossary associated with all Std 829 documents.</p>
<h4 class="h4" id="lev-12.6.4"><strong><em>12.6.4 A Few Comments on Test Logs</em></strong></h4>
<p class="noindent">To be honest, the Std 829 outline is way too much effort for such a simple task. There are a few tips for managing the effort involved in this document.</p>
<h5 class="h5" id="lev-12.6.4.1"><strong>12.6.4.1 Overhead Management</strong></h5>
<p class="noindent">Almost all of the effort that would go into creating an Std 829 LTL outline-compliant document can be eliminated by simply attaching the test log directly to the end of the STP. The test log then inherits all the preface <span epub:type="pagebreak" id="page_306"/>information from the STP, so all you need to document is the information that appears at the very beginning of “<em>Level</em> Test Logs” on <a href="ch12.xhtml#page_303">page 303</a>.</p>
<p class="indent">Note that LTLs have four variants, as typical for all Std 829 level documents: Component Test Logs (aka Unit Test Logs), Component Integration Test Logs (aka Integration Test Logs), System Test Logs (aka System Integration Test Logs), and Acceptance Test Logs (possibly including Factory Acceptance Test Logs or Site Acceptance Test Logs).<sup><a href="ch19_footnote.xhtml#ch12fn21" id="ch12fn21a">21</a></sup></p>
<p class="indent">In reality, it’s rare for there to be much in the way of Component or Component Integration Test Logs. Most frequently, the corresponding test procedures are automated tests. Even when they’re not, the development team usually runs these tests and immediately corrects any defects they find. Because these tests run frequently (often multiple times per day, particularly in teams using Agile-based methodologies), the overhead with documenting these test runs is far too much.</p>
<p class="indent">System Test Logs and Acceptance Test Logs are the variants of the LTL that testers (independent of the development team) run, and hence the ones that require the creation of actual test logs.</p>
<h5 class="h5" id="lev-12.6.4.2"><strong>12.6.4.2 Recordkeeping</strong></h5>
<p class="noindent">The test logs are different from the other Std 829 documents in a very fundamental sense. Most Std 829 documents are static documents; about the only thing you do with them is fill in details like software version numbers and check off verification steps. The basic structure of the document doesn’t change if you run the procedure over and over again. Ultimately, there is no reason to keep any old copies of the test procedure around (like runs of the test procedure that failed in the middle of execution). All you really need to show the customer is the last run of the test procedure where you successfully executed all steps and passed the entire procedure.</p>
<p class="indent">The test logs, unlike the other documents you’ve seen in this chapter thus far, are <em>dynamic</em> documents. They will differ radically from test run to test run (even if nothing else changes, at least all the dates and timestamps will change). Furthermore, a test log isn’t a boilerplate document where you simply fill in a few blanks and check off some checkboxes. It’s essentially a blank slate that you create while actually running the test. If there are failures, or redlines, or commentary, the test log maintains the history of these events. Therefore, it is important to keep all your test logs, even the ones that recorded failed tests. It is highly improbable that any system will be perfect; there will be mistakes and defects you discover during testing. The test logs provide proof that you’ve found, corrected, and retested for these defects.</p>
<p class="indent">If you throw away all the old test logs that document all the defects discovered along the way and present only perfect test logs, any reasonable customer is going to question what you’re hiding. Mistakes and defects are a normal part of the process. If you don’t show that you’ve found and corrected these mistakes, your customers will assume that you haven’t tested the system well enough to find the defects or that you’ve faked the test logs. <span epub:type="pagebreak" id="page_307"/>Keep the old test logs! This proves you’ve done your QA due diligence for your product.</p>
<p class="indent">You could argue that keeping old test procedures to show redlines or interruptions in the test process is also important. However, any redline or interruption that appears on a test procedure document had better show up in the corresponding test log, so you don’t need to keep old test procedures that you’ve actually rerun.</p>
<p class="indent">Note that this does not imply that all test procedures you’ve run should be perfect. If you have properly documented and justified redlines on a test procedure, yet the test execution ran successfully to its conclusion, there is no need to <em>rewrite</em> the test procedure and refill all the checkboxes to include a clean test procedure in your final documentation. If it was successful, even with redlines, leave it alone.<sup><a href="ch19_footnote.xhtml#ch12fn22" id="ch12fn22a">22</a></sup> Redlines don’t indicate a failure of the software system; they are a defect, of course, but in the test procedure itself rather than the software. The goal of the test procedure is to test the software, not the test procedure. If minor changes to the test procedure are all you have, redline them and move on.</p>
<p class="indent">In many organizations, as I’ve said before, if any verification step in a test procedure fails, then after any defects are corrected, the entire procedure must be run from the beginning (a full regression test). For some test procedures or in some organizations, there may be a process in place to temporarily suspend a test procedure, update the software, and then resume the test procedure upon resolving the defect. In such cases, you can treat the verification failure step as though it were a redline: document the original failure in the test log, document the fact that the development team repaired the defect, and then document the correct operation of the software (at the failed verification step) with the new version of the software.<sup><a href="ch19_footnote.xhtml#ch12fn23" id="ch12fn23a">23</a></sup></p>
<h5 class="h5" id="lev-12.6.4.3"><strong>12.6.4.3 Paper vs. Electronic Logs</strong></h5>
<p class="noindent">Some people prefer creating electronic test logs; some organizations or customers demand paper test logs (filled in with pens, not pencils). The problem with electronic logs (especially if you create them using a word processor rather than an application program specifically designed to log test procedure runs) is that they are easily faked. Of course, no great programmer would ever fake a test log. However, there are less-than-great programmers in this world who have faked a test log. Unfortunately, the actions of those few have sullied the reputations of all software engineers. Therefore, it’s best to create test logs that are not easily faked, which often means using paper.</p>
<p class="indent">Someone <em>could</em> fake paper logs; however, it’s a lot more work and usually more obvious. Ultimately, customers are probably going to want hard copies <span epub:type="pagebreak" id="page_308"/>of the test logs; when they want them in electronic form, they’ll probably want scanned images of the hardcopy logs. They will be expecting you to maintain those paper logs in storage for legal reasons.</p>
<p class="indent">Perhaps the best solution is to use a software application specifically designed for creating test logs, one that automatically logs the entries to a database (making it a bit more difficult to fake the data). For the customer, you would print a report from the database to provide a hardcopy (or generate a PDF report if they wanted an electronic copy).</p>
<p class="indent">Regardless of how testers generate the original test log, most organizations will require them to eventually create a paper test log, and then the testers, observers, and other personnel associated with the test run will have to sign and date it to certify that the information is correct and accurate. This is a legal document at this point; someone attempting to fake any data could land in serious legal jeopardy.</p>
<h5 class="h5" id="lev-12.6.4.4"><strong>12.6.4.4 Inclusion in the RTM</strong></h5>
<p class="noindent">Normally, test logs don’t appear in the traceability matrix. However, there is no reason you couldn’t include them there. There is a one-to-many relationship between test procedures (and, therefore, STPs) and test logs. Thus, if you assign a unique identifier (tag) to each test report, you can add that identifier to an appropriate column in the RTM.</p>
<p class="indent">Because test logs have a many-to-one relationship to test procedures, it wouldn’t be a bad idea to model the tag ID on the others that this book presents. For example, use something such as: proj_TL_<em>xxx</em>_<em>yyy</em> where <em>xxx</em> comes from the test procedure tag (for example, <em>005</em> from <em>DAQ_STP_005</em>) and <em>yyy</em> is a (possibly decimal) numeric sequence that creates a unique tag for the test log.</p>
<h3 class="h3" id="lev-12.7"><strong>12.7 Anomaly Reports</strong></h3>
<p class="noindent">When a tester, a development team member, a customer, or anyone else using the system discovers a software defect, the proper way to document it is with an <em>Anomaly Report (AR)</em>, also known as a <em>Bug Report</em> or <em>Defect Report</em>. All too often an AR consists of someone telling a programmer, “Hey, I found a problem in your code.” The programmer then runs off to their machine to correct the problem and there’s no documentation to track the anomaly. This is very unfortunate, because tracking defects in a system is very important to maintaining the quality of that system.</p>
<p class="indent">The AR is the formal way to track system defects. Among other things, it captures the following information:</p>
<ul>
<li class="noindent">Date and time of defect occurrence</li>
<li class="noindent">The person who discovered the defect (or at least, who recorded the defect report in response to some user’s complaint)</li>
<li class="noindent">A description of the defect</li>
<li class="noindent">A procedure for reproducing the defect in the system (assuming the issue is deterministic and is easy enough to reproduce)</li>
<li class="noindent"><span epub:type="pagebreak" id="page_309"/>The impact the defect has on the system (for example, catastrophic, critical, marginal, negligible)</li>
<li class="noindent">The importance of the defect to end users (economic and social impact) so management can assign a priority to correcting it</li>
<li class="noindent">Any possible workarounds to the defect (so users can continue using the system while the development team works on correcting the defect)</li>
<li class="noindent">A discussion of what it might take to correct the defect (including recommendations and conclusions concerning the defect)</li>
<li class="noindent">Current status of the anomaly (for example, “new anomaly,” “development team is working on correction,” “in testing,” “corrected in software version <em>xxx.xxx</em>”)</li>
</ul>
<p class="indent">Naturally, Std 829 has a suggested outline for Anomaly Reports. However, most organizations use defect-tracking software to record defects or anomalies. If you aren’t willing to spend the money on a commercial product, there are many open source products freely available, such as Bugzilla. Most of these products use a database organization that is reasonably compatible with the recommendations from Std 829:</p>
<p class="number1">1 Introduction</p>
<p class="number2">1.1 Document Identifier</p>
<p class="number2">1.2 Scope</p>
<p class="number2">1.3 References</p>
<p class="number1">2 Details</p>
<p class="number2">2.1 Summary</p>
<p class="number2">2.2 Date Anomaly Discovered</p>
<p class="number2">2.3 Context</p>
<p class="number2">2.4 Description of Anomaly</p>
<p class="number2">2.5 Impact</p>
<p class="number2">2.6 Originator’s Assessment of Urgency (see IEEE 1044-1993 [B13])</p>
<p class="number2">2.7 Description of Corrective Action</p>
<p class="number2">2.8 Status of the Anomaly</p>
<p class="number2">2.9 Conclusions and Recommendations</p>
<p class="number1">3 General</p>
<p class="number2">3.1 Document Change Procedures and History</p>
<h4 class="h4" id="lev-12.7.1"><strong><em>12.7.1 Introduction in the Anomaly Reports Document</em></strong></h4>
<p class="noindent">The following subsections describe the components of the AR introduction.</p>
<h5 class="h5" id="lev-12.7.1.1"><strong>12.7.1.1 Document Identifier</strong></h5>
<p class="noindent">This is a unique name that other reports can reference (such as test logs and test reports).</p>
<h5 class="h5" id="lev-12.7.1.2"><span epub:type="pagebreak" id="page_310"/><strong>12.7.1.2 Scope</strong></h5>
<p class="noindent">The Scope section gives a brief description of anything that doesn’t appear elsewhere in the AR.</p>
<h5 class="h5" id="lev-12.7.1.3"><strong>12.7.1.3 References</strong></h5>
<p class="noindent">References include links to other relevant documents, such as test logs and test procedures.</p>
<h4 class="h4" id="lev-12.7.2"><strong><em>12.7.2 Details</em></strong></h4>
<p class="noindent">This section introduces the subsections that follow.</p>
<h5 class="h5" id="lev-12.7.2.1"><strong>12.7.2.1 Summary</strong></h5>
<p class="noindent">Here you give a brief description of the anomaly.</p>
<h5 class="h5" id="lev-12.7.2.2"><strong>12.7.2.2 Date Anomaly Discovered</strong></h5>
<p class="noindent">List the date (and time, if possible/appropriate) when the anomaly was discovered.</p>
<h5 class="h5" id="lev-12.7.2.3"><strong>12.7.2.3 Context</strong></h5>
<p class="noindent">Software version and installation/configuration information goes in the Context section. This section should also refer to relevant test procedures and test logs, if appropriate, which should help to identify this anomaly. If no such test procedure exists for this anomaly, consider suggesting an addition to some test procedure that would catch it.</p>
<h5 class="h5" id="lev-12.7.2.4"><strong>12.7.2.4 Description of Anomaly</strong></h5>
<p class="noindent">Provide an in-depth description of the defect including (if possible) how to reproduce it. The description might include the following information:</p>
<ul>
<li class="noindent">Inputs</li>
<li class="noindent">Actual results</li>
<li class="noindent">Outcome(s) (particularly, the outcomes that vary from the test procedure)</li>
<li class="noindent">Procedure step of failure</li>
<li class="noindent">Environment</li>
<li class="noindent">Was the defect repeatable?</li>
<li class="noindent">Any tests executed immediately prior to failure than might have affected results</li>
<li class="noindent">Tester(s)</li>
<li class="noindent">Observer(s)</li>
</ul>
<h5 class="h5" id="lev-12.7.2.5"><span epub:type="pagebreak" id="page_311"/><strong>12.7.2.5 Impact</strong></h5>
<p class="noindent">Describe the impact this defect will have on system users. Describe any possible workarounds, such as changing the documentation or modifying the use of the system. If possible, estimate cost and time to repair this defect and the risk associated with leaving it in place. Estimate the risk associated with fixing it, which could impact other system features.</p>
<h5 class="h5" id="lev-12.7.2.6"><strong>12.7.2.6 Originator’s Assessment of Urgency</strong></h5>
<p class="noindent">State the level of urgency for a speedy repair. The integrity levels and risk assessment scale from “Integrity Levels and Risk Assessment” on <a href="ch12.xhtml#page_263">page 263</a> are probably a good minimum mechanism for stating the urgency of repair.</p>
<h5 class="h5" id="lev-12.7.2.7"><strong>12.7.2.7 Description of Corrective Action</strong></h5>
<p class="noindent">This section describes the time needed to determine the reason for the defect; an estimate of the time, cost, and risk associated with repairing it; and an estimate of the effort required to retest the system. Include any necessary regression tests to ensure that nothing else is broken by the fix.</p>
<h5 class="h5" id="lev-12.7.2.8"><strong>12.7.2.8 Status of the Anomaly</strong></h5>
<p class="noindent">List the status of the current defect. Std 829 recommends statuses such as “open,” “approved for resolution,” “assigned for resolution,” “fixed,” and “tested with the fix confirmed.”</p>
<h5 class="h5" id="lev-12.7.2.9"><strong>12.7.2.9 Conclusions and Recommendations</strong></h5>
<p class="noindent">This section should provide commentary as to why the defect occurred and recommend possible changes to the development process to prevent similar defects in the future. This section might also suggest additional requirements, test cases, and (modifications to) test procedures to catch the anomaly in the future; this is particularly important if testing discovered the anomaly by accident rather than by running specific test procedure steps to catch this particular defect.</p>
<h5 class="h5" id="lev-12.7.2.10"><strong>12.7.2.10 General</strong></h5>
<p class="noindent">This is the usual end-of-document section in Std 829 documents providing a change history (to the AR format, not to a specific AR) and change procedures. Std 829 does not recommend a glossary.</p>
<h4 class="h4" id="lev-12.7.3"><strong><em>12.7.3 A Few Comments on Anomaly Reports</em></strong></h4>
<p class="noindent">It is worthwhile to bear the following points in mind when dealing with Anomaly Reports.</p>
<h5 class="h5" id="lev-12.7.3.1"><span epub:type="pagebreak" id="page_312"/><strong>12.7.3.1 ARs Don’t Go in the RTM</strong></h5>
<p class="noindent">The purpose of the traceability matrix is to be able to trace requirements of designs and tests to ensure that the system successfully meets all requirements. While one could argue that test logs belong in the RTM, most people don’t bother to put them there because they normally attach test logs directly to the completed test procedures.</p>
<p class="indent">Anomalies, on the other hand, aren’t something whose existence you’re trying to prove; indeed, in a perfect world you’re trying to <em>disprove</em> the existence of anomalies. This doesn’t mean you discard ARs. Just as with test logs, it’s very important to keep all the old ARs around—they provide valuable proof that you’ve done your due diligence when testing the system. More importantly, you want to keep ARs for regression purposes. Sometimes long after a defect has been discovered and corrected, it finds its way into the system again. Having a historical record of ARs makes it possible to go back and examine the original cause and its solution.</p>
<h5 class="h5" id="lev-12.7.3.2"><strong>12.7.3.2 Electronic vs. Paper ARs</strong></h5>
<p class="noindent">As this chapter noted earlier, most organizations use a defect-tracking system to capture and track ARs. Although Std 829 doesn’t specifically suggest or require paper documents (indeed, Std 829 points out that you can use software to track anomalies), the outline form tends to suggest a hardcopy format. But given that most organizations use defect-tracking software, why bother with hardcopy ARs? The main reason is portability in the “you can carry it with you” sense. While using the defect-tracking system makes a lot of sense for system integration, factory acceptance tests, and other tests done at the development site where there is easy access to the tracker, in some cases it may not be available or accessible at an installation during a site acceptance test.<sup><a href="ch19_footnote.xhtml#ch12fn24" id="ch12fn24a">24</a></sup> In such situations, creating ARs on paper and then entering them into the defect-tracking system when possible is probably the best approach.</p>
<h3 class="h3" id="lev-12.8"><strong>12.8 Test Reports</strong></h3>
<p class="noindent">When testing is completed, a test report summarizes the results. As for many of the other test documents, Std 829 describes a wide variety of test reports you can produce. Std 829 defines <em>Level</em> Interim Test Status Reports (LITSR), <em>Level</em> Test Reports (LTR), and Master Test Reports (MTR). Of course, you can substitute <em>Component</em>, <em>Component Integration</em>, <em>System</em>, and <em>Acceptance</em> in place of <em>Level</em> (with the usual common names as well).</p>
<p class="indent">A very large organization might need to produce interim test reports so management can figure out what’s going on in an equally large system. For more information on LITSRs, refer to IEEE Std 829-2008; they are, quite <span epub:type="pagebreak" id="page_313"/>frankly, documentation for documentation’s sake for most projects, but large governmental contracts might explicitly require them.</p>
<p class="indent"><em>Level</em> and Master Test Reports vary according to the size of the project. Most small to medium-sized systems with (typically) a single software application and, therefore, a single STP, will have a single test report, if any at all.</p>
<p class="indent">Once a system grows to the size that it contains several major software applications, there will usually be a test report for each major application and then an MTR as a summary of the results from the individual test reports. The MTR, then, provides an <em>executive-level review</em> of all the tests.</p>
<h4 class="h4" id="lev-12.8.1"><strong><em>12.8.1 Brief Mention of the Master Test Report</em></strong></h4>
<p class="noindentb">As the MTR is generally not a document that individual developers will deal with, this section will simply present the Std 829-suggested outline without further comment and then concentrate on LTRs.</p>
<p class="number1">1 Introduction</p>
<p class="number2">1.1 Document Identifier</p>
<p class="number2">1.2 Scope</p>
<p class="number2">1.3 References</p>
<p class="number1">2 Details of the Master Test Report</p>
<p class="number2">2.1 Overview of All Aggregate Test Results</p>
<p class="number2">2.2 Rationale for Decisions</p>
<p class="number2">2.3 Conclusions and Recommendations</p>
<p class="number1">3 General</p>
<p class="number2">3.1 Glossary</p>
<p class="number2">3.2 Document Change Procedures and History</p>
<p class="indenta">For more information on the MTR, see IEEE Std 829-2008.</p>
<h4 class="h4" id="lev-12.8.2"><em>12.8.2</em> Level <em>Test Reports</em></h4>
<p class="noindentb">Although you could have component/unit test reports and component integration test reports, most organizations leave unit and integration testing to the development department, as upper management generally doesn’t care about the low-level details. Thus, the most common Level <em>Test Reports (LTRs)</em> you’ll see will be System (Integration) Test Reports and Acceptance Test Reports, typically Factory Acceptance Test Reports and Site Acceptance Test Reports. Std 829 outlines LTRs as follows:</p>
<p class="number1">1 Introduction</p>
<p class="number2">1.1 Document Identifier</p>
<p class="number2">1.2 Scope</p>
<p class="number2">1.3 References</p>
<p class="number1"><span epub:type="pagebreak" id="page_314"/>2 Details</p>
<p class="number2">2.1 Overview of Test Results</p>
<p class="number2">2.2 Detailed Test Results</p>
<p class="number2">2.3 Rationale for Decisions</p>
<p class="number2">2.4 Conclusions and Recommendations</p>
<p class="number1">3 General</p>
<p class="number2">3.1 Glossary</p>
<p class="number2">3.2 Document Change Procedures and History</p>
<p class="indenta">Sections 1 (“Introduction”) and 3 (“General”) are the same as for most other Std 829 test documents in this chapter. The core of the test report is in Section 2 (“Details”). The following subsections describe its contents.</p>
<h5 class="h5" id="lev-12.8.2.1"><strong>12.8.2.1 Overview of the Test Results</strong></h5>
<p class="noindent">This section is a summary of the test activities. It would briefly describe the features covered by the tests, testing environment, software/hardware version numbers, and any other general information about the test. The overview should also mention if there was anything special about the testing environment that would yield different results if the test were conducted in a different environment, like a factory.</p>
<h5 class="h5" id="lev-12.8.2.2"><strong>12.8.2.2 Detailed Test Result</strong></h5>
<p class="noindent">Summarize all the results in this section. List all anomalies discovered and their resolution. If the resolution to a defect has been deferred, be sure to provide justification and discuss the impact that defect will have on the system.</p>
<p class="indent">If there were any deviations from the test procedure, explain and justify those deviations. Describe any changes (redlines) to the test procedures.</p>
<p class="indent">This section should also provide a confidence level in the testing process. For example, if the testing process focuses on code coverage, this section should describe the estimated percentage of code coverage that the testing processing achieved.</p>
<h5 class="h5" id="lev-12.8.2.3"><strong>12.8.2.3 Rationale for Decisions</strong></h5>
<p class="noindent">If the team had to make any decisions during the testing process such as deviations from test procedures or failure to correct known anomalies, this section should provide the rationale for those decisions. This section might also justify any conclusions reached (in the next section).</p>
<h5 class="h5" id="lev-12.8.2.4"><strong>12.8.2.4 Conclusions and Recommendations</strong></h5>
<p class="noindent">This section should state any conclusions emanating from the test processing. This section should discuss the product’s fitness for release/production use, and recommend possibilities such as disabling certain, possibly known, <span epub:type="pagebreak" id="page_315"/>anomalous features to allow early release of the system. This section could also recommend stalling the release pending further development and possible debugging.</p>
<h3 class="h3" id="lev-12.9"><strong>12.9 Do You Really Need All of This?</strong></h3>
<p class="noindent">IEEE Std 829-2008 describes a huge volume of documentation. Do you really need to create all this documentation for the next “killer app” you’re developing by yourself in your home office? Of course not. Except for the largest (government-sponsored) applications, the vast majority of the documentation described in Std 829 is complete overkill. For normal projects, you’ll probably want to have the STC, SRL, and STP documents.<sup><a href="ch19_footnote.xhtml#ch12fn25" id="ch12fn25a">25</a></sup> Test logs will simply be an appendix to the STP. Anomaly Reports would be entries in your defect-tracking system (from which you can produce hardcopy reports).</p>
<p class="indent">You can also reduce the size of your STC and STP documents by using automated testing. You probably can’t eliminate all manual tests, but you can get rid of many of them.</p>
<p class="indent">Test reports are easy enough to eliminate in smaller projects. The test log at the end of the STP will likely serve as a reasonable alternative unless you have multiple levels of management demanding full documentation.</p>
<p class="indent">Agile development methodologies might seem like a good alternative for reducing the cost of all this documentation. However, keep in mind that developing, validating, verifying, and maintaining all those automated test procedures also has an associated—and often equivalent—cost.</p>
<h3 class="h3" id="lev-12.10"><strong>12.10 For More Information</strong></h3>
<p class="ref">Dingeldein, Tirena. “5 Best Free and Open Source Bug Tracking Software for Cutting IT Costs.” September 6, 2019. <em><a href="https://blog.capterra.com/top-free-bug-tracking-software/">https://blog.capterra.com/top-free-bug-tracking-software/</a></em>.</p>
<p class="ref">IEEE. “IEEE Std 829-2008: IEEE Standard for Software and System Test Documentation.” July 18, 2008. <em><a href="http://standards.ieee.org/findstds/standard/829-2008.html">http://standards.ieee.org/findstds/standard/829-2008.html</a></em>. This is expensive ($160 US when I last checked), but this is the gold standard. It’s more readable than the SDD standard, but still heavy reading.</p>
<p class="ref">Peham, Thomas. “7 Excellent Open Source Bug Tracking Tools Unveiled by Usersnap.” May 8, 2016. <em><a href="https://usersnap.com/blog/open-source-bug-tracking/">https://usersnap.com/blog/open-source-bug-tracking/</a></em>.</p>
<p class="ref"><span epub:type="pagebreak" id="page_316"/>Plantation Productions, Inc. “Open Source/Open Hardware: Digital Data Acquisition &amp; Control System.” n.d. <em><a href="http://www.plantation-productions.com/Electronics/DAQ/DAQ.html">http://www.plantation-productions.com/Electronics/DAQ/DAQ.html</a></em>. This is where you’ll find the DAQ Data Acquisition Software Review, Software Test Case, Software Test Procedures, and Reverse Traceability Matrix.</p>
<p class="ref">Software Testing Help. “15 Best Bug Tracking Software: Top Defect/Issue Tracking Tools of 2019.” November 14, 2019. <em><a href="http://www.softwaretestinghelp.com/popular-bug-tracking-software/">http://www.softwaretestinghelp.com/popular-bug-tracking-software/</a></em>.</p>
<p class="ref">Wikipedia. “Bug Tracking System.” Last modified April 4, 2020. <a href="https://en.wikipedia.org/wiki/Bug_tracking_system"><em>https://en.wikipedia.org/wiki/Bug_tracking_system</em></a>.</p>
</div>



  </body></html>