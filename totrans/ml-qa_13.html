<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch11"><span epub:type="pagebreak" id="page_69"/><strong><span class="big">11</span><br/>CALCULATING THE NUMBER OF PARAMETERS</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="noindent">How do we compute the number of parameters in a convolutional neural network, and why is this information useful?</p>&#13;
<p class="indent">Knowing the number of parameters in a model helps gauge the model’s size, which affects storage and memory requirements. The following sections will explain how to compute the convolutional and fully connected layer parameter counts.</p>&#13;
<h3 class="h3" id="ch00lev53"><strong>How to Find Parameter Counts</strong></h3>&#13;
<p class="noindent">Suppose we are working with a convolutional network that has two convolutional layers with kernel size 5 and kernel size 3, respectively. The first convolutional layer has 3 input channels and 5 output channels, and the second one has 5 input channels and 12 output channels. The stride of these convolutional layers is 1. Furthermore, the network has two pooling layers, one with a kernel size of 3 and a stride of 2, and another with a kernel size of 5 and a stride of 2. It also has two fully connected hidden layers with 192 and 128 hidden units each, where the output layer is a classification layer for 10 classes. The architecture of this network is illustrated in <a href="ch11.xhtml#ch11fig1">Figure 11-1</a>.<span epub:type="pagebreak" id="page_70"/></p>&#13;
<div class="image"><img id="ch11fig1" src="../images/11fig01.jpg" alt="Image" width="1085" height="476"/></div>&#13;
<p class="figcap"><em>Figure 11-1: A convolutional neural network with two convolutional and two fully connected layers</em></p>&#13;
<p class="indent">What is the number of trainable parameters in this convolutional network? We can approach this problem from left to right, computing the number of parameters for each layer and then summing up these counts to obtain the total number of parameters. Each layer’s number of trainable parameters consists of weights and bias units.</p>&#13;
<h4 class="h4" id="ch00levsec20"><em><strong>Convolutional Layers</strong></em></h4>&#13;
<p class="noindent">In a convolutional layer, the number of weights depends on the kernel’s width and height and the number of input and output channels. The number of bias units depends on the number of output channels only. To illustrate the computation step by step, suppose we have a kernel width and height of 5, one input channel, and one output channel, as illustrated in <a href="ch11.xhtml#ch11fig2">Figure 11-2</a>.</p>&#13;
<div class="image"><img id="ch11fig2" src="../images/11fig02.jpg" alt="Image" width="532" height="241"/></div>&#13;
<p class="figcap"><em>Figure 11-2: A convolutional layer with one input channel and one output channel</em></p>&#13;
<p class="indent">In this case, we have 26 parameters, since we have 5 <em>×</em> 5 = 25 weights via the kernel plus the bias unit. The computation to determine an output value or pixel <em>z</em> is <em>z</em> = <em>b</em> + <em>∑<sub>j</sub> w<sub>j</sub> x<sub>j</sub></em>, where <em>x<sub>j</sub></em> represents an input pixel, <em>w<sub>j</sub></em> represents a weight parameter of the kernel, and <em>b</em> is the bias unit.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_71"/>Now, suppose we have three input channels, as illustrated in <a href="ch11.xhtml#ch11fig3">Figure 11-3</a>.</p>&#13;
<div class="image"><img id="ch11fig3" src="../images/11fig03.jpg" alt="Image" width="476" height="177"/></div>&#13;
<p class="figcap"><em>Figure 11-3: A convolutional layer with three input channels and one output channel</em></p>&#13;
<p class="indent">In that case, we compute the output value by performing the aforementioned operation, <em>∑<sub>j</sub> w<sub>j</sub> x<sub>j</sub></em>, for each input channel and then add the bias unit. For three input channels, this would involve three different kernels with three sets of weights:</p>&#13;
<div class="image1"><img src="../images/f0071-01.jpg" alt="Image" width="461" height="71"/></div>&#13;
<p class="indent">Since we have three sets of weights (<em>w</em><sup>(1)</sup>, <em>w</em><sup>(2)</sup>, and <em>w</em><sup>(3)</sup> for <em>j</em> = [1, . . . , 25]), we have 3 <em>×</em> 25 + 1 = 76 parameters in this convolutional layer.</p>&#13;
<p class="indent">We use one kernel for each output channel, where each kernel is unique to a given output channel. Thus, if we extend the number of output channels from one to five, as shown in <a href="ch11.xhtml#ch11fig4">Figure 11-4</a>, we extend the number of parameters by a factor of 5. In other words, if the kernel for one output channel has 76 parameters, the 5 kernels required for the five output channels will have 5 <em>×</em> 76 = 380 parameters.</p>&#13;
<div class="image"><img id="ch11fig4" src="../images/11fig04.jpg" alt="Image" width="619" height="190"/></div>&#13;
<p class="figcap"><em>Figure 11-4: A convolutional layer with three input channels and five output channels</em></p>&#13;
<p class="indent">Returning to the neural network architecture illustrated in <a href="ch11.xhtml#ch11fig1">Figure 11-1</a> at the beginning of this section, we compute the number of parameters in the convolutional layers based on the kernel size and number of input and output channels. For example, the first convolutional layer has three input channels, five output channels, and a kernel size of 5. Thus, its number of parameters is 5 <em>×</em> (5 <em>×</em> 5 <em>×</em> 3) + 5 = 380. The second convolutional layer, with five input channels, 12 output channels, and a kernel size of 3, has 12 <em>×</em> (3 <em>×</em> 3 <em>×</em> 5) + 12 = 552 parameters. Since the pooling layers do not have any trainable parameters, we can count 380 + 552 = 932 for the convolutional part of this architecture.</p>&#13;
<p class="indent">Next, let’s see how we can compute the number of parameters of fully connected layers.<span epub:type="pagebreak" id="page_72"/></p>&#13;
<h4 class="h4" id="ch00levsec21"><em><strong>Fully Connected Layers</strong></em></h4>&#13;
<p class="noindent">Counting the number of parameters in a fully connected layer is relatively straightforward. A fully connected node connects each input node to each output node, so the number of weights is the number of inputs times the number of outputs plus the bias units added to the output. For example, if we have a fully connected layer with five inputs and three outputs, as shown in <a href="ch11.xhtml#ch11fig5">Figure 11-5</a>, we have 5 <em>×</em> 3 = 15 weights and three bias units, that is, 18 parameters total.</p>&#13;
<div class="image"><img id="ch11fig5" src="../images/11fig05.jpg" alt="Image" width="352" height="517"/></div>&#13;
<p class="figcap"><em>Figure 11-5: A fully connected layer with five inputs and three outputs</em></p>&#13;
<p class="indent">Returning once more to the neural network architecture illustrated in <a href="ch11.xhtml#ch11fig1">Figure 11-1</a>, we can now calculate the parameters in the fully connected layers as follows: 192 <em>×</em> 128 + 128 = 24,704 in the first fully connected layer and 128 <em>×</em> 10 + 10 = 1,290 in the second fully connected layer, the output layer. Hence, we have 24,704 + 1,290 = 25,994 in the fully connected part of this network. After adding the 932 parameters from the convolutional layers and the 25,994 parameters from the fully connected layers, we can conclude that this network’s total number of parameters is 26,926.</p>&#13;
<p class="indent">As a bonus, interested readers can find PyTorch code to compute the number of parameters programmatically in the <em>supplementary/q11-conv-size</em> subfolder at <em><a href="https://github.com/rasbt/MachineLearning-QandAI-book">https://github.com/rasbt/MachineLearning-QandAI-book</a></em>.<span epub:type="pagebreak" id="page_73"/></p>&#13;
<h3 class="h3" id="ch00lev54"><strong>Practical Applications</strong></h3>&#13;
<p class="noindent">Why do we care about the number of parameters at all? First, we can use this number to estimate a model’s complexity. As a rule of thumb, the more parameters there are, the more training data we’ll need to train the model well.</p>&#13;
<p class="indent">The number of parameters also lets us estimate the size of the neural network, which in turn helps us estimate whether the network can fit into GPU memory. Although the memory requirement during training often exceeds the model size due to the additional memory required for carrying out matrix multiplications and storing gradients, model size gives us a ballpark sense of whether training the model on a given hardware setup is feasible.</p>&#13;
<h3 class="h3" id="ch00lev55"><strong>Exercises</strong></h3>&#13;
<p class="number1"><strong>11-1.</strong> Suppose we want to optimize the neural network using a plain stochastic gradient descent (SGD) optimizer or the popular Adam optimizer. What are the respective numbers of parameters that need to be stored for SGD and Adam?</p>&#13;
<p class="number1"><strong>11-2.</strong> Suppose we’re adding three batch normalization (BatchNorm) layers: one after the first convolutional layer, one after the second convolutional layer, and another one after the first fully connected layer (we typically do not want to add BatchNorm layers to the output layer). How many additional parameters do these three BatchNorm layers add to the model?<span epub:type="pagebreak" id="page_74"/></p>&#13;
</div>
</div>
</body></html>