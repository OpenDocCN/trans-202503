<html><head></head><body>
<h2 class="h2" id="ch07"><span epub:type="pagebreak" id="page_111"/><span class="big">7</span><br/>DEPLOYING CONTAINERS TO KUBERNETES</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">We’re now ready to begin running containers on our working Kubernetes cluster. Because Kubernetes has a declarative API, we’ll create various kinds of resources to run them, and we’ll monitor the cluster to see what Kubernetes does for each type of resource.</p>&#13;
<p class="indent">Different containers have different use cases. Some might require multiple identical instances with autoscaling to perform well under load. Other containers might exist solely to run a one-time command. Still others may require a fixed ordering to enable selecting a single primary instance and providing controlled failover to a secondary instance. Kubernetes provides different <em>controller</em> resource types for each of those use cases. We’ll look at each in turn, but we’ll begin with the most fundamental of them, the <em>Pod</em>, which is utilized by all of those use cases.</p>&#13;
<h3 class="h3" id="ch00lev1sec32">Pods</h3>&#13;
<p class="noindent">A Pod is the most basic resource in Kubernetes and is how we run containers. Each Pod can have one or more containers within it. The Pod is used to <span epub:type="pagebreak" id="page_112"/>provide the process isolation we saw in <a href="ch02.xhtml#ch02">Chapter 2</a>. Linux kernel namespaces are used at the Pod and the container level:</p>&#13;
<div class="bqparan">&#13;
<p class="noindent5"><span class="codestrong">mnt</span> Mount points: each container has its own root filesystem; other mounts are available to all containers in the Pod.</p>&#13;
<p class="noindent5"><span class="codestrong">uts</span> Unix time sharing: isolated at the Pod level.</p>&#13;
<p class="noindent5"><span class="codestrong">ipc</span> Interprocess communication: isolated at the Pod level.</p>&#13;
<p class="noindent5"><span class="codestrong">pid</span> Process identifiers: isolated at the container level.</p>&#13;
<p class="noindent5"><span class="codestrong">net</span> Network: isolated at the Pod level.</p>&#13;
</div>&#13;
<p class="indent">The biggest advantage of this approach is that multiple containers can act like processes on the same virtual host, using the <code>localhost</code> address to communicate, while still being based on separate container images.</p>&#13;
<h4 class="h4" id="ch00lev2sec49">Deploying a Pod</h4>&#13;
<p class="noindent">To get started, let’s create a Pod directly. Unlike the previous chapter, in which we used <code>kubectl run</code> to have the Pod specification created for us, we’ll specify it directly using YAML so that we have complete control over the Pod and to prepare us for using controllers to create Pods, providing scalability and failover.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on getting set up.</em></p>&#13;
</div>&#13;
<p class="indent">The automation script for this chapter does a full cluster install with three nodes that run the control plane and regular applications, providing the smallest possible highly available cluster for testing. The automation also creates some YAML files for Kubernetes resources. Here’s a basic YAML resource to create a Pod running NGINX:</p>&#13;
<p class="noindent6"><em>nginx-pod.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: nginx&#13;
spec:&#13;
  containers:&#13;
  - name: nginx&#13;
    image: nginx</pre>&#13;
<p class="indent">Pods are part of the <em>core</em> Kubernetes API, so we just specify a version number of <code>v1</code> for the <code>apiVersion</code>. Specifying <code>Pod</code> as the <code>kind</code> tells Kubernetes exactly what resource we’re creating in the API group. We will see these fields in all of our Kubernetes resources.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_113"/>The <code>metadata</code> field has many uses. For the Pod, we just need to provide the one required field of <code>name</code>. We don’t specify the <code>namespace</code> in the metadata, so by default this Pod will end up in the <code>default</code> Namespace.</p>&#13;
<p class="indent">The remaining field, <code>spec</code>, tells Kubernetes everything it needs to know to run this Pod. For now we are providing the minimal information, which is a list of containers to run, but many other options are available. In this case, we have only one container, so we provide just the name and container image Kubernetes should use.</p>&#13;
<p class="indent">Let’s add this Pod to the cluster. The automation added files to <em>/opt</em>, so we can do it from <code>host01</code> as follows:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/nginx-pod.yaml</span>&#13;
pod/nginx created</pre>&#13;
<p class="indent">In <a href="ch07.xhtml#ch07list1">Listing 7-1</a>, we can check the Pod’s status.</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods -o wide</span>&#13;
NAME    READY   STATUS    RESTARTS   AGE     IP               NODE   ...&#13;
nginx   1/1     Running   0          2m26s   172.31.25.202   host03 ...</pre>&#13;
<p class="caption" id="ch07list1"><em>Listing 7-1: Status of NGINX</em></p>&#13;
<p class="indent">It can take some time before the Pod shows <code>Running</code>, especially if you just set up your Kubernetes cluster and it’s still busy deploying core components. Keep trying this <code>kubectl</code> command to check the status.</p>&#13;
<p class="indent">Instead of typing the <code>kubectl</code> command multiple times, you can also use <code>watch</code>. The <code>watch</code> command is a great way to observe changes in your cluster over time. Just add <code>watch</code> in front of your command, and it will be run for you every two seconds.</p>&#13;
<p class="indent">We added <code>-o wide</code> to the command to see the IP address and node assignment for this Pod. Kubernetes manages that for us. In this case, the Pod was scheduled on <code>host03</code>, so we need to go there to see the running container:</p>&#13;
<pre>root@host03:~# <span class="codestrong1">crictl pods --name nginx</span>&#13;
POD ID         CREATED         STATE  NAME   NAMESPACE  ...&#13;
9f1d6e0207d7e  19 minutes ago  Ready  nginx  default    ...</pre>&#13;
<p class="indent">Run this command on whatever host your NGINX Pod is on.</p>&#13;
<p class="indent">If we collect the Pod ID, we can see the container as well:</p>&#13;
<pre>root@host03:~# <span class="codestrong1">POD_ID=$(crictl pods -q --name nginx)</span>&#13;
root@host03:~# <span class="codestrong1">crictl ps --pod $POD_ID</span>&#13;
CONTAINER      IMAGE          CREATED         STATE    NAME   ...&#13;
9da09b3671418  4cdc5dd7eaadf  20 minutes ago  Running  nginx  ...</pre>&#13;
<p class="indent">This output looks very similar to the output from <code>kubectl get</code> in <a href="ch07.xhtml#ch07list1">Listing 7-1</a>, which is not surprising given that our cluster gets that information from the <code>kubelet</code> service running on this node, which in turn uses the same Container Runtime Interface (CRI) API that <code>crictl</code> is also using to talk to the container engine.</p>&#13;
<h4 class="h4" id="ch00lev2sec50"><span epub:type="pagebreak" id="page_114"/>Pod Details and Logging</h4>&#13;
<p class="noindent">The ability to use <code>crictl</code> with the underlying container engine to explore a container running in the cluster is valuable, but it does require us to connect to the specific host running the container. Much of the time, we can avoid that by using <code>kubectl</code> commands to inspect Pods from anywhere by connecting to our cluster’s API server. Let’s move back to <code>host01</code> and explore the NGINX Pod further.</p>&#13;
<p class="indent">In <a href="ch06.xhtml#ch06">Chapter 6</a>, we saw how we could use <code>kubectl describe</code> to see the status and event log for a cluster node. We can use the same command to see the status and configuration details of other Kubernetes resources. Here’s the event log for our NGINX Pod:</p>&#13;
<pre> root@host01:~# <span class="codestrong1">kubectl describe pod nginx</span>&#13;
 Name:         nginx&#13;
 Namespace: <span class="ent">➊</span> default &#13;
 ...&#13;
 Containers:&#13;
   nginx:&#13;
     Container ID:   containerd://9da09b3671418...&#13;
 ...&#13;
<span class="ent">➋</span> Type    Reason     Age   From               Message&#13;
   ----    ------     ----  ----               -------&#13;
   Normal  Scheduled  22m   default-scheduler  Successfully assigned ...&#13;
   Normal  Pulling    22m   kubelet            Pulling image "nginx"&#13;
   Normal  Pulled     21m   kubelet            Successfully pulled image ...&#13;
   Normal  Created    21m   kubelet            Created container nginx&#13;
   Normal  Started    21m   kubelet            Started container nginx</pre>&#13;
<p class="indent">We can use <code>kubectl describe</code> with many different Kubernetes resources, so we first tell <code>kubectl</code> that we are interested in a Pod and provide the name. Because we didn’t specify a Namespace, Kubernetes will look for this Pod in the <code>default</code> Namespace <span class="ent">➊</span>.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>We use the <span class="codeitalic">default</span> Namespace for most of the examples in this book to save typing, but it’s a good practice to use multiple Namespaces to keep applications separate, both to avoid naming conflicts and to manage access control. We look at Namespaces in more detail in <a href="ch11.xhtml#ch11">Chapter 11</a>.</em></p>&#13;
</div>&#13;
<p class="indent">The <code>kubectl describe</code> command output provides an event log <span class="ent">➋</span>, which is the first place to look for issues when we have problems starting a container.</p>&#13;
<p class="indent">Kubernetes takes a few steps when deploying a container. First, it needs to schedule it onto a node, which requires that node to be available with sufficient resources. Then, control passes to <code>kubelet</code> on that node, which has to interact with the container engine to pull the image, create a container, and start it.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_115"/>After the container is started, <code>kubelet</code> collects the standard out and standard error. We can view this output by using the <code>kubectl logs</code> command:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl logs nginx</span>&#13;
...&#13;
2021/07/13 22:37:03 [notice] 1#1: start worker processes&#13;
2021/07/13 22:37:03 [notice] 1#1: start worker process 33&#13;
2021/07/13 22:37:03 [notice] 1#1: start worker process 34</pre>&#13;
<p class="indent">The <code>kubectl logs</code> command always refers to a Pod because Pods are the basic resource used to run containers, and our Pod has only one container, so we can just specify the name of the Pod as a single parameter to <code>kubectl logs</code>. As before, Kubernetes will look in the <code>default</code> Namespace because we didn’t specify the Namespace.</p>&#13;
<p class="indent">The container output is available even if the container has exited, so the <code>kubectl logs</code> command is the place to look if a container is pulled and started successfully but then crashes. Of course, we have to hope that the container printed a log message explaining why it crashed. In <a href="ch10.xhtml#ch10">Chapter 10</a>, we look at what to do if we can’t get a container going and don’t have any log messages.</p>&#13;
<p class="indent">We’re done with the NGINX Pod, so let’s clean it up:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete -f /opt/nginx-pod.yaml</span>&#13;
pod "nginx" deleted</pre>&#13;
<p class="indent">We can use the same YAML configuration file to delete the Pod, which is convenient when we have multiple Kubernetes resources defined in a single file, as a single command will delete all of them. The <code>kubectl</code> command uses the name of each resource defined in the file to perform the delete.</p>&#13;
<h3 class="h3" id="ch00lev1sec33">Deployments</h3>&#13;
<p class="noindent">To run a container, we need a Pod, but that doesn’t mean we generally want to create the Pod directly. When we create a Pod directly, we don’t get all of the scalability and failover that Kubernetes offers, because Kubernetes will run only one instance of the Pod. This Pod will be allocated to a node only on creation, with no re-allocation even if the node fails.</p>&#13;
<p class="indent">To get scalability and failover, we instead need to create a controller to manage the Pod for us. We’ll look at multiple controllers that can run Pods, but let’s start with the most common: the <em>Deployment</em>.</p>&#13;
<h4 class="h4" id="ch00lev2sec51">Creating a Deployment</h4>&#13;
<p class="noindent">A Deployment manages one or more <em>identical</em> Kubernetes Pods. When we create a Deployment, we provide a Pod template. The Deployment then creates Pods matching that template with the help of a <em>ReplicaSet</em>.</p>&#13;
<div class="box5">&#13;
<p class="boxtitle-d"><span epub:type="pagebreak" id="page_116"/><strong>DEPLOYMENTS AND REPLICASETS</strong></p>&#13;
<p class="noindents">Kubernetes has evolved its controller resources over time. The first type of controller, the <em>ReplicationController</em>, provided only basic functionality. It was replaced by the ReplicaSet, which has improvements in how it identifies which Pods to manage.</p>&#13;
<p class="noindents">Part of the reason to replace ReplicationControllers with ReplicaSets is that ReplicationControllers were becoming more and more complicated, making the code difficult to maintain. The new approach splits up controller responsibility between ReplicaSets and Deployments. ReplicaSets are responsible for basic Pod management, including monitoring Pod status and performing failover. Deployments are responsible for tracking changes to the Pod template caused by configuration changes or container image updates. Deployments and ReplicaSets work together, but the Deployment creates its own ReplicaSet, so we usually need to interact only with Deployments. For this reason, I use the term <em>Deployment</em> generically to refer to features provided by the ReplicaSet, such as monitoring Pods to provide the requested number of replicas.</p>&#13;
</div>&#13;
<p class="indent">Here’s the YAML file we’ll use to create an NGINX Deployment:</p>&#13;
<p class="noindent6"><em>nginx-deploy.yaml</em></p>&#13;
<pre>---&#13;
 kind: Deployment&#13;
 apiVersion: apps/v1 &#13;
 metadata:&#13;
<span class="ent">➊</span> name: nginx &#13;
 spec:&#13;
   replicas: 3 &#13;
   selector: &#13;
     matchLabels:&#13;
       app: nginx&#13;
   template:&#13;
     metadata:&#13;
    <span class="ent">➋</span> labels:&#13;
         app: nginx&#13;
  <span class="ent">➌</span> spec:   &#13;
       containers:&#13;
       - name: nginx&#13;
         image: nginx&#13;
      <span class="ent">➍</span> resources:&#13;
           requests:&#13;
             cpu: "100m"</pre>&#13;
<p class="indent">Deployments are in the <code>apps</code> API group, so we specify <code>apps/v1</code> for <code>apiVersion</code>. Like every Kubernetes resource, we need to provide a unique name <span class="ent">➊</span> to keep this Deployment separate from any others we might create.</p>&#13;
<p class="indent">The Deployment specification has a few important fields, so let’s look at them in detail. The <code>replicas</code> field tells Kubernetes how many identical instances of the Pod we want. Kubernetes will work to keep this many Pods <span epub:type="pagebreak" id="page_117"/>running. The next field, <code>selector</code>, is used to enable the Deployment to find its Pods. The content of <code>matchLabels</code> must exactly match the content in the <code>template.metadata.labels</code> field <span class="ent">➋</span>, or Kubernetes will reject the Deployment.</p>&#13;
<p class="indent">Finally, the content of <code>template.spec</code> <span class="ent">➌</span> will be used as the <code>spec</code> for any Pods created by this Deployment. The fields here can include any configuration we can provide for a Pod. This configuration matches <em>nginx-pod.yaml</em> that we looked at earlier except that we add a CPU resource request <span class="ent">➍</span> so that we can configure autoscaling later on.</p>&#13;
<p class="indent">Let’s create our Deployment from this YAML resource file:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/nginx-deploy.yaml</span>&#13;
deployment.apps/nginx created</pre>&#13;
<p class="indent">We can track the status of the Deployment with <code>kubectl get</code>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get deployment nginx</span>&#13;
NAME    READY   UP-TO-DATE   AVAILABLE   AGE&#13;
nginx   3/3     3            3           4s</pre>&#13;
<p class="indent">When the Deployment is fully up, it will report that it has three replicas ready and available, which means that we now have three separate NGINX Pods managed by this Deployment:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                     READY   STATUS    RESTARTS   AGE&#13;
nginx-6799fc88d8-6vn44   1/1     Running   0          18s&#13;
nginx-6799fc88d8-dcwx5   1/1     Running   0          18s&#13;
nginx-6799fc88d8-sh8qs   1/1     Running   0          18s</pre>&#13;
<p class="indent">The name of each Pod begins with the name of the Deployment. Kubernetes adds some random characters to build the name of the ReplicaSet, followed by more random characters so that each Pod has a unique name. We don’t need to create or manage the ReplicaSet directly, but we can use <code>kubectl get</code> to see it:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get replicasets</span>&#13;
NAME               DESIRED   CURRENT   READY   AGE&#13;
nginx-6799fc88d8   3         3         3       30s</pre>&#13;
<p class="indent">Although we generally interact only with Deployments, it is important to know about the ReplicaSet, as some specific errors encountered when creating Pods are only reported in the ReplicaSet event log.</p>&#13;
<p class="indent">The <code>nginx</code> prefix on the ReplicaSet and Pod names are purely for convenience. The Deployment does not use names to match itself to Pods. Instead, it uses its selector to match the labels on the Pod. We can see these labels if we run <code>kubectl describe</code> on one of the three Pods:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe pod <span class="codeitalic1">nginx-6799fc88d8-6vn44</span></span>&#13;
Name:         nginx-6799fc88d8-6vn44&#13;
Namespace:    default&#13;
<span epub:type="pagebreak" id="page_118"/>...&#13;
Labels:       app=nginx&#13;
...</pre>&#13;
<p class="indent">This matches the Deployment’s selector:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe deployment nginx</span>&#13;
Name:                   nginx&#13;
Namespace:              default&#13;
...&#13;
Selector:               app=nginx&#13;
...</pre>&#13;
<p class="indent">The Deployment queries the API server to identify Pods matching its selector. Whereas the Deployment uses the programmatic API, the <code>kubectl get</code> command in the following example generates a similar API server query, giving us an opportunity to see how that works:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get all -l app=nginx</span>&#13;
NAME                     READY   STATUS    RESTARTS   AGE&#13;
nginx-6799fc88d8-6vn44   1/1     Running   0          69s&#13;
nginx-6799fc88d8-dcwx5   1/1     Running   0          69s&#13;
nginx-6799fc88d8-sh8qs   1/1     Running   0          69s&#13;
&#13;
NAME                               DESIRED   CURRENT   READY   AGE&#13;
replicaset.apps/nginx-6799fc88d8   3         3         3       69s</pre>&#13;
<p class="indent">Using <code>kubectl get all</code> in this case allows us to list multiple different kinds of resources as long as they match the selector. As a result, we see not only the three Pods but also the ReplicaSet that was created by the Deployment to manage those Pods.</p>&#13;
<p class="indent">It may seem strange that the Deployment uses a selector rather than just tracking the Pods it created. However, this design makes it easier for Kubernetes to be self-healing. At any time, a Kubernetes node might go offline, or we might have a network split, during which some control nodes lose their connection to the cluster. If a node comes back online, or the cluster needs to recombine after a network split, Kubernetes must be able to look at the current state of all of the running Pods and figure out what changes are required to achieve the desired state. This might mean that a Deployment that started an additional Pod as the result of a node disconnection would need to shut down a Pod when that node reconnects so that the cluster can maintain the appropriate number of replicas. Using a selector avoids the need for the Deployment to remember all the Pods it has ever created, even Pods on failed nodes.</p>&#13;
<h4 class="h4" id="ch00lev2sec52"><span epub:type="pagebreak" id="page_119"/>Monitoring and Scaling</h4>&#13;
<p class="noindent">Because the Deployment is monitoring its Pods to make sure we have the correct number of replicas, we can delete a Pod, and it will be automatically re-created:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete pod</span> <span class="codestrong1"><span class="codeitalic1">nginx-6799fc88d8-6vn44</span></span>&#13;
pod "nginx-6799fc88d8-6vn44" deleted&#13;
root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                     READY   STATUS    RESTARTS   AGE&#13;
nginx-6799fc88d8-dcwx5   1/1     Running   0          3m52s&#13;
nginx-6799fc88d8-dtddk   1/1     Running   0        <span class="ent">➊</span> 14s&#13;
nginx-6799fc88d8-sh8qs   1/1     Running   0          3m52s</pre>&#13;
<p class="indent">As soon as the old Pod is deleted, the Deployment created a new Pod <span class="ent">➊</span>. Similarly, if we change the number of replicas for the Deployment, Pods are automatically updated. Let’s add another replica:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl scale --replicas=4 deployment nginx</span>&#13;
deployment.apps/nginx scaled&#13;
root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                     READY   STATUS    RESTARTS   AGE&#13;
nginx-6799fc88d8-dcwx5   1/1     Running   0          8m22s&#13;
nginx-6799fc88d8-dtddk   1/1     Running   0          4m44s&#13;
nginx-6799fc88d8-kk7r6   1/1     Running   0        <span class="ent">➊</span> 5s &#13;
nginx-6799fc88d8-sh8qs   1/1     Running   0          8m22s</pre>&#13;
<p class="indent">The first command sets the number of replicas to four. As a result, Kubernetes needs to start a new identical Pod to meet the number we requested <span class="ent">➊</span>. We can scale the Deployment by updating the YAML file and re-running <code>kubectl apply</code>, or we can use the <code>kubectl scale</code> command to edit the Deployment directly. Either way, this is a declarative approach; we are updating the Deployment’s resource declaration; Kubernetes then updates the actual state of the cluster to match.</p>&#13;
<p class="indent">Similarly, scaling the Deployment down causes Pods to be automatically deleted:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl scale --replicas=2 deployment nginx</span>&#13;
deployment.apps/nginx scaled&#13;
root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                     READY   STATUS    RESTARTS   AGE&#13;
nginx-6799fc88d8-dcwx5   1/1     Running   0          10m&#13;
nginx-6799fc88d8-sh8qs   1/1     Running   0          10m</pre>&#13;
<p class="indent">When we scale down, Kubernetes selects two Pods to terminate. These Pods take a moment to finish shutting down, at which point we have only two NGINX Pods running.</p>&#13;
<h4 class="h4" id="ch00lev2sec53"><span epub:type="pagebreak" id="page_120"/>Autoscaling</h4>&#13;
<p class="noindent">For an application that is receiving real requests from users, we would choose the number of replicas necessary to provide a quality application, while scaling down when possible to reduce the amount of resources used by our application. Of course, the load on our application is constantly changing, and it would be tedious to monitor each component of our application continually to scale it independently. Instead, we can have the cluster perform the monitoring and scaling for us using a <em>HorizontalPodAutoscaler</em>. The term <em>horizontal</em> in this case just refers to the fact that the autoscaler can update the number of replicas of the same Pod managed by a controller.</p>&#13;
<p class="indent">To configure autoscaling, we create a new resource with a reference to our Deployment. The cluster then monitors resources used by the Pods and reconfigures the Deployment as needed. We could add a HorizontalPodAutoscaler to our Deployment using the <code>kubectl autoscale</code> command, but using a YAML resource file so that we can keep the autoscale configuration under version control is better. Here’s the YAML file:</p>&#13;
<p class="noindent6"><em>nginx-scaler.yaml</em></p>&#13;
<pre>   ---&#13;
<span class="ent">➊</span> apiVersion: autoscaling/v2&#13;
   kind: HorizontalPodAutoscaler&#13;
   metadata:&#13;
     name: nginx&#13;
     labels:&#13;
       app: nginx&#13;
   spec:&#13;
  <span class="ent">➋</span> scaleTargetRef:&#13;
       apiVersion: apps/v1&#13;
       kind: Deployment&#13;
       name: nginx&#13;
  <span class="ent">➌</span> minReplicas: 1&#13;
     maxReplicas: 10&#13;
     metrics:&#13;
       - type: Resource&#13;
         resource:&#13;
           name: cpu&#13;
           target:&#13;
             type: Utilization&#13;
             averageUtilization: <span class="ent">➍</span> 50</pre>&#13;
<p class="indent">In the <code>metadata</code> field, we add the label <code>app: nginx</code>. This does not change the behavior of the resource; its only purpose is to ensure that this resource shows up if we use an <code>app=nginx</code> label selector in a <code>kubectl get</code> command. This style of tagging the components of an application with consistent metadata is a good practice to help others understand what resources go together and to make debugging easier.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_121"/>This YAML configuration uses version 2 of the autoscaler configuration <span class="ent">➊</span>. Providing new versions of API resource groups is how Kubernetes accommodates future capability without losing any of its backward compatibility. Generally, alpha and beta versions are released for new resource groups before the final configuration is released, and there is at least one version of overlap between the beta version and the final release to enable seamless upgrades.</p>&#13;
<p class="indent">Version 2 of the autoscaler supports multiple resources. Each resource is used to calculate a vote on the desired number of Pods, and the largest number wins. Adding support for multiple resources requires a change in the YAML layout, which is a common reason for the Kubernetes maintainers to create a new resource version.</p>&#13;
<p class="indent">We specify our NGINX Deployment <span class="ent">➋</span> as the target for the autoscaler using its API resource group, kind, and name, which is enough to uniquely identify any resource in a Kubernetes cluster. We then tell the autoscaler to monitor the CPU utilization of the Pods that belong to the Deployment <span class="ent">➍</span>. The autoscaler will work to keep average CPU utilization by the Pods close to 50 percent over the long run, scaling up or down as necessary. However, the number of replicas will never go beyond the range we specify <span class="ent">➌</span>.</p>&#13;
<p class="indent">Let’s create our autoscaler using this configuration:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/nginx-scaler.yaml</span>&#13;
horizontalpodautoscaler.autoscaling/nginx created</pre>&#13;
<p class="indent">We can query the cluster to see that it was created:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get hpa</span>&#13;
NAME    REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE&#13;
nginx   Deployment/nginx   0%/50%    1         10        3          96s</pre>&#13;
<p class="indent">The output shows the autoscaler’s target reference, the current and desired resource utilization, and the maximum, minimum, and current number of replicas.</p>&#13;
<p class="indent">We use <code>hpa</code> as an abbreviation for <code>horizontalpodautoscaler</code>. Kubernetes allows us to use either singular or plural names and provides abbreviations for most of its resources to save typing. For example, we can type <code>deploy</code> for <code>deployment</code> and even <code>po</code> for <code>pods</code>. Every extra keystroke counts!</p>&#13;
<p class="indent">The autoscaler uses CPU utilization data that the <code>kubelet</code> is already collecting from the container engine. This data is centralized by the metrics server we installed as a cluster add-on. Without that cluster add-on, there would be no utilization data, and the autoscaler would not make any changes to the Deployment. In this case, because we’re not really using our NGINX server instances, they aren’t consuming any CPU, and the Deployment is scaled down to a single Pod, the minimum we specified:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                     READY   STATUS    RESTARTS   AGE&#13;
nginx-6799fc88d8-dcwx5   1/1     Running   0          15m</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_122"/>The autoscaler has calculated that only one Pod is needed and has scaled the Deployment to match. The Deployment then selected a Pod to terminate to reach the desired scale.</p>&#13;
<p class="indent">For accuracy, the autoscaler will not use CPU data from the Pod if it recently started running, and it has logic to prevent it from scaling up or down too often, so if you ran through these examples very quickly you might need to wait a few minutes before you see it scale.</p>&#13;
<p class="indent">We explore Kubernetes resource utilization metrics in more detail when we look at limiting resource usage in <a href="ch14.xhtml#ch14">Chapter 14</a>.</p>&#13;
<h3 class="h3" id="ch00lev1sec34">Other Controllers</h3>&#13;
<p class="noindent">Deployments are the most generic and commonly used controller, but Kubernetes has some other useful options. In this section, we explore <em>Job</em>s and <em>CronJob</em>s, <em>StatefulSets</em>, and <em>DaemonSets</em>.</p>&#13;
<h4 class="h4" id="ch00lev2sec54">Jobs and CronJobs</h4>&#13;
<p class="noindent">Deployments are great for application components because we usually want one or more instances to stay running indefinitely. However, for cases for which we need to run a command, either once or on a schedule, we can use a Job. The primary difference is a Deployment ensures that any container that stops running is restarted, whereas a Job can check the exit code of the main process and restart only if the exit code is non-zero, indicating failure.</p>&#13;
<p class="indent">A Job definition looks very similar to a Deployment:</p>&#13;
<p class="noindent6"><em>sleep-job.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: batch/v1&#13;
kind: Job&#13;
metadata:&#13;
  name: sleep&#13;
spec:&#13;
  template:&#13;
    spec:&#13;
      containers:&#13;
      - name: sleep&#13;
        image: busybox&#13;
        command: &#13;
          - "/bin/sleep"&#13;
          - "30"&#13;
      restartPolicy: OnFailure</pre>&#13;
<p class="indent">The <code>restartPolicy</code> can be set to <code>OnFailure</code>, in which case the container will be restarted for a non-zero exit code, or to <code>Never</code>, in which case the Job will be completed when the container exits regardless of the exit code.</p>&#13;
<p class="indent">We can create and view the Job and the Pod it has created:</p>&#13;
<pre><span epub:type="pagebreak" id="page_123"/>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/sleep-job.yaml</span>&#13;
job.batch/sleep created&#13;
root@host01:~# <span class="codestrong1">kubectl get job</span>&#13;
NAME    COMPLETIONS   DURATION   AGE&#13;
sleep   0/1           3s         3s&#13;
root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                     READY   STATUS    RESTARTS   AGE&#13;
...&#13;
sleep-fgcnz              1/1     Running   0          10s</pre>&#13;
<p class="indent">The Job has created a Pod per the specification provided in the YAML file. The Job reflects <code>0/1</code> completions because it is waiting for its Pod to exit successfully.</p>&#13;
<p class="indent">When the Pod has been running for 30 seconds, it exits with a code of zero, indicating success, and the Job and Pod status are updated accordingly:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get jobs</span>&#13;
NAME    COMPLETIONS   DURATION   AGE&#13;
sleep   1/1           31s        40s&#13;
root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                     READY   STATUS      RESTARTS   AGE&#13;
nginx-65db7cf9c9-2wcng   1/1     Running     0          31m&#13;
sleep-fgcnz              0/1     Completed   0          43s</pre>&#13;
<p class="indent">The Pod is still available, which means that we could review its logs if desired, but it shows a status of <code>Completed</code>, so Kubernetes will not try to restart the exited container.</p>&#13;
<p class="indent">A CronJob is a controller that creates Jobs on a schedule. For example, we could set up our sleep Job to run once per day:</p>&#13;
<p class="noindent6"><em>sleep-cronjob.yaml</em></p>&#13;
<pre> ---&#13;
 apiVersion: batch/v1&#13;
 kind: CronJob&#13;
 metadata:&#13;
   name: sleep&#13;
 spec:&#13;
<span class="ent">➊</span> schedule: "0 3 * * *"&#13;
<span class="ent">➋</span> jobTemplate: &#13;
   spec:&#13;
     template:&#13;
       spec:&#13;
         containers:&#13;
           - name: sleep&#13;
             image: busybox&#13;
             command: &#13;
               - "/bin/sleep"&#13;
               - "30"&#13;
         restartPolicy: OnFailure</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_124"/>The entire contents of the Job specification are embedded inside the <code>jobTemplate</code> field <span class="ent">➋</span>. To this, we add a <code>schedule</code> <span class="ent">➊</span> that follows the standard format for the Unix <code>cron</code> command. In this case, <code>0 3 * * *</code> indicates that a Job should be created at 3:00 AM every day.</p>&#13;
<p class="indent">One of Kubernetes’ design principles is that anything could go down at any time. For a CronJob, if the cluster has an issue during the time the Job would be scheduled, the Job might not be scheduled, or it might be scheduled twice, this means that you should take care to write Jobs in an idempotent way so that they can handle missing or duplicated scheduling.</p>&#13;
<p class="indent">If we create this CronJob</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/sleep-cronjob.yaml</span> &#13;
cronjob.batch/sleep created</pre>&#13;
<p class="noindent">it now exists in the cluster, but it does not immediately create a Job or a Pod:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get jobs</span>&#13;
NAME    COMPLETIONS   DURATION   AGE&#13;
sleep   1/1           31s        2m32s&#13;
root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                     READY   STATUS      RESTARTS   AGE&#13;
nginx-65db7cf9c9-2wcng   1/1     Running     0          33m&#13;
sleep-fgcnz              0/1     Completed   0          2m23s</pre>&#13;
<p class="indent">Instead, the CronJob will create a new Job each time its schedule is triggered.</p>&#13;
<h4 class="h4" id="ch00lev2sec55">StatefulSets</h4>&#13;
<p class="noindent">So far, we’ve been looking at controllers that create identical Pods. With both Deployments and Jobs, we don’t really care which Pod is which, or where it is deployed, as long as we run enough instances at the right time. However, that doesn’t always match the behavior we want. For example, even though a Deployment can create Pods with persistent storage, the storage must either be brand new for each new Pod, or the same storage must be shared across all Pods. That doesn’t align well with a “primary and secondary” architecture such as a database. For those cases, we want specific storage to be attached to specific Pods.</p>&#13;
<p class="indent">At the same time, because Pods can come and go due to hardware failures or upgrades, we need a way to manage the replacement of a Pod so that each Pod is attached to the right storage. This is the purpose of a <em>StatefulSet</em>. A StatefulSet identifies each Pod with a number, starting at zero, and each Pod receives matching persistent storage. When a Pod must be replaced, the new Pod is assigned the same numeric identifier and is attached to the same storage. Pods can look at their hostname to determine their identifier, so a StatefulSet is useful both for cases with a fixed primary instance as well as cases for which a primary instance is dynamically chosen.</p>&#13;
<p class="indent">We’ll explore a lot more details related to Kubernetes StatefulSets in the next several chapters, including persistent storage and Services. For this <span epub:type="pagebreak" id="page_125"/>chapter, we’ll look at a basic example of a StatefulSet and then build on it as we introduce other important concepts.</p>&#13;
<p class="indent">For this simple example, let’s create two Pods and show how they each get unique storage that stays in place even if the Pod is replaced. We’ll use this YAML resource:</p>&#13;
<p class="noindent6"><em>sleep-set.yaml</em></p>&#13;
<pre> ---&#13;
 apiVersion: apps/v1&#13;
 kind: StatefulSet&#13;
 metadata:&#13;
    name: sleep&#13;
 spec:&#13;
<span class="ent">➊</span> serviceName: sleep &#13;
   replicas: 2&#13;
   selector:&#13;
     matchLabels:&#13;
       app: sleep&#13;
   template:&#13;
     metadata:&#13;
       labels:&#13;
         app: sleep&#13;
     spec:&#13;
       containers:&#13;
         - name: sleep&#13;
           image: busybox&#13;
           command: &#13;
             - "/bin/sleep"&#13;
             - "3600"&#13;
        <span class="ent">➋</span> volumeMounts: &#13;
             - name: sleep-volume&#13;
               mountPath: /storagedir&#13;
<span class="ent">➌</span> volumeClaimTemplates: &#13;
     - metadata:&#13;
         name: sleep-volume&#13;
       spec:&#13;
         storageClassName: longhorn&#13;
         accessModes:&#13;
           - ReadWriteOnce&#13;
         resources:&#13;
           requests:&#13;
             storage: 10Mi</pre>&#13;
<p class="indent">There are a few important differences here compared to a Deployment or a Job. First, we must declare a <code>serviceName</code> to tie this StatefulSet to a Kubernetes Service <span class="ent">➊</span>. This connection is used to create a Domain Name Service (DNS) entry for each Pod. We must also provide a template for the StatefulSet to use to request persistent storage <span class="ent">➌</span> and then tell Kubernetes where to mount that storage in our container <span class="ent">➋</span>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_126"/>The actual <em>sleep-set.yaml</em> file that the automation scripts install includes the <code>sleep</code> Service definition. We cover Services in detail in <a href="ch09.xhtml#ch09">Chapter 9</a>.</p>&#13;
<p class="indent">Let’s create the <code>sleep</code> StatefulSet:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/sleep-set.yaml</span></pre>&#13;
<p class="indent">The StatefulSet creates two Pods:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get statefulsets</span>&#13;
NAME    READY   AGE&#13;
sleep   2/2     1m14s&#13;
root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME      READY   STATUS    RESTARTS   AGE&#13;
...&#13;
sleep-0   1/1     Running   0          57s&#13;
sleep-1   1/1     Running   0          32s</pre>&#13;
<p class="indent">The persistent storage for each Pod is brand new, so it starts empty. Let’s create some content. The easiest way to do that is from within the container itself, using <code>kubectl exec</code>, which allows us to run commands inside a container, similar to <code>crictl</code>. The <code>kubectl exec</code> command works no matter what host the container is on, even if we’re connecting to our Kubernetes API server from outside the cluster.</p>&#13;
<p class="indent">Let’s write each container’s hostname to a file and print it out so that we can verify it worked:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec sleep-0 -- /bin/sh -c \</span>&#13;
  <span class="codestrong1">'hostname &gt; /storagedir/myhost'</span>&#13;
root@host01:~# <span class="codestrong1">kubectl exec sleep-0 -- /bin/cat /storagedir/myhost</span>&#13;
sleep-0&#13;
root@host01:~# <span class="codestrong1">kubectl exec sleep-1 -- /bin/sh -c \</span>&#13;
  <span class="codestrong1">'hostname &gt; /storagedir/myhost'</span>&#13;
root@host01:~# <span class="codestrong1">kubectl exec sleep-1 -- /bin/cat /storagedir/myhost</span>&#13;
sleep-1</pre>&#13;
<p class="indent">Each of our Pods now has unique content in its persistent storage. Let’s delete one of the Pods and verify that its replacement inherits its predecessor’s storage:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete pod sleep-0</span>&#13;
pod "sleep-0" deleted&#13;
root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME      READY   STATUS    RESTARTS   AGE&#13;
...&#13;
sleep-0   1/1     Running   0          28s&#13;
sleep-1   1/1     Running   0          8m18s&#13;
root@host01:~# <span class="codestrong1">kubectl exec sleep-0 -- /bin/cat /storagedir/myhost</span>&#13;
sleep-0</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_127"/>After deleting <code>sleep-0</code>, we see a new Pod created with the same name, which is different from the Deployment for which a random name was generated for every new Pod. Additionally, for this new Pod, the file we created previously is still present because the StatefulSet attached the same persistent storage to the new Pod it created when the old one was deleted.</p>&#13;
<h4 class="h4" id="ch00lev2sec56">Daemon Sets</h4>&#13;
<p class="noindent">The <em>DaemonSet</em> controller is like a StatefulSet in that the DaemonSet also runs a specific number of Pods, each with a unique identity. However, the DaemonSet runs exactly one Pod per node, which is useful primarily for control plane and add-on components for a cluster, such as a network or storage plug-in.</p>&#13;
<p class="indent">Our cluster already has multiple DaemonSets installed, so let’s look at the <code>calico-node</code> DaemonSet that’s already running, which runs on each node to provide network configuration for all containers on that node.</p>&#13;
<p class="indent">The <code>calico-node</code> DaemonSet is in the <code>calico-system</code> Namespace, so we’ll specify that Namespace to request information about the DaemonSet:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n calico-system get daemonsets</span>&#13;
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   ...&#13;
calico-node   3         3         3       3            3           ...</pre>&#13;
<p class="indent">Our cluster has three nodes, so the <code>calico-node</code> DaemonSet has created three instances. Here’s the configuration of this DaemonSet in YAML format:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n calico-system get daemonset calico-node -o yaml</span>&#13;
apiVersion: apps/v1&#13;
kind: DaemonSet&#13;
metadata:&#13;
...&#13;
  name: calico-node&#13;
  namespace: calico-system&#13;
...&#13;
spec:&#13;
...&#13;
  selector:&#13;
    matchLabels:&#13;
      k8s-app: calico-node&#13;
...</pre>&#13;
<p class="indent">The <code>-o yaml</code> parameter to <code>kubectl get</code> prints out the configuration and status of one or more resources in YAML format, allowing us to inspect Kubernetes resources in detail.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_128"/>The selector for this DaemonSet expects a label called <code>k8s-app</code> to be set to <code>calico-node</code>. We can use this to show just the Pods that this DaemonSet creates:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n calico-system get pods \</span>&#13;
  <span class="codestrong1">-l k8s-app=calico-node -o wide</span>&#13;
NAME                READY   STATUS   ... NODE   ...&#13;
calico-node-h9kjh   1/1     Running  ... host01 ...&#13;
calico-node-rcfk7   1/1     Running  ... host03 ...&#13;
calico-node-wj876   1/1     Running  ... host02 ...</pre>&#13;
<p class="indent">The DaemonSet has created three Pods, each of which is assigned to one of the nodes in our cluster. If we add additional nodes to our cluster, the DaemonSet will schedule a Pod on the new nodes as well.</p>&#13;
<h3 class="h3" id="ch00lev1sec35">Final Thoughts</h3>&#13;
<p class="noindent">This chapter explored Kubernetes from the perspective of a regular cluster user, creating controllers that in turn create Pods with containers. Having this core knowledge of controller resource types is essential for building our applications. At the same time, it’s important to remember that Kubernetes is using the container technology we explored in <a href="part01.xhtml#part01">Part I</a>.</p>&#13;
<p class="indent">One key aspect of container technology is the ability to isolate containers in separate network namespaces. Running containers in a Kubernetes cluster adds additional requirements for networking because we now need to connect containers running on different cluster nodes. In the next chapter, we consider multiple approaches to make this work as we look at overlay networks.</p>&#13;
</body></html>