<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="appendix"><span epub:type="pagebreak" id="page_207"/><strong>APPENDIX: ANSWERS TO THE EXERCISES</strong></h2>&#13;
<h3 class="h3" id="ch00lev149"><strong>Chapter 1</strong></h3>&#13;
<p class="number"><strong>1-1.</strong> The final layer before the output layer (the second fully connected layer in this case) may be most useful for embeddings. However, we could also use all other intermediate layers to create embeddings. Since the later layers tend to learn higher-level features, these later layers are typically more semantically meaningful and better suited for different types of tasks, including related classification tasks.</p>&#13;
<p class="number"><strong>1-2.</strong> One of the traditional methods of input representation that is different from embeddings is one-hot encoding, as discussed in <a href="ch01.xhtml">Chapter 1</a>. In this method, each categorical variable is represented using a binary vector where only one value is “hot” or active (for instance, set to 1), while all other positions remain inactive (for instance, set to 0).</p>&#13;
<p class="numberp">Another representation that is not an embedding is histograms. A typical example of this is image histograms (see <em><a href="https://en.wikipedia.org/wiki/Image_histogram">https://en.wikipedia.org/wiki/Image_histogram</a></em> for examples). These histograms provide a graphical representation of the tonal distribution in a digital image, capturing the intensity distribution of pixels.</p>&#13;
<p class="numberp">Additionally, the bag-of-words model offers another approach distinct from embeddings. In this model, an input sentence is represented as an unordered collection or “bag” of its words, disregarding grammar and even word order. For more details about the bag-of-words model, see <em><a href="https://en.wikipedia.org/wiki/Bag-of-words_model">https://en.wikipedia.org/wiki/Bag-of-words_model</a></em>.</p>&#13;
<h3 class="h3" id="ch00lev150"><strong>Chapter 2</strong></h3>&#13;
<p class="number"><span epub:type="pagebreak" id="page_208"/><strong>2-1.</strong> One way to apply self-supervised learning to video data is by predicting the next frame in the video. This is analogous to next-word prediction in large language models such as GPT. This method challenges the model to anticipate subsequent events or movements in a sequence, giving it a temporal understanding of the content.</p>&#13;
<p class="numberp">Another approach is to predict missing or masked frames. This idea draws inspiration from large language models like BERT, where certain words are masked and the model is tasked with predicting them. In the case of video, entire frames can be masked, and the model learns to interpolate and predict the masked frame based on the context provided by surrounding frames.</p>&#13;
<p class="numberp">Inpainting is yet another avenue for self-supervised learning in videos. Here, instead of masking entire frames, specific pixel areas within a frame are masked. The model is then trained to predict the missing or masked parts, which can help it grasp fine-grained visual details and spatial relationships in the video content.</p>&#13;
<p class="numberp">Lastly, a coloring technique can be used where the video is converted to grayscale and the model is tasked with predicting the color. This not only teaches the model about the original colors of objects, but it also gives insights into lighting, shadows, and the general mood of the scenes.</p>&#13;
<p class="number"><strong>2-2.</strong> We can remove (mask) feature values and train a model to predict these, analogously to classic data imputation. For example, one method that uses this approach is TabNet; see Sercan O. Arik and Tomas Pfister, “TabNet: Attentive Interpretable Tabular Learning” (2019), <em><a href="https://arxiv.org/abs/1908.07442">https://arxiv.org/abs/1908.07442</a></em>.</p>&#13;
<p class="numberp">It is also possible to use contrastive learning by generating augmented versions of the training examples in the original raw feature space or the embedding space. For example, the SAINT and SCARF methods employ this approach. For the former, see Gowthami Some-palli et al., “SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training” (2021), <em><a href="https://arxiv.org/abs/2106.01342">https://arxiv.org/abs/2106.01342</a></em>. For the latter, see Dara Bahri et al., “SCARF: Self-Supervised Contrastive Learning Using Random Feature Corruption” (2021), <em><a href="https://arxiv.org/abs/2106.15147">https://arxiv.org/abs/2106.15147</a></em>.</p>&#13;
<h3 class="h3" id="ch00lev151"><strong>Chapter 3</strong><span epub:type="pagebreak" id="page_209"/></h3>&#13;
<p class="number"><strong>3-1.</strong> Similar to a supervised learning approach, we first divide the dataset into a training set and a test set. We then further divide the training and test sets into subsets, with one image from each class. To design the training task, we consider only a subset of classes, such as the classes (digits) 0, 1, 2, 5, 6, 8, 9. Next, for testing, we use the remaining classes 3, 4, 7. For each classification task, the neural network receives only one example per image.</p>&#13;
<p class="number"><strong>3-2.</strong> Consider a medical imaging scenario for rare diseases. The training dataset may consist of only a few examples corresponding to different types of diseases, and a few-shot system may have only one or a handful of cases for a new, unseen rare disease (not contained in the training set). The task is then to identify a new rare disease based on this limited number of examples.</p>&#13;
<p class="numberp">Another example of a few-shot system is a recommender that has only a limited number of items a user rated. Based on this limited number of examples, the model has to predict future products the user may like. Imagine a warehouse robot that has to learn to recognize new objects as a company increases its inventory. The robot has to learn to recognize and adapt to these new objects based on only a few examples.</p>&#13;
<h3 class="h3" id="ch00lev152"><strong>Chapter 4</strong></h3>&#13;
<p class="number"><strong>4-1.</strong> You might try increasing the size of the initial neural network. It might be possible that the chosen network is too small to contain a suitable subnetwork.</p>&#13;
<p class="numberp">Another option is to try a different random initialization (for example, by changing the random seed). The lottery hypothesis assumes that <em>some</em> randomly initialized networks contain highly accurate subnetworks that can be obtained by pruning, but not all networks may have such subnetworks.</p>&#13;
<p class="number"><strong>4-2.</strong> When training a neural network with ReLU activation functions, specific activations will be set to 0 if the function input is less than 0. This causes certain nodes in the hidden layers not to contribute to the computations; these nodes are sometimes called <em>dead neurons</em>. While ReLU activations do not directly cause sparse weights, the zero activation outputs sometimes lead to zero weights that are not recoverable. This observation supports the lottery hypothesis, which suggests that well-trained networks may contain subnetworks with sparse, trainable weights that can be pruned without loss of accuracy.</p>&#13;
<h3 class="h3" id="ch00lev153"><strong>Chapter 5</strong></h3>&#13;
<p class="number"><strong>5-1.</strong> XGBoost is a tree-based gradient-boosting implementation that does not, at the time of writing, support transfer learning. In contrast to artificial neural networks, XGBoost is a nonparametric model that we cannot readily update as new data arrives; hence, regular transfer learning would not work here.</p>&#13;
<p class="numberp">However, it is possible to use the results of an XGBoost model trained on one task as features for another XGBoost model. Consider an overlapping set of features for both datasets. For example, we could design a classification task in a self-supervised fashion for the combined dataset. We could then train a second XGBoost model on the target dataset that takes the original feature set as input, along with the output of the first XGBoost model.<span epub:type="pagebreak" id="page_210"/></p>&#13;
<p class="number"><strong>5-2.</strong> When applying data augmentations, we usually have to increase the training time as well; it is possible that we needed to train the model for a longer period.</p>&#13;
<p class="numberp">Alternatively, we may have applied too much data augmentation. Augmenting the data too much can result in excessive variations that do not reflect the natural variations in the data, leading to overfitting or poor generalization to new data. In the case of MNIST, this can also include translating or cropping the image in such a way that the digits become unrecognizable due to missing parts.</p>&#13;
<p class="numberp">Another possibility is that we’ve applied naive, domain-inconsistent augmentation. For example, suppose we are mirroring or flipping images vertically or horizontally. For MNIST, this doesn’t make sense, because flipping handwritten digits vertically or horizontally would create numbers that don’t exist in the real world.</p>&#13;
<h3 class="h3" id="ch00lev154"><strong>Chapter 6</strong></h3>&#13;
<p class="number"><strong>6-1.</strong> Tuning the number of training epochs is a simpler and more universal approach. This is especially true for older frameworks that don’t support model checkpointing. Changing the number of training epochs may therefore be an easier solution and is particularly attractive for small datasets and models where each hyperparameter configuration is cheap to run and evaluate. This approach also eliminates the need for monitoring the performance on a validation set during training, making it straightforward and easy to use.</p>&#13;
<p class="numberp">The early-stopping and checkpointing approach is especially useful when working with models that are expensive to train. It’s generally also a more flexible and robust method for preventing overfitting. However, a downside of this approach is that, in noisy training regimes, we may end up prioritizing an early epoch even though the validation set accuracy is not a good estimate of the generalization accuracy.</p>&#13;
<p class="number"><strong>6-2.</strong> One obvious downside of ensemble methods is the increased computational cost. For example, if we build a neural network ensemble of five neural networks, this ensemble can be five times as expensive as every single model.</p>&#13;
<p class="numberp">While we often consider the inferencing costs mentioned above, the increased storage cost is another significant limitation. Nowadays, most computer vision and language models have millions or even billions of parameters that have to be stored in a distributed setting. Model ensembling complicates this further.</p>&#13;
<p class="numberp">Reduced interpretability is yet another cost that we incur when using model ensembles. Understanding and analyzing the predictions of a single model can already be challenging. Depending on the ensembling approach, we add yet another layer of complexity that reduces interpretability.<span epub:type="pagebreak" id="page_211"/></p>&#13;
<h3 class="h3" id="ch00lev155"><strong>Chapter 7</strong></h3>&#13;
<p class="number"><strong>7-1.</strong> The Adam optimizer implements an adaptive method that comes with internal weight parameters. Adam has two optimizer parameters (mean and variance) per model parameter, so instead of only splitting the weight tensors of the model, we also have to split the optimizer states to work around memory limitations. (Note that this is already implemented in most DeepSpeed parallelization techniques.)</p>&#13;
<p class="number"><strong>7-2.</strong> Data parallelism could theoretically work on a CPU, but the benefits would be limited. For example, instead of duplicating the model in CPU memory to train multiple models on different batches of the dataset in parallel, it could make more sense to increase the data throughput.</p>&#13;
<h3 class="h3" id="ch00lev156"><strong>Chapter 8</strong></h3>&#13;
<p class="number"><strong>8-1.</strong> Self-attention has quadratic compute and memory complexity due to the <em>n</em>-to-<em>n</em> comparisons (where <em>n</em> is the input sequence length), which makes transformers computationally costly compared to other neural network architectures. Moreover, decoder-style transformers such as GPT generate outputs one token at a time, which cannot be parallelized during inference (although generating each token is still highly parallelizable, as discussed in <a href="ch08.xhtml">Chapter 8</a>).</p>&#13;
<p class="number"><strong>8-2.</strong> Yes, we can think of self-attention as a form of feature selection, although there are differences between this and other types of feature selection. It is important to differentiate between hard and soft attention in this context. Soft attention computes importance weights for all inputs, whereas hard attention selects a subset of the inputs. Hard attention is more like masking, where certain inputs are set to 0 or 1, while soft attention allows for a continuous range of importance scores. The main difference between attention and feature selection is that feature selection is typically a fixed operation, while attention weights are computed dynamically based on the input. With feature-selection algorithms, the selected features are always the same, whereas with attention, the weights can change based on the input.</p>&#13;
<h3 class="h3" id="ch00lev157"><strong>Chapter 9</strong></h3>&#13;
<p class="number"><strong>9-1.</strong> Automating this evaluation is inherently difficult, and the gold standard is currently based on human evaluation and judgment. However, a few metrics exist as quantitative measures.</p>&#13;
<p class="numberp">To evaluate the diversity of the generated images, one can compare the conditional class distribution and the marginal class distribution of generated samples, using, for example, a Kullback–Leibler-divergence (KL-divergence) regularization term. This measure is also used in the VAE to make the latent space vectors similar to a standard Gaussian. The higher the KL-divergence term, the more diverse the generated images.<span epub:type="pagebreak" id="page_212"/></p>&#13;
<p class="numberp">One can also compare the statistics of generated images to real images in the feature space of a pretrained model, such as a convolutional network trained as an image classifier. A high similarity (or low distance) indicates that the two distributions are close to each other, which is generally a sign of better image quality. This approach is also often known as the <em>Fréchet inception distance approach</em>.</p>&#13;
<p class="number"><strong>9-2.</strong> Like the generators of GANs, VAEs, or diffusion models, a consistency model takes a noise tensor sampled from a simple distribution (such as a standard Gaussian) as input and generates a new image.</p>&#13;
<h3 class="h3" id="ch00lev158"><strong>Chapter 10</strong></h3>&#13;
<p class="number1"><strong>10-1.</strong> Yes, we can make top-<em>k</em> sampling deterministic by setting <em>k</em> = 1 so that the model will always select the word with the highest probability score as the next word when generating the output text.</p>&#13;
<p class="numberp">We can also make nucleus sampling deterministic, such as by setting the probability mass threshold <em>p</em> such that it includes only one item, which either exactly meets or exceeds this threshold. This would make the model always choose the token with the highest probability.</p>&#13;
<p class="number1"><strong>10-2.</strong> In some cases, the random behavior of dropout during inference can be desirable, such as when building model ensembles with a single model. (Without the random behavior in dropout, the model would produce exactly the same results for a given input, which would make an ensemble redundant.)</p>&#13;
<p class="numberp">Moreover, the random inference behavior in dropout can be useful for robustness testing. For critical applications, like healthcare or autonomous driving, it’s essential to understand how slight variations to the model can impact its predictions. By using deterministic dropout patterns, we can simulate these slight variations and test the robustness of the model.</p>&#13;
<h3 class="h3" id="ch00lev159"><strong>Chapter 11</strong></h3>&#13;
<p class="number1"><strong>11-1.</strong> SGD has only the learning rate as a hyperparameter, but it does not have any parameters. Therefore, it does not add any additional parameters to be stored besides the gradients calculated for each weight parameter during backpropagation (including the layer activations required for calculating the gradients).</p>&#13;
<p class="numberp">The Adam optimizer is more complex and requires more storage. Specifically, Adam keeps an exponentially decaying average of past gradients (first moment) and an exponentially decaying average of past squared gradients (second raw moment) for each parameter. Therefore, for each parameter in the network, Adam needs to store two additional values. If we have <em>n</em> parameters in the network, Adam requires storage for 2<em>n</em> additional parameters.</p>&#13;
<p class="numberp"><span epub:type="pagebreak" id="page_213"/>If the network has <em>n</em> trainable parameters, Adam adds 2<em>n</em> parameters to be tracked. For example, in the case of AlexNet, which consists of 26,926 parameters, as calculated in Exercise 1-1, Adam requires 53,852 additional values in total (2 <em>×</em> 26,926).</p>&#13;
<p class="number1"><strong>11-2.</strong> Each BatchNorm layer learns two sets of parameters during training: a set of scaling coefficients (gamma) and a set of shifting coefficients (beta). These are learned so that the model can undo the normalization when it is found to be detrimental to learning. Each of these sets of parameters (gamma and beta) has the same size as the number of channels (or neurons) in the layer they normalize because these parameters are learned separately for each channel (or neuron).</p>&#13;
<p class="numberp">For the first BatchNorm layer following the first convolutional layer with five output channels, this adds 10 additional parameters. For the second BatchNorm layer, following the second convolutional layer with 12 output channels, this adds 24 additional parameters.</p>&#13;
<p class="numberp">The first fully connected layer has 128 output channels, which means 256 additional BatchNorm parameters. The second fully connected layer is not accompanied by a BatchNorm layer since it’s the output layer.</p>&#13;
<p class="numberp">Therefore, BatchNorm adds 10 + 24 + 256 = 290 additional parameters to the network.</p>&#13;
<h3 class="h3" id="ch00lev160"><strong>Chapter 12</strong></h3>&#13;
<p class="number1"><strong>12-1.</strong> Just increasing the stride from 1 to 2 (or larger values) should not affect the equivalence since the kernel size is equal to the input size in both scenarios, so there is no sliding window mechanism at play here.</p>&#13;
<p class="number1"><strong>12-2.</strong> Increasing the padding to values larger than 0 will affect the results. Due to the padded inputs, we will have the sliding window convolutional operation where the equivalence with fully connected layers no longer holds. In other words, the padding would alter the input’s spatial dimensions, which would no longer match the kernel size and would result in more than one output value per feature map.</p>&#13;
<h3 class="h3" id="ch00lev161"><strong>Chapter 13</strong></h3>&#13;
<p class="number1"><strong>13-1.</strong> Using smaller patches increases the number of patches for a given input image, leading to a higher number of tokens being fed into the transformer. This results in increased computational complexity, as the self-attention mechanism in transformers has quadratic complexity with respect to the number of input tokens. Consequently, smaller input patches make the model computationally more expensive.</p>&#13;
<p class="number1"><strong>13-2.</strong> Using larger input patches may result in the loss of finer details and local structures in the input image, which can potentially negatively affect the model’s predictive performance. Interested readers might enjoy the FlexiViT paper that studies the computational and predictive<span epub:type="pagebreak" id="page_214"/>performance trade-offs as a consequence of the patch size and number (Lucas Beyer et al., “FlexiViT: One Model for All Patch Sizes” [2022], <em><a href="https://arxiv.org/abs/2212.08013">https://arxiv.org/abs/2212.08013</a></em>).</p>&#13;
<h3 class="h3" id="ch00lev162"><strong>Chapter 14</strong></h3>&#13;
<p class="number1"><strong>14-1.</strong> Because homophones have different meanings, we expect them to appear in other contexts, such as <em>there</em> and <em>their</em> in “I can see you over there” and “Their paper is very nice.”</p>&#13;
<p class="numberp">Since the distributional hypothesis says that words with similar meanings should appear in similar contexts, homophones do not contradict the distributional hypothesis.</p>&#13;
<p class="number1"><strong>14-2.</strong> The underlying idea of the distributional hypothesis can be applied to other domains, such as computer vision. In the case of images, objects that appear in similar visual contexts are likely to be semantically related. On a lower level, neighboring pixels are likely semantically related, as they are part of the same object; this idea is used in masked autoen-coding for self-supervised learning on image data. (We covered masked autoencoders in <a href="ch02.xhtml">Chapter 2</a>.)</p>&#13;
<p class="numberp">Another example is protein modeling. For example, researchers showed that language transformers trained on protein sequences (a string representation where each letter represents an amino acid, like <em>MNGTEGPNFYVPFSNKTGVV . . .</em>) learn embeddings where similar amino acids cluster together (Alexander Rives et al., “Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences” [2019], <em><a href="https://www.biorxiv.org/content/10.1101/622803v1.full">https://www.biorxiv.org/content/10.1101/622803v1.full</a></em>). The hydrophobic amino acids such as V, I, L, and M appear in one cluster, and aromatic amino acids such as F, W, and Y appear in another cluster. In this context, we can think of an amino acid as an equivalent to a word in a sentence.</p>&#13;
<h3 class="h3" id="ch00lev163"><strong>Chapter 15</strong></h3>&#13;
<p class="number1"><strong>15-1.</strong> Assuming that the existing data does not suffer from privacy concerns, data augmentation helps generate variations of the existing data without the need to collect additional data, which can help with privacy concerns.</p>&#13;
<p class="numberp">However, if the original data includes personally identifiable information, even augmented or synthetic data could potentially be linked back to individuals, especially if the augmentation process doesn’t sufficiently obscure or alter the original data.</p>&#13;
<p class="number1"><strong>15-2.</strong> Data augmentation might be less beneficial if the original dataset is already large and diverse enough that the model isn’t overfitting or underperforming due to a lack of data. This is, for example, often the case when pretraining LLMs. The performance of highly domain-specific models (for example, in the medical, law, and financial domains) could <span epub:type="pagebreak" id="page_215"/>also be adversely affected by techniques such as synonym replacement and back translation due to replacing domain-specific terms with a certain meaning. In general, in contexts of tasks highly sensitive to wording choices, data augmentation must be applied with particular care.</p>&#13;
<h3 class="h3" id="ch00lev164"><strong>Chapter 16</strong></h3>&#13;
<p class="number1"><strong>16-1.</strong> The self-attention mechanism has quadratic time and memory complexity. More precisely, we can express the time and memory complexity of self-attention as <em>O</em>(<em>N</em><sup>2</sup> <em>× d</em>), where <em>N</em> is the length of the sequence and <em>d</em> is the dimensionality of the embedding of each element in the sequence.</p>&#13;
<p class="numberp">This is because self-attention involves computing a similarity score between each pair of elements in the sequence. For example, we have an input matrix <em>X</em> with <em>N</em> tokens (rows) where each token is a <em>d</em>-dimensional embedding (columns).</p>&#13;
<p class="numberp">When we compute the dot product of each token embedding to each other token embedding, we multiply <em>XX<sup>T</sup></em>, which results in an <em>N×N</em> similarity matrix. This multiplication involves <em>d</em> multiplications for a single token pair, and we have <em>N</em><sup>2</sup> such pairs. Hence, we have <em>O</em>(<em>N</em><sup>2</sup><em>×d</em>) complexity. The <em>N×N</em> similarity matrix is then used to compute weighted averages of the sequence elements, resulting in an <em>N×d</em> output representation. This can make self-attention computationally expensive and memory intensive, particularly for long sequences or large values of <em>d</em>.</p>&#13;
<p class="number1"><strong>16-2.</strong> Yes. Interestingly, self-attention may partly be inspired by the spatial attention mechanisms used in convolutional neural networks for image processing (Kelvin Xu et al., “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention” [2015], <em><a href="https://arxiv.org/abs/1502.03044">https://arxiv.org/abs/1502.03044</a></em>). Spatial attention is a mechanism that allows a neural network to focus on specific regions of an image that are relevant to a given task. It works by selectively weighting the importance of different spatial locations in the image, which allows the network to “pay more attention” to certain areas and ignore others.</p>&#13;
<h3 class="h3" id="ch00lev165"><strong>Chapter 17</strong></h3>&#13;
<p class="number1"><strong>17-1.</strong> To adapt a pretrained BERT model for classification, you need to add an output layer for classification, often referred to as a <em>classification head</em>.</p>&#13;
<p class="numberp">As discussed, BERT uses a [CLS] token for the next-sentence prediction task during pretraining. Instead of training it for next-sentence prediction, we can fine-tune a new output layer for our target prediction task, such as sentiment classification.</p>&#13;
<p class="numberp">The embedded [CLS] output vector serves as a summary of the entire input sequence. We can think of it as a feature vector and train a small neural network on top of it, typically a fully connected layer followed by a softmax activation function to predict the class probabilities. The fully connected layer’s output size should match the number <span epub:type="pagebreak" id="page_216"/>of classes in our classification task. Then we can train it using back-propagation as usual. Different fine-tuning strategies (updating all layers versus only the last layer) can then be used to train the model on a supervised dataset, for example.</p>&#13;
<p class="number1"><strong>17-2.</strong> Yes, we can fine-tune a decoder-only model like GPT for classification tasks, although it might not be as effective as using encoder-based models like BERT. In contrast to BERT, we do not need to use a special [CLS], but the fundamental concept is similar to fine-tuning an encoder-style model for classification. We add a classification head (a fully connected layer and a softmax activation) and train it on the embedding (the final hidden state) of the first generated output token. (This is analogous to using the [CLS] token embedding.)</p>&#13;
<h3 class="h3" id="ch00lev166"><strong>Chapter 18</strong></h3>&#13;
<p class="number1"><strong>18-1.</strong> In-context learning is useful if we don’t have access to the model or if we want to adapt the model to similar tasks that the model wasn’t trained to do.</p>&#13;
<p class="numberp">In contrast, fine-tuning is useful for adapting the model to a new target domain. For example, suppose the model was pretrained on a general corpus and we want to apply it to financial data or documents. Here, it would make sense to fine-tune the model on data from that target domain.</p>&#13;
<p class="numberp">Note that in-context learning can be used with a fine-tuned model as well. For example, when a pretrained language model is fine-tuned on a specific task or domain, in-context learning then leverages the model’s ability to generate responses based on the context provided within the input that may be more accurate given the target domain compared to in-context learning without fine-tuning.</p>&#13;
<p class="number1"><strong>18-2.</strong> This is done implicitly. In prefix tuning, adapters, and LoRA, the original knowledge of the pretrained language model is preserved by keeping the core model parameters frozen while introducing additional learnable parameters that adapt to the new task.</p>&#13;
<h3 class="h3" id="ch00lev167"><strong>Chapter 19</strong></h3>&#13;
<p class="number1"><strong>19-1.</strong> If we were using an embedding technique like Word2Vec that processes each word independently, we would expect the cosine similarity between the “cat” embeddings to be 1.0. However, we are using a transformer model to produce the embeddings in this case. Transformers use self-attention mechanisms that consider the whole context (for instance, input text) when producing the embedding vectors. (See <a href="ch16.xhtml">Chapter 16</a> for more information about self-attention.) Since the word <em>cat</em> is used in two different sentences, the BERT model produces a different embedding for these two instances of <em>cat</em>.”</p>&#13;
<p class="number1"><span epub:type="pagebreak" id="page_217"/><strong>19-2.</strong> Switching the candidate and reference texts has the same effect as calculating the maximum cosine similarity scores across columns (as shown in step 5 of <a href="ch19.xhtml#ch19fig3">Figure 19-3</a>) versus rows, which can result in different BERT-Scores for specific texts. That’s why the BERTScore is often computed as an F1 score similar to ROUGE in practice. For instance, we calculate the BERTScore one way (recall), then the other (precision), and then compute the harmonic mean (F1 score).</p>&#13;
<h3 class="h3" id="ch00lev168"><strong>Chapter 20</strong></h3>&#13;
<p class="number1"><strong>20-1.</strong> Random forests, typically based on CART decision trees, cannot be readily updated as new data arrives. Hence, a stateless training approach would be the only viable option. On the other hand, suppose we switched to using neural network models such as recurrent neural networks. In that case, a stateful approach could make more sense since the neural network could be readily updated on new data. (However, in the beginning, comparing stateful and stateless systems side by side is always a good idea before deciding which method works best.)</p>&#13;
<p class="number1"><strong>20-2.</strong> A stateful retraining approach makes the most sense here. Instead of training a new model on a combination of existing data, including user feedback, it makes more sense to update the model based on user feedback. Large language models are usually pretrained in a self-supervised fashion and then fine-tuned via supervised learning. Training large language models is very expensive, so updating the model via stateful retraining makes sense rather than training it from scratch again.</p>&#13;
<h3 class="h3" id="ch00lev169"><strong>Chapter 21</strong></h3>&#13;
<p class="number1"><strong>21-1.</strong> From the information provided, it is unclear whether this is a data-centric approach. The AI system relies heavily on data inputs to make predictions and recommendations, but that’s true for any machine learning approach for AI. To determine whether this approach is an example of data-centric AI, we need to know how the AI system was developed. If it was developed by using a fixed model and refining the training data, this could qualify as a data-centric approach; otherwise, it’s just regular machine learning and predictive modeling.</p>&#13;
<p class="number1"><strong>21-2.</strong> If we are keeping the model fixed—that is, reusing the same ResNet-34 architecture—and are changing only the data augmentation approach to investigate its influence on the model performance, we could consider this a data-centric approach. However, data augmentation is also routinely done as part of any modern machine learning pipeline, and the use of data augmentation alone does not tell us whether an approach is data centric. Under the modern definition, a data-centric approach entails actively studying the difference between various dataset-enhancing techniques while keeping the remaining modeling and training pipeline fixed.<span epub:type="pagebreak" id="page_218"/></p>&#13;
<h3 class="h3" id="ch00lev170"><strong>Chapter 22</strong></h3>&#13;
<p class="number1"><strong>22-1.</strong> One downside of using multi-GPU strategies for inference is the additional communication overhead between the GPUs. However, for inference tasks, which are relatively small compared to training since they don’t require gradient computations and updates, the time it takes to communicate between GPUs could outweigh the time saved by parallelization.</p>&#13;
<p class="numberp">Managing multiple GPUs also means higher equipment and energy costs. In practice, optimizing models for single-GPU or CPU performance is usually more worthwhile. If multiple GPUs are available, processing multiple samples in parallel on separate GPUs often makes more sense than processing the same sample via multiple GPUs.</p>&#13;
<p class="number1"><strong>22-2.</strong> Loop tiling is often combined with vectorization. For example, after applying loop tiling, each tile can be processed using vectorized operations. This allows us to use SIMD instructions on data that is already in the cache, increasing the effectiveness of both techniques.</p>&#13;
<h3 class="h3" id="ch00lev171"><strong>Chapter 23</strong></h3>&#13;
<p class="number1"><strong>23-1.</strong> The problem is that importance weighting assumes the test set distribution matches the deployment distribution. However, this is often not the case for various reasons, such as changing user behavior, evolving product features, or dynamic environments.</p>&#13;
<p class="number1"><strong>23-2.</strong> It’s common to monitor metrics such as classification accuracy, where a drop in performance may indicate a shift in the data. However, this is impractical if we don’t have access to the labels of incoming data.</p>&#13;
<p class="numberp">In cases where it’s infeasible to label new, incoming data, we can use statistical two-sample tests to determine whether the examples come from the same distribution. We can also use adversarial validation, discussed in <a href="ch29.xhtml">Chapter 29</a>. However, these methods won’t help detect concept shifts, as they compare only input distributions, not the relationship between inputs and outputs.</p>&#13;
<p class="numberp">Other methods include measuring the reconstruction error: if we have an autoencoder trained on our source data, we can monitor the reconstruction error on new data. If the error increases significantly, it may indicate a shift in the input distribution.</p>&#13;
<p class="numberp">Outlier detection is another common technique. Here, unusually high rates of data points being identified as outliers could suggest a shift in data distribution.</p>&#13;
<h3 class="h3" id="ch00lev172"><strong>Chapter 24</strong></h3>&#13;
<p class="number1"><strong>24-1.</strong> Trying to predict the number of goals a player scores (based on data from past seasons, for example) is a Poisson regression problem. On the other hand, we could also apply an ordinal regression model to the different players to rank them by the number of goals they will score. <span epub:type="pagebreak" id="page_219"/>However, since the goal difference is constant and can be quantified (for example, the difference between 3 and 4 goals is the same as the difference between 15 and 16 goals), it’s not an ideal problem for an ordinal regression model.</p>&#13;
<p class="number1"><strong>24-2.</strong> This is a ranking issue that resembles an ordinal regression issue, but there are some differences. Since we are aware of only the relative order of the movies, a pairwise ranking algorithm might be a more appropriate solution than an ordinal regression model.</p>&#13;
<p class="numberp">However, if the person is asked to assign numerical labels to each movie on a scale such as 1 to 5 (similar to the star rating system on Amazon), it would be possible to train and use an ordinal regression model on this type of data.</p>&#13;
<h3 class="h3" id="ch00lev173"><strong>Chapter 25</strong></h3>&#13;
<p class="number1"><strong>25-1.</strong> The choice of the confidence level (90 percent, 95 percent, 99 percent, and so forth) affects the width of the confidence interval. A higher confidence level will produce a wider interval because we need to cast a wider net to be more confident that we have captured the true parameter.</p>&#13;
<p class="numberp">Conversely, a lower confidence level produces a narrower interval, reflecting more uncertainty about where the true parameter lies. A 90 percent confidence interval is therefore narrower than a 95 percent confidence interval, reflecting greater uncertainty about the location of the true population parameter. Colloquially speaking, we are 90 percent certain that the true parameter lies within a small range of values. To increase this certainty, we must increase the width to 95 percent or 99 percent.</p>&#13;
<p class="numberp">For example, let’s say we are 90 percent certain that it will rain in the next two weeks in Wisconsin. If we want to make a prediction with 95 percent confidence without collecting additional data, we would have to increase the time interval. For example, we might say that we are 95 percent certain that it will rain within the next four weeks, or 99 percent certain that it will rain within the next two months.</p>&#13;
<p class="number1"><strong>25-2.</strong> Since the model is already trained and stays the same, applying it to each test set would be wasteful. To speed up the process outlined in this section, we technically need to apply the model only once, namely on the original test set. We can then bootstrap the actual and predicted labels directly (instead of the original samples) to create the bootstrapped test sets. We can then compute the test set accuracies based on the bootstrapped labels in each set.</p>&#13;
<h3 class="h3" id="ch00lev174"><strong>Chapter 26</strong></h3>&#13;
<p class="number1"><strong>26-1.</strong> The prediction set size can tell us a lot about the certainty of the prediction. If the prediction set size is small (for example, 1 in classification <span epub:type="pagebreak" id="page_220"/>tasks), it indicates a high level of confidence in the prediction. The algorithm has enough evidence to strongly suggest one specific outcome.</p>&#13;
<p class="numberp">If the prediction set size is larger (for example, 3 in classification tasks), it indicates more uncertainty. The model is less confident about the prediction and considers multiple outcomes to be plausible. In practice, we can use this information to assign more resources to examples with a high prediction set size. For example, we may flag these cases for human verification since the machine learning model is less certain.</p>&#13;
<p class="number1"><strong>26-2.</strong> Absolutely. Confidence intervals are just as applicable to regression models as they are to classification models. In fact, they’re even more versatile in the context of regression. For example, we can compute confidence intervals for the performance of a model, like the mean squared error, using the methods illustrated in <a href="ch25.xhtml">Chapter 25</a>. (But we can also compute confidence intervals for individual predictions and model parameters. If you’re interested in confidence intervals for model parameters, check out my article “Interpretable Machine Learning—Book Review and Thoughts About Linear and Logistic Regression as Interpretable Models” at <em><a href="https://sebastianraschka.com/blog/2020/interpretable-ml-1.html">https://sebastianraschka.com/blog/2020/interpretable-ml-1.html</a></em>.)</p>&#13;
<p class="numberp">We can also compute conformal prediction intervals for regression models. The interval is a range of possible target values instead of a single point estimate. The interpretation of such a prediction interval is that, under the assumption that the future is statistically similar to the past (for instance, based on the data the model was trained on), the true target value for a new instance will fall within this range with a certain confidence level, such as 95 percent.</p>&#13;
<h3 class="h3" id="ch00lev175"><strong>Chapter 27</strong></h3>&#13;
<p class="number1"><strong>27-1.</strong> Since the MAE is based on an absolute value around the distance, it naturally satisfies the first criterion: it can’t be negative. Also, the MAE is the same if we swap the values <em>y</em> and <em>ŷ</em>; hence, it satisfies the second criterion. But how about the triangle inequality? Similar to how the RMSE is the same as the Euclidean distance or L2 norm, the MAE is similar to the L1 norm between two vectors. Since all vector norms satisfy the triangle inequality (Horn and Johnson, <em>Matrix Analysis</em>, Cambridge University Press, 1990), our colleague is incorrect.</p>&#13;
<p class="numberp">Furthermore, even if the MAE were not a proper metric, it could still be a useful model evaluation metric; for example, consider the classification accuracy.</p>&#13;
<p class="number1"><strong>27-2.</strong> The MAE assigns equal weight to all errors, while the RMSE places more emphasis on errors with larger absolute values due to the quadratic exponent. As a result, the RMSE is always at least as large as the MAE. However, no metric is universally better than the other, and they have both been used to assess model performance in countless studies over the years.</p>&#13;
<p class="numberp"><span epub:type="pagebreak" id="page_221"/>If you are interested in additional comparisons between MAE and RMSE, you may like the article by Cort J. Willmott and Kenji Matsuura, “Advantages of the Mean Absolute Error (MAE) Over the Root Mean Square Error (RMSE) in Assessing Average Model Performance” (2005), <em><a href="https://www.int-res.com/abstracts/cr/v30/n1/p79-82">https://www.int-res.com/abstracts/cr/v30/n1/p79-82</a></em>.</p>&#13;
<h3 class="h3" id="ch00lev176"><strong>Chapter 28</strong></h3>&#13;
<p class="number1"><strong>28-1.</strong> This is not a problem if we care about only the average performance. For example, if we have a dataset of 100 training examples, and the model predicts 70 out of the 100 validation folds correctly, we estimate the model accuracy as 70 percent. However, suppose we are interested in analyzing the variance of the estimates from the different folds. In that case, LOOCV is not very useful: since each fold consists of only a single training example, we cannot compute the variance of each fold and compare it to other folds.</p>&#13;
<p class="number1"><strong>28-2.</strong> Another use case of <em>k</em>-fold cross-validation is model ensembling. For example, in 5-fold cross-validation, we train five different models since we have five slightly different training sets. However, instead of training a final model on the whole training set, we can combine the five models into a model ensemble (this is particularly popular on Kaggle). See <a href="ch06.xhtml#ch6fig3">Figure 6-3</a> on <a href="ch06.xhtml#ch6fig3">page 34</a> for an illustration of this process.</p>&#13;
<h3 class="h3" id="ch00lev177"><strong>Chapter 29</strong></h3>&#13;
<p class="number1"><strong>29-1.</strong> As a performance baseline, it’s a good idea to implement a zero-rule classifier, such as a majority class classifier. Since we typically have more training data than test data, we can compute the performance of a model that always predicts <em>Is test? False</em>, which should result in 70 percent accuracy if we have partitioned the original dataset into 70 percent training data and 30 percent test data. If the accuracy of the model trained on the adversarial validation dataset noticeably exceeds this baseline (say, 80 percent), we may have a serious discrepancy issue to investigate further.</p>&#13;
<p class="number1"><strong>29-2.</strong> Overall, this is not a big issue since we are mainly interested in whether there is a strong deviation from a majority class prediction baseline. For instance, if we compare the accuracy of the adversarial validation model against the baseline (rather than 50 percent accuracy), there should be no issue. However, it may be even better to consider evaluation metrics like Matthew’s correlation coefficient or ROC or precision-recall area-under-the-curve values instead of classification accuracy.</p>&#13;
<h3 class="h3" id="ch00lev178"><strong>Chapter 30</strong></h3>&#13;
<p class="number1"><strong>30-1.</strong> While we often think of self-supervised learning and transfer learning as separate approaches, they don’t have to be exclusive. For instance, we <span epub:type="pagebreak" id="page_222"/>could pretrain a model on a labeled or larger unlabeled image dataset using self-supervised learning (in this case, the millions of unlabeled images corresponding to the various computing devices).</p>&#13;
<p class="numberp">Instead of starting with random weights, we can use the neural network weights from self-supervised learning to follow up with transfer learning via the thousands of labeled smartphone pictures. Since smart-phones are related to tablets, transfer learning is a very promising approach here.</p>&#13;
<p class="numberp">Finally, after the self-supervised pretraining and transfer learning, we can fine-tune the model on the hundreds of labeled images of the target task, the tablets.</p>&#13;
<p class="number1"><strong>30-2.</strong> Besides mitigation techniques for the overconfident scores from a neural network’s output layer, we can also consider various ways of ensembling to obtain confidence scores. For instance, instead of disabling dropout during inference, we can leverage dropout to obtain multiple different predictions for a single example to compute the predicted label uncertainty.</p>&#13;
<p class="numberp">Another option is to construct model ensembles from different segments of the training set using <em>k</em>-fold cross-validation, as discussed in the ensemble section of <a href="ch06.xhtml">Chapter 6</a>.</p>&#13;
<p class="numberp">It is also possible to apply conformal prediction methods, discussed in <a href="ch26.xhtml">Chapter 26</a>, to active learning.</p>&#13;
</div>
</div>
</body></html>