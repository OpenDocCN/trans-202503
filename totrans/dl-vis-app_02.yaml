- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: An Overview of Machine Learning
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习概述
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: 'This book is about *deep learning*, a subfield of *machine learning*. The phrase
    *machine learning*describes a growing body of techniques that share a common goal:
    extracting meaningful information from data. Here, *data* refers to anything that
    can be represented numerically. Data can be raw numbers (like stock prices on
    successive days, the masses of different planets, or the heights of people visiting
    a county fair), but it can also be sounds (the words someone speaks into their
    cell phone), pictures (photographs of flowers or cats), words (the text of a newspaper
    article or a novel), behavior (what activities someone enjoys), preferences (the
    music or films someone likes), or anything else that we can collect and describe
    with numbers.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本书讲述的是*深度学习*，它是*机器学习*的一个子领域。*机器学习*这个词描述了一个不断发展的技术体系，这些技术有一个共同的目标：从数据中提取有意义的信息。在这里，*数据*指的是任何可以用数字表示的东西。数据可以是原始数字（例如连续几天的股票价格、不同星球的质量，或是参加县展览的人们的身高），但也可以是声音（某人说话时手机中的声音）、图片（花朵或猫的照片）、文字（报纸文章或小说的文本）、行为（某人喜欢什么活动）、偏好（某人喜欢的音乐或电影）等任何我们能够收集并用数字描述的东西。
- en: Our goal is to discover meaningful information, where it’s up to us to decide
    what’s meaningful. We usually want to find patterns that help us understand the
    data or use past measurements to predict future events. For example, we might
    want to predict a movie someone would like based on movies they’ve already rated,
    read the handwriting on a note, or identify a song from just a few notes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是发现有意义的信息，至于什么是有意义的，由我们自己决定。我们通常希望找到能够帮助我们理解数据的模式，或者用过去的测量值预测未来的事件。例如，我们可能想根据某人已经评分的电影来预测他们会喜欢什么电影，或者识别一个便条上的手写字，或者仅凭几句音符就能辨认出一首歌。
- en: 'We generally find the information we’re after in three steps: we identify the
    information that we want to find, we collect data that we hope will hold that
    information, and then we design and run algorithms to extract as much of that
    information as possible from that data.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常通过三个步骤找到我们需要的信息：我们确定想要寻找的信息，收集我们希望包含这些信息的数据，然后设计并运行算法，从这些数据中提取尽可能多的信息。
- en: 'In this chapter, we’ll cover some of the major movements in machine learning.
    We’ll begin by discussing an early approach to machine learning called an expert
    system. We’ll then discuss three of the major approaches to learning: supervised
    learning, unsupervised learning, and reinforcement learning. We’ll end the chapter
    by looking at deep learning.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍机器学习中的一些主要发展。我们将首先讨论一种早期的机器学习方法——专家系统。接下来，我们将讨论三种主要的学习方法：监督学习、无监督学习和强化学习。最后，我们将讨论深度学习。
- en: Expert Systems
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专家系统
- en: Before deep learning became practical on a widespread basis, a popular approach
    to learning from data involved creating *expert systems*. Still used today, these
    are computer programs intended to encapsulate the thought processes of human experts
    such as doctors, engineers, and even musicians. The idea is to study a human expert
    at work, watch what they do and how they do it, and perhaps ask them to describe
    their process out loud. We capture that thinking and behavior with a set of rules.
    The hope is that a computer could then do the expert’s job just by following those
    rules.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习广泛应用之前，从数据中学习的一个流行方法是创建*专家系统*。这种方法至今仍在使用，它们是旨在封装人类专家（如医生、工程师甚至音乐家）思维过程的计算机程序。其思路是研究人类专家的工作，观察他们的行为和方式，并可能让他们描述自己的过程。我们通过一组规则来捕捉这种思维和行为。希望计算机能够仅仅通过遵循这些规则来完成专家的工作。
- en: These kinds of systems can work well once they’re built, but they’re difficult
    to create and maintain. It’s worth taking a moment to see why. The problem is
    that the key step of producing the rules, called *feature engineering*, can require
    impractical amounts of human intervention and ingenuity. Part of deep learning’s
    success is that it addresses exactly this problem by creating the rules algorithmically.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的系统一旦建立起来就能运行得很好，但它们的创建和维护非常困难。值得花点时间了解其中的原因。问题在于，产生规则的关键步骤，称为*特征工程*，可能需要不切实际的人工干预和创造力。深度学习成功的部分原因就是它通过算法方式创建规则，正好解决了这个问题。
- en: 'Let’s illustrate the problem faced by expert systems with a practical example:
    recognizing digits. Let’s say that we want to teach a computer to recognize the
    number 7\. By talking to people and asking questions, we might come up with a
    set of three small rules that let us distinguish a 7 from all other digits: first,
    7s have a mostly horizontal line near the top of the figure; second, they have
    a mostly northeast-southwest diagonal line; and third, those two lines meet in
    the upper right. The rules are illustrated in [Figure 1-1](#figure1-1).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个实际例子来说明专家系统面临的问题：识别数字。假设我们想教计算机识别数字7。通过与人们交谈并提问，我们可能会提出三条小规则，帮助我们将7与其他数字区分开：首先，7的顶部有一条大致水平的线；其次，它有一条大致的东北—西南方向的对角线；第三，这两条线在右上角相交。这些规则如[图
    1-1](#figure1-1)所示。
- en: This might work well enough until we get a 7 like [Figure 1-2](#figure1-2).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 直到我们遇到像[图 1-2](#figure1-2)中的7，之前的规则可能才会有些效果。
- en: '![f01001](Images/f01001.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![f01001](Images/f01001.png)'
- en: 'Figure 1-1: Top: A handwritten 7\. Bottom: A set of rules for distinguishing
    a handwritten 7 from other digits.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1-1：顶部：手写的7。底部：一组用于区分手写7与其他数字的规则。
- en: '![f01002](Images/f01002.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![f01002](Images/f01002.png)'
- en: 'Figure 1-2: A 7 that would not be recognized by the rules of [Figure 1-1](#figure1-1)
    because of the extra horizontal line'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1-2：由于多出的横线，规则无法识别为[图 1-1](#figure1-1)中的7。
- en: Our set of rules won’t recognize this as a 7, because we hadn’t originally considered
    that some people put a bar through the middle of the diagonal stroke. So now we
    need to add another rule for that special case. In practice, this kind of thing
    happens over and over again to anyone developing an expert system. In a problem
    of any complexity, finding a good and complete set of rules is frequently an overwhelmingly
    difficult task. Turning human expertise into a series of explicit instructions
    often means laboriously uncovering inferences and decisions that people make without
    even realizing it, turning those into huge numbers of instructions, then adjusting
    and hand-tuning those instructions to cover all of the situations that were initially
    overlooked, debugging the rules where they contradict, and so on, in a seemingly
    never-ending series of tasks performed on a massive, complicated set of rules.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的规则集无法将这个识别为7，因为我们最初没有考虑到有些人会在对角线的中间加上一条横线。所以现在我们需要为这种特殊情况添加一个新规则。在实际操作中，这种问题在任何开发专家系统的人身上都会反复发生。在任何复杂的问题中，找到一套好的完整规则常常是一个非常艰巨的任务。将人类的专业知识转化为一系列明确的指令，往往意味着要艰难地揭示人们在不自觉的情况下做出的推论和决策，将这些转化为大量的指令，然后调整和手动调优这些指令，以覆盖最初被忽视的所有情况，调试那些相互矛盾的规则，等等，这一系列任务似乎永无止境，并且处理的是一大堆复杂的规则。
- en: 'This process of finding the rules to accomplish a job is tough work: the rules
    human experts follow are often not explicit, and as we saw, it’s easy to overlook
    exceptions and special cases. Imagine trying to find a comprehensive set of rules
    that can mimic a radiologist’s thought process as they determine whether a smudge
    on an MRI image is benign or not, or the way an air-traffic controller handles
    heavily scheduled air traffic, or how someone drives a car safely in extreme weather
    conditions. To make things even more complex, the technology, laws, and social
    conventions around human activities are constantly changing, requiring us to constantly
    monitor, update, and repair this tangled web of interconnecting rules.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 找到完成任务的规则这一过程是艰苦的工作：人类专家遵循的规则往往不是明确的，正如我们所看到的，很容易忽视例外情况和特殊情形。试想要找到一套完整的规则，能够模仿放射科医生判断MRI图像上的污点是否良性的思维过程，或者空中交通管制员如何处理繁忙的航班调度，抑或一个人在极端天气条件下如何安全地驾驶汽车。更复杂的是，围绕人类活动的技术、法律和社会习惯不断变化，这要求我们不断地监控、更新并修复这些错综复杂的规则网络。
- en: Rule-based expert systems can be made to work in some cases, but the difficulties
    of crafting the right set of rules, making sure they work properly across a wide
    variety of data, and keeping them up to date, makes them impractical as a general
    solution.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的专家系统在某些情况下是可以工作的，但设计一套合适的规则，确保它们能在各种数据下正常工作，并保持更新，这些困难使得它们作为一种通用解决方案变得不切实际。
- en: If we could only find and manage this set of rules, then computers could indeed
    emulate some forms of human decision making. This is just what deep learning is
    all about. These algorithms, given enough training data, can discover the decision-making
    rules *automatically.* We don’t have to explicitly tell the algorithm how to recognize
    a 2 or a 7, because the system figures that out for itself. It can work out whether
    an MRI smudge is benign or not, whether a cellphone’s photo has been ideally exposed,
    or whether a piece of text was really written by some historical figure. These
    are all among the many applications deep learning is already carrying out for
    us.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能找到并管理这一系列规则，那么计算机就可以模仿某些形式的人类决策过程。这正是深度学习的核心所在。这些算法，在有足够的训练数据的情况下，可以*自动*发现决策规则。我们不需要明确告诉算法如何识别数字2或7，因为系统自己会弄明白。它可以判断一个MRI的模糊区域是否是良性的，判断手机照片是否曝光适当，或者判断一段文字是否真的是某个历史人物所写。这些都是深度学习已经在为我们执行的许多应用之一。
- en: The computer discovers the decision-making rules by examining the input data
    and extracting patterns. The system never “understands” what it’s doing, as a
    person does. It has no common sense, awareness, or comprehension. It just measures
    patterns in the training data and then uses those patterns to evaluate new data,
    producing a decision or result based on the examples it was trained on.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机通过检查输入数据并提取模式来发现决策规则。系统从不“理解”它所做的事情，像人类那样。它没有常识、意识或理解力。它只是衡量训练数据中的模式，然后使用这些模式来评估新数据，基于它所训练的示例做出决策或结果。
- en: Generally speaking, we train deep learning algorithms in one of three different
    ways, depending on the data we have and what we want the computer to produce for
    us. Let’s survey them briefly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，根据我们拥有的数据和希望计算机为我们提供的输出，我们会以三种不同的方式训练深度学习算法。让我们简要地了解一下这些方法。
- en: Supervised Learning
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'We’ll first consider *supervised learning*. Here, the word *supervised* is
    a synonym for “labeled.” In supervised learning, we typically give the computer
    pairs of values: an item drawn from a dataset, and a label that we’ve assigned
    to that item.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先考虑*监督学习*。这里，*supervised*一词是“已标注”的同义词。在监督学习中，我们通常会给计算机提供一对对的值：一个来自数据集的项，以及我们为该项分配的标签。
- en: For example, we might be training a system called an *image classifier*, with
    the goal of having it tell us what object is most prominent in a photograph. To
    train this system, we’d give it a collection of images, and accompany each image
    with a label describing the most prominent object. So, for example, we might give
    the computer a picture of a tiger and a label consisting of the word *tiger*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能正在训练一个叫做*图像分类器*的系统，目标是让它告诉我们照片中最突出的是哪个物体。为了训练这个系统，我们会给它一组图像，并为每个图像附上一个标签，描述最显著的物体。所以，举个例子，我们可能会给计算机一张老虎的图片，并附上标签*老虎*。
- en: This idea can be extended to any kind of input. Suppose that we have a few cookbooks
    full of recipes that we’ve tried out, and we’ve kept records on how much we liked
    each dish. In this case, the recipe would be the input, and our rating of it would
    be that recipe’s label. After training a program on all of our cookbooks, we could
    give our trained system a new recipe, and it could predict how much we’d enjoy
    eating the result. Generally speaking, the better we’re able to train the system
    (usually by providing more pieces of training data), the better its prediction
    will be.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这一理念可以扩展到任何类型的输入。假设我们有一些做过的菜谱，记录了我们每道菜的喜好程度。在这种情况下，菜谱就是输入，而我们对它的评分则是该菜谱的标签。在对所有这些菜谱进行训练后，我们可以给我们的训练系统一个新的菜谱，它就能预测我们会多喜欢吃这个菜。一般来说，我们训练系统的能力越强（通常是通过提供更多的训练数据），它的预测结果就会越准确。
- en: Regardless of the type of data, by giving the computer an enormous number of
    pairs of inputs and labels, a successful system designed for the task will gradually
    discover enough rules or patterns from the inputs that it will be able to correctly
    *predict* each provided label. That is, as a result of this training, the system
    has learnedwhat to measure in each input so that it can identify which of its
    learned labels it should return. When it gets the right answer frequently enough
    for our needs, we say that the system has been *trained*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 无论数据类型如何，通过给计算机提供大量的输入和标签对，针对任务设计的成功系统将逐渐从输入中发现足够的规则或模式，进而能够正确地*预测*每个提供的标签。也就是说，通过这种训练，系统学会了在每个输入中应该测量什么，以便能够识别它应该返回的学习标签。当它频繁地得到正确答案以满足我们的需求时，我们称系统已经被*训练*过。
- en: Keep in mind that the computer has no sense of what a recipe actually is, or
    how things taste. It’s just using the data in the input to find the closest matching
    label, using the rules it learned during training.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，计算机并不理解食谱是什么，或者食物的味道如何。它仅仅是利用输入中的数据，按照它在训练过程中学到的规则，找到最接近的匹配标签。
- en: '[Figure 1-3](#figure1-3) shows the results of giving four photographs to a
    trained image classifier.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-3](#figure1-3)展示了将四张照片输入到训练过的图像分类器后的结果。'
- en: These photos were found on the web, and the system had never seen them before.
    In response to each image, the classifier tells us the likelihood for each of
    the 1,000 labels it was trained to recognize. Here we show the top five predictions
    for each photo, with their associated probabilities.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些照片是从网络上找到的，系统以前从未见过它们。针对每张图片，分类器会告诉我们它训练时识别的1,000个标签中的每一个的可能性。这里我们展示了每张照片的前五个预测标签及其相关的概率。
- en: The picture in the upper left of [Figure 1-3](#figure1-3) is a bunch of bananas,
    so ideally we’d like to get back a label like bunch of bananas. But this particular
    classifier wasn’t trained on any images labeled bunch of bananas. The algorithm
    can only return one of the labels it was trained on, in the same way that we can
    only identify objects by the words we know. The closest match it could find from
    the labels it was trained on was just banana, so that’s the label it returned
    to us.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-3](#figure1-3)左上角的图片是一串香蕉，因此理想情况下，我们希望返回一个像“香蕉串”这样的标签。但这个特定的分类器没有针对标注为“香蕉串”的任何图像进行训练。该算法只能返回它训练时使用的标签之一，就像我们只能用我们知道的词汇来识别物体一样。它从训练过的标签中找到的最接近的匹配是“香蕉”，所以它返回了这个标签。'
- en: '![f01003](Images/f01003.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![f01003](Images/f01003.png)'
- en: 'Figure 1-3: Four images and their predicted labels, with probabilities, from
    a deep learning classifier'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图1-3：四张图像及其预测标签和概率，来自深度学习分类器
- en: In the upper left, the computer has very high confidence in the label of banana.
    In the lower right, the computer is about 60 percent confident that the proper
    label is ear. but it’s about 40 percent confident it could be corn. If we follow
    a common practice and return just one label per image to the user, it would be
    helpful to also return the computer’s confidence that the label is correct. If
    the confidence isn’t reassuring, such as only about 60 percent for ear, we might
    decide to try again with a different algorithm, or perhaps even ask a human for
    help.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在左上角，计算机对香蕉标签的信心非常高。在右下角，计算机对耳朵标签的信心大约是60%。但它对玉米的信心大约是40%。如果我们遵循常见的做法，每张图像只返回一个标签给用户，那么同时返回计算机对标签是否正确的信心值也会很有帮助。如果信心值不令人放心，比如耳朵的信心只有大约60%，我们可能决定使用不同的算法重新尝试，或者甚至请求人工帮助。
- en: Unsupervised Learning
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: When we don’t have labels associated with our data, we use techniques that are
    known collectively as *unsupervised learning*. These algorithms learn about relationships
    between the elements of the input, rather than between each input and a label.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的数据没有标签时，我们使用一种被统称为*无监督学习*的技术。这些算法学习的是输入元素之间的关系，而不是每个输入与标签之间的关系。
- en: Unsupervised learning is frequently used for *clustering*, or grouping, pieces
    of data that we think are related. For example, suppose that we’re digging out
    the foundation for a new house, and while excavating, we find the ground is filled
    with old clay pots and vases. We call an archaeologist friend who realizes that
    we’ve found a jumbled collection of ancient pottery, apparently from many different
    places and perhaps even different times.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: The archaeologist doesn’t recognize any of the markings and decorations, so
    she can’t say for sure where each one came from. Some of the marks look like variations
    on the same theme, but other marks look like different symbols. To get a handle
    on the problem, she takes rubbings of the markings and then tries to sort them
    into groups. But there are far too many for her to sort through, and since all
    of her graduate students are working on other projects, she turns to a machine
    learning algorithm to automatically group the markings together in a sensible
    way.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-4](#figure1-4) shows her captured marks and the groupings an algorithm
    might produce.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![f01004](Images/f01004.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-4: Using a clustering algorithm to organize marks on clay pots. Left:
    The markings from the pots. Right: The marks grouped into similar clusters.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Because this technique arranges our data into related groups (or *clusters*),
    we call the process *clustering*, and we refer to this algorithm as a *clustering
    algorithm*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning algorithms can also be used to improve the quality of
    measured data (for example, removing speckles in a photo taken with a cellphone
    camera) or compress datasets so they take up less room on our disks, without losing
    any qualities we care about (such as what MP3 and JPG encoders do for sound and
    images).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes we want to train a computer to learn how to perform a task, but we
    don’t even know the best way to do it ourselves. Maybe we’re playing a complex
    game or writing some music. What’s the next best move to take or the next best
    note to choose? Often there isn’t a single best answer. But we might be able to
    roughly say that one answer is better than another. It would be great to be able
    to train a computer to find the best steps toward a good result by letting it
    try out possible approaches, which we only need to rank in a very general manner,
    such as “probably good,” or “better than the last one.”
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we’re responsible for designing the operation of the elevators
    in a new office building, as in [Figure 1-5](#figure1-5). Our job is to decide
    where elevator cars should wait when they’re not needed, and which car should
    answer a request when someone pushes a call button. Let’s say that our goal is
    to minimize the average wait time for all riders.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![f01005](Images/f01005.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-5: We want to find the best schedule for these elevators.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: How should we do that? The quality of any solution we might imagine will depend
    entirely on the patterns of when and where people want to travel. Maybe in the
    morning everyone’s arriving for work, so we should always bring empty cars to
    the first floor where they’ll be ready for new arrivals. But perhaps at lunch
    everyone wants to go outside, so we should keep idle cars near the top floors,
    always ready to come down and take people to the ground floor. But if it’s raining,
    perhaps most people will instead want to go to the cafeteria on the top floor.
    Day by day, hour by hour, what’s the best policy?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: There probably isn’t any single best policy, so the computer can’t learn to
    give it to us. All we can do is try out different approaches over time, and choose
    the one that seems to be giving us the best results. So we’ll have the computer
    invent a policy, or maybe make a variation on an existing one, and see how well
    it performs. We’ll then give it a score based on the average wait time of our
    riders. After trying out lots of variations, we can pick the policy with the best
    score. And then, over time, as patterns change, we can try out new approaches,
    always searching, but keeping the schedule with the best score.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: This is an example of *reinforcement learning (RL)*. The technique of RL is
    at the heart of the recent game-playing algorithms that have been beating human
    masters at games like Go, and even online strategy games like *StarCraft*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The phrase *deep learning* refers to machine learning algorithms that use a
    series of steps, or *layers,* of computation (Bishop 2006; Goodfellow, Bengio,
    and Courville 2017). We can draw these layers on the page in any way we like as
    long the structure is clear. If we draw the layers vertically, we can imagine
    looking up from the bottom and saying that the system is tall, or, looking down
    from the top and calling it deep. If we draw many layers horizontally, we might
    say that the system is wide. For no particular reason, the “deep” language is
    what caught on, lending its name to the whole field of deep learning.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to keep in mind that these systems are called “deep” only because
    of their appearance when we draw them stacked up vertically. They’re not deep
    in the sense of having profound understanding or penetrating insights. When a
    deep learning system attaches a name to a face in a photo, it has no knowledge
    of what faces are, or what people are, or even that people exist. The computer
    just measures pixels and, using the patterns it learned from the training data,
    produces the most likely label.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Let’s jump many chapters ahead and take a quick look at a deep network, pictured
    in [Figure 1-6](#figure1-6). In this simple network, we start with four input
    numbers, shown at the bottom of the figure. These might be the values of the four
    pixels in a 2 by 2 grayscale image, the closing price of a stock over four sequential
    days, or four samples from a snippet of voice data. Each input value is just a
    floating-point number, such as –2.982 or 3.1142\.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们跳过许多章节，快速看一下一个深度网络，如[图1-6](#figure1-6)所示。在这个简单的网络中，我们从四个输入数字开始，图底部显示了这些数字。这些数字可能是一个2x2灰度图像中四个像素的值，四个连续天数的股票收盘价，或者是某段语音数据的四个样本。每个输入值只是一个浮动的数字，如-2.982或3.1142\。
- en: '![f01006](Images/f01006.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![f01006](Images/f01006.png)'
- en: 'Figure 1-6: A simple deep neural network'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图1-6：一个简单的深度神经网络
- en: Those four values go upward in the diagram into a *layer*, or grouping, of three
    *artificial neurons*. Although they have the word *neuron* in their name and were
    distantly inspired by real neurons, these artificial neurons are extremely simple.
    We’ll see them in detail in Chapter 13, but it’s best to think of them as units
    that perform a tiny calculation and are not remotely as complex as real neurons.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个值在图中向上进入一个*层*，或者叫做一组三个*人工神经元*。尽管它们的名字中有“神经元”这个词，且灵感源自于真实的神经元，但这些人工神经元极其简单。我们将在第13章详细介绍它们，但最好把它们看作是执行小型计算的单元，远远不如真实神经元复杂。
- en: In this diagram, each neuron on layer 1 receives each of the four starting numbers
    as input. Note that each of the 12 lines that take an input value into a neuron
    has a little dot on it. In this figure, each dot represents the idea that the
    input value traveling on that line is multiplied by another number, called the
    *weight,* before it reaches the neuron. These weights are vital to the network,
    and we’ll return to them in a moment.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图示中，第一层的每个神经元接收四个起始数字中的每一个作为输入。请注意，图中每一条将输入值传入神经元的12条线都有一个小点。在这个图中，每个点代表了这样一个概念：沿着这条线传递的输入值在到达神经元之前，会被乘以另一个数字，称为*权重*。这些权重对于网络至关重要，我们稍后会详细讨论它们。
- en: The output of each artificial neuron in layer 1 is a new number. In [Figure
    1-6](#figure1-6), each of those outputs is fed into each of the neurons on layer
    2, and again each value is multiplied by a weight along the way. Finally, the
    values produced by the two neurons on layer 2 are the output of the network. We
    might interpret these output values to be the chance that the input is in each
    of two classes, or the first and last name of the person who spoke that sound
    fragment, or the predicted price of the stock on each of the next two days.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层中每个人工神经元的输出是一个新的数字。在[图1-6](#figure1-6)中，这些输出被输入到第二层的每个神经元中，同样，每个值在传递过程中都会被乘以一个权重。最后，第二层的两个神经元产生的值就是网络的输出。我们可能将这些输出值解释为输入属于两个类别中的每一个的概率，或者说这段语音片段是由哪个人说的，或者是未来两天的股票预测价格。
- en: 'Each of the big circles representing an artificial neuron turns its input values
    into a number. These computations are fixed: once we set up the network, each
    of these neurons will always compute the same output for a given input. So, once
    we’ve chosen the artificial neurons and arranged them into the network of [Figure
    1-6](#figure1-6), almost everything is specified.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 每个表示人工神经元的大圆圈将其输入值转换为一个数字。这些计算是固定的：一旦我们设置好网络，每个神经元在给定输入下总是会计算出相同的输出。所以，一旦我们选择了人工神经元并将它们安排成[图1-6](#figure1-6)中的网络，几乎所有东西都已经被指定好了。
- en: The only things in [Figure 1-6](#figure1-6) that can change are the inputs and
    the weights. That’s the key insight that makes it possible to train the network.
    The weights start out as random numbers. That means the output of the network
    will initially be nonsense, and we’ll never get back the results we want (unless
    we happen to occasionally get lucky).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图1-6](#figure1-6)中，唯一能改变的东西是输入和权重。这是使得网络能够训练的关键洞察。权重一开始是随机数字。这意味着网络的输出最初会是无意义的，我们永远不会得到我们想要的结果（除非我们偶尔运气好）。
- en: To get the network to reliably produce the answers we want, we carefully change
    the weights, just a little at a time, after each mistaken output, to make the
    network more likely to produce the desired values at the output. If we do this
    carefully, then over time the output values will gradually get closer and closer
    to the results we want. Eventually, if we’ve done our job well, the network will
    produce the right answer in response to almost all the data in our training database,
    and we can release our network to the web, or offer it as a product or service.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让网络可靠地生成我们想要的答案，我们在每次错误输出后，都会小心地调整权重，逐步改变，以使网络更有可能生成期望的输出值。如果我们这样小心操作，随着时间的推移，输出值将逐渐接近我们想要的结果。最终，如果我们完成得好，网络将能够对我们训练数据库中的几乎所有数据产生正确的答案，然后我们可以将网络发布到网络上，或将其作为产品或服务提供。
- en: In short, training this network, or teaching it, is nothing more than finding
    values for the weights so that every input produces the desired output. Amazingly,
    this is all there is to it! Even when our networks grow to hundreds of layers
    of many varieties, and tens of thousands of artificial neurons, and millions of
    weights, learning usually means just gradually changing the weights until we get
    the answers we want. More sophisticated networks might learn some other values
    as well, but the weights are always important.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，训练这个网络，或者说是教它，实际上就是为权重找到值，使得每个输入都能产生期望的输出。令人惊讶的是，这就是全部！即使我们的网络发展到拥有数百层、许多种类、成千上万个人工神经元和数百万个权重，学习通常也只是逐渐改变权重，直到我们得到想要的答案。更复杂的网络可能还会学习其他一些值，但权重始终是至关重要的。
- en: One of the beautiful things about this process is that it makes good on the
    promise of feature engineering. For example, consider a system that takes a photo
    as input and tells us what breed of dog is in the picture. When the training process
    for this system has finished and the weights have settled into their best values,
    they have the effect of turning the neurons into little feature detectors. For
    example, one of the neurons on an early layer might produce a large value if it
    “sees” an eye, and another if it “sees” a floppy ear (we’ll see just how this
    is done in Chapter 16). Then later neurons might look for combinations of these,
    such as a bushy tail along with short legs, or dark eyes with a long nose and
    large body, to help it determine the breed. In short, the neurons are looking
    for features, though we never explicitly guided them to. Feature detection is
    just a natural result of training the weights to produce the correct answers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的一个美妙之处在于，它实现了特征工程的承诺。例如，考虑一个接受照片作为输入并告诉我们图片中是什么犬种的系统。当该系统的训练过程完成且权重稳定在最佳值时，这些权重的效果是将神经元转变为小的特征检测器。例如，早期层的一个神经元可能会在“看到”眼睛时产生一个较大的值，另一个则是在“看到”下垂的耳朵时产生一个值（我们将在第16章中看到具体是如何实现的）。然后，后续的神经元可能会寻找这些特征的组合，比如蓬松的尾巴加上短腿，或者黑色的眼睛加上长鼻子和大身体，以帮助判断犬种。简而言之，神经元在寻找特征，尽管我们从未明确引导它们这样做。特征检测只是训练权重以产生正确答案的自然结果。
- en: So although manually building an expert system that acts like a radiologist
    is a near-impossible task, creating a complex deep network and training it successfully
    can implement that agenda automatically. The system finds its own ways of combining
    the values of the pixels in each image into features and then using those features
    to make a determination about whether that image shows healthy tissue or not (Saba
    et al. 2019).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，虽然手动构建一个像放射科医生一样的专家系统几乎是不可能的任务，但创建一个复杂的深度网络并成功训练它，可以自动实现这个目标。系统会找到将每张图片的像素值组合成特征的方式，然后使用这些特征来判断该图片是否显示了健康的组织（Saba等，2019）。
- en: Summary
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we got a general sense of deep learning. We began with expert
    systems, which required too much manual work to be successful in practice. We
    saw that training a deep learning system usually follows one of three approaches.
    Supervised learning means that we provide a label with every piece of data so
    that we can train the system to predict the correct label for new data. Unsupervised
    learning means that we give the system just the data, without labels, so we train
    the system to cluster the data into similar groups. And reinforcement learning
    means that we score various proposals put forth by the computer in the hope that
    it will eventually come up with an acceptably good solution.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at a real, but tiny, deep learning system. The basic structure
    organized artificial neurons into layers. Neurons on each layer communicate with
    the neurons on the previous and following layer. It’s the shape of this structure
    when we draw it in this form (like a tall tower of layers) that gives deep learning
    its name.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we saw the importance of the weights, or the values that multiply every
    number before it arrives at the input of an artificial neuron. When we teach a
    system, or we say it’s learning, all we’re usually doing is adjusting these weights.
    When the weights have found sufficiently good values, the system is able to do
    the job we’ve asked of it.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: The next few chapters will dig into the background that we’ll need to design
    and build deep learning systems.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
