<html><head></head><body>
<h2 class="h2" id="ch05"><span epub:type="pagebreak" id="page_137"/><strong><span class="big">5</span><br/>SWARM OPTIMIZATION</strong></h2>
<div class="image1"><img alt="Image" src="../images/common.jpg"/></div>
<p class="noindent">Swarm techniques do more than optimize mathematical functions. In this chapter, we’ll use randomness to pack circles in a square, place cell towers, enhance images, and organize product placement at the grocery store. We’ll apply the same collection of swarm intelligence and evolutionary algorithms as in the previous chapter.</p>
<h3 class="h3" id="ch00lev1_29"><strong>Packing Circles in a Square</strong></h3>
<p class="noindent">A classic math problem involves packing equal diameter circles in a square. An equivalent formulation is to locate, for a given number of points, positions in the unit square ([0, 1]) where the smallest distance between any pair of points is as large as possible. The point locations correspond to the centers of the best-packed circles. For example, where in the unit square do we put two points to maximize the distance between them? In opposite corners. In that case, the distance between the points is <img alt="Image" src="../images/f0137-01.jpg"/>, and it can’t be any larger.</p>
<p class="indent">What about three points? Four points? Seventeen points? Now the answer isn’t so obvious. We might approach this problem by using the elaborate algorithm detailed in Locatelli and Raber’s 2002 paper, “Packing Equal <span epub:type="pagebreak" id="page_138"/>Circles in a Square: A Deterministic Global Optimization Approach,” but that’s not how we’ll do it here. Instead, we’ll use randomness in the form of a swarm search. We need to map positions in some multidimensional space to candidate solutions and then search this space for the best possible solution.</p>
<p class="indent">If we have <em>n</em> points and want to know the coordinates of <em>n</em> circle centers that are, for each pair, as far apart as possible while still within [0, 1], we need to find <em>n</em> points. At first, we might believe we have an <em>n</em>-dimensional problem. However, the dimensionality is actually 2<em>n</em>: we need both the <em>x</em>- and <em>y</em>-coordinates to specify a point. We know the search’s bounds are [0, 1] for each dimension. Therefore, we’ll use swarms that are 2<em>n</em>-dimensional vectors bounded by [0, 1] where each pair of components is a point, (<em>x</em>, <em>y</em>). In other words, if we want to find five points, each particle is a 10-element vector:</p>
<p class="center"><em><strong>p</strong></em> = (<em>x</em><sub>0</sub>, <em>y</em><sub>0</sub>; <em>x</em><sub>1</sub>, <em>y</em><sub>1</sub>; <em>x</em><sub>2</sub>, <em>y</em><sub>2</sub>; <em>x</em><sub>3</sub>, <em>y</em><sub>3</sub>; <em>x</em><sub>4</sub>, <em>y</em><sub>4</sub>)</p>
<p class="indent">To run a search, we need the dimensionality of the problem and the bounds, both of which we now have. The only remaining issue is the objective function, which tells us how good a solution each particle position represents. The problem specification lights the way: we need to maximize the smallest distance between any pair of points. If there are five points, we calculate the distance between each possible pair, find which distance is the smallest, and return the opposite. Our framework only minimizes, so to maximize, we return the negative. The largest smallest distance between pairs, when made negative, becomes the most negative number.</p>
<h4 class="h4" id="ch00lev2_45"><em><strong>The Swarm Search</strong></em></h4>
<p class="noindent">The code we want is in <em>circles.py</em>. Consider putting the book down and reading through the file to understand the flow. Once you’ve done that, we can begin with the objective function class:</p>
<pre class="pre">class Objective:
    def __init__(self):
        self.fcount = 0

    def Evaluate(self, p):
        self.fcount += 1
        n = p.shape[0]//2
        xy = p.reshape((n,2))
        
        dmin = 10.0
        for i in range(n):
            for j in range(i+1,n):
                d = np.sqrt((xy[i,0]-xy[j,0])**2 + (xy[i,1]-xy[j,1])**2)
                if (d &lt; dmin):
                    dmin = d
        return -dmin</pre>
<p class="indent"><span epub:type="pagebreak" id="page_139"/>The constructor does nothing more than initialize <code>fcount</code>, which counts the number of times <code>Evaluate</code> is called. The <code>Evaluate</code> method is given a position vector (<code>p</code>) that is immediately reshaped into a set of (<em>x</em>, <em>y</em>) pairs (<code>xy</code>).</p>
<p class="indent">The second code paragraph in <code>Evaluate</code> runs through each pairing of points in <code>xy</code> and calculates the Euclidean distance between them. If that distance is the smallest found so far, we keep it in <code>dmin</code>. We want to maximize the smallest distance between any pair of points, so we first find the smallest distance between any two points represented by the particle position.</p>
<p class="indent">The final line returns the negative of <code>dmin</code>. Because the framework minimizes, returning the negative of the smallest pairwise distance forces the framework to <em>maximize</em> this smallest distance—exactly what we want.</p>
<p class="indent">We now have everything we need to implement the search. The main body of <em>circles.py</em> follows the standard approach of pulling values off the command line and setting up the framework objects before calling <code>Optimize</code> to execute the search.</p>
<p class="indent">In code, the essential steps are:</p>
<pre class="pre">rng = RE(kind=kind)
b = Bounds([0]*ndim, [1]*ndim, enforce="clip", rng=rng)
i = RandomInitializer(npart, ndim, bounds=b, rng=rng)
obj = Objective()
swarm = PSO(obj=obj, npart=npart, ndim=ndim, init=i, 
            bounds=b, max_iter=niter, bare=True, rng=rng)
swarm.Optimize()
res = swarm.Results()</pre>
<p class="indent">We create the desired randomness engine, followed by the bounds, initializer, and objective function instance. Notice that the objective function requires no ancillary information.</p>
<p class="indent">The <code>swarm</code> object, <code>PSO</code> configured for bare-bones searching, is followed by <code>Optimize</code> and <code>Results</code>. Not shown is code to report the best set of points and the distance between them before dumping all search results, including a simple plot of the point locations, into the supplied output directory.</p>
<p class="indent">Try running <em>circles.py</em> with no command line options to see what it expects. Now that we have it, let’s use it.</p>
<h4 class="h4" id="ch00lev2_46"><em><strong>The Code</strong></em></h4>
<p class="noindent">Let’s pack some circles. I created two shell scripts, <em>go_circle_results</em> and <em>go_plots</em>. The former runs <em>circles.py</em> for 2 through 20 circles and 7 algorithms: bare-bones PSO, canonical PSO, DE, GWO, Jaya, RO, and GA. The output is stored in the <em>results</em> directory. I recommend starting it in the evening and coming back the next morning, as the framework is designed for clarity, not speed. Run it with:</p>
<pre class="pre">&gt; <span class="codestrong1">sh go_circle_results</span></pre>
<p class="noindent">When <em>go_circle_results</em> finishes, execute <em>go_plots</em> to produce a series of plots showing the circle configuration each algorithm located. My results from <span epub:type="pagebreak" id="page_140"/>this exercise are in <a href="ch05.xhtml#ch05tab01">Table 5-1</a>, though yours will be somewhat different due to the stochastic nature of the swarm searches.</p>
<p class="tabcap" id="ch05tab01"><strong>Table 5-1:</strong> Largest Center Distance, Known and as Found by Each Algorithm</p>
<table class="table-h">
<colgroup>
<col style="width:6%"/>
<col style="width:12%"/>
<col style="width:12%"/>
<col style="width:12%"/>
<col style="width:12%"/>
<col style="width:12%"/>
<col style="width:12%"/>
<col style="width:12%"/>
<col style="width:10%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><em><strong>n</strong></em></th>
<th class="tab_th"><strong>Known</strong></th>
<th class="tab_th"><strong>Bare</strong></th>
<th class="tab_th"><strong>DE</strong></th>
<th class="tab_th"><strong>PSO</strong></th>
<th class="tab_th"><strong>GWO</strong></th>
<th class="tab_th"><strong>Jaya</strong></th>
<th class="tab_th"><strong>RO</strong></th>
<th class="tab_th"><strong>GA</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1">2</td>
<td class="bg1">1.4142</td>
<td class="bg1">1.4142</td>
<td class="bg1">1.4142</td>
<td class="bg1">1.4142</td>
<td class="bg1">1.4142</td>
<td class="bg1">1.4142</td>
<td class="bg1">1.4142</td>
<td class="bg1">1.4134</td>
</tr>
<tr>
<td class="bg">3</td>
<td class="bg">1.0353</td>
<td class="bg">1.0353</td>
<td class="bg">1.0353</td>
<td class="bg">1.0353</td>
<td class="bg">1.0353</td>
<td class="bg">1.0353</td>
<td class="bg">1.0301</td>
<td class="bg">1.0264</td>
</tr>
<tr>
<td class="bg1">4</td>
<td class="bg1">1.0000</td>
<td class="bg1">1.0000</td>
<td class="bg1">1.0000</td>
<td class="bg1">1.0000</td>
<td class="bg1">1.0000</td>
<td class="bg1">1.0000</td>
<td class="bg1">0.9998</td>
<td class="bg1">0.9969</td>
</tr>
<tr>
<td class="bg">5</td>
<td class="bg">0.7071</td>
<td class="bg">0.7071</td>
<td class="bg">0.7070</td>
<td class="bg">0.6250</td>
<td class="bg">0.7025</td>
<td class="bg">0.7071</td>
<td class="bg">0.6796</td>
<td class="bg">0.6052</td>
</tr>
<tr>
<td class="bg1">6</td>
<td class="bg1">0.6009</td>
<td class="bg1">0.5951</td>
<td class="bg1">0.5953</td>
<td class="bg1">0.5995</td>
<td class="bg1">0.5988</td>
<td class="bg1">0.5858</td>
<td class="bg1">0.5723</td>
<td class="bg1">0.5884</td>
</tr>
<tr>
<td class="bg">7</td>
<td class="bg">0.5359</td>
<td class="bg">0.5359</td>
<td class="bg">0.5223</td>
<td class="bg">0.5000</td>
<td class="bg">0.5072</td>
<td class="bg">0.5176</td>
<td class="bg">0.5000</td>
<td class="bg">0.4843</td>
</tr>
<tr>
<td class="bg1">8</td>
<td class="bg1">0.5176</td>
<td class="bg1">0.5090</td>
<td class="bg1">0.5045</td>
<td class="bg1">0.5000</td>
<td class="bg1">0.5002</td>
<td class="bg1">0.4801</td>
<td class="bg1">0.4661</td>
<td class="bg1">0.4355</td>
</tr>
<tr>
<td class="bg">9</td>
<td class="bg">0.5000</td>
<td class="bg">0.5000</td>
<td class="bg">0.4202</td>
<td class="bg">0.5000</td>
<td class="bg">0.4798</td>
<td class="bg">0.5000</td>
<td class="bg">0.4421</td>
<td class="bg">0.4470</td>
</tr>
<tr>
<td class="bg1">10</td>
<td class="bg1">0.4213</td>
<td class="bg1">0.4147</td>
<td class="bg1">0.3697</td>
<td class="bg1">0.4195</td>
<td class="bg1">0.4187</td>
<td class="bg1">0.3517</td>
<td class="bg1">0.3788</td>
<td class="bg1">0.3819</td>
</tr>
<tr>
<td class="bg">11</td>
<td class="bg">0.3980</td>
<td class="bg">0.3978</td>
<td class="bg">0.3296</td>
<td class="bg">0.3694</td>
<td class="bg">0.3895</td>
<td class="bg">0.3918</td>
<td class="bg">0.3588</td>
<td class="bg">0.3787</td>
</tr>
<tr>
<td class="bg1">12</td>
<td class="bg1">0.3887</td>
<td class="bg1">0.3726</td>
<td class="bg1">0.2989</td>
<td class="bg1">0.3717</td>
<td class="bg1">0.3289</td>
<td class="bg1">0.3819</td>
<td class="bg1">0.3496</td>
<td class="bg1">0.3542</td>
</tr>
<tr>
<td class="bg">13</td>
<td class="bg">0.3660</td>
<td class="bg">0.3595</td>
<td class="bg">0.2752</td>
<td class="bg">0.3333</td>
<td class="bg">0.3277</td>
<td class="bg">0.2832</td>
<td class="bg">0.3212</td>
<td class="bg">0.3230</td>
</tr>
<tr>
<td class="bg1">14</td>
<td class="bg1">0.3451</td>
<td class="bg1">0.3354</td>
<td class="bg1">0.2537</td>
<td class="bg1">0.3333</td>
<td class="bg1">0.3116</td>
<td class="bg1">0.3435</td>
<td class="bg1">0.3037</td>
<td class="bg1">0.3204</td>
</tr>
<tr>
<td class="bg">15</td>
<td class="bg">0.3372</td>
<td class="bg">0.3256</td>
<td class="bg">0.2303</td>
<td class="bg">0.3333</td>
<td class="bg">0.3278</td>
<td class="bg">0.2437</td>
<td class="bg">0.2949</td>
<td class="bg">0.2995</td>
</tr>
<tr>
<td class="bg1">16</td>
<td class="bg1">0.3333</td>
<td class="bg1">0.2996</td>
<td class="bg1">0.2269</td>
<td class="bg1">0.2500</td>
<td class="bg1">0.3011</td>
<td class="bg1">0.2220</td>
<td class="bg1">0.2760</td>
<td class="bg1">0.2761</td>
</tr>
<tr>
<td class="bg">17</td>
<td class="bg">0.3060</td>
<td class="bg">0.2985</td>
<td class="bg">0.2062</td>
<td class="bg">0.2913</td>
<td class="bg">0.2952</td>
<td class="bg">0.1992</td>
<td class="bg">0.2658</td>
<td class="bg">0.2721</td>
</tr>
<tr>
<td class="bg1">18</td>
<td class="bg1">0.3005</td>
<td class="bg1">0.2782</td>
<td class="bg1">0.1927</td>
<td class="bg1">0.2808</td>
<td class="bg1">0.2703</td>
<td class="bg1">0.2126</td>
<td class="bg1">0.2516</td>
<td class="bg1">0.2493</td>
</tr>
<tr>
<td class="bg">19</td>
<td class="bg">0.2900</td>
<td class="bg">0.2697</td>
<td class="bg">0.1852</td>
<td class="bg">0.2500</td>
<td class="bg">0.1905</td>
<td class="bg">0.1731</td>
<td class="bg">0.2384</td>
<td class="bg">0.2559</td>
</tr>
<tr>
<td class="bg1">20</td>
<td class="bg1">0.2866</td>
<td class="bg1">0.2632</td>
<td class="bg1">0.1789</td>
<td class="bg1">0.2500</td>
<td class="bg1">0.2419</td>
<td class="bg1">0.1659</td>
<td class="bg1">0.2200</td>
<td class="bg1">0.2342</td>
</tr>
</tbody>
</table>
<p class="indent"><a href="ch05.xhtml#ch05tab01">Table 5-1</a> shows the best-known distance between the points and the distance found by the swarm searches, by algorithm. These numbers will be our gold standard.</p>
<p class="indent">For <em>n</em> &lt; 10, many of the distances are known from geometric arguments, as <a href="ch05.xhtml#ch05tab02">Table 5-2</a> shows.</p>
<p class="tabcap" id="ch05tab02"><strong>Table 5-2:</strong> Known Circle Center Distances</p>
<table class="table-h">
<colgroup>
<col style="width:40%"/>
<col style="width:60%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><em><strong>n</strong></em></th>
<th class="tab_th"><strong>Distance</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1">2</td>
<td class="bg1"><img alt="Image" class="inline" src="../images/f0140-01.jpg"/></td>
</tr>
<tr>
<td class="bg">3</td>
<td class="bg"><img alt="Image" class="inline" src="../images/f0140-02.jpg"/></td>
</tr>
<tr>
<td class="bg1">4</td>
<td class="bg1">1</td>
</tr>
<tr>
<td class="bg">5</td>
<td class="bg"><img alt="Image" class="inline" src="../images/f0140-03.jpg"/></td>
</tr>
<tr>
<td class="bg1">6</td>
<td class="bg1"><img alt="Image" class="inline" src="../images/f0140-04.jpg"/></td>
</tr>
<tr>
<td class="bg">7</td>
<td class="bg"><img alt="Image" class="inline" src="../images/f0140-05.jpg"/></td>
</tr>
<tr>
<td class="bg1">8</td>
<td class="bg1"><img alt="Image" class="inline" src="../images/f0140-06.jpg"/></td>
</tr>
<tr>
<td class="bg">9</td>
<td class="bg">0.5</td>
</tr>
<tr>
<td class="bg1">10</td>
<td class="bg1">0.42127954</td>
</tr>
</tbody>
</table>
<p class="noindent">The expressions come from Table D1 in <em>Unsolved Problems in Geometry</em> by Croft, Falconer, and Guy (Springer, 1991). The <em>plot_results.py</em> file, called by <em>go_plots</em>, uses this table to generate plots showing the packed circles. If the <span epub:type="pagebreak" id="page_141"/>packing is optimal, the circles barely touch. Otherwise, the circles are separated from each other or overlap.</p>
<p class="indent">Examining <a href="ch05.xhtml#ch05tab01">Table 5-1</a> reveals 2 through 4 circles to be straightforward; every algorithm located the best arrangement. For 5 circles, bare-bones PSO, DE, and Jaya converged on the solution. We won’t quibble about the one-ten-thousandth difference between the known distance and differential evolution’s solution.</p>
<p class="indent">The swarms begin to struggle after 5 circles. For 6 circles, no swarm nails the distance, at least to four decimals, but several come pretty close. <a href="ch05.xhtml#ch05fig01">Figure 5-1</a> shows the output plots for each algorithm.</p>
<div class="image"><img alt="Image" id="ch05fig01" src="../images/05fig01.jpg"/></div>
<p class="figcap"><em>Figure 5-1: Packing 6 circles. From top left to right: PSO, GWO, DE, bare-bones PSO, GA, RO, and Jaya.</em></p>
<p class="noindent">While solutions are unique regarding the distance between the circle centers, they aren’t in terms of rotations. The canonical PSO, DE, and GWO results are essentially the same, only rotated by 90 degrees in some cases.</p>
<p class="indent">View all graphs generated by <em>go_plots</em> by the number of circles. As <em>n</em> increases, the swarms struggle more and more, but there are nice <em>n</em> values, like <em>n</em> = 9, where the swarms are more likely to arrive at the highly symmetric solution. Since we’re packing a square, it makes sense that <em>n</em> values that are perfect squares—like 4, 9, and 16—lead to nicely aligned packings. However, only a few algorithms located the ideal <em>n</em> = 9 output, and none found the best <em>n</em> = 16 outcome.</p>
<p class="indent">Let’s move on to a more practical problem.</p>
<h3 class="h3" id="ch00lev1_30"><span epub:type="pagebreak" id="page_142"/><strong>Placing Cell Towers</strong></h3>
<p class="noindent">Placing a cell phone tower is not an academic exercise; real utility and cost are involved. In this section we’ll experiment with a (simplified) cell tower placement problem.</p>
<p class="indent">Our inputs are a collection of cell towers, each with a different effective range, and a mask showing where cell towers can be placed. The output is a collection of locations where the specified towers should be placed to maximize coverage.</p>
<p class="indent">The code we’ll work with is in <em>cell.py</em>. It has the same general structure as the other swarm experiments, but is slightly more advanced because evaluating a particle position involves checking for illegal tower positions and building an image. The <code>Evaluate</code> method of the objective function class is more complex, but still accepts a particle position and returns a score where lower implies a better solution.</p>
<p class="indent">I’ll lay out the plan of attack; then we’ll walk through the essential parts of the code before running some experiments.</p>
<h4 class="h4" id="ch00lev2_47"><em><strong>The Swarm Search</strong></em></h4>
<p class="noindent">We need to translate vectors of numbers within some bounds into possible solutions. We’ll work with cell towers and maps telling us where we can place them. Let’s begin with representing towers and maps.</p>
<p class="indent">The cell towers radiate in all directions, so we’ll represent them as circles where the diameter of the circle indicates the tower’s strength and the center of the circle the tower’s location. Not all towers are of equal strength. We specify a tower with a single floating-point number in (0, 1]; exactly how will become clear momentarily.</p>
<p class="indent">The maps are grayscale images. If a pixel’s value is 0, that pixel is a possible tower location; if the pixel is 255, it’s off-limits. I placed a collection of maps in the <em>maps</em> directory. You can make your own in any graphics program; use 255 to mark regions off-limits to towers and 0 for everything else. The maps need not be square. Note that the larger the map, the slower the search, which is why the supplied maps are rather small.</p>
<p class="indent">Tower sizes are fractions of half the map’s largest dimension. For example, the supplied maps are 80 pixels on a side. Therefore, a tower given as 0.1 has a diameter of 0.1 × 40 = 4 pixels, while a 0.6 tower’s diameter is 0.6 × 40 = 24 pixels. Towers are stored in a text file, one number per line, with the number of lines indicating the number of towers. Examine the files in the <em>towers</em> directory to see what I mean.</p>
<p class="indent">Particle positions represent tower locations. If there are <em>n</em> towers, we need 2<em>n</em>-dimensional particles, as we did for packing circles. Every two elements of a particle position are the center location for a tower. The order in which towers are specified maps, one-to-one, to pairs of particle elements. For example, <em>towers0</em> has six lines for six towers: 0.1, 0.2, 0.3, 0.4, 0.5, 0.6. Therefore, a search using <em>towers0</em> involves 12-dimensional particles</p>
<p class="center">(<em>x</em><sub>0</sub>, <em>y</em><sub>0</sub>; <em>x</em><sub>1</sub>, <em>y</em><sub>1</sub>; <em>x</em><sub>2</sub>, <em>y</em><sub>2</sub>; <em>x</em><sub>3</sub>, <em>y</em><sub>3</sub>; <em>x</em><sub>4</sub>, <em>y</em><sub>4</sub>; <em>x</em><sub>5</sub>, <em>y</em><sub>5</sub>)</p>
<p class="noindent"><span epub:type="pagebreak" id="page_143"/>where (<em>x</em><sub>0</sub>, <em>y</em><sub>0</sub>) is the location of the 0.1 tower, (<em>x</em><sub>1</sub>, <em>y</em><sub>1</sub>) is the location of the 0.2 tower, and so on.</p>
<p class="indent"><a href="ch05.xhtml#ch05fig02">Figure 5-2</a> shows the default maps.</p>
<div class="image"><img alt="Image" id="ch05fig02" src="../images/05fig02.jpg"/></div>
<p class="figcap"><em>Figure 5-2: The default maps</em></p>
<p class="indent">The first map is blank, with no forbidden regions. Any marked area on the other maps is off-limits to a tower. Think of these as roads, parking lots, lakes, and so on.</p>
<p class="indent">We have towers and maps. We know how to represent tower locations and sizes. How do we put towers, maps, and locations together to get a score? We want to cover the map as much as possible by placing towers in allowed locations. Therefore, we want to minimize the number of allowed map pixels that aren’t covered by a tower; we want the number of zero pixels after placing towers to be as small as possible. This sounds like a job for the objective function.</p>
<p class="indent">For a given particle position, the objective function needs to determine whether any proposed cell tower centers are in an illegal location. If even one tower is, the objective function rejects the entire configuration by immediately returning a score of 1.0, the largest possible, implying that none of the map is covered.</p>
<p class="indent">If all proposed tower locations are allowed, it’s time to calculate the coverage. The number of uncovered pixels divided by the number of pixels in the map is a value in [0, 1], where 0 means all pixels are covered. The lower this value, the better the coverage.</p>
<p class="indent">The approach I chose begins with an empty image the same size as the map image. We add towers to the image by adding each pixel covered by the tower to any current pixel value.</p>
<p class="indent"><span epub:type="pagebreak" id="page_144"/>Adding pixels in this way accomplishes two things: first, any pixel of the initially empty image that is still 0 after adding all the pixels covered by towers is uncovered; second, adding pixels tower by tower builds a comprehensible image. We’ll be able to see each tower and its covered region clearly, including areas where towers overlap.</p>
<p class="indent">To recap, for a given particle position, we:</p>
<ol>
<li class="noindent">Convert the tower coordinates to a set of points as we did earlier for packing circles.</li>
<li class="noindent">Return 1.0 as the score if any tower centers land on illegal regions of the map.</li>
<li class="noindent">Add each tower to an initially empty image array, including all covered pixels, if all tower centers are allowed.</li>
<li class="noindent">Return the count of uncovered pixels divided by the total number of pixels as the score.</li>
</ol>
<p class="indent">These steps map particle positions to solutions, thereby generating a single number representing the quality of the solution.</p>
<p class="indent"><a href="ch05.xhtml#ch05fig03">Figure 5-3</a> shows an input map on the left and output generated after a canonical particle swarm search using the six towers in <em>towers0</em> on the right.</p>
<div class="image"><img alt="Image" id="ch05fig03" src="../images/05fig03.jpg"/></div>
<p class="figcap"><em>Figure 5-3: An input mask (left) and resulting tower placement (right)</em></p>
<p class="indent">The towers overlap only slightly, and no centers are in masked areas. Examine the files in the <em>example</em> directory to view these images in more detail.</p>
<p class="indent">Let’s review the essential parts of <em>cell.py</em> to understand how the steps were translated into code.</p>
<h4 class="h4" id="ch00lev2_48"><em><strong>The Code</strong></em></h4>
<p class="noindent">The code in <em>cell.py</em> is relatively complex. Spend some quality time with the file before continuing.</p>
<p class="indent">The most important piece of code is the objective function class and friends; see <a href="ch05.xhtml#ch05list01">Listing 5-1</a>.<span epub:type="pagebreak" id="page_145"/></p>
<pre class="pre">class Objective:
    def __init__(self, image, towers, radius):
        self.image = image.copy()
        self.R, self.C = image.shape
     <span class="ent">➊</span> self.radii = (towers*radius).astype("int32")
        self.fcount = 0

    def Collisions(self, xy):
        n = 0
        for i in range(xy.shape[0]):
            x,y = xy[i]
            if (self.image[x,y] != 0):
                n += 1
        return n

    def Evaluate(self, p):
        self.fcount += 1
        n = p.shape[0]//2
     <span class="ent">➋</span> xy = np.floor(p).astype("uint32").reshape((n,2))
        if (self.Collisions(xy) != 0):
            return 1.0
        empty = np.zeros((self.R, self.C))
     <span class="ent">➌</span> cover = CoverageMap(empty, xy, self.radii)
        zeros = len(np.where(cover == 0)[0])
        uncovered = zeros / (self.R*self.C) 
        return uncovered</pre>
<p class="list" id="ch05list01"><em>Listing 5-1: The objective function class</em></p>
<p class="indent">The constructor stores the map (<code>image</code>), towers vector (<code>towers</code>), and <code>radius</code>, which is one-half the largest dimension of the map image. This sets the largest possible cell tower range; for example, if the tower’s range is 1, then the circle representing the tower has a radius of <code>radius</code> pixels, which is half the height or width of the map image, whichever is larger. Internally, <code>radii</code> is a vector of tower radii in pixels <span class="ent">➊</span>.</p>
<p class="indent">The <code>Evaluate</code> method first reshapes the particle position vector into (<em>x</em>, <em>y</em>) points, as we did for <em>circles.py</em>. In this case, we want pixel coordinates, so <code>floor</code> ensures that points are integer valued <span class="ent">➋</span>.</p>
<p class="indent">The <code>Collisions</code> method first checks whether any proposed cell tower centers are in a forbidden region. This is a simple query against the map image. If the center pixel isn’t 0, count it as a collision. If any collisions happen, return a score of 1.0, implying all pixels are uncovered.</p>
<p class="indent">Assuming no collisions, it’s time to place the towers and calculate the score. An <code>empty</code> image the same size as the map image is created and passed to <code>CoverageMap</code> along with the tower centers (<code>xy</code>) and <code>radii</code> <span class="ent">➌</span>. The return value, <code>cover</code>, is an image similar to the right side of <a href="ch05.xhtml#ch05fig03">Figure 5-3</a>, but without proper scaling to [0, 255]—it is a floating-point array. If an element of <code>cover</code> is 0, that element isn’t in the range of any tower, so we count it with NumPy’s <span epub:type="pagebreak" id="page_146"/><code>where</code> function and divide that count by the number of pixels in the map to calculate the score.</p>
<p class="indent">The <code>CoverageMap</code> method is not of the <code>Objective</code> class because it’s used elsewhere in <em>cell.py</em>. It is, however, critical to the success of the code, so let’s walk through it in some detail (<a href="ch05.xhtml#ch05list02">Listing 5-2</a>).</p>
<pre class="pre">def CoverageMap(image, xy, radii):
    im = image.copy()
    R,C = im.shape
 <span class="ent">➊</span> for k in range(len(radii)):
        x,y = xy[k]
     <span class="ent">➋</span> for i in range(x-radii[k],x+radii[k]):
            for j in range(y-radii[k],y+radii[k]):
                if ((i-x)**2 + (j-y)**2) &lt;= (radii[k]*radii[k]):
                    if i &lt; 0 or j &lt; 0:
                        continue
                    if i &gt;= R or j &gt;= C:
                        continue
                    im[i,j] += 0.5*(k+1)/len(radii)
    imax = im.max()
 <span class="ent">➌</span> for k in range(len(radii)):
        x,y = xy[k]
        im[x,y] = 1.4*imax
    return im</pre>
<p class="list" id="ch05list02"><em>Listing 5-2: Generating the coverage map for a set of tower positions</em></p>
<p class="indent">The <code>CoverageMap</code> method accepts the map image, tower center locations, and tower radii as input. Its goal is to fill in <code>im</code>. Passing an empty image to <code>CoverageMap</code> seems odd at present, but later calls to the function pass the map image itself.</p>
<p class="indent">Towers are applied in turn with center at (<em>x</em>, <em>y</em>) <span class="ent">➊</span>. The (inefficient) double loop <span class="ent">➋</span> examines every pixel in the map that could be inside the range of the current tower. The body of the inner loop asks whether the current pixel, (<em>i</em>, <em>j</em>), is within the disk of the current tower (the <code>if</code> statement). If so, and the (<em>i</em>, <em>j</em>) pixel is within the space of the image, the current pixel value is incremented according to</p>
<p class="center">0.5(<em>k</em> + 1)/<em>n</em></p>
<p class="noindent">where <em>n</em> is the number of towers. This equation increments the pixel value (<code>im</code> is a floating-point array) by an amount specific to each tower. The result leads to the right side of <a href="ch05.xhtml#ch05fig03">Figure 5-3</a>, where each disk is a different intensity and overlaps are visible.</p>
<p class="indent">After all towers are added, a final loop over adds each tower’s center point <span class="ent">➌</span>. The value of the center point is always 1.4 times the intensity of the maximum pixel value to make the center points visible (best viewed on a computer screen). Since <code>im</code> is a floating-point array, it isn’t restricted to [0, 255]. Scaling comes later in the code when writing output images to disk.</p>
<p class="indent"><span epub:type="pagebreak" id="page_147"/>The <code>CoverageMap</code> method returns a two-dimensional array where any remaining zero values are pixels not covered by any tower. The number of zero values scaled by the number of pixels is the final score for the given tower locations.</p>
<p class="indent">The main body of <em>cell.py</em> follows <em>circles.py</em> in form: parse the command line, create framework objects, and do the search. However, instead of calling <code>Optimize</code>, the search is performed by repeated calls to <code>Step</code> so the current best score can be displayed per iteration.</p>
<p class="indent">The search is configured as in <a href="ch05.xhtml#ch05list03">Listing 5-3</a>.</p>
<pre class="pre">rng = RE(kind=kind)
x,y = map_image.shape
lower = [0,0]*len(towers)
upper = [x,y]*len(towers)
b = Bounds(lower, upper, enforce="resample", rng=rng)
ndim = 2*len(towers)
w = x if (x&gt;y) else y
radius = w//2
i = RandomInitializer(npart, ndim, bounds=b, rng=rng)
obj = Objective(map_image, towers, radius)
swarm = DE(obj=obj, npart=npart, ndim=ndim, init=i, bounds=b, 
          max_iter=niter, tol=1e-9, rng=rng)</pre>
<p class="list" id="ch05list03"><em>Listing 5-3: Configuring the search</em></p>
<p class="noindent">Here <code>radius</code> sets the maximum radius for any tower.</p>
<p class="indent">The search itself is a loop (<a href="ch05.xhtml#ch05list04">Listing 5-4</a>).</p>
<pre class="pre">k = 0
swarm.Initialize()
while (not swarm.Done()):
    swarm.Step()
    res = swarm.Results()
    t = "    %5d: gbest = %0.8f" % (k,res["gbest"][-1])
    print(t, flush=True)
    k += 1
res = swarm.Results()</pre>
<p class="list" id="ch05list04"><em>Listing 5-4: Running the search</em></p>
<p class="noindent">The <code>Initialize</code> method configures the swarm, <code>Done</code> returns <code>True</code> when the search is complete (all iterations done or tolerance met), and <code>Step</code> performs one update of the swarm (it acts like headquarters).</p>
<p class="indent">Every iteration calls the <code>Results</code> method to report on the current best value—the fraction of image pixels not covered by a tower. After the loop exits, the final call to <code>Results</code> returns the best set of tower locations. Additional code captures the per-iteration output and generates the output map for the swarm’s best configuration. See the <code>frames</code> option on the command line.</p>
<p class="indent">Finally, the coverage map is generated and stored in the output directory, as shown in <a href="ch05.xhtml#ch05list05">Listing 5-5</a>.</p>
<pre class="pre"><span epub:type="pagebreak" id="page_148"/>p = res["gpos"][-1]
n = p.shape[0]//2
xy = p.astype("uint32").reshape((n,2))
radii = (towers*radius).astype("int32")
cover = CoverageMap(map_image, xy, radii)
c2 = (cover/cover.max())**(0.5)
c2 = c2/c2.max()
img = Image.fromarray((255*c2).astype("uint8"))
img.save(outdir+"/coverage.png")</pre>
<p class="list" id="ch05list05"><em>Listing 5-5: Generating the coverage map</em></p>
<p class="noindent">The swarm best position (<code>p</code>) is converted to a set of (<em>x</em>, <em>y</em>) points, which, along with the corresponding radii for the towers, are passed to <code>CoverageMap</code> along with the input map itself (<code>map_image</code>). Unlike the <code>Evaluate</code> method in the objective function, the map with masked regions is passed rather than an empty image.</p>
<p class="indent">The resulting coverage map (<code>cover</code>) is passed through a square root function to squash intensities before converting to a grayscale image (<code>img</code>) and writing to the output directory.</p>
<p class="indent">By necessity, we skipped code in <em>cell.py</em>, but a careful reading of the file will make those sections clear. Now let’s see what <em>cell.py</em> can do.</p>
<p class="indent">Running <em>cell.py</em> without arguments shows us how to use it:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 cell.py</span>
cell &lt;map&gt; &lt;towers&gt; &lt;npart&gt; &lt;niter&gt; &lt;alg&gt; &lt;kind&gt; &lt;outdir&gt; [frames]

  &lt;map&gt;     -  map image (.png)
  &lt;towers&gt;  -  text file w/towers and ranges
  &lt;npart&gt;   -  number of swarm particles
  &lt;niter&gt;   -  number of swarm iterations
  &lt;alg&gt;     -  DE|RO|PSO|BARE|GWO|JAYA|GA
  &lt;kind&gt;    -  randomness source
  &lt;outdir&gt;  -  output directory (overwritten)
  frames    -  'frames' ==&gt; output frame per iteration</pre>
<p class="indent">Let’s run the code using <em>map_01</em> and <em>towers0</em>. For example:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 cell.py maps/map_01.png towers/towers0 20 100 ga pcg64 test frames</span></pre>
<p class="indent">We’re using the GA with 20 particles for 100 iterations and dumping the output to a directory called <em>test</em>. The <code>frames</code> keyword outputs the current best tower placement per iteration so we can trace the evolution of the swarm visually.</p>
<p class="indent"><span epub:type="pagebreak" id="page_149"/>Notice that 100 iterations is not many compared to the 10,000 or more we used with <em>circles.py</em>. All the manipulation to generate the coverage map takes time, so running for 10,000 iterations is out of the question. Fortunately, we don’t usually need more than a few hundred iterations.</p>
<p class="indent">As <em>cell.py</em> runs, it dumps the current swarm best score along with a summary when the search ends. The output directory contains this text in <em>README.txt</em> along with the original map image (<em>map.png</em>) and the final coverage map (<em>coverage.png</em>). The Python <code>pickle</code> file (<em>.pkl</em>) contains the swarm objects, should you wish to explore the evolution in more detail. The output directory also contains a <em>frames</em> directory holding images representing the swarm’s best configuration by iteration. Page through these files to watch the swarm evolve.</p>
<p class="indent">My run produced a final coverage value of 0.358, meaning about 36 percent of the map wasn’t covered by a tower, as seen in <a href="ch05.xhtml#ch05fig04">Figure 5-4</a>.</p>
<div class="image"><img alt="Image" id="ch05fig04" src="../images/05fig04.jpg"/></div>
<p class="figcap"><em>Figure 5-4: Placing towers</em></p>
<p class="indent">All six towers in <em>towers0</em> are visible in the output. The towers overlap only slightly, which is a good sign. Each tower center avoids masked regions.</p>
<p class="indent">To experiment with <em>cell.py</em>, run the shell scripts <em>go_tower_results</em>, <em>go_towers</em>, and <em>go_towers0</em>. The first applies <em>towers0</em> to all sample maps using all algorithms. The second applies all tower files and algorithms to <em>map_02</em>. Finally, the last one applies each tower file to <em>map_00</em> using only bare-bones PSO to illustrate how the swarm places towers when there are no obstacles.</p>
<p class="indent">Run <em>go_results</em> and <em>go_towers</em>, then run <em>make_results_plot.py</em> and <em>make _towers_plot.py</em> to produce an image file containing all the results with each row showing the output of a different swarm algorithm.</p>
<p class="indent">The final script, <em>go_towers0</em>, produces the output seen in <a href="ch05.xhtml#ch05fig05">Figure 5-5</a>.</p>
<div class="image"><img alt="Image" id="ch05fig05" src="../images/05fig05.jpg"/></div>
<p class="figcap"><em>Figure 5-5: Placing towers on the default maps</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_150"/>It runs bare-bones PSO over the blank map for each tower file. Note that <em>towers0</em> isn’t able to completely cover the map, but the resulting positions do not overlap, meaning bare-bones PSO found an optimal configuration (one of many). The output for <em>towers3</em> is much the same with the small towers not overlapping the larger ones.</p>
<p class="indent">Contrast these results with those for <em>towers1</em> and <em>towers2</em>. It’s not immediately clear whether <em>towers1</em> is capable of completely covering the map, but <em>towers2</em> certainly is—yet tiny parts of the map remained uncovered. I suspect running for more than 300 iterations would take care of this. Does it? Is there a difference in runtime between using any map with masked regions and the blank map? If so, why might that be?</p>
<p class="indent">Feel free to experiment with different custom maps and numbers and sizes of towers to find whether there’s an optimal or more utilitarian mix of tower sizes. What works best, many small towers or a few larger ones?</p>
<p class="indent">Let’s use randomness to implement a “make it pretty” image filter.</p>
<h3 class="h3" id="ch00lev1_31"><strong>Enhancing Images</strong></h3>
<p class="noindent">When I view images I’ve taken with my phone in its gallery, I’m offered the option to remaster the picture; I call it the “make it pretty” filter. In this <span epub:type="pagebreak" id="page_151"/>section, we’ll use a swarm search to implement a “make it pretty” filter for grayscale images.</p>
<p class="indent">Our filter is based on one that has consistently appeared in academic literature. It’s a good example of the tendency to take an existing paper, slightly alter the technique, and publish it as new. A quick review of the literature turned up eight papers all implementing this approach with only a tweak of the optimization algorithm: PSO, Firefly, Cuckoo search, DE, PSO, Cuckoo, Cuckoo, and DE, respectively. While our implementation is yet another in this illustrious line of research, I’m claiming the excuse of pedagogy and do not appeal to novelty or applicability.</p>
<p class="indent">Disclaimers aside, the “make it pretty” filter applies a local image enhancement function to an input grayscale image to make it look nicer. If that sentence is as clear as mud, have no worries—I’ll explain.</p>
<p class="indent">We intend to apply a function to each pixel of the input image to produce a new output pixel. Let’s apply the function</p>
<div class="image1"><img alt="Image" id="ch05equ1" src="../images/f0151-01.jpg"/></div>
<p class="noindent">to the pixel at row <em>i</em> and column <em>j</em>; that is, <em>g<sub>ij</sub></em>. We need to find <em>a</em>, <em>b</em>, <em>m</em>, and <em>k</em> to make the image look as nice as possible.</p>
<p class="indent"><em>G</em> is the original image’s <em>mean intensity</em>, or the value found by adding all the pixel intensities and dividing by the number of pixels. The <em>µ</em> and <em>σ</em> variables are the mean (<em>µ</em>) and standard deviation (<em>σ</em>) of a 3×3 region around the current pixel, <em>g<sub>ij</sub></em>. Imagine a tic-tac-toe (naughts and crosses) board as in <a href="ch05.xhtml#ch05fig06">Figure 5-6</a>.</p>
<div class="image"><img alt="Image" id="ch05fig06" src="../images/05fig06.jpg"/></div>
<p class="figcap"><em>Figure 5-6: Pixel offsets</em></p>
<p class="indent">The 3×3 region slides over the image visiting each pixel, calculates <em>µ</em> and <em>σ</em>, and then uses <a href="ch05.xhtml#ch05equ1">Equation 5.1</a> to create a new pixel value, <img alt="Image" class="inline" src="../images/f0151-02.jpg"/>. Note that <img alt="Image" class="inline" src="../images/f0151-03.jpg"/> is the value of the <em>ij</em> pixel in the output image; it does not update the original image pixel, <em>g<sub>ij</sub></em>.</p>
<p class="indent">The function for updating image pixels has four parameters we need to find, along with other values that depend on the original image and how we apply the function. However, the function only tells us how to update the image for a given <em>a</em>, <em>b</em>, <em>m</em>, and <em>k</em>; it doesn’t say anything about how nice the output image will appear to a human observer. For that, we need an objective function.</p>
<p class="indent"><span epub:type="pagebreak" id="page_152"/>Researchers claim that the following function captures something of what makes an image look pleasant to humans:</p>
<div class="image1"><img alt="Image" src="../images/f0152-01.jpg"/></div>
<p class="noindent">We’ll use <em>F</em> as our objective function. The higher <em>F</em> is, the better the image will look to a human observer—at least, that’s the theory. <em>I</em> is the sum of the pixel intensities of an edge-detected version of the input image. The “edgels” variable is the number of edges in the edge-detected version of the image above a threshold, here 20. Finally, <em>r</em> and <em>c</em> are the image dimensions (rows and columns) and <em>h</em> is the entropy of the image:</p>
<div class="image1"><img alt="Image" src="../images/f0152-02.jpg"/></div>
<p class="noindent">Here <em>p<sub>i</sub></em> is the probability of pixel intensity in bin <em>i</em> of a 64-bin histogram. Entropy, in this sense, refers to the information content of the image.</p>
<p class="indent">This exercise seems superficially similar to curve fitting. We have a function with parameters to be optimized, but the algorithm behind the application of the function has many more steps. However, as we’ll soon learn, the extra effort means little to our swarm algorithms. They still provide floating-point vectors to the objective function and expect a scalar quality measure in return. The swarm is blissfully unaware of what it’s optimizing.</p>
<h4 class="h4" id="ch00lev2_49"><em><strong>The Enhancement Function</strong></em></h4>
<p class="noindent">We have a four-parameter optimization problem: for a given image, we want to find the best <em>a</em>, <em>b</em>, <em>m</em>, and <em>k</em> values, all of which are floating-point numbers. From a framework perspective, the setup is straightforward once we decide on bounds for the parameters. All the cool code will be in the objective function class. The full program is in <em>enhance.py</em>.</p>
<p class="indent">We start with <a href="ch05.xhtml#ch05list06">Listing 5-6</a>, the objective function class, which implements both the elaborate image enhancement and <em>F</em> functions given previously.</p>
<pre class="pre">class Objective:
    def __init__(self, img):
        self.img = img.copy()
        self.fcount = 0

    def F(self, dst):
        r,c = dst.shape
        Is = Image.fromarray(dst).filter(ImageFilter.FIND_EDGES) 
        Is = np.array(Is)
        edgels = len(np.where(Is.ravel() &gt; 20)[0])
        h = np.histogram(dst, bins=64)[0]
        p = h / h.sum()
        i = np.where(p != 0)[0]
        ent = -(p[i]*np.log2(p[i])).sum()<span epub:type="pagebreak" id="page_153"/>
        F = np.log(np.log(Is.sum()))*(edgels/(r*c))*ent
        return F

    def Evaluate(self, p):
        self.fcount += 1
        a,b,m,k = p
        dst = ApplyEnhancement(self.img, a,b,m,k)
        return -self.F(dst)</pre>
<p class="list" id="ch05list06"><em>Listing 5-6: The image enhancement objective function class</em></p>
<p class="indent">The class constructor stores a copy of the original image. The <code>Evaluate</code> method extracts <em>a</em>, <em>b</em>, <em>m</em>, and <em>k</em> from the supplied particle position, passing them and the original image to <code>ApplyEnhancement</code> to return a new image, <code>dst</code>. We pass the new image to the <code>F</code> method to calculate the score. Since we want to maximize <em>F</em>, we return the negative.</p>
<p class="indent">I’ll explain <code>ApplyEnhancement</code> momentarily; let’s focus on <code>F</code> for now. The method is linear and makes heavy use of powerful functions supplied by NumPy and PIL (<code>Image</code> and <code>ImageFilter</code>). Walking through, line by line, makes sense in this case.</p>
<p class="indent">First, we extract the image dimensions (<code>r</code>, <code>c</code>). In the next line, we apply an edge detection filter to the image producing <code>Is</code> from <code>dst</code>. The output of an edge detector looks like <a href="ch05.xhtml#ch05fig07">Figure 5-7</a>.</p>
<div class="image"><img alt="Image" id="ch05fig07" src="../images/05fig07.jpg"/></div>
<p class="figcap"><em>Figure 5-7: An edge detector in action</em></p>
<p class="indent">We recast the PIL image, <code>Is</code>, as a NumPy array before using <code>where</code> to count the number of edge pixels greater than 20. We chose 20 because it’s an empirically selected threshold that works well. The count is stored in <code>edgels</code>.</p>
<p class="indent">The only part of <em>F</em> yet to determine is the entropy, <em>h</em>, here given as <code>ent</code>. To get this, we first need the histogram of the image using 64 bins, conveniently acquired in a single line of code courtesy of NumPy’s <code>histogram</code> function. Scaling the histogram by the sum of all bins converts from counts per bin to an estimate of the probability per bin, <code>p</code>.</p>
<p class="indent">With <code>p</code> in hand, we calculate the entropy by summing the probabilities multiplied by the log, base 2, of the probabilities (<code>ent</code>). The penultimate line in <code>F</code> is a direct analog of the equation for <em>F</em>, the value of which is returned.</p>
<p class="indent"><span epub:type="pagebreak" id="page_154"/>Now for <code>ApplyEnhancement</code> in <a href="ch05.xhtml#ch05list07">Listing 5-7</a>.</p>
<pre class="pre">def ApplyEnhancement(g, a,b,c,k):
    def stats(g,i,j):
        rlo = max(i-1,0); rhi = min(i+1,g.shape[0])
        clo = max(j-1,0); chi = min(j+1,g.shape[1])
        v = g[rlo:rhi,clo:chi].ravel()
        if len(v) &lt; 3:
            return v[0],1.0
        return v.mean(), v.std(ddof=1)

    rows,cols = g.shape
    dst = np.zeros((rows,cols))
    G = g.mean()
    for i in range(rows):
        for j in range(cols):
            m,s = stats(g,i,j)
            dst[i,j] = ((k*G)/(s+b))*(g[i,j]-c*m)+m**a
    dmin = dst.min()
    dmax = dst.max()
    return (255*(dst - dmin) / (dmax - dmin)).astype("uint8")</pre>
<p class="list" id="ch05list07"><em>Listing 5-7: Applying a set of parameters to an image</em></p>
<p class="noindent">We enhance the original image by applying <a href="ch05.xhtml#ch05equ1">Equation 5.1</a>. The output image (<code>dst</code>) is constructed, pixel by pixel, using the local 3×3 region mean (<code>m</code>) and standard deviation (<code>s</code>) in conjunction with the arguments <code>a</code>, <code>b</code>, <code>c</code>, and <code>k</code>. Note that <code>c</code> is <em>m</em> in <a href="ch05.xhtml#ch05equ1">Equation 5.1</a>.</p>
<p class="indent">The helper function, <code>stats</code>, defines the 3×3 region around (<em>i</em>, <em>j</em>), accounting for the image index limits. It then returns the mean and standard deviation. The <code>if</code> catches the edge case where too few pixels exist for a meaningful standard deviation calculation. Notice the <code>ddof</code> keyword in the call to <code>std</code>. By default, NumPy calculates the biased estimator of the variance by dividing by the number of values instead of the unbiased estimate found by dividing by one less than the number of values. Many statistics packages use the unbiased estimator by default. In most cases, especially if there are &lt; 20 values in the dataset, we want the unbiased estimator, so we set <code>ddof=1</code>. Recall that the standard deviation is the square root of the variance.</p>
<p class="indent">All that remains is to configure the search, as shown in <a href="ch05.xhtml#ch05list08">Listing 5-8</a>.</p>
<pre class="pre">orig = np.array(Image.open(src).convert("L"))
img = orig / 256.0
ndim = 4
rng = RE(kind=kind)
b = Bounds([0.0,1.0,0.0,0.5], [1.5,22,1.0,1.5], enforce="resample", rng=rng)
i = RandomInitializer(npart, ndim, bounds=b, rng=rng)<span epub:type="pagebreak" id="page_155"/>
obj = Objective(img)
swarm = GWO(obj=obj, npart=npart, ndim=ndim, init=i, bounds=b, max_iter=niter, rng=rng)</pre>
<p class="list" id="ch05list08"><em>Listing 5-8: Configuring the search</em></p>
<p class="indent">The configuration follows our framework: randomness source (<code>rng</code>), bounds (<code>b</code>), initializer (<code>i</code>), objective function (<code>obj</code>), and a <code>swarm</code> object, here <code>GWO</code>. We force the input image (<code>orig</code>) to grayscale (<code>convert</code>) and scale it by 256 to be in the range [0, 1) (<code>img</code>). It’s not uncommon to manipulate images in this range instead of [0, 255]. After manipulation, the image is scaled to [0, 255] and converted to an integer type before writing it to disk.</p>
<p class="indent">The search is four dimensional (<code>ndim</code>), so there are four bounds. The bounds vary by dimension. The limits <em>a ∈</em> [0, 1.5], <em>b ∈</em> [1.0, 22], <em>m ∈</em> [0, 1], and <em>k ∈</em> [0.5, 1.5] are based on values used in the literature. As we’ll see, they appear to work well, but try experimenting with them, especially if you notice output values near the limits. We’ll soon learn how to find these values after a search.</p>
<p class="indent">Running the search is as simple as calling <code>Optimize</code> on the <code>swarm</code> object, but we want to track the <em>F</em> score as we go, so we’ll loop manually instead (<a href="ch05.xhtml#ch05list09">Listing 5-9</a>).</p>
<pre class="pre">k = 0
swarm.Initialize()
while (not swarm.Done()):
    swarm.Step()
    res = swarm.Results()
    t = "    %5d: gbest = %0.8f" % (k,res["gbest"][-1])
    print(t, flush=True)
    s += t+"\n"
    k += 1
res = swarm.Results()
pickle.dump(res, open(outdir+"/results.pkl","wb"))
a,b,m,k = res["gpos"][-1]
dst = ApplyEnhancement(img, a,b,m,k)
Image.fromarray(dst).save(outdir+"/enhanced.png")
Image.fromarray(orig).save(outdir+"/original.png")</pre>
<p class="list" id="ch05list09"><em>Listing 5-9: Running the search</em></p>
<p class="indent">The search ends after all specified iterations. We then dump the final results (<code>res</code>) to the output directory via <code>pickle</code>. Use the <code>gpos</code> key to return the final set of parameters. To conclude, the image is enhanced with the best set and written to the output directory along with the original image for comparison purposes.</p>
<p class="indent">Does <em>enhance.py</em> work? Let’s find out.</p>
<h4 class="h4" id="ch00lev2_50"><span epub:type="pagebreak" id="page_156"/><em><strong>The Code</strong></em></h4>
<p class="noindent">Run <em>enhance.py</em> without arguments to learn what it expects on the command line:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 enhance.py</span>

enhance &lt;src&gt; &lt;npart&gt; &lt;niter&gt; &lt;alg&gt; &lt;kind&gt; &lt;output&gt;

  &lt;src&gt;    - source grayscale image
  &lt;npart&gt;  - number of particles
  &lt;niter&gt;  - number of iterations
  &lt;alg&gt;    - BARE,RO,DE,PSO,JAYA,GWO,GA
  &lt;kind&gt;   - randomness source
  &lt;output&gt; - output directory (overwritten)</pre>
<p class="noindent">We need to supply the original image, swarm size, iterations, algorithm type, randomness source, and output directory name.</p>
<p class="indent">The <em>images</em> directory contains a set of 128×128-pixel grayscale images that we’ll use for our experiments. The search isn’t particularly fast, given the sequential nature of the framework and the extensive image manipulations each call to the objective function entails, so smaller images work best. The program will work with larger images, which need not be square; all that’s required is patience.</p>
<p class="indent">Give this command line a try:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 enhance.py images/barbara.png 10 60 gwo minstd babs</span>
  0: gbest = -4.77187094
  1: gbest = -4.80063898
  2: gbest = -5.09058855
  3: gbest = -5.09058855
--<span class="codeitalic1">snip</span>--</pre>
<p class="indent">The output shows the current swarm best <em>F</em> score by iteration. The value is negative because we want to maximize <em>F</em>. The command line specified GWO with a swarm of 10 particles, 60 iterations, and an output directory named <em>babs</em>.</p>
<p class="indent">On my system, the search finishes with this output:</p>
<pre class="pre">  59: gbest = -6.09797382

Search results: GWO, 10 particles, 60 iterations

Optimization minimum -6.09797382 (time = 216.419)
(14 best updates, 610 function evaluations)</pre>
<p class="indent"><span epub:type="pagebreak" id="page_157"/>Therefore, the best set of parameters led to <em>F</em> = 6.09797 after 14 swarm best updates. The <em>babs</em> output directory contains</p>
<pre class="pre">enhanced.png
original.png
README.txt
results.pkl</pre>
<p class="noindent">giving us the enhanced image, the original image, the pickled results, and a README file containing all output generated during the search.</p>
<p class="indent">The enhanced image should look sharper with better contrast than the original. Unfortunately, a printed version will likely not show the differences clearly. Nonetheless, <a href="ch05.xhtml#ch05fig08">Figure 5-8</a> shows both images, with the original on the left and the enhanced version on the right.</p>
<div class="image"><img alt="Image" id="ch05fig08" src="../images/05fig08.jpg"/></div>
<p class="figcap"><em>Figure 5-8: The original and enhanced images</em></p>
<p class="noindent">Look, in particular, at the books in the bookcase. They are better defined and show improved contrast.</p>
<p class="indent">To extract the enhancement parameters, load the <em>results.pkl</em> file</p>
<pre class="pre">&gt; <span class="codestrong1">python3</span>
&gt;&gt;&gt; <span class="codestrong1">import numpy as np; import pickle</span>
&gt;&gt;&gt; <span class="codestrong1">res = pickle.load(open("babs/results.pkl","rb"))</span>
&gt;&gt;&gt; <span class="codestrong1">res["gpos"][-1]</span>
array([0.01867829, 1.00785356, 0.45469097, 1.1731131 ])</pre>
<p class="noindent">which tells us that <em>a</em> = 0.01867, <em>b</em> = 1.0078, <em>m</em> = 0.45469, and <em>k</em> = 1.17311. We haven’t used <code>pickle</code> before; it requires a file object (the output of <code>open</code>) and must use binary mode (<code>"rb"</code>).</p>
<p class="indent">There are nine images in the <em>images</em> directory. We’ll run various swarm and evolutionary algorithms against these images, and then collect the resulting output to produce composite images showing the original and the enhanced versions so we might rate each algorithm’s performance. To that end, I created two Python scripts: <em>process_images.py</em> and <em>merge_images.py</em>.</p>
<p class="indent"><span epub:type="pagebreak" id="page_158"/>Run <em>process_images.py</em> first. I recommend starting it in the evening and returning in the morning. The script processes every image in <em>images</em> using each swarm algorithm. The swarm consists of 10 particles and runs for 75 iterations in all cases.</p>
<p class="indent">When <em>process_images.py</em> finishes, use <em>merge_images.py</em> to produce composite images showing the results, which are in the <em>output</em> directory. For example</p>
<pre class="pre">&gt; <span class="codestrong1">python3 merge_images.py zelda zelda_results.png</span></pre>
<p class="noindent">creates <em>zelda_results.png</em>, as in <a href="ch05.xhtml#ch05fig09">Figure 5-9</a>. Canonical PSO looks like the winner here.</p>
<div class="image"><img alt="Image" id="ch05fig09" src="../images/05fig09.jpg"/></div>
<p class="figcap"><em>Figure 5-9: A composite image. From top left: original, bare-bones PSO, DE, GA, GWO, Jaya, canonical PSO, RO.</em></p>
<p class="indent"><a href="ch05.xhtml#ch05tab03">Table 5-3</a> lists the <em>F</em> scores and parameters for each algorithm.</p>
<p class="tabcap" id="ch05tab03"><strong>Table 5-3:</strong> <em>F</em> Scores and Parameters for Each Algorithm</p>
<table class="table-h">
<colgroup>
<col style="width:10%"/>
<col style="width:10%"/>
<col style="width:10%"/>
<col style="width:10%"/>
<col style="width:10%"/>
<col style="width:10%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"/>
<th class="tab_th"><em><strong>F</strong></em></th>
<th class="tab_th"><em><strong>a</strong></em></th>
<th class="tab_th"><em><strong>b</strong></em></th>
<th class="tab_th"><em><strong>m</strong></em></th>
<th class="tab_th"><em><strong>k</strong></em></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1"><strong>GWO</strong></td>
<td class="bg1">4.59328</td>
<td class="bg1">0.00030</td>
<td class="bg1">2.21893</td>
<td class="bg1">0.71926</td>
<td class="bg1">0.99232</td>
</tr>
<tr>
<td class="bg"><strong>Canonical PSO</strong></td>
<td class="bg">5.07767</td>
<td class="bg">0.00585</td>
<td class="bg">1.50612</td>
<td class="bg">0.39545</td>
<td class="bg">1.46734</td>
</tr>
<tr>
<td class="bg1"><strong>Jaya</strong></td>
<td class="bg1">4.31448</td>
<td class="bg1">1.21667</td>
<td class="bg1">1.00493</td>
<td class="bg1">0.76448</td>
<td class="bg1">1.47750</td>
</tr>
<tr>
<td class="bg"><strong>DE</strong></td>
<td class="bg">4.29245</td>
<td class="bg">1.24982</td>
<td class="bg">1.00076</td>
<td class="bg">0.76535</td>
<td class="bg">1.46355</td>
</tr>
<tr>
<td class="bg1"><strong>Bare-bones PSO</strong></td>
<td class="bg1">4.27962</td>
<td class="bg1">1.09779</td>
<td class="bg1">1.01205</td>
<td class="bg1">0.89588</td>
<td class="bg1">1.49625</td>
</tr>
<tr>
<td class="bg"><strong>GA</strong></td>
<td class="bg">4.10057</td>
<td class="bg">1.14040</td>
<td class="bg">1.64773</td>
<td class="bg">0.12196</td>
<td class="bg">1.19704</td>
</tr>
<tr>
<td class="bg1"><strong>RO</strong></td>
<td class="bg1">4.01708</td>
<td class="bg1">1.08478</td>
<td class="bg1">5.07904</td>
<td class="bg1">0.53249</td>
<td class="bg1">0.97552</td>
</tr>
</tbody>
</table>
<p class="indent">The subjectively best-looking image is also the image with the largest <em>F</em> score—a good sign. The GWO image, which has low contrast but is quite sharp, has the second-highest <em>F</em> score. The GWO parameters are also quite different from the others. It’s possible that the enhancement function parameter space has a fairly complex structure, and there are multiple local minima. For the Zelda image, the GWO algorithm seems to have landed apart from the others. Does this happen with the other images? Which algorithm seems to work best overall?</p>
<p class="indent"><span epub:type="pagebreak" id="page_159"/>The program <em>F.py</em> applies a specific set of parameters to an image. What happens if we apply <em>a</em> from the GWO result and <em>b</em>, <em>m</em>, and <em>k</em> from the canonical PSO result?</p>
<pre class="pre">&gt; <span class="codestrong1">python3 F.py images/zelda.png zelda2.png 0.00030 1.50612 0.39545 1.46734</span>
F = 5.28999404</pre>
<p class="noindent">The new <em>F</em> score is higher still, and the output file, <em>zelda2.png</em>, looks even better than the canonical PSO result.</p>
<p class="indent">There is much more to explore here. I’ll provide a few suggestions in “Exercises” on <a href="ch05.xhtml#ch00lev1_33">page 169</a>, including a way to (possibly) enhance color images. For the time being, let’s move on to another experiment, which combines optimization with simulation to maximize profit by figuring out how to best arrange the products in a grocery store.</p>
<h3 class="h3" id="ch00lev1_32"><strong>Arranging a Grocery Store</strong></h3>
<p class="noindent">Have you noticed that grocery stores usually put milk in the back, as far from the entrance as possible? Or that candy is located right at the front, near the checkouts? The placement of products in a grocery store is not an accident; it’s intentional to maximize profit. Many people stop by the store to pick up necessities, like milk, and often pick up something else if they run across it, like candy. Grocery stores arrange the products to maximize such occurrences and increase revenue.</p>
<p class="indent">This section attempts to replicate such an arrangement of products to validate or refute common store practice. This experiment combines optimization with simulation. We’ll optimize product placement in the store using a set of simulated shoppers to evaluate the arrangement. Our goal is an arrangement of products, our objective function score is revenue over a day, and the function itself is a simulation of several hundred shoppers. Randomness is everywhere, from swarm initialization and position updates to the collection of shoppers and their habits. To jump in, read through <em>store.py</em>.</p>
<h4 class="h4" id="ch00lev2_51"><em><strong>The Environment</strong></em></h4>
<p class="noindent">Let’s define our operating environment. Actual stores are essentially two dimensional; there is a layout over some floor space. Our framework uses position vectors, one-dimensional entities. We’ll make the store one-dimensional as well, so a position vector can be a store layout with each element a product. Shoppers will enter the store on the left (index 0) and progress through the store to the right, like in <a href="ch05.xhtml#ch05fig10">Figure 5-10</a>.<span epub:type="pagebreak" id="page_160"/></p>
<div class="image"><img alt="Image" id="ch05fig10" src="../images/05fig10.jpg"/></div>
<p class="figcap"><em>Figure 5-10: Shopping at a one-dimensional grocery store (Illustration by Joseph Kneusel)</em></p>
<p class="indent">People typically go to the store to pick up a specific product; we’ll call this the <em>target product</em>. The shoppers will also have impulse products that they’ll buy if they encounter them before the target product.</p>
<p class="indent">For example, <a href="ch05.xhtml#ch05fig10">Figure 5-10</a> shows two shoppers who are thinking of a target product (signified by the question mark) and an impulse product (the exclamation point).</p>
<p class="indent">The shopper on the left is searching for the diamond product, but will purchase the circle product if they see it. Since they encounter the diamond before the circle, they buy only the diamond product and leave the store.</p>
<p class="indent">The shopper on the right is searching for the triangle product, but they’ll buy the square product if they come across it. They find the square while searching for the triangle and purchase both.</p>
<p class="indent">The shopping simulation requires products, located in the <em>products.pkl</em> file. The file contains three lists, each of 24 elements: counts, names, and prices, in that order. The data comes from an actual collection of products purchased over some period. The products are stored in decreasing purchase frequency, so the item most often purchased is first, and the least often purchased is last.</p>
<p class="indent">We convert the counts to a purchase probability by dividing each by the sum of all the counts. We’ll use the purchase probability in our objective function.</p>
<p class="indent">If there are 24 products in the store, we have 24-dimensional position vectors. We’re searching for the best ordering of products to maximize daily revenue. We’ll visit the simulation part in more detail later, but for the moment, let’s focus on the product order and how we’ll represent it in a swarm.</p>
<p class="indent">At first, we might think to make the position vectors discrete values in [0, 23], where each number is a product, an index into the list of products read from <em>products.pkl</em>. However, I implemented an alternate approach using position vectors in [0, 1).</p>
<p class="indent">Ultimately, we want a vector that places products in a particular order, some permutation of the vector {0, 1, 2, 3, . . . , 23}. The trick is to abstract this permutation so we can still use continuous floating-point values in [0, 1). Instead of using the product numbers directly, we pass each position vector to NumPy’s <code>argsort</code> function, which returns the order of indices that would <span epub:type="pagebreak" id="page_161"/>sort the vector. For a vector of 24 elements, the output of <code>argsort</code> is a permutation of the numbers 0 through 23.</p>
<p class="indent">That this approach works—and we’ll see that it does—is impressive. We’re asking the swarms to generate a collection of floating-point numbers, [0, 1), that are useful only when the additional operation of determining their sort order has happened. The reason it works likely has to do with the fact that using integer values requires some kind of truncation or rounding of a floating-point number where the implemented approach uses the numbers as they are. If changing a particular element of the particle position from 0.304 to 0.288 alters the sort order of the entire vector to a more profitable configuration, the swarm makes use of that change, whereas truncation might call both numbers 0.</p>
<p class="indent">Here are the steps we need to implement:</p>
<ol>
<li class="noindent">Initialize the swarm with 24-element position vectors in [0, 1).</li>
<li class="noindent">Initialize a collection of randomly generated shoppers.</li>
<li class="noindent">Run our usual swarm search where each position vector is evaluated by passing shoppers through the store using the sort order of the current vector as the arrangement of products. Then, tally the amount of money each shopper spends. They’ll always find their target product, but may not find their impulse products. Finally, return the negative of the total spent by the shoppers, as we want to maximize daily revenue.</li>
<li class="noindent">Let the swarm algorithm update positions as usual until all iterations have run.</li>
<li class="noindent">Report the sort order of the best position found as the “ideal” ordering of the products.</li>
</ol>
<p class="indent">The following two sections detail how to implement shoppers and how the objective function works. With those in mind, we’ll be ready to go shopping and see if our simulation agrees with grocery industry experts.</p>
<h4 class="h4" id="ch00lev2_52"><em><strong>The Shoppers</strong></em></h4>
<p class="noindent">A shopper is an instance of the <span class="codeitalic">Shopper</span> class, as shown in <a href="ch05.xhtml#ch05list010">Listing 5-10</a>.</p>
<pre class="pre">class Shopper:
    def __init__(self, fi, pv, rng):
        self.item_values = pv
     <span class="ent">➊</span> self.target = Select(fi,rng)
     <span class="ent">➋</span> self.impulse = np.argsort(rng.random(len(fi)))[:3]
        while (self.target in self.impulse):
            self.impulse = np.argsort(rng.random(len(fi)))[:3]

    def GoShopping(self, products):
        spent = 0.0
        for p in products:<span epub:type="pagebreak" id="page_162"/>
            if (p == self.target):
                spent += self.item_values[p]
             <span class="ent">➌</span> break
            if (p in self.impulse):
                spent += self.item_values[p]
        return spent</pre>
<p class="list" id="ch05list010"><em>Listing 5-10: The</em> <span class="codeitalic">Shopper</span> <em>class</em></p>
<p class="indent">The constructor configures the shopper by selecting the target (<code>target</code>) and impulse (<code>impulse</code>) products. It also keeps a copy of the product prices (<code>item_values</code>) for when it’s time to go shopping.</p>
<p class="indent">A call to <code>Select</code> returns the target product, which is an index into the product list <span class="ent">➊</span>. The <code>Select</code> method takes advantage of the fact that <code>fi</code> is the probability of a product being purchased in decreasing order (<a href="ch05.xhtml#ch05list011">Listing 5-11</a>).</p>
<pre class="pre">def Select(fi, rng):
    t = rng.random()
    c = 0.0
    for i in range(len(fi)):
        c += fi[i]
        if (c &gt;= t):
            return i</pre>
<p class="list" id="ch05list011"><em>Listing 5-11: Selecting a product according to its purchase frequency</em></p>
<p class="indent">We select a random value, [0, 1) (<code>t</code>). We then add successive probabilities for each product to <code>c</code> until equaling or exceeding <code>t</code>. When that happens, as it must because <em>t</em> &lt; 1 and the sum of all product probabilities is 1.0, the current product index is returned (<code>i</code>).</p>
<p class="indent">Let’s see an example to clarify how <code>Select</code> works. Imagine there are five products, each selected with probability:</p>
<p class="center">0.5, 0.3, 0.1, 0.07, 0.03</p>
<p class="noindent">This means product 0 is purchased about 50 percent of the time while product 4 is purchased only 3 percent of the time. The sum is 1.0, or 100 percent. Now, pick a random value in <em>t ∈</em> [0, 1). This will be less than 0.5 half of the time, meaning <code>Select</code> will return index 0. The sum of the first two product probabilities is 0.5 + 0.3 = 0.8. But half the time <em>t</em> &lt; 0.5, so the difference between 0.5 and 0.8 is the fraction of the time 0.5 &gt; <em>t ≤</em> 0.8: 30 percent of the time. Similarly, 10 percent of the time 0.8 &gt; <em>t ≤</em> 0.9, 7 percent of the time 0.9 &gt; <em>t ≤</em> 0.97, and 3 percent of the time 0.97 &gt; <em>t ≤</em> 1.0. Therefore, the index returned by <code>Select</code> reflects the true purchase probability for the item.</p>
<p class="indent"><a href="ch05.xhtml#ch05fig10">Figure 5-10</a> shows a single impulse purchase product. In reality, the simulation selects three unique impulse products that aren’t the target product <span class="ent">➋</span>. The call to <code>argsort</code> returns a permutation of the product indices, so keeping the first three ensures unique products. The <code>while</code> loop repeats the process, if necessary, to ensure that the target isn’t one of the impulse purchases.</p>
<p class="indent"><span epub:type="pagebreak" id="page_163"/>When evaluating a particle position, we call the <code>GoShopping</code> method. It’s passed a list of <code>products</code>, the sort order for the current particle. It then walks through the list, checking if the current product is the target or one of the impulse buys. If it is either, the method adds the price to <code>spent</code> to indicate that the shopper purchased the item. If the item is the target, the loop exits, and any unencountered impulse products are ignored <span class="ent">➌</span>. The method then returns the total spent.</p>
<p class="indent">The <code>Shopper</code> class represents a single shopper. The <code>Objective</code> class manages a collection of shoppers.</p>
<h4 class="h4" id="ch00lev2_53"><em><strong>The Objective Function</strong></em></h4>
<p class="noindent">The <code>Objective</code> class evaluates a single particle position, or a configuration of products, as shown in <a href="ch05.xhtml#ch05list012">Listing 5-12</a>.</p>
<pre class="pre">class Objective:
    def __init__(self, nshoppers, pci, pv, rng):
        self.nshoppers = nshoppers
        self.fcount = 0
        self.shoppers = []
        for i in range(nshoppers):
            shopper = Shopper(pci, pv, rng)
            self.shoppers.append(shopper)

    def Evaluate(self, p):
        self.fcount += 1
     <span class="ent">➊</span> order = np.argsort(p)
        revenue = 0.0
        for i in range(self.nshoppers):
            revenue += self.shoppers[i].GoShopping(order)
        return -revenue</pre>
<p class="list" id="ch05list012"><em>Listing 5-12: The</em> <span class="codeitalic">Objective</span> <em>function class</em></p>
<p class="indent">The constructor builds a list of randomly initialized shoppers, meaning we use the same collection of shoppers for the entire simulation. Here, <code>pci</code> is the probability of each product being selected, most probable first, and <code>pv</code> is the associated price.</p>
<p class="indent">The <code>Evaluate</code> method receives a single particle position; however, we aren’t interested in <code>p</code>’s values, only the <code>order</code> in which they need to be moved to sort them <span class="ent">➊</span>. This is the product order <code>GoShopping</code> uses to determine how much money the shopper spends. To get the total revenue, then, each shopper is asked to go shopping while tallying the amount of money spent (<code>revenue</code>). The objective function value is the negative of this amount (to maximize).</p>
<p class="indent"><span epub:type="pagebreak" id="page_164"/>The remainder of <em>store.py</em> loads the products and then parses the command line:</p>
<pre class="pre">products = pickle.load(open("products.pkl","rb"))
nshoppers = int(sys.argv[1])
npart = int(sys.argv[2])
niter = int(sys.argv[3])
alg = sys.argv[4].upper()
kind = sys.argv[5]</pre>
<p class="noindent">The code then creates the list of purchase probabilities (<code>pci</code>)</p>
<pre class="pre">ci = products[0]  # product counts
ni = products[1]  # product names
pv = products[2]  # product values
pci = ci / ci.sum()  # probability of being purchased
N = len(ci)          # number of products</pre>
<p class="noindent">before initializing the swarm and running the search:</p>
<pre class="pre">ndim = len(ci)
rng = RE(kind=kind)
b = Bounds([0]*ndim, [1]*ndim, enforce="resample", rng=rng)
i = RandomInitializer(npart, ndim, bounds=b, rng=rng)
obj = Objective(nshoppers, pci, pv, rng)
swarm = Jaya(obj=obj, npart=npart, ndim=ndim, init=i, max_iter=niter, bounds=b, rng=rng)
swarm.Optimize()
res = swarm.Results()</pre>
<p class="noindent">The remainder of the file generates a report showing how successful the search was.</p>
<h4 class="h4" id="ch00lev2_54"><em><strong>The Shopping Simulation</strong></em></h4>
<p class="noindent">Enough prep; let’s run and see what output we get:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 store.py 250 20 200 pso mt19937</span>
Maximum daily revenue $1114.28 (time 38.440 seconds)
(25 best updates, 4020 function evaluations)

Product order:
             cream cheese  ( 2.3%) ($1.57)
                  berries  ( 1.9%) ($1.98)
          misc. beverages  ( 1.6%) ($2.37)
                    candy  ( 1.7%) ($2.23)
                  chicken  ( 2.4%) ($1.49)
                     beef  ( 3.0%) ($1.35)
                  dessert  ( 2.1%) ($1.76)
                   onions  ( 1.8%) ($2.10)
                   coffee  ( 3.3%) ($1.23)<span epub:type="pagebreak" id="page_165"/>
              salty snack  ( 2.1%) ($1.66)
                   apples  ( 1.9%) ($1.87)
                   butter  ( 3.1%) ($1.29)
                chocolate  ( 2.8%) ($1.42)
              frankfurter  ( 3.4%) ($1.19)
          root vegetables  ( 6.2%) ($1.01)
            shopping bags  ( 5.6%) ($1.02)
              canned beer  ( 4.4%) ($1.08)
              brown bread  ( 3.7%) ($1.15)
             bottled beer  ( 4.6%) ($1.06)
    fruit/vegetable juice  ( 4.1%) ($1.11)
                   pastry  ( 5.1%) ($1.04)
                   yogurt  ( 7.9%) ($1.01)
               rolls/buns  (10.5%) ($1.00)
               whole milk  (14.5%) ($1.00)

milk rank = 23
candy rank = 3

Upper half median probability of being selected =  2.1
                           median product value = 1.71
Lower half median probability of being selected =  4.8
                           median product value = 1.05</pre>
<p class="indent">The code expects the number of shoppers to simulate (<code>250</code>), the number of particles (<code>20</code>) and iterations (<code>200</code>), the algorithm (<code>pso</code>), and a randomness source (<code>mt19937</code>). The output prints to the screen.</p>
<p class="indent">First, we’re told this run generated $1,114.28 as the maximum daily revenue. The product order is as given, where the first product is at the front of the store, here cream cheese (strangely). The order provides the product name, the probability of purchasing it, and the price.</p>
<p class="indent">Conventional wisdom says to put the milk at the back of the store and the candy at the front. In this case, milk ended up as product 23, which is at the back of the store, while candy was product 3, very close to the front. This run followed conventional wisdom—a good sign.</p>
<p class="indent">The remainder of the output gives the median purchase probability for products in the first half of the store along with the median price for those products, and then again for products in the second half of the store. If the swarm is ordering the store along the lines we expect it to, then lower probability items that cost more will appear in the front part of the store (the top half of the product list). At the same time, higher probability items that generally cost less will appear toward the back of the store. This is precisely what we see in the output: items toward the back are more likely to be purchased and, in general, cost less.</p>
<p class="indent">Overall, the search produced reasonable output validating conventional wisdom.</p>
<p class="indent"><span epub:type="pagebreak" id="page_166"/>The script <em>go_store</em> runs 10 searches for each algorithm capturing the results in the <em>output</em> directory. Run it with</p>
<pre class="pre">&gt; <span class="codestrong1">sh go_store</span></pre>
<p class="noindent">then follow with <em>process_results.py</em>:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 process_results.py</span></pre>
<p class="noindent">This should produce a <em>results</em> directory that contains NumPy files (<em>.npy</em>) holding the milk and candy rankings for each algorithm and run, along with the best revenue for each run by algorithm. Also included is a plot tracking the milk and candy rankings across runs for each algorithm; see <a href="ch05.xhtml#ch05fig11">Figure 5-11</a>.</p>
<div class="image"><img alt="Image" id="ch05fig11" src="../images/05fig11.jpg"/></div>
<p class="figcap"><em>Figure 5-11: Product rankings by algorithm across 10 runs, in which solid circles are milk and open circles are candy</em></p>
<p class="indent">In the figure, solid circles represent where milk ended up in the rankings, and open circles are for candy. Almost all algorithms were able to put milk near the very back of the store, with both PSO variants and Jaya perhaps the most consistent (for this single run of <em>go_store</em>). An obvious exception is RO. While it managed to put milk behind candy on every run, at times the placement wasn’t particularly great. For example, in run 9, milk and candy were almost next to each other.</p>
<p class="indent">The <em>process_results.py</em> code also outputs a summary. For example:<span epub:type="pagebreak" id="page_167"/></p>
<pre class="pre">Mean revenue by algorithm:    t-test, best vs rest:     
    Bare: $1148.59 ( 5.09)        Bare vs   DE: 0.05296 
      DE: $1127.86 ( 8.62)        Bare vs   GA: 0.00017 
      GA: $1112.06 ( 5.81)        Bare vs  GWO: 0.36133 
     GWO: $1141.01 ( 6.30)        Bare vs Jaya: 0.04042 
    Jaya: $1130.88 ( 6.20)        Bare vs  PSO: 0.01244 
     PSO: $1125.73 ( 6.47)        Bare vs   RO: 0.00000 
      RO: $1023.75 (10.03)</pre>
<p class="indent">The first part shows the mean revenue across the 10 runs by algorithm. The standard error of the mean (standard deviation divided by the square root of the number of samples, here 10) is in parentheses. Bare-bones PSO is the winner, averaging a daily revenue of $1,148.59 compared with RO’s meager performance of $1,023.75.</p>
<p class="indent">The right-hand part of the output requires some explanation. I wanted to compare the revenue across runs by algorithm. The code locates the highest performing algorithm, bare-bones PSO in this case, and runs a t-test with it against the others. A <em>t-test</em> is a hypothesis test that asks whether two datasets are plausibly from the same data generating process. The value shown is the p-value, the probability of the observed difference in the means and standard deviations of the two datasets (or greater), given they are from the same data generating process. If the p-value is high, then the two data-sets are likely from the same data generating process, meaning the test’s null hypothesis is likely valid. In this case, the bare-bones PSO result is not meaningfully different from the GWO result, because the p-value is 0.36.</p>
<p class="indent">The smaller the p-value, the more likely that the results aren’t from the same data generating process. For RO and GA, the p-values are very low, giving us confidence that the bare-bones PSO result is better. However, the other p-values are also low. So, is bare-bones PSO head and shoulders above all the others for this task, or was it just lucky this time around?</p>
<p class="indent">To find out, I ran <em>go_store</em> five more times and accumulated the output of <em>process_results.py</em> in the <em>results_per_run.txt</em> file. The best-performing algorithm varied across runs, but there were trends. For three of the six runs, Jaya was the top performer; for two, it was bare-bones PSO; and for one, GWO. The GA and RO results were always the worst. Looking at the p-values from the t-tests, Jaya, bare-bones PSO, and GWO are good algorithms to use for this task, with likely no meaningful difference between Jaya and bare-bones PSO.</p>
<p class="indent">The products in <em>products.pkl</em> are arranged in decreasing order of purchase probability but increasing order of price, meaning the least likely product to be purchased is the most expensive and vice versa. Therefore, we might expect to maximize profit by arranging the products so the least likely but most expensive is first and the most likely, least costly is last, at the back of the store. The order generated by a run of <em>store.py</em> is the swarm’s attempt to meet this ideal.</p>
<p class="indent">Run <em>product_order.py</em>, passing it an output directory (<em>output</em> created by <em>go_store</em>) followed by another directory name, like <em>orders</em>. You’ll generate a <span epub:type="pagebreak" id="page_168"/>collection of plots, one for each algorithm, showing the mean value of each product slot, from 0 through 23, for all 10 runs of each algorithm. Also plotted is the curve reflecting the ideal product ordering, the reverse of the product order in <em>products.pkl</em>.</p>
<p class="indent"><a href="ch05.xhtml#ch05fig12">Figure 5-12</a> shows the mean product cost by position in the store over 10 runs of each algorithm compared to the ideal ordering (smooth curve).</p>
<div class="image"><img alt="Image" id="ch05fig12" src="../images/05fig12.jpg"/></div>
<p class="figcap"><em>Figure 5-12: Comparing the mean swarm value by product order to the ideal. From top left to right: bare-bones PSO, DE, GA, GWO, Jaya, canonical PSO, RO.</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_169"/>First, note that in no way did the swarm search seek to match the ideal sequence. Instead, any match is an emergent effect of the swarms’ attempts to maximize daily profit.</p>
<p class="indent">Second, all algorithms except RO were effective at matching the order of the cheapest products. We also see this in <a href="ch05.xhtml#ch05fig11">Figure 5-11</a> by the consistency of milk placement compared to candy. Most of the variation between algorithms is in the order of the first few products. We also see this in the larger error bars for products near the front of the store compared to those at the back.</p>
<p class="indent">The placement of products near the front of the store might be more difficult because those products are the least often purchased. Both bare-bones PSO and Jaya did reasonably well with more expensive products, but one could argue that GWO more closely matched the ideal curve. This tells us that the differences between algorithms are subtle, at least when viewed in this fashion, though GA was nearly as poor at matching the early product order as RO.</p>
<h3 class="h3" id="ch00lev1_33"><strong>Exercises</strong></h3>
<p class="noindent">Had enough of swarm algorithms? Me neither. Here’s more to explore and contemplate:</p>
<ul>
<li class="noindent"><em>Circles.py</em> used <code>enforce="clip"</code> to pack circles. Change this to <code>enforce= "resample"</code>. If the results are suddenly different, why?</li>
<li class="noindent">Packing circles in a square is a similar problem to packing spheres in a cube. Modify a copy of <em>circles.py</em> to pack spheres in a cube, making a 2D problem a 3D problem. Run your code and compare it to the numbers in <em>sphere_dmin.png</em>. If you get stuck, take a peek at <em>spheres.py</em>.</li>
<li class="noindent">Placing cell towers on an empty map returns (primarily) non-overlapping towers. What sort of output do you get from <em>cell.py</em> if the map is particularly busy with relatively few allowed locations? You can make your own map or give <em>map_busy.png</em> in the <em>maps</em> directory a try. Can the swarm algorithms find places for the towers?</li>
<li class="noindent">The <em>enhance.py</em> file manipulates grayscale images. Modify it to enhance RGB images instead. A crude approach is in the file <em>process_rgb_images.py</em> in the <em>rgb</em> directory. This directory also contains some RGB images (<em>original</em>). Does <em>process_rgb_images.py</em> consistently produce good results? Why? Implement a new version that doesn’t enhance channels independently, but instead seeks a set of parameters that work best across all channels, perhaps by summing <em>F</em> per channel.</li>
<li class="noindent"><span epub:type="pagebreak" id="page_170"/>Modify <em>enhance.py</em> to use <code>ddof=0</code> instead of <code>ddof=1</code>—to use the biased variance and not the unbiased. Do you notice any difference in the results?</li>
<li class="noindent">The grocery store simulation used the same collection of shoppers for each iteration of the swarm. What happens to the results if the collection of shoppers is regenerated before each iteration? Do you expect this to make a difference?</li>
<li class="noindent">The simulation results presented in the grocery store simulation used 250 shoppers. What happens to my claim that Jaya and bare-bones PSO are well suited to the task if there are only five shoppers? Ten? Fifty?</li>
</ul>
<h3 class="h3" id="ch00lev1_34"><strong>Summary</strong></h3>
<p class="noindent">This chapter continued our exploration of swarm optimization algorithms. We learned how to pack circles in squares, place cell towers while avoiding restricted locations, implement a “make it pretty” filter, and mix optimization and simulation to develop a product placement plan for a grocery store.</p>
<p class="indent">We did all this using the same collection of algorithms. We didn’t change a single swarm intelligence or evolutionary algorithm to adapt it to the problem. Instead, casting the problem in the proper form enabled the direct application of the algorithms. This is a powerful ability that’s widely applicable. Many processes in the real world are, in the end, optimization problems, meaning swarm algorithms likely have a role to play. They are general-purpose algorithms, as are many machine learning algorithms to which we now turn.</p>
<p class="indent">The experiments in this chapter and the previous one introduced a powerful approach to general optimization problems. If we can cast the problem as finding the best position in a multidimensional space where each point represents a possible solution, then swarm intelligence and evolutionary algorithms are likely applicable. I can’t emphasize enough the usefulness of this concept.</p>
<p class="indent">We used a simple framework supporting a handful of standard swarm algorithms, of which there are hundreds to choose from—though not all are created equal. We designed the framework to be easy to use and pedagogical instead of performant. The framework acts as a stepping stone to more sophisticated tools, should you often return to swarm algorithms. If that’s the case, then consider exploring more advanced toolkits like these:</p>
<div class="bqparan">
<p class="noindentin"><strong>inspyred</strong>   <em><a href="https://pythonhosted.org/inspyred">https://pythonhosted.org/inspyred</a></em></p>
<p class="noindentin"><strong>pyswarms</strong>   <em><a href="https://github.com/ljvmiranda921/pyswarms">https://github.com/ljvmiranda921/pyswarms</a></em></p>
<p class="noindentin"><strong>DEAP</strong>   <em><a href="https://github.com/DEAP/deap">https://github.com/DEAP/deap</a></em></p>
</div>
<p class="noindent"><span epub:type="pagebreak" id="page_171"/>These toolkits support multiple swarm algorithms, both swarm intelligence and evolutionary, and are designed for performance. Toolkits for other languages include:</p>
<div class="bqparan">
<p class="noindentin"><strong>Java</strong>   <em><a href="https://cs.gmu.edu/~clab/projects/ecj">https://cs.gmu.edu/~clab/projects/ecj</a></em></p>
<p class="noindentin"><strong>C++</strong>   <em><a href="https://eodev.sourceforge.net">https://eodev.sourceforge.net</a></em></p>
</div>
<p class="indent">Swarm algorithms will make another appearance in <a href="ch07.xhtml">Chapter 7</a>, but for now, we’ll explore randomness in the world of artificial intelligence.<span epub:type="pagebreak" id="page_172"/></p>
</body></html>