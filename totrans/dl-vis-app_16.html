<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="313" id="Page_313"/>13</span><br/>
<span class="ChapterTitle">Neural Networks</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">Deep learning algorithms are based on building a network of connected computational elements. The fundamental unit of such networks is a small bundle of computation called an <em>artificial neuron</em>, though it’s often referred to simply as a <em>neuron</em>. The artificial neuron was inspired by human neurons, which are the nerve cells that make up our brain and central nervous system and are largely responsible for our cognitive abilities.</p>
<p>In this chapter, we see what artificial neurons look like and how to arrange them into networks. We then group them into layers, which create deep learning networks. We also look at various ways to configure the outputs of these artificial neurons so that they produce the most useful results.</p>
<h2 id="h1-500723c13-0001"><span epub:type="pagebreak" title="314" id="Page_314"/>Real Neurons</h2>
<p class="BodyFirst">In biology, the term <em>neuron</em> is applied to a wide variety of complex cells distributed throughout every human body. These cells all have similar structure and behavior, but they’re specialized for many different tasks. Neurons are sophisticated pieces of biology that use a mix of chemistry, physics, electricity, timing, proximity, and other means to perform their behaviors and communicate with one another (Julien 2011; Khanna 2018; Lodish et al. 2000; Purves et al. 2001). A highly simplified sketch of a neuron is shown in <a href="#figure13-1" id="figureanchor13-1">Figure 13-1</a>.</p>
<figure>
<img src="Images/F13001.png" alt="F13001" width="844" height="427"/>
<figcaption><p><a id="figure13-1">Figure 13-1</a>: A sketch of a highly simplified biological neuron (in red) with a few major structures identified. This neuron’s outputs are communicated to another neuron (in blue), only partially shown (adapted from Wikipedia 2020b).</p></figcaption>
</figure>
<p>Neurons are information processing machines. One type of information arrives in the form of chemicals called <em>neurotransmitters</em> that temporarily <em>bind</em>, or attach, onto <em>receptor sites</em> located on the neuron (Goldberg 2015). Let’s sketch out what happens next in the broadest possible terms.</p>
<p>The chemicals that bind to the receptor sites cause electrical signals to travel into the body of the neuron. Each of these signals can be either positive or negative. All of the electrical signals arriving at the neuron’s body over a short interval of time are added together and then compared to a <em>threshold</em>. If the total exceeds that threshold, a new signal is sent along the axon to another part of the neuron, causing specific amounts of neurotransmitters to be released into the environment. These molecules then bind with other neurons, and the process repeats.</p>
<p>In this way, information is propagated and modified as it flows through the densely connected network of neurons in the brain and central nervous system. If two neurons are physically close enough to each other that one <span epub:type="pagebreak" title="315" id="Page_315"/>can receive the neurotransmitters released by the other, we say that the neurons are <em>connected</em>, even though they may not be actually touching. There is some evidence that the particular pattern of connections between neurons is as essential to cognition and identity as the neurons themselves (Sporns, Tononi, and Kötter 2005; Seung 2013). A map of an individual’s neuronal connections is called their <em>connectome</em>. Connectomes are as unique as fingerprints or iris patterns.</p>
<p>Although real neurons and their surrounding environment are tremendously complex and subtle, the basic mechanism described here has an appealing elegance. Responding to this, some scientists have attempted to emulate or duplicate the brain by creating enormous numbers of simplified neurons and their environment, in hardware or software, hoping that interesting behavior will emerge (Furber 2012; Timmer 2014). So far, this has not delivered results that most people would call intelligence.</p>
<p>But we can connect up simplified neurons in specific ways to produce great results on a wide range of problems. Those are the types of structures that will be our focus in this chapter, and the rest of this book.</p>
<h2 id="h1-500723c13-0002">Artificial Neurons</h2>
<p class="BodyFirst">The “neurons” we use in machine learning are inspired by real neurons in the same way that a stick figure drawing is inspired by a human body. There’s a resemblance, but only in the most general sense. Almost all of the details are lost along the way, and we’re left with something that’s more of a reminder of the original, rather than even a simplified copy.</p>
<p>This has led to some confusion, particularly in the popular press, where “neural network” is sometimes used as a synonym for “electronic brain,” and from there, it’s only a short step to general intelligence, consciousness, emotions, and perhaps world domination and the elimination of human life. In reality, the neurons we use are so abstracted and simplified from real neurons that many people prefer instead to call them by the more generic name of <em>units</em>. But for better or worse, the word <em>neuron</em>, the phrase <em>neural net</em>, and all the related language are apparently here to stay, so we use them in this book as well.  </p>
<h3 id="h2-500723c13-0001">The Perceptron</h3>
<p class="BodyFirst">The history of artificial neurons may be said to begin in 1943, with the publication of a paper that presented a massively simplified abstraction of a neuron’s basic functions in mathematical form, and described how multiple instances of this object could be connected into a <em>network</em>, or <em>net</em>. The big contribution of this paper was that it proved mathematically that such a network could implement any idea expressed in the language of mathematical logic (McCulloch and Pitts 1943). Since mathematical logic is the basis of machine calculation, that means neurons could perform mathematics. This was a big deal, because it provided a bridge between the fields of math, logic, computing, and neurobiology.</p>
<p><span epub:type="pagebreak" title="316" id="Page_316"/>Building on that insight, in 1957 the <em>perceptron</em> was proposed as a simplified mathematical model of a neuron (Rosenblatt 1962). <a href="#figure13-2" id="figureanchor13-2">Figure 13-2</a> is a block diagram of a single perceptron with four inputs.</p>
<figure>
<img src="Images/f13002.png" alt="f13002" width="694" height="670"/>
<figcaption><p><a id="figure13-2">Figure 13-2</a>: A four-input perceptron </p></figcaption>
</figure>
<p>Every input to a perceptron is represented by a single floating-point number. Each input is multiplied by a corresponding floating-point number called a <em>weight</em>. The results of these multiplications are all added together. Finally, we compare the result to a threshold value. If the result of the summation is greater than 0, the perceptron produces an output of +1, otherwise it’s −1 (in some versions, the outputs are 1 and 0, rather than +1 and −1).</p>
<p>Though the perceptron is a vastly simplified version of a real neuron, it’s proven to be a terrific building block for deep learning systems. </p>
<p>The history of the perceptron is an interesting part of the culture of machine learning, so let’s look at just a couple of its key events; more complete versions may be found online (Estebon 1997; Wikipedia 2020a).</p>
<p>After the principles of the perceptron had been verified in software, a perceptron-based computer was built at Cornell University in 1958. It was a rack of wire-wrapped boards the size of a refrigerator, called the Mark <span epub:type="pagebreak" title="317" id="Page_317"/>I Perceptron (Wikipedia 2020c). The device was built to process images, using a grid of 400 photocells that could digitize an image at a resolution of 20 by 20 pixels (the word <em>pixel</em> hadn’t yet been coined). The weight applied to each input of the perceptron was set by turning a knob that controlled an electrical component called a potentiometer. To automate the learning process, electric motors were attached to the potentiometers so the device could literally turn its own knobs to adjust its weights and thereby change its calculations, and thus its output. The theory guaranteed that, with the right data, the system could learn to separate two different classes of inputs that could be split with a straight line.</p>
<p>Unfortunately, not many interesting problems involve sets of data that are separated by a straight line, and it proved hard to generalize the technique to more complicated arrangements of data. After a few years of stalled progress, a book proved that the original perceptron technique was fundamentally limited (Minsky and Papert 1969). It showed that the lack of progress wasn’t due to a lack of imagination, but the result of theoretical limits built into the structure of a perceptron. Most interesting problems, and even some very simple ones, were provably beyond the ability of a perceptron to solve.</p>
<p>This result seemed to signal the end of perceptrons for many people, and a popular consensus formed that the perceptron approach was a dead end. Enthusiasm, interest, and funding all dried up, and most people directed their research to other problems. This period, which lasted roughly between the 1970s and 1990s, was called the <em>AI winter</em>.</p>
<p>But despite a widespread interpretation that the perceptron book had closed the door on perceptrons in general, in fact it had only shown the limitations of how they’d been used up to that time. Some people thought that writing off the whole idea was an overreaction and that perhaps the perceptron could still be a useful tool if applied in a different way. It took roughly a decade and a half, but this point of view eventually bore fruit when researchers combined perceptrons into larger structures and showed how to train them (Rumelhart, Hinton, and Williams 1986). These combinations easily surpassed the limitations of any single unit. A series of papers then showed that careful arrangements of multiple perceptrons, beefed up with a few minor changes, could solve complex and interesting problems.</p>
<p>This discovery rekindled interest in the field, and soon research with perceptrons became a hot topic once again, producing a steady stream of interesting results that have led to the deep learning systems we use today. Perceptrons remain a core component of many modern deep learning systems.</p>
<h3 id="h2-500723c13-0002">Modern Artificial Neurons</h3>
<p class="BodyFirst">The neurons we use in modern neural networks are only slightly generalized from the original perceptrons. There are two changes: one at the input, and one at the output. These modified structures are still sometimes called perceptrons, but there’s rarely any confusion because the new versions are used almost exclusively. More commonly, they’re just called <em>neurons</em>. Let’s look at these two changes.</p>
<p><span epub:type="pagebreak" title="318" id="Page_318"/>The first change to the perceptron of <a href="#figure13-2">Figure 13-2</a> is to provide each neuron with one more input, which we call the <em>bias</em>. This is a number that doesn’t come from the output of a previous neuron. Instead, it’s a number that’s directly added into the sum of all the weighted inputs. Every neuron has its own bias. <a href="#figure13-3" id="figureanchor13-3">Figure 13-3</a> shows our original perceptron, but with the bias term included.</p>
<figure>
<img src="Images/F13003.png" alt="F13003" width="694" height="665"/>
<figcaption><p><a id="figure13-3">Figure 13-3</a>: The perceptron of <a href="#figure13-2">Figure 13-2</a>, but now with a bias term</p></figcaption>
</figure>
<p>Our second change to the perceptron of <a href="#figure13-2">Figure 13-2</a> is at the output. The perceptron in that figure tests the sum against a threshold of 0, and then produces either a −1 or 1 (or 0 or 1). We generalize this by replacing the testing step with a mathematical function that takes the sum (including the bias) as input and returns a new floating-point value as output. Because the output of a real neuron is called its <em>activation</em>, we call this function that calculates the artificial neuron’s output the <em>activation function</em>. The little test shown in <a href="#figure13-2">Figure 13-2</a> is an activation function, but one that’s rarely used anymore. Later in this chapter we’ll survey a variety of activation functions that have proved to be popular and useful in practice.</p>
<h2 id="h1-500723c13-0003"><span epub:type="pagebreak" title="319" id="Page_319"/>Drawing the Neurons</h2>
<p class="BodyFirst">Let’s identify a convention that’s used by most drawings of artificial neurons. In <a href="#figure13-3">Figure 13-3</a> we showed the weights explicitly, and we also included the multiplication steps to show how the weights multiply the inputs. This takes a lot of room on the page. When we draw diagrams with a lot of neurons, all of these details can make for a cluttered and dense figure. So instead, in virtually all neural network diagrams, the weights and their multiplications are implied.</p>
<p>This is important, and bears repeating: in neural network diagrams, the weights, and the steps where they multiply the inputs, are not drawn. Instead, we’re supposed to know that they are there and mentally include them in the diagram. If we show the weights at all, we typically label the lines from the inputs with the name of the weight. <a href="#figure13-4" id="figureanchor13-4">Figure 13-4</a> shows <a href="#figure13-3">Figure 13-3</a> drawn in this style.</p>
<figure>
<img src="Images/F13004.png" alt="F13004" width="554" height="570"/>
<figcaption><p><a id="figure13-4">Figure 13-4</a>: A neuron is often drawn with the weights on the arrows.</p></figcaption>
</figure>
<p>In <a href="#figure13-4">Figure 13-4</a>, we also changed the threshold test at the end to a little picture. This is a drawing of a function called a <em>step</em>, and it’s meant to give us a visual reminder that any activation function can go into that spot. Basically, a number goes into that step, and a new number comes out, determined by whichever function we choose for the job.</p>
<p><span epub:type="pagebreak" title="320" id="Page_320"/>We usually simplify things again. This time we omit the bias by pretending it’s one of the inputs. This not only makes the diagram simpler, but it makes the math simpler as well, which, in this case, also leads to more efficient algorithms. This simplification is called the <em>bias trick</em> (the word <em>trick</em> comes from mathematics, where it’s a complimentary term sometimes used for a clever simplification of a problem). Rather than change the value of the bias, we set the bias to always be 1, and change the weight applied to it before it gets summed up with the other inputs. <a href="#figure13-5" id="figureanchor13-5">Figure 13-5</a> shows this change in labeling. Though the bias term is always 1 and only its weight can change, we usually ignore the distinction and just talk about the value of the bias.</p>
<figure>
<img src="Images/F13005.png" alt="F13005" width="575" height="565"/>
<figcaption><p><a id="figure13-5">Figure 13-5</a>: The bias trick in action. Rather than show the bias term explicitly, as in <a href="#figure13-4">Figure 13-4</a>, we pretend it’s another input with its own weight.</p></figcaption>
</figure>
<p>We want our artificial neuron diagrams to be as simple as possible because when we start building up networks we’ll be showing lots of neurons at once, so most of these diagrams take two additional steps of simplification. First, they don’t show the bias at all. We’re supposed to remember that the bias is included (along with its weight), but it’s not shown. Second, the weights are often omitted as well, as in <a href="#figure13-6" id="figureanchor13-6">Figure 13-6</a>. This is unfortunate, because the weights are the most important part of the neuron for us. The reason for this is that they are the only things we can change during training. Despite being left out of most drawings, they’re so essential that we repeat the key idea yet again: <em>even though we don’t show the weights explicitly, the weights are always there.</em></p>
<span epub:type="pagebreak" title="321" id="Page_321"/><figure>
<img src="Images/F13006.png" alt="F13006" width="513" height="259"/>
<figcaption><p><a id="figure13-6">Figure 13-6</a>: A typical drawing of an artificial neuron. The bias term and the weights are not shown, but they are definitely present. </p></figcaption>
</figure>
<p>Like real neurons, artificial neurons can be wired up in networks, where each input comes from the output of another neuron. When we connect neurons together into networks, we draw “wires” to connect one neuron’s output to one or more other neurons’ inputs. <a href="#figure13-7" id="figureanchor13-7">Figure 13-7</a> shows this idea visually.</p>
<figure>
<img src="Images/F13007.png" alt="F13007" width="844" height="387"/>
<figcaption><p><a id="figure13-7">Figure 13-7</a>: A piece of a larger network of artificial neurons. Each neuron receives its inputs from other neurons. The dashed lines show connections to and from outside this little cluster.</p></figcaption>
</figure>
<p>This is a <em>neural network</em>. Usually the goal of a network like <a href="#figure13-7">Figure 13-7</a> is to produce one or more values as outputs. We’ll see later how we can interpret the numbers at the outputs in meaningful ways.</p>
<p>Even though we’ve said that we usually don’t draw the weights, in discussions, sometimes it’s useful to refer to individual weights. Let’s look at a common convention for weight names. <a href="#figure13-8" id="figureanchor13-8">Figure 13-8</a> shows six neurons. For convenience, we’ve labeled each neuron with a letter. Each weight corresponds to how the output of one specific neuron is changed on its way to another specific neuron. Each of these connections is shown as a line in the <span epub:type="pagebreak" title="322" id="Page_322"/>figure. To name a weight, we combine the name of the output neuron with the input neuron. For example, the weight that multiplies the output of A before it’s used by D is called AD.</p>
<figure>
<img src="Images/F13008.png" alt="F13008" width="440" height="330"/>
<figcaption><p><a id="figure13-8">Figure 13-8</a>: The weights are named by combining the names of the output and input neurons. </p></figcaption>
</figure>
<p>From a structural point of view, it makes no difference whether we draw the weights inside each neuron, or on the wires that carry values to it. Various authors assume one or the other if it makes their discussion easier to follow, but we can always take the other viewpoint if we like.</p>
<p>In <a href="#figure13-8">Figure 13-8</a>, we named the weight from neuron A to neuron D as AD. Some authors flip this around and write DA, because it’s a more direct match to how we often write the equations. It’s always worth a moment to check which order is being used in diagrams like this.</p>
<h2 id="h1-500723c13-0004">Feed-Forward Networks</h2>
<p class="BodyFirst"><a href="#figure13-7">Figure 13-7</a> showed a neural network with no apparent structure. A key feature of deep learning<em> </em>is that we arrange our neurons into <em>layers</em>. Typically, the neurons on each layer get their inputs only from the previous layer and send their outputs only to the following layer, and neurons do not communicate with other neurons on the same layer (there are, as always, exceptions to these rules).</p>
<p>This organization allows us to process data in stages, with each layer of neurons building on the work done by the previous stage. By analogy, consider an office tower of many floors. The people on any given floor receive their work only from the people on the floor immediately below them, and they pass their work on only to the people on the floor immediately above them. In this analogy, each floor is a layer, and the people are the neurons on that layer.</p>
<p>We say that this type of arrangement processes the data <em>hierarchically</em>. There is some evidence that the human brain is structured to handle some tasks hierarchically, including the processing of sensory data like vision and hearing (Meunier et al. 2009; Serre 2014). But here again, the connection <span epub:type="pagebreak" title="323" id="Page_323"/>between our computer models and real biology is much closer to inspiration than emulation.</p>
<p>It’s amazing that hooking up neurons in a series of layers produces anything useful. As we saw earlier, a single artificial neuron can hardly manage to do anything. It takes a bunch of numerical inputs, weights them, adds the results together, and then passes that result through a little function. This process can identify a straight line that splits a couple of clumps of data, and not much else. But if we assemble many thousands of these little units into layers and use some clever ideas to train them, then, working together, they’re capable of recognizing speech, identifying faces in photographs, and even beating humans at games of logic and skill.</p>
<p>The key to this is organization. Over time people have developed a number of ways to organize layers of neurons, resulting in a collection of common layer structures. The most common network structure arranges the neurons so that information flows in only one direction. We call this a <em>feed-forward network</em> because the data is flowing forward, with earlier neurons feeding, or delivering values to, later neurons. The art of designing a deep learning system lies in choosing the right sequence of layers, and the right hyperparameters, to create the basic architecture. To build a useful architecture for any given application, we need to understand how the neurons relate to one another. Let’s now look at how collections of neurons communicate, and how to set up the initial weights before learning begins. </p>
<h2 id="h1-500723c13-0005">Neural Network Graphs</h2>
<p class="BodyFirst">We usually represent neural networks as <em>graphs</em>. The study of graphs is so large that it is considered a field of mathematics in its own right, called <em>graph theory</em> (Trudeau 1994). Here, we’re going to stick to the basic ideas of graphs, because that’s all we need to organize our neural networks. Though we know we’ll usually be working with layers, let’s start out with some general graphs first, such as those shown in <a href="#figure13-9" id="figureanchor13-9">Figure 13-9</a>. </p>
<p>A graph is made up of <em>nodes</em> (also called <em>vertices</em> or <em>elements</em>), here shown as circles. In this book, nodes are usually neurons, and throughout this book, we occasionally refer to one or more neurons in a network like this as nodes. The nodes are connected by arrows called <em>edges</em> (also called <em>arcs</em>, <em>wires</em>, or simply <em>lines</em>). The arrowhead is often left off when the direction of information flow is consistent in the drawing, which is almost always left to right or bottom to top. Information flows along the edges, carrying the output of one node to the inputs of others. Since information flows in only one direction on each edge, we sometimes call this kind of graph a <em>directed graph.</em></p>
<p>The general idea is that we start things off by putting data into the input node or nodes, and then it flows through the edges, visiting nodes where it is transformed or changed, until it reaches the output node or nodes. No data ever returns to a node once it has left. In other words, information only flows forward, and there are no loops, or <em>cycles</em>. This kind of graph is like a little factory. Raw materials come in one end, and pass through machines that manipulate and combine them, ultimately producing one or more finished products at the end. </p>
<span epub:type="pagebreak" title="324" id="Page_324"/><figure>
<img src="Images/F13009.png" alt="F13009" width="845" height="500"/>
<figcaption><p><a id="figure13-9">Figure 13-9</a>: Two neural networks drawn as graphs. Data flows from node to node along the edges, following the arrows. When the edges are not labeled with an arrow, data usually flows left-to-right or bottom-to-top. (a) Mostly left-to-right flow. (b) Mostly bottom-to-top flow.</p></figcaption>
</figure>
<p>We say that a node near the inputs in <a href="#figure13-9">Figure 13-9</a>(a) is <em>before</em> a node nearer to the outputs, which comes <em>after</em> it. In <a href="#figure13-9">Figure 13-9</a>(b), we’d say a node near the inputs is <em>below</em> a node near the outputs, which is <em>above</em> it. Sometimes this below/above language is used even when the graph is drawn left to right, which can be confusing. It can help to think of <em>below</em> as “closer to the inputs,” and <em>above</em> as “closer to the outputs.”</p>
<p>We also sometimes say that if data flows from one node to another (let’s say it flows from A to B), then node A is an <em>ancestor</em> or <em>parent</em> of B, and node B is a <em>descendant</em> or <em>child</em> of A. </p>
<p>A common rule in neural networks is that there are no loops. This means that data coming out of a node can never make its way back into that same node, no matter how circuitous a path it follows. The formal name for this kind of graph is a <em>directed acyclic graph</em> (or <em>DAG</em>, pronounced to rhyme with “drag”). The word <em>directed</em> here means that the edges have arrows (which may only be implied, as we mentioned earlier). The word <em>acyclic</em> means there are no <em>cycles</em>, or loops. As always, there are exceptions to the rules, but they’re rare. We’ll see one such exception when we discuss recurrent neural networks (RNNs) in Chapter 19.</p>
<p>DAGs are popular in many fields, including machine learning, because they are significantly easier to understand, analyze, and design than arbitrary graphs that have loops. Including loops can introduce <em>feedback</em>, where a node’s output is returned to its input. Anyone who’s moved a live <span epub:type="pagebreak" title="325" id="Page_325"/>microphone too close to a speaker is familiar with how quickly feedback can grow out of control. The acyclic nature of a DAG naturally avoids the feedback problem, which saves us from dealing with this complex issue.</p>
<p>Recall that a graph or network in which data only flows forward from inputs to outputs is called <em>feed-forward</em>. In Chapter 14, we’ll see that a key step in training neural networks involves temporarily flipping the arrows around, sending a particular type of information from the output nodes back to the input nodes. Although the normal flow of data is still feed-forward, when we push data through it backward, generally we call that a <em>feed-backward</em>, <em>backward-flow</em>, or <em>reverse-feed</em> algorithm. We reserve the word <em>feedback</em> for situations in which a loop in the graph can enable a node to receive its own output as input. As we’ve said, we generally avoid feedback in neural networks.</p>
<p>Interpreting graphs like those in <a href="#figure13-9">Figure 13-9</a> usually means picturing the information as it flows along the edges, from one node to the next. But this picture only makes sense if we make some conventional assumptions. Let’s look at those now.</p>
<p>Though we often use the word <em>flow</em> in various forms when referring to how data moves through the graph, this isn’t like the flow of water through pipes. Water flowing through pipes is a <em>continuous</em> process: new molecules of water flow through the pipes at every moment. The graphs we work with (and the neural networks they represent) are <em>discrete</em>: information arrives one chunk at a time, like text messages.</p>
<p>Recall from <a href="#figure13-5">Figure 13-5</a> that we can draw a neural network by placing a weight on each edge (rather than inside a neuron). We call this style of the network a <em>weighted graph.</em> As we saw in <a href="#figure13-6">Figure 13-6</a>, we rarely draw the weights explicitly, but they are implied. It is always the case that in any neural network graph, even if no weights are explicitly shown, we are to understand that a unique weight is on each edge and as a value moves from one neuron to another along that edge that value is multiplied by the weight.</p>
<h2 id="h1-500723c13-0006">Initializing the Weights</h2>
<p class="BodyFirst">Teaching a neural network involves gradually improving the weights. The process begins when we assign initial values to the weights. How should we pick these starting values? It turns out that, in practice, how we initialize the weights can have a big effect on how quickly our network learns.</p>
<p>Researchers have developed theories for good ways to pick the initial values for the weights, and the various algorithms that have proved most useful are each named after the lead authors on the publications that describe them. The <em>LeCun Uniform</em>,<em> Glorot Uniform</em> (or <em>Xavier Uniform</em>), and <em>He Uniform</em> algorithms are all based on selecting initial values from a uniform distribution (LeCun et al. 1998; Glorot and Bengio 2010; He et al. 2015). It probably won’t be much of a surprise that the similarly named <em>LeCun Normal</em>, <em>Glorot Normal</em> (or <em>Xavier Normal</em>), and <em>He Normal</em> initialization methods draw their values from a normal distribution.</p>
<p><span epub:type="pagebreak" title="326" id="Page_326"/>We don’t need to get into the math behind these algorithms. Happily, modern deep learning libraries offer each of these schemes, plus variations on them. Often the technique used by the library by default works great, so we rarely need to explicitly choose how to initialize the weights.</p>
<h2 id="h1-500723c13-0007">Deep Networks</h2>
<p class="BodyFirst">Of the many possible ways to organize neurons in a network, placing them in a series of layers has proven to be both flexible and extremely powerful. Typically, neurons within a layer aren’t connected to one another. Their inputs come from the previous layer, and their outputs go to the next layer.</p>
<p>In fact, the phrase <em>deep learning</em> comes from this structure. If we imagine many layers drawn side by side, we might call the network “wide.” If they were drawn vertically and we stood at the bottom looking up, we might call it “tall.” If we stood at the top and looked down, we might call it “deep.” And that’s all that <em>deep learning</em> means: a network made of a series of layers that we often draw vertically. </p>
<p>A result of organizing neurons in layers is that we can analyze data hierarchically. The early layers process the raw input data, and each subsequent layer is able to use information from neurons on the previous layer to process larger chunks of data. For example, when considering a photograph, the first layer usually looks at the individual pixels. The next layer looks at groups of pixels, the one after that looks at groups of those groups, and so on. Early layers might notice that some pixels are darker than others, whereas later layers might notice that a clump of pixels looks like an eye, and a much later layer might notice the collection of shapes that reveal that the whole image shows a tiger.</p>
<p><a href="#figure13-10" id="figureanchor13-10">Figure 13-10</a> shows an example of a deep learning architecture using three layers.</p>
<figure>
<img src="Images/F13010.png" alt="F13010" width="531" height="400"/>
<figcaption><p><a id="figure13-10">Figure 13-10</a>: A deep learning network</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="327" id="Page_327"/>When we draw the layers vertically, as in <a href="#figure13-10">Figure 13-10</a>, the inputs are almost always drawn at the bottom, and the outputs where we collect our results are almost always drawn at the top.</p>
<p>In <a href="#figure13-10">Figure 13-10</a>, all three layers contain neurons. In practical systems, we usually use lots of other kinds of layers, which we might group together as <em>support layers</em>. We’ll see many such layers in later chapters. When we count the number of layers in a network, we usually don’t count these support layers. <a href="#figure13-10">Figure 13-10</a> would be described as a deep network of three layers.</p>
<p>The topmost layer that contains neurons (Layer 3 in <a href="#figure13-10">Figure 13-10</a>) is called the <em>output layer</em>. </p>
<p>We would probably expect that Layer 1 in <a href="#figure13-10">Figure 13-10</a> would be called the <em>input layer</em>, but it’s not. In a quirk of terminology, the term <em>input layer</em> is applied to the bottom of the network, labeled “Inputs” in <a href="#figure13-10">Figure 13-10</a>. There’s no processing in this “layer.” Instead it’s just the memory where the input values reside. The input layer is an example of a support layer because it has no neurons, and therefore isn’t included when we count the layers in a network. The number of layers we count is called the network’s <em>depth</em>.</p>
<p>If we imagine standing above the top of <a href="#figure13-10">Figure 13-10</a> and looking down, we only see the output layer. If we imagine we are below the bottom and looking up, we only see the input layer. The layers in between aren’t visible to us. Each of these layers between the input and output is called a <em>hidden layer</em>.</p>
<p>Sometimes the stack is drawn left to right, as in <a href="#figure13-11" id="figureanchor13-11">Figure 13-11</a>.</p>
<figure>
<img src="Images/F13011.png" alt="F13011" width="826" height="433"/>
<figcaption><p><a id="figure13-11">Figure 13-11</a>: The same deep network of <a href="#figure13-10">Figure 13-10</a>, but drawn with data flowing left to right</p></figcaption>
</figure>
<p>Even when drawn this way, we still use terms that refer to the vertical orientation. Authors might say that Layer 2 is “above” Layer 1, and “below” Layer 3. We can always keep things straight regardless of how the diagram is drawn if we think of “above” or “higher” as referring to a layer closer to the outputs, and “below” or “lower” as meaning closer to the inputs.</p>
<h2 id="h1-500723c13-0008"><span epub:type="pagebreak" title="328" id="Page_328"/>Fully Connected Layers</h2>
<p class="BodyFirst">A <em>fully connected layer</em> (also called an <em>FC</em>, <em>linear</em>, or <em>dense</em> layer) is a set of neurons that each receive an input from <em>every</em> neuron on the previous layer. For example, if there are three neurons in a dense layer, and four neurons in the preceding layer, then each neuron in the dense layer has four inputs, one from each neuron in the preceding layer, for a total of 3 × 4 = 12 connections, each with an associated weight.</p>
<p><a href="#figure13-12" id="figureanchor13-12">Figure 13-12</a>(a) shows a diagram of a fully connected layer with three neurons, coming after a layer with four neurons.</p>
<figure>
<img src="Images/F13012.png" alt="F13012" width="514" height="271"/>
<figcaption><p><a id="figure13-12">Figure 13-12</a>: A fully connected layer. (a) The colored neurons make up a fully connected layer. Each of the neurons in this layer receives an input from every neuron in the previous layer. (b) Our schematic symbol for a fully connected layer.</p></figcaption>
</figure>
<p><a href="#figure13-12">Figure 13-12</a>(b) shows a schematic shorthand that we’ll use for fully connected layers. The idea is that two neurons are at the top and bottom of the symbol, and the vertical and diagonal lines are the four connections between them. Next to the symbol, we identify how many neurons are in the layer, as we’ve done here with the number 3. When it’s relevant, this is also where we identify that layer’s activation function. If a layer is made up of only dense layers, it is sometimes called a <em>fully connected network</em>, or, in a throwback to earlier terminology, a <em>multilayer perceptron</em> <em>(MLP)</em>.</p>
<p>In later chapters, we’ll see many other types of layers that help us organize our neurons in useful ways. For example, <em>convolution layers</em> and <em>pooling layers</em> have proven very useful for image processing tasks, and we’ll give them a lot of attention.</p>
<h2 id="h1-500723c13-0009">Tensors</h2>
<p class="BodyFirst">We’ve seen that a deep learning system is built from a sequence of layers. And though the output of any neuron is a single number, we often want to talk about the output of an entire layer at once. The key idea that characterizes this collection of output numbers is its shape. Let’s see what that means.</p>
<p>If the layer contains a single neuron, the layer’s output is just a single number. We might describe this as an array, or a list, with one element. Mathematically, we can call this a <em>zero-dimensional array</em>. The number of <span epub:type="pagebreak" title="329" id="Page_329"/>dimensions in an array tells us how many indices we need to use to identify an element. Since a single number needs no indices, that array has zero dimensions.</p>
<p>If we have multiple neurons in a layer, then we can describe their collective output as a list of all the values. Since we need one index to identify a particular output value in this list, this is a one-dimensional (1D) array. <a href="#figure13-13" id="figureanchor13-13">Figure 13-13</a>(a) shows such an array containing 12 elements.</p>
<figure>
<img src="Images/F13013.png" alt="F13013" width="844" height="134"/>
<figcaption><p><a id="figure13-13">Figure 13-13</a>: Three tensors, each with 12 elements. (a) A 1D tensor is a list. (b) A 2D tensor is a grid. (c) A 3D tensor is a volume. In all cases, and in higher-dimensional cases as well, there are no holes and no elements stick out from the block.</p></figcaption>
</figure>
<p>We frequently organize our data into other box-like shapes. For instance, if the input to our system is a black and white image, it can be represented as a 2D array, as in <a href="#figure13-13">Figure 13-13</a>(b), indexed by x and y positions. If it’s a color image, then it can be represented as a 3D array, indexed by x position, y position, and color channel. A 3D shape is shown in <a href="#figure13-13">Figure 13-13</a>(c).</p>
<p>We frequently call a 1D shape an <em>array</em>, <em>list</em>, or <em>vector</em>. To describe a 2D shape we often use the terms <em>grid</em> or <em>matrix</em>, and we can describe a 3D shape as a <em>volume </em>or <em>block</em>. We will often use arrays with even more dimensions. Rather than create a mountain of new terms, we use a single term for any collection of numbers arranged in a box shape with any number of dimensions: a <em>tensor</em> (pronounced ten′-sir).</p>
<p>A tensor is merely a block of numbers with a given number of dimensions and a size in each dimension. It has no holes and no bits sticking out. The term <em>tensor</em> has a more complex meaning in some fields of math and physics, but in machine learning, we use this word to mean a collection of numbers organized into a multidimensional block. Taken together, the number of dimensions and the size in each dimension provide the <em>shape</em> of the tensor.</p>
<p>We often refer to a network’s <em>input tensor</em> (meaning all the input values), and its <em>output tensor</em> (meaning all the output values). The outputs of internal (or hidden) layers have no special name, so we usually say something like “the tensor produced by layer 3” to refer to the multidimensional array of numbers coming out of the neurons on layer 3.</p>
<h2 id="h1-500723c13-0010">Preventing Network Collapse </h2>
<p class="BodyFirst">Earlier we promised to return to activation functions. Let’s look at them now.</p>
<p>Each activation function, while a small piece of the overall structure, is critical to a successful neural network. Without activation functions, the neurons in a network combine, or <em>collapse</em>, into the equivalent of a single neuron. And, as we saw earlier, one neuron has very little computational power.</p>
<p><span epub:type="pagebreak" title="330" id="Page_330"/>Let’s see how a network collapses when it doesn’t have activation functions. <a href="#figure13-14" id="figureanchor13-14">Figure 13-14</a> shows a little network with two inputs (A and B), and five neurons (E through G) on three layers. Every neuron receives an input from every neuron on the previous layer, and each connection has a weight, giving us a total of ten weights, shown in red.</p>
<figure>
<img src="Images/F13014.png" alt="F13014" width="485" height="182"/>
<figcaption><p><a id="figure13-14">Figure 13-14</a>: A little network of two inputs, five neurons, and ten weights</p></figcaption>
</figure>
<p>Let’s suppose for the moment that these neurons don’t have activation functions. Then we can write the output of each neuron as a weighted sum of its inputs, as in <a href="#figure13-15" id="figureanchor13-15">Figure 13-15</a>. In this figure, we’re using the mathematical convention of leaving out the multiplication sign when possible, so 2A is shorthand for 2 × A. </p>
<figure>
<img src="Images/F13015.png" alt="F13015" width="483" height="243"/>
<figcaption><p><a id="figure13-15">Figure 13-15</a>: Each neuron is labeled with the value of its output.</p></figcaption>
</figure>
<p>The outputs of C and D depend only on A and B. Similarly, the outputs of E and F only depend on the outputs of C and D, which means that they, too, ultimately depend only on A and B. The same argument holds for G. If we start with the expression for G, plug in the values for E and F, and then plug in the values for C and D, we get a big expression in terms of A and B. If we do that and simplify, we find that the output of G is 78A + 86B. We can write this as a single neuron with two new weights, as shown in <a href="#figure13-16" id="figureanchor13-16">Figure 13-16</a>.</p>
<p>This output of G in <a href="#figure13-16">Figure 13-16</a> is exactly the same as the output of G in <a href="#figure13-14">Figure 13-14</a>. Our whole network has collapsed into a single neuron! </p>
<p>No matter how big or complicated our neural network is, if it has no activation functions, then it will always be equivalent to a single neuron. This is bad news if we want our network to be able to do anything more than what one neuron can do. </p>
<span epub:type="pagebreak" title="331" id="Page_331"/><figure>
<img src="Images/F13016.png" alt="F13016" width="230" height="160"/>
<figcaption><p><a id="figure13-16">Figure 13-16</a>: This network’s output is exactly the same as the output in <a href="#figure13-14">Figure 13-14</a>.</p></figcaption>
</figure>
<p>In mathematical language, we say that our fully connected network collapsed because it only used addition and multiplication, which are in the category of <em>linear functions</em>. Linear functions can combine as we just saw, but <em>nonlinear functions </em>are fundamentally different and don’t combine this way. By designing activation functions to use <em>nonlinear </em>operations, we prevent this kind of collapse. We sometimes call an activation function a <em>nonlinearity</em>.</p>
<p>There are many different types of activation functions, each producing different results. Generally speaking, the variety is there because in some situations, some functions can run into numerical trouble, making training run more slowly than it should, or even cease altogether. If that happens, we can substitute an alternative activation function that avoids the problem (though of course it has its own weak points).</p>
<p>In practice, a handful of activation functions are all we usually use. When reading the literature and looking at other people’s networks, we sometimes see the rarer activation function. Let’s survey the functions that by most major libraries usually provide and then gather together the most common ones.</p>
<h2 id="h1-500723c13-0011">Activation Functions</h2>
<p class="BodyFirst">An <em>activation function</em> (sometimes also called a <em>transfer function</em>, or a <em>non-linearity</em>) takes a floating-point number as input and returns a new floating-point number as output. We can define these functions by drawing them as little graphs, without any equations or code. The horizontal, or X, axis is the input value, and the vertical, or Y, axis is the output value. To find the output for any input, we locate the input along the X axis, and move directly upward until we hit the curve. That’s the output value.</p>
<p>In theory, we can apply a different activation function to every neuron in our network, but in practice, we usually assign the same activation function to all the neurons in each layer.</p>
<h3 id="h2-500723c13-0003">Straight-Line Functions</h3>
<p class="BodyFirst">Let’s first look at activation functions that are made up of one or more straight lines. <a href="#figure13-17" id="figureanchor13-17">Figure 13-17</a> shows a few “curves” that are just straight lines. </p>
<p>Let’s look at the leftmost example in <a href="#figure13-17">Figure 13-17</a>. If we pick any point on the X axis, and go vertically up until we hit the line, the value of that intersection on the Y axis is the same as the value on the X axis. The output, or y value, of this curve is always the same as the input, or x value. We call this the <em>identity function</em>.</p>
<span epub:type="pagebreak" title="332" id="Page_332"/><figure>
<img src="Images/F13017.png" alt="F13017" width="694" height="241"/>
<figcaption><p><a id="figure13-17">Figure 13-17</a>: Straight-line functions. The leftmost function is called the identity function.</p></figcaption>
</figure>
<p>The other curves in <a href="#figure13-17">Figure 13-17</a> are also straight lines, but they’re tilted to different slopes. We call any curve that’s just a single straight line a <em>linear function</em>, or even (slightly confusingly) a <em>linear curve</em>. </p>
<p>These activation functions do not prevent network collapse. When the activation function is a single straight line, then mathematically, it’s only doing multiplication and addition, and that means it’s a linear function and the network can collapse. These straight-line activation functions usually appear only in two specific situations.</p>
<p>The first application is on a network’s output neurons. There’s no risk of collapse since there are no neurons after the output. The top of <a href="#figure13-18" id="figureanchor13-18">Figure 13-18</a> shows the idea.</p>
<figure>
<img src="Images/F13018.png" alt="F13018" width="694" height="473"/>
<figcaption><p><a id="figure13-18">Figure 13-18</a>: Using the identity as an activation function. Top: The identity function on an output neuron. Bottom: Using an identity function to insert a step of processing between the summation step and a nonlinear activation function for any neuron.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="333" id="Page_333"/>The second situation where we use a straight-line activation function is when we want to insert some processing between the summation step in a neuron and its activation function. In this case, we apply the identity function to the neuron, perform the processing step, and then perform the nonlinear activation function, as shown at the bottom of <a href="#figure13-18">Figure 13-18</a>.</p>
<p>Since we generally want nonlinear activation functions, we need to get away from a single straight line. All of the following activation functions are nonlinear and prevent network collapse.</p>
<h3 id="h2-500723c13-0004">Step Functions</h3>
<p class="BodyFirst">We don’t want a straight line, but we can’t pick just any curve. Our curve needs to be single-valued. As we discussed in Chapter 5, this means that if we look upward from any value of x along the X axis, there’s only one value of y above us. An easy variation on a linear function is to start with a straight line and break it up into several pieces. They don’t even have to join. In the language of Chapter 5, this means that they don’t have to be continuous. </p>
<p><a href="#figure13-19" id="figureanchor13-19">Figure 13-19</a> shows an example of this approach. We call this a <em>stair-step function</em>. In this example, it outputs the value 0 if the input is from 0 to just less than 0.2, but then the output is 0.2 if the input value is from 0.2 to just less than 0.4, and so on. These abrupt jumps don’t violate our rule that the curve has only one y output value for each input x value. </p>
<figure>
<img src="Images/F13019.png" alt="F13019" width="564" height="551"/>
<figcaption><p><a id="figure13-19">Figure 13-19</a>: This curve is made up of multiple straight lines. A filled circle tells us that the y value there is valid, whereas an open circle tells us that there is no curve at that point.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="334" id="Page_334"/>The simplest stair-step function has only a single step. This is a frequent special case, so it gets its own name: the <em>step function</em>. The original perceptron of <a href="#figure13-2">Figure 13-2</a> used a step function as its activation function. A step function is usually drawn as in <a href="#figure13-20" id="figureanchor13-20">Figure 13-20</a>(a). It has one value until some <em>threshold</em> and then it has some other value. </p>
<p>Different people have different preferences for what happens when the input has precisely the value of the threshold. In <a href="#figure13-20">Figure 13-20</a>(a) we’re showing that the value at the threshold is the value of the right side of the step, as shown by the solid dot.</p>
<figure>
<img src="Images/F13020.png" alt="F13020" width="694" height="388"/>
<figcaption><p><a id="figure13-20">Figure 13-20</a>: A step function has two fixed values, one each to the left and right of a threshold value of x.</p></figcaption>
</figure>
<p>Often authors are casual about what happens when the input is exactly at the transition, and draw the picture as in <a href="#figure13-20">Figure 13-20</a>(b) in order to stress the “step” of the function. This is an ambiguous way to draw the curve because we don’t know what value is intended when the input is precisely at the threshold, but it’s a common kind of drawing (often we don’t care which value is used at the threshold, so we can choose whatever we prefer).</p>
<p>A couple of popular versions of the step have their own names. The <em>unit step</em> is 0 to the left of the threshold, and 1 to the right. <a href="#figure13-21" id="figureanchor13-21">Figure 13-21</a> shows this function.</p>
<p>If the threshold value of a unit step is 0, then we give it the more specific name of the <em>Heaviside step</em>, also shown in <a href="#figure13-21">Figure 13-21</a>.</p>
<span epub:type="pagebreak" title="335" id="Page_335"/><figure>
<img src="Images/F13021.png" alt="F13021" width="694" height="381"/>
<figcaption><p><a id="figure13-21">Figure 13-21</a>: Left: The unit step has a value of 0 to the left of the threshold, and 1 to the right. Right: The Heaviside step is a unit step where the threshold is 0.</p></figcaption>
</figure>
<p>Finally, if we have a Heaviside step (so the threshold is at 0) but the value to the left is −1 rather than 0, we call this the <em>sign function</em>, shown in <a href="#figure13-22" id="figureanchor13-22">Figure 13-22</a>. There’s a popular variation of the sign function where input values that are exactly 0 are assigned an output value of 0. Both variations are commonly called “the sign function,” so when the difference matters, it’s worth paying attention to figure out which one is being referred to.</p>
<figure>
<img src="Images/F13022.png" alt="F13022" width="694" height="359"/>
<figcaption><p><a id="figure13-22">Figure 13-22</a>: Two versions of the sign function. Left: Values less than 0 are assigned an output of −1, all others are 1. Right: Like the left, except that an input of exactly 0 gets the value 0.</p></figcaption>
</figure>
<h3 id="h2-500723c13-0005"><span epub:type="pagebreak" title="336" id="Page_336"/>Piecewise Linear Functions</h3>
<p class="BodyFirst">If a function is made up of several pieces, each of which is a straight line, we call it <em>piecewise linear</em>. This is still a nonlinear function as long as the pieces, taken together, don’t form a single straight line.</p>
<p>Perhaps the most popular activation function is a piecewise linear function called a <em>rectifier</em>, or <em>rectified linear unit</em>, which is abbreviated <em>ReLU</em> (note that the e is lowercase). The name comes from an electronics part called a rectifier, which can be used to prevent negative voltages from passing from one part of a circuit to another (Kuphaldt 2017). When the voltage goes negative, the physical rectifier clamps it to 0, and our rectified linear unit does the same thing with the numbers that are fed into it.</p>
<p>The ReLU’s graph is shown in <a href="#figure13-23" id="figureanchor13-23">Figure 13-23</a>. It’s made up of two straight lines, but thanks to the kink, or bend, this is not<em> </em>a linear function. If the input is less than 0, then the output is 0. Otherwise, the output is the same as the input.</p>
<figure>
<img src="Images/F13023.png" alt="F13023" width="569" height="589"/>
<figcaption><p><a id="figure13-23">Figure 13-23</a>: The ReLU, or rectified linear unit. It outputs 0 for all negative inputs, otherwise the output is the input.</p></figcaption>
</figure>
<p>The ReLU activation function is popular because it’s a simple and fast way to include a nonlinearity at the end of our artificial neurons. But there’s a potential problem. As we’ll see in Chapter 14, if changes in the input don’t lead to changes in the output, a network can stop learning. And <span epub:type="pagebreak" title="337" id="Page_337"/>the ReLU has an output of 0 for every negative value. If our input changes from, say, –3 to –2, then the output of ReLU stays at 0. Fixing this problem has led to the development of the ReLU variations that follow. </p>
<p>Despite this issue, ReLU (or leaky ReLU, which we’ll see next) often performs well in practice, and people often use it as their default choice when building a new network, particularly for fully connected layers. Beyond the fact that these activation functions work well in practice, there are good mathematical reasons for wanting to use ReLU (Limmer and Stanczak 2017), though we won’t explore them here.</p>
<p>The <em>leaky ReLU</em> changes the response for negative values. Rather than output a 0 for any negative value, this functions outputs the input, scaled down by a factor of 10. <a href="#figure13-24" id="figureanchor13-24">Figure 13-24</a> shows this function.</p>
<figure>
<img src="Images/F13024.png" alt="F13024" width="570" height="589"/>
<figcaption><p><a id="figure13-24">Figure 13-24</a>: The leaky ReLU is like the ReLU, but it returns a scaled-down value of x when x is negative.</p></figcaption>
</figure>
<p>Of course, there’s no need to always scale down the negative values by a factor of 10. A <em>parametric ReLU</em> lets us choose by how much negative amounts are scaled, as shown in <a href="#figure13-25" id="figureanchor13-25">Figure 13-25</a>.</p>
<p>When using a parametric ReLU, the essential thing is to never select a factor of exactly 1.0, because then we lose the kink, the function becomes a straight line, and any neuron we apply this to collapses with those that immediately follow it.</p>
<span epub:type="pagebreak" title="338" id="Page_338"/><figure>
<img src="Images/F13025.png" alt="F13025" width="694" height="246"/>
<figcaption><p><a id="figure13-25">Figure 13-25</a>: A parametric ReLU is like a leaky ReLU, but the slope for values of x that are less than 0 can be specified.</p></figcaption>
</figure>
<p>Another variation on the basic ReLU is the <em>shifted ReLU</em>, which just moves the bend down and left. <a href="#figure13-26" id="figureanchor13-26">Figure 13-26</a> shows an example.</p>
<figure>
<img src="Images/F13026.png" alt="F13026" width="535" height="552"/>
<figcaption><p><a id="figure13-26">Figure 13-26</a>: The shifted ReLU moves the bend in the ReLU function down and left.</p></figcaption>
</figure>
<p>We can generalize the various flavors of ReLU with an activation function called <em>maxout</em> (Goodfellow et al. 2013). Maxout allows us to define a set of lines. The output of the function at each point is the largest value<em> </em><span epub:type="pagebreak" title="339" id="Page_339"/>among all the lines, evaluated at that point. <a href="#figure13-27" id="figureanchor13-27">Figure 13-27</a> shows maxout with just two lines, forming a ReLU, as well as two other examples that use more lines to create more complex shapes.</p>
<figure>
<img src="Images/F13027.png" alt="F13027" width="694" height="244"/>
<figcaption><p><a id="figure13-27">Figure 13-27</a>: The maxout function lets us build up a function from multiple straight lines. The heavy red line is the output of maxout for each set of lines. </p></figcaption>
</figure>
<p>Another variation on the basic ReLU is to add a small random value to the input before running it through a standard ReLU. This function is called a <em>noisy ReLU</em>.</p>
<h3 id="h2-500723c13-0006">Smooth Functions</h3>
<p class="BodyFirst">As we’ll see in Chapter 14, a key step in teaching neural networks involves computing derivatives for the outputs of neurons, which necessarily involve their activation functions.</p>
<p>The activation functions that we saw in the last section (except for the linear functions) create their nonlinearities by using multiple straight lines with at least one kink in the collection. Mathematically, there is no derivative at the kink between a pair of straight lines, and therefore the function is not linear.</p>
<p>If these kinks prevent the computation of derivatives, which are necessary for teaching a network, why are functions like ReLU useful at all, let alone so popular? It turns out that standard mathematical tools can finesse the sharp corners like those in ReLU and still produce a derivative (Oppenheim and Nawab 1996). These tricks don’t work on all functions, but one of the principles that guided the development of the functions we saw earlier is that they allow these methods to be used.</p>
<p>An alternative to using multiple straight lines and then patching up the problems is to use smooth functions that inherently have a derivative everywhere. That is, they’re smooth everywhere. Let’s look at a few popular and smooth activation functions.</p>
<p>The <em>softplus</em> function simply smooths out the ReLU, as shown in <a href="#figure13-28" id="figureanchor13-28">Figure 13-28</a>.</p>
<span epub:type="pagebreak" title="340" id="Page_340"/><figure>
<img src="Images/F13028.png" alt="F13028" width="486" height="501"/>
<figcaption><p><a id="figure13-28">Figure 13-28</a>: The softplus function is a smoothed version of the ReLU.</p></figcaption>
</figure>
<p>We can smooth out the shifted ReLU as well. This is called the <em>exponential ReLU</em>, or <em>ELU</em> (Clevert, Unterthiner, and Hochreiter 2016). It’s shown in <a href="#figure13-29" id="figureanchor13-29">Figure 13-29</a>.</p>
<figure>
<img src="Images/F13029.png" alt="F13029" width="486" height="500"/>
<figcaption><p><a id="figure13-29">Figure 13-29</a>: The exponential ReLU, or ELU</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="341" id="Page_341"/>Another way to smooth out the ReLU is called <em>swish</em> (Ramachandran, Zoph, and Le 2017). <a href="#figure13-30" id="figureanchor13-30">Figure 13-30</a> shows what this looks like. In essence it’s a ReLU, but with a small, smooth bump just left of 0, which then flattens out.</p>
<figure>
<img src="Images/F13030.png" alt="F13030" width="486" height="500"/>
<figcaption><p><a id="figure13-30">Figure 13-30</a>: The swish activation function</p></figcaption>
</figure>
<p>Another popular smooth activation function is the <em>sigmoid</em>, also called the <em>logistic function</em> or <em>logistic curve</em>. This is a smoothed-out version of the Heaviside step. The name <em>sigmoid</em> comes from the resemblance of the curve to an S shape, while the other names refer to its mathematical interpretation. <a href="#figure13-31" id="figureanchor13-31">Figure 13-31</a> shows this function.</p>
<p>Closely related to the sigmoid is another mathematical function called the <em>hyperbolic tangent</em>. It’s much like the sigmoid, only negative values are sent to –1 rather than to 0. The name comes from the curve’s origins in trigonometry. It’s a big name, so it’s usually written simply as <em>tanh</em>. This is shown in <a href="#figure13-32" id="figureanchor13-32">Figure 13-32</a>.</p>
<p>We say that the sigmoid and tanh functions both <em>squash</em> their entire input range from negative to positive infinity into a small range of output values. The sigmoid squashes all inputs to the range [0, 1], while tanh squashes them to [−1, 1].</p>
<span epub:type="pagebreak" title="342" id="Page_342"/><figure>
<img src="Images/F13031.png" alt="F13031" width="486" height="491"/>
<figcaption><p><a id="figure13-31">Figure 13-31</a>: The S-shaped sigmoid function is also called the logistic function or logistic curve. It has a value of 0 for very negative inputs, and a value of 1 for very positive inputs. For inputs in the range of about −6 to 6, it smoothly transitions between the two.</p></figcaption>
</figure>
<figure>
<img src="Images/F13032.png" alt="F13032" width="486" height="500"/>
<figcaption><p><a id="figure13-32">Figure 13-32</a>: The hyperbolic tangent function, written tanh, is S-shaped like the sigmoid of <a href="#figure13-31">Figure 13-31</a>. The key differences are that it returns a value of −1 for very negative inputs, and the transition zone is a bit narrower.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="343" id="Page_343"/>The two are shown on top of one another in <a href="#figure13-33" id="figureanchor13-33">Figure 13-33</a>.</p>
<figure>
<img src="Images/F13033.png" alt="F13033" width="462" height="475"/>
<figcaption><p><a id="figure13-33">Figure 13-33</a>: The sigmoid function (orange) and tanh function (teal), both plotted for the range −8 to 8</p></figcaption>
</figure>
<p>Another smooth activation function uses a sine wave, as shown in <a href="#figure13-34" id="figureanchor13-34">Figure 13-34</a> (Sitzmann 2020). This squashes the outputs to the range [–1, 1] like tanh, but it doesn’t saturate (or stop changing) for inputs that are far from 0.</p>
<figure>
<img src="Images/F13034n.png" alt="F13034n" width="486" height="456"/>
<figcaption><p><a id="figure13-34">Figure 13-34</a>: A sine wave activation function</p></figcaption>
</figure>
<h3 id="h2-500723c13-0007"><span epub:type="pagebreak" title="344" id="Page_344"/>Activation Function Gallery</h3>
<p class="BodyFirst"><a href="#figure13-35" id="figureanchor13-35">Figure 13-35</a> summarizes the activation functions we’ve discussed.</p>
<figure>
<img src="Images/F13035n.png" alt="F13035n" width="848" height="849"/>
<figcaption><p><a id="figure13-35">Figure 13-35</a>: A gallery of popular activation functions</p></figcaption>
</figure>
<h3 id="h2-500723c13-0008">Comparing Activation Functions</h3>
<p class="BodyFirst">ReLU used to be the most popular activation function, but in recent years, the leaky ReLU has been gaining in popularity. This is a result of practice: networks with leaky ReLU often learn faster. </p>
<p>The reason is that ReLU has a problem, which we mentioned earlier. When a ReLU’s input is negative, its output is 0. If the input is a large negative number, then changing it by a small amount still results in a negative <span epub:type="pagebreak" title="345" id="Page_345"/>input to ReLU and an unchanged output of 0. This means that the derivative is also zero. As we’ll see in Chapter 14, when a neuron’s derivative goes to zero, not only does it stop learning, but it also makes it more likely that the neurons that precede it in the network will stop learning as well. Because a neuron whose output never changes no longer participates in learning, we sometimes use rather drastic language and say that the neuron has <em>died</em>. The leaky ReLU has been gaining in popularity over ReLU because, by providing an output that isn’t the same for every negative input, its derivative is not 0, and thus it does not die. The sine wave function also has a non-zero derivative almost everywhere (except at the very top and bottom of each wave).</p>
<p>After ReLU and leaky ReLU, sigmoid and tanh are probably the next most popular functions. Their appeal is that they’re smooth, and the outputs are bounded to [0, 1] or [–1, 1]. Experience has shown that networks learn most efficiently when all the values flowing through it are in a limited range.</p>
<p>There is no firm theory to tell us which activation function works best in a specific layer of a specific network. We usually start by making the same choices that have worked in other, similar networks that we’ve seen, and then we try alternatives if learning goes too slowly. </p>
<p>A few rules of thumb give us a good starting point in many situations. Generally speaking, we often apply ReLU or leaky ReLU to most neurons on hidden layers, particularly fully connected layers. For regression networks, we often use no activation function on the final layer (or if we must supply one, we use a linear activation function, which amounts to the same thing), because we care about the specific output value. When we’re classifying with just two classes, we have just a single output value. Here we often apply a sigmoid to push the output clearly to one class or the other. For classification networks with more than two classes, we almost always use a somewhat different kind of activation function, which we’ll look at next.</p>
<h2 id="h1-500723c13-0012">Softmax</h2>
<p class="BodyFirst">There’s an operation that we typically apply only to the output neurons of a classifier neural network, and even then, only if there are two or more output neurons. It’s not an activation function in the sense that we’ve been using the term because it takes as input the outputs of <em>all</em> the output neurons simultaneously. It processes them together and then produces a new output value for each neuron. Though it’s not quite an activation function, it’s close enough in spirit to activation functions to merit including it in this discussion.</p>
<p>The technique is called <em>softmax</em>. The purpose of softmax is to turn the raw numbers that come out of a classification network into class probabilities. </p>
<p>It’s important to note that softmax takes the place of any activation function we’d otherwise apply to those output neurons. That is, we give them no activation function (or, equivalently, apply the linear function) and then run those outputs into softmax.</p>
<p><span epub:type="pagebreak" title="346" id="Page_346"/>The mechanics of this process are involved with the mathematics of how the network computes its predictions, so we won’t go into those details here. The general idea is shown in <a href="#figure13-36" id="figureanchor13-36">Figure 13-36</a>: <em>scores</em> come in, and <em>probabilities</em> come out.</p>
<figure>
<img src="Images/F13036n.png" alt="F13036n" width="696" height="382"/>
<figcaption><p><a id="figure13-36">Figure 13-36</a>: The softmax function takes all the network’s outputs and modifies them simultaneously. The result is that the scores are turned into probabilities.</p></figcaption>
</figure>
<p>Each output neuron presents a value, or score, that corresponds to how much the network thinks the input is of that class. In <a href="#figure13-36">Figure 13-36</a> we’re assuming that we have three classes in our data, named A, B, and C, so each of the three output neurons gives us a score for its class. The larger the score, the more certain the system is that the input belongs to that class.</p>
<p>If one class has a larger score than some other class, it means the network thinks that class is more likely. That’s useful. But the scores aren’t designed to be compared in any other convenient way. For instance, if the score for A is twice that of B, it doesn’t mean that A is twice as likely as B. It just means that A is more likely. Because making comparisons like “twice as likely” is so useful, we use softmax to turn the output scores into probabilities. Now, if the softmax output of A is twice that of B, then indeed A is twice as probable as B. That’s such a useful way to look at the network’s output that we almost always use softmax at the end of a classification network.</p>
<p>Any set of numbers that we want to treat as probabilities must satisfy two criteria: the values all lie between 0 and 1, and they add up to 1. If we just modify each output of the network independently, we don’t know the other values, so we can’t make sure they added up to anything in particular. When we hand all the outputs to softmax, it can simultaneously adjust all the values so that they sum to 1. </p>
<p>Let’s look at softmax in action. Consider the top-left graph of <a href="#figure13-37" id="figureanchor13-37">Figure 13-37</a>. </p>
<span epub:type="pagebreak" title="347" id="Page_347"/><figure>
<img src="Images/F13037.png" alt="F13037" width="844" height="415"/>
<figcaption><p><a id="figure13-37">Figure 13-37</a>: The softmax function takes all the network’s outputs and modifies them simultaneously. The result is that the scores are turned into probabilities. Top row: Scores from a classifier. Bottom row: Results of running the scores in the top row through softmax. Note that the graphs in the upper row use different vertical scales.</p></figcaption>
</figure>
<p>The top left of <a href="#figure13-37">Figure 13-37</a> shows the outputs for a classifier with six output neurons, which we’ve labeled A through F. In this example, all six of these values are between 0 and 1. From this graph, we can see that the value for class B is 0.1 and the value for class C is 0.8. As we’ve discussed, it is a mistake to conclude from this that the input is 8 times more likely to be in class C than class B, because these are scores and not probabilities. We can say that class C is more likely than class B, but anything more requires some math. To usefully compare these outputs to one another, we can apply softmax to carry out that math, and change them into probabilities. </p>
<p>We show the output of softmax in the graph in the lower left. These are the probabilities of the input belonging to each of the six classes. It’s interesting to note that the big values, like C and F, get scaled down by a lot, but the small values, like B, are hardly scaled at all. This is a natural result of how scores between 0 and 1 turn into probabilities. But the ordering of the bars by size is still the same as it was for the scores (with C the largest, then F, then D, and so on). From the probabilities produced by softmax in the lower figure, we can see that class C has a probability of about 0.25, and class B has a probability of about 0.15. We can conclude that the input is a little more than 1.5 times more probable to be in class C than class B. </p>
<p>The middle and right columns of <a href="#figure13-37">Figure 13-37</a> show the outputs for two other hypothetical networks and inputs, before and after softmax. The three examples show that the output of softmax depends on whether the inputs are all less than 1. The input ranges in <a href="#figure13-37">Figure 13-37</a>, reading left to right, are [0, 0.8], [0, 8], and [0, 3]. Softmax always preserves the <span epub:type="pagebreak" title="348" id="Page_348"/>ordering of its inputs (that is, if we sort the inputs from largest to smallest, they match a similar sort on the outputs). But when some input values are greater than 1, the largest value tends to stand out more. We say that softmax <em>exaggerates</em> the influence of the output with the largest value. Sometimes we also say that softmax <em>crushes</em> the other values, making the largest one dominate the others more obviously.</p>
<p><a href="#figure13-37">Figure 13-37</a> shows that the input range makes a big difference in the output of softmax. Softmax also has an interesting behavior depending on whether the inputs values are all less than 1, all greater than 1, or mixed.</p>
<p>At the far left of <a href="#figure13-37">Figure 13-37</a> all of the inputs are all less than 1, in the range [0, 0.8].</p>
<p>In the middle column, the inputs are all greater than 1, in the range [0, 8]. Notice that in the output, the value of D (corresponding to the 8) clearly dominates all of the other values. Softmax has exaggerated the differences among the outputs, making it easier to pick out D as the largest.</p>
<p>On the far right of <a href="#figure13-37">Figure 13-37</a> we have values both less and greater than 1, in the range [0, 3]. Here the exaggeration effect is somewhere between the left column, where all inputs are less than 1, and the middle column, where all inputs are greater than 1.</p>
<p>In all cases, though, softmax gives us back probabilities that are each between 0 and 1, and sum up to 1. The ordering of the inputs is always preserved, so the sequence of largest to smallest input is also the largest to smallest output.</p>
<h2 id="h1-500723c13-0013">Summary</h2>
<p class="BodyFirst">Real, biological neurons are sophisticated nerve cells that process information using a fabulously complex range of chemical, electrical, and mechanical processes. They serve as the inspiration for a simple bit of computation that we call an artificial neuron, despite the enormous gulf between the computer version and its biological namesake. An artificial neuron multiplies each input value by a corresponding weight, adds the results, then passes that through an activation function. We can assemble artificial neurons into networks. Typically, those networks are DAGs: they are directed (information flows in only one direction), they are acyclic (no neuron ever receives its own output as an input), and they are graphs (the neurons are connected to one another). Input data enters at one end, and the network’s results appear at the other.</p>
<p>We saw how, if we’re not careful in constructing our network, the entire network can collapse into a single neuron. We prevent this by using the activation function, a small function that takes each neuron’s output and turns it into a new number. These functions are designed to be nonlinear, meaning that they cannot be described merely by operations such as addition and multiplication. It is this nonlinearity that prevents the network from being equivalent to a single neuron. We concluded the chapter by looking at some of the more common activation functions, and how softmax can turn the numbers we get from a neural network into class probabilities. </p>
<p><span epub:type="pagebreak" title="349" id="Page_349"/>The only difference between untrained deep learning systems and those that have been trained and are ready for deployment is in the value of the weights. The goal of training, or learning, is to find values for the weights so that the network’s output is correct for as many samples as possible. Since the weights start out with random numbers, we need some principled way to find these new, useful values. In Chapters 14 and 15, we’ll see how neural networks learn by looking at the two key algorithms that gradually improve the starting weights, transforming a network’s outputs into accurate, useful results.</p>
</section>
</div></body></html>