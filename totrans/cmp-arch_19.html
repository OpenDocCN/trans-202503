<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="chn"><span epub:type="pagebreak" id="page_397"/><strong>16</strong></h2>&#13;
<h2 class="cht"><strong>FUTURE ARCHITECTURES</strong></h2>&#13;
<div class="image1"><img src="../images/f0397-01.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="chq">Historically, looking at academic research that’s close to transitioning to industry tends to accurately predict what will happen over the next decade. At present, research is still mostly done on semiconductor-based technologies, but there are some researchers looking at alternatives. While it’s hard to predict much further than a decade ahead, we’ll look at some current ideas that might one day go somewhere beyond the present electricity-based computing age. We’ll go roughly in order of uncertainty, starting with some close-to-market developments associated with the current “new golden age” of architecture, then traveling through research labs studying optical and DNA architectures, neural architectures, and quantum computing, and finally moving on to speculative ideas based on more distant theories of physics.</p>&#13;
<h3 class="h3" id="lev316"><span epub:type="pagebreak" id="page_398"/>The New Golden Age</h3>&#13;
<p class="noindent">Architecture is cool again! In the 2010s, trends like the maker, open source, and “mindful design” movements helped drive the resurgence of interest in architecture. Rebelling against the prepackaged black-box interfaces sold to them, artists, innovators, hipsters, and steampunks instead chose to gain greater understanding, control, and satisfaction over the technology in their lives by opening up these boxes and looking at and modifying what’s inside. In the professional world, commercial architecture careers over the next decade seem likely to focus on low-cost, low-power embedded and smart systems rather than desktops, laptops, and servers.</p>&#13;
<p class="indent">The 2010s was also a decade of parallelization and centralized computing, with computation moving off the desktop into the “cloud” of dedicated centralized computing and data centers. It’s widely expected that the next step in computer evolution will be the disappearance of desktops and even laptops, replaced by a multitude of small, low-power devices all around the real world that are in constant communication with the cloud, relaying data to the cloud for processing. Smartphones and tablets are early versions of this, but we expect to see even cheaper and smaller devices all over the real world, enabling smart homes, smart farms, and smart cities.</p>&#13;
<p class="indent">A recent trend identified by Hennessy and Patterson is the demand for custom, domain-specific architectures. In this view, GPUs and NPUs are only the beginning of a new wave of custom silicon designed to accelerate specific, single tasks. It’s likely that architects will work on these designs as part of larger teams—for example, working more closely with machine learning engineers and cryptographers to understand and accelerate their algorithms. This would create a cultural shift in computer science, bringing architects back into the mainstream, and requiring everyone else to understand and interact with their work as they did in the 1980s.</p>&#13;
<h4 class="h4" id="lev317"><em>Open Source Architectures</em></h4>&#13;
<p class="noindent">For the first time in architectural history, open source thinking has extended into the creation of fully open source hardware and software tooling stacks—RISC-V, BOOM, and Chisel—for professional-quality, state-of-the-art chip design. Along with new affordable FPGAs, these enable anyone to access equipment that was previously only available to a handful of secretive and elite architecture companies. Now almost anyone can be the creator of anything and see and hack the entire stack, from the level of transistors to operating systems. Now is thus the best time to be involved in architecture—even better than the 8-bit days, when hackers could see the ISAs but were still only customers of their chipmakers.</p>&#13;
<p class="indent">Open source hardware designs have even started to appear for entire consumer PCs, such as the ARM-based Olimex TERES laptop, which users often modify through PCB design software and 3D printing. Open source interest is also being driven by end users, who are feeling increasingly uneasy about the proprietary architecture of individual CPUs that may be back-doored at the digital logic level. For example, Intel has been accused of <span epub:type="pagebreak" id="page_399"/>hiding and running an entire operating system based on MINIX inside its processors, which can communicate with its Intel home to say potentially anything about what the machine is doing. Open source architectures may become standard and expected—an architectural revolution analogous to the open source software revolution of the 2000s.</p>&#13;
<p class="indent">While large-scale fabrication is only possible in expensive fab plants, a few companies are large enough to make new masks and fabricate experimental chips on a fairly regular basis. These big companies sometimes now allow researchers and hobbyists to fabricate their own real ASICs for free or low cost by including their designs in an otherwise unused corner of their masks and wafers (for example, <em><a href="https://developers.google.com/silicon">https://developers.google.com/silicon</a></em>). There’s also been recent progress allowing makers to fabricate their own simpler chips in their garages using open source hardware methods. Sam Zeloof pioneered this approach and in 2021 was able to place and connect 1,200 transistors on a chip—about half the number used in the Intel 4004.</p>&#13;
<p class="indent">Openness is also becoming an issue in the cloud. There are currently significant concerns around moving from desktop computing—where everyone owns their own computer—to the 2020s cloud, where the computers are owned by a small number of large, powerful companies. This has raised some questions: who will control these computers and the data on them, and how can users be sure that their computations and data aren’t being spied on or resold by these companies or other actors?</p>&#13;
<p class="indent">These concerns might drive new architecture trends. The <em>open cloud</em> concept calls for replacing corporate clouds hosted in dedicated computing centers with a shared, loose, decentralized, federated network of ordinary citizens’ machines, in their homes. Everyone will have a small, always-on server in their home, a cross between a high-end router, NAS drive, and Intel NUC. These servers will enable non-technical home internet users to easily host their own websites and media streams. They’ll also enable fully open source search engines (YaCy), social media (Mastodon), video storage and streaming (PeerTube), video conferences (Matrix), and physical goods marketplaces (OpenBazaar) to replace big tech equivalents by distributing their computations and using cryptographic methods and currencies to ensure trust. The FreedomBox website already has a working software distribution that you can run today on your Raspberry Pi to do some of this. New architectures may be needed to optimize for these use cases.</p>&#13;
<p class="indent">While hackers and makers can now get their hands on these nice tools, big companies with big resources aren’t standing still. They continue to develop smaller and more advanced systems to try to stay ahead, as we’ll see next.</p>&#13;
<h4 class="h4" id="lev318"><em>Atomic-Scale Transistors</em></h4>&#13;
<p class="noindent">We saw in <a href="ch04.xhtml">Chapter 4</a> that Moore’s law for clock speed is over, but Moore’s law for silicon transistor density is still holding up. The density law can’t go on forever either, though, because we’ll hit a point where a transistor is the same size as an atom, and then it will be impossible to go any smaller with <span epub:type="pagebreak" id="page_400"/>semiconductors. Quantum effects will also kick in as we approach this point, leading to inherent uncertainties about where things are and what they represent. Moore’s law for density suggests this will occur around 2060.</p>&#13;
<p class="indent">IBM can currently manipulate single atoms into simple shapes. For example, <a href="ch16.xhtml#ch16fig1">Figure 16-1</a> shows an electron microscope image of a copper surface in which each dot is a single atom, placed and read with their technology.</p>&#13;
<div class="image"><img id="ch16fig1" src="../images/f0400-01.jpg" alt="Image" width="552" height="311"/></div>&#13;
<p class="figcap"><em>Figure 16-1: IBM manipulates single atoms to create images</em></p>&#13;
<p class="indent">The fuzzy, wave-like quality of this image is due to quantum effects. At this scale, it becomes inherently uncertain where the atoms are and how they’re moving around. These atoms don’t yet function as transistors or computers, but they can, for example, be used for data storage, and IBM would eventually like to develop the technology toward single atom–based computation.</p>&#13;
<p class="indent">Before we get to this scale, but after conventional semiconductors hit fundamental size limits, nanotechnologies such as carbon nanotubes and graphene might be used to build smaller transistors; this is a current research area. In 2022, researchers at Tsinghua University fabricated a graphene transistor about the size of a single carbon atom, running millions of times faster than silicon.</p>&#13;
<h4 class="h4" id="lev319"><em>3D Silicon Architectures</em></h4>&#13;
<p class="noindent">Classical chip layouts were 2D, with a good bit of graph theory and complexity theory needed to optimize the design and minimize the wiring. As we saw in <a href="ch04.xhtml#ch04fig19">Figure 4-19</a>, current CPUs can be made with a few layers of <em>overlapping</em> copper wires, whose 3D structure greatly reduces the wiring. Modern chips still place transistors in a single layer, on the base of the chip, but allow several (typically 2 to 10) layers of wires to be formed on top, insulated from one another by filler materials.</p>&#13;
<p class="indent">It’s possible that today’s basic layering technologies will grow incrementally to add more and more layers of wires and transistors, culminating in a move from 2D silicon chips to fully 3D silicon cubes.</p>&#13;
<p class="indent">However, silicon cubes will create issues around power supplies and heat, requiring something analogous to the brain’s blood supply system mixed around the computing elements to get energy in and heat out of the dense 3D structure. We don’t currently know how this should be done. The <span epub:type="pagebreak" id="page_401"/>chip design community has, for living memory, been so focused on 2D layout concerns that it’s not clear how it could move to thinking in 3D.</p>&#13;
<p class="indent">RAM usually has lower usage and heat requirements than processing because most of the time it just sits there doing nothing in a serial computer. Therefore, it’s easier to make 3D RAM than it is to make 3D CPUs. There have been recent commercial attempts at 3D RAM, such as Micron’s Hybrid Memory Cube.</p>&#13;
<p class="indent">One source of inspiration for 3D CPU design might come from today’s <em>Minecraft</em> gaming community. <em>Minecraft</em> can act as a Church-powerful computer, using its redstone elements as switches. Fans have already constructed several functioning CPU components inside it, looking similar to <a href="ch04.xhtml#ch04fig19">Figure 4-19</a>, and even whole CPUs such as “ANDROSII.” Unlike previous generations, these players have grown up with <em>Minecraft</em>’s inherent three-dimensionality, so instead of laying out their processors on 2D circuit boards or ICs, they’ve instinctively evolved inherently 3D architectures to optimize their layouts, completely free from manufacturing constraints and the 2D thinking built into the silicon industry.</p>&#13;
<h4 class="h4" id="lev320"><em>10,000-Year Memory</em></h4>&#13;
<p class="noindent">What will happen to your data when you die? Will anyone be able to read your files or view your videos thousands of years into the future? Or even 10 years into the future?</p>&#13;
<p class="indent">The clay tablets from 4,000 years ago that we saw earlier (<a href="ch01.xhtml#ch01fig5">Figure 1-5</a>) are still perfectly readable. Paper was an advance over clay tablets in terms of speed and capacity, but it doesn’t survive as long. As memory technology has advanced and miniaturized, it’s gotten faster and increased its capacity, but at the expense of robustness, both to physical decay and to “bit rot” or other technological incompatibilities. All the tertiary and offline storage options we’ve seen will decay in 100 years. Commercial data centers keep data “alive” by continually copying it to new physical media. Spinning hard disks break and are replaced; tapes and optical discs decay and are replaced. But this relies on continual attention by human maintainers, employed by a company that continues to exist and doesn’t go bankrupt or get bought out by new owners who don’t want to continue maintaining it.</p>&#13;
<p class="indent">Research efforts are currently underway to find longer-term storage options as durable as clay tablets, but at modern data sizes. M-disc is a recent optical disc format, backward compatible with Blu-ray, that is claimed to store 100GB for 1,000 years. In 2018, the Arch Mission Foundation deposited a DVD-sized nickel disk onto the moon’s surface, containing a full backup of Wikipedia and other documents deemed useful for rebooting humanity in the event of total data loss on Earth. They claim it will last for at least 10,000 years. Glass laser nanostructuring, as developed at the University of Southampton, may store 350TB in a 1-inch cube of very hard glass, with a 14 billion–year lifetime. It’s a similar idea to the 3D markings you see in glass trophies, etched deep inside their structure with lasers.</p>&#13;
<p class="indent">Lasers might also be used to perform computations, as in optical architectures; we’ll turn to these now.</p>&#13;
<h3 class="h3" id="lev321"><span epub:type="pagebreak" id="page_402"/>Optical Architectures</h3>&#13;
<p class="noindent">We’ve mostly looked at computers that are based on the flow of electrons. Electrons have mass, so they must travel slower than the speed of light. Energy is also required to give momentum to their mass so that they can move around. Light, on the other hand, has no mass, so it moves faster than electrons, at the speed of light (about 300 million meters per second). As this is the physical speed limit of everything in the universe, since the 1960s researchers have asked whether we can compute with light instead of electrons. Like electrons in electricity, light comes in discrete units called <em>photons</em>, and the engineering field that studies how to manipulate them is called <em>photonics</em>.</p>&#13;
<h4 class="h4" id="lev322"><em>Optical Transistors</em></h4>&#13;
<p class="noindent">The speed of electric current is different from the speed of electrons themselves; current usually flows with each electron only moving a small distance, pushing the next electron forward in the circuit. Electrons moving through wire are in a complex environment with many collisions as they bump around, backward and forward, in random walks. The speed of individual electrons drifting along wire is thus very slow, around 1 meter per hour, while the speed of the current in copper wire can be around 90 percent of the speed of light in a vacuum. Thus, a naive expectation that light will compute much faster than electrons seems overly optimistic; switching over our entire hardware technology for just a 10 percent speedup, from 90 percent to 100 percent of light speed, seems not so useful.</p>&#13;
<p class="indent">However, optical systems have different advantages: there’s higher throughput and lower energy consumption than when using electrons because of the lower noise in light propagation. That’s why we already use light for routine high-bandwidth, long-distance networking—that is, fiber optics. Optical computing doesn’t seem so far-fetched when you remember that most of your internet and phone traffic is already sent around the world via fiber optics.</p>&#13;
<p class="indent">The difference between mere information transfer and actual computation is that in computation, data elements need to physically interact with one another via some kind of device analogous to a transistor, which in turn would build up logic gates and the rest of the architectural hierarchy. The key problem for optical computing, however, is that photons don’t naturally interact with one another. In physics terms, they’re bosons rather than fermions, which means that if two of them “collide” they just go straight through each other instead of bouncing off each other. This is great for optical communication, but not for optical computing.</p>&#13;
<p class="indent">To make an optical transistor, we thus need some form of electro-optical hybrid technology, in which photons can interact with electrons and vice versa to perform computation. Transferring energy between photons and electrons is slow, however, and uses up energy. Such devices currently exist in large photonics labs, made of lasers and precision equipment on optical tables. These systems fill whole rooms and implement only a few hybrid <span epub:type="pagebreak" id="page_403"/>optical-electronic transistors. Their scale is reminiscent of early electronic computers of the early 20th century. But like those large electronic computers, research also aims to miniaturize them once the basic principles are worked out, probably via photolithography (chip masking) processes similar to the ones used to make conventional electronics; current plans involve using silicon as the electronics substrate, similar to conventional chips.</p>&#13;
<h4 class="h4" id="lev323"><em>Optical Correlators</em></h4>&#13;
<p class="noindent">An <em>optical correlator</em> (or <em>4f system</em>) is a special case of optical computation that has become very practical in the last few years. Rather than being Church powerful, an optical correlator is used for a single purpose: to implement and speed up a single algorithm, the <em>discrete Fourier transform (DFT)</em>. The DFT converts streams of spatial and time-series data, such as in sound and video codecs, into frequency-based representations. It uses this equation:</p>&#13;
<div class="imagec"><img src="../images/f0403-01.jpg" alt="Image" width="451" height="75"/></div>&#13;
<p class="indent">For audio signals, the DFT results correspond roughly to the underlying frequencies that generated the signals. For images and video, they correspond roughly to different textures useful in recognition and compression. This is such a basic operation, and used so heavily, that it’s worth optimizing it with dedicated hardware, as is currently done by many CISC and DSP instructions.</p>&#13;
<p class="indent">A key computational property of the DFT is that it speeds up the common operation of convolution (or filtering). For one-dimensional signals, convolution is defined as:</p>&#13;
<div class="imagec"><img src="../images/f0403-02.jpg" alt="Image" width="301" height="75"/></div>&#13;
<p class="noindent">Here, <em>N</em> is the length of the signal <em>y</em>. Implementing this equation directly results in an <em>O</em>(<em>N</em><sup>2</sup>) algorithm, though the <em>fast Fourier transform (FFT)</em> is a faster <em>O</em>(<em>N</em> log <em>N</em>) algorithm based on a mathematically equivalent rearrangement of the equation. The FFT is the fastest known implementation of DFT for a serial computer; it’s been described as “the most important numerical algorithm of our lifetime.”</p>&#13;
<p class="indent">Convolution in the source domain is equivalent to multiplication in the Fourier domain. So rather than convolve two raw signals in the raw domain, it can be faster to Fourier transform them both, multiply these transforms together, and use a final DFT to convert back into the raw domain:</p>&#13;
<div class="imagec"><img src="../images/f0403-03.jpg" alt="Image" width="472" height="29"/></div>&#13;
<p class="indent">When a single ray of laser light goes through a single tiny hole, it’s <em>diffracted</em> to produce a diffraction pattern of light on the other side. It can be shown that if this light signal is passed through a lens, positioned at its focal length <em>f</em> from the image, then at the same distance <em>f</em> on the other side <span epub:type="pagebreak" id="page_404"/>of the lens, an image is formed that happens to be the DFT of the original image. This was an unexpected and coincidental property of the mathematics of diffraction and lensing, but once discovered it provided an ultra-fast, <em>O</em>(1) physical device to compute DFTs at the speed of light.</p>&#13;
<p class="indent">Once we have this Fourier image, <em>X</em>, we can implement convolution in <em>O</em>(1) by multiplying pointwise by the DFT of our filter, <em>Y</em>. We precompute <em>Y</em> offline and manufacture a physical filter—just like the colored filters put in front of theater stage lights to change their properties. For most DSP applications, such as video processing, we’ll want to apply the same filter <em>y</em> to many images <em>x</em> in a rapid sequence, so we have to compute <em>Y</em> only once. Passing the light image <em>X</em> through this physical filter has the effect of multiplying it by <em>Y</em>, equivalent to convolution <em>x</em> ∗ <em>y</em> in the raw domain. The DFT is self-inverse, so to obtain the final convolution we pass this image through a second lens, of the same focal length, again positioned at distance <em>f</em> from both its input and output. The final result can then be viewed as an image at distance 4<em>f</em> from the original input (hence the name <em>4f system</em>). The complete system, illustrated in <a href="ch16.xhtml#ch16fig2">Figure 16-2</a>, computes the entire convolution for fixed <em>Y</em> in <em>O</em> (1) time, at the speed of light.</p>&#13;
<div class="image"><img id="ch16fig2" src="../images/f0404-01.jpg" alt="Image" width="564" height="301"/></div>&#13;
<p class="figcap"><em>Figure 16-2: A 4f filter structure</em></p>&#13;
<p class="indent">This structure has been known since the 1960s but has only recently become practical by piggybacking off well-funded commercial smartphone screen technology. It requires a way to filter laser light through very small but high-resolution images, both to create the initial input image <em>x</em>, and to create changeable filter patterns <em>Y</em>. <em>Spatial light modulators (SLMs)</em> are a similar display technology to 4K smartphone displays, originally developed for use in high-end digital overhead projectors. SLMs from these projectors can be taken almost off the shelf and used to create fast, efficient input and filter displays for 4f filters. To complete the setup, an image sensor is also needed to read off the final convolved image. Smartphone digital camera CMOS sensors have been developed, almost symmetrically with display technology, to provide similar resolutions and frame rates required, and can again be used almost off the shelf. Systems built from these components at the time of writing might use 4 megapixels at 15 kHz frame rates.</p>&#13;
<h4 class="h4" id="lev324"><span epub:type="pagebreak" id="page_405"/><em>Optical Neural Networks</em></h4>&#13;
<p class="noindent">Practical optical correlators have become available at the same time that deep learning has revolutionized commercial machine learning. Deep learning has so far consisted of running 1970s neural network algorithms on fast, parallel GPU architectures. In many cases, however, it could be massively accelerated using optical correlators. This is because many problems, especially object recognition in images and video, have a spatially invariant structure, meaning the properties of images don’t vary significantly based on which part of the image is being looked at; similar objects are found at all locations around the image. This structure enables <em>convolutional neural networks (CNNs)</em> to use the same weights in all nodes within each layer of the network. Mathematically, the effect of this is that each network layer can be viewed as performing a convolution of the layer’s inputs with a single weight vector. Computing these convolutions thus becomes the main workhorse operation of these neural networks.</p>&#13;
<p class="indent">The first practical demonstration of an optical CNN was in 2018, and UK company Optalysys is now commercializing this technology by producing prototypes of consumer optical correlators, as shown in <a href="ch16.xhtml#ch16fig3">Figure 16-3</a>.</p>&#13;
<div class="image"><img id="ch16fig3" src="../images/f0405-01.jpg" alt="Image" width="423" height="298"/></div>&#13;
<p class="figcap"><em>Figure 16-3: An optical correlator PCIe card</em></p>&#13;
<p class="indent">This device can now be plugged into a desktop PCIe slot to replace GPUs for deep learning and other applications.</p>&#13;
<h3 class="h3" id="lev325">DNA Architectures</h3>&#13;
<p class="noindent">Beginning around 2000, labs have investigated <em>DNA computing</em> as a way to solve hard computation problems using massive biological parallelism. DNA molecules, as found in living cells, may be viewed (depending on one’s conception of representation) as performing computations, and it’s been shown that they can encode and efficiently solve computationally NP-hard problems such as the traveling salesperson problem. To understand DNA computing, we’ll need a bit of background information.</p>&#13;
<p class="indent">DNA (deoxyribonucleic acid) is the “source code” for life on Earth. In cellular organisms, every cell contains a complete copy of the whole organism’s code (genome) in a set of large double-helix molecules (chromosomes) inside the cell nucleus.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_406"/>Small parts (genes) of the information encoded in the DNA molecule are copied (transcribed) onto RNA (ribonucleic acid) molecules, which then move out of the nucleus and form construction sites for particular protein molecules to be built. These protein molecules build up the actual body of the organism. This process is known as the <em>central dogma</em> of molecular biology: DNA makes RNA; RNA makes proteins.</p>&#13;
<p class="indent">Each rung of DNA’s double-helix ladder is formed from a matching pair of <em>nucleotides</em>, small (around 20 atoms) organic molecules of which there are four types: A, T, C, and G. Each has one partner to make pairs: A and T go together; C and G go together. Humans have 23 chromosomes containing a total of about 3 gigapairs of nucleotides. DNA thus uses a base 4 data representation with symbols A, T, C, and G, and the source code for the human genome is about 6 gigabits. This is a similar size to an operating system, and like an operating system, the human genome has been distributed on a single CD-ROM.</p>&#13;
<p class="indent">DNA technology used to be expensive; for example, it took $100 million to sequence the first human genome in 2001. But it has recently rapidly fallen in price, reaching $1,000 in 2015 and $100 in 2023. This decline in price means the time is ripe to consider DNA as a medium for computation.</p>&#13;
<h4 class="h4" id="lev326"><em>Synthetic Biology</em></h4>&#13;
<p class="noindent">Rather than using DNA to store source code for making proteins, as in nature, <em>synthetic biologists</em> can use DNA to represent, edit, select, and copy arbitrary data. This enables Church computers to be constructed using DNA data representation and processing.</p>&#13;
<p class="indent">ATCG strings of DNA can be edited via cutting, splicing, and inserting symbols, as in an ASCII text editor. This is done using custom enzymes that promote the desired reactions. A small set of these enzymes is now well known and can be used routinely to perform these operations.</p>&#13;
<p class="indent">As for the strings themselves, it’s now surprisingly easy to produce your own arbitrary sequences of DNA, which can then be used to store and compute in base 4 as a string of ATCG symbols. It can almost now be done at home using a modified consumer inkjet printer, with its usual cyan, magenta, yellow, and black (CMYK) inks replaced by solutions of ATCG molecules. DNA manufacturing can also be performed on industrial-chemistry scales, making huge numbers of identical or related molecules in a liquid the size of a swimming pool. Consider that just a glass of water contains around 10<sup>24</sup> water molecules, more than all the bits of data in the world.</p>&#13;
<p class="indent">Information can be read back from physical DNA using electrophoresis, the same technique used for DNA fingerprinting in crime scene investigations. The <em>polymerase chain reaction (PCR)</em> also provides a method to select and copy one particular strand of DNA from a large solution of different strands, the equivalent of extracting a substring from a string.</p>&#13;
<h4 class="h4" id="lev327"><span epub:type="pagebreak" id="page_407"/><em>DNA Computing</em></h4>&#13;
<p class="noindent">Computationally, PCR provides a fast search algorithm. If we can make a liquid containing billions of strands, each encoding a different candidate answer to a computational problem, then we can use PCR to quickly pick out and read off the correct answer.</p>&#13;
<p class="indent">PCR is a chain reaction, meaning it continues to run and to expand its effects exponentially over time. If the mixture contains just a single DNA strand containing the search string, then that strand will be copied, then each of the copies will also be copied, and so on, until almost the whole liquid ends up full of billions of copies of the answer. This means that a sample of the liquid analyzed by electrophoresis will almost certainly show the desired result.</p>&#13;
<p class="indent">In 1994, Leonard Adelman successfully used DNA computations to solve a seven-city traveling salesperson problem. The traveling salesperson is a classic NP-hard problem that asks for the shortest route someone can take to visit each of <em>N</em> cities and return home, given a matrix of distances between them. Adelman represented the identity of each city with a short DNA string, then represented routes as strings concatenating these identifiers.</p>&#13;
<p class="indent">As in standard traveling salesperson formulations, the shortest route question was reformulated as an <em>O</em></p>&#13;
<p class="indent"><em>(n</em>) series of Boolean questions of the form “Does there exist a route with length less than <em>n</em>?” This question, along with the distance metrics between the cities, was encoded as a primer, which binds only to DNA strands representing routes with the desired property (having a length less than <em>n</em>). For each <em>n</em>, a chemical solution was prepared consisting of many copies of strands of every possible route, in a human-scale vat. The primer was mixed in, then PCR applied to amplify any successful result. Electrophoresis was used to read off the results. This was able to find shortest routes for <em>N</em> = 7 cities.</p>&#13;
<p class="indent">This doesn’t mean that <em>P</em> = <em>NP</em> for DNA computers; in time, this is <em>O</em> (<em>n</em>), but it still requires exponential resources in the number of molecules. It’s just that with DNA there are a lot of molecules available. DNA is thus able to solve much larger instances of NP-hard problems than other technologies, but like all technologies, there will exist problem sizes that are still too large due to the nature of NP-hardness.</p>&#13;
<p class="indent">Current research is trying to move DNA computing architectures out of vats in biology labs and into miniaturized biochemical chips that will operate more like normal silicon computers. DNA computing seems unlikely to replace electronics for day-to-day computing tasks, such as running desktop applications, but it might become useful as co-processing in scientific computing for solving large, hard computational problems.</p>&#13;
<h3 class="h3" id="lev328">Neural Architectures</h3>&#13;
<p class="noindent">Neuroscience has been an important influence on architecture since at least John von Neumann’s <em>Draft Report on the EDVAC</em>, which used many neural ideas as direct inspiration. Hardware neural networks have been researched <span epub:type="pagebreak" id="page_408"/>for many decades, but the 2010s saw them take off spectacularly with the GPUs used for deep learning. In the 2020s, NPUs began to appear on mobile phones and in the cloud for machine learning. Computational neuro-science research continues and may inspire radically different computer architectures, beyond current neural networks used in deep learning. As with all computer architectures, we’ll here consider the brain’s architecture on multiple levels of hierarchy, from its equivalents of transistors, through neurons (brain cells), up to its equivalent of computer design.</p>&#13;
<h4 class="h4" id="lev329"><em>Transistors vs. Ion Channels</em></h4>&#13;
<p class="noindent">Recall that a transistor is a digital switch, about 10 nm in diameter in modern chips. It has an input and output, and if you activate the switch, current flows between them. We’ve seen that transistors work by balancing several chemical and physical forces, and the switch tips this balance to allow the current to flow. Transistors (and chips in general) are made from semiconductors based around silicon, which makes four chemical bonds with neighboring atoms. Really understanding transistors needs chemistry and quantum mechanics.</p>&#13;
<p class="indent">The brain analog of the transistor isn’t the neuron but the <em>ion channel</em>, which is a subcomponent of a neuron, as shown in <a href="ch16.xhtml#ch16fig4">Figure 16-4</a>.</p>&#13;
<div class="image"><img id="ch16fig4" src="../images/f0408-01.jpg" alt="Image" width="412" height="199"/></div>&#13;
<p class="figcap"><em>Figure 16-4: An ion channel, in closed (left) and open (right) states. Ligands (3) bind to the channel (1) to open it, allowing ions (2) to flow through it.</em></p>&#13;
<p class="indent">Ion channels are single-molecule digital switches, also about 10 nm in diameter, made from proteins and built into the membranes of neurons. Depending on their switching state, they either allow or don’t allow certain chemicals to flow between the inside and the outside of the neuron. Their switching states are determined by the balance of electrical and chemical forces, which can be tipped when another chemical binds to the ion channel, or when a voltage is applied to it.</p>&#13;
<p class="indent">Ion channels (and brains in general) are based around carbon, which makes four chemical bonds with neighboring atoms. As with transistors, really understanding ion channels needs chemistry and quantum mechanics.</p>&#13;
<h4 class="h4" id="lev330"><span epub:type="pagebreak" id="page_409"/><em>Logic Gates vs. Neurons</em></h4>&#13;
<p class="noindent">Neurons (<a href="ch16.xhtml#ch16fig5">Figure 16-5</a>) are usually considered as the basic unit of computation in brains.</p>&#13;
<div class="image"><img id="ch16fig5" src="../images/f0409-01.jpg" alt="Image" width="798" height="492"/></div>&#13;
<p class="figcap"><em>Figure 16-5: A neuron</em></p>&#13;
<p class="indent">Neurons are about 1 µm in diameter. Computationally, they’re built from many ion channels. They also have many other cellular structures needed to support their existence and power requirements. They function as boxes that take a number of digital inputs and give one digital output, somewhat like the multi-input AND gate seen in <a href="ch06.xhtml#ch06fig2">Figure 6-2</a>. A multi-input AND gate’s function can be written mathematically using a Boolean algebra equation:</p>&#13;
<div class="imagec"><img src="../images/f0409-02.jpg" alt="Image" width="259" height="83"/></div>&#13;
<p class="noindent">Here, <em>b</em> is the Boolean “sum squashing function”:</p>&#13;
<p class="equ"><em>b</em> (<em>x</em>) = (<em>x</em> ≥ <em>N</em>)</p>&#13;
<p class="indent">Logic gates such as a multi-input AND are clocked, so their inputs and outputs are considered valid only for short periods of time, until the next computation begins.</p>&#13;
<p class="indent">In simple computational models, as typically used in current machine learning neural networks—and coded as a GPU kernel as in <a href="ch15.xhtml">Chapter 15</a>—a single neuron’s function is assumed to be given by the equation:</p>&#13;
<div class="imagec"><img src="../images/f0409-03.jpg" alt="Image" width="308" height="84"/></div>&#13;
<p class="noindent">Here the <em>w</em>s are adjustable weight values modified during learning, and <em>f</em> is the same squashing function:</p>&#13;
<p class="equ"><em>f</em> (<em>x</em>) = (<em>x</em> ≥ <em>N</em>)</p>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_410"/>This notation assumes that one of the inputs is set permanently to 1, rather than containing any actual data. This special input is known as a <em>bias</em> or <em>affinating</em> input; it’s needed to make most neural network models work.</p>&#13;
<p class="indent">Neurons typically “fire” for short periods of time, so their inputs and outputs are considered valid only for short periods of time until the next computation begins. Unlike logic gates, there is typically a lot of noise in neurons, which can be modeled by adding random numbers to their inputs. Some models consider this noise to be an important probabilistic aspect of their computation.</p>&#13;
<p class="indent">This is a very simple model of neuron function, and close relations of it work well for current machine learning applications, such as the <em>f</em> = <em>reLU</em> GPU neuron we built in <a href="ch15.xhtml">Chapter 15</a>. However, real biological neurons come in hundreds of different shapes and sizes that may have much more complex behaviors, and these have been argued to include more complex computations such as summations, multiplications, divisions, exponentiation, logarithms, temporal memory, and filtering. This school of thought emphasizes the complexity of neurons as whole living and computing cells in themselves, and reminds us of the complex computations performed by other single cells such as bacteria and sponge cells.</p>&#13;
<h4 class="h4" id="lev331"><em>Copper Wires vs. Chemical Signals</em></h4>&#13;
<p class="noindent">Let’s compare the wiring in the brain with the wiring in a chip. In a chip, we use photolithography to first lay down layers of transistors on a 2D plane. In modern chips, we then lay down a few overlapping layers of copper wire on top of the transistors to make connections between them, as we saw in <a href="ch04.xhtml#ch04fig19">Figure 4-19</a>. Communication over these wires is very fast and accurate, as it’s purely electrical. Messages are digital, meaning the wire is either high or low voltage, which can be viewed as representing 1 or 0.</p>&#13;
<p class="indent">Neurons are usually long, extended cells, including a long <em>axon</em> component that functions as a wire to carry information around the physical brain. Human axons range from 1 µm to 2 m long—the longest are the axons in the neurons connecting your toe to your brain. Communication is slow and noisy, as messages travel along axons via a complex biochemical process involving ion channels opening and closing to move chemicals in and out of the cell. When the end of the axon connects to another neuron (at a joint called a <em>synapse</em>), there’s a second biochemical process in which chemicals released from the first cell travel into the second one. Messages are digital: the axon is either firing or not firing, which can be viewed as representing 1 or 0. Architecturally, a whole neuron is thus analogous to a logic gate with a single long output wire.</p>&#13;
<h4 class="h4" id="lev332"><em>Simple Machines vs. Cortical Columns</em></h4>&#13;
<p class="noindent">The next architectural level is the simple machines level. In human-designed computers, simple machines are made from several logic gates that together perform some single useful function. There are many different standard simple machines, such as adders, decoders, and registers, each specialized <span epub:type="pagebreak" id="page_411"/>for a particular task. Typical simple machines can be laid out with TTL chips, as we saw in <a href="ch05.xhtml#ch05fig13">Figure 5-13</a>.</p>&#13;
<p class="indent">This is the least understood level of brain architecture, and therefore the most exciting topic for scientific research. Some researchers argue that the human cortex consists entirely of repeated <em>cortical column</em> microcircuits, each composed of a few hundred or thousand instances of a handful of different types of biological neuron, occupying a cylinder around 20 µm in diameter and 2 mm in depth.</p>&#13;
<p class="indent">The neurons forming the cortical column microcircuit are arranged across six distinct layers of cortex and always connected in the same specific way, as shown in <a href="ch16.xhtml#ch16fig6">Figure 16-6</a>. We know the connectivity between the populations of the different types of neurons here, but not the connectivity between individual neurons or the weights of the connections. There’s at least some superficial similarity between the structure in <a href="ch16.xhtml#ch16fig6">Figure 16-6</a> and the RAM seen in <a href="ch06.xhtml#ch06fig22">Figure 6-22</a>.</p>&#13;
<div class="image"><img id="ch16fig6" src="../images/f0411-01.jpg" alt="Image" width="468" height="578"/></div>&#13;
<p class="figcap"><em>Figure 16-6: The cortical microcircuit architecture</em></p>&#13;
<p class="indent">Some computer scientists have speculated that these microcircuits might function as building blocks of probabilistic or other calculations. The precise wiring of the module circuit remains unclear and requires advances in brain imaging technology before we can run a “debugger” on it to learn what it’s actually doing. Unlike with digital logic microcircuits, there appears to be only this one cortical microcircuit, which is used all over the cortex. Reverse engineering the cortical microcircuit is one of the biggest science challenges of the 21st century. It needs computer architects and their experience to help suggest computational functions, alongside biological neuroscientists to collect data and link to their biological knowledge, and <span epub:type="pagebreak" id="page_412"/>physicists to design new experimental equipment able to see this data. Nobel Prizes seem likely for those who crack its code.</p>&#13;
<h4 class="h4" id="lev333"><em>Chips vs. Cortex</em></h4>&#13;
<p class="noindent">At the highest level of structure, the cortex is surprisingly similar to chips. This is because they’re both laid out in 2D planes, and composed of tens of fairly independent modules with connections between them. For chips, we’re used to seeing 2D layouts such as in <a href="ch11.xhtml#ch11fig5">Figure 11-5</a>. For brains, it’s less obvious because the 2D sheet of the cortex is crumpled like a discarded piece of paper into three dimensions. It can easily be uncrumpled, though, and spread out across a 2D surface to show its true structure (<a href="ch16.xhtml#ch16fig7">Figure 16-7</a>). It’s this sheet that contains the six-layer microcircuits discussed earlier.</p>&#13;
<div class="image"><img id="ch16fig7" src="../images/f0412-01.jpg" alt="Image" width="791" height="315"/></div>&#13;
<p class="figcap"><em>Figure 16-7: The cortex appears three-dimensional (left), but uncrumpled it becomes a 2D sheet, like a paper page or silicon chip (right). The modules, known as Brodmann areas, are labeled with numbers.</em></p>&#13;
<p class="indent">The modules of cortex are known as <em>areas</em>, and most have been associated with particular functions and activities, such as vision, hearing, touch, and planning. Within each module, connectivity always follows the cortical microcircuit architecture and (arguably) a columnar structure, (arguably) with strong connectivity within each column and weaker connectivity between columns. Most modules have large bundles of axons that send output information to other modules. Projections are always from and to the same layers within these modules, as part of the microcircuit. We know which modules send outputs to which others, but not the detailed connectivity of which neurons connect to which within them.</p>&#13;
<p class="indent">This is all very like chip architectures, which also often have tens of modular components, each having strong connections within them, and more limited bundles of connections flowing between them. Modern chips have several layers of 3D-printed copper wires to connect components together, sharing the brain’s basic plan of a 2D layout with 3D connections linking different areas. A big difference, though, is that all the component areas of the brain share the same internal architecture of layers and columns, while the component areas of chips usually contain completely different designs.</p>&#13;
<h4 class="h4" id="lev334"><span epub:type="pagebreak" id="page_413"/><em>Parallel vs. Serial Computation</em></h4>&#13;
<p class="noindent">Consider how modules are connected together in a CPU or a cortex. CPUs are inherently serial machines, designed to execute programs of instructions in sequence. As such, a CPU has a clearly defined “top” of its design hierarchy, the control unit, that acts as the executive telling all the other modules what to do and when to do it; we saw this in <a href="ch07.xhtml#ch07fig13">Figure 7-13</a>.</p>&#13;
<p class="indent">A cortex also has a hierarchy, with frontal areas thought to be involved in executive control and posterior (rear) areas more involved in running perception and action. Perception and action for each sense (vision, touch, and so on) are known to consist of hierarchies of areas; for example, lower visual areas process edges and corners, and higher ones detect faces and named people. These areas all run in parallel, and they’re composed of columns that also all run in parallel. The frontal areas seem to coordinate the overall activity, but the perception and action areas can function by themselves when the frontal areas are damaged.</p>&#13;
<p class="indent">None of these modules are active unless triggered by the <em>thalamus</em>, which in this context looks and acts somewhat like a CPU control unit, and can be seen in the lower part of <a href="ch16.xhtml#ch16fig6">Figure 16-6</a>.</p>&#13;
<p class="indent">While the modules relay information directly to one another, they also communicate with regions of the thalamus that appear to mirror their structure and act to turn them on and off and resolve conflicts between them.</p>&#13;
<p class="indent">When you introspect your own subjective experience of computing solutions to complex high-level perception and action-planning problems, it may appear that your brain is operating like a serial machine, imagining and testing out different hypotheses and actions in a sequence. This observation can be supported by the more objective evidence that other humans take <em>O(N)</em> time for such tasks when timed in a lab. Internally, however, we also think of the brain as a massively parallel system, with all its neurons potentially in use simultaneously. This is similar to thinking of the CPU first as a serial processor, then thinking of it as a massively parallel digital logic circuit in which all its billions of transistors are potentially in use simultaneously. Outside the brain and CPU, both have external modules, whether connected by a spinal cord or a bus.</p>&#13;
<p class="indent">The <em>hippocampus</em> is a special part of the cortex: it sits right at the top of the hierarchy, and its microcircuit is a little different from the rest. Instead of having cortical layers that process data to send up to higher regions, the hippocampus has different layers, called DG, CA1, and CA3, that include feedback connections, connecting what would usually be outputs back into themselves. Rather than send computations up to more abstract layers of processing, they’re sent through <em>time</em> to the same, functionally highest, area of the cortex. Computational architectures have been developed based on the hippocampus, on the assumption that it’s used as a form of spatiotemporal memory. These architectures enable robots to navigate and map the space and objects around them.</p>&#13;
<p class="indent">Architects have been intrigued and inspired by the brain throughout the electronic era. Current interest in deep learning has brought some of these links into mainstream architectures, such as the neural processing <span epub:type="pagebreak" id="page_414"/>units now found in many phones. These architectures are loosely based on models of neurons and on hierarchical cortical areas. But we’ve seen here that real brains include much additional complexity—ion channels, cortical microcircuits, and emergent serial computation from parallel structures—that may provide inspiration for further developments. It’s common for philosophers to debate whether any silicon-based simulation of brain structures could fully replicate human intelligence or consciousness. Those arguing against typically invoke properties of physics that don’t usually appear in silicon, such as quantum effects. However, computer scientists have begun to explore computing with some of these effects too, as we’ll see in the next sections.</p>&#13;
<h3 class="h3" id="lev335">Quantum Architectures</h3>&#13;
<p class="noindent">Quantum computing is based on the physics of quantum mechanics, which is famously strange and counterintuitive. In quantum mechanics, objects no longer have precise locations or velocities; rather, they exist in wave-like states that range over many possible locations and velocities. These states define the probabilities of actually seeing the object at one of these locations or velocities when you look at it. Quantum concepts are genuinely mind-blowing, and will radically change your whole view of reality, causation, and time.</p>&#13;
<p class="indent">A full presentation of quantum mechanics or quantum computing is beyond the scope of this book. Here I can only give a flavor of the concepts and a glimpse at what the basic equations look like. It’s worth pointing out, however, that modern quantum computing can be studied with little or no reference to the usual presentation of quantum mechanics given in physics, making the field somewhat easier to approach. In particular, computer science is a mostly discrete subject, dealing with 0s, 1s, and sums, rather than the continuous real numbers and integrals typical of quantum mechanics. The discretized mathematics used in quantum computing requires only high school linear and matrix algebra, complex numbers, and probability.</p>&#13;
<h4 class="h4" id="lev336"><em>A Cartoon Version of Quantum Mechanics</em></h4>&#13;
<p class="noindent">The following is <em>not a correct presentation of quantum mechanics</em> and is intended only as a cartoon to introduce some key concepts.</p>&#13;
<p class="indent">Suppose that objects in the world don’t just exist in a single state at a time. For example, a cat inside a box may at the same time be both standing up, being alive, and also lying down, being dead. This famous example is known as <em>the superposed cat</em>. Suppose that the cat is locked inside the box along with a piece of radioactive material. Radioactive material decays completely at random: its behavior can’t be predicted in any way. A radiation decay detector is placed next to it, and connects to a mechanism that releases poison gas into the box, killing the cat if radiation is detected and leaving it alive if not.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_415"/>You leave this experimental apparatus alone for, say, 10 minutes. You might know something about the strength of the radiation, so you can say that after 10 minutes there’s some probability, say 20 percent, that a decay has taken place and the cat is dead, and some other probability, say 80 percent, that it hasn’t taken place and the cat is alive. We might represent the current “state” of the cat by a distribution such as:</p>&#13;
<p class="equ"><em>Cat</em> = {<em>alive</em> : 0.8, <em>dead</em> : 0.2}</p>&#13;
<p class="noindent">Classically—that is, without considering quantum mechanics—you would normally think of this distribution as being a property of your own <em>knowledge</em> rather than a property of the <em>world</em>. You would assume that the cat is actually in only one state or the other, either alive or dead. It’s just that your brain doesn’t know which, so <em>it</em> (your brain) contains a model carrying the two states and the probabilities.</p>&#13;
<p class="indent">In quantum mechanics this is absolutely and demonstrably <em>not</em> the case. The two versions of the cat aren’t only in your head but are also both actually out there in the world in some sense. Roughly, we imagine two versions of reality—one where the cat is alive and one in which it’s dead—existing together until the moment you open the box. When that moment comes, reality “decides” which state will be the actual one, randomly but according to the probabilities, and the other state goes away forever. We say that your act of observing the cat changes its state, from existing as two versions with probabilities to existing as a single version.</p>&#13;
<p class="indent">Now that we have the basic idea, let’s take a look at the math version. You aren’t expected to understand all of the math symbols, technical terms, or commands used in the rest of this section. If you happen to be familiar with linear algebra and complex numbers, then you can follow the details, but otherwise it’s okay just to glance over them to get a flavor of the field.</p>&#13;
<h4 class="h4" id="lev337"><em>The Math Version of Quantum Mechanics</em></h4>&#13;
<p class="noindent">The correct presentation of quantum mechanics consists of four rules; superposition, observation, action, and combination.</p>&#13;
<h4 class="h4a"><strong>Rule of Superposition</strong></h4>&#13;
<p class="noindent">Objects exist in a <em>superposition</em> of states, each with a complex number amplitude whose squared moduli sum to 1. For example:</p>&#13;
<div class="imagec"><img src="../images/f0415-01.jpg" alt="Image" width="155" height="90"/></div>&#13;
<p class="noindent">Here, <img class="middle" src="../images/f0415-02.jpg" alt="Image" width="70" height="22"/> and the rows of the vector represent the amplitudes of the cat being dead (binary state 0) and alive (binary state 1), respectively.</p>&#13;
<h4 class="h4a"><span epub:type="pagebreak" id="page_416"/><strong>Rule of Observation</strong></h4>&#13;
<p class="noindent">When you observe states in a basis, they collapse to one of the basis states of the observation basis, at random, according to the moduli of their <em>squared</em> amplitudes. For example:</p>&#13;
<div class="imagec"><img src="../images/f0416-01.jpg" alt="Image" width="554" height="65"/></div>&#13;
<p class="indent">These results are always real numbers between 0 and 1, representing the probabilities of observing each possibility.</p>&#13;
<h4 class="h4a"><strong>Rule of Action</strong></h4>&#13;
<p class="noindent">Any physical action, including computation, performed on the system—apart from observation—is modeled by a unitary matrix. The matrix operates on the state by ordinary matrix multiplication. For example, the action of a NOT gate is modeled by a matrix that swaps the dead and live states’ amplitudes:</p>&#13;
<div class="imagec"><img src="../images/f0416-02.jpg" alt="Image" width="458" height="91"/></div>&#13;
<p class="indent">Here, the NOT matrix, like all unitary matrices, preserves the property of state vectors that the sum of their probabilities, from the rule of observation, is 1.</p>&#13;
<h4 class="h4a"><strong>Rule of Combination</strong></h4>&#13;
<p class="noindent">The state of two objects considered together is the joint state formed by the tensor product of the individuals:</p>&#13;
<div class="imagec"><img src="../images/f0416-03.jpg" alt="Image" width="244" height="29"/></div>&#13;
<p class="indent">The |⟩ notation used in quantum mechanics and quantum computation is called <em>ket notation</em>. For discrete computer scientists, this simply denotes a column vector, which in other subjects is sometimes denoted by underlining, using an arrow, or using bold. The name comes from a pun on the word <em>bracket</em>. Inner products are sometimes written ⟨<strong><em>a</em></strong>| <strong><em>b</em></strong> ⟩ = <strong><em>a<sup>T</sup>b</em></strong>, which is called a “braket.” If we write ⟨<strong><em>a</em></strong>| = <strong><em>a <sup>T</sup></em></strong> and |<em>b</em>⟩ = <strong><em>b</em></strong>, then we can call these two new symbols the “bra” and the “ket,” which together make a “braket.”</p>&#13;
<h4 class="h4" id="lev338"><em>Quantum Registers of Qubits</em></h4>&#13;
<p class="noindent">Could we exploit the apparent existence of interacting parallel realities as a form of parallel computation? If we could distribute computational work across the parallel realities, as we would more normally distribute across multiple CPUs in a single reality, and then somehow find a way to combine the results into the single reality in which we happen to be ourselves, then we could exploit the vast additional computational resources in those other realities. We could build a single CPU and have it compute many things at once across the parallel realities, instead of needing to build many CPUs. <span epub:type="pagebreak" id="page_417"/>This idea, first proposed by Richard Feynman in 1988, is the beginning of quantum computing.</p>&#13;
<p class="indent">Consider the superposed cat. We could use the cat’s aliveness and dead-ness as a 1-bit data representation via the encoding <em>dead</em> = 0 and <em>alive</em> = 1. Call this a <em>qubit</em>, for <em>quantum bit</em>. We could then build an <em>N</em>-bit register by placing <em>N</em> of these cats-in-boxes in a row, to store words, as in a classical register based on flip-flops. Until we open the boxes in the register, there are multiple realities in which the cats inside them are alive and dead. When we open them, we see just one version of reality and that becomes the reality we experience ourselves.</p>&#13;
<p class="indent">Like a classical register, an <em>N</em>-qubit quantum register has 2<sup><em><sup>N</sup></em></sup> possible states. Each of these states of the whole register can exist at the same time in a “parallel world.” This is a much larger set of states than just the <em>N</em> cats.</p>&#13;
<p class="indent">You can play with this using a quantum computer simulator such as QCF (Quantum Computing Functions). In QCF, you can begin by creating non-superposed register states, such as:</p>&#13;
<pre>&gt;&gt; <span class="codestrong1">phi_1 = bin2vec('011')</span>&#13;
[0 0 0 1 0 0 0 0]</pre>&#13;
<p class="noindent">The resulting output shows a state vector for a 3-bit register that’s entirely in the 011 state (representing the decimal number 3). There’s zero amplitude of being in the zeroth state 000; zero amplitude of being in the first state, 001; zero amplitude of being in the second state, 010; full amplitude, 1, of being in the third state, 011; and zero amplitude of the other states, up to the seventh, 111.</p>&#13;
<p class="indent">QCF also has a command to create similar, non-superposed states directly from the decimal numbers being represented; for example, to create a 3-bit register entirely in the state representing decimal 5, use this command:</p>&#13;
<pre>&gt;&gt; <span class="codestrong1">phi_2 = dec2vec(5, 3)</span>&#13;
[0 0 0 0 0 1 0 0]</pre>&#13;
<p class="indent">So far, these are only the same single states that a classical 3-bit register could exist in. Next, we can simulate a register in a superposition of both of these states at the same time, such as:</p>&#13;
<pre>&gt;&gt; <span class="codestrong1">psi = [1/sqrt(2)*phi_1 + 1/sqrt(2)*phi_2]</span>&#13;
[0 0 0 0.7071 0 0.7071 0 0 ]</pre>&#13;
<p class="noindent">To simulate a measurement (observation) of this register, we can do this:</p>&#13;
<pre>&gt;&gt; <span class="codestrong1">psi = measure(psi)</span></pre>&#13;
<p class="noindent">This will randomly produce one of the following two outputs, with probabilities given by the squared amplitudes:</p>&#13;
<pre>[0 0 0 0 0 1 0 0]&#13;
[0 0 0 1 0 0 0 0]</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_418"/>The individual qubits’ states aren’t independent; they’re <em>entangled</em>. In our QCF example, the first two observed bits must read either 01 or 10; they can’t read 11 or 00. Therefore, if you look at the first bit initially and see a 0, it means you’ll see a 1 if you later look at the second bit, and vice versa. This remains the case even if the qubits are physically transported millions of miles apart before either observation.</p>&#13;
<p class="indent">The register in our example may be modeled as existing in eight (that is, 2 binary digits ^ 3 bits) states at the same time, across a set of eight “parallel worlds.” The number of worlds grows exponentially with register size; for example, a 64-bit quantum register has 2<sup>64</sup> ≈ 2 × 10<sup>19</sup> states, the same number as the number of addresses in the entire address space of a 64-bit machine, existing all at the same time in a single register.</p>&#13;
<p class="indent">Physicists usually don’t like to talk in terms of “parallel worlds.” Instead, they prefer to “shut up and calculate” to predict the outcome of a particular scenario: it’s all just math once they’ve been given a system to analyze. To <em>create</em> new quantum programs, however, it’s useful for computer scientists to think of each state of the register existing in a parallel world. Thinking in this way helps you to visualize what you’re creating and suggests ideas for what to create next.</p>&#13;
<h4 class="h4" id="lev339"><em>Computation Across Worlds</em></h4>&#13;
<p class="noindent">The amplitudes, but not the contents, of the states can affect one another in certain ways that are very limited by the rules of quantum mechanics, enabling interaction between the parallel worlds during computation. The billion-dollar question in quantum computing is always: how do we read back the results? We observe only <em>one</em> of the parallel worlds, and the one we get is random, so we need to find mechanisms that ensure either that the result we want to see exists in all of the worlds, or that the world we observe happens to be the one with a single copy of the result in it.</p>&#13;
<p class="indent">For example, we might try to parallelize the traveling salesperson problem by superposing a register so that each world has part of the register encoding a different possible route. Within each world, we then calculate the length of that route and store it in another part of the register. Then we answer a question such as “Does this route have a length less than 5?” and store the result as a single bit in a third part of the register. But our task is to answer a question about the whole set of possible routes, such as “Does any route have length less than 5?” This is a function of the information stored in all of the worlds.</p>&#13;
<p class="indent">Finding ways to get that information all into one place that we’re guaranteed, or even just likely, to see when we make an observation forms the hard part of quantum algorithm design, and as far as we know, these ways all introduce large computational complexity overheads. As a result (again, as far as we know), quantum computers aren’t able to make <em>P = NP</em>, but they <em>are</em> able to speed up <em>NP</em> problems to lower complexities within <em>NP</em>. Most quantum algorithms, such as <em>Grover’s algorithm</em>, work by gradually updating state amplitudes so that all worlds that we don’t want to see cancel each other out, <span epub:type="pagebreak" id="page_419"/>and only the world that we do want to see remains with a large probability of appearing in the actual world. This is a similar approach to DNA computing’s PCR, which also performs computing over time to amplify the desired solution at the expense of the others. Some researchers believe that quantum computers will provide a general speedup of <img class="middle" src="../images/f0419-01.jpg" alt="Image" width="75" height="28"/> via this approach, but theory is still needed to confirm this.</p>&#13;
<p class="indent">There are a few particular problems, such as breaking public key cryptography, that are known to have larger speedups due to their structures being especially close matches to the quantum laws. Finding and classifying these special cases is a current research topic.</p>&#13;
<h4 class="h4" id="lev340"><em>Practical Quantum Architectures</em></h4>&#13;
<p class="noindent">Small-scale quantum computers, having just a few qubits, have been successfully constructed and demonstrated to prove that the concept works. The main barrier to larger practical quantum computers is <em>decoherence</em>. This is the problem that <em>any</em> interaction between a superposed system and anything in the rest of the world tends to spread out the superposition into that thing and then into the rest of the world. The amount of superposition behaves roughly like a fixed resource, so once it leaks out of your computer it’s gone and it can’t be used in your computation anymore. Quantum engineers are working hard to design ways to isolate quantum systems from all outside influence. This is a somewhat similar problem to nuclear fusion, in which we set off a nuclear explosion and then try to use magnets to keep it controlled and contained from its surroundings.</p>&#13;
<p class="indent"><em>Adiabatic quantum computing</em> is sometimes reported in the media—notably by the company D-Wave Systems, which has successfully sold devices to Google and the US government—as successfully performing quantum computing with 1,000 bits or more. However, adiabatic quantum computing isn’t quantum computing in the sense we’ve discussed. It’s a different physical process based on a very different mathematical model that assumes time is continuous rather than discrete, so that an infinite number of observations can be made in any given time interval. Completely opposite to quantum computing, it relies on observations (or decoherence, in some views) taking place continually, rather than trying to shield the system from them; the observations form the essential part of the actual computation. Many quantum computing researchers are highly skeptical of these claims, noting that there’s a long history of cranks in normal computer science claiming to have made <em>P = NP</em> via models that similarly assume infinite amounts of computation performed in finite time intervals.</p>&#13;
<p class="indent"><em>Rose’s law</em> has been proposed as a quantum version of Moore’s law, hypothesizing that the number of qubits in working quantum computers is currently doubling every two years.</p>&#13;
<h3 class="h3" id="lev341"><span epub:type="pagebreak" id="page_420"/>Future Physics Architectures</h3>&#13;
<p class="noindent">Beyond what’s currently called quantum computing, we might more generally turn to modern physics and ask what else it’s discovered that might also be made into computing machinery.</p>&#13;
<p class="indent">Our best current theory of physics, the <em>Standard Model</em>, is based on <em>quantum field theory (QFT)</em>, which combines quantum mechanics with special (but not general) relativity to model reality as comprising a set of fields that each cover space and interact with one another. Each field corresponds roughly to one type of particle, and as in basic quantum mechanics, its amplitudes are those of finding a particle there if we look for it. Unlike basic quantum mechanics, the fields are also able to represent probabilities of finding multiple particles at locations, and it’s possible for these particles to interact and transform into one another in various ways.</p>&#13;
<p class="indent">The Standard Model specifies particular fields and interactions to make a quantum field theory with 17 types of particle. (More accurately: the fields are a gauge quantum field containing the internal symmetries of the unitary product group <em>SU</em>(3) × <em>SU</em>(2) × <em>U</em>(1), and the 17 particle types emerge as patterns across several of these fields.) The Standard Model has been tested experimentally since the 1960s and hasn’t changed since then. CERN confirmed the final Higgs field in 2012. A few anomalies are now known that suggest a better model might one day be found.</p>&#13;
<p class="indent">Particle accelerators such as CERN have perfected the ability to not only observe but also control individual particles of the fields. Beams of different types of particles can be reliably produced, collided with each other or with test objects, and the individual particles flying out of the collision observed.</p>&#13;
<p class="indent">Particle physics has thus given rise to particle engineering, in which this technology is reused not to do science but to build practical engineered systems for other purposes. Governments have funded particle physics for many decades, not for inherent interest in what the world is made of, but because of weaponization potential. The beams firing around CERN can kill anything in their path. American 1980s BEAR experiments put an accelerator in space, able to produce and fire beams over huge distances, trying to destroy satellites—and eventually ground targets—with laser-like precision. Accelerators and detectors are also being repurposed for treating brain cancer. By firing proton beams through the brain and detecting changes in their speeds, we can infer tumor structures with higher accuracy and less damage than other methods. Once these are known, beam strength can be turned up to destroy the tumors, again more accurately than with other methods.</p>&#13;
<p class="indent">Now that particle engineering has begun to develop, it’s natural to ask if, like mechanical, electrical, and electronic engineering before it, it can be used to construct new computer hardware. It might one day be possible to use particles other than electrons and photons from the Standard Model to store and compute with data—for example, creating a Higgs boson–based computer. Such computers might be constructed by accelerating particles, then using their interactions to form computations, perhaps as microscopic billiard ball logic gates, such as seen in <a href="ch05.xhtml">Chapter 5</a>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_421"/>QFT isn’t a complete theory of physics, because it omits gravity, which is instead modeled by Einstein’s <em>general relativity (GR)</em>. GR is incompatible with QFT because, unlike QFT, it allows space and time to change shape, bending around mass. We rely on relativistic engineering every day—for example, for time correction among GPS satellites, to correct for warping of telescope images, and to correct paths for missions to Mars and elsewhere. As predicted by Einstein, gravitational waves were observed in 2016, and are now becoming a new tool for astronomy. These effects are small and subtle. In contrast, while engineering systems to actively manipulate and exploit bending space-time is possible in theory, it requires astronomical scales of energy and mass. It may take centuries or millennia, or be impossible, to obtain these. Gödel’s “closed timelike curves” can occur in GR if space-time loops around on itself to form a “wormhole” shortcut path between perhaps engineerable points in time and space, including backward time travel.</p>&#13;
<p class="indent">Observers in GR may see events occur in different temporal orders depending on where they are and how they move. The notion of a sequential <em>program</em> becomes problematic if observers can’t agree on the order in which instructions are executed, with later stages of execution appearing to cause earlier ones. Time runs at different speeds for different GR observers, so if we live on a large mass, we could make a computer run faster by sending it far away from this mass. However, accelerating and decelerating it for this journey have the opposite effect of slowing its time, which would need to be balanced against any gains.</p>&#13;
<p class="indent"><em>Hypercomputation</em> theorists have claimed theoretical machines with formal powers stronger than Church computers. They could use GR to predict their own future behavior by looking at their own past in a closed timelike curve, and thus solve the halting problem. This would require a radical update of our concept of computation.</p>&#13;
<p class="indent">QFT and GR famously don’t fit together, so we have no working “Grand Unified Theory” (GUT) to explain the structure of reality. Current attempts include “string theory/M-theory,” “loop quantum gravity,” and “twistor theory,” but none actually work yet. Some of these theories postulate the existence of extra dimensions. Some theories try to model a “graviton” as an additional particle, to treat gravity similarly to the other forces in the Standard Model. This might be of interest for computation, because any gravitons must have zero mass and travel at the speed of light, like photons, but must also be able to interact with each other, unlike photons. This would avoid the non-interaction problem of speed-of-light photonic computers.</p>&#13;
<p class="indent">Meanwhile, discoveries about galaxy and galaxy supercluster structure and motion are challenging both QFT and relativity. Observations appear to require either the invention of new “dark matter” and “dark energy” particles, such as “axions,” or the replacement of relativity with a new theory. If we find that the world is made from superstrings, twistors, gravitons, or axions, then we can also look for ways to use their properties to represent data and perform computation.</p>&#13;
<h3 class="h3" id="lev342"><span epub:type="pagebreak" id="page_422"/>Summary</h3>&#13;
<p class="noindent">A new golden age of architecture is upon us. There’s never been a better time to get involved in architecture, both as a user and as an architect. Open source hardware and software now enables you to design and build serious CPUs at home, and to contribute them to the community.</p>&#13;
<p class="indent">Taking the long view of computing history, as in <a href="ch01.xhtml">Chapter 1</a>, suggests that modern ICs are just one of many possible computing technologies that come and go. The end of Moore’s law for clock speed has already forced us to move to parallel architectures, but Moore’s law for transistor density must also end as we reach scales of single atoms and quantum effects. This may force us to switch to entirely new technologies.</p>&#13;
<p class="indent">Optical computing is limited by photons’ non-interactivity with one another, though at least in the special case of convolution filters it’s possible to make use of interactions within their waves, which are a coincidentally good fit to current deep learning computations.</p>&#13;
<p class="indent">DNA computing seems unlikely to appear on consumer desktops, but may have a niche for solving large one-off NP-hard problems. Your university or public transportation timetable might one day be optimized by a swimming pool full of DNA.</p>&#13;
<p class="indent">The human brain continues to inspire new architecture ideas. Going beyond current deep learning architectures, it could lead to ideas for micro-circuit-based simple machines and the emergence of serial behavior from massively parallel systems.</p>&#13;
<p class="indent">Quantum computing is now a well-understood theory, but with research still progressing around its difficult implementation and only a theoretical understanding of what speedups it can provide. Quantum computing is based on quantum mechanics, which has been superseded by QFT and perhaps by attempts at GUTs. Some of these theories are still glints in physicists’ eyes, but as with every other technology, from rocks to gears to silicon chips, they may also one day form the basis for future computer architectures.</p>&#13;
<h3 class="h3" id="lev343">Exercises</h3>&#13;
<h4 class="h4a"><strong>Crank Speedups</strong></h4>&#13;
<p class="noindent">You could solve any computation problem in 1 second of wall clock time using an Analytical Engine if you assume that you can turn its crank at arbitrary higher and higher speeds. Why would that not work? What might this tell us about adiabatic quantum computing claims?</p>&#13;
<h4 class="h4a"><strong>Challenging</strong></h4>&#13;
<ol class="number">&#13;
<li class="tm">Download QCF from <em><a href="https://github.com/charles-fox/qcf">https://github.com/charles-fox/qcf</a></em> and work through the examples shown in the “Quantum Architectures” section on <a href="ch16.xhtml#lev335">page 414</a>.</li>&#13;
<li class="tm">QCF comes with a longer tutorial that builds up to running Grover’s algorithm; work through this.</li>&#13;
<li class="tm"><span epub:type="pagebreak" id="page_423"/>Which technology do you think will yield practical new computers first: quantum, optical, DNA, neural, or other? Write a blog post articulating why.</li>&#13;
</ol>&#13;
<h3 class="h3" id="lev344">Further Reading</h3>&#13;
<ul class="bullet">&#13;
<li class="tm">For DIY fabrication, see Stephen Cass, “The Garage Fab,” <em>IEEE Spectrum</em> 55, no. 1 (2018): 17–18.</li>&#13;
<li class="tm">For graphene transistors, see F. Wu et al., “Vertical MoS2 Transistors with Sub-1-nm Gate Lengths,” <em>Nature</em> 603 (2022): 259–264.</li>&#13;
<li class="tm">An example of 3D integrated circuits is Vasilis Pavlidis, Ioannis Savidis, and Eby Friedman, <em>Three-Dimensional Integrated Circuit Design</em>, 2nd ed. (Burlington: Morgan Kaufmann, 2017).</li>&#13;
<li class="tm">For details of 10,000-year storage, see J. Zhang et al., “5D Data Storage by Ultrafast Laser Nanostructuring in Glass,” paper presented at CLEO: Science and Innovations, San Jose, June 2013.</li>&#13;
<li class="tm">For a general introduction to optical computing, see Jürgen Jahns and Sing H. Lee, eds., <em>Optical Computing Hardware</em> (Boston: Academic Press, 1994).</li>&#13;
<li class="tm">For details of deep learning with optical correlators, see J. Chang et al., “Hybrid Optical-Electronic Convolutional Neural Networks with Optimized Diffractive Optics for Image Classification,” <em>Scientific Reports</em> 8, no. 12324 (2018).</li>&#13;
<li class="tm">For a popular science introduction to DNA computing, see Martyn Amos, <em>Genesis Machines: The New Science of Biocomputation</em> (London: Atlantic Books, 2006).</li>&#13;
<li class="tm">For details of the traveling salesperson problem with DNA computing, see J. Lee et al., “Solving Traveling Salesman Problems with DNA Molecules Encoding Numerical Values,” <em>BioSystems</em> 781, no. 3 (2004): 39–47.</li>&#13;
<li class="tm">For details of DNA inkjet printing, see T. Goldmann and J. Gonzalez, “DNA-Printing: Utilization of a Standard Inkjet Printer for the Transfer of Nucleic Acids to Solid Supports,” <em>Journal of Biochemical and Biophysical Methods</em> 42, no. 3 (2000): 105–110.</li>&#13;
<li class="tm">The definitive text on quantum computing is Michael A. Nielsen and Isaac L. Chuang, <em>Quantum Computation and Quantum Information</em> (Cambridge: Cambridge University Press, 2000).</li>&#13;
<li class="tm">For the origin of quantum computing, including links to heat, energy, and information issues in computing, see Richard Feynman, <em>The Feynman Lectures on Computation</em> (London: Westview Press, 1996).</li>&#13;
<li class="tm">For an overview of biological neural architectures, see Larry Swanson, <em>Brain Architecture: Understanding the Basic Plan</em> (Oxford: Oxford University Press, 2011).</li>&#13;
<li class="tm"><span epub:type="pagebreak" id="page_424"/>For the definitive guide to the many complex computations by single neurons, beyond simple models, see Christof Koch, <em>Biophysics of Computation</em> (Oxford: Oxford University Press, 1999).</li>&#13;
<li class="tm">For examples of advanced computations performed by single-cell organisms, see R. Lahoz-Beltra, J. Navarro, P. Marijuan, “Bacterial Computing: A Form of Natural Computing and Its Applications,” <em>Frontiers in Microbiology</em> 5, no. 101 (2014).</li>&#13;
<li class="tm">See <em><a href="https://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html">https://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html</a></em> for an interactive 3D view of human cortical microcircuit connectivity.</li>&#13;
<li class="tm">For a popular science introduction to future physics, see Brian Greene, <em>The Elegant Universe: Superstrings, Hidden Dimensions, and the Quest for the Ultimate Theory</em> (New York: Vintage, 2000).</li>&#13;
</ul>&#13;
</div>
</div>
</body></html>