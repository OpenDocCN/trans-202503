- en: '**15**'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**PARALLEL ARCHITECTURES**'
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Image](../images/f0355-01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: 'As we’ve discussed, computing is splitting along two paths: low-power systems,
    forming the Internet of Things; and high-power computing centers, forming the
    cloud. In previous chapters, we’ve looked at the low-power IoT side of the split:
    embedded and smart systems. This chapter will look at the high-power, high-performance
    systems found in the cloud. Specifically, we’ll look at parallelism, the backbone
    of cloud computing.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: The rise of parallelism is related to Moore’s two laws. While Moore’s law for
    density says we can still put more and more transistors on chips, Moore’s law
    for clock speed is now over, meaning we can’t clock single CPUs faster anymore.
    The number of fetch-decode-execute cycles per second is no longer increasing,
    so we need to find new uses for the extra available transistors to try to do more
    work *within* each cycle, rather than making faster cycles.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'For a while, we got away with using the extra silicon to boost classical, serial
    architectures: we made more and more complex CISC instructions to get more work
    per instruction; we added more and bigger registers levels of cache onto the CPU
    silicon; we replicated structures such as arithmetic logic units (ALUs) to enable
    simultaneous execution of branches; and we constructed fancier pipelines and out-of-order
    machines. Together, these techniques have recently delivered double digit–percentage
    yearly gains in instructions per cycle (IPC) rather than cycles per second. But
    we may be running out of easy wins in these areas, so we have to think more in
    terms of digital logic being inherently parallel. Luckily for us, it is.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: We’ve already encountered register- and instruction-level parallelism. Register-level
    parallelism is the simultaneous per-column execution of digital logic acting on
    bits of a register. For example, all the bits in a word can be negated at the
    same time rather than in sequence. Instruction-level parallelism includes pipelining,
    branch prediction, eager execution, and out-of-order execution (OOOE). These concepts
    don’t appear at the instruction set architecture (ISA) level; they’re invisible
    to the assembly programmer. From the programmer’s perspective, they just make
    serial programs execute faster.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll focus on the higher-level parallelisms that are visible
    in the ISA and therefore may require the attention of the assembly and perhaps
    also the high level–language programmer. We’ll begin by thinking about parallel
    foundations. Then we’ll turn to the two main types of parallelism: *single instruction,
    multiple data (SIMD)*, as found in modern CPUs and GPUs, and *multiple instruction,
    multiple data (MIMD)*, as found in multicores and cloud computing centers. Finally,
    we’ll wrap up by considering more radical, instructionless forms of parallelism
    that might take architecture beyond the concepts of CPUs and programs.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Serial vs. Parallel Thinking
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of the silicon in a serial computer is used to form memory that sits around
    doing nothing until it’s called on to load from or store to the CPU. In this sense,
    serial computing is like having 1,000 people send all their work to a single worker,
    then stand around waiting for the results to come back to them. This effect is
    known as the *serial bottleneck*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel computing frees these 1,000 people to all work for themselves. Each
    becomes an active unit of computation: they pass data directly to one another
    as needed, and they get massively more work done than if they were standing around
    waiting for that one worker. Similar gains can occur if we use all the digital
    logic in a computer to constantly perform computation rather than wait around
    for the CPU.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel thus seems to be obviously faster and better than serial computing.
    But at least until the 2010s, computer scientists tended to get stuck in “serial
    thinking.” Most people are at some point taught the concept of programming using
    a recipe, assuming that only you are in the kitchen and that you’re going to perform
    a sequence of tasks, such as:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is fine on a small scale, but if you’re a head chef running a chain of
    popular restaurants, you’d be in charge of a team of workers, and you’d have to
    schedule in optimal ways to produce the food more efficiently. The field of operations
    research is all about optimally scheduling work like this.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: How do we take a sequence of instructions like the chicken soup recipe and get
    everything done in the shortest period of time? There are well-known algorithms
    to do this. For example, Henry Gantt’s charts, like the one shown in [Figure 15-1](ch15.xhtml#ch15fig1),
    are used to display and reason about sequences of tasks running in parallel over
    time.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0357-01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-1: A parallel Gantt chart for cooking chicken soup*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Simple algorithms exist to generate optimal timings for the tasks, given a list
    of dependencies—that is, a list of which tasks depend on the completion of which
    others before they can begin. A critical path can be calculated for the network,
    which is the sequence of jobs that need to be done on time because they’re the
    bottlenecks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Bletchley Park made heavy use of this style of computation. Machines weren’t
    the only types of computers used there: it was still the era of human computation,
    where “computer” was a human job title. Human computers would sit in a computing
    division ([Figure 15-2](ch15.xhtml#ch15fig2)), all doing parts of computations
    under a manager allocating and scheduling the work in parallel. These programmer
    managers thought about how to break down a large mathematical computation into
    components, distribute the tasks, and collect the results together.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0358-01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-2: A human computing division working in parallel, with a manager
    (standing) scheduling the work*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Given that the management of teams of parallel workers has existed for a long
    time, and is based on how to design a program of work to efficiently accomplish
    a task, why do so many programmers basically ignore it and think instead in terms
    of recipes and serial computing? If the history of computing had been different
    and started from an operations research perspective rather than from serial algorithms,
    we might have had a much better foundation. Programming—and perhaps the foundations
    of computer science—is now having to move toward parallel thinking due to the
    end of Moore’s law for clock speed. For example, today’s school children might
    write their first ever program in Scratch with multiple sprites all running code
    in parallel. And professional programmers are increasingly having to think in
    terms of SIMD and MIMD, which we’ll study next.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于并行工作团队的管理已经存在很长时间，并且其基础是如何设计工作程序以高效完成任务，为什么那么多程序员基本上忽视它，而是转而思考食谱和串行计算呢？如果计算机历史从运筹学的角度开始，而不是从串行算法开始，我们可能会有一个更好的基础。由于摩尔定律对时钟速度的影响已经结束，编程——甚至可能是计算机科学的基础——现在不得不转向并行思维。例如，今天的孩子们可能会在Scratch中编写他们的第一个程序，所有的角色（精灵）都在并行运行代码。而专业程序员则越来越需要以SIMD和MIMD为思维方式进行思考，这就是我们接下来要学习的内容。
- en: Single Instruction, Multiple Data on CPU
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CPU上的单指令多数据
- en: Our first type of parallelism—single instruction, multiple data—means we’re
    going to take a single instruction (for example, “add one”) and execute that instruction
    uniformly on multiple data items at once. We can split SIMD systems into CPU-
    and GPU-based implementations. Here we’ll look at the CPU-based implementation;
    in the next section, we’ll look at the GPU-based one.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一种并行处理类型——单指令多数据——意味着我们将采取单一指令（例如，“加一”）并在多个数据项上同时执行该指令。我们可以将SIMD系统分为基于CPU和基于GPU的实现。这里我们将关注基于CPU的实现；在下一部分，我们将探讨基于GPU的实现。
- en: '*Introduction to SIMD*'
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*SIMD简介*'
- en: 'SIMD on CPU is a very CISC-style approach: it involves creating additional
    instructions and digital logic to perform parallel operations as single instructions.
    SIMD instructions pack more than one piece of data into a word, then define instructions
    to apply the same instruction to each piece of data in parallel. For example,
    on a 64-bit machine, instead of using a 64-bit register to store one big 64-bit
    integer, we can partition it into four 16-bit chunks that each hold one 16-bit
    integer. We can then use instructions that understand this packing and operate
    on all four chunks at the same time.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: CPU上的SIMD是一种非常CISC风格的方法：它涉及创建额外的指令和数字逻辑，以便将并行操作作为单个指令执行。SIMD指令将多个数据项打包到一个字中，然后定义指令对每个数据项并行应用相同的操作。例如，在一个64位的机器上，我们可以将一个64位寄存器分割成四个16位的块，每个块存储一个16位的整数。然后我们可以使用理解这种打包方式的指令，同时对四个块进行操作。
- en: In a standard CPU, you might have an `ADD` instruction that adds integers from
    registers r1 and r2, storing the result in r3\. In an SIMD machine, however, you’ll
    have an instruction called something like `SIMD-ADD`, still using the same three
    registers, but using a different data representation to perform addition on pairs
    of 16-bit values from the two registers simultaneously; it then stores the output
    in the third register, packed similarly.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的CPU中，你可能有一个`ADD`指令，它将寄存器r1和r2中的整数相加，并将结果存储在r3中。然而，在SIMD机器中，你将有一个类似`SIMD-ADD`的指令，依然使用相同的三个寄存器，但使用不同的数据表示法同时对来自两个寄存器的16位值对进行加法运算；然后将结果以类似的方式存储在第三个寄存器中。
- en: '**NOTE**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*SIMD instructions originated in early supercomputers, such as the famous 1960s
    Cray supercomputers. SIMD was first brought from supercomputers to desktops by
    Intel via their MMX instructions.*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*SIMD指令起源于早期的超级计算机，如著名的1960年代Cray超级计算机。SIMD最早是通过英特尔的MMX指令从超级计算机引入桌面计算机的。*'
- en: SIMD can split a 64-bit register into two 32-bit chunks, four 16-bit chunks,
    or eight 8-bit chunks. The four-way split is especially useful for 3D games. It’s
    common for 3D programmers to represent 3D coordinates using *four*-dimensional
    vectors, with the fourth dimension serving as a scaling factor to enable *affine
    transformations*. These are transformations like translations and rotations computed
    using simple matrix-vector multiplications. The 16-bit precision of the numbers
    is usually acceptable for games (though maybe not for serious scientific 3D simulations).
    We’re lucky to live in a world whose number of dimensions, when affinated, is
    a power of 2!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD可以将一个64位寄存器分割为两个32位块、四个16位块或八个8位块。四分法对于3D游戏特别有用。3D程序员通常使用*四*维向量表示3D坐标，其中第四维作为缩放因子，用于实现*仿射变换*。这些是通过简单的矩阵-向量乘法计算的变换，如平移和旋转。对于游戏来说，通常16位精度的数字是可以接受的（虽然对于严肃的科学3D模拟可能不够）。我们很幸运生活在一个经过仿射变换后，维度数是2的幂的世界！
- en: SIMD is also a good fit for images and video, in which pixel colors are often
    represented by four numbers for RGB and alpha (as discussed in [Chapter 2](ch02.xhtml)).
    More generally, for most types of multimedia, including audio, it’s common to
    need to do many copies of the same operations for signal processing, so SIMD can
    speed this up even when there’s no obvious 4D structure.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD也非常适合图像和视频处理，其中像素颜色通常由四个数字表示，分别代表RGB和alpha通道（如在[第2章](ch02.xhtml)中讨论的）。更一般来说，对于大多数类型的多媒体，包括音频，通常需要对信号处理执行许多相同操作的拷贝，因此即使没有明显的4D结构，SIMD也能加速这一过程。
- en: SIMD instructions can be created for use with any ordinary registers, but they’ve
    become more interesting as register sizes have increased to 64 bits. Some architectures
    also include extra registers that are longer than their word length, known as
    *vector* registers; these can store 128, 256, or 512 bits, and they’re intended
    primarily for use with SIMD instructions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD指令可以为任何普通寄存器创建，但随着寄存器大小增加到64位，它们变得更加有趣。一些架构还包括比其字长更长的额外寄存器，称为*向量*寄存器；这些寄存器可以存储128、256或512位，主要用于SIMD指令。
- en: Now that we understand the theory of CPU SIMD, let’s look at a concrete example
    of how it’s implemented in x86.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了CPU SIMD的理论，接下来我们来看一个x86中如何实现它的具体例子。
- en: '*SIMD on x86*'
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*x86上的SIMD*'
- en: We saw the names of the various x86 architectures of the 64-bit era in [Chapter
    13](ch13.xhtml). Beyond the basic amd64 instruction set, most of these architectures
    have focused on adding extensions using different forms of parallelism. Most of
    these ideas originated in high-performance computing and high-end servers but
    have also been introduced to desktop architectures.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第13章](ch13.xhtml)中看到了64位时代各种x86架构的名称。除了基本的amd64指令集之外，这些架构大多数集中在使用不同形式的并行性添加扩展。这些想法大多源自高性能计算和高端服务器，但也已引入到桌面架构中。
- en: The classic CISC approach has been to use the extra transistors to add more
    simple machines and instructions to the ISA, each intended to do more work than
    the regular instructions. This has led to thousands of new CISC instructions,
    added for all manner of special cases such as cryptography, multimedia processing,
    and machine learning. There have been disagreements over the standards for these
    extensions. Everyone implements the same base amd64 ISA, but different manufacturers
    extend it in different ways to add extensions. They try to get users hooked on
    their versions and to desert competitors (a well-known strategy called *embrace-extend-extinguish*).
    This creates headaches for compiler writers who have to create multiple back-ends
    to optimize for the different extensions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的CISC方法是利用额外的晶体管增加更多简单的机器和指令到ISA中，每个指令旨在比常规指令做更多的工作。这导致了成千上万的新CISC指令的出现，针对各种特殊情况，如加密、多媒体处理和机器学习。对于这些扩展的标准存在一些争议。每个人都实现了相同的基础amd64
    ISA，但不同的制造商以不同的方式扩展它，添加各自的扩展。它们试图让用户依赖自己的版本，并抛弃竞争对手（这是一种著名的策略，叫做*拥抱-扩展-消灭*）。这给编译器开发者带来了麻烦，因为他们必须为不同的扩展创建多个后端来进行优化。
- en: Most of the new registers and instructions added to x86 during the 64-bit era
    have been for SIMD. [Figure 15-3](ch15.xhtml#ch15fig3) shows the complete user
    register set of modern amd64.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在64位时代，x86新增的大部分寄存器和指令都与SIMD有关。[图15-3](ch15.xhtml#ch15fig3)展示了现代amd64的完整用户寄存器集。
- en: '![Image](../images/f0360-01.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0360-01.jpg)'
- en: '*Figure 15-3: The full register set for amd64*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15-3：amd64的完整寄存器集*'
- en: The SIMD registers are the ones with “MM” in their names. Notice how new SIMD
    registers have appeared over time, usually by extending an existing register to
    have more bits. When extensions are made, x86 backward compatibility requires
    the original shorter form to still be named and usable, as well as the extended
    form. This requires many different versions of instructions to be provided.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD 寄存器是那些名称中包含“MM”的寄存器。注意到，随着时间的推移，新的 SIMD 寄存器逐渐出现，通常通过扩展现有的寄存器来增加更多的位数。当进行扩展时，x86
    向后兼容性要求原本较短的形式仍然要有名字并且可用，同时也要有扩展后的形式。这就要求提供多种版本的指令。
- en: '**MMX**'
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**MMX**'
- en: '*MMX* was the first x86 SIMD extension. It’s never been officially defined
    what MMX stands for, and indeed this has been a matter of legal trademarking debate
    between Intel and AMD. Suggestions include “matrix math extensions” and “multimedia
    extensions.”'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*MMX* 是第一个 x86 SIMD 扩展。至今没有正式定义 MMX 代表什么，事实上，这一直是英特尔和 AMD 之间关于商标的法律争议问题。有人提出的建议包括“矩阵数学扩展”和“多媒体扩展”。'
- en: MMX extended the previous amd64 floating-point registers to 64 bits, similar
    to how 32-bit registers, such as EAX, were extended to the 64-bit RAX. The new
    registers have names MM0 through MM7 and still exist on modern machines.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: MMX 将之前的 amd64 浮点寄存器扩展到了 64 位，类似于 32 位寄存器（如 EAX）扩展为 64 位的 RAX。新的寄存器命名为 MM0 到
    MM7，并且在现代机器上仍然存在。
- en: Each MMX register can be used for integer-only SIMD as either a single 64-bit
    integer, two 32-bit integers, four 16-bit integers, or eight 8-bit integers. Integer
    SIMD is particularly useful and fast for processing images, including for 2D sprite-based
    games and video codecs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 MMX 寄存器可以作为一个单独的 64 位整数、两个 32 位整数、四个 16 位整数或八个 8 位整数来进行仅整数的 SIMD 操作。整数 SIMD
    特别适用于处理图像，包括 2D 精灵游戏和视频编解码器。
- en: 'MMX instructions begin with `p` for “packed,” such as `paddd` for “packed add
    doubles.” New move instructions—`movb, movw`, and `movd`—copy arrays of bytes,
    words, or doubles into single MMX registers. For example, the following defines
    two arrays of 32-bit doubles: *a* = [4, 3] and *b* = [1, 5]. It loads *a* as packed
    doubles into MM0 and *b* into MM1\. It then packed adds the doubles, leaving [5,
    8] in MM0:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: MMX 指令以 `p` 表示“打包”，例如 `paddd` 表示“打包加双字”。新的移动指令——`movb`, `movw` 和 `movd`——将字节、字或双字数组复制到单个
    MMX 寄存器中。例如，下面定义了两个 32 位双字数组：*a* = [4, 3] 和 *b* = [1, 5]。它将 *a* 作为打包的双字加载到 MM0
    中，将 *b* 加载到 MM1 中。然后，它对这些双字进行打包加法运算，最终将 [5, 8] 存储在 MM0 中：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: MMX adds a *lot* of new instructions, because every arithmetic operation has
    to exist in each of the packed forms for bytes, words, and doubles.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: MMX 添加了*大量*的新指令，因为每个算术操作必须在字节、字（word）和双字（double）打包形式中都存在。
- en: '**SSE**'
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**SSE**'
- en: Intel’s version of x86 SIMD has been extended several times since MMX, as SSE,
    SSE2, SSE3, SSE4, and SSE4.2 (where SSE stands for streaming SIMD extensions).
    AMD’s latest incompatible competitor is, confusingly, called SSE4a. Unlike MMX,
    the SSE series provides for floating-point SIMD as well as integers. This makes
    it particularly useful for accelerating 3D math for games and other physics simulations.
    (MMX was unsuccessful by the benchmarks of its time, which focused heavily on
    the 3D game *Quake*.)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 英特尔的 x86 SIMD 版本自 MMX 以来已经多次扩展，分别为 SSE、SSE2、SSE3、SSE4 和 SSE4.2（其中 SSE 代表流式 SIMD
    扩展）。AMD 最新的、不可兼容的竞争版本令人困惑地被称为 SSE4a。与 MMX 不同，SSE 系列不仅支持整数的 SIMD 还支持浮点数。这使得它在加速游戏和其他物理仿真中的
    3D 数学计算时特别有用。（根据当时的基准测试，MMX 在 3D 游戏 *Quake* 中的表现并不成功。）
- en: Unlike MMX’s extension of the old floating-point registers, SSE adds completely
    new, 128-bit vector registers, called XMM0 through XMM31\. The number of these
    has grown with the SSE versions. They can be split into 8-, 16-, 32-, or 64-bit
    chunks, with chunks representing either floating points or integers. Each arithmetic
    operation thus has many instructions depending on these choices.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与 MMX 扩展旧的浮点寄存器不同，SSE 添加了完全新的 128 位向量寄存器，命名为 XMM0 到 XMM31。这些寄存器的数量随着 SSE 版本的更新而增加。它们可以被拆分为
    8 位、16 位、32 位或 64 位的块，每个块可以表示浮点数或整数。因此，每个算术操作会有多种指令，具体取决于这些选择。
- en: Most SSE instructions have the letter `p` for “packed” added to their mnemonics,
    at either the beginning or the end. For example, the top-left of [Figure 15-4](ch15.xhtml#ch15fig4)
    shows an SSE compare for equality with the `cmpeqps` instruction. The name comes
    from `cmpeq`, the standard x86 instruction, plus `ps` to indicate “packed, single-precision.”
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 SSE 指令在助记符的开头或结尾加上字母 `p` 来表示“打包（packed）”。例如，[图 15-4](ch15.xhtml#ch15fig4)的左上方展示了用于相等比较的
    SSE 指令 `cmpeqps`。该名称来自于标准的 x86 指令 `cmpeq`，再加上 `ps` 表示“打包，单精度”。
- en: '![Image](../images/f0361-01.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0361-01.jpg)'
- en: '*Figure 15-4: The contents of two SSE registers, XMM0 and XMM1, as SSE instructions
    are carried out, comparing the two sets of data in different ways: equality (top
    left), inequality (top right), less than (bottom left), and not less than (bottom
    right)*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15-4：两个 SSE 寄存器 XMM0 和 XMM1 中的内容，执行 SSE 指令时，以不同方式比较这两组数据：相等（左上）、不等（右上）、小于（左下）和不小于（右下）*'
- en: In the top-right of [Figure 15-4](ch15.xhtml#ch15fig4), the `compneqps` instruction
    similarly extends `cmpneq` (compare not equal) to SSE.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 15-4](ch15.xhtml#ch15fig4)的右上角，`compneqps` 指令类似地将 `cmpneq`（不等比较）扩展到 SSE。
- en: 'The following code shows examples of getting arrays of floats in and out of
    SSE’s XMM registers and performing arithmetic on them:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何将浮点数数组从 SSE 的 XMM 寄存器中读出并写入，并对其执行算术运算：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, the `addps` instruction adds the four numbers in XMM1 to the four numbers
    in XMM0, and stores the result in XMM0\. For the first float, the result will
    be 1.1 + 5.5 = 6.6\. The `mulps` instruction multiplies the four numbers in XMM1
    with the results from the previous calculation (in XMM0), and stores the result
    in XMM0\. For the first floats, this result will be 5.5 × 6.6 = 36.3\. The `subps`
    instruction subtracts the four numbers from `v2` (in XMM1, still unchanged) from
    the result of the previous calculation (in XMM0). For the first float, its result
    will be 36.3 – 5.5 = 30.8.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`addps` 指令将 XMM1 中的四个数字加到 XMM0 中的四个数字，并将结果存储回 XMM0。对于第一个浮点数，结果将是 1.1 + 5.5
    = 6.6。`mulps` 指令将 XMM1 中的四个数字与先前计算的结果（XMM0 中的结果）相乘，并将结果存储回 XMM0。对于第一个浮点数，结果将是
    5.5 × 6.6 = 36.3。`subps` 指令将来自 `v2`（仍未改变的 XMM1 中）的四个数字从先前计算的结果（XMM0 中）中减去。对于第一个浮点数，结果将是
    36.3 – 5.5 = 30.8。
- en: '**AVX**'
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**AVX**'
- en: Two generations of *Advanced Vector Extensions (AVX)* have added longer vectors
    than SSE, having 256- and 512-bit lengths. The new 256-bit registers are called
    YMM0 through YMM31, and the new 512 registers are called ZMM0 through ZMM31.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 两代 *高级矢量扩展（AVX）* 增加了比 SSE 更长的向量，具有 256 位和 512 位的长度。新的 256 位寄存器称为 YMM0 到 YMM31，新的
    512 位寄存器称为 ZMM0 到 ZMM31。
- en: 'AVX instructions often have the same names as, and behave similarly to, SSE
    instructions, but they start with a `v`. For example, to add eight pairs of 32-bit
    (double) floating-point numbers using AVX-256, we can do this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: AVX 指令通常与 SSE 指令具有相同的名称，并且行为类似，但它们以 `v` 开头。例如，要使用 AVX-256 对八对 32 位（双精度）浮点数进行加法，我们可以这样做：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note how the form of AVX arithmetic is different from MMX and SSE, with addition
    now taking three operands rather than two.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 AVX 算术的形式与 MMX 和 SSE 的不同，现在加法需要三个操作数，而不是两个。
- en: '**Domain-Specific Instructions Using SIMD**'
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**使用 SIMD 的领域特定指令**'
- en: 'As mentioned earlier, SIMD is usually considered a very CISC approach, as it
    involves adding lots of new instructions to the ISA. Initially, these arise from
    the many combinations of packing styles, data types, and arithmetic operations.
    Going beyond simple replication of arithmetic across the chunks, CISC SIMD has
    also tended to create further complex instructions. These might include *horizontal
    SIMD*, which means instructions that *combine* information from multiple chunks
    in the same register. For example, there are instructions that find the minimum
    of multiple chunks in a register: `phminposuw` in SSE or `vphminposuw` in AVX.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，SIMD 通常被认为是一种非常 CISC 的方法，因为它涉及向 ISA 添加大量新指令。最初，这些指令来源于许多不同的打包方式、数据类型和算术操作的组合。在简单地跨块复制算术操作的基础上，CISC
    SIMD 还倾向于创建更复杂的指令。这些指令可能包括 *水平 SIMD*，意味着那些 *合并* 来自同一寄存器中多个块的信息的指令。例如，有一些指令可以找到寄存器中多个块的最小值：SSE
    中的 `phminposuw` 或 AVX 中的 `vphminposuw`。
- en: Horizontal SIMD instructions also sometimes sequence simpler SIMD instructions
    together. For example, “dot product of packed double-precision floating-point
    values” (`dppd` on SSE; `vdppd` on AVX) is a single instruction that performs
    a complete vector dot product, often used in games, 3D simulations, and machine
    learning. This consists of first SIMD multiplying pairs of chunks, then summing
    the results horizontally along the register.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 水平 SIMD 指令有时也会将简单的 SIMD 指令串联在一起。例如，“打包双精度浮点值的点积” (`dppd` 在 SSE 上；`vdppd` 在 AVX
    上) 是一条执行完整向量点积的单一指令，常用于游戏、3D 仿真和机器学习中。它首先执行 SIMD 相乘一对对的块，然后在寄存器中水平地对结果进行求和。
- en: 'Cryptography has been a major source of CISC SIMD extensions. For example,
    128-bit AES is the NSA-approved standard for internet encryption. It’s computed
    via four steps: ShiftRows, SubBytes, MixColumns, and AddRoundKey. Intel has added
    CISC instructions for each of these steps, and also a single mega-instruction
    that combines them all to perform an entire round (`aesenc` on SSE; `vaesenc`
    on AVX). If, like most end users, you spend the bulk of your computing time streaming
    videos over HTTPS, then this CISC approach gives a useful targeted speedup for
    your use case. But Intel’s extensions have been controversial, with Linus Torvalds
    stating that the NSA and Intel have likely back-doored them in the digital logic,
    and advising Linux programmers not to use them.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning—specifically, neural network—operations have been the latest
    target for SIMD CISC, via Intel’s Vector Neural Network Instructions (AVX512-VNNI)
    and Brain Floating Point (AVX512-BF16) extensions to AVX-512, which arrived in
    Golden Cove and are marketed together as *DL Boost*. For example, “multiply and
    add unsigned and signed bytes with saturation” (`vpdpbusds`) performs a full neuron’s
    sigmoid-like activation from its inputs and weights in a single instruction. Some
    researchers have been able to train neural networks faster than on a GPU using
    these and similar SIMD CISC instructions, so this is now a competition between
    CPU and GPU architectures.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '**Compiler Writers and SIMD**'
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The only compiler writers who understand and care about x86 SIMD are the ones
    working for Intel and AMD, so proprietary CISC compilers are likely to go faster
    than open source or third-party compilers (such as gcc) for numerical code on
    CISC architectures.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Intel has released various C libraries, implemented with its own compilers,
    that convert high-level numerical code into SIMD instructions. These include Integrated
    Performance Primitives (IPP), the Math Kernel Library (MKL), and the IPEX PyTorch-to-AVX
    compiler for neural networks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Open source compiler writers find it hard to get excited about particular proprietary
    hardware extensions and CISC, and they generally prefer to spend their valuable,
    scarce time on more general-purpose work to benefit the wider community, such
    as generating beautiful RISC code that will be accelerated through methods like
    pipelining and OOOE.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '**SIMD ON RISC-V**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: SIMD instructions are a fundamentally CISCy idea—they add lots of new instructions
    and digital logic, making the instruction set more complex. However, SIMD extensions
    have also been proposed for RISC-V, such as P for parallel SIMD instructions and
    V for vector instructions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: No real-world architecture is purely RISC or CISC nowadays, and there’s no law
    against a primarily RISC-style architecture such as RISC-V adding some CISCy features,
    especially as RISC-V’s extension system makes them completely optional. There
    have, however, been loud opposing voices in the open source RISC-V community,
    offended by this potential CISC insertion. Even its founders have published an
    “SIMD considered harmful” warning.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在没有任何现实世界中的架构是纯粹的 RISC 或 CISC，而且没有法律禁止像 RISC-V 这样主要采用 RISC 风格的架构加入一些 CISC 特性，特别是因为
    RISC-V 的扩展系统使得这些特性完全是可选的。然而，在开源 RISC-V 社区中确实有一些强烈反对的声音，他们对这种潜在的 CISC 插入感到不满。即使是其创始人也发布了“SIMD
    被认为有害”的警告。
- en: Good RISC style is rather to make use of extra available silicon to optimize
    pipelines and OOOE, for example by replicating ALUs, registers, and other components
    needed to run several branches in parallel. This approach may be made harder by
    the existence of SIMD instructions, especially the most extreme CISCy, multi-step
    ones, such as dot products, which do both multiplication and addition. Multicores
    are generally more acceptable to RISC, and RISC-V has an A extension for atomic
    memory instructions that provides multicore transactions for them.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 好的 RISC 风格实际上是利用额外的可用硅来优化流水线和超出顺序执行（OOOE），例如通过复制 ALU、寄存器和其他组件来支持多个分支的并行执行。SIMD
    指令的存在，特别是最极端的 CISC 风格的多步指令，如点积（既进行乘法又进行加法），可能会使这种方法变得更加困难。多核通常更适合 RISC，RISC-V
    也有一个用于原子内存指令的 A 扩展，提供了多核事务支持。
- en: SIMD on GPU
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPU 上的 SIMD
- en: SIMD appears on a much larger scale in GPUs. SIMD on CPU gives speedups of 2
    to 64 times, based on the number of chunks packed into a word. By contrast, a
    GPU can scale to thousands of identical instructions running simultaneously across
    the data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD 在 GPU 中出现的规模要大得多。CPU 上的 SIMD 提供了 2 到 64 倍的加速，具体取决于每个字中打包的块的数量。相比之下，GPU
    可以扩展到数千条相同的指令同时在数据上运行。
- en: In [Chapter 13](ch13.xhtml), we saw how graphics cards evolved, from providing
    hardware implementations of graphics commands, to providing their own parallel
    machine code for non-graphical computing. Initially, this was very hard, geeky
    work, involving encoding big computational algorithms into shaders as if they
    were graphics computations, exploiting the highly parallel 3D rendering hardware,
    then decoding the resulting images to obtain the computational output.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 13 章](ch13.xhtml) 中，我们看到显卡是如何发展的，从提供图形命令的硬件实现，到为非图形计算提供自己的并行机器码。最初，这是一项非常困难、极客化的工作，涉及将大规模的计算算法编码成着色器，仿佛它们是图形计算，利用高度并行的
    3D 渲染硬件，然后解码生成的图像以获取计算结果。
- en: GPU manufacturers quickly noticed this as a new market and redesigned their
    shader languages into general-purpose GPU instruction sets for general-purpose
    SIMD computing. These can be used to implement graphics shaders as before, but
    now also to implement general, non-graphical SIMD computations. This evolution
    has occurred rapidly to form GPUs that aren’t built for graphics at all, but rather
    for general higher-power scientific and machine learning computation, especially
    neural networks. This is why “graphics processing unit” is now a misnomer; a modern
    GPU is really more of a “general parallel unit.”
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 制造商迅速意识到这是一个新市场，并重新设计了他们的着色器语言，转变为通用 SIMD 计算的通用 GPU 指令集。这些指令集不仅可以用于实现之前的图形着色器，现在还可以用于实现通用的非图形
    SIMD 计算。这一演变迅速发生，形成了不再专为图形设计的 GPU，而是为了通用更高功率的科学计算和机器学习计算，尤其是神经网络。因此，“图形处理单元”现在已经是一个误称；现代
    GPU 更像是一个“通用并行单元”。
- en: '*GPU Architecture*'
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*GPU 架构*'
- en: It used to be difficult to discuss GPU architectures in general because they
    were each developed by different companies according to different, secret designs.
    However, several of these manufacturers got together to agree on *Khronos* standards,
    which define ways of thinking about GPU hardware architecture at a level of abstraction
    common to most of them. This enables most GPUs, and also some other devices, to
    be viewed as if their hardware was implemented as the standard architecture, so
    the programmer doesn’t have to care about their individual details so much. Programmers
    can also easily swap one GPU for another, including between manufacturers, as
    long as the new manufacturer provides software tools to convert programs from
    Khronos standards to their more specific machine codes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Khronos defines a hierarchy of named entities. We have a single *host* (the
    computer), which may have multiple *compute devices* inside (the physical GPU
    cards or chips). There are multiple *compute units (CUs)* in these, and they each
    contain *processing elements (PEs)*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: The main structure is the CU, whose PEs contain their own registers and ALUs,
    but share a single program counter, instruction register, and control unit. This
    creates the SIMD, as each processing element within the CU executes the same instruction
    from the PC in parallel, but on its own data from its own registers. A CU may
    also contain other structures such as a cache and some shared memory, allowing
    the PEs to communicate with one another. Compute devices typically package several
    independent CUs together. SIMD exists only within single CUs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '*Khronos standards are designed to be generalizable, not just to many different
    types of GPU but also to any other SIMD-implementing technologies. For example,
    they could also be implemented on an FPGA or on an SIMD CPU in some cases. This
    is why the generic name “compute device” is used in place of “GPU.”*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: The die shot in [Figure 15-5](ch15.xhtml#ch15fig5) shows what GPU silicon actually
    looks like. It shows that the die is arranged much more regularly than the layout
    of a CPU, with square CUs split evenly throughout and a general cache in the middle.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0366-01.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-5: A die shot taken from an Nvidia Pascal GPU chip*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '*Nvidia GPU Assembly Programming*'
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In CPUs, SIMD is expressed by single instructions that perform a fixed number
    (for example, four or eight) of identical operations in parallel. Programs are
    written as a series of such instructions, with one instruction referenced from
    the program counter executing at a time. In GPUs, however, SIMD is usually expressed
    differently: we want to enable large and arbitrary numbers of copies of the instruction
    to run in parallel, rather than a fixed number.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Khronos defines software-level concepts to express GPU SIMD programs. A *kernel*
    is a usually small function written by the user programmer, with the intent of
    each (assembled) line of the code running as a single instruction on multiple
    data. A *work-item* is one instance of the kernel—that is, the sequence of instructions
    as applied to a single piece of data by running on one processing element. A *work-group*
    is the collection of work-item instances running over the multiple data items.
    Unlike CPU SIMD, kernel code is written by describing its effects on a single
    work-item. When you run a kernel, you choose and specify how many work-items you
    want to launch in parallel.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Khronos定义了软件级的概念，用来表达GPU的SIMD程序。*内核*是用户程序员编写的通常较小的函数，目的是让代码中的每一行（汇编后的）作为单个指令在多个数据上运行。*工作项*是内核的一个实例——也就是说，通过在一个处理元素上运行，指令序列应用到单个数据项上。*工作组*是多个工作项实例的集合，它们在多个数据项上并行运行。与CPU的SIMD不同，内核代码是通过描述其对单个工作项的影响来编写的。当你运行一个内核时，你可以选择并指定要并行启动多少个工作项。
- en: 'Graphics shaders are traditionally small, simple programs that perform a fixed
    sequence of operations on each pixel. They’re thus well suited to SIMD, with work-items
    for each pixel stepping through the same instructions in the same order. However,
    other kinds of compute kernels may require branching. This presents a problem,
    somewhat in the same spirit as pipeline hazards, where different work-items need
    to take different branches. Taking different branches destroys the SIMD because
    the work-items are no longer running the same instructions. There are two methods
    to deal with kernel branching: masking and subgroups.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图形着色器通常是小型、简单的程序，它们对每个像素执行一系列固定的操作。因此，它们非常适合SIMD，每个像素的工作项按照相同的顺序执行相同的指令。然而，其他类型的计算内核可能需要分支。这就会出现一个问题，类似于流水线冒险的情况，不同的工作项需要执行不同的分支。执行不同的分支会破坏SIMD，因为工作项不再执行相同的指令。处理内核分支有两种方法：屏蔽和子组。
- en: Like 1980s CPU designers, modern GPU designers each maintain their own, mutually
    incompatible ISAs that define their platforms. Nvidia is the most popular GPU
    designer at the time of writing, so we’ll learn to program their ISA as an example
    of programming GPUs in general. As with other systems we’ve programmed in this
    book, we’ll simplify the truth a little in order to make learning easier. We’ll
    here assume that all general-purpose Nvidia GPUs implement a single ISA called
    PTX (Parallel Thread Execution), and we’ll learn to program in PTX assembly. You
    can assemble and run PTX programs on any general-purpose Nvidia GPU.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 就像1980年代的CPU设计师一样，现代GPU设计师每个人都维护自己的、不兼容的ISA（指令集架构），这些ISA定义了他们的平台。Nvidia是撰写本文时最流行的GPU设计公司，因此我们将以编程Nvidia的ISA为例，学习如何编程GPU。与我们在本书中编程的其他系统一样，为了简化学习，我们将略微简化事实。我们假设所有通用的Nvidia
    GPU都实现了一个名为PTX（并行线程执行）的单一ISA，我们将学习如何用PTX汇编编程。你可以在任何通用的Nvidia GPU上汇编和运行PTX程序。
- en: '**Data Movement and Arithmetic**'
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**数据移动和算术运算**'
- en: 'The following is a simple PTX kernel program. Like all kernels, many copies
    forming a work-group are intended to run in SIMD parallel, so this code describes
    only the actions of a single work-item:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简单的PTX内核程序。像所有内核一样，构成工作组的多个副本旨在并行运行，因此这段代码仅描述了单个工作项的操作：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'PTX assembly is written with semicolons at line ends, and two slashes for comments.
    Register names are conventionally written starting with a percent symbol. We’ll
    use four groups of registers: `r` denotes 32-bit integer registers; `rd` denotes
    64-bit (double) int registers; `fd` denotes 64-bit (double) floating-point registers;
    and our fourth group, `tid`, denotes internal registers used to store information
    about the parallelism.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: PTX汇编代码的行尾使用分号，并且用双斜杠表示注释。寄存器名称通常以百分号符号（%）开头。我们将使用四组寄存器：`r`表示32位整数寄存器；`rd`表示64位（双精度）整数寄存器；`fd`表示64位（双精度）浮点寄存器；第四组，`tid`，表示用于存储并行性信息的内部寄存器。
- en: As usual, most instructions use three operands, with the first being the destination
    and the others being inputs. As we have several types of register available, most
    instruction names use Amiga-like suffixes, separated by periods, to indicate which
    version is being used. For example, `add.s64` means addition for 64-bit signed
    integers, while `mult.wide.f64` means wide (full) multiplication of 64-bit floats.
    Load `(ld)` and store `(st)` have suffixes to indicate whether to use global or
    local memory. The `cvt` instruction means convert, and with various suffixes it
    converts numbers between signed and unsigned integers and floats of the different
    bit sizes. As usual, `ret` is return.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，大多数指令使用三个操作数，第一个是目标，其他的是输入。由于我们有多种寄存器可用，大多数指令名称使用类似 Amiga 的后缀，通过句点分隔，用来表示使用的是哪一版本。例如，`add.s64`
    表示对 64 位有符号整数的加法运算，而 `mult.wide.f64` 表示对 64 位浮点数进行宽（全）乘法运算。加载 `(ld)` 和存储 `(st)`
    指令有后缀，表示是否使用全局内存或本地内存。`cvt` 指令表示转换，通过各种后缀，它可以在不同位大小的有符号和无符号整数与浮点数之间转换。像往常一样，`ret`
    表示返回。
- en: The above program, as well as the rest of the PTX programs shown here, assumes
    at the start that registers rd1 through rd3 contain the global memory addresses
    of three arrays of doubles, with rd1 and rd2 being inputs, which we’ll nickname
    `x` and `w`, and rd3 being the output, which we’ll nickname `out`. (The reason
    for these conventions will become clear later, when we get to neurons.)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的程序以及这里展示的其他 PTX 程序都假设开始时寄存器 rd1 到 rd3 包含三个双精度数组的全局内存地址，其中 rd1 和 rd2 是输入，我们将其昵称为
    `x` 和 `w`，而 rd3 是输出，我们将其昵称为 `out`。（这些约定的原因稍后会变得清晰，尤其是在我们谈到神经元时。）
- en: The program’s function is very simple. It completely ignores the two inputs.
    It then obtains its threadID, which is a unique integer assigned to each work-item
    in the work-group. For example, if we were to launch a work-group of 5,000 copies
    of the program, each one would be given a unique threadID in the range 0 to 4,999
    in its `tid.x` register during the launch. The work-item then writes a copy of
    its threadID into the corresponding element of the output array. For example,
    the 573rd work-item, with threadID 573, will write the floating-point number 573.0
    into the 573rd element of `out`. If we launch a work-group of 5,000 copies in
    SIMD, they’ll each write a single such number into `out` simultaneously, so that
    the `out` array then contains the list of numbers from 0 to 4,999 when they complete
    together.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序的功能非常简单。它完全忽略了两个输入。然后它获取其 threadID，这是分配给工作组中每个工作项的唯一整数。例如，如果我们启动一个包含 5000
    个程序副本的工作组，每个副本在启动时都会在其 `tid.x` 寄存器中获得一个唯一的 threadID，范围从 0 到 4999。然后，工作项将其 threadID
    的副本写入输出数组的相应元素。例如，第 573 个工作项，threadID 为 573，将浮点数 573.0 写入 `out` 数组的第 573 个元素。如果我们以
    SIMD 启动一个包含 5000 个副本的工作组，它们每个都会同时将这样一个数字写入 `out`，因此当它们一起完成时，`out` 数组将包含从 0 到 4999
    的数字列表。
- en: Although PTX uses 64-bit words (which can be restricted to 32-bit, as seen in
    the example), it still uses byte addressing. This means that adding 1 to an address
    moves forward through memory by 8 bits. To move along by a 64-bit word, we have
    to add 8 to an address. The program thus works by obtaining its threadID, multiplying
    it by 8, adding the result to the address of `out`, and storing a floating-point
    version of the threadID at that address. The final `out` array thus contains [0,
    1, 2, 3, 4, 5, . . .].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 PTX 使用 64 位字（如在示例中所见，它可以限制为 32 位），但它仍然使用字节寻址。这意味着地址加 1 会使内存向前移动 8 位。要按 64
    位字进行移动，我们必须将地址加 8。因此，程序的工作原理是获取其 threadID，将其乘以 8，将结果加到 `out` 的地址上，并在该地址存储 threadID
    的浮点版本。最终的 `out` 数组因此包含 [0, 1, 2, 3, 4, 5, . . .]。
- en: '**Branching**'
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**分支**'
- en: The definition of SIMD is that parallel copies of the kernel execute identical
    instructions together as the program executes. Branching runs smoothly in SIMD
    in GPUs if and only if all of the copies take the same branches, but it becomes
    complex to handle if they need to take different branches.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD 的定义是内核的并行副本在程序执行时一起执行相同的指令。如果所有副本都走相同的分支，GPU 中的分支在 SIMD 中顺利运行，但如果它们需要走不同的分支，则变得复杂。
- en: 'For example, the following PTX kernel uses branching:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，下面的 PTX 内核使用了分支：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The first two and last four lines are the same as the previous program. But
    after the first two lines, the program tests if the threadID is less than 4\.
    If so, it adds 3.1 to the threadID. If not, it multiplies the threadID by 10.0\.
    Whichever of these results was obtained is then placed in the threadID-th element
    of `out` as before. On completion, `out` thus contains:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The complexity here is that we only want a work-item to execute some of the
    lines if a condition is true or false. In PTX, we first test for the condition
    (less than, `lt`) and set a *predicate* register to true or false to store the
    result. Predicate registers are internal registers, usually written as `p1, p2`,
    and so on, which can be set and tested similarly to the status flags we’ve seen
    in the Analytical Engine and other systems. Unlike those flags, there are many
    predicate registers that can each store predicates for long periods without overwriting
    the previous comparison result. Once we’ve set a predicate, we can indicate that
    some lines should be executed only if the predicate is true, or if it is false.
    These indicators are known as *predicate guards*, and in PTX assembly are written
    as, for example, `@%p1` at the start of a line. Different GPUs, including different
    Nvidia models, may handle predicate guards in two ways: masking or subgroups.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '*Masking* is a simple, pure SIMD method suitable for small branches, such as
    `if...else` statements without jumps. The kernel is executed in SIMD at all times,
    meaning that all copies share the same program counter and execute the same line
    of code at the same time. If a line is guarded, the PE tests the predicate, and
    if the line should not execute, then the PE replaces it with an NOP (no operator),
    as in a CPU pipeline stall. This enables multiple work-items to remain synchronized,
    with those that need to execute the instruction doing so while the others wait
    around for them via these NOPs. This wastes some time, with PEs executing NOPs
    from one branch and real instructions from the other, but it enables all work-items
    to stay synchronized as SIMD at all times.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '*Subgroups* (a Khronos term, aka “local groups,” “warps,” “waves,” or “wavefronts”
    by some manufacturers) are a more heavyweight solution that goes beyond pure SIMD
    to accommodate conditional jumps. All the work-items in a work-group start out
    running in pure SIMD until a predicate guard is encountered. When this occurs,
    the work-group is split into two subgroups, with work-items in one subgroup taking
    the branch and those in the other not taking it. The subgroups are then treated
    as two independent SIMD programs and are executed independently, either on two
    different CUs, if available, or in series on a single CU if only one is available.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Every branch in the program creates an additional subgroup split, so, for example,
    a program with four branches in a series can lead to 2⁴ = 16 subgroups. At this
    point, the number of physically available CUs determines the efficiency of execution,
    rather than the PEs within a CU. This is clearly not sustainable for larger programs
    with many possible branching series. However, subgroups can be merged back together
    (“resynchronized”) if the programmer can find a way to do so. Typically this can
    be done when branching has occurred due to different work-items taking different
    numbers of loop repetitions. In this case, the programmer can ask the work-items
    whose loops have completed first to wait until the others have also completed;
    this is known as a synchronization *barrier* and is represented by a special barrier
    instruction in assembly and machine code.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: The challenges of branching make SIMD quite a restrictive style of programming.
    It’s well suited to graphics shaders, which typically have no or minimal branching,
    but it’s tricky for programs that require parallel threads to take many different
    branches. Neural networks and physical simulations are two major classes of code
    that have similar minimal-branching structure to graphics; thus, they’ve greatly
    benefited from GPU acceleration. If you need different threads to be doing completely
    different things from one another, however, then SIMD isn’t appropriate. You need
    MIMD, as seen later in the chapter.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '**Large Work-Groups**'
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes we need to run more copies of a kernel than there are available PEs
    in the CU. For example, a pixel shader needs to run for every one of about eight
    million pixels for a 4K display, while only thousands or tens of thousands of
    PEs may be available.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'In these cases, a similar subgroup method can be used as in branching: you
    split the work-group into a number of smaller subgroups such that each subgroup
    runs physically together in SIMD on the PEs, and multiple subgroups can either
    run in series on a single CU or simultaneously across several available independent
    CUs. Unlike in branching, these subgroups are chosen to exactly match the total
    number of PEs. These maximal-sized subgroups are known as *blocks* by some manufacturers,
    with the set of subgroups known as a *grid*.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Work-items within every subgroup will be allocated the same set of threadIDs,
    corresponding to the position of the PE in its CU. It’s common to need to convert
    between these and the global “jobID.” For example, if you have eight million pixels
    to compute and 10,000 PEs, the four-millionth job needs to know that it should
    write to the four-millionth pixel rather than to its threadID-th pixel, which
    has a maximum value of 10,0000.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a common need, so PTX provides some extra machinery to assist with
    programming it, as in the following example:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, two additional internal registers, `ntid.x` and `ctaid.x`, are loaded
    automatically during kernel launches, with the subgroup size and a new ID saying
    which subgroup is being run. By multiplying and adding these using the dedicated
    `mad` instruction, we recover the global job ID and proceed as usual. (The rest
    of the program is the same as the first one, storing a float version of this jobID
    at the jobID-th location in `out`. The difference is that this now works for much
    larger `out` arrays—with millions of elements.)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '**A GPU Neuron**'
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now let’s look at a larger example kernel, which computes a neuron for a convolutional
    deep neural network (CNN). This is roughly how GPUs are used in machine learning:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, 0d3DA5FD7FE1796495 is floating-point zero. As in all our examples, we
    assume at the start that registers rd1 through rd3 contain the global memory addresses
    of three arrays of doubles, with rd1 and rd2 being inputs which we nickname *x*
    and *w*; rd3 is the output, which we will nickname *out*. The nicknames *x* and
    *w* are chosen because the neuron computes:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0372-01.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: Here, *reLU(a*) = *a* if *a* > 0 and 0 otherwise (*reLU* standing for rectified
    linear unit). *x* is a 1D signal such as a sound wave, and *w* are weights that
    are shared by and convolved across the work-group of neurons.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The program is based on a loop that iterates over the terms of the sum in the
    above equation. During each iteration *i*, it brings *w*[*i*] and *x*[*i*] into
    registers and multiplies them. Each of these *w*[*i*]*x*[*x*] terms is then added
    into a cumulative sum (`cumsum`). Predicate `p1` is used to determine the end
    of the loop. The *reLU* function is especially easy to implement and fast to run,
    which is why it’s used. We use another predicate, `p0`, to check if `cumsum` >
    0\. If it is, the *reLU* output in fd1 is set to `cumsum`, and otherwise to zero.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: The recent “deep learning revolution” in machine learning owes much more to
    the ability of GPU SIMD to run models like this at massive scales than it does
    to any new algorithms.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '*SASS Dialects*'
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As a manufacturer releases new models, they may modify their ISA, usually by
    extending it with additional instructions, but often also by breaking backward
    compatibility with older versions (unlike the x86 tradition of retaining backward
    compatibility at any cost). For example, Nvidia’s ISAs are named after famous
    scientists, such as Tesla (2006), Fermi (2010), Kepler (2012), Maxwell (2014),
    Pascal (2016), Volta (2017), Turing (2018), Ampere (2020), Lovelace (2022), and
    Hopper (2022). They share a core set of similar instructions, but with some variations
    between them.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Each of these ISAs has its own assembly language dialect, known as a SASS, whose
    instructions correspond directly to machine code. These assembly languages are
    each compatible only with their particular architecture, so they change every
    couple of years. They aren’t officially documented, and don’t present a stable
    platform for user programmers to learn. Nvidia developed PTX as a single stable
    assembly representation, usable by human programmers, that gets translated during
    assembly to the appropriate SASS dialect.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows Turing SASS together with corresponding Turing executable
    machine code, as assembled from the neuron PTX example shown earlier, together
    with wrapper code to interface it to input and output parameters:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The hex seen here is the actual executable code that’s transferred over the
    bus and run on the GPU for the neural network; it’s a direct translation of the
    SASS assembly. (Compare this with the Baby machine code seen in [Chapter 7](ch07.xhtml)—it’s
    not really so different!)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: SASS dialects aren’t officially documented, but we—and the internet—can make
    some guesses as to likely meanings of some instructions based on what we’ve seen
    in the corresponding PTX. `MOV` is a move instruction, with operands being either
    registers or memory locations such as `c[][]` to obtain inputs to the kernel call.
    `LDG` loads from global memory, and `STG` stores to global memory and is used
    to return the output of the kernel call. `TID` is the threadID, which tells us
    which work-item we’re running. `IADD` and `FADD` are integer and floating-point
    addition. `SHL` and `SHR` are shift left and right. `XMAD` is “integer short multiply
    and add.” `BRA` is branch, `NOP` is null operation. `@P0` is a predicate guard,
    where `P0`’s value is set in the previous line by the `ISETP` instruction. The
    usual `JMP`, `CALL`, and `RET` are also provided for control flow.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: SASS dialects also have dedicated instructions for graphics operations. For
    example, there’s `SUST`, surface store, to actually write to the graphics surface,
    as well as instructions to load and query textures and barrier sync (`BAR`).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: To get the executable code onto the GPU, and then to specify when and how many
    copies to launch, the host needs a CPU program. For general computation, you need
    to write this yourself, using tools provided by the GPU manufacturer. For graphics,
    driver software such as Vulkan will do this work if you tell it where your kernel
    (known as a shader in this context) is and what type of shading it does (vertex
    or pixel).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '*Recent GPUs may have many additional features and optimizations, including
    many CISC-like specialist instructions, and even their own CPU SIMD–style instructions
    to split up registers into parts and operate on them together. Recent approaches
    to branching have begun to abandon SIMD altogether and assign separate program
    counters to PEs, resulting in the machine looking more like the MIMD systems in
    the following sections than conventional SIMD GPUs.*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '*Higher-Level GPU Programming*'
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PTX, and occasionally SASS, code is currently written by hand in some cases,
    where human creativity and knowledge of the underlying architecture can allow
    for speed optimizations. However, it’s more common to use higher-level languages
    to compile into GPU assemblers in order to achieve portability between different
    GPUs and to make programming easier.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '*CUDA* is Nvidia’s proprietary C-like language that compiles to PTX and then
    SASS, but not to anything usable by other manufacturers’ GPUs. For example, this
    CUDA program adds two vectors together element-wise:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It can be compiled to PTX with Nvidia’s `nvcc` compiler:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*SPIR-V* (pronounced “spear vee,” as unlike with RISC-V, this *V* is for “Vulkan”)
    is the Khronos standard for representing GPU kernels in an assembly-like language.
    As PTX generalizes over many Nvidia architectures, SPIR-V is intended to generalize
    over *all* manufacturers’ architectures. Like PTX, it’s designed to be converted
    into assembly languages for each specific architecture. As different architectures
    may have different numbers of registers, SPIR-V doesn’t describe registers at
    all. Instead, each instruction’s result is given a unique ID number, which can
    be used similarly to a register ID. When someone writes a converter program for
    a new architecture, they need to think about how to best make use of the available
    registers to realize the computations described in this way. Intel has also worked
    on converting SPIR-V to x86 SIMD, enabling its CPUs to compete against GPUs to
    execute the same code. The following shows SPIR-V code for a roughly equivalent
    kernel to the vector addition seen in the CUDA example:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Third-party open source efforts are underway at the time of writing to compile
    CUDA to SPIR-V, though they aren’t supported by Nvidia. Nvidia does, however,
    accept SPIR-V as input, providing closed tools to compile it to SASS, via PTX
    and another intermediate language, NVVM.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '*OpenCL* is another Khronos open standard, defining a language similar to Nvidia’s
    CUDA. Open source compilers are available from OpenCL to SPIR-V. The following
    is an OpenCL kernel roughly equivalent to the CUDA example:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*GLSL* is the Khronos standard graphical shader language, which also compiles
    to SPIR-V. A sample of GLSL implementing Gouraud shading (from *[https://www.learnopengles.com/tag/gouraud-shading/](https://www.learnopengles.com/tag/gouraud-shading/)*)
    is shown here:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, `lightVector` is the vector from a light to a vertex, and `diffuse` is
    the diffuse component given by the dot product of the light vector and vertex
    normal. If the normal and light vector point in the same direction, then it will
    get maximum illumination. The color is multiplied by the diffuse illumination
    level to give the final display color.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Instruction, Multiple Data
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SIMD is like a lot of people acting on the same instruction. *Multiple instruction,
    multiple data (MIMD)*, on the other hand, is like a lot of people acting on a
    lot of different instructions. Think of SIMD as a gym class with a trainer shouting
    out instructions, and the class all moving together in response. MIMD, then, is
    more like a gym where everyone has their own personal trainer telling them each
    to do different exercises at the same time. As with SIMD, there are multiple different
    flavors of MIMD, which we’ll explore here.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '*MIMD on a Single Processor*'
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The simplest MIMD can occur on a single CPU, in architectures called *very long
    instruction words (VLIW)*. VLIW architectures are related to the vector architectures
    in SIMD. Vector architectures have multiple data items packed into a single large
    register, with a single instruction acting on all of the entities packed into
    that register. In VLIW, each entity in the register has different operations performed
    on it. For example, instead of adding 1 to everything, we could add 1 to the first
    number, divide the second by 7, and multiply the last two together, storing them
    somewhere else.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: This may seem counterintuitive, but there are certain combinations of instructions
    that tend to reappear. For example, when writing a video codec, there are standard
    complex mathematical operations that you repeat over and over on different data.
    You can design a single long instruction word to perform this exact specialist
    sequence of operations. For example, a single VLIW instruction `ADDABCFPMDEFINCGSFTH`
    might mean “integer add register A to register B, store result in C; floating-point
    multiply registers D and E, store in F; increment register G; and bit-shift register
    H”—all in a single instruction! This might, for example, be a standard but intensive
    part of a video codec computation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '*Shared-Memory MIMD*'
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A step above single-CPU MIMD is *shared-memory MIMD*, in which multiple CPUs
    share an addressed memory space. They can communicate with one another by loading
    and storing data within this space. If the CPUs are identical, the style of parallelism
    is known as *symmetric multi-processing (SMP)*. If the CPUs differ, the style
    of parallelism is known as asymmetric multiprocessing (AMP). When the CPUs are
    located on the same CPU piece of silicon, they’re known as *cores*, and the parallelism
    is called *multicore*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: AMP shared memory goes back to the 1980s, when separate coprocessor chips were
    sometimes plugged in alongside the main CPU for extra operations such as floating-point
    computation. For example, the Sega Megadrive used a Z80 as a second processor
    to look after sound, freeing up its main 68000.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: SMP shared-memory computer designs have also existed all through the history
    of x86, beginning with mainboards hosting two or more physical 8086 chips sharing
    the bus and memory.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'With shared-memory MIMD, we have to think about how cache levels should be
    shared. Often the L1 and maybe the L2 cache are stored inside a single CPU and
    are specific to it, while the L3 and maybe the L2 cache are shared between the
    CPUs. This makes managing the caches quite complex. Imagine two CPUs are accessing
    the same address of RAM, caching it independently. The first CPU writes to the
    cache, changing the value for the address. Remember the different cache write
    algorithms we looked at in [Chapter 10](ch10.xhtml): What is the cache going to
    do when the CPU tells it to update the location? Will it just update the local
    cache? Will it send the change straight back to main memory, or wait until the
    cache line is victimized before doing so? If the second CPU tries to read from
    the same address in main memory, how can we ensure it will get the newly updated
    version? We have to be careful when there are multiple CPUs, as they all have
    the ability to write out to shared memory equally, which requires extra communication
    between the CPUs so that the values can be updated and the data can remain in
    sync across all CPUs and their shared cache.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '**Multicore on x86**'
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multicore silicon is now the most common type of shared-memory MIMD, and is
    probably found in your desktop, laptop, and phone. The first dual-core x86 chip
    was the AMD Athlon X2, made from two Hammer K8 cores on the same silicon. This
    was soon followed by Intel’s dual-core Core 2\. Both companies quickly followed
    with 4-, 8-, and 16-core processors—including extra cores from hyperthreading—and
    by 2020 were able to produce 64 cores on high-end processors. [Figure 15-6](ch15.xhtml#ch15fig6)
    shows a die shot of an eight-core Zen2 chiplet.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '*Chiplets* such as this are a recent innovation that split up a large chip
    into several smaller pieces of silicon that are placed together in the same plastic
    package. This is done because chips are now so large and complex that the statistical
    probability of a manufacturing error occurring somewhere has become significant.
    Traditionally, whole chips had to be discarded if any error was present. By using
    chiplets, only the single chiplet with an error needs to be discarded. Multiple
    copies of the chiplet shown here can be combined together—and with additional
    I/O chiplets—to place many more CPUs into a single package than would otherwise
    be reliably possible.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 15-6](ch15.xhtml#ch15fig6), each core has its own L1 and L2 caches,
    with an L3 cache shared between them. Notice how the subcomponents of the cores
    have an almost organic quality in their layouts, like growing mold. This is because—unlike
    the older CPUs we’ve seen in die shots—they were laid out not by human designers
    but by automated routing algorithms, which prioritize efficiency over beauty or
    human comprehension.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The cores run independently, with the onus on the software to perform MIMD using
    them. Some new instructions have been added to the x86 ISA to make this programming
    easier, however, such as Intel’s Transactional Synchronization Extensions (TSX).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0380-01.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-6: A die shot of an AMD Zen2 chiplet, showing eight cores (four
    rectangles spanning the top, four spanning the bottom) and an L3 cache (eight
    rectangles in the vertical center)*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '**LOOP VS. MAP PROGRAMMING**'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequential and parallel thinking lead to two different ways of programming
    multiple copies of work: loops and maps. For example, in the following code we
    have an array containing four elements that need to be processed. Thinking sequentially,
    you’d create a loop and do something with each element in sequence:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The problem with using a loop for this type of work is that it conflates two
    ideas. First, it expresses that we would like each element of data to be processed.
    But second, it also specifies the order in which to process them—in this case,
    starting with the leftmost element and working rightwards one element at a time.
    The first idea is usually what we actually want to express, and if parallelism
    is available we don’t care about the ordering; rather, we want to allow the machine
    to order the work in whatever way gets it done most efficiently.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Some programming languages now provide libraries that parallelize regular code
    across multicores for sections of programs; here’s an example from Python:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This says that we would like a pool of four computations to take place, in any
    order, such that each computation is performed on one of the elements from the
    data. Assigning data elements to tasks is called *mapping*, hence the `map` function
    here.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: If you have four cores in your computer, you can run this Python code, and if
    the system is set up properly, it will know to run on the four cores in parallel
    to complete the task.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '**Non-uniform Memory Access**'
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Non-uniform memory access (NUMA)* architectures are shared-memory designs
    in which the speed of access to memory differs according to which part of memory
    is accessed and by which CPU.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: NUMA requires specialist programmers to understand the architecture and manually
    design programs to take full advantage of it. This includes considering where
    data is located in memory and trying to group data and processors together so
    that loads and stores are done on the fastest available parts of memory.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: As an example of NUMA, say we have four physical enclosures (cases), each containing
    several CPUs and RAM. Initially, this may look like four separate computers, but
    the memory from all four enclosures is connected and mapped together, sharing
    a single address space. These aren’t independent computers; they are, arguably,
    a single multicore computer. Unlike with a regular shared-memory machine, however,
    it takes longer for a CPU to access memory in another enclosure than it would
    to access memory in its own enclosure.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '*You can address 16 exiwords of memory with 64-bit addressing, which is 16
    exibytes if byte addressing is used. This is large enough to cover the entire
    shared memory of current supercomputers. If we want efficient shared-memory computing
    to go above this, however, we may need to move to 128-bit architectures.*'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: NUMA is used in high-performance computing (HPC), where devices are also known
    as *supercomputers* or “big iron.” These are made from many physical, enclosed
    computers, known as *nodes*, each containing one or more CPUs together with memory,
    all connected by cables. Unlike regular networking, these *interconnect* connections
    and the digital logic controlling them are designed to enable direct access to
    the address spaces of each machine. One possibility for interconnect is to physically
    extend a single main bus along cables connecting all the machines such that every
    CPU, RAM, and I/O module share the same bus. Another option is to map all external
    addresses for a machine to a single I/O module in that machine, which caches all
    loads and stores to these addresses and arranges for them to take place by communicating
    with similar I/O modules on the remote machines. Such communications can also
    include remote DMA (RDMA) to enable large CPU-free bulk transfers between RAM
    and secondary storage across nodes. Most NUMA architectures include an additional
    layer of cache, which locally caches the data from remote machines. This is known
    as *cache-coherent* or cc-NUMA.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: The world’s most powerful publicly known supercomputer in 2022 was *AMD Frontier*,
    at the US Department of Energy’s Oak Ridge National Laboratory, shown in [Figure
    15-7](ch15.xhtml#ch15fig7).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0382-01.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-7: The AMD Frontier supercomputer*'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Frontier consists of 74 liquid-cooled HPE Cray EX cabinets, each containing
    eight chassis of eight blades. Each blade has two AMD CPUs and eight GPUs, giving
    around 9,400 CPUs and 37,000 GPUs in total. It’s capable of performing one quintillion
    floating-point operations per second, known as an *exaflop*. It has 700 PB of
    storage, managed using the Lustre filesystem. The secret sauce is the interconnect
    system, known as HPE Slingshot, which is used with the HyperTransport protocol
    and over 90 miles of cabling—including direct point-to-point connections between
    every pair of nodes—to provide NUMA-style memory, making memory on remote nodes
    appear and act as if it were local. Slingshot uses a similar amount of space,
    electronics, and power as the compute nodes.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: NUMA supercomputers are used for tasks such as weather and climate prediction,
    physical simulation, and brain modeling, taking advantage of the topographical
    nature of these domains and linking that to the topographical hierarchy of the
    NUMA system. *Topography* means the connectivity over physical space; in the context
    of climate prediction, we’re talking about modeling the 3D space of Earth’s atmosphere.
    Each point interacts heavily with adjacent points, with the amount of interaction
    decreasing with the distance between points. Each point has its own data properties,
    such as wind velocity, temperature, humidity, and wind pressure. To predict what’s
    going to happen over the next few days or months, we discretize the space into
    small chunks and give each chunk to a processor to look after, with neighboring
    chunks of atmosphere given to neighboring processors in the NUMA hierarchy. The
    processor computes details and makes predictions, factoring in data from other
    local chunks.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: NUMA is also sometimes implemented within a single physical computer enclosure,
    in high-end workstations and servers. These systems are more likely to run many
    small, non-interacting programs than a single large scientific program, so specialist
    programming is less likely to be needed.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '*MIMD Distributed Computing*'
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Distributed computing* means that we have multiple CPUs that each have their
    own address space and aren’t directly accessible to each other. Often, these address
    spaces are each contained in separate physical boxes, such as server or ATX cases.
    CPUs in different address spaces can communicate only with one another using I/O.
    Depending on your definition of a computer, these systems can look a lot like
    many separate computers, loosely connected by networking I/O. But in other cases,
    the work they do can be so tightly coupled that it makes more sense to think of
    them as a single, multicore machine that just happens to have multiple address
    spaces linked by slower I/O networks, like an extreme form of NUMA.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Servers are computers designed to remain powered on at all times that are often
    used for distributed computing as well as for providing online services such as
    websites and databases. Any computer connected to the internet can be used as
    a server, including desktop PCs and Raspberry Pis, but specialized computer designs
    have evolved to better meet servers’ high-reliability requirements. These include
    dual power supplies and auto power-on after an outage to reduce downtime due to
    power grid failures; efficient heat-flow designs and use of ECC-RAM (as in [Chapter
    10](ch10.xhtml)) to reduce internal failures; 19-inch unit form factors to enable
    rack mounting; and various forms of physical security to reduce human interference.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at a few forms of distributed computing.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster Computing**'
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There may be a lot of constant communication between the nodes in a cluster.
    Beowulf is a particular informal standard for building clusters from commodity
    computers (often many old, recycled desktops). Cluster computing, especially Beowulf,
    tends to be quite hacky, amateur, and ad-hoc, but can produce powerful systems
    from low-end machines.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '**Grid Computing**'
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Grid computing*, also known as the *single program, multiple data (SPMD)*
    style of programming, is where we give the same program but separate data to multiple
    identical computers. The computers don’t run the program’s instructions in sync;
    rather, they can all branch differently, depending on the data, running copies
    of the same program all at different places in its execution. Grid computing is
    well suited for applications in data science, speech recognition, data mining,
    bioinformatics, and media processing. Here you have terabytes or more of information
    and want the machines to chug away on separate chunks of it at the same time.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: A characteristic of this style is that all the machines are exactly the same,
    and are kept that way by a dedicated technician, who hosts them in a secure environment.
    Lots of identically high-spec servers are stacked together in racks to guarantee
    that each instance of your program will run fast and in exactly the same way as
    the others.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Grid computers don’t use shared memory; rather, they’re connected by networks
    via I/O. Network capacity is largely used for the compute nodes to access data
    on storage nodes hosting hard drives, rather than to communicate with one another.
    Typically, work is divided into chunks that are sent to compute nodes, which then
    work independently of one another on their given chunks—this is in contrast to
    supercomputers, which are all about dense communication between the processors.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Because of the relatively weak connections between nodes, grids are sometimes
    built from nodes hosted at geographically separate locations. For example, the
    CERN super-grid links many smaller grids at many universities around the world,
    enabling them to spread load between them for analyzing big data from millions
    of particle physics experiments, as needed to find subtle statistical evidence
    of the Higgs boson.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '**Decentralized Computing**'
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we get even looser in our types of parallelism, we get to *decentralized
    computing*. This is somewhat like multi-site grid computing, but the connection
    between devices is weaker still. A grid is a stack of the same machines kept running,
    healthy, and operating identically by a professional IT technician. By contrast,
    decentralized computing takes a lot of consumer-grade, non-identical computers,
    possibly all owned by different people in different countries, and connects them
    to each other, typically via the public internet. The machines have no shared
    memory, aren’t treated as trusted, and have even less communication across nodes.
    There’s no professional maintaining the machines, nor is there a large setup cost
    for buying identical components.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Decentralized computing became popular in the 1990s with the famous Search for
    Extraterrestrial Intelligence (SETI) project. SETI collected big data from large
    radio telescopes pointed at candidate parts of the night sky, then analyzed it
    to look for alien communications signals. You could download the SETI program
    on your desktop, which ran as a screensaver (see [Figure 15-8](ch15.xhtml#ch15fig8))
    when your computer was powered on but not otherwise busy.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0385-01.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-8: The SETI software analyzing radio telescope data for alien communications
    on a home computer*'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The program would connect to the main SETI server to register, and be sent one
    or more chunks of data to analyze. It would return the results to the server,
    which collected them together with the results from other computers.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: HTCondor is modern software that enables arbitrary compute jobs to run decentralized
    in the background on regular desktop PCs, for example turning unused desktops
    in an office or classroom into a grid.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Unlike grid computing, the worker machines are no longer under the control of
    the central manager, so trust and reliability can’t be assumed. The manager might
    send work to workers that don’t return a reply or that return fraudulent results.
    A standard mitigation is to send the same work to three workers and check that
    they all return the same result—or if two agree, then the third is cheating.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '**Cloud Computing**'
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The logical evolution of decentralized computing would have been, and might
    still be, for ordinary computer users around the world to routinely connect together
    and trade their unused CPU cycles with each other. This way, when you need to
    run your giant machine learning model, you can run it on one million CPUs all
    around the world that are otherwise sitting idle apart from displaying screensavers,
    instead of having to buy your own personal grid. Then, for the other 99.9 percent
    of the year, you would similarly allow other people to use your own CPU for parts
    of their large computation when it isn’t otherwise being maxed out. Why this still
    hasn’t happened is an interesting social and economic question.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we’ve seen—as with other aspects of the internet—a few big companies
    move in to dominate the market for distributed computing by maintaining their
    own collections of loosely connected machines, known as *clouds* or *cloud computing*.
    These remove some of the trust, reliability, and payment issues from open distributed
    computing, but at the cost of concerns around privacy and loss of control and
    freedom.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute and Storage**'
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A long-standing debate in distributed computing asks whether it’s better to
    store data on the same machines that are doing the computations, or on separate
    machines.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Separating computation from storage means having two different types of machines
    in your distributed network: some specialized for storing data and others specialized
    for computing power. This has the advantage that any available compute node can
    be used to perform computation on any data. Typically, a software filesystem is
    used on the storage nodes, which makes them appear and function as if they were
    a single, very large hard drive. The separation enables the two types of machine
    to be better specialized for their purposes and to be upgraded independently of
    one another; it also makes it easy to balance the ratio of storage to computing
    power. When computation isn’t needed, the compute nodes can be switched off to
    save energy, or made available to other users. When stored data isn’t accessed
    for long periods, it can be relocated down the memory pyramid to tertiary or offline
    memory, then brought back as needed. The disadvantage of this approach is that
    it requires lots of network communication to constantly move data around from
    where it’s stored to where it’s used, which may become a bottleneck.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Co-location, on the other hand, means having a single type of machine that stores
    a small part of the data on a local hard drive and also performs computation on
    it. A big dataset can be split across many such machines, each of which performs
    the required computations on the data that it hosts locally, with networking used
    only to transmit the results and to update the data. The advantage here is that
    network communications are minimized, but the disadvantage is that the computation
    for a data chunk can be performed only by the single machine that hosts it, making
    machines easily over- or under-used.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Which approach works best depends on the relative speed and costs of networking,
    storage, and computing technologies, which change over time, providing much employment
    for IT consultants swapping between them. Traditional clusters tended to use separation,
    relying on fast networking such as InfiniBand to move the data around quickly.
    However, programmers would sometimes switch to co-location, taking local cache
    copies of data from storage for applications requiring the data to be reread quickly,
    many times. In the 2000s, co-location became more popular, with the *map-reduce*
    algorithm used by search engines finding broader applications through the open
    source software Hadoop and Spark. Map-reduce uses the map replacement for loops
    discussed earlier, but in a recursive manner, with jobs recursively subdividing
    their work to pass to other machines, then collating and merging (that is, reducing)
    their results.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: More recently, the move to cloud computing has seen gains in networking speeds
    in data centers, clearer cost savings from separating storage and computation,
    and the need to dynamically reallocate users and work to different physical machines,
    which has all made separation more attractive again. Some systems try to combine
    elements of both styles, allowing data to be transported over networks to available
    compute nodes, but preferring the data to be computed on its original node if
    possible. Any future move from clouds to decentralized computing, which has slower
    networking than cloud data centers, would likely encourage another swing back
    to co-location.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Instructionless Parallelism
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SIMD and MIMD both extend the classical CPU concept of fetching, decoding, and
    executing a sequential program of instructions. But there are many ways to use
    digital logic in parallel that don’t involve creating CPUs and programs of instructions
    at all. Let’s look at some of them here.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '*Dataflow Architectures*'
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike computer scientists, engineers never got hung up on Turing machine serialism
    in the first place. As they deal with the physical world, engineers tend to view
    electronic information processing systems, both analog and digital, as physical
    groups of devices connected together and all operating at all times according
    to the laws of physics, as in a mechanical machine. For them, such systems have
    always been parallel and designed using circuit diagrams, such as [Figure 15-9](ch15.xhtml#ch15fig9),
    rather than as sequential programs of instructions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0388-01.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-9: A diagram showing parallel information processing in an analog
    guitar distortion pedal*'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Rewriting structures like the one in [Figure 15-9](ch15.xhtml#ch15fig9) as sequential
    programs would often appear to engineers to be bizarre, inefficient computer science
    madness. These circuits are composed of hardware components each doing their thing,
    all at the same time, with data flowing continually around connections between
    them. Working with digital logic presents a similar view of the world, until we
    reach the level of [Chapter 7](ch07.xhtml), where we choose to use such logic
    to implement a serial CPU. But digital logic doesn’t have to be used just for
    that purpose. It can also be used in the engineering style of just continuing
    to design higher- and higher-level parallel machines, running together, with connections
    between them. Most of the engineers’ circuit designs, both analog and digital,
    can be translated to LogiSim (or Verilog, VHDL, or Chisel) networks of this form.
    Analog data values can be converted to one of the digital representations we’ve
    seen, and analog operations on them converted to digital arithmetic simple machines.
    As with CPUs, these designs can be burned onto ASIC or FPGA silicon.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: This approach can be especially efficient for signal processing computations,
    in which a pipeline of processing steps is required. For example, a guitar effects
    unit might require steps of compression, distortion, delay, and reverb. Rather
    than implement these steps in a sequence, they can all exist together in a pipeline,
    as is the case when a guitarist chains together several analog hardware pedals
    implementing one effect each.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '*Dataflow Compilers*'
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Dataflow languages* such as PyTorch, TensorFlow, and Theano, and MATLAB’s
    Simulink, are higher-level languages for specifying parallel information processing.
    These languages enable the programmer to represent the elements of symbolic mathematical
    calculations and their dependencies on one another, and then use specialist compilers
    targeting various types of parallel hardware to order and parallelize them. For
    example, [Figure 15-10](ch15.xhtml#ch15fig10) shows a graphical dataflow description
    of a neural network calculation.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0389-01.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-10: The dataflow of a PyTorch neural network calculation*'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Like hardware description languages, these aren’t *programming* languages, but
    instead are *declarative* languages, more like writing XML or databases than writing
    traditional imperative programs as sequences of instructions. (SPIR-V can also
    be considered as a mid-level dataflow language due to its abstraction of registers
    to identifiers.) Compilers may also exist from dataflow languages to hardware
    description languages such as Verilog and VHDL, as well as to GPU, CPU SIMD, or
    serial CPU instructions.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: An ongoing research area is how to automatically compile a regular, serial,
    C-like language into a dataflow language. OOOE is perhaps just the first tip of
    the iceberg here, optimizing machine code instructions only in small windows of
    time, but we can imagine a day when entire programs are transformed similarly
    and automatically into Verilog or perhaps SPIR-V by using advanced parallel algorithms
    and complexity theory to extract the most parallelized form of the program. Modern
    compilers can do this for “trivial” cases such as converting loops to maps when
    iterations of the loops clearly don’t affect one another. In general, however,
    this is difficult work, not least because so much computer science theory is built
    on serial machines; it may be that big future ideas are needed to rebuild the
    subject with parallelism as a more fundamental starting point. Functional programming
    languages may form part of the solution, as they limit the amount of visible state,
    making it easier to split work into independent and parallelizable pieces.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '*Hardware Neural Networks*'
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A particular application of dataflow architectures is to enable fast hardware
    implementations of the backpropagation neural network algorithm. We’ve known since
    the 1960s that this algorithm is able to recognize and classify any pattern, given
    enough data and computing time. We’ve also known that it’s highly parallelizable,
    with its neural network being constructed from many “neuron” units that can compute
    independently and pass messages to their neighbors.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: During the 2010s, GPU architectures first enabled these computations to be implemented
    cheaply in parallel, and were found to enable successful and accurate recognition
    of complex patterns such as faces in images and words in speech. This created
    a huge commercial demand for even faster, specialized architectures to implement
    the backpropagation algorithm even more efficiently than on GPUs.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main approaches to hardware neural networks in current use: FPGAs
    and NPUs. Let’s consider them now.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '**Backpropagation on FGPAs**'
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Researchers have been building backpropagation neural networks on parallel FPGAs
    for many decades. FPGA designs may try to physically lay out circuits in terms
    of component modules for each neuron, or they may just leave the layout to a Chisel
    or Verilog compiler, which tends to produce random-looking circuits that implement
    the same logic, sometimes more efficiently. During the 2010s, these systems were
    built at larger scales for commercial use, especially by “big tech” companies
    for use in *training* neural networks to make predictions about their big data.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '**Backpropagation on Neural Processing Units**'
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A recent architecture trend has been the production of similar parallel neural
    network hardware on ASICs, which run faster than FPGAs. Such chips are known as
    *neural processor units (NPUs)* or *tensor processor units (TPUs)*.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Some of these are designed as high-power systems for use in *training* neural
    network models, typically deployed in clusters in racks in computing centers.
    Others are designed as low-power embedded systems for use in *running* pretrained
    networks for real-time pattern recognition, and are included in smartphones and
    IoT devices (for example, Intel Neural Compute Sticks and the Arduino-based Genuino).
    These units can power applications such as Snapchat’s real-time face recognition
    and filtering. The difference between Moore’s law for clock speed and for transistor
    size has been a major driver of these systems, with phone designers having lots
    of spare silicon to use up and looking for things to do with it. NPUs were initially
    “pushed” by manufacturers onto phones, looking for applications, rather than “pulled”
    by consumer demand.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve seen several forms of parallelism in previous chapters, beginning with
    Babbage’s parallel arithmetic instructions and register-level parallelism (ripple-carry
    adders), then instruction-level parallelism such as pipelining and OOOE. At those
    levels, the programmer still writes a serial program and doesn’t need to know
    or care that parallelism is making the program run faster.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the parallelisms seen in this chapter, SIMD and MIMD, *do* affect
    the programmer, who needs to understand their details and write programs to best
    take advantage of them. We looked at architectures in order of the tightness of
    their parallelism, beginning with systems that are clearly single computers and
    gradually making the parallel executions more independent until the systems look
    more like multiple computers connected by networks.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: SIMD is where a single instruction is executed multiple times in parallel, on
    multiple different data items. It can be found in CPUs and GPUs. Typically, user
    assembly programming for CPU requires thinking in terms of parallel SIMD instructions
    with fixed, power-of-two parallel copies, while on GPU the ISAs may be structured
    in terms of instructions for a single thread, allowing more variation in the number
    of threads launched. The CPU style doesn’t easily enable programs with branches,
    while the GPU style does so via masking or serial split subgroup execution.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: MIMD is a looser form of parallelism that can enable different programs to run
    on different machines. This includes shared-memory systems, in which all processors
    can load and store in the same address space. These systems can be multicore CPUs
    located in the same physical box as RAM, or large NUMA supercomputers in which
    memory in physically further away boxes takes longer to access than nearby memory.
    Distributed systems are looser still, as each processor or small group of processors
    has its own address space, and communication between nodes occurs only via network
    I/O.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The boundary between a single versus multiple computers seems blurry. Most people
    would consider that a CPU with SIMD instructions is a single computer. It’s harder
    to classify a NUMA supercomputer or a grid system. Decentralized systems such
    as SETI and Bitcoin combine resources from machines around the world to behave
    in similar ways to grids. Today, almost every computer has been connected to the
    internet at some point, where it has communicated with others, perhaps becoming
    part of a single global computation and computer.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: There are still many programmers untrained in parallel algorithms who see them
    as the exotic stuff of graduate research degrees. The traditional view of parallel
    programming was that “by the time you’ve finished writing your fancy parallel
    thing, Intel will have made a faster processor that makes my serial C code go
    faster than yours.” This doesn’t work anymore. Programming now has to be done
    in parallel because the serial silicon-based architecture has reached its limit.
    This may require some quite foundational change to computer science as a whole.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Will you as a programmer have to care about parallel programming? There are
    several possible futures here. In one, you go on writing serial programs as you
    do now, with clever programmers writing compilers to turn those into parallel
    systems. Another scenario, happening now, involves a few programmers creating
    specific libraries to do parallel computing operations, and you calling single
    functions in your serial program to run each one. A third scenario is that you’ll
    need to write more and more SIMD programs by yourself, requiring a significant
    change to your programming style. A fourth is that you’ll need to become an MIMD
    programmer, which is likely a larger style change. Along the way, you might switch
    your loops to maps, and perhaps from imperative to functional programming. Or
    perhaps you’ll stop programming altogether and, like engineers, just design hardware
    circuits to perform computations using a declarative language. This is now a big
    open question, with many programmers placing their career bets by choosing which
    styles to learn.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**x86 SIMD**'
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Try running the x86 MMX, SSE, and AVX codes shown in this chapter. You can run
    them on bare metal using *.iso* files as in [Chapter 13](ch13.xhtml). Or, if you
    have some understanding of operating systems, see the [Appendix](bm01.xhtml) for
    how to run them from inside your operating system. This is a faster way to do
    x86 assembly development.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '**Nvidia PTX Programming**'
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you have access to an Nvidia GPU—either on your own PC or via a free cloud
    service with GPU options such as *[https://colab.google](https://colab.google)*—you
    can compile, edit, and run the chapter’s PTX examples. We assumed in the examples
    that someone or something will be calling the kernels and sending inputs to them.
    To create that linkage, create a file *mykernel.ptx* with the following code in
    it:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Paste in the code from any of the examples below the indicated line to wrap
    them up. Then assemble to Nvidia executable (cubin) code with:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, the `-arch` argument is the code for the Nvidia model you’re using—for
    example, `sm_75` is the real name for Turing.
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If you’d like to inspect the executable as human-readable hex and SASS, this
    can be done with:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You now need some code to run on the host CPU to manage the process of sending
    this executable to the GPU. You also need to send data inputs for it to run on,
    as well as commands to launch the desired number of kernels and print out their
    results. The following code will do all this, and can be used with any kernel
    that’s been wrapped the way we’ve discussed.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Compile and run this with Nvidia’s `nvcc` tool:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You should see the result printed on the host terminal.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**More Challenging**'
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you’d like to try programming in SASS, or even Nvidia machine code, third-party
    SASS assemblers are available and documented for many of the Nvidia architectures.
    At *[https://github.com/daadaada/turingas](https://github.com/daadaada/turingas)*
    you can find a SASS assembler for Volta, Turing, and Ampere; this site also has
    SASS assembly code examples and links to similar assemblers for Fermi, Maxwell,
    and Kepler. Nvidia provides a SASS debugger tool at *[https://docs.nvidia.com/gameworks/content/developertools/desktop/ptx_sass_assembly_debugging.htm](https://docs.nvidia.com/gameworks/content/developertools/desktop/ptx_sass_assembly_debugging.htm)*,
    and a GPU emulator at *[https://github.com/gpgpu-sim/gpgpu-sim_distribution](https://github.com/gpgpu-sim/gpgpu-sim_distribution)*.
    Nvidia lists the meaning of SASS mnemonics, but not their arguments and semantics,
    at *[https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html)*.
    Some of the third-party SASS assemblers include useful example SASS programs.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you have access to a non-Nvidia GPU, find its make and model and see if there’s
    a public ISA and assembler available for it, similar to PTX or SASS. Assemblers
    are sometimes created and documented by third-party reverse engineers, even if
    a GPU manufacturer doesn’t make or document one itself. How does it compare to
    the CPU ISAs you’ve seen?
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simulate a cluster of PCs by running multiple instances of Virtual-Box, as used
    in [Chapter 13](ch13.xhtml). Research how to install and run SGE, MPI, or HTCondor
    across them.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you know neural network theory, add backpropagation to the GPU neuron. Add
    code to create and run several layers of several neurons each to learn and run
    some pattern recognition.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Reading
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For a reverse engineering of, and third-party open source assembler for, the
    Nvidia Kepler architecture, see X. Zhang et al., “Understanding the GPU Microarchitecture
    to Achieve Bare-Metal Performance Tuning,” in *Proceedings of the 22nd ACM SIGPLAN
    Symposium on Principles and Practice of Parallel Programming* (New York: Association
    for Computing Machinery, 2017).'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a fully open source hardware GPU architecture, see MIAOW, *[https://raw.githubusercontent.com/wiki/VerticalResearchGroup/miaow/files/MIAOW_Architecture_Whitepaper.pdf](https://raw.githubusercontent.com/wiki/VerticalResearchGroup/miaow/files/MIAOW_Architecture_Whitepaper.pdf)*.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more on SPIR-V, see J. Kessenich, “An introduction to SPIR-V,” *[https://registry.khronos.org/SPIR-V/papers/WhitePaper.pdf](https://registry.khronos.org/SPIR-V/papers/WhitePaper.pdf)*.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For an example of compiling Python into CPU-less dataflow digital logic, see
    K. Jurkans and C. Fox, “Python Subset to Digital Logic Dataflow Compiler for Robots
    and IoT,” in *International Symposium on Intelligent and Trustworthy Computing,
    Communications, and Networking (ITCCN-2023)* (Exeter, UK: IEEE, 2023).'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
