- en: '**15**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**PARALLEL ARCHITECTURES**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Image](../images/f0355-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As we’ve discussed, computing is splitting along two paths: low-power systems,
    forming the Internet of Things; and high-power computing centers, forming the
    cloud. In previous chapters, we’ve looked at the low-power IoT side of the split:
    embedded and smart systems. This chapter will look at the high-power, high-performance
    systems found in the cloud. Specifically, we’ll look at parallelism, the backbone
    of cloud computing.'
  prefs: []
  type: TYPE_NORMAL
- en: The rise of parallelism is related to Moore’s two laws. While Moore’s law for
    density says we can still put more and more transistors on chips, Moore’s law
    for clock speed is now over, meaning we can’t clock single CPUs faster anymore.
    The number of fetch-decode-execute cycles per second is no longer increasing,
    so we need to find new uses for the extra available transistors to try to do more
    work *within* each cycle, rather than making faster cycles.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a while, we got away with using the extra silicon to boost classical, serial
    architectures: we made more and more complex CISC instructions to get more work
    per instruction; we added more and bigger registers levels of cache onto the CPU
    silicon; we replicated structures such as arithmetic logic units (ALUs) to enable
    simultaneous execution of branches; and we constructed fancier pipelines and out-of-order
    machines. Together, these techniques have recently delivered double digit–percentage
    yearly gains in instructions per cycle (IPC) rather than cycles per second. But
    we may be running out of easy wins in these areas, so we have to think more in
    terms of digital logic being inherently parallel. Luckily for us, it is.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve already encountered register- and instruction-level parallelism. Register-level
    parallelism is the simultaneous per-column execution of digital logic acting on
    bits of a register. For example, all the bits in a word can be negated at the
    same time rather than in sequence. Instruction-level parallelism includes pipelining,
    branch prediction, eager execution, and out-of-order execution (OOOE). These concepts
    don’t appear at the instruction set architecture (ISA) level; they’re invisible
    to the assembly programmer. From the programmer’s perspective, they just make
    serial programs execute faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll focus on the higher-level parallelisms that are visible
    in the ISA and therefore may require the attention of the assembly and perhaps
    also the high level–language programmer. We’ll begin by thinking about parallel
    foundations. Then we’ll turn to the two main types of parallelism: *single instruction,
    multiple data (SIMD)*, as found in modern CPUs and GPUs, and *multiple instruction,
    multiple data (MIMD)*, as found in multicores and cloud computing centers. Finally,
    we’ll wrap up by considering more radical, instructionless forms of parallelism
    that might take architecture beyond the concepts of CPUs and programs.'
  prefs: []
  type: TYPE_NORMAL
- en: Serial vs. Parallel Thinking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of the silicon in a serial computer is used to form memory that sits around
    doing nothing until it’s called on to load from or store to the CPU. In this sense,
    serial computing is like having 1,000 people send all their work to a single worker,
    then stand around waiting for the results to come back to them. This effect is
    known as the *serial bottleneck*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel computing frees these 1,000 people to all work for themselves. Each
    becomes an active unit of computation: they pass data directly to one another
    as needed, and they get massively more work done than if they were standing around
    waiting for that one worker. Similar gains can occur if we use all the digital
    logic in a computer to constantly perform computation rather than wait around
    for the CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel thus seems to be obviously faster and better than serial computing.
    But at least until the 2010s, computer scientists tended to get stuck in “serial
    thinking.” Most people are at some point taught the concept of programming using
    a recipe, assuming that only you are in the kitchen and that you’re going to perform
    a sequence of tasks, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is fine on a small scale, but if you’re a head chef running a chain of
    popular restaurants, you’d be in charge of a team of workers, and you’d have to
    schedule in optimal ways to produce the food more efficiently. The field of operations
    research is all about optimally scheduling work like this.
  prefs: []
  type: TYPE_NORMAL
- en: How do we take a sequence of instructions like the chicken soup recipe and get
    everything done in the shortest period of time? There are well-known algorithms
    to do this. For example, Henry Gantt’s charts, like the one shown in [Figure 15-1](ch15.xhtml#ch15fig1),
    are used to display and reason about sequences of tasks running in parallel over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0357-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-1: A parallel Gantt chart for cooking chicken soup*'
  prefs: []
  type: TYPE_NORMAL
- en: Simple algorithms exist to generate optimal timings for the tasks, given a list
    of dependencies—that is, a list of which tasks depend on the completion of which
    others before they can begin. A critical path can be calculated for the network,
    which is the sequence of jobs that need to be done on time because they’re the
    bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bletchley Park made heavy use of this style of computation. Machines weren’t
    the only types of computers used there: it was still the era of human computation,
    where “computer” was a human job title. Human computers would sit in a computing
    division ([Figure 15-2](ch15.xhtml#ch15fig2)), all doing parts of computations
    under a manager allocating and scheduling the work in parallel. These programmer
    managers thought about how to break down a large mathematical computation into
    components, distribute the tasks, and collect the results together.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0358-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-2: A human computing division working in parallel, with a manager
    (standing) scheduling the work*'
  prefs: []
  type: TYPE_NORMAL
- en: Given that the management of teams of parallel workers has existed for a long
    time, and is based on how to design a program of work to efficiently accomplish
    a task, why do so many programmers basically ignore it and think instead in terms
    of recipes and serial computing? If the history of computing had been different
    and started from an operations research perspective rather than from serial algorithms,
    we might have had a much better foundation. Programming—and perhaps the foundations
    of computer science—is now having to move toward parallel thinking due to the
    end of Moore’s law for clock speed. For example, today’s school children might
    write their first ever program in Scratch with multiple sprites all running code
    in parallel. And professional programmers are increasingly having to think in
    terms of SIMD and MIMD, which we’ll study next.
  prefs: []
  type: TYPE_NORMAL
- en: Single Instruction, Multiple Data on CPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our first type of parallelism—single instruction, multiple data—means we’re
    going to take a single instruction (for example, “add one”) and execute that instruction
    uniformly on multiple data items at once. We can split SIMD systems into CPU-
    and GPU-based implementations. Here we’ll look at the CPU-based implementation;
    in the next section, we’ll look at the GPU-based one.
  prefs: []
  type: TYPE_NORMAL
- en: '*Introduction to SIMD*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'SIMD on CPU is a very CISC-style approach: it involves creating additional
    instructions and digital logic to perform parallel operations as single instructions.
    SIMD instructions pack more than one piece of data into a word, then define instructions
    to apply the same instruction to each piece of data in parallel. For example,
    on a 64-bit machine, instead of using a 64-bit register to store one big 64-bit
    integer, we can partition it into four 16-bit chunks that each hold one 16-bit
    integer. We can then use instructions that understand this packing and operate
    on all four chunks at the same time.'
  prefs: []
  type: TYPE_NORMAL
- en: In a standard CPU, you might have an `ADD` instruction that adds integers from
    registers r1 and r2, storing the result in r3\. In an SIMD machine, however, you’ll
    have an instruction called something like `SIMD-ADD`, still using the same three
    registers, but using a different data representation to perform addition on pairs
    of 16-bit values from the two registers simultaneously; it then stores the output
    in the third register, packed similarly.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*SIMD instructions originated in early supercomputers, such as the famous 1960s
    Cray supercomputers. SIMD was first brought from supercomputers to desktops by
    Intel via their MMX instructions.*'
  prefs: []
  type: TYPE_NORMAL
- en: SIMD can split a 64-bit register into two 32-bit chunks, four 16-bit chunks,
    or eight 8-bit chunks. The four-way split is especially useful for 3D games. It’s
    common for 3D programmers to represent 3D coordinates using *four*-dimensional
    vectors, with the fourth dimension serving as a scaling factor to enable *affine
    transformations*. These are transformations like translations and rotations computed
    using simple matrix-vector multiplications. The 16-bit precision of the numbers
    is usually acceptable for games (though maybe not for serious scientific 3D simulations).
    We’re lucky to live in a world whose number of dimensions, when affinated, is
    a power of 2!
  prefs: []
  type: TYPE_NORMAL
- en: SIMD is also a good fit for images and video, in which pixel colors are often
    represented by four numbers for RGB and alpha (as discussed in [Chapter 2](ch02.xhtml)).
    More generally, for most types of multimedia, including audio, it’s common to
    need to do many copies of the same operations for signal processing, so SIMD can
    speed this up even when there’s no obvious 4D structure.
  prefs: []
  type: TYPE_NORMAL
- en: SIMD instructions can be created for use with any ordinary registers, but they’ve
    become more interesting as register sizes have increased to 64 bits. Some architectures
    also include extra registers that are longer than their word length, known as
    *vector* registers; these can store 128, 256, or 512 bits, and they’re intended
    primarily for use with SIMD instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the theory of CPU SIMD, let’s look at a concrete example
    of how it’s implemented in x86.
  prefs: []
  type: TYPE_NORMAL
- en: '*SIMD on x86*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We saw the names of the various x86 architectures of the 64-bit era in [Chapter
    13](ch13.xhtml). Beyond the basic amd64 instruction set, most of these architectures
    have focused on adding extensions using different forms of parallelism. Most of
    these ideas originated in high-performance computing and high-end servers but
    have also been introduced to desktop architectures.
  prefs: []
  type: TYPE_NORMAL
- en: The classic CISC approach has been to use the extra transistors to add more
    simple machines and instructions to the ISA, each intended to do more work than
    the regular instructions. This has led to thousands of new CISC instructions,
    added for all manner of special cases such as cryptography, multimedia processing,
    and machine learning. There have been disagreements over the standards for these
    extensions. Everyone implements the same base amd64 ISA, but different manufacturers
    extend it in different ways to add extensions. They try to get users hooked on
    their versions and to desert competitors (a well-known strategy called *embrace-extend-extinguish*).
    This creates headaches for compiler writers who have to create multiple back-ends
    to optimize for the different extensions.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the new registers and instructions added to x86 during the 64-bit era
    have been for SIMD. [Figure 15-3](ch15.xhtml#ch15fig3) shows the complete user
    register set of modern amd64.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0360-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-3: The full register set for amd64*'
  prefs: []
  type: TYPE_NORMAL
- en: The SIMD registers are the ones with “MM” in their names. Notice how new SIMD
    registers have appeared over time, usually by extending an existing register to
    have more bits. When extensions are made, x86 backward compatibility requires
    the original shorter form to still be named and usable, as well as the extended
    form. This requires many different versions of instructions to be provided.
  prefs: []
  type: TYPE_NORMAL
- en: '**MMX**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*MMX* was the first x86 SIMD extension. It’s never been officially defined
    what MMX stands for, and indeed this has been a matter of legal trademarking debate
    between Intel and AMD. Suggestions include “matrix math extensions” and “multimedia
    extensions.”'
  prefs: []
  type: TYPE_NORMAL
- en: MMX extended the previous amd64 floating-point registers to 64 bits, similar
    to how 32-bit registers, such as EAX, were extended to the 64-bit RAX. The new
    registers have names MM0 through MM7 and still exist on modern machines.
  prefs: []
  type: TYPE_NORMAL
- en: Each MMX register can be used for integer-only SIMD as either a single 64-bit
    integer, two 32-bit integers, four 16-bit integers, or eight 8-bit integers. Integer
    SIMD is particularly useful and fast for processing images, including for 2D sprite-based
    games and video codecs.
  prefs: []
  type: TYPE_NORMAL
- en: 'MMX instructions begin with `p` for “packed,” such as `paddd` for “packed add
    doubles.” New move instructions—`movb, movw`, and `movd`—copy arrays of bytes,
    words, or doubles into single MMX registers. For example, the following defines
    two arrays of 32-bit doubles: *a* = [4, 3] and *b* = [1, 5]. It loads *a* as packed
    doubles into MM0 and *b* into MM1\. It then packed adds the doubles, leaving [5,
    8] in MM0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: MMX adds a *lot* of new instructions, because every arithmetic operation has
    to exist in each of the packed forms for bytes, words, and doubles.
  prefs: []
  type: TYPE_NORMAL
- en: '**SSE**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Intel’s version of x86 SIMD has been extended several times since MMX, as SSE,
    SSE2, SSE3, SSE4, and SSE4.2 (where SSE stands for streaming SIMD extensions).
    AMD’s latest incompatible competitor is, confusingly, called SSE4a. Unlike MMX,
    the SSE series provides for floating-point SIMD as well as integers. This makes
    it particularly useful for accelerating 3D math for games and other physics simulations.
    (MMX was unsuccessful by the benchmarks of its time, which focused heavily on
    the 3D game *Quake*.)
  prefs: []
  type: TYPE_NORMAL
- en: Unlike MMX’s extension of the old floating-point registers, SSE adds completely
    new, 128-bit vector registers, called XMM0 through XMM31\. The number of these
    has grown with the SSE versions. They can be split into 8-, 16-, 32-, or 64-bit
    chunks, with chunks representing either floating points or integers. Each arithmetic
    operation thus has many instructions depending on these choices.
  prefs: []
  type: TYPE_NORMAL
- en: Most SSE instructions have the letter `p` for “packed” added to their mnemonics,
    at either the beginning or the end. For example, the top-left of [Figure 15-4](ch15.xhtml#ch15fig4)
    shows an SSE compare for equality with the `cmpeqps` instruction. The name comes
    from `cmpeq`, the standard x86 instruction, plus `ps` to indicate “packed, single-precision.”
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0361-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-4: The contents of two SSE registers, XMM0 and XMM1, as SSE instructions
    are carried out, comparing the two sets of data in different ways: equality (top
    left), inequality (top right), less than (bottom left), and not less than (bottom
    right)*'
  prefs: []
  type: TYPE_NORMAL
- en: In the top-right of [Figure 15-4](ch15.xhtml#ch15fig4), the `compneqps` instruction
    similarly extends `cmpneq` (compare not equal) to SSE.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows examples of getting arrays of floats in and out of
    SSE’s XMM registers and performing arithmetic on them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `addps` instruction adds the four numbers in XMM1 to the four numbers
    in XMM0, and stores the result in XMM0\. For the first float, the result will
    be 1.1 + 5.5 = 6.6\. The `mulps` instruction multiplies the four numbers in XMM1
    with the results from the previous calculation (in XMM0), and stores the result
    in XMM0\. For the first floats, this result will be 5.5 × 6.6 = 36.3\. The `subps`
    instruction subtracts the four numbers from `v2` (in XMM1, still unchanged) from
    the result of the previous calculation (in XMM0). For the first float, its result
    will be 36.3 – 5.5 = 30.8.
  prefs: []
  type: TYPE_NORMAL
- en: '**AVX**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Two generations of *Advanced Vector Extensions (AVX)* have added longer vectors
    than SSE, having 256- and 512-bit lengths. The new 256-bit registers are called
    YMM0 through YMM31, and the new 512 registers are called ZMM0 through ZMM31.
  prefs: []
  type: TYPE_NORMAL
- en: 'AVX instructions often have the same names as, and behave similarly to, SSE
    instructions, but they start with a `v`. For example, to add eight pairs of 32-bit
    (double) floating-point numbers using AVX-256, we can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note how the form of AVX arithmetic is different from MMX and SSE, with addition
    now taking three operands rather than two.
  prefs: []
  type: TYPE_NORMAL
- en: '**Domain-Specific Instructions Using SIMD**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As mentioned earlier, SIMD is usually considered a very CISC approach, as it
    involves adding lots of new instructions to the ISA. Initially, these arise from
    the many combinations of packing styles, data types, and arithmetic operations.
    Going beyond simple replication of arithmetic across the chunks, CISC SIMD has
    also tended to create further complex instructions. These might include *horizontal
    SIMD*, which means instructions that *combine* information from multiple chunks
    in the same register. For example, there are instructions that find the minimum
    of multiple chunks in a register: `phminposuw` in SSE or `vphminposuw` in AVX.'
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal SIMD instructions also sometimes sequence simpler SIMD instructions
    together. For example, “dot product of packed double-precision floating-point
    values” (`dppd` on SSE; `vdppd` on AVX) is a single instruction that performs
    a complete vector dot product, often used in games, 3D simulations, and machine
    learning. This consists of first SIMD multiplying pairs of chunks, then summing
    the results horizontally along the register.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cryptography has been a major source of CISC SIMD extensions. For example,
    128-bit AES is the NSA-approved standard for internet encryption. It’s computed
    via four steps: ShiftRows, SubBytes, MixColumns, and AddRoundKey. Intel has added
    CISC instructions for each of these steps, and also a single mega-instruction
    that combines them all to perform an entire round (`aesenc` on SSE; `vaesenc`
    on AVX). If, like most end users, you spend the bulk of your computing time streaming
    videos over HTTPS, then this CISC approach gives a useful targeted speedup for
    your use case. But Intel’s extensions have been controversial, with Linus Torvalds
    stating that the NSA and Intel have likely back-doored them in the digital logic,
    and advising Linux programmers not to use them.'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning—specifically, neural network—operations have been the latest
    target for SIMD CISC, via Intel’s Vector Neural Network Instructions (AVX512-VNNI)
    and Brain Floating Point (AVX512-BF16) extensions to AVX-512, which arrived in
    Golden Cove and are marketed together as *DL Boost*. For example, “multiply and
    add unsigned and signed bytes with saturation” (`vpdpbusds`) performs a full neuron’s
    sigmoid-like activation from its inputs and weights in a single instruction. Some
    researchers have been able to train neural networks faster than on a GPU using
    these and similar SIMD CISC instructions, so this is now a competition between
    CPU and GPU architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '**Compiler Writers and SIMD**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The only compiler writers who understand and care about x86 SIMD are the ones
    working for Intel and AMD, so proprietary CISC compilers are likely to go faster
    than open source or third-party compilers (such as gcc) for numerical code on
    CISC architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Intel has released various C libraries, implemented with its own compilers,
    that convert high-level numerical code into SIMD instructions. These include Integrated
    Performance Primitives (IPP), the Math Kernel Library (MKL), and the IPEX PyTorch-to-AVX
    compiler for neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Open source compiler writers find it hard to get excited about particular proprietary
    hardware extensions and CISC, and they generally prefer to spend their valuable,
    scarce time on more general-purpose work to benefit the wider community, such
    as generating beautiful RISC code that will be accelerated through methods like
    pipelining and OOOE.
  prefs: []
  type: TYPE_NORMAL
- en: '**SIMD ON RISC-V**'
  prefs: []
  type: TYPE_NORMAL
- en: SIMD instructions are a fundamentally CISCy idea—they add lots of new instructions
    and digital logic, making the instruction set more complex. However, SIMD extensions
    have also been proposed for RISC-V, such as P for parallel SIMD instructions and
    V for vector instructions.
  prefs: []
  type: TYPE_NORMAL
- en: No real-world architecture is purely RISC or CISC nowadays, and there’s no law
    against a primarily RISC-style architecture such as RISC-V adding some CISCy features,
    especially as RISC-V’s extension system makes them completely optional. There
    have, however, been loud opposing voices in the open source RISC-V community,
    offended by this potential CISC insertion. Even its founders have published an
    “SIMD considered harmful” warning.
  prefs: []
  type: TYPE_NORMAL
- en: Good RISC style is rather to make use of extra available silicon to optimize
    pipelines and OOOE, for example by replicating ALUs, registers, and other components
    needed to run several branches in parallel. This approach may be made harder by
    the existence of SIMD instructions, especially the most extreme CISCy, multi-step
    ones, such as dot products, which do both multiplication and addition. Multicores
    are generally more acceptable to RISC, and RISC-V has an A extension for atomic
    memory instructions that provides multicore transactions for them.
  prefs: []
  type: TYPE_NORMAL
- en: SIMD on GPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SIMD appears on a much larger scale in GPUs. SIMD on CPU gives speedups of 2
    to 64 times, based on the number of chunks packed into a word. By contrast, a
    GPU can scale to thousands of identical instructions running simultaneously across
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 13](ch13.xhtml), we saw how graphics cards evolved, from providing
    hardware implementations of graphics commands, to providing their own parallel
    machine code for non-graphical computing. Initially, this was very hard, geeky
    work, involving encoding big computational algorithms into shaders as if they
    were graphics computations, exploiting the highly parallel 3D rendering hardware,
    then decoding the resulting images to obtain the computational output.
  prefs: []
  type: TYPE_NORMAL
- en: GPU manufacturers quickly noticed this as a new market and redesigned their
    shader languages into general-purpose GPU instruction sets for general-purpose
    SIMD computing. These can be used to implement graphics shaders as before, but
    now also to implement general, non-graphical SIMD computations. This evolution
    has occurred rapidly to form GPUs that aren’t built for graphics at all, but rather
    for general higher-power scientific and machine learning computation, especially
    neural networks. This is why “graphics processing unit” is now a misnomer; a modern
    GPU is really more of a “general parallel unit.”
  prefs: []
  type: TYPE_NORMAL
- en: '*GPU Architecture*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It used to be difficult to discuss GPU architectures in general because they
    were each developed by different companies according to different, secret designs.
    However, several of these manufacturers got together to agree on *Khronos* standards,
    which define ways of thinking about GPU hardware architecture at a level of abstraction
    common to most of them. This enables most GPUs, and also some other devices, to
    be viewed as if their hardware was implemented as the standard architecture, so
    the programmer doesn’t have to care about their individual details so much. Programmers
    can also easily swap one GPU for another, including between manufacturers, as
    long as the new manufacturer provides software tools to convert programs from
    Khronos standards to their more specific machine codes.
  prefs: []
  type: TYPE_NORMAL
- en: Khronos defines a hierarchy of named entities. We have a single *host* (the
    computer), which may have multiple *compute devices* inside (the physical GPU
    cards or chips). There are multiple *compute units (CUs)* in these, and they each
    contain *processing elements (PEs)*.
  prefs: []
  type: TYPE_NORMAL
- en: The main structure is the CU, whose PEs contain their own registers and ALUs,
    but share a single program counter, instruction register, and control unit. This
    creates the SIMD, as each processing element within the CU executes the same instruction
    from the PC in parallel, but on its own data from its own registers. A CU may
    also contain other structures such as a cache and some shared memory, allowing
    the PEs to communicate with one another. Compute devices typically package several
    independent CUs together. SIMD exists only within single CUs.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Khronos standards are designed to be generalizable, not just to many different
    types of GPU but also to any other SIMD-implementing technologies. For example,
    they could also be implemented on an FPGA or on an SIMD CPU in some cases. This
    is why the generic name “compute device” is used in place of “GPU.”*'
  prefs: []
  type: TYPE_NORMAL
- en: The die shot in [Figure 15-5](ch15.xhtml#ch15fig5) shows what GPU silicon actually
    looks like. It shows that the die is arranged much more regularly than the layout
    of a CPU, with square CUs split evenly throughout and a general cache in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0366-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-5: A die shot taken from an Nvidia Pascal GPU chip*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Nvidia GPU Assembly Programming*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In CPUs, SIMD is expressed by single instructions that perform a fixed number
    (for example, four or eight) of identical operations in parallel. Programs are
    written as a series of such instructions, with one instruction referenced from
    the program counter executing at a time. In GPUs, however, SIMD is usually expressed
    differently: we want to enable large and arbitrary numbers of copies of the instruction
    to run in parallel, rather than a fixed number.'
  prefs: []
  type: TYPE_NORMAL
- en: Khronos defines software-level concepts to express GPU SIMD programs. A *kernel*
    is a usually small function written by the user programmer, with the intent of
    each (assembled) line of the code running as a single instruction on multiple
    data. A *work-item* is one instance of the kernel—that is, the sequence of instructions
    as applied to a single piece of data by running on one processing element. A *work-group*
    is the collection of work-item instances running over the multiple data items.
    Unlike CPU SIMD, kernel code is written by describing its effects on a single
    work-item. When you run a kernel, you choose and specify how many work-items you
    want to launch in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graphics shaders are traditionally small, simple programs that perform a fixed
    sequence of operations on each pixel. They’re thus well suited to SIMD, with work-items
    for each pixel stepping through the same instructions in the same order. However,
    other kinds of compute kernels may require branching. This presents a problem,
    somewhat in the same spirit as pipeline hazards, where different work-items need
    to take different branches. Taking different branches destroys the SIMD because
    the work-items are no longer running the same instructions. There are two methods
    to deal with kernel branching: masking and subgroups.'
  prefs: []
  type: TYPE_NORMAL
- en: Like 1980s CPU designers, modern GPU designers each maintain their own, mutually
    incompatible ISAs that define their platforms. Nvidia is the most popular GPU
    designer at the time of writing, so we’ll learn to program their ISA as an example
    of programming GPUs in general. As with other systems we’ve programmed in this
    book, we’ll simplify the truth a little in order to make learning easier. We’ll
    here assume that all general-purpose Nvidia GPUs implement a single ISA called
    PTX (Parallel Thread Execution), and we’ll learn to program in PTX assembly. You
    can assemble and run PTX programs on any general-purpose Nvidia GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Movement and Arithmetic**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following is a simple PTX kernel program. Like all kernels, many copies
    forming a work-group are intended to run in SIMD parallel, so this code describes
    only the actions of a single work-item:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'PTX assembly is written with semicolons at line ends, and two slashes for comments.
    Register names are conventionally written starting with a percent symbol. We’ll
    use four groups of registers: `r` denotes 32-bit integer registers; `rd` denotes
    64-bit (double) int registers; `fd` denotes 64-bit (double) floating-point registers;
    and our fourth group, `tid`, denotes internal registers used to store information
    about the parallelism.'
  prefs: []
  type: TYPE_NORMAL
- en: As usual, most instructions use three operands, with the first being the destination
    and the others being inputs. As we have several types of register available, most
    instruction names use Amiga-like suffixes, separated by periods, to indicate which
    version is being used. For example, `add.s64` means addition for 64-bit signed
    integers, while `mult.wide.f64` means wide (full) multiplication of 64-bit floats.
    Load `(ld)` and store `(st)` have suffixes to indicate whether to use global or
    local memory. The `cvt` instruction means convert, and with various suffixes it
    converts numbers between signed and unsigned integers and floats of the different
    bit sizes. As usual, `ret` is return.
  prefs: []
  type: TYPE_NORMAL
- en: The above program, as well as the rest of the PTX programs shown here, assumes
    at the start that registers rd1 through rd3 contain the global memory addresses
    of three arrays of doubles, with rd1 and rd2 being inputs, which we’ll nickname
    `x` and `w`, and rd3 being the output, which we’ll nickname `out`. (The reason
    for these conventions will become clear later, when we get to neurons.)
  prefs: []
  type: TYPE_NORMAL
- en: The program’s function is very simple. It completely ignores the two inputs.
    It then obtains its threadID, which is a unique integer assigned to each work-item
    in the work-group. For example, if we were to launch a work-group of 5,000 copies
    of the program, each one would be given a unique threadID in the range 0 to 4,999
    in its `tid.x` register during the launch. The work-item then writes a copy of
    its threadID into the corresponding element of the output array. For example,
    the 573rd work-item, with threadID 573, will write the floating-point number 573.0
    into the 573rd element of `out`. If we launch a work-group of 5,000 copies in
    SIMD, they’ll each write a single such number into `out` simultaneously, so that
    the `out` array then contains the list of numbers from 0 to 4,999 when they complete
    together.
  prefs: []
  type: TYPE_NORMAL
- en: Although PTX uses 64-bit words (which can be restricted to 32-bit, as seen in
    the example), it still uses byte addressing. This means that adding 1 to an address
    moves forward through memory by 8 bits. To move along by a 64-bit word, we have
    to add 8 to an address. The program thus works by obtaining its threadID, multiplying
    it by 8, adding the result to the address of `out`, and storing a floating-point
    version of the threadID at that address. The final `out` array thus contains [0,
    1, 2, 3, 4, 5, . . .].
  prefs: []
  type: TYPE_NORMAL
- en: '**Branching**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The definition of SIMD is that parallel copies of the kernel execute identical
    instructions together as the program executes. Branching runs smoothly in SIMD
    in GPUs if and only if all of the copies take the same branches, but it becomes
    complex to handle if they need to take different branches.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following PTX kernel uses branching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The first two and last four lines are the same as the previous program. But
    after the first two lines, the program tests if the threadID is less than 4\.
    If so, it adds 3.1 to the threadID. If not, it multiplies the threadID by 10.0\.
    Whichever of these results was obtained is then placed in the threadID-th element
    of `out` as before. On completion, `out` thus contains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The complexity here is that we only want a work-item to execute some of the
    lines if a condition is true or false. In PTX, we first test for the condition
    (less than, `lt`) and set a *predicate* register to true or false to store the
    result. Predicate registers are internal registers, usually written as `p1, p2`,
    and so on, which can be set and tested similarly to the status flags we’ve seen
    in the Analytical Engine and other systems. Unlike those flags, there are many
    predicate registers that can each store predicates for long periods without overwriting
    the previous comparison result. Once we’ve set a predicate, we can indicate that
    some lines should be executed only if the predicate is true, or if it is false.
    These indicators are known as *predicate guards*, and in PTX assembly are written
    as, for example, `@%p1` at the start of a line. Different GPUs, including different
    Nvidia models, may handle predicate guards in two ways: masking or subgroups.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Masking* is a simple, pure SIMD method suitable for small branches, such as
    `if...else` statements without jumps. The kernel is executed in SIMD at all times,
    meaning that all copies share the same program counter and execute the same line
    of code at the same time. If a line is guarded, the PE tests the predicate, and
    if the line should not execute, then the PE replaces it with an NOP (no operator),
    as in a CPU pipeline stall. This enables multiple work-items to remain synchronized,
    with those that need to execute the instruction doing so while the others wait
    around for them via these NOPs. This wastes some time, with PEs executing NOPs
    from one branch and real instructions from the other, but it enables all work-items
    to stay synchronized as SIMD at all times.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Subgroups* (a Khronos term, aka “local groups,” “warps,” “waves,” or “wavefronts”
    by some manufacturers) are a more heavyweight solution that goes beyond pure SIMD
    to accommodate conditional jumps. All the work-items in a work-group start out
    running in pure SIMD until a predicate guard is encountered. When this occurs,
    the work-group is split into two subgroups, with work-items in one subgroup taking
    the branch and those in the other not taking it. The subgroups are then treated
    as two independent SIMD programs and are executed independently, either on two
    different CUs, if available, or in series on a single CU if only one is available.'
  prefs: []
  type: TYPE_NORMAL
- en: Every branch in the program creates an additional subgroup split, so, for example,
    a program with four branches in a series can lead to 2⁴ = 16 subgroups. At this
    point, the number of physically available CUs determines the efficiency of execution,
    rather than the PEs within a CU. This is clearly not sustainable for larger programs
    with many possible branching series. However, subgroups can be merged back together
    (“resynchronized”) if the programmer can find a way to do so. Typically this can
    be done when branching has occurred due to different work-items taking different
    numbers of loop repetitions. In this case, the programmer can ask the work-items
    whose loops have completed first to wait until the others have also completed;
    this is known as a synchronization *barrier* and is represented by a special barrier
    instruction in assembly and machine code.
  prefs: []
  type: TYPE_NORMAL
- en: The challenges of branching make SIMD quite a restrictive style of programming.
    It’s well suited to graphics shaders, which typically have no or minimal branching,
    but it’s tricky for programs that require parallel threads to take many different
    branches. Neural networks and physical simulations are two major classes of code
    that have similar minimal-branching structure to graphics; thus, they’ve greatly
    benefited from GPU acceleration. If you need different threads to be doing completely
    different things from one another, however, then SIMD isn’t appropriate. You need
    MIMD, as seen later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Large Work-Groups**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes we need to run more copies of a kernel than there are available PEs
    in the CU. For example, a pixel shader needs to run for every one of about eight
    million pixels for a 4K display, while only thousands or tens of thousands of
    PEs may be available.
  prefs: []
  type: TYPE_NORMAL
- en: 'In these cases, a similar subgroup method can be used as in branching: you
    split the work-group into a number of smaller subgroups such that each subgroup
    runs physically together in SIMD on the PEs, and multiple subgroups can either
    run in series on a single CU or simultaneously across several available independent
    CUs. Unlike in branching, these subgroups are chosen to exactly match the total
    number of PEs. These maximal-sized subgroups are known as *blocks* by some manufacturers,
    with the set of subgroups known as a *grid*.'
  prefs: []
  type: TYPE_NORMAL
- en: Work-items within every subgroup will be allocated the same set of threadIDs,
    corresponding to the position of the PE in its CU. It’s common to need to convert
    between these and the global “jobID.” For example, if you have eight million pixels
    to compute and 10,000 PEs, the four-millionth job needs to know that it should
    write to the four-millionth pixel rather than to its threadID-th pixel, which
    has a maximum value of 10,0000.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a common need, so PTX provides some extra machinery to assist with
    programming it, as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here, two additional internal registers, `ntid.x` and `ctaid.x`, are loaded
    automatically during kernel launches, with the subgroup size and a new ID saying
    which subgroup is being run. By multiplying and adding these using the dedicated
    `mad` instruction, we recover the global job ID and proceed as usual. (The rest
    of the program is the same as the first one, storing a float version of this jobID
    at the jobID-th location in `out`. The difference is that this now works for much
    larger `out` arrays—with millions of elements.)
  prefs: []
  type: TYPE_NORMAL
- en: '**A GPU Neuron**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now let’s look at a larger example kernel, which computes a neuron for a convolutional
    deep neural network (CNN). This is roughly how GPUs are used in machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, 0d3DA5FD7FE1796495 is floating-point zero. As in all our examples, we
    assume at the start that registers rd1 through rd3 contain the global memory addresses
    of three arrays of doubles, with rd1 and rd2 being inputs which we nickname *x*
    and *w*; rd3 is the output, which we will nickname *out*. The nicknames *x* and
    *w* are chosen because the neuron computes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0372-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *reLU(a*) = *a* if *a* > 0 and 0 otherwise (*reLU* standing for rectified
    linear unit). *x* is a 1D signal such as a sound wave, and *w* are weights that
    are shared by and convolved across the work-group of neurons.
  prefs: []
  type: TYPE_NORMAL
- en: The program is based on a loop that iterates over the terms of the sum in the
    above equation. During each iteration *i*, it brings *w*[*i*] and *x*[*i*] into
    registers and multiplies them. Each of these *w*[*i*]*x*[*x*] terms is then added
    into a cumulative sum (`cumsum`). Predicate `p1` is used to determine the end
    of the loop. The *reLU* function is especially easy to implement and fast to run,
    which is why it’s used. We use another predicate, `p0`, to check if `cumsum` >
    0\. If it is, the *reLU* output in fd1 is set to `cumsum`, and otherwise to zero.
  prefs: []
  type: TYPE_NORMAL
- en: The recent “deep learning revolution” in machine learning owes much more to
    the ability of GPU SIMD to run models like this at massive scales than it does
    to any new algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '*SASS Dialects*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As a manufacturer releases new models, they may modify their ISA, usually by
    extending it with additional instructions, but often also by breaking backward
    compatibility with older versions (unlike the x86 tradition of retaining backward
    compatibility at any cost). For example, Nvidia’s ISAs are named after famous
    scientists, such as Tesla (2006), Fermi (2010), Kepler (2012), Maxwell (2014),
    Pascal (2016), Volta (2017), Turing (2018), Ampere (2020), Lovelace (2022), and
    Hopper (2022). They share a core set of similar instructions, but with some variations
    between them.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these ISAs has its own assembly language dialect, known as a SASS, whose
    instructions correspond directly to machine code. These assembly languages are
    each compatible only with their particular architecture, so they change every
    couple of years. They aren’t officially documented, and don’t present a stable
    platform for user programmers to learn. Nvidia developed PTX as a single stable
    assembly representation, usable by human programmers, that gets translated during
    assembly to the appropriate SASS dialect.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows Turing SASS together with corresponding Turing executable
    machine code, as assembled from the neuron PTX example shown earlier, together
    with wrapper code to interface it to input and output parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The hex seen here is the actual executable code that’s transferred over the
    bus and run on the GPU for the neural network; it’s a direct translation of the
    SASS assembly. (Compare this with the Baby machine code seen in [Chapter 7](ch07.xhtml)—it’s
    not really so different!)
  prefs: []
  type: TYPE_NORMAL
- en: SASS dialects aren’t officially documented, but we—and the internet—can make
    some guesses as to likely meanings of some instructions based on what we’ve seen
    in the corresponding PTX. `MOV` is a move instruction, with operands being either
    registers or memory locations such as `c[][]` to obtain inputs to the kernel call.
    `LDG` loads from global memory, and `STG` stores to global memory and is used
    to return the output of the kernel call. `TID` is the threadID, which tells us
    which work-item we’re running. `IADD` and `FADD` are integer and floating-point
    addition. `SHL` and `SHR` are shift left and right. `XMAD` is “integer short multiply
    and add.” `BRA` is branch, `NOP` is null operation. `@P0` is a predicate guard,
    where `P0`’s value is set in the previous line by the `ISETP` instruction. The
    usual `JMP`, `CALL`, and `RET` are also provided for control flow.
  prefs: []
  type: TYPE_NORMAL
- en: SASS dialects also have dedicated instructions for graphics operations. For
    example, there’s `SUST`, surface store, to actually write to the graphics surface,
    as well as instructions to load and query textures and barrier sync (`BAR`).
  prefs: []
  type: TYPE_NORMAL
- en: To get the executable code onto the GPU, and then to specify when and how many
    copies to launch, the host needs a CPU program. For general computation, you need
    to write this yourself, using tools provided by the GPU manufacturer. For graphics,
    driver software such as Vulkan will do this work if you tell it where your kernel
    (known as a shader in this context) is and what type of shading it does (vertex
    or pixel).
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Recent GPUs may have many additional features and optimizations, including
    many CISC-like specialist instructions, and even their own CPU SIMD–style instructions
    to split up registers into parts and operate on them together. Recent approaches
    to branching have begun to abandon SIMD altogether and assign separate program
    counters to PEs, resulting in the machine looking more like the MIMD systems in
    the following sections than conventional SIMD GPUs.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Higher-Level GPU Programming*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PTX, and occasionally SASS, code is currently written by hand in some cases,
    where human creativity and knowledge of the underlying architecture can allow
    for speed optimizations. However, it’s more common to use higher-level languages
    to compile into GPU assemblers in order to achieve portability between different
    GPUs and to make programming easier.
  prefs: []
  type: TYPE_NORMAL
- en: '*CUDA* is Nvidia’s proprietary C-like language that compiles to PTX and then
    SASS, but not to anything usable by other manufacturers’ GPUs. For example, this
    CUDA program adds two vectors together element-wise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'It can be compiled to PTX with Nvidia’s `nvcc` compiler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*SPIR-V* (pronounced “spear vee,” as unlike with RISC-V, this *V* is for “Vulkan”)
    is the Khronos standard for representing GPU kernels in an assembly-like language.
    As PTX generalizes over many Nvidia architectures, SPIR-V is intended to generalize
    over *all* manufacturers’ architectures. Like PTX, it’s designed to be converted
    into assembly languages for each specific architecture. As different architectures
    may have different numbers of registers, SPIR-V doesn’t describe registers at
    all. Instead, each instruction’s result is given a unique ID number, which can
    be used similarly to a register ID. When someone writes a converter program for
    a new architecture, they need to think about how to best make use of the available
    registers to realize the computations described in this way. Intel has also worked
    on converting SPIR-V to x86 SIMD, enabling its CPUs to compete against GPUs to
    execute the same code. The following shows SPIR-V code for a roughly equivalent
    kernel to the vector addition seen in the CUDA example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Third-party open source efforts are underway at the time of writing to compile
    CUDA to SPIR-V, though they aren’t supported by Nvidia. Nvidia does, however,
    accept SPIR-V as input, providing closed tools to compile it to SASS, via PTX
    and another intermediate language, NVVM.
  prefs: []
  type: TYPE_NORMAL
- en: '*OpenCL* is another Khronos open standard, defining a language similar to Nvidia’s
    CUDA. Open source compilers are available from OpenCL to SPIR-V. The following
    is an OpenCL kernel roughly equivalent to the CUDA example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '*GLSL* is the Khronos standard graphical shader language, which also compiles
    to SPIR-V. A sample of GLSL implementing Gouraud shading (from *[https://www.learnopengles.com/tag/gouraud-shading/](https://www.learnopengles.com/tag/gouraud-shading/)*)
    is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here, `lightVector` is the vector from a light to a vertex, and `diffuse` is
    the diffuse component given by the dot product of the light vector and vertex
    normal. If the normal and light vector point in the same direction, then it will
    get maximum illumination. The color is multiplied by the diffuse illumination
    level to give the final display color.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Instruction, Multiple Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SIMD is like a lot of people acting on the same instruction. *Multiple instruction,
    multiple data (MIMD)*, on the other hand, is like a lot of people acting on a
    lot of different instructions. Think of SIMD as a gym class with a trainer shouting
    out instructions, and the class all moving together in response. MIMD, then, is
    more like a gym where everyone has their own personal trainer telling them each
    to do different exercises at the same time. As with SIMD, there are multiple different
    flavors of MIMD, which we’ll explore here.
  prefs: []
  type: TYPE_NORMAL
- en: '*MIMD on a Single Processor*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The simplest MIMD can occur on a single CPU, in architectures called *very long
    instruction words (VLIW)*. VLIW architectures are related to the vector architectures
    in SIMD. Vector architectures have multiple data items packed into a single large
    register, with a single instruction acting on all of the entities packed into
    that register. In VLIW, each entity in the register has different operations performed
    on it. For example, instead of adding 1 to everything, we could add 1 to the first
    number, divide the second by 7, and multiply the last two together, storing them
    somewhere else.
  prefs: []
  type: TYPE_NORMAL
- en: This may seem counterintuitive, but there are certain combinations of instructions
    that tend to reappear. For example, when writing a video codec, there are standard
    complex mathematical operations that you repeat over and over on different data.
    You can design a single long instruction word to perform this exact specialist
    sequence of operations. For example, a single VLIW instruction `ADDABCFPMDEFINCGSFTH`
    might mean “integer add register A to register B, store result in C; floating-point
    multiply registers D and E, store in F; increment register G; and bit-shift register
    H”—all in a single instruction! This might, for example, be a standard but intensive
    part of a video codec computation.
  prefs: []
  type: TYPE_NORMAL
- en: '*Shared-Memory MIMD*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A step above single-CPU MIMD is *shared-memory MIMD*, in which multiple CPUs
    share an addressed memory space. They can communicate with one another by loading
    and storing data within this space. If the CPUs are identical, the style of parallelism
    is known as *symmetric multi-processing (SMP)*. If the CPUs differ, the style
    of parallelism is known as asymmetric multiprocessing (AMP). When the CPUs are
    located on the same CPU piece of silicon, they’re known as *cores*, and the parallelism
    is called *multicore*.
  prefs: []
  type: TYPE_NORMAL
- en: AMP shared memory goes back to the 1980s, when separate coprocessor chips were
    sometimes plugged in alongside the main CPU for extra operations such as floating-point
    computation. For example, the Sega Megadrive used a Z80 as a second processor
    to look after sound, freeing up its main 68000.
  prefs: []
  type: TYPE_NORMAL
- en: SMP shared-memory computer designs have also existed all through the history
    of x86, beginning with mainboards hosting two or more physical 8086 chips sharing
    the bus and memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'With shared-memory MIMD, we have to think about how cache levels should be
    shared. Often the L1 and maybe the L2 cache are stored inside a single CPU and
    are specific to it, while the L3 and maybe the L2 cache are shared between the
    CPUs. This makes managing the caches quite complex. Imagine two CPUs are accessing
    the same address of RAM, caching it independently. The first CPU writes to the
    cache, changing the value for the address. Remember the different cache write
    algorithms we looked at in [Chapter 10](ch10.xhtml): What is the cache going to
    do when the CPU tells it to update the location? Will it just update the local
    cache? Will it send the change straight back to main memory, or wait until the
    cache line is victimized before doing so? If the second CPU tries to read from
    the same address in main memory, how can we ensure it will get the newly updated
    version? We have to be careful when there are multiple CPUs, as they all have
    the ability to write out to shared memory equally, which requires extra communication
    between the CPUs so that the values can be updated and the data can remain in
    sync across all CPUs and their shared cache.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multicore on x86**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multicore silicon is now the most common type of shared-memory MIMD, and is
    probably found in your desktop, laptop, and phone. The first dual-core x86 chip
    was the AMD Athlon X2, made from two Hammer K8 cores on the same silicon. This
    was soon followed by Intel’s dual-core Core 2\. Both companies quickly followed
    with 4-, 8-, and 16-core processors—including extra cores from hyperthreading—and
    by 2020 were able to produce 64 cores on high-end processors. [Figure 15-6](ch15.xhtml#ch15fig6)
    shows a die shot of an eight-core Zen2 chiplet.
  prefs: []
  type: TYPE_NORMAL
- en: '*Chiplets* such as this are a recent innovation that split up a large chip
    into several smaller pieces of silicon that are placed together in the same plastic
    package. This is done because chips are now so large and complex that the statistical
    probability of a manufacturing error occurring somewhere has become significant.
    Traditionally, whole chips had to be discarded if any error was present. By using
    chiplets, only the single chiplet with an error needs to be discarded. Multiple
    copies of the chiplet shown here can be combined together—and with additional
    I/O chiplets—to place many more CPUs into a single package than would otherwise
    be reliably possible.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 15-6](ch15.xhtml#ch15fig6), each core has its own L1 and L2 caches,
    with an L3 cache shared between them. Notice how the subcomponents of the cores
    have an almost organic quality in their layouts, like growing mold. This is because—unlike
    the older CPUs we’ve seen in die shots—they were laid out not by human designers
    but by automated routing algorithms, which prioritize efficiency over beauty or
    human comprehension.
  prefs: []
  type: TYPE_NORMAL
- en: The cores run independently, with the onus on the software to perform MIMD using
    them. Some new instructions have been added to the x86 ISA to make this programming
    easier, however, such as Intel’s Transactional Synchronization Extensions (TSX).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0380-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-6: A die shot of an AMD Zen2 chiplet, showing eight cores (four
    rectangles spanning the top, four spanning the bottom) and an L3 cache (eight
    rectangles in the vertical center)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**LOOP VS. MAP PROGRAMMING**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequential and parallel thinking lead to two different ways of programming
    multiple copies of work: loops and maps. For example, in the following code we
    have an array containing four elements that need to be processed. Thinking sequentially,
    you’d create a loop and do something with each element in sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The problem with using a loop for this type of work is that it conflates two
    ideas. First, it expresses that we would like each element of data to be processed.
    But second, it also specifies the order in which to process them—in this case,
    starting with the leftmost element and working rightwards one element at a time.
    The first idea is usually what we actually want to express, and if parallelism
    is available we don’t care about the ordering; rather, we want to allow the machine
    to order the work in whatever way gets it done most efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some programming languages now provide libraries that parallelize regular code
    across multicores for sections of programs; here’s an example from Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This says that we would like a pool of four computations to take place, in any
    order, such that each computation is performed on one of the elements from the
    data. Assigning data elements to tasks is called *mapping*, hence the `map` function
    here.
  prefs: []
  type: TYPE_NORMAL
- en: If you have four cores in your computer, you can run this Python code, and if
    the system is set up properly, it will know to run on the four cores in parallel
    to complete the task.
  prefs: []
  type: TYPE_NORMAL
- en: '**Non-uniform Memory Access**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Non-uniform memory access (NUMA)* architectures are shared-memory designs
    in which the speed of access to memory differs according to which part of memory
    is accessed and by which CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: NUMA requires specialist programmers to understand the architecture and manually
    design programs to take full advantage of it. This includes considering where
    data is located in memory and trying to group data and processors together so
    that loads and stores are done on the fastest available parts of memory.
  prefs: []
  type: TYPE_NORMAL
- en: As an example of NUMA, say we have four physical enclosures (cases), each containing
    several CPUs and RAM. Initially, this may look like four separate computers, but
    the memory from all four enclosures is connected and mapped together, sharing
    a single address space. These aren’t independent computers; they are, arguably,
    a single multicore computer. Unlike with a regular shared-memory machine, however,
    it takes longer for a CPU to access memory in another enclosure than it would
    to access memory in its own enclosure.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*You can address 16 exiwords of memory with 64-bit addressing, which is 16
    exibytes if byte addressing is used. This is large enough to cover the entire
    shared memory of current supercomputers. If we want efficient shared-memory computing
    to go above this, however, we may need to move to 128-bit architectures.*'
  prefs: []
  type: TYPE_NORMAL
- en: NUMA is used in high-performance computing (HPC), where devices are also known
    as *supercomputers* or “big iron.” These are made from many physical, enclosed
    computers, known as *nodes*, each containing one or more CPUs together with memory,
    all connected by cables. Unlike regular networking, these *interconnect* connections
    and the digital logic controlling them are designed to enable direct access to
    the address spaces of each machine. One possibility for interconnect is to physically
    extend a single main bus along cables connecting all the machines such that every
    CPU, RAM, and I/O module share the same bus. Another option is to map all external
    addresses for a machine to a single I/O module in that machine, which caches all
    loads and stores to these addresses and arranges for them to take place by communicating
    with similar I/O modules on the remote machines. Such communications can also
    include remote DMA (RDMA) to enable large CPU-free bulk transfers between RAM
    and secondary storage across nodes. Most NUMA architectures include an additional
    layer of cache, which locally caches the data from remote machines. This is known
    as *cache-coherent* or cc-NUMA.
  prefs: []
  type: TYPE_NORMAL
- en: The world’s most powerful publicly known supercomputer in 2022 was *AMD Frontier*,
    at the US Department of Energy’s Oak Ridge National Laboratory, shown in [Figure
    15-7](ch15.xhtml#ch15fig7).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0382-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-7: The AMD Frontier supercomputer*'
  prefs: []
  type: TYPE_NORMAL
- en: Frontier consists of 74 liquid-cooled HPE Cray EX cabinets, each containing
    eight chassis of eight blades. Each blade has two AMD CPUs and eight GPUs, giving
    around 9,400 CPUs and 37,000 GPUs in total. It’s capable of performing one quintillion
    floating-point operations per second, known as an *exaflop*. It has 700 PB of
    storage, managed using the Lustre filesystem. The secret sauce is the interconnect
    system, known as HPE Slingshot, which is used with the HyperTransport protocol
    and over 90 miles of cabling—including direct point-to-point connections between
    every pair of nodes—to provide NUMA-style memory, making memory on remote nodes
    appear and act as if it were local. Slingshot uses a similar amount of space,
    electronics, and power as the compute nodes.
  prefs: []
  type: TYPE_NORMAL
- en: NUMA supercomputers are used for tasks such as weather and climate prediction,
    physical simulation, and brain modeling, taking advantage of the topographical
    nature of these domains and linking that to the topographical hierarchy of the
    NUMA system. *Topography* means the connectivity over physical space; in the context
    of climate prediction, we’re talking about modeling the 3D space of Earth’s atmosphere.
    Each point interacts heavily with adjacent points, with the amount of interaction
    decreasing with the distance between points. Each point has its own data properties,
    such as wind velocity, temperature, humidity, and wind pressure. To predict what’s
    going to happen over the next few days or months, we discretize the space into
    small chunks and give each chunk to a processor to look after, with neighboring
    chunks of atmosphere given to neighboring processors in the NUMA hierarchy. The
    processor computes details and makes predictions, factoring in data from other
    local chunks.
  prefs: []
  type: TYPE_NORMAL
- en: NUMA is also sometimes implemented within a single physical computer enclosure,
    in high-end workstations and servers. These systems are more likely to run many
    small, non-interacting programs than a single large scientific program, so specialist
    programming is less likely to be needed.
  prefs: []
  type: TYPE_NORMAL
- en: '*MIMD Distributed Computing*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Distributed computing* means that we have multiple CPUs that each have their
    own address space and aren’t directly accessible to each other. Often, these address
    spaces are each contained in separate physical boxes, such as server or ATX cases.
    CPUs in different address spaces can communicate only with one another using I/O.
    Depending on your definition of a computer, these systems can look a lot like
    many separate computers, loosely connected by networking I/O. But in other cases,
    the work they do can be so tightly coupled that it makes more sense to think of
    them as a single, multicore machine that just happens to have multiple address
    spaces linked by slower I/O networks, like an extreme form of NUMA.'
  prefs: []
  type: TYPE_NORMAL
- en: Servers are computers designed to remain powered on at all times that are often
    used for distributed computing as well as for providing online services such as
    websites and databases. Any computer connected to the internet can be used as
    a server, including desktop PCs and Raspberry Pis, but specialized computer designs
    have evolved to better meet servers’ high-reliability requirements. These include
    dual power supplies and auto power-on after an outage to reduce downtime due to
    power grid failures; efficient heat-flow designs and use of ECC-RAM (as in [Chapter
    10](ch10.xhtml)) to reduce internal failures; 19-inch unit form factors to enable
    rack mounting; and various forms of physical security to reduce human interference.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at a few forms of distributed computing.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster Computing**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There may be a lot of constant communication between the nodes in a cluster.
    Beowulf is a particular informal standard for building clusters from commodity
    computers (often many old, recycled desktops). Cluster computing, especially Beowulf,
    tends to be quite hacky, amateur, and ad-hoc, but can produce powerful systems
    from low-end machines.
  prefs: []
  type: TYPE_NORMAL
- en: '**Grid Computing**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Grid computing*, also known as the *single program, multiple data (SPMD)*
    style of programming, is where we give the same program but separate data to multiple
    identical computers. The computers don’t run the program’s instructions in sync;
    rather, they can all branch differently, depending on the data, running copies
    of the same program all at different places in its execution. Grid computing is
    well suited for applications in data science, speech recognition, data mining,
    bioinformatics, and media processing. Here you have terabytes or more of information
    and want the machines to chug away on separate chunks of it at the same time.'
  prefs: []
  type: TYPE_NORMAL
- en: A characteristic of this style is that all the machines are exactly the same,
    and are kept that way by a dedicated technician, who hosts them in a secure environment.
    Lots of identically high-spec servers are stacked together in racks to guarantee
    that each instance of your program will run fast and in exactly the same way as
    the others.
  prefs: []
  type: TYPE_NORMAL
- en: Grid computers don’t use shared memory; rather, they’re connected by networks
    via I/O. Network capacity is largely used for the compute nodes to access data
    on storage nodes hosting hard drives, rather than to communicate with one another.
    Typically, work is divided into chunks that are sent to compute nodes, which then
    work independently of one another on their given chunks—this is in contrast to
    supercomputers, which are all about dense communication between the processors.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the relatively weak connections between nodes, grids are sometimes
    built from nodes hosted at geographically separate locations. For example, the
    CERN super-grid links many smaller grids at many universities around the world,
    enabling them to spread load between them for analyzing big data from millions
    of particle physics experiments, as needed to find subtle statistical evidence
    of the Higgs boson.
  prefs: []
  type: TYPE_NORMAL
- en: '**Decentralized Computing**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we get even looser in our types of parallelism, we get to *decentralized
    computing*. This is somewhat like multi-site grid computing, but the connection
    between devices is weaker still. A grid is a stack of the same machines kept running,
    healthy, and operating identically by a professional IT technician. By contrast,
    decentralized computing takes a lot of consumer-grade, non-identical computers,
    possibly all owned by different people in different countries, and connects them
    to each other, typically via the public internet. The machines have no shared
    memory, aren’t treated as trusted, and have even less communication across nodes.
    There’s no professional maintaining the machines, nor is there a large setup cost
    for buying identical components.
  prefs: []
  type: TYPE_NORMAL
- en: Decentralized computing became popular in the 1990s with the famous Search for
    Extraterrestrial Intelligence (SETI) project. SETI collected big data from large
    radio telescopes pointed at candidate parts of the night sky, then analyzed it
    to look for alien communications signals. You could download the SETI program
    on your desktop, which ran as a screensaver (see [Figure 15-8](ch15.xhtml#ch15fig8))
    when your computer was powered on but not otherwise busy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0385-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-8: The SETI software analyzing radio telescope data for alien communications
    on a home computer*'
  prefs: []
  type: TYPE_NORMAL
- en: The program would connect to the main SETI server to register, and be sent one
    or more chunks of data to analyze. It would return the results to the server,
    which collected them together with the results from other computers.
  prefs: []
  type: TYPE_NORMAL
- en: HTCondor is modern software that enables arbitrary compute jobs to run decentralized
    in the background on regular desktop PCs, for example turning unused desktops
    in an office or classroom into a grid.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike grid computing, the worker machines are no longer under the control of
    the central manager, so trust and reliability can’t be assumed. The manager might
    send work to workers that don’t return a reply or that return fraudulent results.
    A standard mitigation is to send the same work to three workers and check that
    they all return the same result—or if two agree, then the third is cheating.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cloud Computing**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The logical evolution of decentralized computing would have been, and might
    still be, for ordinary computer users around the world to routinely connect together
    and trade their unused CPU cycles with each other. This way, when you need to
    run your giant machine learning model, you can run it on one million CPUs all
    around the world that are otherwise sitting idle apart from displaying screensavers,
    instead of having to buy your own personal grid. Then, for the other 99.9 percent
    of the year, you would similarly allow other people to use your own CPU for parts
    of their large computation when it isn’t otherwise being maxed out. Why this still
    hasn’t happened is an interesting social and economic question.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we’ve seen—as with other aspects of the internet—a few big companies
    move in to dominate the market for distributed computing by maintaining their
    own collections of loosely connected machines, known as *clouds* or *cloud computing*.
    These remove some of the trust, reliability, and payment issues from open distributed
    computing, but at the cost of concerns around privacy and loss of control and
    freedom.
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute and Storage**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A long-standing debate in distributed computing asks whether it’s better to
    store data on the same machines that are doing the computations, or on separate
    machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Separating computation from storage means having two different types of machines
    in your distributed network: some specialized for storing data and others specialized
    for computing power. This has the advantage that any available compute node can
    be used to perform computation on any data. Typically, a software filesystem is
    used on the storage nodes, which makes them appear and function as if they were
    a single, very large hard drive. The separation enables the two types of machine
    to be better specialized for their purposes and to be upgraded independently of
    one another; it also makes it easy to balance the ratio of storage to computing
    power. When computation isn’t needed, the compute nodes can be switched off to
    save energy, or made available to other users. When stored data isn’t accessed
    for long periods, it can be relocated down the memory pyramid to tertiary or offline
    memory, then brought back as needed. The disadvantage of this approach is that
    it requires lots of network communication to constantly move data around from
    where it’s stored to where it’s used, which may become a bottleneck.'
  prefs: []
  type: TYPE_NORMAL
- en: Co-location, on the other hand, means having a single type of machine that stores
    a small part of the data on a local hard drive and also performs computation on
    it. A big dataset can be split across many such machines, each of which performs
    the required computations on the data that it hosts locally, with networking used
    only to transmit the results and to update the data. The advantage here is that
    network communications are minimized, but the disadvantage is that the computation
    for a data chunk can be performed only by the single machine that hosts it, making
    machines easily over- or under-used.
  prefs: []
  type: TYPE_NORMAL
- en: Which approach works best depends on the relative speed and costs of networking,
    storage, and computing technologies, which change over time, providing much employment
    for IT consultants swapping between them. Traditional clusters tended to use separation,
    relying on fast networking such as InfiniBand to move the data around quickly.
    However, programmers would sometimes switch to co-location, taking local cache
    copies of data from storage for applications requiring the data to be reread quickly,
    many times. In the 2000s, co-location became more popular, with the *map-reduce*
    algorithm used by search engines finding broader applications through the open
    source software Hadoop and Spark. Map-reduce uses the map replacement for loops
    discussed earlier, but in a recursive manner, with jobs recursively subdividing
    their work to pass to other machines, then collating and merging (that is, reducing)
    their results.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, the move to cloud computing has seen gains in networking speeds
    in data centers, clearer cost savings from separating storage and computation,
    and the need to dynamically reallocate users and work to different physical machines,
    which has all made separation more attractive again. Some systems try to combine
    elements of both styles, allowing data to be transported over networks to available
    compute nodes, but preferring the data to be computed on its original node if
    possible. Any future move from clouds to decentralized computing, which has slower
    networking than cloud data centers, would likely encourage another swing back
    to co-location.
  prefs: []
  type: TYPE_NORMAL
- en: Instructionless Parallelism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SIMD and MIMD both extend the classical CPU concept of fetching, decoding, and
    executing a sequential program of instructions. But there are many ways to use
    digital logic in parallel that don’t involve creating CPUs and programs of instructions
    at all. Let’s look at some of them here.
  prefs: []
  type: TYPE_NORMAL
- en: '*Dataflow Architectures*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike computer scientists, engineers never got hung up on Turing machine serialism
    in the first place. As they deal with the physical world, engineers tend to view
    electronic information processing systems, both analog and digital, as physical
    groups of devices connected together and all operating at all times according
    to the laws of physics, as in a mechanical machine. For them, such systems have
    always been parallel and designed using circuit diagrams, such as [Figure 15-9](ch15.xhtml#ch15fig9),
    rather than as sequential programs of instructions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0388-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-9: A diagram showing parallel information processing in an analog
    guitar distortion pedal*'
  prefs: []
  type: TYPE_NORMAL
- en: Rewriting structures like the one in [Figure 15-9](ch15.xhtml#ch15fig9) as sequential
    programs would often appear to engineers to be bizarre, inefficient computer science
    madness. These circuits are composed of hardware components each doing their thing,
    all at the same time, with data flowing continually around connections between
    them. Working with digital logic presents a similar view of the world, until we
    reach the level of [Chapter 7](ch07.xhtml), where we choose to use such logic
    to implement a serial CPU. But digital logic doesn’t have to be used just for
    that purpose. It can also be used in the engineering style of just continuing
    to design higher- and higher-level parallel machines, running together, with connections
    between them. Most of the engineers’ circuit designs, both analog and digital,
    can be translated to LogiSim (or Verilog, VHDL, or Chisel) networks of this form.
    Analog data values can be converted to one of the digital representations we’ve
    seen, and analog operations on them converted to digital arithmetic simple machines.
    As with CPUs, these designs can be burned onto ASIC or FPGA silicon.
  prefs: []
  type: TYPE_NORMAL
- en: This approach can be especially efficient for signal processing computations,
    in which a pipeline of processing steps is required. For example, a guitar effects
    unit might require steps of compression, distortion, delay, and reverb. Rather
    than implement these steps in a sequence, they can all exist together in a pipeline,
    as is the case when a guitarist chains together several analog hardware pedals
    implementing one effect each.
  prefs: []
  type: TYPE_NORMAL
- en: '*Dataflow Compilers*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Dataflow languages* such as PyTorch, TensorFlow, and Theano, and MATLAB’s
    Simulink, are higher-level languages for specifying parallel information processing.
    These languages enable the programmer to represent the elements of symbolic mathematical
    calculations and their dependencies on one another, and then use specialist compilers
    targeting various types of parallel hardware to order and parallelize them. For
    example, [Figure 15-10](ch15.xhtml#ch15fig10) shows a graphical dataflow description
    of a neural network calculation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0389-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-10: The dataflow of a PyTorch neural network calculation*'
  prefs: []
  type: TYPE_NORMAL
- en: Like hardware description languages, these aren’t *programming* languages, but
    instead are *declarative* languages, more like writing XML or databases than writing
    traditional imperative programs as sequences of instructions. (SPIR-V can also
    be considered as a mid-level dataflow language due to its abstraction of registers
    to identifiers.) Compilers may also exist from dataflow languages to hardware
    description languages such as Verilog and VHDL, as well as to GPU, CPU SIMD, or
    serial CPU instructions.
  prefs: []
  type: TYPE_NORMAL
- en: An ongoing research area is how to automatically compile a regular, serial,
    C-like language into a dataflow language. OOOE is perhaps just the first tip of
    the iceberg here, optimizing machine code instructions only in small windows of
    time, but we can imagine a day when entire programs are transformed similarly
    and automatically into Verilog or perhaps SPIR-V by using advanced parallel algorithms
    and complexity theory to extract the most parallelized form of the program. Modern
    compilers can do this for “trivial” cases such as converting loops to maps when
    iterations of the loops clearly don’t affect one another. In general, however,
    this is difficult work, not least because so much computer science theory is built
    on serial machines; it may be that big future ideas are needed to rebuild the
    subject with parallelism as a more fundamental starting point. Functional programming
    languages may form part of the solution, as they limit the amount of visible state,
    making it easier to split work into independent and parallelizable pieces.
  prefs: []
  type: TYPE_NORMAL
- en: '*Hardware Neural Networks*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A particular application of dataflow architectures is to enable fast hardware
    implementations of the backpropagation neural network algorithm. We’ve known since
    the 1960s that this algorithm is able to recognize and classify any pattern, given
    enough data and computing time. We’ve also known that it’s highly parallelizable,
    with its neural network being constructed from many “neuron” units that can compute
    independently and pass messages to their neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: During the 2010s, GPU architectures first enabled these computations to be implemented
    cheaply in parallel, and were found to enable successful and accurate recognition
    of complex patterns such as faces in images and words in speech. This created
    a huge commercial demand for even faster, specialized architectures to implement
    the backpropagation algorithm even more efficiently than on GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main approaches to hardware neural networks in current use: FPGAs
    and NPUs. Let’s consider them now.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Backpropagation on FGPAs**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Researchers have been building backpropagation neural networks on parallel FPGAs
    for many decades. FPGA designs may try to physically lay out circuits in terms
    of component modules for each neuron, or they may just leave the layout to a Chisel
    or Verilog compiler, which tends to produce random-looking circuits that implement
    the same logic, sometimes more efficiently. During the 2010s, these systems were
    built at larger scales for commercial use, especially by “big tech” companies
    for use in *training* neural networks to make predictions about their big data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Backpropagation on Neural Processing Units**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A recent architecture trend has been the production of similar parallel neural
    network hardware on ASICs, which run faster than FPGAs. Such chips are known as
    *neural processor units (NPUs)* or *tensor processor units (TPUs)*.
  prefs: []
  type: TYPE_NORMAL
- en: Some of these are designed as high-power systems for use in *training* neural
    network models, typically deployed in clusters in racks in computing centers.
    Others are designed as low-power embedded systems for use in *running* pretrained
    networks for real-time pattern recognition, and are included in smartphones and
    IoT devices (for example, Intel Neural Compute Sticks and the Arduino-based Genuino).
    These units can power applications such as Snapchat’s real-time face recognition
    and filtering. The difference between Moore’s law for clock speed and for transistor
    size has been a major driver of these systems, with phone designers having lots
    of spare silicon to use up and looking for things to do with it. NPUs were initially
    “pushed” by manufacturers onto phones, looking for applications, rather than “pulled”
    by consumer demand.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve seen several forms of parallelism in previous chapters, beginning with
    Babbage’s parallel arithmetic instructions and register-level parallelism (ripple-carry
    adders), then instruction-level parallelism such as pipelining and OOOE. At those
    levels, the programmer still writes a serial program and doesn’t need to know
    or care that parallelism is making the program run faster.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the parallelisms seen in this chapter, SIMD and MIMD, *do* affect
    the programmer, who needs to understand their details and write programs to best
    take advantage of them. We looked at architectures in order of the tightness of
    their parallelism, beginning with systems that are clearly single computers and
    gradually making the parallel executions more independent until the systems look
    more like multiple computers connected by networks.
  prefs: []
  type: TYPE_NORMAL
- en: SIMD is where a single instruction is executed multiple times in parallel, on
    multiple different data items. It can be found in CPUs and GPUs. Typically, user
    assembly programming for CPU requires thinking in terms of parallel SIMD instructions
    with fixed, power-of-two parallel copies, while on GPU the ISAs may be structured
    in terms of instructions for a single thread, allowing more variation in the number
    of threads launched. The CPU style doesn’t easily enable programs with branches,
    while the GPU style does so via masking or serial split subgroup execution.
  prefs: []
  type: TYPE_NORMAL
- en: MIMD is a looser form of parallelism that can enable different programs to run
    on different machines. This includes shared-memory systems, in which all processors
    can load and store in the same address space. These systems can be multicore CPUs
    located in the same physical box as RAM, or large NUMA supercomputers in which
    memory in physically further away boxes takes longer to access than nearby memory.
    Distributed systems are looser still, as each processor or small group of processors
    has its own address space, and communication between nodes occurs only via network
    I/O.
  prefs: []
  type: TYPE_NORMAL
- en: The boundary between a single versus multiple computers seems blurry. Most people
    would consider that a CPU with SIMD instructions is a single computer. It’s harder
    to classify a NUMA supercomputer or a grid system. Decentralized systems such
    as SETI and Bitcoin combine resources from machines around the world to behave
    in similar ways to grids. Today, almost every computer has been connected to the
    internet at some point, where it has communicated with others, perhaps becoming
    part of a single global computation and computer.
  prefs: []
  type: TYPE_NORMAL
- en: There are still many programmers untrained in parallel algorithms who see them
    as the exotic stuff of graduate research degrees. The traditional view of parallel
    programming was that “by the time you’ve finished writing your fancy parallel
    thing, Intel will have made a faster processor that makes my serial C code go
    faster than yours.” This doesn’t work anymore. Programming now has to be done
    in parallel because the serial silicon-based architecture has reached its limit.
    This may require some quite foundational change to computer science as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: Will you as a programmer have to care about parallel programming? There are
    several possible futures here. In one, you go on writing serial programs as you
    do now, with clever programmers writing compilers to turn those into parallel
    systems. Another scenario, happening now, involves a few programmers creating
    specific libraries to do parallel computing operations, and you calling single
    functions in your serial program to run each one. A third scenario is that you’ll
    need to write more and more SIMD programs by yourself, requiring a significant
    change to your programming style. A fourth is that you’ll need to become an MIMD
    programmer, which is likely a larger style change. Along the way, you might switch
    your loops to maps, and perhaps from imperative to functional programming. Or
    perhaps you’ll stop programming altogether and, like engineers, just design hardware
    circuits to perform computations using a declarative language. This is now a big
    open question, with many programmers placing their career bets by choosing which
    styles to learn.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**x86 SIMD**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Try running the x86 MMX, SSE, and AVX codes shown in this chapter. You can run
    them on bare metal using *.iso* files as in [Chapter 13](ch13.xhtml). Or, if you
    have some understanding of operating systems, see the [Appendix](bm01.xhtml) for
    how to run them from inside your operating system. This is a faster way to do
    x86 assembly development.
  prefs: []
  type: TYPE_NORMAL
- en: '**Nvidia PTX Programming**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you have access to an Nvidia GPU—either on your own PC or via a free cloud
    service with GPU options such as *[https://colab.google](https://colab.google)*—you
    can compile, edit, and run the chapter’s PTX examples. We assumed in the examples
    that someone or something will be calling the kernels and sending inputs to them.
    To create that linkage, create a file *mykernel.ptx* with the following code in
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Paste in the code from any of the examples below the indicated line to wrap
    them up. Then assemble to Nvidia executable (cubin) code with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the `-arch` argument is the code for the Nvidia model you’re using—for
    example, `sm_75` is the real name for Turing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If you’d like to inspect the executable as human-readable hex and SASS, this
    can be done with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You now need some code to run on the host CPU to manage the process of sending
    this executable to the GPU. You also need to send data inputs for it to run on,
    as well as commands to launch the desired number of kernels and print out their
    results. The following code will do all this, and can be used with any kernel
    that’s been wrapped the way we’ve discussed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile and run this with Nvidia’s `nvcc` tool:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see the result printed on the host terminal.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**More Challenging**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you’d like to try programming in SASS, or even Nvidia machine code, third-party
    SASS assemblers are available and documented for many of the Nvidia architectures.
    At *[https://github.com/daadaada/turingas](https://github.com/daadaada/turingas)*
    you can find a SASS assembler for Volta, Turing, and Ampere; this site also has
    SASS assembly code examples and links to similar assemblers for Fermi, Maxwell,
    and Kepler. Nvidia provides a SASS debugger tool at *[https://docs.nvidia.com/gameworks/content/developertools/desktop/ptx_sass_assembly_debugging.htm](https://docs.nvidia.com/gameworks/content/developertools/desktop/ptx_sass_assembly_debugging.htm)*,
    and a GPU emulator at *[https://github.com/gpgpu-sim/gpgpu-sim_distribution](https://github.com/gpgpu-sim/gpgpu-sim_distribution)*.
    Nvidia lists the meaning of SASS mnemonics, but not their arguments and semantics,
    at *[https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html)*.
    Some of the third-party SASS assemblers include useful example SASS programs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you have access to a non-Nvidia GPU, find its make and model and see if there’s
    a public ISA and assembler available for it, similar to PTX or SASS. Assemblers
    are sometimes created and documented by third-party reverse engineers, even if
    a GPU manufacturer doesn’t make or document one itself. How does it compare to
    the CPU ISAs you’ve seen?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simulate a cluster of PCs by running multiple instances of Virtual-Box, as used
    in [Chapter 13](ch13.xhtml). Research how to install and run SGE, MPI, or HTCondor
    across them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you know neural network theory, add backpropagation to the GPU neuron. Add
    code to create and run several layers of several neurons each to learn and run
    some pattern recognition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For a reverse engineering of, and third-party open source assembler for, the
    Nvidia Kepler architecture, see X. Zhang et al., “Understanding the GPU Microarchitecture
    to Achieve Bare-Metal Performance Tuning,” in *Proceedings of the 22nd ACM SIGPLAN
    Symposium on Principles and Practice of Parallel Programming* (New York: Association
    for Computing Machinery, 2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a fully open source hardware GPU architecture, see MIAOW, *[https://raw.githubusercontent.com/wiki/VerticalResearchGroup/miaow/files/MIAOW_Architecture_Whitepaper.pdf](https://raw.githubusercontent.com/wiki/VerticalResearchGroup/miaow/files/MIAOW_Architecture_Whitepaper.pdf)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more on SPIR-V, see J. Kessenich, “An introduction to SPIR-V,” *[https://registry.khronos.org/SPIR-V/papers/WhitePaper.pdf](https://registry.khronos.org/SPIR-V/papers/WhitePaper.pdf)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For an example of compiling Python into CPU-less dataflow digital logic, see
    K. Jurkans and C. Fox, “Python Subset to Digital Logic Dataflow Compiler for Robots
    and IoT,” in *International Symposium on Intelligent and Trustworthy Computing,
    Communications, and Networking (ITCCN-2023)* (Exeter, UK: IEEE, 2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
