- en: '**11'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MEMORY ARCHITECTURE AND ORGANIZATION**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/comm1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This chapter discusses memory hierarchy—the different types and performance
    levels of memory found in computer systems. Although programmers often treat all
    forms of memory as though they are equivalent, using memory improperly can have
    a negative impact on performance. In this chapter you’ll see how to make the best
    use of the memory hierarchy within your programs.
  prefs: []
  type: TYPE_NORMAL
- en: '**11.1 The Memory Hierarchy**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most modern programs benefit by having a large amount of very fast memory. Unfortunately,
    as a memory device gets larger, it tends to be slower. For example, cache memories
    are very fast, but they are also small and expensive. Main memory is inexpensive
    and large, but it is slow, requiring wait states. The memory hierarchy provides
    a way to compare the cost and performance of different types of memory. [Figure
    11-1](ch11.xhtml#ch11fig01) shows one variant of the memory hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-1: The memory hierarchy*'
  prefs: []
  type: TYPE_NORMAL
- en: At the top level of the memory hierarchy are the CPU’s general-purpose *registers*.
    Registers provide the fastest access to data possible on the CPU. The register
    file is also the smallest memory object in the hierarchy (for example, the 32-bit
    80x86 has just eight general-purpose registers, and the x86-64 variants have up
    to 16 general-purpose registers). Because it is impossible to add more registers
    to a CPU, they are also the most expensive memory locations. Even if we count
    the FPU, MMX/AltiVec/Neon, SSE/SIMD, AVX/2/-512, and other CPU registers in this
    portion of the memory hierarchy, it does not change the fact that CPUs have a
    very limited number of registers, and the cost per byte of register memory is
    quite high.
  prefs: []
  type: TYPE_NORMAL
- en: Working our way down, the *level-one (L1) cache* system is the next highest
    performance subsystem in the memory hierarchy. As with registers, the CPU manufacturer
    usually provides the L1 cache on the chip, and you cannot expand it. Its size
    is usually small, typically between 4KB and 32KB, though this is much larger than
    the register memory available on the CPU chip. Although the L1 cache size is fixed
    on the CPU, the cost per cache byte is much lower than the cost per register byte,
    because the cache contains more storage than is available in all the registers
    combined, and the system designer’s cost for both memory types equals the price
    of the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: '*Level-two (L2) cache* is present on some CPUs, but not all. For example, Intel
    i3, i5, i7, and i9 CPUs include an L2 cache as part of their package, but some
    of Intel’s older Celeron chips do not. The L2 cache is generally much larger than
    the L1 cache (for example, 256KB to 1MB as compared with 4KB to 32KB). On CPUs
    with a built-in L2 cache, the cache is not expandable. It still costs less than
    the L1 cache, because we amortize the cost of the CPU across all the bytes in
    the two caches, and the L2 cache is larger.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Level-three (L3) cache* is present on all but the oldest Intel processors.
    The L3 cache is larger still than the L2 cache (typically 8MB on later Intel chips).'
  prefs: []
  type: TYPE_NORMAL
- en: The *main-memory* subsystem comes below the L3 (or L2, if there is no L3) cache
    system in the memory hierarchy. Main memory is the general-purpose, relatively
    low-cost memory—typically DRAM or something similarly inexpensive—found in most
    computer systems. However, there are many differences in main-memory technology
    that result in variations in speed. The main-memory types include standard DRAM,
    synchronous DRAM (SDRAM), double data rate DRAM (DDRAM), DDR3, DDR4, and so on.
    Generally, you won’t find a mixture of these technologies in the same computer
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Below main memory is the *[NUMA](gloss01.xhtml#gloss01_175)* memory subsystem.
    NUMA, which stands for *Non-Uniform Memory Access*, is a bit of a misnomer. The
    term implies that different types of memory have different access times, which
    describes the entire memory hierarchy; in [Figure 11-1](ch11.xhtml#ch11fig01),
    however, it refers to blocks of memory that are electronically similar to main
    memory but, for one reason or another, operate significantly slower. A good example
    of NUMA is the memory on a video (or graphics) card. Another example is flash
    memory, which has significantly slower access and transfer times than standard
    semiconductor RAM. Other peripheral devices that provide a block of memory to
    be shared between the CPU and the peripheral usually have slow access times as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: Most modern computer systems implement a *[virtual memory](gloss01.xhtml#gloss01_256)*
    scheme that simulates main memory using a mass storage disk drive. A virtual memory
    subsystem is responsible for transparently copying data between the disk and main
    memory as programs need it. While disks are significantly slower than main memory,
    the cost per bit is also three orders of magnitude lower for disks. Therefore,
    it’s far less expensive to keep data on magnetic storage or on a solid-state drive
    (SSD) than in main memory.
  prefs: []
  type: TYPE_NORMAL
- en: '*File storage* memory also uses disk media to store program data. However,
    whereas the virtual memory subsystem is responsible for transferring data between
    disk (or SSD) and main memory as programs require, it is the program’s responsibility
    to store and retrieve file storage data. In many instances, it’s a bit slower
    to use file storage memory than it is to use virtual memory, which is why file
    storage memory is lower in the memory hierarchy.^([1](footnotes.xhtml#fn11_1a))'
  prefs: []
  type: TYPE_NORMAL
- en: Next comes *network storage*. At this level in the memory hierarchy, programs
    keep data on a different memory system that connects to the computer system via
    a network. Network storage can be virtual memory, file storage memory, or *distributed
    shared memory (DSM)*, where processes running on different computer systems share
    data stored in a common block of memory and communicate changes to that block
    across the network.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual memory, file storage, and network storage are examples of *online memory
    subsystems*. Memory access within these memory subsystems is slower than accessing
    main memory. However, when a program requests data from one of these three online
    memory subsystems, the memory device will respond to the request as quickly as
    its hardware allows. This is not true for the remaining levels in the memory hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: The *near-line* and *[offline storage](gloss01.xhtml#gloss01_180)* subsystems
    may not be ready to respond immediately to a program’s request for data. An offline
    storage system keeps its data in electronic form (usually magnetic or optical)
    but on storage media that are not necessarily connected to the computer system
    that needs the data. Examples of offline storage include magnetic tapes, unattached
    external disk drives, disk cartridges, optical disks, USB memory sticks, SD cards,
    and floppy diskettes. When a program needs to access data stored offline, it must
    stop and wait for someone or something to mount the appropriate media on the computer
    system. This delay can be quite long (perhaps the computer operator decided to
    take a coffee break?).
  prefs: []
  type: TYPE_NORMAL
- en: Near-line storage uses the same types of media as offline storage, but rather
    than requiring an external source to mount the media before its data is available
    for access, the near-line storage system holds the media in a special robotic
    jukebox device that can automatically mount the desired media when a program requests
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Hardcopy storage is simply a printout, in one form or another, of data. If a
    program requests some data, and that data exists only in hardcopy form, someone
    will have to manually enter the data into the computer. Paper or other hardcopy
    media is probably the least expensive form of memory, at least for certain data
    types.
  prefs: []
  type: TYPE_NORMAL
- en: '**11.2 How the Memory Hierarchy Operates**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The whole point of the memory hierarchy is to allow reasonably fast access to
    a large amount of memory. If only a little memory were necessary, we’d use fast
    static RAM (the circuitry that cache memory uses) for everything. If speed wasn’t
    an issue, we’d use virtual memory for everything. The memory hierarchy enables
    us to take advantage of the principles of *spatial locality of reference* and
    *temporality of reference* to move frequently referenced data into fast memory
    and leave rarely referenced data in slower memory. Unfortunately, during the course
    of a program’s execution, the sets of oft-used and seldom-used data change. We
    can’t simply distribute our data throughout the various levels of the memory hierarchy
    when the program starts and then leave the data alone as the program executes.
    Instead, the different memory subsystems need to be able to accommodate changes
    in spatial locality or temporality of reference during the program’s execution
    by dynamically moving data between subsystems.
  prefs: []
  type: TYPE_NORMAL
- en: Moving data between the registers and memory is strictly a program function.
    The program loads data into registers and stores register data into memory using
    machine instructions like `mov`. It is the programmer’s or compiler’s responsibility
    to keep heavily referenced data in the registers as long as possible; the CPU
    will not automatically place data in general-purpose registers in order to achieve
    higher performance.
  prefs: []
  type: TYPE_NORMAL
- en: Programs explicitly control access to registers, main memory, and those memory-hierarchy
    subsystems only at the file storage level and below. Programs are largely unaware
    of the memory hierarchy between the register level and main memory. In particular,
    cache access and virtual memory operations are generally transparent to the program;
    that is, access to these levels of the memory hierarchy usually occurs without
    any intervention on a program’s part. Programs simply access main memory, and
    the hardware and operating system take care of the rest.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if a program always accesses main memory, it will run slowly, because
    modern DRAM main-memory subsystems are much slower than the CPU. The job of the
    cache memory subsystems and of the CPU’s cache controller is to move data between
    main memory and the L1, L2, and L3 caches so that the CPU can quickly access oft-requested
    data. Likewise, it is the virtual memory subsystem’s responsibility to move oft-requested
    data from hard disk to main memory (if even faster access is needed, the caching
    subsystem will then move the data from main memory to cache).
  prefs: []
  type: TYPE_NORMAL
- en: With few exceptions, most memory subsystem accesses take place transparently
    between one level of the memory hierarchy and the level immediately below or above
    it. For example, the CPU rarely accesses main memory directly. Instead, when the
    CPU requests data from memory, the L1 cache subsystem takes over. If the requested
    data is in the cache, the L1 cache subsystem returns the data to the CPU, and
    that concludes the memory access. If the requested data isn’t present in the L1
    cache, the L1 cache subsystem passes the request down to the L2 cache subsystem.
    If the L2 cache subsystem has the data, it returns this data to the L1 cache,
    which then returns the data to the CPU. Requests for the same data in the near
    future will be fulfilled by the L1 cache rather than the L2 cache, because the
    L1 cache now has a copy of the data. After the L2 cache, the L3 cache kicks in.
  prefs: []
  type: TYPE_NORMAL
- en: If none of the L1, L2, or L3 cache subsystems have a copy of the data, the request
    goes to main memory. If the data is found in main memory, the main-memory subsystem
    passes it to the L3 cache, which then passes it to the L2 cache, which then passes
    it to the L1 cache, which then passes it to the CPU. Once again, the data is now
    in the L1 cache, so any requests for this data in the near future will be fulfilled
    by the L1 cache.
  prefs: []
  type: TYPE_NORMAL
- en: If the data is not present in main memory but exists in virtual memory on some
    storage device, the operating system takes over, reads the data from disk or some
    other device (such as a network storage server), and passes the data to the main-memory
    subsystem. Main memory then passes the data through the caches to the CPU as previously
    described.
  prefs: []
  type: TYPE_NORMAL
- en: Because of spatial locality and temporality, the largest percentage of memory
    accesses takes place in the L1 cache subsystem. The next largest percentage of
    accesses takes place in the L2 cache subsystem. After that, the L3 cache system
    handles most accesses. The most infrequent accesses take place in virtual memory.
  prefs: []
  type: TYPE_NORMAL
- en: '**11.3 Relative Performance of Memory Subsystems**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Looking again at [Figure 11-1](ch11.xhtml#ch11fig01), notice that the speed
    of the various memory hierarchy levels increases as you go up. Exactly how much
    faster is each successive level in the memory hierarchy? The short answer is that
    the speed gradient isn’t uniform. The speed difference between any two contiguous
    levels ranges from “almost no difference” to “four orders of magnitude.”
  prefs: []
  type: TYPE_NORMAL
- en: Registers are, unquestionably, the best place to store data you need to access
    quickly. Accessing a register never requires any extra time, and most machine
    instructions that access data can access register data. Furthermore, instructions
    that access memory often require extra bytes (displacement bytes) as part of the
    instruction encoding. This makes instructions longer and, often, slower.
  prefs: []
  type: TYPE_NORMAL
- en: Intel’s instruction timing tables for the 80x86 claim that an instruction like
    `mov(`someVar`,` `ecx);` should run as fast as an instruction like `mov(ebx,`
    `ecx);`. However, if you read the fine print, you’ll find that Intel makes this
    claim based on several assumptions about the former instruction. First, it assumes
    that someVar’s value is present in the L1 cache memory. If it is not, the cache
    controller has to look in the L2 cache, in the L3 cache, in main memory, or, worse,
    on disk in the virtual memory subsystem. All of a sudden, an instruction that
    should execute in 0.25 nanoseconds on a 4 GHz processor (that is, in one clock
    cycle) requires several milliseconds to execute. That’s a difference of over six
    orders of magnitude. It’s true that future accesses of this variable will take
    place in just one clock cycle because it will subsequently be stored in the L1
    cache. But even if you access someVar’s value one million times while it’s still
    in the cache, the average time of each access will still be about two cycles because
    of how long it takes to access someVar the very first time.
  prefs: []
  type: TYPE_NORMAL
- en: Granted, the likelihood that some variable will be located on disk in the virtual
    memory subsystem is quite low. However, there’s still a difference in performance
    of a couple orders of magnitude between the L1 cache subsystem and the main-memory
    subsystem. Therefore, if the program has to retrieve the data from main memory,
    999 memory accesses later, you’re still paying an average cost of two clock cycles
    to access data that Intel’s documentation claims should take one cycle.
  prefs: []
  type: TYPE_NORMAL
- en: The difference in speed between the L1, L2, and L3 cache systems isn’t so dramatic
    unless the secondary or tertiary cache is not packaged together on the CPU. On
    a 4 GHz processor, the L1 cache must respond within 0.25 nanoseconds if the cache
    operates with zero wait states (some processors actually introduce wait states
    in L1 cache accesses, but CPU designers try to avoid this). Accessing data in
    the L2 cache is always slower than in the L1 cache, and always includes the equivalent
    of at least one wait state, and probably more.
  prefs: []
  type: TYPE_NORMAL
- en: There are several reasons why L2 cache accesses are slower than L1 accesses.
    First, it takes the CPU time to determine that the data it’s seeking is not in
    the L1 cache. By the time it does that, the memory access cycle is nearly complete,
    and there’s no time to access the data in the L2 cache. Secondly, the circuitry
    of the L2 cache may be slower than the circuitry of the L1 cache in order to make
    the L2 cache less expensive. Third, L2 caches are usually 16 to 64 times larger
    than L1 caches, and larger memory subsystems tend to be slower than smaller ones.
    All this amounts to additional wait states for accessing data in the L2 cache.
    As noted earlier, the L2 cache can be as much as one order of magnitude slower
    than the L1 cache. The same situation occurs when you have to access data in the
    L3 cache.
  prefs: []
  type: TYPE_NORMAL
- en: The L1, L2, and L3 caches also differ in the amount of data the system fetches
    when there is a cache miss (see [Chapter 6](ch06.xhtml#ch06)). When the CPU fetches
    data from or writes data to the L1 cache, it generally fetches or writes only
    the data requested. If you execute a `mov(al,` `memory);` instruction, the CPU
    writes only a single byte to the cache. Likewise, if you execute the `mov(mem32,`
    `eax);` instruction, the CPU reads exactly 32 bits from the L1 cache. However,
    access to memory subsystems below the L1 cache does not work in small chunks like
    this. Usually, memory subsystems move blocks of data, or *[cache lines](gloss01.xhtml#gloss01_42)*,
    whenever accessing lower levels of the memory hierarchy. For example, if you execute
    the `mov(mem32,` `eax);` instruction, and `mem32`’s value is not in the L1 cache,
    the cache controller doesn’t simply read `mem32`’s 32 bits from the L2 cache,
    assuming that it’s present there. Instead, the cache controller will actually
    read a whole block of bytes (generally 16, 32, or 64 bytes, depending on the particular
    processor) from the L2 cache. The hope is that the program exhibits spatial locality
    so that reading a block of bytes will speed up future accesses to adjacent objects
    in memory. Unfortunately, the `mov(mem32,` `eax);` instruction doesn’t complete
    until the L1 cache reads the entire cache line from the L2 cache. This excess
    time is known as *[latency](gloss01.xhtml#gloss01_133)*. If the program does not
    access memory objects adjacent to `mem32` in the future, this latency is lost
    time.
  prefs: []
  type: TYPE_NORMAL
- en: A similar performance gulf separates the L2 and L3 caches and L3 and main memory.
    Main memory is typically one order of magnitude slower than the L3 cache; L3 accesses
    are much slower than L2 access. To speed up access to adjacent memory objects,
    the L3 cache reads data from main memory in cache lines. Likewise, L2 cache reads
    cache lines from L3.
  prefs: []
  type: TYPE_NORMAL
- en: Standard DRAM is two to three orders of magnitude faster than SSD storage (which
    is an order of magnitude faster than hard drives, which is why hard disks often
    have their own DRAM-based caches). To overcome this, there’s usually a difference
    of two to three orders of magnitude in size between the L3 cache and the main
    memory so that the difference in speed between disk and main memory matches that
    between the main memory and the L3 cache. (Balancing performance characteristics
    in the memory hierarchy is a goal to strive for in order to effectively use the
    different types of memory.)
  prefs: []
  type: TYPE_NORMAL
- en: We won’t consider the performance of the other memory-hierarchy subsystems in
    this chapter, as they are more or less under programmer control. Because their
    access is not automatic, very little can be said about how frequently a program
    will access them. However, in [Chapter 12](ch12.xhtml#ch12) we’ll look at some
    considerations for these storage devices.
  prefs: []
  type: TYPE_NORMAL
- en: '**11.4 Cache Architecture**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Up to this point, we have treated the cache as a magical place that automatically
    stores data when we need it, perhaps fetching new data as the CPU requires it.
    But how exactly does the cache do this? And what happens when it is full and the
    CPU is requesting additional data that’s not there? In this section, we’ll look
    at the internal cache organization and try to answer these two questions, along
    with a few others.
  prefs: []
  type: TYPE_NORMAL
- en: Programs access only a small amount of data at a given time, and a cache that
    is sized accordingly will improve their performance. Unfortunately, the data that
    programs want rarely sits in contiguous memory locations—it’s usually spread out
    all over the address space. Therefore, cache design has to account for the fact
    that the cache must map data objects at widely varying addresses in memory.
  prefs: []
  type: TYPE_NORMAL
- en: As noted in the previous section, cache memory is not organized in a single
    group of bytes. Instead, it’s usually organized in blocks of *[cache lines](gloss01.xhtml#gloss01_42)*,
    with each line containing some number of bytes (typically a small power of 2 like
    16, 32, or 64), as shown in [Figure 11-2](ch11.xhtml#ch11fig02).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-2: Possible organization of an 8KB cache*'
  prefs: []
  type: TYPE_NORMAL
- en: We can attach a different noncontiguous address to each of the cache lines.
    Cache line 0 might correspond to addresses `$10000` through `$1000F`, and cache
    line 1 might correspond to addresses `$21400` through `$2140F`. Generally, if
    a cache line is *n* bytes long, it will hold *n* bytes from main memory that fall
    on an *n*-byte boundary. In the example in [Figure 11-2](ch11.xhtml#ch11fig02),
    the cache lines are 16 bytes long, so a cache line holds blocks of 16 bytes whose
    addresses fall on 16-byte boundaries in main memory (in other words, the LO 4
    bits of the address of the first byte in the cache line are always 0).
  prefs: []
  type: TYPE_NORMAL
- en: 'When the cache controller reads a cache line from a lower level in the memory
    hierarchy, where does the data go in the cache? The answer is determined by the
    caching scheme in use. There are three different caching schemes: direct-mapped
    cache, fully associative cache, and *n*-way set associative cache.'
  prefs: []
  type: TYPE_NORMAL
- en: '***11.4.1 Direct-Mapped Cache***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In a *direct-mapped cache* (also known as the *one-way set associative cache*),
    a particular block of main memory is always loaded into—mapped to—the exact same
    cache line, determined by a small number of bits in the data block’s memory address.
    [Figure 11-3](ch11.xhtml#ch11fig03) shows how a cache controller could select
    the appropriate cache line for an 8KB cache with 512 16-byte cache lines and a
    32-bit main-memory address.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-3: Selecting a cache line in a direct-mapped cache*'
  prefs: []
  type: TYPE_NORMAL
- en: A cache with 512 cache lines requires 9 bits to select one of the cache lines
    (2⁹ = 512). In this example, bits 4 through 12 of the address determine which
    cache line to use (assuming we number the cache lines from 0 to 511), while bits
    0 through 3 determine the particular byte within the 16-byte cache line.
  prefs: []
  type: TYPE_NORMAL
- en: The direct-mapped caching scheme is very easy to implement. Extracting 9 (or
    some other number of) bits from the memory address and using the result as an
    index into the array of cache lines is trivial and fast, though this design may
    not make effective use of all the cache memory.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the caching scheme in [Figure 11-3](ch11.xhtml#ch11fig03) maps
    address 0 to cache line 0\. It also maps addresses `$2000` (8KB), `$4000` (16KB),
    `$6000` (24KB), `$8000` (32KB), and every other address that is a multiple of
    8 kilobytes to cache line 0\. This means that if a program is constantly accessing
    data at addresses that are multiples of 8KB and not accessing any other locations,
    the system will use only cache line 0, leaving all the other cache lines unused.
    In this extreme case, the cache is effectively limited to the size of one cache
    line, and each time the CPU requests data at an address that is mapped to, but
    not present in, cache line 0, it has to go down to a lower level in the memory
    hierarchy to access that data.
  prefs: []
  type: TYPE_NORMAL
- en: '***11.4.2 Fully Associative Cache***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In a fully associative cache subsystem, the cache controller can place a block
    of bytes in any one of the cache lines present in the cache memory. While this
    is the most flexible cache system, the extra circuitry to achieve full associativity
    is expensive and, worse, can slow down the memory subsystem. Most L1 and L2 caches
    are not fully associative for this reason.
  prefs: []
  type: TYPE_NORMAL
- en: '***11.4.3 n-Way Set Associative Cache***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If a fully associative cache is too complex, too slow, and too expensive to
    implement, but a direct-mapped cache is too inefficient, an *n*-way set associative
    cache is a compromise between the two. In an *n*-way set associative cache, the
    cache is broken up into sets of *n* cache lines. The CPU determines the particular
    set to use based on some subset of the memory address bits, just as in the direct-mapping
    scheme, and the cache controller uses a fully associative mapping algorithm to
    determine which one of the *n* cache lines within the set to use.
  prefs: []
  type: TYPE_NORMAL
- en: For example, an 8KB two-way set associative cache subsystem with 16-byte cache
    lines organizes the cache into 256 cache-line sets with two cache lines each.
    Eight bits from the memory address determine which one of these 256 different
    sets will contain the data. Once the cache-line set is determined, the cache controller
    maps the block of bytes to one of the two cache lines within the set (see [Figure
    11-4](ch11.xhtml#ch11fig04)). This means two different memory addresses located
    on 8KB boundaries (addresses having the same value in bits 4 through 11) can both
    appear simultaneously in the cache. However, a conflict will occur if you attempt
    to access a third memory location at an address that is an even multiple of 8KB.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-4: A two-way set associative cache*'
  prefs: []
  type: TYPE_NORMAL
- en: A four-way set associative cache puts four associative cache lines in each cache-line
    set. In an 8KB cache like the one in [Figure 11-4](ch11.xhtml#ch11fig04), a four-way
    set associative caching scheme would have 128 cache-line sets with four cache
    lines each. This would allow the cache to maintain up to four different blocks
    of data without a conflict, each of which would map to the same cache line in
    a direct-mapped cache.
  prefs: []
  type: TYPE_NORMAL
- en: A two- or four-way set associative cache is much better than a direct-mapped
    cache and considerably less complex than a fully associative cache. The more cache
    lines we have in each cache-line set, the closer we come to creating a fully associative
    cache, with all the attendant problems of complexity and speed. Most cache designs
    are direct-mapped, two-way set associative, or four-way set associative. The various
    members of the 80x86 family make use of all three.
  prefs: []
  type: TYPE_NORMAL
- en: Matching the Caching Scheme to the Access Type
  prefs: []
  type: TYPE_NORMAL
- en: Despite its downsides, the direct-mapped cache is, in fact, very effective for
    data that you access sequentially rather than randomly. Because the CPU typically
    executes machine instructions sequentially, instruction bytes can be stored very
    effectively in a direct-mapped cache. However, programs tend to access data more
    randomly than they access code, so data is better stored in a twoway or four-way
    set associative cache.
  prefs: []
  type: TYPE_NORMAL
- en: Because of these different access patterns, many CPU designers use separate
    caches for data and machine instruction bytes—for example, an 8KB data cache and
    an 8KB instruction cache rather than a single 16KB unified cache. The advantage
    of this approach is that each cache can use the caching scheme that‛s most appropriate
    for the particular values it will store. The drawback is that the two caches are
    now each half the size of a unified cache, which may cause more cache misses than
    would occur with a unified cache. The choice of an appropriate cache organization
    is a difficult one, beyond the scope of this book, and can be made only after
    you‛ve analyzed many programs running on the target processor.
  prefs: []
  type: TYPE_NORMAL
- en: '***11.4.4 Cache-Line Replacement Policies***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Thus far, we’ve answered the question, “Where do we put a block of data in the
    cache?” Now we turn to the equally important question, “What happens if a cache
    line isn’t available when we want to put a block of data in it?”
  prefs: []
  type: TYPE_NORMAL
- en: For a direct-mapped cache architecture, the cache controller simply replaces
    whatever data was formerly in the cache line with the new data. Any subsequent
    reference to the old data will result in a cache miss, and the cache controller
    will have to restore that old data to the cache by replacing whatever data is
    in that line.
  prefs: []
  type: TYPE_NORMAL
- en: For a two-way set associative cache, the replacement algorithm is a bit more
    complex. As you’ve seen, whenever the CPU references a memory location, the cache
    controller uses some subset of the address’s bits to determine the cache-line
    set that should be used to store the data. Then, using some fancy circuitry, the
    cache controller determines whether the data is already present in one of the
    two cache lines in the destination set. If the data isn’t there, the CPU has to
    retrieve it from memory, and the controller has to pick one of the two lines to
    use. If either or both of the cache lines are currently unused, the controller
    picks an unused line. However, if both cache lines are currently in use, the controller
    must pick one of them and replace its data with the new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The controller cannot predict the cache line whose data will be referenced
    first and replace the other cache line, but it can use the principle of temporality:
    if a memory location has been referenced recently, it’s likely to be referenced
    again in the very near future. This implies the following corollary: if a memory
    location hasn’t been accessed in a while, it’s likely to be a long time before
    the CPU accesses it again. Therefore, many cache controllers use the *least recently
    used (LRU)* algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: An LRU policy is easy to implement in a two-way set associative cache system,
    using a single bit for each set of two cache lines. Whenever the CPU accesses
    one of the two cache lines this bit is set to `0`, and whenever the CPU accesses
    the other cache line, this bit is set to `1`. Then, when a replacement is necessary,
    the cache controller replaces the LRU cache line, indicated by the inverse of
    this bit.
  prefs: []
  type: TYPE_NORMAL
- en: For four-way (and greater) set associative caches, maintaining the LRU information
    is a bit more difficult, which is one reason the circuitry for such caches is
    more complex. Because of the complications LRU might introduce, other replacement
    policies are sometimes used instead. Two of them, *first-in, first-out (FIFO)*
    and *random*, are easier to implement than LRU, but they have their own problems.
    A full discussion of their pros and cons is beyond the scope of this book, but
    you can find more information in a text on computer architecture or operating
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: '***11.4.5 Cache Write Policies***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: What happens when the CPU writes data to memory? The simple answer, and the
    one that results in the quickest operation, is that the CPU writes the data to
    the cache. However, what happens when the cache-line data is subsequently replaced
    by data that is read from memory? If the modified contents of the cache line are
    not written to main memory, they will be lost. The next time the CPU attempts
    to access that data, it will reload the cache line with the old data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly, any data written to the cache must ultimately be written to main memory
    as well. Caches use two common write policies: *write-through* and *write-back*.'
  prefs: []
  type: TYPE_NORMAL
- en: The write-through policy states that any time data is written to the cache,
    the cache immediately turns around and writes a copy of that cache line to main
    memory. The CPU does not have to halt while the cache controller writes the data
    from cache to main memory. So, unless the CPU needs to access main memory shortly
    after the write occurs, this operation takes place in parallel with the program’s
    execution. Because the write-through policy updates main memory with the new value
    as soon as possible, it is a better policy to use when two different CPUs are
    communicating through shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: Still, a write operation takes some time, during which it’s likely that a CPU
    will want to access main memory, so this policy may not be a high-performance
    solution. Worse, suppose the CPU reads from and writes to the memory location
    several times in succession. With a write-through policy in place, the CPU will
    saturate the bus with cache-line writes, and this will significantly hamper the
    program’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: With the write-back policy, writes to the cache are not immediately written
    to main memory; instead, the cache controller updates main memory later. This
    scheme tends to be higher performance, because several writes to the same cache
    line within a short time period won’t generate multiple writes to main memory.
  prefs: []
  type: TYPE_NORMAL
- en: To determine which cache lines must be written back to main memory, the cache
    controller usually maintains a *[dirty bit](gloss01.xhtml#gloss01_75)* within
    each one. The cache system sets this bit whenever it writes data to the cache.
    At some later time, the cache controller checks the dirty bit to determine if
    it must write the cache line to memory. For example, whenever the cache controller
    replaces a cache line with other data from memory, it first checks the dirty bit,
    and if that bit is set, the controller writes that cache line to memory before
    going through with the cache-line replacement. Note that this increases the latency
    time during a cache-line replacement. This latency could be reduced if the cache
    controller were able to write dirty cache lines to main memory while no other
    bus access was occurring. Some systems provide this functionality, and others
    do not for economic reasons.
  prefs: []
  type: TYPE_NORMAL
- en: '***11.4.6 Cache Use and Software***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A cache subsystem is not a panacea for slow memory access, and can in fact actually
    *hurt* an application’s performance. In order for a cache system to be effective,
    software must be written with the cache behavior in mind. Particularly, good software
    must exhibit either spatial or temporal locality of reference—which the software
    designer accomplishes by placing oft-used variables adjacent in memory so they
    tend to fall into the same cache lines—and avoid data structures and access patterns
    that force the cache to frequently replace cache lines.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that an application accesses data at several different addresses that
    the cache controller would map to the same cache line. With each access, the cache
    controller must read in a new cache line (possibly flushing the old one back to
    memory if it is dirty). As a result, each memory access incurs the latency cost
    of retrieving a cache line from main memory. This degenerate case, known as *[thrashing](gloss01.xhtml#gloss01_246)*,
    can slow down the program by one to two orders of magnitude, depending on the
    speed of main memory and the size of a cache line. We’ll take another look at
    thrashing a little later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A benefit of the cache subsystem on modern 80x86 CPUs is that it automatically
    handles many misaligned data references. Remember, there’s a performance penalty
    for accessing words or double-word objects at an address that is not an even multiple
    of that object’s size. By providing some fancy logic, Intel’s designers have eliminated
    this penalty as long as the data object is located completely within a cache line.
    However, if the object crosses a cache line, the penalty still applies.
  prefs: []
  type: TYPE_NORMAL
- en: '**11.5 NUMA and Peripheral Devices**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although most of the RAM in a system is based on high-speed DRAM interfaced
    directly with the processor’s bus, not all memory is connected to the CPU this
    way. Sometimes a large block of RAM is part of a peripheral device—for example,
    a video card, network interface card, or USB controller—and you communicate with
    that device by writing data to its RAM. Unfortunately, the access time to the
    RAM on these peripheral devices is often much slower than the access time to main
    memory. In this section, we’ll use the video card as an example, although NUMA
    performance applies to other devices and memory technologies as well.
  prefs: []
  type: TYPE_NORMAL
- en: A typical video card interfaces with a CPU through a *Peripheral Component Interconnect
    Express (PCI-e)* bus inside the computer system. Though 16-lane PCI-e buses are
    fast, memory access is still much faster. Game programmers long ago discovered
    that manipulating a copy of the screen data in main memory and writing that data
    to the video card RAM only periodically (typically once every 1/60 of a second
    during video retrace, to avoid flicker) is much faster than writing directly to
    the video card every time you want to make a change.
  prefs: []
  type: TYPE_NORMAL
- en: Caches and the virtual memory subsystem operate transparently (that is, applications
    are unaware of the underlying operations taking place), but NUMA memory does not,
    so programs that write to NUMA devices must minimize the number of accesses whenever
    possible (for example, by using an offscreen bitmap to hold temporary results).
    If you’re actually storing and retrieving data on a NUMA device, like a flash
    memory card, you must explicitly cache the data yourself.
  prefs: []
  type: TYPE_NORMAL
- en: '**11.6 Virtual Memory, Memory Protection, and Paging**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In a modern operating system such as Android, iOS, Linux, macOS, or Windows,
    it is very common to have several different programs running concurrently in memory.
    This presents several problems:'
  prefs: []
  type: TYPE_NORMAL
- en: How do you keep the programs from interfering with one another’s memory?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If two programs both expect to load a value into memory at address `$1000`,
    how can you load both values and execute both programs at the same time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens if the computer has 64GB of memory, and you decide to load and
    execute three different applications, two of which require 32GB and one that requires
    16GB (not to mention the memory that the OS requires for its own purposes)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The answers to all these questions lie in the virtual memory subsystem that
    modern processors support.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual memory on CPUs such as the 80x86 gives each process its own 32-bit address
    space.^([2](footnotes.xhtml#fn11_2a)) This means that address `$1000` in one program
    is physically different from address `$1000` in a separate program. The CPU achieves
    this sleight of hand by mapping the *virtual addresses* used by programs to different
    *physical addresses* in actual memory. The virtual address and the physical address
    don’t have to be the same, and usually they aren’t. For example, program 1’s virtual
    address `$1000` might actually correspond to physical address `$215000`, while
    program 2’s virtual address `$1000` might correspond to physical memory address
    `$300000`. The CPU accomplishes this using *paging*.
  prefs: []
  type: TYPE_NORMAL
- en: The concept behind paging is quite simple. First, you break up memory into blocks
    of bytes called *pages*. A page in main memory is comparable to a cache line in
    a cache subsystem, although pages are usually much larger than cache lines. For
    example, the 32-bit 80x86 CPUs use a page size of 4,096 bytes; 64-bit variants
    allow larger page sizes.
  prefs: []
  type: TYPE_NORMAL
- en: For each page, you use a lookup table to map the HO bits of a virtual address
    to the HO bits of the physical address in memory, and you use the LO bits of the
    virtual address as an index into that page. For example, with a 4,096-byte page,
    you’d use the LO 12 bits of the virtual address as the offset (0..4095) within
    the page, and the upper 20 bits as an index into a lookup table that returns the
    actual upper 20 bits of the physical address (see [Figure 11-5](ch11.xhtml#ch11fig05)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-5: Translating a virtual address to a physical address*'
  prefs: []
  type: TYPE_NORMAL
- en: A 20-bit index into the page table would require over one million entries in
    the page table. If each entry is a 32-bit value, the page table would be 4MB long—larger
    than many of the programs that would run in memory! However, by using a multilevel
    page table, you can easily create a page table for most small programs that is
    only 8KB long. The details are unimportant here. Just rest assured that you don’t
    need a 4MB page table unless your program consumes the entire 4GB address space.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you study [Figure 11-5](ch11.xhtml#ch11fig05) for a few moments, you’ll
    probably discover one problem with using a page table—it requires two separate
    memory accesses in order to retrieve the data stored at a single physical address
    in memory: one to fetch a value from the page table, and one to read from or write
    to the desired memory location. To prevent cluttering the data or instruction
    cache with page-table entries, which increases the number of cache misses for
    data and instruction requests, the page table uses its own cache, known as the
    *translation lookaside buffer (TLB)*. This cache typically has 64 to 512 entries
    on modern Intel processors—enough to handle a fair amount of memory without a
    miss. Because a program typically works with less data than this at any given
    time, most page-table accesses come from the cache rather than main memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As noted, each entry in the page table contains 32 bits, even though the system
    really only needs 20 bits to remap each virtual address to a physical address.
    Intel, on the 80x86, uses some of the remaining 12 bits to provide memory protection
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: One bit marks whether a page is read/write or read-only.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One bit determines whether you can execute code on that page.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A number of bits determine whether the application can access that page or if
    only the operating system can do so.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A number of bits determine if the CPU has written to the page but hasn’t yet
    written to the physical memory address corresponding to it (that is, whether the
    page is “dirty” or not, and whether the CPU has accessed the page recently).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One bit determines whether the page is actually present in physical memory or
    if it exists on secondary storage somewhere.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your applications do not have access to the page table (reading and writing
    the page table is the operating system’s responsibility), so they cannot modify
    these bits. However, some operating systems provide functions you can call if
    you want to change certain bits in the page table (for example, Windows allows
    you to set a page to read-only).
  prefs: []
  type: TYPE_NORMAL
- en: Beyond remapping memory so multiple programs can coexist in main memory, paging
    also provides a mechanism whereby the operating system can move infrequently used
    pages to secondary storage. Locality of reference applies not only to cache lines
    but to pages in main memory as well. At any given time, a program will access
    only a small percentage of the pages in main memory that contain data and instruction
    bytes; this set of pages is known as the *working set*. Although the working set
    varies slowly over time, for small periods of time it remains constant. Therefore,
    there’s little need for the remainder of the program to consume valuable main-memory
    storage that some other process could be using. If the operating system can save
    the currently unused pages to disk, the main memory they would consume is available
    for other programs that need it.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the problem with moving data out of main memory is that eventually
    the program might actually need it. If you attempt to access a page of memory,
    and the page-table bit tells the memory management unit (MMU) that the page isn’t
    present in main memory, the CPU interrupts the program and passes control to the
    operating system. The operating system reads the corresponding page of data from
    the disk drive and copies it to some available page in main memory. This process
    is nearly identical to the process used by a fully associative cache subsystem,
    except that accessing the disk is much slower than accessing main memory. In fact,
    you can think of main memory as a fully associative write-back cache with 4,096-byte
    cache lines, which caches the data that is stored on the disk drive. Placement
    and replacement policies and other behaviors are very similar for caches and main
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*For more information on how the operating system swaps pages between main
    memory and secondary storage, consult a textbook on operating system design.*'
  prefs: []
  type: TYPE_NORMAL
- en: Because each program has a separate page table, and programs themselves don’t
    have access to the page tables, programs cannot interfere with one another’s operation.
    That is, a program cannot change its page tables in order to access data found
    in another process’s *[address space](gloss01.xhtml#gloss01_7)*. If your program
    crashes by overwriting itself, it cannot crash other programs at the same time.
    This is a big benefit of a paging memory system.
  prefs: []
  type: TYPE_NORMAL
- en: If two programs want to cooperate and share data, they can do so by placing
    that data in a memory area that they share. All they have to do is tell the operating
    system that they want to share some pages of memory. The operating system returns
    to each process a pointer to some segment of memory whose physical address is
    the same for both processes. Under Windows, you can achieve this by using *[memory-mapped
    files](gloss01.xhtml#gloss01_153)*; see the operating system documentation for
    more details. macOS and Linux also support memory-mapped files as well as some
    special shared-memory operations; again, see the OS documentation for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although this discussion applies specifically to the 80x86 CPU, multilevel
    paging systems are common on other CPUs as well. Page sizes tend to vary from
    about 1KB to 4MB, depending on the CPU. For CPUs that support an address space
    larger than 4GB, some CPUs use an *inverted page table* or a *three-level page
    table*. Although the details are beyond the scope of this chapter, the basic principle
    remains the same: the CPU moves data between main memory and the disk in order
    to keep oft-accessed data in main memory as much of the time as possible. These
    other page-table schemes are good at reducing the size of the page table when
    an application uses only a fraction of the available memory space.'
  prefs: []
  type: TYPE_NORMAL
- en: Thrashing
  prefs: []
  type: TYPE_NORMAL
- en: 'Thrashing is a degenerate case that can cause the overall system performance
    to drop to the speed of a lower level in the memory hierarchy, like main memory
    or, worse yet, the disk drive. There are two primary causes of thrashing:'
  prefs: []
  type: TYPE_NORMAL
- en: Insufficient memory at a given level in the memory hierarchy to properly contain
    the programs’ working sets of cache lines or pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A program that does not exhibit locality of reference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is insufficient memory to hold a working set of pages or cache lines,
    the memory system will constantly be replacing one block of data in the cache
    or main memory with another block of data from main memory or the disk. As a result,
    the system winds up operating at the speed of the slower memory in the memory
    hierarchy. A common example of thrashing occurs with virtual memory. A user may
    have several applications running at the same time, and the sum total of the memory
    required by these programs’ working sets is greater than all of the physical memory
    available to the programs. As a result, when the operating system switches between
    the applications it has to copy each application’s data, and possibly program
    instructions, to and from disk. Because switching between programs is often much
    faster than retrieving data from the disk, this slows the programs down tremendously.
  prefs: []
  type: TYPE_NORMAL
- en: As already discussed, if the program does not exhibit locality of reference
    and the lower memory subsystems are not fully associative, thrashing can occur
    even if there is free memory at the current level in the memory hierarchy. To
    revisit our earlier example, suppose an 8KB L1 caching system uses a direct-mapped
    cache with 512 16-byte cache lines. If a program references data objects 8KB apart
    on *every* access, the system will have to replace the same line in the cache
    over and over again with the data from main memory. This occurs even though the
    other 511 cache lines are currently unused.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce thrashing when insufficient memory is the problem, you can simply
    add memory. If that’s not an option, you can try to run fewer processes concurrently
    or modify your program so that it references less memory over a given period.
    To reduce thrashing when locality of reference is the culprit, you should restructure
    your program and its data structures so its memory references are physically closer.
  prefs: []
  type: TYPE_NORMAL
- en: '**11.7 Writing Software That Is Cognizant of the Memory Hierarchy**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Software that is aware of memory performance behavior can run much faster than
    software that is not. Although a system’s caching and paging facilities may perform
    reasonably well for typical programs, it’s easy to write software that would run
    faster even if the caching system were not present. The best software is written
    to take maximum advantage of the memory hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: 'A classic example of a bad design is the following loop, which initializes
    a two-dimensional array of integer values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Believe it or not, that code runs much slower on a modern CPU than the following
    sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The only difference between the two code sequences is that the `i` and `j`
    indices are swapped when accessing elements of the array. This minor modification
    can be responsible for a one or two order of magnitude difference in their respective
    runtimes! To understand why, remember that the C programming language uses row-major
    ordering for two-dimensional arrays in memory. That means the second code sequence
    accesses sequential locations in memory, exhibiting spatial locality of reference.
    The first code sequence, however, accesses array elements in the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If integers are 4 bytes each, then this sequence will access the double-word
    values at offsets 0; 1,024; 2,048; 3,072; and so on, from the base address of
    the array, which are distinctly *not* sequential. Most likely, this code will
    load only *n* integers into an *n*-way set associative cache and then immediately
    cause thrashing thereafter, as each subsequent array element has to be copied
    from the cache into main memory to prevent that data from being overwritten.
  prefs: []
  type: TYPE_NORMAL
- en: The second code sequence does not exhibit thrashing. Assuming 64-byte cache
    lines, the second code sequence will store 16 integer values into the same cache
    line before having to load another cache line from main memory, replacing an existing
    one. As a result, this second code sequence spreads out the cost of retrieving
    the cache line from memory over 16 memory accesses rather than over a single access,
    as occurs with the first code sequence.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to accessing variables sequentially in memory, there are several
    other variable declaration tricks you can use to maximize the performance of the
    memory hierarchy. First, declare together all variables you use within a common
    code sequence. In most languages, this will allocate storage for the variables
    in physically adjacent memory locations, thus supporting spatial locality as well
    as temporal locality. Second, use local (automatic) variables, because most languages
    allocate local storage on the stack and, as the system references the stack frequently,
    variables on the stack tend to be in the cache. Third, declare your scalar variables
    together, and separately from your array and record variables. Access to any one
    of several adjacent scalar variables generally forces the system to load all of
    the adjacent objects into the cache.
  prefs: []
  type: TYPE_NORMAL
- en: In general, study the memory access patterns your program exhibits and adjust
    your application accordingly. You can spend hours rewriting your code in hand-optimized
    assembly language trying to achieve a 10 percent performance improvement, but
    if you instead modify the way your program accesses memory, it’s not unheard of
    to see an order of magnitude improvement in performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**11.8 Runtime Memory Organization**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Operating systems like macOS, Linux, or Windows put different types of data
    into different sections (or *segments*) of main memory. Although it’s possible
    to control the memory organization by running a linker and specifying various
    parameters, by default Windows loads a typical program into memory using the organization
    shown in [Figure 11-6](ch11.xhtml#ch11fig06) (macOS and Linux are similar, though
    they rearrange some of the sections).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-6: Typical Windows runtime memory organization*'
  prefs: []
  type: TYPE_NORMAL
- en: The operating system reserves the lowest memory addresses, and your application
    generally cannot access data (or execute instructions) at these addresses. One
    reason the OS reserves this space is to help detect `NULL` pointer references.
    Programmers often initialize a pointer with `NULL` (`0`) to indicate that it is
    not valid. Should you attempt to access memory location `0` under such an OS,
    it will generate a *general protection fault* to indicate that you’ve accessed
    a memory location that doesn’t contain valid data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The remaining seven sections of memory hold different types of data associated
    with your program:'
  prefs: []
  type: TYPE_NORMAL
- en: The code section holds the program’s machine instructions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The constant section contains compiler-generated read-only data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The read-only data section holds user-defined data that can only be read, never
    written.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The static section stores user-defined, initialized, static variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The storage, or BSS, section holds user-defined uninitialized variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The stack section maintains local variables and other temporary data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The heap section maintains dynamic variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Often, a compiler will combine the code, constant, and read-only data sections
    because they all contain read-only data.*'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, a given application can live with the default layouts chosen
    for these sections by the compiler and linker/loader. In some cases, however,
    knowing the memory layout can help you develop shorter programs. For example,
    combining the code, constants, and read-only data sections into a single read-only
    section can save padding space that the compiler/linker might otherwise place
    between them. Although these savings are probably insignificant for large applications,
    they can have a big impact on the size of a small program.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections discuss each of these memory areas in detail.
  prefs: []
  type: TYPE_NORMAL
- en: '***11.8.1 Static and Dynamic Objects, Binding, and Lifetime***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Before exploring the memory organization of a typical program, we need to define
    a few terms: binding, lifetime, static, and dynamic.'
  prefs: []
  type: TYPE_NORMAL
- en: '*[Binding](gloss01.xhtml#gloss01_30)* is the process of associating an attribute
    with an object. For example, when you assign a value to a variable, the value
    is *bound* to that variable at the point of the assignment. This bond remains
    until you bind some other value to the variable (via another assignment operation).
    Likewise, if you allocate memory for a variable while the program is running,
    the variable is bound to the address at that point. They remain bound until you
    associate a different address with the variable. Binding needn’t occur at runtime.
    For example, values are bound to constant objects during compilation, and these
    bonds cannot change while the program is running.'
  prefs: []
  type: TYPE_NORMAL
- en: The *[lifetime](gloss01.xhtml#gloss01_136)* of an attribute extends from the
    point when you first bind that attribute to an object to the point when you break
    that bond, perhaps by binding a different attribute to the object. For example,
    the lifetime of a variable is from the time you first associate memory with the
    variable to the moment you deallocate that variable’s storage.
  prefs: []
  type: TYPE_NORMAL
- en: '*Static* objects are those that have an attribute bound to them prior to the
    application’s execution (usually during compilation or during the linking phase,
    though it is possible to bind values even earlier). Constants are good examples
    of static objects; they have the same value bound to them throughout program execution.
    Global (program-level) variables in programming languages like Pascal, C/C++,
    and Ada are also examples of static objects in that they have the same address
    bound to them throughout the program’s lifetime. The lifetime of a static object,
    therefore, extends from the point at which the program first begins execution
    to the point when the application terminates.'
  prefs: []
  type: TYPE_NORMAL
- en: Associated with static binding is the notion of identifier *[scope](gloss01.xhtml#gloss01_225)*—the
    section of the program where the identifier’s name is bound to the object. As
    names exist only during compilation, scope qualifies as a static attribute in
    compiled languages. (In interpretive languages, where the interpreter maintains
    the identifier names during program execution, scope can be a nonstatic attribute.)
    The scope of a local variable is generally limited to the procedure or function
    in which you declare it (or to any nested procedure or function declarations in
    block structured languages like Pascal or Ada), and the name is not visible outside
    the subroutine. In fact, it’s possible to reuse an identifier’s name in a different
    scope (that is, in a different function or procedure). In that case, the second
    occurrence of the identifier will be bound to a different object than its first
    occurrence.
  prefs: []
  type: TYPE_NORMAL
- en: '*Dynamic* objects are those that have some attribute assigned to them during
    program execution. While it is running, the program may choose to change that
    attribute (*dynamically*). The lifetime of that attribute begins when the application
    binds the attribute to the object and ends when the program breaks that bond.
    If the program never breaks the bond, the attribute’s lifetime extends from the
    point of association to the point the program terminates. The system binds dynamic
    attributes to an object at runtime, after the application begins execution.'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*An object may have a combination of static and dynamic attributes. For example,
    a static variable has an address bound to it for the entire execution time of
    the program, but it could have different values bound to it throughout the program’s
    lifetime. Any given attribute, however, is either static or dynamic; it cannot
    be both.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***11.8.2 The Code, Read-Only, and Constant Sections***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The code section in memory contains the machine instructions for a program.
    Your compiler translates each statement you write into a sequence of one or more
    byte values. The CPU interprets these byte values as machine instructions during
    program execution.
  prefs: []
  type: TYPE_NORMAL
- en: Most compilers also attach a program’s read-only data to the code section because,
    like the code instructions, the read-only data is already write-protected. However,
    it is perfectly possible under Windows, macOS, Linux, and many other operating
    systems to create a separate section in the executable file and mark it as read-only.
    As a result, some compilers support a separate read-only data section. Such sections
    contain initialized data, tables, and other objects that the program should not
    change during program execution.
  prefs: []
  type: TYPE_NORMAL
- en: The constant section shown in [Figure 11-6](ch11.xhtml#ch11fig06) typically
    contains data that the compiler generates (as opposed to user-defined read-only
    data). Most compilers actually emit this data directly to the code section. This
    is why, as previously noted, in most executable files, you’ll find a single section
    that combines the code, read-only data, and constant data sections.
  prefs: []
  type: TYPE_NORMAL
- en: '***11.8.3 The Static Variables Section***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Many languages enable you to initialize a global variable during the compilation
    phase. For example, in C/C++ you could use statements like the following to provide
    initial values for these static objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In C/C++ and other languages, the compiler places these initial values in the
    executable file. When you execute the application, the OS loads the portion of
    the executable file that contains these static variables into memory so that the
    values appear at the addresses associated with those static variables. Therefore,
    when the program shown here first begins execution, `i` and `ch` will have these
    values bound to them.
  prefs: []
  type: TYPE_NORMAL
- en: '***11.8.4 The Storage Variables Section***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The storage variables (or BSS) section is where compilers typically put static
    objects that don’t have an explicit value associated with them. BSS stands for
    “block started by a symbol,” which is an old assembly language term describing
    a pseudo-opcode you would use to allocate storage for an uninitialized static
    array. In modern operating systems like Windows and Linux, the compiler/linker
    puts all uninitialized variables into a BSS section that simply tells the OS how
    many bytes to set aside for that section. When the OS loads the program into memory,
    it reserves sufficient memory for all the objects in the BSS section and fills
    this range of memory with `0`s.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the BSS section in the executable file doesn’t actually contain any
    data, so programs that declare uninitialized static objects (especially large
    arrays) in a BSS section will consume less disk space.
  prefs: []
  type: TYPE_NORMAL
- en: However, not all compilers actually use a BSS section. Some Microsoft languages
    and linkers, for example, simply place the uninitialized objects in the static/read-only
    data section and explicitly give them an initial value of `0`. Although Microsoft
    claims that this scheme is faster, it certainly makes executable files larger
    if your code has large, uninitialized arrays (because each byte of the array winds
    up in the executable file—something that would not happen if the compiler placed
    the array in a BSS section).
  prefs: []
  type: TYPE_NORMAL
- en: '***11.8.5 The Stack Section***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The stack is a data structure that expands and contracts in response to procedure
    invocations and returns to calling routines, among other things. At runtime, the
    system places all automatic variables (nonstatic local variables), subroutine
    parameters, temporary values, and other objects in the stack section of memory
    in a special data structure called an *[activation record](gloss01.xhtml#gloss01_5)*
    (which is aptly named, as the system creates it when a subroutine first begins
    execution, and deallocates it when the subroutine returns to its caller). Therefore,
    the stack section in memory is very busy.
  prefs: []
  type: TYPE_NORMAL
- en: Most CPUs implement the stack using a register called the *stack pointer*. Some
    CPUs, however, don’t provide an explicit stack pointer, instead using a general-purpose
    register for stack implementation. If a CPU provides a stack pointer, we say that
    it supports a *hardware stack*; if it uses a general-purpose register, then we
    say that it uses a *software-implemented stack*. The 80x86 provides a hardware
    stack, while the MIPS Rx000 CPU family uses a software-implemented stack. Systems
    that provide hardware stacks can generally manipulate data on the stack using
    fewer instructions than systems that implement the stack in software. In theory,
    a hardware stack actually slows down all instructions the CPU executes, but in
    practice, the 80x86 CPU is one of the fastest CPUs around, providing ample proof
    that having a hardware stack doesn’t necessarily mean you’ll wind up with a slow
    CPU.
  prefs: []
  type: TYPE_NORMAL
- en: '***11.8.6 The Heap Section and Dynamic Memory Allocation***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Although simple programs may need only static and automatic variables, sophisticated
    programs need to be able to allocate and deallocate storage dynamically (at runtime)
    under program control. The C and HLA languages provide the `malloc()` and `free()`
    functions for this purpose, C++ provides `new()` and `delete()`, Pascal uses `new()`
    and `dispose()`, and other languages include comparable routines. These memory
    allocation routines have a few things in common: they let the programmer request
    how many bytes of storage to allocate, they return a *pointer* to the newly allocated
    storage (that is, the address of that storage), and they provide a facility for
    returning the storage space to the system once it is no longer needed, so the
    system can reuse it in a future allocation call. Dynamic memory allocation takes
    place in a section of memory known as the *heap*.'
  prefs: []
  type: TYPE_NORMAL
- en: Generally, an application refers to data on the heap using pointer variables
    either implicitly or explicitly; some languages, like Java, implicitly use pointers
    behind the scenes. Thus, objects in heap memory are usually known as *anonymous
    variables* because we refer to them by their memory address (via pointers) rather
    than by name.
  prefs: []
  type: TYPE_NORMAL
- en: The OS and application create the heap section in memory after the program begins
    execution; the heap is never a part of the executable file. Generally, the OS
    and language runtime libraries maintain the heap for an application. Despite the
    variations in memory management implementations, it’s still a good idea for you
    to have a basic idea of how heap allocation and deallocation operate, because
    using them inappropriately will have a very negative impact on your application
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**11.8.6.1 A Simple Memory Allocation Scheme**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An extremely simple (and fast) memory allocation scheme would return a pointer
    to a block of memory whose size the caller requests. It would carve out allocation
    requests from the heap, returning blocks of memory that are currently unused.
  prefs: []
  type: TYPE_NORMAL
- en: A very simple memory manager might maintain a single variable (a *free space*
    pointer) pointing to the heap. Whenever a memory allocation request comes along,
    the system makes a copy of this heap pointer and returns it to the application;
    then the heap management routines add the size of the memory request to the address
    held in the pointer variable and verify that the memory request doesn’t try to
    use more memory than is available in the heap (some memory managers return an
    error indication, like a `NULL` pointer, when the memory request is too large,
    and others raise an exception). As the heap management routines increment the
    free space pointer, they effectively mark all previous memory as “unavailable
    for future requests.”
  prefs: []
  type: TYPE_NORMAL
- en: '**11.8.6.2 Garbage Collection**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The problem with this simple memory management scheme is that it wastes memory,
    because there’s no *garbage collection* mechanism for the application to free
    the memory so it can be reused later. Garbage collection—that is, reclaiming memory
    when an application has finished using it—is one of the main purposes of a heap
    management system.
  prefs: []
  type: TYPE_NORMAL
- en: The only catch is that supporting garbage collection requires some overhead.
    The memory management code will need to be more sophisticated, will take longer
    to execute, and will require some additional memory to maintain the internal data
    structures the heap management system uses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider an easy implementation of a heap manager that supports garbage
    collection. This simple system maintains a (linked) list of free memory blocks.
    Each free memory block in the list requires two double-word values: one specifying
    the size of the free block, and the other containing a link to the next free block
    in the list (that is, a pointer), as shown in [Figure 11-7](ch11.xhtml#ch11fig07).'
  prefs: []
  type: TYPE_NORMAL
- en: The system initializes the heap with a `NULL` link pointer, and the size field
    contains the size of the heap’s entire free space. When a memory allocation request
    comes along, the heap manager searches through the list to find a free block with
    enough memory to satisfy the request. This search process is one of the defining
    characteristics of a heap manager. Some common search algorithms are first-fit
    search and best-fit search. A *[first-fit search](gloss01.xhtml#gloss01_98)*,
    as its name suggests, scans the list of blocks until it finds the *first* block
    of memory large enough to satisfy the allocation request. A *[best-fit search](gloss01.xhtml#gloss01_26)*
    scans the entire list and finds the *smallest* block large enough to satisfy the
    request. The advantage of the best-fit algorithm is that it tends to preserve
    larger blocks better than the first-fit algorithm, so the system is still able
    to satisfy larger subsequent allocation requests when they arrive. The first-fit
    algorithm, on the other hand, just grabs the first suitably large block it finds,
    even if there’s a smaller block that would suffice, which may limit the system’s
    ability to handle future large memory requests.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-7: Heap management using a list of free memory blocks*'
  prefs: []
  type: TYPE_NORMAL
- en: Still, the first-fit algorithm does have a couple of advantages over the best-fit
    algorithm. The most obvious is that it is usually faster. The best-fit algorithm
    has to scan through every block in the free block list in order to find the smallest
    one large enough to satisfy the allocation request (unless, of course, it finds
    a perfectly sized block along the way). The first-fit algorithm can stop once
    it finds a block large enough to satisfy the request.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first-fit algorithm also tends to suffer less from a degenerate condition
    known as *external fragmentation*. Fragmentation occurs after a long sequence
    of allocation and deallocation requests. Remember, when the heap manager satisfies
    a memory allocation request, it usually creates two blocks of memory: one in-use
    block for the request, and one free block that contains the remaining bytes from
    the original block (assuming the request did not exactly match the block size).
    After operating for a while, the best-fit algorithm may have produced lots of
    leftover blocks of memory that are too small to satisfy an average memory request,
    making them effectively unusable. As these small fragments accumulate throughout
    the heap, they can end up consuming a fair amount of memory. This can lead to
    a situation where the heap doesn’t have a sufficiently large block to satisfy
    a memory allocation request even though there is enough total free memory available
    (spread throughout the heap). See [Figure 11-8](ch11.xhtml#ch11fig08) for an example
    of this condition.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-8: Memory fragmentation*'
  prefs: []
  type: TYPE_NORMAL
- en: There are other memory allocation strategies in addition to the first-fit and
    best-fit search algorithms. Some of these execute faster, some have less memory
    overhead, some are easy to understand (and some are very complex), some produce
    less fragmentation, and some can combine and use noncontiguous blocks of free
    memory. Memory/heap management is one of the more heavily studied subjects in
    computer science, and there’s a considerable amount of literature explaining the
    benefits of one scheme over another. For more information on memory allocation
    strategies, check out a good book on OS design.
  prefs: []
  type: TYPE_NORMAL
- en: '**11.8.6.3 Freeing Allocated Memory**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Memory allocation is only half of the story. As mentioned earlier, the heap
    manager has to provide a call that allows an application to return memory it no
    longer needs for future reuse. In C and HLA, for example, an application accomplishes
    this by calling the `free()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'At first blush, `free()` might seem like a very simple function to write: just
    append the previously allocated and now unused block to the end of the free list.
    The problem with this trivial implementation is that it almost guarantees that
    the heap becomes fragmented to the point of being unusable in very short order.
    Consider the situation in [Figure 11-9](ch11.xhtml#ch11fig09).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-9: Freeing a memory block*'
  prefs: []
  type: TYPE_NORMAL
- en: If a trivial implementation of `free()` simply takes the block to be freed and
    appends it to the free list, the memory organization in [Figure 11-9](ch11.xhtml#ch11fig09)
    produces three free blocks. However, because these three blocks are contiguous,
    the heap manager should really combine them into a single free block, so that
    it will be able to satisfy a larger request. Unfortunately, this operation would
    require it to scan the free block list to determine if there are any free blocks
    adjacent to the block the system is freeing.
  prefs: []
  type: TYPE_NORMAL
- en: While you could come up with a data structure that makes it easier to combine
    adjacent free blocks, such schemes generally add 8 or more bytes of overhead with
    each block on the heap. Whether or not this is a reasonable tradeoff depends on
    the average size of a memory allocation. If the applications that use the heap
    manager tend to allocate small objects, the extra overhead for each memory block
    could wind up consuming a large percentage of the heap space. However, if the
    most allocations are large, then the few bytes of overhead won’t matter much.
  prefs: []
  type: TYPE_NORMAL
- en: '**11.8.6.4 The OS and Memory Allocation**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The performance of the algorithms and data structures used by the heap manager
    is only one piece of the performance puzzle. Ultimately, the heap manager needs
    to request blocks of memory from the operating system. At one extreme, the OS
    handles all memory allocation requests directly. At the other extreme, the heap
    manager is a runtime library routine that links with your application, first requesting
    large blocks of memory from the OS and then doling out pieces of them as allocation
    requests arrive from the application.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with making direct memory allocation requests to the operating system
    is that OS API calls are often very slow. This is because they generally involve
    switching between kernel mode and user mode on the CPU (which is not fast). Therefore,
    a heap manager that the OS implements directly will not perform well if your application
    makes frequent calls to the memory allocation and deallocation routines.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the high overhead of an OS call, most languages implement their own
    versions of the `malloc()` and `free()` functions within their runtime library.
    On the very first memory allocation, the `malloc()` routine requests a large block
    of memory from the OS, and the application’s `malloc()` and `free()` routines
    manage this block of memory themselves. If an allocation request comes along that
    the `malloc()` function cannot fulfill in the block it originally created, `malloc()`
    will request another large block (generally much larger than the request) from
    the OS and add that block to the end of its free list. Because the application’s
    `malloc()` and `free()` routines call the OS only occasionally, the application
    doesn’t suffer the performance hit associated with frequent OS calls.
  prefs: []
  type: TYPE_NORMAL
- en: Most standard heap management functions perform reasonably for a typical program.
    However, keep in mind that the procedures are very implementation- and language-specific;
    it’s dangerous to assume that `malloc()` and `free()` are relatively efficient
    when writing software that requires high-performance components. The only portable
    way to ensure a high-performance heap manager is to develop your own application-specific
    set of allocation/deallocation routines. Writing such routines is beyond the scope
    of this book, but you should know you have this option.
  prefs: []
  type: TYPE_NORMAL
- en: '**11.8.6.5 Heap Memory Overhead**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A heap manager often exhibits two types of overhead: performance (speed) and
    memory (space). Until now, this discussion has mainly dealt with the performance
    aspects, but now we’ll turn our attention to memory.'
  prefs: []
  type: TYPE_NORMAL
- en: Each block the system allocates requires some amount of overhead beyond the
    storage the application requests; at the very least, this overhead is a few bytes
    to keep track of the block’s size. Fancier (higher-performance) schemes may require
    additional bytes, but typically the overhead is between 4 and 16 bytes. The heap
    manager can keep this information in a separate internal table, or it can attach
    the block size and other memory management information directly to the block it
    allocates.
  prefs: []
  type: TYPE_NORMAL
- en: Saving this information in an internal table has a couple of advantages. First,
    it is difficult for the application to accidentally overwrite the information
    stored there; attaching the data to the heap memory blocks themselves doesn’t
    protect as well against this possibility. Second, putting memory management information
    in an internal data structure allows the memory manager to determine whether a
    given pointer is valid (that is, whether it points at some block of memory that
    the heap manager believes it has allocated).
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of attaching the control information directly to each block that
    the heap manager allocates is that it’s very easy to locate this information,
    whereas storing the information in an internal table might require a search operation.
  prefs: []
  type: TYPE_NORMAL
- en: Another issue that affects the overhead associated with the heap manager is
    the *allocation granularity*—the minimum number of bytes the heap manager supports.
    Although most heap managers allow you to request an allocation as small as 1 byte,
    they may actually allocate some minimum number of bytes greater than 1\. To ensure
    an allocated object is aligned on a reasonable address for that object, most heap
    managers allocate memory blocks on a 4-, 8-, or 16-byte boundary. For performance
    reasons, many heap managers begin each allocation on a typical cache-line boundary,
    usually 16, 32, or 64 bytes.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever the granularity, if the application requests some number of bytes that
    is less than or not a multiple of the heap manager’s granularity, the heap manager
    will allocate extra bytes of storage so that the complete allocation is an even
    multiple of the granularity value. This amount varies by heap manager (and possibly
    even by version of a specific heap manager), so an application should never assume
    that it has more memory available than it requests.
  prefs: []
  type: TYPE_NORMAL
- en: The extra memory the heap manager allocates results in another form of fragmentation
    called *internal fragmentation*. Like external fragmentation, internal fragmentation
    produces small amounts of leftover memory throughout the system that cannot satisfy
    future allocation requests. Assuming random-sized memory allocations, the average
    amount of internal fragmentation that occurs on each allocation is half the granularity
    size. Fortunately, the granularity size is quite small for most memory managers
    (typically 16 bytes or less), so after thousands and thousands of memory allocations
    you’ll lose only a couple dozen or so kilobytes to internal fragmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Between the costs associated with allocation granularity and the memory control
    information, a typical memory request may require between 4 and 16 bytes, plus
    whatever the application requests. If you’re making large memory allocation requests
    (hundreds or thousands of bytes), the overhead bytes won’t consume a large percentage
    of memory on the heap. However, if you allocate lots of small objects, the memory
    consumed by internal fragmentation and memory control information may represent
    a significant portion of your heap area. For example, consider a simple memory
    manager that always allocates blocks of data on 4-byte boundaries and requires
    a single 4-byte length value that it attaches to each allocation request for memory
    storage. This means that the minimum amount of storage the heap manager requires
    for each allocation is 8 bytes. If you make a series of `malloc()` calls to allocate
    a single byte, the application won’t be able to use almost 88 percent of the memory
    it allocates. Even if you allocate 4-byte values on each allocation request, the
    heap manager consumes 67 percent of the memory for overhead purposes. However,
    if your average allocation is a block of 256 bytes, the overhead requires only
    about 2 percent of the total memory allocation. In short, the larger your allocation
    request, the less impact the control information and internal fragmentation will
    have on your heap.
  prefs: []
  type: TYPE_NORMAL
- en: Many software engineering studies in computer science journals have found that
    memory allocation/deallocation requests cause a significant loss of performance.
    In such studies, the authors often obtained performance improvements of 100 percent
    or better just by implementing their own simplified, application-specific, memory
    management algorithms rather than calling the standard runtime library or OS kernel
    memory allocation code. Hopefully, this section has made you aware of this potential
    problem in your own code.
  prefs: []
  type: TYPE_NORMAL
- en: '**11.9 For More Information**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hennessy, John L., and David A. Patterson. *Computer Architecture: A Quantitative
    Approach*. 5th ed. Waltham, MA: Elsevier, 2012.'
  prefs: []
  type: TYPE_NORMAL
