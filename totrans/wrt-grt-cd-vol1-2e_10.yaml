- en: '**11'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**11'
- en: MEMORY ARCHITECTURE AND ORGANIZATION**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 内存架构与组织**
- en: '![Image](../images/comm1.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/comm1.jpg)'
- en: This chapter discusses memory hierarchy—the different types and performance
    levels of memory found in computer systems. Although programmers often treat all
    forms of memory as though they are equivalent, using memory improperly can have
    a negative impact on performance. In this chapter you’ll see how to make the best
    use of the memory hierarchy within your programs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论内存层次结构——计算机系统中不同类型和性能级别的内存。尽管程序员通常将所有形式的内存视为等同，但不正确地使用内存可能会对性能产生负面影响。本章将展示如何在程序中最好地利用内存层次结构。
- en: '**11.1 The Memory Hierarchy**'
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**11.1 内存层次结构**'
- en: Most modern programs benefit by having a large amount of very fast memory. Unfortunately,
    as a memory device gets larger, it tends to be slower. For example, cache memories
    are very fast, but they are also small and expensive. Main memory is inexpensive
    and large, but it is slow, requiring wait states. The memory hierarchy provides
    a way to compare the cost and performance of different types of memory. [Figure
    11-1](ch11.xhtml#ch11fig01) shows one variant of the memory hierarchy.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代程序都能从大量非常快速的内存中受益。不幸的是，随着内存设备变大，它往往会变得更慢。例如，缓存内存非常快速，但它们也很小且昂贵。主内存便宜且容量大，但它很慢，需要等待周期。内存层次结构提供了一种比较不同类型内存的成本和性能的方式。[图
    11-1](ch11.xhtml#ch11fig01) 显示了内存层次结构的一种变种。
- en: '![image](../images/11fig01.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig01.jpg)'
- en: '*Figure 11-1: The memory hierarchy*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-1：内存层次结构*'
- en: At the top level of the memory hierarchy are the CPU’s general-purpose *registers*.
    Registers provide the fastest access to data possible on the CPU. The register
    file is also the smallest memory object in the hierarchy (for example, the 32-bit
    80x86 has just eight general-purpose registers, and the x86-64 variants have up
    to 16 general-purpose registers). Because it is impossible to add more registers
    to a CPU, they are also the most expensive memory locations. Even if we count
    the FPU, MMX/AltiVec/Neon, SSE/SIMD, AVX/2/-512, and other CPU registers in this
    portion of the memory hierarchy, it does not change the fact that CPUs have a
    very limited number of registers, and the cost per byte of register memory is
    quite high.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 内存层次结构的顶层是 CPU 的通用 *寄存器*。寄存器提供了对数据的最快访问方式。寄存器文件也是层次结构中最小的内存对象（例如，32 位的 80x86
    只有八个通用寄存器，而 x86-64 变种最多有 16 个通用寄存器）。由于无法在 CPU 中增加更多寄存器，因此它们也是最昂贵的内存位置。即使我们将 FPU、MMX/AltiVec/Neon、SSE/SIMD、AVX/2/-512
    和其他 CPU 寄存器计算在内，这一部分内存层次结构的寄存器数量仍然非常有限，而且每个寄存器字节的成本相当高。
- en: Working our way down, the *level-one (L1) cache* system is the next highest
    performance subsystem in the memory hierarchy. As with registers, the CPU manufacturer
    usually provides the L1 cache on the chip, and you cannot expand it. Its size
    is usually small, typically between 4KB and 32KB, though this is much larger than
    the register memory available on the CPU chip. Although the L1 cache size is fixed
    on the CPU, the cost per cache byte is much lower than the cost per register byte,
    because the cache contains more storage than is available in all the registers
    combined, and the system designer’s cost for both memory types equals the price
    of the CPU.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进入 *一级缓存（L1 Cache）* 系统，它是内存层次结构中下一个性能最好的子系统。与寄存器类似，CPU 制造商通常将 L1 缓存集成在芯片上，且无法扩展。其大小通常较小，通常在
    4KB 到 32KB 之间，尽管这比 CPU 芯片上可用的寄存器内存要大得多。虽然 L1 缓存的大小在 CPU 上是固定的，但每个缓存字节的成本远低于每个寄存器字节的成本，因为缓存存储的容量超过了所有寄存器总和，而系统设计者为这两种内存类型支付的成本等于
    CPU 的价格。
- en: '*Level-two (L2) cache* is present on some CPUs, but not all. For example, Intel
    i3, i5, i7, and i9 CPUs include an L2 cache as part of their package, but some
    of Intel’s older Celeron chips do not. The L2 cache is generally much larger than
    the L1 cache (for example, 256KB to 1MB as compared with 4KB to 32KB). On CPUs
    with a built-in L2 cache, the cache is not expandable. It still costs less than
    the L1 cache, because we amortize the cost of the CPU across all the bytes in
    the two caches, and the L2 cache is larger.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*二级缓存（L2 Cache）* 在某些 CPU 上存在，但并非所有 CPU 都有。例如，英特尔的 i3、i5、i7 和 i9 CPU 包括 L2 缓存作为其一部分，但一些英特尔旧版
    Celeron 芯片则没有。L2 缓存通常比 L1 缓存大得多（例如，256KB 到 1MB，相较于 4KB 到 32KB）。在具有内建 L2 缓存的 CPU
    上，缓存无法扩展。它的成本仍然低于 L1 缓存，因为我们将 CPU 的成本分摊到两个缓存的所有字节上，而 L2 缓存更大。'
- en: '*Level-three (L3) cache* is present on all but the oldest Intel processors.
    The L3 cache is larger still than the L2 cache (typically 8MB on later Intel chips).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*三级缓存（L3）*出现在几乎所有Intel处理器中，除了最旧的型号。L3缓存比L2缓存更大（在较新的Intel芯片上通常为8MB）。'
- en: The *main-memory* subsystem comes below the L3 (or L2, if there is no L3) cache
    system in the memory hierarchy. Main memory is the general-purpose, relatively
    low-cost memory—typically DRAM or something similarly inexpensive—found in most
    computer systems. However, there are many differences in main-memory technology
    that result in variations in speed. The main-memory types include standard DRAM,
    synchronous DRAM (SDRAM), double data rate DRAM (DDRAM), DDR3, DDR4, and so on.
    Generally, you won’t find a mixture of these technologies in the same computer
    system.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*主内存*子系统位于L3（如果没有L3则是L2）缓存系统下方，属于内存层次结构中的一部分。主内存是通用的、相对低成本的内存——通常是DRAM或类似的便宜内存——存在于大多数计算机系统中。然而，主内存技术存在许多差异，导致速度有所不同。主内存类型包括标准DRAM、同步DRAM（SDRAM）、双倍数据速率DRAM（DDRAM）、DDR3、DDR4等。通常，你不会在同一台计算机系统中发现这些技术的混合。'
- en: Below main memory is the *[NUMA](gloss01.xhtml#gloss01_175)* memory subsystem.
    NUMA, which stands for *Non-Uniform Memory Access*, is a bit of a misnomer. The
    term implies that different types of memory have different access times, which
    describes the entire memory hierarchy; in [Figure 11-1](ch11.xhtml#ch11fig01),
    however, it refers to blocks of memory that are electronically similar to main
    memory but, for one reason or another, operate significantly slower. A good example
    of NUMA is the memory on a video (or graphics) card. Another example is flash
    memory, which has significantly slower access and transfer times than standard
    semiconductor RAM. Other peripheral devices that provide a block of memory to
    be shared between the CPU and the peripheral usually have slow access times as
    well.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 主内存下方是*NUMA*内存子系统。NUMA代表*非统一内存访问*，这有点误导。该术语意味着不同类型的内存具有不同的访问时间，这描述了整个内存层次结构；然而，在[图11-1](ch11.xhtml#ch11fig01)中，它指的是与主内存电子特性相似，但因某种原因操作速度明显较慢的内存块。NUMA的一个典型例子是视频（或图形）卡上的内存。另一个例子是闪存，它的访问和传输时间比标准半导体RAM慢得多。其他提供内存块以供CPU和外设共享的外部设备通常也具有较慢的访问时间。
- en: Most modern computer systems implement a *[virtual memory](gloss01.xhtml#gloss01_256)*
    scheme that simulates main memory using a mass storage disk drive. A virtual memory
    subsystem is responsible for transparently copying data between the disk and main
    memory as programs need it. While disks are significantly slower than main memory,
    the cost per bit is also three orders of magnitude lower for disks. Therefore,
    it’s far less expensive to keep data on magnetic storage or on a solid-state drive
    (SSD) than in main memory.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代计算机系统实现了一个*虚拟内存*方案，利用大容量存储磁盘驱动器来模拟主内存。虚拟内存子系统负责在程序需要时，透明地将数据从磁盘复制到主内存，反之亦然。虽然磁盘的速度明显慢于主内存，但磁盘每比特的成本也低三个数量级。因此，将数据保存在磁存储器或固态硬盘（SSD）上比保存在主内存中便宜得多。
- en: '*File storage* memory also uses disk media to store program data. However,
    whereas the virtual memory subsystem is responsible for transferring data between
    disk (or SSD) and main memory as programs require, it is the program’s responsibility
    to store and retrieve file storage data. In many instances, it’s a bit slower
    to use file storage memory than it is to use virtual memory, which is why file
    storage memory is lower in the memory hierarchy.^([1](footnotes.xhtml#fn11_1a))'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*文件存储*内存也使用磁盘介质来存储程序数据。然而，虚拟内存子系统负责在程序需要时，将数据在磁盘（或SSD）和主内存之间传输，而存储和检索文件存储数据则由程序负责。在许多情况下，使用文件存储内存的速度比使用虚拟内存稍慢，这也是文件存储内存在内存层次结构中较低的原因。^([1](footnotes.xhtml#fn11_1a))'
- en: Next comes *network storage*. At this level in the memory hierarchy, programs
    keep data on a different memory system that connects to the computer system via
    a network. Network storage can be virtual memory, file storage memory, or *distributed
    shared memory (DSM)*, where processes running on different computer systems share
    data stored in a common block of memory and communicate changes to that block
    across the network.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是*网络存储*。在内存层次结构的这一层，程序将数据保存在通过网络连接到计算机系统的不同内存系统中。网络存储可以是虚拟内存、文件存储内存或*分布式共享内存（DSM）*，其中不同计算机系统上运行的进程共享存储在公共内存块中的数据，并通过网络交流对该内存块的更改。
- en: Virtual memory, file storage, and network storage are examples of *online memory
    subsystems*. Memory access within these memory subsystems is slower than accessing
    main memory. However, when a program requests data from one of these three online
    memory subsystems, the memory device will respond to the request as quickly as
    its hardware allows. This is not true for the remaining levels in the memory hierarchy.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟内存、文件存储和网络存储是*在线内存子系统*的例子。这些内存子系统中的内存访问速度比访问主内存要慢。然而，当程序请求来自这些三种在线内存子系统的数据时，内存设备会尽可能快地响应请求，取决于其硬件的响应能力。这对内存层次结构中其余的层次并不适用。
- en: The *near-line* and *[offline storage](gloss01.xhtml#gloss01_180)* subsystems
    may not be ready to respond immediately to a program’s request for data. An offline
    storage system keeps its data in electronic form (usually magnetic or optical)
    but on storage media that are not necessarily connected to the computer system
    that needs the data. Examples of offline storage include magnetic tapes, unattached
    external disk drives, disk cartridges, optical disks, USB memory sticks, SD cards,
    and floppy diskettes. When a program needs to access data stored offline, it must
    stop and wait for someone or something to mount the appropriate media on the computer
    system. This delay can be quite long (perhaps the computer operator decided to
    take a coffee break?).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*近线*和*[离线存储](gloss01.xhtml#gloss01_180)*子系统可能无法立即响应程序对数据的请求。离线存储系统将数据以电子形式（通常是磁性或光学形式）存储在介质上，但这些介质未必与需要数据的计算机系统相连接。离线存储的例子包括磁带、未连接的外部磁盘驱动器、磁盘盒、光盘、USB
    存储棒、SD 卡和软盘。当程序需要访问离线存储的数据时，它必须停止并等待某人或某个设备将适当的介质挂载到计算机系统上。这种延迟可能相当长（也许计算机操作员决定去喝杯咖啡？）。'
- en: Near-line storage uses the same types of media as offline storage, but rather
    than requiring an external source to mount the media before its data is available
    for access, the near-line storage system holds the media in a special robotic
    jukebox device that can automatically mount the desired media when a program requests
    it.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 近线存储使用与离线存储相同类型的介质，但它不需要外部源在数据可供访问之前挂载介质。近线存储系统将介质存放在一个特殊的机器人唱盘设备中，程序请求时，该设备可以自动挂载所需的介质。
- en: Hardcopy storage is simply a printout, in one form or another, of data. If a
    program requests some data, and that data exists only in hardcopy form, someone
    will have to manually enter the data into the computer. Paper or other hardcopy
    media is probably the least expensive form of memory, at least for certain data
    types.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 硬拷贝存储只是以某种形式打印出来的数据。如果程序请求某些数据，而这些数据仅以硬拷贝形式存在，那么必须由某人手动将数据输入计算机。纸张或其他硬拷贝介质可能是最便宜的存储形式，至少对于某些数据类型来说是如此。
- en: '**11.2 How the Memory Hierarchy Operates**'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**11.2 内存层次结构的运作**'
- en: The whole point of the memory hierarchy is to allow reasonably fast access to
    a large amount of memory. If only a little memory were necessary, we’d use fast
    static RAM (the circuitry that cache memory uses) for everything. If speed wasn’t
    an issue, we’d use virtual memory for everything. The memory hierarchy enables
    us to take advantage of the principles of *spatial locality of reference* and
    *temporality of reference* to move frequently referenced data into fast memory
    and leave rarely referenced data in slower memory. Unfortunately, during the course
    of a program’s execution, the sets of oft-used and seldom-used data change. We
    can’t simply distribute our data throughout the various levels of the memory hierarchy
    when the program starts and then leave the data alone as the program executes.
    Instead, the different memory subsystems need to be able to accommodate changes
    in spatial locality or temporality of reference during the program’s execution
    by dynamically moving data between subsystems.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 内存层次结构的核心目的是为了允许合理快速地访问大量内存。如果只需要少量内存，我们会为所有操作使用快速的静态 RAM（缓存内存使用的电路）。如果速度不是问题，我们会为所有操作使用虚拟内存。内存层次结构使我们能够利用*空间局部性*和*时间局部性*的原理，将经常访问的数据移入快速内存，将不常访问的数据留在较慢的内存中。不幸的是，在程序执行过程中，常用数据和不常用数据的集合会发生变化。我们不能在程序开始时就简单地将数据分配到各个内存层级中，并在程序执行过程中不再处理这些数据。相反，不同的内存子系统需要能够在程序执行过程中根据空间局部性或时间局部性的变化，通过动态移动数据来适应这些变化。
- en: Moving data between the registers and memory is strictly a program function.
    The program loads data into registers and stores register data into memory using
    machine instructions like `mov`. It is the programmer’s or compiler’s responsibility
    to keep heavily referenced data in the registers as long as possible; the CPU
    will not automatically place data in general-purpose registers in order to achieve
    higher performance.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在寄存器和内存之间移动数据完全是程序的功能。程序使用诸如`mov`之类的机器指令将数据加载到寄存器中，并将寄存器中的数据存储到内存中。程序员或编译器有责任尽可能长时间将频繁访问的数据保留在寄存器中；CPU不会自动将数据放入通用寄存器以提高性能。
- en: Programs explicitly control access to registers, main memory, and those memory-hierarchy
    subsystems only at the file storage level and below. Programs are largely unaware
    of the memory hierarchy between the register level and main memory. In particular,
    cache access and virtual memory operations are generally transparent to the program;
    that is, access to these levels of the memory hierarchy usually occurs without
    any intervention on a program’s part. Programs simply access main memory, and
    the hardware and operating system take care of the rest.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 程序只能在文件存储层及以下的级别显式控制对寄存器、主内存和那些内存层次子系统的访问。程序通常并不意识到寄存器级别和主内存之间的内存层次关系。特别是，缓存访问和虚拟内存操作通常对程序是透明的；也就是说，对这些内存层次的访问通常在程序不干预的情况下发生。程序只是访问主内存，硬件和操作系统负责其余部分。
- en: Of course, if a program always accesses main memory, it will run slowly, because
    modern DRAM main-memory subsystems are much slower than the CPU. The job of the
    cache memory subsystems and of the CPU’s cache controller is to move data between
    main memory and the L1, L2, and L3 caches so that the CPU can quickly access oft-requested
    data. Likewise, it is the virtual memory subsystem’s responsibility to move oft-requested
    data from hard disk to main memory (if even faster access is needed, the caching
    subsystem will then move the data from main memory to cache).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果程序总是访问主内存，它会运行得很慢，因为现代DRAM主内存子系统的速度远远慢于CPU。缓存内存子系统和CPU缓存控制器的工作是将数据从主内存移动到L1、L2和L3缓存，以便CPU能够快速访问经常请求的数据。同样，虚拟内存子系统的职责是将经常请求的数据从硬盘移动到主内存（如果需要更快的访问，缓存子系统则会将数据从主内存移动到缓存）。
- en: With few exceptions, most memory subsystem accesses take place transparently
    between one level of the memory hierarchy and the level immediately below or above
    it. For example, the CPU rarely accesses main memory directly. Instead, when the
    CPU requests data from memory, the L1 cache subsystem takes over. If the requested
    data is in the cache, the L1 cache subsystem returns the data to the CPU, and
    that concludes the memory access. If the requested data isn’t present in the L1
    cache, the L1 cache subsystem passes the request down to the L2 cache subsystem.
    If the L2 cache subsystem has the data, it returns this data to the L1 cache,
    which then returns the data to the CPU. Requests for the same data in the near
    future will be fulfilled by the L1 cache rather than the L2 cache, because the
    L1 cache now has a copy of the data. After the L2 cache, the L3 cache kicks in.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除少数例外，大多数内存子系统的访问都是透明的，在内存层次结构的一个级别与其上下一个级别之间进行。例如，CPU很少直接访问主内存。相反，当CPU请求数据时，L1缓存子系统接管此请求。如果请求的数据在缓存中，L1缓存子系统将数据返回给CPU，内存访问过程结束。如果请求的数据不在L1缓存中，L1缓存子系统会将请求传递给L2缓存子系统。如果L2缓存子系统有该数据，它将数据返回给L1缓存，然后L1缓存将数据返回给CPU。未来对相同数据的请求将由L1缓存而非L2缓存来满足，因为L1缓存现在拥有该数据的副本。L2缓存之后，L3缓存接管。
- en: If none of the L1, L2, or L3 cache subsystems have a copy of the data, the request
    goes to main memory. If the data is found in main memory, the main-memory subsystem
    passes it to the L3 cache, which then passes it to the L2 cache, which then passes
    it to the L1 cache, which then passes it to the CPU. Once again, the data is now
    in the L1 cache, so any requests for this data in the near future will be fulfilled
    by the L1 cache.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果L1、L2或L3缓存子系统都没有数据的副本，请求将传送到主内存。如果在主内存中找到数据，主内存子系统将其传递给L3缓存，L3缓存再将其传递给L2缓存，L2缓存再传递给L1缓存，L1缓存最终将其传递给CPU。数据现在已经位于L1缓存中，因此未来对该数据的任何请求都将由L1缓存来满足。
- en: If the data is not present in main memory but exists in virtual memory on some
    storage device, the operating system takes over, reads the data from disk or some
    other device (such as a network storage server), and passes the data to the main-memory
    subsystem. Main memory then passes the data through the caches to the CPU as previously
    described.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据不在主内存中，而是存储在某个存储设备上的虚拟内存中，操作系统会接管，先从磁盘或其他设备（如网络存储服务器）读取数据，再将数据传递给主内存子系统。然后，主内存通过缓存将数据传递给CPU，如前所述。
- en: Because of spatial locality and temporality, the largest percentage of memory
    accesses takes place in the L1 cache subsystem. The next largest percentage of
    accesses takes place in the L2 cache subsystem. After that, the L3 cache system
    handles most accesses. The most infrequent accesses take place in virtual memory.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于空间局部性和时间局部性，大部分内存访问发生在L1缓存子系统。接下来，大部分访问发生在L2缓存子系统。之后，L3缓存系统处理大部分访问。最不频繁的访问发生在虚拟内存中。
- en: '**11.3 Relative Performance of Memory Subsystems**'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**11.3 内存子系统的相对性能**'
- en: Looking again at [Figure 11-1](ch11.xhtml#ch11fig01), notice that the speed
    of the various memory hierarchy levels increases as you go up. Exactly how much
    faster is each successive level in the memory hierarchy? The short answer is that
    the speed gradient isn’t uniform. The speed difference between any two contiguous
    levels ranges from “almost no difference” to “four orders of magnitude.”
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 再看一下[图11-1](ch11.xhtml#ch11fig01)，注意到随着层次的上升，各个内存层级的速度增加。那么，每个连续层级的速度差异有多大呢？简短的回答是，速度梯度不是均匀的。任何两个相邻层级之间的速度差异从“几乎没有差别”到“四个数量级”不等。
- en: Registers are, unquestionably, the best place to store data you need to access
    quickly. Accessing a register never requires any extra time, and most machine
    instructions that access data can access register data. Furthermore, instructions
    that access memory often require extra bytes (displacement bytes) as part of the
    instruction encoding. This makes instructions longer and, often, slower.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 寄存器无疑是存储你需要快速访问的数据的最佳位置。访问寄存器从不需要额外时间，大多数访问数据的机器指令可以访问寄存器数据。此外，访问内存的指令通常需要额外的字节（位移字节）作为指令编码的一部分。这使得指令变得更长，且通常执行更慢。
- en: Intel’s instruction timing tables for the 80x86 claim that an instruction like
    `mov(`someVar`,` `ecx);` should run as fast as an instruction like `mov(ebx,`
    `ecx);`. However, if you read the fine print, you’ll find that Intel makes this
    claim based on several assumptions about the former instruction. First, it assumes
    that someVar’s value is present in the L1 cache memory. If it is not, the cache
    controller has to look in the L2 cache, in the L3 cache, in main memory, or, worse,
    on disk in the virtual memory subsystem. All of a sudden, an instruction that
    should execute in 0.25 nanoseconds on a 4 GHz processor (that is, in one clock
    cycle) requires several milliseconds to execute. That’s a difference of over six
    orders of magnitude. It’s true that future accesses of this variable will take
    place in just one clock cycle because it will subsequently be stored in the L1
    cache. But even if you access someVar’s value one million times while it’s still
    in the cache, the average time of each access will still be about two cycles because
    of how long it takes to access someVar the very first time.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 英特尔的80x86指令时序表声称，像`mov(`someVar`,` `ecx);`这样的指令应该和`mov(ebx,` `ecx);`一样快速。然而，如果你仔细阅读细则，会发现英特尔是基于几个假设做出这一声明的。首先，它假设someVar的值已经存在于L1缓存中。如果不在，缓存控制器必须先在L2缓存中查找，再到L3缓存、主内存，甚至更糟的是，在虚拟内存子系统的磁盘上查找。突然之间，原本应该在4
    GHz处理器上的0.25纳秒（即一个时钟周期）内执行的指令，竟然需要几毫秒才能完成。这是一个超过六个数量级的差异。确实，未来访问该变量时，将只需一个时钟周期，因为它会被存储在L1缓存中。但是，即使你在缓存中访问someVar的值一百万次，每次访问的平均时间仍然大约是两个时钟周期，因为第一次访问someVar时需要的时间比较长。
- en: Granted, the likelihood that some variable will be located on disk in the virtual
    memory subsystem is quite low. However, there’s still a difference in performance
    of a couple orders of magnitude between the L1 cache subsystem and the main-memory
    subsystem. Therefore, if the program has to retrieve the data from main memory,
    999 memory accesses later, you’re still paying an average cost of two clock cycles
    to access data that Intel’s documentation claims should take one cycle.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，某些变量在虚拟内存子系统中存储在磁盘上的可能性是相当低的。然而，L1 缓存子系统和主内存子系统之间仍然存在几倍数量级的性能差异。因此，如果程序必须从主内存中检索数据，经过
    999 次内存访问后，你仍然需要支付大约两个时钟周期的平均成本，而英特尔的文档声称这应该只需要一个周期。
- en: The difference in speed between the L1, L2, and L3 cache systems isn’t so dramatic
    unless the secondary or tertiary cache is not packaged together on the CPU. On
    a 4 GHz processor, the L1 cache must respond within 0.25 nanoseconds if the cache
    operates with zero wait states (some processors actually introduce wait states
    in L1 cache accesses, but CPU designers try to avoid this). Accessing data in
    the L2 cache is always slower than in the L1 cache, and always includes the equivalent
    of at least one wait state, and probably more.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除非二级或三级缓存没有与 CPU 一起打包，否则 L1、L2 和 L3 缓存系统之间的速度差异不会那么显著。在 4 GHz 的处理器上，如果缓存在零等待周期下工作，L1
    缓存必须在 0.25 纳秒内响应（一些处理器在 L1 缓存访问中实际上会引入等待周期，但 CPU 设计师会尽量避免这种情况）。访问 L2 缓存中的数据总是比
    L1 缓存慢，并且总是至少包含一个等待周期，相对可能更多。
- en: There are several reasons why L2 cache accesses are slower than L1 accesses.
    First, it takes the CPU time to determine that the data it’s seeking is not in
    the L1 cache. By the time it does that, the memory access cycle is nearly complete,
    and there’s no time to access the data in the L2 cache. Secondly, the circuitry
    of the L2 cache may be slower than the circuitry of the L1 cache in order to make
    the L2 cache less expensive. Third, L2 caches are usually 16 to 64 times larger
    than L1 caches, and larger memory subsystems tend to be slower than smaller ones.
    All this amounts to additional wait states for accessing data in the L2 cache.
    As noted earlier, the L2 cache can be as much as one order of magnitude slower
    than the L1 cache. The same situation occurs when you have to access data in the
    L3 cache.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: L2 缓存访问速度慢于 L1 缓存访问的原因有几个。首先，CPU 需要时间来判断它要寻找的数据是否不在 L1 缓存中。当它做出判断时，内存访问周期几乎已经完成，已经没有时间去访问
    L2 缓存中的数据。其次，为了降低成本，L2 缓存的电路可能比 L1 缓存的电路更慢。第三，L2 缓存通常比 L1 缓存大 16 到 64 倍，而较大的内存子系统往往比较小的更慢。这一切都会导致访问
    L2 缓存时需要额外的等待周期。如前所述，L2 缓存的访问速度可能比 L1 缓存慢一个数量级。当你必须访问 L3 缓存中的数据时，也会出现同样的情况。
- en: The L1, L2, and L3 caches also differ in the amount of data the system fetches
    when there is a cache miss (see [Chapter 6](ch06.xhtml#ch06)). When the CPU fetches
    data from or writes data to the L1 cache, it generally fetches or writes only
    the data requested. If you execute a `mov(al,` `memory);` instruction, the CPU
    writes only a single byte to the cache. Likewise, if you execute the `mov(mem32,`
    `eax);` instruction, the CPU reads exactly 32 bits from the L1 cache. However,
    access to memory subsystems below the L1 cache does not work in small chunks like
    this. Usually, memory subsystems move blocks of data, or *[cache lines](gloss01.xhtml#gloss01_42)*,
    whenever accessing lower levels of the memory hierarchy. For example, if you execute
    the `mov(mem32,` `eax);` instruction, and `mem32`’s value is not in the L1 cache,
    the cache controller doesn’t simply read `mem32`’s 32 bits from the L2 cache,
    assuming that it’s present there. Instead, the cache controller will actually
    read a whole block of bytes (generally 16, 32, or 64 bytes, depending on the particular
    processor) from the L2 cache. The hope is that the program exhibits spatial locality
    so that reading a block of bytes will speed up future accesses to adjacent objects
    in memory. Unfortunately, the `mov(mem32,` `eax);` instruction doesn’t complete
    until the L1 cache reads the entire cache line from the L2 cache. This excess
    time is known as *[latency](gloss01.xhtml#gloss01_133)*. If the program does not
    access memory objects adjacent to `mem32` in the future, this latency is lost
    time.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: L1、L2 和 L3 缓存在发生缓存未命中时系统获取的数据量也有所不同（参见 [第 6 章](ch06.xhtml#ch06)）。当 CPU 从 L1
    缓存中获取或写入数据时，通常只会获取或写入请求的数据。如果你执行 `mov(al,` `memory);` 指令，CPU 只会向缓存写入一个字节。类似地，如果你执行
    `mov(mem32,` `eax);` 指令，CPU 会从 L1 缓存中读取恰好 32 位的数据。然而，访问 L1 缓存以下的内存子系统并不像这样按小块操作。通常，内存子系统在访问内存层级的更低级别时，会移动数据块或
    *[缓存行](gloss01.xhtml#gloss01_42)*。例如，如果你执行 `mov(mem32,` `eax);` 指令，并且 `mem32`
    的值不在 L1 缓存中，缓存控制器不会仅仅从 L2 缓存中读取 `mem32` 的 32 位数据，假设它在那里存在。相反，缓存控制器会从 L2 缓存中读取一整块字节（通常为
    16、32 或 64 字节，具体取决于处理器）。希望程序能表现出空间局部性，从而读取一块字节可以加速对内存中相邻对象的未来访问。不幸的是，`mov(mem32,`
    `eax);` 指令不会完成，直到 L1 缓存从 L2 缓存中读取完整的缓存行。这段额外的时间被称为 *[延迟](gloss01.xhtml#gloss01_133)*。如果程序在未来不再访问与
    `mem32` 相邻的内存对象，这段延迟时间就成了浪费的时间。
- en: A similar performance gulf separates the L2 and L3 caches and L3 and main memory.
    Main memory is typically one order of magnitude slower than the L3 cache; L3 accesses
    are much slower than L2 access. To speed up access to adjacent memory objects,
    the L3 cache reads data from main memory in cache lines. Likewise, L2 cache reads
    cache lines from L3.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: L2 和 L3 缓存以及 L3 和主内存之间存在类似的性能差距。主内存通常比 L3 缓存慢一个数量级；L3 访问比 L2 访问慢得多。为了加速对相邻内存对象的访问，L3
    缓存会以缓存行的形式从主内存中读取数据。同样，L2 缓存也会从 L3 中读取缓存行。
- en: Standard DRAM is two to three orders of magnitude faster than SSD storage (which
    is an order of magnitude faster than hard drives, which is why hard disks often
    have their own DRAM-based caches). To overcome this, there’s usually a difference
    of two to three orders of magnitude in size between the L3 cache and the main
    memory so that the difference in speed between disk and main memory matches that
    between the main memory and the L3 cache. (Balancing performance characteristics
    in the memory hierarchy is a goal to strive for in order to effectively use the
    different types of memory.)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 标准 DRAM 比 SSD 存储快两个到三个数量级（SSD 存储比硬盘快一个数量级，这也是为什么硬盘通常有自己的基于 DRAM 的缓存）。为了克服这个问题，L3
    缓存和主内存之间通常存在两到三个数量级的差异，以便磁盘和主内存之间的速度差异与主内存和 L3 缓存之间的速度差异相匹配。（在内存层级中平衡性能特征是一个目标，旨在有效利用不同类型的内存。）
- en: We won’t consider the performance of the other memory-hierarchy subsystems in
    this chapter, as they are more or less under programmer control. Because their
    access is not automatic, very little can be said about how frequently a program
    will access them. However, in [Chapter 12](ch12.xhtml#ch12) we’ll look at some
    considerations for these storage devices.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们不考虑其他内存层级子系统的性能，因为它们或多或少由程序员控制。由于它们的访问不是自动的，所以很难说程序将会多频繁地访问它们。然而，在 [第
    12 章](ch12.xhtml#ch12)中，我们将讨论一些关于这些存储设备的考虑因素。
- en: '**11.4 Cache Architecture**'
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**11.4 缓存架构**'
- en: Up to this point, we have treated the cache as a magical place that automatically
    stores data when we need it, perhaps fetching new data as the CPU requires it.
    But how exactly does the cache do this? And what happens when it is full and the
    CPU is requesting additional data that’s not there? In this section, we’ll look
    at the internal cache organization and try to answer these two questions, along
    with a few others.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将缓存视为一个神奇的地方，它在需要时自动存储数据，或许会根据CPU的需求获取新的数据。但缓存究竟是如何做到这一点的呢？当缓存已满而CPU请求额外数据时，会发生什么情况？在这一节中，我们将探讨缓存的内部组织结构，并尝试解答这两个问题以及其他一些问题。
- en: Programs access only a small amount of data at a given time, and a cache that
    is sized accordingly will improve their performance. Unfortunately, the data that
    programs want rarely sits in contiguous memory locations—it’s usually spread out
    all over the address space. Therefore, cache design has to account for the fact
    that the cache must map data objects at widely varying addresses in memory.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 程序在任意时刻只访问少量数据，按照这种方式大小调整的缓存将提高程序的性能。不幸的是，程序需要的数据通常不会在连续的内存位置上——它通常会分布在整个地址空间中。因此，缓存设计必须考虑到缓存需要将位于内存中不同地址的数据对象进行映射。
- en: As noted in the previous section, cache memory is not organized in a single
    group of bytes. Instead, it’s usually organized in blocks of *[cache lines](gloss01.xhtml#gloss01_42)*,
    with each line containing some number of bytes (typically a small power of 2 like
    16, 32, or 64), as shown in [Figure 11-2](ch11.xhtml#ch11fig02).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，缓存内存并不是以单一字节组的方式组织的。相反，它通常以*【缓存行】(gloss01.xhtml#gloss01_42)*的块形式组织，每一行包含一定数量的字节（通常是像16、32或64这样的2的幂次），如[图11-2](ch11.xhtml#ch11fig02)所示。
- en: '![image](../images/11fig02.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig02.jpg)'
- en: '*Figure 11-2: Possible organization of an 8KB cache*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-2：8KB缓存的可能组织方式*'
- en: We can attach a different noncontiguous address to each of the cache lines.
    Cache line 0 might correspond to addresses `$10000` through `$1000F`, and cache
    line 1 might correspond to addresses `$21400` through `$2140F`. Generally, if
    a cache line is *n* bytes long, it will hold *n* bytes from main memory that fall
    on an *n*-byte boundary. In the example in [Figure 11-2](ch11.xhtml#ch11fig02),
    the cache lines are 16 bytes long, so a cache line holds blocks of 16 bytes whose
    addresses fall on 16-byte boundaries in main memory (in other words, the LO 4
    bits of the address of the first byte in the cache line are always 0).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将不同的非连续地址附加到每个缓存行上。缓存行0可能对应地址`$10000`到`$1000F`，缓存行1可能对应地址`$21400`到`$2140F`。通常，如果一个缓存行的长度为*n*字节，它将包含主内存中位于*n*字节边界上的*n*字节数据。在[图11-2](ch11.xhtml#ch11fig02)中的例子里，缓存行的长度为16字节，因此一个缓存行包含16字节的块，这些块在主内存中位于16字节的边界上（换句话说，缓存行中第一个字节地址的低4位总是0）。
- en: 'When the cache controller reads a cache line from a lower level in the memory
    hierarchy, where does the data go in the cache? The answer is determined by the
    caching scheme in use. There are three different caching schemes: direct-mapped
    cache, fully associative cache, and *n*-way set associative cache.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当缓存控制器从内存层次结构中的较低层读取一个缓存行时，数据会被存放到缓存的哪个位置？答案取决于使用的缓存方案。缓存方案有三种不同的类型：直接映射缓存、完全关联缓存和*n*路集合关联缓存。
- en: '***11.4.1 Direct-Mapped Cache***'
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***11.4.1 直接映射缓存***'
- en: In a *direct-mapped cache* (also known as the *one-way set associative cache*),
    a particular block of main memory is always loaded into—mapped to—the exact same
    cache line, determined by a small number of bits in the data block’s memory address.
    [Figure 11-3](ch11.xhtml#ch11fig03) shows how a cache controller could select
    the appropriate cache line for an 8KB cache with 512 16-byte cache lines and a
    32-bit main-memory address.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在*直接映射缓存*（也称为*单路集合关联缓存*）中，主内存中的特定数据块总是被加载到——映射到——相同的缓存行，该缓存行由数据块内存地址中的少量位确定。[图11-3](ch11.xhtml#ch11fig03)展示了缓存控制器如何为一个拥有512个16字节缓存行和32位主内存地址的8KB缓存选择合适的缓存行。
- en: '![image](../images/11fig03.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig03.jpg)'
- en: '*Figure 11-3: Selecting a cache line in a direct-mapped cache*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-3：在直接映射缓存中选择缓存行*'
- en: A cache with 512 cache lines requires 9 bits to select one of the cache lines
    (2⁹ = 512). In this example, bits 4 through 12 of the address determine which
    cache line to use (assuming we number the cache lines from 0 to 511), while bits
    0 through 3 determine the particular byte within the 16-byte cache line.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有512个缓存行的缓存需要9位来选择其中一个缓存行（2⁹ = 512）。在这个例子中，地址的第4到第12位决定使用哪个缓存行（假设我们将缓存行编号为0到511），而地址的第0到第3位则决定16字节缓存行内的具体字节。
- en: The direct-mapped caching scheme is very easy to implement. Extracting 9 (or
    some other number of) bits from the memory address and using the result as an
    index into the array of cache lines is trivial and fast, though this design may
    not make effective use of all the cache memory.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 直接映射缓存方案非常容易实现。从内存地址中提取9个（或其他数量的）位，并将结果用作缓存行数组的索引是非常简单且快速的，尽管这种设计可能无法有效利用所有的缓存内存。
- en: For example, the caching scheme in [Figure 11-3](ch11.xhtml#ch11fig03) maps
    address 0 to cache line 0\. It also maps addresses `$2000` (8KB), `$4000` (16KB),
    `$6000` (24KB), `$8000` (32KB), and every other address that is a multiple of
    8 kilobytes to cache line 0\. This means that if a program is constantly accessing
    data at addresses that are multiples of 8KB and not accessing any other locations,
    the system will use only cache line 0, leaving all the other cache lines unused.
    In this extreme case, the cache is effectively limited to the size of one cache
    line, and each time the CPU requests data at an address that is mapped to, but
    not present in, cache line 0, it has to go down to a lower level in the memory
    hierarchy to access that data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[图11-3](ch11.xhtml#ch11fig03)中的缓存方案将地址0映射到缓存行0。它还将地址`$2000`（8KB）、`$4000`（16KB）、`$6000`（24KB）、`$8000`（32KB）以及所有其他是8KB倍数的地址映射到缓存行0。这意味着，如果一个程序不断访问的是8KB倍数的地址，并且不访问其他位置，系统将仅使用缓存行0，其他所有缓存行将未被使用。在这种极端情况下，缓存实际上被限制为一个缓存行的大小，并且每次CPU请求一个映射到缓存行0但未存在于其中的地址时，必须降级到内存层次结构的较低级别来访问该数据。
- en: '***11.4.2 Fully Associative Cache***'
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***11.4.2 完全关联缓存***'
- en: In a fully associative cache subsystem, the cache controller can place a block
    of bytes in any one of the cache lines present in the cache memory. While this
    is the most flexible cache system, the extra circuitry to achieve full associativity
    is expensive and, worse, can slow down the memory subsystem. Most L1 and L2 caches
    are not fully associative for this reason.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个完全关联的缓存子系统中，缓存控制器可以将字节块放置在缓存内存中任意一个缓存行中。虽然这是最灵活的缓存系统，但实现完全关联所需的额外电路非常昂贵，甚至可能会拖慢内存子系统的速度。由于这个原因，大多数L1和L2缓存并不是完全关联的。
- en: '***11.4.3 n-Way Set Associative Cache***'
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***11.4.3 n路集合关联缓存***'
- en: If a fully associative cache is too complex, too slow, and too expensive to
    implement, but a direct-mapped cache is too inefficient, an *n*-way set associative
    cache is a compromise between the two. In an *n*-way set associative cache, the
    cache is broken up into sets of *n* cache lines. The CPU determines the particular
    set to use based on some subset of the memory address bits, just as in the direct-mapping
    scheme, and the cache controller uses a fully associative mapping algorithm to
    determine which one of the *n* cache lines within the set to use.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果完全关联缓存过于复杂、过于缓慢且过于昂贵，而直接映射缓存又过于低效，那么*n*路集合关联缓存是两者之间的折衷方案。在*n*路集合关联缓存中，缓存被划分为*n*个缓存行的集合。CPU根据内存地址位的某个子集来确定使用哪个集合，就像在直接映射方案中一样，缓存控制器则使用完全关联映射算法来确定在该集合内使用哪一个缓存行。
- en: For example, an 8KB two-way set associative cache subsystem with 16-byte cache
    lines organizes the cache into 256 cache-line sets with two cache lines each.
    Eight bits from the memory address determine which one of these 256 different
    sets will contain the data. Once the cache-line set is determined, the cache controller
    maps the block of bytes to one of the two cache lines within the set (see [Figure
    11-4](ch11.xhtml#ch11fig04)). This means two different memory addresses located
    on 8KB boundaries (addresses having the same value in bits 4 through 11) can both
    appear simultaneously in the cache. However, a conflict will occur if you attempt
    to access a third memory location at an address that is an even multiple of 8KB.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个8KB二路集合关联缓存子系统，具有16字节的缓存线，将缓存组织为256个缓存行集合，每个集合包含两条缓存线。内存地址的八位用于确定这256个不同集合中的哪一个将包含数据。一旦确定了缓存行集合，缓存控制器将数据块映射到该集合中的两个缓存线之一（见[图11-4](ch11.xhtml#ch11fig04)）。这意味着位于8KB边界上的两个不同内存地址（在第4到第11位上的值相同的地址）可以同时出现在缓存中。然而，如果尝试访问一个地址为8KB的偶数倍的第三个内存位置，则会发生冲突。
- en: '![image](../images/11fig04.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig04.jpg)'
- en: '*Figure 11-4: A two-way set associative cache*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-4：二路集合关联缓存*'
- en: A four-way set associative cache puts four associative cache lines in each cache-line
    set. In an 8KB cache like the one in [Figure 11-4](ch11.xhtml#ch11fig04), a four-way
    set associative caching scheme would have 128 cache-line sets with four cache
    lines each. This would allow the cache to maintain up to four different blocks
    of data without a conflict, each of which would map to the same cache line in
    a direct-mapped cache.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 四路集合关联缓存将四条关联缓存线放入每个缓存行集合中。在像[图11-4](ch11.xhtml#ch11fig04)中的8KB缓存中，四路集合关联缓存方案将有128个缓存行集合，每个集合包含四条缓存线。这将使得缓存能够在没有冲突的情况下保持最多四个不同的数据块，每个数据块都会映射到直接映射缓存中的同一缓存行。
- en: A two- or four-way set associative cache is much better than a direct-mapped
    cache and considerably less complex than a fully associative cache. The more cache
    lines we have in each cache-line set, the closer we come to creating a fully associative
    cache, with all the attendant problems of complexity and speed. Most cache designs
    are direct-mapped, two-way set associative, or four-way set associative. The various
    members of the 80x86 family make use of all three.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 二路或四路集合关联缓存比直接映射缓存要好得多，而且比全关联缓存复杂度低得多。每个缓存行集合中的缓存线越多，我们就越接近创建全关联缓存，但也会带来复杂性和速度上的问题。大多数缓存设计都是直接映射、二路集合关联或四路集合关联。80x86系列的各种成员都使用这三种缓存方案。
- en: Matching the Caching Scheme to the Access Type
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 将缓存方案与访问类型匹配
- en: Despite its downsides, the direct-mapped cache is, in fact, very effective for
    data that you access sequentially rather than randomly. Because the CPU typically
    executes machine instructions sequentially, instruction bytes can be stored very
    effectively in a direct-mapped cache. However, programs tend to access data more
    randomly than they access code, so data is better stored in a twoway or four-way
    set associative cache.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管直接映射缓存有其缺点，但对于顺序访问的数据，它实际上非常有效，而非随机访问的数据。因为CPU通常按顺序执行机器指令，所以指令字节可以非常有效地存储在直接映射缓存中。然而，程序访问数据的方式往往比访问代码更为随机，因此数据更适合存储在二路或四路集合关联缓存中。
- en: Because of these different access patterns, many CPU designers use separate
    caches for data and machine instruction bytes—for example, an 8KB data cache and
    an 8KB instruction cache rather than a single 16KB unified cache. The advantage
    of this approach is that each cache can use the caching scheme that‛s most appropriate
    for the particular values it will store. The drawback is that the two caches are
    now each half the size of a unified cache, which may cause more cache misses than
    would occur with a unified cache. The choice of an appropriate cache organization
    is a difficult one, beyond the scope of this book, and can be made only after
    you‛ve analyzed many programs running on the target processor.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些不同的访问模式，许多CPU设计师为数据和机器指令字节使用独立的缓存——例如，使用一个8KB数据缓存和一个8KB指令缓存，而不是单一的16KB统一缓存。这种方法的优点是，每个缓存都可以使用最适合它将存储的特定值的缓存方案。缺点是这两个缓存的大小各为统一缓存的一半，可能导致比统一缓存更多的缓存未命中。选择合适的缓存组织结构是一个困难的问题，超出了本书的范围，只有在分析了目标处理器上运行的多个程序后，才能做出选择。
- en: '***11.4.4 Cache-Line Replacement Policies***'
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***11.4.4 缓存行替换策略***'
- en: Thus far, we’ve answered the question, “Where do we put a block of data in the
    cache?” Now we turn to the equally important question, “What happens if a cache
    line isn’t available when we want to put a block of data in it?”
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经回答了“我们将数据块存放在哪里”的问题。现在，我们转向同样重要的问题：“如果我们想要将数据块放入缓存行，但缓存行不可用，会发生什么？”
- en: For a direct-mapped cache architecture, the cache controller simply replaces
    whatever data was formerly in the cache line with the new data. Any subsequent
    reference to the old data will result in a cache miss, and the cache controller
    will have to restore that old data to the cache by replacing whatever data is
    in that line.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于直接映射缓存架构，缓存控制器只是简单地用新数据替换缓存行中原本的数据。任何后续对旧数据的引用都会导致缓存未命中，缓存控制器将不得不通过替换该行中的任何数据，将旧数据恢复到缓存中。
- en: For a two-way set associative cache, the replacement algorithm is a bit more
    complex. As you’ve seen, whenever the CPU references a memory location, the cache
    controller uses some subset of the address’s bits to determine the cache-line
    set that should be used to store the data. Then, using some fancy circuitry, the
    cache controller determines whether the data is already present in one of the
    two cache lines in the destination set. If the data isn’t there, the CPU has to
    retrieve it from memory, and the controller has to pick one of the two lines to
    use. If either or both of the cache lines are currently unused, the controller
    picks an unused line. However, if both cache lines are currently in use, the controller
    must pick one of them and replace its data with the new data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二路组相联缓存，替换算法要复杂一些。如你所见，每当CPU引用一个内存位置时，缓存控制器会使用地址的某些位来确定应该用来存储数据的缓存行组。然后，借助一些精密电路，缓存控制器判断数据是否已经存在于目标组的两个缓存行中的一个。如果数据不存在，CPU必须从内存中获取它，控制器则需要从两个缓存行中选择一个来使用。如果其中一个或两个缓存行当前未被使用，控制器会选择未使用的缓存行。然而，如果两个缓存行都正在使用，控制器必须从中选择一个，并用新数据替换它的内容。
- en: 'The controller cannot predict the cache line whose data will be referenced
    first and replace the other cache line, but it can use the principle of temporality:
    if a memory location has been referenced recently, it’s likely to be referenced
    again in the very near future. This implies the following corollary: if a memory
    location hasn’t been accessed in a while, it’s likely to be a long time before
    the CPU accesses it again. Therefore, many cache controllers use the *least recently
    used (LRU)* algorithm.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器无法预测哪个缓存行的数据会首先被引用并替换其他缓存行，但它可以使用时效性原理：如果一个内存位置最近被引用过，那么它很可能在不久的将来再次被引用。这意味着以下推论：如果一个内存位置有一段时间没有被访问，它很可能要很长时间后CPU才会再次访问它。因此，许多缓存控制器使用*最近最少使用（LRU）*算法。
- en: An LRU policy is easy to implement in a two-way set associative cache system,
    using a single bit for each set of two cache lines. Whenever the CPU accesses
    one of the two cache lines this bit is set to `0`, and whenever the CPU accesses
    the other cache line, this bit is set to `1`. Then, when a replacement is necessary,
    the cache controller replaces the LRU cache line, indicated by the inverse of
    this bit.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在二路组相联缓存系统中，LRU策略很容易实现，只需为每组两个缓存行使用一个比特位。每当CPU访问其中一个缓存行时，该比特位会被设置为`0`，而每当CPU访问另一个缓存行时，该比特位会被设置为`1`。然后，当需要替换时，缓存控制器将替换LRU缓存行，该行由该比特位的反值指示。
- en: For four-way (and greater) set associative caches, maintaining the LRU information
    is a bit more difficult, which is one reason the circuitry for such caches is
    more complex. Because of the complications LRU might introduce, other replacement
    policies are sometimes used instead. Two of them, *first-in, first-out (FIFO)*
    and *random*, are easier to implement than LRU, but they have their own problems.
    A full discussion of their pros and cons is beyond the scope of this book, but
    you can find more information in a text on computer architecture or operating
    systems.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于四路（或更多路）组相联缓存，维护LRU信息会更加困难，这也是此类缓存电路更复杂的原因之一。由于LRU可能带来的复杂性，其他替换策略有时会代替它使用。其中两种，*先进先出（FIFO）*和*随机*，比LRU更容易实现，但它们也有各自的问题。它们的优缺点的全面讨论超出了本书的范围，但你可以在计算机架构或操作系统的书籍中找到更多信息。
- en: '***11.4.5 Cache Write Policies***'
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***11.4.5 缓存写入策略***'
- en: What happens when the CPU writes data to memory? The simple answer, and the
    one that results in the quickest operation, is that the CPU writes the data to
    the cache. However, what happens when the cache-line data is subsequently replaced
    by data that is read from memory? If the modified contents of the cache line are
    not written to main memory, they will be lost. The next time the CPU attempts
    to access that data, it will reload the cache line with the old data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当 CPU 向内存写入数据时会发生什么？简单的答案是，CPU 将数据写入缓存，这也是最快的操作。然而，当缓存行数据随后被从内存读取的数据替换时会发生什么？如果修改过的缓存行内容没有写入主存，它们将丢失。下次
    CPU 访问该数据时，它将重新加载带有旧数据的缓存行。
- en: 'Clearly, any data written to the cache must ultimately be written to main memory
    as well. Caches use two common write policies: *write-through* and *write-back*.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，任何写入缓存的数据最终都必须写入主存。缓存使用两种常见的写入策略：*写直达*和*写回*。
- en: The write-through policy states that any time data is written to the cache,
    the cache immediately turns around and writes a copy of that cache line to main
    memory. The CPU does not have to halt while the cache controller writes the data
    from cache to main memory. So, unless the CPU needs to access main memory shortly
    after the write occurs, this operation takes place in parallel with the program’s
    execution. Because the write-through policy updates main memory with the new value
    as soon as possible, it is a better policy to use when two different CPUs are
    communicating through shared memory.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 写直达策略规定，每次将数据写入缓存时，缓存会立即将该缓存行的副本写入主存。CPU 在缓存控制器将数据从缓存写入主存时不需要暂停。因此，除非 CPU 在写操作后需要立即访问主存，否则该操作与程序的执行并行进行。由于写直达策略会尽可能快地用新值更新主存，当两个不同的
    CPU 通过共享内存进行通信时，这是一种更好的策略。
- en: Still, a write operation takes some time, during which it’s likely that a CPU
    will want to access main memory, so this policy may not be a high-performance
    solution. Worse, suppose the CPU reads from and writes to the memory location
    several times in succession. With a write-through policy in place, the CPU will
    saturate the bus with cache-line writes, and this will significantly hamper the
    program’s performance.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，写操作需要一定时间，在此期间 CPU 很可能想要访问主存，因此这种策略可能不是高性能的解决方案。更糟糕的是，假设 CPU 连续多次从内存位置读取并写入数据。使用写直达策略时，CPU
    将使总线饱和，进行缓存行写入，这将显著影响程序的性能。
- en: With the write-back policy, writes to the cache are not immediately written
    to main memory; instead, the cache controller updates main memory later. This
    scheme tends to be higher performance, because several writes to the same cache
    line within a short time period won’t generate multiple writes to main memory.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用写回策略时，写入缓存的数据不会立即写入主存；相反，缓存控制器会稍后更新主存。此方案通常性能更高，因为在短时间内对同一缓存行的多次写入不会生成多次写入主存。
- en: To determine which cache lines must be written back to main memory, the cache
    controller usually maintains a *[dirty bit](gloss01.xhtml#gloss01_75)* within
    each one. The cache system sets this bit whenever it writes data to the cache.
    At some later time, the cache controller checks the dirty bit to determine if
    it must write the cache line to memory. For example, whenever the cache controller
    replaces a cache line with other data from memory, it first checks the dirty bit,
    and if that bit is set, the controller writes that cache line to memory before
    going through with the cache-line replacement. Note that this increases the latency
    time during a cache-line replacement. This latency could be reduced if the cache
    controller were able to write dirty cache lines to main memory while no other
    bus access was occurring. Some systems provide this functionality, and others
    do not for economic reasons.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定哪些缓存行必须写回主存，缓存控制器通常会在每个缓存行中维护一个*[脏位](gloss01.xhtml#gloss01_75)*。每当缓存控制器将数据写入缓存时，系统会设置此位。在稍后的时间里，缓存控制器检查脏位以判断是否需要将缓存行写入内存。例如，每当缓存控制器用内存中的其他数据替换缓存行时，它首先检查脏位，如果该位被设置，控制器将在进行缓存行替换之前将该缓存行写入内存。请注意，这会增加缓存行替换期间的延迟时间。如果缓存控制器能够在没有其他总线访问的情况下将脏缓存行写入主存，延迟时间可能会减少。有些系统提供此功能，而有些系统由于经济原因不提供此功能。
- en: '***11.4.6 Cache Use and Software***'
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***11.4.6 缓存使用与软件***'
- en: A cache subsystem is not a panacea for slow memory access, and can in fact actually
    *hurt* an application’s performance. In order for a cache system to be effective,
    software must be written with the cache behavior in mind. Particularly, good software
    must exhibit either spatial or temporal locality of reference—which the software
    designer accomplishes by placing oft-used variables adjacent in memory so they
    tend to fall into the same cache lines—and avoid data structures and access patterns
    that force the cache to frequently replace cache lines.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存子系统并不是解决内存访问慢的灵丹妙药，实际上它可能会*损害*应用程序的性能。为了让缓存系统有效，软件必须在设计时考虑缓存行为。特别地，好的软件必须表现出空间或时间局部性——软件设计师通过将常用变量放置在内存相邻位置，以确保它们尽可能地落入同一缓存行——并避免使用会导致缓存频繁替换缓存行的数据结构和访问模式。
- en: Suppose that an application accesses data at several different addresses that
    the cache controller would map to the same cache line. With each access, the cache
    controller must read in a new cache line (possibly flushing the old one back to
    memory if it is dirty). As a result, each memory access incurs the latency cost
    of retrieving a cache line from main memory. This degenerate case, known as *[thrashing](gloss01.xhtml#gloss01_246)*,
    can slow down the program by one to two orders of magnitude, depending on the
    speed of main memory and the size of a cache line. We’ll take another look at
    thrashing a little later in this chapter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个应用程序访问多个不同地址的数据，这些地址会被缓存控制器映射到同一缓存行。每次访问时，缓存控制器必须读取一个新的缓存行（如果旧缓存行有脏数据，可能需要将其刷新回内存）。因此，每次内存访问都会带来从主内存获取缓存行的延迟成本。这个退化情况被称为*
    [thrashing](gloss01.xhtml#gloss01_246) *，它可能会使程序变慢一个到两个数量级，具体取决于主内存的速度和缓存行的大小。我们将在本章稍后再次讨论thrashing。
- en: A benefit of the cache subsystem on modern 80x86 CPUs is that it automatically
    handles many misaligned data references. Remember, there’s a performance penalty
    for accessing words or double-word objects at an address that is not an even multiple
    of that object’s size. By providing some fancy logic, Intel’s designers have eliminated
    this penalty as long as the data object is located completely within a cache line.
    However, if the object crosses a cache line, the penalty still applies.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现代80x86 CPU上缓存子系统的一个好处是它可以自动处理许多不对齐的数据引用。记住，如果访问的单词或双字对象的地址不是该对象大小的偶数倍，会产生性能惩罚。通过提供一些复杂的逻辑，Intel的设计师消除了这个惩罚，只要数据对象完全位于缓存行内。然而，如果该对象跨越了一个缓存行，惩罚仍然存在。
- en: '**11.5 NUMA and Peripheral Devices**'
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**11.5 NUMA 和外部设备**'
- en: Although most of the RAM in a system is based on high-speed DRAM interfaced
    directly with the processor’s bus, not all memory is connected to the CPU this
    way. Sometimes a large block of RAM is part of a peripheral device—for example,
    a video card, network interface card, or USB controller—and you communicate with
    that device by writing data to its RAM. Unfortunately, the access time to the
    RAM on these peripheral devices is often much slower than the access time to main
    memory. In this section, we’ll use the video card as an example, although NUMA
    performance applies to other devices and memory technologies as well.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管系统中的大多数RAM是基于与处理器总线直接连接的高速DRAM，但并非所有内存都以这种方式连接到CPU。有时，一大块RAM是外部设备的一部分——例如，显卡、网络接口卡或USB控制器——你通过将数据写入该设备的RAM来与设备通信。不幸的是，这些外部设备的RAM访问时间通常比主内存的访问时间要慢得多。在本节中，我们将以显卡为例，尽管NUMA性能同样适用于其他设备和内存技术。
- en: A typical video card interfaces with a CPU through a *Peripheral Component Interconnect
    Express (PCI-e)* bus inside the computer system. Though 16-lane PCI-e buses are
    fast, memory access is still much faster. Game programmers long ago discovered
    that manipulating a copy of the screen data in main memory and writing that data
    to the video card RAM only periodically (typically once every 1/60 of a second
    during video retrace, to avoid flicker) is much faster than writing directly to
    the video card every time you want to make a change.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一块典型的显卡通过计算机系统内部的*外部组件互联高速总线（PCI-e）*与CPU接口。尽管16通道PCI-e总线非常快速，但内存访问仍然要快得多。游戏程序员早就发现，将屏幕数据的副本放在主内存中处理，并只在每次视频回扫时（通常是每秒1/60次，以避免闪烁）将这些数据写入显卡的RAM，比每次想要更改时直接写入显卡要快得多。
- en: Caches and the virtual memory subsystem operate transparently (that is, applications
    are unaware of the underlying operations taking place), but NUMA memory does not,
    so programs that write to NUMA devices must minimize the number of accesses whenever
    possible (for example, by using an offscreen bitmap to hold temporary results).
    If you’re actually storing and retrieving data on a NUMA device, like a flash
    memory card, you must explicitly cache the data yourself.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存和虚拟内存子系统是透明操作的（也就是说，应用程序无法察觉底层的操作），但NUMA内存不是，所以写入NUMA设备的程序必须尽可能最小化访问次数（例如，通过使用离屏位图来保存临时结果）。如果你实际在NUMA设备上存储和检索数据，例如在闪存卡上，你必须显式地自己缓存数据。
- en: '**11.6 Virtual Memory, Memory Protection, and Paging**'
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**11.6 虚拟内存、内存保护与分页**'
- en: 'In a modern operating system such as Android, iOS, Linux, macOS, or Windows,
    it is very common to have several different programs running concurrently in memory.
    This presents several problems:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代操作系统中，如Android、iOS、Linux、macOS或Windows，多个不同的程序通常会同时在内存中运行。这会带来几个问题：
- en: How do you keep the programs from interfering with one another’s memory?
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何防止程序互相干扰彼此的内存呢？
- en: If two programs both expect to load a value into memory at address `$1000`,
    how can you load both values and execute both programs at the same time?
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个程序都希望将一个值加载到内存中的地址`$1000`，那么如何才能同时加载这两个值并同时执行这两个程序呢？
- en: What happens if the computer has 64GB of memory, and you decide to load and
    execute three different applications, two of which require 32GB and one that requires
    16GB (not to mention the memory that the OS requires for its own purposes)?
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果计算机有64GB内存，并且你决定加载并执行三个不同的应用程序，其中两个需要32GB，一个需要16GB（更别提操作系统为自身目的所需的内存）会发生什么呢？
- en: The answers to all these questions lie in the virtual memory subsystem that
    modern processors support.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题的答案都在现代处理器支持的虚拟内存子系统中。
- en: Virtual memory on CPUs such as the 80x86 gives each process its own 32-bit address
    space.^([2](footnotes.xhtml#fn11_2a)) This means that address `$1000` in one program
    is physically different from address `$1000` in a separate program. The CPU achieves
    this sleight of hand by mapping the *virtual addresses* used by programs to different
    *physical addresses* in actual memory. The virtual address and the physical address
    don’t have to be the same, and usually they aren’t. For example, program 1’s virtual
    address `$1000` might actually correspond to physical address `$215000`, while
    program 2’s virtual address `$1000` might correspond to physical memory address
    `$300000`. The CPU accomplishes this using *paging*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在像80x86这样的CPU上，虚拟内存为每个进程提供其自己的32位地址空间^([2](footnotes.xhtml#fn11_2a))。这意味着一个程序中的地址`$1000`在物理上与另一个程序中的地址`$1000`不同。CPU通过将程序使用的*虚拟地址*映射到实际内存中的不同*物理地址*来实现这种“魔术”。虚拟地址和物理地址不必相同，而且通常它们不是。例如，程序1的虚拟地址`$1000`可能实际上对应物理地址`$215000`，而程序2的虚拟地址`$1000`可能对应物理内存地址`$300000`。CPU通过*分页*实现这一点。
- en: The concept behind paging is quite simple. First, you break up memory into blocks
    of bytes called *pages*. A page in main memory is comparable to a cache line in
    a cache subsystem, although pages are usually much larger than cache lines. For
    example, the 32-bit 80x86 CPUs use a page size of 4,096 bytes; 64-bit variants
    allow larger page sizes.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 分页的概念非常简单。首先，你将内存分割成叫做*页面*的字节块。主内存中的一个页面可以与缓存子系统中的缓存行相比较，尽管页面通常比缓存行要大得多。例如，32位的80x86
    CPU使用的页面大小为4,096字节；而64位变种允许更大的页面大小。
- en: For each page, you use a lookup table to map the HO bits of a virtual address
    to the HO bits of the physical address in memory, and you use the LO bits of the
    virtual address as an index into that page. For example, with a 4,096-byte page,
    you’d use the LO 12 bits of the virtual address as the offset (0..4095) within
    the page, and the upper 20 bits as an index into a lookup table that returns the
    actual upper 20 bits of the physical address (see [Figure 11-5](ch11.xhtml#ch11fig05)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个页面，你使用查找表将虚拟地址的高位映射到内存中物理地址的高位，并将虚拟地址的低位作为该页面的索引。例如，使用4,096字节的页面时，你将虚拟地址的低12位用作页面内的偏移量（0..4095），而将高20位作为查找表的索引，该表返回物理地址的实际高20位（参见[图11-5](ch11.xhtml#ch11fig05)）。
- en: '![image](../images/11fig05.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig05.jpg)'
- en: '*Figure 11-5: Translating a virtual address to a physical address*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-5：将虚拟地址转换为物理地址*'
- en: A 20-bit index into the page table would require over one million entries in
    the page table. If each entry is a 32-bit value, the page table would be 4MB long—larger
    than many of the programs that would run in memory! However, by using a multilevel
    page table, you can easily create a page table for most small programs that is
    only 8KB long. The details are unimportant here. Just rest assured that you don’t
    need a 4MB page table unless your program consumes the entire 4GB address space.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 20位索引进入页表会在页表中需要超过一百万个条目。如果每个条目是32位的，那么页表的大小将是4MB——比许多运行在内存中的程序还要大！然而，通过使用多级页表，你可以轻松地为大多数小程序创建一个仅有8KB长的页表。具体细节在这里不重要。只需要放心，除非你的程序使用了整个4GB的地址空间，否则你不需要一个4MB的页表。
- en: 'If you study [Figure 11-5](ch11.xhtml#ch11fig05) for a few moments, you’ll
    probably discover one problem with using a page table—it requires two separate
    memory accesses in order to retrieve the data stored at a single physical address
    in memory: one to fetch a value from the page table, and one to read from or write
    to the desired memory location. To prevent cluttering the data or instruction
    cache with page-table entries, which increases the number of cache misses for
    data and instruction requests, the page table uses its own cache, known as the
    *translation lookaside buffer (TLB)*. This cache typically has 64 to 512 entries
    on modern Intel processors—enough to handle a fair amount of memory without a
    miss. Because a program typically works with less data than this at any given
    time, most page-table accesses come from the cache rather than main memory.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你稍微研究一下[图11-5](ch11.xhtml#ch11fig05)，你可能会发现使用页表的一个问题——它需要两次独立的内存访问才能检索存储在内存中单一物理地址的数据：一次是从页表中获取值，另一次是从所需的内存位置读取或写入数据。为了防止将页表条目堆积在数据或指令缓存中，从而增加数据和指令请求的缓存未命中次数，页表使用了它自己的缓存，称为*转换旁路缓冲区（TLB）*。这个缓存通常在现代Intel处理器中有64到512个条目——足够处理相当数量的内存而不会发生未命中。因为程序通常在任何给定时刻处理的数据少于这个数量，所以大多数页表访问来自缓存，而非主内存。
- en: 'As noted, each entry in the page table contains 32 bits, even though the system
    really only needs 20 bits to remap each virtual address to a physical address.
    Intel, on the 80x86, uses some of the remaining 12 bits to provide memory protection
    information:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，页表中的每个条目包含32位，尽管系统实际上只需要20位来将每个虚拟地址映射到物理地址。Intel在80x86上使用剩余的12位来提供内存保护信息：
- en: One bit marks whether a page is read/write or read-only.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一位标志位表示页面是可读写还是只读。
- en: One bit determines whether you can execute code on that page.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一位标志位决定你是否可以在该页面上执行代码。
- en: A number of bits determine whether the application can access that page or if
    only the operating system can do so.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 若干位决定应用程序是否可以访问该页面，还是仅操作系统可以访问。
- en: A number of bits determine if the CPU has written to the page but hasn’t yet
    written to the physical memory address corresponding to it (that is, whether the
    page is “dirty” or not, and whether the CPU has accessed the page recently).
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 若干位决定CPU是否已写入页面，但尚未写入对应的物理内存地址（即页面是否“脏”以及CPU是否最近访问了该页面）。
- en: One bit determines whether the page is actually present in physical memory or
    if it exists on secondary storage somewhere.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一位标志位决定页面是否实际存在于物理内存中，还是存在于某个地方的二级存储中。
- en: Your applications do not have access to the page table (reading and writing
    the page table is the operating system’s responsibility), so they cannot modify
    these bits. However, some operating systems provide functions you can call if
    you want to change certain bits in the page table (for example, Windows allows
    you to set a page to read-only).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你的应用程序无法访问页表（读取和写入页表是操作系统的责任），因此它们不能修改这些位。然而，一些操作系统提供了你可以调用的函数，若你想更改页表中的某些位（例如，Windows允许你将一个页面设置为只读）。
- en: Beyond remapping memory so multiple programs can coexist in main memory, paging
    also provides a mechanism whereby the operating system can move infrequently used
    pages to secondary storage. Locality of reference applies not only to cache lines
    but to pages in main memory as well. At any given time, a program will access
    only a small percentage of the pages in main memory that contain data and instruction
    bytes; this set of pages is known as the *working set*. Although the working set
    varies slowly over time, for small periods of time it remains constant. Therefore,
    there’s little need for the remainder of the program to consume valuable main-memory
    storage that some other process could be using. If the operating system can save
    the currently unused pages to disk, the main memory they would consume is available
    for other programs that need it.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 除了重新映射内存使多个程序能够在主内存中共存外，分页还提供了一种机制，使操作系统能够将不常用的页面移动到辅助存储。引用局部性不仅适用于缓存行，也适用于主内存中的页面。在任何给定时刻，程序只会访问包含数据和指令字节的主内存中少量的页面；这组页面被称为*工作集*。尽管工作集随时间变化缓慢，但在较短的时间内，它保持不变。因此，程序的其余部分不需要占用其他进程可能需要的宝贵主内存存储。如果操作系统能够将当前未使用的页面保存到磁盘，那么它们将释放的主内存空间可以供其他需要的程序使用。
- en: Of course, the problem with moving data out of main memory is that eventually
    the program might actually need it. If you attempt to access a page of memory,
    and the page-table bit tells the memory management unit (MMU) that the page isn’t
    present in main memory, the CPU interrupts the program and passes control to the
    operating system. The operating system reads the corresponding page of data from
    the disk drive and copies it to some available page in main memory. This process
    is nearly identical to the process used by a fully associative cache subsystem,
    except that accessing the disk is much slower than accessing main memory. In fact,
    you can think of main memory as a fully associative write-back cache with 4,096-byte
    cache lines, which caches the data that is stored on the disk drive. Placement
    and replacement policies and other behaviors are very similar for caches and main
    memory.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，将数据从主内存移出的一个问题是，程序最终可能确实需要这些数据。如果你尝试访问一个内存页，而页表位告诉内存管理单元（MMU）该页不在主内存中，CPU将中断程序并将控制权交给操作系统。操作系统从磁盘驱动器读取相应的页面数据，并将其复制到主内存中的某个空闲页面。这一过程几乎与完全关联缓存子系统使用的过程相同，唯一不同的是访问磁盘比访问主内存要慢得多。事实上，你可以将主内存视为一个具有4,096字节缓存行的完全关联写回缓存，它缓存了存储在磁盘驱动器上的数据。缓存和主内存的置换策略及其他行为非常相似。
- en: '**NOTE**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*For more information on how the operating system swaps pages between main
    memory and secondary storage, consult a textbook on operating system design.*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*有关操作系统如何在主内存和辅助存储之间交换页面的更多信息，请参考操作系统设计的教科书。*'
- en: Because each program has a separate page table, and programs themselves don’t
    have access to the page tables, programs cannot interfere with one another’s operation.
    That is, a program cannot change its page tables in order to access data found
    in another process’s *[address space](gloss01.xhtml#gloss01_7)*. If your program
    crashes by overwriting itself, it cannot crash other programs at the same time.
    This is a big benefit of a paging memory system.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 因为每个程序都有一个独立的页表，而且程序本身无法访问这些页表，所以程序之间无法相互干扰。也就是说，一个程序不能修改其页表以访问另一个进程的*［地址空间］(gloss01.xhtml#gloss01_7)*中的数据。如果你的程序通过覆盖自身而崩溃，它不会同时崩溃其他程序。这是分页内存系统的一个重要优点。
- en: If two programs want to cooperate and share data, they can do so by placing
    that data in a memory area that they share. All they have to do is tell the operating
    system that they want to share some pages of memory. The operating system returns
    to each process a pointer to some segment of memory whose physical address is
    the same for both processes. Under Windows, you can achieve this by using *[memory-mapped
    files](gloss01.xhtml#gloss01_153)*; see the operating system documentation for
    more details. macOS and Linux also support memory-mapped files as well as some
    special shared-memory operations; again, see the OS documentation for more details.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个程序想要协作并共享数据，它们可以通过将数据放入它们共享的内存区域来实现。它们只需要告诉操作系统它们希望共享某些内存页。操作系统将返回每个进程一个指向某个内存段的指针，该段的物理地址对两个进程来说是相同的。在
    Windows 系统中，你可以通过使用*[内存映射文件](gloss01.xhtml#gloss01_153)*来实现；更多细节请参见操作系统文档。macOS
    和 Linux 也支持内存映射文件以及一些特殊的共享内存操作；同样，详情请参见操作系统文档。
- en: 'Although this discussion applies specifically to the 80x86 CPU, multilevel
    paging systems are common on other CPUs as well. Page sizes tend to vary from
    about 1KB to 4MB, depending on the CPU. For CPUs that support an address space
    larger than 4GB, some CPUs use an *inverted page table* or a *three-level page
    table*. Although the details are beyond the scope of this chapter, the basic principle
    remains the same: the CPU moves data between main memory and the disk in order
    to keep oft-accessed data in main memory as much of the time as possible. These
    other page-table schemes are good at reducing the size of the page table when
    an application uses only a fraction of the available memory space.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本讨论特别适用于 80x86 CPU，但多级分页系统在其他 CPU 上也很常见。页面大小通常在约 1KB 到 4MB 之间，具体取决于 CPU。对于支持大于
    4GB 地址空间的 CPU，一些 CPU 使用*反向页表*或*三层页表*。尽管这些细节超出了本章的讨论范围，但基本原理是相同的：CPU 在主内存和磁盘之间移动数据，以尽量将经常访问的数据保持在主内存中。这些其他的页表方案在应用程序仅使用部分可用内存空间时，能够有效减少页表的大小。
- en: Thrashing
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 抖动
- en: 'Thrashing is a degenerate case that can cause the overall system performance
    to drop to the speed of a lower level in the memory hierarchy, like main memory
    or, worse yet, the disk drive. There are two primary causes of thrashing:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 抖动是一个退化的情况，可能会导致整个系统性能下降到内存层次结构中较低层次的速度，如主内存，或者更糟糕的是，磁盘驱动器。抖动的主要原因有两个：
- en: Insufficient memory at a given level in the memory hierarchy to properly contain
    the programs’ working sets of cache lines or pages
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在内存层次结构中的某个级别上，内存不足以适当地容纳程序的工作集缓存行或页面
- en: A program that does not exhibit locality of reference
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不具备引用局部性的程序
- en: If there is insufficient memory to hold a working set of pages or cache lines,
    the memory system will constantly be replacing one block of data in the cache
    or main memory with another block of data from main memory or the disk. As a result,
    the system winds up operating at the speed of the slower memory in the memory
    hierarchy. A common example of thrashing occurs with virtual memory. A user may
    have several applications running at the same time, and the sum total of the memory
    required by these programs’ working sets is greater than all of the physical memory
    available to the programs. As a result, when the operating system switches between
    the applications it has to copy each application’s data, and possibly program
    instructions, to and from disk. Because switching between programs is often much
    faster than retrieving data from the disk, this slows the programs down tremendously.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果内存不足以容纳工作集页面或缓存行，内存系统将不断地用主内存或磁盘中的另一个数据块替换缓存或主内存中的一个数据块。结果，系统的运行速度就会降到内存层次结构中较慢的内存的速度。虚拟内存中常见的抖动现象就是如此。例如，一个用户可能同时运行多个应用程序，这些程序所需的内存总和大于可用的物理内存。因此，当操作系统在应用程序之间切换时，它必须将每个应用程序的数据，可能还有程序指令，从磁盘中读入或写出。由于在程序之间切换通常比从磁盘中获取数据要快得多，这会使程序运行速度大幅下降。
- en: As already discussed, if the program does not exhibit locality of reference
    and the lower memory subsystems are not fully associative, thrashing can occur
    even if there is free memory at the current level in the memory hierarchy. To
    revisit our earlier example, suppose an 8KB L1 caching system uses a direct-mapped
    cache with 512 16-byte cache lines. If a program references data objects 8KB apart
    on *every* access, the system will have to replace the same line in the cache
    over and over again with the data from main memory. This occurs even though the
    other 511 cache lines are currently unused.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，如果程序没有展示引用局部性，并且低层内存子系统不是完全关联的，即使当前内存层级中有空闲内存，抖动也可能发生。让我们回顾之前的例子，假设一个 8KB
    的 L1 缓存系统使用一个直接映射的缓存，拥有 512 个 16 字节的缓存行。如果一个程序在*每次*访问时引用间隔为 8KB 的数据对象，那么系统将不得不反复用主内存中的数据替换缓存中的相同缓存行。即使其他
    511 个缓存行当前未被使用，这种情况仍然会发生。
- en: To reduce thrashing when insufficient memory is the problem, you can simply
    add memory. If that’s not an option, you can try to run fewer processes concurrently
    or modify your program so that it references less memory over a given period.
    To reduce thrashing when locality of reference is the culprit, you should restructure
    your program and its data structures so its memory references are physically closer.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当内存不足导致抖动（thrashing）时，你可以通过增加内存来解决问题。如果无法增加内存，可以尝试减少并发运行的进程数量，或者修改程序，使其在给定时间内引用更少的内存。若是引用局部性（locality
    of reference）导致了抖动，则应重构程序及其数据结构，使其内存引用物理上更加接近。
- en: '**11.7 Writing Software That Is Cognizant of the Memory Hierarchy**'
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**11.7 编写能够理解内存层级的软件**'
- en: Software that is aware of memory performance behavior can run much faster than
    software that is not. Although a system’s caching and paging facilities may perform
    reasonably well for typical programs, it’s easy to write software that would run
    faster even if the caching system were not present. The best software is written
    to take maximum advantage of the memory hierarchy.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 意识到内存性能行为的软件可以比那些没有意识到的运行得更快。虽然一个系统的缓存和分页机制对于典型程序可能表现得相当好，但其实很容易编写出即使没有缓存系统也能运行得更快的软件。最好的软件是能够最大化利用内存层级结构的。
- en: 'A classic example of a bad design is the following loop, which initializes
    a two-dimensional array of integer values:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的坏设计案例是以下循环，它初始化了一个二维整数数组：
- en: '[PRE0]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Believe it or not, that code runs much slower on a modern CPU than the following
    sequence:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 信不信由你，以下代码在现代 CPU 上运行的速度比前面的代码要慢得多：
- en: '[PRE1]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The only difference between the two code sequences is that the `i` and `j`
    indices are swapped when accessing elements of the array. This minor modification
    can be responsible for a one or two order of magnitude difference in their respective
    runtimes! To understand why, remember that the C programming language uses row-major
    ordering for two-dimensional arrays in memory. That means the second code sequence
    accesses sequential locations in memory, exhibiting spatial locality of reference.
    The first code sequence, however, accesses array elements in the following order:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这两段代码的唯一不同之处在于，在访问数组元素时，`i` 和 `j` 索引被交换了。这个小修改可能导致它们的运行时间相差一到两个数量级！要理解为什么，记住
    C 语言对二维数组在内存中的排列使用的是行主序（row-major ordering）。这意味着第二段代码在内存中访问的是顺序位置，展示了空间局部性（spatial
    locality of reference）。而第一段代码则按以下顺序访问数组元素：
- en: '[PRE2]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If integers are 4 bytes each, then this sequence will access the double-word
    values at offsets 0; 1,024; 2,048; 3,072; and so on, from the base address of
    the array, which are distinctly *not* sequential. Most likely, this code will
    load only *n* integers into an *n*-way set associative cache and then immediately
    cause thrashing thereafter, as each subsequent array element has to be copied
    from the cache into main memory to prevent that data from being overwritten.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果整数占用 4 字节，那么该序列将从数组的基地址开始，分别访问偏移量为 0、1,024、2,048、3,072 等的双字（double-word）值，这些值显然是*非*顺序的。很可能，这段代码会将
    *n* 个整数加载到一个 *n*-路集合关联缓存中，然后立刻导致抖动，因为每个随后的数组元素都必须从缓存复制到主内存中，以防数据被覆盖。
- en: The second code sequence does not exhibit thrashing. Assuming 64-byte cache
    lines, the second code sequence will store 16 integer values into the same cache
    line before having to load another cache line from main memory, replacing an existing
    one. As a result, this second code sequence spreads out the cost of retrieving
    the cache line from memory over 16 memory accesses rather than over a single access,
    as occurs with the first code sequence.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 第二段代码序列没有出现抖动。假设使用64字节的缓存行，第二段代码序列将在加载另一个缓存行并替换已有缓存行之前，将16个整数值存储到同一个缓存行中。因此，第二段代码序列将内存中缓存行的检索成本分摊到16次内存访问中，而不是像第一段代码序列那样仅通过一次访问来完成。
- en: In addition to accessing variables sequentially in memory, there are several
    other variable declaration tricks you can use to maximize the performance of the
    memory hierarchy. First, declare together all variables you use within a common
    code sequence. In most languages, this will allocate storage for the variables
    in physically adjacent memory locations, thus supporting spatial locality as well
    as temporal locality. Second, use local (automatic) variables, because most languages
    allocate local storage on the stack and, as the system references the stack frequently,
    variables on the stack tend to be in the cache. Third, declare your scalar variables
    together, and separately from your array and record variables. Access to any one
    of several adjacent scalar variables generally forces the system to load all of
    the adjacent objects into the cache.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 除了按顺序访问内存中的变量外，你还可以使用几种其他的变量声明技巧来最大化内存层次结构的性能。首先，将在公共代码序列中使用的所有变量一起声明。在大多数语言中，这将为变量分配物理上相邻的内存位置，从而支持空间局部性和时间局部性。其次，使用局部（自动）变量，因为大多数语言将局部存储分配到栈上，而系统频繁访问栈，栈上的变量往往会被缓存。第三，将标量变量一起声明，并与数组和记录变量分开。访问任何一个相邻的标量变量通常会迫使系统将所有相邻的对象加载到缓存中。
- en: In general, study the memory access patterns your program exhibits and adjust
    your application accordingly. You can spend hours rewriting your code in hand-optimized
    assembly language trying to achieve a 10 percent performance improvement, but
    if you instead modify the way your program accesses memory, it’s not unheard of
    to see an order of magnitude improvement in performance.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，研究你程序表现出的内存访问模式，并相应地调整你的应用程序。你可以花费数小时用手动优化的汇编语言重写代码，试图提高10%的性能，但如果你改进程序访问内存的方式，通常会看到性能提升一个数量级的情况。
- en: '**11.8 Runtime Memory Organization**'
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**11.8 运行时内存组织**'
- en: Operating systems like macOS, Linux, or Windows put different types of data
    into different sections (or *segments*) of main memory. Although it’s possible
    to control the memory organization by running a linker and specifying various
    parameters, by default Windows loads a typical program into memory using the organization
    shown in [Figure 11-6](ch11.xhtml#ch11fig06) (macOS and Linux are similar, though
    they rearrange some of the sections).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 像macOS、Linux或Windows这样的操作系统将不同类型的数据放入主内存的不同区段（或*段*）。虽然可以通过运行链接器并指定各种参数来控制内存组织，但默认情况下，Windows会按照[图11-6](ch11.xhtml#ch11fig06)中所示的组织方式将典型程序加载到内存中（macOS和Linux也类似，尽管它们重新排列了一些区段）。
- en: '![image](../images/11fig06.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/11fig06.jpg)'
- en: '*Figure 11-6: Typical Windows runtime memory organization*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-6：典型的Windows运行时内存组织*'
- en: The operating system reserves the lowest memory addresses, and your application
    generally cannot access data (or execute instructions) at these addresses. One
    reason the OS reserves this space is to help detect `NULL` pointer references.
    Programmers often initialize a pointer with `NULL` (`0`) to indicate that it is
    not valid. Should you attempt to access memory location `0` under such an OS,
    it will generate a *general protection fault* to indicate that you’ve accessed
    a memory location that doesn’t contain valid data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统保留了最低的内存地址，你的应用程序通常不能访问这些地址的数据（或执行这些地址的指令）。操作系统保留这些空间的一个原因是帮助检测`NULL`指针引用。程序员通常会将指针初始化为`NULL`（`0`），表示指针无效。如果你尝试在这样的操作系统中访问内存位置`0`，它会生成*通用保护错误*，以表明你访问了一个不包含有效数据的内存位置。
- en: 'The remaining seven sections of memory hold different types of data associated
    with your program:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的七个内存区段存储与你程序相关的不同类型的数据：
- en: The code section holds the program’s machine instructions.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码段存储程序的机器指令。
- en: The constant section contains compiler-generated read-only data.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常量区包含编译器生成的只读数据。
- en: The read-only data section holds user-defined data that can only be read, never
    written.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只读数据段保存用户定义的只能读取、不能写入的数据。
- en: The static section stores user-defined, initialized, static variables.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态区保存用户定义的已初始化静态变量。
- en: The storage, or BSS, section holds user-defined uninitialized variables.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储区或BSS区保存用户定义的未初始化变量。
- en: The stack section maintains local variables and other temporary data.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 栈区维护着局部变量和其他临时数据。
- en: The heap section maintains dynamic variables.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆区维护动态变量。
- en: '**NOTE**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Often, a compiler will combine the code, constant, and read-only data sections
    because they all contain read-only data.*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*通常，编译器会将代码、常量和只读数据段合并在一起，因为它们都包含只读数据。*'
- en: Most of the time, a given application can live with the default layouts chosen
    for these sections by the compiler and linker/loader. In some cases, however,
    knowing the memory layout can help you develop shorter programs. For example,
    combining the code, constants, and read-only data sections into a single read-only
    section can save padding space that the compiler/linker might otherwise place
    between them. Although these savings are probably insignificant for large applications,
    they can have a big impact on the size of a small program.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，给定的应用程序可以使用编译器和链接器/加载器为这些区段选择的默认布局。然而，在某些情况下，了解内存布局可以帮助你开发更短的程序。例如，将代码、常量和只读数据区段合并为一个只读区段，可以节省编译器/链接器可能在它们之间插入的填充空间。尽管这些节省对大型应用程序来说可能微不足道，但对小程序的大小影响却可能非常大。
- en: The following sections discuss each of these memory areas in detail.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节将详细讨论这些内存区域。
- en: '***11.8.1 Static and Dynamic Objects, Binding, and Lifetime***'
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***11.8.1 静态与动态对象、绑定与生命周期***'
- en: 'Before exploring the memory organization of a typical program, we need to define
    a few terms: binding, lifetime, static, and dynamic.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨典型程序的内存组织结构之前，我们需要定义一些术语：绑定、生命周期、静态和动态。
- en: '*[Binding](gloss01.xhtml#gloss01_30)* is the process of associating an attribute
    with an object. For example, when you assign a value to a variable, the value
    is *bound* to that variable at the point of the assignment. This bond remains
    until you bind some other value to the variable (via another assignment operation).
    Likewise, if you allocate memory for a variable while the program is running,
    the variable is bound to the address at that point. They remain bound until you
    associate a different address with the variable. Binding needn’t occur at runtime.
    For example, values are bound to constant objects during compilation, and these
    bonds cannot change while the program is running.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*[绑定](gloss01.xhtml#gloss01_30)* 是将属性与对象关联的过程。例如，当你给一个变量赋值时，这个值就被*绑定*到该变量上，直到你将另一个值赋给该变量为止。类似地，如果你在程序运行时为一个变量分配内存，这个变量就绑定到那个地址，直到你将不同的地址与该变量关联。绑定不一定发生在运行时。例如，在编译时，值会被绑定到常量对象上，而这些绑定在程序运行时不能改变。'
- en: The *[lifetime](gloss01.xhtml#gloss01_136)* of an attribute extends from the
    point when you first bind that attribute to an object to the point when you break
    that bond, perhaps by binding a different attribute to the object. For example,
    the lifetime of a variable is from the time you first associate memory with the
    variable to the moment you deallocate that variable’s storage.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*[生命周期](gloss01.xhtml#gloss01_136)* 是指一个属性从你首次将该属性绑定到对象开始，到你断开这个绑定（例如通过将另一个属性绑定到该对象）为止的过程。例如，一个变量的生命周期是从你首次为该变量分配内存开始，到你释放该变量的存储空间为止。'
- en: '*Static* objects are those that have an attribute bound to them prior to the
    application’s execution (usually during compilation or during the linking phase,
    though it is possible to bind values even earlier). Constants are good examples
    of static objects; they have the same value bound to them throughout program execution.
    Global (program-level) variables in programming languages like Pascal, C/C++,
    and Ada are also examples of static objects in that they have the same address
    bound to them throughout the program’s lifetime. The lifetime of a static object,
    therefore, extends from the point at which the program first begins execution
    to the point when the application terminates.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*静态*对象是指在应用程序执行之前就绑定了某个属性的对象（通常是在编译时或链接阶段，尽管也有可能在更早的时候就绑定值）。常量就是静态对象的一个很好的例子；它们在整个程序执行过程中绑定着相同的值。像
    Pascal、C/C++ 和 Ada 等编程语言中的全局（程序级别）变量也是静态对象的例子，因为它们在程序生命周期内始终绑定着相同的地址。因此，静态对象的生命周期从程序开始执行的时刻起，直到应用程序终止为止。'
- en: Associated with static binding is the notion of identifier *[scope](gloss01.xhtml#gloss01_225)*—the
    section of the program where the identifier’s name is bound to the object. As
    names exist only during compilation, scope qualifies as a static attribute in
    compiled languages. (In interpretive languages, where the interpreter maintains
    the identifier names during program execution, scope can be a nonstatic attribute.)
    The scope of a local variable is generally limited to the procedure or function
    in which you declare it (or to any nested procedure or function declarations in
    block structured languages like Pascal or Ada), and the name is not visible outside
    the subroutine. In fact, it’s possible to reuse an identifier’s name in a different
    scope (that is, in a different function or procedure). In that case, the second
    occurrence of the identifier will be bound to a different object than its first
    occurrence.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与静态绑定相关的是标识符的*作用域*概念——标识符名称绑定到对象的程序部分。由于名称仅在编译期间存在，作用域在编译语言中被视为静态属性。（在解释性语言中，解释器在程序执行期间维护标识符名称，因此作用域可以是非静态属性。）局部变量的作用域通常限制在声明它的过程或函数内（或者在像
    Pascal 或 Ada 这样的块结构语言中的任何嵌套过程或函数声明内），并且该名称在子程序外不可见。事实上，可以在不同的作用域中重用标识符的名称（即，在不同的函数或过程内）。在这种情况下，标识符的第二次出现将绑定到与第一次出现不同的对象。
- en: '*Dynamic* objects are those that have some attribute assigned to them during
    program execution. While it is running, the program may choose to change that
    attribute (*dynamically*). The lifetime of that attribute begins when the application
    binds the attribute to the object and ends when the program breaks that bond.
    If the program never breaks the bond, the attribute’s lifetime extends from the
    point of association to the point the program terminates. The system binds dynamic
    attributes to an object at runtime, after the application begins execution.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*动态*对象是指在程序执行过程中某个属性被赋予的对象。在程序运行时，程序可能会选择动态地更改该属性。该属性的生命周期从应用程序将属性绑定到对象的那一刻开始，直到程序断开该绑定时结束。如果程序从未断开该绑定，那么该属性的生命周期将从关联时刻延续到程序终止时刻。系统在程序执行期间将动态属性绑定到对象。'
- en: '**NOTE**'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*An object may have a combination of static and dynamic attributes. For example,
    a static variable has an address bound to it for the entire execution time of
    the program, but it could have different values bound to it throughout the program’s
    lifetime. Any given attribute, however, is either static or dynamic; it cannot
    be both.*'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*一个对象可能同时拥有静态和动态属性的组合。例如，一个静态变量在程序的整个执行时间内都绑定着一个地址，但它可以在程序生命周期内绑定不同的值。然而，任何给定的属性要么是静态的，要么是动态的；它不可能同时是两者。*'
- en: '***11.8.2 The Code, Read-Only, and Constant Sections***'
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***11.8.2 代码、只读和常量部分***'
- en: The code section in memory contains the machine instructions for a program.
    Your compiler translates each statement you write into a sequence of one or more
    byte values. The CPU interprets these byte values as machine instructions during
    program execution.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 内存中的代码部分包含程序的机器指令。您的编译器将您编写的每个语句转换为一个或多个字节值的序列。在程序执行过程中，CPU 将这些字节值解释为机器指令。
- en: Most compilers also attach a program’s read-only data to the code section because,
    like the code instructions, the read-only data is already write-protected. However,
    it is perfectly possible under Windows, macOS, Linux, and many other operating
    systems to create a separate section in the executable file and mark it as read-only.
    As a result, some compilers support a separate read-only data section. Such sections
    contain initialized data, tables, and other objects that the program should not
    change during program execution.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数编译器还将程序的只读数据附加到代码区块，因为像代码指令一样，只读数据已经是写保护的。然而，在Windows、macOS、Linux以及许多其他操作系统中，完全可以在可执行文件中创建一个单独的区块并将其标记为只读。因此，一些编译器支持一个独立的只读数据区块。这些区块包含已初始化的数据、表格和程序在执行过程中不应修改的其他对象。
- en: The constant section shown in [Figure 11-6](ch11.xhtml#ch11fig06) typically
    contains data that the compiler generates (as opposed to user-defined read-only
    data). Most compilers actually emit this data directly to the code section. This
    is why, as previously noted, in most executable files, you’ll find a single section
    that combines the code, read-only data, and constant data sections.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11-6](ch11.xhtml#ch11fig06)中所示的常量区块通常包含编译器生成的数据（与用户定义的只读数据相对）。大多数编译器实际上会直接将这些数据写入代码区块。这就是为什么如前所述，在大多数可执行文件中，你会发现有一个单一的区块，它结合了代码区块、只读数据区块和常量数据区块。'
- en: '***11.8.3 The Static Variables Section***'
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***11.8.3 静态变量区块***'
- en: 'Many languages enable you to initialize a global variable during the compilation
    phase. For example, in C/C++ you could use statements like the following to provide
    initial values for these static objects:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 许多语言允许你在编译阶段初始化全局变量。例如，在C/C++中，你可以使用如下语句为这些静态对象提供初始值：
- en: '[PRE3]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In C/C++ and other languages, the compiler places these initial values in the
    executable file. When you execute the application, the OS loads the portion of
    the executable file that contains these static variables into memory so that the
    values appear at the addresses associated with those static variables. Therefore,
    when the program shown here first begins execution, `i` and `ch` will have these
    values bound to them.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在C/C++以及其他语言中，编译器将这些初始值放置在可执行文件中。当你执行应用程序时，操作系统会将包含这些静态变量的可执行文件部分加载到内存中，以便这些值出现在与静态变量相关的地址上。因此，当此程序首次开始执行时，`i`和`ch`将绑定这些值。
- en: '***11.8.4 The Storage Variables Section***'
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***11.8.4 存储变量区块***'
- en: The storage variables (or BSS) section is where compilers typically put static
    objects that don’t have an explicit value associated with them. BSS stands for
    “block started by a symbol,” which is an old assembly language term describing
    a pseudo-opcode you would use to allocate storage for an uninitialized static
    array. In modern operating systems like Windows and Linux, the compiler/linker
    puts all uninitialized variables into a BSS section that simply tells the OS how
    many bytes to set aside for that section. When the OS loads the program into memory,
    it reserves sufficient memory for all the objects in the BSS section and fills
    this range of memory with `0`s.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 存储变量（或BSS）区块是编译器通常放置没有显式值的静态对象的地方。BSS代表“由符号开始的区块”（Block Started by Symbol），这是一个旧的汇编语言术语，用来描述一种伪操作码，通常用于为未初始化的静态数组分配存储空间。在像Windows和Linux这样的现代操作系统中，编译器/链接器会将所有未初始化的变量放入BSS区块，这个区块仅告诉操作系统为该区块保留多少字节。当操作系统将程序加载到内存中时，它会为BSS区块中的所有对象保留足够的内存，并将这部分内存填充为`0`。
- en: Note that the BSS section in the executable file doesn’t actually contain any
    data, so programs that declare uninitialized static objects (especially large
    arrays) in a BSS section will consume less disk space.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，可执行文件中的BSS区块实际上不包含任何数据，因此在BSS区块中声明未初始化的静态对象（尤其是大型数组）的程序将占用较少的磁盘空间。
- en: However, not all compilers actually use a BSS section. Some Microsoft languages
    and linkers, for example, simply place the uninitialized objects in the static/read-only
    data section and explicitly give them an initial value of `0`. Although Microsoft
    claims that this scheme is faster, it certainly makes executable files larger
    if your code has large, uninitialized arrays (because each byte of the array winds
    up in the executable file—something that would not happen if the compiler placed
    the array in a BSS section).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并不是所有编译器都实际使用BSS部分。例如，一些Microsoft语言和链接器只是将未初始化的对象放置在静态/只读数据部分，并显式地给它们一个初始值`0`。尽管Microsoft声称这种方案更快，但如果你的代码包含大型未初始化数组，这无疑会使可执行文件变得更大（因为数组的每个字节最终都出现在可执行文件中——如果编译器将数组放在BSS部分，这种情况就不会发生）。
- en: '***11.8.5 The Stack Section***'
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***11.8.5 堆栈部分***'
- en: The stack is a data structure that expands and contracts in response to procedure
    invocations and returns to calling routines, among other things. At runtime, the
    system places all automatic variables (nonstatic local variables), subroutine
    parameters, temporary values, and other objects in the stack section of memory
    in a special data structure called an *[activation record](gloss01.xhtml#gloss01_5)*
    (which is aptly named, as the system creates it when a subroutine first begins
    execution, and deallocates it when the subroutine returns to its caller). Therefore,
    the stack section in memory is very busy.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 堆栈是一种数据结构，它根据过程调用和返回等操作动态扩展和收缩。在运行时，系统将所有自动变量（非静态局部变量）、子程序参数、临时值以及其他对象放置在内存中的堆栈部分，采用一种称为*[激活记录](gloss01.xhtml#gloss01_5)*的特殊数据结构（这个名字恰如其分，因为系统在子程序开始执行时创建它，并在子程序返回到调用者时销毁它）。因此，内存中的堆栈部分非常繁忙。
- en: Most CPUs implement the stack using a register called the *stack pointer*. Some
    CPUs, however, don’t provide an explicit stack pointer, instead using a general-purpose
    register for stack implementation. If a CPU provides a stack pointer, we say that
    it supports a *hardware stack*; if it uses a general-purpose register, then we
    say that it uses a *software-implemented stack*. The 80x86 provides a hardware
    stack, while the MIPS Rx000 CPU family uses a software-implemented stack. Systems
    that provide hardware stacks can generally manipulate data on the stack using
    fewer instructions than systems that implement the stack in software. In theory,
    a hardware stack actually slows down all instructions the CPU executes, but in
    practice, the 80x86 CPU is one of the fastest CPUs around, providing ample proof
    that having a hardware stack doesn’t necessarily mean you’ll wind up with a slow
    CPU.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数CPU使用一个叫做*栈指针*的寄存器来实现堆栈。然而，一些CPU并不提供显式的栈指针，而是使用通用寄存器来实现堆栈。如果CPU提供栈指针，我们称它支持*硬件栈*；如果它使用通用寄存器，则称它使用*软件实现的栈*。80x86提供硬件栈，而MIPS
    Rx000系列CPU则使用软件实现的栈。提供硬件栈的系统通常可以比实现软件栈的系统用更少的指令操作堆栈上的数据。理论上，硬件栈会减慢CPU执行所有指令的速度，但实际上，80x86
    CPU是最快的CPU之一，这充分证明了拥有硬件栈并不一定意味着CPU会变慢。
- en: '***11.8.6 The Heap Section and Dynamic Memory Allocation***'
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***11.8.6 堆部分与动态内存分配***'
- en: 'Although simple programs may need only static and automatic variables, sophisticated
    programs need to be able to allocate and deallocate storage dynamically (at runtime)
    under program control. The C and HLA languages provide the `malloc()` and `free()`
    functions for this purpose, C++ provides `new()` and `delete()`, Pascal uses `new()`
    and `dispose()`, and other languages include comparable routines. These memory
    allocation routines have a few things in common: they let the programmer request
    how many bytes of storage to allocate, they return a *pointer* to the newly allocated
    storage (that is, the address of that storage), and they provide a facility for
    returning the storage space to the system once it is no longer needed, so the
    system can reuse it in a future allocation call. Dynamic memory allocation takes
    place in a section of memory known as the *heap*.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管简单程序可能只需要静态和自动变量，但复杂的程序需要能够在程序控制下动态地分配和释放存储（在运行时）。C语言和HLA语言为此提供了`malloc()`和`free()`函数，C++提供了`new()`和`delete()`，Pascal使用`new()`和`dispose()`，其他语言也包括类似的例程。这些内存分配例程有一些共同点：它们允许程序员请求分配多少字节的存储，它们返回一个指向新分配存储的*指针*（即该存储的地址），并且它们提供一个将存储空间返回给系统的功能，一旦存储不再需要，系统就可以在未来的分配调用中重新利用这部分空间。动态内存分配发生在被称为*堆*的内存部分。
- en: Generally, an application refers to data on the heap using pointer variables
    either implicitly or explicitly; some languages, like Java, implicitly use pointers
    behind the scenes. Thus, objects in heap memory are usually known as *anonymous
    variables* because we refer to them by their memory address (via pointers) rather
    than by name.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: The OS and application create the heap section in memory after the program begins
    execution; the heap is never a part of the executable file. Generally, the OS
    and language runtime libraries maintain the heap for an application. Despite the
    variations in memory management implementations, it’s still a good idea for you
    to have a basic idea of how heap allocation and deallocation operate, because
    using them inappropriately will have a very negative impact on your application
    performance.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '**11.8.6.1 A Simple Memory Allocation Scheme**'
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An extremely simple (and fast) memory allocation scheme would return a pointer
    to a block of memory whose size the caller requests. It would carve out allocation
    requests from the heap, returning blocks of memory that are currently unused.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: A very simple memory manager might maintain a single variable (a *free space*
    pointer) pointing to the heap. Whenever a memory allocation request comes along,
    the system makes a copy of this heap pointer and returns it to the application;
    then the heap management routines add the size of the memory request to the address
    held in the pointer variable and verify that the memory request doesn’t try to
    use more memory than is available in the heap (some memory managers return an
    error indication, like a `NULL` pointer, when the memory request is too large,
    and others raise an exception). As the heap management routines increment the
    free space pointer, they effectively mark all previous memory as “unavailable
    for future requests.”
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '**11.8.6.2 Garbage Collection**'
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The problem with this simple memory management scheme is that it wastes memory,
    because there’s no *garbage collection* mechanism for the application to free
    the memory so it can be reused later. Garbage collection—that is, reclaiming memory
    when an application has finished using it—is one of the main purposes of a heap
    management system.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: The only catch is that supporting garbage collection requires some overhead.
    The memory management code will need to be more sophisticated, will take longer
    to execute, and will require some additional memory to maintain the internal data
    structures the heap management system uses.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider an easy implementation of a heap manager that supports garbage
    collection. This simple system maintains a (linked) list of free memory blocks.
    Each free memory block in the list requires two double-word values: one specifying
    the size of the free block, and the other containing a link to the next free block
    in the list (that is, a pointer), as shown in [Figure 11-7](ch11.xhtml#ch11fig07).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: The system initializes the heap with a `NULL` link pointer, and the size field
    contains the size of the heap’s entire free space. When a memory allocation request
    comes along, the heap manager searches through the list to find a free block with
    enough memory to satisfy the request. This search process is one of the defining
    characteristics of a heap manager. Some common search algorithms are first-fit
    search and best-fit search. A *[first-fit search](gloss01.xhtml#gloss01_98)*,
    as its name suggests, scans the list of blocks until it finds the *first* block
    of memory large enough to satisfy the allocation request. A *[best-fit search](gloss01.xhtml#gloss01_26)*
    scans the entire list and finds the *smallest* block large enough to satisfy the
    request. The advantage of the best-fit algorithm is that it tends to preserve
    larger blocks better than the first-fit algorithm, so the system is still able
    to satisfy larger subsequent allocation requests when they arrive. The first-fit
    algorithm, on the other hand, just grabs the first suitably large block it finds,
    even if there’s a smaller block that would suffice, which may limit the system’s
    ability to handle future large memory requests.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig07.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-7: Heap management using a list of free memory blocks*'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Still, the first-fit algorithm does have a couple of advantages over the best-fit
    algorithm. The most obvious is that it is usually faster. The best-fit algorithm
    has to scan through every block in the free block list in order to find the smallest
    one large enough to satisfy the allocation request (unless, of course, it finds
    a perfectly sized block along the way). The first-fit algorithm can stop once
    it finds a block large enough to satisfy the request.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'The first-fit algorithm also tends to suffer less from a degenerate condition
    known as *external fragmentation*. Fragmentation occurs after a long sequence
    of allocation and deallocation requests. Remember, when the heap manager satisfies
    a memory allocation request, it usually creates two blocks of memory: one in-use
    block for the request, and one free block that contains the remaining bytes from
    the original block (assuming the request did not exactly match the block size).
    After operating for a while, the best-fit algorithm may have produced lots of
    leftover blocks of memory that are too small to satisfy an average memory request,
    making them effectively unusable. As these small fragments accumulate throughout
    the heap, they can end up consuming a fair amount of memory. This can lead to
    a situation where the heap doesn’t have a sufficiently large block to satisfy
    a memory allocation request even though there is enough total free memory available
    (spread throughout the heap). See [Figure 11-8](ch11.xhtml#ch11fig08) for an example
    of this condition.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig08.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-8: Memory fragmentation*'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: There are other memory allocation strategies in addition to the first-fit and
    best-fit search algorithms. Some of these execute faster, some have less memory
    overhead, some are easy to understand (and some are very complex), some produce
    less fragmentation, and some can combine and use noncontiguous blocks of free
    memory. Memory/heap management is one of the more heavily studied subjects in
    computer science, and there’s a considerable amount of literature explaining the
    benefits of one scheme over another. For more information on memory allocation
    strategies, check out a good book on OS design.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '**11.8.6.3 Freeing Allocated Memory**'
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Memory allocation is only half of the story. As mentioned earlier, the heap
    manager has to provide a call that allows an application to return memory it no
    longer needs for future reuse. In C and HLA, for example, an application accomplishes
    this by calling the `free()` function.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'At first blush, `free()` might seem like a very simple function to write: just
    append the previously allocated and now unused block to the end of the free list.
    The problem with this trivial implementation is that it almost guarantees that
    the heap becomes fragmented to the point of being unusable in very short order.
    Consider the situation in [Figure 11-9](ch11.xhtml#ch11fig09).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/11fig09.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-9: Freeing a memory block*'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: If a trivial implementation of `free()` simply takes the block to be freed and
    appends it to the free list, the memory organization in [Figure 11-9](ch11.xhtml#ch11fig09)
    produces three free blocks. However, because these three blocks are contiguous,
    the heap manager should really combine them into a single free block, so that
    it will be able to satisfy a larger request. Unfortunately, this operation would
    require it to scan the free block list to determine if there are any free blocks
    adjacent to the block the system is freeing.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: While you could come up with a data structure that makes it easier to combine
    adjacent free blocks, such schemes generally add 8 or more bytes of overhead with
    each block on the heap. Whether or not this is a reasonable tradeoff depends on
    the average size of a memory allocation. If the applications that use the heap
    manager tend to allocate small objects, the extra overhead for each memory block
    could wind up consuming a large percentage of the heap space. However, if the
    most allocations are large, then the few bytes of overhead won’t matter much.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '**11.8.6.4 The OS and Memory Allocation**'
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The performance of the algorithms and data structures used by the heap manager
    is only one piece of the performance puzzle. Ultimately, the heap manager needs
    to request blocks of memory from the operating system. At one extreme, the OS
    handles all memory allocation requests directly. At the other extreme, the heap
    manager is a runtime library routine that links with your application, first requesting
    large blocks of memory from the OS and then doling out pieces of them as allocation
    requests arrive from the application.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: The problem with making direct memory allocation requests to the operating system
    is that OS API calls are often very slow. This is because they generally involve
    switching between kernel mode and user mode on the CPU (which is not fast). Therefore,
    a heap manager that the OS implements directly will not perform well if your application
    makes frequent calls to the memory allocation and deallocation routines.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Because of the high overhead of an OS call, most languages implement their own
    versions of the `malloc()` and `free()` functions within their runtime library.
    On the very first memory allocation, the `malloc()` routine requests a large block
    of memory from the OS, and the application’s `malloc()` and `free()` routines
    manage this block of memory themselves. If an allocation request comes along that
    the `malloc()` function cannot fulfill in the block it originally created, `malloc()`
    will request another large block (generally much larger than the request) from
    the OS and add that block to the end of its free list. Because the application’s
    `malloc()` and `free()` routines call the OS only occasionally, the application
    doesn’t suffer the performance hit associated with frequent OS calls.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Most standard heap management functions perform reasonably for a typical program.
    However, keep in mind that the procedures are very implementation- and language-specific;
    it’s dangerous to assume that `malloc()` and `free()` are relatively efficient
    when writing software that requires high-performance components. The only portable
    way to ensure a high-performance heap manager is to develop your own application-specific
    set of allocation/deallocation routines. Writing such routines is beyond the scope
    of this book, but you should know you have this option.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '**11.8.6.5 Heap Memory Overhead**'
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A heap manager often exhibits two types of overhead: performance (speed) and
    memory (space). Until now, this discussion has mainly dealt with the performance
    aspects, but now we’ll turn our attention to memory.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Each block the system allocates requires some amount of overhead beyond the
    storage the application requests; at the very least, this overhead is a few bytes
    to keep track of the block’s size. Fancier (higher-performance) schemes may require
    additional bytes, but typically the overhead is between 4 and 16 bytes. The heap
    manager can keep this information in a separate internal table, or it can attach
    the block size and other memory management information directly to the block it
    allocates.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Saving this information in an internal table has a couple of advantages. First,
    it is difficult for the application to accidentally overwrite the information
    stored there; attaching the data to the heap memory blocks themselves doesn’t
    protect as well against this possibility. Second, putting memory management information
    in an internal data structure allows the memory manager to determine whether a
    given pointer is valid (that is, whether it points at some block of memory that
    the heap manager believes it has allocated).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of attaching the control information directly to each block that
    the heap manager allocates is that it’s very easy to locate this information,
    whereas storing the information in an internal table might require a search operation.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Another issue that affects the overhead associated with the heap manager is
    the *allocation granularity*—the minimum number of bytes the heap manager supports.
    Although most heap managers allow you to request an allocation as small as 1 byte,
    they may actually allocate some minimum number of bytes greater than 1\. To ensure
    an allocated object is aligned on a reasonable address for that object, most heap
    managers allocate memory blocks on a 4-, 8-, or 16-byte boundary. For performance
    reasons, many heap managers begin each allocation on a typical cache-line boundary,
    usually 16, 32, or 64 bytes.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Whatever the granularity, if the application requests some number of bytes that
    is less than or not a multiple of the heap manager’s granularity, the heap manager
    will allocate extra bytes of storage so that the complete allocation is an even
    multiple of the granularity value. This amount varies by heap manager (and possibly
    even by version of a specific heap manager), so an application should never assume
    that it has more memory available than it requests.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: The extra memory the heap manager allocates results in another form of fragmentation
    called *internal fragmentation*. Like external fragmentation, internal fragmentation
    produces small amounts of leftover memory throughout the system that cannot satisfy
    future allocation requests. Assuming random-sized memory allocations, the average
    amount of internal fragmentation that occurs on each allocation is half the granularity
    size. Fortunately, the granularity size is quite small for most memory managers
    (typically 16 bytes or less), so after thousands and thousands of memory allocations
    you’ll lose only a couple dozen or so kilobytes to internal fragmentation.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Between the costs associated with allocation granularity and the memory control
    information, a typical memory request may require between 4 and 16 bytes, plus
    whatever the application requests. If you’re making large memory allocation requests
    (hundreds or thousands of bytes), the overhead bytes won’t consume a large percentage
    of memory on the heap. However, if you allocate lots of small objects, the memory
    consumed by internal fragmentation and memory control information may represent
    a significant portion of your heap area. For example, consider a simple memory
    manager that always allocates blocks of data on 4-byte boundaries and requires
    a single 4-byte length value that it attaches to each allocation request for memory
    storage. This means that the minimum amount of storage the heap manager requires
    for each allocation is 8 bytes. If you make a series of `malloc()` calls to allocate
    a single byte, the application won’t be able to use almost 88 percent of the memory
    it allocates. Even if you allocate 4-byte values on each allocation request, the
    heap manager consumes 67 percent of the memory for overhead purposes. However,
    if your average allocation is a block of 256 bytes, the overhead requires only
    about 2 percent of the total memory allocation. In short, the larger your allocation
    request, the less impact the control information and internal fragmentation will
    have on your heap.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Many software engineering studies in computer science journals have found that
    memory allocation/deallocation requests cause a significant loss of performance.
    In such studies, the authors often obtained performance improvements of 100 percent
    or better just by implementing their own simplified, application-specific, memory
    management algorithms rather than calling the standard runtime library or OS kernel
    memory allocation code. Hopefully, this section has made you aware of this potential
    problem in your own code.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '**11.9 For More Information**'
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hennessy, John L., and David A. Patterson. *Computer Architecture: A Quantitative
    Approach*. 5th ed. Waltham, MA: Elsevier, 2012.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
