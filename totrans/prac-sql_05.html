<html><head></head><body><div id="sbo-rt-content"><section>&#13;
<header>&#13;
<h1 class="chapter">&#13;
<span class="ChapterNumber"><span epub:type="pagebreak" title="59" id="Page_59"/>5</span><br/>&#13;
<span class="ChapterTitle">Importing and Exporting Data</span></h1>&#13;
</header>&#13;
<figure class="opener">&#13;
<img src="Images/chapterart.png" alt="" width="200" height="200"/>&#13;
</figure>&#13;
<p class="ChapterIntro">So far, you’ve learned how to add a handful of rows to a table using SQL <code>INSERT</code> statements. A row-by-row insert is useful for making quick test tables or adding a few rows to an existing table. But it’s more likely you’ll need to load hundreds, thousands, or even millions of rows, and no one wants to write separate <code>INSERT</code> statements in those situations. Fortunately, you don’t have to.</p>&#13;
<p>If your data exists in a <em>delimited</em> text file, with one table row per line of text and each column value separated by a comma or other character, PostgreSQL can import the data in bulk via its <code>COPY</code> command. This command is a PostgreSQL-specific implementation with options for including or excluding columns and handling various delimited text types.</p>&#13;
<p>In the opposite direction, <code>COPY</code> will also <em>export</em> data from PostgreSQL tables or from the result of a query to a delimited text file. This technique is handy when you want to share data with colleagues or move it into another format, such as an Excel file.</p>&#13;
<p><span epub:type="pagebreak" title="60" id="Page_60"/>I briefly touched on <code>COPY</code> for export in the “Understanding Characters” section of <span class="xref" itemid="xref_target_Chapter 4">Chapter 4</span>, but in this chapter, I’ll discuss import and export in more depth. For importing, I’ll start by introducing you to one of my favorite datasets: annual US Census population estimates by county.</p>&#13;
<p>Three steps form the outline of most of the imports you’ll do:</p>&#13;
<ol class="decimal">&#13;
<li value="1">Obtain the source data in the form of a delimited text file.</li>&#13;
<li value="2">Create a table to store the data.</li>&#13;
<li value="3">Write a <code>COPY</code> statement to perform the import.</li>&#13;
</ol>&#13;
<p>After the import is done, we’ll check the data and look at additional options for importing and exporting.</p>&#13;
<p>A delimited text file is the most common file format that’s portable across proprietary and open source systems, so we’ll focus on that file type. If you want to transfer data from another database program’s proprietary format directly to PostgreSQL—for example, from Microsoft Access or MySQL—you’ll need to use a third-party tool. Check the PostgreSQL wiki at <a href="https://wiki.postgresql.org/wiki/" class="LinkURL">https://wiki.postgresql.org/wiki/</a> and search for “Converting from other databases to PostgreSQL” for a list of tools and options.</p>&#13;
<p>If you’re using SQL with another database manager, check the other database’s documentation for how it handles bulk imports. The MySQL database, for example, has a <code>LOAD DATA INFILE</code> statement, and Microsoft’s SQL Server has its own <code>BULK INSERT</code> command.</p>&#13;
<h2 id="h1-501065c05-0001">Working with Delimited Text Files</h2>&#13;
<p class="BodyFirst">Many software applications store data in a unique format, and translating one data format to another is about as easy as trying to read the Cyrillic alphabet when one understands only English. Fortunately, most software can import from and export to a delimited text file, which is a common data format that serves as a middle ground.</p>&#13;
<p>A delimited text file contains rows of data, each of which represents one row in a table. In each row, each data column is separated, or delimited, by a particular character. I’ve seen all kinds of characters used as delimiters, from ampersands to pipes, but the comma is most commonly used; hence the name of a file type you’ll see often is <em>comma-separated values (CSV)</em>. The terms <em>CSV</em> and <em>comma-delimited</em> are interchangeable.</p>&#13;
<p>Here’s a typical data row you might see in a comma-delimited file:</p>&#13;
<pre><code>John,Doe,123 Main St.,Hyde Park,NY,845-555-1212</code></pre>&#13;
<p>Notice that a comma separates each piece of data—first name, last name, street, town, state, and phone—without any spaces. The commas tell the software to treat each item as a separate column, upon either import or export. Simple enough.</p>&#13;
<h3 id="h2-501065c05-0001"><span epub:type="pagebreak" title="61" id="Page_61"/>Handling Header Rows</h3>&#13;
<p class="BodyFirst">A feature you’ll often find inside a delimited text file is a <em>header row</em>. As the name implies, it’s a single row at the top, or <em>head</em>, of the file that lists the name of each data column. Often, a header is added when data is exported from a database or a spreadsheet. Here’s an example with the delimited row I’ve been using. Each item in a header row corresponds to its respective column:</p>&#13;
<pre><code>FIRSTNAME,LASTNAME,STREET,CITY,STATE,PHONE&#13;
John,Doe,123 Main St.,Hyde Park,NY,845-555-1212</code></pre>&#13;
<p>Header rows serve a few purposes. For one, the values in the header row identify the data in each column, which is particularly useful when you’re deciphering a file’s contents. Second, some database managers (although not PostgreSQL) use the header row to map columns in the delimited file to the correct columns in the import table. PostgreSQL doesn’t use the header row, so we don’t want to import that row to a table. We use the <code>HEADER</code> option in the <code>COPY</code> command to exclude it. I’ll cover this with all <code>COPY</code> options in the next section.</p>&#13;
<h3 id="h2-501065c05-0002">Quoting Columns That Contain Delimiters</h3>&#13;
<p class="BodyFirst">Using commas as a column delimiter leads to a potential dilemma: what if the value in a column includes a comma? For example, sometimes people combine an apartment number with a street address, as in 123 Main St., Apartment 200. Unless the system for delimiting accounts for that extra comma, during import the line will appear to have an extra column and cause the import to fail.</p>&#13;
<p>To handle such cases, delimited files use an arbitrary character called a <em>text qualifier</em> to enclose a column that includes the delimiter character. This acts as a signal to ignore that delimiter and treat everything between the text qualifiers as a single column. Most of the time in comma-delimited files the text qualifier used is the double quote. Here’s the example data again, but with the street name column surrounded by double quotes:</p>&#13;
<pre><code>FIRSTNAME,LASTNAME,STREET,CITY,STATE,PHONE&#13;
John,Doe,"123 Main St., Apartment 200",Hyde Park,NY,845-555-1212</code></pre>&#13;
<p>On import, the database will recognize that double quotes signify one column regardless of whether it finds a delimiter within the quotes. When importing CSV files, PostgreSQL by default ignores delimiters inside double-quoted columns, but you can specify a different text qualifier if your import requires it. (And, given the sometimes-odd choices made by IT professionals, you may indeed need to employ a different character.)</p>&#13;
<p>Finally, in CSV mode, if PostgreSQL finds two consecutive text qualifiers inside a double-quoted column, it will remove one. For example, let’s say PostgreSQL finds this:</p>&#13;
<pre><code>"123 Main St."" Apartment 200"</code></pre>&#13;
<p><span epub:type="pagebreak" title="62" id="Page_62"/>If so, it will treat that text as a single column upon import, leaving just one of the qualifiers:</p>&#13;
<pre><code>123 Main St." Apartment 200</code></pre>&#13;
<p>A situation like that could indicate an error in the formatting of your CSV file, which is why, as you’ll see later, it’s always a good idea to review your data after importing.</p>&#13;
<h2 id="h1-501065c05-0002">Using COPY to Import Data </h2>&#13;
<p class="BodyFirst">To import data from an external file into our database, we first create a table in our database that matches the columns and data types in our source file. Once that’s done, the <code>COPY</code> statement for the import is just the three lines of code in <a href="#listing5-1" id="listinganchor5-1">Listing 5-1</a>.</p>&#13;
<pre><code><span class="CodeAnnotationHang" aria-label="annotation1">1</span> COPY <var>table_name</var>&#13;
<span class="CodeAnnotationHang" aria-label="annotation2">2</span> FROM '<var>C:\YourDirectory\your_file.csv</var>'&#13;
<span class="CodeAnnotationHang" aria-label="annotation3">3</span> WITH (FORMAT CSV, HEADER);</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing5-1">Listing 5-1</a>: Using <code>COPY</code> for data import</p>&#13;
<p>We start the block of code with the <code>COPY</code> keyword <span class="CodeAnnotation" aria-label="annotation1">1</span> followed by the name of the target table, which must already exist in your database. Think of this syntax as meaning, “Copy data to my table called <var>table_name</var>.”</p>&#13;
<p>The <code>FROM</code> keyword <span class="CodeAnnotation" aria-label="annotation2">2</span> identifies the full path to the source file, and we enclose the path in single quotes. The way you designate the path depends on your operating system. For Windows, begin with the drive letter, colon, backslash, and directory names. For example, to import a file located on my Windows desktop, the <code>FROM</code> line would read as follows: </p>&#13;
<pre><code>FROM 'C:\Users\Anthony\Desktop\<var>my_file.csv</var>'</code></pre>&#13;
<p>On macOS or Linux, start at the system root directory with a forward slash and proceed from there. Here’s what the <code>FROM</code> line might look like when importing a file located on my macOS desktop:</p>&#13;
<pre><code>FROM '/Users/anthony/Desktop/<var>my_file.csv</var>'</code></pre>&#13;
<p>For the examples in the book, I use the Windows-style path <var>C:\YourDirectory\</var> as a placeholder. Replace that with the path where you stored the CSV file you downloaded from GitHub.</p>&#13;
<p>The <code>WITH</code> keyword <span class="CodeAnnotation" aria-label="annotation3">3</span> lets you specify options, surrounded by parentheses, that you use to tailor your input or output file. Here we specify that the external file should be comma-delimited and that we should exclude the file’s header row in the import. It’s worth examining all the options in the official PostgreSQL documentation at <a href="https://www.postgresql.org/docs/current/sql-copy.html" class="LinkURL">https://www.postgresql.org/docs/current/sql-copy.html</a>, but here is a list of the options you’ll commonly use:</p>&#13;
<p class="ListHead"><b><span epub:type="pagebreak" title="63" id="Page_63"/>Input and output file format</b></p>&#13;
<ol class="none">&#13;
<li>Use the <code>FORMAT</code> <var>format_name</var> option to specify the type of file you’re reading or writing. Format names are <code>CSV</code>, <code>TEXT</code>, or <code>BINARY</code>. Unless you’re deep into building technical systems, you’ll rarely encounter a need to work with <code>BINARY</code>, where data is stored as a sequence of bytes. More often, you’ll work with standard CSV files. In the <code>TEXT</code> format, a <em>tab</em> character is the delimiter by default (although you can specify another character), and backslash characters such as <code>\r</code> are recognized as their ASCII equivalents—in this case, a carriage return. The <code>TEXT</code> format is used mainly by PostgreSQL’s built-in backup programs.</li>&#13;
</ol>&#13;
<p class="ListHead"><b>Presence of a header row</b></p>&#13;
<ol class="none">&#13;
<li>On import, use <code>HEADER</code> to specify that the source file has a header row that you want to exclude. The database will start importing with the second line of the file so that the column names in the header don’t become part of the data in the table. (Be sure to check your source CSV to make sure this is what you want; not every CSV comes with a header row!) On export, using <code>HEADER</code> tells the database to include the column names as a header row in the output file, which helps a user understand the file’s contents.</li>&#13;
</ol>&#13;
<p class="ListHead"><b>Delimiter</b></p>&#13;
<ol class="none">&#13;
<li>The <code>DELIMITER</code> <code>'</code><var>character</var><code>'</code> option lets you specify which character your import or export file uses as a delimiter. The delimiter must be a single character and cannot be a carriage return. If you use <code>FORMAT CSV</code>, the assumed delimiter is a comma. I include <code>DELIMITER</code> here to show that you have the option to specify a different delimiter if that’s how your data arrived. For example, if you received pipe-delimited data, you would treat the option this way: <code>DELIMITER '|'</code>.</li>&#13;
</ol>&#13;
<p class="ListHead"><b>Quote character</b></p>&#13;
<ol class="none">&#13;
<li>Earlier, you learned that in a CSV file, commas inside a single column value will mess up your import unless the column value is surrounded by a character that serves as a text qualifier, telling the database to handle the value within as one column. By default, PostgreSQL uses the double quote, but if the CSV you’re importing uses a different character for the text qualifier, you can specify it with the <code>QUOTE</code> <code>'</code><var>quote_character</var><code>'</code> option.</li>&#13;
</ol>&#13;
<p>Now that you better understand delimited files, you’re ready to import one.</p>&#13;
<h2 id="h1-501065c05-0003">Importing Census Data Describing Counties</h2>&#13;
<p class="BodyFirst">The dataset you’ll work with in this import exercise is considerably larger than the <code>teachers</code> table you made in <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span>. It contains census population estimates for every county in the United States and is 3,142 rows deep and 16 columns wide. (Census counties include some geographies with <span epub:type="pagebreak" title="64" id="Page_64"/>other names: parishes in Louisiana, boroughs and census areas in Alaska, and cities, particularly in Virginia.)</p>&#13;
<p>To understand the data, it helps to know a little about the US Census Bureau, a federal agency that tracks the nation’s demographics. Its best-known program is a full count of the population it undertakes every 10 years, most recently in 2020. That data, which enumerates the age, gender, race, and ethnicity of each person in the country, is used to determine how many members from each state make up the 435-member US House of Representatives. In recent decades, faster-growing states such as Texas and Florida have gained seats, while slower-growing states such as New York and Ohio have lost representatives in the House.</p>&#13;
<p>The data we’ll work with are the census’ annual population estimates. These use the most recent 10-year census count as a base, and they factor in births, deaths, and domestic and international migration to produce population estimates each year for the nation, states, counties, and other geographies. In lieu of an annual physical count, it’s the best way to get an updated measure on how many people live where in the United States. For this exercise, I compiled select columns from the 2019 US Census county-level population estimates (plus a few descriptive columns from census geographic data) into a file named <em>us_counties_pop_est_2019.csv</em>. You should have this file on your computer if you followed the directions in the section “Downloading Code and Data from GitHub” in <span class="xref" itemid="xref_target_Chapter 1">Chapter 1</span>. If not, go back and do that now.</p>&#13;
<aside epub:type="sidebar">&#13;
<div class="top hr"><hr/></div>&#13;
<section class="note">&#13;
<h2><span class="NoteHead">Note</span></h2>&#13;
<p>	The 2019-vintage population estimates we’re using do not reflect the split in 2019 of the former Valdez-Cordova census area into two new Alaska county equivalents. That change increased the number of US counties to 3,143.</p>&#13;
<div class="bottom hr"><hr/></div>&#13;
</section>&#13;
</aside>&#13;
<p>Open the file with a text editor. You should see a header row that begins with these columns:</p>&#13;
<pre><code>state_fips,county_fips,region,state_name,county_name, <var>--snip--</var></code></pre>&#13;
<p>Let’s explore the columns by examining the code for creating the import table.</p>&#13;
<h3 id="h2-501065c05-0003">Creating the us_counties_pop_est_2019 Table</h3>&#13;
<p class="BodyFirst">The code in <a href="#listing5-2" id="listinganchor5-2">Listing 5-2</a> shows the <code>CREATE TABLE</code> script. In pgAdmin click the <code>analysis</code> database that you created in <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span>. (It’s best to store the data in this book in <code>analysis</code> because we’ll reuse some of it in later chapters.) From the pgAdmin menu bar, select <b>Tools</b><span class="MenuArrow">▶</span><b>Query Tool</b>. You can type the code into the tool or copy and paste it from the files you downloaded from GitHub. Once you have the script in the window, run it.</p>&#13;
<pre><code>CREATE TABLE us_counties_pop_est_2019 (&#13;
    <span class="CodeAnnotationHang" aria-label="annotation1">1</span> state_fips text,&#13;
    county_fips text,&#13;
    <span class="CodeAnnotationHang" aria-label="annotation2">2</span> region smallint,&#13;
    <span class="CodeAnnotationHang" aria-label="annotation3">3</span> state_name text,&#13;
<span epub:type="pagebreak" title="65" id="Page_65"/>    county_name text,&#13;
    <span class="CodeAnnotationHang" aria-label="annotation4">4</span> area_land bigint,&#13;
    area_water bigint,&#13;
    <span class="CodeAnnotationHang" aria-label="annotation5">5</span> internal_point_lat numeric(10,7),&#13;
    internal_point_lon numeric(10,7),&#13;
    <span class="CodeAnnotationHang" aria-label="annotation6">6</span> pop_est_2018 integer,&#13;
    pop_est_2019 integer,&#13;
    births_2019 integer,&#13;
    deaths_2019 integer,&#13;
    international_migr_2019 integer,&#13;
    domestic_migr_2019 integer,&#13;
    residual_2019 integer,&#13;
    <span class="CodeAnnotationHang" aria-label="annotation7">7</span> CONSTRAINT counties_2019_key PRIMARY KEY (state_fips, county_fips)&#13;
);</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing5-2">Listing 5-2</a>: <code>CREATE TABLE</code> statement for census county population estimates</p>&#13;
<p>Return to the main pgAdmin window, and in the object browser, right-click and refresh the <code>analysis</code> database. Choose <b>Schemas</b><span class="MenuArrow">▶</span><b>public</b><span class="MenuArrow">▶</span><b>Tables</b> to see the new table. Although it’s empty, you can see the structure by running a basic <code>SELECT</code> query in pgAdmin’s Query Tool:</p>&#13;
<pre><code>SELECT * FROM us_counties_pop_est_2019;</code></pre>&#13;
<p>When you run the <code>SELECT</code> query, you’ll see the columns in the table you created appear in the pgAdmin Data Output pane. No data rows exist yet. We need to import them.</p>&#13;
<h3 id="h2-501065c05-0004">Understanding Census Columns and Data Types</h3>&#13;
<p class="BodyFirst">Before we import the CSV file into the table, let’s walk through several of the columns and the data types I chose in <a href="#listing5-2">Listing 5-2</a>. As my guide, I used two official census data dictionaries: one for the estimates found at <a href="https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2010-2019/co-est2019-alldata.pdf" class="LinkURL">https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2010-2019/co-est2019-alldata.pdf</a> and one for the decennial count that includes the geographic columns at <a href="http://www.census.gov/prod/cen2010/doc/pl94-171.pdf" class="LinkURL">http://www.census.gov/prod/cen2010/doc/pl94-171.pdf</a>. I’ve given some columns more readable names in the table definition. Relying on a data dictionary when possible is good practice, because it helps you avoid misconfiguring columns or potentially losing data. Always ask if one is available, or do an online search if the data is public.</p>&#13;
<p>In this set of census data, and thus the table you just made, each row displays the population estimates and components of annual change (births, deaths, and migration) for one county. The first two columns are the county’s <code>state_fips</code> <span class="CodeAnnotation" aria-label="annotation1">1</span> and <code>county_fips</code>, which are the standard federal codes for those entities. We use <code>text</code> for both because those codes can contain leading zeros that would be lost if we stored the values as integers. For example, Alaska’s <code>state_fips</code> is <code>02</code>. If we used an integer type, that leading <code>0</code> would be stripped on import, leaving <code>2</code>, which is the wrong code for the state. Also, we won’t be doing any math with this value, so don’t need integers. It’s always important to distinguish codes from numbers; these state and county values are actually labels as opposed to numbers used for math.</p>&#13;
<p><span epub:type="pagebreak" title="66" id="Page_66"/>Numbers from 1 to 4 in <code>region</code> <span class="CodeAnnotation" aria-label="annotation2">2</span> represent the general location of a county in the United States: the Northeast, Midwest, South, or West. No number is higher than 4, so we define the columns with type <code>smallint</code>. The <code>state_name</code> <span class="CodeAnnotation" aria-label="annotation3">3</span> and <code>county_name</code> columns contain the complete name of both the state and county, stored as <code>text</code>.</p>&#13;
<p>The number of square meters for land and water in the county are recorded in <code>area_land</code> <span class="CodeAnnotation" aria-label="annotation4">4</span> and <code>area_water</code>, respectively. The two, combined, comprise a county’s total area. In certain places—such as Alaska, where there’s lots of land to go with all that snow—some values easily surpass the <code>integer</code> type’s maximum of 2,147,483,647. For that reason, we’re using <code>bigint</code>, which will handle the 377,038,836,685 square meters of land in the Yukon-Koyukuk census area with room to spare.</p>&#13;
<p>The latitude and longitude of a point near the center of the county, called an <em>internal point</em>, are specified in <code>internal_point_lat</code> and <code>internal_point_lon</code> <span class="CodeAnnotation" aria-label="annotation5">5</span>, respectively. The Census Bureau—along with many mapping systems—expresses latitude and longitude coordinates using a <em>decimal degrees</em> system. <em>Latitude</em> represents positions north and south on the globe, with the equator at 0 degrees, the North Pole at 90 degrees, and the South Pole at −90 degrees.</p>&#13;
<p><em>Longitude</em> represents locations east and west, with the <em>Prime Meridian</em> that passes through Greenwich in London at 0 degrees longitude. From there, longitude increases both east and west (positive numbers to the east and negative to the west) until they meet at 180 degrees on the opposite side of the globe. The location there, known as the <em>antimeridian</em>, is used as the basis for the <em>International Date Line</em>.</p>&#13;
<p>When reporting interior points, the Census Bureau uses up to seven decimal places. With a value up to 180 to the left of the decimal, we need to account for a maximum of 10 digits total. So, we’re using <code>numeric</code> with a precision of <code>10</code> and a scale of <code>7</code>.</p>&#13;
<aside epub:type="sidebar">&#13;
<div class="top hr"><hr/></div>&#13;
<section class="note">&#13;
<h2><span class="NoteHead">Note</span></h2>&#13;
<p>	PostgreSQL, through the PostGIS extension, can store geometric data, which includes points that represent latitude and longitude in a single column. We’ll explore geometric data when we cover geographical queries in <span class="xref" itemid="xref_target_Chapter 15">Chapter 15</span>.</p>&#13;
<div class="bottom hr"><hr/></div>&#13;
</section>&#13;
</aside>&#13;
<p>Next, we reach a series of columns <span class="CodeAnnotation" aria-label="annotation6">6</span> that contain the county’s population estimates and components of change. <a href="#table5-1" id="tableanchor5-1">Table 5-1</a> lists their definitions.</p>&#13;
<figure>&#13;
<figcaption class="TableTitle"><p><a id="table5-1">Table 5-1</a>: Census Population Estimate Columns</p></figcaption>&#13;
<table id="table-501065c05-0001" border="1">&#13;
<thead>&#13;
<tr>&#13;
<td><b>Column name</b></td>&#13;
<td><b>Description</b></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><code>pop_est_2018</code></td>&#13;
<td>Estimated population on July 1, 2018</td>&#13;
</tr>&#13;
<tr>&#13;
<td><code>pop_est_2019</code></td>&#13;
<td>Estimated population on July 1, 2019</td>&#13;
</tr>&#13;
<tr>&#13;
<td><code>births_2019</code></td>&#13;
<td>Number of births from July 1, 2018, to June 30, 2019</td>&#13;
</tr>&#13;
<tr>&#13;
<td><code>deaths_2019</code></td>&#13;
<td>Number of deaths from July 1, 2018, to June 30, 2019</td>&#13;
</tr>&#13;
<tr>&#13;
<td><code>international_migr_2019</code></td>&#13;
<td>Net international migration from July 1, 2018, to June 30, 2019</td>&#13;
</tr>&#13;
<tr>&#13;
<td><code>domestic_migr_2019</code></td>&#13;
<td>Net domestic migration from July 1, 2018, to June 30, 2019</td>&#13;
</tr>&#13;
<tr>&#13;
<td><code>residual_2019</code></td>&#13;
<td>Number used to adjust estimates for consistency</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
</figure>&#13;
<p><span epub:type="pagebreak" title="67" id="Page_67"/>Finally, the <code>CREATE TABLE</code> statement ends with a <code>CONSTRAINT</code> clause <span class="CodeAnnotation" aria-label="annotation7">7</span> specifying that the columns <code>state_fips</code> and <code>county_fips</code> will serve as the table’s primary key. This means that the combination of those columns is unique for every row in the table, a concept we’ll cover extensively in <span class="xref" itemid="xref_target_Chapter 8">Chapter 8</span>. For now, let’s run the import.</p>&#13;
<h3 id="h2-501065c05-0005">Performing the Census Import with COPY</h3>&#13;
<p class="BodyFirst">Now you’re ready to bring the census data into the table. Run the code in <a href="#listing5-3" id="listinganchor5-3">Listing 5-3</a>, remembering to change the path to the file to match the location of the data on your computer.</p>&#13;
<pre><code>COPY us_counties_pop_est_2019&#13;
FROM '<var>C:\YourDirectory\</var>us_counties_pop_est_2019.csv'&#13;
WITH (FORMAT CSV, HEADER);</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing5-3">Listing 5-3</a>: Importing census data using <code>COPY</code></p>&#13;
<p>When the code executes, you should see the following message in pgAdmin:</p>&#13;
<pre><code>COPY 3142&#13;
Query returned successfully in 75 msec.</code></pre>&#13;
<p>That’s good news: the import CSV has the same number of rows. If you have an issue with the source CSV or your import statement, the database will throw an error. For example, if one of the rows in the CSV had more columns than in the target table, you’d see an error message in the Data Output pane of pgAdmin that provides a hint as to how to fix it:</p>&#13;
<pre><code>ERROR: extra data after last expected column&#13;
Context: COPY us_counties_pop_est_2019, line 2: "01,001,3,Alabama, ..."</code></pre>&#13;
<p>Even if no errors are reported, it’s always a good idea to visually scan the data you just imported to ensure everything looks as expected.</p>&#13;
<h3 id="h2-501065c05-0006">Inspecting the Import</h3>&#13;
<p class="BodyFirst">Start with a <code>SELECT</code> query of all columns and rows:</p>&#13;
<pre><code>SELECT * FROM us_counties_pop_est_2019;</code></pre>&#13;
<p>There should be 3,142 rows displayed in pgAdmin, and as you scroll left and right through the result set, each column should have the expected values. Let’s review some columns that we took particular care to define with the appropriate data types. For example, run the following query to show the counties with the largest <code>area_land</code> values. We’ll use a <code>LIMIT</code> clause, which will cause the query to return only the number of rows we want; here, we’ll ask for three:</p>&#13;
<pre><code>SELECT county_name, state_name, area_land&#13;
FROM us_counties_pop_est_2019&#13;
ORDER BY area_land DESC&#13;
LIMIT 3;</code></pre>&#13;
<p><span epub:type="pagebreak" title="68" id="Page_68"/>This query ranks county-level geographies from largest land area to smallest in square meters. We defined <code>area_land</code> as <code>bigint</code> because the largest values in the field are bigger than the upper range provided by regular <code>integer</code>. As you might expect, big Alaskan geographies are at the top:</p>&#13;
<pre><code>county_name                  state_name    area_land&#13;
-------------------------    ----------    ------------&#13;
Yukon-Koyukuk Census Area    Alaska        377038836685&#13;
North Slope Borough          Alaska        230054247231&#13;
Bethel Census Area           Alaska        105232821617</code></pre>&#13;
<p>Next, let’s check the latitude and longitude columns of <code>internal_point_lat</code> and <code>internal_point_lon</code>, which we defined with <code>numeric(10,7)</code>. This code sorts the counties by longitude from the greatest to smallest value. This time, we’ll use <code>LIMIT</code> to retrieve five rows:</p>&#13;
<pre><code>SELECT county_name, state_name, internal_point_lat, internal_point_lon&#13;
FROM us_counties_pop_est_2019&#13;
ORDER BY internal_point_lon DESC&#13;
LIMIT 5;</code></pre>&#13;
<p>Longitude measures locations from east to west, with locations west of the Prime Meridian in England represented as negative numbers starting with −1, −2, −3, and so on, the farther west you go. We sorted in descending order, so we’d expect the easternmost counties of the United States to show at the top of the query result. Instead—surprise!—there’s a lone Alaska geography at the top:</p>&#13;
<pre><code>      county_name          state_name  internal_point_lat  internal_point_lon&#13;
-------------------------- ----------  ------------------  ------------------&#13;
Aleutians West Census Area Alaska              51.9489640         179.6211882&#13;
Washington County          Maine               44.9670088         -67.6093542&#13;
Hancock County             Maine               44.5649063         -68.3707034&#13;
Aroostook County           Maine               46.7091929         -68.6124095&#13;
Penobscot County           Maine               45.4092843         -68.6666160</code></pre>&#13;
<p>Here’s why: the Alaskan Aleutian Islands extend so far west (farther west than Hawaii) that they cross the antimeridian at 180 degrees longitude. Once past the antimeridian, longitude turns positive, counting back down to 0. Fortunately, it’s not a mistake in the data; however, it’s a fact you can tuck away for your next trivia team competition.</p>&#13;
<p>Congratulations! You have a legitimate set of government demographic data in your database. I’ll use it to demonstrate exporting data with <code>COPY</code> later in this chapter, and then you’ll use it to learn math functions in <span class="xref" itemid="xref_target_Chapter 6">Chapter 6</span>. Before we move on to exporting data, let’s examine a few additional importing techniques.</p>&#13;
<h2 id="h1-501065c05-0004">Importing a Subset of Columns with COPY</h2>&#13;
<p class="BodyFirst">If a CSV file doesn’t have data for all the columns in your target database table, you can still import the data you have by specifying which columns <span epub:type="pagebreak" title="69" id="Page_69"/>are present in the data. Consider this scenario: you’re researching the salaries of all town supervisors in your state so you can analyze government spending trends by geography. To get started, you create a table called <code>supervisor_salaries</code> with the code in <a href="#listing5-4" id="listinganchor5-4">Listing 5-4</a>.</p>&#13;
<pre><code>CREATE TABLE supervisor_salaries (&#13;
    id integer GENERATED ALWAYS AS IDENTITY PRIMARY KEY,&#13;
    town text,&#13;
    county text,&#13;
    supervisor text,&#13;
    start_date date,&#13;
    salary numeric(10,2),&#13;
    benefits numeric(10,2)&#13;
);</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing5-4">Listing 5-4</a>: Creating a table to track supervisor salaries</p>&#13;
<p>You want columns for the town and county, the supervisor’s name, the date they started, and salary and benefits (assuming you just care about current levels). You’re also adding an auto-incrementing <code>id</code> column as a primary key. However, the first county clerk you contact says, “Sorry, we only have town, supervisor, and salary. You’ll need to get the rest from elsewhere.” You tell them to send a CSV anyway. You’ll import what you can.</p>&#13;
<p>I’ve included such a sample CSV you can download via the book’s resources at <a href="https://www.nostarch.com/practical-sql-2nd-edition/" class="LinkURL">https://www.nostarch.com/practical-sql-2nd-edition/</a>, called <em>supervisor_salaries.csv</em>. If you view the file with a text editor, you should see these two lines at the top:</p>&#13;
<pre><code>town,supervisor,salary&#13;
Anytown,Jones,67000</code></pre>&#13;
<p>You could try to import it using this basic <code>COPY</code> syntax:</p>&#13;
<pre><code>COPY supervisor_salaries&#13;
FROM '<var>C:\YourDirectory\</var>supervisor_salaries.csv'&#13;
WITH (FORMAT CSV, HEADER);</code></pre>&#13;
<p>But if you do, PostgreSQL will return an error:</p>&#13;
<pre><code>ERROR: invalid input syntax for type integer: "Anytown"&#13;
Context: COPY supervisor_salaries, line 2, column id: "Anytown"&#13;
SQL state: 22P04</code></pre>&#13;
<p>The problem is that your table’s first column is the auto-incrementing <code>id</code>, but your CSV file begins with the text column <code>town</code>. Even if your CSV file had an integer present in its first column, the <code>GENERATED ALWAYS AS IDENTITY</code> keywords would prevent you from adding a value to <code>id</code>. The workaround for this situation is to tell the database which columns in the table are present in the CSV, as shown in <a href="#listing5-5" id="listinganchor5-5">Listing 5-5</a>.</p>&#13;
<pre><code><span epub:type="pagebreak" title="70" id="Page_70"/>COPY supervisor_salaries <span class="CodeAnnotationCode" aria-label="annotation1">1</span> (town, supervisor, salary)&#13;
FROM '<var>C:\YourDirectory\</var>supervisor_salaries.csv'&#13;
WITH (FORMAT CSV, HEADER);</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing5-5">Listing 5-5</a>: Importing salaries data from CSV to three table columns</p>&#13;
<p>By noting in parentheses <span class="CodeAnnotation" aria-label="annotation1">1</span> the three present columns after the table name, we tell PostgreSQL to only look for data to fill those columns when it reads the CSV. Now, if you select the first couple of rows from the table, you’ll see those columns filled with the appropriate values:</p>&#13;
<pre><code>id     town      county    supervisor    start_date     salary       benefits&#13;
--   --------    ------    ----------    ----------    ----------    --------&#13;
1    Anytown               Jones                       67000.00&#13;
2    Bumblyburg            Larry                       74999.00</code></pre>&#13;
<h2 id="h1-501065c05-0005">Importing a Subset of Rows with COPY</h2>&#13;
<p class="BodyFirst">Starting with PostgreSQL version 12, you can add a <code>WHERE</code> clause to a <code>COPY</code> statement to filter which rows from the source CSV you import into a table. You can see how this works using the supervisor salaries data.</p>&#13;
<p>Start by clearing all the data you already imported into <code>supervisor_salaries</code> using a <code>DELETE</code> query.</p>&#13;
<pre><code>DELETE FROM supervisor_salaries;</code></pre>&#13;
<p>This will remove data from the table, but it will not reset the <code>id</code> column’s <code>IDENTITY</code> column sequence. We’ll cover how to do that when we discuss table design in <span class="xref" itemid="xref_target_Chapter 8">Chapter 8</span>. When that query finishes, run the <code>COPY</code> statement in <a href="#listing5-6" id="listinganchor5-6">Listing 5-6</a>, which adds a <code>WHERE</code> clause that filters the import to include only rows in which the <code>town</code> column in the CSV input matches New Brillig.</p>&#13;
<pre><code>COPY supervisor_salaries (town, supervisor, salary)&#13;
FROM '<em>C:\YourDirectory\</em>supervisor_salaries.csv'&#13;
WITH (FORMAT CSV, HEADER)&#13;
WHERE town = 'New Brillig';</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing5-6">Listing 5-6</a>: Importing a subset of rows with <code>WHERE</code></p>&#13;
<p>Next, run <code>SELECT * FROM supervisor_salaries;</code> to view the contents of the table. You should see just one row:</p>&#13;
<pre><code>id    town     county supervisor start_date  salary   benefits&#13;
-- ----------- ------ ---------- ---------- --------- --------&#13;
10 New Brillig        Carroll               102690.00</code></pre>&#13;
<p>This is a handy shortcut. Now, let’s see how to use a temporary table to do even more data wrangling during an import.</p>&#13;
<h2 id="h1-501065c05-0006"><span epub:type="pagebreak" title="71" id="Page_71"/>Adding a Value to a Column During Import</h2>&#13;
<p class="BodyFirst">What if you know that “Mills” is the name that should be added to the <code>county</code> column during the import, even though that value is missing from the CSV file? One way to modify your import to include the name is by loading your CSV into a <em>temporary table</em> before adding it to <code>supervisors_salary</code>. Temporary tables exist only until you end your database session. When you reopen the database (or lose your connection), those tables disappear. They’re handy for performing intermediary operations on data as part of your processing pipeline; we’ll use one to add the county name to the <code>supervisor_salaries</code> table as we import the CSV.</p>&#13;
<p>Again, clear the data you’ve imported into <code>supervisor_salaries</code> using a <code>DELETE</code> query. When it completes, run the code in <a href="#listing5-7" id="listinganchor5-7">Listing 5-7</a>, which will make a temporary table and import your CSV. Then, we will query data from that table and include the county name for an insert into the <code>supervisor_salaries</code> table.</p>&#13;
<pre><code><span class="CodeAnnotationHang" aria-label="annotation1">1</span> CREATE TEMPORARY TABLE supervisor_salaries_temp&#13;
    (LIKE supervisor_salaries INCLUDING ALL);&#13;
&#13;
<span class="CodeAnnotationHang" aria-label="annotation2">2</span> COPY supervisor_salaries_temp (town, supervisor, salary)&#13;
FROM '<var>C:\YourDirectory\</var>supervisor_salaries.csv'&#13;
WITH (FORMAT CSV, HEADER);&#13;
&#13;
<span class="CodeAnnotationHang" aria-label="annotation3">3</span> INSERT INTO supervisor_salaries (town, county, supervisor, salary)&#13;
SELECT town, 'Mills', supervisor, salary&#13;
FROM supervisor_salaries_temp;&#13;
&#13;
<span class="CodeAnnotationHang" aria-label="annotation4">4</span> DROP TABLE supervisor_salaries_temp;</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing5-7">Listing 5-7</a>: Using a temporary table to add a default value to a column during import</p>&#13;
<p>This script performs four tasks. First, we create a temporary table called <code>supervisor_salaries_temp</code> <span class="CodeAnnotation" aria-label="annotation1">1</span> based on the original <code>supervisor_salaries</code> table by passing as an argument the <code>LIKE</code> keyword followed by the source table name. The keywords <code>INCLUDING ALL</code> tell PostgreSQL to not only copy the table rows and columns but also components such as indexes and the <code>IDENTITY</code> settings. Then we import the <em>supervisor_salaries.csv</em> file <span class="CodeAnnotation" aria-label="annotation2">2</span> into the temporary table using the now-familiar <code>COPY</code> syntax.</p>&#13;
<p>Next, we use an <code>INSERT</code> statement<span class="CodeAnnotation" aria-label="annotation 3"> 3</span> to fill the salaries table. Instead of specifying values, we employ a <code>SELECT</code> statement to query the temporary table. That query specifies <code>Mills</code> as the value for the second column, not as a column name, but as a string inside single quotes.</p>&#13;
<p>Finally, we use <code>DROP TABLE</code> to erase the temporary table <span class="CodeAnnotation" aria-label="annotation4">4</span> since we’re done using it for this import. The temporary table will automatically disappear when you disconnect from the PostgreSQL session, but this removes it now in case we want to do another import and use a fresh temporary table for another CSV.</p>&#13;
<p><span epub:type="pagebreak" title="72" id="Page_72"/>After you run the query, run a <code>SELECT</code> statement on the first couple of rows to see the effect:</p>&#13;
<pre><code>id     town      county      supervisor   start_date    salary    benefits&#13;
--   --------    ---------   ----------   ----------   ---------  --------&#13;
11   Anytown     Mills       Jones                     67000.00&#13;
12   Bumblyburg  Mills       Larry                     74999.00</code></pre>&#13;
<p>You’ve filled the <code>county</code> field with a value even though your source CSV didn’t have one. The path to this import might seem laborious, but it’s instructive to see how data processing can require multiple steps to get the desired results. The good news is that this temporary table demo is an apt indicator of the flexibility SQL offers to control data handling.</p>&#13;
<h2 id="h1-501065c05-0007">Using COPY to Export Data</h2>&#13;
<p class="BodyFirst">When exporting data with <code>COPY</code>, rather than using <code>FROM</code> to identify the source data, you use <code>TO</code> for the path and name of the output file. You control how much data to export—an entire table, just a few columns, or the results of a query.</p>&#13;
<p>Let’s look at three quick examples.</p>&#13;
<h3 id="h2-501065c05-0007">Exporting All Data</h3>&#13;
<p class="BodyFirst">The simplest export sends everything in a table to a file. Earlier, you created the table <code>us_counties_pop_est_2019</code> with 16 columns and 3,142 rows of census data. The SQL statement in <a href="#listing5-8" id="listinganchor5-8">Listing 5-8</a> exports all the data to a text file named <em>us_counties_export.txt</em>. To demonstrate the flexibility you have in choosing output options, the <code>WITH</code> keyword tells PostgreSQL to include a header row and use the pipe symbol instead of a comma for a delimiter. I’ve used the <em>.txt</em> file extension here for two reasons. First, it demonstrates that you can name your file with an extension other than <em>.csv</em>; second, we’re using a pipe for a delimiter, not a comma, so I want to avoid calling the file <em>.csv</em> unless it truly has commas as a separator.</p>&#13;
<p>Remember to change the output directory to your preferred save location.</p>&#13;
<pre><code>COPY us_counties_pop_est_2019&#13;
TO '<var>C:\YourDirectory\</var>us_counties_export.txt'&#13;
WITH (FORMAT CSV, HEADER, DELIMITER '|');</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing5-8">Listing 5-8</a>:<code> </code>Exporting an entire table with <code>COPY</code></p>&#13;
<p>View the export file with a text editor to see the data in this format (I’ve truncated the results):</p>&#13;
<pre><code>state_fips|county_fips|region|state_name|county_name| <var>--snip--</var>&#13;
01|001|3|Alabama|Autauga County <var>--snip--</var></code></pre>&#13;
<p><span epub:type="pagebreak" title="73" id="Page_73"/>The file includes a header row with column names, and all columns are separated by the pipe delimiter.</p>&#13;
<h3 id="h2-501065c05-0008">Exporting Particular Columns</h3>&#13;
<p class="BodyFirst">You don’t always need (or want) to export all your data: you might have sensitive information, such as Social Security numbers or birthdates, that need to remain private. Or, in the case of the census county data, maybe you’re working with a mapping program and only need the county name and its geographic coordinates to plot the locations. We can export only these three columns by listing them in parentheses after the table name, as shown in <a href="#listing5-9" id="listinganchor5-9">Listing 5-9</a>. Of course, you must enter these column names precisely as they’re listed in the data for PostgreSQL to recognize them.</p>&#13;
<pre><code>COPY us_counties_pop_est_2019&#13;
    (county_name, internal_point_lat, internal_point_lon)&#13;
TO '<var>C:\YourDirectory\</var>us_counties_latlon_export.txt'&#13;
WITH (FORMAT CSV, HEADER, DELIMITER '|');</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing5-9">Listing 5-9</a>: Exporting selected columns from a table with <code>COPY</code></p>&#13;
<h3 id="h2-501065c05-0009">Exporting Query Results</h3>&#13;
<p class="BodyFirst">Additionally, you can add a query to <code>COPY</code> to fine-tune your output. In <a href="#listing5-10" id="listinganchor5-10">Listing 5-10</a> we export the name and state of only those counties whose names contain the letters <code>mill</code>, catching it in either uppercase or lowercase by using the case-insensitive <code>ILIKE</code> and the <code>%</code> wildcard character we covered in “Using LIKE and ILIKE with WHERE” in <span class="xref" itemid="xref_target_Chapter 3">Chapter 3</span>. Also note that for this example, I’ve removed the <code>DELIMITER</code> keyword from the <code>WITH</code> clause. As a result, the output will default to comma-separated values.</p>&#13;
<pre><code>COPY (&#13;
    SELECT county_name, state_name&#13;
    FROM us_counties_pop_est_2019&#13;
    WHERE county_name ILIKE '%mill%'&#13;
     )&#13;
TO '<var>C:\YourDirectory\</var>us_counties_mill_export.csv'&#13;
WITH (FORMAT CSV, HEADER);</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing5-10">Listing 5-10</a>:<code> </code>Exporting query results with <code>COPY</code></p>&#13;
<p>After running the code, your output file should have nine rows with county names including Miller, Roger Mills, and Vermillion:</p>&#13;
<pre><code>county_name,state_name&#13;
Miller County,Arkansas&#13;
Miller County,Georgia&#13;
Vermillion County,Indiana&#13;
<var>--snip--</var></code></pre>&#13;
<h2 id="h1-501065c05-0008"><span epub:type="pagebreak" title="74" id="Page_74"/>Importing and Exporting Through pgAdmin</h2>&#13;
<p class="BodyFirst">At times, the SQL <code>COPY</code> command won’t be able to handle certain imports and exports. This typically happens when you’re connected to a PostgreSQL instance running on a computer other than yours. A machine in a cloud computing environment such as Amazon Web Services is a good example. In that scenario, PostgreSQL’s <code>COPY</code> command will look for files and file paths that exist on that remote machine; it can’t find files on your local computer. To use <code>COPY</code>, you’d need to transfer your data to the remote server, but you might not always have the rights to do that.</p>&#13;
<p>One workaround is to use pgAdmin’s built-in import/export wizard. In pgAdmin’s object browser (the left vertical pane), locate the list of tables in your <code>analysis</code> database by choosing <b>Databases</b><span class="MenuArrow">▶</span><b>analysis</b><span class="MenuArrow">▶</span><b>Schemas</b><span class="MenuArrow">▶</span><b>public</b><span class="MenuArrow">▶</span><b>Tables</b>.</p>&#13;
<p>Next, right-click the table you want to import to or export from, and select <b>Import/Export</b>. A dialog appears that lets you choose to either import or export from that table, as shown in <a href="#figure5-1" id="figureanchor5-1">Figure 5-1</a>.</p>&#13;
<figure>&#13;
<img src="Images/f05001.png" alt="f05001" class="keyline" width="694" height="450"/>&#13;
<figcaption><p><a id="figure5-1">Figure 5-1</a>: The pgAdmin Import/Export dialog</p></figcaption>&#13;
</figure>&#13;
<p>To import, move the Import/Export slider to <b>Import</b>. Then click the three dots to the right of the <b>Filename</b> box to locate your CSV file. From the Format drop-down list, choose <b>csv</b>. Then adjust the header, delimiter, quoting, and other options as needed. Click <b>OK</b> to import the data.</p>&#13;
<p>To export, use the same dialog and follow similar steps.</p>&#13;
<p>In <span class="xref" itemid="xref_target_Chapter 18">Chapter 18</span>, when we discuss using PostgreSQL from your computer’s command line, we’ll explore another way to accomplish this using a utility called <code>psql</code> and its <code>\copy</code> command. pgAdmin’s import/export wizard actually uses <code>\copy</code> in the background but gives it a friendlier face.</p>&#13;
<h2 id="h1-501065c05-0009"><span epub:type="pagebreak" title="75" id="Page_75"/>Wrapping Up</h2>&#13;
<p class="BodyFirst">Now that you’ve learned how to bring external data into your database, you can start digging into a myriad of datasets, whether you want to explore one of the thousands of publicly available datasets, or data related to your own career or studies. Plenty of data is available in CSV format or a format easily convertible to CSV. Look for data dictionaries to help you understand the data and choose the right data type for each field.</p>&#13;
<p>The census data you imported as part of this chapter’s exercises will play a starring role in the next chapter, in which we explore math functions with SQL.</p>&#13;
<aside epub:type="sidebar">&#13;
<div class="top hr"><hr/></div>&#13;
<section class="box">&#13;
<h2>try it yourself</h2>&#13;
<p class="BoxBodyFirst">Continue your exploration of data import and export with these exercises. Remember to consult the PostgreSQL documentation at <a href="https://www.postgresql.org/docs/current/sql-copy.html" class="LinkURL">https://www.postgresql.org/docs/current/sql-copy.html</a> for hints:</p>&#13;
<ol>&#13;
<li value="1">Write a <code>WITH</code> statement to include with <code>COPY</code> to handle the import of an imaginary text file whose first couple of rows look like this:&#13;
<pre><code>id:movie:actor&#13;
50:#Mission: Impossible#:Tom Cruise</code></pre>&#13;
</li>&#13;
<li value="2">Using the table <code>us_counties_pop_est_2019</code> you created and filled in this chapter, export to a CSV file the 20 counties in the United States that had the most births. Make sure you export only each county’s name, state, and number of births. (Hint: births are totaled for each county in the column <code>births_2019</code>.)</li>&#13;
<li value="3">Imagine you’re importing a file that contains a column with these values:&#13;
<pre><code>17519.668&#13;
20084.461&#13;
18976.335</code></pre></li>&#13;
</ol>&#13;
<p>Will a column in your target table with data type <code>numeric(3,8)</code> work for these values? Why or why not?</p>&#13;
<div class="bottom hr"><hr/></div>&#13;
</section>&#13;
</aside>&#13;
</section>&#13;
</div></body></html>