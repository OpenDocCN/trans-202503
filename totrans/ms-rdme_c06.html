<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" epub:prefix="index: http://www.index.com/" lang="en" xml:lang="en">
	<head>
		<title>Chapter 6: Testing</title>
		<link href="NSTemplate_v1.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:7db14923-61d0-434f-baa0-3e20bf74259e" name="Adept.expected.resource"/>
	</head>
	<body epub:type="bodymatter chapter">
		<section>
			<header>
				<h1 class="chapter"><span class="ChapterNumber"><span epub:type="pagebreak" id="Page_89" title="89"/>6</span><br/><span class="ChapterTitle">Testing</span></h1>
			</header>
			<p class="BodyFirst"><span class="DropCap">W</span>riting, running, and fixing tests can feel like busywork. In fact, it’s easy for tests to <em>be</em> busywork. Bad tests add developer overhead without providing value and can increase test suite instability. This chapter will teach you to test effectively. We’ll discuss what tests are used for, different test types, different test tools, how to test responsibly, and how to deal with nondeterminism in tests.</p>
			<h2 id="h1-501836c06-0001">The Many Uses of Tests</h2>
			<p class="BodyFirst">Most developers know the fundamental function of tests: tests check that code works. But tests serve other purposes as well. They protect the code from future changes that unintentionally alter its behavior, encourage clean code, force developers to use their own APIs, document how components are to be interacted with, and serve as a playground for experimentation.</p>
			<p>
				Above all, tests verify that software behaves as expected. Unpredictable behavior causes problems for users, developers, and operators. Initially, tests show that code works as specified. Tests then remain to shield existing behavior from new changes. When an old test fails, a decision <span epub:type="pagebreak" id="Page_90" title="90"/>must be made: Did the developer intend to change behavior, or was a bug introduced?</p>
			<p>
				Test writing also forces developers to think about the interface and implementation of their program. Developers usually first interact with their code in tests. New code will have rough edges; testing exposes clumsy interface design early so it can be corrected. Tests also expose messy implementation. <em>Spaghetti code</em>, or code that has too many dependencies, is difficult to test. Writing tests forces developers to keep their code well factored by improving separation of concerns and reducing tight coupling.</p>
			<p>
				Code cleanliness side effects in tests are so strong that <em>test-driven development</em><em> (TDD</em><em>)</em> has become commonplace. TDD is the practice of writing tests before code. The tests fail when written, and then code is written to make them pass. TDD forces developers to think about behavior, interface design, and integration before cranking out a bunch of code.</p>
			<p>Tests serve as a form of documentation, illustrating how the code is meant to be interacted with. They are the first place an experienced programmer starts reading to understand a new codebase. Test suites are a great playground. Developers run tests with debuggers attached to step-through code. As bugs are discovered or questions about behavior arise, new tests can be added to understand them.</p>
			<h2 id="h1-501836c06-0002">Types of Tests</h2>
			<p class="BodyFirst">There are dozens of different test types and testing methodologies. Our goal is not to cover the full breadth of this topic but to discuss the most common types—unit, integration, system, performance, and acceptance tests—to give you a firm foundation to build on.</p>
			<p><em>Unit tests</em> verify “units” of code—a single method or behavior. Unit tests should be fast, small, and focused. Speed is important because these tests run frequently—often on developer laptops. Small tests that focus <span epub:type="pagebreak" id="Page_91" title="91"/>on a single unit of code make it easier to understand what has broken when a test fails.</p>
			<p><em>Integration tests</em> verify that multiple components work together. If you find yourself instantiating multiple objects that interact with each other in a test, you’re probably writing an integration test. Integration tests are often slower to execute and require a more elaborate setup than unit tests. Developers run integration tests less frequently, so the feedback loop is longer. These tests can flush out problems that are difficult to identify by testing standalone units individually.</p>
			<aside epub:type="sidebar">
				<div class="top hr">
					<hr/>
				</div>
				<section class="box trade">
					<h2>It’s Only Obvious in Retrospect</h2>
					<p class="BoxBodyFirst">A few years ago, Dmitriy was shopping for a new dishwasher appliance. He read online reviews, went to a store, dutifully examined all the specs, considered the trade-offs, and finally settled on the model he liked best. The salesperson who insisted on guiding Dmitriy through the aisles checked the inventory, got ready to put in an order, and, just as his hand hovered over the <span class="KeyCaps">enter</span> key, paused. “Is this dishwasher going into a corner in your kitchen, by any chance?” “Why, yes, it is.” “And is there a drawer that comes out of a cabinet at a 90-degree angle to where this dishwasher is going, such that it slides into the space right in front of the dishwasher door?” “Why, yes, there is such a drawer.” “Ah,” the salesperson said, removing his hand from the keyboard. “You will want a different dishwasher.” The model Dmitriy selected had a handle that protruded from the door, which would have completely blocked the drawer from coming out. The perfectly functioning dishwasher and the perfectly functioning cabinet were completely incompatible. Clearly, the salesperson had seen this particular integration scenario fail before! (The solution was to purchase a similar model with an inset door handle.)</p>
					<div class="bottom hr">
						<hr/>
					</div>
				</section>
			</aside>
			<p><span epub:type="pagebreak" id="Page_92" title="92"/><em>System tests</em> verify a whole system. End-to-end (e2e, for short) workflows are run to simulate real user interactions in preproduction environments. Approaches to system test automation vary. Some organizations require that system tests pass before a release, which means all components are tested and released in lockstep. Other organizations ship such large systems that synchronizing releases is not realistic; these organizations often run extensive integration tests and supplement them with continuous synthetic monitoring production tests. <em>Synthetic monitoring</em> scripts run in production to simulate user registration, browse for and purchase an item, and so on. Synthetic monitoring requires instrumentation that allows billing, accounting, and other systems to distinguish these production tests from real activity.</p>
			<p><em>Performance tests</em>, such as load and stress tests, measure system performance under different configurations. <em>Load tests</em> measure performance under various levels of load: for example, how a system performs when 10, 100, or 1,000 users access it concurrently. <em>Stress tests</em> push system load to the point of failure. Stress testing exposes how far a system is capable of going and what happens under excessive load. These tests are useful for capacity planning and defining SLOs.</p>
			<p><em>Acceptance tests</em> are performed by a customer, or their proxy, to validate that the delivered software meets acceptance criteria. These tests are fairly common in enterprise software, where formal acceptance tests and criteria are laid out as part of an expensive contract. The <em>International Standards Organization (ISO)</em> requires acceptance tests that validate explicit business requirements as part of their security standard; certification auditors will ask for evidence of documentation for both the requirements and the corresponding tests. Less formal acceptance tests, found in less regulated organizations, are variations on the theme of “I just changed a thing; can you let me know if everything still looks good?”</p>
			<aside epub:type="sidebar">
				<div class="top hr">
					<hr/>
				</div>
				<section class="box trade">
					<h2><span epub:type="pagebreak" id="Page_93" title="93"/>Testing in the Real World</h2>
					<p class="BoxBodyFirst">We looked at test setups of many successful open source projects while writing this chapter. Many projects were missing certain flavors of tests, while others were inconsistent about the separation—intermingling “unit” and “integration” tests. It’s important to know what these categories mean and the trade-offs between them. Still, don’t get too wrapped up in getting it perfectly right. Successful projects make real-world pragmatic testing decisions, and so should you. If you see an opportunity to improve the tests and test suites, by all means, do it! Don’t get hung up on naming and categorization, and refrain from passing judgment if the setup is not quite right; software entropy is a powerful force (see Chapter 3).</p>
					<div class="bottom hr">
						<hr/>
					</div>
				</section>
			</aside>
			<h2 id="h1-501836c06-0003">Test Tools</h2>
			<p class="BodyFirst">Test tools fall into several categories: test-writing tools, test execution frameworks, and code quality tools. <em>Test-writing tools</em> like mocking libraries help you write clean and efficient tests. <em>Test frameworks</em> help run tests by modeling a test’s lifecycle from setup to teardown. Test frameworks also save test results, integrate with build systems, and provide other helpers. <em>Code quality tools</em> are used to analyze code coverage and code complexity, find bugs through static analysis, and check for style errors. Analysis tools are usually set up to run as part of a build or compile step.</p>
			<p>Every tool added to your setup comes with baggage. Everyone must understand the tool, along with all of its idiosyncrasies. The tool might depend on many other libraries, which will further increase the complexity of the system. Some tools slow tests down. Therefore, avoid outside tools until you can justify the complexity trade-offs, and make sure your team is bought in.</p>
			<h3 id="h2-501836c06-0001"><span epub:type="pagebreak" id="Page_94" title="94"/>Mocking Libraries</h3>
			<p class="BodyFirst">Mocking libraries are commonly used in unit tests, particularly in object-oriented code. Code often depends on external systems, libraries, or objects. <em>Mocks</em> replace external dependencies with stubs that mimic the interface provided by the real system. Mocks implement functionality required for the test by responding to inputs with hard-coded responses.</p>
			<p>Eliminating external dependencies keeps unit tests fast and focused. Mocking remote systems allows tests to bypass network calls, simplifying the setup and avoiding slow operations. Mocking methods and objects allows developers to write focused unit tests that exercise just one specific behavior.</p>
			<p>
				Mocks also keep application code from becoming riddled with test-specific methods, parameters, or variables. Test-specific changes are difficult to maintain, make code hard to read, and cause confusing bugs (don’t add Boolean <code>isTest</code> parameters to your methods!). Mocks help developers access protected methods and variables without modifying regular code.</p>
			<p>While mocking is useful, don’t overdo it. Mocks with complex internal logic make your tests brittle and hard to understand. Start with basic inline mocks inside a unit test, and don’t write a shared mock class until you begin repeating mocking logic between tests.</p>
			<p>An excessive reliance on mocks is a code smell that suggests tight code coupling. Whenever reaching for a mock, consider whether code could be refactored to remove the dependency on the mocked system. Separating computation and data transformation logic from I/O code helps simplify testing and makes the program less brittle.</p>
			<h3 id="h2-501836c06-0002">Test Frameworks</h3>
			<p class="BodyFirst">Test frameworks help you write and execute tests. You’ll find frameworks that help coordinate and execute unit tests, integration tests, performance tests, and even UI tests. Frameworks do the following:</p>
			<ul>
				<li>Manage test setup and teardown</li>
				<li><span epub:type="pagebreak" id="Page_95" title="95"/>Manage test execution and orchestration</li>
				<li>Generate test result reports</li>
				<li>Provide tooling such as extra assertion methods</li>
				<li>Integrate with code coverage tools</li>
			</ul>
			<p>Setup and teardown methods allow developers to specify steps, such as data structure setup or file cleanup, that need to be executed before or after each test or set of tests. Many test frameworks give multiple options for setup and teardown execution—before each test, before all tests in a file, or before all tests in a build. Read documentation before using setup and teardown methods to make sure you’re using them correctly. Don’t expect teardown methods to run in all circumstances. For example, teardown won’t occur if a test fails catastrophically, causing the whole test process to exit.</p>
			<p>Test frameworks help control the speed and isolation of tests through test orchestration. Tests can be executed serially or in parallel. Serial tests are run one after the next. Running one test at a time is safer because tests have less chance of impacting one another. Parallel execution is faster but more error prone due to shared state, resource, or other contamination.</p>
			<p>Frameworks can be configured to start a new process between each test. This further isolates tests, since each test will start fresh. Beware that starting new processes for each test is an expensive operation. See “Determinism in Tests” later in this chapter for more on test isolation.</p>
			<p>Test reports help developers debug failed builds. Reports give a detailed readout of which tests passed, failed, or were skipped. When a test fails, reports show which assertion failed. Reports also organize logs and stack traces per test so developers can quickly debug failures. Beware: it’s not always obvious where test results are stored—a summary is printed to the console, while the full report is written to disk. Look in test and build directories if you have trouble locating a report.</p>
			<h3 id="h2-501836c06-0003"><span epub:type="pagebreak" id="Page_96" title="96"/>Code Quality Tools</h3>
			<p class="BodyFirst">Take advantage of tools that help you write quality code. Tools that enforce code quality rules are called <em>linters</em>. Linters run static analysis and perform style checks. Code quality monitoring tools report metrics such as complexity and test coverage.</p>
			<p><em>Static code analyzers</em> look for common mistakes, like leaving file handles open or using unset variables. Static analyzers are particularly important for dynamic languages like Python and JavaScript, which do not have a compiler to catch syntax errors. Analyzers look for known code smells and highlight questionable code but are not immune to false positives, so you should think critically about problems reported by static analyzers and override false positives with code annotations that tell the analyzer to ignore particular violations.</p>
			<p><em>Code style checkers</em> ensure all source code is formatted the same way: max characters per line, camelCasing versus snake_casing, proper indentation, that sort of thing. A consistent style helps multiple programmers collaborate on a shared codebase. We highly recommend setting up your IDE so that all style rules are automatically applied.</p>
			<p><em>Code complexity</em><em> tools</em> guard against overly complex logic by calculating <em>cyclomatic complexity</em>, or, roughly, the number of paths through your code. The higher your code’s complexity, the more difficult it is to test, and the more defects it is likely to contain. Cyclomatic complexity generally increases with the size of the codebase, so a high overall score is not necessarily bad; however, a sudden jump in complexity can be cause for concern, as can individual methods of high complexity.</p>
			<p><em>Code coverage</em><em> tools</em> measure how many lines of code were exercised by the test suite. If your change lowers code coverage, you should write more tests. Make sure that tests are exercising any new changes that you’ve made. Aim for reasonable coverage (the rule of thumb is between 65 and 85 percent). Remember that coverage alone isn’t a good measure of test quality: it can be quite misleading, both when it is high and when it is low. Checking automatically generated code like scaffolding or serialization <span epub:type="pagebreak" id="Page_97" title="97"/>classes can create misleadingly low coverage metrics. Conversely, obsessively creating unit tests to get to 100 percent coverage doesn’t guarantee that your code will integrate safely.</p>
			<p>Engineers have a tendency to fixate on code quality metrics. Just because a tool finds a quality issue doesn’t mean that it’s actually a problem, nor does it mean that it’s worth fixing immediately. Be pragmatic with codebases that fail quality checks. Don’t let code get worse, but avoid disruptive stop-the-world cleanup projects. Use Chapter 3’s “Technical Debt” section as a guide to determine when to fix code quality issues.</p>
			<h2 id="h1-501836c06-0004">Writing Your Own Tests</h2>
			<p class="BodyFirst">You are responsible for making sure your team’s code works as expected. Write your own tests; don’t expect others to clean up after you. Many companies have formal <em>quality assurance (QA)</em> teams with varying responsibilities, including the following:</p>
			<ul>
				<li>Writing black-box or white-box tests</li>
				<li>Writing performance tests</li>
				<li>Performing integration, user acceptance, or system tests</li>
				<li>Providing and maintaining test tools</li>
				<li>Maintaining test environments and infrastructure</li>
				<li>Defining formal test certification and release processes</li>
			</ul>
			<p>QA teams can help you verify your code is stable, but never “throw code over the fence” to have them do all of the testing. QA teams don’t write unit tests anymore; those days are long gone. If you are in a company with a formal QA team, find out what they are responsible for and how to engage with them. If they’re embedded within your team, they are likely attending scrum and sprint planning meetings (see Chapter 12 for more on Agile development). If they’re a centralized organization, getting their help might require opening tickets or submitting some formal request.</p>
			<h3 id="h2-501836c06-0004"><span epub:type="pagebreak" id="Page_98" title="98"/>Write Clean Tests</h3>
			<p class="BodyFirst">Write tests with the same care that you write other code. Tests introduce dependencies, require maintenance, and need to be refactored over time. Hacky tests have a high maintenance cost, which slows down future development. Hacky tests are also less stable and less likely to provide reliable results.</p>
			<p>Use good programming practices on tests. Document how tests work, how they can be run, and why they were written. Avoid hard-coded values, and don’t duplicate code. Use design best practices to maintain a separation of concerns and to keep tests cohesive and decoupled.</p>
			<p>Focus on testing fundamental functionality rather than implementation details. This helps when the codebase gets refactored, since tests will still run after the refactoring. If your test code is too tightly coupled with implementation particulars, changes to the main body of code will break tests. These breakages stop meaning something broke, and just signal that the code changed. This does not provide value.</p>
			<p>Keep test dependencies separate from your regular code dependencies. If a test requires a library to run, don’t force the entire codebase to depend on the library. Most build and packaging systems will allow you to define dependencies specifically for tests; take advantage of this feature.</p>
			<h3 id="h2-501836c06-0005">Don’t Overdo Testing</h3>
			<p class="BodyFirst">Don’t get swept up writing tests. It’s easy to lose track of which tests are worth writing. Write tests that fail meaningfully. Avoid chasing higher code coverage just to boost coverage metrics. Testing thin database wrappers, third-party libraries, or basic variable assignments is worthless even if it boosts coverage metrics. Focus on tests that have the largest effect on code risk.</p>
			<p>A failing test should tell the developer that something important has changed about the behavior of the program. Tests that fail when trivial changes are made, or when one valid implementation is replaced with another valid implementation, create busywork and desensitize the programmer. One should not need to fix the tests when the code is not broken.</p>
			<p><span epub:type="pagebreak" id="Page_99" title="99"/>Use code coverage as a guide, not a rule. High code coverage does not guarantee correctness. Exercising code in a test counts toward coverage, but it doesn’t mean that it was exercised usefully. It’s entirely possible for critical errors to exist in codebases with 100 percent test coverage. Chasing a specific code coverage percentage is myopic.</p>
			<p>Don’t handcraft tests for autogenerated code such as web framework scaffolding or OpenAPI clients. If your coverage tools aren’t configured to ignore generated code, the tools will report the code as untested. Fix the coverage tool configuration in such cases. Code generators are thoroughly tested, so testing generated code is a waste of time (unless you manually introduce changes to generated files, in which case you should test them). If for some reason you discover a real need to test generated code, figure out a way to add tests to the generator.</p>
			<p>
				Focus effort on the highest value tests. Tests take time to write and maintain. Focusing on high-value tests yields the most benefit for the cost. Use a risk matrix to find areas to focus on. A <em>risk matrix</em> defines risk as the likelihood and impact of a failure.</p>
			<p><a href="#figure6-1" id="figureanchor6-1">Figure 6-1</a> is a sample risk matrix. The likelihood of a failure is measured on the y-axis, and the impact of the failure is measured on the x-axis. The intersection of the event’s likelihood and impact defines its risk.</p>
			<figure>
				<img alt="f06001" src="image_fi/501836c06/f06001.png"/>
				<figcaption>
					<p><a id="figure6-1">Figure 6-1</a>: Risk matrix</p>
				</figcaption>
			</figure>
			<p><span epub:type="pagebreak" id="Page_100" title="100"/>Tests shift code risk down the chart—more testing makes failures less likely. Focus on high-likelihood, high-impact areas of the code first. Low-risk or throwaway code, like a proof of concept, isn’t worth testing.</p>
			<h2 id="h1-501836c06-0005">Determinism in Tests</h2>
			<p class="BodyFirst"><em>Deterministic code</em> always produces the same output for the same input. By contrast, <em>nondeterministic code</em> can return different results for the same inputs. A unit test that invokes a call to a remote web service on a network socket is nondeterministic; if the network fails, the test will fail. Nondeterministic tests are a problem that plague many projects. It’s important to understand why nondeterministic tests are bad, how to fix them, and how to avoid writing them.</p>
			<p>
				Nondeterministic tests degrade test value. Intermittent test failures (known as <em>flapping tests</em>) are hard to reproduce and debug because they don’t happen every run, or even every tenth run. You don’t know whether the problem is with the test or with your code. Because flapping tests don’t provide meaningful information, developers might ignore them and check in broken code as a result.</p>
			<p>Intermittently failing tests should be disabled or fixed immediately. Run a flapping test repeatedly in a loop to reproduce the failure. IDEs have features to run tests iteratively, but a loop in a shell also works. Sometimes the nondeterminism is caused by interactions between tests or specific machine configurations—you’ll have to experiment. Once you’ve reproduced the failure, you can fix it by eliminating the nondeterminism or fixing the bug.</p>
			<p>Nondeterminism is often introduced by improper handling of sleep, timeouts, and random number generation. Tests that leave side effects or interact with remote systems also cause nondeterminism. Escape nondeterminism by making time and randomness deterministic, cleaning up after tests, and avoiding network calls.</p>
			<h3 id="h2-501836c06-0006"><span epub:type="pagebreak" id="Page_101" title="101"/>Seed Random Number Generators</h3>
			<p class="BodyFirst"><em>Random number generators</em><em> (RNGs</em><em>)</em> must be seeded with a value that dictates the random numbers you get from it. By default, random number generators will use the system clock as a seed. System clocks change over time, so two runs of a test with a random number generator will yield different results—nondeterminism.</p>
			<p>Seed random number generators with a constant to force them to deterministically generate the same sequence every time they run. Tests with constantly seeded generators will always pass or always fail.</p>
			<h3 id="h2-501836c06-0007">Don’t Call Remote Systems in Unit Tests</h3>
			<p class="BodyFirst">Remote system calls require network hops, which are unstable. Network calls can time out, which introduces nondeterminism into unit tests. A test might pass hundreds of times and then fail once due to network timeout. Remote systems are also unreliable; they can be shut off, restarted, or frozen. If a remote system is degraded, your test will fail.</p>
			<p>Avoiding remote calls (which are slow) also keeps unit tests fast and portable. Speed and portability are critical for unit tests since developers run them frequently and locally on development machines. Unit tests that depend on remote systems aren’t portable because a host machine running a test must have access to the remote system, and remote test systems are often in internal integration test environments that aren’t easily reachable.</p>
			<p>You can eliminate remote system calls in unit tests by using mocks or by refactoring code so remote systems are only required for integration tests.</p>
			<h3 id="h2-501836c06-0008">Inject Clocks</h3>
			<p class="BodyFirst">Code that depends on specific intervals of time can cause nondeterminism if not handled correctly. External factors like network latency and CPU speed affect how long operations take, and system clocks progress <span epub:type="pagebreak" id="Page_102" title="102"/>independently. Code that waits 500 ms for something to happen is brittle. A test will pass if the code runs in 499 ms but fail when it runs in 501 ms. Static system clock methods like <code>now</code> or <code>sleep</code> signal that your code is time dependent. Use injectable clocks rather than static time methods so you can control the timing that your code sees in a test.</p>
			<p>
				The following <code>SimpleThrottler</code> Ruby class illustrates the problem. <code>SimpleThrottler</code> invokes a <code>throttle</code> method when the operation count exceeds a threshold, but the clock is not injectable:</p>
			<pre><code>class SimpleThrottler def initialize(max_per_sec=1000) @max_per_sec = max_per_sec @last_sec = Time.now.to_i @count_this_sec = 0 end def do_work @count_this_sec += 1 # ... end def maybe_throttle if Time.now.to_i == @last_sec and @count_this_sec &gt; @max_per_sec throttle() @count_this_sec = 0 end @last_sec = Time.now.to_i end def throttle # ... end
end</code></pre>
			<p>
				In the previous example, we can’t guarantee that the <code>maybe_throttle</code> condition will be triggered in a test. Two consecutive operations can take an unbounded amount of time to run if the test machine is degraded or the operating system decides to schedule the test process unfairly. Without control of the clock, it’s impossible to test the throttling logic properly.</p>
			<p><span epub:type="pagebreak" id="Page_103" title="103"/>Instead, make system clocks injectable. Injectable clocks will let you use mocks to precisely control the passage of time in your tests.</p>
			<pre><code>class SimpleThrottler def initialize(max_per_sec=1000, clock=Time) @max_per_sec = max_per_sec @clock = clock @last_sec = clock.now.to_i @count_this_sec = 0 end def do_work @count_this_sec += 1 # ... end def maybe_throttle if @clock.now.to_i == @last_sec and @count_this_sec &gt; @max_per_sec throttle() @count_this_sec = 0 end @last_sec = @clock.now.to_i end def throttle # ... end
end</code></pre>
			<p>
				This approach, called <em>dependency injection</em>, allows tests to override clock behavior by injecting a mock into the clock parameter. The mock can return integers that trigger <code>maybe_throttle</code>. Regular code can default to the regular system clock.</p>
			<h3 id="h2-501836c06-0009">Avoid Sleeps and Timeouts</h3>
			<p class="BodyFirst">Developers often use <code>sleep()</code> calls or timeouts when a test requires work in a separate thread, process, or machine to complete before the test can validate its results. The problem with this technique is that it assumes that the other thread of execution will finish in a specific amount of time, which is not something you can rely on. If the <span epub:type="pagebreak" id="Page_104" title="104"/>language virtual machine or interpreter garbage collects, or the operating system decides to starve the process executing the test, your tests will (sometimes) fail.</p>
			<p>Sleeping in tests, or setting long timeouts, also slows down your test execution and therefore your development and debugging process. If you have a test that sleeps for 30 minutes, the fastest your tests will ever execute is 30 minutes. If you have a high (or no) timeout, your tests can get stuck.</p>
			<p>If you find yourself tempted to sleep or set a timeout in a test, see if you can restructure the test so that everything will execute deterministically. If not, that’s okay, but make an honest effort. Determinism isn’t always possible when testing concurrent or asynchronous code.</p>
			<h3 id="h2-501836c06-0010">Close Network Sockets and File Handles</h3>
			<p class="BodyFirst">Many tests leak operating system resources because developers assume that tests are short lived and that the operating system will clean everything when the test terminates. However, test execution frameworks often use the same process for multiple tests, which means leaked system resources like network sockets or file handles won’t be immediately cleaned.</p>
			<p>Leaked resources cause nondeterminism. Operating systems have a cap on the number of sockets and file handles and will begin rejecting new requests when too many resources are leaked. A test that is unable to open new sockets or file handles will fail. Leaked network sockets also break tests that use the same port. Even if tests are run serially, the second will fail to bind to the port since it was opened but not closed previously.</p>
			<p>Use standard resource management techniques for narrowly scoped resources, like try-with-resource or with blocks. Resources that are shared among tests should be closed using setup and teardown methods.</p>
			<h3 id="h2-501836c06-0011"><span epub:type="pagebreak" id="Page_105" title="105"/>Bind to Port Zero</h3>
			<p class="BodyFirst">Tests should not bind to a specific network port. Static port binding causes nondeterminism: a test that runs fine on one machine will fail on another if the port is already taken. Binding all tests to the same port is a common practice; these tests will run fine serially but fail when run in parallel. Test failures will be nondeterministic since the ordering of test execution isn’t always the same.</p>
			<p>Instead, bind network sockets to port zero, which makes the operating system automatically pick an open port. Tests can retrieve the port that was picked and use that value through the remainder of the test.</p>
			<h3 id="h2-501836c06-0012">Generate Unique File and Database Paths</h3>
			<p class="BodyFirst">Tests should not write to statically defined locations. Data persistence has the same problem as network port binding. Constant filepaths and database locations cause tests to interfere with each other.</p>
			<p>
				Dynamically generate unique filenames, directory paths, and database or table names. Dynamic IDs let tests run in parallel since they will all read and write to a separate location. Many languages provide utility libraries to generate temporary directories safely (like <code>tempfile</code> in Python). Appending UUIDs to file paths or database locations also works.</p>
			<h3 id="h2-501836c06-0013">Isolate and Clean Up Leftover Test State</h3>
			<p class="BodyFirst">Tests that don’t clean up state cause nondeterminism. State exists anywhere that data persists, usually in memory or on disk. Global variables like counters are common in-memory state, while databases and files are common disk state. A test that inserts a database record and asserts that one row exists will fail if another test has written to the same table. The same test will pass when run alone on a clean database. Leftover state also fills disk space, which destabilizes the test environment.</p>
			<p><span epub:type="pagebreak" id="Page_106" title="106"/>Integration test environments are complex to set up, so they are often shared. Many tests run in parallel, reading and writing to the same datastores. Be careful in such environments, as sharing resources leads to unexpected test behavior. Tests can affect each other’s performance and stability. Shared datastores can cause tests to interfere with each other’s data. Follow our guidance in the earlier “Generate Unique File and Database Paths” section to avoid collisions.</p>
			<p>You must reset state whether your tests pass or not; don’t let failed tests leave debris behind. Use setup and teardown methods to delete test files, clean databases, and reset in-memory test state between each execution. Rebuild environments between test suite runs to rid test machines of leftover state. Tools like containers or machine virtualization make it easy to throw away entire machines and start new ones; however, discarding and starting new virtual machines is slower than running setup and teardown methods, so such tools are best used on large groups of tests.</p>
			<h3 id="h2-501836c06-0014">Don’t Depend on Test Order</h3>
			<p class="BodyFirst">Tests should not depend on a specific order of execution. Ordering dependencies usually happen when a test writes data and a subsequent test assumes the data is written. This pattern is bad for many reasons:</p>
			<ul>
				<li>If the first test breaks, the second will break, too.</li>
				<li>It’s harder to parallelize the tests, since you can’t run the second test until the first is done.</li>
				<li>Changes to the first test might accidentally break the second.</li>
				<li>Changes to the test runner might cause your tests to run in a different order.</li>
			</ul>
			<p>
				Use setup and teardown methods to share logic between tests. Provision data for each test in the setup method, and clean up the data in the <span epub:type="pagebreak" id="Page_107" title="107"/>teardown. Resetting state between each run will keep tests from breaking each other when they mutate the state.</p>
			<h2 id="h1-501836c06-0006">Do’s and Don’ts</h2>
			<table border="1" class="trade" id="tabular-501836c06-0001">
				<thead>
					<tr>
						<td><b>Do’s</b></td>
						<td><b>Don’ts</b></td>
					</tr>
				</thead>
				<tbody>
					<tr>
						<td><b>DO</b> use tests to reproduce bugs.<br/></td>
						<td><b>DON’T</b> ignore the cost of adding new testing tools.<br/></td>
					</tr>
					<tr>
						<td><b>DO</b> use mocking tools to help write unit tests.<br/></td>
						<td><b>DON’T</b> depend on others to write tests for you.<br/></td>
					</tr>
					<tr>
						<td><b>DO</b> use code quality tools to verify coverage, formatting, and complexity.<br/></td>
						<td><b>DON’T</b> write tests just to boost code coverage.<br/></td>
					</tr>
					<tr>
						<td><b>DO</b> seed random number generators in tests.<br/></td>
						<td><b>DON’T</b> depend solely on code coverage as a measure of quality.<br/></td>
					</tr>
					<tr>
						<td><b>DO</b> close network sockets and file handles in tests.<br/></td>
						<td><b>DON’T</b> use avoidable sleeps and timeouts in tests.<br/></td>
					</tr>
					<tr>
						<td><b>DO</b> generate unique filepaths and database IDs in tests.<br/></td>
						<td><b>DON’T</b> call remote systems in unit tests.<br/></td>
					</tr>
					<tr>
						<td><b>DO</b> clean up leftover test state between test executions.<br/></td>
						<td><b>DON’T</b> depend on test execution order.<br/></td>
					</tr>
				</tbody>
			</table>
			<h2 id="h1-501836c06-0007">Level Up</h2>
			<p class="BodyFirst">Many (long) books have been written on software testing. We suggest targeting specific test techniques rather than reading exhaustive test textbooks.</p>
			<p><em>Unit Testing</em> by Vladimir Khorikov (Manning Publications, 2020) is the place to go if you want more on testing best practices. It covers the philosophy of unit testing and common unit test patterns and anti-patterns. Despite its name, the book also touches on integration testing.</p>
			<p><span epub:type="pagebreak" id="Page_108" title="108"/>Kent Beck’s <em>Test-Driven Development</em> (Addison-Wesley Professional, 2002) covers TDD in detail. TDD is a great skill to have. If you find yourself in an organization that practices TDD, this book is a must.</p>
			<p>
				Look at the section on property-based testing in <em>The Pragmatic Programmer</em> by Andrew Hunt and David Thomas (Addison-Wesley Professional, 1999). We left property-based testing on the cutting-room floor, but if you want to expand your capabilities, property-based testing is a great technique to learn.</p>
			<p>
				Elisabeth Hendrickson’s <em>Explore It!</em> (Pragmatic Bookshelf, 2013) discusses exploratory testing to learn about code. If you are dealing with complex code, <em>Explore It!</em> is a good read.</p>
			<aside class="endnote" epub:type="rearnote">
</aside>
		</section>
	</body>
</html>