- en: '**5'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: THE BETA DISTRIBUTION**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This chapter builds on the ideas behind the binomial distribution from the previous
    chapter to introduce another probability distribution, the *beta distribution*.
    You use the beta distribution to estimate the probability of an event for which
    you’ve already observed a number of trials and the number of successful outcomes.
    For example, you would use it to estimate the probability of flipping a heads
    when so far you have observed 100 tosses of a coin and 40 of those were heads.
  prefs: []
  type: TYPE_NORMAL
- en: 'While exploring the beta distribution, we’ll also look at the differences between
    probability and statistics. Often in probability texts, we are given the probabilities
    for events explicitly. However, in real life, this is rarely the case. Instead,
    we are given data, which we use to come up with estimates for probabilities. This
    is where statistics comes in: it allows us to take data and make estimates about
    what probabilities we’re dealing with.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A Strange Scenario: Getting the Data**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here’s the scenario for this chapter. One day you walk into a curiosity shop.
    The owner greets you and, after you browse for a bit, asks if there is anything
    in particular you’re looking for. You respond that you’d love to see the strangest
    thing he has to show you. He smiles and pulls something out from behind the counter.
    You’re handed a black box, about the size of a Rubik’s Cube, that seems impossibly
    heavy. Intrigued, you ask, “What does it do?”
  prefs: []
  type: TYPE_NORMAL
- en: The owner points out a small slit on the top of the box and another on the bottom.
    “If you put a quarter in the top,” he tells you, “sometimes two come out the bottom!”
    Excited to try this out, you grab a quarter from your pocket and put it in. You
    wait and nothing happens. Then the shop owner says, “And sometimes it just eats
    your quarter. I’ve had this thing a while, and I’ve never seen it run out of quarters
    or get too full to take more!”
  prefs: []
  type: TYPE_NORMAL
- en: Perplexed by this but eager to make use of your newfound probability skills,
    you ask, “What’s the probability of getting two quarters?” The owner replies quizzically,
    “I have no idea. As you can see, it’s just a black box, and there are no instructions.
    All I know is how it behaves. Sometimes you get two quarters back, and sometimes
    it eats your quarter.”
  prefs: []
  type: TYPE_NORMAL
- en: '***Distinguishing Probability, Statistics, and Inference***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While this is a somewhat unusual everyday problem, it’s actually an extremely
    common type of probability problem. In all of the examples so far, outside of
    the first chapter, we’ve known the probability of all the possible events, or
    at least how much we’d be willing to bet on them. In real life we are almost never
    sure what the exact probability of any event is; instead, we just have observations
    and data.
  prefs: []
  type: TYPE_NORMAL
- en: This is commonly considered the division between probability and statistics.
    In probability, we know exactly how probable all of our events are, and what we
    are concerned with is how likely certain observations are. For example, we might
    be told that there is 1/2 probability of getting heads in a fair coin toss and
    want to know the probability of getting exactly 7 heads in 20 coin tosses.
  prefs: []
  type: TYPE_NORMAL
- en: 'In statistics, we would look at this problem backward: assuming you observe
    7 heads in 20 coin tosses, what is the probability of getting heads in a single
    coin toss? As you can see, in this example we don’t know what the probability
    is. In a sense, statistics is probability in reverse. The task of figuring out
    probabilities given data is called *inference*, and it is the foundation of statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Collecting Data***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The heart of statistical inference is data! So far we have only a single sample
    from the strange box: you put in a quarter and got nothing back. All we know at
    this point is that it’s possible to lose your money. The shopkeeper said you can
    win, but we don’t know that for sure yet.'
  prefs: []
  type: TYPE_NORMAL
- en: We want to estimate the probability that the mysterious box will deliver two
    quarters, and to do that, we first need to see how frequently you win after a
    few more tries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The shopkeeper informs you that he’s just as curious as you are and will gladly
    donate a roll of quarters—containing $10 worth of quarters, or 40 quarters—provided
    you return any winnings to him. You put a quarter in, and happily, two more quarters
    pop out! Now we have two pieces of data: the mystical box does in fact pay out
    sometimes, and sometimes it eats the coin.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given our two observations, one where you lose the quarter and another where
    you win, you might guess naively that *P*(two quarters) = 1/2\. Since our data
    is so limited, however, there is still a range of probabilities we might consider
    for the true rate at which this mysterious box returns two coins. To gather more
    data, you’ll use the rest of the quarters in the roll. In the end, including your
    first quarter, you get:'
  prefs: []
  type: TYPE_NORMAL
- en: 14 wins
  prefs: []
  type: TYPE_NORMAL
- en: 27 losses
  prefs: []
  type: TYPE_NORMAL
- en: Without doing any further analysis, you might intuitively want to update your
    guess that *P*(two quarters) = 1/2 to *P*(two quarters) = 14/41\. But what about
    your original guess—does your new data mean it’s impossible that 1/2 is the real
    probability?
  prefs: []
  type: TYPE_NORMAL
- en: '***Calculating the Probability of Probabilities***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To help solve this problem, let’s look at our two possible probabilities. These
    are just our hypotheses about the rate at which the magic box returns two quarters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0047-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To simplify, we’ll assign each hypothesis a variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0047-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Intuitively, most people would say that *H*[2] is more likely because this is
    exactly what we observed, but we need to demonstrate this mathematically to be
    sure.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can think of this problem in terms of how well each hypothesis explains
    what we saw, so in plain English: “How probable is what we observed if *H*[1]
    were true versus if *H*[2] were true?” As it turns out, we can easily calculate
    this using the binomial distribution from [Chapter 4](ch04.xhtml#ch04). In this
    case, we know that *n* = 41 and *k* = 14, and for now, we’ll assume that *p* =
    *H*[1] or *H*[2]. We’ll use *D* as a variable for our data. When we plug these
    numbers into the binomial distribution, we get the following results (recall that
    you can do this with the formula for the binomial distribution in [Chapter 4](ch04.xhtml#ch04)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0048-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In other words, if *H*[1] were true and the probability of getting two coins
    was 1/2, then the probability of observing 14 occasions where we get two coins
    out of 41 trials would be about 0.016\. However, if *H*[2] were true and the real
    probability of getting two coins out of the box was 14/41, then the probability
    of observing the same outcomes would be about 0.130.
  prefs: []
  type: TYPE_NORMAL
- en: This shows us that, given the data (observing 14 cases of getting two coins
    out of 41 trials), *H*[2] is almost 10 times more probable than *H*[1]! However,
    it also shows that neither hypothesis is *impossible* and that there are, of course,
    many other hypotheses we could make based on our data. For example, we might read
    our data as *H*[3] *P*(two coins) = 15/42\. If we wanted to look for a pattern,
    we could also pick every probability from 0.1 to 0.9, incrementing by 0.1; calculate
    the probability of the observed data in each distribution; and develop our hypothesis
    from that. [Figure 5-1](ch05.xhtml#ch05fig01) illustrates what each value looks
    like in the latter case.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/05fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-1: Visualization of different hypotheses about the rate of getting
    two quarters*'
  prefs: []
  type: TYPE_NORMAL
- en: Even with all these hypotheses, there’s no way we could cover every possible
    eventuality because we’re not working with a finite number of hypotheses. So let’s
    try to get more information by testing more distributions. If we repeat the last
    experiment, testing each possibility at certain increments starting with 0.01
    and ending with 0.99, incrementing by only 0.01 would give us the results in [Figure
    5-2](ch05.xhtml#ch05fig02).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/05fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-2: We see a definite pattern emerging when we look at more hypotheses.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We may not be able to test every possible hypothesis, but it’s clear a pattern
    is emerging here: we see something that looks like a distribution representing
    what we believe is the behavior of the black box.'
  prefs: []
  type: TYPE_NORMAL
- en: This seems like valuable information; we can easily see where the probability
    is highest. Our goal, however, is to model our beliefs in all possible hypotheses
    (that is, the full probability distribution of our beliefs). There are still two
    problems with our approach. First, because there’s an infinite number of possible
    hypotheses, incrementing by smaller and smaller amounts doesn’t accurately represent
    the entire range of possibilities—we’re always missing an infinite amount. In
    practice, this isn’t a huge problem because we often don’t care about the extremes
    like 0.000001 and 0.0000011, but the data would be more useful if we could represent
    this infinite range of possibilities a bit more accurately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, if you looked at the graph closely, you may have noticed a larger problem
    here: there are at least 10 dots above 0.1 right now, and we have an infinite
    number of points to add. This means that our probabilities *don’t sum to 1*! From
    the rules of probability, we know that the probabilities of all our possible hypotheses
    must sum to 1\. If they don’t, it means that some hypotheses are not covered.
    If they add up to more than 1, we would be violating the rule that probabilities
    must be between 0 and 1\. Even though there are infinitely many possibilities
    here, we still need them all to sum to 1\. This is where the beta distribution
    comes in.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Beta Distribution**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To solve both of these problems, we’ll be using the beta distribution. Unlike
    the binomial distribution, which breaks up nicely into discrete values, the beta
    distribution represents a continuous range of values, which allows us to represent
    our infinite number of possible hypotheses.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the beta distribution with a *probability density function (PDF)*,
    which is very similar to the probability mass function we use in the binomial
    distribution, but is defined for continuous values. Here is the formula for the
    PDF of the beta distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0050-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now this looks like a much more terrifying formula than the one for our binomial
    distribution! But it’s actually not that different. We won’t build this formula
    entirely from scratch like we did with the probability mass function, but let’s
    break down some of what’s happening here.
  prefs: []
  type: TYPE_NORMAL
- en: '***Breaking Down the Probability Density Function***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s first take a look at our parameters: *p*, α (lowercase Greek letter alpha),
    and β (lowercase Greek letter beta).'
  prefs: []
  type: TYPE_NORMAL
- en: '***p*** Represents the probability of an event. This corresponds to our different
    hypotheses for the possible probabilities for our black box.'
  prefs: []
  type: TYPE_NORMAL
- en: '**α** Represents how many times we observe an event we care about, such as
    getting two quarters from the box.'
  prefs: []
  type: TYPE_NORMAL
- en: '**β** Represents how many times the event we care about *didn’t* happen. For
    our example, this is the number of times that the black box ate the quarter.'
  prefs: []
  type: TYPE_NORMAL
- en: The total number of trials is α + β. This is different than the binomial distribution,
    where we have *k* observations we’re interested in and a finite number of *n*
    total trials.
  prefs: []
  type: TYPE_NORMAL
- en: 'The top part of the PDF function should look pretty familiar because it’s almost
    the same as the binomial distribution’s PMF, which looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0050-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the PDF, rather than *p^k* × (1 – *p*)^(*n*–*k*), we have *p*^(α–1) × (1
    – *p*)^(β–1) where we subtract 1 from the exponent terms. We also have another
    function in the denominator of our equation: the *beta* function (note the lowercase)
    for which the beta distribution is named. We subtract 1 from the exponent and
    use the beta function to *normalize* our values—this is the part that ensures
    our distribution sums to 1\. The beta function is the *integral* from 0 to 1 of
    *p*^(α–1) × (1 – *p*)^(β–1). We’ll talk about integrals more in the next section,
    but you can think of this as the sum of all the possible values of *p*^(α–1) ×
    (1 – *p*)^(β–1) when *p* is every number between 0 and 1\. A discussion of how
    subtracting 1 from the exponents and dividing by the beta functions normalizes
    our values is beyond the scope of this chapter; for now, you just need to know
    that this allows our values to sum to 1, giving us a workable probability.'
  prefs: []
  type: TYPE_NORMAL
- en: What we get in the end is a function that describes the probability of each
    possible hypothesis for our true belief in the probability of getting two heads
    from the box, given that we have observed α examples of one outcome and β examples
    of another. Remember that we arrived at the beta distribution by comparing how
    well different binomial distributions, each with its own probability *p*, described
    our data. In other words, the beta distribution represents how well all possible
    binomial distributions describe the data observed.
  prefs: []
  type: TYPE_NORMAL
- en: '***Applying the Probability Density Function to Our Problem***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When we plug in our values for our black box data and visualize the beta distribution,
    shown in [Figure 5-3](ch05.xhtml#ch05fig03), we see that it looks like a smooth
    version of the plot in [Figure 5-2](ch05.xhtml#ch05fig02). This illustrates the
    PDF of Beta(14,27).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/05fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-3: Visualizing the beta distribution for our data collected about
    the black box*'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, most of the plot’s density is less than 0.5, as we would expect
    given that our data shows that fewer than half of the quarters placed in the black
    box returned two quarters.
  prefs: []
  type: TYPE_NORMAL
- en: The plot also shows that it’s very unlikely the black box will return two quarters
    at least half the time, which is the point at which we break even if we continually
    put quarters in the box. We’ve figured out that we’re more likely to lose money
    than make money through the box, without sacrificing too many quarters. While
    we can see the distribution of our beliefs by looking at a plot, we’d still like
    to be able to quantify exactly how strongly we believe that “the probability that
    the true rate at which the box returns two quarters is less than 0.5.” To do this,
    we need just a bit of calculus (and some R).
  prefs: []
  type: TYPE_NORMAL
- en: '***Quantifying Continuous Distributions with Integration***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The beta distribution is fundamentally different from the binomial distribution
    in that with the latter, we are looking at the distribution of *k*, the number
    of outcomes we care about, which is always something we can count. For the beta
    distribution, however, we are looking at the distribution of *p*, for which we
    have an infinite number of possible values. This leads to an interesting problem
    that might be familiar if you’ve studied calculus before (but it’s okay if you
    haven’t!). For our example of α=14 and β=27, we want to know: what is the probability
    that the chance of getting two coins is 1/2?'
  prefs: []
  type: TYPE_NORMAL
- en: While it’s easy to ask the likelihood of an exact value with the binomial distribution
    thanks to its finite number of outcomes, this is a really tricky question for
    a continuous distribution. We know that the fundamental rule of probability is
    that the sum of all our values must be 1, but each of our individual values is
    *infinitely* small, meaning the probability of any specific value is in practice
    0.
  prefs: []
  type: TYPE_NORMAL
- en: 'This may seem strange if you aren’t familiar with continuous functions from
    calculus, so as a quick explanation: this is just the logical consequence of having
    something made up of an infinite number of pieces. Imagine, for example, you divide
    a 1-pound bar of chocolate (pretty big!) into two pieces. Each piece would then
    weigh 1/2 a pound. If you divided it into 10 pieces, each piece would weigh 1/10
    a pound. As the number of pieces you divide the chocolate into grows, each piece
    becomes so small you can’t even see it. For the case where the number of pieces
    goes to infinity, eventually those pieces disappear!'
  prefs: []
  type: TYPE_NORMAL
- en: Even though the individual pieces disappear, we can still talk about ranges.
    For example, even if we divided a 1-pound bar of chocolate into infinitely many
    pieces, we can still add up the weight of the pieces in one half of the chocolate
    bar. Similarly, when talking about probability in continuous distributions, we
    can sum up ranges of values. But if every specific value is 0, then isn’t the
    sum just 0 as well?
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where calculus comes in: in calculus, there’s a special way of summing
    up infinitely small values called the *integral*. If we want to know whether the
    probability that the box will return a coin is less than 0.5 (that is, the value
    is somewhere between 0 and 0.5), we can sum it up like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0053-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If you’re rusty on calculus, the stretched-out *S* is the continuous function
    equivalent to ∑ for discrete functions. It’s just a way to express that we want
    to add up all the little bits of our function (see [Appendix B](app02.xhtml#app02)
    for a quick overview of the basic principles of calculus).
  prefs: []
  type: TYPE_NORMAL
- en: 'If this math is starting to look too scary, don’t worry! We’ll use R to calculate
    this for us. R includes a function called `dbeta()` that is the PDF for the beta
    distribution. This function takes three arguments, corresponding to *p*, α, and
    β. We use this together with R’s `integrate()` function to perform this integration
    automatically. Here we calculate the probability that the chance of getting two
    coins from the box is 0.5, given the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The “absolute error” message appears because computers can’t perfectly calculate
    integrals so there is always some error, though usually it is far too small for
    us to worry about. This result from R tells us that there is a 0.98 probability
    that, given our evidence, the true probability of getting two coins out of the
    black box is less than 0.5\. This means it would not be good idea to put any more
    quarters in the box, since you very likely won’t break even.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reverse-Engineering the Gacha Game**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In real-life situations, we almost never know the true probabilities for events.
    That’s why the beta distribution is one of our most powerful tools for understanding
    our data. In the Gacha game in [Chapter 4](ch04.xhtml#ch04), we knew the probability
    of each card we wanted to pull. In reality, the game developers are very unlikely
    to give players this information, for many reasons (such as not wanting players
    to calculate how unlikely they are to get the card they want). Now suppose we
    are playing a new Gacha game called *Frequentist Fighters!* and it also features
    famous statisticians. This time, we are pulling for the Bradley Efron card.
  prefs: []
  type: TYPE_NORMAL
- en: We don’t know the rates for the card, but we really want that card—and more
    than one if possible. We spend a ridiculous amount of money and find that from
    1,200 cards pulled, we received only 5 Bradley Efron cards. Our friend is thinking
    of spending money on the game but only wants to do it if there is a better than
    0.7 probability that the chance of pulling a Bradley Efron is greater than 0.005.
  prefs: []
  type: TYPE_NORMAL
- en: Our friend has asked us to figure out whether he should spend the money and
    pull. Our data tells us that of 1,200 cards pulled, only 5 were Bradley Efron,
    so we can visualize this as Beta(5,1195), shown in [Figure 5-4](ch05.xhtml#ch05fig04)
    (remember that the total cards pulled is α + β).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/05fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-4: The beta distribution for getting a Bradley Efron card given our
    data*'
  prefs: []
  type: TYPE_NORMAL
- en: 'From our visualization we can see that nearly all the probability density is
    below 0.01\. We need to know exactly how much is above 0.005, the value that our
    friend cares about. We can solve this by integrating over the beta distribution
    in R, as earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This tells us the probability that the rate of pulling a Bradley Efron card
    is 0.005 or greater, given the evidence we have observed, is only 0.29\. Our friend
    will pull for this card only if the probability is around 0.7 or greater, so based
    on the evidence from our data collection, our friend should *not* try his luck.
  prefs: []
  type: TYPE_NORMAL
- en: '**Wrapping Up**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this chapter, you learned about the beta distribution, which is closely
    related to the binomial distribution but behaves quite differently. We built up
    to the beta distribution by observing how well an increasing number of possible
    binomial distributions explained our data. Because our number of possible hypotheses
    was infinite, we needed a continuous probability distribution that could describe
    all of them. The beta distribution allows us to represent how strongly we believe
    in all possible probabilities for the data we observed. This enables us to perform
    statistical inference on observed data by determining which probabilities we might
    assign to an event and how strongly we believe in each one: a probability of probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: The major difference between the beta distribution and the binomial distribution
    is that the beta distribution is a *continuous* probability distribution. Because
    there are an infinite number of values in the distribution, we cannot sum results
    the same way we do in a discrete probability distribution. Instead, we need to
    use calculus to sum ranges of values. Fortunately, we can use R instead of solving
    tricky integrals by hand.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Try answering the following questions to make sure you understand how we can
    use the Beta distribution to estimate probabilities. The solutions can be found
    at *[https://nostarch.com/learnbayes/](https://nostarch.com/learnbayes/)*.
  prefs: []
  type: TYPE_NORMAL
- en: You want to use the beta distribution to determine whether or not a coin you
    have is a fair coin—meaning that the coin gives you heads and tails equally. You
    flip the coin 10 times and get 4 heads and 6 tails. Using the beta distribution,
    what is the probability that the coin will land on heads more than 60 percent
    of the time?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You flip the coin 10 more times and now have 9 heads and 11 tails total. What
    is the probability that the coin is fair, using our definition of fair, give or
    take 5 percent?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data is the best way to become more confident in your assertions. You flip the
    coin 200 more times and end up with 109 heads and 111 tails. Now what is the probability
    that the coin is fair, give or take 5 percent?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
