<html><head></head><body>
<section>&#13;
<header>&#13;
<h1 class="chapter">&#13;
<span class="ChapterNumber"><span epub:type="pagebreak" title="97" id="Page_97"/>6</span><br/>&#13;
<span class="ChapterTitle">Coming in Midstream</span>&#13;
</h1>&#13;
</header>&#13;
<p class="ChapterIntro"><span class="DropCap">S</span>o far, this book has been operating under the assumption that you are <em>initiating</em> the modernization effort at your organization. We’ve considered strategy that assumes you’re on-site to do the planning in the first place. The organization might have attempted to modernize before you were employed there, but I’ve assumed that the current modernization effort is something you started. However, most modernization efforts I’ve been involved with in my career have not looked like this. Organizations tend to underestimate the amount of work and level of investment modernization requires. An unfortunate consequence of that assumption is that they do not seek out expertise until they are in trouble.</p>&#13;
<p>In my career, the number of modernization efforts I have kicked off is dwarfed by the number of modernization efforts I have parachuted into. I would love to have the luxury of participating in the planning and assessment phases, but rarely do technical leaders think that is necessary.</p>&#13;
<p><span epub:type="pagebreak" title="98" id="Page_98"/>This chapter describes what to do when you’re coming in midstream and the project is already in trouble. Activities can get messy when you’re attempting to change legacy systems, and this chapter is full of emergency “break glass here” techniques for untangling the mess.</p>&#13;
<p>When a project takes months or years of sustained commitment, no shortage of things can go wrong. In Chapter 3, I mentioned that most modernization stories begin with failure. Coming in when plans are already in motion and not going well limits your options. Pushing the reset button and going back to the drawing board may do more harm than good. A combat medic’s first job is to stop the bleeding, not order a bunch of X-rays and put together a diet and exercise plan. To be effective when you’re coming into a project that has already started, your role needs to adapt. First you need to stop the bleeding, and then you can do your analysis and long-term planning.</p>&#13;
<h2 id="h1-501188c06-0001">Finding the Bleed</h2>&#13;
<p class="BodyFirst">Of course, technology projects do not literally bleed; therefore, identifying the most urgent issues can be a challenge. In this chapter, we discuss the situations I have seen the most often, but I want us to start with some general guidance first.</p>&#13;
<p><b>Find responsibility gaps</b>. There will always be a disconnect between responsibilities formally delegated and actual responsibilities or functionality. Conway’s law tells us that the technical architecture and the organization’s structure are general equivalents, but no system is a one-to-one mapping of its organization. There are parts of the system with shared ownership, parts that no one is responsible for at all, parts where responsibilities are split in unintuitive ways. When looking for bad technology, debt, or security issues, the most productive places to mine are gaps between what two components of the same organization officially own.</p>&#13;
<p><span epub:type="pagebreak" title="99" id="Page_99"/>Organizations tend to have responsibility gaps in the following areas:</p>&#13;
<ul>&#13;
<li>So-called 20 percent projects, or tools and services built (usually by a single engineer) as a side project.</li>&#13;
<li>Interfaces. Not so much visual design but common components that were built to standardize experience or style before the organization was large enough to run a team to maintain them.</li>&#13;
<li>New specializations. Is the role of a data engineer closer to a database administrator or a data scientist?</li>&#13;
<li>Product engineering versus whatever the product runs on. Dev-Ops/site reliability engineering (SRE) didn’t solve that problem; this just moved it under more abstraction layers. If you’ve automated your infrastructure configuration, great—who maintains the automation tools?</li>&#13;
</ul>&#13;
<p>When there’s a responsibility gap, the organization has a blind spot. Debt collects, vulnerabilities go unpatched, and institutional knowledge is gradually lost.</p>&#13;
<p><b>Study the cadence, topics, and invite lists of meetings</b>. Too often, meetings are maladapted attempts to solve problems. So if you want to know what parts of the project are suffering the most, pay attention to what the team is having meetings about, how often meetings are held, and who is being dragged into those meetings. In particular, look for meetings with long invite lists. Large meetings are less effective than small meetings, but they do convincingly spread the blame around by giving everyone the impression that all parties were consulted and all opinions were explored. Meetings with ever-expanding invite lists suggest something is wrong in that area of the project.</p>&#13;
<p>Other red flags around meetings include teams that are having planning sessions longer than an hour and teams where check-in meetings are scheduled with less than 48 hours’ notice.</p>&#13;
<p><span epub:type="pagebreak" title="100" id="Page_100"/><b>Pay attention to the rhetoric of career-minded leaders</b>. It’s harsh to say it, but people react to a struggling project in basically two ways. There are the people who roll up their sleeves and focus on helping, even if helping means unglamorous work not usually part of their responsibilities, and then there are the people who spend the time they could be helping drafting excuses that explain why the failure is not their fault. Large, messy, in-progress projects will likely have a mix of both people; look for the second type. The problems they are running away from tend to be the messiest ones.</p>&#13;
<p><b>Look for compounding problems</b>. Coming in midstream means the project hasn’t officially failed yet, and what people are getting wrong, they are probably doubling and tripling down on. Projects are rarely doomed by one critical error. It’s far more likely that the organization was drowning in dysfunctional structures for months leading up to the failure.</p>&#13;
<p>All of these examples are places where natural human reactions actively make the problem worse instead of better. Having unclear responsibilities means teams feel like they are asked to pick up the slack for someone else too often. They become self-righteous and start ignoring tasks that aren’t part of their jobs as they see it, making the situation worse. Meetings slow down work, which almost always leads to more meetings. Career-minded leaders claim failure was beyond their control, implicitly blaming the team. They make their employees feel unsafe, which encourages them to avoid the problem areas as well.</p>&#13;
<p>If a project is failing, you need to earn both the trust and respect of the team already at work to course-correct. The best way to do that is by finding a compounding problem and halting its cycle. If an organization is having too many meetings, cut all of them and gradually reintroduce them one by one. If career-minded leaders are damaging psychological safety, start educating people about blameless postmortems and just culture. Talk to people and observe how the team behaves as a unit. When you can, it is always better to set up someone else for victory rather than solving the problem yourself.</p>&#13;
<p><span epub:type="pagebreak" title="101" id="Page_101"/>The rest of this chapter describes various in-progress failures I have seen and what we did to pull the project out of a death spiral.</p>&#13;
<h2 id="h1-501188c06-0002">Mess: Fixing Things That Are Not Broken</h2>&#13;
<p class="BodyFirst">We’ve already looked at a number of reasons organizations try to fix things that aren’t broken.</p>&#13;
<ul>&#13;
<li>They assume new technology is more advanced than older technology.</li>&#13;
<li>They aspire to artificial consistency.</li>&#13;
<li>They confuse success with quality.</li>&#13;
<li>They optimize past the point of diminishing returns.</li>&#13;
</ul>&#13;
<p>The preeminent target of an organization’s desire to fix things that are not broken is the monolith. A monolith in the context of software engineering is a tightly coupled application that configures a variety of functions and features so that they run on a single discrete computing resource. Monoliths were a problem that web development invented. Before the internet reached the scale that made distributed computing possible, there was little reason not to design programs to run on one machine. Lately, it seems like no engineer can bear to suffer a monolith to live. Monolith is the ultimate dirty word. Engineers complain about them endlessly. No one ever wants to admit to building one. Every successful large technical organization seems to have at least one conference talk about the heroic multiyear campaign it staged to remove a monolith.</p>&#13;
<p>But if monoliths are so awful, why do so many organizations end up with them?</p>&#13;
<p>The opposite of a monolith is service-oriented architecture. Instead of designing the application to host all its functionality on a single machine, functionality is broken up into services. Ideally, each service has a single <span epub:type="pagebreak" title="102" id="Page_102"/>goal, and typically each has its own set of computing resources. The application is created by coordinating the interaction of these services.</p>&#13;
<p>Building a product from the beginning with a service-oriented architecture is usually a mistake. Because you don’t have the proper product/market fit figured out yet, integrations and data contracts become a major pain point. A data contract is an implicit agreement written in code between two services that must communicate with one another. We call it a contract because both sides need to send and receive data in the same format for the communication to work. If the server decides to change what data it’s sending and the client is not updated accordingly, communication between the services breaks down.</p>&#13;
<p>When a team is pivoting and iterating, when the feedback loop between the customer and team is at its shortest, data contracts get broken all the time. Features get added, removed, or moved around. Assumptions get made and either validated or thrown out. Before organizations find a product market fit, they can pivot in wild and unpredictable ways. For example, YouTube started as a video dating service. Groupon started as a platform for organizing social actions. Slack started as an online multiplayer video game. Slack was actually the second time its founder had started building an online game only to realize that the real product was something completely different. His earlier startup, Flickr, had the same origin story.</p>&#13;
<p>In general, the level of abstraction your design has should be inversely proportional to the number of untested assumptions you’re making. The more abstractions a given design includes, the more difficult changing APIs without breaking data contracts becomes. The more often you break contracts, the more often a team has to stop new work and redo old work. When the product hasn’t even launched yet, forcing teams to redo work over and over again doesn’t improve the odds of success.</p>&#13;
<p>That’s why monoliths are so great during the early stages of a product. They are tightly coupled, but their complexity and level of abstraction are low. When an engineer makes a change that breaks another part of the <span epub:type="pagebreak" title="103" id="Page_103"/>system, she knows it immediately and has access to the code to fix the problem she caused.</p>&#13;
<p>Once again, focus on the balance between complexity and coupling. Complex systems have large surface areas. Every process takes more steps, and every part needs its own team to handle its maintenance correctly. The downsides of complexity can be mitigated by running more teams and facilitating communication and knowledge sharing between them. If an organization is able to do that, it can achieve the benefits that can come from making systems more complex. Well-built complex systems often allow for greater customization. They can operate at a larger scale with greater flexibility.</p>&#13;
<p>Tightly coupled systems, on the other hand, achieve flexibility by strategically breaking themselves. Every programmer has deployed at least one cheap hack to get around an API or inheritance pattern, usually tacking on a comment that reads “Ugh, do this the right way later.” Tightly coupled systems become messy because they accrue debt with each workaround that is deployed. The downsides of tight coupling can be mitigated with engineering standards dictating how to extend, modify, and ultimately play nicely with the coupling. They can also be mitigated by honoring the engineering team’s commitment to refactoring on occasion. The benefits of tight coupling are that one person can hold enough knowledge of the system in her head to anticipate behavior in a variety of conditions. The architecture is simpler and, therefore, cheaper and easier to run.</p>&#13;
<p>A system has a lifecycle. When it is new, it’s often run by a small team and has much more to gain from being tightly coupled than it does from being complex. Small teams building new things frequently throw everything out and start over. Small teams have an easier time honoring engineering standards because there are fewer people to bring to consensus. Even when small teams are at big organizations, they tend to build monoliths because the advantages of a monolith are pretty compelling when you don’t know whether what you’re building will be successful <span epub:type="pagebreak" title="104" id="Page_104"/>and need to change things fast, even if your method of changing them is poor. At small organizations, we find people are doing several different jobs at once with roles not so clearly defined. Everyone in the same space is using the same resources. In short, small organizations build monoliths because small organizations <em>are</em> monoliths.</p>&#13;
<p>Large organizations benefit more from complex systems because they have robust operational units to support them. They have the teams to run and maintain all the moving parts of the system—its platform, its monitoring, and so on. They rarely throw everything out and start over, because they operate at a scale where trying to do that would mean a major migration. Large organizations do well when they transition their monoliths to services, because the problems around communication and knowledge sharing that need to be solved to make complex systems work are problems that large organizations have to solve anyway.</p>&#13;
<p>But nobody starts a large organization, just as nobody gives birth to a teenager. They grow up, and as they grow up, the ideal point on the complexity–coupling spectrum shifts. Most monoliths will eventually have to be rethought and redesigned, but trying to pinpoint when is like trying to predict the exact moment you will outgrow a favorite sweater. Some organizations will wait too long, and some will do it too soon. Don’t believe anyone who tells you that ditching your monolith is the solution to all your problems. Monoliths can and do scale. Sometimes they are more expensive to scale, but the notion that it is impossible to scale monoliths is false. The issue is that by still having a monolith, you might be giving up benefits that could have a huge impact on operational excellence.</p>&#13;
<p>Fixing things that are not broken means you’re taking on all the risks of a modernization but will not be able to find the compelling value add and build the momentum that keeps things going. Nontechnical stakeholders will see time and money spent and not understand what the point of it was. This demoralizes engineers and violates trust with the team. Fixing the wrong thing makes it harder to secure the resources to <span epub:type="pagebreak" title="105" id="Page_105"/>finish and makes it much harder to sell the organization on future modernization efforts that might be more necessary.</p>&#13;
<h3 id="h2-501188c06-0001">Figuring Out Whether Something Needs to Be Fixed</h3>&#13;
<p class="BodyFirst">Treating monoliths as inherently bad pushes organizations into fixing them when they’re not broken. I had a friend who used to say her greatest honor was hearing a system she built had to be rewritten in order to scale it. This meant she had built something that people loved and found useful to the point where they needed to scale it. Most people in technology do not go into building a system with that expectation. The assumption is that the best way to build something is to build it in such a way that it doesn’t need any significant changes for a long time. Optimizing to minimize rewrites might seem like a sensible strategy, but if not properly reined in, it invites behavior that ultimately makes systems more brittle.</p>&#13;
<p>Neal Ford, director and software architect at ThoughtWorks, had a saying I’m fond of repeating to engineers on my teams: “Metawork is more interesting than work.” Left to their own devices, software engineers will almost invariably over-engineer things to tackle bigger, more complex, long-view problems instead of the problems directly in front of them. For example, engineering teams might take a break from working on an application to write a scaffolding tool for future applications. Rather than writing SQL queries, teams might write their own object relational mapping (ORM). Rather than building a frontend, teams might build a design system with every form component they might ever need perfectly styled.</p>&#13;
<p>Decisions motivated by wanting to avoid rewriting code later are usually bad decisions. In general, any decision made to please or impress imagined spectators with the superficial elegance of your approach is a bad one. If you’re coming into a project where team members are fixing something that isn’t broken, you can be sure they are doing so because they are afraid of the way their product looks to other people. They are <span epub:type="pagebreak" title="106" id="Page_106"/>ashamed of their working, successful technology, and you have to figure out how to convince them not to be ashamed so that they can focus on fixing things that are actually broken.</p>&#13;
<p>Set the expectation that all systems need to be rewritten eventually. Engineers at the highest level write programs that have to be revised. No one is smart enough to anticipate every new use case or feature, every advancement in hardware, or every adjustment or shift that might require code to be rewritten. What works for a large organization might suffocate a small one. Good technologists should focus on what brings the most benefit and highest probability of success to the table at the current moment, with the confidence of knowing they have nothing to prove.</p>&#13;
<p>This requires getting consensus from engineering on what it means to be broken in the first place. I’ve mentioned SLOs/SLAs before, and I will point to them again: define what level of value a system needs to bring to the user. If an ugly piece of code meets its SLO, it might not be broken, it might be just an ugly piece of code. Technology doesn’t need to be beautiful or to impress other people to be effective, and all technologists are ultimately in the business of producing effective technology.</p>&#13;
<h3 id="h2-501188c06-0002">But . . . What About Conventions?</h3>&#13;
<p class="BodyFirst">Setting the expectation that all code will eventually need to be rewritten does mean that occasionally code needs to be rewritten to bring it in line with modern conventions or to clear debt. The issue of what is worth fixing is full of nuance. When I talk about not fixing things that aren’t broken, I’m talking about not breaking up monoliths for the sake of breaking up monoliths and not rewriting code to fit the newest trends for the sake of looking good to outsiders. There are plenty of times when changes needed for long-term performance are hard to justify with existing SLOs alone. Technical debt rarely effects performance in a predictable way. A system could badly need a refactor but look fine on a monitoring dashboard until the day it falls apart all at once. In <span epub:type="pagebreak" title="107" id="Page_107"/>deciding whether to spend the time and money realigning a system with a given convention, here are some other ways to think about value add other than SLOs:</p>&#13;
<ol class="none">&#13;
<li><span class="RunInHead"><span class="Caps">Age</span></span>  The older the convention, the more likely it is to be buried deeply in various parts of a modern stack. Legacy systems that don’t conform find that the tools and options available to them get smaller and smaller.</li>&#13;
<li><span class="RunInHead"><span class="Caps">Justification</span></span>  Why do people who promote this convention promote it? Is it good security practice? Have there been well-documented cases of the convention preventing serious failure?</li>&#13;
<li><span class="RunInHead"><span class="Caps">Advocates</span></span>  Where is this convention coming from? Is it a big organization many other organizations will have to do business with?</li>&#13;
<li><span class="RunInHead"><span class="Caps">Openness</span></span>  Is the convention based on or tied to open standards? Are people blocked from adopting this convention by licensing or other proprietary issues?</li>&#13;
</ol>&#13;
<h3 id="h2-501188c06-0003">When Does Breaking Up Add Value?</h3>&#13;
<p class="BodyFirst">Since this section has spent a lot of time debunking the suggestion that monoliths are inherently bad and need to be broken up, it makes sense to close it with some advice on when to break up monoliths.</p>&#13;
<p>Monoliths can be scaled, but depending on how activity is growing, they may be difficult to scale efficiently. For example, if one part of the system is using more resources than other parts, it makes sense to change to an architecture that allows that piece to be given additional resources while not affecting the other parts of the system.</p>&#13;
<p>More often than not, monoliths are broken up because of the way the organization is scaling. If you have hundreds or even thousands of engineers contributing to the same code base, the potential for miscommunication and conflict is almost infinite. Coordinating between teams <span epub:type="pagebreak" title="108" id="Page_108"/>sharing ownership on the same monolith often pushes organizations back into a traditional release cycle model where one team tests and assembles a set of updates that go to production in a giant package. This slows development down, and more important, it slows down rollbacks that affect the organization’s ability to respond to failure.</p>&#13;
<p>Breaking up the monolith into services that roughly correspond to what each team owns means that each team can control its own deploys. Development speeds up. Add a layer of complexity in the form of formal, testable API specs, and the system can facilitate communication between those teams by policing how they are allowed to change downstream interactions.</p>&#13;
<h2 id="h1-501188c06-0003">The Compounding Problem: Diminishing Trust</h2>&#13;
<p class="BodyFirst">Large, expensive projects kicked off to fix things that are not broken break trust with the nontechnical parts of the organization. It inconveniences colleagues, frustrates them, and sometimes confuses them. A modernization effort needs buy-in beyond engineering to be successful. Spending time and money on changes that will have no visible impact on the business or mission side of operations makes it hard to secure that buy-in in the future.</p>&#13;
<p>Unfortunately, software engineers are socialized around the idea that their discipline is so difficult, nonengineers are incapable of understanding even the most basic concepts. Resistance from the nontechnical side of an organization tends to be dismissed as ignorance. That means once trust is violated, a cycle is started. The harder securing buy-in for modernization becomes, the more convinced engineering becomes that the problem is their nontechnical colleagues’ intelligence and common sense. Engineering stops even trying to speak to the values and needs of the business side of the organization. The more out-of-touch their proposals become with the organization’s needs, the less trust engineering will command.</p>&#13;
<h2 id="h1-501188c06-0004"><span epub:type="pagebreak" title="109" id="Page_109"/>Solution: Formal Methods</h2>&#13;
<p class="BodyFirst">Course-correcting a team that is fixing things that are not broken is a long process. The only thing worse than fixing the wrong thing is leaving an attempt to fix the wrong thing unfinished. Half-finished initiatives create confusing, poorly documented, and harder to maintain systems. If you’re coming in early enough that not much has been moved around, by all means, stop the team from doing what it’s doing.</p>&#13;
<p>Otherwise, you have to stay committed. Your first task has to be getting their initiative to a place where you can stop work without creating a Frankenstein monster. Once you’ve figured out where that point is, the next challenge is figuring out how to tack on value to the process so that the organization can recover from its mistake stronger.</p>&#13;
<p>Monolith breakups and other large-scale redesigns offer an opportunity to change process as well as change code. A silver lining in fixing something that is not broken can be found in treating the fix as an opportunity to experiment with and improve engineering practices. If the organization lacks proper testing, take the opportunity to build out and mature test suites. If the organization doesn’t have monitoring, consider what tools might work for the new architecture. If the organization has never done incident response or on-call rotations, use the creation of new services to establish those practices.</p>&#13;
<p>If the organization does all of these things already, introduce formal methods.</p>&#13;
<p><em>Formal methods</em> are techniques for applying mathematical checks to software designs to prove their correctness. In attempting to prove correctness, formal methods can highlight bugs that would otherwise be impossible to find just by studying the code. The most accessible form of formal methods is called <em>formal specification</em>. It consists of writing out the design as a specification with a markup language that a model checker can parse and run analysis on. These model checkers take the valid inputs defined by the spec and map out every possible combination of output <span epub:type="pagebreak" title="110" id="Page_110"/>based on the design. Then they compare all those possible outputs to the rules the spec has defined for valid outputs, looking for a result that violates the assertions of the spec.</p>&#13;
<p>As of this writing, formal methods are not commonly used by software teams. The learning curve is steep, and resources for beginners are practically nonexistent. The community of users itself is small and skewed slightly toward academia. However, an engineering team doesn’t need everyone to know how to write a spec to start using formal methods. An organization can start with just one engineer who works with other teams to draft and refine specs, the same way engineering teams often have a small pool of designers they work with to draft and refine UX.</p>&#13;
<p>Formal methods help engineering teams consider a broader array of conditions and scaling factors. They also improve communication between teams by giving everyone a reference detailing the design and expected behaviors of a system.</p>&#13;
<p>If you can’t find anyone who can make sense of TLA+ syntax or Alloy or Petri nets, one slightly easier way to begin introducing formal methods is with contract testing. <em>Contract testing</em> is a form of automated testing that checks whether components of a system have broken their data contracts with one another. When breaking up a monolith into services, honoring these contracts or clearly communicating when they need to be broken is essential to building, integrating, and maintaining a high-performing system. Contract testing is not a form of formal specification per se, but rolling it out follows roughly the same process. It requires every endpoint to have a spec written in a specific markup language that the contract testing tool can parse and check for inconsistencies.</p>&#13;
<p>Strongly typed languages sometimes can do contract testing without any additional tools if repositories are set up correctly. For example, if the service owner is responsible for writing the endpoints, the client libraries, and the mocks of the service for testing, they can test for breaking changes on their own.</p>&#13;
<h2 id="h1-501188c06-0005"><span epub:type="pagebreak" title="111" id="Page_111"/>Mess: Forgotten and Lost Systems</h2>&#13;
<p class="BodyFirst">Large organizations lose systems. I don’t mean the systems go down; I mean the organizations forget they have them and occasionally lose the records of their existence. Entire product lines are designed to handle this problem: searching for VMs on networks, transversing connections, inspecting dependencies, and managing inventory. It’s amazing how common it is, because this seems like something that just shouldn’t happen. How can an organization continue to spend money on something it does not know exists?</p>&#13;
<p>When an organization is in startup mode, it typically has a small engineering team that handles basically everything. Groups then constantly break off and reform as the architecture is built out. At some point, an organization likely will start to create divisions and delegate ownership, but that’s a game of musical chairs that will often leave some parts of the architecture without a seat when the music stops.</p>&#13;
<p>Software without maintainers is a key place to find all kinds of monsters, but how do you find what is unowned and forgotten?</p>&#13;
<p>One potential approach is to trace the activities of the engineers who were around when things were small. In those early days, strong engineers tend to hop from project to project, applying themselves wherever urgency and interest coincide. Not much thought is likely given to transition planning, because the software is new and would be stable for a while without much in the way of maintenance. If the software is particularly well made, it might slip into obscurity, quietly humming away completely unnoticed because it has never seemed to need maintenance before. Trace the movements of those early engineers as the software was originally being built. What did they touch, and who owns it now?</p>&#13;
<p>Another option is to follow the money. Forgotten services still consume resources the organization must pay for. At the least, some record of those transactions should exist. If you’re using a commercial cloud provider, start tagging your instances automatically. Doing so will highlight images that are unaccounted for.</p>&#13;
<h2 id="h1-501188c06-0006"><span epub:type="pagebreak" title="112" id="Page_112"/>The Compounding Problem: Crippling Risk Avoidance</h2>&#13;
<p class="BodyFirst">When an architecture is so complex or so old that entire pieces of it are forgotten, engineers can feel as if they are working in a minefield. No one plans effectively for the unknown unless they plan effectively for failure. Without the ability to accept and adapt to failure, the unknown traps individual contributors in a catch-22. Changing a system with unclear boundaries and missing components is likely to trigger an outage. Not taking action increases the odds of failure eventually, but not failure that can be traced to one particular decision or action.</p>&#13;
<p>Engineers make decisions that are worse for the health of systems overall but are less likely to trigger outages that they can be blamed for as individuals. Maintaining the system becomes a game of hot potato, with every passing year increasing the risks to greater and greater extremes. Although many of the engineers caught in this trap understand they are choosing the worst possible outcome for everyone, the level of complexity of the system makes it impossible for them ever to feel like they know the system well enough to change it safely.</p>&#13;
<h2 id="h1-501188c06-0007">Solution: Chaos Testing</h2>&#13;
<p class="BodyFirst">Ultimately, you must accept that it might not be possible to track down and account for all systems. Even when you find them, figuring out exactly what they do could be difficult. If you’re coming into a project with an organization that has forgotten systems, you’re probably dealing with a team that is paralyzed by this reality. The engineers might have gotten stuck in the planning phase as they try fruitlessly to figure out whether the latest inventory is correct. They are probably scared of deploying any changes at all to any system, for fear of finding another forgotten system that’s also a critical dependency.</p>&#13;
<p>You have to be comfortable with the unknown. You can do that by emphasizing <em>resilience</em> over reliability. Reliability is important, but too <span epub:type="pagebreak" title="113" id="Page_113"/>many organizations use reliability to aim for perfection, which is the exact opposite of what they should be trying to accomplish. Site reliability engineers typically talk about performance in terms of number of nines—that is, whether a service is up and running 99.9 percent of the time (three nines), 99.99 percent of the time (four nines), or 99.999 percent of the time (five nines). Since these numbers are calculated as part of SLAs and since SLAs are written into the contract between the organization and its customer, nontechnical people in the organization tend to misunderstand the value of the number of nines. More nines are not always better.</p>&#13;
<p><em>Five nines</em> means a service has fewer than 5.25 minutes of downtime per year. So if something goes wrong, an engineer has only a few minutes to wake up, log on, diagnose, and fix it. And even if she is capable of pulling that off, failure can happen only once a year. A former colleague of mine and an experienced engineer from Google used to like to say, “Anything over four nines is basically a lie.” The more nines you are trying to guarantee, the more risk-averse engineering teams will become, and the more they will avoid necessary improvements. Remember, to get five nines or more, they have only seconds to respond to incidents. That’s a lot of pressure.</p>&#13;
<p>SLAs/SLOs are valuable because they give people a budget for failure. When organizations stop aiming for perfection and accept that all systems will occasionally fail, they stop letting their technology rot for fear of change and invest in responding faster to failure. That’s the idea anyway. Some organizations can’t be talked out of wanting five or even six nines of availability. In those cases, <em>mean time to recovery (MTTR)</em> is a more useful statistic to push than reliability. MTTR tracks how long it takes the organization to recover from failure.</p>&#13;
<p>When we encountered systems that had been forgotten and we couldn’t figure out what they were doing, we would usually just turn them off and see what happened. For an older generation of technologists, this seems reckless, but modern-day engineering teams refer to this <span epub:type="pagebreak" title="114" id="Page_114"/>practice as <em>chaos testing</em>. Resilience in engineering is all about recovering stronger from failure. That means better monitoring, better documentation, and better processes for restoring services, but you can’t improve any of that if you don’t occasionally fail.</p>&#13;
<p>The rationale around provoking failure deliberately is that if something unexpected does happen, it happens when everyone is on high alert and at a time the organization scheduled specifically for that purpose. When we turned off a system, we waited for someone to complain. That person was either the system owner or the owner of a downstream dependency, but either way, we ended the experiment with more information about what the system was doing than we started with.</p>&#13;
<p>If no one complained, we tended just to leave the system off and move on. Having one less component to modernize was still a win. Do we sometimes find out months later that the system we turned off was in fact doing something essential? I won’t lie; it does occasionally happen, but that’s why investing in testing and monitoring is so important for systems at scale of any age. If something is important enough to build a component specifically to do it, there should be some way of alerting system owners when it doesn’t happen.</p>&#13;
<h2 id="h1-501188c06-0008">Mess: Institutional Failures</h2>&#13;
<p class="BodyFirst">If a bad pattern is used in one part of a system, it’s everywhere in the system. Sometimes an organization doesn’t know that string concatenation on database queries is a bad idea (for example). More likely, the bad pattern you’re seeing is a result of shifting norms around technical best practices. Remember the days when Facebook thought HTTPS could be optional? What would have been secure practice a few years ago is already riddled with easily exploitable holes.</p>&#13;
<p>It stands to reason, therefore, that if you have a piece of software no one has put much thought into maintaining for a few years, there are going to be problems, and those problems are going to be systemic. They <span epub:type="pagebreak" title="115" id="Page_115"/>will be patterns repeated throughout the system. What reason would the engineering team have to do things differently?</p>&#13;
<p>Lately, I’ve been seeing this kind of rot taking hold within months, rather than years. Particularly on security issues, the turnaround between secure and cracked seems to grow shorter and shorter all the time. If no one has touched something in six months, that is a good place to start the search for problems.</p>&#13;
<p>Once you’ve found a problem, the next step is to determine whether it’s a pattern or just a mistake. Security vulnerabilities from out-of-date dependencies are obviously not a pattern. Accidentally removing something that was once in the code is not a pattern. Not escaping inputs, storing secrets in plaintext, returning more information than the requester needs—those are patterns.</p>&#13;
<p>Code-checking software can sometimes be useful in tracking down all the instances of a bad pattern. But some problems do not reveal themselves easily and require actual human beings. If you’ve found such a problem, the first thing to do is define the context around the code. What is it doing? What type of requests trigger it, and what processes and services does it call? The nice thing about patterns is that if you know their context, you can predict them. If a piece of bad code calls a database, the natural place to look for other pieces of bad code is other places that call that database.</p>&#13;
<p>In the worst-case scenario, the problems cross application boundaries. Part of analyzing the context of a bad pattern should be its providence. In other words, who built this thing? If the same team built two applications at about the same time, it’s unlikely completely different development practices were used.</p>&#13;
<h2 id="h1-501188c06-0009">The Compounding Problem: No Owners</h2>&#13;
<p class="BodyFirst">The trouble with systemic issues, whether they’re in the code base or the culture, is that no one actually owns them. If they affect everyone and <span epub:type="pagebreak" title="116" id="Page_116"/>everyone participates in them, the only people with the authority to fix them are the people the least equipped to do so. A CEO or cabinet secretary isn’t going to have much luck neglecting their responsibilities in order to dig into implementation issues of one system, no matter how large or critical. Such a leader could delegate the responsibility to a more tactical subordinate, but that appointee would likely find themselves fighting endless political battles.</p>&#13;
<p>Problems that impact multiple organizational units require coordination across those boundaries to fix. The more importance an organization gives those boundaries—building budgets and hiring cycles around them—the more people at the top of those units will police their boundaries. This sets up political battles that are often self-reinforcing. Leaders have their fiefdoms. They fought hard for the resources they have. If they reroute even a small portion of those resources to institutional problems while their peers ignore the problem and the problem is not solved, those resources could be permanently forfeited. When there is no precedent for cross-functional collaboration, who will take on the risk of being the first mover?</p>&#13;
<h2 id="h1-501188c06-0010">Solution: Code Yellow</h2>&#13;
<p class="BodyFirst">Systemic problems almost always appear midstream. When we find them, I like to document these issues for the wider organization as <em>BOLOs</em> (for <em>be on the lookouts</em>). We send out a short announcement explaining the problem in plain English, pointing to specific examples we have found and establishing a point of contact on our team for other teams to reach out to if they find similar issues. If the problem is particularly serious, we will set up short talks about the issue, demonstrating what the bad code looks like, how to recognize it, and describe appropriate and inappropriate fixes. Sometimes we reach out to other teams specifically.</p>&#13;
<p>Broadly, these techniques are part of a methodology called <em>Code Yellow</em>, which is a cross-functional team created to tackle an issue <span epub:type="pagebreak" title="117" id="Page_117"/>critical to operational excellence. The term <em>Code Yellow</em> refers both to the team and the process that governs the team’s activities. This was a practice developed at Google to handle issues that were beyond the scope of what any one part of the organization owned. And unlike other processes at Google, it didn’t end up documented and commented on in a thousand different management books, so the only people who seem to know what a Code Yellow is or how to run one are former Google people or individuals trained by former Google people. It has spread to other engineering organizations through oral tradition in that way.</p>&#13;
<p>The purpose of a Code Yellow is to create momentum. When a legacy system has performance, stability, or security issues that are both systemic and entangled with other issues, it can be overwhelming and demoralizing. Nobody makes things better, because everybody becomes distracted by the total volume of problems. No single improvement feels like it will make enough of an impact to turn the tide.</p>&#13;
<p>Code Yellows have the following critical features that ensure their success over other project management approaches:</p>&#13;
<p><b>The Code Yellow leader has escalated privileges</b>. The leader gets to commandeer any and all resources needed for the Code Yellow effort. This includes people, conference rooms, offices, and so on. The leaders can pull these resources off of other teams without approval from the normal chain of command and without lengthy explanation or discussion.</p>&#13;
<p><b>The leader serves as a central point of contact for the effort</b>. Code Yellow issues are often both systemic and sensitive in nature. The organization may not know the full scope of the issue when declaring the Code Yellow. By creating a central point of contact, teams across the organization can refer issues to the Code Yellow leader and receive clear and specific guidance. Unrelated issues can be diagnosed and dispatched easily.</p>&#13;
<p><b>The team is small</b>. Team composition may change as the leader pulls in experts from other teams and releases them, but the size of the team at any one point in time stays less than eight people. Those people should <span epub:type="pagebreak" title="118" id="Page_118"/>be able to implement solutions; they are not simply representatives from other teams.</p>&#13;
<p><b>The team is focused</b>. When assigned to a Code Yellow, team members are relieved of all other roles and responsibilities so they can focus their energies 100 percent on the Code Yellow.</p>&#13;
<p><b>The Code Yellow is temporary</b>. Before declaring a Code Yellow, the organization should set success criteria. At what level of improvement is the situation no longer critical and the remaining work can be placed on the road maps of existing teams with proper prioritization? Code Yellows can last for months, but they should not run for quarters. The temporary nature of a Code Yellow is what helps conquer the political rivalries that otherwise make systemic problems harder to solve. A Code Yellow guarantees that only resources that are urgently needed will be commandeered and that they will be returned as soon as the crisis is over.</p>&#13;
<p>An issue warrants a Code Yellow if it is urgent and the scope is beyond what one cohesive unit of an organization can handle. That usually means a security or a system reliability problem. Occasionally Code Yellows can be used for more nuanced issues that affect the organization’s overall competitiveness. In 2008, Google called a Code Yellow after internal studies demonstrated how latency negatively affected users’ long-term behaviors:</p>&#13;
<blockquote>&#13;
<p><em>One might think that the minuscule amounts of latency involved in the experiment would be negligible—they ranged between 100 and 400 milliseconds. But even those tiny hiccups in delivering search results acted as a deterrent to future searches. The reduction in the number of searches was small but significant, and were measurable even with 100 milliseconds (one-tenth of a second) latency. What’s more, even after the delays were removed, the people exposed to the slower results would take a long time to resume their previous level of searching.</em></p>&#13;
<p><em>[. . .]</em></p>&#13;
<p><em><span epub:type="pagebreak" title="119" id="Page_119"/>This Code Yellow kicked off at a TGIF where Hölzle metered the performance of various Google products around the world, with a running ticker on the big screen in Charlie’s Café pinpointing the deficiencies. “You could hear a pin drop in the room when people were watching how stunningly slow things were, like Gmail in India,” says Gabriel Stricker, a Google PR director.<sup class="FootnoteReference"><a id="c06-footnoteref-1" href="#c06-footnote-1">1</a></sup></em></p>&#13;
</blockquote>&#13;
<p>In 2010, another Code Yellow was called to deal with the aftermath of Operation Aurora, a Chinese government cyberattack that rooted Google’s corporate network and allowed Chinese intelligence to steal information.</p>&#13;
<p>In 2015, the Chromium team (the open source project that backs Google Chrome) called a Developer Productivity Code Yellow to improve performance so that it would be easier to attract and retain contributors.</p>&#13;
<p>All of these are critical issues, but they all look different. They have different scopes. Only one presented itself as a traditional crisis. But in each case, the problem would have been difficult for a single team or solitary division to solve. By building a small, empowered team to start off the response, Google was able to create focus and momentum that made impossible problems seem solvable.</p>&#13;
<p>Code Yellows end when the issue is out of the critical stage, not when the problem is fully resolved. Part of the Code Yellow should be developing a plan for executing on long-term improvements, upgrades, and development work. How is post–Code Yellow work assigned? Who holds people accountable? The composition of the Code Yellow team should reflect this; if the long-term work will involve specific teams, members of those teams should be part of the Code Yellow. At the end of the Code Yellow, those members return to their teams and continue the Code Yellow work as part of their regular road maps.</p>&#13;
<p><span epub:type="pagebreak" title="120" id="Page_120"/>It’s worthwhile for leadership to have a high tolerance for risk when defining the line of criticality. Code Yellows become less effective the longer and less urgent the work becomes. Having a good plan of how the work will be completed once normal status is restored and holding people accountable produces a better outcome than depending on a small elite team to save you.</p>&#13;
<h3 id="h2-501188c06-0004">Calling a Code Yellow</h3>&#13;
<p class="BodyFirst">Code Yellows are declared by the lowest-level leader, not the highest. They should be declared only by leaders who have authority over all the affected parts of the organization. In a small organization, Code Yellows are usually called by someone very senior, but as the organization grows, this practice becomes inefficient and bureaucratic. By granting the authority to the lowest-level leader with authority over all affected areas, the organization is able to continue to move quickly in response to critical issues.</p>&#13;
<p>In other words, if the problem is an engineering issue that spans teams under multiple directors, the person with the authority to call a Code Yellow is the vice president of engineering to whom those directors report. If the issue also involves teams of another VP, the person with the authority is one level up from the VPs.</p>&#13;
<p>Sometimes the scope of a Code Yellow changes to affect a larger part of the organization as the team uncovers more details. In that case, it is not customary to relitigate the decision to call the Code Yellow itself, although some adjustments to communication strategy and success criteria may be warranted. Status meetings may be expanded to include leaders from other groups, for example.</p>&#13;
<p>Code Yellows are not generally called by engineering managers or directors because the scope of their field of influence should be small enough to manage the problem via other project management strategies. Code Yellows are for systemic problems; a problem that fits entirely within the domain of a single engineering manager without touching or affecting any other group is not systemic.</p>&#13;
<h3 id="h2-501188c06-0005"><span epub:type="pagebreak" title="121" id="Page_121"/>Running a Code Yellow</h3>&#13;
<p class="BodyFirst">The Code Yellow leader plays a similar role to that of an incident commander, assigning tasks to team members and serving as the final decision-maker. They need to have enough technical knowledge and implementation experience to do so with confidence. They also need to be able to devote 100 percent of their attention to the Code Yellow. For these reasons, senior leaders are usually a poor choice to run a Code Yellow. Clearing their calendars to focus on a Code Yellow blocks other important things in the organization. They might not understand how the product or architecture works with the detail necessary to make quick decisions. However, the leader need not necessarily be from engineering. Product managers can make excellent Code Yellow leaders, as can staff engineers and principal engineers.</p>&#13;
<p>Ideally, the Code Yellow leader should have enough experience with the organization to know who the experts are, which teams own which parts of the product, and so on and so forth. This allows them to keep the team small and limit the amount of discussion around identifying resources.</p>&#13;
<p>When declaring a Code Yellow, it’s important that the wider organization be made aware of it. If the organization is large, it may not be necessary to broadcast an announcement about the Code Yellow to all employees, but every team that might have relevant information or resources that will be reassigned in the Code Yellow needs to know. This helps smooth the path for the leader when he or she approaches other teams.</p>&#13;
<p>Because Code Yellows tend to be sensitive, it’s not necessary to provide a great deal of detail in the announcement. If the Code Yellow was triggered by issues, tickets, or discussions that are accessible to everyone, the announcement should link to those internal conversations for reference. Otherwise, the Code Yellow announcement can just define the scope as it is known at the time (for example, “We’re declaring a Code Yellow on application security”).</p>&#13;
<p><span epub:type="pagebreak" title="122" id="Page_122"/>The Code Yellow announcement must clearly identify who the leader is, generally with a statement like “John Doe may reach out to you about this.”</p>&#13;
<p>Part of the leader’s responsibility during a Code Yellow is handling communication about the Code Yellow, which includes keeping leadership briefed on progress. Although Code Yellows can be stressful, the more time the leader spends in status update meetings with senior leadership about the Code Yellow, the less time that person spends working on resolving the Code Yellow.</p>&#13;
<p>Daily 5- to 15-minute standup calls strike a happy medium but are not required. Some organizations will create either a physical or remote “war room” where the Code Yellow team members operate. If the organization’s monitoring tools are robust enough to handle it without significant engineering effort, setting up dashboards to track key metrics around the Code Yellow can help keep everyone focused.</p>&#13;
<h2 id="h1-501188c06-0011">Mess: Leadership Has Lost the Room</h2>&#13;
<p class="BodyFirst"><em>Losing the room</em> is a sports term. It means a coach has lost the respect of his or her players. The team, instead of following orders and working together, struggles to self-organize.</p>&#13;
<p>This book spends a lot of time discussing value and momentum because success with legacy modernization is less about technical implementation and more about the morale of the team doing the modernizing. So, what can you do if the team you’re taking over is so demoralized they won’t listen to you long enough to exercise other techniques presented in this book?</p>&#13;
<p>People are often too quick to equate morale issues with character flaws. Incentives play a much larger role in who’s effective at an organization than some fanciful notion of their character. Organizations that refuse to take responsibility for the situations in which they put their own employees struggle to achieve operational excellence. They discover <span epub:type="pagebreak" title="123" id="Page_123"/>they possess a unique ability to find and hire the few bad apples in a pool of hundreds of candidates. They watch talent with options walk away and complain about the lack of loyalty, integrity, or mental toughness.</p>&#13;
<p>Remember, no one wants to suck at their job. Popular culture sells the myth about lazy, stupid, uncaring bureaucrats. It’s easy to dismiss people that way. Buying into the idea that those kinds of problems are really character flaws means you don’t have to recognize that you’ve created an environment where people feel trapped. They are caught between conflicting incentives with no way to win.</p>&#13;
<p>An organization doesn’t have to be part of the government to be a bureaucracy. When a leader has lost the room, it is usually because the organization has pushed the engineering team back into a place where it is not possible to succeed. It’s important not to lose sight of the element of bad faith in this outcome. Teams don’t reject their leaders because a project fails or even because a project fails multiple times. Teams reject their leaders when they feel that success was snatched from them. Either they made a real contribution that was ignored or credited to someone else, or their efforts to achieve operational excellence were sabotaged by the leadership in the organization.</p>&#13;
<p>Sometimes you can restore trust and bring a team back from the dead just by changing the scenery. Remove old leaders when teams lose faith in them and replace them with new leaders who will gain their trust rather than assume they have it.</p>&#13;
<p>But the longer a situation of bad faith is allowed to continue, the deeper the psychological roots grow. The lack of trust in an organization and its leadership can diminish the trust a team has in themselves. Being betrayed by your own leadership is traumatic. One of the ways people process that trauma is by wondering if they deserved it. Removing rejected leaders might solve superficial problems, but it doesn’t restore teams back to excellence.</p>&#13;
<p>People without confidence self-sabotage. They create self-fulfilling prophecies and display signs of learned helplessness. For example, I had <span epub:type="pagebreak" title="124" id="Page_124"/>a team once that experienced a high rate of failed deploys, triggering some problem in production at least once a week that required a rollback. The organization had also been leaning on them to produce more and more while cutting their staff and restricting their resources. I took over that team after their old manager was fired, and it was obvious from our first few conversations that the problem wasn’t their engineering skills. They had been asked to improve a piece of legacy technology that had not been updated in a while. It had almost no testing, no monitoring, and a complex deploy process.</p>&#13;
<p>The reason the legacy system had not been updated in a while was because the organization had been regularly refusing requests to staff a team for the task or invest anything significant in resources. To top it off, the whole infrastructure for this system that processed millions of transactions had been maintained for years by one person.</p>&#13;
<p>The team members were completely demoralized. They had lost faith in their ability to ship code safely, so they backed off larger, more creative solutions to technical challenges that might have helped them. They became resigned to their situation, as if outages were inevitable.</p>&#13;
<p>They didn’t test things. When things went wrong, they didn’t do a thorough investigation and confirm what the failure points had been. They avoided those things not because they didn’t understand that they were important, but because they had lost faith in their own abilities. After so many failures and years of denied resource requests, they felt other people in the organization assumed they were bad engineers and were desperate to avoid confirming that.</p>&#13;
<p>That scenario might sound counterintuitive. If they were so scared of failing, they should have tested more, investigated deeper. Why would they stick with a process that they knew was bad and would increase their likelihood of failure? Like Schrödinger’s cat, if they don’t have a proper process, they could be both alive and dead at the same time. If they don’t have a proper process, they never have to confront the potential reality <span epub:type="pagebreak" title="125" id="Page_125"/>that they are just bad engineers. It is always possible that a better process would fix all their problems.</p>&#13;
<p>However, if they implemented a better process and still failed anyway, they would lose this mental lifeline they were hanging on to. The team doomed itself to failure because they were afraid of learning that the problem the whole time was them—not their process, not the organization’s denial of resources, not the inexperienced manager.</p>&#13;
<h2 id="h1-501188c06-0012">The Compounding Problem: Self-Sabotaging Teams</h2>&#13;
<p class="BodyFirst">Confidence comes before success. Success rarely creates confidence. When teams don’t have confidence in themselves, they will always find something to debunk successful outcomes. They got lucky. The outcome wasn’t as good as it should have been or could have been had another team been in charge. The successful outcome did not outweigh past failures.</p>&#13;
<p>When people can’t accept successful outcomes, they tend to avoid success completely. They self-sabotage because the status quo is safe.</p>&#13;
<p>Confidence problems are always compounding. The only thing that convinces people to stop belittling themselves is knowing they have the trust and acceptance of their peers.</p>&#13;
<h2 id="h1-501188c06-0013">Solution: Murder Boards</h2>&#13;
<p class="BodyFirst">A murder board is a technique I picked up in government and repurposed for engineering teams. In government, we used them to prep people for Congressional testimony or confirmation hearings, but applying them to a technical challenge was not completely unheard of. NASA’s Ames Research Center uses them for satellite launches and requesting funding for research.</p>&#13;
<p><span epub:type="pagebreak" title="126" id="Page_126"/>The way a murder board works is you put together a panel of experts who will ask questions, challenge assumptions, and attempt to poke holes in a plan or proposal put in front of them by the person or group the murder board exercise is intended to benefit. It’s called a murder board because it’s supposed to be combative. The experts aren’t just trying to point out flaws in the proposal; they are trying to outright <em>murder</em> the ideas.</p>&#13;
<p>Murder boards are one of those techniques that are really appropriate only in specific circumstances. To be a productive and beneficial exercise, it is essential that the murder board precedes an extremely stressful event. Murder boards have two goals. The first is to prepare candidates for a stressful event by making sure they have an answer for every question, a response to every concern, and mitigation strategy for every foreseeable problem. The second goal of a murder board is to build candidates’ confidence. If they go into the stressful event knowing that they survived the murder board process, they will know that every aspect of their plan or testimony has been battle-tested.</p>&#13;
<p>I scheduled a murder board for my team with the bad process because I understood that before anything else could get better, the team members needed to learn that their colleagues in engineering did not look down on them. They needed to see that everyone wanted them to succeed, that their past experience of having goal posts moved, resources promised and then taken away, and actions taken in bad faith was over.</p>&#13;
<p>They also needed to overcome their fear that they weren’t good enough to improve their process or that an improved process wouldn’t affect their odds of success. I asked them to write out their plan to test, deploy, and monitor one key upcoming change and be prepared to defend it. They were not thrilled by the idea of doing a murder board. It took me a while to persuade them the exercise could be good. Part of the reason they worried was they felt this exercise would invite colleagues to micromanage them, talk down to them, or treat them like they were stupid and could not be trusted.</p>&#13;
<p><span epub:type="pagebreak" title="127" id="Page_127"/>I argued that this was an opportunity to prove to everyone how difficult their engineering challenge was. They would leave the murder board with a new process. If the new process still failed, everyone would know it was vetted by the best engineers at the organization and that they couldn’t have done any better. In this way, I used the murder board to resolve their fear of opening Schrödinger’s box. Failure under a better process would not prove they were bad engineers. That process had survived the murder board and still failed.</p>&#13;
<p>To accomplish those goals, it is essential that both sides of the murder board know that the purpose of the exercise is to make the candidate stronger. There must not be any doubt that everyone is on the same team, working for the candidate’s benefit. Criticism should be tough, nitpicky, and unforgiving but delivered only if it is relevant to the stressful event to come—in this case, a deploy. For that reason, we don’t do murder boards when there is no upcoming stressful event to ground them.</p>&#13;
<p>It is useful to set some boundaries with the board. We do not use the murder board space to dredge up old failures or grudges. We do not use the murder board space to put people down or insult them. We don’t use it to make grand speeches. The board can ask questions, point out flaws, or provide hypothetical situations. They can provide detailed explanations to elaborate fully on a problem they want to highlight, but they should avoid doing so and let team members speak in their own words as much as possible. Most important, the board’s commentary should be exclusively <em>negative</em>, even if there are strong advantages to the plan being presented. Murder boards build confidence because they are <em>survived</em>.</p>&#13;
<h2 id="h1-501188c06-0014">Stopping the Bleed</h2>&#13;
<p class="BodyFirst">The techniques discussed in this chapter are all about transitioning troubled projects to a state where problems are not compounding in cycles. If the organization is making changes that will not provide enough value to justify their expense, boost the value of those changes by turning them <span epub:type="pagebreak" title="128" id="Page_128"/>into a vehicle of better engineering practices. If the organization is paralyzed by missing information and unknown complications, promote resilience and eliminate the fear of failure. If problems extend beyond what any one team can solve by itself, allow the organization to temporarily reorganize itself around the problem. If teams are demoralized to the point where they are hurting themselves, challenge other parts of the organization to contribute to their success.</p>&#13;
<p>Legacy modernization projects do not fail because one mistake was made or something went wrong once. They fail because the organization deploys solutions that actually reinforce unsuccessful conditions. If you’re coming into a project in the middle, your most important task as a leader is figuring out where those cycles are and stopping them.</p>&#13;
</section>&#13;
<section class="footnotes">&#13;
<aside class="FootnoteEntry">&#13;
<p><sup class="FootnoteReference"><a id="c06-footnote-1" href="#c06-footnoteref-1">1</a></sup> Steven Levy, <em>In the Plex: How Google Thinks, Works, and Shapes Our Lives </em>(New York: Simon &amp; Schuster, 2011).</p>&#13;
</aside>&#13;
</section>&#13;
</body></html>