- en: '**4'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4'
- en: DEALING WITH LARGE NUMBERS OF FEATURES**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大量特征**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: In the previous chapter, we talked about *overfitting*—that is, using too many
    features in a given setting. Having a large number of features may also cause
    issues with long computation times. This chapter is all about reducing the size
    of our feature set—in other words, *dimension reduction*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了*过拟合*——即在给定的设置中使用过多的特征。特征数量过多也可能导致计算时间过长的问题。本章将重点介绍如何减少特征集的大小——换句话说，*降维*。
- en: Note that it’s not just a need to use *fewer* features; we also need to decide
    *which* features, or even which *combinations* of features, to use. We’ll cover
    principal component analysis (PCA), one of the best-known techniques for dealing
    with large values of *p*, which is based on forming new features by combining
    old ones and then using just a few of the new ones as our feature set.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这不仅仅是需要使用*更少*的特征；我们还需要决定使用*哪些*特征，甚至是哪些*特征组合*。我们将讨论主成分分析（PCA），这是一种处理较大*p*值的著名技术，其原理是通过组合旧特征来形成新特征，然后仅使用这些新特征中的一部分作为我们的特征集。
- en: '4.1 Pitfall: Computational Issues in Large Datasets'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 陷阱：大型数据集中的计算问题
- en: Again, overfitting is a major issue in ML. As noted in the last chapter, plugging
    the term into Google gave me 6,560,000 results! And, in addition to predictive-accuracy
    problems of overfitting, we also need to worry about computation. The larger the
    number of features, the longer our computation will take.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，过拟合是机器学习中的一个主要问题。正如上一章所述，在谷歌中搜索这个术语，我得到了6,560,000个结果！除了过拟合带来的预测准确性问题，我们还需要担心计算问题。特征数量越多，我们的计算时间就越长。
- en: In some cases, computation times can be extremely challenging. For example,
    one write-up of AlexNet, a neural network for image classification, reported that
    the network takes *five or six days* to train on two extremely powerful computers.^([1](footnote.xhtml#ch4fn1))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，计算时间可能会变得非常具有挑战性。例如，关于AlexNet（一个用于图像分类的神经网络）的报道指出，该网络在两台极为强大的计算机上训练需要*五到六天*的时间。^([1](footnote.xhtml#ch4fn1))
- en: Moreover, the energy usage in the computation can be staggering.^([2](footnote.xhtml#ch4fn2))
    Training a large natural language processing model can use energy whose production
    results in emission of over 78,000 pounds of CO[2] into the atmosphere. By comparison,
    the average automobile will emit about 126,000 pounds of CO[2] in its lifetime.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，计算过程中的能耗可能非常惊人。^([2](footnote.xhtml#ch4fn2)) 训练一个大型自然语言处理模型可能会消耗大量能源，产生的二氧化碳排放量超过78,000磅。相比之下，一辆普通汽车的使用寿命内大约会排放126,000磅二氧化碳。
- en: Most readers of this book will not encounter exceptionally large applications
    like the above. But computation can be a significant problem even on merely “large-ish”
    datasets, such as some considered in this chapter. Your code’s run time—for a
    single function call—may not be measured in days, but it certainly could run into
    minutes or, in some cases, even hours.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的大多数读者不会遇到像上述那样的特别大的应用。但即便是在“较大”的数据集上，计算仍然可能成为一个显著的问题，例如本章中讨论的一些数据集。单个函数调用的运行时间可能不需要以天来计量，但肯定会达到几分钟，甚至在某些情况下，可能需要几小时。
- en: In addition, a large dataset may use too much memory. A dataset with a million
    rows and 1,000 columns has a billion elements. At 8 bytes each, that’s 8GB of
    RAM. The amount of memory the algorithm uses may be several multiples of that.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，较大的数据集可能占用过多的内存。一个拥有百万行和1,000列的数据集包含十亿个元素。每个元素占用8字节，那么就需要8GB的内存。算法使用的内存量可能是这个数值的几倍。
- en: 'Discussions of dimension reduction seldom mention excessive data loss due to
    NA values, but it can be a major factor. If there is even one NA value in a given
    row of our data, most ML software libraries will discard the entire row. The more
    columns we have, the more likely it is that any given row will have at least one
    NA value, and thus the more likely the row will be discarded. In other words:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 降维的讨论很少提到由于NA值导致的数据丢失，但它可能是一个重要因素。如果数据中的某一行包含一个NA值，大多数机器学习软件库将会丢弃这一整行。列数越多，任何给定的行出现至少一个NA值的可能性就越大，因此该行被丢弃的可能性也越大。换句话说：
- en: If our data is prone to NA values, the larger *p* is, the smaller our effective
    value of *n*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据容易出现NA值，那么*p*越大，我们的有效*n*值就越小。
- en: Thus we have yet another incentive to drop some features. This will result in
    an increase in the number of complete cases included in our analysis, resulting
    in better prediction ability.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有了另一个丢弃一些特征的动力。这将导致我们分析中完整案例的数量增加，从而提高预测能力。
- en: 'As usual, it’s a trade-off: if we remove too many features, some may have substantial
    predictive ability. So, we hope to discard a few less-important ones.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，这是一种权衡：如果我们删除太多特征，可能会丧失一些具有重要预测能力的特征。因此，我们希望舍弃一些不太重要的特征。
- en: 4.2 Introduction to Dimension Reduction
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 维度减少简介
- en: In this chapter, and in this book generally, our primary tool for attacking
    the problems posed by big data will be to reduce *p*, the number of features in
    our dataset. This is known as *dimension reduction*. While we could take the approach
    of simply removing features we believe won’t be very helpful, there are other,
    more systematic methods that can be applied.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章以及本书中，我们攻克大数据问题的主要工具将是减少*p*，即数据集中的特征数量。这被称为*维度减少*。虽然我们可以采取简单地删除我们认为不太有用的特征的方法，但也有其他更系统的方式可以应用。
- en: 'Dimension reduction has two goals, which are both equally important:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 维度减少有两个目标，这两个目标同样重要：
- en: Avoid overfitting. If we don’t have ![Image](../images/prootn.jpg) (see [Section
    3.1.3](ch03.xhtml#ch03lev1sec3)), we should consider possibly reducing *p*.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 避免过拟合。如果我们没有![Image](../images/prootn.jpg)（参见[第3.1.3节](ch03.xhtml#ch03lev1sec3)），我们应该考虑减少*p*。
- en: Reduce computation. With larger datasets, k-NN and most of the methods in this
    book will have challenging computational requirements, and one obvious solution
    is to reduce *p*.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少计算。对于更大的数据集，k-NN和本书中的大多数方法将面临巨大的计算要求，而一个显而易见的解决方案是减少*p*。
- en: '***4.2.1 Example: The Million Song Dataset***'
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***4.2.1 示例：百万歌曲数据集***'
- en: Say you’ve run into a recording of an old song without a label specifying its
    name and you wish to identify it. The Million Song Dataset allows us to predict
    a song’s release year from 90 audio characteristics, so let’s try it out, since
    knowing the year might help you find the song’s title. You can download the data
    from the UC Irvine Machine Learning Repository.^([3](footnote.xhtml#ch4fn3)) (Actually
    there are only about 0.5 million songs in this version.)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你遇到了没有标签的旧歌曲录音，无法确定其名称，你想要识别它。《百万歌曲数据集》可以让我们根据90个音频特征预测歌曲的发行年份，让我们尝试一下，因为知道年份可能有助于找到歌曲的标题。你可以从UC
    Irvine机器学习库下载数据^[3](footnote.xhtml#ch4fn3)。（实际上，这个版本的数据集只有大约50万首歌曲。）
- en: A dataset of this size may involve a substantial computational burden. Let’s
    investigate that, finding the time to do just a single prediction.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这样规模的数据集可能会涉及相当大的计算负担。让我们调查一下，看看仅仅做一次预测需要多少时间。
- en: Read in the data from the downloaded file and assign the result to `yr`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从下载的文件中读取数据，并将结果分配给`yr`。
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that in spite of the *.txt* suffix in the filename, it is actually a CSV
    file, so we used `read.csv()`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管文件名以*.txt*结尾，但它实际上是一个CSV文件，因此我们使用了`read.csv()`。
- en: 'Also, the above file read was slow. The reader may consider using `fread()`
    from the `data.table` package on large files like this. Here the call would be:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，上面的文件读取速度较慢。读者可以考虑在像这样的较大文件上使用`data.table`包中的`fread()`。这里的调用将是：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Since `kNN()` requires data frame or R matrix input ([Appendix C](app03.xhtml#appc)),
    we needed that last line to convert from a `data.table` to a data frame.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`kNN()`需要数据框或R矩阵输入（[附录 C](app03.xhtml#appc)），我们需要使用最后一行代码将`data.table`转换为数据框。
- en: 'Let’s take a look:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We see there are over 515,000 rows (that is, 515,000 songs) and 91 columns.
    That first column is the year of release, followed by 90 columns of arcane audio
    measurements. That means column 1 is our outcome variable and the remaining 90
    columns are (potential) features.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到数据集中有超过515,000行（即515,000首歌曲）和91列。第一列是发行年份，接下来的90列是晦涩的音频测量值。这意味着第一列是我们的结果变量，其余90列是（潜在的）特征。
- en: '***4.2.2 The Need for Dimension Reduction***'
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***4.2.2 维度减少的必要性***'
- en: We have 90 features here. With over 500,000 data points, the rough rule of thumb
    ![Image](../images/prootn.jpg) from [Section 3.1.3](ch03.xhtml#ch03lev1sec3) says
    we probably could use all 90 features without overfitting. But k-NN requires a
    lot of computation, so dimension is still an important issue.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里有90个特征。对于超过50万的数据点，根据[第3.1.3节](ch03.xhtml#ch03lev1sec3)的粗略经验法则，可能我们可以使用所有90个特征而不发生过拟合。但k-NN需要大量计算，所以维度仍然是一个重要问题。
- en: As an example of the computational burden, let’s see how long it takes to predict
    one data point, say, the first row of our data. As explained in [Section 1.17](ch01.xhtml#ch01lev17),
    to predict just one data point, it is faster to call `kNN()` directly rather than
    calling its wrapper, `qeKNN()`. The latter operates on a large scale, which we
    don’t need in this instance, and which will compute many quantities not needed
    here. (It computes the estimated regression function at every point of the training
    set. This is effective only if we plan to do a lot of predictions in the future.)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 作为计算负担的一个例子，我们来看预测一个数据点需要多长时间，比如预测数据的第一行。正如在[第1.17节](ch01.xhtml#ch01lev17)中解释的那样，要预测一个数据点，直接调用`kNN()`比调用它的封装函数`qeKNN()`要快。后者是大规模运行的，我们在这个实例中并不需要它，而且它会计算许多这里不需要的量。（它在训练集的每个点上计算估计的回归函数，仅当我们计划未来做大量预测时才有效。）
- en: 'Recall that the arguments of `kNN()` are the *X* data, the *Y* data, the data
    point to be predicted, and *k*. Here, then, is the code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，`kNN()`的参数是*X*数据、*Y*数据、待预测的数据点以及*k*。以下是代码：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That’s over 39 seconds just to predict one data point. If we predict a substantial
    portion of the entire original dataset, say, with `holdout` = 100000, that means
    we incur that 39-second wait 100,000 times, which would be prohibitive.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅预测一个数据点就需要39秒多。如果我们预测整个原始数据集的一个重要部分，比如`holdout` = 100000，那就意味着我们需要等待39秒100,000次，这将是不可承受的。
- en: Thus we may want to cut down the size of our feature set, whether out of computational
    concerns, as was the case here, or in other cases because of a need to avoid overfitting.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可能希望缩减特征集的大小，无论是出于计算考虑（如本文所述），还是在其他情况下为了避免过拟合。
- en: Feature selection is yet another aspect of ML that has no perfect solution but
    for which some pretty good approaches have been developed. Before we get into
    the main approach this chapter deals with, PCA, let’s look at a few other techniques
    to get a better sense of the challenges we face when selecting features.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择是机器学习的另一个方面，虽然没有完美的解决方案，但已有一些相当好的方法。在我们进入本章讨论的主要方法PCA之前，先看几个其他方法，以便更好地理解在选择特征时我们面临的挑战。
- en: 4.3 Methods for Dimension Reduction
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 降维方法
- en: 'Now that we see the need for dimension reduction, how can we achieve it? We’ll
    cover a few approaches here: consolidation and embedding, *all-possible subsets*,
    and PCA. These are generally applicable, and we will present some techniques for
    some specific ML methods later in the book.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看到了降维的必要性，那么如何实现它呢？我们将在这里介绍几种方法：整合与嵌入、*所有可能的子集*以及PCA。这些方法具有广泛的适用性，书中的后续章节将针对一些特定的机器学习方法介绍更多技术。
- en: '***4.3.1 Consolidation and Embedding***'
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***4.3.1 整合与嵌入***'
- en: Economists talk of *proxy* variables. We may wish to have data on some variable
    *U* but, lacking it, use instead a variable *V* for which we do have data and
    that captures much of the essence of *U*. *V* is said to be a “proxy” for *U*.
    Related techniques in ML are *consolidation* and *embedding*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 经济学家谈到*代理*变量。我们可能希望获取某个变量*U*的数据，但由于缺乏它，改为使用我们有数据的变量*V*，而且*V*能很好地捕捉到*U*的本质。*V*被称为*U*的“代理”变量。机器学习中的相关技术包括*整合*和*嵌入*。
- en: In the context of dimension reduction, proxies can have a different use. Say
    we do have data on *U*, but this variable is categorical with a huge number of
    categories (that is, a huge number of levels in the R factor implementation of
    the variable). That would mean having a huge number of dummy variables, or high
    dimension. One way to reduce dimension in such settings would be to combine levels,
    yielding a new categorical variable *V* with fewer dummies. Or even better, we
    may be able to use a numerical *V*, thus no dummies, just that one variable.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在降维的背景下，代理变量可能有不同的用途。假设我们确实有*U*的数据，但这个变量是分类的，且具有大量的类别（即在R中表示变量的因子实现有大量级别）。这意味着会有大量的虚拟变量，或者说是高维度。在这种情况下，减少维度的一种方法是合并级别，从而得到一个新的分类变量*V*，并减少虚拟变量的数量。更好的方法是，我们可能能够使用一个数值型的*V*，这样就不需要虚拟变量，只有那个变量。
- en: Again consider the ZIP code example of [Section 3.1.1](ch03.xhtml#ch03lev1sec1),
    where the hypothetical goal was to estimate parka sales. If there were 42,000
    ZIP codes, then in terms of dummy variables, we would have 42,000 dummies. We
    might cut that down by, say, choosing to use only the first digit of the ZIP code.
    That would mean, for instance, combining levels 90000, 90001, . . . , 99999 in
    a single level, 9\. (Not all of those ZIP codes actually exist, but the principle
    is the same.) The ZIP codes for UC Davis and UCLA, 95616 and 90024, respectively,
    would now both reduce to simply 9.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑[第3.1.1节](ch03.xhtml#ch03lev1sec1)中的邮政编码示例，其中假设的目标是估计派克大衣的销量。如果有42,000个邮政编码，那么在虚拟变量的情况下，我们会有42,000个虚拟变量。我们可以通过选择仅使用邮政编码的首位数字来减少这个数字。例如，可以将90000、90001、...、99999这些邮政编码合并成一个单一的层级9。（虽然并不是所有这些邮政编码都存在，但原理是相同的。）加州大学戴维斯分校和加州大学洛杉矶分校的邮政编码分别为95616和90024，现在这两个邮政编码都将简化为9。
- en: Obviously, this would result in loss of information; it would induce some bias
    into the Bias-Variance Trade-off. But we would still get fairly good geographic
    detail—for instance, the 9s are all on the West Coast—for the purpose of, say,
    predicting parka purchases. That way, we’d have a lot of data points for each
    of the 10 (abbreviated) ZIP codes 0 through 9, thus reducing variance.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这将导致信息的丢失；它将引入一些偏差，影响偏差-方差平衡。但我们仍然可以得到相当不错的地理细节——例如，9这个数字代表的是西海岸——用于预测派克大衣购买等目的。这样，我们就可以为每个10个（缩写）邮政编码0到9收集大量的数据点，从而减少方差。
- en: This would reduce 42,000 dummies to 9\. Better yet, we might choose to use an
    *embedding*. We could fetch the average daily winter temperature per ZIP code
    from government data sites (remember, we’re selling parkas), and use that temperature
    instead of ZIP code as our feature. Now we would have only 1 feature, not 42,000,
    and not even 9.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把42,000个虚拟变量减少到9个。更好的是，我们可能选择使用*嵌入*。我们可以从政府数据网站获取每个邮政编码的平均每日冬季温度（记住，我们在卖派克大衣），并使用这个温度代替邮政编码作为特征。这样，我们就只需要一个特征，而不是42,000个，甚至不是9个。
- en: '***4.3.2 The All Possible Subsets Method***'
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***4.3.2 所有可能子集方法***'
- en: One might think, “To choose a good feature set, why not look at all possible
    subsets of features? We would find MAPE or OME for each one and then use the subset
    that minimizes that quantity.” This is called the *All Possible Subsets Method*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有人会想，“为了选择一个好的特征集，为什么不查看所有可能的特征子集呢？我们可以为每个子集计算MAPE或OME，然后使用最小化该指标的子集。”这就是所谓的*所有可能子集方法*。
- en: 'In the song dataset, for instance, in predicting holdout data, we would proceed
    as follows: for each subset of our 90 columns, we could predict our holdout set,
    then use the set of columns with the best prediction record. There are two big
    problems here:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在歌曲数据集中，在预测留出数据时，我们可以按以下步骤进行操作：对于我们的90个列的每个子集，我们可以预测我们的留出集，然后使用预测效果最佳的列集合。这里有两个大问题：
- en: We’d need tons of computation time. The 2-feature sets alone would number over
    4,000\. The number of feature sets of all sizes is 2^(90), one of those absurd
    figures like “number of atoms in the universe” that one often sees in the press.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要大量的计算时间。仅仅是2个特征集就有超过4,000个。所有大小的特征集的数量是2^(90)，这是类似“宇宙中原子的数量”这样荒谬的数字，常见于新闻报道中。
- en: We’d risk serious p-hacking issues. The chance of some pair of columns accidentally
    looking very accurate in predicting release year is probably very high.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可能会面临严重的p-hacking问题。某些列对预测发行年份的准确度可能会偶然看起来非常高，这种机会可能非常大。
- en: '***4.3.3 Principal Components Analysis***'
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***4.3.3 主成分分析***'
- en: 'But this method isn’t infeasible after all if we change our features by using
    one of the most popular approaches to dimension reduction: PCA. In the song data,
    for instance, instead of having to investigate an impossible 2^(90) number of
    subsets, we need look only at 90\. Here is how it works.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们通过使用一种最流行的降维方法来改变特征，这种方法也并非完全不可行：主成分分析（PCA）。例如，在歌曲数据中，我们不需要调查不可能的2^(90)个子集，而只需关注90个特征。其原理如下。
- en: To apply PCA to the Million Song Dataset, we will replace our 90 features with
    90 new ones, to be constructed as 90 combinations of the originals. In our data
    frame, we’d replace the 90 original feature columns with these 90 new features,
    known as *principal components (PCs)*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将PCA应用到百万歌曲数据集，我们将用90个新的特征替换原来的90个特征，这90个新特征将作为原始特征的90种组合来构建。在我们的数据框中，我们将用这90个新特征来替代原始的90个特征列，这些新特征被称为*主成分（PCs）*。
- en: Sounds like no gain, right? We are hoping to *reduce* the number of features,
    whereas in the above scenario we simply swapped one set of 90 features for another
    set of that size. But as will be seen below, we will accomplish that goal by using
    the All Possible Subsets Method on these new features, though in a certain ordering.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来没有什么进展，对吧？我们希望*减少*特征数量，而在上述场景中，我们只是将一组90个特征换成了另一组同样大小的特征。但正如下文所示，我们将通过对这些新特征使用“所有可能子集法”，尽管是按照某种顺序进行的，从而实现这一目标。
- en: Our first subset to check will be PC1; then the pair PC1 and PC2; then the triple
    PC1, PC2, PC3; and so on. That means only 90 subsets in all, and typically we
    stop well short of 90 anyway.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先检查的子集是PC1；接着是PC1和PC2的组合；然后是PC1、PC2和PC3的三重组合；依此类推。也就是说，总共有90个子集，通常我们会在达到90个之前就停止。
- en: You’ll see that it will be a lot easier to choose subsets among these new features
    than among the original 90.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，在这些新特征中选择子集比在原始的90个特征中选择要容易得多。
- en: 4.3.3.1 PC Properties
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.3.3.1 主成分特性
- en: The PCs have two special properties. First, they are uncorrelated; roughly speaking,
    the value of one has no effect on the others. In that sense, we might think of
    them as not duplicating each other. Why is that important? If, after reducing
    our number of predictors, some of them were to partially duplicate each other,
    it would seem that we should reduce the number even further. Thus having uncorrelated
    features means unduplicated features, and we feel like we’ve achieved a minimal,
    nonduplicative set.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分有两个特殊的属性。首先，它们是非相关的；大致来说，一个主成分的值不会影响其他主成分。从这个意义上讲，我们可以认为它们互不重复。这为什么重要？如果在减少预测变量数量之后，某些变量之间发生了部分重复，那就意味着我们应该进一步减少变量数量。因此，拥有非相关特征意味着没有重复特征，我们觉得自己已经达到了一个最小且不重复的特征集。
- en: Second, the PCs are arranged in order of decreasing variance. Since a feature
    with small variance is essentially constant, it probably won’t be a useful feature.
    Accordingly, we might retain just the PCs that are of substantial variance. Since
    the variances are ordered, that means retaining, say, the *m* PCs having the largest
    variances. Note that *m* then becomes another hyperparameter.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，主成分（PCs）是按方差递减的顺序排列的。因为一个方差小的特征本质上是常数，它可能不会是一个有用的特征。因此，我们可能只保留那些具有较大方差的主成分。由于方差是有顺序的，这意味着我们可以保留，例如，方差最大的*m*个主成分。请注意，*m*因此成为了另一个超参数。
- en: Each PC is a *linear combination* of our original features—that is, a PC is
    a sum of constants times the features. If, for instance, the latter were Height,
    Weight, and Age, a PC might be 0.12 Height + 0.59 Weight − 0.02 Age. Recall from
    algebra that these numbers, 0.12, 0.59, and −0.02, are called *coefficients*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 每个主成分（PC）都是我们原始特征的*线性组合*——也就是说，一个PC是常数与特征的乘积之和。例如，如果后者是身高、体重和年龄，一个PC可能是0.12
    身高 + 0.59 体重 − 0.02 年龄。回想一下代数知识，这些数字0.12、0.59和−0.02被称为*系数*。
- en: In ML, one’s focus is prediction rather than description. For instance, in the
    bike rental data, someone may be interested in investigating *how* the features
    affect ridership, say, how many extra riders we might have on holidays. That would
    be a description application, which is not usually a concern in ML. So, those
    coefficients, such as 0.12, 0.59, and −0.02, are not very relevant for us. Instead,
    the point is that we are creating new features as combinations of the original
    ones, and we may not examine the coefficients themselves.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，重点是预测而不是描述。例如，在自行车租赁数据中，可能有人有兴趣研究*特征如何影响骑行人数*，比如假期期间我们可能会有多少额外的骑行者。这将是一个描述性应用，而这通常不是机器学习中的关注点。所以，像0.12、0.59和−0.02这样的系数对我们来说并不是很重要。相反，关键在于我们正在将原始特征组合成新的特征，我们可能并不会直接检查这些系数本身。
- en: 'With these new features, we have many fewer candidate sets to choose from:
    the first PC, the first two PCs, the first three PCs, and so on. We are choosing
    from just 90 candidate feature sets rather than the unimaginable 2^(90) we would
    have if we looked at all possible subsets. We can choose among those subsets using
    cross-validation or another method introduced below. Remember, smaller sets save
    computation time (for k-NN now and other methods later) and help avoid overfitting
    and p-hacking.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些新特征，我们需要从更少的候选集合中进行选择：第一个PC，第一个两个PC，前三个PC，依此类推。我们只需从90个候选特征集中的选择，而不是从我们如果查看所有可能的子集将得到的2^(90)个子集中选择。我们可以通过交叉验证或下文介绍的其他方法来从这些子集中选择。请记住，较小的集合可以节省计算时间（对于k-NN方法以及其他方法），并帮助避免过拟合和p-hacking。
- en: 4.3.3.2 PCA in the Million Song Dataset
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.3.3.2 PCA在百万歌曲数据集中的应用
- en: Base-R includes several functions to do PCA, including the one we’ll use here,
    `prcomp()`. CRAN also has packages that implement especially fast PCA algorithms,
    such as `RSpectra`, which are very useful for large datasets.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Base-R包含了多个用于进行PCA的函数，其中包括我们在这里使用的`prcomp()`。CRAN还有实现特别快速的PCA算法的软件包，例如`RSpectra`，这些算法对于大数据集非常有用。
- en: 'We’ll mainly use `qePCA()`, a `regtools` function that wraps `prcomp()`, but
    you should get at least some exposure to the latter function itself, as follows.
    Here is the call:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将主要使用`qePCA()`，这是一个封装了`prcomp()`的`regtools`函数，但你应该至少对后者有一些了解，方法如下。以下是调用方式：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Remember, PCA is for the *X* data, not *Y*, so we skipped the year field here,
    which is in column 1.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，PCA是针对*X*数据的，而不是*Y*，所以我们在这里跳过了年字段，它位于第一列。
- en: 'As an example, let’s say we’ve decided to use the first 20 PCs. We do this
    by retaining the first 20 columns of the `rotation` component of the output, which
    contains the coefficients of the PCs (such as the 0.59 and so on above). Let’s
    first understand this component:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，假设我们决定使用前20个主成分。我们通过保留输出中`rotation`部分的前20列来实现这一点，`rotation`包含了主成分的系数（例如上面的0.59等）。让我们先了解这个组件：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'So, `rt` has 90 rows and 90 columns, with numerical entries. We see that the
    names of the rows and columns are `V2`, `V3`, . . . , and `PC1`, `PC2`, . . .
    , respectively. The row names come from our feature names (the first is *Y*, not
    a feature):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，`rt`有90行和90列，且所有条目都是数值型的。我们看到行名和列名分别是`V2`、`V3`，...和`PC1`、`PC2`，...。行名来自我们的特征名（第一个是*Y*，不是特征）：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The column names are for the PCs. We have 90 features, thus 90 PCs, named `PC1`,
    `PC2`, and so on.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些列名代表主成分（PCs）。我们有90个特征，因此有90个主成分，命名为`PC1`、`PC2`，依此类推。
- en: 'In this example, we have said that we want to use only the first 20 PCs for
    dimension reduction. Thus we will discard columns PC21 through PC90 in `rt`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们已经说过，我们只想使用前20个主成分来进行降维。因此，我们将丢弃`rt`中的PC21至PC90列：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'So, we’re now using only those first 20 PCs. Now we convert our original data
    accordingly:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在只使用前20个主成分。现在我们根据这些主成分转换原始数据：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is our new *X* data. We’ll look at how to use it shortly, but first, what
    happened in that `predict()` call? The function is not actually predicting anything
    here. It simply converts the original features to new ones. Let’s take a closer
    look at that.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的新*X*数据。我们稍后会看看如何使用它，但首先，`predict()`调用中发生了什么呢？该函数实际上并没有进行任何预测。它只是将原始特征转换为新的特征。我们来仔细看看这个过程。
- en: First, recall that, in R, `predict()` is a *generic* function (see [Section
    1.5.1](ch01.xhtml#ch01lev5sec1)). The `prcomp()` function returns an object of
    class `'prcomp'` (check this by typing `class(pcout)` for the data above). So,
    the call to `predict()` here gets relayed to `predict.prcomp()`, which the R people
    wrote for the purpose here—to do conversion from old features to new ones.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，回想一下，在R中，`predict()`是一个*通用*函数（请参见[第1.5.1节](ch01.xhtml#ch01lev5sec1)）。`prcomp()`函数返回一个`'prcomp'`类的对象（通过输入`class(pcout)`来检查上述数据）。因此，`predict()`的调用会转发到`predict.prcomp()`，这是R开发人员为此目的编写的函数——将旧特征转换为新特征。
- en: 'So, our call `predict(pcout,yr[,-1])` tells R, “Please convert the features
    in `yr[,-1]` to PC features, based on what’s in `pcout`.” Since we previously
    had altered `pcout$rotation` to use just 20 PCs, now calling `predict()` on `pcout`
    will generate a new 20-column data frame to use instead of our original 90-column
    one:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们的调用`predict(pcout,yr[,-1])`告诉R，“请根据`pcout`中的内容，将`yr[,-1]`中的特征转换为主成分特征。”由于我们之前已经将`pcout$rotation`更改为只使用前20个主成分，因此现在调用`predict()`时，`pcout`将生成一个新的20列数据框，替代我们原来的90列数据框：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We have the same number of rows as in the old *X*—as we should since it’s still
    the song data (albeit transformed), with 515,345 songs. But now we have 20 columns
    instead of 90, representing the fact that we now have only 20 features. Note,
    by the way, that none of these new columns was a column in `yr`; each new column
    is a certain mixture of the original ones.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的行数与旧的*X*相同——因为它仍然是歌曲数据（尽管已被转换），共有515,345首歌。但现在我们有20列，而不是90列，表示我们现在只拥有20个特征。顺便提一下，这些新列中没有一个是`yr`中的列；每一列都是原始列的某种组合。
- en: Now, how would we do k-NN prediction using PCA? Imagine that we have a new case
    whose 90 audio measures are the same as the eighth song in our dataset, except
    that there is a value 32.6 in the first column. Let’s store that in `w`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何使用PCA进行k-NN预测呢？假设我们有一个新的案例，它的90个音频指标与数据集中第八首歌完全相同，只是第一列的值为32.6。我们将其存储在`w`中。
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Since we changed our *X* data in our training set to PCA form, we’ll do the
    same for `w`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将训练集中的*X*数据转换成了PCA形式，因此也要对`w`进行相同的处理：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We would then call `qeKNN()` on our new data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们会在新的数据上调用`qeKNN()`。
- en: To do so directly, though, would be tedious, and we’ll usually use the labor-saving
    `qePCA()` function instead. But let’s put prediction aside for the moment while
    we discuss the issue of choosing the number of PCs to use.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 但直接这么做会很繁琐，通常我们会使用省时的`qePCA()`函数。但在我们讨论如何选择使用的PC数量时，先将预测问题暂时放一边。
- en: '***4.3.4 But Now We Have Two Hyperparameters***'
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***4.3.4 但现在我们有了两个超参数***'
- en: Before now, to attain the most accurate predictions, we only needed to try a
    range of values for the number of nearest neighbors *k*, but in this case, we’ll
    have to test a range of values for both *k* and the number of PCs *m*. It’s nothing
    new, of course, since we always needed to decide which features to use; overfitting
    can arise from having both too small a value of *k* and too many features. Now,
    by converting to PCA, we at least have formalized the latter in a hyperparameter
    *m*. So, we’ve actually made life a little easier.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，为了获得最准确的预测，我们只需要尝试一系列邻居数量*k*的值，但在这种情况下，我们将需要测试*k*和PC数量*m*的多个值。这当然不是什么新鲜事，因为我们一直需要决定使用哪些特征；过拟合可能来源于*k*值过小或特征过多。现在，通过转换为PCA，我们至少将后者形式化为超参数*m*。因此，实际上我们让事情变得稍微容易一些。
- en: Say we try each of 10 *k* values paired with each of 10 *m* values and find
    the holdout MAPE for each pair. We would then choose to use the pair with the
    smallest MAPE. But that’s 10 × 10 = 100 pairs to try, say, with K-fold cross-validation
    applied to each pair (that is, running many holdout sets for each pair).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们尝试每个10个*k*值与每个10个*m*值组合，并找出每对组合的保留MAPE值。然后，我们会选择MAPE最小的那一对。但这意味着我们需要尝试10
    × 10 = 100对，假设对每对组合应用K折交叉验证（即对每对组合运行多个保留集）。
- en: 'Recall there are two major problems with this. First, each pair will take substantial
    computation time, making for a very long total run time, and second, we should
    be concerned about potential p-hacking (see [Section 1.13](ch01.xhtml#ch01lev13)):
    one of those 100 pairs may seem to predict song release year especially accurately,
    simply by coincidence.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这里有两个主要问题。首先，每对组合都需要大量的计算时间，导致总运行时间非常长；其次，我们应该关注潜在的p-hacking问题（见[第1.13节](ch01.xhtml#ch01lev13)）：其中一个100对中的组合可能仅仅是巧合，恰好预测出了歌曲的发布日期。
- en: 'As an alternative, we might try to choose *m* directly. A common intuitive
    approach is to look at the variances of the PCs (squares of standard deviations).
    A variable with small variance is essentially constant and thus presumably of
    little or no use in predicting *Y*. So, the idea is to discard PCs having small
    variance. Let’s take a look:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种替代方法，我们也许可以直接选择*m*。一种常见的直观方法是查看PC的方差（标准差的平方）。一个方差很小的变量本质上是常数，因此可能对预测*Y*几乎没有帮助。因此，思路是丢弃方差很小的PC。我们来看看：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: That `e` notation means powers of 10\. The first one, for instance, means 4.471212
    × 10⁶.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`e`表示10的幂。例如，第一个表示4.471212 × 10⁶。'
- en: We see the variances rapidly decrease. The twelfth one is about 90,000, which
    is tiny relative to the first, at over 4 million. We might therefore decide to
    take *m* = 12\. Once we’ve done that, we are back to the case of choosing just
    one hyperparameter, *k*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到方差迅速减小。第十二个方差约为90,000，相比第一个方差超过400万，几乎可以忽略不计。因此，我们可能决定选择*m* = 12。一旦这样做，我们就回到了只需要选择一个超参数*k*的情况。
- en: There is still no magic formula for choosing the number of PCs *m*, but the
    point is that once we decide on a value of *m*, we can now choose *k* separately.
    We may, for instance, choose *m* in this intuitive manner and then use cross-validation
    to get *k*. This may be easier than finding the best (*k*, *m*) pair simultaneously.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 目前仍然没有选择PC数量*m*的魔法公式，但关键是，一旦我们决定了*m*的值，就可以单独选择*k*。例如，我们可以用这种直观的方式选择*m*，然后通过交叉验证来得到*k*。这可能比同时寻找最佳的(*k*,
    *m*)组合要容易。
- en: 'It is common for analysts to look at those variances as a cumulative proportion
    of the total. Here R’s `cumsum()` (cumulative sums) function will come in handy.
    This is how that function works:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 分析师通常会将这些方差看作总方差的累积比例。这里，R的`cumsum()`（累积和）函数会派上用场。该函数的工作方式如下：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s apply that to the PC variances:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其应用到PC方差上：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: So, if, for example, we were to take *m* = 12, our chosen PCs would make up
    about 89 percent of total variance in this data. Or we might think that’s too
    much dimension reduction and opt for, say, a 95 percent cutoff, thus using *m*
    = 23 PCs.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们选择*m* = 12，我们选择的主成分大约能够解释数据总方差的89%。或者我们可能认为这降维过多，选择一个95%的方差截断值，因此使用*m*
    = 23个主成分。
- en: '**NOTE**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Note the phrasing “might,” “if, for example,” and “say” here. As we’ve seen
    before, for many things in ML, there are no mechanical, formulaic “recipes” for
    one’s course of action. Once again, keep in mind that ML is an art, not a science.
    And one becomes skilled in that art with experience, not by recipes.*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*请注意此处的措辞“可能”，“例如，如果”和“假设”。正如我们之前所见，机器学习中的许多事情并没有机械式、公式化的“方案”。再次提醒，机器学习是一门艺术，而非科学。只有通过经验，才能在这门艺术上获得精通，而不是通过固定的“方案”。*'
- en: '***4.3.5 Using the qePCA() Wrapper***'
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***4.3.5 使用qePCA()包装函数***'
- en: Our goal, then, is to first use PCA for dimension reduction and then do our
    k-NN prediction with the first *m* PCs as features. To do this, the following
    rather elaborate set of actions would need to be performed. Fortunately, there
    is a function to automate these steps, but as usual, we need to understand those
    steps first before turning to the convenience function.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是首先使用PCA进行降维，然后使用前*m*个主成分作为特征进行k-NN预测。为了实现这一点，需要执行以下一系列相对复杂的操作。幸运的是，有一个函数可以自动化这些步骤，但像往常一样，我们首先需要理解这些步骤，然后再转向方便的函数。
- en: Say the *X* and *Y* portions of our training data are stored in `trnX` and `trnY`,
    respectively.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的训练数据的*X*部分和*Y*部分分别存储在`trnX`和`trnY`中。
- en: We call `prcomp(trnX)` to calculate the PC coefficients. Let’s name this result
    `pcout`, as above.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们调用`prcomp(trnX)`来计算主成分系数。我们将这个结果命名为`pcout`，如上所述。
- en: We limit the number of columns in `pcout$rotation` to reflect the desired number
    of PCs.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`pcout$rotation`中的列数限制为反映所需的主成分数。
- en: We call `predict(pcout,trnX)` to convert our *X* data to PC form, say, `pcX`.
    Now our new training set consists of `pcX` and `trnY`.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们调用`predict(pcout,trnX)`将我们的*X*数据转换为主成分形式，假设为`pcX`。现在我们的新训练集由`pcX`和`trnY`组成。
- en: We now apply `qeKNN()` on `pcX` and `trnY`. Say we name the result `knnout`.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在在`pcX`和`trnY`上应用`qeKNN()`。假设我们将结果命名为`knnout`。
- en: Subsequently, when new *X* data comes in, say, `newX`, we would first call `predict(pcout,newX)`
    to convert to PC form. Say we name the results `pcNewx`.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随后，当新的*X*数据（例如`newX`）到来时，我们首先调用`predict(pcout,newX)`将其转换为主成分形式。假设我们将结果命名为`pcNewx`。
- en: For our *Y* prediction, we then call `predict(knnout,pcNewX)` to obtain our
    predicted *Y*.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于我们的*Y*预测，我们接着调用`predict(knnout,pcNewX)`来获得预测的*Y*。
- en: By the way, did you notice that we are using `predict()` in two different ways
    here, one to convert to PC form and another to do k-NN prediction? Here we see
    the R concept of generic functions in action. In that first context, R relays
    the `predict()` call to `predict.prcomp()`, while in the second, the relay is
    to `predict.qeKNN()`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，你注意到我们在这里使用`predict()`有两种不同的方式吗，一种是转换为主成分形式，另一种是进行k-NN预测？在这里我们看到了R语言中通用函数的概念。在第一个上下文中，R将`predict()`调用转发给`predict.prcomp()`，而在第二个上下文中，转发给`predict.qeKNN()`。
- en: There are a lot of steps in the procedure above, but it does have a pattern,
    which implies that we should automate it with code. That is the purpose of the
    wrapper function `qePCA()`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 上述过程包含了很多步骤，但它确实有一定的模式，这意味着我们应该通过代码来自动化它。这正是`qePCA()`包装函数的目的。
- en: 'With `qePCA()`, one specifies the data and *Y* column name, as in other `qe*`-series
    functions, and also specifies the following: the desired ML method (say, k-NN),
    the usual hyperparameters for that method (*k*), and the desired proportion of
    total variance for the PCs.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`qePCA()`时，指定数据和*Y*列名，如同其他`qe*`系列函数，并且还指定以下内容：所需的机器学习方法（例如k-NN）、该方法的常见超参数（*k*）以及主成分的总方差所需的比例。
- en: For instance, the call
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，调用
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: states that we want to predict the year (`V1`) in the song data using k-NN and
    *k* = 25\. It also says we want as many PCs as will give us 85 percent of the
    total variance of the features.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 该语句表示我们想使用k-NN预测歌曲数据中的年份（`V1`），并且*k* = 25。它还表明，我们希望使用足够的主成分来解释特征总方差的85%。
- en: So, how well do we predict using *k* = 25 and 85 percent total variance?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们使用*k* = 25和85%的总方差来进行预测的效果如何？
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In guessing which year a song was released, we will, on average, be off by about
    7 years.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在猜测一首歌的发行年份时，我们的平均误差大约是7年。
- en: 'To later predict a new case—say, `w`, the example song we used in [Section
    4.3.3.2](ch04.xhtml#ch04lev3sec3sec2) —we would make the call:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测一个新的案例——比如说 `w`，我们在[第4.3.3.2节](ch04.xhtml#ch04lev3sec3sec2)中使用的示例歌曲——我们会做出如下调用：
- en: '[PRE17]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: So, we’d guess that such a song was released around the year 1995.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们会猜测这首歌大约是在1995年发布的。
- en: All of this is far simpler than the above multistep process; `qePCA()` saves
    us a lot of work!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些比上面的多步骤过程要简单得多；`qePCA()`节省了我们很多工作！
- en: '***4.3.6 PCs and the Bias-Variance Trade-off***'
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***4.3.6 主成分和偏差-方差权衡***'
- en: Now that we know how to use PCA for dimension reduction and subsequent prediction,
    let’s return to the issue of how to choose the number of nearest neighbors *k*
    and number of PCs *m*. Note that choosing *m* is equivalent to choosing the variance
    proportion in the last argument in `qePCA()`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何使用PCA进行降维和随后的预测，让我们回到如何选择最近邻数 *k* 和主成分数 *m* 的问题。请注意，选择 *m* 等同于选择 `qePCA()`
    中最后一个参数的方差比例。
- en: As usual, we have a bias-variance issue, for both *k* and *m*. We’ve discussed
    the trade-off in terms of *k* before; what about *m*?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们面临着偏差-方差问题，无论是对于 *k* 还是 *m*。我们之前已经讨论过 *k* 的权衡，那么 *m* 呢？
- en: 'Actually, the situation for *m* is not new either. Recall what we said in [Section
    3.1.1](ch03.xhtml#ch03lev1sec1) regarding the `mlb` dataset:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，*m* 的情况也并不新鲜。回想一下我们在[第3.1.1节](ch03.xhtml#ch03lev1sec1)中关于 `mlb` 数据集所说的：
- en: We might predict weight from height and age. But what if we were to omit height
    from our feature set? That would induce a bias. Roughly speaking, we’d be tacitly
    assuming everyone is of middling height, which would result in our tending to
    overpredict the weight of shorter players while underpredicting that of the taller
    ones.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会根据身高和年龄预测体重。但如果我们从特征集中省略了身高呢？那将导致偏差。粗略地说，我们实际上是在默许每个人的身高是中等的，这会导致我们倾向于高估矮个子运动员的体重，而低估高个子运动员的体重。
- en: In other words, omitting a feature induces a bias. Or, equivalently, adding
    features reduces bias. Since the PCs *are* features, we see that the more PCs
    we use, the smaller the bias. But that same section goes on to point out that
    the more features we have, the higher the variance in our predictions, which is
    not good. Note, too, that using more PCs results in longer computation time.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，省略一个特征会导致偏差。或者，等效地，添加特征会减少偏差。由于主成分（PCs）*就是*特征，我们可以看到，使用的主成分越多，偏差越小。但同一部分还指出，特征越多，预测的方差越大，这不是好事。还需要注意的是，使用更多的主成分会导致更长的计算时间。
- en: To illustrate this, let’s devise a little experiment to investigate the effect
    of varying *m*, the number of PCs.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，让我们设计一个小实验，研究变化的 *m*（主成分数）对结果的影响。
- en: 'Since our song dataset is rather large, let’s consider a random subset and
    see how well we do with proportions of total variance at various levels. We’ll
    use a subset of, say, 25,000, and check computation time and prediction accuracy:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的歌曲数据集相当大，让我们考虑一个随机子集，看看在不同的方差比例水平下，我们能做到什么程度。我们将使用一个子集，比如说25,000个样本，并检查计算时间和预测准确度：
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The results are shown in [Table 4-1](ch04.xhtml#ch4tab1). The first column shows
    the proportion of total variance, leading to a number of PCs *m* shown in the
    fourth column. In viewing the latter, remember that the full number of possible
    PCs is 90\. The second column shows run time. It increases with *m*, of course;
    a larger *m* means k-NN must do more computation.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如[表4-1](ch04.xhtml#ch4tab1)所示。第一列显示了总方差的比例，进而得出第四列中显示的主成分数 *m*。在查看后者时，记得完整的主成分数是90。第二列显示了运行时间，显然它随着
    *m* 增加而增加；更大的 *m* 意味着 k-NN 必须进行更多的计算。
- en: The most important column in this investigation is the third, the MAPE values.
    We see that for a large increase in computation time, we attain only a moderate
    reduction in MAPE. Moreover, the best MAPE uses only 11 of the 90 PCs.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，最重要的列是第三列，即MAPE值。我们看到，在计算时间大幅增加的情况下，MAPE的降低仅为适度。而且，最佳的MAPE只使用了90个主成分中的11个。
- en: However, always keep in mind that these MAPE values are subject to sampling
    variation. They come from holdout sets, and as you know, holdout sets are randomly
    chosen. For each variance proportion level, we should look at several holdout
    sets, not just one, using cross-validation. We could do this by applying `replicMeans()`
    to the `qePCA()` call, though it would be time-consuming.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请始终记住，这些MAPE值受到抽样变化的影响。它们来自留出集，而如你所知，留出集是随机选择的。对于每个方差比例水平，我们应该查看多个留出集，而不仅仅是一个，使用交叉验证。我们可以通过将
    `replicMeans()` 应用于 `qePCA()` 调用来做到这一点，尽管这会非常耗时。
- en: That means we cannot be sure that *m* = 11 is best. At the least, though, we
    see that the data indicates that we should probably use a lot fewer than 90 PCs.
    Also, in addition to the famous Bias-Variance Trade-off, there is the trade-off
    involving the analyst’s time. We may feel that setting *m* = 11 is good enough.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们不能确定 *m* = 11 是最佳选择。至少，我们可以看到数据表明我们应该使用远少于90个主成分。此外，除了著名的偏差-方差权衡外，还有一个涉及分析师时间的权衡。我们可能会觉得设置
    *m* = 11 已经足够好了。
- en: '**Table 4-1:** Behavior for Different Values of *m*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**表4-1：** 不同 *m* 值的行为'
- en: '| **pcaProp** | **Time (s.)** | **MAPE** | **# of PCs** |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| **pcaProp** | **时间 (秒)** | **MAPE** | **主成分数量** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0.15 | 1.137 | 8.40020 | 2 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 0.15 | 1.137 | 8.40020 | 2 |'
- en: '| 0.25 | 1.230 | 7.83712 | 3 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 0.25 | 1.230 | 7.83712 | 3 |'
- en: '| 0.35 | 2.051 | 8.12444 | 6 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 0.35 | 2.051 | 8.12444 | 6 |'
- en: '| 0.45 | 7.222 | 7.46536 | 11 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 0.45 | 7.222 | 7.46536 | 11 |'
- en: '| 0.55 | 17.812 | 7.88648 | 16 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 0.55 | 17.812 | 7.88648 | 16 |'
- en: '| 0.65 | 35.269 | 7.66808 | 24 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 0.65 | 35.269 | 7.66808 | 24 |'
- en: '| 0.75 | 51.041 | 7.55740 | 33 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 0.75 | 51.041 | 7.55740 | 33 |'
- en: '| 0.85 | 72.916 | 7.85736 | 46 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 0.85 | 72.916 | 7.85736 | 46 |'
- en: '| 0.95 | 94.761 | 7.72288 | 66 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 0.95 | 94.761 | 7.72288 | 66 |'
- en: Furthermore, remember as discussed in [Section 3.1.2](ch03.xhtml#ch03lev1sec2),
    the larger *n* is, the larger we can afford to make *p*. In the above analysis,
    we had *n* = 25000 and *p* = *m* and settled on *m* = 11\. But for the full dataset,
    *n* = 500000, presumably we should use more than 11 PCs.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，记得在 [第3.1.2节](ch03.xhtml#ch03lev1sec2) 中讨论过，*n* 越大，我们可以承受的 *p* 就越大。在上述分析中，我们设定
    *n* = 25000 且 *p* = *m*，最终选择了 *m* = 11。但对于完整数据集，*n* = 500000，我们应该使用超过11个主成分。
- en: Thus, we may be tempted to run the above code on the full dataset. Yet even
    for *n* = 25000, the run time was about half an hour; it would be many hours for
    the full dataset of more than 500,000 records. So, we may just settle for *m*
    = 11 and possibly do a more refined analysis later if time permits.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可能会想要在完整数据集上运行上述代码。然而，即便是 *n* = 25000，运行时间也大约为半小时；对于超过500,000条记录的完整数据集，可能需要几个小时。因此，我们可能会选择
    *m* = 11，并在时间允许的情况下稍后进行更精细的分析。
- en: 4.4 The Curse of Dimensionality
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 维度灾难
- en: The *Curse of Dimensionality (CoD)* says that ML gets harder and harder as the
    number of features grows. For instance, mathematical theory shows that in high
    dimensions, every point is approximately the same distance to every other point.
    Let’s briefly discuss the intuition underlying this bizarre situation. Clearly,
    it has implications for k-NN, which relies on distances, and indeed for some,
    if not all, other ML methods as well.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*维度灾难（CoD）* 表示随着特征数量的增加，机器学习变得越来越困难。例如，数学理论表明，在高维空间中，每个点与其他点的距离大致相同。让我们简要讨论一下这种奇异现象背后的直觉。显然，这对依赖距离的k-NN方法有影响，实际上对于一些，甚至可能是所有其他机器学习方法也有影响。'
- en: To get a rough idea of the CoD, consider data consisting of students’ grades
    in mathematics, literature, history, geography, and so on. The distance between
    the data vectors of Students A and B would be the square root of
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了大致理解维度灾难，考虑由学生在数学、文学、历史、地理等科目的成绩组成的数据。学生A和B的成绩数据向量之间的距离将是以下差异的平方和的平方根：
- en: (math grade*[A]* − math grade*[B]*)² + (lit grade*[A]* − lit grade*[B]*)² +
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: (数学成绩*[A]* − 数学成绩*[B]*)² + (文学成绩*[A]* − 文学成绩*[B]*)² +
- en: (history grade*[A]* − history grade*[B]*)² + (geo grade*[A]* − geo grade*[B]*)²
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: (历史成绩*[A]* − 历史成绩*[B]*)² + (地理成绩*[A]* − 地理成绩*[B]*)²
- en: That expression is a sum, and one can show that sums with a large number of
    terms (only four here, but we could have many more) have small standard deviations
    relative to their means. A quantity with small standard deviation is nearly constant,
    so in high dimensions—that is, in settings with large *p* (a large number of features)—distances
    are nearly constant.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 那个表达式是一个求和，且可以证明，具有大量项的和（这里只有四项，但我们可以有更多项）相对于其均值具有较小的标准差。具有小标准差的量几乎是常数，因此在高维空间中——即在具有大量
    *p*（特征数目多）的设置中——距离几乎是常数。
- en: This is why k-NN fares poorly in high dimensions. This issue is not limited
    to k-NN; much of the ML toolbox has similar problems. The computation of linear/logistic
    regression (see [Chapter 8](ch08.xhtml)) involves a sum of *p* terms, and similar
    computations arise in support vector machines and neural networks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么在高维空间中k-NN表现不佳的原因。这个问题不仅限于k-NN；大多数机器学习工具箱中的方法都有类似的问题。线性回归/逻辑回归的计算（参见 [第8章](ch08.xhtml)）涉及到
    *p* 项的求和，类似的计算也出现在支持向量机和神经网络中。
- en: In fact, lots of problems arise in high dimensions. Some analysts lump them
    all together into the CoD. Whatever one counts as CoD, clearly, higher dimensions
    are a challenge—all the more reason to perform dimension reduction.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，许多问题出现在高维空间中。一些分析师将它们都归类为CoD。不管CoD的定义是什么，显然，高维度是一个挑战——这也是进行维度降维的更多理由。
- en: 4.5 Other Methods of Dimension Reduction
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 其他维度降维方法
- en: Dimension reduction is one of the most actively researched and debated issues
    in ML and statistics. While this chapter focused on PCA, a common approach to
    the problem, there are a number of other approaches.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 维度降维是机器学习和统计学中最活跃的研究和争论问题之一。本章虽然聚焦于PCA这种常见的方法，但实际上还有许多其他方法。
- en: '***4.5.1 Feature Ordering by Conditional Independence***'
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***4.5.1 基于条件独立性的特征排序***'
- en: One approach that I find useful (and contributed slightly to its development)
    is *Feature Ordering by Conditional Independence (FOCI)*. It is based on solid
    mathematical principles (too complex to explain here) and works quite well, I’ve
    found.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现一个有用的方法（并且我为其发展略有贡献）是*基于条件独立性的特征排序（FOCI）*。它基于坚实的数学原理（这里过于复杂，无法详细解释），并且效果相当不错。
- en: 'The `qeML` package includes a FOCI wrapper, `qeFOCI`. Here is the call in its
    basic form:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`qeML`包包含了一个FOCI封装函数，`qeFOCI`。以下是它的基本调用方式：'
- en: '[PRE19]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Since the computational requirement can be large for this method, there are
    also parallelized options, such as the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此方法的计算需求可能较大，因此也有并行化选项，例如以下方法：
- en: '[PRE20]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This will split the computation into chunks, with each of your computer’s cores
    working on one chunk.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把计算分成多个部分，计算机的每个核心将处理其中一个部分。
- en: 'Let’s try FOCI on the census data from [Section 3.2.3](ch03.xhtml#ch03lev2sec3):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来尝试对[第3.2.3节](ch03.xhtml#ch03lev2sec3)中的人口普查数据应用FOCI：
- en: '[PRE21]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that `qeFOCI()` converts R factors to dummy variables. So, we see assessment
    of, for instance, each of the occupations. The one coded 102 seems to have good
    predictive power, while occupation 106 perhaps less so.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`qeFOCI()`将R因子转换为虚拟变量。因此，我们可以看到对每个职业的评估。例如，编码为102的职业似乎具有良好的预测能力，而职业106的预测能力则可能较差。
- en: The `stepT` component of the output gives a type of correlation; the more predictors
    we add, the greater the predictive collective power of the variables. If we wish
    to be more conservative, we might cut off our choice when this correlation seems
    to level off, say, after 6 or 7 variables in this case.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中的`stepT`部分给出了某种类型的相关性；我们增加预测变量时，变量的预测集体能力会变强。如果我们希望更为保守，可能会在相关性趋于平稳时选择截断，例如在本例中，可能在6或7个变量时就停止。
- en: 'How about the song data? To ease the computational burden, I took a 10 percent
    subsample of the data and ran with 2 cores. Yet even then, it ran for more than
    20 minutes:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 那么歌曲数据呢？为了减轻计算负担，我对数据进行了10%的子抽样，并使用了2个核心来进行计算。即便如此，它仍然运行了超过20分钟：
- en: '[PRE22]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: There were 2 cores used, and each applied FOCI to its chunk of data. The first
    core chose variables 1, 14, 2, and so on, while the second chose some of the same
    variables but also different ones; for instance, the second core chose variable
    50, but the first did not. We designed the parallel version of the algorithm to
    take the union of the two sets of variables, so variable 50 does appear in the
    final list. Only the top 7 were chosen, though, as the correlation did not increase
    past that point.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了2个核心，每个核心都将FOCI应用于其数据部分。第一个核心选择了变量1、14、2，依此类推，而第二个核心选择了一些相同的变量，也有不同的变量；例如，第二个核心选择了变量50，但第一个核心没有。我们设计的并行算法会取这两个变量集合的并集，因此变量50出现在最终的变量列表中。不过，最终只选择了前7个变量，因为相关性在此之后并没有增加。
- en: '***4.5.2 Uniform Manifold Approximation and Projection***'
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***4.5.2 均匀流形近似与投影***'
- en: The Uniform Manifold Approximation and Projection (UMAP) method is similar in
    usage pattern to PCA in that we find new variables as functions of the original
    ones and then retain only the top few in terms of predictive ability. The difference,
    though, is that with UMAP, the new variables are complex nonlinear functions of
    the old ones.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀流形近似与投影（UMAP）方法的使用模式与PCA类似，我们通过原始变量找到新的变量，并且只保留在预测能力上排名前几的变量。不过不同的是，使用UMAP时，新的变量是原始变量的复杂非线性函数。
- en: The `qeML` package has a wrapper for UMAP, `qeUMAP()`. As noted, it is used
    in a similar manner to `qePCA()`. We will not pursue this further here in the
    book, but the reader is urged to give it a try.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`qeML`包为UMAP提供了一个封装函数，`qeUMAP()`。如前所述，它的使用方式与`qePCA()`类似。在本书中我们不会进一步探讨这个话题，但建议读者尝试一下。'
- en: 4.6 Going Further Computationally
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 更深入的计算方法
- en: For very large datasets, I highly recommend the `data.table` package. The `bigmemory`
    package can help with memory limitations, though it is for specialists who understand
    computers at the operating system level. Also, for those who know SQL databases,
    there are several packages that interface to such data, such as `RSQLite` and
    `dplyr`.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非常大的数据集，我强烈推荐使用`data.table`包。`bigmemory`包可以帮助解决内存限制问题，尽管它是为那些了解操作系统级别计算机的专家设计的。另外，对于熟悉SQL数据库的人来说，有几个包可以与这种数据接口，如`RSQLite`和`dplyr`。
- en: 4.7 Conclusions
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 结论
- en: In this chapter, we’ve developed an understanding of how computation issues
    interact with the Bias-Variance Trade-off when we work with largescale data. We
    may, for instance, wish to restrict our number of features well before reaching
    the point at which adding more variables is statistically unprofitable. Our featured
    remedy here has been PCA, though we have briefly cited others.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经了解了在处理大规模数据时，计算问题如何与偏差-方差权衡相互作用。例如，我们可能希望在添加更多变量变得统计上不再有利之前，就限制特征的数量。我们在这里介绍的解决方法是主成分分析（PCA），虽然我们也简要提到过其他方法。
