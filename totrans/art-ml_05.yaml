- en: '**4'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DEALING WITH LARGE NUMBERS OF FEATURES**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the previous chapter, we talked about *overfitting*—that is, using too many
    features in a given setting. Having a large number of features may also cause
    issues with long computation times. This chapter is all about reducing the size
    of our feature set—in other words, *dimension reduction*.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it’s not just a need to use *fewer* features; we also need to decide
    *which* features, or even which *combinations* of features, to use. We’ll cover
    principal component analysis (PCA), one of the best-known techniques for dealing
    with large values of *p*, which is based on forming new features by combining
    old ones and then using just a few of the new ones as our feature set.
  prefs: []
  type: TYPE_NORMAL
- en: '4.1 Pitfall: Computational Issues in Large Datasets'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Again, overfitting is a major issue in ML. As noted in the last chapter, plugging
    the term into Google gave me 6,560,000 results! And, in addition to predictive-accuracy
    problems of overfitting, we also need to worry about computation. The larger the
    number of features, the longer our computation will take.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, computation times can be extremely challenging. For example,
    one write-up of AlexNet, a neural network for image classification, reported that
    the network takes *five or six days* to train on two extremely powerful computers.^([1](footnote.xhtml#ch4fn1))
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the energy usage in the computation can be staggering.^([2](footnote.xhtml#ch4fn2))
    Training a large natural language processing model can use energy whose production
    results in emission of over 78,000 pounds of CO[2] into the atmosphere. By comparison,
    the average automobile will emit about 126,000 pounds of CO[2] in its lifetime.
  prefs: []
  type: TYPE_NORMAL
- en: Most readers of this book will not encounter exceptionally large applications
    like the above. But computation can be a significant problem even on merely “large-ish”
    datasets, such as some considered in this chapter. Your code’s run time—for a
    single function call—may not be measured in days, but it certainly could run into
    minutes or, in some cases, even hours.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, a large dataset may use too much memory. A dataset with a million
    rows and 1,000 columns has a billion elements. At 8 bytes each, that’s 8GB of
    RAM. The amount of memory the algorithm uses may be several multiples of that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussions of dimension reduction seldom mention excessive data loss due to
    NA values, but it can be a major factor. If there is even one NA value in a given
    row of our data, most ML software libraries will discard the entire row. The more
    columns we have, the more likely it is that any given row will have at least one
    NA value, and thus the more likely the row will be discarded. In other words:'
  prefs: []
  type: TYPE_NORMAL
- en: If our data is prone to NA values, the larger *p* is, the smaller our effective
    value of *n*.
  prefs: []
  type: TYPE_NORMAL
- en: Thus we have yet another incentive to drop some features. This will result in
    an increase in the number of complete cases included in our analysis, resulting
    in better prediction ability.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, it’s a trade-off: if we remove too many features, some may have substantial
    predictive ability. So, we hope to discard a few less-important ones.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Introduction to Dimension Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, and in this book generally, our primary tool for attacking
    the problems posed by big data will be to reduce *p*, the number of features in
    our dataset. This is known as *dimension reduction*. While we could take the approach
    of simply removing features we believe won’t be very helpful, there are other,
    more systematic methods that can be applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dimension reduction has two goals, which are both equally important:'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid overfitting. If we don’t have ![Image](../images/prootn.jpg) (see [Section
    3.1.3](ch03.xhtml#ch03lev1sec3)), we should consider possibly reducing *p*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduce computation. With larger datasets, k-NN and most of the methods in this
    book will have challenging computational requirements, and one obvious solution
    is to reduce *p*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '***4.2.1 Example: The Million Song Dataset***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Say you’ve run into a recording of an old song without a label specifying its
    name and you wish to identify it. The Million Song Dataset allows us to predict
    a song’s release year from 90 audio characteristics, so let’s try it out, since
    knowing the year might help you find the song’s title. You can download the data
    from the UC Irvine Machine Learning Repository.^([3](footnote.xhtml#ch4fn3)) (Actually
    there are only about 0.5 million songs in this version.)
  prefs: []
  type: TYPE_NORMAL
- en: A dataset of this size may involve a substantial computational burden. Let’s
    investigate that, finding the time to do just a single prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Read in the data from the downloaded file and assign the result to `yr`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that in spite of the *.txt* suffix in the filename, it is actually a CSV
    file, so we used `read.csv()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, the above file read was slow. The reader may consider using `fread()`
    from the `data.table` package on large files like this. Here the call would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Since `kNN()` requires data frame or R matrix input ([Appendix C](app03.xhtml#appc)),
    we needed that last line to convert from a `data.table` to a data frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We see there are over 515,000 rows (that is, 515,000 songs) and 91 columns.
    That first column is the year of release, followed by 90 columns of arcane audio
    measurements. That means column 1 is our outcome variable and the remaining 90
    columns are (potential) features.
  prefs: []
  type: TYPE_NORMAL
- en: '***4.2.2 The Need for Dimension Reduction***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have 90 features here. With over 500,000 data points, the rough rule of thumb
    ![Image](../images/prootn.jpg) from [Section 3.1.3](ch03.xhtml#ch03lev1sec3) says
    we probably could use all 90 features without overfitting. But k-NN requires a
    lot of computation, so dimension is still an important issue.
  prefs: []
  type: TYPE_NORMAL
- en: As an example of the computational burden, let’s see how long it takes to predict
    one data point, say, the first row of our data. As explained in [Section 1.17](ch01.xhtml#ch01lev17),
    to predict just one data point, it is faster to call `kNN()` directly rather than
    calling its wrapper, `qeKNN()`. The latter operates on a large scale, which we
    don’t need in this instance, and which will compute many quantities not needed
    here. (It computes the estimated regression function at every point of the training
    set. This is effective only if we plan to do a lot of predictions in the future.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the arguments of `kNN()` are the *X* data, the *Y* data, the data
    point to be predicted, and *k*. Here, then, is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: That’s over 39 seconds just to predict one data point. If we predict a substantial
    portion of the entire original dataset, say, with `holdout` = 100000, that means
    we incur that 39-second wait 100,000 times, which would be prohibitive.
  prefs: []
  type: TYPE_NORMAL
- en: Thus we may want to cut down the size of our feature set, whether out of computational
    concerns, as was the case here, or in other cases because of a need to avoid overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection is yet another aspect of ML that has no perfect solution but
    for which some pretty good approaches have been developed. Before we get into
    the main approach this chapter deals with, PCA, let’s look at a few other techniques
    to get a better sense of the challenges we face when selecting features.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Methods for Dimension Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we see the need for dimension reduction, how can we achieve it? We’ll
    cover a few approaches here: consolidation and embedding, *all-possible subsets*,
    and PCA. These are generally applicable, and we will present some techniques for
    some specific ML methods later in the book.'
  prefs: []
  type: TYPE_NORMAL
- en: '***4.3.1 Consolidation and Embedding***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Economists talk of *proxy* variables. We may wish to have data on some variable
    *U* but, lacking it, use instead a variable *V* for which we do have data and
    that captures much of the essence of *U*. *V* is said to be a “proxy” for *U*.
    Related techniques in ML are *consolidation* and *embedding*.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of dimension reduction, proxies can have a different use. Say
    we do have data on *U*, but this variable is categorical with a huge number of
    categories (that is, a huge number of levels in the R factor implementation of
    the variable). That would mean having a huge number of dummy variables, or high
    dimension. One way to reduce dimension in such settings would be to combine levels,
    yielding a new categorical variable *V* with fewer dummies. Or even better, we
    may be able to use a numerical *V*, thus no dummies, just that one variable.
  prefs: []
  type: TYPE_NORMAL
- en: Again consider the ZIP code example of [Section 3.1.1](ch03.xhtml#ch03lev1sec1),
    where the hypothetical goal was to estimate parka sales. If there were 42,000
    ZIP codes, then in terms of dummy variables, we would have 42,000 dummies. We
    might cut that down by, say, choosing to use only the first digit of the ZIP code.
    That would mean, for instance, combining levels 90000, 90001, . . . , 99999 in
    a single level, 9\. (Not all of those ZIP codes actually exist, but the principle
    is the same.) The ZIP codes for UC Davis and UCLA, 95616 and 90024, respectively,
    would now both reduce to simply 9.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, this would result in loss of information; it would induce some bias
    into the Bias-Variance Trade-off. But we would still get fairly good geographic
    detail—for instance, the 9s are all on the West Coast—for the purpose of, say,
    predicting parka purchases. That way, we’d have a lot of data points for each
    of the 10 (abbreviated) ZIP codes 0 through 9, thus reducing variance.
  prefs: []
  type: TYPE_NORMAL
- en: This would reduce 42,000 dummies to 9\. Better yet, we might choose to use an
    *embedding*. We could fetch the average daily winter temperature per ZIP code
    from government data sites (remember, we’re selling parkas), and use that temperature
    instead of ZIP code as our feature. Now we would have only 1 feature, not 42,000,
    and not even 9.
  prefs: []
  type: TYPE_NORMAL
- en: '***4.3.2 The All Possible Subsets Method***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One might think, “To choose a good feature set, why not look at all possible
    subsets of features? We would find MAPE or OME for each one and then use the subset
    that minimizes that quantity.” This is called the *All Possible Subsets Method*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the song dataset, for instance, in predicting holdout data, we would proceed
    as follows: for each subset of our 90 columns, we could predict our holdout set,
    then use the set of columns with the best prediction record. There are two big
    problems here:'
  prefs: []
  type: TYPE_NORMAL
- en: We’d need tons of computation time. The 2-feature sets alone would number over
    4,000\. The number of feature sets of all sizes is 2^(90), one of those absurd
    figures like “number of atoms in the universe” that one often sees in the press.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’d risk serious p-hacking issues. The chance of some pair of columns accidentally
    looking very accurate in predicting release year is probably very high.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '***4.3.3 Principal Components Analysis***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'But this method isn’t infeasible after all if we change our features by using
    one of the most popular approaches to dimension reduction: PCA. In the song data,
    for instance, instead of having to investigate an impossible 2^(90) number of
    subsets, we need look only at 90\. Here is how it works.'
  prefs: []
  type: TYPE_NORMAL
- en: To apply PCA to the Million Song Dataset, we will replace our 90 features with
    90 new ones, to be constructed as 90 combinations of the originals. In our data
    frame, we’d replace the 90 original feature columns with these 90 new features,
    known as *principal components (PCs)*.
  prefs: []
  type: TYPE_NORMAL
- en: Sounds like no gain, right? We are hoping to *reduce* the number of features,
    whereas in the above scenario we simply swapped one set of 90 features for another
    set of that size. But as will be seen below, we will accomplish that goal by using
    the All Possible Subsets Method on these new features, though in a certain ordering.
  prefs: []
  type: TYPE_NORMAL
- en: Our first subset to check will be PC1; then the pair PC1 and PC2; then the triple
    PC1, PC2, PC3; and so on. That means only 90 subsets in all, and typically we
    stop well short of 90 anyway.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll see that it will be a lot easier to choose subsets among these new features
    than among the original 90.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3.1 PC Properties
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The PCs have two special properties. First, they are uncorrelated; roughly speaking,
    the value of one has no effect on the others. In that sense, we might think of
    them as not duplicating each other. Why is that important? If, after reducing
    our number of predictors, some of them were to partially duplicate each other,
    it would seem that we should reduce the number even further. Thus having uncorrelated
    features means unduplicated features, and we feel like we’ve achieved a minimal,
    nonduplicative set.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the PCs are arranged in order of decreasing variance. Since a feature
    with small variance is essentially constant, it probably won’t be a useful feature.
    Accordingly, we might retain just the PCs that are of substantial variance. Since
    the variances are ordered, that means retaining, say, the *m* PCs having the largest
    variances. Note that *m* then becomes another hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: Each PC is a *linear combination* of our original features—that is, a PC is
    a sum of constants times the features. If, for instance, the latter were Height,
    Weight, and Age, a PC might be 0.12 Height + 0.59 Weight − 0.02 Age. Recall from
    algebra that these numbers, 0.12, 0.59, and −0.02, are called *coefficients*.
  prefs: []
  type: TYPE_NORMAL
- en: In ML, one’s focus is prediction rather than description. For instance, in the
    bike rental data, someone may be interested in investigating *how* the features
    affect ridership, say, how many extra riders we might have on holidays. That would
    be a description application, which is not usually a concern in ML. So, those
    coefficients, such as 0.12, 0.59, and −0.02, are not very relevant for us. Instead,
    the point is that we are creating new features as combinations of the original
    ones, and we may not examine the coefficients themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these new features, we have many fewer candidate sets to choose from:
    the first PC, the first two PCs, the first three PCs, and so on. We are choosing
    from just 90 candidate feature sets rather than the unimaginable 2^(90) we would
    have if we looked at all possible subsets. We can choose among those subsets using
    cross-validation or another method introduced below. Remember, smaller sets save
    computation time (for k-NN now and other methods later) and help avoid overfitting
    and p-hacking.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3.2 PCA in the Million Song Dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Base-R includes several functions to do PCA, including the one we’ll use here,
    `prcomp()`. CRAN also has packages that implement especially fast PCA algorithms,
    such as `RSpectra`, which are very useful for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll mainly use `qePCA()`, a `regtools` function that wraps `prcomp()`, but
    you should get at least some exposure to the latter function itself, as follows.
    Here is the call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Remember, PCA is for the *X* data, not *Y*, so we skipped the year field here,
    which is in column 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let’s say we’ve decided to use the first 20 PCs. We do this
    by retaining the first 20 columns of the `rotation` component of the output, which
    contains the coefficients of the PCs (such as the 0.59 and so on above). Let’s
    first understand this component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'So, `rt` has 90 rows and 90 columns, with numerical entries. We see that the
    names of the rows and columns are `V2`, `V3`, . . . , and `PC1`, `PC2`, . . .
    , respectively. The row names come from our feature names (the first is *Y*, not
    a feature):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The column names are for the PCs. We have 90 features, thus 90 PCs, named `PC1`,
    `PC2`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we have said that we want to use only the first 20 PCs for
    dimension reduction. Thus we will discard columns PC21 through PC90 in `rt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we’re now using only those first 20 PCs. Now we convert our original data
    accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is our new *X* data. We’ll look at how to use it shortly, but first, what
    happened in that `predict()` call? The function is not actually predicting anything
    here. It simply converts the original features to new ones. Let’s take a closer
    look at that.
  prefs: []
  type: TYPE_NORMAL
- en: First, recall that, in R, `predict()` is a *generic* function (see [Section
    1.5.1](ch01.xhtml#ch01lev5sec1)). The `prcomp()` function returns an object of
    class `'prcomp'` (check this by typing `class(pcout)` for the data above). So,
    the call to `predict()` here gets relayed to `predict.prcomp()`, which the R people
    wrote for the purpose here—to do conversion from old features to new ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, our call `predict(pcout,yr[,-1])` tells R, “Please convert the features
    in `yr[,-1]` to PC features, based on what’s in `pcout`.” Since we previously
    had altered `pcout$rotation` to use just 20 PCs, now calling `predict()` on `pcout`
    will generate a new 20-column data frame to use instead of our original 90-column
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We have the same number of rows as in the old *X*—as we should since it’s still
    the song data (albeit transformed), with 515,345 songs. But now we have 20 columns
    instead of 90, representing the fact that we now have only 20 features. Note,
    by the way, that none of these new columns was a column in `yr`; each new column
    is a certain mixture of the original ones.
  prefs: []
  type: TYPE_NORMAL
- en: Now, how would we do k-NN prediction using PCA? Imagine that we have a new case
    whose 90 audio measures are the same as the eighth song in our dataset, except
    that there is a value 32.6 in the first column. Let’s store that in `w`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we changed our *X* data in our training set to PCA form, we’ll do the
    same for `w`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We would then call `qeKNN()` on our new data.
  prefs: []
  type: TYPE_NORMAL
- en: To do so directly, though, would be tedious, and we’ll usually use the labor-saving
    `qePCA()` function instead. But let’s put prediction aside for the moment while
    we discuss the issue of choosing the number of PCs to use.
  prefs: []
  type: TYPE_NORMAL
- en: '***4.3.4 But Now We Have Two Hyperparameters***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before now, to attain the most accurate predictions, we only needed to try a
    range of values for the number of nearest neighbors *k*, but in this case, we’ll
    have to test a range of values for both *k* and the number of PCs *m*. It’s nothing
    new, of course, since we always needed to decide which features to use; overfitting
    can arise from having both too small a value of *k* and too many features. Now,
    by converting to PCA, we at least have formalized the latter in a hyperparameter
    *m*. So, we’ve actually made life a little easier.
  prefs: []
  type: TYPE_NORMAL
- en: Say we try each of 10 *k* values paired with each of 10 *m* values and find
    the holdout MAPE for each pair. We would then choose to use the pair with the
    smallest MAPE. But that’s 10 × 10 = 100 pairs to try, say, with K-fold cross-validation
    applied to each pair (that is, running many holdout sets for each pair).
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall there are two major problems with this. First, each pair will take substantial
    computation time, making for a very long total run time, and second, we should
    be concerned about potential p-hacking (see [Section 1.13](ch01.xhtml#ch01lev13)):
    one of those 100 pairs may seem to predict song release year especially accurately,
    simply by coincidence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an alternative, we might try to choose *m* directly. A common intuitive
    approach is to look at the variances of the PCs (squares of standard deviations).
    A variable with small variance is essentially constant and thus presumably of
    little or no use in predicting *Y*. So, the idea is to discard PCs having small
    variance. Let’s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: That `e` notation means powers of 10\. The first one, for instance, means 4.471212
    × 10⁶.
  prefs: []
  type: TYPE_NORMAL
- en: We see the variances rapidly decrease. The twelfth one is about 90,000, which
    is tiny relative to the first, at over 4 million. We might therefore decide to
    take *m* = 12\. Once we’ve done that, we are back to the case of choosing just
    one hyperparameter, *k*.
  prefs: []
  type: TYPE_NORMAL
- en: There is still no magic formula for choosing the number of PCs *m*, but the
    point is that once we decide on a value of *m*, we can now choose *k* separately.
    We may, for instance, choose *m* in this intuitive manner and then use cross-validation
    to get *k*. This may be easier than finding the best (*k*, *m*) pair simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is common for analysts to look at those variances as a cumulative proportion
    of the total. Here R’s `cumsum()` (cumulative sums) function will come in handy.
    This is how that function works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s apply that to the PC variances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: So, if, for example, we were to take *m* = 12, our chosen PCs would make up
    about 89 percent of total variance in this data. Or we might think that’s too
    much dimension reduction and opt for, say, a 95 percent cutoff, thus using *m*
    = 23 PCs.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Note the phrasing “might,” “if, for example,” and “say” here. As we’ve seen
    before, for many things in ML, there are no mechanical, formulaic “recipes” for
    one’s course of action. Once again, keep in mind that ML is an art, not a science.
    And one becomes skilled in that art with experience, not by recipes.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***4.3.5 Using the qePCA() Wrapper***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our goal, then, is to first use PCA for dimension reduction and then do our
    k-NN prediction with the first *m* PCs as features. To do this, the following
    rather elaborate set of actions would need to be performed. Fortunately, there
    is a function to automate these steps, but as usual, we need to understand those
    steps first before turning to the convenience function.
  prefs: []
  type: TYPE_NORMAL
- en: Say the *X* and *Y* portions of our training data are stored in `trnX` and `trnY`,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: We call `prcomp(trnX)` to calculate the PC coefficients. Let’s name this result
    `pcout`, as above.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We limit the number of columns in `pcout$rotation` to reflect the desired number
    of PCs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We call `predict(pcout,trnX)` to convert our *X* data to PC form, say, `pcX`.
    Now our new training set consists of `pcX` and `trnY`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We now apply `qeKNN()` on `pcX` and `trnY`. Say we name the result `knnout`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subsequently, when new *X* data comes in, say, `newX`, we would first call `predict(pcout,newX)`
    to convert to PC form. Say we name the results `pcNewx`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For our *Y* prediction, we then call `predict(knnout,pcNewX)` to obtain our
    predicted *Y*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By the way, did you notice that we are using `predict()` in two different ways
    here, one to convert to PC form and another to do k-NN prediction? Here we see
    the R concept of generic functions in action. In that first context, R relays
    the `predict()` call to `predict.prcomp()`, while in the second, the relay is
    to `predict.qeKNN()`.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of steps in the procedure above, but it does have a pattern,
    which implies that we should automate it with code. That is the purpose of the
    wrapper function `qePCA()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With `qePCA()`, one specifies the data and *Y* column name, as in other `qe*`-series
    functions, and also specifies the following: the desired ML method (say, k-NN),
    the usual hyperparameters for that method (*k*), and the desired proportion of
    total variance for the PCs.'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the call
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: states that we want to predict the year (`V1`) in the song data using k-NN and
    *k* = 25\. It also says we want as many PCs as will give us 85 percent of the
    total variance of the features.
  prefs: []
  type: TYPE_NORMAL
- en: So, how well do we predict using *k* = 25 and 85 percent total variance?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In guessing which year a song was released, we will, on average, be off by about
    7 years.
  prefs: []
  type: TYPE_NORMAL
- en: 'To later predict a new case—say, `w`, the example song we used in [Section
    4.3.3.2](ch04.xhtml#ch04lev3sec3sec2) —we would make the call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: So, we’d guess that such a song was released around the year 1995.
  prefs: []
  type: TYPE_NORMAL
- en: All of this is far simpler than the above multistep process; `qePCA()` saves
    us a lot of work!
  prefs: []
  type: TYPE_NORMAL
- en: '***4.3.6 PCs and the Bias-Variance Trade-off***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now that we know how to use PCA for dimension reduction and subsequent prediction,
    let’s return to the issue of how to choose the number of nearest neighbors *k*
    and number of PCs *m*. Note that choosing *m* is equivalent to choosing the variance
    proportion in the last argument in `qePCA()`.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, we have a bias-variance issue, for both *k* and *m*. We’ve discussed
    the trade-off in terms of *k* before; what about *m*?
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually, the situation for *m* is not new either. Recall what we said in [Section
    3.1.1](ch03.xhtml#ch03lev1sec1) regarding the `mlb` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: We might predict weight from height and age. But what if we were to omit height
    from our feature set? That would induce a bias. Roughly speaking, we’d be tacitly
    assuming everyone is of middling height, which would result in our tending to
    overpredict the weight of shorter players while underpredicting that of the taller
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, omitting a feature induces a bias. Or, equivalently, adding
    features reduces bias. Since the PCs *are* features, we see that the more PCs
    we use, the smaller the bias. But that same section goes on to point out that
    the more features we have, the higher the variance in our predictions, which is
    not good. Note, too, that using more PCs results in longer computation time.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this, let’s devise a little experiment to investigate the effect
    of varying *m*, the number of PCs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our song dataset is rather large, let’s consider a random subset and
    see how well we do with proportions of total variance at various levels. We’ll
    use a subset of, say, 25,000, and check computation time and prediction accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The results are shown in [Table 4-1](ch04.xhtml#ch4tab1). The first column shows
    the proportion of total variance, leading to a number of PCs *m* shown in the
    fourth column. In viewing the latter, remember that the full number of possible
    PCs is 90\. The second column shows run time. It increases with *m*, of course;
    a larger *m* means k-NN must do more computation.
  prefs: []
  type: TYPE_NORMAL
- en: The most important column in this investigation is the third, the MAPE values.
    We see that for a large increase in computation time, we attain only a moderate
    reduction in MAPE. Moreover, the best MAPE uses only 11 of the 90 PCs.
  prefs: []
  type: TYPE_NORMAL
- en: However, always keep in mind that these MAPE values are subject to sampling
    variation. They come from holdout sets, and as you know, holdout sets are randomly
    chosen. For each variance proportion level, we should look at several holdout
    sets, not just one, using cross-validation. We could do this by applying `replicMeans()`
    to the `qePCA()` call, though it would be time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: That means we cannot be sure that *m* = 11 is best. At the least, though, we
    see that the data indicates that we should probably use a lot fewer than 90 PCs.
    Also, in addition to the famous Bias-Variance Trade-off, there is the trade-off
    involving the analyst’s time. We may feel that setting *m* = 11 is good enough.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 4-1:** Behavior for Different Values of *m*'
  prefs: []
  type: TYPE_NORMAL
- en: '| **pcaProp** | **Time (s.)** | **MAPE** | **# of PCs** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.15 | 1.137 | 8.40020 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.25 | 1.230 | 7.83712 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.35 | 2.051 | 8.12444 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.45 | 7.222 | 7.46536 | 11 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.55 | 17.812 | 7.88648 | 16 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.65 | 35.269 | 7.66808 | 24 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.75 | 51.041 | 7.55740 | 33 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.85 | 72.916 | 7.85736 | 46 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.95 | 94.761 | 7.72288 | 66 |'
  prefs: []
  type: TYPE_TB
- en: Furthermore, remember as discussed in [Section 3.1.2](ch03.xhtml#ch03lev1sec2),
    the larger *n* is, the larger we can afford to make *p*. In the above analysis,
    we had *n* = 25000 and *p* = *m* and settled on *m* = 11\. But for the full dataset,
    *n* = 500000, presumably we should use more than 11 PCs.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we may be tempted to run the above code on the full dataset. Yet even
    for *n* = 25000, the run time was about half an hour; it would be many hours for
    the full dataset of more than 500,000 records. So, we may just settle for *m*
    = 11 and possibly do a more refined analysis later if time permits.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 The Curse of Dimensionality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *Curse of Dimensionality (CoD)* says that ML gets harder and harder as the
    number of features grows. For instance, mathematical theory shows that in high
    dimensions, every point is approximately the same distance to every other point.
    Let’s briefly discuss the intuition underlying this bizarre situation. Clearly,
    it has implications for k-NN, which relies on distances, and indeed for some,
    if not all, other ML methods as well.
  prefs: []
  type: TYPE_NORMAL
- en: To get a rough idea of the CoD, consider data consisting of students’ grades
    in mathematics, literature, history, geography, and so on. The distance between
    the data vectors of Students A and B would be the square root of
  prefs: []
  type: TYPE_NORMAL
- en: (math grade*[A]* − math grade*[B]*)² + (lit grade*[A]* − lit grade*[B]*)² +
  prefs: []
  type: TYPE_NORMAL
- en: (history grade*[A]* − history grade*[B]*)² + (geo grade*[A]* − geo grade*[B]*)²
  prefs: []
  type: TYPE_NORMAL
- en: That expression is a sum, and one can show that sums with a large number of
    terms (only four here, but we could have many more) have small standard deviations
    relative to their means. A quantity with small standard deviation is nearly constant,
    so in high dimensions—that is, in settings with large *p* (a large number of features)—distances
    are nearly constant.
  prefs: []
  type: TYPE_NORMAL
- en: This is why k-NN fares poorly in high dimensions. This issue is not limited
    to k-NN; much of the ML toolbox has similar problems. The computation of linear/logistic
    regression (see [Chapter 8](ch08.xhtml)) involves a sum of *p* terms, and similar
    computations arise in support vector machines and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, lots of problems arise in high dimensions. Some analysts lump them
    all together into the CoD. Whatever one counts as CoD, clearly, higher dimensions
    are a challenge—all the more reason to perform dimension reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Other Methods of Dimension Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dimension reduction is one of the most actively researched and debated issues
    in ML and statistics. While this chapter focused on PCA, a common approach to
    the problem, there are a number of other approaches.
  prefs: []
  type: TYPE_NORMAL
- en: '***4.5.1 Feature Ordering by Conditional Independence***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One approach that I find useful (and contributed slightly to its development)
    is *Feature Ordering by Conditional Independence (FOCI)*. It is based on solid
    mathematical principles (too complex to explain here) and works quite well, I’ve
    found.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `qeML` package includes a FOCI wrapper, `qeFOCI`. Here is the call in its
    basic form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the computational requirement can be large for this method, there are
    also parallelized options, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This will split the computation into chunks, with each of your computer’s cores
    working on one chunk.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try FOCI on the census data from [Section 3.2.3](ch03.xhtml#ch03lev2sec3):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note that `qeFOCI()` converts R factors to dummy variables. So, we see assessment
    of, for instance, each of the occupations. The one coded 102 seems to have good
    predictive power, while occupation 106 perhaps less so.
  prefs: []
  type: TYPE_NORMAL
- en: The `stepT` component of the output gives a type of correlation; the more predictors
    we add, the greater the predictive collective power of the variables. If we wish
    to be more conservative, we might cut off our choice when this correlation seems
    to level off, say, after 6 or 7 variables in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'How about the song data? To ease the computational burden, I took a 10 percent
    subsample of the data and ran with 2 cores. Yet even then, it ran for more than
    20 minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: There were 2 cores used, and each applied FOCI to its chunk of data. The first
    core chose variables 1, 14, 2, and so on, while the second chose some of the same
    variables but also different ones; for instance, the second core chose variable
    50, but the first did not. We designed the parallel version of the algorithm to
    take the union of the two sets of variables, so variable 50 does appear in the
    final list. Only the top 7 were chosen, though, as the correlation did not increase
    past that point.
  prefs: []
  type: TYPE_NORMAL
- en: '***4.5.2 Uniform Manifold Approximation and Projection***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Uniform Manifold Approximation and Projection (UMAP) method is similar in
    usage pattern to PCA in that we find new variables as functions of the original
    ones and then retain only the top few in terms of predictive ability. The difference,
    though, is that with UMAP, the new variables are complex nonlinear functions of
    the old ones.
  prefs: []
  type: TYPE_NORMAL
- en: The `qeML` package has a wrapper for UMAP, `qeUMAP()`. As noted, it is used
    in a similar manner to `qePCA()`. We will not pursue this further here in the
    book, but the reader is urged to give it a try.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Going Further Computationally
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For very large datasets, I highly recommend the `data.table` package. The `bigmemory`
    package can help with memory limitations, though it is for specialists who understand
    computers at the operating system level. Also, for those who know SQL databases,
    there are several packages that interface to such data, such as `RSQLite` and
    `dplyr`.
  prefs: []
  type: TYPE_NORMAL
- en: 4.7 Conclusions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we’ve developed an understanding of how computation issues
    interact with the Bias-Variance Trade-off when we work with largescale data. We
    may, for instance, wish to restrict our number of features well before reaching
    the point at which adding more variables is statistically unprofitable. Our featured
    remedy here has been PCA, though we have briefly cited others.
  prefs: []
  type: TYPE_NORMAL
