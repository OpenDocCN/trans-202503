<html><head></head><body>
<h2 class="h2" id="ch09"><span epub:type="pagebreak" id="page_151"/><span class="big">9</span><br/>SERVICE AND INGRESS NETWORKS</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">A decent amount of complexity was involved in creating a cluster-wide network so that all of our Pods could communicate with one another. At the same time, we still don’t have all of the networking functionality we need to build scalable, resilient applications. We need networking that supports load balancing our application components across multiple instances and provides the ability to send traffic to new Pod instances as existing instances fail or need to be upgraded. Additionally, the Pod network is designed to be private, meaning that it is directly reachable only from within the cluster. We need additional traffic routing so that external users can reach our application components running in containers.</p>&#13;
<p class="indent">In this chapter, we’ll look at Service and Ingress networking. Kubernetes Service networking provides an entire additional networking layer on top <span epub:type="pagebreak" id="page_152"/>of Pod networking, including dynamic discovery and load balancing. We’ll see how this networking layer works and how we can use it to expose our application components to the rest of the cluster as scalable, resilient services. We’ll then look at how Ingress configuration provides traffic routing for these Services to expose them to external users.</p>&#13;
<h3 class="h3" id="ch00lev1sec40">Services</h3>&#13;
<p class="noindent">Putting together Deployments and overlay networking, we have the ability to create multiple identical container instances with a unique IP address for each. Let’s create an NGINX Deployment to illustrate:</p>&#13;
<p class="noindent6"><em>nginx-deploy.yaml</em></p>&#13;
<pre>---&#13;
kind: Deployment&#13;
apiVersion: apps/v1&#13;
metadata:&#13;
  name: nginx&#13;
spec:&#13;
  replicas: 5&#13;
  selector:&#13;
    matchLabels:&#13;
      app: nginx&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: nginx&#13;
    spec:&#13;
      containers:&#13;
      - name: nginx&#13;
        image: nginx</pre>&#13;
<p class="indent">This is similar to Deployments we’ve seen previously. In this case we’re asking Kubernetes to maintain five Pods for us, each running an NGINX web server.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on getting set up.</em></p>&#13;
</div>&#13;
<p class="indent">The automated scripts have already placed this file in <em>/opt</em>, so we can apply it to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/nginx-deploy.yaml</span>&#13;
deployment.apps/nginx created</pre>&#13;
<p class="indent">After these Pods are running, we can check that they’ve been distributed across the cluster and each one has an IP address:</p>&#13;
<pre><span epub:type="pagebreak" id="page_153"/>root@host01:~# <span class="codestrong1">kubectl get pods -o wide</span>&#13;
NAME                     READY   STATUS    ... IP                NODE    ...&#13;
nginx-6799fc88d8-2wqc7   1/1     Running   ... 172.31.239.231   host01   ...&#13;
nginx-6799fc88d8-78bwx   1/1     Running   ... 172.31.239.229   host01   ...&#13;
nginx-6799fc88d8-dtx7s   1/1     Running   ... 172.31.89.240    host02   ...&#13;
nginx-6799fc88d8-wh479   1/1     Running   ... 172.31.239.230   host01   ...&#13;
nginx-6799fc88d8-zwx27   1/1     Running   ... 172.31.239.228   host01   ...</pre>&#13;
<p class="indent">If these containers were merely clients of some server, that might be all we need to do. For example, if our application architecture was driven by sending and receiving messages, as long as these containers could connect to the messaging server, they’d be able to function as required. However, because these containers act as servers, clients need to be able to find them and connect.</p>&#13;
<p class="indent">As it is, our separate NGINX instances aren’t very practical for clients to use. Sure, it’s possible to connect to any one of these NGINX server Pods directly. For example, we can communicate with the first one in the list using its IP address:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">curl -v http://<span class="codeitalic1">172.31.239.231</span></span>&#13;
*   Trying 172.31.239.231:80...&#13;
* Connected to 172.31.239.231 (172.31.239.231) port 80 (#0)&#13;
&gt; GET / HTTP/1.1&#13;
...&#13;
&lt; HTTP/1.1 200 OK&#13;
&lt; Server: nginx/1.21.3&#13;
...</pre>&#13;
<p class="indent">Unfortunately, just choosing one instance is not going to provide load balancing or failover. Additionally, we don’t have any way of knowing ahead of time what the Pod IP address is going to be, and every time we make any changes to the Deployment, the Pods will be re-created and get new IP addresses.</p>&#13;
<p class="indent">The solution to this situation needs to have two main features. First, we need to have a well-known name that clients can use to find a server. Second, we need a consistent IP address so that when a client has identified a server, it can continue to use the same address for connections even as Pod instances come and go. This is exactly what Kubernetes provides with a <em>Service</em>.</p>&#13;
<h4 class="h4" id="ch00lev2sec62">Creating a Service</h4>&#13;
<p class="noindent">Let’s create a Service for our NGINX Deployment and see what that gets us. <a href="ch09.xhtml#ch09list1">Listing 9-1</a> presents the resource YAML file.</p>&#13;
<p class="noindent6"><em>nginx-service.yaml</em></p>&#13;
<pre>---&#13;
kind: Service&#13;
apiVersion: v1&#13;
<span epub:type="pagebreak" id="page_154"/>metadata:&#13;
  name: nginx&#13;
spec:&#13;
  selector:&#13;
    app: nginx&#13;
  ports:&#13;
  - protocol: TCP&#13;
    port: 80&#13;
    targetPort: 80</pre>&#13;
<p class="caption" id="ch09list1"><em>Listing 9-1: NGINX Service</em></p>&#13;
<p class="indent">First, a Service has a <span class="literal">selector</span> much like a Deployment. This selector is used in the same way: to identify the Pods that will be associated with the Service. However, unlike a Deployment, a Service does not manage its Pods in any way; it simply routes traffic to them.</p>&#13;
<p class="indent">The traffic routing is based on the ports we identify in the <span class="literal">ports</span> field. Because the NGINX server is listening on port 80, we need to specify that as the <span class="literal">targetPort</span>. We can use any <span class="literal">port</span> we want, but it’s simplest to keep it the same, especially as 80 is the default port for HTTP.</p>&#13;
<p class="indent">Let’s apply this Service to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/nginx-service.yaml</span> &#13;
service/nginx created</pre>&#13;
<p class="indent">We can now see that the Service was created:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get services</span>&#13;
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE&#13;
kubernetes   ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP   14d&#13;
nginx        ClusterIP   10.100.221.220   &lt;none&gt;        80/TCP    25s</pre>&#13;
<p class="indent">This <span class="literal">nginx</span> Service has the default type of <span class="literal">ClusterIP</span>. Kubernetes has automatically assigned a cluster IP address for this Service. The IP address is in an entirely different address space from that of our Pods.</p>&#13;
<p class="indent">Using the selector, this Service will identify our NGINX server Pods and automatically start load balancing traffic to them. As Pods matching the selector come and go, the Service will automatically update its load balancing accordingly. As long as the Service exists, it will keep the same IP address, so clients have a consistent way of finding our NGINX server instances.</p>&#13;
<p class="indent">Let’s verify that we can reach an NGINX server through the Service:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">curl -v http://<span class="codeitalic1">10.100.221.220</span></span>&#13;
*   Trying 10.100.221.220:80...&#13;
* Connected to 10.100.221.220 (10.100.221.220) port 80 (#0)&#13;
&gt; GET / HTTP/1.1&#13;
<span epub:type="pagebreak" id="page_155"/>...&#13;
&lt; HTTP/1.1 200 OK&#13;
&lt; Server: nginx/1.21.3&#13;
...</pre>&#13;
<p class="indent">We can see that the Service has correctly identified all five NGINX Pods:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe service nginx</span>&#13;
Name:              nginx&#13;
Namespace:         default&#13;
...&#13;
Selector:          app=nginx&#13;
...&#13;
Endpoints:         172.31.239.228:80,172.31.239.229:80,172.31.239.230:80 &#13;
+ 2 more...&#13;
...</pre>&#13;
<p class="indent">The <span class="literal">Endpoints</span> field shows that the Service is currently routing traffic to all five NGINX Pods. As a client, we don’t need to know which Pod was used to handle our request. We interact solely with the Service IP address and allow the Service to choose an instance for us.</p>&#13;
<p class="indent">Of course, for this example, we had to look up the IP address of the Service. To make it easier on clients, we still should provide a well-known name.</p>&#13;
<h4 class="h4" id="ch00lev2sec63">Service DNS</h4>&#13;
<p class="noindent">Kubernetes provides a well-known name for each Service through a DNS (Domain Name System) server that is dynamically updated with the name and IP address of every Service in the cluster. Each Pod is configured with this DNS server such that a Pod can use the name of the Service to connect to an instance.</p>&#13;
<p class="indent">Let’s create a Pod that we can use to try this out:</p>&#13;
<p class="noindent6"><em>pod.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: pod&#13;
spec:&#13;
  containers:&#13;
  - name: pod&#13;
    image: alpine&#13;
    command: &#13;
      - "sleep"&#13;
      - "infinity"</pre>&#13;
<p class="indent">We’re using <span class="literal">alpine</span> rather than <span class="literal">busybox</span> as the image for this Pod because we’ll want to use some DNS commands that require us to install a more full-featured DNS client.</p>&#13;
<div class="note">&#13;
<p class="notet"><span epub:type="pagebreak" id="page_156"/><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>BusyBox makes a great debug image for Kubernetes clusters because it’s tiny and has many useful commands. However, in the interest of keeping BusyBox tiny, it’s typical for the commands to include only the most popular options. Alpine makes a great alternative for debugging. The default Alpine image uses BusyBox to provide many of its initial commands, but it’s possible to replace them with a full-featured alternative by just installing the appropriate package.</em></p>&#13;
</div>&#13;
<p class="indent">Next, create the Pod:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/pod.yaml</span> &#13;
pod/pod created</pre>&#13;
<p class="indent">After it’s running, let’s use it to connect to our NGINX Service, as demonstrated in <a href="ch09.xhtml#ch09list2">Listing 9-2</a>.</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod -- wget -O - http://nginx</span>&#13;
Connecting to nginx (10.100.221.220:80)&#13;
...&#13;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;&#13;
...</pre>&#13;
<p class="caption" id="ch09list2"><em>Listing 9-2: Connect to NGINX Service</em></p>&#13;
<p class="indent">We were able to use the name of the Service, <span class="literal">nginx</span>, and that name resolved to the Service IP address. This worked because our Pod is configured to talk to the DNS server that’s built in to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod -- cat /etc/resolv.conf</span> &#13;
search default.svc.cluster.local svc.cluster.local cluster.local &#13;
nameserver 10.96.0.10&#13;
options ndots:5</pre>&#13;
<p class="indent">We print out the file <em>/etc/resolv.conf</em> inside the container because this is the file that is used to configure DNS.</p>&#13;
<p class="indent">The name server <span class="literal">10.96.0.10</span> referenced is itself a Kubernetes Service, but it’s in the <span class="literal">kube-system</span> Namespace, so we need to look there for it:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n kube-system get services</span>&#13;
NAME            TYPE       CLUSTER-IP      ... PORT(S)                  AGE&#13;
kube-dns        ClusterIP  10.96.0.10      ... 53/UDP,53/TCP,9153/TCP   14d&#13;
metrics-server  ClusterIP  10.105.140.176  ... 443/TCP                  14d</pre>&#13;
<p class="indent">The <span class="literal">kube-dns</span> Service connects to a DNS server Deployment called CoreDNS that listens for changes to Services in the Kubernetes cluster. CoreDNS updates the DNS server configuration as required to stay up to date with the current cluster configuration.</p>&#13;
<h4 class="h4" id="ch00lev2sec64">Name Resolution and Namespaces</h4>&#13;
<p class="noindent">DNS names in a Kubernetes cluster are based on the Namespace as well as the cluster domain. Because our Pod is in the <span class="literal">default</span> Namespace, it has <span epub:type="pagebreak" id="page_157"/>been configured with a search path of <span class="literal">default.svc.cluster.local</span> as the first entry in the list, so it will search the <span class="literal">default</span> Namespace first when looking for Services. This is why we were able to use the bare Service name <span class="literal">nginx</span> to find the <span class="literal">nginx</span> Service—that Service is also in the <span class="literal">default</span> Namespace.</p>&#13;
<p class="indent">We could have also found the same Service using the fully qualified name:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod -- wget -O - http://nginx.default.svc</span>&#13;
Connecting to nginx.default.svc (10.100.221.220:80)&#13;
...&#13;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;&#13;
...</pre>&#13;
<p class="indent">Understanding this interaction between Namespaces and Service lookup is important. One common deployment pattern for a Kubernetes cluster is to deploy the same application multiple times to different Namespaces and use simple hostnames for application components to communicate with one another. This pattern is often used to deploy a “development” and “production” version of an application to the same cluster. If we’re planning to use this pattern, we need to be sure that we stick to bare hostnames when our application components try to find one another; otherwise, we could end up communicating with the wrong version of our application.</p>&#13;
<p class="indent">Another important configuration item in <em>/etc/resolv.conf</em> is the <span class="literal">ndots</span> entry. The <span class="literal">ndots</span> entry tells the hostname resolver that when it sees a hostname with four or fewer dots, it should try appending the various search domains <em>prior</em> to performing an absolute search without any domain appended. This is critical to make sure that we try to find services inside the cluster before reaching outside the cluster.</p>&#13;
<p class="indent">As a result, when we used the name <span class="literal">nginx</span> in <a href="ch09.xhtml#ch09list2">Listing 9-2</a>, the DNS resolver within our container immediately tried <span class="literal">nginx.default.svc.cluster.local</span> and found the correct Service.</p>&#13;
<p class="indent">To make sure this is clear, let’s try one more example: looking up a Service in another Namespace. The <span class="literal">kube-system</span> Namespace has a <span class="literal">metrics-server</span> Service. To find it, let’s use the standard host lookup <span class="literal">dig</span> command in our Pod.</p>&#13;
<p class="indent">Our Pod is using Alpine Linux, so we need to install the <span class="literal">bind-tools</span> package to get access to <span class="literal">dig</span>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod -- apk add bind-tools</span>&#13;
...&#13;
OK: 13 MiB in 27 packages</pre>&#13;
<p class="indent">Now, let’s try looking up <span class="literal">metrics-server</span> using the bare name first:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod -- dig +search metrics-server</span>&#13;
...&#13;
;; Got answer:&#13;
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: SERVFAIL, id: 38423&#13;
...</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_158"/>We add the <span class="literal">+search</span> flag onto the command to tell <span class="literal">dig</span> to use the search path information from <em>/etc/resolv.conf</em>. However, even with that flag, we don’t find the Service, because our Pod is in the <span class="literal">default</span> Namespace, so the search path doesn’t lead <span class="literal">dig</span> to look in the <span class="literal">kube-system</span> Namespace.</p>&#13;
<p class="indent">Let’s try again, this time specifying the correct Namespace:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod -- dig +search metrics-server.kube-system</span>&#13;
...&#13;
;; ANSWER SECTION:&#13;
metrics-server.kube-system.svc.cluster.local. 30 IN A 10.105.140.176&#13;
...</pre>&#13;
<p class="indent">This lookup works, and we are able to get the IP address for the <span class="literal">metrics-server</span> Service. It works because the search path includes <span class="literal">svc.cluster.local</span> as its second entry. After initially trying <span class="literal">metrics-server.kube-system.default.svc.cluster.local</span>, which doesn’t work, <span class="literal">dig</span> then tries <span class="literal">metrics-server.kube-system.svc.cluster.local</span>, which does.</p>&#13;
<h4 class="h4" id="ch00lev2sec65">Traffic Routing</h4>&#13;
<p class="noindent">We’ve seen how to create and use Services, but we haven’t yet looked at how the actual traffic routing works. It turns out that Service network traffic works in a way that’s completely different from the overlay networks we saw in <a href="ch08.xhtml#ch08">Chapter 8</a>, which can lead to some confusion.</p>&#13;
<p class="indent">For example, because we can use <span class="literal">wget</span> to reach an NGINX server instance using the <span class="literal">nginx</span> Service name, we might expect to be able to use <span class="literal">ping</span>, as well, but that doesn’t work:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod -- ping -c 3 nginx</span>&#13;
PING nginx (10.100.221.220): 56 data bytes&#13;
&#13;
--- nginx ping statistics ---&#13;
3 packets transmitted, 0 packets received, 100% packet loss&#13;
command terminated with exit code 1</pre>&#13;
<p class="indent">Name resolution worked as expected, so <span class="literal">ping</span> knew what destination IP address to use for its ICMP packets. But there was no reply from that IP address. We could look at every host and container network interface in our cluster and never find an interface that carries the Service IP address of <span class="literal">10.100.221.220</span>. So how is our HTTP traffic getting through to an NGINX Service instance?</p>&#13;
<p class="indent">On every node in our cluster, there is a component called <span class="literal">kube-proxy</span> that configures traffic routing for Services. <span class="literal">kube-proxy</span> is run as a DaemonSet in the <span class="literal">kube-system</span> Namespace. Each <span class="literal">kube-proxy</span> instance watches for changes to Services in the cluster and configures the Linux firewall to route traffic.</p>&#13;
<p class="indent">We can use <span class="literal">iptables</span> commands to look at the firewall configuration to see how <span class="literal">kube-proxy</span> has configured traffic routing for our <span class="literal">nginx</span> Service:</p>&#13;
<pre>   <span epub:type="pagebreak" id="page_159"/>root@host01:~# <span class="codestrong1">iptables-save | grep 'default/nginx cluster IP'</span>&#13;
<span class="ent">➊</span> -A KUBE-SERVICES ! -s 172.31.0.0/16 -d 10.100.221.220/32 -p tcp -m comment &#13;
    --comment "default/nginx cluster IP" -m tcp --dport 80 -j KUBE-MARK-MASQ&#13;
<span class="ent">➋</span> -A KUBE-SERVICES -d 10.100.221.220/32 -p tcp -m comment --comment &#13;
   "default/nginx cluster IP" -m tcp --dport 80 -j KUBE-SVC-2CMXP7HKUVJN7L6M</pre>&#13;
<p class="indent">The <span class="literal">iptables-save</span> command backs up all of the current Linux firewall rules, so it’s useful for printing out all rules. The <span class="literal">grep</span> command searches for the comment string that <span class="literal">kube-proxy</span> applies to the Service rules it creates. In this example, <span class="literal">kube-proxy</span> has created two rules for the Service as a whole. The first rule <span class="ent">➊</span> looks for traffic destined for our Service that is <em>not</em> coming from the Pod network. This traffic must be marked for Network Address Translation (NAT) masquerade so that the source of any reply traffic will be rewritten to be the Service IP address rather than the actual Pod that handles the request. The second rule <span class="ent">➋</span> sends all traffic destined for the Service to a separate rule chain that will send it to a Pod instance. Note that in both cases, the rules only match for TCP traffic that is destined for port 80.</p>&#13;
<p class="indent">We can examine this separate rule chain to see how the actual routing to individual Pods works. Be sure to replace the name of the rule chain in this command with the one shown in the previous output:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">iptables-save | grep</span> <span class="codestrong1"><span class="codeitalic1">KUBE-SVC-2CMXP7HKUVJN7L6M</span></span>&#13;
...&#13;
-A KUBE-SVC-2CMXP7HKUVJN7L6M ... -m statistic --mode random &#13;
  --probability 0.20000000019 -j KUBE-SEP-PIVU7ZHMCSOWIZ2Z&#13;
-A KUBE-SVC-2CMXP7HKUVJN7L6M ... -m statistic --mode random &#13;
  --probability 0.25000000000 -j KUBE-SEP-CFQXKE74QEHFB7VJ&#13;
-A KUBE-SVC-2CMXP7HKUVJN7L6M ... -m statistic --mode random &#13;
  --probability 0.33333333349 -j KUBE-SEP-DHDWEJZ7MGGIR5XF&#13;
-A KUBE-SVC-2CMXP7HKUVJN7L6M ... -m statistic --mode random &#13;
  --probability 0.50000000000 -j KUBE-SEP-3S3S2VJCXSAISE2Z&#13;
-A KUBE-SVC-2CMXP7HKUVJN7L6M ... -j KUBE-SEP-AQWD2Y25T24EHSNI</pre>&#13;
<p class="indent">The output shows five rules, corresponding to each of the five NGINX Pod instances the Service’s selector matched. The five rules together provide random load balancing across all the instances so that each one has an equal chance of being selected for new connections.</p>&#13;
<p class="indent">It may seem strange that the <span class="literal">probability</span> figure increases for each rule. This is necessary because the rules are evaluated sequentially. For the first rule, we want a 20 percent chance of choosing the first instance. However, if we don’t select the first instance, only four instances are left, so we want a 25 percent chance of choosing the second one. The same logic applies until we get to the last instance, which we always want to choose if we’ve skipped all the others.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_160"/>Let’s quickly verify that these rules go to the expected destination (again, be sure to replace the name of the rule chain in this command):</p>&#13;
<pre>root@host01:~# <span class="codestrong1">iptables-save | grep</span> <span class="codestrong1"><span class="codeitalic1">KUBE-SEP-PIVU7ZHMCSOWIZ2Z</span></span>&#13;
...&#13;
-A KUBE-SEP-PIVU7ZHMC ... -s 172.31.239.235/32 ... --comment "default/nginx" -j KUBE-MARK-MASQ&#13;
-A KUBE-SEP-PIVU7ZHMCSOWIZ2Z -p tcp ... -m tcp -j DNAT --to-destination 172.31.239.235:80</pre>&#13;
<p class="indent">This output shows two rules. The first is the other half of the NAT masquerade configuration, as we mark all packets that leave our Pod instance so that they can have their source address rewritten to appear to come from the Service. The second rule is the one that actually routes Service traffic to a specific Pod as it performs a rewrite of the destination address so that a packet originally destined for the Service IP is now destined for a Pod. From there, the overlay networking takes over to actually send the packet to the correct container.</p>&#13;
<p class="indent">With this understanding of how Service traffic is actually routed, it makes sense that our ICMP packets didn’t make it through. The firewall rule that <span class="literal">kube-proxy</span> created applies only to TCP traffic destined for port 80. As a result, there was no firewall rule to rewrite our ICMP packets and therefore no way for them to make it to a networking stack that could reply to them. Similarly, if we have a container that’s listening on multiple ports, we will be able to connect to any of those ports by directly using the Pod IP address, but the Service IP address will route traffic only if we explicitly declare that port in the Service specification. It can be a significant source of confusion when deploying an application where the Pod starts up as expected and listens for traffic, but a misconfiguration of the Service means that the traffic is not being routed to all of the correct destination ports.</p>&#13;
<h3 class="h3" id="ch00lev1sec41">External Networking</h3>&#13;
<p class="noindent">We now have enough layers of networking to meet all of our internal cluster communication needs. Each Pod has its own IP address and has connectivity to other Pods as well as the control plane, and with Service networking we have automatic load balancing and failover based on running multiple Pod instances with a Service. However, we’re still missing the ability for external users to access services running in our cluster.</p>&#13;
<p class="indent">To provide access for external users, we can no longer rely solely on the cluster-specific IP address ranges that we use for Pods and Services, given that external networks don’t recognize those address ranges. Instead, we’ll need a way to allocate externally routable IP addresses to our Services, either by explicitly associating an IP address with a Service or by using an <em>ingress controller</em> that listens to external traffic and routes it to Services.</p>&#13;
<h4 class="h4" id="ch00lev2sec66"><span epub:type="pagebreak" id="page_161"/>External Services</h4>&#13;
<p class="noindent">The <span class="literal">nginx</span> Service we created earlier was a <span class="literal">ClusterIP</span> Service, the default Service type. Kubernetes supports multiple Service types, including Service types that are made for Services that need to be exposed externally:</p>&#13;
<div class="bqparan">&#13;
<p class="noindent5"><span class="codestrong">None</span> Also known as a <em>headless</em> Service, it’s used to enable tracking of selected Pods but without an IP address or any network routing behavior.</p>&#13;
<p class="noindent5"><span class="codestrong">ClusterIP</span> The default Service type that provides tracking of selected Pods, a cluster IP address that is routed internally, and a well-known name in the cluster DNS.</p>&#13;
<p class="noindent5"><span class="codestrong">NodePort</span> Extends <span class="literal">ClusterIP</span> and also provides a port on all nodes in the cluster that is routed to the Service.</p>&#13;
<p class="noindent5"><span class="codestrong">LoadBalancer</span> Extends <span class="literal">NodePort</span> and also uses an underlying cloud provider to obtain an IP address that is externally reachable.</p>&#13;
<p class="noindent5"><span class="codestrong">ExternalName</span> Aliases a well-known Service name in the cluster DNS to some external DNS name. Used to make external resources appear to be in-cluster Services.</p>&#13;
</div>&#13;
<p class="indent">Of these Service types, the <span class="literal">NodePort</span> and <span class="literal">LoadBalancer</span> types are most useful for exposing Services outside the cluster. The <span class="literal">LoadBalancer</span> type seems the most straightforward, as it simply adds an external IP to the Service. However, it requires integration with an underlying cloud environment to create the external IP address when the Service is created, to route traffic from that IP address to the cluster’s nodes, and to create a DNS entry outside the cluster that enables external users to find the Service as a host on some pre-registered domain that we already own, rather than a <span class="literal">cluster.local</span> domain that works only within the cluster.</p>&#13;
<p class="indent">For this reason, a <span class="literal">LoadBalancer</span> Service is most useful for cases in which we know what cloud environment we’re using and we’re creating Services that will live for a long time. For HTTP traffic, we can get most of the benefit of a <span class="literal">LoadBalancer</span> Service by using a <span class="literal">NodePort</span> Service together with an ingress controller, with the added feature of better support for dynamically deploying new applications with new Services.</p>&#13;
<p class="indent">Before moving on to an ingress controller, let’s turn our existing <span class="literal">nginx</span> Service into a <span class="literal">NodePort</span> Service so that we can look at the effect. We can do this using a patch file:</p>&#13;
<p class="noindent6"><em>nginx-nodeport.yaml</em></p>&#13;
<pre>---&#13;
spec:&#13;
  type: NodePort</pre>&#13;
<p class="indent">A patch file allows us to update only the specific fields we care about. In this case, we are updating only the type of the Service. For this to work, we just need to specify that one changed field in its correct position in the hierarchy, which allows Kubernetes to know what field to modify. We don’t <span epub:type="pagebreak" id="page_162"/>need to change the selector or ports for our Service, only the type, so the patch is very simple.</p>&#13;
<p class="indent">Let’s use the patch:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl patch svc nginx --patch-file /opt/nginx-nodeport.yaml</span> &#13;
service/nginx patched</pre>&#13;
<p class="indent">For this command, we must specify the resource to be patched and a patch file to be used. The result is identical to if we had edited the YAML resource file for the Service and then used <span class="literal">kubectl apply</span> again.</p>&#13;
<p class="indent">The Service now looks a little different:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get service nginx</span>&#13;
NAME    TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE&#13;
nginx   NodePort   10.100.221.220   &lt;none&gt;        80:31326/TCP   2h</pre>&#13;
<p class="indent">A <span class="literal">NodePort</span> Service provides all the behavior of a <span class="literal">ClusterIP</span> Service, so we still have a cluster IP associated with our <span class="literal">nginx</span> Service. The Service even kept the same cluster IP. The only change is the <span class="literal">PORT</span> field now shows that the Service port 80 is attached to node port 31326.</p>&#13;
<p class="indent">The <span class="literal">kube-proxy</span> Service on every cluster node is listening on this port (be sure to use the correct node port for your Service):</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ss -nlp | grep</span> <span class="codestrong1"><span class="codeitalic1">31326</span></span>&#13;
tcp   LISTEN 0  4096  .0.0.0:31326 ... users:(("kube-proxy",pid=3339,fd=15))</pre>&#13;
<p class="indent">As a result, we can still use the <span class="literal">nginx</span> Service name inside our Pod, but we can also use the NodePort from the host:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti pod -- wget -O - http://nginx</span>&#13;
Connecting to nginx (10.100.221.220:80)&#13;
...&#13;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;&#13;
...&#13;
root@host01:~# <span class="codestrong1">wget -O - http://host01:<span class="codeitalic1">31326</span></span>&#13;
...&#13;
Connecting to host01 (host01)|127.0.2.1|:31326... connected.&#13;
...&#13;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&#13;
...</pre>&#13;
<p class="indent">Because <span class="literal">kube-proxy</span> is listening on all network interfaces, we’ve successfully exposed this Service to external users.</p>&#13;
<h4 class="h4" id="ch00lev2sec67">Ingress Services</h4>&#13;
<p class="noindent">Although we’ve successfully exposed our NGINX Service outside the cluster, we still don’t provide a great user experience for external users. To use the <span class="literal">NodePort</span> Service, external users will need to know the IP address of at least one of our cluster nodes, and they’ll need to know the exact port on which <span epub:type="pagebreak" id="page_163"/>each Service is listening. That port could change if the Service is deleted and re-created. We could partially address this by telling Kubernetes which port to use for the <span class="literal">NodePort</span>, but we don’t want to do this with any arbitrary Service because multiple Services may choose the same port.</p>&#13;
<p class="indent">What we really need is a single external entry point to our cluster that keeps track of multiple services that are available and uses rules to route traffic to them. This way, we can do all of our routing configuration inside the cluster so that Services can come and go dynamically. At the same time, we can have a single well-known entry point for our cluster that all external users can use.</p>&#13;
<p class="indent">For HTTP traffic, Kubernetes provides exactly this capability, calling it an <em>Ingress</em>. To configure our cluster to route external HTTP traffic to Services, we need to define the set of Ingress resources that specify the routing and to deploy the ingress controller that receives and routes the traffic. We already installed our ingress controller when we set up our cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n ingress-nginx get deploy</span>&#13;
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE&#13;
ingress-nginx-controller   1/1     1            1           15d&#13;
root@host01:~# <span class="codestrong1">kubectl -n ingress-nginx get svc</span>&#13;
NAME                      TYPE        ... PORT(S)               ...&#13;
ingress-nginx-controller  NodePort    ... 80:80/TCP,443:443/TCP ...&#13;
...</pre>&#13;
<p class="indent">Our ingress controller includes a Deployment and a Service. As the Service is of type <span class="literal">NodePort</span>, we know that <span class="literal">kube-proxy</span> is listening to ports 80 and 443 on all of our cluster’s nodes, ready to route traffic to the associated Pod.</p>&#13;
<p class="indent">As the name implies, our ingress controller is actually an instance of an NGINX web server; however, in this case NGINX is solely acting as an HTTP reverse proxy rather than serving any web content of its own. The ingress controller listens for changes to Ingress resources in the cluster and reconfigures NGINX to connect to backend servers based on the rules that are defined. These rules use host or path information from the HTTP request to select a Service for the request.</p>&#13;
<p class="indent">Let’s create an Ingress resource to route traffic to the <span class="literal">nginx</span> Service we defined in <a href="ch09.xhtml#ch09list1">Listing 9-1</a>. Here’s the resource we’ll create:</p>&#13;
<p class="noindent6"><em>nginx-ingress.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: networking.k8s.io/v1&#13;
kind: Ingress&#13;
metadata:&#13;
  name: web01&#13;
spec:&#13;
  rules:&#13;
    - host: web01&#13;
      http:&#13;
        paths:&#13;
          - path: /&#13;
<span epub:type="pagebreak" id="page_164"/>            pathType: Prefix&#13;
            backend:&#13;
              service:&#13;
                name: nginx&#13;
                port:&#13;
                  number: 80</pre>&#13;
<p class="indent">This resource instructs the ingress controller to look at the HTTP <span class="literal">Host</span> header. If it sees <span class="literal">web01</span> as the <span class="literal">Host</span> header, it then tries to match against a path in the <span class="literal">paths</span> we specified. In this case, all paths will match the path <span class="literal">/</span> prefix, so all traffic will be routed to the <span class="literal">nginx</span> Service.</p>&#13;
<p class="indent">Before we apply this to the cluster, let’s confirm what happens if we try to use a hostname that the ingress controller doesn’t recognize. We’ll use the high-availability IP address that’s associated with our cluster, as the cluster’s load balancer will forward that to one of the instances:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">curl -vH "Host:web01" http://192.168.61.10</span>&#13;
...&#13;
&gt; Host:web01&#13;
...&#13;
&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;&#13;
...</pre>&#13;
<p class="indent">The <span class="literal">-H "Host:web01"</span> flag in the <span class="literal">curl</span> command tells <span class="literal">curl</span> to use the value <span class="literal">host01</span> as the <span class="literal">Host</span> header in the HTTP request. This is necessary given that we don’t have a DNS server in our example cluster that can turn <span class="literal">web01</span> into our cluster’s IP address.</p>&#13;
<p class="indent">As we can see, the NGINX server that’s acting as the ingress controller is configured to reply with a <span class="literal">404 Not Found</span> error message whenever it gets a request that doesn’t match any configured Ingress resource. In this case, because we haven’t created any Ingress resources yet, any request will get this response.</p>&#13;
<p class="indent">Let’s apply the <span class="literal">web01</span> Ingress resource to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/nginx-ingress.yaml</span> &#13;
ingress.networking.k8s.io/web01 created</pre>&#13;
<p class="indent">Now that the Ingress resource exists, as <a href="ch09.xhtml#ch09list3">Listing 9-3</a> illustrates, HTTP port 80 requests on both the cluster high-availability IP and individual hosts are routed to the <span class="literal">nginx</span> Service:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">curl -vH "Host:web01" http://host01</span>&#13;
...&#13;
&gt; Host:web01&#13;
...&#13;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;&#13;
...&#13;
root@host01:~# <span class="codestrong1">curl -vH "Host:web01" http://192.168.61.10</span>&#13;
...&#13;
<span epub:type="pagebreak" id="page_165"/>&gt; Host:web01&#13;
...&#13;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;&#13;
...</pre>&#13;
<p class="caption" id="ch09list3"><em>Listing 9-3: NGINX via Ingress</em></p>&#13;
<p class="indent">The output in both cases is the same, showing that traffic is being routed to the <span class="literal">nginx</span> Service.</p>&#13;
<p class="indent">In the <span class="literal">web01-ingress</span> resource, we were able to use the bare name of the <span class="literal">nginx</span> Service. The Service name lookup is based on where the Ingress resource is located. Because we created the Ingress resource in the default Namespace, that is where it looks first for Services.</p>&#13;
<p class="indent">Putting this all together, we now have a high-availability solution to route traffic from external users to HTTP servers in our cluster. This combines our cluster’s high-availability IP address <span class="literal">192.168.61.10</span> with an ingress controller exposed as a <span class="literal">NodePort</span> Service on port 80 of all our cluster’s nodes. The ingress controller can be dynamically configured to expose additional Services by creating new Ingress resources.</p>&#13;
<h4 class="h4" id="ch00lev2sec68">Ingress in Production</h4>&#13;
<p class="noindent">The <span class="literal">curl</span> command in <a href="ch09.xhtml#ch09list3">Listing 9-3</a> still looks a little strange, as we’re required to override the HTTP <span class="literal">Host</span> header manually. We need to perform a few additional steps to use Ingress resources to expose services in a production cluster.</p>&#13;
<p class="indent">First, we need our cluster to have an externally routable IP address together with a well-known name that is registered in DNS. The best way to do that is with a wildcard DNS scheme so that all hosts in a given domain are all routed to the cluster’s external IP. For example, if we own the domain <span class="literal">cluster.example.com</span>, we could create a DNS entry so that <span class="literal">*.cluster.example.com</span> routes to the cluster’s external IP address.</p>&#13;
<p class="indent">This approach still works with larger clusters that span multiple networks. We just need to have multiple IP addresses associated with the DNS entry, possibly using location-aware DNS servers that route clients to the closest service.</p>&#13;
<p class="indent">Next, we need to create an SSL certificate for our ingress controller that includes our wildcard DNS as a Subject Alternative Name (SAN). This will allow our ingress controller to provide a secure HTTP connection for external users no matter what specific service hostname they are using.</p>&#13;
<p class="indent">Finally, when we define our Services, we need to specify the fully qualified domain name for the <span class="literal">host</span> field. For the preceding example, we would specify <span class="literal">web01.cluster.example.com</span> rather than just <span class="literal">web01</span>.</p>&#13;
<p class="indent">After we’ve performed these additional steps, any external user would be able to connect via HTTPS to the fully qualified hostname of our Service, such as <span class="literal">https://web01.cluster.example.com</span>. This hostname would resolve to our cluster’s external IP address, and the load balancer would route it to one of the cluster’s nodes. At that point, our ingress controller, listening on the standard port of 443, would offer its wildcard certificate, which would match <span epub:type="pagebreak" id="page_166"/>what the client expects. As soon as the secure connection is established, the ingress controller would inspect the HTTP <span class="literal">Host</span> header and proxy a connection to the correct Service, sending back the HTTP response to the client.</p>&#13;
<p class="indent">The advantage of this approach is that after we have it set up, we can deploy a new Ingress resource at any time to expose a Service externally, and as long as we choose a unique hostname, it won’t collide with any other exposed Service. After the initial setup, all of the configuration is maintained within the cluster itself, and we still have a highly available configuration for all of our Services.</p>&#13;
<h3 class="h3" id="ch00lev1sec42">Final Thoughts</h3>&#13;
<p class="noindent">Routing network traffic in a Kubernetes cluster might involve a great deal of complexity, but the end result is straightforward: we can deploy our application components to a cluster, with automatic scaling and failover, and external users can access our application using a well-known name without having to know how the application is deployed or how many container instances we’re using to meet demand. If we build our application to be resilient, our application containers can upgrade to new versions or restart in response to failure without users even being aware of the change.</p>&#13;
<p class="indent">Of course, if we’re going to build application components that are resilient, it’s important to know what can go wrong in deploying containers. In the next chapter, we’ll look at some common issues with deploying containers to a Kubernetes cluster and how to debug them.</p>&#13;
</body></html>