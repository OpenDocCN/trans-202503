- en: '**INTRODUCTION**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thanks to rapid advancements in deep learning, we have seen a significant expansion
    of machine learning and AI in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: This progress is exciting if we expect these advancements to create new industries,
    transform existing ones, and improve the quality of life for people around the
    world. On the other hand, the constant emergence of new techniques can make it
    challenging and time-consuming to keep abreast of the latest developments. Nonetheless,
    staying current is essential for professionals and organizations that use these
    technologies.
  prefs: []
  type: TYPE_NORMAL
- en: I wrote this book as a resource for readers and machine learning practitioners
    who want to advance their expertise in the field and learn about techniques that
    I consider useful and significant but that are often overlooked in traditional
    and introductory textbooks and classes. I hope you’ll find this book a valuable
    resource for obtaining new insights and discovering new techniques you can implement
    in your work.
  prefs: []
  type: TYPE_NORMAL
- en: '**Who Is This Book For?**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Navigating the world of AI and machine learning literature can often feel like
    walking a tightrope, with most books positioned at either end: broad beginner’s
    introductions or deeply mathematical treatises. This book illustrates and discusses
    important developments in these fields while staying approachable and not requiring
    an advanced math or coding background.'
  prefs: []
  type: TYPE_NORMAL
- en: This book is for people with some experience with machine learning who want
    to learn new concepts and techniques. It’s ideal for those who have taken a beginner
    course in machine learning or deep learning or have read an equivalent introductory
    book on the topic. (Throughout this book, I will use *machine learning* as an
    umbrella term for machine learning, deep learning, and AI.)
  prefs: []
  type: TYPE_NORMAL
- en: '**What Will You Get Out of This Book?**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This book adopts a unique Q&A style, where each brief chapter is structured
    around a central question related to fundamental concepts in machine learning,
    deep learning, and AI. Every question is followed by an explanation, with several
    illustrations and figures, as well as exercises to test your understanding. Many
    chapters also include references for further reading. These bite-sized nuggets
    of information provide an enjoyable jumping-off point on your journey from machine
    learning beginner to expert.
  prefs: []
  type: TYPE_NORMAL
- en: The book covers a wide range of topics. It includes new insights about established
    architectures, such as convolutional networks, that allow you to utilize these
    technologies more effectively. It also discusses more advanced techniques, such
    as the inner workings of large language models (LLMs) and vision transformers.
    Even experienced machine learning researchers and practitioners will encounter
    something new to add to their arsenal of techniques.
  prefs: []
  type: TYPE_NORMAL
- en: While this book will expose you to new concepts and ideas, it’s not a math or
    coding book. You won’t need to solve any proofs or run any code while reading.
    In other words, this book is a perfect travel companion or something you can read
    on your favorite reading chair with your morning coffee or tea.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to Read This Book**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each chapter of this book is designed to be self-contained, offering you the
    freedom to jump between topics as you wish. When a concept from one chapter is
    explained in more detail in another, I’ve included chapter references you can
    follow to fill in gaps in your understanding.
  prefs: []
  type: TYPE_NORMAL
- en: However, there’s a strategic sequence to the chapters. For example, the early
    chapter on embeddings sets the stage for later discussions on self-supervised
    learning and few-shot learning. For the easiest reading experience and the most
    comprehensive grasp of the content, my recommendation is to approach the book
    from start to finish.
  prefs: []
  type: TYPE_NORMAL
- en: Each chapter is accompanied by optional exercises for readers who want to test
    their understanding, with an answer key located at the end of the book. In addition,
    for any papers referenced in a chapter or further reading on that chapter’s topic,
    you can find the complete citation information in that chapter’s “References”
    section.
  prefs: []
  type: TYPE_NORMAL
- en: The book is structured into five main parts centered on the most important topics
    in machine learning and AI today.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Part I: Neural Networks and Deep Learning](part01.xhtml)** covers questions
    about deep neural networks and deep learning that are not specific to a particular
    subdomain. For example, we discuss alternatives to supervised learning and techniques
    for reducing overfitting, which is a common problem when using machine learning
    models for real-world problems where data is limited.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 1: Embeddings, Latent Space, and Representations](ch01.xhtml)**   Delves
    into the distinctions and similarities between embedding vectors, latent vectors,
    and representations. Elucidates how these concepts help encode information in
    the context of machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 2: Self-Supervised Learning](ch02.xhtml)**    Focuses on self-supervised
    learning, a method that allows neural networks to utilize large, unlabeled datasets
    in a supervised manner.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 3: Few-Shot Learning](ch03.xhtml)**    Introduces few-shot learning,
    a specialized supervised learning technique tailored for small training datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 4: The Lottery Ticket Hypothesis](ch04.xhtml)**    Explores the
    idea that randomly initialized neural networks contain smaller, efficient subnetworks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 5: Reducing Overfitting with Data](ch05.xhtml)**    Addresses the
    challenge of overfitting in machine learning, discussing strategies centered on
    data augmentation and the use of unlabeled data to reduce overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 6: Reducing Overfitting with Model Modifications](ch06.xhtml)**    Extends
    the conversation on overfitting, focusing on model-related solutions like regularization,
    opting for simpler models, and ensemble techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 7: Multi-GPU Training Paradigms](ch07.xhtml)**    Explains various
    training paradigms for multi-GPU setups to accelerate model training, including
    data and model parallelism.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 8: The Success of Transformers](ch08.xhtml)**    Explores the popular
    transformer architecture, highlighting features like attention mechanisms, parallelization
    ease, and high parameter counts.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 9: Generative AI Models](ch09.xhtml)**    Provides a comprehensive
    overview of deep generative models, which are used to produce various media forms,
    including images, text, and audio. Discusses the strengths and weaknesses of each
    model type.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 10: Sources of Randomness](ch10.xhtml)**    Addresses the various
    sources of randomness in the training of deep neural networks that may lead to
    inconsistent and non-reproducible results during both training and inference.
    While randomness can be accidental, it can also be intentionally introduced by
    design.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Part II: Computer Vision](part02.xhtml)** focuses on topics mainly related
    to deep learning but specific to computer vision, many of which cover convolutional
    neural networks and vision transformers.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 11: Calculating the Number of Parameters](ch11.xhtml)**    Explains
    the procedure for determining the parameters in a convolutional neural network,
    which is useful for gauging a model’s storage and memory requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 12: Fully Connected and Convolutional Layers](ch12.xhtml)**    Illustrates
    the circumstances in which convolutional layers can seamlessly replace fully connected
    layers, which can be useful for hardware optimization or simplifying implementations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 13: Large Training Sets for Vision Transformers](ch13.xhtml)**    Probes
    the rationale behind vision transformers requiring more extensive training sets
    compared to conventional convolutional neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Part III: Natural Language Processing](part03.xhtml)** covers topics around
    working with text, many of which are related to transformer architectures and
    self-attention.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 14: The Distributional Hypothesis](ch14.xhtml)**    Delves into
    the distributional hypothesis, a linguistic theory suggesting that words appearing
    in the same contexts tend to possess similar meanings, which has useful implications
    for training machine learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 15: Data Augmentation for Text](ch15.xhtml)**    Highlights the
    significance of data augmentation for text, a technique used to artificially increase
    dataset sizes, which can help with improving model performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 16: Self-Attention](ch16.xhtml)**    Introduces self-attention,
    a mechanism allowing each segment of a neural network’s input to refer to other
    parts. Self-attention is a key mechanism in modern large language models.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 17: Encoder- and Decoder-Style Transformers](ch17.xhtml)**    Describes
    the nuances of encoder and decoder transformer architectures and explains which
    type of architecture is most useful for each language processing task.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 18: Using and Fine-Tuning Pretrained Transformers](ch18.xhtml)**    Explains
    different methods for fine-tuning pretrained large language models and discusses
    their strengths and weaknesses.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 19: Evaluating Generative Large Language Models](ch19.xhtml)**    Lists
    prominent evaluation metrics for language models like Perplexity, BLEU, ROUGE,
    and BERTScore.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Part IV: Production and Deployment](part04.xhtml)** covers questions pertaining
    to practical scenarios, such as increasing inference speeds and various types
    of distribution shifts.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 20: Stateless and Stateful Training](ch20.xhtml)**    Distinguishes
    between stateless and stateful training methodologies used in deploying models.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 21: Data-Centric AI](ch21.xhtml)**    Explores data-centric AI,
    which priori-tizes refining datasets to enhance model performance. This approach
    contrasts with the conventional model-centric approach, which emphasizes improving
    model architectures or methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 22: Speeding Up Inference](ch22.xhtml)**    Introduces techniques
    to enhance the speed of model inference without tweaking the model’s architecture
    or compromising accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 23: Data Distribution Shifts](ch23.xhtml)**    Post-deployment,
    AI models may face discrepancies between training data and real-world data distributions,
    known as data distribution shifts. These shifts can deteriorate model performance.
    This chapter categorizes and elaborates on common shifts like covariate shift,
    concept drift, label shift, and domain shift.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Part V: Predictive Performance and Model Evaluation](part05.xhtml)** dives
    deeper into various aspects of squeezing out predictive performance, such as changing
    the loss function, setting up *k*-fold cross-validation, and dealing with limited
    labeled data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 24: Poisson and Ordinal Regression](ch24.xhtml)**    Highlights
    the differences between Poisson and ordinal regression. Poisson regression is
    suitable for count data that follows a Poisson distribution, like the number of
    colds contracted on an airplane. In contrast, ordinal regression caters to ordered
    categorical data without assuming equidistant categories, such as disease severity.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 25: Confidence Intervals](ch25.xhtml)**    Delves into methods for
    constructing confidence intervals for machine learning classifiers. Reviews the
    purpose of confidence intervals, discusses how they estimate unknown population
    parameters, and introduces techniques such as normal approximation intervals,
    bootstrapping, and retraining with various random seeds.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 26: Confidence Intervals vs. Conformal Predictions](ch26.xhtml)**    Discusses
    the distinction between confidence intervals and conformal predictions and describes
    the latter as a tool for creating prediction intervals that cover actual outcomes
    with specific probability.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 27: Proper Metrics](ch27.xhtml)**    Focuses on the essential properties
    of a proper metric in mathematics and computer science. Examines whether commonly
    used loss functions in machine learning, such as mean squared error and cross-entropy
    loss, satisfy these properties.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 28: The *k* in *k*-Fold Cross-Validation](ch28.xhtml)**    Explores
    the role of the *k* in *k*-fold cross-validation and provides insight into the
    advantages and disadvantages of selecting a large *k*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 29: Training and Test Set Discordance](ch29.xhtml)**    Addresses
    the scenario where a model performs better on a test dataset than the training
    dataset. Offers strategies to discover and address discrepancies between training
    and test datasets, introducing the concept of adversarial validation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 30: Limited Labeled Data](ch30.xhtml)**    Introduces various techniques
    to enhance model performance in situations where data is limited. Covers data
    labeling, bootstrapping, and paradigms such as transfer learning, active learning,
    and multimodal learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Online Resources**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I’ve provided optional supplementary materials on GitHub with code examples
    for certain chapters to enhance your learning experience (see *[https://github.com/rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)*).
    These materials are designed as practical extensions and deep dives into topics
    covered in the book. You can use them alongside each chapter or explore them after
    reading to solidify and expand your knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Without further ado, let’s dive in.
  prefs: []
  type: TYPE_NORMAL
