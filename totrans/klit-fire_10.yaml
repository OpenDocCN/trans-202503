- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Future-Proofing
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 未来可持续性
- en: The best way to complete a modernization project is by ensuring that you won’t
    have to go through the whole process again in a few years. Future-proofing isn’t
    about preventing mistakes; it’s about knowing how to maintain and evolve technology
    gradually.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 完成现代化项目的最佳方式是确保在几年内你不会再次经历整个过程。未来可持续性不是关于避免错误；而是知道如何逐步维护和发展技术。
- en: 'Two types of problems will cause us to rethink a working system as it ages.
    The first are usage changes. The second are deteriorations. *Scaling challenges*
    are the change in usage type: we have more traffic or a different type of traffic
    from what we had before. Maybe more people are using the system than were before,
    or we’ve added a bunch of features that over time have changed the purpose for
    which people are using the technology.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 两种类型的问题会促使我们重新思考一个正在老化的工作系统。第一种是使用变化。第二种是衰退。*扩展挑战*是使用类型的变化：我们有了更多的流量或不同类型的流量，跟以前不一样了。也许使用系统的人比之前多了，或者我们添加了一些新功能，随着时间推移，人们使用该技术的目的发生了变化。
- en: '*Usage changes* do not have a constant pace and are, therefore, hard to predict.
    A system might never have scaling challenges. A system can reach a certain level
    of usage and never go any further. Or it can double or triple in size in a brief
    period. Or it can slowly increase in scale for years. What scaling challenges
    will look like if they do happen will depend on a number of factors. Because changes
    to the system’s usage are hard to anticipate, they are hard to normalize. This
    is an advantage. When we normalize something, we stop thinking about it, stop
    factoring it into our decisions, and sometimes even forget it exists.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*使用变化*的速度并不恒定，因此很难预测。一个系统可能永远不会面临扩展挑战。一个系统可能达到某个使用水平后再也不会增长。或者它可以在短短一段时间内扩展一倍或三倍。或者它可以在多年中缓慢扩展。若真的发生扩展挑战，它们会是什么样子，将取决于多种因素。由于系统使用的变化难以预料，它们也很难被标准化。这是一种优势。当我们标准化某些东西时，我们就不再思考它，不再把它考虑进我们的决策中，有时甚至会忘记它的存在。'
- en: '*Deteriorations,* on the other hand, are inevitable. They represent a natural
    linear progression toward an unavoidable end state. Other factors may speed them
    up or slow them down, but eventually, we know what the final outcome will be.
    For example, no changes in usage were going to eliminate the 9th of September
    from the calendar year of 1999\. It was going to happen at some point, regardless
    of the system behavior of the machines that were programmed to use 9/9/99 as a
    null value assigned to columns when the date was missing.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*衰退*，另一方面，是不可避免的。它们代表了向不可避免的最终状态自然线性发展的过程。其他因素可能会加速或减缓它们的进程，但最终我们知道最终的结果会是什么。例如，1999年9月9日的日期不会因为使用变化而从1999年的日历中消失。不管那些被编程为在日期缺失时将9/9/99作为空值赋给列的机器系统行为如何，9月9日的事件终究会发生。'
- en: Memory leaks are another good example of this kind of change. System usage might
    influence exactly when the leak creates a major problem, but low system usage
    will not change the fact that a memory leak exists that will eventually be a problem.
    The only way to escape the problem is to fix it.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 内存泄漏是这种变化的另一个很好的例子。系统使用可能会影响泄漏何时成为一个重大问题，但低系统使用量并不会改变内存泄漏的存在，内存泄漏最终会成为问题。解决问题的唯一方法是修复它。
- en: Hardware lifecycles are another example. Eventually chips, disks, and circuit
    boards all fail and have to be replaced.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件生命周期是另一个例子。最终，芯片、硬盘和电路板都会出现故障，并需要更换。
- en: These kinds of deteriorations are dangerous because people forget about them.
    For a long time, their effects go unnoticed, until one day they finally and completely
    break. If the organization is particularly unlucky, the problem is deeply embedded
    in the system, and it’s not immediately clear what has even broken in the first
    place.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这些衰退类型是危险的，因为人们会忘记它们。它们的影响会长时间不被察觉，直到某一天它们最终完全崩溃。如果组织特别不幸，问题已经深深嵌入系统中，并且一开始并不清楚究竟是什么出故障了。
- en: Consider, for example, Y2K. An alarming number of computer programs were designed
    with a two-digit year, which became a problem in the year 2000 when the missing
    first two digits were different from what the program assumed they were. Most
    technical people know the Y2K story, but did you know that Y2K wasn’t the first
    short-sighted programming mistake of this nature? Nor will it be the last.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，考虑一下Y2K问题。一个令人担忧的情况是，很多计算机程序设计时采用了两位数的年份，这在2000年时成了问题，因为缺失的前两位数字与程序假设的数字不同。大多数技术人员都知道Y2K故事，但你知道吗，Y2K并不是第一次出现这种目光短浅的编程错误？也不会是最后一次。
- en: Time
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间
- en: It’s unbelievable how often software engineers have screwed up time in programs.
    In the 1960s, some programs had only one-digit years. The TOPS-10 operating system
    had only enough bits to represent dates between January 1, 1964, and January 4,
    1975\. Engineers patched this problem, adding three more bits so that TOPS-10
    could represent dates up to February 1, 2052, but they took those bits from existing
    data structures, thinking they were unused. It turns out that some programs on
    TOPS-10 had already repurposed those areas of storage, which led to some wonky
    bugs.^([1](#c10-footnote-1))
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 软件工程师在程序中常常把时间搞砸，这实在是令人难以置信。在1960年代，一些程序只有一位数的年份。TOPS-10 操作系统只能表示1964年1月1日到1975年1月4日之间的日期。工程师通过修补这个问题，添加了三个位，使TOPS-10能够表示直到2052年2月1日的日期，但他们从现有的数据结构中取走了这些位，以为它们没有被使用。结果发现，TOPS-10上的一些程序已经重新利用了这些存储区域，这导致了一些奇怪的错误。^([1](#c10-footnote-1))
- en: How much storage should be dedicated to dates is a constant problem. It would
    be unwise and impractical to allocate unlimited storage for time, and yet any
    amount of storage eventually will run out. Programmers must decide how many years
    will pass before the idea that their program will still be functioning seems unlikely.
    At least in the early days of computers, the tendency was to underestimate the
    lifecycle of software. It’s easy for a functioning piece of software to remain
    in place for 10, 20, 30 years, or more. But in the early days of computing, two
    or three decades seemed like a long time. If time was given only enough space
    to reach 1975, the fix might carry it over to 1986\. Certain operating systems
    in 1989 programmed limits to reach maturity in 1997—and so on, and on, and on.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 应该为日期分配多少存储空间是一个永恒的问题。为时间分配无限存储空间是不明智且不切实际的，然而任何数量的存储最终都会用完。程序员必须决定，在多长时间后，程序仍在运行的可能性看起来不大。至少在计算机的早期，倾向于低估软件生命周期。一个正常运行的软件很容易保持运作10年、20年、30年，甚至更长时间。但在计算机发展的早期，两三十年看起来是很长的时间。如果时间只给出足够的空间直到1975年，那么修复可能会延续到1986年。1989年，某些操作系统编程限制了系统的成熟时间为1997年——如此这般，循环往复。
- en: These programs are still with us, and we haven’t reached all of their maturity
    dates just yet. In , a date format created by the World Computer Corporation will
    reach its storage limit, and we have no idea whether any existing systems use
    it. Of greater concern is the year 2038 when Unix’s 32-bit dates reach their limit.
    While most modern Unix implementations have switched to 64-bit dates instead,
    the Network Time Protocol’s (NTP) 32-bit date components will overflow on February
    7, 2036, giving us a potential preview. NTP handles syncing the clocks of computers
    that talk with each other over the internet. Computer clocks that are too badly
    out of sync—typically five minutes or more—have trouble creating secure connections.
    This requirement goes back to MIT’s Kerberos version 5 spec in 2005, which used
    time to keep attackers from resetting their clocks to continue using expired tickets.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些程序仍然在我们身边，我们还没有到达它们的所有“成熟”日期。某个由世界计算机公司（World Computer Corporation）创建的日期格式将在某一年达到存储限制，我们不知道是否有现有系统在使用它。更值得关注的是2038年，届时Unix的32位日期将达到极限。虽然大多数现代Unix实现已经转向64位日期，但网络时间协议（NTP）的32位日期组件将在2036年2月7日发生溢出，这为我们提供了一个潜在的预览。NTP负责同步互联网上互相通信的计算机时钟。计算机时钟若严重不同步——通常是五分钟或更长时间——会导致无法建立安全连接。这个要求源自MIT在2005年发布的Kerberos版本5规范，它利用时间来防止攻击者重置时钟，继续使用过期的票证。
- en: We don’t know what kinds of problems NTP and Unix rollovers will cause. Most
    computers are probably long upgraded and will be unaffected. With any luck, the
    2038 milestone will pass us by with little fanfare, just as Y2K did before it.
    But time bugs don’t need to trigger global meltdowns to have dramatic and expensive
    impacts. Past time bugs have temporarily cleared pension funds, messed with text
    messages, crashed video games, and disabled parking meters. In 2010, 20 million
    chip and PIN bank cards became unusable in Germany thanks to a time bug.^([2](#c10-footnote-2))
    In 2013, NASA lost control of the $330 million Deep Impact probe thanks to a time
    bug similar to the 2038 issue.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不知道NTP和Unix溢出将引发哪些问题。大多数计算机可能已经升级，因此不会受到影响。如果运气好，2038年的这一时刻可能会悄然过去，就像Y2K问题一样。但是，时间漏洞并不需要触发全球性的灾难来产生戏剧性的和昂贵的影响。过去的时间漏洞曾一度清空养老金基金，干扰短信发送，崩溃视频游戏，甚至让停车表无法正常工作。2010年，由于时间漏洞，德国有2000万张芯片和密码银行卡无法使用^([2](#c10-footnote-2))。2013年，NASA因为一个类似2038问题的时间漏洞失去了对3.3亿美元的“深空撞击”（Deep
    Impact）探测器的控制。
- en: Time bugs are tricky because they detonate decades, or sometimes centuries,
    after they were introduced. IBM mainframes built in the 1970s reach a rollover
    point on September 18, 2042\. Some Texas Instruments calculators do not accept
    dates beyond December 31, 2049\. Some Nokia phones accept dates only up to December
    31, 2079\. Several programming languages and frameworks use timestamp objects
    that overflow on April 11, 2262.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 时间漏洞很棘手，因为它们往往在引入后几十年，甚至有时是几个世纪才会引发问题。IBM在1970年代制造的主机将在2042年9月18日达到溢出点。某些得克萨斯仪器（Texas
    Instruments）计算器无法接受2049年12月31日之后的日期。某些诺基亚手机只接受到2079年12月31日的日期。多个编程语言和框架使用的时间戳对象将在2262年4月11日发生溢出。
- en: It’s not that programmers don’t know these bugs exist. It’s just hard to imagine
    the technology of today sticking around until 2262\. At the same time, people
    who were programming room-sized mainframes in the 1960s never thought their code
    would last for decades, but we now know programs this old are still in production.
    By the time the year 2000 came around, that old software (and sometimes the machines
    that came with it) had not only not been retired but was also being maintained
    by technologists two or three generations divorced from its creation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 并不是程序员不知道这些漏洞的存在，只是很难想象今天的技术会持续到2262年。同时，20世纪60年代编程房间大小的主机的工程师们也从未想过他们的代码会延续几十年，但现在我们知道，这些老旧的程序依然在生产环境中运行。到了2000年，旧软件（有时还有与之配套的机器）不仅没有被淘汰，反而由与其创作有两三代代际差距的技术人员在维护。
- en: Resolving time bugs is usually fairly straightforward—when we know about them.
    The problem is we tend to forget that they’re approaching. We have already seen
    systems fail thanks to the 2038 bug. Programs in financial institutions that must
    calculate out interest payments 20 or 30 years into the future act like early
    warning detection systems for these types of errors. Still, organizations must
    know the state of their legacy systems (in other words, whether they’ve been patched)
    and be aware that these incidents are happening.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 解决时间漏洞通常相对直接——当我们知道它们的存在时。问题在于，我们往往忘记它们即将到来。我们已经看到由于2038漏洞而导致的系统故障。金融机构中必须计算20到30年后利息支付的程序，实际上是这些类型错误的早期预警检测系统。然而，组织必须了解他们的遗留系统状态（换句话说，是否已经打上补丁），并意识到这些事件正在发生。
- en: Unescapable Migrations
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无法逃避的迁移
- en: Future-proofing systems does not mean building them so that you never have to
    redesign them or migrate them. That is impossible. It means building and, more
    important, *maintaining* to avoid a lengthy modernization project where normal
    operations have to be reorganized to make progress. The secret to future-proofing
    is making migrations and redesigns normal routines that don’t require heavy lifting.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 面向未来的系统建设并不意味着构建一个永远不需要重新设计或迁移的系统。那是不可能的。它意味着构建并且，更重要的是，*保持*系统更新，以避免一个需要重组正常运作的长期现代化项目。面向未来的秘诀在于将迁移和重新设计变成不需要大力操作的日常例行事务。
- en: Most modern engineering organizations already know how to do this with usage
    changes—they monitor for increased activity and scale infrastructure up or down
    as needed. If given proper time and prioritization, they will refactor and redesign
    components of the system to better reflect the most likely long-term usage patterns.
    Making updates to the system early and often is just a matter of discipline. Those
    that neglect to devote a little bit of time to cleaning their technical debt will
    be forced into cumbersome and risky legacy modernization efforts instead.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代工程组织已经知道如何处理使用变更——他们会监控活动的增加，并根据需要扩展或缩减基础设施。如果给予足够的时间和优先级，他们会重构和重新设计系统组件，以更好地反映最可能的长期使用模式。早期并频繁地对系统进行更新，只是一个纪律性的问题。那些忽视清理技术负债的组织，最终将不得不进行繁琐且风险较大的遗留系统现代化工作。
- en: One of my favorite metaphors for setting a cadence for early and often updates
    comes from the podcast *Legacy Code Rocks* ([https://www.legacycode.rocks/](https://www.legacycode.rocks/)).
    Launching a new feature is like having a house party. The more house parties you
    have in your house before you clean things up, the worse condition your house
    will be in. Although there isn’t a hard-and-fast rule here that will work for
    everyone, automatically scheduling some time to reevaluate usage changes and technical
    debt after every *n* feature launches will normalize the process of updating the
    system in ways that will ensure its long-term health. When people associate refactoring
    and necessary migrations with a system somehow being built wrong or breaking,
    they will put off doing them until things are falling apart. When the process
    of managing these changes is part of the routine, like getting a haircut or changing
    the oil in your car, the system can be future-proofed by gradually modernizing
    it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我最喜欢的关于为频繁更新设定节奏的比喻来自播客*Legacy Code Rocks*（[https://www.legacycode.rocks/](https://www.legacycode.rocks/)）。推出新功能就像举办一个家庭聚会。在你清理房子之前，举办的家庭聚会越多，房子的状况就越差。虽然这里没有一条适用于所有人的硬性规则，但在每次推出*n*个新功能后，自动安排时间重新评估使用变更和技术负债，将使得更新系统的过程规范化，从而确保其长期健康。当人们把重构和必要的迁移与系统构建错误或故障挂钩时，他们会推迟执行这些操作，直到事情开始崩溃。而当这些变更管理过程成为日常工作的一部分时，就像剪头发或换车油一样，系统可以通过逐步现代化来确保未来的可持续性。
- en: Deteriorations require a different tact. Sometimes they can be monitored. As
    batteries age, for example, their performance slides in a way that can be captured
    and tracked. Some deteriorations are more sudden. Time bugs don’t give any warning
    before they explode. If the organization has forgotten about it, there’s nothing
    to monitor.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于恶化问题，需要采取不同的策略。有时它们可以被监控。例如，电池随着时间的推移会老化，它们的性能下降是可以被捕捉和追踪的。某些恶化问题则更加突然。时间错误在爆发之前不会发出任何警告。如果组织已经忘记了它，就没有什么可以监控的了。
- en: It would be naive to say that you should never build a deteriorating change
    into your system; those issues are often unavoidable. The mistake is assuming
    it is not possible that the system will still be operational when the issue matures.
    Technology has a way of extending its life for much longer than people realize.
    Some of the control panels for switches on the New York City subway date back
    to the 1930s. The Salisbury cathedral clock started running in 1368\. There’s
    a lightbulb over Livermore California’s Fire Station 6 that has been on since
    1901\. All around the world, our day-to-day lives are governed by machines long
    past their assumed expiration dates.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 说你永远不应该在系统中引入逐渐恶化的变化是天真的；这些问题往往是不可避免的。错误在于假设当问题成熟时，系统仍然无法运行。技术有一种方式可以将其寿命延长，远远超过人们的预期。例如，纽约市地铁的部分开关控制面板可以追溯到1930年代。索尔兹伯里大教堂的钟表自1368年开始运行。在加利福尼亚州利弗莫尔市的消防站6号楼上，有一个自1901年以来一直亮着的灯泡。全世界各地，我们的日常生活都受到那些远超预期过期日期的机器的控制。
- en: 'Instead, managing deteriorations comes down to these two practices:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，管理逐渐恶化的问题归结为以下两种做法：
- en: If you’re introducing something that will deteriorate, build it to fail gracefully.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你引入了一些会逐渐恶化的东西，那就让它优雅地失败。
- en: Shorten the time between upgrades so that people have plenty of practice doing
    them.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩短升级之间的时间间隔，让人们有足够的时间来实践这些升级。
- en: Failing Gracefully
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优雅失败
- en: The reason Y2K and similar bugs do not trigger the end of human civilization
    is because they do not impact every system affected by them with uniform intensity.
    There is a lot of variation in how different machines, different programming languages,
    and different software will handle the same problem. Some systems will panic;
    some will simply move on. Whether it is better for the system to panic and crash
    or to ignore the issue and move on largely depends on whether the failure is in
    the critical path of a transaction.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Y2K 和类似的 bug 没有导致人类文明的终结，原因在于它们不会以均匀的强度影响所有受其影响的系统。不同的机器、不同的编程语言和不同的软件处理同一个问题的方式有很大的差异。有些系统会触发恐慌，有些则会继续运行。是否让系统触发恐慌并崩溃，或者忽略问题并继续执行，通常取决于失败是否发生在事务的关键路径上。
- en: Failing gracefully does not always mean the system avoids crashing. If a bug
    breaks a daily batch job calculating accrued interest on bank accounts, the system
    recovering from the error by defaulting to zero and moving on is not failing gracefully.
    That’s an outcome that if allowed to fail silently will upset a lot of people
    very quickly, whereas a panic would alert the engineering team immediately to
    the problem so it could be resolved and the batch job rerun.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 优雅地失败并不总意味着系统避免崩溃。如果一个 bug 导致银行账户的日常批处理任务计算累计利息失败，系统通过默认归零并继续执行来从错误中恢复，这并不是优雅地失败。如果允许这种错误默默地失败，会让很多人很快感到不满，而恐慌会立即提醒工程团队问题所在，以便解决并重新运行批处理任务。
- en: '**How close is the error to a user interface?** If the error is something potentially
    triggered by user input, failing gracefully means catching the error and logging
    the event but ultimately directing the user to try again with a useful message
    explaining the problem.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**错误离用户界面有多近？** 如果错误是可能由用户输入触发的，那么优雅地失败意味着捕获错误并记录事件，但最终引导用户再次尝试，并提供一个有用的消息来解释问题。'
- en: '**Will the error block other independent processes?** Why is it blocking other
    processes? Blocking implies shared resources, which would suggest that processes
    are not as independent as originally thought. For truly independent processes,
    it is probably okay to log the error but ultimately let the system move on.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**错误是否会阻塞其他独立进程？** 为什么会阻塞其他进程？阻塞意味着共享资源，这表明进程并不像最初想的那样独立。对于真正独立的进程，记录错误并最终让系统继续运行可能是可以接受的。'
- en: '**Is the error in one step of a larger algorithm?** If so, you likely have
    no choice but to trigger a panic. If you could eliminate a step in a multistep
    process and not affect the final outcome, you should probably rethink whether
    those steps are necessary.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**错误是否出现在更大算法的一个步骤中？** 如果是这样，你可能别无选择，只能触发恐慌。如果你可以在一个多步骤的过程中省略一个步骤，并且不影响最终结果，那么你应该重新考虑这些步骤是否必要。'
- en: '**Will the error corrupt data?** In most cases, bad data is worse than no data.
    If a certain error is likely to corrupt data, you must panic upon the error so
    the problem can be resolved.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**错误会破坏数据吗？** 在大多数情况下，坏数据比没有数据更糟。如果某个错误可能会破坏数据，你必须在错误发生时触发恐慌，以便解决问题。'
- en: These are good things to consider when programming in unavoidable deteriorations.
    This thought exercise is less useful when you don’t know that you have no choice
    but to program in a potential bug. You can’t know what you don’t know.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在编程时，考虑到不可避免的退化是很重要的。当你没有意识到你别无选择，只能在潜在的 bug 中编程时，这种思维练习就不太有用了。你无法知道自己不知道的事情。
- en: But, it’s worthwhile to take some time to consider how your software would handle
    issues like the date being 20 years off, time moving backward for a second, numbers
    appearing that are technically impossible (like 11:59:60 pm), or storage drives
    suddenly disappearing.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，花些时间考虑你的软件如何处理日期偏差20年、时间倒退一秒、出现技术上不可能的数字（比如11:59:60 pm）或者存储驱动器突然消失等问题，还是值得的。
- en: When in doubt, default to panicking. It’s more important that errors get noticed
    so that they can be resolved.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当有疑虑时，默认触发恐慌。更重要的是让错误被发现，以便能够解决它们。
- en: Less Time Between Upgrades, Not More
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 升级之间的时间应该更短，而不是更长
- en: A few years ago, I got one of those cheesy letter boards for my kitchen—you
    know, the ones you put inspirational messages on like “Live life in full bloom”
    or “Love makes this house a home.” Except, mine says “The truth is counterintuitive.”
    Our gut instinct with deteriorations is to push them as far back as possible if
    we cannot eliminate them altogether. Personally, I feel this is a mistake. I know
    from experience that the more often engineers have to do things, the better they
    get at doing them, and the more likely they are to remember that they need to
    be done and plan accordingly.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，我为我的厨房买了一个那种俗气的字母板——你知道的，就是那种上面写“生命如花，尽情绽放”或“爱使这个家成为家”的板子。只是，我的写着“真相是反直觉的”。我们在面对退化问题时的直觉反应是尽可能推迟它们，如果不能完全消除它们的话。就个人而言，我认为这是一种错误。我知道通过经验，工程师们做事情的频率越高，他们做得越好，越容易记得需要做的事，并做出相应的计划。
- en: For example, in 2019 there were two important time bugs. The first was a rollover
    of GPS’s epoch; the second was a leap second.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，2019年发生了两个重要的时间错误。第一个是GPS纪元的回滚；第二个是闰秒。
- en: The GPS rollover is a problem identical to the time bugs already described.
    GPS represents weeks in a storage block of 10 bytes. That means it can store up
    to 1024 values, and 1024 weeks is 19.7 years. As with Y2K, when GPS gets to week
    number 1025, it resets to zero, and the computer has no way of knowing that it
    shouldn’t backdate everything by 20 years.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: GPS回滚是一个与之前描述的时间错误完全相同的问题。GPS通过一个10字节的存储块表示周数。这意味着它可以存储最多1024个值，1024周等于19.7年。与Y2K问题类似，当GPS达到第1025周时，它会重置为零，而计算机无法识别这一点，导致它把所有数据回溯20年。
- en: This had happened only once before, in 1999\. Although commercial GPS has been
    available since the 1980s, it had not really caught on by 1999\. The chips that
    powered the receiver were too expensive, and their convenience would not be realized
    until computers became fast enough to overlay that data with calculations determining
    routes or associating physical landmarks with their coordinates. As the helpful
    bits of GPS were not yet market-ready, consumers were more sensitive to the privacy
    concerns of the technology. In 1997, employees for United Parcel Service (UPS)
    famously went on strike after UPS tried to install GPS receivers in all of their
    trucks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这一情况在1999年只发生过一次。尽管商用GPS从1980年代起就已出现，但到1999年时，它并没有真正普及。接收器所用的芯片价格过高，直到计算机足够快，能够将这些数据与路线计算或将物理地标与其坐标关联时，GPS的便利性才得以体现。由于GPS的有用功能尚未准备好进入市场，消费者对该技术的隐私问题更为敏感。1997年，美国联合包裹公司（UPS）因试图在所有卡车上安装GPS接收器而爆发了著名的罢工。
- en: So, the impacts of the first GPS rollover were minor, because GPS was not popular.
    By 2019, however, the world was a completely different place. Twenty years is
    a long time in technology. Not only were virtually all cellphones equipped with
    GPS chips, but any number of applications had been built on top of GPS.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，第一次GPS回滚的影响是微乎其微的，因为GPS当时并不普及。然而，到2019年，世界已经发生了翻天覆地的变化。二十年在技术领域是很长的时间。几乎所有手机都配备了GPS芯片，且各种应用程序都已在GPS的基础上开发出来。
- en: As it turns out, people replace GPS-enabled devices a lot. Mobile app updates
    for many users are seamless and automatic. We are so used to getting new phones
    every two or three years that the rollover of 2019 was mainly uneventful. Users
    with older-model mobile phones experienced some problems but were encouraged to
    buy new phones from their vendors instead.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 结果证明，人们更换配备GPS的设备非常频繁。对于许多用户来说，移动应用更新是无缝且自动的。我们已经习惯了每两到三年就换一部新手机，因此2019年的回滚事件大部分并没有引起大规模的混乱。使用旧款手机的用户遇到了一些问题，但他们被鼓励从运营商处购买新手机。
- en: 'The second time bug of 2019, a leap second, went a slightly different way.
    A leap second is exactly what it sounds like: an extra second tacked on to the
    year to keep computer clocks in sync with the solar cycle. Unlike a leap year,
    leap seconds are not predictable. How many seconds between sun up and sun down
    depends on the earth’s rotational speed, which is changing. Different forces push
    the earth to speed up, and others push the earth to slow down.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年的第二个时间错误，闰秒，情况略有不同。闰秒字面意思就是：在一年中增加一个额外的秒数，以便让计算机时钟与太阳周期保持同步。与闰年不同，闰秒无法预测。日出到日落之间的秒数取决于地球的自转速度，而这个速度是不断变化的。不同的力量促使地球加速自转，另一些力量则让地球减速。
- en: 'Here’s a fun fact: one of the many forces changing the speed of the earth’s
    rotation is climate change. Ice weighs down the land masses on Earth, and when
    it melts, those land masses start to drift up toward the poles. This makes the
    earth spin faster and days fractions of a second shorter.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的事实：改变地球自转速度的众多因素之一就是气候变化。冰层压迫着地球的陆地块，当冰层融化时，这些陆地块开始向极地漂移。这使得地球自转加快，白天的时间也变得更短。
- en: There have been 28 leap seconds between 1972 and 2020, but as some forces slow
    the earth down and some forces speed it up, there can be significant gaps between
    years with leap seconds. After the leap second in 1999, it was six years before
    another was needed. There were no leap seconds between 2009 and 2012\. There was
    a leap second in both 2015 and 2016, but nothing in the next three years.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从1972年到2020年，共发生了28次闰秒，但由于一些力量使地球减速，而另一些力量使其加速，因此每年是否有闰秒之间可能会有显著的间隔。在1999年的闰秒之后，直到六年后才需要下一次闰秒。2009到2012年之间没有闰秒。2015年和2016年都发生了闰秒，但接下来的三年里没有发生任何闰秒。
- en: Leap seconds are never fun, but if the reports of problems experienced during
    each recent leap second can be considered comprehensive, they are worse after
    a long gap than they would be otherwise. Even gaps as short as three years are
    long enough for new technologies either to be developed or to get much more traction
    than they had before. Abstractions and assumptions are made, and they settle into
    working systems and then are forgotten.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 闰秒从来都不是一件愉快的事，但如果可以将每次最近的闰秒问题报告作为参考，长时间的间隔比起其他情况，问题会更加严重。即便是三年这样短的间隔，也足以让新技术得到开发，或者比之前更受欢迎。抽象和假设被提出，它们逐渐融入工作系统中，最终被遗忘。
- en: The industries around cloud computing and smartphones started to grow just as
    a multiyear gap in leap seconds was approaching. By the time the next leap second
    event occurred, huge platforms were running on technologies that had not existed
    during the last one. These technologies were built by engineers who may not even
    have been familiar with the concept of a leap second in the first place. Some
    service owners failed to patch updates to manage the leap second in a timely manner.
    Reddit, Gawker Media, Mozilla, and Qantas Airways all experienced problems.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算和智能手机相关的行业正是在闰秒的多年间隔即将到来之时开始发展的。等到下一个闰秒事件发生时，庞大的平台已在没有当时技术的基础上运行。这些技术是由那些可能根本不了解闰秒概念的工程师构建的。一些服务提供商未能及时修补更新以管理闰秒。Reddit、Gawker
    Media、Mozilla和Qantas Airways都遇到了问题。
- en: This was followed by another multiyear gap before the leap second of 2015 created
    issues for Twitter, Instagram, Pinterest, Netflix, Amazon, and Beats 1 (now Apple
    Music 1). By comparison, 2016’s leap second went out with a whimper. With just
    a six-month gap, it seems to have triggered problems only in a small number of
    machines across CloudFlare’s 102 data centers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 随后是另一个多年间隔，直到2015年的闰秒才给Twitter、Instagram、Pinterest、Netflix、Amazon和Beats 1（现为Apple
    Music 1）带来了问题。相比之下，2016年的闰秒几乎没有引起什么反响。仅仅六个月的间隔，这次闰秒似乎只在CloudFlare的102个数据中心中的少数机器上引发了问题。
- en: And the 2019 leap second at the end of another multiyear gap? It cancelled more
    than 400 flights when Collins Aerospace’s Automatic dependent surveillance–broadcast
    (ADS–B) system failed to adjust correctly. ADS–B was not new, but the FAA had
    released a rule requiring it on planes by 2020, so its adoption was much greater
    than it had been at the time of the previous leap second.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 那么2019年的闰秒呢？它发生在又一个多年间隔后，导致了超过400次航班取消，因为Collins Aerospace的自动依赖监视广播（ADS-B）系统未能正确调整。ADS-B并不新鲜，但FAA已经发布了要求到2020年飞机必须配备的规定，因此它的采用比上一轮闰秒时要广泛得多。
- en: As a general rule, we get better at dealing with problems the more often we
    have to deal with them. The longer the gap between the maturity date of deteriorations,
    the more likely knowledge has been lost and critical functionality has been built
    without taking the inevitable into account. Although the GPS rollover came at
    the end of a 20-year gap, it benefited from the accelerated upgrade cycle of devices
    most likely to be affected. Few people have 20-year-old cellphones or tablets.
    Leap seconds, on the other hand, have pretty consistently caused chaos when there’s
    a gap between the current one and the last one.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们处理问题的能力随着我们处理问题的次数增加而变得更强。当退化现象的成熟日期之间的间隙越长，丧失的知识就越多，而在没有考虑不可避免因素的情况下，关键功能的构建也变得更加可能。尽管GPS周转是在20年的间隙后发生的，但由于最可能受影响的设备的升级周期加速，它受益匪浅。很少有人使用20年的老手机或平板电脑。另一方面，闰秒的引入在每次间隙发生时，几乎始终会导致混乱。
- en: Some deteriorations have such short gaps at scale and don’t need the organization
    to do any extra meddling. For example, the average storage drive has a lifespan
    of three to five years. If you have one drive—for example, the one in your computer—you
    can mitigate the risks of this inevitable failure by regularly backing things
    up and just replacing the computer when the drive ultimately fails.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一些退化现象在大规模下具有非常短的间隙，组织无需额外干预。例如，普通的存储驱动器寿命为三到五年。如果你拥有一块驱动器——例如你电脑里的驱动器——通过定期备份并在驱动器最终故障时更换电脑，你可以降低这种不可避免的故障带来的风险。
- en: If you are running a data center, you need a strategy to keep drive failure
    from crippling operations. You need to back up regularly and restore almost instantaneously.
    That might seem like a huge engineering challenge, but the architecture to create
    such resilience is built in to the scale. Data centers don’t have just a few hard
    drives and three- to five-year gaps when they need to be replaced. Data centers
    often have thousands to hundreds of thousands of drives that are failing *constantly*.
    In 2008, Google announced it had sorted a petabyte of data in six hours with 4,000
    computers using 48,000 storage drives. A single run always resulted in at least
    one of the 48,000 drives dying.^([3](#c10-footnote-4)) A formal study of the issue
    done at about the same time pegged the annual drive failure rate at 3 percent.^([4](#c10-footnote-5))
    At 3 percent failure rate, once you get into the hundreds of thousands of drives,
    you start seeing multiple drives failing every day.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在运营数据中心，你需要一个策略，以防止驱动器故障瘫痪运营。你需要定期备份，并几乎可以瞬间恢复。这看似是一个巨大的工程挑战，但创建这种韧性的架构在规模中本身就已经内建。数据中心不仅仅拥有几块硬盘和需要在三到五年内更换的驱动器间隔。数据中心通常有成千上万到数十万块驱动器在*持续*故障。2008年，谷歌宣布它用4,000台计算机和48,000个存储驱动器在六小时内整理了一个PB的数据。一次运行总是会导致至少一块48,000个驱动器中的硬盘损坏。^([3](#c10-footnote-4))
    同一时期进行的正式研究表明，年驱动器故障率为3%。^([4](#c10-footnote-5)) 以3%的故障率，一旦进入数十万块驱动器，你开始看到每天都有多块驱动器故障。
- en: While no one would argue that drive failures are pleasant, they do not trigger
    outages once data centers reach a scale where handling drive failure is a regular
    occurrence. So rather than lengthening the period between inevitable changes,
    it might be better to shorten it to ensure engineering teams are building with
    the assumption of the inevitable at the forefront of their thoughts and that the
    teams that would have to resolve the issue understand what to do.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然没有人会认为驱动器故障是愉快的，但一旦数据中心达到足够的规模以至于处理驱动器故障成为常规操作时，它们并不会引发停机。因此，与其延长不可避免变化之间的间隔，倒不如缩短它，以确保工程团队在构建时考虑到不可避免的因素，并且负责解决问题的团队知道该如何应对。
- en: A Word of Caution About Automation
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于自动化的警告
- en: The second solution people gravitate to if a deteriorating change cannot be
    eliminated altogether is to automate its resolution. In some cases, this kind
    of automation adds a lot of value with relatively little risk. For example, failing
    regularly to renew TLS/SSL certificates could cause an entire system to grind
    to a halt suddenly and without warning. Automating the process of renewing them
    means the certificates themselves can have shorter lifespans, which increases
    the security benefit of using them.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果无法完全消除恶化的变化，人们通常会选择自动化其解决方案。在某些情况下，这种自动化能够带来相对较小风险下的巨大价值。例如，定期未能更新TLS/SSL证书可能导致整个系统突然停摆，且没有任何预警。自动化更新证书的过程意味着这些证书可以有更短的有效期，这增加了它们的安全性。
- en: 'The main thing to consider when thinking about automating a problem away is
    this: If the automation fails, will it be clear what has gone wrong? In most cases,
    expired TLS/SSL certificates trigger obvious alerts. Either the connection is
    refused, at which point the validity of the certificate should be on the checklist
    of likely culprits, or the user receives a warning that the connection is insecure.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑将问题自动化解决时，最需要考虑的是：如果自动化失败，是否能清楚地知道出了什么问题？在大多数情况下，过期的TLS/SSL证书会触发明显的警报。要么连接被拒绝，这时证书的有效性应当在检查列表中成为可能的嫌疑问题，要么用户会收到警告，提示连接不安全。
- en: Automation is more problematic when it obscures or otherwise encourages engineers
    to forget what the system is actually doing under the hood. Once that knowledge
    is lost, nothing built on top of those automated activities will include fail-safes
    in case of automation failure. The automated tasks become part of the platform,
    which is fine if the engineers in charge of the platform are aware of them and
    take responsibility for them.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当自动化掩盖了或以其他方式让工程师忘记系统实际上在幕后做了什么时，问题会更加严重。一旦这种知识丧失，所有建立在这些自动化活动上的系统将不会包含任何防范自动化失败的保障措施。自动化的任务成为平台的一部分，如果负责平台的工程师清楚它们的存在并对此负责，那这没有问题。
- en: Few programmers consider what would happen should garbage collection suddenly
    fail to execute correctly. Memory management used to be a critical part of programming,
    but now the responsibility is largely automated away. This works because the concern
    is always top of mind for software engineers who develop programming languages
    that have automated garbage collection.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有程序员会考虑如果垃圾回收突然无法正确执行，会发生什么。内存管理曾经是编程中的一个关键部分，但现在大部分责任已经自动化。这之所以可行，是因为开发具有自动垃圾回收功能的编程语言的软件工程师始终把这一问题放在心上。
- en: In other words, automation is beneficial when it’s clear who is responsible
    for the automation working in the first place and when failure states give users
    enough information to understand how they should triage the issue. Automation
    that encourages people to forget about it creates responsibility gaps. Automation
    that fails either silently or with unclear error messages at best wastes a lot
    of valuable engineering time and at worst triggers unpredictable and dangerous
    side effects.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，自动化在明确谁对其正常工作负责，以及失败状态能够为用户提供足够信息以了解如何进行问题排查时是有益的。鼓励人们忘记自动化的做法会造成责任空白。自动化如果悄无声息地失败或出现不明确的错误信息，至少会浪费大量宝贵的工程时间，最严重的情况是引发不可预知和危险的副作用。
- en: Building Something Wrong the Correct Way
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正确地构建错误的东西
- en: Throughout this book and in this chapter especially, the message has been don’t
    build for scale before you have scale. Build something that you will have to redesign
    later, even if that means building a monolith to start. Build something wrong
    and update it often.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这一章，特别是全书的核心信息是：在没有规模之前，不要为规模做准备。构建一些你之后必须重构的东西，即使这意味着一开始构建一个庞大的单体应用。做错了事情并频繁更新它。
- en: The secret to building technology “wrong” but in the correct way is to understand
    that successful complex systems are made up of stable simple systems. Before your
    system can be scaled with practical approaches, like load balancers, mesh networks,
    queues, and other trappings of distributive computing, the simple systems have
    to be stable and performant. Disaster comes from trying to build complex systems
    right away and neglecting the foundation with which all system behavior—planned
    and unexpected—will be determined.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 构建“错误”但正确的技术的秘诀在于理解，成功的复杂系统是由稳定的简单系统构成的。在使用负载均衡器、网格网络、队列和分布式计算等技术进行系统扩展之前，简单系统必须是稳定的且具备良好的性能。灾难通常源自于试图立刻构建复杂系统，而忽视了决定所有系统行为（包括计划内和意外情况）基础的稳定性。
- en: 'A good way to estimate how much complexity your system can handle is to ask
    yourself: How large is the team for this? Each additional layer of complexity
    will require a monitoring strategy and ultimately human beings to interpret what
    the monitors are telling them. Figure a minimum of three people per service. For
    the purposes of this discussion, a service is a subsystem that has its own repository
    of code (although Google famously keeps all its source code in a monolith repository),
    has dedicated resources (either VMs or separate containers), and is assumed to
    be loosely coupled from other components of the system.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 估算你的系统能承受多少复杂度的一个好方法是问自己：这个团队有多大？每增加一层复杂度，就需要一个监控策略，并最终需要人来解读监控器所提供的信息。每个服务至少需要三个人。为了便于讨论，一个服务是指具有自己代码库的子系统（尽管谷歌著名地将所有源代码存放在一个单一的代码库中），拥有专用资源（无论是虚拟机还是独立容器），并且假设它与系统的其他组件是松耦合的。
- en: The minimum on-call rotation is six people. So, a large service with a team
    of six can have a separate on-call rotation, or two small services can share a
    rotation among their teams. People can, of course, be on multiple teams, or the
    same team can run multiple services, but a person cannot be well versed in an
    infinite number of topics, so for every additional service, expect the level of
    expertise to be cut in half. In general, I prefer engineers not take on more than
    two services, but I will make exceptions when services are related.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最小的值班轮换人数是六人。所以，一个有六人的大型服务可以有独立的值班轮换，或者两个小型服务可以在他们的团队之间共享一个轮换。当然，人员可以同时属于多个团队，或者同一个团队可以运营多个服务，但一个人不可能精通无限数量的主题，所以每增加一个服务，预计专家水平会减半。一般来说，我更倾向于不让工程师负责超过两个服务，但如果服务相关，我会做出例外。
- en: I lay out these restrictions only to give you a framework from which to think
    about the capacity of the human beings on which your system relies to future-proof
    it. You can change the exact numbers to fit what you think is realistic if you
    like. The tendency among engineers is to build with an eye toward infinite scale.
    Lots of teams model their systems after white papers from Google or Amazon when
    they do not have the team to maintain a Google or an Amazon. What the human resources
    on a team can support is the upper bound on the level of system complexity. Inevitably
    the team will grow, the usage of the system will grow, and many of these architectural
    decisions will have to be revised. That’s fine.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我列出这些限制只是为了给你提供一个框架，帮助你思考你的系统所依赖的人的能力，以确保系统的未来可扩展性。如果你愿意，可以根据你的实际情况调整具体数字。工程师们的倾向是将系统设计得具有无限扩展性。许多团队在没有足够团队支持的情况下，会根据谷歌或亚马逊的白皮书来建模他们的系统，而忽视了他们并没有一个能够维护谷歌或亚马逊规模的团队。团队所能支持的人力资源，就是系统复杂性的上限。不可避免地，团队会增长，系统的使用也会增长，许多这些架构决策将不得不进行修订。这是可以接受的。
- en: 'Here’s an example: Service A needs to send data to Service B. The team maintaining
    the complete system has about 11 people on it. Four people are on operations,
    maintaining the servers and building tooling to help enforce standards. Four people
    are on the data science team, designing models and writing the code to implement
    them, and the remaining three people build the web services. That three-person
    team maintains Service B but also another service elsewhere in the system. The
    data science team maintains Service A, but also two other services.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子：服务A需要向服务B发送数据。维护整个系统的团队大约有11个人。四个人负责运营，维护服务器并构建工具来帮助执行标准。四个人在数据科学团队，设计模型并编写代码实现这些模型，其余的三个人负责构建Web服务。这个三人团队维护服务B，同时还维护系统中的另一个服务。数据科学团队负责维护服务A，同时也负责另外两个服务。
- en: Both of those teams are a bit overloaded for their staffing levels, but the
    usage of the system is low, so the pressure isn’t too great.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个团队的人员配置稍显过载，但系统的使用率较低，因此压力并不大。
- en: So, how should Service A talk to Service B?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，服务A应该如何与服务B通信呢？
- en: The first suggestion is to set up a message queue so that communication between
    A and B is decoupled and resilient. That would be the most scalable solution,
    but it would also require someone to set up the message queue and the workers,
    monitor them, and respond when something goes wrong. Which team is responsible
    for that? Cynical engineers will probably say operations. This is usually what
    happens when teams cannot support what they are building. Certain parts of the
    system get abandoned, and the only people who pay attention to them are the teams
    that are in charge of the infrastructure itself (and usually only when something
    is on fire).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个建议是建立一个消息队列，使得A与B之间的通信解耦并具有弹性。这将是最具可扩展性的解决方案，但也需要有人来设置消息队列和工作节点，监控它们，并在出现问题时进行响应。哪个团队负责这一切呢？持悲观看法的工程师可能会说是运维团队。通常，当团队无法支撑他们所构建的东西时，就会发生这种情况。系统的某些部分被抛弃，唯一关注它们的团队通常是负责基础设施的团队（通常只有在出现重大问题时才会关注）。
- en: Although a message queue is more scalable, a simpler solution with tighter coupling
    would probably get better results to start. Service A could just send an HTTP
    request to Service B. Delegation of responsibilities on triage is built in. If
    the error is thrown on the Service B side, the team that owns Service B is alerted.
    If it’s thrown on the Service A side, the team that owns Service A is alerted.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然消息队列更具可扩展性，但一个更简单的方案，结合更紧密的耦合，可能在开始时会获得更好的结果。服务A可以直接向服务B发送HTTP请求。责任的委派已经内建。如果错误发生在服务B端，服务B的团队会被通知。如果错误发生在服务A端，服务A的团队会被通知。
- en: But what about network issues? It’s true that networks sometimes fail, but if
    we assume that both of these services are hosted on a major cloud provider, the
    chances of a one-off network issue that causes no other problems are unlikely.
    Networking issues are not subtle, and they are generally a product of misconfiguration
    rather than gremlins.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，网络问题怎么办呢？的确，网络有时会出现故障，但如果我们假设这两个服务都托管在一个主要的云服务提供商上，那么导致没有其他问题的单次网络故障的可能性很小。网络问题并不微妙，通常是配置错误造成的，而不是外部干扰因素。
- en: The HTTP request solution is wrong in the correct way because migrating from
    an HTTP request between Service A and Service B to a message queue later is straightforward.
    While we are temporarily losing built-in fault tolerance and accepting a higher
    scaling burden, it creates a system that is easier for the current teams to maintain.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP请求方案是正确的错误，因为将服务A和服务B之间的HTTP请求迁移到消息队列是简单的。虽然我们暂时失去了内建的容错能力并承受了更高的扩展负担，但它创建了一个当前团队更容易维护的系统。
- en: The counterexample would be if we swapped the order of the HTTP request and
    had Service B poll Service A for new data. While this is also less complex than
    a message queue, it is unnecessarily resource-intensive. Service A does not produce
    a constant stream of new data, and by polling Service B, it may spend hours or
    even days sending meaningless requests. Moving from this to a queue would require
    significant changes to the code. There’s little value to building things wrong
    this way.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 反例是如果我们交换HTTP请求的顺序，让服务B轮询服务A以获取新数据。虽然这比消息队列简单，但它不必要地消耗资源。服务A并不是持续产生新数据，通过轮询服务B，它可能花费数小时甚至数天发送没有意义的请求。从这种方式转变为使用队列将需要对代码进行重大修改。以这种错误的方式构建系统几乎没有什么价值。
- en: Feedback Loops
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反馈回路
- en: Another way to think about this is to sketch out how maintaining this system
    will create feedback loops across engineering. Thinking about how work gets done
    in terms of flows, delays, stocks, and goals can help clarify whether the level
    of work required to maintain a system of a given complexity is feasible.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考方式是描绘出维护这个系统如何在工程中产生反馈回路。从工作流、延迟、库存和目标的角度思考工作完成的方式，有助于澄清维护某一复杂系统所需的工作量是否可行。
- en: 'Let’s take another look at the question of Service A and Service B. We know
    we have seven people working on these two services and that each person has an
    eight-hour workday. Service B’s team is split between that and another service,
    so we can assume they have a budget of four hours per service they own. With three
    people, that’s about 12 hours per day. Service A’s team is maintaining a total
    of three services, so they have a budget of 2.5 hours per person and 10 hours
    per service per day. A model like this might have the following characteristics:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Stocks A *stock* is any element that can accrue or drain over time. The traditional
    example of a system model is a bathtub filling with water. The water is a stock.
    In this model, technical debt will accrue for each service constantly regardless
    of the level of work. Debt will be paid down by spending work hours. The tasks
    in our workweek are also a stock that our teams will burn down as they operate.
    That eight-hour day is also a stock. When the system is stable, the eight hours
    are fully spent and fully restored each day.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Flows A *flow* is a change that either increases or decreases a stock. In the
    bathtub example, the rate of water coming out of the faucet is a flow, and if
    the drain is open, the rate of water coming out of the bathtub is another flow.
    In our model, at any time, people can work more than eight hours a day, but doing
    so will decrease their ultimate productivity and require them to work less than
    eight hours a day later. We can represent this by assuming that we’re borrowing
    the extra hours for the next day’s budget. Tasks are completed by spending work
    hours; we might keep our model simple and say every task is worth an hour, or
    we might separate tasks into small, medium, and large sizes with different number-of-hours
    costs for each option. Spending work hours decreases the stock of technical debt
    or work tasks, depending on how those hours are applied.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Delays Good systems models acknowledge that not everything is instantaneous.
    *Delays* represent gaps of time in how flows respond. With our model, new work
    does not immediately replace old work; it is planned and assigned in one-week
    increments. We can view the period between each task assignment as being a delay.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feedback *Feedback loops* form when the change in stock affects the nature of
    the flow, either positively or negatively. In our model, when people work more
    than their total eight-hour budget, they lose future hours. The more hours they
    work, the more hours they have to borrow to maintain a stretch of eight-hour days
    in a row. Eventually, they have to take time off to normalize. Alternatively,
    they could borrow hours by spending more of their budget on Service A or Service
    B, but that means the other services they are responsible for will be neglected,
    and their technical debt will accrue unchecked.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visually, we might represent that model like in [Figure 10-1](#figure10-1).
    The solid lines represent flows, and the dotted lines represent variables that
    influence the rate of flows.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Work hours come into the model via our schedule but are affected by a stock
    representing burnout. If burnout is high, work hours fall; if work hours are high,
    burnout rises. How much of our available work hours on any given day is devoted
    to tasks on one service depends on the size of our team and the number of services
    or projects the team is trying to maintain at the same time. The more we are able
    to devote to work tasks, the more we ship. When work tasks are completed, whatever
    extra time is left is directed to improving our technical debt.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![f10001](../Images/f10001.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-1: Feedback loops in the team’s workload'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Although this visual model might just look like an illustration, we can actually
    program it for real and use it to explore how our team manages its work in various
    conditions. Two tools popular with system thinkers for these kinds of models are
    Loopy ([https://ncase.me/loopy/](https://ncase.me/loopy/)). and InsightMaker ([https://insightmaker.com/](https://insightmaker.com/)).
    Both are free and open source, and both allow you to experiment with different
    configurations and interactions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s just think through a couple scenarios. Suppose we have a sprint
    with 24 hours of work tasks for Service A and for Service B. That shouldn’t be
    a problem; Service A’s team has a weekly capacity of 50 hours a week for Service
    A, and Service B’s team has a weekly capacity of 60 hours a week for Service B.
    With 24 hours of sprint tasks, each team has plenty of extra time to burn down
    technical debt.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: But what happens if a sprint has 70 hours of work? Service A’s team could handle
    that if every one of the team’s four people borrowed five hours that week from
    the next week, but the team would have no time to manage technical debt and would
    have only 30 hours of time for Service A the following week.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'What if 70 hours of work were the norm for sprints? The teams would slowly
    burn out while having no ability to rethink the system design or manage their
    debt. The model is unstable, but we can restore equilibrium by doing one of the
    following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: The team transfers ownership of one of their services to another team, giving
    them more hours a day to spend on their tasks for Service A or Service B.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The team allows technical debt to accrue on one or all of their services until
    a service fails.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The team works more and more until individuals burn out, at which point they
    become unavailable for a period of time.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the things that the teams might do to try to reestablish equilibrium
    is change the design so that the integration pattern means less work for Service
    A’s lower-capacity team. Suppose that instead of connecting over HTTP, Service
    B connected directly to Service A’s database to get the data it needs. Service
    A’s team would no longer have to build an endpoint to receive requests from Service
    B, which means they could better balance their workload and manage their maintenance
    responsibilities, but the model would reach equilibrium at the expense of the
    quality of the overall architecture.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 团队可能会采取的一种方式是，改变设计，使得集成模式能减轻服务A的低容量团队的工作量。假设，服务B不再通过HTTP连接，而是直接连接到服务A的数据库来获取所需数据。服务A的团队将不再需要构建一个接收来自服务B请求的端点，这意味着他们可以更好地平衡工作负载，管理维护责任，但这种模式会以整体架构质量的牺牲为代价达到平衡。
- en: If you’re a student of Fred Brooks’s*The Mythical Man-Month*, you might object
    to the premise of this model. It suggests that one possible solution is to add
    more people to the team, and we know that software is not successfully built in
    man-hours. More people do not make software projects go faster.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是Fred Brooks的*《神话般的工程人月》*的读者，你可能会对这个模型的前提提出异议。它暗示其中一个可能的解决方案是增加团队成员，而我们知道软件并不是通过人小时来成功构建的。更多的人并不会让软件项目进展得更快。
- en: But the point of this type of model is not to plan a road map or budget head
    count. It’s to help people consider the engineering team as a system of interconnected
    parts. Bad software is unmaintained software. Future-proofing means constantly
    rethinking and iterating on the existing system. People don’t go into building
    a service thinking that they will neglect it until it’s a huge liability for their
    organization. People fail to maintain services because they are not given the
    time or resources to maintain them.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这种模型的核心并不是规划路线图或预算人员数量，而是帮助人们将工程团队视为一个相互连接的系统。糟糕的软件就是没有得到维护的软件。面向未来意味着不断地重新思考和迭代现有系统。人们在构建服务时并不会认为他们会忽视它，直到它成为组织的巨大负担。人们未能维护服务，是因为没有给予足够的时间或资源来进行维护。
- en: If you know approximately how much work is in an average sprint and how many
    people are on the team, you can reason about the likelihood that a team of that
    size will be able to successfully maintain X number of services. If the answer
    is no, the design of the architecture is probably too complex for the current
    team.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你大致了解一个平均迭代周期中的工作量以及团队人数，你就可以推测该团队是否有可能成功地维护X个服务。如果答案是否定的，那么架构设计可能对当前的团队来说过于复杂。
- en: Don’t Stop the Bus
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不要停下公交车
- en: In summary, systems age in two different ways. Their usage patterns change,
    which require them to be scaled up and down, or the resources that back them deteriorate
    up to the point where they fail. Legacy modernizations themselves are anti-patterns.
    A healthy organization running a healthy system should be able to evolve it over
    time without rerouting resources to a formal modernization effort.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，系统的老化有两种方式。其使用模式发生变化，要求进行规模的扩展或收缩，或者支撑它们的资源逐渐退化，最终导致它们失败。遗留系统现代化本身就是反模式。一个健康的组织运行一个健康的系统，应该能够随着时间的推移不断演进，而不需要重新调整资源去进行正式的现代化努力。
- en: To achieve that healthy state, though, we have to be able to see the levels
    and hierarchy of the systems of systems we’re building. Our technical systems
    are made up of smaller systems that must be stable. Our engineering team behaves
    as another system, establishing feedback loops that determine how much time and
    energy they can spend on the upgrades necessary to evolve a technology. The engineering
    system and the technical system are not separate from each other.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要实现这种健康的状态，我们必须能够看到我们所构建的系统系统的层次和层级关系。我们的技术系统由必须保持稳定的小型系统组成。我们的工程团队也表现为另一个系统，建立反馈回路，决定他们可以投入多少时间和精力来进行技术演进所需的升级。工程系统和技术系统不是相互独立的。
- en: 'I once had a senior executive tell me, “You’re right about the seriousness
    of this security vulnerability, Marianne, but we can’t stop the bus.” What he
    meant by this was that he didn’t want to devote resources to fixing it because
    he was worried it would slow down new development. He was right, but he was right
    only because the organization had been ignoring the problem in question for two
    or three years. Had they addressed it when it was discovered, they could have
    done so with minimum investment. Instead, the problem multiplied as engineers
    copied the bad code into other systems and built more things on top of it. They
    had a choice: slow down the bus or wait for the wheels to fall off the bus.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经有一位高级主管告诉我：“你说得对，马里安，这个安全漏洞很严重，但我们不能停下‘公交车’。”他的意思是，他不想投入资源来修复这个问题，因为他担心这样会拖慢新开发进度。他是对的，但只有在组织忽视这个问题两三年之后，他才是对的。如果他们在问题被发现时就解决了，可能只需要最低的投入。相反，问题随着工程师将不良代码复制到其他系统，并在其上构建更多内容，逐渐扩大了。他们面临着一个选择：减慢“公交车”的速度，还是等待“公交车”的车轮掉下来。
- en: Organizations choose to keep the bus moving as fast as possible because they
    can’t see all the feedback loops. Shipping new code gets attention, while technical
    debt accrues silently and without fanfare. It’s not the age of a system that causes
    it to fail, but the pressure of what the organization has forgotten about it slowly
    building toward an explosion.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 组织选择尽可能快地推动“公交车”前进，因为他们看不见所有的反馈回路。发布新代码引起了关注，而技术债务则默默累积，毫无声息。导致系统失败的不是它的使用年限，而是组织对它的忽视，逐渐积累的压力最终导致了爆炸性后果。
