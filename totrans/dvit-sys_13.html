<html><head></head><body>
<h2 class="h2" id="ch13"><span epub:type="pagebreak" id="page_617"/><span class="big">13</span><br/>THE OPERATING SYSTEM</h2>&#13;
<div class="imagec"><img alt="image" src="../images/common.jpg"/></div>&#13;
<p class="noindents">The <em>operating system</em> (OS) is a special system software layer that sits between the computer hardware and application programs running on the computer (see <a href="ch13.xhtml#ch13fig1">Figure 13-1</a>). The OS software is persistent on the computer, from power-on to power-off. Its primary purpose is to <em>manage</em> the underlying hardware components to efficiently run program workloads and to make the computer <em>easy to use</em>.</p>&#13;
<div class="imagec" id="ch13fig1"><img alt="image" src="../images/13fig01.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-1: The OS is special system software between the user and the hardware. It manages the computer’s hardware and implements abstractions to make the hardware easier to use.</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_618"/>One of the ways in which the OS makes the computer hardware easy to use is in its support for initiating programs to run on the computer. Consider what happens when a user double-clicks an icon or types the name of a program executable at a shell prompt (e.g., <span class="literal">./a.out</span>) to start a program running on the underlying system. The OS handles all the details of this operation, such as loading the program from disk into RAM and initializing the CPU to start running the program instructions; the OS hides from users these types of low-level actions that are necessary to run the user’s program on the computer.</p>&#13;
<p class="indent">One example of how the OS makes efficient use of system resources is by implementing <em>multiprogramming</em>, which means allowing more than a single program to run on the computer at a time. Multiprogramming does not necessarily mean that all the programs are running simultaneously on the computer hardware. In fact, the set of running programs in the system is typically much larger than the number of CPU cores. Instead, it means that the OS shares hardware resources, including the CPU, among several programs running in the system. For example, when one program needs data that is currently on disk, the OS can put another program on the CPU while the first program waits for the data to become available. Without multiprogramming, the CPU would sit idle whenever the program running on the computer accesses slower hardware devices. To support multiprogramming, the OS needs to implement an abstraction of a running program, called a <em>process</em>. The process abstraction enables the OS to manage the set of multiple programs that are running on the system at any given time.</p>&#13;
<p class="indent">Some example operating systems include Microsoft’s Windows, Apple’s macOS and iOS, Oracle’s Solaris, and open-source Unix variants such as OpenBSD and Linux. We use Linux examples in this book. However, all of these other general-purpose operating systems implement similar functionality, albeit sometimes in different ways.</p>&#13;
<h5 class="h5" id="lev3_99">The Kernel</h5>&#13;
<p class="noindent">The term <em>operating system</em> is often used to refer to a large set of system-level software that performs some kind of resource management and that implements “easy-to-use” abstractions of the underlying system. In this chapter, we focus on the operating system <em>kernel</em>; thus, when we use the term OS alone, we mean the OS kernel.</p>&#13;
<p class="indent">The OS kernel implements core OS functionality—the functionality necessary for any use of the system. This functionality includes managing the computer hardware layer to run programs, implementing and managing OS abstractions exported to users of the system (e.g., files are an OS abstraction on top of stored data), and implementing interfaces to the user applications layer and to the hardware device layer. The kernel implements <em>mechanisms</em> to enable the hardware to run programs and to implement its abstractions such as processes. Mechanisms are the “how” part of OS functionality. The kernel also implements <em>policies</em> for efficiently managing the computer hardware and for governing its abstractions. Policies dictate the “what,” “when,” and “to whom” part of OS functionality. For example, a mechanism implements <span epub:type="pagebreak" id="page_619"/>initializing the CPU to run instructions from a particular process, and a policy decides which process gets to run next on the CPU.</p>&#13;
<p class="indent">The kernel implements a programming interface for users of the system: the <em>system call interface</em>. Users and programs interact with the OS through its system call interface. For example, if a program wants to know the current time of day, it can obtain that information from the OS by invoking the <span class="literal">gettimeofday</span> system call system call.</p>&#13;
<p class="indent">The kernel also provides an interface for interacting with hardware devices (the <em>device interface</em>). Typically, I/O devices such as hard disk drives (HDDs), keyboards, and solid-state drives (SSDs) interact with the kernel through this interface. These devices come with special device driver software that runs in the OS and handles transferring data to or from a specific device. The device driver software interacts with the OS through the OS’s device interface; a new device can be added to a computer system by loading its device driver code, written to conform to the OS’s device interface, into the OS. The kernel directly manages other hardware devices, such as the CPU and RAM. <a href="ch13.xhtml#ch13fig2">Figure 13-2</a> shows the OS kernel layer between the user applications and the computer hardware, including its programming interface to users and its hardware device interface.</p>&#13;
<div class="imagec" id="ch13fig2"><img alt="image" src="../images/13fig02.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-2: The OS kernel: core OS functionality necessary to use the system and facilitate cooperation between I/O devices and users of the system</em></p>&#13;
<p class="indent">In the rest of this chapter, we examine the role the operating system plays in running programs and in efficiently managing system resources. Our discussion is primarily focused on the mechanism (the “how”) of the OS functionality and the implementation of two primary OS abstractions: a <em>process</em> (a running program) and <em>virtual memory</em> (a view of process memory space that is abstracted from its underlying physical storage in RAM or secondary storage).</p>&#13;
<h3 class="h3" id="lev1_99"><span epub:type="pagebreak" id="page_620"/>13.1 How the OS Works and How It Runs</h3>&#13;
<p class="noindent">Part of the job of the OS is to support programs running on the system. To start a program running on a computer, the OS allocates a portion of RAM for the running program, loads the program’s binary executable from disk into RAM, creates and initializes OS state for the process associated with this running program, and initializes the CPU to start executing the process’s instructions (e.g., the CPU registers need to be initialized by the OS to fetch and execute the process’s instructions). <a href="ch13.xhtml#ch13fig3">Figure 13-3</a> illustrates these steps.</p>&#13;
<div class="imagec" id="ch13fig3"><img alt="image" src="../images/13fig03.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-3: The steps that the OS takes to start a new program running on the underlying hardware</em></p>&#13;
<p class="indent">Like user programs, the OS is also software that runs on the computer hardware. The OS, however, is special system software that manages all system resources and implements the interface for users of the computer system; it is necessary for using the computer system. Because the OS is software, its binary executable code runs on the hardware just like any other program: its data and instructions are stored in RAM and its instructions are fetched and executed by the CPU just like a user’s program instructions are. As a result, for the OS to run, its binary executable needs to be loaded into RAM and the CPU initialized to start running OS code. However, because the OS is responsible for the task of running code on the hardware, it needs some help to get started running.</p>&#13;
<h4 class="h4" id="lev2_219">13.1.1 OS Booting</h4>&#13;
<p class="noindent">The process of the OS loading and initializing itself on the computer is known as <em>booting</em>—the OS “pulls itself up by its bootstraps,” or <em>boots</em> itself on the computer. The OS needs a little help to initially get loaded onto the computer and to begin running its boot code. To initiate the OS code to start running, code stored in computer firmware (nonvolatile memory in the hardware) runs when the computer first powers up; <em>BIOS</em> (Basic Input/Output System) and <em>UEFI</em> (Unified Extensible Firmware Interface) are two examples of this type of firmware. On power-up, BIOS or UEFI runs <span epub:type="pagebreak" id="page_621"/>and does just enough hardware initialization to load the first chunk of the OS (its boot block) from disk into RAM and to start running boot block instructions on the CPU. Once the OS starts running, it loads the rest of itself from disk, discovers and initializes hardware resources, and initializes its data structures and abstractions to make the system ready for users.</p>&#13;
<h4 class="h4" id="lev2_220">13.1.2 Getting the OS to Do Something: Interrupts and Traps</h4>&#13;
<p class="noindent">After the OS finishes booting and initializing the system for use, it then just waits for something to do. Most operating systems are implemented as <em>interrupt-driven systems</em>, meaning that the OS doesn’t run until some entity needs it to do something—the OS is woken up (interrupted from its sleep) to handle a request.</p>&#13;
<p class="indent">Devices in the hardware layer may need the OS to do something for them. For example, a <em>network interface card</em> (NIC) is a hardware interface between a computer and a network. When the NIC receives data over its network connection, it interrupts (or wakes up) the OS to handle the received data (see <a href="ch13.xhtml#ch13fig4">Figure 13-4</a>). For example, the OS may determine that the data received by the NIC is part of a web page that was requested by a web browser; it then delivers the data from the NIC to the waiting web browser process.</p>&#13;
<p class="indent">Requests to the OS also come from user applications when they need access to protected resources. For example, when an application wants to write to a file, it makes a <em>system call</em> to the OS, which wakes up the OS to perform the write on its behalf (see <a href="ch13.xhtml#ch13fig4">Figure 13-4</a>). The OS handles the system call by writing the data to a file stored on disk.</p>&#13;
<div class="imagec" id="ch13fig4"><img alt="image" src="../images/13fig04.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-4: In an interrupt-driven system, user-level programs make system calls, and hardware devices issue interrupts to initiate OS actions.</em></p>&#13;
<p class="indent">Interrupts that come from the hardware layer, such as when a NIC receives data from the network, are typically referred to as hardware interrupts, or just <em>interrupts</em>. Interrupts that come from the software layer as the result of instruction execution, such as when an application makes a system call, are typically referred to as <em>traps</em>. That is, a system call “traps into the OS,” which handles the request on behalf of the user-level program. Exceptions from either layer may also interrupt the OS. For example, a hard disk drive may interrupt the OS if a read fails due to a bad disk block, and an application program may trigger a trap to the OS if it executes a divide instruction that divides by zero.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_622"/>System calls are implemented using special trap instructions that are defined as part of the CPU’s instruction set architecture (ISA). The OS associates each of its system calls with a unique identification number. When an application wants to invoke a system call, it places the desired call’s number in a known location (the location varies according to the ISA) and issues a trap instruction to interrupt the OS. The trap instruction triggers the CPU to stop executing instructions from the application program and to start executing OS instructions that handle the trap (run the OS trap handler code). The trap handler reads the user-provided system call number and executes the corresponding system call implementation.</p>&#13;
<p class="indent">Here’s an example of what a <span class="literal">write</span> system call might look like on an IA32 Linux system:</p>&#13;
<p class="programs">&#13;
/* C code */<br/>&#13;
ret = write(fd, buff, size);<br/>&#13;
<br/>&#13;
# IA32 translation<br/>&#13;
write:<br/>&#13;
<br/>&#13;
...            # set up state and parameters for OS to perform write<br/>&#13;
movl $4, %eax  # load 4 (unique ID for write) into register eax<br/>&#13;
int  $0x80     # trap instruction: interrupt the CPU and transition to the OS<br/>&#13;
addl $8, %ebx  # an example instruction after the trap instruction<br/>&#13;
</p>&#13;
<p class="indent">The first instruction (<span class="literal">movl $4, %eax</span>) puts the system call number for <span class="literal">write</span> (4) into register <span class="literal">eax</span>. The second instruction (<span class="literal">int $0x80</span>) triggers the trap. When the OS trap handler code runs, it uses the value in register <span class="literal">eax</span> (4) to determine which system call is being invoked and runs the appropriate trap handler code (in this case it runs the <span class="literal">write</span> handler code). After the OS handler runs, the OS continues the program’s execution at the instruction right after the trap instruction (<span class="literal">addl</span> in this example).</p>&#13;
<p class="indent">Unlike system calls, which come from executing program instructions, hardware interrupts are delivered to the CPU on an interrupt bus. A device places a signal, typically a number indicating the type of interrupt, on the CPU’s interrupt bus (see <a href="ch13.xhtml#ch13fig5">Figure 13-5</a>). When the CPU detects the signal on its interrupt bus, it stops executing the current process’s instructions and starts executing OS interrupt handler code. After the OS handler code runs, the OS continues the process’s execution at the application instruction that was being executed when the interrupt occurred.</p>&#13;
<div class="imagec" id="ch13fig5"><img alt="image" src="../images/13fig05.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_623"/><em>Figure 13-5: A hardware device (disk) sends a signal to the CPU on the interrupt bus to trigger OS execution on its behalf.</em></p>&#13;
<p class="indent">If a user program is running on the CPU when an interrupt (or trap) occurs, the CPU runs the OS’s interrupt (or trap) handler code. When the OS is done handling an interrupt, it resumes executing the interrupted user program at the point it was interrupted.</p>&#13;
<p class="indent">Because the OS is software, and its code is loaded into RAM and run on the CPU just like user program code, the OS must protect its code and state from regular processes running in the system. The CPU helps by defining two execution modes.</p>&#13;
<div class="number">&#13;
<p class="number">1. In <em>user mode</em>, a CPU executes only user-level instructions and accesses only the memory locations that the operating system makes available to it. The OS typically prevents a CPU in user mode from accessing the OS’s instructions and data. User mode also restricts which hardware components the CPU can directly access. In <em>kernel mode</em>, a CPU executes any instructions and accesses any memory location (including those that store OS instructions and data). It can also directly access hardware components and execute special instructions.</p>&#13;
</div>&#13;
<p class="indent">When OS code is run on the CPU, the system runs in kernel mode, and when user-level programs run on the CPU, the system runs in user mode. If the CPU is in user mode and receives an interrupt, the CPU switches to kernel mode, fetches the interrupt handler routine, and starts executing the OS handler code. In kernel mode, the OS can access hardware and memory locations that are not allowed in user mode. When the OS is done handling the interrupt, it restores the CPU state to continue executing user-level code at the point at which the program left off when interrupted and returns the CPU back to user mode (see <a href="ch13.xhtml#ch13fig6">Figure 13-6</a>).</p>&#13;
<div class="imagec" id="ch13fig6"><img alt="image" src="../images/13fig06.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-6: The CPU and interrupts. User code running on the CPU is interrupted (at time X on the time line), and OS interrupt handler code runs. After the OS is done handling the interrupt, user code execution is resumed (at time Y on the time line).</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_624"/>In an interrupt-driven system, interrupts can happen at any time, meaning that the OS can switch from running user code to interrupt handler code at any machine cycle. One way to efficiently support this execution context switch from user mode to kernel mode is to allow the kernel to run within the execution context of every process in the system. At boot time, the OS loads its code at a fixed location in RAM that is mapped into the top of the address space of every process (see <a href="ch13.xhtml#ch13fig7">Figure 13-7</a>), and initializes a CPU register with the starting address of the OS handler function. On an interrupt, the CPU switches to kernel mode and executes OS interrupt handler code instructions that are accessible at the top addresses in every process’s address space. Because every process has the OS mapped to the same location at the top of its address space, the OS interrupt handler code is able to execute quickly in the context of any process that is running on the CPU when an interrupt occurs. This OS code can be accessed only in kernel mode, protecting the OS from user-mode accesses; during regular execution a process runs in user mode and cannot read or write to the OS addresses mapped into the top of its address space.</p>&#13;
<div class="imagec" id="ch13fig7"><img alt="image" src="../images/13fig07.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-7: Process address space: the OS kernel is mapped into the top of every process’s address space.</em></p>&#13;
<p class="indent">Although mapping the OS code into the address space of every process results in fast kernel code execution on an interrupt, many modern processors have features that expose vulnerabilities to kernel protections when the OS is mapped into every process like this. As of the January 2018 announcement of the Meltdown hardware exploit,<sup><a href="ch13.xhtml#fn13_1" id="rfn13_1">1</a></sup> operating systems are separating kernel memory and user-level program memory in ways that protect against this exploit but that also result in less efficient switching to kernel mode to handle interrupts.</p>&#13;
<h3 class="h3" id="lev1_100">13.2 Processes</h3>&#13;
<p class="noindent">One of the main abstractions implemented by the operating system is a <em>process</em>. A process represents an instance of a program running in the system, which includes the program’s binary executable code, data, and execution <em>context</em>. The context tracks the program’s execution by maintaining its register values, stack location, and the instruction it is currently executing.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_625"/>Processes are necessary abstractions in <em>multiprogramming</em> systems, which support multiple processes existing in the system at the same time. The process abstraction is used by the OS to keep track of individual instances of programs running in the system, and to manage their use of system resources.</p>&#13;
<p class="indent">The OS provides each process with a “lone view” abstraction of the system. That is, the OS isolates processes from one another and gives each process the illusion that it’s controlling the entire machine. In reality, the OS supports many active processes and manages resource sharing among them. The OS hides the details of sharing and accessing system resources from the user, and the OS protects processes from the actions of other processes running in the system.</p>&#13;
<p class="indent">For example, a user may simultaneously run two instances of a Unix shell program along with a web browser on a computer system. The OS creates three processes associated with these three running programs: one process for each separate execution of the Unix shell program, and one process for the web browser. The OS handles switching between these three processes running on the CPU, and it ensures that as a process runs on the CPU, only the execution state and system resources allocated to the process can be accessed.</p>&#13;
<h4 class="h4" id="lev2_221">13.2.1 Multiprogramming and Context Switching</h4>&#13;
<p class="noindent">Multiprogramming enables the OS to make efficient use of hardware resources. For example, when a process running on the CPU needs to access data that are currently on disk, rather than have the CPU sit idle waiting for the data to be read into memory, the OS can give the CPU to another process and let it run while the read operation for the original process is being handled by the disk. By using multiprogramming, the OS can mitigate some of the effects of the memory hierarchy on its program workload by keeping the CPU busy executing some processes while other processes are waiting to access data in the lower levels of the memory hierarchy.</p>&#13;
<p class="indent">General-purpose operating systems often implement <em>timesharing</em>, which is multiprogramming wherein the OS schedules each process to take turns executing on the CPU for short time durations (known as a <em>time slice</em> or <em>quantum</em>). When a process completes its time slice on the CPU, the OS removes the process from the CPU and lets another run. Most systems define time slices to be a few milliseconds (10<sup>-3</sup> seconds), which is a long time in terms of CPU cycles but is not noticeable to a human.</p>&#13;
<p class="indent">Timesharing systems further support the “lone view” of the computer system to the user; because each process frequently executes on the CPU for short bursts of time, the fact that they are all sharing the CPU is usually imperceptible to the user. Only when the system is very heavily loaded might a user notice the effects of other processes in the system. The Unix command <span class="literal">ps -A</span> lists all the processes running in the system—you may be surprised by how many there are. The <span class="literal">top</span> command is also useful for seeing the state of the system as it runs by displaying the set of processes that currently use the most system resources (such as CPU time and memory space).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_626"/>In multiprogrammed and timeshared systems, processes run <em>concurrently</em>, meaning that their executions overlap in time. For example, the OS may start running process A on the CPU, and then switch to running process B for a while, and later switch back to running process A some more. In this scenario, processes A and B run concurrently because their execution on the CPU overlaps due to the OS switching between the two.</p>&#13;
<h5 class="h5" id="lev3_100">13.2.1.1 Context Switching</h5>&#13;
<p class="noindent">The <em>mechanism</em> behind multiprogramming determines how the OS swaps one process running on the CPU with another. The <em>policy</em> aspect of multiprogramming governs scheduling the CPU, or picking which process from a set of candidate processes gets to use the CPU next and for how long. We focus primarily on the mechanism of implementing multiprogramming. Operating systems textbooks cover scheduling policies in more detail.</p>&#13;
<p class="indent">The OS performs <em>context switching</em>, or swapping process state on the CPU, as the primary mechanism behind multiprogramming (and time-sharing). There are two main steps to performing a CPU context switch:</p>&#13;
<div class="number">&#13;
<p class="number">1. The OS saves the context of the current process running on the CPU, including all of its register values (PC, stack pointers, general-purpose register, condition codes, etc.), its memory state, and some other state (for example, the state of system resources it uses, like open files).</p>&#13;
<p class="number">2. The OS restores the saved context from another process on the CPU and starts the CPU running this other process, continuing its execution from the instruction where it left off.</p>&#13;
</div>&#13;
<p class="indent">One part of context switching that may seem impossible to accomplish is that the OS’s code that implements context switching must run on the CPU while it saves (restores) a process’s execution contexts from (to) the CPU; the instructions of the context switching code need to use CPU hardware registers to execute, but the register values from the process being context switched off the CPU need to be saved by the context switching code. Computer hardware provides some help to make this possible.</p>&#13;
<p class="indent">At boot time, the OS initialized the hardware, including initializing the CPU state, so that when the CPU switches to kernel mode on an interrupt, the OS interrupt handler code starts executing and the interrupted process’s execution state is protected from this execution. Together, the computer hardware and OS perform some of the initial saving of the user-level execution context, enough that the OS code can run on the CPU without losing the execution state of the interrupted process. For example, register values of the interrupted process need to be saved so that when the process runs again on the CPU, the process can continue from the point at which it left off, using its register values. Depending on the hardware support, saving the user-level process’s register values may be done entirely by the hardware or may be done almost entirely in software as the first part of the kernel’s interrupt handling code. At a minimum, the process’s program counter (PC) <span epub:type="pagebreak" id="page_627"/>value needs to be saved so that its value is not lost when the kernel interrupt handler address is loaded into the PC.</p>&#13;
<p class="indent">After the OS starts running, it executes its full process context switching code, saving the full execution state of the process running on the CPU and restoring the saved execution state of another process onto the CPU. Because the OS runs in kernel mode it is able to access any parts of computer memory and can execute privileged instructions and access any hardware registers. As a result, its context switching code is able to access and save the CPU execution state of any process to memory, and it is able to restore from memory the execution state of any process to the CPU. OS context switching code completes by setting up the CPU to execute the restored process’s execution state, and by switching the CPU to user mode. Once switched to user mode, the CPU executes instructions, and uses execution state from the process that the OS context switched onto the CPU.</p>&#13;
<h4 class="h4" id="lev2_222">13.2.2 Process State</h4>&#13;
<p class="noindent">In multiprogrammed systems, the OS must track and manage the multiple processes existing in the system at any given time. The OS maintains information about each process, including:</p>&#13;
<ul>&#13;
<li class="noindent">A <em>process id</em> (PID), which is a unique identifier for a process. The <span class="literal">ps</span> command lists information about processes in the system, including their PID values.</li>&#13;
<li class="noindent">The address space information for the process.</li>&#13;
<li class="noindent">The execution state of the process (e.g., CPU register values, stack location).</li>&#13;
<li class="noindent">The set of resources allocated to the process (e.g., open files).</li>&#13;
<li class="noindent">The current <em>process state</em>, which is a value that determines its eligibility for execution on the CPU.</li>&#13;
</ul>&#13;
<p class="indent">Over the course of its lifetime, a process moves through several states, which correspond to different categories of process execution eligibility. One way that the OS uses process state is to identify the set of processes that are candidates for being scheduled on the CPU.</p>&#13;
<p class="indent">The set of process execution states are:</p>&#13;
<ul>&#13;
<li class="noindent"><em>Ready</em>: The process could run on the CPU but is not currently scheduled (it is a candidate for being context switched on to the CPU). Once a new process is created and initialized by the OS, it enters the ready state (it is ready for the CPU to start executing its first instruction). In a timesharing system, if a process is context switched off the CPU because its time slice is up, it is also placed in the <em>ready</em> state (it is ready for the CPU to execute its next instruction, but it used up its time slice and has to wait its turn to get scheduled again on the CPU).</li>&#13;
<li class="noindent"><span epub:type="pagebreak" id="page_628"/><em>Running</em>: The process is scheduled on the CPU and is actively executing instructions.</li>&#13;
<li class="noindent"><em>Blocked</em>: The process is waiting for some event before it can continue being executed. For example, the process is waiting for some data to be read in from disk. Blocked processes are not candidates for being scheduled on the CPU. After the event on which the process is blocked occurs, the process moves to the <em>ready</em> state (it is ready to run again).</li>&#13;
<li class="noindent"><em>Exited</em>: The process has exited but still needs to be completely removed from the system. A process exits due to its completing the execution of its program instructions, or by exiting with an error (e.g., it tries to divide by zero), or by receiving a termination request from another process. An exited process will never run again, but it remains in the system until final clean-up associated with its execution state is complete.</li>&#13;
</ul>&#13;
<p class="indent"><a href="ch13.xhtml#ch13fig8">Figure 13-8</a> shows the lifetime of a process in the system, illustrating how it moves between different states. Note the transitions (arrows) from one state to another. For example, a process can enter the Ready state in one of three ways: first, if it is newly created by the OS; second, if it was blocked waiting for some event and the event occurs; and third, if it was running on the CPU and its time slice is over and the OS context switches it off to give another Ready process its turn on the CPU.</p>&#13;
<div class="imagec" id="ch13fig8"><img alt="image" src="../images/13fig08.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-8: The states of a process during its lifetime</em></p>&#13;
<div class="g-box">&#13;
<p class="box-title"><span epub:type="pagebreak" id="page_629"/>PROCESS RUNTIME</p>&#13;
<p class="noindentt">Programmers often use a process’s completion time as a metric to evaluate its performance. For noninteractive programs, a faster runtime typically indicates a better, or more optimal, implementation. For example, in comparing two programs that compute the prime factors of a large number, the one that correctly completes the task faster is preferable.</p>&#13;
<p class="noindentt">There are two different measures of the runtime of a process. The first is total <em>wall time</em> (or wall-clock time). Wall time is the duration between the start and completion of a process; it is the elapsed time from the process’s start to finish as measured by a clock hanging on a wall. Wall time includes the time that the process is in the Running state executing on the CPU, as well as time that the process is in the Blocked state waiting for an event like I/O as well as the time that the process spends in the Ready state waiting for its turn to be scheduled to run on the CPU. In multiprogrammed and timeshared systems, the wall time of a process can slow down due to other processes running concurrently on the system and sharing system resources.</p>&#13;
<p class="noindentt">The second measure of process runtime is total <em>CPU time</em> (or process time). CPU time measures just the amount of time the process spends in the Running state executing its instructions on the CPU. CPU time does not include the time the process spends in the Blocked or Ready states. As a result, a process’s total CPU time is not affected by other processes concurrently running on the system.</p>&#13;
</div>&#13;
<h4 class="h4" id="lev2_223">13.2.3 Creating (and Destroying) Processes</h4>&#13;
<p class="noindent">An OS creates a new process when an existing process makes a system call requesting it to do so. In Unix, the <span class="literal">fork</span> system call creates a new process. The process calling <span class="literal">fork</span> is the <em>parent</em> process and the new process it creates is its <em>child</em> process. For example, if you run <span class="literal">a.out</span> in a shell, the shell process calls the <span class="literal">fork</span> system call to request that the OS create a new child process that will be used to run the <span class="literal">a.out</span> program. Another example is a web browser process that calls <span class="literal">fork</span> to create child processes to handle different browsing events. A web browser may create a child process to handle communication with a web server when a user loads a web page. It may create another process to handle user mouse input, and other processes to handle separate browser windows or tabs. A multiple-process web browser like this is able to continue handling user requests through some of its child browser processes, while at the same time some of its other child browser processes may be blocked waiting for remote web server responses or for user mouse clicks.</p>&#13;
<p class="indent">A <em>process hierarchy</em> of parent–child relationships exists between the set of processes active in the system. For example, if process <em>A</em> makes two calls to <span class="literal">fork</span>, two new child processes are created, <em>B</em> and <em>C</em>. If process C then calls <span class="literal">fork</span>, another new process, <em>D</em>, will be created. Process C is the child of A, and the parent of D. Processes B and C are siblings (they share a common parent process, process A). Process A is the ancestor of B, C, and D. This example is illustrated in <a href="ch13.xhtml#ch13fig9">Figure 13-9</a>.</p>&#13;
<div class="imagec" id="ch13fig9"><img alt="image" src="../images/13fig09.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_630"/><em>Figure 13-9: An example process hierarchy created by a parent process (A) calling <span class="literal">fork</span> twice to create two child processes (B and C). C’s call to <span class="literal">fork</span> creates its child process, D. To list the process hierarchy on Linux systems, run <span class="literal">pstree</span>, or <span class="literal">ps -Aef --forest</span>.</em></p>&#13;
<p class="indent">Since existing processes trigger process creation, a system needs at least one process to create any new processes. At boot time, the OS creates the first user-level process in the system. This special process, named <span class="literal">init</span>, sits at the very top of the process hierarchy as the ancestor of all other processes in the system.</p>&#13;
<h4 class="h4" id="lev2_224">fork</h4>&#13;
<p class="noindent">The <span class="literal">fork</span> system call is used to create a process. At the time of the fork, the child inherits its execution state from its parent. The OS creates a <em>copy</em> of the calling (parent) process’s execution state at the point when the parent calls <span class="literal">fork</span>. This execution state includes the parent’s address space contents, CPU register values, and any system resources it has allocated (e.g., open files). The OS also creates a new <em>process control struct</em>, an OS data structure for managing the child process, and it assigns the child process a unique PID. After the OS creates and initializes the new process, the child and parent are concurrent—they both continue running and their executions overlap as the OS context switches them on and off the CPU.</p>&#13;
<p class="indent">When the child process is first scheduled by the OS to run on the CPU, it starts executing at the point at which its parent left off—at the return from the <span class="literal">fork</span> call. This is because <span class="literal">fork</span> gives the child a copy of its parent’s execution state (the child executes using its own copy of this state when it starts running). From the programmer’s point of view, <em>a call to</em> <span class="codeitalic">fork</span> <em>returns twice</em>: once in the context of the running parent process, and once in the context of the running child process.</p>&#13;
<p class="indent">In order to differentiate the child and parent processes in a program, a call to <span class="literal">fork</span> returns different values to the parent and child. The child process always receives a return value of 0, whereas the parent receives the child’s PID value (or –1 if <span class="literal">fork</span> fails).</p>&#13;
<p class="indent">For example, the following code snippet shows a call to the <span class="literal">fork</span> system call that creates a new child process of the calling process:</p>&#13;
<p class="programs"><span epub:type="pagebreak" id="page_631"/>&#13;
pid_t pid;<br/>&#13;
<br/>&#13;
pid = fork();   /* create a new child process */<br/>&#13;
<br/>&#13;
print("pid = %d\n", pid);  /* both parent and child execute this */<br/>&#13;
</p>&#13;
<p class="indent">After the call to <span class="literal">fork</span> creates a new child process, the parent and child processes both continue executing, in their separate execution contexts, at the return point of the <span class="literal">fork</span> call. Both processes assign the return value of <span class="literal">fork</span> to their <span class="literal">pid</span> variable and both call <span class="literal">printf</span>. The child process’s call prints out 0 and the parent process prints out the child’s PID value.</p>&#13;
<p class="indent"><a href="ch13.xhtml#ch13fig10">Figure 13-10</a> shows an example of what the process hierarchy looks like after this code’s execution. The child process gets an exact copy of the parent process’s execution context at the point of the fork, but the value stored in its variable <span class="literal">pid</span> differs from its parent because <span class="literal">fork</span> returns the child’s PID value (14 in this example) to the parent process, and 0 to the child.</p>&#13;
<div class="imagec" id="ch13fig10"><img alt="image" src="../images/13fig10.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-10: A process (PID 12) calls <span class="literal">fork</span> to create a new child process. The new child process gets an exact copy of its parent’s address and execution state, but gets its own process identifier (PID 14). <span class="literal">fork</span> returns 0 to the child process and the child’s PID value (14) to the parent.</em></p>&#13;
<p class="indent">Often, the programmer wants the child and parent processes to perform different tasks after the <span class="literal">fork</span> call. A programmer can use the different return values from <span class="literal">fork</span> to trigger the parent and child processes to execute different code branches. For example, the following code snippet creates a new child process and uses the return value from <span class="literal">fork</span> to have the child and parent processes execute different code branches after the call:</p>&#13;
<p class="programs">&#13;
pid_t pid;<br/>&#13;
<br/>&#13;
pid = fork();   /* create a new child process */<br/>&#13;
<br/>&#13;
if (pid == 0) {<br/>&#13;
    /* only the child process executes this code */<br/>&#13;
    ...<br/>&#13;
<span epub:type="pagebreak" id="page_632"/>} else if (pid != -1)  {<br/>&#13;
    /* only the parent process executes this code */<br/>&#13;
    ...<br/>&#13;
}<br/>&#13;
</p>&#13;
<p class="indent">It is important to remember that as soon as they’re created, the child and parent processes run concurrently in their own execution contexts, modifying their separate copies of program variables and possibly executing different branches in the code.</p>&#13;
<p class="indent">Consider the following program<sup><a href="ch13.xhtml#fn13_2" id="rfn13_2">2</a></sup> that contains a call to <span class="literal">fork</span> with branching on the value of <span class="literal">pid</span> to trigger the parent and child processes to execute different code (this example also shows a call to <span class="literal">getpid</span> that returns the PID of the calling process):</p>&#13;
<p class="programs">&#13;
#include &lt;stdio.h&gt;<br/>&#13;
#include &lt;stdlib.h&gt;<br/>&#13;
#include &lt;unistd.h&gt;<br/>&#13;
<br/>&#13;
int main() {<br/>&#13;
<br/>&#13;
    pid_t pid, mypid;<br/>&#13;
<br/>&#13;
    printf("A\n");<br/>&#13;
<br/>&#13;
    pid = fork();   /* create a new child process */<br/>&#13;
<br/>&#13;
    if(pid == -1) {  /* check and handle error return value */<br/>&#13;
        printf("fork failed!\n");<br/>&#13;
        exit(pid);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    if (pid == 0) { /* the child process */<br/>&#13;
        mypid = getpid();<br/>&#13;
        printf("Child: fork returned %d, my pid %d\n", pid, mypid);<br/>&#13;
<br/>&#13;
    } else  {  /* the parent process */<br/>&#13;
        mypid = getpid();<br/>&#13;
        printf("Parent: fork returned %d, my pid %d\n", pid, mypid);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    printf("B:%d\n", mypid);<br/>&#13;
<br/>&#13;
    return 0;<br/>&#13;
}<br/>&#13;
</p>&#13;
<p class="indent">When run, this program’s output might look like the following (assume that the parent’s PID is 12 and the child’s is 14):</p>&#13;
<span epub:type="pagebreak" id="page_633"/>&#13;
<p class="programs">A<br/>&#13;
Parent: fork returned 14, my pid 12<br/>&#13;
B:12<br/>&#13;
Child: fork returned 0, my pid 14<br/>&#13;
B:14</p>&#13;
<p class="indent">In fact, the program’s output could look like any of the possible options shown in <a href="ch13.xhtml#ch13tab1">Table 13-1</a> (and you will often see more than one possible ordering of output if you run the program multiple times). In <a href="ch13.xhtml#ch13tab1">Table 13-1</a>, the parent prints B:12 and the child B:14 in this example, but the exact PID values will vary from run to run.</p>&#13;
<p class="tabcap" id="ch13tab1"><strong>Table 13-1:</strong> All Six Possible Orderings of Example Program Output</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:15%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab"><strong>Option 1</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Option 2</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Option 3</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Option 4</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Option 5</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Option 6</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">A</p></td>&#13;
<td style="vertical-align: top"><p class="tab">A</p></td>&#13;
<td style="vertical-align: top"><p class="tab">A</p></td>&#13;
<td style="vertical-align: top"><p class="tab">A</p></td>&#13;
<td style="vertical-align: top"><p class="tab">A</p></td>&#13;
<td style="vertical-align: top"><p class="tab">A</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Parent...</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Parent...</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Parent...</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Child...</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Child...</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Child...</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Child...</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Child...</p></td>&#13;
<td style="vertical-align: top"><p class="tab">B:12</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Parent...</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Parent...</p></td>&#13;
<td style="vertical-align: top"><p class="tab">B:14</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">B:12</p></td>&#13;
<td style="vertical-align: top"><p class="tab">B:14</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Child...</p></td>&#13;
<td style="vertical-align: top"><p class="tab">B:12</p></td>&#13;
<td style="vertical-align: top"><p class="tab">B:14</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Parent...</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">B:14</p></td>&#13;
<td style="vertical-align: top"><p class="tab">B:12</p></td>&#13;
<td style="vertical-align: top"><p class="tab">B:14</p></td>&#13;
<td style="vertical-align: top"><p class="tab">B:14</p></td>&#13;
<td style="vertical-align: top"><p class="tab">B:12</p></td>&#13;
<td style="vertical-align: top"><p class="tab">B:12</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">These six different output orderings are possible because after the <span class="literal">fork</span> system call returns, the parent and child processes are concurrent and can be scheduled to run on the CPU in many different orderings, resulting in any possible interleaving of their instruction sequences. Consider the execution time line of this program, shown in <a href="ch13.xhtml#ch13fig11">Figure 13-11</a>. The dotted line represents concurrent execution of the two processes. Depending on when each is scheduled to run on the CPU, one could execute both its <span class="literal">printf</span> statements before the other, or the execution of their two <span class="literal">printf</span> statements could be interleaved, resulting in any of the possible outcomes shown in <a href="ch13.xhtml#ch13tab1">Table 13-1</a>. Because only one process, the parent, exists before the call to <span class="literal">fork</span>, A is always printed by the parent before any of the output after the call to <span class="literal">fork</span>.</p>&#13;
<div class="imagec" id="ch13fig11"><img alt="image" src="../images/13fig11.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-11: The execution time line of the program. Only the parent process exists before the call to <span class="literal">fork</span>. After <span class="literal">fork</span> returns, both run concurrently (shown in the dotted lines).</em></p>&#13;
<h4 class="h4" id="lev2_225">13.2.4 exec</h4>&#13;
<p class="noindent">Usually a new process is created to execute a program that is different from that of its parent process. This means that <span class="literal">fork</span> is often called to create a <span epub:type="pagebreak" id="page_634"/>process with the intention of running a new program from its starting point (i.e., starting its execution from its first instruction). For example, if a user types <span class="literal">./a.out</span> in a shell, the shell process forks a new child process to run <span class="literal">a.out</span>. As two separate processes, the shell and the <span class="literal">a.out</span> process are protected from each other; they cannot interfere with each other’s execution state.</p>&#13;
<p class="indent">While <span class="literal">fork</span> creates the new child process, it does not cause the child to run <span class="literal">a.out</span>. To initialize the child process to run a new program, the child process calls one of the <em>exec</em> system calls. Unix provides a family of exec system calls that trigger the OS to overlay the calling process’s image with a new image from a binary executable file. In other words, an exec system call tells the OS to overwrite the calling process’s address space contents with the specified <span class="literal">a.out</span> and to reinitialize its execution state to start executing the very first instruction in the <span class="literal">a.out</span> program.</p>&#13;
<p class="indent">One example of an exec system call is <span class="literal">execvp</span>, whose function prototype is as follows:</p>&#13;
<p class="programs">int execvp(char *filename, char *argv[]);</p>&#13;
<p class="indent">The <span class="literal">filename</span> parameter specifies the name of a binary executable program to initialize the process’s image, and <span class="literal">argv</span> contains the command line arguments to pass into the <span class="literal">main</span> function of the program when it starts executing.</p>&#13;
<p class="indent">Here’s an example code snippet that, when executed, creates a new child process to run <span class="literal">a.out</span>:</p>&#13;
<p class="programs">&#13;
pid_t pid;<br/>&#13;
int ret;<br/>&#13;
char *argv[2];<br/>&#13;
<br/>&#13;
argv[0] = "a.out";  // initialize command line arguments for main<br/>&#13;
argv[1] = NULL;<br/>&#13;
<br/>&#13;
pid = fork();<br/>&#13;
if (pid == 0) { /* child process */<br/>&#13;
    ret = execvp("a.out", argv);<br/>&#13;
    if (ret &lt; 0) {<br/>&#13;
        printf("Error: execvp returned!!!\n");<br/>&#13;
        exit(ret);<br/>&#13;
    }<br/>&#13;
}<br/>&#13;
</p>&#13;
<p class="indent">The <span class="literal">argv</span> variable is initialized to the value of the <span class="literal">argv</span> argument that is passed to the <span class="literal">main</span> function of <span class="literal">a.out</span>:</p>&#13;
<p class="programs">int main(int argc, char *argv) { ...</p>&#13;
<p class="indent"><span class="literal">execvp</span> will figure out the value to pass to <span class="literal">argc</span> based on this <span class="literal">argv</span> value (in this case, 1).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_635"/><a href="ch13.xhtml#ch13fig12">Figure 13-12</a> shows what the process hierarchy would look like after executing this code.</p>&#13;
<div class="imagec" id="ch13fig12"><img alt="image" src="../images/13fig12.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-12: When the child process calls <span class="literal">execvp</span> (left), the OS replaces its image with <span class="literal">a.out</span> (right) and initializes the child process to start running the <span class="literal">a.out</span> program from its beginning.</em></p>&#13;
<p class="indent">Something to note in the previous example code is its seemingly odd error message after the call to <span class="literal">execvp</span>: why would returning from an exec system call be an error? If the exec system call is successful, then the error detection and handling code immediately following it will never be executed because the process will now be executing code in the <span class="literal">a.out</span> program instead of this code (the process’s address space contents have been changed by exec). That is, when a call to an exec function is successful, the process doesn’t continue its execution at the return of the exec call. Because of this behavior, the following code snippet is equivalent to the previous one (however, that code is typically easier to understand):</p>&#13;
<p class="programs">&#13;
pid_t pid;<br/>&#13;
int ret;<br/>&#13;
<br/>&#13;
pid = fork();<br/>&#13;
if (pid == 0) { /* child process */<br/>&#13;
    ret = execvp("a.out", argv);<br/>&#13;
    printf("Error: execvp returned!!!\n");  /* only executed if execvp fails */<br/>&#13;
    exit(ret);<br/>&#13;
}<br/>&#13;
</p>&#13;
<h4 class="h4" id="lev2_226">13.2.5 exit and wait</h4>&#13;
<p class="noindent">To terminate, a process calls the <span class="literal">exit</span> system call, which triggers the OS to clean up most of the process’s state. After running the exit code, a process notifies its parent process that it has exited. The parent is responsible for cleaning up the exited child’s remaining state from the system.</p>&#13;
<p class="indent">Processes can be triggered to exit in several ways. First, a process may complete all of its application code. Returning from its <span class="literal">main</span> function leads to a process invoking the <span class="literal">exit</span> system call. Second, a process can perform an <span epub:type="pagebreak" id="page_636"/>invalid action, such as dividing by zero or dereferencing a null pointer, that results in its exiting. Finally, a process can receive a <em>signal</em> from the OS or another process, telling it to exit (in fact, dividing by zero and null pointer dereferences result in the OS sending the process <span class="literal">SIGFPE</span> and <span class="literal">SIGSEGV</span> signals telling it to exit).</p>&#13;
<div class="g-box">&#13;
<p class="box-title">SIGNALS</p>&#13;
<p class="noindentt">A <em>signal</em> is a software interrupt that the OS delivers to a process. Signals are a method by which related processes can communicate with one another. The OS provides an interface for one process to send a signal to another, and for it to communicate with processes (to send a process a <span class="literal">SIGSEGV</span> signal when it dereferences a null pointer, for example).</p>&#13;
<p class="noindentt">When a process receives a signal, it is interrupted to run special signal handler code. A system defines a fixed number of signals to communicate various meanings, each differentiated by a unique signal number. The OS implements default signal handlers for each signal type, but programmers can register their own user-level signal handler code to override the default actions of most signals for their application.</p>&#13;
<p class="noindentt">“Signals” on <a href="ch13.xhtml#lev2_231">page 657</a> contains more information about signals and signal handling.</p>&#13;
</div>&#13;
<p class="indent">If a shell process wants to terminate its child process running <span class="literal">a.out</span>, it can send the child a <span class="literal">SIGKILL</span> signal. When the child process receives the signal, it runs signal handler code for <span class="literal">SIGKILL</span> that calls <span class="literal">exit</span>, terminating the child process. If a user types CTRL-C in a Unix shell that is currently running a program, the child process receives a <span class="literal">SIGINT</span> signal. The default signal handler for <span class="literal">SIGINT</span> also calls <span class="literal">exit</span>, resulting in the child process exiting.</p>&#13;
<p class="indent">After executing the <span class="literal">exit</span> system call, the OS delivers a <span class="literal">SIGCHLD</span> signal to the process’s parent process to notify it that its child has exited. The child becomes a <em>zombie</em> process; it moves to the Exited state and can no longer run on the CPU. The execution state of a zombie process is partially cleaned up, but the OS still maintains a little information about it, including about how it terminated.</p>&#13;
<p class="indent">A parent process <em>reaps</em> its zombie child (cleans up the rest of its state from the system) by calling the <span class="literal">wait</span> system call. If the parent process calls <span class="literal">wait</span> before its child process exits, then the parent process blocks until it receives a <span class="literal">SIGCHLD</span> signal from the child. The <span class="literal">waitpid</span> system call is a version of <span class="literal">wait</span> that takes a PID argument, allowing a parent to block while waiting for the termination of a specific child process.</p>&#13;
<p class="indent"><a href="ch13.xhtml#ch13fig13">Figure 13-13</a> shows the sequence of events that occur when a process exits.</p>&#13;
<div class="imagec" id="ch13fig13"><span epub:type="pagebreak" id="page_637"/><img alt="image" src="../images/13fig13.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-13: Process exit. Left: The child process calls the <span class="literal">exit</span> system call to clean up most of its execution state. Middle: After running <span class="literal">exit</span>, the child process becomes a zombie (it is in the Exited state and cannot run again), and its parent process is sent a <span class="literal">SIGCHLD</span> signal, notifying it that its child is exited. Right: The parent calls <span class="literal">waitpid</span> to reap its zombie child (cleans up the rest of the child’s state from the system).</em></p>&#13;
<p class="indent">Because the parent and child processes execute concurrently, the parent may call <span class="literal">wait</span> before its child exits, or the child can exit before the parent calls <span class="literal">wait</span>. If the child is still executing when the parent process calls <span class="literal">wait</span>, then the parent blocks until the child exits (the parent enters the Blocked state waiting for the <span class="literal">SIGCHLD</span> signal event to happen). The blocking behavior of the parent can be seen if you run a program (<span class="literal">a.out</span>) in the foreground of a shell—the shell program doesn’t print out a shell prompt until <span class="literal">a.out</span> terminates, indicating that the shell parent process is blocked on a call to <span class="literal">wait</span>, waiting until it receives a <span class="literal">SIGCHLD</span> from its child process running <span class="literal">a.out</span>.</p>&#13;
<p class="indent">A programmer can also design the parent process code so that it will never block waiting for a child process to exit. If the parent implements a <span class="literal">SIGCHLD</span> signal handler that contains the call to <span class="literal">wait</span>, then the parent only calls <span class="literal">wait</span> when there is an exited child process to reap, and thus it doesn’t block on a <span class="literal">wait</span> call. This behavior can be seen by running a program in the background in a shell (<span class="literal">a.out &amp;</span>). The shell program will continue executing, print out a prompt, and execute another command as its child runs <span class="literal">a.out</span>. Here’s an example of how you might see the difference between a parent blocking on <span class="literal">wait</span> vs. a nonblocking parent that only calls <span class="literal">wait</span> inside a <span class="literal">SIGCHLD</span> signal handler (make sure you execute a program that runs for long enough to notice the difference):</p>&#13;
<p class="programs">&#13;
$  <span class="codestrong1">a.out</span>        # shell process forks child and calls wait<br/>&#13;
<br/>&#13;
$  <span class="codestrong1">a.out &amp;</span>      # shell process forks child but does not call wait<br/>&#13;
$  <span class="codestrong1">ps</span>           # (the shell can run ps and a.out concurrently)<br/>&#13;
</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_638"/>Following is an example code snippet containing <span class="literal">fork</span>, <span class="literal">exec</span>, <span class="literal">exit</span>, and <span class="literal">wait</span> system calls (with error handling removed for readability). This example is designed to test your understanding of these system calls and their effects on the execution of the processes. In this example, the parent process creates a child process and waits for it to exit. The child then forks another child to run the <span class="literal">a.out</span> program (the first child is the parent of the second child). It then waits for its child to exit.</p>&#13;
<p class="programs">&#13;
pid_t pid1, pid2, ret;<br/>&#13;
int status;<br/>&#13;
<br/>&#13;
printf("A\n");<br/>&#13;
<br/>&#13;
pid1 = fork();<br/>&#13;
if (pid1 == 0 ) {       /* child 1 */<br/>&#13;
    printf("B\n");<br/>&#13;
<br/>&#13;
    pid2 = fork();<br/>&#13;
    if (pid2 == 0 ){    /* child 2 */<br/>&#13;
        printf("C\n");<br/>&#13;
        execvp("a.out", NULL);<br/>&#13;
    } else {            /* child 1 (parent of child 2) */<br/>&#13;
        ret = wait(&amp;status);<br/>&#13;
        printf("D\n");<br/>&#13;
        exit(0);<br/>&#13;
    }<br/>&#13;
} else {                /* original parent */<br/>&#13;
    printf("E\n");<br/>&#13;
    ret = wait(&amp;status);<br/>&#13;
    printf("F\n");<br/>&#13;
}<br/>&#13;
</p>&#13;
<p class="indent"><a href="ch13.xhtml#ch13fig14">Figure 13-14</a> illustrates the execution time line of process create/running/blocked/exit events from executing the preceding example. The dotted lines represent times when a process’s execution overlaps with its child or descendants: the processes are concurrent and can be scheduled on the CPU in any order. Solid lines represent dependencies on the execution of the processes. For example, Child 1 cannot call <span class="literal">exit</span> until it has reaped its exited child process, Child 2. When a process calls <span class="literal">wait</span>, it blocks until its child exits. When a process calls <span class="literal">exit</span>, it never runs again. The program’s output is annotated along each process’s execution time line at points in its execution when the corresponding <span class="literal">printf</span> statement can occur.</p>&#13;
<div class="imagec" id="ch13fig14"><span epub:type="pagebreak" id="page_639"/><img alt="image" src="../images/13fig14.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-14: The execution time line for the example program, showing a possible sequence of <span class="literal">fork</span>, <span class="literal">exec</span>, <span class="literal">wait</span>, and <span class="literal">exit</span> calls from the three processes. Solid lines represent dependencies in the order of execution between processes, and dotted lines concurrent execution points. Parent is the parent process of Child 1, and Child 1 is the parent of Child 2.</em></p>&#13;
<p class="indent">After the calls to <span class="literal">fork</span> are made in this program, the parent process and first child process run concurrently, thus the call to <span class="literal">wait</span> in the parent could be interleaved with any instruction of its child. For example, the parent process could call <span class="literal">wait</span> and block before its child process calls <span class="literal">fork</span> to create its child process. <a href="ch13.xhtml#ch13tab2">Table 13-2</a> lists all possible outputs from running the example program.</p>&#13;
<p class="tabcap" id="ch13tab2"><strong>Table 13-2:</strong> All Possible Output Orderings from the Program</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab"><strong>Option 1</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Option 2</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Option 3</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Option 4</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">A</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">A</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">A</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">A</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">B</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">B</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">B</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">E</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">C</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">C</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">E</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">B</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">D</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">E</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">C</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">C</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">E</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">D</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">D</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">D</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">F</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">F</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">F</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">F</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">The program outputs in <a href="ch13.xhtml#ch13tab2">Table 13-2</a> are all possible because the parent runs concurrently with its descendant processes until it calls <span class="literal">wait</span>. Thus, the parent’s call to <span class="literal">printf("E\n")</span> can be interleaved at any point between the start and the exit of its descendant processes.</p>&#13;
<h3 class="h3" id="lev1_101">13.3 Virtual Memory</h3>&#13;
<p class="noindent">The OS’s process abstraction provides each process with a virtual memory space. <em>Virtual memory</em> is an abstraction that gives each process its own private, logical address space in which its instructions and data are stored. Each process’s virtual address space can be thought of as an array of addressable <span epub:type="pagebreak" id="page_640"/>bytes from address 0 up to some maximum address. For example, on 32-bit systems the maximum address is 2<sup>32</sup> – 1. Processes cannot access the contents of one another’s address spaces. Some parts of a process’s virtual address space come from the binary executable file it’s running (e.g., the <em>text</em> portion contains program instructions from the <span class="literal">a.out</span> file). Other parts of a process’s virtual address space are created at runtime (e.g., the <em>stack</em>).</p>&#13;
<p class="indent">Operating systems implement virtual memory as part of the <em>lone view</em> abstraction of processes. That is, each process only interacts with memory in terms of its own virtual address space rather than the reality of many processes simultaneously sharing the computer’s physical memory (RAM). The OS also uses its virtual memory implementation to protect processes from accessing one another’s memory spaces. As an example, consider the following simple C program:</p>&#13;
<p class="programs">&#13;
/* a simple program */<br/>&#13;
#include &lt;stdio.h&gt;<br/>&#13;
<br/>&#13;
int main(int argc, char* argv[]) {<br/>&#13;
    int x, y;<br/>&#13;
<br/>&#13;
    printf("enter a value: ");<br/>&#13;
    scanf("%d", &amp;y);<br/>&#13;
<br/>&#13;
    if (y &gt; 10) {<br/>&#13;
        x = y;<br/>&#13;
    } else {<br/>&#13;
        x = 6;<br/>&#13;
    }<br/>&#13;
    printf("x is %d\n", x);<br/>&#13;
<br/>&#13;
    return 0;<br/>&#13;
}<br/>&#13;
</p>&#13;
<p class="indent">If two processes simultaneously execute this program, they each get their own copy of stack memory as part of their separate virtual address spaces. As a result, if one process executes <span class="literal">x = 6</span> it will have no effect on the value of <span class="literal">x</span> in the other process—each process has its own copy of <span class="literal">x</span>, in its private virtual address space, as shown in <a href="ch13.xhtml#ch13fig15">Figure 13-15</a>.</p>&#13;
<div class="imagec" id="ch13fig15"><span epub:type="pagebreak" id="page_641"/><img alt="image" src="../images/13fig15.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-15: Two executions of <span class="literal">a.out</span> results in two processes, each running isolated instances of the <span class="literal">a.out</span> program. Each process has its own private virtual address space, containing its copies of program instructions, global variables, and stack and heap memory space. For example, each may have a local variable <span class="literal">x</span> in the stack portion of their virtual address spaces.</em></p>&#13;
<p class="indent">A process’s virtual address space is divided into several sections, each of which stores a different part of the process’s memory. The top part (at the lowest addresses) is reserved for the OS and can only be accessed in kernel mode. The text and data parts of a process’s virtual address space are initialized from the program executable file (<span class="literal">a.out</span>). The text section contains the program instructions, and the data section contains global variables (the data portion is actually divided into two parts, one for initialized global variables and the other for uninitialized globals).</p>&#13;
<p class="indent">The stack and heap sections of a process’s virtual address space vary in size as the process runs. Stack space grows in response to the process making function calls, and shrinks as it returns from functions. Heap space grows when the process dynamically allocates memory space (via calls to <span class="literal">malloc</span>), and shrinks when the process frees dynamically allocated memory space (via calls to <span class="literal">free</span>). The heap and stack portions of a process’s memory are typically located far apart in its address space to maximize the amount of space either can use. Typically, the stack is located at the bottom of a process’s address space (near the maximum address), and grows upward into lower addresses as stack frames are added to the top of the stack in response to a function call.</p>&#13;
<div class="g-box">&#13;
<p class="box-title"><span epub:type="pagebreak" id="page_642"/>ABOUT HEAP AND STACK MEMORY</p>&#13;
<p class="noindentt">The actual total capacity of heap and stack memory space does not typically change on every call to <span class="literal">malloc</span> and <span class="literal">free</span>, nor on every function call and return. Instead, these actions often only make changes to how much of the currently allocated heap and stack parts of the virtual memory space are actively being used by the process. Sometimes, however, these actions do result in changes to the total capacity of the heap or stack space.</p>&#13;
<p class="noindentt">The operating system is responsible for managing a process’s virtual address space, including changing the total capacity of heap and stack space. The system calls <span class="literal">brk</span>, <span class="literal">sbrk</span>, or <span class="literal">mmap</span> can be used to request that the OS change the total capacity of heap memory. C programmers do not usually invoke these system calls directly. Instead, C programmers call the standard C library function <span class="literal">malloc</span> (and <span class="literal">free</span>) to allocate (and free) heap memory space. Internally, the standard C library’s user-level heap manager may invoke one of these system calls to request that the OS change the size of heap memory space to satisfy a <span class="literal">malloc</span> request.</p>&#13;
</div>&#13;
<h4 class="h4" id="lev2_227">13.3.1 Memory Addresses</h4>&#13;
<p class="noindent">Because processes operate within their own virtual address spaces, operating systems must make an important distinction between two types of memory addresses. <em>Virtual addresses</em> refer to storage locations in a process’s virtual address space, and <em>physical addresses</em> refer to storage locations in physical memory (RAM).</p>&#13;
<h5 class="h5" id="lev3_101">Physical Memory (RAM) and Physical Memory Addresses</h5>&#13;
<p class="noindent">From <a href="ch11.xhtml#ch11">Chapter 11</a>, we know that physical memory (RAM) can be viewed as an array of addressable bytes in which addresses range from 0 to a maximum address value based on the total size of RAM. For example, in a system with 2 gigabytes (GB) of RAM, physical memory addresses range from 0 to 2<sup>31</sup> – 1 (1 GB is 2<sup>30</sup> bytes, so 2 GB is 2<sup>31</sup> bytes).</p>&#13;
<p class="indent">In order for the CPU to run a program, the program’s instructions and data must be loaded into RAM by the OS; the CPU cannot directly access other storage devices (e.g., disks). The OS manages RAM and determines which locations in RAM should store the virtual address space contents of a process. For example, if two processes, P1 and P2, run the earlier example program, then P1 and P2 have separate copies of the <span class="literal">x</span> variable, each stored at a different location in RAM. That is, P1’s <span class="literal">x</span> and P2’s <span class="literal">x</span> have different physical addresses. If the OS gave P1 and P2 the same physical address for their <span class="literal">x</span> variables, then P1 setting <span class="literal">x</span> to 6 would also modify P2’s value of <span class="literal">x</span>, violating the per-process private virtual address space.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_643"/>At any point in time, the OS stores in RAM the address space contents from many processes as well as OS code that it may map into every process’s virtual address space (OS code is typically loaded starting at address 0x0 of RAM). <a href="ch13.xhtml#ch13fig16">Figure 13-16</a> shows an example of the OS and three processes (P1, P2, and P3) loaded into RAM. Each process gets its own separate physical storage locations for its address space contents (e.g., even if P1 and P2 run the same program, they get separate physical storage locations for their variable <span class="literal">x</span>).</p>&#13;
<div class="imagec" id="ch13fig16"><img alt="image" src="../images/13fig16.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-16: Example RAM contents showing OS loaded at address 0x0, and processes loaded at different physical memory addresses in RAM. If P1 and P2 are running the same <span class="literal">a.out</span>, P1’s physical address for <span class="literal">x</span> is different from P2’s physical address for <span class="literal">x</span>.</em></p>&#13;
<h5 class="h5" id="lev3_102">Virtual Memory and Virtual Addresses</h5>&#13;
<p class="noindent">Virtual memory is the per-process view of its memory space, and <em>virtual addresses</em> are addresses in the process’s view of its memory. If two process run the same binary executable, then they have exactly the same virtual addresses for function code and for global variables in their address spaces (the virtual addresses of dynamically allocated space in heap memory and of local variables on the stack may vary slightly between the two processes due to runtime differences in their two separate executions). In other words, both processes will have the same virtual addresses for the location of their <span class="literal">main</span> function, and the same virtual address for the location of a global variable <span class="literal">x</span> in their address spaces, as shown in <a href="ch13.xhtml#ch13fig17">Figure 13-17</a>.</p>&#13;
<div class="imagec" id="ch13fig17"><span epub:type="pagebreak" id="page_644"/><img alt="image" src="../images/13fig17.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-17: Example virtual memory contents for two processes running the same <span class="literal">a.out</span> file. P1 and P2 have the same virtual address for global variable <span class="literal">x</span>.</em></p>&#13;
<h4 class="h4" id="lev2_228">13.3.2 Virtual Address to Physical Address Translation</h4>&#13;
<p class="noindent">A program’s assembly and machine code instructions refer to virtual addresses. As a result, if two processes execute the same <span class="literal">a.out</span> program, the CPU executes instructions with identical virtual addresses to access corresponding parts of their two separate virtual address spaces. For example, supposing that <span class="literal">x</span> is at virtual address 0x24100, then assembly instructions to set <span class="literal">x</span> to 6 might look like this:</p>&#13;
<p class="programs">&#13;
movl $0x24100, %eax    # load 0x24100 into register eax<br/>&#13;
movl $6, (%eax)        # store 6 at memory address 0x24100<br/>&#13;
</p>&#13;
<p class="indent">At runtime the OS loads each of the processes’ <span class="literal">x</span> variables at different physical memory addresses (at different locations in RAM). This means that whenever the CPU executes a load or store instruction to memory that specify virtual addresses, the virtual address from the CPU must be translated to its corresponding physical address in RAM before reading or writing the bytes from RAM.</p>&#13;
<p class="indent">Because virtual memory is an important and core abstraction implemented by operating systems, processors generally provide some hardware support for virtual memory. An OS can make use of this hardware-level virtual memory support to perform virtual to physical address translations quickly, avoiding having to trap to the OS to handle every address translation. A particular OS chooses how much of the hardware support for paging it uses in its implementation of virtual memory. There is often a trade-off in speed versus flexibility when choosing a hardware-implemented feature versus a software-implemented feature.</p>&#13;
<p class="indent">The <em>memory management unit</em> (MMU) is the part of the computer hardware that implements address translation. Together, the MMU hardware and the OS translate virtual to physical addresses when applications access memory. The particular hardware/software split depends on the specific <span epub:type="pagebreak" id="page_645"/>combination of hardware and OS. At its most complete, MMU hardware performs the full translation: it takes a virtual address from the CPU and translates it to a physical address that is used to address RAM (as shown in <a href="ch13.xhtml#ch13fig18">Figure 13-18</a>). Regardless of the extent of hardware support for virtual memory, there will be some virtual-to-physical translations that the OS has to handle. In our discussion of virtual memory, we assume a more complete</p>&#13;
<p class="indent">MMU that minimizes the amount of OS involvement required for address translation.</p>&#13;
<div class="imagec" id="ch13fig18"><img alt="image" src="../images/13fig18.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-18: The memory management unit (MMU) maps virtual to physical addresses. Virtual addresses are used in instructions executed by the CPU. When the CPU needs to fetch data from physical memory, the virtual address is first translated by the MMU to a physical addresses that is used to address RAM.</em></p>&#13;
<p class="indent">The OS maintains virtual memory mappings for each process to ensure that it can correctly translate virtual to physical addresses for any process that runs on the CPU. During a context switch, the OS updates the MMU hardware to refer to the swapped-on process’s virtual-to-physical memory mappings. The OS protects processes from accessing one another’s memory spaces by swapping the per-process address mapping state on a context switch—swapping the mappings on a context switch ensures that one process’s virtual addresses will not map to physical addresses storing another process’s virtual address space.</p>&#13;
<h4 class="h4" id="lev2_229">13.3.3 Paging</h4>&#13;
<p class="noindent">Although many virtual memory systems have been proposed over the years, paging is now the most widely used implementation of virtual memory. In a <em>paged virtual memory</em> system, the OS divides the virtual address space of each process into fixed-sized chunks called <em>pages</em>. The OS defines the page size for the system. Page sizes of a few kilobytes are commonly used in general-purpose operating systems today—4 KB (4,096 bytes) is the default page size on many systems.</p>&#13;
<p class="indent">Physical memory is similarly divided by the OS into page-sized chunks called <em>frames</em>. Because pages and frames are defined to be the same size, any <span epub:type="pagebreak" id="page_646"/>page of a process’s virtual memory can be stored in any frame of physical RAM.</p>&#13;
<p class="indent">In a paging system, pages and frames are the same size, so any page of virtual memory can be loaded into (stored) in any physical frame of RAM; a process’s pages do not need to be stored in contiguous RAM frames (at a sequence of addresses all next to one another in RAM); and not every page of virtual address space needs to be loaded into RAM for a process to run.</p>&#13;
<p class="indent"><a href="ch13.xhtml#ch13fig19">Figure 13-19</a> shows an example of how pages from a process’s virtual address space may map to frames of physical RAM.</p>&#13;
<div class="imagec" id="ch13fig19"><img alt="image" src="../images/13fig19.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-19: Paged virtual memory. Individual pages of a process’s virtual address space are stored in RAM frames. Any page of virtual address space can be loaded into (stored at) any frame of physical memory. In this example, P1’s virtual page 1000 is stored in physical frame 100, and its <a href="ch09.xhtml#page_500">page 500</a> resides in frame 513. P2’s virtual page 1000 is stored in physical frame 880, and its <a href="ch04.xhtml#page_230">page 230</a> resides in frame 102.</em></p>&#13;
<h5 class="h5" id="lev3_103">Virtual and Physical Addresses in Paged Systems</h5>&#13;
<p class="noindent">Paged virtual memory systems divide the bits of a virtual address into two parts: the high-order bits specify the <em>page number</em> on which the virtual address is stored, and the low-order bits correspond to the <em>byte offset</em> within the page (which byte from the top of the page corresponds to the address).</p>&#13;
<p class="indent">Similarly, paging systems divide physical addresses into two parts: the high-order bits specify the <em>frame number</em> of physical memory, and the low-order bits specify the <em>byte offset</em> within the frame. Because frames and pages are the same size, the byte offset bits in a virtual address are identical to the byte offset bits in its translated physical address. Virtual addresses differ <span epub:type="pagebreak" id="page_647"/>from their translated physical addresses in their high-order bits, which specify the virtual page number and physical frame number.</p>&#13;
<div class="imagec" id="ch13fig20"><img alt="image" src="../images/13fig20.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-20: The address bits in virtual and physical addresses</em></p>&#13;
<p class="indent">For example, consider a (very tiny) system with 16-bit virtual addresses, 14-bit physical addresses, and 8-byte pages. Because the page size is eight bytes, the low-order three bits of physical and virtual addresses define the byte offset into a page or frame—three bits can encode eight distinct byte offset values, 0–7 (2<sup>3</sup> is 8). This leaves the high-order 13 bits of the virtual address for specifying the page number and the high-order 11 bits of the physical address for specifying frame number, as shown in the example in <a href="ch13.xhtml#ch13fig21">Figure 13-21</a>.</p>&#13;
<div class="imagec" id="ch13fig21"><img alt="image" src="../images/13fig21.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-21: Virtual and physical address bit divisions in an example system with 16-bit virtual addresses, 14-bit physical addresses, and a page size of 8 bytes.</em></p>&#13;
<p class="indent">In the example in <a href="ch13.xhtml#ch13fig21">Figure 13-21</a>, virtual address 43357 (in decimal) has a byte offset of 5 (0b101 in binary), the low-order 3 bits of the address, and a page number of 5419 (0b1010100101011), the high-order 13 bits of the address. This means that the virtual address is at byte 5 from the top of page 5419.</p>&#13;
<p class="indent">If this page of virtual memory is loaded into frame 43 (0b00000101011) of physical memory, then its physical address is 349 (0b00000101011101), where the low-order 3 bits (0b101) specify the byte offset, and the high-order 11 bits (0b00000101011) specify the frame number. This means that the physical address is at byte 5 from the top of frame 43 of RAM.</p>&#13;
<h5 class="h5" id="lev3_104"><span epub:type="pagebreak" id="page_648"/>Page Tables for Virtual-to-Physical Page Mapping</h5>&#13;
<p class="noindent">Because every page of a process’s virtual memory space can map to a different frame of RAM, the OS must maintain mappings for every virtual page in the process’s address space. The OS keeps a per-process <em>page table</em> that it uses to store the process’s virtual page number to physical frame number mappings. The page table is a data structure implemented by the OS that is stored in RAM. <a href="ch13.xhtml#ch13fig22">Figure 13-22</a> shows an example of how the OS may store two process’s page tables in RAM. The page table of each process stores the mappings of its virtual pages to their physical frames in RAM such that any pages of virtual memory can be stored in any physical frame of RAM.</p>&#13;
<div class="imagec" id="ch13fig22"><img alt="image" src="../images/13fig22.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-22: Every process has a page table containing its virtual page to physical frame mappings. Page tables, stored in RAM, are used by the system to translate process’s virtual addresses to physical addresses that are used to address locations in RAM. This example shows the separate page tables stored in RAM for processes P1 and P2, each page table with its own virtual page to physical frame mappings.</em></p>&#13;
<p class="indent">For each page of virtual memory, the page table stores one <em>page table entry</em> (PTE) that contains the frame number of physical memory (RAM) storing the virtual page. A PTE may also contain other information about the virtual page, including a <em>valid bit</em> that is used to indicate whether the PTE stores a valid mapping. If a page’s valid bit is zero, then the page of the process’s virtual address space is not currently loaded into physical memory.</p>&#13;
<div class="imagec" id="ch13fig23"><span epub:type="pagebreak" id="page_649"/><img alt="image" src="../images/13fig23.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-23: A page table entry (PTE) stores the frame number (23) of the frame of RAM in which the virtual page is loaded. We list the frame number (23) in decimal, although it is really encoded in binary in the PTE entry (0…010111). A valid bit of 1 indicates that this entry stores a valid mapping.</em></p>&#13;
<h5 class="h5" id="lev3_105">Using a Page Table to Map Virtual Addresses to Physical Addresses</h5>&#13;
<p class="noindent">There are four steps to translating a virtual address to a physical address (shown in <a href="ch13.xhtml#ch13fig24">Figure 13-24</a>). The particular OS/hardware combination determines which of the OS or the hardware performs all or part of each step. We assume a full-featured MMU that performs as much of the address translation as possible in hardware in describing these steps, but on some systems the OS may perform parts of these steps.</p>&#13;
<div class="number">&#13;
<p class="number">1. First, the MMU divides the bits of the virtual address into two parts: for a page size of 2<sup><em>k</em></sup> bytes, the low-order <em>k</em> bits (VA bits <em>k –</em> 1 to 0) encode the byte offset (<em>d</em>) into the page, and the high-order <em>n – k</em> bits (VA bits <em>n –</em> 1 to <em>k</em>) encode the virtual page number (<em>p</em>).</p>&#13;
<p class="number">2. Next, the page number value (<em>p</em>) is used by the MMU as an index into the page table to access the PTE for page <em>p</em>. Most architectures have a <em>page table base register</em> (PTBR) that stores the RAM address of the running process’s page table. The value in the PTBR is combined with the page number value (<em>p</em>) to compute the address of the PTE for page <em>p</em>.</p>&#13;
<p class="number">3. If the valid bit in the PTE is set (is 1), then the frame number in the PTE represents a valid VA to PA mapping. If the valid bit is 0, then a page fault occurs, triggering the OS to handle this address translation (we discuss the OS page fault handling later).</p>&#13;
<p class="number">4. The MMU constructs the physical address using the frame number (<em>f</em>) bits from the PTE entry as the high-order bits, and the page offset (<em>d</em>) bits from the VA as the low-order bits of the physical address.</p>&#13;
</div>&#13;
<div class="imagec" id="ch13fig24"><span epub:type="pagebreak" id="page_650"/><img alt="image" src="../images/13fig24.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-24: A process’s page table is used to perform virtual to physical address translations. The PTBR stores the base address of the currently running process’s page table.</em></p>&#13;
<h5 class="h5" id="lev3_106">An Example: Mapping VA to PA with a Page Table</h5>&#13;
<p class="noindent">Consider an example (tiny) paging system for which the page size is 4 bytes, the virtual addresses are 6 bits (the high-order 4 bits are the page number and the low-order 2 bits are the byte offset), and the physical addresses are 7 bits.</p>&#13;
<p class="indent">Assume that the page table for process P1 in this system looks like <a href="ch13.xhtml#ch13tab3">Table 13-3</a> (values are listed in both decimal and binary).</p>&#13;
<p class="tabcap" id="ch13tab3"><strong>Table 13-3:</strong> Process P1’s Page Table</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:35%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:35%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>Entry</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>Valid</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>Frame #</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0 (0b0000)</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">23 (0b10111)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1 (0b0001)</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">17 (0b10001)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">2 (0b0010)</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">11 (0b01011)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">3 (0b0011)</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">16 (0b10000)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">4 (0b0100)</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">8 (0b01000)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">5 (0b0101)</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">14 (0b01110)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">⋮</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">⋮</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">⋮</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">15 (0b1111)</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">30 (0b11110)</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">Using the information provided in this example suggests several important things about address sizes, parts of addresses, and address translation.</p>&#13;
<p class="indent">First, the size of (number of entries in) the page table is determined by the number of bits in the virtual address and the page size in the system. The high-order 4 bits of each 6-bit virtual address specifies the page number, so there are 16 (2<sup>4</sup>) total pages of virtual memory. Since the page table has one entry for each virtual page, there are a total of 16 page table entries in each process’s page table.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_651"/>Second, the size of each page table entry (PTE) depends on the number of bits in the physical address and the page size in the system. Each PTE stores a valid bit and a physical frame number. The valid bit requires a single bit. The frame number requires 5 bits because physical addresses are 7 bits and the page offset is the low-order 2 bits (to address the 4 bytes on each page), which leaves the 5 high-order bits for the frame number. Thus, each PTE entry requires 6 bits: 1 for the valid bit, and 5 for the frame number.</p>&#13;
<p class="indent">Third, the maximum sizes of virtual and physical memory are determined by the number of bits in the addresses. Because virtual addresses are 6 bits, 2<sup>6</sup> bytes of memory can be addressed, so each process’s virtual address space is 2<sup>6</sup> (or 64) bytes. Similarly, the maximum size of physical memory is 2<sup>7</sup> (or 128) bytes.</p>&#13;
<p class="indent">Finally, the page size, the number of bits in virtual and physical addresses, and the page table determine the mapping of virtual to physical addresses. For example, if process P1 executes an instruction to load a value from its virtual address 0b001110, its page table is used to convert the virtual address to physical address 0b1000010, which is then used to access the value in RAM.</p>&#13;
<p class="indent">The virtual address (VA) to physical address (PA) translation steps are:</p>&#13;
<div class="number">&#13;
<p class="number">1. Separate the VA bits into the page number (<em>p</em>) and byte offset (<em>d</em>) bits: the high-order four bits are the page number (0b0011 or <a href="preface.xhtml#page_3">page 3</a>) and the lower-order two bits are the byte offset into the page (0b10 or byte 2).</p>&#13;
<p class="number">2. Use the page number (3) as an index into the page table to read the PTE for virtual <a href="preface.xhtml#page_3">page 3</a> (PT[3]: valid:1 frame#:16).</p>&#13;
<p class="number">3. Check the valid bit for a valid PTE mapping. In this case, the valid bit is 1, so the PTE contains a valid mapping, meaning that virtual memory <a href="preface.xhtml#page_3">page 3</a> is stored in physical memory frame 16.</p>&#13;
<p class="number">4. Construct the physical address using the five-bit frame number from the PTE as the high-order address bits (0b10000), and the low-order two-bit offset from the virtual address (0b10) as the lower-order two bits: the physical address is 0b1000010 (in RAM frame 16 at byte offset 2).</p>&#13;
</div>&#13;
<h5 class="h5" id="lev3_107">Paging Implementation</h5>&#13;
<p class="noindent">Most computer hardware provides some support for paged virtual memory, and together the OS and hardware implement paging on a given system. At a minimum, most architectures provide a page table base register (PTBR) that stores the base address of the currently running process’s page table. To perform virtual-to-physical address translations, the virtual page number part of a virtual address is combined with the value stored in the PTBR to find the PTE entry for the virtual page. In other words, the virtual page number is an index into the process’s page table, and its value combined with the PTBR value gives the RAM address of the PTE for page <em>p</em> (e.g., PTBR + <em>p</em> × (PTE size) is the RAM address of the PTE for page <em>p</em>). Some architectures may support the full page table lookup by manipulating PTE <span epub:type="pagebreak" id="page_652"/>bits in hardware. If not, then the OS needs to be interrupted to handle some parts of page table lookup and accessing the PTE bits to translate virtual addresses to physical addresses.</p>&#13;
<p class="indent">On a context switch, the OS <em>saves and restores</em> the PTBR values of processes to ensure that when a process runs on the CPU it accesses its own virtual-to-physical address mappings from its own page table in RAM. This is one mechanism through which the OS protects processes’ virtual address spaces from one another; changing the PTBR value on context switch ensures that a process cannot access the VA–PA mappings of another process, and thus it cannot read or write values at physical addresses that store the virtual address space contents of any other processes.</p>&#13;
<h5 class="h5" id="lev3_108">An Example: Virtual to Physical Address Mappings of Two Processes</h5>&#13;
<p class="noindent">As an example, consider an example system (<a href="ch13.xhtml#ch13tab4">Table 13-4</a>) with eight-byte pages, seven-bit virtual addresses, and six-bit physical addresses.</p>&#13;
<p class="tabcap" id="ch13tab4"><strong>Table 13-4:</strong> Example Process Page Tables</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:16%"/>&#13;
<col style="width:16%"/>&#13;
<col style="width:16%"/>&#13;
<col style="width:16%"/>&#13;
<col style="width:16%"/>&#13;
<col style="width:20%"/>&#13;
</colgroup>&#13;
<tr>&#13;
<td colspan="3" style="vertical-align: top"><p class="tab-c"><strong>P1’s Page Table</strong></p></td>&#13;
<td colspan="3" style="vertical-align: top"><p class="tab-c"><strong>P2’s Page Table</strong></p></td>&#13;
</tr>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>Entry</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>Valid</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>Frame #</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>Entry</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>Valid</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>Frame #</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">4</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">6</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">5</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c"/></td>&#13;
<td style="vertical-align: top"><p class="tab-c">⋮</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"/></td>&#13;
<td style="vertical-align: top"><p class="tab-c"/></td>&#13;
<td style="vertical-align: top"><p class="tab-c">⋮</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"/></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c">11</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">7</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">11</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">3</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab-c"/></td>&#13;
<td style="vertical-align: top"><p class="tab-c">⋮</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"/></td>&#13;
<td style="vertical-align: top"><p class="tab-c"/></td>&#13;
<td style="vertical-align: top"><p class="tab-c">⋮</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"/></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">Given the current state of the (partially shown) page tables of two processes (P1 and P2) in <a href="ch13.xhtml#ch13tab4">Table 13-4</a>, let’s compute the physical addresses for the following sequence of virtual memory addresses generated from the CPU (each address is prefixed by the process that is running on the CPU):</p>&#13;
<p class="programs">&#13;
P1: 0000100<br/>&#13;
P1: 0000000<br/>&#13;
P1: 0010000<br/>&#13;
              &lt;---- context switch<br/>&#13;
P2: 0010000<br/>&#13;
P2: 0001010<br/>&#13;
P2: 1011001<br/>&#13;
              &lt;---- context switch<br/>&#13;
P1: 1011001<br/>&#13;
</p>&#13;
<p class="indent">First, determine the division of bits in virtual and physical addresses. Since the page size is eight bytes, the three <span epub:type="pagebreak" id="page_653"/>low-order bits of every address encodes the page offset (<em>d</em>). Virtual addresses are seven bits. Thus, with three bits for the page offset, this leaves the four high-order bits for specifying the page number (<em>p</em>). Since physical addresses are six bits long and the low-order three are for the page offset, the high-order three bits specify the frame number.</p>&#13;
<p class="indent">Next, for each virtual address, use its page number bits (<em>p</em>) to look up in the process’s page table the PTE for page <em>p</em>. If the valid bit (<em>v</em>) in the PTE is set, then use the frame number (<em>f</em>) for the high-order bits of the PA. The low-order bits of the PA come from the byte-offset bits (<em>d</em>) of the VA.</p>&#13;
<p class="indent">The results are shown in <a href="ch13.xhtml#ch13tab5">Table 13-5</a> (note which page table is being used for the translation of each address).</p>&#13;
<p class="tabcap" id="ch13tab5"><strong>Table 13-5:</strong> Address Mappings for the Example Sequence of Memory Accesses from Processes P1 and P2</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:10%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:15%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab"><strong>Process</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Virtual address</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><em>p</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><em>d</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>PTE</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><em>f</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><em>d</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Physical address</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">P1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0000100</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">100</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">PT[0]: 1(<em>v</em>), 3(<em>f</em>)</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">011</p></td>&#13;
<td style="vertical-align: top"><p class="tab">100</p></td>&#13;
<td style="vertical-align: top"><p class="tab">011100</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">P1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0000000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">000</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">PT[0]: 1(<em>v</em>), 3(<em>f</em>)</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">011</p></td>&#13;
<td style="vertical-align: top"><p class="tab">000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">011000</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">P1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0010000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0010</p></td>&#13;
<td style="vertical-align: top"><p class="tab">000</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">PT[2]: 1(<em>v</em>), 6(<em>f</em>)</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">110</p></td>&#13;
<td style="vertical-align: top"><p class="tab">000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">110000</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td colspan="8" style="vertical-align: top"><p class="tab-c"><strong>Context switch P1 to P2</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">P2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0010000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0010</p></td>&#13;
<td style="vertical-align: top"><p class="tab">000</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">PT[2]: 1(<em>v</em>), 5(<em>f</em>)</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">101</p></td>&#13;
<td style="vertical-align: top"><p class="tab">000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">101000</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">P2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0001010</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0001</p></td>&#13;
<td style="vertical-align: top"><p class="tab">010</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">PT[1]: 1(<em>v</em>), 4(<em>f</em>)</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">100</p></td>&#13;
<td style="vertical-align: top"><p class="tab">010</p></td>&#13;
<td style="vertical-align: top"><p class="tab">100010</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">P2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1011001</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1011</p></td>&#13;
<td style="vertical-align: top"><p class="tab">001</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">PT[11]: 0(<em>v</em>), 3(<em>f</em>)</span></p></td>&#13;
<td colspan="3" style="vertical-align: top"><p class="tab-c">Page fault (valid bit 0)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td colspan="8" style="vertical-align: top"><p class="tab-c"><strong>Context switch P2 to P1</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">P1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1011001</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1011</p></td>&#13;
<td style="vertical-align: top"><p class="tab">001</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">PT[11]: 1(<em>v</em>), 7(<em>f</em>)</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">111</p></td>&#13;
<td style="vertical-align: top"><p class="tab">001</p></td>&#13;
<td style="vertical-align: top"><p class="tab">111001</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">As one example, consider the first address accesses by process P1. When P1 accesses its virtual address 8 (0b0000100), the address is divided into its page number 0 (0b0000) and its byte offset 4 (0b100). The page number, 0, is used to look up PTE entry 0, whose valid bit is 1, indicating a valid page mapping entry, and whose frame number is 3 (0b011). The physical address (0b011100) is constructed using the frame number (0b011) as the high-order bits and the page offset (0b100) as the low-order bits.</p>&#13;
<p class="indent">When process P2 is context switched on the CPU, its page table mappings are used (note the different physical addresses when P1 and P2 access the same virtual address 0b0010000). When P2 accesses a PTE entry with a 0 valid bit, it triggers a page fault to the OS to handle.</p>&#13;
<h4 class="h4" id="lev2_230">13.3.4 Memory Efficiency</h4>&#13;
<p class="noindent">One of the primary goals of the operating system is to efficiently manage hardware resources. System performance is particularly dependent on how the OS manages the memory hierarchy. For example, if a process accesses data that are stored in RAM, then the process will run much faster than if those data are on disk.</p>&#13;
<p class="indent">The OS strives to increase the degree of multiprogramming in the system in order to keep the CPU busy doing real work while some processes are blocked waiting for an event like disk I/O. However, because RAM is fixed-size storage, the OS must make decisions about which process to load in RAM at any point in time, possibly limiting the degree of multiprogramming in the system. Even systems with a large amount of RAM (10s or 100s <span epub:type="pagebreak" id="page_654"/>of gigabytes) often cannot simultaneously store the full address space of every process in the system. As a result, an OS can make more efficient use of system resources by running processes with only parts of their virtual address spaces loaded in RAM.</p>&#13;
<h5 class="h5" id="lev3_109">Implementing Virtual Memory Using RAM, Disk, and Page Replacement</h5>&#13;
<p class="noindent">From “Locality” on <a href="ch11.xhtml#lev1_88">page 552</a>, we know that memory references usually exhibit a very high degree of locality. In terms of paging, this means that processes tend to access pages of their memory space with a high degree of temporal or spatial locality. It also means that at any point in its execution, a process is not typically accessing large extents of its address space. In fact, processes typically never access large extents of their full address spaces. For example, processes typically do not use the full extent of their stack or heap memory space.</p>&#13;
<p class="indent">One way in which the OS can make efficient use of both RAM and CPU is to treat RAM as a cache for disk. In doing so, the OS allows processes to run in the system only having some of their virtual memory pages loaded into physical frames of RAM. Their other virtual memory pages remain on secondary storage devices such as disk, and the OS only brings them into RAM when the process accesses addresses on these pages. This is another part of the OS’s <em>virtual memory</em> abstraction—the OS implements a view of a single large physical “memory” that is implemented using RAM storage in combination with disk or other secondary storage devices. Programmers do not need to explicitly manage their program’s memory, nor do they need to handle moving parts in and out of RAM as their program needs it.</p>&#13;
<p class="indent">By treating RAM as a cache for disk, the OS keeps in RAM only those pages from processes’ virtual address spaces that are being accessed or have been accessed recently. As a result, processes tend to have the set of pages that they are accessing stored in fast RAM and the set of pages that they do not access frequently (or at all) stored on slower disk. This leads to more efficient use of RAM because the OS uses RAM to store pages that are actually being used by running processes, and doesn’t waste RAM space to store pages that will not be accessed for a long time or ever. It also results in more efficient use of the CPU by allowing more processes to simultaneously share RAM space to store their active pages, which can result in an increase in the number of ready processes in the system, reducing times when the CPU is idle due to all the processes waiting for some event like disk I/O.</p>&#13;
<p class="indent">In virtual memory systems, however, processes sometimes try to access a page that is currently not stored in RAM (causing a <em>page fault</em>). When a page fault occurs, the OS needs to read the page from disk into RAM before the process can continue executing. The MMU reads a PTE’s valid bit to determine whether it needs to trigger a page fault exception. When it encounters a PTE whose valid bit is zero, it traps to the OS, which takes the following steps:</p>&#13;
<div class="number">&#13;
<p class="number">1. The OS finds a free frame (e.g., frame <em>j</em>) of RAM into which it will load the faulted page.</p>&#13;
<p class="number"><span epub:type="pagebreak" id="page_655"/>2. It next issues a read to the disk to load the page from disk into frame <em>j</em> of RAM.</p>&#13;
<p class="number">3. When the read from disk has completed, the OS updates the PTE entry, setting the frame number to <em>j</em> and the valid bit to 1 (this PTE for the faulted page now has a valid mapping to frame <em>j</em>).</p>&#13;
<p class="number">4. Finally, the OS restarts the process at the instruction that caused the page fault. Now that the page table holds a valid mapping for the page that faulted, the process can access the virtual memory address that maps to an offset in physical frame <em>j</em>.</p>&#13;
</div>&#13;
<p class="indent">To handle a page fault, the OS needs to keep track of which RAM frames are free so that it can find a free frame of RAM into which the page read from disk can be stored. Operating systems often keep a list of free frames that are available for allocating on a page fault. If there are no available free RAM frames, then the OS picks a frame and replaces the page it stores with the faulted page. The PTE of the replaced page is updated, setting its valid bit to 0 (this page’s PTE mapping is no longer valid). The replaced page is written back to disk if its in-RAM contents differ from its on-disk version; if the owning process wrote to the page while it was loaded in RAM, then the RAM version of the page needs to be written to disk before being replaced so that the modifications to the page of virtual memory are not lost. PTEs often include a <em>dirty bit</em> that is used to indicate if the in-RAM copy of the page has been modified (written to). During page replacement, if the dirty bit of the replaced page is set, then the page needs to be written to disk before being replaced with the faulted page. If the dirty bit is 0, then the on-disk copy of the replaced page matches the in-memory copy, and the page does not need to be written to disk when replaced.</p>&#13;
<p class="indent">Our discussion of virtual memory has primarily focused on the <em>mechanism</em> part of implementing paged virtual memory. However, there is an important <em>policy</em> part of paging in the OS’s implementation. The OS needs to run a <em>page replacement policy</em> when free RAM is exhausted in the system. A page replacement policy picks a frame of RAM that is currently being used and replaces its contents with the faulted page; the current page is <em>evicted</em> from RAM to make room for storing the faulted page. The OS needs to implement a good page replacement policy for selecting which frame in RAM will be written back to disk to make room for the faulted page. For example, an OS might implement the <em>least recently used</em> (LRU) policy, which replaces the page stored in the frame of RAM that has been accessed least recently. LRU works well when there is a high degree of locality in memory accesses. There are many other policies that an OS may choose to implement. See an OS textbook for more information about page replacement policies.</p>&#13;
<h5 class="h5" id="lev3_110">Making Page Accesses Faster</h5>&#13;
<p class="noindent">Although paging has many benefits, it also results in a significant slowdown to every memory access. In a paged virtual memory system, every load and store to a virtual memory address requires <em>two</em> RAM accesses: the first reads the page table entry to get the frame number for virtual-to-physical address <span epub:type="pagebreak" id="page_656"/>translation, and the second reads or writes the byte(s) at the physical RAM address. Thus, in a paged virtual memory system, every memory access is twice as slow as in a system that supports direct physical RAM addressing.</p>&#13;
<p class="indent">One way to reduce the additional overhead of paging is to cache page table mappings of virtual page numbers to physical frame numbers. When translating a virtual address, the MMU first checks for the page number in the cache. If found, then the page’s frame number mapping can be grabbed from the cache entry, avoiding one RAM access for reading the PTE.</p>&#13;
<p class="indent">A <em>translation look-aside buffer</em> (TLB) is a hardware cache that stores (page number, frame number) mappings. It is a small, fully associative cache that is optimized for fast lookups in hardware. When the MMU finds a mapping in the TLB (a TLB hit), a page table lookup is not needed, and only one RAM access is required to execute a load or store to a virtual memory address. When a mapping is not found in the TLB (a TLB miss), then an additional RAM access to the page’s PTE is required to first construct the physical address of the load or store to RAM. The mapping associated with a TLB miss is added into the TLB. With good locality of memory references, the hit rate in the TLB is very high, resulting in fast memory accesses in paged virtual memory—most virtual memory accesses require only a single RAM access. <a href="ch13.xhtml#ch13fig25">Figure 13-25</a> shows how the TLB is used in virtual-to-physical address mappings.</p>&#13;
<div class="imagec" id="ch13fig25"><img alt="image" src="../images/13fig25.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-25: The translation look-aside buffer (TLB) is a small hardware cache of virtual page to physical frame mappings. The TLB is first searched for an entry for page <em>p</em>. If found, then no page table lookup is needed to translate the virtual address to its physical address.</em></p>&#13;
<h3 class="h3" id="lev1_102">13.4 Interprocess Communication</h3>&#13;
<p class="noindent">Processes are one of the primary abstractions implemented by the OS. Private virtual address spaces are an important abstraction in multiprogrammed systems and are one way in which the OS prevents processes from interfering with one another’s execution state. However, sometimes a user or programmer <span epub:type="pagebreak" id="page_657"/>may want their application processes to communicate with one another (or to share some of their execution state) as they run.</p>&#13;
<p class="indent">Operating systems typically implement support for several types of interprocess communication, or ways in which processes can communicate or share their execution state. <em>Signals</em> are a very restricted form of interprocess communication by which one process can send a signal to another process to notify it of some event. Processes can also communicate using <em>message passing</em>, in which the OS implements an abstraction of a message communication channel that is used by a process to exchange messages with another process. Finally, the OS may support interprocess communication through <em>shared memory</em> that allows a process to share all or part of its virtual address space with other processes. Processes with shared memory can read or write to addresses in shared space to communicate with one another.</p>&#13;
<h4 class="h4" id="lev2_231">13.4.1 Signals</h4>&#13;
<p class="noindent">A <em>signal</em> is a software interrupt that is sent by one process to another process via the OS. When a process receives a signal, its current execution point is interrupted by the OS to run signal handler code. If the signal handler returns, the process’s execution continues from where it was interrupted to handle the signal. Sometimes the signal handler causes the process to exit, and thus it does not continue its execution from where it left off.</p>&#13;
<p class="indent">Signals are similar to hardware interrupts and traps but are different from both. Whereas a trap is a synchronous software interrupt that occurs when a process explicitly invokes a system call, signals are asynchronous—a process may be interrupted by the receipt of a signal at any point in its execution. Signals also differ from asynchronous hardware interrupts in that they are triggered by software rather than hardware devices.</p>&#13;
<p class="indent">A process can send another process a signal by executing the <span class="literal">kill</span> system call, which requests that the OS post a signal to another process. The OS handles posting the signal to the target process and setting its execution state to run the signal handler code associated with the particular posted signal.</p>&#13;
<p class="note"><strong><span class="black">Note</span></strong></p>&#13;
<p class="note1">The name of the <span class="literal">kill</span> system call is potentially misleading as well as unfortunately violent. Although it can be (and often is) used to deliver a termination signal, it is also used to send any other type of signal to a process.</p>&#13;
<p class="indent">The OS itself also uses signals to notify processes of certain events. For example, the OS posts a <span class="literal">SIGCHLD</span> signal to a process when one of its child processes exits.</p>&#13;
<p class="indent">Systems define a fixed number of signals (e.g., Linux defines 32 different signals). As a result, signals provide a limited way in which processes can communicate with one another, as opposed to other interprocess communication methods such as messaging or shared memory.</p>&#13;
<p class="indent"><a href="ch13.xhtml#ch13tab6">Table 13-6</a> lists some of the defined signals. See the man page (<span class="literal">man 7 signal</span>) for additional examples.</p>&#13;
<p class="tabcap" id="ch13tab6"><span epub:type="pagebreak" id="page_658"/><strong>Table 13-6:</strong> Example Signals Used for Interprocess Communication</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:80%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab"><strong>Signal</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Description</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">SIGSEGV</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Segmentation fault (e.g., dereferencing a null pointer)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">SIGINT</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Interrupt process (e.g., CTRL-C in terminal window to kill process)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">SIGCHLD</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Child process has exited (e.g., a child is now a zombie after running <span class="literal">exit</span>)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">SIGALRM</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Notify a process when a timer goes off (e.g., <span class="literal">alarm(2)</span> every 2 secs)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">SIGKILL</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Terminate a process (e.g., <span class="literal">pkill -9 a.out</span>)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">SIGBUS</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Bus error occurred (e.g., a misaligned memory address to access an <span class="literal">int</span> value)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">SIGSTOP</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Suspend a process, move to Blocked state (e.g., CTRL-Z)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">SIGCONT</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Continue a blocked process (move it to the Ready state; e.g., <span class="literal">bg</span> or <span class="literal">fg</span>)</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">When a process receives a signal, one of several default actions can occur: the process can terminate, the signal can be ignored, the process can be blocked, or the process can be unblocked.</p>&#13;
<p class="indent">The OS defines a default action and supplies the default signal handler code for every signal number. Application programmers, however, can change the default action of most signals and can write their own signal handler code. If an application program doesn’t register its own signal handler function for a particular signal, then the OS’s default handler executes when the process receives a signal. For some signals, the OS-defined default action cannot be overridden by application signal handler code. For example, if a process receives a <span class="literal">SIGKILL</span> signal, the OS will always force the process to exit, and receiving a <span class="literal">SIGSTOP</span> signal will always block the process until it receives a signal to continue (<span class="literal">SIGCONT</span>) or to exit (<span class="literal">SIGKILL</span>).</p>&#13;
<p class="indent">Linux supports two different system calls that can be used to change the default behavior of a signal or to register a signal handler on a particular signal: <span class="literal">sigaction</span> and <span class="literal">signal</span>. Because <span class="literal">sigaction</span> is POSIX compliant and more featureful, it should be used in production software. However, we use <span class="literal">signal</span> in our example code because it is easier to understand.</p>&#13;
<p class="indent">Following is an example program<sup><a href="ch13.xhtml#fn13_3" id="rfn13_3">3</a></sup> that registers signal handlers for <span class="literal">SIGALRM</span>, <span class="literal">SIGINT</span>, and <span class="literal">SIGCONT</span> signals using the <span class="literal">signal</span> system call (error handling is removed for readability):</p>&#13;
<p class="programs">&#13;
/*<br/>&#13;
 * Example of signal handlers for SIGALRM, SIGINT, and SIGCONT<br/>&#13;
 *<br/>&#13;
 * A signal handler function prototype must match:<br/>&#13;
 *   void handler_function_name(int signum);<br/>&#13;
 *<br/>&#13;
 * Compile and run this program, then send this process signals by executing:<br/>&#13;
 *  kill -INT  pid  (or Ctrl-C) will send a SIGINT<br/>&#13;
 *  kill -CONT pid  (or Ctrl-Z fg) will send a SIGCONT<br/>&#13;
 */<br/>&#13;
<span epub:type="pagebreak" id="page_659"/>#include &lt;stdio.h&gt;<br/>&#13;
#include &lt;stdlib.h&gt;<br/>&#13;
#include &lt;unistd.h&gt;<br/>&#13;
#include &lt;signal.h&gt;<br/>&#13;
<br/>&#13;
/* signal handler for SIGALRM */<br/>&#13;
void sigalarm_handler(int sig) {<br/>&#13;
    printf("BEEP, signal number %d\n.", sig);<br/>&#13;
    fflush(stdout);<br/>&#13;
    alarm(5);  /* sends another SIGALRM in 5 seconds */<br/>&#13;
}<br/>&#13;
<br/>&#13;
/* signal handler for SIGCONT */<br/>&#13;
void sigcont_handler(int sig) {<br/>&#13;
    printf("in sigcont handler function, signal number %d\n.", sig);<br/>&#13;
    fflush(stdout);<br/>&#13;
}<br/>&#13;
<br/>&#13;
/* signal handler for SIGINT */<br/>&#13;
void sigint_handler(int sig) {<br/>&#13;
    printf("in sigint handler function, signal number %d...exiting\n.", sig);<br/>&#13;
    fflush(stdout);<br/>&#13;
    exit(0);<br/>&#13;
}<br/>&#13;
<br/>&#13;
/* main: register signal handlers and repeatedly block until receive signal */<br/>&#13;
int main() {<br/>&#13;
<br/>&#13;
    /* Register signal handlers. */<br/>&#13;
    if (signal(SIGCONT, sigcont_handler) == SIG_ERR) {<br/>&#13;
        printf("Error call to signal, SIGCONT\n");<br/>&#13;
        exit(1);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    if (signal(SIGINT, sigint_handler) == SIG_ERR) {<br/>&#13;
        printf("Error call to signal, SIGINT\n");<br/>&#13;
        exit(1);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    if (signal(SIGALRM, sigalarm_handler) == SIG_ERR) {<br/>&#13;
        printf("Error call to signal, SIGALRM\n");<br/>&#13;
        exit(1);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    printf("kill -CONT %d to send SIGCONT\n", getpid());<br/>&#13;
<br/>&#13;
    alarm(5);  /* sends a SIGALRM in 5 seconds */<br/>&#13;
<br/>&#13;
    while(1) {<br/>&#13;
        pause(); /* wait for a signal to happen */<br/>&#13;
    }<br/>&#13;
}<br/>&#13;
</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_660"/>When run, the process receives a <span class="literal">SIGALRM</span> every 5 seconds (due to the call to <span class="literal">alarm</span> in <span class="literal">main</span> and <span class="literal">sigalarm_handler</span>). The <span class="literal">SIGINT</span> and <span class="literal">SIGCONT</span> signals can be triggered by running the <span class="literal">kill</span> or <span class="literal">pkill</span> commands in another shell. For example, if the process’s PID is 1234 and its executable is <span class="literal">a.out</span>, then the following shell command sends the process <span class="literal">SIGINT</span> and <span class="literal">SIGCONT</span> signals, triggering their signal handler functions to run:</p>&#13;
<p class="programs">&#13;
$ <span class="codestrong1">pkill -INT a.out</span><br/>&#13;
$ <span class="codestrong1">kill  -INT 1234</span><br/>&#13;
<br/>&#13;
$ <span class="codestrong1">pkill -CONT a.out</span><br/>&#13;
$ <span class="codestrong1">kill  -CONT 1234</span><br/>&#13;
</p>&#13;
<h5 class="h5" id="lev3_111">Writing a SIGCHLD handler</h5>&#13;
<p class="noindent">Recall that when a process terminates, the OS delivers a <span class="literal">SIGCHLD</span> signal to its parent process. In programs that create child processes, the parent process does not always want to block on a call to <span class="literal">wait</span> until its child processes exit. For example, when a shell program runs a command in the background, it continues to run concurrently with its child process, handling other shell commands in the foreground as the child process runs in the background. A parent process, however, needs to call <span class="literal">wait</span> to reap its zombie child processes after they exit. If not, the zombie processes will never die and will continue to hold on to some system resources. In these cases, the parent process can register a signal handler on <span class="literal">SIGCHLD</span> signals. When the parent receives a <span class="literal">SIGCHLD</span> from an exited child process, its handler code runs and makes calls to <span class="literal">wait</span> to reap its zombie children.</p>&#13;
<p class="indent">Following is a code snippet showing the implementation of a signal handler function for <span class="literal">SIGCHLD</span> signals. This snippet also shows parts of a <span class="literal">main</span> function that register the signal handler function for the <span class="literal">SIGCHLD</span> signal (note that this should be done before any calls to <span class="literal">fork</span>):</p>&#13;
<p class="programs">&#13;
/*<br/>&#13;
 * signal handler for SIGCHLD: reaps zombie children<br/>&#13;
 *  signum: the number of the signal (will be 20 for SIGCHLD)<br/>&#13;
 */<br/>&#13;
void sigchld_handler(int signum) {<br/>&#13;
    int status;<br/>&#13;
    pid_t pid;<br/>&#13;
<br/>&#13;
    /*<br/>&#13;
     * reap any and all exited child processes<br/>&#13;
     * (loop because there could be more than one)<br/>&#13;
     */<br/>&#13;
    while( (pid = waitpid(-1, &amp;status, WNOHANG)) &gt; 0) {<br/>&#13;
        /* uncomment debug print stmt to see what is being handled<br/>&#13;
        printf("signal %d me:%d child: %d\n", signum, getpid(), pid);<br/>&#13;
         */<br/>&#13;
    }<br/>&#13;
}<br/>&#13;
<br/>&#13;
int main() {<br/>&#13;
<br/>&#13;
    /* register SIGCHLD handler: */<br/>&#13;
    if ( signal(SIGCHLD, sigchild_handler) == SIG_ERR) {<br/>&#13;
        printf("ERROR signal failed\n");<br/>&#13;
    exit(1);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    ...<br/>&#13;
<br/>&#13;
    /* create a child process */<br/>&#13;
    pid = fork();<br/>&#13;
    if(pid == 0) {<br/>&#13;
        /* child code...maybe call execvp */<br/>&#13;
        ...<br/>&#13;
    }<br/>&#13;
    /* the parent continues executing concurrently with child */<br/>&#13;
    ...<br/>&#13;
</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_661"/>This example passes –1 as the PID to <span class="literal">waitpid</span>, which means “reap any zombie child process.” It also passes the <span class="literal">WNOHANG</span> flag, which means that the call to <span class="literal">waitpid</span> does not block if there are no zombie child processes to reap. Also note that <span class="literal">waitpid</span> is called inside a <span class="literal">while</span> loop that continues as long as it returns a valid PID value (as long as it reaps a zombie child process). It is important that the signal handler function calls <span class="literal">waitpid</span> in a loop because as it is running, the process could receive additional <span class="literal">SIGCHLD</span> signals from other exited child process. The OS doesn’t keep track of the number of <span class="literal">SIGCHLD</span> signals a process receives, it just notes that the process received a <span class="literal">SIGCHLD</span> and interrupts its execution to run the handler code. As a result, without the loop, the signal handler could miss reaping some zombie children.</p>&#13;
<p class="indent">The signal handler executes whenever the parent receives a <span class="literal">SIGCHLD</span> signal, regardless of whether the parent is blocked on a call to <span class="literal">wait</span> or <span class="literal">waitpid</span>. If the parent is blocked on a call to <span class="literal">wait</span> when it receives a <span class="literal">SIGCHLD</span>, it wakes up and runs the signal handler code to reap one or more of its zombie children. It then continues execution at the point in the program after the call to <span class="literal">wait</span> (it just reaped an exited child process). If, however, the parent is blocked on a call to <span class="literal">waitpid</span> for a specific child, then the parent may or may not <span epub:type="pagebreak" id="page_662"/>continue to block after its signal handler code runs to reap an exited child. The parent process continues execution after its call to <span class="literal">waitpid</span> if the signal handler code reaped the child for which it was waiting. Otherwise, the parent continues to block on the call to <span class="literal">waitpid</span> to wait for the specified child to exit. A call to <span class="literal">waitpid</span> with a PID of a nonexistent child process (perhaps one that was previously reaped in the signal handler loop) does not block the caller.</p>&#13;
<h4 class="h4" id="lev2_232">13.4.2 Message Passing</h4>&#13;
<p class="noindent">One way in which processes with private virtual address spaces can communicate is through <em>message passing</em>—by sending and receiving messages to one another. Message passing allows programs to exchange arbitrary data rather than just a small set of predefined messages like those supported by signals. And operating systems typically implement a few different types of message passing abstractions that processes can use to communicate.</p>&#13;
<p class="indent">The message passing interprocess communication model consists of three parts:</p>&#13;
<div class="number">&#13;
<p class="number">1. Processes allocate some type of message channel from the OS. Example message channel types include <em>pipes</em> for one-way communication, and <em>sockets</em> for two-way communication. There may be additional connection setup steps that processes need to take to configure the message channel.</p>&#13;
<p class="number">2. Processes use the message channel to send and receive messages to one another.</p>&#13;
<p class="number">3. Processes close their end of the message channel when they are done using it.</p>&#13;
</div>&#13;
<p class="indent">A <em>pipe</em> is a one-way communication channel for two processes running on the same machine. One-way means that one end of the pipe is for sending messages (or writing to) only, and the other end of the pipe is for receiving messages (or for reading from) only. Pipes are commonly used in shell commands to send the output from one process to the input of another process.</p>&#13;
<p class="indent">For example, consider the following command entered at a bash shell prompt that creates a pipe between two processes (the <span class="literal">cat</span> process outputs the contents of file <span class="literal">foo.c</span> and the pipe (<span class="literal">|</span>) redirects that output to the input of the <span class="literal">grep</span> command that searches for the string “factorial” in its input):</p>&#13;
<p class="programs">$ <span class="codestrong1">cat foo.c | grep factorial</span></p>&#13;
<p class="indent">To execute this command, the bash shell process calls the <span class="literal">pipe</span> system call to request that the OS creates a pipe communication. The pipe will be used by the shell’s two child processes (<span class="literal">cat</span> and <span class="literal">grep</span>). The shell program sets up the <span class="literal">cat</span> process’s <span class="literal">stdout</span> to write to the write end of the pipe and the <span class="literal">grep</span> process’s <span class="literal">stdin</span> to read from the read end of the pipe, so that when the child processes are created and run, the <span class="literal">cat</span> process’s output will be sent as input to the <span class="literal">grep</span> process (see <a href="ch13.xhtml#ch13fig26">Figure 13-26</a>).</p>&#13;
<div class="imagec" id="ch13fig26"><img alt="image" src="../images/13fig26.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_663"/><em>Figure 13-26: Pipes are unidirectional communication channels for processes on the same system. In this example, the <span class="literal">cat</span> process sends the <span class="literal">grep</span> process information by writing to the write end of the pipe. The <span class="literal">grep</span> process receives this information by reading from the read end of the pipe.</em></p>&#13;
<p class="indent">While pipes transmit data from one process to another in only one direction, other message passing abstractions allow processes to communicate in both directions. A <em>socket</em> is a two-way communication channel, which means that each end of a socket can be used for both sending and receiving messages. Sockets can be used by communicating processes running on the same computer or running on different computers connected by a network (see <a href="ch13.xhtml#ch13fig27">Figure 13-27</a>). The computers could be connected by a <em>local area network</em> (LAN), which connects computers in a small area, such as a network in a university computer science department. The communicating processes could also be on different LANs, connected to the internet. As long as there exists some path through network connections between the two machines, the processes can use sockets to communicate.</p>&#13;
<div class="imagec" id="ch13fig27"><img alt="image" src="../images/13fig27.jpg"/></div>&#13;
<p class="figcap"><em>Figure 13-27: Sockets are bidirectional communication channels that can be used by communicating processes on different machines connected by a network.</em></p>&#13;
<p class="indent">Because each individual computer is its own system (hardware and OS), and because the OS on one system does not know about or manage resources on the other system, message passing is the only way in which processes on different computers can communicate. To support this type of communication, operating systems need to implement a common message passing protocol for sending and receiving messages over a network. TCP/IP is one example of a messaging protocol that can be used to send messages over the internet. When a process wants to send a message to another, it makes <span epub:type="pagebreak" id="page_664"/>a <span class="literal">send</span> system call, passing the OS a socket on which it wants to transmit, the message buffer and possibly additional information about the message or its intended recipient. The OS takes care of packing up the message in the message buffer and sending it out over the network to the other machine. When an OS receives a message from the network, it unpacks the message and delivers it to the process on its system that has requested to receive the message. This process may be in a Blocked state waiting for the message to arrive. In this case, receipt of the message makes the process Ready to run again.</p>&#13;
<p class="indent">There are many system software abstractions built on top of message passing that hide the message passing details from the programmer. However, any communication between processes on different computers must use message passing at the lowest levels (communicating through shared memory or signals is not an option for processes running on different systems). In <a href="ch15.xhtml#ch15">Chapter 15</a>, we discuss message passing and the abstractions built atop it in more detail.</p>&#13;
<h4 class="h4" id="lev2_233">13.4.3 Shared Memory</h4>&#13;
<p class="noindent">Message passing using sockets is useful for bidirectional communication between processes running on the same machine and between processes running on different machines. However, when two processes are running on the same machine, they can take advantage of shared system resources to communicate more efficiently than by using message passing.</p>&#13;
<p class="indent">For example, an operating system can support interprocess communication by allowing processes to share all or part of their virtual address spaces. One process can read and write values to the shared portion of its address space to communicate with other processes sharing the same memory region.</p>&#13;
<p class="indent">One way that the OS can implement partial address space sharing is by setting entries in the page tables of two or more processes to map to the same physical frames. <a href="ch13.xhtml#ch13fig28">Figure 13-28</a> illustrates an example mapping. To communicate, one process writes a value to an address on a shared page, and another process subsequently reads the value.</p>&#13;
<div class="imagec" id="ch13fig28"><img alt="image" src="../images/13fig28.jpg"/></div>&#13;
<p class="figcap"><span epub:type="pagebreak" id="page_665"/><em>Figure 13-28: The OS can support sharing pages of virtual address space by setting entries in the page tables of sharing processes to the same physical frame number (e.g., frame 100). Note that processes do not need to use the same virtual address to refer to the shared page of physical memory.</em></p>&#13;
<p class="indent">If the OS supports partial shared memory, then it implements an interface to the programmer for creating and attaching to shared pages (or shared regions/segments) of memory. In Unix systems, the system call <span class="literal">shmget</span> creates or attaches to a shared memory segment. Each shared memory segment corresponds to a contiguous set of virtual addresses whose physical mappings are shared with other processes attaching to the same shared memory segment.</p>&#13;
<p class="indent">Operating systems also typically support sharing a single, full virtual address space. A <em>thread</em> is the OS abstraction of an execution control flow. A process has a single thread of execution control flow in a single virtual address space. A multithreaded process has multiple concurrent threads of execution control flow in a single, shared virtual address space—all threads share the full virtual address space of their containing process.</p>&#13;
<p class="indent">Threads can easily share execution state by reading and writing to shared locations in their common address space. For example, if one thread changes the value of a global variable, all other threads see the result of that change.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_666"/>On a multiprocessor systems (SMP or multicore), individual threads of a multithreaded process can be scheduled to run simultaneously, <em>in parallel</em>, on the multiple cores. In <a href="ch14.xhtml#ch14">Chapter 14</a>, we discuss threads and parallel multithreaded programming in more detail.</p>&#13;
<h3 class="h3" id="lev1_103">13.5 Summary and Other OS Functionality</h3>&#13;
<p class="noindent">In this chapter, we examined what an operating system is, how it works, and the role it plays in running application programs on the computer. As the system software layer between the computer hardware and application programs, the OS efficiently manages the computer hardware and implements abstractions that make the computer easier to use. Operating systems implement two abstractions, processes and virtual memory, to support multiprogramming (allowing more than one program running on the computer system at a time). The OS keeps track of all the processes in the system and their state, and it implements context switching of processes running on the CPU cores. The OS also provides a way for processes to create new processes, to exit, and to communicate with one another. Through virtual memory, the OS implements the abstraction of a private virtual memory space for each process. The virtual memory abstraction protects processes from seeing the effects of other processes sharing the computer’s physical memory space. Paging is one implementation of virtual memory that maps individual pages of each process’s virtual address space to frames of physical RAM space. Virtual memory is also a way in which the OS makes more efficient use of RAM; by treating RAM as a cache for disk, it allows pages of virtual memory space to be stored in RAM or on disk.</p>&#13;
<p class="indent">Our focus in this chapter on the operating system’s role in running a program, including the abstractions and mechanisms it implements to efficiently run programs, is in no way complete. There are many other implementation options and details and policy issues related to processes and process management, and to virtual memory and memory management. Additionally, operating systems implement many other important abstractions, functionality, and policies for managing and using the computer. For example, the OS implements filesystem abstractions for accessing stored data, protection mechanisms and security policies to protect users and the system, and scheduling policies for different OS and hardware resources.</p>&#13;
<p class="indent">Modern operating systems also implement support for interprocess communication, networking, and parallel and distributed computing. In addition, most operating systems include <em>hypervisor</em> support, which virtualizes the system hardware and allows the host OS to run multiple virtual guest operating systems. Virtualization supports the host OS that manages the computer hardware in booting and running multiple other operating systems on top of itself, each with its own private virtualized view of the underlying hardware. The host operating system’s hypervisor support manages the virtualization, including protection and sharing of the underlying physical resources among the guest operating systems.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_667"/>Finally, most operating systems provide some degree of extensibility by which a user (often a system administrator) can tune the OS. For example, most Unix-like systems allow users (usually requiring root, or superuser, privileges) to change sizes of OS buffers, caches, swap partitions, and to select from a set of different scheduling policies in OS subsystems and hardware devices. Through these modifications, a user can tune the system for the type of application workloads they run. These types of operating systems often support <em>loadable kernel modules</em>, which are executable code that can be loaded into the kernel and run in kernel mode. Loadable kernel modules are often used to add additional abstractions or functionality into the kernel as well as for loading device driver code into the kernel that is used to handle managing a particular hardware device. For more breadth and depth of coverage of operating systems, we recommend reading an operating systems textbook, such as <em>Operating Systems: Three Easy Pieces</em>.<sup><a href="ch13.xhtml#fn13_4" id="rfn13_4">4</a></sup></p>&#13;
<h3 class="h3" id="lev1_104">Notes</h3>&#13;
<p class="fnote"><a href="ch13.xhtml#rfn13_1" id="fn13_1">1.</a> Meltdown and Spectre. <em><a href="https://meltdownattack.com/">https://meltdownattack.com/</a></em></p>&#13;
<p class="fnote"><a href="ch13.xhtml#rfn13_2" id="fn13_2">2.</a> Available at <em><a href="https://diveintosystems.org/book/C13-OS/_attachments/fork.c">https://diveintosystems.org/book/C13-OS/_attachments/fork.c</a></em>.</p>&#13;
<p class="fnote"><a href="ch13.xhtml#rfn13_3" id="fn13_3">3.</a> Available at <em><a href="https://diveintosystems.org/book/C13-OS/_attachments/signals.c">https://diveintosystems.org/book/C13-OS/_attachments/signals.c</a></em>.</p>&#13;
<p class="fnote"><a href="ch13.xhtml#rfn13_4" id="fn13_4">4.</a> Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau, <em>Operating Systems: Three Easy Pieces</em>, Arpaci-Dusseau Books, 2018.<span epub:type="pagebreak" id="page_668"/></p>&#13;
</body></html>