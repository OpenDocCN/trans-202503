- en: '**8**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**ADVANCED CPU DESIGN**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Image](../images/f0181-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The previous chapter presented a minimal CPU design in digital logic. In this
    chapter, we’ll look at extending that basic design to increase performance. These
    extensions include using more registers, using stack architectures that improve
    subroutine capabilities and speed, adding interrupt requests to enable I/O and
    operating systems, floating-point hardware, and pipelining and out-of-order execution
    to enable “superscalar” execution of more than one instruction per clock cycle.
    At this level of complexity we won’t give full details on how to implement the
    extensions yourself with digital logic, but you’re welcome to try!
  prefs: []
  type: TYPE_NORMAL
- en: Number of User Registers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we’ve discussed, the Baby is an example of an accumulator architecture,
    meaning it has only a single user-accessible register: the accumulator. All loads
    go to the accumulator, all stores are taken from it, and when we do two-element
    arithmetic, such as subtraction, the first element comes from the accumulator
    and the second directly from RAM, as in a load.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Accumulator architectures are relatively simple to implement, and they give
    rise to simple instruction sets. The load, store, and arithmetic instructions
    each need only a single operand. For example, to add the numbers stored at addresses
    50A3[16] and 463F[16], we load the content of the first address into the accumulator,
    then have an “accumulative add” (`AADD`) instruction that adds the content of
    the second address into the accumulator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once these instructions execute, the accumulator contains the result of the
    addition.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, accumulator architectures require any data being used to
    be moved in and out of the CPU every time the data is needed. This can slow the
    system down, as RAM is typically slower than the CPU. To avoid this slowdown,
    it can be helpful to provide additional user registers inside the CPU. These extra
    registers allow more than one datum to be brought into the CPU at a time, so that
    multiple calculations can be performed without the need for further RAM access.
    The 8-bit machines of the 1980s typically had a small number of additional user
    registers, while modern machines might have tens or even hundreds of user registers.
  prefs: []
  type: TYPE_NORMAL
- en: Especially in scientific numerical computing, the ideal for assembly programmers
    is often to load *all* relevant data into multiple registers at the start of a
    computation; this allows huge amounts of heavy number crunching within the CPU
    without requiring any further memory access. In some ways, having more registers
    makes assembly programming easier, and it allows faster-running programs to be
    written.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s always a trade-off around how many user registers a CPU should have,
    however, as the additional registers come at a cost: they use up a lot of extra
    silicon, which adds to the costs of design and manufacturing. They’re also bigger
    and use more energy. Then there’s the increasing complexity of the instruction
    set, which in turn requires more silicon in the control unit (CU), with similar
    additional costs. Likewise, the increasing complexity of the instruction set makes
    life more complicated for the assembly programmer, whether a human or a compiler.
    Load, store, and arithmetic instructions now need to have additional operands
    to say which register or registers are to be used, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Some architectures let us have it both ways: they provide a dedicated accumulator
    register and accumulative arithmetic instructions, as well as a set of regular
    user registers. This allows assembly programmers to take advantage of possibly
    simpler and faster instructions on the accumulator while retaining the flexibility
    to work with the other registers as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Number of Instructions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The set of available instructions for a CPU, known as its *instruction set architecture
    (ISA)*, defines the interface between what the programmer can see and use, and
    what needs to be implemented by the CPU designer. As with any interface, designing
    an ISA always involves trade-offs. In this case, there’s a trade-off between making
    the assembly language programmer’s (or more likely today, the compiler writer’s)
    life easy and pleasant, versus making the digital logic implementer’s life easy
    and bug-free. An ISA that contains instructions in the shapes of human thinking
    is easier to program and to write compilers for, but it may be hard to implement
    in digital logic. An ISA that reflects what’s easiest to make in digital logic
    is easy to build and test, but it may not be easy to program or compile to. Then
    there are also trade-offs between making human assembly programmers happy versus
    making compiler writers happy.
  prefs: []
  type: TYPE_NORMAL
- en: CISC and RISC are the two historically opposing philosophies of architecture.
    Most systems actually blur elements of both in various ways, but the CISC versus
    RISC distinction is still useful to structure our thinking and to consider what
    aspects of practical designs are more “CISCy” or more “RISCy.”
  prefs: []
  type: TYPE_NORMAL
- en: '*CISC*, pronounced “sisc,” stands for *complex instruction set computing*.
    CISC emphasizes the creation of lots of instructions in ISAs. These can include
    adding many variations on basic instructions that each act as new instructions.
    For example, loading, storing, and adding can be done in different ways by different
    instructions. CISC style might also create new instructions that perform more
    complex arithmetic than we’ve seen so far, such as the kinds of instructions found
    in scientific calculators, and even dedicated instructions for particular operations
    used in signal processing or cryptography.'
  prefs: []
  type: TYPE_NORMAL
- en: On the opposing side of the debate is *reduced instruction set computing*, or
    *RISC*, which says hardware is nasty, expensive to develop, and difficult to debug,
    so we should make the processor as lean and mean as we can, then do all of the
    more error-prone work in software, as software is much nicer and cheaper to create
    and debug. RISC style is to keep the instruction set as small as possible, then
    focus on making it run as fast as possible. Single complex instructions found
    in CISC can be performed in RISC using longer sequences of more basic instructions,
    which you try to make go as fast together as the single CISC instruction due to
    their simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: Duration of Instructions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our Baby implementation, our CU is based on a regularly repeating counter
    cycle that runs independently of any of the events it triggers. Once you start
    the counter running, its actions follow a fixed sequence that’s completely predestined
    and blind to what the rest of the CPU is doing. This is known as an *open-loop*
    architecture, as there’s no feedback to the counter about the rest of the CPU’s
    state. Open-loop architectures are relatively easy to design and to debug because
    of this independence, which is why we used this style for our Baby. The Analytical
    Engine also uses this style, via its regularly rotating barrel CU.
  prefs: []
  type: TYPE_NORMAL
- en: In a *closed-loop* architecture, by contrast, the timing of triggers isn’t set
    by a central counter. Instead, each stage of work is responsible for triggering
    the next stage when it’s ready to do so. For example, rather than triggering the
    decode stage from a central counter, it can be triggered by a wire that the fetch
    stage activates when its own work is done.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of the closed-loop approach is that some instructions may be simpler
    than others, requiring fewer ticks to complete. These can use only the ticks that
    are necessary, then trigger the next instruction as soon as possible, rather than
    sitting around doing nothing. For example, in our Baby implementation some instructions
    (`SUB, LDN`) need to do work on tick 4, while others (such as `JMP`) do nothing
    during that tick.
  prefs: []
  type: TYPE_NORMAL
- en: Open-loop style is usually associated with RISC, due to RISC’s emphasis on making
    *all* instructions simple and fast. Closed-loop is associated with CISC, as CISC
    may want to include single instructions that perform a lot of complex work and
    take many ticks to complete, as well as short, fast ones.
  prefs: []
  type: TYPE_NORMAL
- en: Different Addressing Modes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RISC and CISC present different ideas about how much work should be done by
    a single instruction, and how many different versions of each instruction should
    be provided. In particular, multiple variant instructions can be created that
    combine memory access with arithmetic.
  prefs: []
  type: TYPE_NORMAL
- en: 'RISC aims to reduce the size of the instruction set by maintaining a clean
    separation between memory access instructions and arithmetic instructions. For
    example, a program to add two numbers together would use two instructions to load
    each of the two numbers into registers, a third instruction to add them and put
    the result in another register, then a fourth to store the result in memory, such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This separation is often taken as the main defining feature of RISC.
  prefs: []
  type: TYPE_NORMAL
- en: 'CISC, in contrast, aims to provide multiple variations of the `ADD` instruction
    to make the programmer’s life easier. In addition to `ADD`, which adds the contents
    of two registers, we could create another instruction such as `ADDM` for “add
    from memory” that would enable the four-line RISC-style addition program to be
    written with a single instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We can interpret this as “add the values stored in memory addresses 50A3[16]
    and 463F[16] and store the result in A4B5[16].” This makes the assembly programmer’s
    life easier, but makes the architect’s life harder, as they now need to build
    extra digital logic in the decoder to decode this extra instruction, as well as
    additional digital logic in the CU to arrange the sequence of load, arithmetic,
    and store operations, which were coded explicitly in the RISC version. This design
    is often taken as the defining feature of CISC.
  prefs: []
  type: TYPE_NORMAL
- en: 'A CISC-style ISA might also include further variations, such as an instruction
    to add the content of one memory location (50A3[16]) to the content of one register
    (R1) and then store the result in a register (R3). For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It likewise might include an instruction to add the contents of one memory
    location (50A3[16]) to the contents of one register (R1) and then store the result
    in a memory location (A4B5[16]):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Another common variant is to add instructions that use *indirect addressing*,
    meaning the operand of the instruction contains the *address of the address* to
    be used. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This means “add the value stored at the address 50A3[16] to the value stored
    at the address 463F[16] and store the result at A4B5[16].” This is a quite complex
    instruction that requires the contents of 50A3[16] and 463F[16] to be loaded into
    registers, but then these values themselves to be interpreted as addresses, and
    the values at *those* addresses loaded into registers before performing the addition
    and store. This sounds like a fairly obscure thing to want to do, but it is very
    common and useful when compiling high-level languages, such as C, that have *pointers*.
    The indirection operations allow for a fast and efficient hardware implementation
    of pointer commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, there are also variations on this indirect form of instruction,
    such as an instruction to perform indirection on just one argument and add the
    result to a register’s contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We could dream up further variations, such as using the contents of registers
    as memory addresses for the indirection, performing more than two layers of indirection,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '*Offset addressing* (aka *index addressing*) modes are another popular ISA
    inclusion. The idea here is that assembly programmers often need to make repeated
    use of many variables that they tend to store close together in memory. Their
    life can be made easier if they can first use a new instruction to specify the
    address of this general region of variable storage, such as A7B2[16], then refer
    to each individual variable by the *difference* between its address and this region’s
    address, such as 0[16], 1[16], 2[16], 3[16], and so on, to pick each of the variables
    in order. Here we’d use new offset instructions such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This adds the contents of addresses A7B2[16] and A7B3[16] and stores the result
    in A7B4[16].
  prefs: []
  type: TYPE_NORMAL
- en: Offset addressing was especially nice for assembly programmers in the 1980s,
    working on machines with 8-bit words but with 16-bit address spaces. This was
    because they would otherwise need to use two words and two registers every time
    they wanted to represent a 16-bit address. Using offsets, they could instead divide
    memory into 256 *pages* of 256 addresses each. They could then choose to work
    on a single page at a time, using the page’s start address as the offset. Then
    they would need only a single word to specify an address within the page. For
    example, A7B4[16] would be considered to be location B4[16] on page A7[16].
  prefs: []
  type: TYPE_NORMAL
- en: To implement offset addressing, an additional register is usually added to the
    CPU design and used to store the offset. Its value can then be joined onto new
    operands to form complete addresses when needed by later instructions.
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to see how the size of an instruction set can get very large once
    all these variations come into play. We’ve only considered variants of a single
    instruction, `ADD`. To be consistent, an ISA must typically create the same chosen
    variations for *every* type of arithmetic instruction, which can lead to hundreds
    of new instructions in total.
  prefs: []
  type: TYPE_NORMAL
- en: Subroutines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The word *subroutine* is one of many names—subprograms, procedures, functions,
    methods—for a very similar concept: a piece of code sitting somewhere in memory
    that will do something when your main program *calls* it, and will *return* to
    the same line in the main program after it was called. This last bit distinguishes
    a subroutine from a simple jump like the `goto` statement, which forgets where
    it was called from. The invention of the subroutine is generally credited to Maurice
    Wilkes and his team around 1950.'
  prefs: []
  type: TYPE_NORMAL
- en: In early high-level languages, only one level of subroutine calling was allowed
    at a time. You had a main program that could call subroutines, but subroutines
    could not then call other subroutines. More modern high-level languages rely on
    the ability for subroutines to hierarchically call other subroutines—including
    themselves, as in *recursion*—to encapsulate complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The term *subroutine* is generally used at the level of architecture and assembly
    language. The other names are used in higher-level languages and have historically
    had somewhat different meanings that have never been formally defined or used
    consistently across most languages. The following list is an attempt at definitions
    that capture what the words *would* mean if they were ever used consistently by
    language designers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Function** This is the easiest to define formally, as it’s a concept used
    in the most heavily formalized functional programming languages. A function is
    (ideally) a mathematical object that takes arguments as inputs and returns a value
    computed only from these inputs and not from anything else beyond them. The function
    shouldn’t have any other *side effects*, meaning it shouldn’t affect anything
    else.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Procedures** This is a name for subroutines in some languages that may or
    may not take inputs and don’t usually return an output, so they act only via side
    effects. Some older languages allow only one level of calling, meaning procedures
    can’t call other procedures.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Method** This is a concept that comes up in object-oriented programming to
    name a subroutine associated with an object. A method may both return a value,
    like a function, and have side effects, like a procedure.'
  prefs: []
  type: TYPE_NORMAL
- en: All of these names are heavily abused and confused by practical programming
    languages; for example, it’s common for languages to have “functions” that produce
    side effects and don’t return values. Functional programming languages are more
    likely to enforce the mathematical concept, but even some functional programming
    languages allow side effects.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at how to implement subroutines.
  prefs: []
  type: TYPE_NORMAL
- en: '*Stackless Architectures*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It’s possible to implement subroutines purely in software, without any additional
    hardware or instructions. You could do this by writing programs with jump instructions
    and then having some convention to keep track of the return address. However,
    this is hard work for the programmer and slow for the computer.
  prefs: []
  type: TYPE_NORMAL
- en: Early subroutine-capable architectures such as ENIAC added dedicated CPU instructions
    to call and return, and simple hardware in the CPU to execute them. One approach
    used a single return address location in hardware. A special dedicated internal
    register can be easily built into a CPU to store a return address. This allows
    the main program to call and return from one subroutine at a time; once you’re
    inside the subroutine, you can’t call and return from another subroutine, because
    this would overwrite the single return address.
  prefs: []
  type: TYPE_NORMAL
- en: To enable subroutines to call one another (including recursively calling themselves),
    architectures can add a hardware stack.
  prefs: []
  type: TYPE_NORMAL
- en: '*Stack Architectures*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A *stack* is a simple data structure with two operations, push and pop. It behaves
    like a physical stack of papers on your desk. You can *push* a new document to
    the top of the stack when it arrives, and you can *pop* only the top document
    on the stack by picking it off and removing it. You can’t take documents from
    lower down in the stack.
  prefs: []
  type: TYPE_NORMAL
- en: Using a stack, you can create a full trail of addresses to return through in
    the case of nested subroutines. Each time a subroutine is called, its return address
    is pushed to the stack. When the subroutine returns, this address is popped off
    the stack and used to set the program counter.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping track of subroutines this way can be especially useful in cases of recursion.
    The stack typically grows very large during recursive execution and (hopefully)
    is reduced as the program completes and data is read and removed from the top
    of the stack. A *stack overflow* error is a failure condition where we run out
    of stack space; if you use a specific chunk of memory to hold your stack, and
    you run out of space, the program will create a stack overflow error as you try
    to write outside the stack boundaries. This usually happens because of something
    that’s gone wrong in an infinite loop of functions calling themselves or each
    other. In modern computers, stacks are much less resource-expensive to implement
    than they used to be, so they’re the standard way to store return addresses.
  prefs: []
  type: TYPE_NORMAL
- en: '*Hardware stacks* are found in most modern machines from the 8-bit era onward.
    They use hardware digital logic implementations of the stack concept in their
    CUs to enable arbitrary subroutine calling with enhanced speed and security. These
    stack architectures have an extra, dedicated *stack pointer register*, an internal
    register that contains a pointer to the top of the stack. The stack itself may
    be stored in some area of RAM, with access to this part of RAM often restricted
    at the hardware level. For example, a stack architecture might have dedicated
    digital logic to test all load and store instructions from the user, to make sure
    they aren’t trying to access the stack’s portion of RAM. This prevents malicious
    programmers from interfering with the stack.'
  prefs: []
  type: TYPE_NORMAL
- en: Some stack architectures hide their internal workings from the user, and provide
    only new call- and return-style instructions in the instruction set. When executed,
    the call instructions will activate digital logic that automatically pushes the
    program counter to the stack, increments the stack pointer, and jumps to the subroutine.
    Likewise, the return instructions will pop the program counter, decrement the
    stack pointer, and jump back to the calling function.
  prefs: []
  type: TYPE_NORMAL
- en: Other stack architectures allow full user access to the contents of the stack
    in addition to or instead of call and return. For example, some designs provide
    instructions such as `PHA`, for PusH Accumulator, and `POPA`, for POP Accumulator.
    The former pushes whatever is in the accumulator onto the stack and increments
    the stack pointer, and the latter pops the stack to the accumulator and decrements
    the stack pointer. This design provides a method to pass arguments to subroutines
    by storing them on the stack along with the return addresses.
  prefs: []
  type: TYPE_NORMAL
- en: '**CALLING CONVENTIONS**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whatever stack or stackless architecture is used to enable subroutines, it
    will rely on the programmer to maintain a consistent *calling convention* so the
    different parts of the program can correctly understand one another as they pass
    and receive arguments. This is especially important when subroutines are written
    by a different author from the caller code, as is the case when a general-use
    library of subroutines is provided. A calling convention includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where arguments, return values, and return addresses are placed: in registers,
    on the call stack, a mix of both, or in other memory structures'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The order and format in which arguments are passed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How a return value is delivered from the callee back to the caller: in a register,
    on the stack, or elsewhere in RAM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the task of setting up for and cleaning up after a function call is divided
    between the caller and the callee
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether and how metadata describing the arguments is passed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calling conventions aren’t part of CPU architecture. Rather, they’re social
    agreements between programmers. A given architecture can often be used with any
    one of many different possible calling conventions. In some cases, CPU architects
    will suggest a convention to try to discourage fragmentation between their users.
    In other (or sometimes, the same!) cases, programmers create their own conventions
    and standards wars break out when they need to interface their programs.
  prefs: []
  type: TYPE_NORMAL
- en: Calling conventions also define additional features for compatibility between
    modern high-level languages and compilers. For example, compiled executable code
    from two languages, such as C and C++, can link to and call one another’s subroutines
    as long as they obey the same calling convention.
  prefs: []
  type: TYPE_NORMAL
- en: Floating-Point Units
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We saw in [Chapter 2](ch02.xhtml) how floating-point numbers are represented
    with a sign, an exponent, and a mantissa. *Floating-point registers* are specialized
    user registers designed to store floating-point data representations for use in
    floating-point computations.
  prefs: []
  type: TYPE_NORMAL
- en: Performing arithmetic on these representations is more complicated than the
    arithmetic logic unit (ALU) operations on integers seen so far. To multiply two
    floating points, for example, we need to multiply their mantissas, add their exponents,
    and multiply their signs. To add two floating-point numbers, we need to shift
    one of them by the difference in their exponents, then add them, and possibly
    shift again and update the exponent. Dividing can be error-prone when a large
    number is divided by a small one, and can also result in special cases defined
    to yield infinity or NaN (not a number) representations.
  prefs: []
  type: TYPE_NORMAL
- en: This can all be done by combining simple ALU-style operations together and using
    new components of digital logic. The resulting structure is called a *floating-point
    unit (FPU)*. FPUs are complex pieces of digital logic and expensive to design;
    they also take up lots of silicon and are prone to bugs. In 1994, Intel made an
    error implementing the FPU in their Pentium chip that cost them half a billion
    dollars in recalls and reputational damage.
  prefs: []
  type: TYPE_NORMAL
- en: FPUs appeared in the 1980s, not inside the CPU but as optional additional chips.
    For example, the Intel 8086 CPU could be paired with an optional extra FPU chip,
    the lesser-known 8087\. Nowadays, FPUs have all moved onto the CPU and behave
    similarly to their ALU counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to see what the dedicated registers and instructions on a modern
    FPU look like, they take up most of a full book, volume 3 of the amd64 reference
    manuals, which you’ll meet in [Chapter 13](ch13.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Pipelining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Everything we’ve looked at so far involves writing a program in assembly language,
    compiling that into machine code, and executing the machine code from top to bottom,
    with some branching and looping. Fundamentally, the instructions are brought in
    one at a time, and each individual instruction is executed before the next is
    brought in. Most modern CPUs don’t work like this. Instead, they work on parts
    of multiple instructions in parallel. We’ll look at many more forms of parallelism
    in [Chapter 15](ch15.xhtml), but those that operate at this CPU level are known
    as *instruction-level parallelism*, and we’ll study them here.
  prefs: []
  type: TYPE_NORMAL
- en: '*Pipelining* is a form of instruction-level parallelism that appeared in the
    32-bit era. A pipeline is like a production line, where there are multiple workers
    doing tasks at the same time, as in Henry Ford’s 1913 car factory, shown in [Figure
    8-1](ch08.xhtml#ch08fig1).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0190-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-1: Ford’s 1913 production line*'
  prefs: []
  type: TYPE_NORMAL
- en: Ford assigned one specialized task to each worker and positioned them at fixed
    locations along a conveyor belt. Car parts moved along the conveyor, with each
    worker in turn doing their work on each car part.
  prefs: []
  type: TYPE_NORMAL
- en: Now replace Ford’s car parts with instructions from your machine code program,
    and imagine them being run down this production line. Instead of human workers,
    you have parts of the CPU performing tasks such as fetch, decode, and execute.
    Suppose you’ve written 20 lines of code, and assume there’s no jumping or branching.
    In the CPU designs we’ve seen so far, a single instruction is placed on the production
    line and passes by the fetch, decode, and execute workers in turn. Once it gets
    to the end of the production line, the next instruction is placed at the start
    of the line. Most of the workers thus end up standing around doing nothing for
    most of the time when it isn’t their turn to work.
  prefs: []
  type: TYPE_NORMAL
- en: We could extract much higher efficiency from our workers by keeping the whole
    conveyor belt full of instructions the whole time, rather than waiting for one
    to finish before starting the next one. One worker could be executing one instruction
    at the same time that a second worker is decoding the next instruction and a third
    worker is fetching the instruction after that.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different ways of dividing up the work of a CPU into such stages,
    depending on the architecture type. The classic split considers fetch, decode,
    and execute stages. Our LogiSim Baby used a cycle of five ticks. Modern CPUs can
    have many more subdivisions—there are around 37 stages in a modern Intel processor’s
    pipeline!
  prefs: []
  type: TYPE_NORMAL
- en: At the digital logic level, pipelines can be implemented by having the CU trigger
    multiple components at the same time rather than one at a time. It’s common to
    show pipelines as diagrams like [Figure 8-2](ch08.xhtml#ch08fig2).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0191-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-2: A diagram showing how instructions are handled in a basic pipeline*'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 8-2](ch08.xhtml#ch08fig2), clock cycles go across from left to right,
    and four instructions (represented by squares) are shown passing through the pipeline.
    This is a four-stage pipeline, so it can work on up to four instructions at once
    (as shown on clock cycle 4), each at a different stage of processing.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelining is simpler and more efficient for open-loop, RISC architectures,
    in which all instructions have equal durations so they can progress evenly along
    the pipeline. It can quickly become complex and less efficient for closed-loop,
    CISC architectures, where different durations for different instructions must
    be taken into account, and where some parts of the pipeline get left empty as
    shorter instructions execute alongside longer ones.
  prefs: []
  type: TYPE_NORMAL
- en: '*Hazards*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are several well-known scenarios, called *hazards*, in which problems
    occur in pipeline execution. Let’s take a look at the main types of hazards. Then
    we’ll consider how to fix them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Branching Hazards**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A *branching hazard* occurs when, somewhere down the pipeline, an `if` statement
    is found and the other, earlier stages were working to complete one outcome of
    the branch, but you need to go to the other outcome instead. When a conditional
    branch is reached, you don’t know which condition will be followed until the branch
    gets executed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Hazards**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If two of the workers are trying to hit on the same memory location—to fetch
    and output to, for example—bad things are going to happen. We can split these
    *data hazards* into three main categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Read after write** This is where you have two instructions, the first trying
    to write to memory and the second trying to read from the same address. The logic
    of the program is supposed to be that the value that gets read should be equal
    to what has just been written. But when pipelining is in play, it may be possible
    for the RAM access of the read to occur before RAM has been changed by the write.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Write after read** This is the other way around: here two instructions are
    supposed to first read the old RAM value and then update it with a write. But
    when they are interleaved by pipelining, it may be possible for the part of the
    write instruction that actually changes the RAM value to occur before the part
    of the read that accesses it.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Write after write** This is where two write instructions interfere with one
    another when trying to write to the same address. The program logic is supposed
    to be that the first one writes, then the second one, leaving the address containing
    the second one. But again, pipelining may interleave stages of their executions,
    in some cases performing the intended first write after the second.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Structural Hazards**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The third type of hazard, a *structural hazard*, is where multiple stages are
    fighting for resources at the same time. In the production line example, the factory
    might contain one physical calculator on a shelf behind the workers, for their
    shared use. There may come a time when two of the workers both want to use this
    calculator at once. For example, one might need to check if something equals zero,
    while the other needs to execute an addition operation. The digital CPU analog
    of this would be two regions of digital logic computing two pipeline states both
    needing to access the ALU or memory at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: '*Hazard Correction*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pipelining tends to work well for all types of signal processing—including audio,
    video, and radio processing—because there isn’t much branching to handle. The
    same kind of data is expected to flow through the pipeline in real time and always
    be processed in the same way. For example, the codecs in your digital TV or laptop
    used to decode and display movies will reliably chug through frame after frame
    of incoming video and audio, doing the same operations to decode and display each
    one, in the same order. They don’t usually have to look at the content of the
    signals and change their behavior in response to this content.
  prefs: []
  type: TYPE_NORMAL
- en: Hazards become more problematic when you’re doing computations that are continually
    checking the results and changing their flow based on this state. As soon as you
    have programs with branches—and to a lesser extent jumps and subroutines—you have
    to think about how to address hazards. Let’s go through a few general strategies.
  prefs: []
  type: TYPE_NORMAL
- en: '**Programming to Avoid Hazards**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Skilled assembly programmers can write assembly code to avoid many hazards,
    if they understand the architecture. This often involves considering groups of
    neighboring instructions and thinking about how they could affect one another
    in the pipeline, and changing the order of some instructions to make them further
    apart and less likely to affect one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nowadays, most programming is done in compiled languages, so some of the tricks
    that end-user programmers once employed have moved into compilers. A good compiler
    can inspect the assembly code it’s produced and look for places likely to lead
    to hazards. It can tweak this code as a human programmer would to reduce the likelihood
    of the hazard. For example, the order of instructions that don’t affect each other
    can be swapped to make two accesses of the same data occur further apart in the
    execution. Of course, there’s still a human behind these sorts of optimizations:
    the authors of the compiler, who have likely taken a strong interest in hazards
    and how to avoid them.'
  prefs: []
  type: TYPE_NORMAL
- en: Some ISAs provide a *null operation (NOP)* as an extra instruction that means
    “do nothing.” NOP instructions still go through the pipeline, taking up time slots,
    so a human programmer or compiler can insert them between hazard-causing instructions
    to spread them out and avert the hazard. This typically requires less intelligence
    than reordering instructions, but will slow down execution as the NOPs are processed
    through the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stalling**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Stalling* (sometimes known as *bubbling*) simply means putting the result
    of the pipeline on hold to allow some stage to complete its work. For example,
    if there’s a structural hazard and two stages want to use the ALU at the same
    time, we just let one of them use it and tell everyone else to do nothing until
    the ALU is free. In the production line analogy, this is like the system used
    in factories where if a worker gets into trouble they can hit a button to stop
    the conveyor belt to give them time to fix the problem.'
  prefs: []
  type: TYPE_NORMAL
- en: To allow for stalling, additional digital logic can be added to the CPU to detect
    the upcoming potential occurrence of hazards—for example, temporally ceasing to
    trigger stages for the next instructions as soon as a jump or branch is seen to
    be coming in. This is a heavyweight solution that has a large time cost if it’s
    used frequently. As with NOPs, the whole pipelining system is effectively disabled
    around hazards, so if we have to do this all the time then we might as well just
    use a non-pipelined CPU. But stalling is relatively simple and cheap in terms
    of silicon and design time to implement.
  prefs: []
  type: TYPE_NORMAL
- en: '**Redoing Work**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Redoing work* means that as a potential hazard instruction is being processed,
    we allow the following instruction to begin its cycle as normal, with the hope
    that the hazard won’t actually occur. If we later complete execution of the potential
    hazard instruction and find that a hazard *has* actually occurred, then we throw
    away the work that’s been done on the next instruction and do it again.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, when a branch instruction arrives, we assume that it won’t be taken,
    and begin fetching and decoding the following instructions at the same time as
    testing the branch’s condition. If we then find the branch is not to be taken,
    the work already done on the next instructions is useful and is kept, progressing
    to execution. But if we find that the branch is to be taken, we discard the work
    on the subsequent instructions and start fetching and decoding different ones
    from the branch target address.
  prefs: []
  type: TYPE_NORMAL
- en: This strategy is more efficient than stalling, where a performance hit is taken
    at every *potential* hazard, even the ones that don’t end up actually occurring.
    Say there are 100 branches in your program, only half of which are taken; stalling
    would delay all 100 of them, whereas redoing work delays only 50.
  prefs: []
  type: TYPE_NORMAL
- en: '**Eager Execution**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Eager execution* means executing *both* possible branches at the same time
    for a short period, and then killing the one not taken later on, once we figure
    out which it should be. For example, a typical use of eager execution occurs in
    instruction sequences such as (using Baby assembly):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This sequence first asks if a condition is true; depending on the result, it
    loads from either address 10 or address 11\. In eager execution, we begin fetching,
    decoding, and executing both lines 2 and 3 while we are still executing line 1\.
    Only later, once the result of the line 1 comparison is known, do we decide which
    of line 2 or 3 is wanted. We keep the work that’s been done on the desired line
    and throw away the work done on the unwanted one.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing eager execution requires doubling up our physical digital logic
    to perform twice as much computation in parallel during the period of uncertainty.
    This could involve having two physical copies of the ALU, registers, and execution
    logic. This can be a good use for the additional silicon that the transistor density
    form of Moore’s law currently provides, while not allowing faster clocks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Branch Prediction**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Branch prediction* is where we try to predict whether a branch will be taken
    *before* actually executing it. Such prediction may initially sound impossible
    (surely the very meaning of execution is to find out what the branch will do),
    but we can often make use of prior knowledge to give us at least a better-than-random
    guess.'
  prefs: []
  type: TYPE_NORMAL
- en: For branch hazards, the redoing work approach can be viewed as always predicting
    that branches won’t be taken. It begins to fetch and decode the instruction from
    the next numerical address while working on execution of the branch instruction.
    Branch prediction generalizes redoing work by trying to make a more accurate prediction
    about whether a branch will be taken. Fetch and decode can then begin for whichever
    branch is predicted, and work redone only in cases where the prediction turns
    out to be incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: Branch prediction remains an active area of research, with several strategies
    under investigation. One is to assume that all branches *are* taken—essentially
    the opposite assumption of the redoing work approach. If users wrote only programs
    whose branches originated from `if` statements, then these branches would have
    a 50/50 chance of being taken, in the absence of any other information. However,
    many or most of the branches that appear in practical machine code originate from
    loops rather than `if` statements, and the usual purpose of a loop is to repeat
    many, rather than zero or one, times. Therefore, when branches originate from
    loops, they usually *are* taken because the user wants to loop a few times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Large-scale statistical studies of real-world machine code found in the wild
    have confirmed this: their estimates range from 50 to 90 percent of branches being
    taken.'
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, another strategy is for the human programmer or compiler to provide
    hints about which branches will be taken. This could include human programmers
    adding special comments to their assembly or high-level code, or compilers using
    code analysis to create their own predictions and annotations. For some compilation
    tasks, this is easy to do—for example, if a user program says “repeat 100 times,”
    then we can make a good prediction. Predictions are harder—in some cases uncomputable—to
    make in the case of `while` loops.
  prefs: []
  type: TYPE_NORMAL
- en: A third, state-of-the-art approach is to use dynamic runtime branch prediction,
    which involves building statistical or machine learning classifiers into CPU digital
    logic and using them to make on-the-fly predictions. As with all prediction systems,
    this requires choosing some features of programs that may be informative about
    temporally and spatially nearby branch behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: Simpler cases include keeping a log of observed frequencies of branch-taking
    at each branch instruction during execution of the user program, and using these
    frequencies as probabilistic prediction for which way the branches will go if
    the same instructions are executed again.
  prefs: []
  type: TYPE_NORMAL
- en: More advanced cases now include linear regression and even neural network classifiers
    built from digital logic and pretrained on large collections of machine code gathered
    from real-world programs in the wild. These may be trained on all kinds of features
    of the machine code, such as values of opcodes and operands in many lines before
    and after a branch instruction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Operand Forwarding**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Operand forwarding* is a technique for avoiding data hazards by adding digital
    logic to directly route the result of an instruction to become an input to a next
    or nearby instruction. For example, consider this program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This computes R3 = R1 + R2, then R4 = R3 + R1, where all the operands are registers.
    Here, instruction 2 requires the result of instruction 1 to be placed in R3 before
    instruction 2 can execute. This will result in a data hazard for most pipelines.
    However, the value destined for R3 may in fact be available on the output lines
    of the ALU during execution of instruction 1, but before it appears in its destination
    register. By connecting a physical wire directly from the ALU output to where
    the data is required (such as an ALU input), we can bypass the wait for the result
    to be deposited and reread, and instead start using it immediately.
  prefs: []
  type: TYPE_NORMAL
- en: Out-of-Order Execution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Out-of-order execution (OOOE)* is a more advanced form of instruction-level
    parallelism than pipelining. It involves actually swapping around the order of
    instructions as they come into the CPU, so they’re executed in a different order
    than they appear in the program. OOOE architectures were first enabled in theory
    in 1966 by Tomasulo’s algorithm, and appeared commercially in the 1990s.'
  prefs: []
  type: TYPE_NORMAL
- en: The key to OOOE is recognizing that instructions in serial programs can often
    be swapped without changing their results. For example, if we have some variables
    being assigned values, we can make those assignments at any time before the variables
    are next used without affecting the result; we’ll end up with the same state overall.
    This gives us freedom to swap instructions around to prevent pipeline hazards
    from occurring and to maximize efficiency. Whether we have single or multiple
    hardware copies of CPU substructures available, we can choose orderings that try
    to make the best use of these resources, keeping them all as busy as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how OOOE works, consider the following program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: There are six instructions here. Instruction 1, for example, sets register R1’s
    contents to the result of dividing the contents of register R4 by the contents
    of register R7\. We’ll assume we have simple machines available for division and
    multiplication, but that they’re slower than those for addition and subtraction.
    The left of [Figure 8-3](ch08.xhtml#ch08fig3) graphs the dependencies between
    the instructions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0197-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-3: A dataflow graph (left) and a schedule (right) for the sample
    program, allowing for OOOE*'
  prefs: []
  type: TYPE_NORMAL
- en: As an example of a dependency, consider that instruction 2 can’t begin execution
    until instruction 1 is complete, because instruction 1 writes to register 1, which
    is needed as an input to instruction 2\. The dependency graph shows that we don’t
    need to execute the instructions exactly in their original order. As long as any
    instruction is executed after all of its parents in the graph, the result will
    be the same. We can reorder the sequence of instructions and/or execute them in
    parallel as long as the arrows in the graph are respected.
  prefs: []
  type: TYPE_NORMAL
- en: The right of [Figure 8-3](ch08.xhtml#ch08fig3) shows one possible ordering for
    execution of the instructions. Here, instructions 1, 3, and 4 are executed in
    parallel to start. Instruction 5 comes after 3 and 4, but can still occur in parallel
    with 1 (which takes longer, being a more complex division operation). Instruction
    2 can occur once 1 has finished, and 6 (another longer instruction, as a multiply)
    must come last, since it needs the results of both 2 and 5\. Depending on how
    many ALUs we have available, we could execute this or similar schedules much faster
    than a single series or even a pipeline of the original program.
  prefs: []
  type: TYPE_NORMAL
- en: OOOE is usually performed by digital logic in the CPU, in real time during program
    execution. Usually only a short window—such as 10 or 20 instructions—around the
    current instruction in the program is considered for reordering.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you extend the idea of OOOE into reordering and parallelizing entire programs,
    you’ll arrive at GPU dataflows, which you’ll meet in [Chapter 15](ch15.xhtml).*'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperthreading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a basic CPU, only the fetching hardware is active during the fetch stage,
    only the decoder is active during the decode stage, and only the ALU or CU is
    active during the execute stage. Pipelining and OOOE are two ways to make better
    use of the CPU hardware resources that are otherwise idle during the fetch-decode-execute
    cycle, by having them work on parts of multiple instructions at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: '*Hyperthreading* is another way to make use of CPU resources when they would
    otherwise be sitting idle during the cycle. Rather than work on consecutive instructions
    from one program, we put them all together to form a second virtual CPU core that
    operates on a separate set of instructions. Each component of this virtual core
    runs out of phase with its use in the main CPU core, when it would otherwise be
    idle. By collecting all the components together, all out of phase, we create a
    whole extra CPU, keeping all the silicon in constant use at all times.'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperthreading was conceived in the 1970s and became widespread in commercial
    CPUs during the 2000s. It effectively doubles the number of apparent cores over
    the number of physical cores in a device, which is why you often see your computer
    report having twice the number of cores that were advertised on the hardware you
    bought.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperthreading has the advantage over pipelining that you no longer have to
    worry about hazards because the two cores can operate completely independently
    of one other. On the other hand, it doesn’t increase the speed of any one program.
    It also requires additional digital logic to read, store, and write the states
    of the two virtual CPUs at the right times, and duplication of some hardware components,
    so that one doesn’t affect the other. In practice, pipelining and hyperthreading
    may be used together, especially when pipelines are broken down into many smaller
    stages. Figuring out how to balance them is advanced work that’s beyond the scope
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beyond a minimal CPU such as the Baby, architects are faced with many decisions
    about what trade-offs to make between speed, usability, silicon size, and energy
    costs. Adding more features to a CPU, such as more registers, ALU and floating-point
    simple machines, stacks, and different addressing modes, can make life easier
    and faster for the user programmer or compiler, but at the cost of silicon and
    energy. Likewise, adding more instructions can make life easier for some programmers
    and compilers who may ask for them, but harder for others who have to keep up
    with the extra complexity. Giving all instructions the same fixed duration makes
    life easier for pipeline and OOOE designers and CPU debuggers, but may be less
    efficient if a few complex instructions that require long execution times are
    in use.
  prefs: []
  type: TYPE_NORMAL
- en: 'RISC is a style that generally aims to keep instructions and instruction sets
    small and simple, while making use of extra silicon to speed up the instructions
    via more registers, pipelines, and OOOE. CISC is the opposite style: it prefers
    to make use of extra silicon to add more complex instructions and create larger
    instruction sets. The two styles tend to fit different applications, as we’ll
    see in [Chapters 13](ch13.xhtml) and [14](ch14.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Even with the most advanced CPU design, your computing experience would be very
    limited without input, output, and memory, and the next two chapters will look
    at how to add these to your computer.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Confusing a Pipeline**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Design the simplest assembly program needed to confuse a basic pipeline, making
    it run as slowly as a non-pipelined system. Try to extend this program to further
    confuse each of the hazard-handling strategies as much as possible. How likely
    are such programs to occur in practice, and what might be done to avoid them?
  prefs: []
  type: TYPE_NORMAL
- en: '**Challenging**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Try to build an FPU in LogiSim, based on the floating-point data representation
    seen in [Chapter 2](ch02.xhtml). Consider how previously seen simple machines
    can be combined in each of the arithmetic operations of addition, subtraction,
    multiplication, and division. For example, when two floats are multiplied, their
    exponents are added.
  prefs: []
  type: TYPE_NORMAL
- en: '**More Challenging**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Try to extend the previous LogiSim Baby design with some minimal pipelining.
    For example, you could try to increment the program counter and start performing
    the next fetch while the current instruction is executing. The hard part is dealing
    with branching hazards. You may want to assume initially that the branch will
    be taken, then add logic to clear things out and start again if this turns out
    to be incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the origin of the controversially named “von Neumann” architecture, see
    John von Neumann, “First Draft of a Report on the EDVAC,” June 30, 1945, *[https://history-computer.com/Library/edvac.pdf](https://history-computer.com/Library/edvac.pdf)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the invention of subroutines, see Maurice Wilkes, David Wheeler, and Stanley
    Gill, *The Preparation of Programs for an Electronic Digital Computer: With Special
    Reference to the EDSAC and the Use of a Library of Subroutines* (Cambridge, MA:
    Addison-Wesley, 1951).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Human Resource Machine*, *Shenzen I/O*, and *TIS-100* are educational video
    games that present CPU-like environments with different instruction sets and goals
    for you to explore.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
