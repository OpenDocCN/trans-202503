- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Computational Geometry for Facial Recognition
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 面部识别的计算几何学
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: 'Let’s leave behind the world of resource distribution and look at another field
    where computational geometry can help you: facial recognition. *Facial recognition*
    is the process of examining the features of a face and determining if it matches
    a previously seen face and, if so, to what degree. Most babies can recognize familiar
    faces by three to four months old, but unfortunately, just because babies can
    do it doesn’t mean it’s easy, at least not for computers. We’ll need to combine
    computational geometry and machine learning algorithms for our program to perform
    at a similar level.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们抛开资源分配的世界，看看计算几何学如何帮助你进入另一个领域：面部识别。*面部识别*是检查面部特征并确定其是否与先前见过的面部匹配，并且如果匹配的话，匹配的程度。大多数婴儿在三到四个月大时就能识别熟悉的面孔，但不幸的是，仅仅因为婴儿能做到这一点并不意味着这很容易，至少对计算机来说并非如此。我们需要结合计算几何学和机器学习算法，以便我们的程序能够达到类似的水平。
- en: We’ll begin this chapter with a brief look at facial recognition. We’ll then
    cover the main algorithm, and, as usual, put it to work in the proof of concept,
    which will cover loading, cleaning, and modeling data stored in a database of
    facial images. We’ll process each image in the database to extract the most important
    facial features for determining which person is in an image, and then we’ll use
    this information to build a model that can match a never-before-seen image of
    a face to the appropriate person.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从简要介绍面部识别开始本章。然后，我们将介绍主要的算法，并像往常一样，将其应用于概念验证中，涵盖加载、清理和建模存储在面部图像数据库中的数据。我们将处理数据库中的每一张图像，提取最重要的面部特征，以确定图像中的人是谁，然后我们将利用这些信息建立一个模型，将一张前所未见的面部图像与相应的人匹配。
- en: 'To achieve this we’ll be relying on various machine learning (ML) tools. ML
    algorithms seek to identify or “learn” the relationship between input data and
    output values using two broad categories of solutions: unsupervised learning and
    supervised learning.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将依赖各种机器学习（ML）工具。ML算法旨在通过两大类解决方案来识别或“学习”输入数据与输出值之间的关系：无监督学习和监督学习。
- en: In *supervised learning* we provide the algorithm a set of input data along
    with the proper output we’d like the program to learn for that input (called its
    *class*). In our case, the input data will be a face’s geometry and the class
    we want to predict is the associated person’s name, which makes this a *discrete
    classification* problem. If the class value we wanted to predict were a continuous
    number (such as the price of a house), this would be called a *regression* problem
    (you may already be familiar with linear regression from statistics class).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在*监督学习*中，我们向算法提供一组输入数据，并提供我们希望程序为这些输入学习的正确输出（称为其*类别*）。在我们的案例中，输入数据将是一个面部的几何特征，而我们想要预测的类别是与之相关的人的姓名，这使得这是一个*离散分类*问题。如果我们想要预测的类别值是一个连续的数字（例如房子的价格），这将被称为*回归*问题（你可能已经熟悉线性回归，它是统计学中的一个概念）。
- en: '*Unsupervised learning* seeks to discover previously unknown relationships
    and largely deals with clustering data together based on different inputs to find
    interesting groupings. Because we don’t know the goal going in, unsupervised learning
    is also sometimes called *exploratory analysis*. We won’t be doing much unsupervised
    learning in this project, so I’ll leave it to you to dive into this topic more
    on your own. By the time you complete this chapter, you should feel comfortable
    working with image data, extracting geometric facial characteristics, and training
    supervised learning classifiers for your own facial recognition projects.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*无监督学习*旨在发现先前未知的关系，主要通过根据不同的输入将数据进行聚类，以发现有趣的分组。因为我们在开始时不知道目标是什么，所以下无监督学习有时也被称为*探索性分析*。在这个项目中我们不会做太多的无监督学习，所以我会把这个话题留给你自己深入研究。当你完成本章后，你应该能熟练处理图像数据，提取几何面部特征，并训练监督学习分类器，用于你自己的面部识别项目。'
- en: Uses of Facial Recognition in Security
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面部识别在安全中的应用
- en: Today it isn’t hard to find examples of facial recognition being used in security.
    Facial recognition systems have become a widespread and generally accepted part
    of life. Industries that are already benefiting from the technology include retail
    stores, casinos, cell phone manufacturers, and law enforcement agencies. It wasn’t
    always this way, though. The core of the technology has been slowly progressing
    since the 1960s, when Woodrow Wilson Bledsoe invented a way for people to manually
    encode the geometry of a face using an electronic surface and a conductive stylus.
    Using the stylus, users would mark a set of standardized facial features like
    the bridge of the nose, eyebrows, and chin. The program would then measure the
    geometry between these points and shapes and create a geometric map of the input
    face. The data was then compared against a database of previously recorded traces
    to produce the name of the person the face was most closely matched to. Of course,
    this method was time consuming and prone to operator error. For the next 40 or
    so years, researchers kept improving on the algorithms by defining more standard
    points to measure and using image analysis techniques to automatically identify
    these key points in a picture of a face.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，面部识别在安全领域的应用并不难找到。面部识别系统已经成为生活中广泛应用且被普遍接受的一部分。已经从这项技术中受益的行业包括零售商店、赌场、手机制造商和执法机构。然而，这并非一直如此。这项技术的核心自1960年代以来一直在缓慢进展，当时伍德罗·威尔逊·布莱德索（Woodrow
    Wilson Bledsoe）发明了一种方法，允许人们通过电子表面和导电笔手动编码面部几何形状。使用导电笔，用户可以标记出一组标准化的面部特征，如鼻梁、眉毛和下巴。然后，程序会测量这些点和形状之间的几何关系，并创建输入面部的几何图。数据随后会与先前记录的面部痕迹数据库进行比较，以确定与该面部最匹配的人的姓名。当然，这种方法既耗时又容易出错。在接下来的四十年里，研究人员通过定义更多的标准化测量点和使用图像分析技术自动识别面部照片中的这些关键点，持续改进算法。
- en: It wasn’t until the turn of the century that facial recognition started to move
    from science fiction and research labs to reality. One highly publicized case
    happened in January 2001, when the city of Tampa, Florida, used a facial recognition
    system to record and analyze the face of every attendee during Super Bowl XXXV
    in the hope of spotting criminals with warrants issued for their arrest. The program
    is credited with identifying a few petty criminals in attendance, but is largely
    considered a failure due to the high cost and large number of false positives.
    Worse, it prompted a large backlash from privacy advocacy groups including the
    Electronic Frontier Foundation and the American Civil Liberties Union.^([1](b01.xhtml#c10-endnote-001))
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 直到本世纪初，面部识别才开始从科幻小说和研究实验室走向现实。2001年1月发生了一个备受关注的案例，当时佛罗里达州坦帕市使用面部识别系统记录并分析了超级碗
    XXXV 上每位观众的面部，希望能够发现那些有逮捕令的罪犯。该程序被认为成功地识别出了几名出席的轻微犯罪分子，但由于高昂的成本和大量的误报，整体上被视为失败。更糟的是，它引发了包括电子前沿基金会（Electronic
    Frontier Foundation）和美国公民自由联盟（American Civil Liberties Union）在内的隐私倡导团体的强烈反弹。^([1](b01.xhtml#c10-endnote-001))
- en: The negative publicity did little to slow down the growth of the technology,
    though. Florida once again made the news for being one of the first states to
    adopt facial recognition technology as an accepted tool for police. In 2009, the
    Pinellas County Sheriff’s Office announced a program that allows officers to tap
    into the photo archives of the Florida Department of Highway Safety and Motor
    Vehicles. Within two years an estimated 170 deputies had been outfitted with cameras
    that could be immediately cross-checked against the faces in the database. Since
    then, the availability of cheap processing power, shrinking data storage costs,
    and the large number of facial data sets have allowed the technology to move from
    government programs into the security programmer’s toolbox. We’ll discuss more
    about the privacy and ethical concerns in the next section, “[Ethics of Facial
    Recognition Research](#h1-502567c10-0002).”
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管负面宣传没有显著减缓这项技术的发展，但它却未能阻止技术的快速增长。佛罗里达州再次因成为首批将面部识别技术作为警察接受工具的州之一而登上新闻。2009年，皮内拉斯县警长办公室宣布了一项程序，允许警察访问佛罗里达州公路安全和机动车辆管理局的照片档案。两年内，估计有170名副警长配备了可以立即与数据库中的面部进行交叉验证的摄像头。此后，廉价的处理能力、不断降低的数据存储成本以及大量的面部数据集，使得这项技术从政府项目进入了安全程序员的工具箱。我们将在下一节
    “[面部识别研究的伦理问题](#h1-502567c10-0002)” 中讨论更多隐私和伦理方面的问题。
- en: Now you can build an effective facial recognition system for the cost of a decent
    camera (the $20 to $40 Raspberry Pi camera modules work great for this) and some
    cloud processing costs. I’m going to stay platform-agnostic with the concepts,
    but every major cloud service provider has some offering that allows you to translate
    the code we’ll write into a distributed scalable version (we’ll discuss cloud
    deployments more in [Chapter 13](c13.xhtml)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以以一台普通相机的成本建立一个有效的面部识别系统（$20 至 $40 的树莓派摄像头模块非常适合这个用途），再加上一些云处理费用。我将在概念层面保持平台无关，但每个主要的云服务提供商都有一些产品，可以让你将我们所编写的代码转换为一个分布式可扩展的版本（我们将在[第13章](c13.xhtml)中进一步讨论云部署）。
- en: The hardest part is collecting the database of images. For a good facial recognition
    data set, you need multiple pictures of the same person under different lighting
    conditions and with different facial poses. The features of the faces need to
    be distinguishable, so contrast is also important. We’ll discuss more about image
    quality in the section “[Processing Image Data](#h2-502567c10-0003)” later in
    the chapter. For now, the takeaway is that the quality of the images in the data
    set will have a drastic impact on an ML algorithm’s ability to distinguish between
    faces. Old, blurry, grainy photos with low contrast will make the process much
    harder, if not impossible.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最难的部分是收集图像数据库。为了获得一个良好的面部识别数据集，你需要在不同的光照条件和不同的面部姿势下拍摄同一个人的多张照片。面部特征需要具有可区分性，因此对比度也很重要。我们将在本章稍后的“[处理图像数据](#h2-502567c10-0003)”部分进一步讨论图像质量。目前需要记住的是，数据集中图像的质量将对机器学习算法区分面部的能力产生重大影响。旧的、模糊的、颗粒感强、对比度低的照片会让这一过程变得更加困难，甚至可能变得不可能。
- en: We’ll be using a facial recognition data set published by the computer science
    and engineering department at the University of Essex.^([2](b01.xhtml#c10-endnote-002))
    One section of images in the data (labeled *faces94*) is fairly stationary. Researchers
    had the subjects sit at a fixed distance from the camera and asked them to speak
    while a sequence of images was taken. The speech introduces facial expression
    variation, which allows the underlying classification algorithm to understand
    how the facial shape changes for an individual and gives it a better chance of
    properly classifying an input image, even if the face is in a pose the algorithm
    hasn’t seen previously. The second part of the data set (labeled *faces95*) is
    more dynamic and introduces variation in scale, perspective, and lighting by asking
    the subject to take a step toward the camera as a set of 20 images was taken with
    a fixed-placement camera. The movement forward causes the head to appear larger
    in the later photos. It also changes the cast shadow and highlights on the face.
    Finally, the background for these images is a red curtain, which also introduces
    a degree of difficulty because the imperfect surface can make it challenging for
    the algorithm to detect the edges of features. Being able to properly classify
    faces despite all this variation will allow your program to operate more reliably
    in the wild, where you may not always be able to get a clean, stable image with
    a solid, still background.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用由埃塞克斯大学计算机科学与工程系发布的面部识别数据集。^([2](b01.xhtml#c10-endnote-002)) 数据集中的一部分（标注为*faces94*）相对静态。研究人员让受试者坐在距离摄像机固定的位置，并要求他们在拍摄一系列图像的同时讲话。讲话引入了面部表情的变化，这使得底层的分类算法能够理解个体面部形状的变化，并有更大的机会正确分类输入图像，即使面部姿势是算法之前未见过的。数据集的第二部分（标注为*faces95*）则更加动态，通过要求受试者向摄像机走近，从而在拍摄一组
    20 张图像时引入了尺度、视角和光照的变化。向前移动使得后续照片中的头部看起来更大，也改变了面部的投影阴影和高光。最后，这些图像的背景是一块红色的窗帘，这也增加了难度，因为不完美的表面可能使算法在检测面部特征的边缘时面临挑战。能够在这些变化的情况下正确分类面部，将使你的程序在实际环境中更可靠地运行，因为你可能无法每次都获得一个清晰、稳定的图像，背景也可能不总是坚固或静止的。
- en: Ethics of Facial Recognition Research
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面部识别研究的伦理
- en: Before we move on, let’s talk about the ethics of facial recognition research.
    There are many potential uses for, and abuses of, being able to automatically
    identify a person based on an image of their face. Facial recognition software
    can be used by first responders to identify victims after a disaster, or it can
    be used by dictators to identify political activists. As an analyst and developer,
    you’ll have to be very cautious when dealing with facial recognition projects
    in the wild. You never really know whose hands your software may end up in.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们来谈谈人脸识别研究的伦理问题。能够根据人脸图像自动识别一个人有许多潜在的用途和滥用方式。人脸识别软件可以被急救人员用来在灾难发生后识别受害者，或者被独裁者用来识别政治活动家。作为分析师和开发人员，在处理人脸识别项目时，你必须非常小心。你永远不知道你的软件最终会落入谁的手中。
- en: To start, you should become familiar with all the privacy laws that might apply
    to your situation. International regulations like the European Union’s General
    Data Protection Regulation consider biometric data such as facial analysis models
    *personally identifiable information*, which requires stricter security controls
    around its collection, processing, and storage. Facial recognition has even been
    banned from use by police forces in some US cities, like Boston, Massachusetts,
    due to the high error rate and potential for serious repercussions in the event
    of a mistaken identity. Knowing what laws and regulations apply to your project
    will help you navigate the other ethical questions more easily. The best way to
    avoid ethical and legal troubles when it comes to privacy is to gain informed
    consent from the people being included in the data. I strongly urge you to decline
    any project where you aren’t able or allowed to collect informed consent.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你应该熟悉所有可能适用于你情况的隐私法律。像欧盟的《通用数据保护条例》这样的国际法规认为生物识别数据，如人脸分析模型，是*个人可识别信息*，因此要求在其收集、处理和存储过程中实施更严格的安全控制。由于错误率高且在身份错误识别时可能造成严重后果，一些美国城市（如马萨诸塞州的波士顿）甚至禁止警方使用人脸识别技术。了解适用于你项目的法律和法规，将帮助你更轻松地应对其他伦理问题。避免隐私方面伦理和法律问题的最佳方式是获得被纳入数据中的人的知情同意。我强烈建议你拒绝任何无法或不允许收集知情同意的项目。
- en: Aside from outright abuses, there are other ethical issues that are harder to
    spot. Racial and ethnic biases remain a key concern of developing facial recognition
    models. Facial recognition algorithms often achieve mean classification accuracy
    over 90 percent, but researchers have shown that this error rate doesn’t apply
    equally across all demographic groups. Several independent tests found the poorest
    accuracy for facial recognition was consistently for dark-skinned black females
    between the ages of 18 and 30\. These unintentional biases are the result of technological
    and social choices made by the data collectors. Decisions like the type of camera
    lens and the locations where data was collected all have a subtle but definite
    impact on the overall representation of the population in the data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 除了明显的滥用行为外，还有一些更难以察觉的伦理问题。种族和族裔偏见仍然是开发人脸识别模型时的关键关注点。人脸识别算法通常能达到90%以上的平均分类准确率，但研究人员已表明，这一错误率在不同人口群体中并不均等。几项独立测试发现，面部识别的最低准确率通常出现在18至30岁之间、肤色较深的黑色女性身上。这些无意的偏见是数据收集者在技术和社会选择中所做决策的结果。比如相机镜头类型、数据收集地点等决策，都会对数据中人口的整体代表性产生微妙但明确的影响。
- en: I’ve included this facial recognition project here, despite these ethical concerns,
    because I think it presents an excellent learning opportunity. We’ll be using
    publicly available data that was gathered with the consent of the individuals
    and the knowledge that the images would be used for research like ours, which
    is perfectly ethical. The data set I selected is relatively small and represents
    a limited number of demographics. This could lead us to make falsely optimistic
    performance predictions, so we wouldn’t want to use it for developing any type
    of production system. It will serve as a good starting point, though. We can use
    it to illustrate the workflow and even test some parts of the algorithm. When
    you’re ready to develop a facial recognition system for a real project, you’ll
    have the knowledge and tools to collect a truly great data set that accurately
    reflects the diversity of humankind.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些伦理问题，我仍然将这个面部识别项目放在这里，因为我认为它提供了一个极好的学习机会。我们将使用公开可用的数据，这些数据是在个人同意并知晓图像将用于像我们这样的研究的情况下收集的，这是完全符合伦理的。我选择的数据集相对较小，代表了有限数量的人群。这可能导致我们做出过于乐观的性能预测，因此我们不希望将其用于开发任何类型的生产系统。不过，它会作为一个很好的起点。我们可以用它来说明工作流程，甚至测试算法的某些部分。当你准备为一个真实项目开发面部识别系统时，你将拥有知识和工具来收集一个真正能准确反映人类多样性的数据集。
- en: The Facial Recognition Algorithm
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面部识别算法
- en: 'Despite the fact that we’re using ML algorithms, the core of the facial recognition
    process remains remarkably similar to the one Bledsoe created more than 50 years
    ago. We’ll use a set of 68 facial points to create geometric maps for a database
    of faces, then use those maps to train an ML algorithm to compare an input face
    with previously seen faces and predict the closest match. Generally speaking,
    facial recognition is a *computer vision* problem: it deals with teaching computers
    to recognize information encoded in visual data, like pictures and video. This
    also falls into the broader category of *multiclass classification* problems,
    where the class to be predicted is from a set of three or more potential classifications.
    When a multiclass algorithm runs, it compares the input to the data recorded for
    each class and determines which class the input is most likely to belong to. We’re
    going to treat each individual person in the data set as a class we’re interested
    in learning to predict, and the input will be an image containing the face of
    a previously analyzed person. The algorithm we’ll use is a supervised learning
    algorithm; again, this means that when we train our algorithm, or model, it will
    have access to a list of the correct classifications, which it will use to correct
    previous mistakes and improve future predictions.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们使用的是机器学习算法，但面部识别过程的核心与Bledsoe在50多年前创建的过程仍然非常相似。我们将使用68个面部特征点来为面部数据库创建几何图，随后利用这些图来训练机器学习算法，将输入的面部图像与之前见过的面部进行比较，并预测最接近的匹配。一般而言，面部识别是一个*计算机视觉*问题：它涉及教计算机识别编码在视觉数据中的信息，如图片和视频。这也属于*多类分类*问题的范畴，其中需要预测的类别来自三种或更多的潜在分类。当多类算法运行时，它会将输入与为每个类别记录的数据进行比较，并确定输入最可能属于哪个类别。我们将把数据集中的每个人当作我们感兴趣的类别，输入则是包含已分析过的个体面部的图像。我们将使用的算法是监督学习算法；再次强调，这意味着在训练我们的算法或模型时，它将访问正确分类的列表，并利用该列表纠正先前的错误并改进未来的预测。
- en: There’s a large number of potential classes (222 unique individuals in the final
    analysis) and a relatively small sample size for each class (approximately 20
    images per individual), making our goal even more difficult. To counter this,
    we’ll collect a huge number of statistics for each image and let the algorithm
    decide which subset of these measurements allows for the best decision-making
    power.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 存在大量潜在类别（最终分析中有222个独特个体），而每个类别的样本量相对较小（每个个体约有20张图片），这使得我们的目标更加困难。为了解决这个问题，我们将为每张图片收集大量统计数据，并让算法决定哪些测量子集能够提供最佳的决策能力。
- en: Using Decision Tree Classifiers
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用决策树分类器
- en: 'The classification is handled by an algorithm called a *random forest classifier*,
    which is an expanded version of a *decision tree classifier*. There are many benefits
    to decision tree algorithms: they are fast to train, can produce a human-readable
    model, and perform well for multiclass problems like facial recognition. Let’s
    examine a classic example as a way of illustrating how they work. Suppose we want
    to write a program that predicts if a person is likely to go out and play a round
    of golf on a given day, based on the weather. A decision tree would be a great
    choice for this type of problem because it will generate a list of rules we can
    use to examine the weather on any given day. Consider the decision tree in [Figure
    10-1](#figure10-1).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是通过一个叫做*随机森林分类器*的算法处理的，它是*决策树分类器*的扩展版本。决策树算法有许多优点：它们训练速度快，能生成易于理解的模型，并且在面部识别等多分类问题中表现良好。我们通过一个经典示例来说明它们是如何工作的。假设我们想编写一个程序，根据天气预测某人在某一天是否可能出去打高尔夫球。决策树是这类问题的理想选择，因为它将生成一系列规则，我们可以用这些规则来检查某一天的天气情况。请参考[图
    10-1](#figure10-1)中的决策树。
- en: '![](image_fi/502567c10/f10001.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502567c10/f10001.png)'
- en: 'Figure 10-1: An example of a golf decision tree'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-1：高尔夫决策树示例
- en: You can see in [Figure 10-1](#figure10-1) that each branch represents a Boolean
    decision in the data (such as `Outlook_overcast <= 0.5`). To read the tree, you
    start at the topmost box (called the *root node*) and follow the proper logical
    branches until you reach the bottommost box (called a *leaf* or *leaves*). Some
    statistics about the underlying data that generated the decision are listed below
    the Boolean decision. Each row (called an *instance* in data science parlance)
    is sent through the decision tree one at a time until it ends up at one of the
    leaves; along the way, the algorithm records how the data influenced the growth
    of the tree. The simplest statistic is `samples`. This is the total number of
    rows that reached this point in the decision tree during creation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 10-1](#figure10-1)中，你可以看到每个分支表示数据中的一个布尔决策（例如`Outlook_overcast <= 0.5`）。阅读决策树时，从最顶部的框（称为*根节点*）开始，沿着正确的逻辑分支向下，直到到达最底部的框（称为*叶节点*或*叶子*）。一些关于生成决策的底层数据统计信息会列在布尔决策下方。每一行（在数据科学术语中称为*实例*）都会一次性通过决策树，直到它到达某个叶节点；在这个过程中，算法会记录数据如何影响树的生长。最简单的统计信息是`samples`。这是在创建过程中，经过此决策点的总行数。
- en: In [Figure 10-1](#figure10-1) you can see that the root node received 14 samples
    of data. Since this is the root node, it processed every row in the data set,
    so this can also be interpreted as a summary of the data at the start of the algorithm.
    You can see the count of each class in the `values` statistic. For each possible
    class in the data, there’s an integer representing the number of rows within that
    particular class. In our example there are two potential classes, `Not_Play` and
    `Play`. Looking at the root node again, you can see the values `[5,9]`, which
    means five of those samples belonged to the `Not_Play` class and nine belonged
    to the `Play` class.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 10-1](#figure10-1)中，你可以看到根节点收到了14个数据样本。由于这是根节点，它处理了数据集中的每一行，因此这也可以被解释为算法开始时的数据摘要。你可以看到`values`统计中的每个类别的计数。对于数据中每个可能的类别，有一个整数表示该类别中的行数。在我们的示例中，有两个潜在的类别，`Not_Play`和`Play`。再次查看根节点，你可以看到值`[5,9]`，这意味着这14个样本中，有5个属于`Not_Play`类别，9个属于`Play`类别。
- en: The `gini` statistic contains the *Gini impurity* coefficient, which you can
    think of as a measure of the distribution of classes that reach that particular
    node, called the *purity* of the node. Formally, the Gini impurity coefficient
    can be written as
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`gini`统计信息包含*基尼不纯度*系数，你可以将其看作是到达该节点的类别分布的度量，称为该节点的*纯度*。正式地，基尼不纯度系数可以写作'
- en: '![](image_fi/502567c10/m10001.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502567c10/m10001.png)'
- en: where *n* is the number of classes in the data and *p*[*i*] is the probability
    of an instance being classified to the *i*th class. The resulting score for a
    node can range between 0 and 1\. If all the instances in a node belong to a single
    class, then it is completely pure and will receive a Gini score of 0\. A score
    of 1 means that the instance classes are randomly distributed and there’s no predictability.
    Scores between these two extremes denote some level of class purity, with lower
    scores being purer (and therefore better for decision-making purposes) than higher
    ones. The goal is to find pure leaf nodes that contain only one class of data
    (a Gini score of 0). That would mean the logic that led to that leaf was capable
    of making a perfect decision between classes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *n* 是数据中类别的数量，*p*[*i*] 是实例被分类为第 *i* 类的概率。节点的结果得分范围在 0 到 1 之间。如果节点中的所有实例都属于单一类别，则该节点是完全纯净的，Gini
    得分为 0。得分为 1 表示实例类别是随机分布的，无法预测。介于这两个极端之间的得分表示某种程度的类别纯度，得分越低，纯度越高（因此对于决策目的越好）。目标是找到只包含单一类别数据的纯叶节点（Gini
    得分为 0）。这意味着导致该叶节点的逻辑能够在类别之间做出完美的决策。
- en: To see an example of this, follow the `False` branch from the root node (down
    and to the right in [Figure 10-1](#figure10-1)). You can tell this is a leaf node
    because there’s no Boolean expression at the top of the box and there are no branches
    extending from it. The `class` statistic shows the majority class for each node;
    when dealing with a leaf node like this, we can think of it as the probable class
    for data that reaches that point. With all that in mind, we can interpret this
    branch logically as “if the weather is overcast, predict the `Play` class” since
    we’re dealing with Boolean values (0 or 1) and the decision criteria is `Outlook_overcast
    > 0.5`. The `samples` and count and the `values` statistic show that four samples
    reached this leaf node, all of which were of the `Play` class.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看这个例子，请沿着根节点的`False`分支（在[图 10-1](#figure10-1)中向下和向右）进行操作。你可以通过观察没有布尔表达式的框顶部分，并且没有从中延伸出来的分支来判断这是一个叶节点。`class`统计量显示了每个节点的多数类；在处理这样的叶节点时，我们可以将其视为到达该点的数据的可能类别。考虑到这一点，我们可以逻辑地解读这个分支为“如果天气是阴天，则预测`Play`类”，因为我们处理的是布尔值（0或1），而决策标准是`Outlook_overcast
    > 0.5`。`samples`和计数以及`values`统计量显示有四个样本到达了这个叶节点，且所有样本都是`Play`类。
- en: In [Figure 10-1](#figure10-1), none of the leaf nodes have more than a single
    class in them, making this a perfectly pure tree. Of course, more often than not,
    this isn’t the case, and one or two stragglers from other classes (called *outliers*)
    may show up in a leaf where the majority is of another class. In these cases,
    you can try to find additional data splits that would improve the purity of each
    leaf, but at some point you’ll have to accept the performance as “good enough”
    since it’s unlikely you’ll find a perfect split.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 10-1](#figure10-1)中，所有叶节点都只有单一类别，因此这是一个完全纯净的树。当然，通常情况下并非如此，可能会有一两个来自其他类别的异常值（称为*离群点*）出现在一个叶节点中，而该叶节点的主要类别却是另一类。在这种情况下，你可以尝试找到更多的数据划分，以提高每个叶节点的纯度，但最终你必须接受性能为“足够好”，因为很难找到一个完美的划分。
- en: In the context of our facial recognition problem, we’ll classify an input face
    by converting it to geometric information and then sending that information through
    the decision tree until it lands at a leaf node, which will use the majority class
    to predict the likely subject who matches the input face. Although the Boolean
    decisions the algorithm makes are more complicated than `Outlook_overcast > 0.5`,
    the principle remains the same.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的面部识别问题中，我们将通过将输入的面部信息转换为几何信息，然后将这些信息传递到决策树中，直到到达叶节点，在该叶节点使用多数类来预测最可能匹配输入面部的主体。虽然算法所做的布尔决策比`Outlook_overcast
    > 0.5`更复杂，但原则是相同的。
- en: The problem with a traditional decision tree is that it’s susceptible to the
    data’s initial conditions and configuration because it processes the samples in
    order. Rerunning the same decision tree algorithm on a shuffled version of the
    same data is likely to produce a significantly different tree each time. This
    means if you plan to use a decision tree, you’ll need to train several versions
    with different mixtures of data to make sure the performance is repeatable (in
    this context, *performance* refers to the accuracy of predicting each class).
    This led researchers to design random forests. A random forest algorithm repeatedly
    creates individual decision trees with semi-randomized starting data (called *bagging*).
    To classify a new data sample, the instance is sent down each generated tree and
    the resulting class predictions are tallied. Finally, the majority class from
    all the trees’ guesses is predicted as the most likely classification.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 传统决策树的问题在于，它容易受到数据初始条件和配置的影响，因为它按顺序处理样本。对同一数据的洗牌版本重新运行相同的决策树算法，很可能每次都会生成显著不同的树。这意味着，如果你计划使用决策树，你需要用不同的数据组合训练多个版本，以确保性能是可重复的（在此上下文中，*性能*指的是预测每个类别的准确度）。这促使研究人员设计了随机森林。随机森林算法通过半随机化的起始数据（称为*自助法*）反复创建单独的决策树。为了分类一个新的数据样本，该实例会沿着每棵生成的树传递，结果的类别预测会被统计。最后，所有树的猜测中多数的类别被预测为最可能的分类。
- en: Having a large number of decision trees generated from different data mixtures
    will help ensure that the overall prediction is less susceptible to the starting
    condition of any given tree. We’ll discuss the random forest more once we start
    building the model in the proof of concept, but before we get there, we need to
    cover exactly how we’re going to collect the data we need. Let’s turn to converting
    a picture of a face into a set of geometric data. In the next section, we’ll discuss
    how to find important facial features and convert them into numeric representations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过不同数据组合生成大量决策树将有助于确保整体预测不太容易受到任何给定树的起始条件的影响。我们将在开始构建概念验证模型时更详细地讨论随机森林，但在此之前，我们需要先确定如何收集所需的数据。让我们先来看看如何将一张面部图片转化为一组几何数据。在接下来的部分，我们将讨论如何找到重要的面部特征并将它们转换为数值表示。
- en: Representing Facial Geometry
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 表示面部几何形状
- en: The first step in defining our facial recognition application is to figure out
    how to divide an image of a face into measurable shapes. I mentioned before that
    we’ll use 68 points in an image to mark the features of the face. To save on development
    time and achieve our goal with less upfront coding, we’ll leverage a previously
    trained ML model, `shape_predictor_68_face_landmarks` ([http://dlib.net](http://dlib.net))^([3](b01.xhtml#c10-endnote-003)),
    which identifies the points of interest in a frontal view of a human face. [Figure
    10-2](#figure10-2) shows the 68 points laid approximately where they would fall
    on a face.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 定义我们的面部识别应用程序的第一步是弄清楚如何将一张面部图像划分为可测量的形状。我之前提到过，我们将使用图像中的 68 个点来标记面部特征。为了节省开发时间并通过更少的前期编码实现我们的目标，我们将利用一个先前训练好的机器学习模型，`shape_predictor_68_face_landmarks`（[http://dlib.net](http://dlib.net))^([3](b01.xhtml#c10-endnote-003))，它可以识别正面人脸上的兴趣点。[图
    10-2](#figure10-2)显示了大致标出在面部上应落的位置的 68 个点。
- en: '![](image_fi/502567c10/f10002.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502567c10/f10002.png)'
- en: 'Figure 10-2: The points of interest on a face, generated by an algorithm (image
    source: [https://i.stack.imgur.com/OBgDf.png](https://i.stack.imgur.com/OBgDf.png))'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-2：由算法生成的面部兴趣点（图片来源：[https://i.stack.imgur.com/OBgDf.png](https://i.stack.imgur.com/OBgDf.png)）
- en: The map of geometric features includes the jawline (points 0–16), left and right
    eyebrows (points 17–21 and 22–26, respectively), nose (points 27–30 for the bridge
    and 31–35 for the base), left and right eyes (points 36–41 and 42–47, respectively),
    and the mouth (points 48–59 for the exterior of the lips and 60–67 for the interior).
    When the algorithm receives an image of a face, it adjusts the locations of each
    point to try to match the positioning of the input face. We’ll use the adjusted
    locations of these points to create Shapely shapes representing the different
    features. We’ll then compute some geometric statistics about the face, such as
    the distance between the eyes, the length of the nose, and so on, to create a
    statistical representation of the face. [Listing 10-1](#listing10-1) shows how
    to load the model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Listing 10-1: Loading the facial landmark detector'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: The model is part of the dlib library, which wraps several C++ functions in
    Python goodness so you can leverage the speed of C++ for scientific computation
    and the friendly syntax of Python for everything else. The `get_front_face_detector`
    function returns a previously trained model for detecting faces in images based
    on a method known as *histogram of oriented gradients (HOG)*. The detector counts
    occurrences of gradient orientation in localized portions of an image, meaning
    it examines only a small box of pixels at a time; this is similar to the way a
    human might examine a picture with a magnifying glass to focus on detailed areas
    (except there’s no distortion of the pixels in this case). The output of the `get_front_face_detector`
    function is a list of `(``index``,` `rectangle_coordinates``)` tuples, one for
    each detected face. We store this information in a variable named `detector`,
    which we’ll use to help the predictor focus in on the faces. The actual facial
    feature location is handled by a shape predictor that takes in an image region
    containing some object (in this case, a face) and outputs a set of point locations
    that define the pose of the object. To load the predictor, we tell dlib the path
    to the model we’re interested in, which, in this case, is *facial_model/shape_predictor_68_face_landmarks.dat*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: With the face detector and landmark detector components defined, we’re ready
    to start processing images. It’s very important to the accuracy and reliability
    of your facial recognition system (and any other predictive algorithms, for that
    matter) that you process the test images exactly the same as the training images;
    otherwise, the difference in processing may corrupt the results in unpredictable
    ways. The next section will describe a modular bit of code we can use in the proof
    of concept to handle both the training data creation and the test image processing
    independent of the modeling functions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Processing Image Data
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many possible ways to process image data so that predictive algorithms
    can build a model based on their information; they all share the same goal, however,
    of converting the data into a normalized format. The way you intend to predict
    faces has a lot of influence on what processing steps, if any, you should take
    before converting the picture into a feature set. For example, you may choose
    to preserve color information by creating a predictor for each of the three color
    channels (red, green, and blue) individually, in which case you wouldn’t want
    to convert the image to grayscale, as we do here. Other operations are fairly
    common regardless of the final processing plan. Operations such as cropping the
    image to the facial region or resizing the image help ensure that all samples
    are consistently scaled and the features are somewhat normalized in the end. [Listing
    10-2](#listing10-2) shows the function for processing images in the *.jpeg* file
    format.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 处理图像数据的方法有很多种，这样预测算法就能基于其信息构建模型；然而，它们都有一个共同的目标，那就是将数据转换为标准化格式。你打算如何预测人脸，决定了在将图片转换为特征集之前，是否需要进行某些处理步骤。例如，你可以选择通过为每个颜色通道（红色、绿色和蓝色）分别创建预测器来保留颜色信息，这样的话你就不需要像这里一样将图像转换为灰度图像。无论最终的处理计划如何，一些操作都是相当常见的。诸如将图像裁剪为面部区域或调整图像大小等操作有助于确保所有样本一致地缩放，并且特征最终会在一定程度上标准化。[清单
    10-2](#listing10-2) 显示了处理 *.jpeg* 文件格式图像的函数。
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Listing 10-2: A function for processing a single *.jpeg* image'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 10-2：处理单个 *.jpeg* 图像的函数
- en: We start by defining the `process_jpg` function. The only parameter we need
    is the path to the *.jpeg* image, stored in `file_path`. We use the cv2 (short
    for *computer vision 2*) library’s `imread` function to read the file into a data
    array representing the pixel values for each color channel. Then we resize the
    image data using the `imutils.resize` function. We scale the images so they have
    a width of 300 pixels using the `width` parameter; the height of the image will
    be calculated based on this new width to avoid distorting the features. Finally,
    we convert the resized image data to grayscale using the `cv2.cvtColor` function
    and return the result. (I chose to convert the image data to grayscale since color
    information won’t help inform our geometric analysis.)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义了 `process_jpg` 函数。我们需要的唯一参数是 *.jpeg* 图像的路径，存储在 `file_path` 中。我们使用 cv2（*计算机视觉
    2* 的缩写）库的 `imread` 函数将文件读取为一个数据数组，表示每个颜色通道的像素值。然后，我们使用 `imutils.resize` 函数调整图像数据的大小。我们将图像缩放到宽度为
    300 像素，使用 `width` 参数；图像的高度会根据新的宽度计算，以避免扭曲特征。最后，我们使用 `cv2.cvtColor` 函数将调整大小后的图像数据转换为灰度图像，并返回结果。（我选择将图像数据转换为灰度图像，因为颜色信息不会帮助我们进行几何分析。）
- en: '[Figure 10-3](#figure10-3) shows an example result.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-3](#figure10-3) 显示了一个示例结果。'
- en: '![](image_fi/502567c10/f10003.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502567c10/f10003.png)'
- en: 'Figure 10-3: Processed facial images ready for analysis'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-3：处理过的面部图像，准备进行分析
- en: You can see that we get back grayscale images scaled to 300 pixels wide by 450
    pixels tall. Note that the facial features aren’t distorted; this is because of
    the scaling method we used. We’ll use these two images to exemplify the rest of
    the process. Both images have decent contrast and the facial features are not
    obstructed (by things like sunglasses, hats, or heavy makeup), making them good
    candidates. Both also have some areas that will ultimately prove more difficult
    for the algorithm, as you’ll see.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们返回的是宽 300 像素、高 450 像素的灰度图像。注意，面部特征没有被扭曲；这是因为我们使用的缩放方法。我们将使用这两张图像来举例说明接下来的过程。两张图像的对比度都不错，面部特征没有被遮挡（如墨镜、帽子或浓妆），因此它们是很好的候选图像。同时，两张图像也有一些区域最终会证明对算法来说更具挑战性，正如你所看到的那样。
- en: Resizing the images and adjusting the color to grayscale before we continue
    processing them will allow us to get consistent, repeatable samples to work from,
    regardless of slight differences in picture quality, scale, and lighting. These
    steps are just the beginning. You should consider other image transformations
    you could apply (such as increasing the brightness or playing with the contrast
    values) to give the algorithm the best chance of success when analyzing the image.
    In the next section, we’ll take our processed images and begin the work of actually
    locating and analyzing the facial features.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Locating Facial Landmarks
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we’ve defined our image processing step as a function, we can call
    it at the beginning of our facial landmark code to ensure that we’re working on
    a grayscale version of the resized image. Our next task, shown in [Listing 10-3](#listing10-3),
    is to write the function that will locate the facial landmarks we’ll use to define
    the rest of the facial structure.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Listing 10-3: Locating feature landmarks in an image'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: The `locate_landmarks` function takes in a filepath as its only argument. Then,
    we start by calling the `process_jpg` function from [Listing 10-2](#listing10-2)
    to get the processed image from the file argument ❶. I prefer to work on a copy
    of the image ❷ to avoid accidentally overwriting the source with any changes.
    Once we’ve copied the image, we locate the rectangular region for each face in
    the input using the `detector` we created in [Listing 10-1](#listing10-1) ❸. The
    result is a list of tuples containing the index of the face and the coordinates
    corresponding to the rectangular area that contains it. For the sample data there
    will only ever be one rectangle in the list, but you could extend the function
    to handle the case of multiple faces for a future project.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Next, we loop over the list of rectangles ❹ and send each to the `predictor`
    we also set up in [Listing 10-1](#listing10-1) ❺. The result is the list of point
    coordinates in the order shown in [Figure 10-1](#figure10-1). We use the function
    `shape_to_np` ❻, which is a simple helper function to convert the shape’s (*x*,
    *y*) coordinates into a NumPy array.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here we create a NumPy array of zeros, named `coords`, to hold the 68 coordinate
    pairs. Next, we loop over all of the indices in the `shape.part` list. For each
    part we create a tuple from the `x` and `y` attributes and assign the tuple to
    the `coords` array at the same index. Once we’ve collected all the coordinate
    pairs, we return the resulting array of tuples.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Back in [Listing 10-3](#listing10-3), to make things easier going forward, we’ve
    built a dictionary that is keyed off the feature name and contains the list of
    points defining the feature. The `face_utils.FACIAL_LANDMARKS_IDXS.items` function
    ❼ returns a list of tuples that conveniently provides the feature name and a nested
    tuple that defines the start and end index in `shape` that correspond to the feature.
    We loop over each of these definitions and create a corresponding entry in the
    `feature_coordinates` dictionary ❽. If there’s more than one face in the picture,
    the index number of the face will be appended to the feature name to keep them
    separated.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在[清单10-3](#listing10-3)中，为了方便后续操作，我们构建了一个以特征名称为键的字典，字典中包含定义该特征的点列表。`face_utils.FACIAL_LANDMARKS_IDXS.items`函数❼返回一个包含元组的列表，该元组提供了特征名称和定义`shape`中对应特征的起始和结束索引的嵌套元组。我们遍历这些定义，并在`feature_coordinates`字典中创建相应的条目❽。如果图像中有多张人脸，面部的索引编号将附加到特征名称后面，以便区分它们。
- en: Next, we create a list of all the points in the face data and append it to a
    list ❾; this will be used to calculate the convex hull of the face in the proof
    of concept. Formally speaking, a *convex hull* is the smallest convex polygon
    that encloses a set of points such that each point in the set lies within the
    polygon or on its perimeter. Remember from [Chapter 7](c07.xhtml) that a convex
    polygon is one where all its interior angles are less than 180 degrees. You can
    think of convex hulls as the result you’d get if you could stretch a rubber band
    around the outside of all the points in a shape.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个包含所有面部数据点的列表，并将其附加到一个列表中❾；这个列表将在概念验证中用于计算面部的凸包。从形式上讲，*凸包*是包含一组点的最小凸多边形，使得该集合中的每个点都位于该多边形内部或其边界上。记得在[第7章](c07.xhtml)中提到，凸多边形是所有内部角小于180度的多边形。你可以将凸包想象成如果你把一根橡皮筋拉紧围绕所有点的外部，你所得到的结果。
- en: Finally, we add the resulting list to the `feature_coordinates` dictionary ❿.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将生成的列表添加到`feature_coordinates`字典中❿。
- en: '[Figure 10-4](#figure10-4) shows the results of running the algorithm against
    our two test images.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-4](#figure10-4)展示了对我们两张测试图像运行算法的结果。'
- en: '![](image_fi/502567c10/f10004.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502567c10/f10004.png)'
- en: 'Figure 10-4: The results of landmark detection'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-4：地标检测结果
- en: The gray dots are located where the algorithm believes each feature of the face
    is placed. The black outline around the entire facial region represents the resulting
    convex hull. As you can see, the program did a decent job. For the most part,
    the dots correctly hit the landmarks we’re looking for. Where the program fell
    short is finding the jawline on the woman in the left image, because the contrast
    between her dark hair and the background is more defined than the difference between
    her jawline and the background. The measurements of her face are going to be skewed,
    which, unless they’re skewed the exact same way every time, will result in bad
    training data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 灰色的点表示算法认为每个面部特征的位置。整个面部区域的黑色轮廓表示生成的凸包。如你所见，程序做得相当不错。大部分情况下，点正确地定位了我们要寻找的地标。程序表现不佳的地方是无法找到左图中女性的下巴线，因为她的深色头发与背景的对比度比下巴线与背景的对比度更加明显。她面部的测量值会出现偏差，除非每次偏差的方式完全相同，否则会导致不良的训练数据。
- en: The algorithm did a much better job finding the man’s jawline, even though it’s
    hidden behind his beard. But the algorithm has failed to find the edges of his
    nose properly. It’s a fairly small difference from the actual position, however,
    and though it will impact a few measurements, the overall face shape remains in
    proportion and thus would generate usable training data. After a few months of
    doing these tests and examining the results, you’ll be able to look at an image
    and closely estimate how well the landmark detector will perform given the processing
    steps you define.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个人的下巴被胡须遮住，算法还是更好地找到了下巴线。但算法未能正确找到鼻子的边缘。虽然与实际位置之间的差异相当小，而且虽然这会影响一些测量值，但整体面部形状保持了比例，因此生成的训练数据仍然可用。在进行这些测试并检查结果几个月后，你将能够查看一张图像，并大致估算出根据你定义的处理步骤，地标检测器的表现如何。
- en: 'After calling the `locate_landmarks` function, you should get back the dictionary
    keyed by the feature name, which allows you to refer to the collection of points
    using a human-friendly name. For example, to create polygons representing the
    left and right eyes, you can use this code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can then use Shapely to measure the minimum distance between the shapes:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Or you could measure the difference in area between the two eyes:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: There are many other potential characteristics you could use, but we’ll dive
    into those more when we start to build the training data set in the next section.
    Now that we’ve defined the functions we need, we can start building the proof
    of concept for our facial recognition system.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'The Proof of Concept: Developing a Facial Recognition System'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The proof of concept for this project is separated into two parts. The first
    part builds the training data from the set of facial images. Here, we’ll prepare
    each image for processing and define the statistics to collect from it. This is
    where computational geometry is going to help us. The image processing steps may
    take several minutes (or longer with much larger data sets), so it makes sense
    to compute this on its own and store the results to a file for processing later.
    This spares us from having to run expensive calculations over and over. It also
    makes adding new images to the data set easier. Rather than rebuilding the entire
    data set to retrain the model, we only need to process the new images before retraining.
    Be warned: trying to process all of these images in memory isn’t going to work.
    We’ll need to process the files one at a time to keep memory usage manageable.
    You’ll see how later in the chapter.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: The second portion of the proof of concept defines the ML algorithm and trains
    it on the previously computed statistical data. We’ll test the algorithm multiple
    times using a cross-validation process called *leave one out (LOO)*. We’ll run
    the validation once for each class in the data, during which the LOO algorithm
    selects an image from that class to hold out from the training data (hence the
    name). The model is then trained on the rest of the data. After training the model,
    we’ll give it the selected image to classify, then total the results for each
    class to estimate overall performance. The major benefit of the LOO method of
    validation is that it provides the training algorithm with the most information
    because only a single instance is removed before training. Since we have a limited
    number of images for each face, we need to give the training algorithm the best
    chance possible to succeed.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Facial Statistics
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first part of the proof of concept is in the *facial_recognition_poc_1.py*
    file. It covers the code you’ve seen up to now, but we’ll expand on it to create
    the final training data set. There are a number of ways you could approach collecting
    statistical information about the facial structure. My first attempts involved
    tessellating the face in different ways and measuring the predictive power of
    different sections. [Figure 10-5](#figure10-5) shows the method that scored the
    best, applied to the two example faces.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502567c10/f10005.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-5: A result from automated facial tessellation'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: The tessellation treated the nose, eyes, and inner mouth portions as holes in
    the main facial polygon. The primary problem with this method turned out to be
    too much noise from similar triangles that didn’t contribute to the structural
    knowledge at all (for example, all of the triangles making up the chin); this
    created a large number of similar-valued variables across all the faces. Another
    problem with this method is that tessellation doesn’t include all the shapes of
    interest. For example, creating a triangle from the outer points of both eyes
    and the bottom of the chin would indicate the tilt of the entire head. Why am
    I bothering to tell you about this if it didn’t work? Because it’s important to
    realize that trial and error is necessary. Thinking about why a method failed
    can be more informative than considering why it works!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'During my research I found a paper from the Facial Identification Scientific
    Working Group (FISWG) that does an excellent job describing the standard facial
    statistics.^([4](b01.xhtml#c10-endnote-004)) Ultimately, I changed tactics from
    automatic tessellation to explicitly defining 62 measures taken mostly from the
    reference material. To help with defining the measurements in the code, you can
    create some variables that represent key points by name. For example:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can use these points to create the measurements in a way that allows you
    to return to the code months later and still understand which variables relate
    to which points of the face. [Listing 10-4](#listing10-4) shows a sample of the
    metrics from the proof of concept.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Listing 10-4: Defining the geometric statistics with Shapely'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The three major types of statistics we collect are areas, distances, and ratios.
    Area metrics convert a set of facial points into a polygon object and then record
    the `area` property of the shape ❶. Distance metrics create a `LineString` object
    from two or more points and then record the `length` property ❷. Ratios are derived
    metrics that compare the values of two previously created metrics. For example,
    here we’ve compared the length of the line between the upper lip and bottom of
    the nose (colloquially called the Cupid’s bow) to the total vertical height of
    the face (measured from the chin to the top of the nose) ❸. Ratios should compare
    only statistics of like types. The ratio of one area to another area or the length
    of one distance to another makes sense, but the ratio of a distance to an area
    doesn’t make much sense in this context.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: On top of the 62 explicit metrics, I’ve included the x-coordinate and y-coordinate
    as separate features in the data for a total of 214 data points per image. We’ll
    let the model-building algorithm determine which of these features are most informative
    in the “[Feature Engineering](#h2-502567c10-0008)” section of this chapter, but
    for now, defining more statistics means a higher chance of finding some meaningful
    ones.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Memory Management
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned previously, trying to process all of these images in memory isn’t
    feasible. Images contain a large amount of information, and trying to open a large
    number at the same time for processing will quickly fill your memory buffers.
    Instead, the proof of concept opens a single image at a time and computes all
    the statistics. It saves the data and immediately closes the image to move on
    to the next. [Listing 10-5](#listing10-5) shows the structure of the loop.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Listing 10-5: Looping over image files for processing'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: The `get_image_files` function is another helper function in the proof of concept
    that recursively walks a directory structure collecting filenames that don’t end
    with *.txt* ❶. Once we’ve done that, we loop over each filepath in the resulting
    list and pass it to the `locate_landmarks` function we defined in [Listing 10-3](#listing10-3)
    ❷. Some images in the data set aren’t clear enough for the landmark detector to
    find all the features. In these cases, the feature dictionary won’t have the proper
    number of keys, and we can skip any further processing ❸. The snipped section
    is where we’ll add the code to create all of the facial statistics using the method
    shown in [Listing 10-4](#listing10-4).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Once all the data points are created, we convert the dictionary into a pandas
    `Series` object ❹ and append it to the collection of faces. After all the images
    are processed, we create a `DataFrame` from the list of `Series` objects ❺. Finally,
    we save the results to a *.csv* file for later use ❻. Running the script will
    conclude the first portion of the proof of concept by creating a data set derived
    from the geometric statistics. We’ll use this data to train the classifier we
    develop in the next two sections.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有数据点创建完成，我们将字典转换为 pandas `Series` 对象 ❹，并将其附加到人脸集合中。所有图像处理完成后，我们从 `Series`
    对象列表中创建一个 `DataFrame` ❺。最后，我们将结果保存到 *.csv* 文件中，以供以后使用 ❻。运行脚本将通过创建一个由几何统计数据派生的数据集来结束概念验证的第一部分。我们将使用这些数据来训练接下来两部分中开发的分类器。
- en: Data Loading
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据加载
- en: At this point we’ve created our geometric data set and saved it to a *.csv*.
    The second stage of the proof of concept is located in the file *face_recognition_poc_2.py*
    and picks up with loading this previously created data into pandas. [Listing 10-6](#listing10-6)
    shows how to load the *facial_geometry.csv* and prepare the data for the association
    calculations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经创建了几何数据集并将其保存为 *.csv* 文件。概念验证的第二阶段位于文件 *face_recognition_poc_2.py*
    中，从加载之前创建的数据到 pandas 开始。[示例 10-6](#listing10-6) 展示了如何加载 *facial_geometry.csv*
    并为关联计算准备数据。
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Listing 10-6: Preparing the *facial_geometry.csv* data for training'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 10-6：为训练准备 *facial_geometry.csv* 数据
- en: We start by calling the pandas `read_csv` function to get the data from the
    previous step ❶. The creation of the file results in an `Unnamed` index row, which
    we drop to save memory ❷. The next step is to define the `category` variable.
    The `name` field contains a randomly assigned fake name to make the data realistic
    while still preserving the data subjects’ privacy. This is the column we’re interested
    in building a model to predict, but pandas treats it as a text string by default,
    so we convert it to a categorical column using the `astype` function with the
    type `category` ❸. We collect the name of all the categorical columns using the
    `select_dtypes` function. The result is a list of column names in `faces_df` that
    have the type `category` ❹. Currently the `category` column should be the only
    result in the list, but it still makes it easier to reference all categorical
    variables this way should you choose to add more categorical information in the
    future.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先调用 pandas 的 `read_csv` 函数来获取上一阶段的数据 ❶。文件创建后会生成一个 `Unnamed` 索引行，我们将其删除以节省内存
    ❷。下一步是定义 `category` 变量。`name` 字段包含一个随机分配的假名，以使数据看起来真实，同时仍然保护数据主体的隐私。这一列是我们感兴趣的预测目标，但
    pandas 默认将其视为文本字符串，因此我们使用 `astype` 函数将其转换为 `category` 类型的分类列 ❸。我们使用 `select_dtypes`
    函数收集所有分类列的名称。结果是 `faces_df` 中具有 `category` 类型的列名列表 ❹。目前 `category` 列应该是唯一的结果，但通过这种方式引用所有分类变量会更加方便，如果将来你希望添加更多分类信息。
- en: Because pandas automatically assigns a numeric index to each category derived
    from a categorical column, we overwrite the content of the `category` column with
    that numeric index using the `apply` function ❺. For convenience, we create a
    lookup table so we can convert category IDs to names or vice versa by copying
    the `name` and `category` columns from `faces_df` and saving them into another
    `DataFrame` object called `name_map` ❻.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 pandas 会自动为从分类列派生的每个类别分配一个数字索引，我们使用 `apply` 函数用该数字索引覆盖 `category` 列的内容 ❺。为了方便起见，我们创建了一个查找表，以便通过复制
    `faces_df` 中的 `name` 和 `category` 列并将它们保存到另一个 `DataFrame` 对象 `name_map` 中来转换类别
    ID 与名称之间的映射 ❻。
- en: That wraps up our data loading code. At this point, we’ve loaded the previously
    created *facial_geometry.csv* file and converted the categories, which are the
    subjects’ names, into a format pandas can understand. The next step is to set
    aside a *true holdout set*, or the instances from the data that are never used
    during the feature engineering, training, and performance estimation phases. I
    chose three instances as the size of the holdout set so that the model training
    portion had plenty of data left to learn the model from. One major problem with
    ML occurs when researchers accidentally give the algorithm direct or indirect
    access to the answers for the test data; this biases the model, so it performs
    excellently on the test set but will likely fail horribly in practice.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的数据加载代码的全部内容。此时，我们已经加载了之前创建的*facial_geometry.csv*文件，并将类别（即主题名称）转换为pandas能够理解的格式。下一步是保留一个*真实保留集*，即在特征工程、训练和性能评估阶段从未使用过的数据实例。我选择了三个实例作为保留集的大小，以便模型训练部分有足够的数据来学习模型。机器学习的一个主要问题是研究人员不小心让算法直接或间接访问测试数据的答案；这样会使模型产生偏见，导致它在测试集上表现优秀，但在实际应用中可能会严重失败。
- en: As an example, suppose all subjects have 20 images except the three subjects
    that are held back for a true holdout set, which have only 19 images remaining
    in the data. If the algorithm had access to the count of images for each subject,
    it could narrow down the list to only those three subjects, even though this information
    isn’t likely to translate well in a production system where subjects will have
    different numbers of photos to work from. So, to make sure we haven’t tainted
    our result, the algorithm will hold back the three samples from different classes
    (one picture from three different people) to test the final model against. Using
    a true holdout set is equivalent to testing with three never-before-seen pictures,
    which is as close to testing on production requirements as you’re going to get.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设所有的主题都有20张图片，除了为真实保留集保留的三个主题，它们在数据中只剩下19张图片。如果算法可以访问每个主题的图片数量，它就能将列表缩小到这三个主题，尽管这些信息在生产系统中可能不适用，因为主题的照片数量可能不同。因此，为了确保我们的结果没有被污染，算法将从不同类别中保留三个样本（来自三个人的各一张照片），以测试最终模型。使用真实保留集相当于测试三张从未见过的照片，这与生产需求的测试非常接近。
- en: In [Listing 10-7](#listing10-7) we create the three-instance holdout set.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 10-7](#listing10-7)中，我们创建了三实例的保留集。
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Listing 10-7: Randomly selecting a true holdout data set'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10-7：随机选择一个真实的保留数据集
- en: We use the `choice` function to randomly choose a subject name from a list of
    unique names ❶. It would be better to do this by ID for real data, since the probability
    of two people in a corporate data set having the same name is fairly high. Luckily,
    we don’t have to worry about this in the sample data because the names were generated
    using faker (a library to randomly generate data that looks authentic), so the
    probability is much lower. If the selected subject is already in the holdout set,
    we continue to choose names randomly until we find one who isn’t.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`choice`函数从唯一名称的列表❶中随机选择一个主题名称。对于真实数据，最好通过ID来做，因为在公司数据集中，两个人有相同名字的概率相当高。幸运的是，我们在样本数据中不需要担心这个问题，因为这些名字是使用faker（一个生成看起来真实数据的库）随机生成的，因此名字相同的概率要低得多。如果选中的主题已经在保留集中了，我们会继续随机选择名称，直到找到一个不在保留集中的主题。
- en: Next, we gather all the instances for the randomly selected subject ❷. We use
    the `choice` function once again to select a random instance index from the group
    of instances ❸. The result is a dictionary keyed off the subject name with a value
    that indicates the randomly selected instance’s index. Once the indices have been
    collected, we use a list comprehension to collect all the indices from the dictionary
    ❹. We copy the actual instance data from the `faces_df` object into a separate
    `DataFrame` ❺. Finally, we remove the instances from `faces_df` so they won’t
    be used in the association matrix calculations ❻.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为随机选择的主题❷收集所有实例。我们再次使用`choice`函数从实例组❸中选择一个随机的实例索引。结果是一个以主题名称为键的字典，值表示随机选择的实例索引。一旦收集到索引，我们使用列表推导式从字典中收集所有的索引❹。然后，我们将实际的实例数据从`faces_df`对象复制到一个单独的`DataFrame`❺。最后，我们从`faces_df`中删除这些实例，以便它们不会在关联矩阵计算中被使用❻。
- en: Feature Engineering
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征工程
- en: Now that the image processing is complete, it’s time to move on to the actual
    model training code. For this portion of the proof of concept, we’re going to
    apply feature engineering and ML to our previously generated facial data. Our
    goal is to produce a predictive model capable of identifying a subject based on
    a previously unprocessed image containing a face it has previously analyzed the
    geometry for (using the holdout set we created in [Listing 10-7](#listing10-7)).
    To achieve our goal, we need to cut the excess noise from our data set so our
    algorithm can focus on the really informative measurements. That’s where feature
    engineering comes in.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most important steps in any ML project, feature engineering involves
    mathematically analyzing the relationship between the different variables in the
    data and the class value we’re interested in predicting to determine which ones
    add the most useful information. Your ability to predict anything useful is directly
    tied to the quality and quantity of the data available. These days, a lack of
    data is hardly a problem. Quite the opposite: we usually have so much data about
    a topic that figuring out what’s really important to the outcome is nearly impossible
    for a human to do by hand. The important relationships can get drowned in the
    noise of useless data. To counter this problem, researchers use one or more feature
    engineering algorithms, which score the features in the data based on their contribution
    to some value we want to predict (in this case, the subject name associated with
    the face). We’re going to apply three steps to progressively whittle the features
    down to only those we’re confident are contributing to the model’s accuracy.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Association Matrix
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One popular method of scoring features is an *association matrix*, which determines
    which features in a large list of features are most correlated to one another.
    The result of running an association algorithm is an *n* × *n* matrix where *n*
    is the number of features in the data. Each cell contains the correlation score
    between the two features defined by the column and row. [Figure 10-6](#figure10-6)
    shows a portion of the association matrix for the facial data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502567c10/f10006.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-6: A feature association matrix'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: The correlation between a variable and itself will always be 1.0, so you can
    ignore those instances. What we’re most interested in are those features that
    have a high correlation with the feature we want to predict. Note that the correlation
    is taken as an absolute value to treat negative and positive correlation as equally
    important. Taking highly correlated variables together offers the best chance
    to properly predict the value of interest.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: The limitation to normal correlation approaches is that you must be correlating
    some continuous (real) numeric values. In this case, we want to measure the correlation
    of continuous variables with a discrete categorical variable (a subject’s name),
    so standard correlation measures won’t work. Instead the matrix was calculated
    using another correlation score known as Theil’s U, which handles categorical
    data, with code borrowed from a blog post by Shaked Zychlinski ([https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9](https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9)).
    All of the functions are located in the *nominal.py* file with the author’s original
    descriptions of how they operate, so I’ll focus on how we integrate the `association`
    function into our facial recognition system.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 10-8](#listing10-8) shows how we can calculate the association matrix.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Listing 10-8: Calculating the association matrix of the feature set'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The `association` function from the *nominal.py* file takes in a `DataFrame`
    object to perform the calculation on; here, that’s `faces_df`. You need to pass
    in a list of nominal columns. This is where the `cat_columns` variable defined
    in [Listing 10-6](#listing10-6) comes in handy; we won’t need to edit the code,
    even if we add more categorical information. To use Theil’s U as the calculation,
    we must set the `theil_u` parameter to `True`. By default, the `association` function
    just displays the results to the screen, but we want to use the data to programmatically
    select which features to use in the model, so we set the `return_results` parameter
    to `True` to also return the result as a matrix.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have an association score for each feature, we can collect the
    top predictors (those with high association scores with the `category` column)
    into a list so we can compare results with the next two feature engineering steps.
    An iterative approach may be to start with the 10 top-performing features and
    see if you can train a functional model. Continue increasing the number of features
    by 5 to 10 until you find the lowest number that produces a reliable model. Another
    approach (and the one I prefer) is to set a predictive threshold for the features
    you want to keep as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Picking a point to cull features is as much an art as a science. After some
    trial and error, I found that I could keep features that scored higher than 0.95
    and still get good performance from the final model. The `key_features` variable
    contains a list of the 19 column names that have an association score greater
    than 0.95 with regard to the `name` column; this includes the `name` and `category`
    columns, which have a score of 1.0, as I mentioned earlier.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Still, the association matrix is just one indicator of predictive power. To
    be really sure we’re picking the best set of features, we’ll run another feature
    selection algorithm and compare the best performers from both to see which features
    are in both lists. Those features will have a very good chance to improve our
    prediction’s accuracy.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Mutual Information Classification
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If one measure of association is good, then two should be great, right? In this
    case we can apply a second method of ranking features to gain even more insight
    into which features will be most helpful. The *mutual information (MI)* score
    between two variables is a non-negative value that measures the dependency between
    the features. It is equal to 0 only when two random variables are completely independent.
    Higher values indicate higher dependency.^([5](b01.xhtml#c10-endnote-005)) [Listing
    10-9](#listing10-9) shows how to calculate the MI using scikit-learn.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Listing 10-9: Calculating the MI contribution for each feature'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: The first argument is the feature matrix we want to calculate the MI score for.
    To avoid tainting the data, we drop the `name` and `category` features from the
    set with an inline call to the `drop` function, which doesn’t remove the columns
    from the actual data, just the temporary feature list passed into the algorithm.
    The next argument is the list of instance categories that will be used internally
    to train a classifier; we pass `faces_df["category"]` as it contains the classes
    of the data we’re interested in finding the MI scores in relation to. You can
    set the `discrete_features` parameter to a list of feature labels to treat explicitly
    as discrete values, or you can let the algorithm attempt to detect the discrete
    features by setting it to `auto`, as we’ve done here. The results are calculated
    using a nearest-neighbor classifier, so once again a bit of trial and error can
    be necessary to find the right number of neighbors. An iterative method can help
    you find a good setting for this parameter. After a few test runs, I settled on
    seven neighbors. The result of the call to `mutual_info_classif` is a list of
    values in the same order as the columns in the `faces_df` data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: As before, we can collect the top performers by selecting all the features with
    an MI score greater than 1\. We’ll compare this to the list of key features generated
    from the association matrix to create an even smaller list of features that are
    highly informative when it comes to predicting the value in the `category` column.
    [Listing 10-10](#listing10-10) shows how.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Listing 10-10: Finding the overlap between best feature lists'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: The `zip` function combines the column names from the data with the results
    in the `contributing` variable we defined in [Listing 10-9](#listing10-9) to create
    a list of tuples with the structure `(``column name``,` `MI score``)`. We then
    use a list comprehension to filter the results into a list of column names that
    had a score greater than or equal to 1.0\. Finally, we compare the column names
    in the `key_features` list to the columns in the `mi_scores` list. Any column
    that’s in both lists makes it to the `reduced_features` list, which represents
    those features that have scored well on both association and mutual information
    with respect to the categorical variable we want to predict. At this point there
    should be only nine columns left in the running, so we could stop here if we wanted.
    We’ve reduced the data set from more than 200 features down to just 9\. In practice,
    you could probably get reliable performance modeling off of these, but I like
    to push things a bit—let’s see how extreme we can go. We’ll do one more feature
    engineering pass to see if we can concentrate the predictive power even more.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Correlation Ratio
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In statistics, the *correlation ratio* is a measure of the relationship between
    the statistical distribution within individual categories (in this case, geometric
    descriptions of photos for the same person) and the distribution across the whole
    population or sample (geometric descriptions of all photos in the data set). The
    measure is defined as the ratio of two standard deviations representing each variation,
    or the ratio of a feature’s variation within a class compared to its variance
    over the whole set. An ideal feature would have low variance within a single category
    but high variance between categories.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, this is the same as saying we want features that are consistent
    for one person but differ between people. One example would be the distance between
    the outside points of the eyes. For a single person, we’d expect this measurement
    to be fairly consistent, but we’d expect measurements between two different people
    to produce a larger difference.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 10-11](#listing10-11) shows how to gather the correlation ratio for
    the `faces_df` data.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Listing 10-11: Calculating the correlation ratio for the features'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'The `correlation_ratio` function ❶ also resides in the *nominal.py* file. It
    takes in a `Series` object representing the feature we want to score, along with
    another `Series` representing the categorical feature. The value is in the range
    of real values between 0 and 1, where 0 means a category cannot be determined
    by the feature’s measurement, and 1 means a category can be determined with absolute
    certainty. We assign the resulting value to a dictionary keyed off the column
    name. Once all the features have been scored, we sort the dictionary using the
    `sorted` function. The result is a list of tuples with the structure `(``column
    name``,` `value``)` sorted from worst performance to best ❷. We loop over the
    last 21 entries and compare each column name to the `reduced_features` list. If
    a column is in both lists, it goes into the final feature set called `reduced_key_features`
    ❸. The result is a list of the four most predictive features from the data:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Each of these features was in the list of top performers for all three feature
    selection methods, which is a strong indication of its ability to classify data.
    In the next step, we’ll use only these four features to train our models. In a
    production system, you could use this information to reduce the number of statistics
    you collect during the first phase. Clearly most of the measurements I defined
    in the first phase weren’t necessary to distinguish faces in the data, but in
    the beginning you rarely know what will be useful, so collecting a large number
    of data points and letting the algorithms do the work can reveal unexpected relationships.
    It’s very important to note these features are specific to this data set. You
    can’t do feature selection on one image set and expect those features to translate
    perfectly to every other image data set.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Model Training
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now the data is finally ready for modeling. We’re going to create the reduced
    data set using the four features selected during feature engineering. We’ll then
    establish a null hypothesis by scoring a simple classifier on the data. Finally,
    we’ll build the real thing using the random forest classifier to perform the final
    classifications for the three images held out from the beginning.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the Data
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first step is to split the data into training and testing sets. [Listing
    10-12](#listing10-12) defines the two data sets as well as the object to handle
    the test data generation.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Listing 10-12: Creating the training and testing splits'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, the variable `X` is used to denote the test data (which doesn’t
    contain the classification information). Here we take only the subset of the features
    defined in `reduced_key_features` from the `faces_df` data to define `X`. We use
    the variable `y` to hold the corresponding class information from the `category`
    column. Finally, we use the `split` method from the `LeaveOneOut` class to create
    a set of *n* copies of the data in `X` (where *n* is the number of distinct classes).
    The result is a list of tuples with the form `(``training indexes``,` `test indexes``)`
    called `splits`. In the LOO validation scheme, each split has a different image
    removed from the data set to test on, so the test indices will always contain
    a single instance ID and the training indices will have the rest. As I mentioned
    before, the LOO validation method gives the algorithm the most training information.
    It’s also closer to how the system would be used in production where an image
    of a subject’s face would be compared to a facial database.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Establishing a Baseline
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To establish a baseline score, we begin by modeling one or more *dummy classifiers*,
    which use very simple prediction methods (such as random guessing or guessing
    the majority class) to establish a worst-case performance score. [Listing 10-13](#listing10-13)
    shows the API for working with scikit-learn classifiers.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Listing 10-13: Training a baseline dummy classifier'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: We use the `DummyClassifier` class from scikit-learn (which has the same API
    as the actual classifiers) to define the baseline model. Passing the `uniform`
    argument ❶ creates a model that will randomly guess the class from the set of
    possible classes. We loop over the splits defined previously to create the training
    and testing instances from the corresponding index lists. The `X_train` and `y_train`
    variables ❷ are used to train the model (if the model is one that requires training),
    while the `X_test` and `y_test` variables ❸ will be used to score the resulting
    model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: We use the `cross_val_score` function ❹ to get an a priori performance estimate
    for the split. The function takes a classifier object and the two parts of the
    training data set. The `cv` parameter sets the number of folds that will be used
    to validate the model. The default is five folds, but the data includes a class
    that has only four images, so if we don’t set this to `4`, Python will print a
    bunch of warnings. The function performs the four-fold cross-validation using
    the classifier object passed in. To get the average score across all four folds,
    we take the sum of the scores and divide it by the number of folds ❺. We save
    the average score into a list so we can analyze it after the loop completes.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Next, we fit the `DummyClassifier` object with the training data ❻. *Fitting*
    a classifier is the proper terminology for training the model on the data. Nothing
    happens internally when we do this for our dummy classifier, but it’s the proper
    workflow when we go to use a more sophisticated classifier in the next step, so
    it’s best to stick to convention.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we use the fitted classifier to predict the outcome from the `y_test`
    data set ❼, which is the one image held out by the LOO algorithm. The result of
    the `predict` function is the class the algorithm thinks the data belongs to.
    We compare the predicted class to the actual class ❽ and increment the hit or
    missed count accordingly.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the loop completes, we calculate the average cross-validation score by
    summing the `scores` list and dividing by the length of the list ❾. The result
    is the average of average scores for cross-validation, which is a decent indicator
    of real-world performance. In my tests the `DummyClassifier` scored about 0.4
    percent accuracy. To validate the performance estimate, we also calculate the
    ratio of hits to misses ❿. During my testing, the actual score was just under
    0.6 percent (26 hits out of 4,457 chances). This result serves as the null hypothesis
    for our conclusion going forward (if you aren’t familiar with hypothesis testing,
    check out this article: [https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing](https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing)).
    If our actual classifier can do better than 0.6 percent correct classification,
    then the model is having some impact on the correctness beyond random coincidence.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Random Forest
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now it’s time to implement the random forest classifier. Since the API is shared
    between all scikit-learn classifiers, the code remains largely unchanged from
    the dummy classifier code in [Listing 10-13](#listing10-13). [Listing 10-14](#listing10-14)
    shows the changes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Listing 10-14: A decision tree algorithm definition'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Rather than defining a dummy classifier, we use the `RandomForestClassifier`
    class from scikit-learn’s `ensemble` module ❶. *Ensemble* classifiers use multiple
    classifiers internally and aggregate the predictions into a single prediction.
    In this case, each internal classifier is a random tree trained off a random sampling
    of the input data—hence the random *forest*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The `n_estimators` parameter defines the number of internal classifiers to train.
    The `min_samples_split` parameter defines the minimum number of instances used
    to train the internal classifiers. The `min_samples_leaf` parameter tells the
    random trees the minimum number of samples to consider a valid leaf node. Setting
    this parameter to higher values will start to automatically prune less useful
    logic branches from the resulting decision trees. If you look back at the decision
    tree in [Figure 10-1](#figure10-1), you can see that the leaf nodes at the very
    bottom of the tree have only one sample each. Since the data set was small to
    begin with, that’s fine, but if the number of samples at a leaf node is low when
    there’s an abundance of data, it most likely means that logic branch doesn’t add
    as much information as other leaves with more samples covered. You can once again
    use manual or automated parameter tuning to find an optimal configuration here.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: The second change is to use the `randint` function to randomly select splits
    ❷ rather than iterating over all of them in order. I chose 50 random splits for
    no special reason; I encourage you to find a number that works better. We use
    a `while` loop to ensure the split hasn’t already been used to train a random
    forest ❸.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: From this point on, the code is identical to the previous dummy classification
    code (you can compare cells 23 and 25 in the *Facial_Recognition_notebook2_Modeling.ipynb*
    notebook to see this). Just be sure to rename the `dc` object references to `rfc`.
    And with that, the code should be ready to run. The result of my test gave a five-fold
    cross-fold estimated performance of 76.5 percent and a test set performance of
    72 percent (36 hits out of 50 chances). It may not seem like 72 percent is all
    that great, but considering the fact that we’re predicting the correct outcome
    using only four geometric features, it’s pretty impressive!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Holdout Images
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the final validation of the model, we’re going to give it the three true
    holdout images and see if it can predict the correct subject. Remember that these
    images haven’t been used by any portion of the code up to now, so they’re completely
    new to the model. Given the previous result of 72 percent accuracy, it would be
    fair to guess the result should be two or three correct out of the three possible.
    [Listing 10-15](#listing10-15) shows how we run the trained classifier on the
    holdout images.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Listing 10-15: Testing with the true holdout data'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: The `category` column from the `real_X` data set defines the proper classes
    we want predicted. We define the `test_X` data by taking the `reduced_key_features`
    subset from the `real_X` data set. Then we create the `RandomForestClassifier`
    object exactly as before. When we fit the model, we use the entire data set not
    in the true holdout set. Then we call the `predict` function on the `test_X` data.
    The result is a list of the class indices that should match the three indices
    in the `real_y` list. To compare the two easily, we can use the `zip` function
    to combine the two lists and print it out.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the result from my test while writing this:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: A perfect score! It’s important to realize that, due to the stochastic nature
    of the algorithm, your results will change between runs. The dummy classifier
    in my tests ranged from 0.2 percent to 0.6 percent, while the `RandomForestClassifier`
    regularly scores above 72 percent. If you get an odd result, such as zero correct
    classifications from the holdout set, try rerunning the code. With an expected
    precision of 72 percent, there’s still a 28 percent chance one holdout image will
    be misclassified, a 7.8 percent chance that two will be misclassified (0.28^2
    = 0.078), and a 2.2 percent chance of the algorithm misclassifying all three holdout
    images (0.28^3 = 0.022).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: At this point we’ve proven our concept is viable. You can refine the process
    to improve the accuracy and reliability of the algorithm, but we’ve proven that
    we could in fact use computational geometry to produce a functional facial recognition
    system. Clearly, the result is no fluke, given the dreadful performance of the
    baseline classifier, so we’ve achieved our goal of properly predicting three subjects
    from previously unseen images.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the accuracy, you should also consider adding a measure of confidence
    in the prediction. What we’ve done by predicting a single class is called a *hard
    classification*. The problem with it is that you’ll always get back some prediction,
    regardless of whether there’s any reason to believe it’s accurate. You could instead
    opt for a *soft classifier*, which predicts the likelihood that a test instance
    belongs to any given class in the data. After fitting the classifier, you can
    use the scikit-learn `predict_proba` function instead of the standard `predict`
    to get back the list of probability-like scores. By examining these scores, you
    can get a sense of how “sure” the algorithm is in a prediction. You can calibrate
    the random forest classifier to get better probability scores and set a threshold
    confidence level to accept or reject a classification. You can see an example
    of using probability predictions in the *Facial_Recognition_notebook2_Modeling.ipynb*
    notebook in the section labeled “Soft Prediction.”
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: The final step in our proof of concept is to save our work for future use. In
    the next section, we’ll cover saving and reloading the trained model in a way
    that’s suitable for deployment in modern production environments.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Model Persistence
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It wouldn’t be practical if you had to retrain the model every time you wanted
    to classify an image of a face. Training a model like this from any realistic
    facial database will take hours on a single machine. Luckily, we can store the
    results of the trained model and then load that saved state into one or more processing
    applications without having to train them on the data directly.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'The scikit-learn documentation on model persistence ([https://scikit-learn.org/stable/model_persistence.html](https://scikit-learn.org/stable/model_persistence.html))
    recommends using a library called joblib to handle storing the data in a *pickled*
    format. As you might know, pickle is Python’s most popular data serialization
    and storage library and is capable of storing complex Python objects to a file
    on disk. The joblib library includes two functions, `dump` and `load`, which are
    convenience wrappers around the pickle library. The following code uses the `joblib.dump`
    function to save the trained model to a file named *trained_facial_model.pkl*:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now we can load the previously trained model into another program by simply
    calling the `joblib.load` function:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: From this point, you can treat the `loaded_model` object the same way you would
    treat the previously trained `rfc` model. If you run the `type` function on the
    `loaded_model` object, you’ll see `<class 'sklearn.ensemble._forest.RandomForestClassifier'>`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: One of the major benefits to saving a trained model as a pickled object is that
    we separate the training of the model from the use of the model. When we need
    to incorporate new data into our model (such as images of a new person’s face),
    we can rerun the model training code without interrupting any running analysis.
    From the point the training process completes, all future runs of the code that
    loads the model can use the updated model, allowing for seamless updates.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a note on security: it’s well known that unpickling a specially crafted
    malicious object can result in arbitrary code execution. Since the `joblib.load`
    function just wraps the `pickle.load` function, the same is true for it as well.
    To reduce your risk, you should never unpickle or load an object from an untrusted
    source. When you develop an application that loads pickled models in production,
    you need to make sure you add some type of data integrity check *before* you unpickle
    the model.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter we’ve explored all the steps necessary to develop a functional
    facial recognition system using our knowledge of computational geometry and a
    healthy dose of machine learning principles. With some tweaks and further testing,
    you can definitely improve the system’s performance. By combining the geometric
    information used here with other nongeometric analysis—such as color palette histograms,
    wavelet transformations, or Eigenfaces—you can get the performance well above
    95 percent. Some researchers have reported accuracy as high as 99.96 percent (0.04
    percent error rate, as of 2020).^([6](b01.xhtml#c10-endnote-006)) This was under
    optimal conditions on a test set developed by the US National Institute of Standards
    and Technology (NIST) for scoring vendors of facial recognition systems.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Improving the accuracy and confidence thresholds is very important when you
    plan to apply these facial recognition systems in a security context. Misclassifying
    a face that isn’t in the data as one that is—in other words, a false positive—could
    lead to accidental authentication of something very sensitive. Another potential
    security risk researchers have proven under test conditions is the use of photos
    or specially designed masks to bypass facial recognition systems.^([7](b01.xhtml#c10-endnote-007))
    Clearly there’s still a lot of open area for research and improvement in this
    field. To continue developing the system, you can combine all of the elements
    into a processing pipeline using a platform like TensorFlow or Spark to distribute
    the computational load across many computers and begin to fix (or attack) some
    of these problems yourself. You can also translate the principles into other image
    classification domains, such as fingerprint analysis or software failure analysis.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: This ends our adventure through computational geometry. I hope the last three
    projects have shown you how flexible a tool it can be. Oftentimes, translating
    all or part of a problem into a geometric representation can help you understand
    the essence better. There’s a vast amount of theory left for you to explore on
    your own. With the foundations you’ve picked up here, the rest will be much easier
    to understand and apply in meaningful security applications.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: We’ll revisit some computational geometry in the next part of the book, “[The
    Art Gallery Problem](p03.xhtml),” where we’ll use it to help analyze the shapes
    of rooms and the locations of resources, much in the same way we applied Voronoi
    tessellation to analyze the distribution of Portland’s fire stations. Let’s turn
    there now!
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
