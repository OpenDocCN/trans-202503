<html><head></head><body>
<div id="sbo-rt-content"><h2 class="h2" id="ch04"><span epub:type="pagebreak" id="page_61"/><strong><span class="big">4</span><br/>NEURAL NETWORKS: BRAIN-LIKE AI</strong></h2>
<div class="image1"><img alt="Image" height="189" src="../images/common.jpg" width="189"/></div>
<p class="noindentsa">Connectionism seeks to provide a substrate from which intelligence might emerge. Today, connectionism means neural networks, with <em>neural</em> being a nod to biological neurons. Despite the name, however, the relationship between the two is superficial. Biological neurons and artificial neurons may possess a similar configuration, but they operate in an entirely different manner.</p>
<p class="indent">Biological neurons accept input on their dendrites, and when a sufficient number of inputs are active they “fire” to produce a short-lived voltage spike on their axons. In other words, biological neurons are off until they’re on. Some 800 million years of animal evolution have made the process considerably more complex, but that’s the essence.</p>
<p class="indent">The artificial neurons of a neural network likewise possess inputs and outputs, but instead of firing, the neurons are mathematical functions with continuous behavior. Some models spike like biological neurons, but we ignore them in this book. The neural networks powering the AI revolution operate continuously.</p>
<p class="indent"><span epub:type="pagebreak" id="page_62"/>Think of a biological neuron like a light switch. It’s off until there is a reason (sufficient input) to turn it on. The biological neuron doesn’t turn on and stay on but flashes on and off, like flicking the switch. An artificial neuron is akin to a light with a dimmer switch. Turn the switch a tiny amount to produce a small amount of light; turn the switch further, and the light’s brightness changes proportionally. This analogy isn’t accurate in all cases, but it conveys the essential notion that artificial neurons are not all or nothing. Instead, they produce output in proportion to their input according to some function. The fog will lift as we work through the chapter, so don’t worry if this makes little sense at present.</p>
<p class="center">****</p>
<p class="indent"><a href="ch04.xhtml#ch04fig01">Figure 4-1</a> is the most critical figure in the book. It’s also one of the simplest, as is to be expected if the connectionist approach is on the right track. If we understand what <a href="ch04.xhtml#ch04fig01">Figure 4-1</a> represents and how it operates, we have the core understanding necessary to make sense of modern AI.</p>
<div class="image"><img alt="Image" height="358" id="ch04fig01" src="../images/ch04fig01.jpg" width="560"/></div>
<p class="figcap"><em>Figure 4-1: The humble (artificial) neuron</em></p>
<p class="indent"><a href="ch04.xhtml#ch04fig01">Figure 4-1</a> contains three squares, a circle, five arrows, and labels like “<em>x</em><sub>0</sub>” and “Output.” Let’s examine each in turn, beginning with the squares on the left.</p>
<p class="indent">Standard practice presents neural networks with the inputs on the left and data flow to the right. In <a href="ch04.xhtml#ch04fig01">Figure 4-1</a>, the three squares labeled <em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>, and <em>x</em><sub>2</sub> are the inputs to the neuron. They are the three features of a feature vector, what we want the neuron to process to give us an output leading to a class label.</p>
<p class="indent">The circle is labeled <em>h</em>, a standard notation for the <a href="glossary.xhtml#glo1"><em>activation function</em></a>. The activation function’s job is to accept input to the neuron and produce an output value, the arrow heading off to the right in <a href="ch04.xhtml#ch04fig01">Figure 4-1</a>.</p>
<p class="indent">The three input squares are connected to the circle (the <a href="glossary.xhtml#glo74"><em>node</em></a>) by arrows, one from each input square. The arrows’ labels—<em>w</em><sub>0</sub>, <em>w</em><sub>1</sub>, and <em>w</em><sub>2</sub>—are the <a href="glossary.xhtml#glo100"><em>weights</em></a>. Every input to the neuron has an associated weight. The lone <em>b</em> <span epub:type="pagebreak" id="page_63"/>linked to the circle by an arrow is the <a href="glossary.xhtml#glo13"><em>bias</em></a>. It’s a number, as are the weights, the input <em>x</em>s, and the output. For this neuron, three numbers come in, and one number goes out.</p>
<p class="indent">The neuron operates like this:</p>
<ol>
<li class="noindent">Multiply every input value, <em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>, and <em>x</em><sub>2</sub>, by its associated weight, <em>w</em><sub>0</sub>, <em>w</em><sub>1</sub>, and <em>w</em><sub>2</sub>.</li>
<li class="noindent">Add all the products from step 1 together along with the bias value, <em>b</em>. This produces a single number.</li>
<li class="noindent">Give the single number to <em>h</em>, the activation function, to produce the output, also a single number.</li>
</ol>
<p class="noindent">That’s all a neuron does: it multiplies its inputs by the weights, sums the products, adds the bias value, and passes that total to the activation function to produce the output.</p>
<p class="indent">Virtually all the fantastic accomplishments of modern AI are due to this primitive construct. String enough of these together in the correct configuration, and you have a model that can learn to identify dog breeds, drive a car, or translate from French to English. Well, you do if you have the magic weight and bias values, which training gives us. These values are so important to neural networks that one company has adopted “Weights &amp; Biases” as its name; see <a href="https://www.wandb.ai"><em>https://www.wandb.ai</em></a>.</p>
<p class="indent">We have choices for the activation function, but in modern networks it’s most often the rectified linear unit (ReLU) mentioned in <a href="ch02.xhtml">Chapter 2</a>. The ReLU is a question: is the input (the sum of the inputs multiplied by the weights plus the bias) less than zero? If so, the output is zero; otherwise, it’s whatever the input is.</p>
<p class="indent">Can something as straightforward as a lone neuron be useful? It can. As an experiment, I trained the neuron in <a href="ch04.xhtml#ch04fig01">Figure 4-1</a> using three features from the iris flower dataset from <a href="ch01.xhtml">Chapter 1</a> as input. Recall, this dataset contains measurements of the parts of three different species of iris. After training, I tested the neuron with an unused test set that had 30 feature vectors. The neuron correctly classified 28, for an accuracy of 93 percent.</p>
<p class="indent">I trained the neuron by searching for a set of three weights and a bias value producing an output that, when rounded to the nearest whole number, matched the class label for an iris flower—either 0, 1, or 2. This is not the standard way to train a neural network, but it works for something as modest as a single neuron. We’ll discuss standard network training later in the chapter.</p>
<p class="indent">A single neuron can learn, but complex inputs baffle it. Complex inputs imply we need a more complex model. Let’s give our single neuron some friends.</p>
<p class="indent">Convention arranges neurons in layers, with the outputs from the previous layer the inputs to the following layer. Consider <a href="ch04.xhtml#ch04fig02">Figure 4-2</a>, which shows networks with two, three, and eight nodes in the layer after the input. Arranging the network in layers simplifies the implementation in code and facilitates the standard training procedure. That said, there is no requirement to use layers if an alternative way to train the model can be found.</p>
<div class="image"><img alt="Image" height="327" id="ch04fig02" src="../images/ch04fig02.jpg" width="561"/></div>
<p class="figcap"><em>Figure 4-2: Two-, three-, and eight-node networks</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_64"/>Let’s begin with the two-node network at the upper left. The three inputs (squares) are there, but this time there are two circles in the middle layer and a single circle on the right. The inputs are fully connected to the two nodes in the middle layer, meaning a line connects each input square to each middle layer node. The middle layer outputs are connected to a single node on the far right, from which the network’s output comes.</p>
<p class="indent">The middle layers of a neural network between the input on the left and the output on the right are known as <a href="glossary.xhtml#glo54"><em>hidden layers</em></a>. For example, the networks of <a href="ch04.xhtml#ch04fig02">Figure 4-2</a> each have one hidden layer with 2, 3, and 8 nodes, respectively.</p>
<p class="indent">A network with this configuration is suitable for a binary classification task, class 0 versus class 1, where the output is a single number representing the model’s belief that the input is a member of class 1. Therefore, the rightmost node uses a different activation function known as a <a href="glossary.xhtml#glo90"><em>sigmoid</em></a> (also called a logistic). The sigmoid produces an output between 0 and 1. This is also the range used to represent a probability, so many people refer to the output of a node with a sigmoid activation function as a probability. This is not generally accurate, but we can live with the sloppiness. The nodes of the hidden layer all use ReLU activation functions.</p>
<p class="indent">How many weights and biases must we learn to implement the two-node network in <a href="ch04.xhtml#ch04fig02">Figure 4-2</a>? We need one weight for each line (except the output arrow) and one bias value for each node. Therefore, we need eight weights and three bias values. For the model at the lower left, we need 12 weights and 4 biases. Finally, for the 8-node model, we need to learn 32 weights and 9 bias values. As the number of nodes in a layer increases, the number of weights increases even faster. This fact alone restrained neural networks for years, as potentially useful models were too big for a single computer’s memory. Of course, model size is relative. OpenAI’s GPT-3 has over 175 billion weights, and while they aren’t talking about how large GPT-4 is, rumor puts it at 1.7 <em>trillion</em> weights.</p>
<p class="indent">We need a two-class dataset to explore the models in <a href="ch04.xhtml#ch04fig02">Figure 4-2</a>. The dataset we’ll use is a classic one that attempts to distinguish between two <span epub:type="pagebreak" id="page_65"/>cultivars of grapes used to make wine in a particular region of Italy. Unfortunately, the wines represented by the dataset are, it seems, no longer known. (That’s how old the dataset is.) However, we know that models don’t care about the labels—they use numbers—so we’ll use 0 and 1 as the labels.</p>
<p class="indent">We need three features, <em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>, and <em>x</em><sub>2</sub>. The features we’ll use are alcohol content in percent, malic acid, and total phenols. The goal is to train the models in <a href="ch04.xhtml#ch04fig02">Figure 4-2</a> to see how well each performs when identifying an unknown wine given measurements of the three features.</p>
<p class="indent">I trained the two-neuron model using a training set of 104 samples and a test set of 26 samples. This means I used 104 triplets of measured alcohol content, malic acid level, and total phenols, knowing the proper output label, class 0 or class 1. The training set conditioned the two-neuron model to give values to all eight weights and three biases. I promise we will discuss how training works, but for now, assume it happens so we can explore how neural networks behave. The trained model achieved an accuracy on the test set of 81 percent, meaning it was right better than 8 times out of 10. That’s not too bad for such a small model and training set.</p>
<p class="indent"><a href="ch04.xhtml#ch04fig03">Figure 4-3</a> presents the trained two-neuron model. I added the weights to the links and the biases to the nodes so you can see them. I think it’s worth looking at the numbers at least once, and it’s best to do that with a simple model.</p>
<div class="image"><img alt="Image" height="311" id="ch04fig03" src="../images/ch04fig03.jpg" width="562"/></div>
<p class="figcap"><em>Figure 4-3: The two-neuron model trained on the wine dataset</em></p>
<p class="indent">Let’s use the model with two test samples to understand the process. The two test samples consist of three numbers each, the values of the features, (<em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>):</p>
<div class="bqparan">
<table>
<colgroup>
<col style="width:30%"/>
<col style="width:70%"/>
</colgroup>
<tbody>
<tr>
<td><p class="indent"><strong>Sample 1</strong></p></td>
<td><p class="indent">(–0.7359, 0.9795, –0.1333)</p></td>
</tr>
<tr>
<td><p class="indent"><strong>Sample 2</strong></p></td>
<td><p class="indent">( 0.0967, –1.2138, –1.0500)</p></td>
</tr>
</tbody>
</table>
</div>
<p class="indent">You may have a question at this point. I said the features were alcohol content in percent, malic acid level, and total phenols. While I have no idea what the units are for measuring malic acid or total phenols, a percent is a percent, so why is <em>x</em><sub>0</sub> for the first sample a small negative number? We can’t have a negative percentage of alcohol.</p>
<p class="indent"><span epub:type="pagebreak" id="page_66"/>The answer has to do with <a href="glossary.xhtml#glo82"><em>preprocessing</em></a>. Raw data, like the percent alcohol, is seldom used with machine learning models as is. Instead, each feature is adjusted by subtracting the average value of the feature over the training set and dividing that result by a measure of how scattered the data is around the average value (the standard deviation). The original alcohol content was 12.29 percent, a reasonable value for wine, but after scaling, it became –0.7359.</p>
<p class="indent">Let’s classify sample 1 using the learned weights and biases in <a href="ch04.xhtml#ch04fig03">Figure 4-3</a>. The input to the top neuron is each feature multiplied by the weight on the line connecting that feature to the neuron, then summed with the bias value. The first feature gives us 0.4716 × –0.7359, the second 0.0399 × 0.9795, and the third –0.3902 × –0.1333, with bias value 0.0532. Adding all of these together gives – 0.2028. This is the number passed to the activation function, a ReLU. Since it is negative, the ReLU returns 0, meaning the output from the top node is 0. Repeating the calculation for the bottom node gives 0.1720 as the input to the ReLU. That’s a positive number, so the ReLU returns 0.1720 as the output.</p>
<p class="indent">The outputs of the two nodes in the middle layer are now used as the inputs to the final node on the right. As before, we multiply the outputs by the weights, add them along with the bias value, and pass that to the activation function. In this case, the activation function is not a ReLU but a sigmoid.</p>
<p class="indent">The top node’s output is 0, and the bottom’s output is 0.1720. Multiplying these by their respective weights, summing, and adding the bias value of 2.2277 gives us 1.9502 as the argument to the sigmoid activation function, producing 0.8755 as the network’s output for the first input sample.</p>
<p class="indent">How should we interpret this output? Here’s where we learn an important aspect of neural networks:</p>
<div class="bq">
<p class="noindent">Neural networks don’t tell us the actual class label for the input, but only their confidence in one label relative to another.</p>
</div>
<p class="indent">Binary models output a confidence value that we’re interpreting as the probability of the input belonging to class 1. Probabilities are numbers between 0 (no chance) and 1 (absolutely assured). Humans are generally more comfortable with percentages, which we get by multiplying the probability by 100. Therefore, we can say that the network is a little more than 87 percent confident that this input represents an instance of class 1.</p>
<p class="indent">In practice, we use a threshold—a cutoff value—to decide which label to assign. The most common approach for binary models is a threshold of 50 percent. If the output exceeds 50 percent (probability 0.5), we assign the input to class 1. This output is above 50 percent, so we assign “class 1” as the label. This sample is from class 1, meaning the network’s assigned label is correct.</p>
<p class="indent">We can repeat these calculations for the second input sample, (0.0967, –1.2138, –1.0500). I’ll leave walking through it to you as an exercise, but the network’s output for sample 2 is 0.4883. In other words, the network’s confidence that this sample belongs to class 1 is 49 percent. The cutoff is 50 percent, so we reject the class 1 label and assign this input to class 0. The actual <span epub:type="pagebreak" id="page_67"/>class is class 1, so, in this instance, the network is wrong—it assigned a class 1 sample to class 0. Oops.</p>
<p class="indent">Is this a useful model? The answer depends on the context. We’re classifying wine by cultivar. If the model’s output is wrong 20 percent of the time, which is one time in five, is that acceptable? I suspect not, but there might be other tasks where a model with this level of accuracy is acceptable.</p>
<p class="indent">Neural networks offer some control over how their outputs are interpreted. For example, we might not use 50 percent as the cutoff. If we make it lower, say, 40 percent, we’ll capture more class 1 samples, but at the expense of mistakenly identifying more actual class 0 samples as class 1. In other words, we get to trade off one kind of error for another.</p>
<p class="indent">Let’s bring the other models in <a href="ch04.xhtml#ch04fig02">Figure 4-2</a> into the mix. I trained all three models using the same training and test sets used for <a href="ch04.xhtml#ch04fig03">Figure 4-3</a>. I repeated the process 240 times for each of the three models. Here are the average accuracies:</p>
<div class="bqparan">
<table>
<colgroup>
<col style="width:40%"/>
<col style="width:60%"/>
</colgroup>
<tbody>
<tr>
<td><p class="indent"><strong>2-node</strong></p></td>
<td><p class="indent">81.5 percent</p></td>
</tr>
<tr>
<td><p class="indent"><strong>3-node</strong></p></td>
<td><p class="indent">83.6 percent</p></td>
</tr>
<tr>
<td><p class="indent"><strong>8-node</strong></p></td>
<td><p class="indent">86.2 percent</p></td>
</tr>
</tbody>
</table>
</div>
<p class="indent">The model’s performance improves as the number of nodes in the hidden layer increases. This makes intuitive sense, as a more complex model (more nodes) implies the ability to learn more complex associations hidden within the training set.</p>
<p class="indent">I suspect you now have a new question: why did I train each model 240 times and report the average accuracy over all 240 models? Here’s another critical thing to understand about neural networks:</p>
<div class="bq">
<p class="noindent">Neural networks are randomly initialized, such that repeated training leads to differently performing models even when using the same training data.</p>
</div>
<p class="indent">The phrase “randomly initialized” demands clarification. Look again at <a href="ch04.xhtml#ch04fig03">Figure 4-3</a>. The numbers representing the weights and biases came from an iterative process. This means that an initial set of weights and biases are updated repeatedly, each time moving the network toward a better and better approximation of whatever function it is that links the input feature vectors and the output labels. Approximating this function well is what we want the network to do.</p>
<p class="indent">Why not initialize all the weights to the same value? The answer is that doing so forces the weights to learn similar characteristics of the data, which is something we don’t want, and in the end the model will perform poorly. If we set all of the initial weights to zero, the model does not learn at all.</p>
<p class="indent">An initial set of values are necessary for the iterative process to work. How should we pick the initial values? That’s an important question, and the answer for our current level of understanding is “at random,” meaning we roll dice, in a sense, to get the initial value for each weight and bias. The iterative process then refines these values to arrive at the final set in <a href="ch04.xhtml#ch04fig03">Figure 4-3</a>.</p>
<p class="indent"><span epub:type="pagebreak" id="page_68"/>However, the iterative process doesn’t always end in the same place. Pick a different random set of initial weights and biases, and the network will converge to a different set of final values. For example, the network in <a href="ch04.xhtml#ch04fig03">Figure 4-3</a> achieved an accuracy of 81 percent, as mentioned previously. Here are 10 more accuracies for the same network trained and tested on the same data:</p>
<p class="center">89, 85, 73, 81, 81, 81, 81, 85, 85, 85</p>
<p class="indent">The accuracies range from a high of 89 percent to a low of 73 percent. All that changed between each training session was the collection of initial weights and biases. This is an often overlooked issue with neural networks. Networks should be trained multiple times, if feasible, to gather data on their effectiveness or, as with the 73 percent version of the network, to understand that a bad set of initial values was used purely by chance. I should also mention that the wide variation in the accuracy of this network is related to its being relatively small and containing only a few weights and biases. Larger models tend to be more consistent when trained repeatedly.</p>
<p class="indent">We’ve already covered a lot of ground, so a recap is in order:</p>
<ul>
<li class="noindent">The fundamental unit of a neural network is the neuron, also called a node.</li>
<li class="noindent">Neurons multiply their inputs by weights, sum those products, add a bias value, and pass all of that to the activation function to produce an output value.</li>
<li class="noindent">Neural networks are collections of individual neurons, typically arranged in layers with the output of the current layer the input to the following layer.</li>
<li class="noindent">Training a neural network assigns values to the weights and biases by iteratively adjusting an initial, randomly selected set.</li>
<li class="noindent">Binary neural networks produce an output that roughly corresponds to the probability of the input belonging to class 1.</li>
</ul>
<p class="center">****</p>
<p class="indent">Now that we know what a neural network is and how it’s used, we finally come to the crux of the matter: where do the magic weights and biases come from in the first place? In <a href="ch02.xhtml">Chapter 2</a>, I briefly mentioned that neural networks improved in the 1980s thanks to two essential algorithms: backpropagation and gradient descent. These are the algorithms at the heart of neural network training.</p>
<p class="indent">We discussed optimization, the process of finding the best of something according to some criteria, in <a href="ch03.xhtml">Chapter 3</a> in reference to support vector machines. Training a neural network is also an optimization process, involving learning the weights and biases that best fit the training data. Care must be taken, however, to make it more likely that the learned weights and biases fit general trends in the training data rather than the details of the specific training data itself. What I mean by that will become apparent as we learn more about the training process.</p>
<p class="indent"><span epub:type="pagebreak" id="page_69"/>The general training algorithm is:</p>
<ol>
<li class="noindent">Select the model’s architecture, including the number of hidden layers, nodes per layer, and activation function.</li>
<li class="noindent">Randomly but intelligently initialize all the weights and biases associated with the selected architecture.</li>
<li class="noindent">Run the training data, or a subset, through the model and calculate the average error. This is the <a href="glossary.xhtml#glo45"><em>forward pass</em></a>.</li>
<li class="noindent">Use backpropagation to determine how much each weight and bias contributes to that error.</li>
<li class="noindent">Update the weights and biases according to the gradient descent algorithm. This and the previous step make up the <a href="glossary.xhtml#glo11"><em>backward pass</em></a>.</li>
<li class="noindent">Repeat from step 3 until the network is considered “good enough.”</li>
</ol>
<p class="indent">These six steps include many important terms. It’s worth our time to ensure that we have an idea of what each means. In this chapter, <a href="glossary.xhtml#glo3"><em>architecture</em></a> refers to the number of layers, typically hidden layers, used by the network. We have our input feature vector, and we can imagine each hidden layer working collectively to accept an input vector and produce an output vector, which then becomes the input to the next layer, and so on. For binary classifiers, the network’s output is a single node producing a value from 0 to 1. We’ll learn later in the book that this idea can be extended to multiclass outputs.</p>
<p class="indent">The algorithm indicates that training is an iterative process that repeats many times. Iterative processes have a starting point. If you want to walk from point A to point B, place one foot in front of the other. That’s the iterative part. Point A is the starting point. For a neural network, the architecture implies a set of weights and biases. The initial values assigned to those weights and biases are akin to point A, with training akin to placing one foot in front of the other.</p>
<p class="indent">The algorithm uses the phrase “average error.” What error? Here’s where a new concept enters the picture. Intuitively, we can see that simply picking some initial values for the weights and biases is not likely to lead to a network able to classify the training data accurately. Remember, we know the inputs and the expected outputs for the training data.</p>
<p class="indent">Say we push training sample 1 through the network to give us an output value, perhaps 0.44. If we know that sample 1 belongs to class 1, the error made by the network is the difference between the expected output and the actual output. Here, that’s 1 – 0.44, or 0.56. A good model might instead have produced an output of 0.97 for this sample, giving an error of only 0.03. The smaller the error, the better the model is at classifying the sample. If we push all the training data through the network, or a representative subset of it, we can calculate the error for each training sample and find the average over the entire training set. That’s the measure used by the (to be described) backpropagation and gradient descent algorithms to update the weights and biases.</p>
<p class="indent"><span epub:type="pagebreak" id="page_70"/>Finally, the training algorithm says to push data through the network, get an error, update the weights and biases, and repeat until the network is “good enough.” In a way, good enough is when the error, also called the <a href="glossary.xhtml#glo63"><em>loss</em></a>, is as close to zero as possible. If the network produces 0 as the output for all class 0 samples and 1 as the output for all class 1 samples, then it performs perfectly on the training data, and the error will be zero. That’s certainly good enough, but we must be careful. Sometimes when that happens the network is <a href="glossary.xhtml#glo79"><em>overfitting</em></a>, meaning it’s learned all the details of the training data without actually learning the general trends of the data that will allow it to perform well when used with unknown inputs in the wild.</p>
<p class="indent">In practice, overfitting is addressed in several ways, the best of which is acquiring more training data. We use the training data as a stand-in for all the possible data that could be produced by whatever process we are trying to model. Therefore, more training data means a better representation of that data collection. It’s the interpolate versus extrapolate issue we discussed in <a href="ch01.xhtml">Chapter 1</a>.</p>
<p class="indent">However, getting more training data might not be possible. Alternatives include tweaking the training algorithm to introduce things that keep the network from focusing on irrelevant details of the training data while learning. One such technique you may hear mentioned is <em>weight decay</em>, which penalizes the network if it makes the weight values too large.</p>
<p class="indent">Another common approach is <a href="glossary.xhtml#glo26"><em>data augmentation</em></a>. Out of training data? No worries, data augmentation will invent some by slightly modifying the data you already have. Data augmentation takes the existing training data and mutates it to produce new data that might plausibly have been created by the same process that made the actual training data. For example, if the training sample is a picture of a dog, it will still be a picture of a dog if you rotate it, shift it up a few pixels, flip it left to right, and so on. Each transformation produces a new training sample. It might seem like cheating, but in practice, data augmentation is a powerful <a href="glossary.xhtml#glo86"><em>regularizer</em></a> that keeps the network from overfitting during training.</p>
<p class="indent">Let’s return for a moment to initialization, as its importance was not sufficiently appreciated for many years.</p>
<p class="indent">At first, weight initialization meant nothing more than “pick a small random number” like 0.001 or –0.0056. That worked much of the time. However, it didn’t work consistently, and when it did work, the network’s behavior wasn’t stellar.</p>
<p class="indent">Shortly after the advent of deep learning, researchers revisited the “small random value” idea in search of a more principled approach to initialization. The fruit of those efforts is the way neural networks are initialized to this day. Three factors need to be considered: the form of the activation function, the number of connections coming from the layer below (<em>fan-in</em>), and the number of outputs to the layer above (<em>fan-out</em>). Formulas were devised to use all three factors to select the initial weights for each layer. Bias values are usually initialized to zero. It isn’t difficult to demonstrate that networks so initialized perform better than those initialized the old-fashioned way.</p>
<p class="indent"><span epub:type="pagebreak" id="page_71"/>We have two steps of the training algorithm yet to discuss: backpropagation and gradient descent. Backpropagation is often presented first because its output is necessary for gradient descent. However, I think it’s more intuitive to understand what gradient descent is doing, then fill in the missing piece it needs with what backpropagation provides. Despite the unfamiliar names, I am certain you already understand the essence of both algorithms.</p>
<p class="center">****</p>
<p class="indent">You’re standing in a vast, open grassland of rolling hills. How did you get here? You strain your brain, but no answer comes. Then, finally, you spy a small village to the north, in the valley far below. Perhaps the people there can give you some answers. But what’s the best way to get there?</p>
<p class="indent">You want to go north and down, in general, but you must also respect the contour of the land. You always want to move from a higher to a lower position. You can’t go due north because a large hill is in your way. You could head northeast; the terrain is flatter there, but going that way will make your journey a long one, as the land drops slowly. So, you decide to head northwest, as that moves you both north and down more steeply than to the east. You take a step to the northwest, then pause to reassess your position to decide which direction to move in next.</p>
<p class="indent">Repeating this two-stage process of examining your current position to determine the direction that best moves you both northward and downward, then taking a step in that direction, is your best bet for reaching the village in the valley. You may not make it; you might get stuck in a small canyon out of which you can’t climb. But overall, you’ll make progress toward your goal by consistently moving in a direction that is north and down relative to your current position.</p>
<p class="indent">Following this process, known as <a href="glossary.xhtml#glo52"><em>gradient descent</em></a>, lets us adjust a neural network’s initial weights and biases to give us ever better-performing models. In other words, gradient descent trains the model.</p>
<p class="indent">The three-dimensional world of the grassland surrounding the village corresponds to the <em>n</em>-dimensional world of the network, where <em>n</em> is the total number of weights and biases whose values we are trying to learn. Choosing a direction to head in from your current position and then moving some distance in that direction is a gradient descent step. Repeated gradient descent steps move you closer and closer to the village.</p>
<p class="indent">Gradient descent seeks the minimum position, the village in the valley—but the minimum of what? For a neural network, gradient descent aims to adjust the weights and biases of the network to minimize the error over the training set.</p>
<p class="indent">The vast, open grassland of rolling hills represents the error function, the average error over the training data when using the weight and bias values corresponding to your current position. This means that each position in the grassland implies a complete set of network weights and biases. The position of the village corresponds to the smallest error the network can make on the training set. The hope is that a model that has a small error on its training set will make few errors on unknown inputs when used in <span epub:type="pagebreak" id="page_72"/>the wild. Gradient descent is the algorithm that moves through the space of weights and biases to minimize the error.</p>
<p class="indent">Gradient descent is an optimization algorithm, again telling us that training a neural network is an optimization problem, a problem where we need to find the best set of something. While this is true, it is also true that training a neural network is subtly different from other optimization problems. As mentioned previously, we don’t necessarily want the smallest possible error on the training data, but rather the model that best generalizes to unknown inputs. We want to avoid overfitting. I’ll demonstrate visually what that means later in the chapter.</p>
<p class="indent">Gradient descent moves through the landscape of the error function. In everyday use, a gradient is a change in something, like the steepness of a road or a color gradient varying smoothly from one shade to another. Mathematically, a gradient is the multidimensional analog of the slope of a curve at a point. The steepest direction to move is down the maximum gradient. The slope of a line at a point on a curve is a helpful representation of the gradient, so contemplating slopes is a worthy use of our time.</p>
<p class="indent"><a href="ch04.xhtml#ch04fig04">Figure 4-4</a> shows a curve with four lines touching it at different points. The lines represent the slope at those points. The slope indicates how quickly the value of the function changes in the vicinity of the point. The steeper the line, the faster the function’s value changes as you move along the <em>x</em>-axis.</p>
<div class="image"><img alt="Image" height="416" id="ch04fig04" src="../images/ch04fig04.jpg" width="557"/></div>
<p class="figcap"><em>Figure 4-4: A curve with the slope at various points marked</em></p>
<p class="indent">Line B marks the lowest point on the curve. This is the <a href="glossary.xhtml#glo51"><em>global minimum</em></a> and the point that an optimization algorithm seeks to find. Notice that the line touching this point is entirely horizontal. Mathematically, this means <span epub:type="pagebreak" id="page_73"/>that the slope of line B is zero. This is true at the minima (and maxima) of functions.</p>
<p class="indent">The point touched by line B is the global minimum, but there are three other minima in the plot. These are <em>local minima</em>, points where the slope of the line touching those points is also zero. Ideally, an optimization algorithm would avoid these points, favoring the global minimum.</p>
<p class="indent">Line A is steep and points toward the global minimum. Therefore, if we were at the point on the curve touched by line A, we could move quickly toward the global minimum by taking steps in the indicated direction. Moreover, as the slope is steep here, we can take reasonably large steps down to the valley.</p>
<p class="indent">Line C is also steep but heads toward one of the local minima, the one just beyond 3 on the <em>x</em>-axis. A gradient descent algorithm that only knows how to move down the gradient will locate that local minimum and become stuck there. The same applies to line D, which heads toward the local minimum between 4 and 5 on the <em>x</em>-axis.</p>
<p class="indent">What are the takeaways from <a href="ch04.xhtml#ch04fig04">Figure 4-4</a>? First, gradient descent moves down the gradient, or slope, from some point. Here the curve is one- dimensional, so the point is a specific value of <em>x</em>. Gradient descent uses the value of the slope at that point to pick a direction and a step size proportional to the steepness of the slope. A steep slope means we can take a larger step to end up at a new <em>x</em> value closer to a minimum. A shallow slope implies a smaller step.</p>
<p class="indent">For example, suppose we are initially at the point where line A touches the curve. The slope is steep, so we take a big step toward the global minimum. After the step, we look at the slope again, but this time it’s the slope at the new point on the <em>x</em>-axis. Using that slope, we take another step, then another, and another until we get to a point where the slope is essentially zero. That’s the minimum, so we stop.</p>
<p class="indent">The one-dimensional case is straightforward enough because at each point there is only one slope, so there is only one direction to go. However, recalling the vast, open grassland, we know that from any point there are an infinite number of directions we might head in, many of which are useful in that they move us northward and downward. One of these directions, the direction of the maximum gradient, is the steepest and moves us most quickly toward our desired destination, and that’s the direction we step in. Repeating the process, using the maximum gradient direction each time, accomplishes in multiple dimensions what we did in one dimension. To be precise, we step in the direction <em>opposite</em> the maximum gradient because the maximum gradient points away from the minimum, not toward it.</p>
<p class="indent"><a href="ch04.xhtml#ch04fig05">Figure 4-5</a> presents gradient descent in two dimensions. The figure shows a contour plot. Imagine an open pit mine with terraced levels: the lighter the shade, the deeper into the mine, but also the flatter the slope. That is, lighter shades imply shallower slopes.</p>
<div class="image"><img alt="Image" height="417" id="ch04fig05" src="../images/ch04fig05.jpg" width="558"/></div>
<p class="figcap"><em>Figure 4-5: Gradient descent in two dimensions</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_74"/>The figure shows the path taken by gradient descent for three starting positions: the circle, the triangle, and the square. Initially, the slopes are steep, so the step sizes are big, but the slopes become shallow as the minimum is approached, implying smaller steps. Eventually, gradient descent reaches the minimum, regardless of the starting point.</p>
<p class="indent">We’ve discussed gradient descent in one and two dimensions because we can visualize the process. We understand now that we have always known the algorithm and used it ourselves whenever we walk from a higher elevation to a lower one. Honestly, this is all that training a neural network does. The initial set of weights and biases is nothing more than a single starting point in an <em>n</em>-dimensional space. Gradient descent uses the maximum gradient from that initial starting position to march toward a minimum. Each new position in the <em>n</em>-dimensional space is a new set of the <em>n</em> weights and biases generated from the previous set based on the steepness of the gradient. When the gradient gets very small, we claim victory and fix the weights and biases, believing the network to be trained.</p>
<p class="indent">Gradient descent depends on slopes, on the value of the gradient. But where do the gradients come from? Gradient descent minimizes the loss function, or the error made by the network. The error over the training set is a function of each weight and bias value in the network. The gradient represents how much each weight and bias contributes to the overall error.</p>
<p class="indent">For example, suppose we know how much weight 3 (whatever weight that labels) contributes to the network’s error as measured by the mistakes the network makes on the training set. In that case, we know the steepness of the gradient should we change weight 3’s value, keeping all other weights and biases the same. That steepness, multiplied by a step size, gives us a value to subtract from weight 3’s current value. By subtracting, we move in the direction opposite to the maximum gradient. Repeating the calculation <span epub:type="pagebreak" id="page_75"/>for every weight and bias in the network takes a step in the <em>n</em>-dimensional space. This is what gradient descent does during training.</p>
<p class="indent"><a href="glossary.xhtml#glo10"><em>Backpropagation</em></a> is the algorithm that gives us the steepness values per weight and bias. Backpropagation is an application of a well-known rule from differential calculus, the branch of mathematics telling us how one thing changes as another changes. Speed is an example. Speed indicates how distance changes with time. It’s even in how we talk about speed: miles per hour or kilometers per hour. Backpropagation gives us the “speed” representing how the network’s error changes with a change in any weight or bias value. Gradient descent uses these “speeds,” multiplied by a scale factor known as the <a href="glossary.xhtml#glo61"><em>learning rate</em></a>, to step to the next position in the <em>n</em>-dimensional space represented by the <em>n</em> weights and biases of the network.</p>
<p class="indent">For example, the “big” network in <a href="ch04.xhtml#ch04fig02">Figure 4-2</a> has 32 weights and 9 biases; therefore, training that network with gradient descent means moving through a 41-dimensional space to find the 41 weight and bias values giving us the smallest error averaged over the training set.</p>
<p class="indent">The algorithm is called “backpropagation” because it calculates the “speed” values for each weight and bias, beginning with the network’s output layer and then moving backward, layer by layer, to the input layer. That is, it moves backward through the network to propagate the error from a layer to the previous layer.</p>
<p class="indent">The take-home message is this:</p>
<div class="bq">
<p class="noindent">Gradient descent uses the gradient direction supplied by backpropagation to iteratively update the weights and biases to minimize the network’s error over the training set.</p>
</div>
<p class="indent">And that, in a nutshell, is how neural networks are trained.</p>
<p class="center">****</p>
<p class="indent">The ability to train a neural network with backpropagation and gradient descent is a bit of a fluke. It shouldn’t work. Gradient descent with backpropagation is a <em>first-order</em> optimization approach. First-order optimization works best with simple functions, and the error surfaces of a neural network are anything but. However, Fortuna has smiled upon us, and it does work, and rather well at that. There is as yet no rigorous mathematical explanation beyond the realization that the local minima of the error function are all pretty much the same, meaning if you land in one and can’t get out, that’s often just fine.</p>
<p class="indent">There is another empirical explanation, but to understand that, we must learn more about the training process. The six-step training algorithm I gave earlier in the chapter talks about running the training set, or a subset of it, through the network, and repeating until things are “good enough.” Let me expand on the process implied by these steps.</p>
<p class="indent">Each pass of training data through the network, a forward pass followed by a backward pass, results in a gradient descent step as shown in <a href="ch04.xhtml#ch04fig05">Figure 4-5</a>. If the training set is small, all of it is used in the forward pass, meaning all of it is used by gradient descent to decide where to step next. A complete pass through the training data is called an <a href="glossary.xhtml#glo37"><em>epoch</em></a>; therefore, using all the training <span epub:type="pagebreak" id="page_76"/>data in the forward and backward passes results in one gradient descent step per epoch.</p>
<p class="indent">Modern machine learning datasets are often massive, making it computationally infeasible to use all of the training data for each gradient descent step. Instead, a small, randomly selected subset of the data, known as a <a href="glossary.xhtml#glo67"><em>minibatch</em></a>, is passed through the network for the forward and backward passes. Using minibatches dramatically reduces the computational overhead during gradient descent, resulting in many steps per epoch. Minibatches also provide another benefit that helps overcome the “this approach to training shouldn’t work” issue.</p>
<p class="indent">Suppose we had a mathematical function representing the error made by the network. In that case, we could use centuries-old calculus techniques to find the exact form of each weight and bias’s contribution to the error; gradient descent would know the best direction to step each time. Unfortunately, the world isn’t that kind. We don’t know the mathematical form of the error function (there isn’t likely one to know), so we have to approximate with our training data. This approximation improves when using more training data to determine the error. This fact argues for using all the training data for each gradient descent step. However, we already know this is computationally extremely taxing in many cases.</p>
<p class="indent">The compromise is to use minibatches for each gradient descent step. The calculations are no longer too taxing, but the approximation of the actual gradient is worse because we are estimating it with fewer data points. Randomly selecting something is often attached to the word “stochastic,” so training with minibatches is known as <a href="glossary.xhtml#glo91"><em>stochastic gradient descent</em></a>. Stochastic gradient descent, in one form or another, is the standard training approach used by virtually all modern AI.</p>
<p class="indent">At first blush, stochastic gradient descent sounds like a losing proposition. Sure, we can calculate many gradient descent steps before the heat death of the universe, but our gradient fidelity is low, and we’re likely moving in the wrong direction through the error space. That can’t be good, can it?</p>
<p class="indent">Here’s where Fortuna smiles on humanity a second time. Not only has she given us the ability to train complex models with first-order gradient descent because local minima are (assumed) roughly equivalent; she’s also arranged things so that the “wrong” gradient direction found by stochastic gradient descent is often what we need to avoid local minima early in the training process. In other words, walking slightly northeast when we should head due north is a blessing in disguise that allows us to train large neural networks.</p>
<p class="center">****</p>
<p class="indent">We’re ready to move on to the next chapter. However, before we do, let’s apply traditional neural networks to the dinosaur footprint dataset. We’ll compare the results to the classical models of <a href="ch03.xhtml">Chapter 3</a>.</p>
<p class="indent">We need first to select an architecture: that is, the number of hidden layers, the number of nodes per layer, and the type of activation function for each node. The dinosaur footprint dataset has two classes: ornithischian <span epub:type="pagebreak" id="page_77"/>(class 0) and theropod (class 1). Therefore, the output node should use a sigmoid activation function to give us a likelihood of class 1 membership. The network’s output value estimates the probability that the input image represents a theropod. If the probability is above 50 percent, we’ll assign the input to class 1; otherwise, into class 0 it goes. We’ll stick with rectified linear unit activations for the hidden layer nodes, as we have for all the models in this chapter. All that remains is to select the number of hidden layers and the number of nodes per layer.</p>
<p class="indent">There are 1,336 training samples in the footprints dataset. That’s not a lot, and we aren’t augmenting the dataset, so we need a smallish model. Large models, meaning many nodes and layers, require large training sets; otherwise, there are too many weights and biases to learn relative to the number of training samples. Therefore, we’ll limit ourselves to trying at most two hidden layer models for the footprints dataset. As for the number of nodes in the hidden layers, we’ll let the first hidden layer vary from very small to nearly twice the input size of 1,600 features (the 40×40-pixel image unraveled). If we try a second hidden layer, we’ll restrict the number of nodes to no more than half the number in the first hidden layer.</p>
<p class="indent">First, we’ll train a collection of one- and two-layer architectures. Second, we’ll train the best performing of those 100 times to give us an average level of performance. <a href="ch04.xhtml#ch04tab1">Table 4-1</a> presents the trial models’ results.</p>
<p class="tabcap" id="ch04tab1"><strong>Table 4-1:</strong> Trial Architectures with the Dinosaur Footprint Dataset</p>
<div class="bqparan">
<table class="all">
<colgroup>
<col style="width:30%"/>
<col style="width:30%"/>
<col style="width:40%"/>
</colgroup>
<thead>
<tr>
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Accuracy (%)</strong></p></th>
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Architecture</strong></p></th>
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Weights and biases</strong></p></th>
</tr>
</thead>
<tbody>
<tr>
<td class="gray" style="vertical-align: top"><p class="noindent">59.4</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">10</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">16,021</p></td>
</tr>
<tr>
<td style="vertical-align: top"><p class="noindent-tab">77.0</p></td>
<td style="vertical-align: top"><p class="noindent-tab">400</p></td>
<td style="vertical-align: top"><p class="noindent-tab">640,801</p></td>
</tr>
<tr>
<td class="gray" style="vertical-align: top"><p class="noindent">76.7</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">800</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">1,281,601</p></td>
</tr>
<tr>
<td style="vertical-align: top"><p class="noindent-tab"><strong>81.2</strong></p></td>
<td style="vertical-align: top"><p class="noindent-tab"><strong>2,400</strong></p></td>
<td style="vertical-align: top"><p class="noindent-tab"><strong>3,844,801</strong></p></td>
</tr>
<tr>
<td class="gray" style="vertical-align: top"><p class="noindent">75.8</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">100, 50</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">165,201</p></td>
</tr>
<tr>
<td style="vertical-align: top"><p class="noindent-tab"><strong>81.2</strong></p></td>
<td style="vertical-align: top"><p class="noindent-tab"><strong>800, 100</strong></p></td>
<td style="vertical-align: top"><p class="noindent-tab"><strong>1,361,001</strong></p></td>
</tr>
<tr>
<td class="gray" style="vertical-align: top"><p class="noindent">77.9</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">2,400, 800</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">5,764,001</p></td>
</tr>
</tbody>
</table>
</div>
<p class="indent">The network with a mere 10 nodes in its hidden layer was the worst, returning an accuracy of about 60 percent. A binary classifier that does nothing but flips a coin is correct about 50 percent of the time, so the 10-node network is performing only slightly above chance. We don’t want that one. Most of the other networks return accuracies in the mid- to upper 70s.</p>
<p class="indent">The two models in <strong>bold</strong> each produced just over 81 percent accuracy. The first used a single hidden layer of 2,400 nodes. The second used a hidden layer of 800 nodes, followed by another with 100 nodes. Both models produced the same accuracy on the test set, but the 2,400-node model had nearly three times as many weights and biases as the two-layer model, so we’ll go with the two-layer model. (Bear in mind that the results in <a href="ch04.xhtml#ch04tab1">Table 4-1</a> represent a single training session, not the average of many. We’ll fix that shortly.)</p>
<p class="indent"><span epub:type="pagebreak" id="page_78"/>The two-layer model is still relatively large. We’re trying to learn 1.4 million parameters to condition the model to correctly classify the dinosaur footprint images. That’s a lot of parameters to learn, especially with a training set of only 1,336 samples. Fully connected neural networks grow quickly in terms of the number of parameters required. We’ll revisit this observation in <a href="ch05.xhtml">Chapter 5</a> when discussing convolutional neural networks.</p>
<p class="indent">We have our architecture: two hidden layers using rectified linear activation functions with 800 and 100 nodes, respectively, followed by a single node using a sigmoid to give us a likelihood of class 1 membership. Training the model 100 times on the footprints dataset returned an average accuracy of 77.4 percent, with a minimum of 69.3 percent and a maximum of 81.5 percent. Let’s put this result in its proper relation to those of <a href="ch03.xhtml">Chapter 3</a>; see <a href="ch04.xhtml#ch04tab2">Table 4-2</a>.</p>
<p class="tabcap" id="ch04tab2"><strong>Table 4-2:</strong> Dinosaur Footprint Models</p>
<div class="bqparan">
<table class="all">
<colgroup>
<col style="width:50%"/>
<col style="width:50%"/>
</colgroup>
<thead>
<tr>
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Model</strong></p></th>
<th class="table-h" style="vertical-align: top"><p class="noindent-tab"><strong>Accuracy (%)</strong></p></th>
</tr>
</thead>
<tbody>
<tr>
<td class="gray" style="vertical-align: top"><p class="noindent">RF300</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">83.3</p></td>
</tr>
<tr>
<td style="vertical-align: top"><p class="noindent-tab">RBF SVM</p></td>
<td style="vertical-align: top"><p class="noindent-tab">82.4</p></td>
</tr>
<tr>
<td class="gray" style="vertical-align: top"><p class="noindent">7-NN</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">80.0</p></td>
</tr>
<tr>
<td style="vertical-align: top"><p class="noindent-tab">3-NN</p></td>
<td style="vertical-align: top"><p class="noindent-tab">77.6</p></td>
</tr>
<tr>
<td class="gray" style="vertical-align: top"><p class="noindent"><strong>MLP</strong></p></td>
<td class="gray" style="vertical-align: top"><p class="noindent"><strong>77.4</strong></p></td>
</tr>
<tr>
<td style="vertical-align: top"><p class="noindent-tab">1-NN</p></td>
<td style="vertical-align: top"><p class="noindent-tab">76.1</p></td>
</tr>
<tr>
<td class="gray" style="vertical-align: top"><p class="noindent">Linear SVM</p></td>
<td class="gray" style="vertical-align: top"><p class="noindent">70.7</p></td>
</tr>
</tbody>
</table>
</div>
<p class="indent">Recall that RF300 means a random forest with 300 trees, SVM refers to a support vector machine, and, somewhat confusingly, NN refers to a nearest neighbor classifier. I’m using MLP (multilayer perceptron) as a stand-in for our neural network. <a href="glossary.xhtml#glo70"><em>Multilayer perceptron</em></a> is an old but still common name for the traditional neural networks we’ve been discussing in this chapter—notice the link back to Rosenblatt’s original Perceptron from the late 1950s.</p>
<p class="indent">Our neural network wasn’t the best performer on this dataset. In fact, it was one of the worst. Additional tweaking might move it up a place or two on the list, but this level of performance is typical, in my experience, and contributed to the general perception (pun intended) before the deep learning revolution that neural networks are “meh” models—run-of-the-mill, nothing to write home about.</p>
<p class="center">****</p>
<p class="indent">This chapter introduced the fundamental ideas behind modern neural networks. The remainder of the book builds on the basic concepts covered in this chapter. Here are the principal takeaways:</p>
<ul>
<li class="noindent">Neural networks are collections of nodes (neurons) that accept multiple inputs and produce a single number as output.</li>
<li class="noindent">Neural networks are often arranged in layers so that the current layer’s input is the previous layer’s output.</li>
<li class="noindent"><span epub:type="pagebreak" id="page_79"/>Neural networks are randomly initialized, so repeated training leads to differently performing models.</li>
<li class="noindent">Neural networks are trained by gradient descent, using the gradient direction supplied by backpropagation to update the weights and biases iteratively.</li>
</ul>
<p class="indent">Now, let’s press on to investigate convolutional neural networks, the architecture that ushered in the deep learning revolution. This chapter brought us to the early 2000s. The next moves us to 2012 and beyond.</p>
<div class="box5">
<p class="boxtitle-c"><strong>KEY TERMS</strong></p>
<p class="noindent">activation function, architecture, backward pass, bias, data augmentation, epoch, forward pass, global minimum, gradient descent, hidden layer, learning rate, local minimum, loss, minibatch, multilayer perceptron, neuron, node, overfitting, preprocessing, rectified linear unit, regularizer, sigmoid, stochastic gradient descent, weight<span epub:type="pagebreak" id="page_80"/></p>
</div>
</div></body></html>