- en: '**3'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3'
- en: SEQUENTIAL LOGIC**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**顺序逻辑**'
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: The combinatorial logic you learned about in the last chapter “goes with the
    flow.” In other words, the outputs change in response to the inputs. But we can’t
    build computers out of combinatorial logic alone, because it doesn’t give us any
    way to remove something from the flow and remember it. You can’t add up all the
    numbers from 1 to 100, for example, unless you can keep track of where you are.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你在上一章中学习的组合逻辑是“随流而动”。换句话说，输出会响应输入而变化。但我们不能仅仅用组合逻辑来构建计算机，因为它没有提供任何从流中移除某物并记住它的方法。例如，你无法加总从1到100的所有数字，除非你能够跟踪自己处于何处。
- en: You’ll learn about *sequential logic* in this chapter. The term comes from the
    word *sequence*, which means “one thing after another in time.” As a human, you
    have intuitive knowledge about time, just as you do about counting on your fingers,
    but that doesn’t mean that time is natural for digital circuitry. We have to create
    it somehow.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍*顺序逻辑*。该术语来自于*序列*一词，意为“时间中一个接一个的事物”。作为人类，你对时间有直观的认识，就像你知道如何用手指计数一样，但这并不意味着时间对数字电路来说是自然的。我们必须以某种方式创造它。
- en: Combinatorial logic deals only with the present state of inputs. Sequential
    logic, however, deals with both the present and the past. In this chapter, you’ll
    learn about circuitry both for generating time and for remembering things. We’ll
    trace some of the various technologies that have been used for these purposes
    from their early roots through the present day.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 组合逻辑仅处理输入的当前状态。然而，顺序逻辑既处理当前状态，也处理过去的状态。在本章中，你将学习用于生成时间和记忆的电路。我们将追溯一些自早期至今用于这些目的的各种技术。
- en: '**Representing Time**'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**表示时间**'
- en: We measure time using some sort of *periodic* function, such as the rotation
    of the Earth. We call one full rotation a day, which we subdivide into smaller
    units such as hours, minutes, and seconds. We could define a second as 1/86,400^(th)
    of an Earth rotation, since there are 86,400 seconds in a day.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用某种*周期性*函数来测量时间，比如地球的旋转。我们把一次完整的旋转叫做一天，并将其划分为小时、分钟和秒等更小的单位。我们可以将一秒定义为地球旋转的1/86,400^(th)，因为一天有86,400秒。
- en: In addition to using an external event like the rotation of the Earth, we can
    also generate our own periodic functions by applying certain elements of physics,
    such as the time that it takes for a pendulum to swing. This technique produced
    the “tick tock” sound in old grandfather clocks. Of course, to be useful, the
    pendulum has to be calibrated to the measured length of a second.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用地球旋转这样的外部事件，我们还可以通过应用物理学的某些元素来生成我们自己的周期性函数，例如摆钟摆动所需的时间。这种技术在古老的挂钟中产生了“滴答滴答”的声音。当然，要有用，摆钟必须根据秒的测量长度进行校准。
- en: With computers, we’re working with electronics, so we need a periodic electrical
    signal. We could generate one by placing a switch so that it’s whacked by a pendulum.
    But unless you’re a serious steampunk geek, you probably don’t want a pendulum-powered
    computer. We’ll learn about more modern approaches in the next section.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机中，我们处理的是电子学，因此我们需要一个周期性的电信号。我们可以通过设置一个开关，让它被摆钟敲击，从而生成一个周期信号。但除非你是一个狂热的蒸汽朋克迷，否则你可能不希望计算机是由摆钟驱动的。我们将在下一节学习更现代的解决方法。
- en: '***Oscillators***'
  id: totrans-10
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***振荡器***'
- en: 'Let’s look at a trick we can do with an inverter: we can connect the output
    to the input, as shown in [Figure 3-1](ch03.xhtml#ch03fig01).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下用反相器能做的一个技巧：我们可以将输出连接到输入，如[图3-1](ch03.xhtml#ch03fig01)所示。
- en: '![Image](../images/03fig01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/03fig01.jpg)'
- en: '*Figure 3-1: An oscillator*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3-1：一个振荡器*'
- en: This produces *feedback*, just like what you get when a microphone is too close
    to a loudspeaker. The output of the inverter bounces back and forth, or *oscillates*,
    between 0 and 1\. The speed at which it oscillates is a function of the propagation
    delay (see “[Propagation Delay](ch02.xhtml#ch02lev2sec14)” on [page 57](ch02.xhtml#page_57)),
    and that tends to vary with temperature. It would be useful to have an oscillator
    with a stable frequency so that we could generate an accurate time reference.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生*反馈*，就像麦克风太靠近扬声器时的情况一样。反相器的输出会在0和1之间来回*振荡*。它振荡的速度是传播延迟的函数（参见“[传播延迟](ch02.xhtml#ch02lev2sec14)”在[第57页](ch02.xhtml#page_57)的内容），而且通常会随着温度的变化而变化。拥有一个稳定频率的振荡器会非常有用，这样我们就能生成准确的时间参考。
- en: A cost-effective way to do this is with a crystal. Yes, very new age. Crystals,
    like magnets, have a relationship with electricity. If you attach *electrodes*
    (wires) to a crystal and give it a squeeze, it’ll generate electricity. And if
    you put some electricity on those wires, the crystal will bend. This is called
    the *piezoelectric* effect, and it was discovered by brothers Paul-Jacques (1855–1941)
    and Pierre (1859–1906) Curie in the late 1800s. The piezoelectric effect has all
    sorts of applications. A crystal can pick up sound vibrations, making a microphone.
    Sound vibrations generated by applying electricity to crystals are responsible
    for the annoying beeps made by many appliances. You can spot a crystal in a circuit
    diagram by the symbol shown in [Figure 3-2](ch03.xhtml#ch03fig02).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一种经济高效的方法是使用晶体。没错，非常现代。像磁铁一样，晶体与电有关系。如果你将*电极*（导线）连接到晶体上并对其施加压力，它会产生电力。如果你给这些导线施加一些电流，晶体就会弯曲。这就是所谓的*压电效应*，它是由保罗-雅克（1855–1941）和皮埃尔（1859–1906）居里兄弟在19世纪末发现的。压电效应有各种各样的应用。晶体可以拾取声音振动，制造麦克风。通过施加电流到晶体产生的声音振动是许多家电发出烦人“嘀嘀声”的原因。在电路图中，你可以通过[图3-2](ch03.xhtml#ch03fig02)中显示的符号找到晶体。
- en: '![Image](../images/03fig02.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/03fig02.jpg)'
- en: '*Figure 3-2: The crystal schematic symbol*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3-2：晶体的示意符号*'
- en: A crystal oscillator alternately applies electricity to a crystal and receives
    electricity back, using electronic single-pole, double-throw switches. The time
    it takes a crystal to do this is predictable and very accurate. Quartz is one
    of the best crystal materials to use. That’s why you see advertisements for accurate
    quartz timepieces. Keep in mind when you see the price tag on a fancy watch that
    a really good crystal retails for only about 25 cents.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 晶体振荡器交替地向晶体施加电流并接收回电，使用电子单极双掷开关。晶体完成这个过程的时间是可以预测的，而且非常精确。石英是最好的晶体材料之一。这就是为什么你会看到关于精确石英时钟的广告。记住，当你看到一款豪华手表的价格标签时，一块真正好的晶体零售价仅约为25美分。
- en: '***Clocks***'
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***时钟***'
- en: Oscillators give us a way to measure time, as you’ve seen. Computers need to
    keep time for obvious reasons, like being able to play a video at a consistent
    speed. But there’s another, lower-level reason why time is important. In [Chapter
    2](ch02.xhtml#ch02), we discussed how propagation delay affects the time that
    it takes circuitry to do things. Time gives us a way to wait, for example, for
    the worst-case delay in an adder before looking at the result so that we know
    it’s stable and correct.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 振荡器为我们提供了测量时间的方法，正如你所见。计算机需要保持时间，这有显而易见的原因，例如能够以一致的速度播放视频。但还有一个更低级的原因，说明时间为什么很重要。在[第二章](ch02.xhtml#ch02)中，我们讨论了传播延迟如何影响电路完成任务的时间。时间为我们提供了一种等待的方法，例如，在查看加法器结果之前，等待最坏情况下的延迟，以确保结果稳定且正确。
- en: Oscillators supply clocks to computers. A computer’s clock is like the drummer
    in a marching band; it sets the pace for the circuitry. The maximum clock speed
    or fastest tempo is determined by the propagation delays.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 振荡器为计算机提供时钟。计算机的时钟就像行进乐队中的鼓手；它为电路定下节奏。最大时钟速度或最快节奏由传播延迟决定。
- en: Component manufacturing involves a lot of statistics because there’s a lot of
    variance from part to part. The *binning* process puts components into different
    bins, or piles, depending on their measured characteristics. The fastest parts
    that fetch the highest price go into one bin; slower, less expensive parts go
    into another; and so on. It’s not practical to have an infinite number of bins,
    so there’s variance within the parts in a bin, although it’s less than the variance
    for the whole lot of parts. This is one reason why propagation delays are specified
    as a range; manufacturers provide minimum and maximum values in addition to a
    typical value. A common logic circuit design error is to use the typical values
    instead of the minimums and maximums. When you hear about people *overclocking*
    their computers, it means they’re gambling that their part was statistically in
    the middle of its bin and that its clock can be increased by some amount without
    causing the part to fail.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 组件制造涉及大量的统计数据，因为不同零件之间有很大的差异。*分选*过程根据测量的特性将组件分到不同的桶或堆中。最快的零件，价格最高的，进入一个桶；较慢的、价格较低的零件进入另一个桶；依此类推。没有实际可行的方法可以有无限数量的桶，所以同一桶中的零件仍然会有差异，尽管比整个批次的差异要小。这也是为什么传播延迟通常被指定为一个范围的原因；制造商提供最小值、最大值以及典型值。一个常见的逻辑电路设计错误是使用典型值而不是最小值和最大值。当你听说有人*超频*他们的计算机时，意味着他们在赌博，假设他们的零件在其分选桶的中间位置，且他们的时钟可以在不导致零件失败的情况下增加一定量。
- en: '***Latches***'
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***触发器***'
- en: Now that we have a source of time, let’s try to remember a single bit of information.
    We can do that with feedback, such as tying the output of an OR gate back to an
    input, as shown in [Figure 3-3](ch03.xhtml#ch03fig03). This doesn’t create an
    oscillator such as we saw in [Figure 3-1](ch03.xhtml#ch03fig01), since there’s
    no inversion. Assume that *out* starts off at 0 in the circuit in [Figure 3-3](ch03.xhtml#ch03fig03).
    Now, if *in* goes to 1, *out* does too, and because it’s connected to another
    input it stays that way, even if *in* goes back to 0\. In other words, it remembers.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了时间源，让我们尝试记住一个单一的位信息。我们可以通过反馈来实现这一点，例如将或门的输出反馈到输入，如[图 3-3](ch03.xhtml#ch03fig03)所示。这并没有像[图
    3-1](ch03.xhtml#ch03fig01)中看到的那样创建一个振荡器，因为没有反向作用。假设在[图 3-3](ch03.xhtml#ch03fig03)中的电路中，*out*一开始是0。如果*in*变为1，*out*也会变为1，且由于它连接到另一个输入，它会保持这一状态，即使*in*回到0。换句话说，它记住了。
- en: '![Image](../images/03fig03.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/03fig03.jpg)'
- en: '*Figure 3-3: An OR gate latch*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-3: 或门触发器*'
- en: Of course, this scheme needs some work because there’s no way to make *out*
    be 0 again. We need a way to reset it by disconnecting the feedback, as shown
    in [Figure 3-4](ch03.xhtml#ch03fig04).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个方案需要一些改进，因为无法将*out*再设为0。我们需要一种通过断开反馈来重置它的方法，如[图 3-4](ch03.xhtml#ch03fig04)所示。
- en: '![Image](../images/03fig04.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/03fig04.jpg)'
- en: '*Figure 3-4: An AND-OR gate latch*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-4: 与或门触发器*'
- en: Note that we’ve labeled the output of the inverter *reset*. Putting a line over
    a symbol is hardware-speak meaning “the opposite.” It means that something is
    true when it’s a 0 and false when it’s a 1\. Sometimes this is referred to as
    *active low* instead of *active high*, meaning that it does its thing when it’s
    0 instead of 1\. The line is pronounced “bar,” so in speech the signal would be
    referred to as “reset bar.”
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已将反相器的输出标记为*reset*。符号上加一条线是硬件术语，表示“相反”。这意味着当信号为0时它为真，信号为1时它为假。有时这被称为*低电平有效*，与*高电平有效*相对，意味着它在信号为0时才起作用，而不是在信号为1时。“条”字是指线，所以在口语中该信号会被称为“reset条”。
- en: When *reset* is low, *reset* is high, so the output from the OR gate is fed
    back into the input. When *reset* goes high, *reset* goes low, breaking that feedback
    so that *out* goes to 0.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当*reset*为低电平时，*reset*为高电平，因此来自或门的输出反馈到输入。当*reset*变为高电平时，*reset*变为低电平，断开反馈，使得*out*变为0。
- en: '[Figure 3-5](ch03.xhtml#ch03fig05) shows an *S-R latch*, a slightly cleverer
    way of building a bit of memory. *S-R* stands for *set-reset*. It has active low
    inputs and *complementary* outputs, meaning one is active low and one is active
    high. You could build a version of this that has active high inputs by using NOR
    gates, but NOR gates are often more power-hungry than NAND gates, in addition
    to being more complicated and expensive to build.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-5](ch03.xhtml#ch03fig05)展示了一种*S-R 触发器*，这是一种稍微聪明一些的内存构建方式。*S-R*代表*设置-复位*。它具有低电平有效的输入和*互补*输出，意味着一个是低电平有效，另一个是高电平有效。你可以通过使用NOR门构建一个具有高电平有效输入的版本，但NOR门通常比NAND门消耗更多电力，而且比NAND门更复杂、制造成本更高。'
- en: '![Image](../images/03fig05.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-5: An S-R latch*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: The case where both *set* and *reset* are active is weird and not intended for
    use, because both outputs are true. Also, if both inputs become inactive (that
    is, transition from 0 to 1) at the same time, the state of the outputs is not
    predictable because it’s dependent on the propagation delays.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The circuit in [Figure 3-5](ch03.xhtml#ch03fig05) has a nice property that the
    circuit in [Figure 3-4](ch03.xhtml#ch03fig04) does not, which is that its design
    is symmetrical. That means the propagation delays are similar for both the *set*
    and *reset* signals.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '***Gated Latches***'
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now that we have some way of remembering information, let’s look at what it
    takes to remember something at a point in time. The circuit in [Figure 3-6](ch03.xhtml#ch03fig06)
    has an extra pair of gates added to the inputs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/03fig06.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-6: A gated S-R latch*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, when the *gate* input is inactive (high), it doesn’t matter
    what *set* and *reset* are doing; the outputs won’t change because the inputs
    to the S and R gates will both be 1.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Because we want to remember one bit of information, the next improvement we
    can make is to add an inverter between the *set* and *reset* inputs so that we
    need only a single data input, which we’ll abbreviate as *D*. This modification
    is shown in [Figure 3-7](ch03.xhtml#ch03fig07).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/03fig07.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-7: A gated* D *latch*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Now, if *D* is a 1 when the *gate* is low, the *Q* output will be set to 1\.
    Likewise, if *D* is a 0 when the *gate* is low, the *Q* output will be set to
    0\. Changes on *D* when *gate* is high have no effect. That means we can remember
    the state of *D*. You can see this in the timing diagram shown in [Figure 3-8](ch03.xhtml#ch03fig08).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/03fig08.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-8: A gated* D *latch timing diagram*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this circuit is that changes in *D* pass through whenever the
    *gate* is low, as you can see in the shaded section. This means we have to count
    on *D* being “well-behaved” and not changing when the “gate” is “open.” It would
    be better if we could make the opening instantaneous. We’ll see how to do that
    in the next section.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '***Flip-Flops***'
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we discussed in the last section, we want to minimize the chances of getting
    incorrect results due to changing data. The way that’s commonly done is to use
    the transition between logic levels to grab the data instead of grabbing it when
    the logic level has a particular value. These transitions are called *edges*.
    You can think of an edge as a decision criterion for time. Back in [Figure 3-8](ch03.xhtml#ch03fig08),
    you can see the almost-instantaneous transition between logic levels. Edge-triggered
    latches are called *flip-flops*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Latches are a building block used to make flip-flops. We can construct a positive
    edge-triggered flip-flop called a *D flip-flop* by cleverly combining three S-R
    latches, as shown in [Figure 3-9](ch03.xhtml#ch03fig09). *Positive edge-triggered*
    means that the flip-flop operates on the transition from a logic 0 to a logic
    1; a *negative edge-triggered* flip-flop would operate on the transition from
    a logic 1 to a logic 0.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/03fig09.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-9: A* D *flip-flop design*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: This circuit can be somewhat mind-boggling. The two gates on the right form
    an S-R latch. We know from [Figure 3-5](ch03.xhtml#ch03fig05) that those outputs
    won’t change unless either *S* or *R* goes low.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-10](ch03.xhtml#ch03fig10) shows how the circuit behaves for various
    values of *D* and *clock*. The thin lines show logic 0s; the thick lines are logic
    1s.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/03fig10.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-10: A* D *flip-flop operation*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Starting at the left, you can see that when the clock is 0, the value of *D*
    doesn’t matter because both *S* and *R* are high, so the state of the latch on
    the right-hand side of [Figure 3-9](ch03.xhtml#ch03fig09) is unchanged. Moving
    toward the right, you can see in the next two diagrams that if *R* is low, changing
    the value of *D* has no effect. Likewise, the two rightmost diagrams show that
    if *S* is low, changing the value of *D* has no effect. The upshot is that changes
    to *D* have no effect when the clock is either high or low.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at what happens when the clock changes from low to high, as
    shown in [Figure 3-11](ch03.xhtml#ch03fig11).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/03fig11.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-11: A* D *flip-flop positive edge operation*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: You can see on the left that when the clock is low and *D* is high, *S* and
    *R* are high, so nothing changes. But when the clock changes to 1, *S* goes low,
    which changes the state of the flip-flop. On the right, you can see similar behavior
    when *D* is low and the clock goes high, causing *R* to go low and changing the
    flip-flop state. You saw in [Figure 3-10](ch03.xhtml#ch03fig10) that no other
    changes matter.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: In 1918 British physicists William Eccles and Frank Jordan invented the first
    electronic version of a flip-flop, which used vacuum tubes. [Figure 3-12](ch03.xhtml#ch03fig12)
    shows the diagram for a slightly less antique *D flip-flop* called the 7474.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/03fig12.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-12: A D flip-flop*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: The D flip-flop has complementary *Q* and *Q* (outputs and *S* (set) and *R*
    (reset) inputs. It’s a little confusing, as the diagram shows *S* and *R*; it’s
    the combination of those with the ○ that make them *S* and *R*. So, except for
    the mysterious things on the left-hand side, it’s just like our S-R latch. The
    mysterious things are two extra inputs, *D* for data and *CK* for clock, which
    is represented by the triangle. It’s positive edge-triggered, so the value of
    the *D* input is stored whenever the signal on the *CK* goes from a 0 to a 1.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Edge-triggered devices have other timing considerations in addition to propagation
    delay. There is the *setup time*, which is the amount of time before the clock
    edge that the signal must be stable, and the *hold time*, which is the amount
    of time after the clock edge that the signal must be stable. These are shown in
    [Figure 3-13](ch03.xhtml#ch03fig13).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 边沿触发设备除了传播延迟之外还有其他时序考虑因素。还有*建立时间*，即在时钟边沿之前，信号必须保持稳定的时间；以及*保持时间*，即在时钟边沿之后，信号必须保持稳定的时间。这些在[图3-13](ch03.xhtml#ch03fig13)中有所展示。
- en: '![Image](../images/03fig13.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/03fig13.jpg)'
- en: '*Figure 3-13: Setup and hold times*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3-13：建立时间和保持时间*'
- en: As you can see, we don’t have to care what’s happening on the *D* input except
    during the setup and hold times surrounding the clock edge. And, as with all other
    logic, the output is stable after the propagation delay time and stays stable
    independent of the *D* input. Setup and hold times are typically denoted by t[setup]
    and t[hold].
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们不必关心除时钟边沿周围的建立和保持时间外，*D*输入上发生了什么。而且，像所有其他逻辑一样，输出在传播延迟时间后是稳定的，并且保持稳定，不受*D*输入的影响。建立时间和保持时间通常用t[setup]和t[hold]表示。
- en: The edge behavior of flip-flops works well with clocks. We’ll see an example
    in the next section.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 触发器的边沿行为与时钟配合得很好。我们将在下一节中看到一个示例。
- en: '***Counters***'
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***计数器***'
- en: Counting is a common application of flip-flops. For example, we could count
    time from an oscillator and drive a display with a decoder to make a digital clock.
    [Figure 3-14](ch03.xhtml#ch03fig14) shows a circuit that produces a 3-bit number
    (C[2], C[1], C[0]) that is the count of the number of times the *signal* changes
    from 0 to 1\. The *reset* signal can be used to set the counter to 0.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 计数是触发器的一个常见应用。例如，我们可以从振荡器计数时间，并通过解码器驱动显示器来制作一个数字时钟。[图3-14](ch03.xhtml#ch03fig14)展示了一个电路，它产生一个3位数字（C[2]，C[1]，C[0]），该数字表示*信号*从0到1变化的次数。*复位*信号可用于将计数器设置为0。
- en: '![Image](../images/03fig14.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/03fig14.jpg)'
- en: '*Figure 3-14: A 3-bit ripple counter*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3-14：一个3位的波纹计数器*'
- en: This counter is called a *ripple counter* because the result ripples from left
    to right, not because it’s useful for counting bottles of cheap wine. C[0] changes
    C[1], C[1] changes C[2], and so on if there are more bits. Since the *D* input
    of each flip-flop is connected to its *Q* output, it will change state on every
    positive transition of the *CK* signal.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计数器被称为*波纹计数器*，因为结果是从左到右传播的，而不是因为它对计算便宜葡萄酒瓶子的数量有用。C[0]改变C[1]，C[1]改变C[2]，如果有更多位的话就如此类推。由于每个触发器的*D*输入连接到其*Q*输出，它将在每个*CK*信号的正向跃变时改变状态。
- en: This is also called an *asynchronous* counter because everything just happens
    when it gets around to it. The problem with asynchronous systems is that it’s
    hard to know when to look at the result. The outputs (C[2], C[1], C[0]) are invalid
    during rippling. You can see how it takes longer to get a result for each successive
    bit in [Figure 3-15](ch03.xhtml#ch03fig15), where the gray areas represent undefined
    values due to propagation delay.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这也被称为*异步*计数器，因为一切事情都是在它有空时发生的。异步系统的问题是很难知道何时查看结果。在波纹传递期间，输出（C[2]，C[1]，C[0]）是无效的。你可以在[图3-15](ch03.xhtml#ch03fig15)中看到，随着每个位的连续推进，得到结果的时间变得更长，其中灰色区域代表由于传播延迟导致的未定义值。
- en: '![Image](../images/03fig15.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/03fig15.jpg)'
- en: '*Figure 3-15: Ripple counter timing*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3-15：波纹计数器时序*'
- en: The *timing diagram* on the left shows that we get a valid 3-bit number after
    the propagation delays settle out. But on the right, you can see that we’re trying
    to count faster than the propagation delays permit, so there are times where no
    valid number is produced.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的*时序图*显示，我们在传播延迟稳定后获得一个有效的3位数字。但是在右侧，你可以看到我们试图以超过传播延迟允许的速度进行计数，因此有时没有产生有效的数字。
- en: This is a variation of the problem we saw with the ripple-carry adder back in
    [Figure 2-41](ch02.xhtml#ch02fig41). Just as we were able to solve that problem
    with the carry look-ahead design, we can address the ripple problem with a *synchronous*
    counter design.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在[图2-41](ch02.xhtml#ch02fig41)中看到的波纹进位加法器问题的变种。正如我们通过进位预见设计解决了那个问题一样，我们也可以通过*同步*计数器设计来解决波纹问题。
- en: Unlike the ripple counter, the synchronous counter outputs all change at the
    same time (in sync). This implies that all the flip-flops are clocked in parallel.
    A 3-bit synchronous counter is shown in [Figure 3-16](ch03.xhtml#ch03fig16).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与波纹计数器不同，同步计数器的所有输出同时变化（同步）。这意味着所有触发器是并行时钟的。在[图3-16](ch03.xhtml#ch03fig16)中展示了一个3位同步计数器。
- en: '![Image](../images/03fig16.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/03fig16.jpg)'
- en: '*Figure 3-16: A 3-bit synchronous counter*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-16：一个 3 位同步计数器*'
- en: You can see that all the flip-flops in the counter change state at the same
    time because they’re all clocked at the same time. Although propagation delay
    is still a factor in knowing when the outputs are valid, the cascade effect has
    been eliminated.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到计数器中的所有触发器同时改变状态，因为它们在同一时刻都被时钟驱动。尽管传播延迟仍然是决定输出何时有效的一个因素，但级联效应已经被消除。
- en: Counters are yet another functional building block, which means they have their
    own schematic symbol. In this case it’s yet another rectangular box, as you can
    see in [Figure 3-17](ch03.xhtml#ch03fig17).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 计数器是另一种功能性构建模块，这意味着它们有自己的电路符号。在这种情况下，它还是另一个矩形框，如你在[图 3-17](ch03.xhtml#ch03fig17)中看到的那样。
- en: '![Image](../images/03fig17.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/03fig17.jpg)'
- en: '*Figure 3-17: A counter schematic symbol*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-17：计数器的电路符号*'
- en: The figure includes a number of inputs we haven’t seen before. Counters are
    available that have some or all of these inputs. Most counters have a *CLR* input
    that clears the counter, setting it to 0\. Also common is an *EN* input that enables
    the counter—the counter doesn’t count unless enabled. Some counters can count
    in either direction; the *U/**D* input selects up or down. Finally, some counters
    have data inputs *D*[0–n] and a load signal *LD* that allows the counter to be
    set to a specific value.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 该图包括一些我们之前没有见过的输入。市场上有带有这些输入的计数器。大多数计数器都有一个*CLR*输入，用来清除计数器，将其设置为 0。另一个常见的输入是*EN*，它使能计数器——计数器只有在启用时才会计数。某些计数器可以向上或向下计数；*U/**D*输入选择向上或向下计数。最后，一些计数器有数据输入*D*[0–n]和一个加载信号*LD*，允许将计数器设置为特定值。
- en: Now that we have counters, we can use them to keep track of time. But that’s
    not the only thing we can do with flip-flops. We’ll start learning how to remember
    large amounts of information in the next section.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了计数器，可以用它们来跟踪时间。但这并不是我们可以用触发器做的唯一事情。我们将在下一节开始学习如何记住大量的信息。
- en: '***Registers***'
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***寄存器***'
- en: D flip-flops are good for remembering things. It’s a common enough application
    that you can get *registers*, which are a bunch of D flip-flops in a single package
    that share a common clock. [Figure 3-18](ch03.xhtml#ch03fig18) shows an example
    of a register holding the result of addition using the adder circuit discussed
    earlier.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: D 触发器适用于记忆数据。它是一个常见的应用，你可以得到*寄存器*，它们是将多个 D 触发器集成在一个封装中的部件，共享一个时钟。[图 3-18](ch03.xhtml#ch03fig18)展示了一个寄存器的例子，它保存了使用前面讨论的加法器电路的加法结果。
- en: '![Image](../images/03fig18.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/03fig18.jpg)'
- en: '*Figure 3-18: A register holding an adder result*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-18：一个保存加法器结果的寄存器*'
- en: Once the output of the adder has been clocked into the register, the operands
    can change without changing the result. Note that registers often have *enable*
    inputs similar to those we saw for counters.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦加法器的输出被时钟信号输入到寄存器中，操作数就可以发生变化而不改变结果。注意，寄存器通常具有类似于我们在计数器中看到的*使能*输入。
- en: '**Memory Organization and Addressing**'
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**内存组织和寻址**'
- en: We’ve seen that flip-flops are useful when we need to remember a bit and that
    registers are handy when we need to remember a collection of bits. What do we
    do when we need to remember a lot more information, though? For example, what
    if we want to be able to store several different addition results?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，当我们需要记住一个比特时，触发器很有用；当我们需要记住一组比特时，寄存器很方便。那么当我们需要记住更多信息时该怎么办呢？例如，如果我们希望能够存储多个不同的加法结果呢？
- en: 'Well, we can start with a big pile of registers. But now we have a new problem:
    how to specify the register we want to use. This situation looks like [Figure
    3-19](ch03.xhtml#ch03fig19).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们可以从一堆寄存器开始。但是现在我们面临一个新问题：如何指定我们想要使用的寄存器。这个情况看起来像是[图 3-19](ch03.xhtml#ch03fig19)。
- en: '![Image](../images/03fig19.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/03fig19.jpg)'
- en: '*Figure 3-19: Multiple registers*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-19：多个寄存器*'
- en: One way to solve this problem is to assign each register a number, as in the
    figure. We can have this number or *address* specify the register using one of
    our standard building blocks, the decoder from “[Building Decoders](ch02.xhtml#ch02lev2sec17)”
    on [page 63](ch02.xhtml#page_63). The decoder outputs are connected to the enable
    inputs on the registers.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是为每个寄存器分配一个编号，如图所示。我们可以使用这个编号或*地址*通过我们标准的构建模块之一——来自“[构建解码器](ch02.xhtml#ch02lev2sec17)”的解码器来指定寄存器，解码器的输出连接到寄存器的使能输入端。
- en: Next we need to be able to select the output from the addressed register. Fortunately,
    we learned how to build selectors in “[Building Selectors](ch02.xhtml#ch02lev2sec19)”
    on [page 65](ch02.xhtml#page_65), and they’re just what we need.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要能够选择来自目标寄存器的输出。幸运的是，我们在[第65页](ch02.xhtml#page_65)的“[构建选择器](ch02.xhtml#ch02lev2sec19)”中学习了如何构建选择器，它们正是我们需要的。
- en: 'Systems often have multiple memory components that need to be hooked together.
    Time for yet another of our standard building blocks: the *tri-state* output.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 系统通常有多个内存组件需要连接在一起。是时候用我们的标准构建模块之一：*三态*输出了。
- en: Putting it all together, a memory component looks like [Figure 3-20](ch03.xhtml#ch03fig20).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容整合起来，一个内存组件看起来像[图3-20](ch03.xhtml#ch03fig20)。
- en: '![Image](../images/03fig20.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/03fig20.jpg)'
- en: '*Figure 3-20: A memory component*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3-20：内存组件*'
- en: Memory components have a lot of electrical connections. If we want to do something
    with 32-bit numbers, we would need 32 connections each for the inputs and the
    outputs, plus connections for the address, control signals, and power. Programmers
    don’t have to worry about how to fit circuitry into packages or how to route wires,
    but hardware designers do. We can cut down on the number of connections by realizing
    that memory rarely needs to be read and written at the same time. We can get by
    with one set of data connections plus a *read/* *write* control. [Figure 3-21](ch03.xhtml#ch03fig21)
    shows a schematic of a simplified memory chip. The *enable* control turns the
    whole thing on and off so that multiple memory chips can be connected together.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 内存组件有很多电气连接。如果我们要处理32位数字，我们需要为输入和输出各自提供32个连接，同时还需要地址、控制信号和电源的连接。程序员不需要担心如何将电路适配到封装中或如何布线，但硬件设计师需要考虑这些问题。我们可以通过意识到内存很少需要同时进行读取和写入来减少连接数量。我们可以使用一组数据连接，再加上*读/*
    *写*控制来满足需求。[图3-21](ch03.xhtml#ch03fig21)显示了一个简化版内存芯片的原理图。*使能*控制用来开启和关闭整个系统，以便可以将多个内存芯片连接在一起。
- en: '![Image](../images/03fig21.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/03fig21.jpg)'
- en: '*Figure 3-21: A simplified memory chip*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3-21：简化的内存芯片*'
- en: You’ll notice that the figure uses big fat arrows for the address and data instead
    of showing the individual signals. We call groups of related signals *buses*,
    so the memory chip has an *address bus* and a *data bus*. Yup, it’s mass transit
    for bits.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，图中使用了粗大的箭头来表示地址和数据，而不是显示单个信号。我们将相关信号组称为*总线*，因此内存芯片有一个*地址总线*和一个*数据总线*。没错，这就像是位的公共交通系统。
- en: The next challenge in memory chip packaging comes when the memory size increases
    and lots of address bits need connections. Referring back to [Table 1-2](ch01.xhtml#ch01tab02)
    in [Chapter 1](ch01.xhtml#ch01), we’d need 32 address connections for a 4-GiB
    memory component.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 内存芯片封装中的下一个挑战是，当内存大小增加，需要连接大量地址位时。参考[第1章](ch01.xhtml#ch01)中的[表1-2](ch01.xhtml#ch01tab02)，我们需要32个地址连接来连接一个4-GiB的内存组件。
- en: 'Memory designers and road planners deal with similar traffic-management issues.
    Many cities are organized into grids, and that’s also how memory chips are laid
    out internally. You can see several rectangular regions that are chunks of memory
    in the CPU photomicrograph shown back in [Figure 2-3](ch02.xhtml#ch02fig03). The
    address is partitioned into two chunks: a *row* address and a *column* address.
    A memory location is addressed internally using the intersection of the row and
    column, as shown in [Figure 3-22](ch03.xhtml#ch03fig22).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 内存设计师和道路规划者处理着类似的交通管理问题。许多城市按网格组织，这也是内存芯片内部布局的方式。在[图2-3](ch02.xhtml#ch02fig03)中显示的CPU显微照片中，你可以看到几个矩形区域，这些区域是内存的一部分。地址被划分为两部分：*行*地址和*列*地址。内存位置通过行和列的交点来寻址，如[图3-22](ch03.xhtml#ch03fig22)所示。
- en: '![Image](../images/03fig22.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/03fig22.jpg)'
- en: '*Figure 3-22: Row and column addressing*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3-22：行和列寻址*'
- en: Obviously we don’t need to worry about the number of address lines in the 16-location
    memory shown in this figure. But what if there were a lot more? We could halve
    the number of address lines by *multiplexing* the row and column addresses. All
    we would need is registers on the memory chip to save them, as shown in [Figure
    3-23](ch03.xhtml#ch03fig23).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们不需要担心图中显示的16个内存位置的地址线数量。但如果有更多内存位置呢？我们可以通过*复用*行和列地址来减少地址线数量。我们所需要的只是内存芯片上的寄存器来保存这些地址，如[图3-23](ch03.xhtml#ch03fig23)所示。
- en: '![Image](../images/03fig23.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/03fig23.jpg)'
- en: '*Figure 3-23: Memory with address registers*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3-23：带地址寄存器的内存*'
- en: Since the address comes in two parts, it follows that performance would be better
    if we only had to change one part, such as by setting the row address and then
    varying the column address. This is what we find in today’s large memory chips.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Memory chips are described by their size in depth × width format. For example,
    a 256 × 8 chip would have 256 8-bit wide memory locations; a 64 Mib × 1 chip would
    have 64 mebibits.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '***Random-Access Memory***'
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The memory we’ve talked about so far is called *random-access memory*, or *RAM.*
    With RAM, the entire width of any memory location can be read or written in any
    order.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '*Static RAM*, or *SRAM*, is expensive but fast. It takes six transistors for
    each bit. Because transistors take up space, SRAM isn’t a great choice for storing
    billions or trillions of bits.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '*Dynamic memory (DRAM)* is a clever hack. Electrons are stored in microscopic
    buckets called *capacitors*, using only one transistor for the lids. The problem
    is, these buckets leak, so it’s necessary to *refresh* the memory every once in
    a while, which means regularly topping off the buckets. You have to be careful
    that the topping off doesn’t occur at a critical time that would conflict with
    accessing the memory; this was a problem with one of the first DRAM-based computers,
    the DEC LSI-11\. One of the interesting side effects of DRAM is that the buckets
    leak more when light shines on them. This enables them to be used as digital cameras.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'DRAM is used for large memory chips because of its high density (number of
    bits per area). Large memory chips mean lots of addresses, which means that DRAM
    chips use the multiplexed addressing scheme discussed in the previous section.
    Because of other internal design considerations, it’s only faster to save the
    row address using the row address strobe and then to vary the column address via
    the column address strobe. It’s an overused term, but rows are sometimes called
    *pages*. It’s comparable to reading a book like this one; it’s much easier to
    scan a page than it is to flip pages. Or, as stated by the great performance pioneer
    Jimmy Durante, best performance is a-ras-a-ma-cas. This is a very important consideration
    in programming: keeping things that are used together in the same row greatly
    improves performance.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Both SRAM and DRAM are *volatile* memory, which means that data can be lost
    when the power is interrupted. *Core* memory is an antique *nonvolatile* type
    of RAM that stores bits in *toroidal* (doughnut-shaped) pieces of iron, which
    you can see in [Figure 3-24](ch03.xhtml#ch03fig24). Toroids were magnetized in
    one direction for a 0 and the other for a 1\. The physics of toroids is cool because
    they’re very resistant to electromagnetic interference from outside the doughnut.
    In this type of memory, cores were arranged in a grid called a *plane* with row
    and column wires through them. There was also a third wire, called the *sense*
    wire, because the only way to read the state of a bit was to try to change it
    and then sense what happened. Of course, if you sensed that it changed, you had
    to change it back or the data would be lost, making the bit useless. That required
    a lot of circuitry in addition to all the stitching. Core was actually three-dimensional
    memory, as planes were assembled into bricks.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: While core is antique technology, the nonvolatile characteristic is still prized,
    and research continues making commercially practical *magnetoresistive* memory
    that combines the best of core memory and RAM.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/03fig24.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-24: Core memory*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '***Read-Only Memory***'
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Read-only memory*, or *ROM*, is not a very accurate name. Memory that could
    only be read but never written wouldn’t be useful. Even though the name has stuck,
    it’s more accurate to say that ROM is write-once memory. ROM can be written once
    and then read multiple times. ROM is important for devices that need to have a
    program built in, such as a microwave oven; you wouldn’t want to have to program
    your microwave every time you needed popcorn.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: One of the early forms of ROM was the Hollerith card, which later became known
    as the *IBM card*, shown in [Figure 3-25](ch03.xhtml#ch03fig25). Bits were punched
    into pieces of paper. Really! They were pretty cheap because American inventor
    Herman Hollerith (1860–1929) was big into cutting corners. Hollerith invented
    the card in the late 1800s, although it might be more accurate to say that he
    appropriated the idea from the Jacquard loom, which was invented by Joseph Marie
    Jacquard in 1801\. The Jacquard loom used punched cards to control the weaving
    pattern. Of course, Jacquard borrowed the idea from Basile Bouchon, who had invented
    a punched paper tape–controlled loom in 1725\. Sometimes it’s hard to distinguish
    between invention and appropriation, because the future is built on the past.
    Keep this in mind when you hear people arguing for longer and more restrictive
    patent and copyright laws; progress slows if we can’t build on the past.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/03fig25.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-25: An IBM card*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Early IBM card readers used switches to read the bits. Cards would be slid under
    a row of springy wires that poked through the holes and made contact with a piece
    of metal on the other side. Later versions, which worked by shining light through
    the holes onto a row of *photodetectors* on the other side, were considerably
    faster.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '*Punched paper tape* is a related ROM technology; rolls of paper tape with
    holes punched in it were used to represent bits (see [Figure 3-26](ch03.xhtml#ch03fig26)).
    Tape had an advantage over cards in that dropping a deck of cards would scramble
    the data. Then again, tape could tear and was difficult to repair; many a masking
    tape repair job clogged up the works.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/03fig26.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-26: Punched paper tape*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Cards and tape were very slow because they had to be physically moved in order
    to be read.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: A ROM variation called *core rope memory* was used in the Apollo flight computer
    (see [Figure 3-27](ch03.xhtml#ch03fig27)). Because it could be written only by
    sewing, it was impervious to interference—which is important in the harsh environment
    of space.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/03fig27.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-27: Core rope memory from the Apollo guidance computer*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: IBM cards and paper tape were *sequential* memory; that is, the data was read
    in order. Card readers couldn’t go backward, so they were really only good for
    long-term storage of data. The contents had to be read into some sort of RAM in
    order to be used. The first commercial availability of a single-chip microprocessor,
    the Intel 4004 in 1971, created demand for better program storage technology.
    These first microprocessors were used for devices like calculators that ran a
    fixed program. Along came *mask-programmable* ROM. A *mask* is a stencil used
    as part of the integrated circuit–manufacturing process. You’d write a program
    and send the bit pattern off to a semiconductor manufacturer along with a really
    big check. They’d turn it into a mask, and you’d get back a chip containing your
    program. It was read-only because there was no way to change it without writing
    another big check and having a different mask made. Mask-programmable ROM could
    be read in a random-access manner.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Masks were so expensive that they could be justified only for high-volume applications.
    Along came *programmable read-only memory (PROM)*, ROM chips that you could program
    yourself, but only once. The original mechanism for PROM involved melting nichrome
    (a nickel-chromium alloy) fuses on the chip. Nichrome is the same stuff that makes
    the glowing wires in your toaster.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: People would go through a big pile of PROM chips quickly when developing a program.
    Engineers are pain-adverse, so next came *erasable programmable read-only memory
    (EPROM)*. These chips were like PROMs, except that they had a quartz window on
    top and you could erase them by putting them under a special ultraviolet light.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Life got better with the introduction of *electrically erasable programmable
    read-only memory* (what a mouthful!), or *EEPROM*. This is an EPROM chip that
    can be erased electrically—no light, no quartz window. Erasing EEPROM is comparatively
    very slow, though, so it’s not something you want to do a lot. EEPROMs are technically
    RAM, since it’s possible to read and write the contents in any order. But because
    they’re slow to write and more expensive than RAM, they’re used as a substitute
    for ROMs.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '**Block Devices**'
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It takes time to talk to memory. Imagine you had to go to the store every time
    you needed a cup of flour. It’s much more practical to go to the store once and
    bring home a whole sack of flour. Larger memory devices use this principle. Think
    warehouse shopping for bits.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '*Disk drives*, also known as *mass storage*, are great for storing immense
    amounts of data. An 8-TB drive cost less than $200 when this book was written.
    They’re often referred to as *mass storage*. Some religious institutions use mass
    storage for their ceremonies in between use. Disk drives store bits on rotating
    magnetic platters, sort of like a lazy Susan. Bits periodically come around to
    where you’re sitting, and you use your hand to pluck them off or put them on.
    In a disk drive, your hand is replaced by the *disk head*.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Disk drives are relatively slow compared to other types of memory. If you want
    something that just passed by the head, you have to wait almost an entire rotation
    for it to come around again. Modern disks spin at 7,200 rotations per minute (RPM),
    which means a rotation takes slightly longer than 8 milliseconds. The big problem
    with disk drives is that they’re mechanical and wear out. Bearing wear is one
    of the big causes of disk failure. The difference between commercial and consumer-grade
    devices is primarily the amount of grease in the bearing—manufacturers are able
    to charge hundreds of dollars for something that costs less than a penny. Disk
    drives store data by magnetizing areas on the disk, which makes them nonvolatile
    just like core memory.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Disk drives are a trade-off between speed and density. They’re slow because
    of the time it takes for the bits you want to show up under the head, but because
    the data is being brought to the head, no space is required for address and data
    connections, unlike, for example, in a DRAM. [Figure 3-28](ch03.xhtml#ch03fig28)
    shows the insides of a disk drive. They’re built in sealed containers because
    dust and dirt would cause them to fail.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/03fig28.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-28: A disk drive*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Disks are block-addressable rather than byte-addressable. A *block* (historically
    called a *sector*) is the smallest unit that can be accessed. Disks have historically
    had 512-byte sectors, although newer devices have 4,096-byte sectors. That means
    in order to change a byte on a disk, you have to read an entire block, change
    the byte, and then write back the entire block. Disks contain one or more *platters*
    that are laid out as shown in [Figure 3-29](ch03.xhtml#ch03fig29).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/03fig29.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-29: Disk layout*'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Since all of the sectors contain the same number of bits, the *bit density*
    (bits/mm²) is greater at the center of each platter than it is at the outer edge.
    This is wasteful because there’s clearly room to cram more bits onto the outer
    tracks. Newer disks address this problem by dividing the disk into a set of *radial
    zones*, effectively having more sectors in the outer zones than in the inner ones.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: There are a couple of numbers that describe the performance of disk drives.
    Modern disks have a head on an actuator arm that moves radially across the disk;
    the position of the head divides the disks into tracks. The *seek time* is the
    amount of time that it takes to move the head from one track to another. It would,
    of course, be much faster to have one head per track so that seeking wasn’t necessary;
    you could get that on very old disk drives, but the tracks are too close together
    on modern disks to make that practical. In addition to the seek time, there’s
    the time it takes for the part of the disk you’re interested in to rotate so that
    it’s under the head, called *rotational latency*, which as we saw above is in
    the millisecond range.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Disk drives are often called *hard drives*. Originally, all disk drives were
    hard drives. The distinction arose when cheap removable storage devices called
    *floppy disks* appeared on the scene. Floppy disks were bendable, so calling the
    other type “hard” made them easy to differentiate.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'An antiquated variation on disk drives is *magnetic drum* storage, which was
    just what it sounds like: a rotating magnetic drum with stripes of heads on it.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '*Magnetic tape* is another nonvolatile storage technology that uses reels of
    magnetized tape. It is way slower than a disk drive, and it can take a long time
    to wind the tape to the requested position. Early Apple computers used consumer-grade
    audio cassettes for magnetic tape storage.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '*Optical disks* are similar to magnetic disks except that they use light instead
    of magnetism. You know these as CDs and DVDs. A big advantage of optical disks
    is that they can be mass-produced via printing. Preprinted disks are ROMs. PROM-equivalent
    versions that can be written once (CD-R, DVD-R) are also available, as are versions
    that can be erased and rewritten (CD-RW). [Figure 3-30](ch03.xhtml#ch03fig30)
    shows a close-up of a portion of an optical disk.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/03fig30.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-30: Optical disk data*'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '**Flash Memory and Solid State Disks**'
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Flash memory* is the most recent incarnation of EEPROM. It’s good solution
    for some applications, like music players and digital cameras. It works by storing
    electrons in buckets just like DRAM. In this case, the buckets are bigger and
    better built so they don’t leak. But the lid hinges on the buckets eventually
    wear out if they’re opened and closed too many times. Flash memory can be erased
    more quickly than EEPROM and is cheaper to make. It works like RAM for reading
    and also for writing a blank device filled with 0s. But although 0s can be turned
    into 1s, they can’t be turned back without being erased first. Flash memory is
    internally divided into blocks, and only blocks can be erased, not individual
    locations. Flash memory devices are random-access for reads, and block-access
    for writes.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Disk drives are slowly being replaced by *solid-state disk drives*, which are
    pretty much just flash memory packaged up to look like a disk drive. Right now
    their price per bit is much higher than spinning disks, but that’s expected to
    change. Because flash memory wears out, solid-state drives include a processor
    that keeps track of the usages in different blocks and tries to even it out so
    that all blocks wear out at the same rate.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '**Error Detection and Correction**'
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You never know when a stray cosmic ray is going to hit a piece of memory and
    corrupt the data. It would be nice to know when this happens and even nicer to
    be able to repair the damage. Of course, such improvements cost money and are
    not typically found in consumer-grade devices.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: We’d like to be able to detect errors without having to store a complete second
    copy of the data. And that wouldn’t work anyway, because we wouldn’t know which
    copy was correct. We could store two extra copies and assume that the matching
    pair (if any) is the right one. Computers designed for very harsh environments
    do this. They also use a more expensive circuit design that doesn’t burn up when
    hit by a proton. For example, the space shuttle had redundant computers and a
    voting system in the event that an error was detected.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'We can test for a 1-bit error using a method called *parity*. The idea is to
    add up the number of bits that are set to 1 and use an extra bit to store whether
    that sum is an odd or even number. We can do this by taking the XOR of the bits.
    There are two forms of this: in *even parity* the sum of the bits is used, and
    in *odd parity* the complement of the sum of the bits is used. This choice may
    seem, well, odd, but the nomenclature comes from the number of 1s or 0s including
    the parity bit.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: The left half of [Figure 3-31](ch03.xhtml#ch03fig31) shows the calculation of
    even parity; there are four 1s, so the parity is 0\. The right half shows the
    checking of the parity; a 0 out means that the data is good, or at least as good
    as we can tell with parity. The big problem with parity is that it’s one place
    where two wrongs sure look like a right; it only catches odd numbers of errors.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/03fig31.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-31: Even parity generation and checking*'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: There are more complicated methods, such as Hamming codes, invented by American
    mathematician Richard Hamming (1915–1998), which take more bits and allow for
    more errors to be detected and for some to be corrected. *Error checking and correcting
    (ECC)* memory chips are available that include this circuitry. They’re typically
    used in big data centers, not in consumer devices.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Methods like parity are good for data that is constantly changing. There are
    less expensive methods that allow for verification of static block data, such
    as a computer program. The simplest of these is the *checksum*, where the contents
    of every data location are summed into some *n*-bit value and the overflow bits
    are thrown away. The checksum can be compared against the program, usually just
    before it is run. The larger the checksum value (that is, larger *n*), the lower
    the chance of getting a false positive.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '*Cyclic redundancy checks*, or *CRCs*, are a mathematically better replacement
    for checksums. Hash codes are another. The goal is to calculate a verification
    number that is unique enough for the data so that for most changes, the check
    will no longer be correct.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware vs. Software**'
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The techniques used to make PROMs, EEPROMs, and flash aren’t just limited to
    memory. We’ll soon see how computer hardware is constructed from logic circuits.
    And since you’re learning programming, you know that programs include logic in
    their code, and you may know that computers expose logic to programs via their
    instruction sets. What’s the difference between doing that in hardware versus
    software? It’s a blurry line. To a large degree, there is little distinction except
    that it’s much easier to build software since there are no additional costs other
    than design time.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: You’ve probably heard the term *firmware*, which originally just referred to
    software in a ROM. But most firmware now lives in flash memory or even RAM, so
    the difference is minimal. And it’s even more complicated than that. It used to
    be that chips were designed by geeks who laid out circuits by sticking colored
    masking tape on big sheets of clear Mylar. In 1979 American scientists and engineers
    Carver Mead and Lynn Conway changed the world with their publication of *Introduction
    to VLSI Systems*, which helped kick-start the electronic design automation (EDA)
    industry. Chip design became software. Chips today are designed using specialized
    programming languages such as Verilog, VHDL, and SystemC.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Much of the time, a computer programmer is simply given a piece of hardware
    to use. But you might get the opportunity to participate in the design of a system
    that includes both hardware and software. The design of the interface between
    hardware and software is critical. There are countless examples of chips with
    unusable, unprogrammable, and unnecessary features.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Integrated circuits are expensive to make. In the early days, all chips were
    *full custom* designs. Chips are built up in layers, with the actual components
    on the bottom and metal layers on top to wire them together. *Gate arrays* were
    an attempt to lower the cost for some applications; a set of predesigned components
    was available, and only the metal layers were custom. Just like with memory, these
    were supplanted by PROM-equivalent versions that you could program yourself. And
    there was an EPROM equivalent that could be erased and reprogrammed.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Modern *field-programmable gate arrays (FPGAs)* are the flash memory equivalent;
    they can be reprogrammed in software. In many cases, using an FPGA is cheaper
    than using other components. FPGAs are very rich in features; for example, you
    can get a large FPGA that contains a couple of ARM processor cores. Intel recently
    purchased Altera and may include FPGAs on its processor chips. There’s a good
    chance you’ll work on a project containing one of these devices, so be prepared
    to turn your software into hardware.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, you’ve learned where computers get their sense of time. You
    were introduced to sequential logic, which, along with combinatorial logic from
    [Chapter 2](ch02.xhtml#ch02), provides us with all of the fundamental hardware
    building blocks. And you’ve learned something about how memory is built. We’ll
    put all of this knowledge together to make a computer in [Chapter 4](ch04.xhtml#ch04).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
