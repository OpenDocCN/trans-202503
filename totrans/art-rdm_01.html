<html><head></head><body>
<h2 class="h2" id="ch01"><span epub:type="pagebreak" id="page_1"/><strong><span class="big">1</span><br/>THE NATURE OF RANDOMNESS</strong></h2>
<div class="image1"><img alt="Image" src="../images/common.jpg"/></div>
<p class="noindent">Random processes power the systems we’ll develop later in this book. This chapter introduces specific random processes, from those that are truly random to those that are deterministic but still random enough to use—that is, pseudorandom and quasirandom processes.</p>
<p class="indent">We’ll begin with a brief discussion of the relationship between probability and randomness. After learning how to determine whether a process is random, we’ll explore truly random processes, meaning processes strongly influenced by true randomness. We’ll also learn the difference between pseudorandom and quasirandom processes. Finally, we’ll use Python to create the <span class="literal">RE</span> class, the randomness engine we’ll use in all of our experiments.</p>
<h3 class="h3" id="ch00lev1_5"><strong>Probability and Randomness</strong></h3>
<p class="noindent"><em>Probability distributions</em> represent the possible values a <em>random variable</em> can take and how likely each value is to occur. For us, a random variable is the output of a random process.</p>
<p class="indent">Probability distributions come in two varieties. <em>Continuous</em> probability distributions return values from an infinite set, meaning any real number in the allowed range. Here, the word <em>real</em> means elements of the set of real <span epub:type="pagebreak" id="page_2"/>numbers, denoted <span class="ent">ℝ</span>, which are all the numbers on the number line. <em>Discrete</em> probability distributions are restricted to returning values from a finite set of values, like the heads or tails of a coin or the numbers on a die.</p>
<p class="indent">Random processes generate values, known as <em>samples</em>, that come from some kind of probability distribution, be it continuous or discrete. For example, a coin flip delivers samples that are either heads or tails, while a die delivers samples from the set {1, 2, 3, 4, 5, 6} (assuming a standard six-sided die).</p>
<p class="indent">If a random process returns a single number, how do we know what distribution it’s sampling from? In some cases, we have theoretical knowledge, but in other cases, all we can do is generate many samples. Over time, the relative frequency with which each possible outcome appears will become evident and serve as a stand-in for the true probability distribution.</p>
<h4 class="h4" id="ch00lev2_1"><em><strong>Discrete Distributions</strong></em></h4>
<p class="noindent">As an example of a discrete probability distribution, suppose that, thanks to the generosity of a local wizard, I have in my possession a one-of-a-kind die with only three sides. The numbers 0, 1, or 2 appear on the faces of my three-sided die. Therefore, each roll of the die results in one of the three sides face up. When I roll my die 50,000 times, keeping a tally of the number of times each side appears face up, I get the results in <a href="ch01.xhtml#ch01tab01">Table 1-1</a>.</p>
<p class="tabcap" id="ch01tab01"><strong>Table 1-1:</strong> Rolling a Three-Sided Die 50,000 Times</p>
<table class="table-h">
<colgroup>
<col style="width:60%"/>
<col style="width:40%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><strong>Face</strong></th>
<th class="tab_th"><strong>Count</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1">0</td>
<td class="bg1">33,492</td>
</tr>
<tr>
<td class="bg">1</td>
<td class="bg">8,242</td>
</tr>
<tr>
<td class="bg1">2</td>
<td class="bg1">8,266</td>
</tr>
</tbody>
</table>
<p class="indent">The results indicate that the probability of getting a 0, 1, or 2 is not equal: 0 appeared 33,492/50,000 = 66.984 percent of the time, 1 appeared 16.484 percent of the time, and 2 appeared 16.532 percent of the time. If I were to repeat the experiment, the exact count for each outcome would be slightly different, but it’s clear that many such experiments would lead to outcomes in which 0 appears approximately 67 percent of the time and 1 and 2 each appears 16.5 percent of the time. Notice that 67 percent + 16.5 percent + 16.5 percent = 100 percent, as it should if the only possible outcomes from rolling my magic die are 0, 1, and 2 in the proportion 67 : 16.5 : 16.5.</p>
<p class="indent">Rolling the three-sided die is a random process that samples from a probability distribution, one where the likelihood of a 0 is 67 percent and the likelihood of a 1 or a 2 is 16.5 percent each. There are a finite number of possible outputs, so the probability distribution is discrete.</p>
<p class="indent">Let’s try two more experiments that sample from discrete probability distributions. The first flips a fair coin 50,000 times. The second rolls a standard six-sided die 50,000 times. As before, let’s tally the different <span epub:type="pagebreak" id="page_3"/>outcomes, using 0 for tails and 1 for heads. <a href="ch01.xhtml#ch01tab02">Table 1-2</a> shows the results for the coin flips.</p>
<p class="tabcap" id="ch01tab02"><strong>Table 1-2:</strong> Flipping a Fair Coin 50,000 Times</p>
<table class="table-h">
<colgroup>
<col style="width:60%"/>
<col style="width:40%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><strong>Side</strong></th>
<th class="tab_th"><strong>Count</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg">0</td>
<td class="bg">25,040</td>
</tr>
<tr>
<td class="bg">1</td>
<td class="bg">24,960</td>
</tr>
</tbody>
</table>
<p class="indent"><a href="ch01.xhtml#ch01tab03">Table 1-3</a> shows the results for the die rolls.</p>
<p class="tabcap" id="ch01tab03"><strong>Table 1-3:</strong> Rolling a Standard Die 50,000 Times</p>
<table class="table-h">
<colgroup>
<col style="width:60%"/>
<col style="width:40%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><strong>Face</strong></th>
<th class="tab_th"><strong>Count</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1">1</td>
<td class="bg1">8,438</td>
</tr>
<tr>
<td class="bg">2</td>
<td class="bg">8,252</td>
</tr>
<tr>
<td class="bg1">3</td>
<td class="bg1">8,292</td>
</tr>
<tr>
<td class="bg">4</td>
<td class="bg">8,367</td>
</tr>
<tr>
<td class="bg1">5</td>
<td class="bg1">8,336</td>
</tr>
<tr>
<td class="bg">6</td>
<td class="bg">8,315</td>
</tr>
</tbody>
</table>
<p class="indent">Just as my magic three-sided die’s outcomes are not equally likely, the coin flips and standard die rolls deliver each outcome with equal probability. The counts are nearly uniform. Such <em>uniform distributions</em>, those that produce each possible outcome with equal probability, are far and away the most common type of distribution we’ll harness in this book.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>To run this experiment, I didn’t actually flip a coin and roll a die 50,000 times. Instead, I used a pseudorandom generator, discussed later in this chapter.</em></p>
</div>
<h4 class="h4" id="ch00lev2_2"><em><strong>Continuous Distributions</strong></em></h4>
<p class="noindent">Think of continuous probability distributions as the limit of discrete distributions. For example, a six-sided die selects from six possible outcomes. A 20-sided die, sometimes called a D20, selects from 20 possible outcomes. If we could somehow let the number of sides extend to infinity, then such a die would choose, on each roll, from an infinite number of outcomes. This is what a continuous probability distribution does.</p>
<p class="indent">Suppose a random process generates real numbers in the range 0 to 1 such that all real numbers are equally likely. In that case, the random process is sampling from a continuous uniform distribution, just as a die samples from a discrete uniform distribution. We’ll make heavy use of the continuous uniform distribution as we proceed through the book. Likewise, we’ll occasionally use the <em>normal</em> (sometimes called <em>Gaussian</em>) distribution.</p>
<p class="indent">Introducing some math notation at this point will make it easier to understand what comes later. The uniform distribution we’ll use most often <span epub:type="pagebreak" id="page_4"/>draws samples from [0, 1), meaning the sample is greater than or equal to 0 and strictly less than 1, that is, 0 ≤ <em>x</em> &lt; 1. When writing a range like [0, 1), note whether a square bracket or a parenthesis is used. The square bracket means the limit is included in the range, but a parenthesis excludes the limit. Therefore, (0, 1) specifies a range where all possible real numbers except 0 and 1 are allowed. Similarly, [0, 1] includes both 0 and 1, while [0, 1) includes 0 but excludes 1. The pseudorandom and quasirandom processes described later in the chapter usually produce outputs in the range [0, 1).</p>
<p class="indent">A uniform distribution is straightforward, whether continuous or discrete: each possible outcome is equally likely to appear. However, in a normal distribution, some values are more likely to be produced than others.</p>
<p class="indent">The best way to appreciate a normal distribution is to examine its histogram. For example, <a href="ch01.xhtml#ch01fig01">Figure 1-1</a> shows the distribution of 60 million samples from a normal distribution.</p>
<div class="image"><img alt="Image" id="ch01fig01" src="../images/01fig01.jpg"/></div>
<p class="figcap"><em>Figure 1-1: A histogram showing normal (curve) and uniform (line) distributions</em></p>
<p class="indent">The values in <a href="ch01.xhtml#ch01fig01">Figure 1-1</a> range from approximately −6 to 6. Values around 0 are most likely, with those at the extremes least likely. The normal distribution is widespread; many physical phenomena follow this distribution. Crucially, when we take a large set of samples from <em>any</em> distribution, the mean values will follow a normal distribution. This is the <em>central limit theorem</em>, and it’s fundamental to statistics.</p>
<p class="indent">In the previous section, <a href="ch01.xhtml#ch01tab02">Table 1-2</a> and <a href="ch01.xhtml#ch01tab03">Table 1-3</a> tally the number of heads and tails and the number of times each side of a die appeared. A histogram is a graphical representation of such a table. The possible outputs are placed into bins of some specified width. For a die, the natural bin width is 1 so that each side falls into the bin with the same label.</p>
<p class="indent">For a continuous distribution, the bins cover a range. For example, if we have a process that generates numbers in the range [0, 1), and we want <span epub:type="pagebreak" id="page_5"/>10 bins, then we might make each bin 0.1 wide. A sample of <em>x</em> = 0.3052 will then fall into bin 3, counting from 0, because:</p>
<div class="image1"><img alt="Image" src="../images/f0005-01.jpg"/></div>
<p class="noindent">Likewise, a sample of <em>x</em> = 0.0451 will fall into bin 0, and so on. When all samples have been placed in a bin, the histogram plots either the count in each bin or the fraction of samples that fell into that bin. The fraction is found by dividing each bin’s count by the sum of all the bins. A histogram using fractions per bin approximates the true probability distribution.</p>
<p class="indent">Let’s return to <a href="ch01.xhtml#ch01fig01">Figure 1-1</a>. The figure uses 1,000 bins, which explains why the curve looks more like a curve than a bar plot. The figure plots the fraction in each bin, not the count, which lets us compare different distributions without worrying about the number of samples used to generate the histogram. As the number of samples increases, such a histogram becomes a better approximation of the true probability distribution.</p>
<p class="indent">The horizontal line in <a href="ch01.xhtml#ch01fig01">Figure 1-1</a> represents a continuous uniform distribution selecting values in the range [–6, 6). If each value is equally likely, then on average, each bin will be equally populated and 1/1,000 = 0.001, thereby explaining the <em>y</em>-axis value for the uniform distribution.</p>
<p class="indent">We need only remember that the random processes powering our experiments generate values according to some distribution, primarily the uniform or normal distributions. There are many other standard distributions, but we won’t explore them in this book.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>To learn more about probability and statistics, I recommend</em> Statistics Done Wrong <em>by Alex Reinhart (2015) or my book</em> Math for Deep Learning <em>(2021), both available from No Starch Press. You’ll find discussions of probability and statistics, along with differential calculus, all of which we’ll touch on at various points throughout this book.</em></p>
</div>
<p class="noindent">We need to know how close our random processes come to being, well, random. Before we dive into randomness engines proper, let’s consider how to approach testing the output of a random process.</p>
<h3 class="h3" id="ch00lev1_6"><strong>Testing for Randomness</strong></h3>
<p class="noindent">How can we know that the output of a random process is truly random? The short answer is: we can’t. However, that shouldn’t deter us. We’re not attempting to solve deep philosophical issues, as interesting as they might be. Instead, we seek what’s good enough to accomplish our immediate goals and no more.</p>
<p class="indent">Is this string of binary digits random?</p>
<p class="center">0101010011100000110000011101101111111011100000</p>
<span epub:type="pagebreak" id="page_6"/>
<p class="noindent">Well, it looks kind of random, but how can we tell? Earlier, we used the frequency of each possible output to tell us whether the sample matches expectations. A random process generating 0 or 1 with expected equal likelihood should match as well. In this case, there are 23 zeros and 23 ones. Does that indicate that the sequence is random?</p>
<p class="indent">You begin to see what I mean when I say we can’t tell whether a process is truly random. All we can do is apply tests to increase our confidence in our belief that the sequence israndom. We can check the expected frequency of the various outputs, but that’s not sufficient. For example, the following sequences also have equal numbers of zeros and ones:</p>
<div class="image1"><img alt="Image" src="../images/f0006-01.jpg"/></div>
<p class="noindent">Most of us wouldn’t consider either sequence to be particularly random.</p>
<p class="indent">In truth, none of the previous three sequences are the output of a random process. The first one is the binary representation of the operation codes for a 6502 microprocessor program to display the letter A on the screen of an old Apple II computer. The bit patterns for the opcodes are not random, but depend critically on the internal architecture of the microprocessor. I made the other two sequences by hand to have equal numbers of zeros and ones.</p>
<p class="indent">Over the years, researchers have invented many statistical tests designed, collectively, to detect whether a sequence of values is worthy of being called random. We have no space here to dive into these tests, but they go far beyond frequencies and consider short- and long-term correlations of all kinds. A few such test suites are DieHarder, TestU01, and PractRand. These suites generally require vast collections of values, far more than we can work with here.</p>
<p class="indent">So what’s a person to do? We cannot prove that a randomness engine is generating random outputs, but we can gain enough confidence to believe more or less strongly. To accomplish this, we’ll use a command line program called <span class="literal">ent</span>, from the word <em>entropy</em>. It applies a small set of statistical tests that might influence our beliefs.</p>
<p class="indent">Many Linux distributions include <span class="literal">ent</span>, but if yours doesn’t, install it using the following command:</p>
<pre class="pre">&gt; <span class="codestrong1">sudo apt-get install ent</span></pre>
<p class="indent">Check the following website for a compiled Windows version (along with a link to its GitHub repository, should you wish to examine <span class="literal">ent</span>’s source code): <em><a href="https://www.fourmilab.ch/random">https://www.fourmilab.ch/random</a></em>. To install <span class="literal">ent</span> on macOS in the previous command, replace <span class="literal">sudo apt-get</span> with <span class="literal">brew</span>.</p>
<p class="indent">The <span class="literal">ent</span> program requires a file of bytes, which it assumes are uniformly distributed in the range [0, 255]. This means that the random process under test must have its output converted into a set of uniformly distributed bytes. We’ll learn how to do this later in the chapter.</p>
<p class="indent">For now, the book’s GitHub page includes a file we can use to understand <span class="literal">ent</span>’s output, <em>ent_test.bin</em>. Pass it to <span class="literal">ent</span> like so:</p>
<span epub:type="pagebreak" id="page_7"/>
<pre class="pre">&gt; <span class="codestrong1">ent ent_test.bin</span>
Entropy = 7.999996 bits per byte.

Optimum compression would reduce the size
of this 40000000 byte file by 0 percent.

Chi square distribution for 40000000 samples is 241.36, and randomly
would exceed this value 72.09 percent of the times.

Arithmetic mean value of data bytes is 127.5064 (127.5 = random).
Monte Carlo value for Pi is 3.141776714 (error 0.01 percent).
Serial correlation coefficient is -0.000234 (totally uncorrelated = 0.0).</pre>
<p class="indent">The file <em>ent_test.bin</em> contains bytes generated by a good, but rarely used, pseudorandom number generator called MWC (multiply-with-carry). We might expect <span class="literal">ent</span> to report that the file is random. However, <span class="literal">ent</span> doesn’t make that determination for us. Instead, <span class="literal">ent</span> runs a set of six statistical tests and reports the results, leaving it up to us to decide whether those results warrant a belief toward randomness.</p>
<p class="indent">The first test measures the entropy of the bytes. <em>Entropy</em> is a measure of disorder in a system. To a physicist, entropy is related to the number of microstates of a system—for example, the position and momentum of the molecules in a gas leading to the same macroscopic values of large-scale properties like temperature and pressure and the ways in which those molecules can be arranged. The entropy reported by <span class="literal">ent</span>, however, is deeper than that. It is the <em>Shannon entropy</em>, a measure of information content. In this case, it’s expressed in bits. There are 8 bits in a byte, so a maximally random sequence would have an entropy of 8.0, meaning information content is maximized. Our test file has an entropy of 7.999996 bits per byte, which is extremely close to 8, a good sign.</p>
<p class="indent">We use the entropy reported by <span class="literal">ent</span> to estimate how much it’s possible to compress the file. It’s an alternative presentation of the entropy. Compression algorithms work by taking advantage of the information contained in a file, as measured by its entropy. The lower the entropy, the more redundant the data, and the lower the information content. If the information content is low, there is another way to express the information that takes less space. However, if the file is random and entropy is maximized, there is no alternative way to express the file’s contents, so it cannot be compressed.</p>
<p class="indent">Next, <span class="literal">ent</span> applies a <em>χ</em><sup>2</sup> test. The important part here is the percentage reported. If this percentage is below 5 percent or above 95 percent, then the expected frequencies—that is, the number of times each byte value appears—is suspect. Here, we have 72 percent, so we’re again on solid ground.</p>
<p class="indent">If the sequence of bytes is random, we might, correctly, expect the average value of the bytes to be 255/2 = 127.5. Here, we have an average value of 127.5064, which is quite close.</p>
<p class="indent">It’s possible to estimate <em>π</em> with random numbers; <span class="literal">ent</span> uses this as another test of randomness. In this case, <span class="literal">ent</span> arrives at an estimate 0.01 percent off <span epub:type="pagebreak" id="page_8"/>from the number of digits shown. If there is something in the sequence of bytes that biases the simulation, then it should manifest itself in the calculated <em>π</em> value. We’ll use random numbers to estimate <em>π</em> in <a href="ch03.xhtml">Chapter 3</a>.</p>
<p class="indent">The final output line applies a statistical test to measure how related byte <em>n</em> is to byte <em>n</em> + 1; that is, it pays attention to the order of the bytes. If the bytes are not serially correlated, at least to the level of one to the next, the resulting coefficient will be zero. Here, it’s ever so slightly negative but quite close to zero.</p>
<p class="indent">All in all, then, <span class="literal">ent</span>’s report gives us strong confidence that the contents of the file <em>ent_test.bin</em> are worthy of being called random. Would you secure your bank account with this level of confidence? I sincerely hope not, but we’re not interested in cryptography; we’re interested in random processes, real or synthetic, that are sufficiently random to power our experiments. For that, <span class="literal">ent</span> is the only tool we need.</p>
<p class="indent">However, <span class="literal">ent</span>’s output is needlessly verbose, especially since we’ll use it often in this chapter. Let’s define an abbreviated version. Instead of the output shown earlier, I’ll report <span class="literal">ent</span> results like so:</p>
<pre class="pre">entropy: 7.999996
chi2   : 72.09
mean   : 127.5064
pi     : 3.141776714 (0.01)
corr   : -0.000234</pre>
<p class="indent">Let’s put our nifty randomness detector to work. We’ll begin with truly random processes.</p>
<h3 class="h3" id="ch00lev1_7"><strong>Truly Random Processes</strong></h3>
<p class="noindent">In this section, we’ll review several sources generally accepted as true random processes: experimenting with coins, dice, electrical noise in different forms, and the decay of radioactive elements. We’ll use the datasets we create here for experiments later in the book. The random processes covered in this section also provide a comparison against the next section’s pseudorandom processes, which merely give the appearance of randomness.</p>
<p class="indent">Humanity has, over the centuries, developed multiple ways of generating randomness, including coin flips and dice rolls. Let’s consider these to see if we might trust them as randomness engines.</p>
<h4 class="h4" id="ch00lev2_3"><em><strong>Flipping Coins</strong></em></h4>
<p class="noindent">Most people consider a coin flip to be a reasonable source of randomness, but is that really the case? In 2009, two undergraduate students at the University of California, Berkeley, flipped a coin a total of 40,000 times while keeping track of the coin’s starting orientation, either heads up or tails up. <a href="ch01.xhtml#ch01tab04">Table 1-4</a> shows what they found (data used with permission).</p>
<span epub:type="pagebreak" id="page_9"/>
<p class="tabcap" id="ch01tab04"><strong>Table 1-4:</strong> Flipping 40,000 Coins by Hand</p>
<table class="table-h">
<colgroup>
<col style="width:40%"/>
<col style="width:20%"/>
<col style="width:20%"/>
<col style="width:20%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><strong>Orientation</strong></th>
<th class="tab_th"><strong>Heads</strong></th>
<th class="tab_th"><strong>Tails</strong></th>
<th class="tab_th"><strong>p-value</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg">Heads up</td>
<td class="bg">10,231</td>
<td class="bg">9,770</td>
<td class="bg">0.0011</td>
</tr>
<tr>
<td class="bg">Tails up</td>
<td class="bg">9,985</td>
<td class="bg">10,016</td>
<td class="bg">0.8265</td>
</tr>
</tbody>
</table>
<p class="indent">A glance at <a href="ch01.xhtml#ch01tab04">Table 1-4</a> shows that when the coin was facing heads up, there were more heads at the end of the flip. The same is true for tails up; more tails were measured. We can use the <em>χ</em><sup>2</sup> test to see if these proportions are in concert with the expected 50-50 split of a fair coin. The resulting p-values are in the rightmost column.</p>
<p class="indent">A p-value is the probability of measuring the observed number of heads and tails, given that the null hypothesis is true. In statistical testing, the <em>null hypothesis</em> is the hypothesis being tested. In this case, for the <em>χ</em><sup>2</sup> test, the null hypothesis is that the observed number of heads and tails is consistent with equal likelihoods. The p-value provides evidence for or against this hypothesis. If the p-value is below the standard, somewhat arbitrary rule-of-thumb threshold of 0.05 (5 percent), then we say that the p-value is <em>statistically significant</em>, and we claim evidence against the null hypotheses.</p>
<p class="indent">The smaller the p-value, the stronger our evidence becomes. For a p-value threshold of 0.05, we might expect to falsely reject the null hypothesis about 1 time in 20, while for a p-value of 0.01, the false rejection rate becomes 1 in 100, and so on as the p-values get smaller and smaller. However, only death and taxes are certain. A small p-value is not <em>proof</em> of anything; it’s only an indicator, a reason to believe or not to believe, though with perhaps strong evidence.</p>
<p class="indent">Look again at the Heads up row in <a href="ch01.xhtml#ch01tab04">Table 1-4</a>. The p-value is 0.0011, or 0.11 percent. According to the <em>χ</em><sup>2</sup> test, there is a 0.11 percent probability of the observed counts (or a more extreme difference), given the null hypothesis is true. Therefore, we have evidence in favor of rejecting the null hypothesis. In other words, we have evidence for believing that Subject 1, who did the heads-up portion of the experiment, was not random, but was instead biased toward heads.</p>
<p class="indent">Subject 2, however, produced results consistent with the null hypothesis. For her, the p-value was 0.8265, or 83 percent. Again, the p-value, in this case, means the <em>χ</em><sup>2</sup> test reports a probability of 83 percent for observing the counts, given the null hypothesis is true. This makes perfect sense, so we have evidence supporting the null hypothesis for the tails-up case.</p>
<p class="indent">The <em>χ</em><sup>2</sup> test compared the tallies with the expected 50-50 split. We can do one more test: a <em>t-test</em>. The t-test compares two datasets and returns a p-value we interpret as the likelihood the datasets were generated by the same process. In this case, the t-test between the heads-up and tails-up data-sets returned a p-value of 0.0139, or 1.39 percent, again below the standard 0.05 threshold. This serves as evidence that the two datasets are likely drawn from different processes.</p>
<p class="indent">What does that mean in this case? We have a single set of flips from two subjects, and each subject only flipped coins with the same side up each <span epub:type="pagebreak" id="page_10"/>time. It’s conceivable, but not proven, that Subject 1 was highly consistent in her flips and, as such, biased the coin tosses so that heads were favored when heads were the starting condition. For us, this fun example serves as an indication that humans are not to be trusted to act randomly.</p>
<p class="indent">We have evidence that Subject 1 biased the coin flips. Are we stuck with the bias? Actually, no. American mathematician and computer scientist John von Neumann came up with a clever algorithm to make a biased coin fair. The algorithm is straightforward:</p>
<ol>
<li class="noindent">Flip the biased coin twice.</li>
<li class="noindent">If both flips came up the same—that is, both heads or both tails—start again from step 1.</li>
<li class="noindent">Otherwise, keep the result of the first flip and disregard the second.</li>
</ol>
<p class="indent">Applying this algorithm to the sequence of heads and tails generated by Subject 1 leaves us with 2,475 heads and 2,538 tails. The <em>χ</em><sup>2</sup> test delivers a p-value of 0.37, which is well above 0.05 and strong evidence that the resulting dataset is now acting as expected.</p>
<p class="indent">Why does von Neumann’s algorithm work? Consider a biased coin where the probability of getting a heads is not 0.5 but 0.8, implying that the probability of a tails is 0.2, since probabilities add to 1. In that case, flipping the coin twice will lead to the four possible combinations of heads and tails with the following probabilities:</p>
<div class="bq"><img alt="Image" src="../images/f0010-01.jpg"/></div>
<p class="noindent">Recall that if events are independent, as even the flips of a biased coin are, then the probabilities multiply. Also, the probability of getting heads followed by tails equals that of getting tails followed by heads. Therefore, consistently selecting the first (or the second) in either of these cases must lead to selecting heads or tails with equal likelihood.</p>
<p class="indent">The file <em>40000cointosses.csv</em> contains the dataset used in this experiment with the associated code in <em>40000cointosses.py</em>.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>Please review the original web page for other comments on how the experiment was conducted, including suitable warnings about taking the results as anything more than a hint that further experimentation might uncover something interesting:</em> <a href="https://www.stat.berkeley.edu/∼ldous/Real-World/coin_tosses.html">https://www.stat.berkeley.edu/∼ldous/Real-World/coin_tosses.html</a>.</p>
</div>
<h4 class="h4" id="ch00lev2_4"><em><strong>Rolling Dice</strong></em></h4>
<p class="noindent">Flipping a fair coin is a random process, as is rolling a fair die. But is there really such a thing as a “fair” die? Imperfections in the manufacturing process, a slight deviation in shape, or unequal density throughout the die’s <span epub:type="pagebreak" id="page_11"/>body might lead to a bias. Still, overall, and especially for our purposes, we might believe that dice rolls are random enough to be useful.</p>
<p class="indent">I gathered 14 six-sided dice and rolled them, en masse, using a dice cup from a game. I then took a picture of the results so I could count how many of each face appeared. I repeated this process 50 times for a total of 700 dice rolls. <a href="ch01.xhtml#ch01tab05">Table 1-5</a> shows the results.</p>
<p class="tabcap" id="ch01tab05"><strong>Table 1-5:</strong> Tallying Dice Rolls</p>
<table class="table-h">
<colgroup>
<col style="width:60%"/>
<col style="width:40%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><strong>Outcome</strong></th>
<th class="tab_th"><strong>Count</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1">1</td>
<td class="bg1">122</td>
</tr>
<tr>
<td class="bg">2</td>
<td class="bg">98</td>
</tr>
<tr>
<td class="bg1">3</td>
<td class="bg1">106</td>
</tr>
<tr>
<td class="bg">4</td>
<td class="bg">126</td>
</tr>
<tr>
<td class="bg1">5</td>
<td class="bg1">119</td>
</tr>
<tr>
<td class="bg">6</td>
<td class="bg">129</td>
</tr>
</tbody>
</table>
<p class="indent">As before, if the dice are fair, we expect each count to be the same. For 700 rolls, we expect each possible output to appear 700/6 ≈ 117 times—a spartan number because of the small scale of the experiment, but sufficient for us to acknowledge the chief concern of this section: to master our appreciation of truly random processes.</p>
<p class="indent">The counts in <a href="ch01.xhtml#ch01tab05">Table 1-5</a> are not 117 but deviate, often by quite a bit. Does this mean the dice are loaded? Perhaps, but we’ll never know for sure; we can only gain evidence favoring one answer over another. The <em>χ</em><sup>2</sup> test is our tool of choice here, as it was for the preceding coin flips. Applying it returns a p-value of 0.28, well above the threshold of 0.05 that is generally accepted as statistically significant. Therefore, we do not reject the null hypothesis and believe that the dice are reasonably fair and, consequently, a potential source of true randomness.</p>
<p class="indent">Macroscopic physical systems are likely insufficient if the goal is to create truly random numbers; there are too many biases involved, though we can use an algorithm like von Neumann’s to improve the situation. Also, a physical randomness engine, perhaps based on automatic dice rolls, will degrade over time, further introducing bias. Therefore, we must search in a different direction. Let’s now consider processes more suitable for a randomness engine.</p>
<div class="box">
<p class="box-title"><strong>ROULETTE WHEELS</strong></p>
<p class="box-para">Roulette is another physical process that comes to mind when contemplating potential sources of randomness. In roulette, people bet on the position in which a marble will ultimately land when rolled against a spinning wheel. It is a favorite target for people trying to game the system because players can make <span epub:type="pagebreak" id="page_12"/>bets while the wheel is spinning. On the face of it, a roulette wheel should be as random as rolling dice, but mechanical defects, especially if the wheel is tilted even the tiniest bit, bias the final ball position to the advantage of the clever.</p>
<p class="box-para">The first occurrence of gaming a roulette wheel that I could find happened around 1880 when Englishman Joseph Jagger, a textile worker, realized that imperfections in the construction of the roulette wheels in Monte Carlo enabled him to predict the final ball position reliably enough to win more often than he lost. His success forced improvements to the design of roulette wheels.</p>
<p class="box-para">Around 1960, Edward Thorp, working with Claude Shannon, constructed what is likely the first wearable computer for the sole purpose of gaming roulette. The full account is in Thorp’s paper, titled “The Invention of the First Wearable Computer,” published in 1998. The computer was small, with only 12 transistors, and was operated by a footswitch concealed in a shoe. As the roulette wheel turned, the footswitch started a timer that delivered one of eight tones to a tiny earpiece, each tone indicting a predicted octant in which the ball was more likely to fall. The system, though fragile, worked and was tried in Las Vegas in 1961 with some success.</p>
<p class="box-para">In the 1970s, J. Doyne Farmer and Norman Packard essentially repeated that experiment using a microcomputer. Like Thorp and Shannon, they were similarly successful in the casinos; see Farmer’s brief page at <em><a href="http://www.doynefarmer.com/roulette">http://www.doynefarmer.com/roulette</a></em> or the more detailed account in Thomas Bass’s book <em>The Eudaemonic Pie</em>, which is available online via the Internet Archive (<em><a href="https://archive.org">https://archive.org</a></em>).</p>
</div>
<h4 class="h4" id="ch00lev2_5"><em><strong>Using Voltage</strong></em></h4>
<p class="noindent">Most desktop computers have a microphone input jack. A program like Audacity can record samples from this input device. We might expect recording when no microphone is connected to give us an empty file, but it doesn’t. The microphone input is an analog input and is susceptible to electronic noise: minor, random variations in voltage due to the nature of the components involved and other environmental factors. We’ll use the variation of this voltage as a randomness engine.</p>
<p class="indent">This experiment requires you to record a WAV file. It doesn’t matter which tool you use to do so. I used Audacity, an open source sound editor available for most systems. Install it under Ubuntu with the following command:</p>
<pre class="pre">&gt; <span class="codestrong1">sudo apt-get install audacity</span></pre>
<p class="noindent">Visit <em><a href="https://www.audacityteam.org">https://www.audacityteam.org</a></em> to install Audacity for Windows and macOS.</p>
<p class="indent">We want to record from the microphone input using a single channel (mono) and at a high sampling rate, like 176,400 Hz (samples per second). For Audacity, this means changing the project rate to 176,400 (see the lower left of the screen) and changing the recording channel drop-down menu to <strong>Mono</strong>. We also need to select the microphone as the input source. The name of this device on your system is beyond my powers of clairvoyance, but <span epub:type="pagebreak" id="page_13"/>experimentation with the drop-down menu next to the microphone icon will likely turn up the proper device. To test the input, select <strong>Click to Start Monitoring</strong>. You should see a slowly varying sound bar for only one channel (for example, the left channel). If not, play around a bit more until you do.</p>
<p class="indent">To record a sample, click the red record button. I suggest recording for several minutes. To stop, click the square stop button. Use the appropriate option on the File menu to export the recording as a WAV file, setting the output format in the File Save dialog to <strong>32-bit float PCM</strong>. If you can’t use this method to record a WAV file, there are several small ones available on the book’s website that you can use with the upcoming code.</p>
<p class="indent">Audio signals are represented as a continuous voltage that changes with time. Sampling an audio signal means measuring the instantaneous voltage at a set time interval, the sampling rate, and turning that voltage into a number over some range. For example, a compact disc uses 16-bit samples, so each voltage is assigned a number in the range −32,768 to 32,767. The sampling rate decides how frequently the voltage is measured.</p>
<p class="indent">When a digitized signal is played back, meaning converted back into a voltage to drive a speaker, for instance, the quality of the sound depends on how many numbers were used to represent the signal and how often it was sampled. For our experiment, the samples are represented as 32-bit floating-point numbers, and the sampling rate is 176,400 Hz. By comparison, a compact disc samples at 44,100 Hz.</p>
<p class="indent"><a href="ch01.xhtml#ch01fig02">Figure 1-2</a> shows a subset of audio samples and their corresponding histogram.</p>
<div class="image"><img alt="Image" id="ch01fig02" src="../images/01fig02.jpg"/></div>
<p class="figcap"><em>Figure 1-2: The audio samples (left) and corresponding histogram (right)</em></p>
<p class="indent">The <em>x</em>-axis in <a href="ch01.xhtml#ch01fig02">Figure 1-2</a> is time, that is, the sample number, and the <em>y</em>-axis is the floating-point sample value, or the digitized value representing the voltage at that time. The horizontal line is the mean value of all the samples in the file; <a href="ch01.xhtml#ch01fig02">Figure 1-2</a> shows only the first 200. As we might expect, the mean value is virtually 0.</p>
<p class="indent">The right side of <a href="ch01.xhtml#ch01fig02">Figure 1-2</a> shows us the histogram of all the samples in a two-second clip. We’ve seen this shape before; it’s nearly identical to <a href="ch01.xhtml#ch01fig01">Figure 1-1</a>, and it tells us that the noise samples are normally distributed <span epub:type="pagebreak" id="page_14"/>with a mean value of 0. We’ll use this observation to convert the audio stream into a file of random bytes.</p>
<p class="indent">We cannot use the recording as is; we must process the samples to make them more random, using the code in <em>silence.py</em>. Let’s walk through it part by part.</p>
<p class="indent">First, we <span class="literal">import</span> some library routines:</p>
<pre class="pre">import sys
import numpy as np
from scipy.io.wavfile import read as wavread</pre>
<p class="noindent">The <span class="literal">sys</span> module provides an interface to the command line; <span class="literal">numpy</span> we know. To read WAV files, we need the <span class="literal">read</span> function from SciPy. Here, I’m renaming it <span class="literal">wavread</span>.</p>
<p class="indent">At the bottom of the script, we load the desired WAV file and process it:</p>
<pre class="pre">s, d = wavread(sys.argv[1])
print("sampling rate: %d" % s)
n = len(d)//2
a = MakeBytes(d[:n])
b = MakeBytes(d[n:])
if (len(a) &lt; len(b)):
    c = a[::-1] ^ b[:len(a)]
else:
    c = a[:len(b)] ^ b[::-1]
c.tofile(sys.argv[2])</pre>
<p class="indent">The <span class="literal">wavread</span> function returns the sampling rate (<span class="literal">s</span>) and the samples themselves as a NumPy vector (<span class="literal">d</span>). We display the sampling rate, then split the samples into two halves and pass each half to <span class="literal">MakeBytes</span> before assigning the return values <span class="literal">a</span> and <span class="literal">b</span>, respectively. <span class="literal">MakeBytes</span> turns a vector of audio samples into a vector of bytes.</p>
<p class="indent">The final set of bytes is in <span class="literal">c</span>. This is the exclusive-OR (XOR) of the bytes in <span class="literal">a</span> and <span class="literal">b</span>. XOR is a logical operation on the bits of an integer. If one of the inputs to XOR is 1 and the other 0, then the output is 1. If the inputs are the same, the output is 0. I remember the phrase “one or the other, but not both.” XOR is different from the standard OR operation, which outputs 1 if any input is 1, including if both are 1, as follows:</p>
<p class="center">1 XOR 1 = 0, but 1 OR 1 = 1</p>
<p class="noindent">Using one part of the generated stream of bytes to modify the other is a powerful way to alter the bit patterns, which adds to the random nature of the output. In <em>silence.py</em>, one of the byte streams is reversed (<span class="literal">[::-1]</span>) before XOR to add that much more randomness to the process.</p>
<p class="indent">The number of bytes returned by <span class="literal">MakeBytes</span> depends on the actual samples passed to it, not the number of samples. Therefore, it is likely that the vector <span class="literal">a</span> will be of different length than <span class="literal">b</span>, hence the <span class="literal">if</span> statement and indexing based on <span class="literal">len</span>. When <span class="literal">c</span> is ready, it’s written to disk in binary via <span class="literal">tofile</span>.</p>
<p class="indent"><span epub:type="pagebreak" id="page_15"/>All the action is in the <span class="literal">MakeBytes</span> function. Converting the stream of audio samples into bytes requires four steps:</p>
<ol>
<li class="noindent">Subtract the overall mean from each sample.</li>
<li class="noindent">Convert each sample to a bit based on its sign: 1 if positive, 0 if negative.</li>
<li class="noindent">De-bias the bits using the von Neumann algorithm from the previous section.</li>
<li class="noindent">Group each set of 8 bits into an output byte.</li>
</ol>
<p class="indent">The function <span class="literal">MakeBytes</span> performs each of these steps, using the following code:</p>
<pre class="pre">def MakeBytes(A):
 <span class="ent">➊</span> t = A - A.mean()
 <span class="ent">➋</span> thresh = (t.max()-t.min())/100.0
    w = []
    for i in range(len(t)):
        if (np.abs(t[i]) &lt; thresh):
            continue
        w.append(1 if t[i] &gt; 0 else 0)
 <span class="ent">➌</span> b = []
    k = 0
    while (k &lt; len(w)-1):
        if (w[k] != w[k+1]):
            b.append(w[k])
        k += 2
 <span class="ent">➍</span> n = len(b)//8
    c = np.array(b[:8*n]).reshape((n,8))
    z = []
    for i in range(n):
        t = (c[i] * np.array([128,64,32,16,8,4,2,1])).sum()
        z.append(t)
    return np.array(z).astype("uint8")</pre>
<p class="indent">The code passes in the samples as <span class="literal">A</span>, a NumPy vector. Now come the four steps. First, we subtract any mean value (<span class="literal">t</span>) <span class="ent">➊</span>. Next, we define <span class="literal">thresh</span> to be 1 percent of the maximum range of the samples. I’ll explain why momentarily.</p>
<p class="indent">Step 2 says to use the sign of each sample as a bit <span class="ent">➋</span>. We observed that the samples are normally distributed, and the mean is now zero because we subtracted it, so roughly half the samples will be negative and half positive. This sounds a lot like a coin flip: either one value or another. Therefore, we make the positive samples ones and the negative samples zeros. But what’s the point of <span class="literal">thresh</span>?</p>
<p class="indent">The bits are collected in the list <span class="literal">w</span>, initially empty. The loop after initializing <span class="literal">w</span> examines each sample and asks if the sample’s absolute value is less than 1 percent of the maximum range. If it is, we ignore that sample <span epub:type="pagebreak" id="page_16"/>(<span class="literal">continue</span>); otherwise, we use the sample’s sign to add a new bit to <span class="literal">w</span>. There may be a tiny, nonrandom signal with a low amplitude in the samples. Ignoring samples close to zero helps remove any such signal. In effect, we are saying we’re interested in only “large” deviations from the mean of zero.</p>
<p class="indent">We now have a long list of individual bits (<span class="literal">w</span>). These bits might be biased, representing a collection of unfair coin tosses. To handle this, we de-bias by using the von Neumann algorithm, which makes a new list of bits in <span class="literal">b</span> <span class="ent">➌</span>.</p>
<p class="indent">Finally, we convert each set of eight bits in <span class="literal">b</span> into a new list of bytes in <span class="literal">z</span> <span class="ent">➍</span> by reshaping <span class="literal">b</span> into an <em>N</em>×8 array before multiplying each row of that array, column by column, by the value of each bit position in a byte. When all is said and done, the code converts the list of bytes into a NumPy array and returns it.</p>
<p class="indent">You may want to review the code again to ensure you follow each step. The central concept is that we took advantage of the fact that the samples are normally distributed to generate a stream of bits that, when de-biased, form the stream of bytes used to make the final output of the code.</p>
<p class="indent">To test out this code, I made a 30-minute recording using Audacity and saved the samples in the file <em>silence.wav</em>. I then used <em>silence.py</em> to convert this large WAV file into <em>silence.bin</em>.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>The file</em> silence.wav <em>is too large to include in the book’s repository. However, if your heart is set on having it, contact me, and I’ll see what I can do.</em></p>
</div>
<p class="indent">Here’s the command line I used to convert the WAV file:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 -W ignore silence.py silence.wav silence.bin</span></pre>
<p class="noindent">The <span class="literal">wavread</span> function tends to complain about WAV file elements it doesn’t understand. Adding <span class="literal">-W ignore</span> to the command line suppresses these warnings.</p>
<p class="indent">The resulting output file, <em>silence.bin</em>, is not quite 6MB. It’s a collection of random bytes, so we can pass it to <span class="literal">ent</span> to get a report:</p>
<pre class="pre">entropy: 7.999867
chi2   : 0.01 
mean   : 127.4948
pi     : 3.136872354 (0.15)
corr   : 0.000610</pre>
<p class="indent">These are reasonably good values. The only value that is not where we might want it to be is <em>χ</em><sup>2</sup>, but we can live with that. Keep <em>silence.bin</em> on hand to use as a randomness engine later in the book.</p>
<h3 class="h3" id="ch00lev1_8"><strong>Random Physical Processes</strong></h3>
<p class="noindent">Many physical processes are random, though possibly biased. In this section, we’ll explore three physical processes leading to randomness: atmospheric radio frequency noise, plasma and charged particle rates as detected by the <span epub:type="pagebreak" id="page_17"/><em>Voyager 1</em> and <em>Voyager 2</em> spacecraft during their 40-plus-year mission, and the decay of radioactive isotopes.</p>
<h4 class="h4" id="ch00lev2_6"><em><strong>Atmospheric Radio Frequency Noise</strong></em></h4>
<p class="noindent">In 1997, Mads Haahr, a computer science professor at Trinity College, Dublin, started <em><a href="https://www.random.org">https://www.random.org</a></em>, a website dedicated to generating truly random numbers from atmospheric radio frequency noise. The site has been running ever since and offers free random numbers to internet users, along with a host of paid services involving random numbers. You can take a look at the services offered at <em><a href="https://www.random.org">https://www.random.org</a></em>, but for the purposes of this book, we’ll stick to the free collection of random bytes.</p>
<p class="indent">There are two primary ways to get random data from the site. First, random data in 16KB chunks are available at <em><a href="https://www.random.org/bytes">https://www.random.org/bytes</a></em>. Simply select the desired format and download. However, since we need larger collections of bytes to use as a randomness engine for our experiments, we are better served by using the archive available at <em><a href="https://archive.random.org">https://archive.random.org</a></em>. Downloading files directly from the site requires a small fee. However, if you are comfortable using torrents, the files can be legally acquired for free. Our reference Linux distribution includes Transmission, a client for accessing torrents. The application is also available for other operating systems; visit <em><a href="https://transmissionbt.com">https://transmissionbt.com</a></em> for download instructions.</p>
<p class="indent">As a test, I used Transmission to download several months’ worth of random bytes (the Binary Files option). I then combined all the binary files into one using the <span class="literal">cat</span> command. For example, a command like this will merge all binary files ending in <em>.bin</em> into the file <em>random_bytes</em>:</p>
<pre class="pre">&gt; <span class="codestrong1">cat *.bin &gt;random_bytes</span></pre>
<p class="indent">I then passed <em>random_bytes</em> (about 126MB) to <span class="literal">ent</span> to get the following results:</p>
<pre class="pre">entropy: 7.999999
chi2   : 98.32
mean   : 127.4880
pi     : 3.142011833 (0.01)
corr   : -0.000015</pre>
<p class="indent">We see that <span class="literal">ent</span> is quite happy with <em>random_bytes</em>. I have a much larger collection of bits from <em><a href="http://random.org">random.org</a></em>, including bits from two previous years. It’s over 500MB in size, and <span class="literal">ent</span> likes this one as well:</p>
<pre class="pre">entropy: 8.000000
chi2   : 40.68
mean   : 127.5023
pi     : 3.141375348 (0.01)
corr   : 0.000021</pre>
<p class="noindent">The code shows that the entropy is maximized, with 8 bits per byte.</p>
<span epub:type="pagebreak" id="page_18"/>
<h4 class="h4" id="ch00lev2_7"><em><strong>Voyager Plasma and Charged Particle Data</strong></em></h4>
<p class="noindent">In 1977, NASA launched the twin <em>Voyager</em> spacecraft to explore the outer solar system. <em>Voyager 1</em> encountered Jupiter and Saturn before heading out of the solar system. <em>Voyager 2</em> passed by Jupiter, Saturn, Uranus, and Neptune on its grand tour. To date, <em>Voyager 2</em> is the only spacecraft to explore Uranus and Neptune. In August 2012, <em>Voyager 1</em> became the first human-made object to leave the solar system and enter interstellar space. As of this writing, October 2023, both spacecraft are still performing well and have enough power for perhaps a decade.</p>
<p class="indent">The <em>Voyager</em>s are best known for the fantastic images they returned. However, both spacecraft carry multiple scientific instruments for measuring the environment through which they travel. This includes devices for measuring plasma protons and the flux of other charged particles and nuclei. These measurements are not entirely random but vary around certain values, much like the microphone input we used previously. Therefore, it’s possible to use the <em>Voyager</em> data to construct a binary file suitable for use as a source of randomness.</p>
<p class="indent">The file <em>voyager_plasma_lecp.bin</em> contains bytes formed from a set of <em>Voyager</em> datafiles, including plasma proton counts, density, and temperature for 1977 through 1980, and low-energy charged particle fluxes (particles passing through an area over time) from 1977 through 2021. I used the code in <em>process_vgr_data.py</em> to merge several smaller files to process the individual plasma and charged particle datasets.</p>
<p class="indent">For each type of data—plasma protons, low-energy protons, low-energy ions, and cosmic ray protons—the process was the same: find the median value over the time interval and mark each observation as a 1 bit if above the median and a 0 bit if below. To expand the collection of bits, I repeated this process for the median along with 80 percent of the median and 120 percent of the median. I then de-biased the final collection of bits using the von Neumann algorithm.</p>
<p class="indent">The resulting file contains 77,265 bytes, and <span class="literal">ent</span> reports the following:</p>
<pre class="pre">entropy: 7.995518
chi2   : 0.01
mean   : 127.5484
pi     : 3.150733867 (0.29)
corr   : -0.001136</pre>
<p class="noindent">These are reasonable values.</p>
<p class="indent"><a href="ch01.xhtml#ch01fig03">Figure 1-3</a> shows sample <em>Voyager</em> data from the plasma and low-energy charged particle experiments.</p>
<span epub:type="pagebreak" id="page_19"/>
<div class="image"><img alt="Image" id="ch01fig03" src="../images/01fig03.jpg"/></div>
<p class="figcap"><em>Figure 1-3: Sample</em> Voyager <em>data. Clockwise from top left: low-energy ions, cosmic ray protons, plasma proton temperature, and plasma proton speed.</em></p>
<p class="indent">The dashed line in each plot marks the median value over the dataset. Observations above the median became 1 bits, and those below the median 0 bits. The plot on the upper right shows the flux of cosmic ray protons over the entire mission. The vertical dashed line marks August 2012, the date when <em>Voyager 1</em> officially left the solar system. Notice the increase in cosmic ray protons after this time.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>The</em> Voyager <em>datasets were gathered from multiple websites. They are presented, typically, in text format and required a fair bit of processing on my part to whip them into shape. The files used inconsistent formatting, had typos in places, and, on occasion, used data from the wrong year. If you want the files as I used them, please contact me directly.</em></p>
</div>
<span epub:type="pagebreak" id="page_20"/>
<h4 class="h4" id="ch00lev2_8"><em><strong>Radioactive Decay</strong></em></h4>
<p class="noindent">From 1996 through December 2022, the HotBits website (<em><a href="https://www.fourmilab.ch/hotbits">https://www.fourmilab.ch/hotbits</a></em>) delivered truly random data to the public using the most random of random processes: radioactive decay.</p>
<p class="indent">Radioactive elements, like the cesium-137 used by HotBits, are unstable. Eventually, all such atoms decay by some process to another element, in this case, barium-137. The number of protons in an atom determines which element it is. Cesium has 55 protons in its nucleus; that’s its <em>atomic number</em>. <em>Isotopes</em> are versions of an element where the number of neutrons, also in the nucleus, varies. The sum of the two, ignoring the very light electrons, gives the <em>atomic mass</em>. If cesium-137 has 55 protons, it must have 82 neutrons. When an atom of cesium-137 decays, one of the neutrons converts into a proton, changing the atomic number to 56 and thereby converting the atom to barium-137, an atom with 56 protons and 81 neutrons. Barium-137 is stable, so the decay process stops. This process varies for different elements. For example, the decay of uranium passes through many stages to reach a stable isotope of lead.</p>
<p class="indent">When a neutron becomes a proton, it releases a beta particle (an electron) and an antineutrino. Neutrinos have virtually no mass and are almost undetectable. However, a Geiger counter easily detects beta particles. This is how the HotBits site generates random bits. To be complete, the decay from cesium to barium passes through two stages: the barium nucleus begins in a metastable state, then, about two minutes later, returns to the ground state by emitting a gamma ray. A gamma ray is a high-energy photon, that is, light.</p>
<p class="indent">The time when a particular decay will happen is governed by quantum physics and all the “weirdness” we associate with it. For radioactive decay, the weirdness at play is <em>quantum tunneling</em>, the fact that even though the cesium atom lacks the energy to, in effect, push itself up and out of the bowl it’s in, it nevertheless has a nonzero probability of doing so. There is no classical physics analog for quantum tunneling.</p>
<p class="indent">HotBits takes advantage of this unpredictability by using the timing between two pairs of detections to output a 1 or a 0. First, a beta particle is detected, then another. The time interval between the two detections is denoted as <em>T</em><sub>1</sub>. Next, another pair is detected with that time interval labeled <em>T</em><sub>2</sub>. If <em>T</em><sub>1</sub> ≠ <em>T</em><sub>2</sub>, a bit is generated. If <em>T</em><sub>1</sub> &lt; <em>T</em><sub>2</sub>, the bit is a 0; otherwise, <em>T</em><sub>1</sub> &gt; <em>T</em><sub>2</sub> and the output bit is a 1. The sense of the comparison is reversed after each bit to frustrate any systematic bias introduced by the physical setup.</p>
<p class="indent">HotBits required an API key to request up to 2,048 bytes at a time. There was a strict limit to the number of bytes granted in a 24-hour period. I downloaded bytes daily as I worked on this book and now have one 3,033,216-byte file. It’s pretty good data according to <span class="literal">ent</span>:</p>
<pre class="pre">entropy: 7.999935
chi2   : 18.90
mean   : 127.5246
<span epub:type="pagebreak" id="page_21"/>pi     : 3.144891758 (0.11)
corr   : 0.000100</pre>
<p class="indent">In this section, we’ve discussed several different processes that generate random data. While all of them are, as we’ll see later, useful, none can generate massive quantities of random numbers—and we’ll sometimes need millions of random numbers later in the book. We have no choice but to turn to what we might call synthetic random processes and generate random numbers using deterministic means. The use of the word <em>deterministic</em> in the previous sentence should bother you, but I suspect you’ll be more comfortable with the idea of simulating a random process via a deterministic process by the end of the next section.</p>
<h3 class="h3" id="ch00lev1_9"><strong>Deterministic Processes</strong></h3>
<p class="noindent">As von Neumann once said, “Anyone who attempts to generate random numbers by deterministic means is, of course, living in a state of sin.” The very idea of a random process is its unpredictability, such that knowledge of what came before is of no utility in predicting what will come after. By definition, a deterministic process follows a predictable algorithm; therefore, it cannot possibly be a true random process.</p>
<p class="indent">Why use deterministic processes, then? Even if the process is deterministic, it can approximate a random process to the level where the outputs are helpful. A <em>pseudorandom process</em> approximates a random process. We’ll make heavy use of pseudorandom processes throughout the book.</p>
<p class="indent">This section also covers how pseudorandom processes are related to <em>quasirandom processes</em>. Finally, we’ll discuss two hybrid processes likely already available on your computer.</p>
<h4 class="h4" id="ch00lev2_9"><em><strong>Pseudorandom Numbers</strong></em></h4>
<p class="noindent">There are many ways to approximate a random process that delivers pseudorandom numbers on demand. Here, I’ll introduce one such approach for use in our future experiments.</p>
<p class="indent">A <em>linear congruential generator (LCG)</em> is a simple approach to creating a sequence of pseudorandom numbers. The numbers generated by an LCG are good enough for a video game and for many of our experiments, but they are statistically weak and not recommended for serious use, like in cryptography.</p>
<p class="indent">The best way to understand an LCG is to dive right in:</p>
<p class="center"><em>x</em><sub><em>i</em> + 1</sub> = (<em>ax<sub>i</sub></em> + <em>c</em>) mod <em>m</em></p>
<p class="noindent">The entire generator is that single equation, where <em>x</em> is the value produced by the generator. The subscript means that the next value in the sequence, <em>x</em><sub><em>i</em> + 1</sub>, is derived from the previous, <em>x<sub>i</sub></em>. The initial value, <em>x</em><sub>0</sub>, is the <em>seed</em>, the value that primes the generator. Virtually all pseudorandom generators use a seed of some kind.</p>
<p class="indent"><span epub:type="pagebreak" id="page_22"/>The equation consists of two parts. The first is <em>ax<sub>i</sub></em> + <em>c</em>, where <em>a</em> and <em>c</em> are carefully chosen positive integers. The second part takes the result of the first, <em>y</em>, and calculates <em>y</em> mod <em>m</em>, where <em>m</em> is another carefully chosen positive integer. The mod operator refers to the modulo, which is nothing more than the remainder. To calculate it, the generator first finds <em>y</em>/<em>m</em> using integer division, then the remainder, which must be a number in the range [0, <em>m</em>). The final result is then used as both the output of the generator and the value used to calculate the following output.</p>
<p class="indent">As an example of how this works, let me show you an LCG in action. I’ll use small numbers, which helps in understanding, but would be terrible choices in practice, as we’ll see:</p>
<p class="center"><em>x</em><sub><em>i</em> + 1</sub> = (3<em>x<sub>i</sub></em> + 0) mod 7</p>
<p class="indent">The seed must be less than 7, so let’s use <em>x</em><sub>0</sub> = 4. Here’s the sequence produced by this LCG:</p>
<div class="image1"><img alt="Image" src="../images/f0022-02.jpg"/></div>
<p class="noindent">The output might seem a bit random, but after <em>x</em><sub>5</sub>, the sequence begins to repeat. All pseudorandom generators repeat eventually. The number of outputs before repeating determines the generator’s <em>period</em>. Here the period is 6, with 4, 5, 1, 3, 2, 6 repeating forever.</p>
<p class="indent">With properly chosen constants, the period can be much larger. For our experiments, we’ll use a set of constants popular in the 1980s, which came to be called the <em>MINSTD (minimum standard generator)</em>. It produces a sequence of unsigned integers with a period of 2<sup>31</sup> ≈ 10<sup>9</sup>. That’s a reasonable period for many applications, but there’s more to a good generator than its period. The closer the pseudorandom generator is to a truly random sequence, the better. Statistical test suites like <span class="literal">ent</span>, or more professional ones, seek to uncover all manner of correlations in the sequence spit out by the generator. With those test suites, it quickly becomes evident that MINSTD is a poor generator for serious work, but sufficient for our purposes.</p>
<p class="indent">I created a file of 100 million bytes using MINSTD and handed it to <span class="literal">ent</span>. I got the following results:</p>
<pre class="pre">entropy: 7.999998
chi2   : 75.99
mean   : 127.5058
pi     : 3.141177006 (0.01)
corr   : -0.000144</pre>
<p class="noindent">The output is very reasonable, so MINSTD will likely be of use to us.</p>
<p class="indent">The generator uses <em>a</em> = 48,271, <em>c</em> = 0, and <em>m</em> = 2,147,483,647 to make the generating equation:</p>
<p class="center"><em>x</em><sub><em>i</em> + 1</sub> = 48271<em>x<sub>i</sub></em> mod 2147483647</p>
<p class="indent"><span epub:type="pagebreak" id="page_23"/>If you’re familiar with how computers store numbers internally, you’ll notice that <em>m</em> is not using all 32 bits of a 32-bit integer. Instead, <em>m</em> = 2<sup>31</sup> – 1 is the largest positive integer that can be stored in a <em>signed</em> 32-bit value. Because of the modulo operation, the output of the generator must be in the range [0, 2<sup>31</sup> – 1), implying the generator can create at most only some 2 billion unique values.</p>
<p class="indent">If we need to generate a sequence of bytes, [0, 255], then we’re good. But what if we want to create a sequence of 64-bit floating-point values, what the C language calls a <span class="literal">double</span>? In that case, we have to do more work. The simplest approach is to divide <em>x</em> by <em>m</em>, since that must produce a number in the range [0, 1). For many applications, that’s sufficient, but it still only delivers a value selected from a set of 2 billion or so numbers. You don’t need to know the details of this, but a 64-bit floating-point value can store more than that because it uses a 52-bit base-2 mantissa. If we want to make full use of what a 64-bit floating-point value can give us, we need to generate two outputs from MINSTD and assign 52 bits extracted from both of them to the mantissa of a floating-point value with an exponent of 0. Thankfully, we don’t need that much precision, but it’s worth noting that simply dividing by <em>m</em> isn’t giving you all that you might think it is.</p>
<p class="indent">I previously mentioned that pseudorandom generators use seed values, a starting value. This requirement is a double-edged sword. Setting the seed to a specific number causes the generator to repeatedly output the same sequence of values, but the downside is that sometimes we might need to jump through hoops to make sure we aren’t using the same seed repeatedly.</p>
<p class="indent">For example, MINSTD with a seed of <em>x</em> = 8,675,309 will produce this sequence each time:</p>
<p class="center">0.00304057,  0.77134655,  0.66908364,  0.33651287,  0.8128977, . . .</p>
<p class="noindent">Meanwhile, using a seed of <em>x</em> = 1,234 will consistently produce:</p>
<p class="center">0.02773777,  0.93004224,  0.06911496,  0.24831591,  0.45733623, . . .</p>
<p class="noindent">The floating-point numbers are produced by dividing <em>x</em> by <em>m</em> = 2,147,483,647.</p>
<p class="indent">Often, we’ll use a pseudorandom generator, like those built into NumPy, without specifying a seed value. In that case, we want an unrepeatable sequence, at least unrepeatable for us, because we won’t know the seed selected. If no seed value is supplied, NumPy selects a value from <span class="literal">/dev/urandom</span>, a special system device discussed later in the chapter.</p>
<p class="indent">The pseudorandom generator we’ll use most often is PCG64, the generator NumPy uses by default. It’s a good generator, with a period of 2<sup>128</sup>. NumPy’s old default generator was the Mersenne Twister. Statistically, it’s still quite good, much better than MINSTD. The Mersenne Twister’s period is 2<sup>19,937</sup> – 1, which explains its more common name: MT19937. To be precise, its period is a huge number, 6,002 digits, and is utterly without meaning in human terms. PCG64’s period isn’t anything to scoff at either:</p>
<p class="center">2<sup>128</sup> = 340, 282, 366, 920, 938, 463, 463, 374, 607, 431, 768, 211, 456</p>
<p class="noindent"><span epub:type="pagebreak" id="page_24"/>That’s a number similarly without meaning in human terms. None of our experiments will come anywhere close to exhausting the sequence generated by either MT19937 or PCG64, both of which we’ll use frequently throughout the book.</p>
<p class="indent">The output of these generators follows a uniform distribution. Any other distribution, like a normal distribution, can be formed from a collection of uniformly distributed numbers. For example, it is straightforward to generate normally distributed numbers via the Box-Muller transformation, which maps two uniformly distributed numbers, <em>u</em><sub>1</sub> and <em>u</em><sub>2</sub>, to two normally distributed numbers:</p>
<div class="image1"><img alt="Image" src="../images/f0024-01.jpg"/></div>
<p class="indent">There’s much more to say about pseudorandom generators; many computer scientists have spent their careers working with them. However, we now understand all we need for our purposes.</p>
<h4 class="h4" id="ch00lev2_10"><em><strong>Quasirandom Sequences</strong></em></h4>
<p class="noindent">For some experiments, we’ll want random data that is <em>space-filling</em>. By that I mean a generator producing a sequence that appears random but that will, over time, fill space more or less evenly. A purely random process offers us no such guarantee, but a quasirandom sequence does.</p>
<p class="indent">The difference between a random or pseudorandom sequence and a quasirandom sequence is best understood via example. <a href="ch01.xhtml#ch01fig04">Figure 1-4</a> compares two sequences of 40 points, one generated pseudorandomly and one generated quasirandomly.</p>
<div class="image"><img alt="Image" id="ch01fig04" src="../images/01fig04.jpg"/></div>
<p class="figcap"><em>Figure 1-4: Two sets of 40 points, random (at 0.1) and quasirandom (at 0.2)</em></p>
<p class="indent">The lower sequence, at <em>y</em> = 0.1, was generated pseudorandomly. It covers the range from 0 to 1 (the <em>x</em>-axis), but does so with gaps. The second sequence, at <em>y</em> = 0.2, is a quasirandom sequence. It covers the same range, but is more consistent, with no crowding or gaps. Given enough samples, both sequences will eventually fill in the entire range, but the quasirandom sequence does so by scattering values more or less evenly throughout.</p>
<p class="indent">Our quasirandom process uses what is known as a <em>Halton sequence</em>. A Halton sequence is based on a prime number that divides the interval [0, 1) first by the base (the prime), then the base squared, then cubed, and so on. For a base of 2, the interval is first divided in half, then quarters, then <span epub:type="pagebreak" id="page_25"/>eighths, and so on. This process will eventually cover the interval in the infinite limit and fills the interval approximately evenly. For example, <a href="ch01.xhtml#ch01tab06">Table 1-6</a> shows the first few values of the Halton sequence for the given base (rounded to three digits).</p>
<p class="tabcap" id="ch01tab06"><strong>Table 1-6:</strong> Halton Sequences</p>
<table class="table-h">
<colgroup>
<col style="width:20%"/>
<col style="width:80%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><strong>Base</strong></th>
<th class="tab_th"><strong>Sequence</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1">2</td>
<td class="bg1"><span class="literal">0, 0.5 ,  0.25 , 0.75 , 0.125, 0.625, 0.375, 0.875</span></td>
</tr>
<tr>
<td class="bg">3</td>
<td class="bg"><span class="literal">0, 0.333, 0.667, 0.111, 0.444, 0.778, 0.222, 0.556</span></td>
</tr>
<tr>
<td class="bg1">5</td>
<td class="bg1"><span class="literal">0, 0.2 ,  0.4 ,  0.6 ,  0.8 ,  0.04 , 0.24 , 0.44</span></td>
</tr>
</tbody>
</table>
<p class="noindent">Each sequence begins with 0 and is entirely deterministic, like a pseudorandom generator with a fixed seed.</p>
<p class="indent">If the quasirandom sequence is so predictable, why is it useful? There are times when it is more important to fill space evenly than purely randomly. For example, some of our experiments will involve searching a multidimensional space to locate a point in that space that we consider best, for some definition of best. In that case, it often makes more sense to initialize the search so that the locations evaluated at first represent all of the space more or less equally.</p>
<p class="indent">Quasirandom sequences are most useful in combination. <a href="ch01.xhtml#ch01fig04">Figure 1-4</a> used a single quasirandom sequence with a base of 2 to fill in a one-dimensional space, the interval from 0 to 1. What if, instead, we wanted to fill in a two-dimensional space, like the <em>xy</em>-plane? For that, we need pairs of numbers. The seemingly obvious thing to do is sample twice to use the first number as the <em>x</em>-coordinate and the second as the <em>y</em>-coordinate. Let’s see what happens when we apply this approach to pseudorandom and quasi-random numbers.</p>
<p class="indent"><a href="ch01.xhtml#ch01fig05">Figure 1-5</a> attempts to fill 2D space with points.</p>
<div class="image"><img alt="Image" id="ch01fig05" src="../images/01fig05.jpg"/></div>
<p class="figcap"><em>Figure 1-5: From left to right: examples of bad quasirandom, pseudorandom, and good quasirandom sequences</em></p>
<p class="indent">In the middle of <a href="ch01.xhtml#ch01fig05">Figure 1-5</a>, I plotted 500 pairs sampled from a pseudorandom generator. The points are randomly distributed in space, but there are regions with fewer points and regions with more points, as expected. Next, I repeated this exercise using a quasirandom sequence with base 2. <span epub:type="pagebreak" id="page_26"/>That is, I asked the sequence for two samples, one after the other, and plotted them as a point. The result is on the left in <a href="ch01.xhtml#ch01fig05">Figure 1-5</a>. Clearly, something strange is happening.</p>
<p class="indent">The plot isn’t a mistake; the quasirandom sequence is doing precisely what it’s supposed to do. We’ve defined a random process as one where knowledge of previous values is of no utility in predicting subsequent values. That’s what the pseudorandom generator supplied; therefore, we can use sequentially generated values as coordinates for a point in 2D space. However, the quasirandom sequence offers no assurance that previous values do not indicate what comes next. Instead, the sequence is entirely predictable. Pairs of samples will be in a simple relationship to each other, which is what we see on the left in <a href="ch01.xhtml#ch01fig05">Figure 1-5</a>.</p>
<p class="indent">This does not mean we cannot use quasirandom sequences beyond one-dimensional cases. The trick is to use a different base for each dimension. Here, we have two dimensions, so we need <em>two</em> quasirandom sequences, each with a different base. The plot on the right in <a href="ch01.xhtml#ch01fig05">Figure 1-5</a> was generated this way. The <em>x</em>-coordinates are the first 500 samples from a quasirandom sequence with base 2, and the <em>y</em>-coordinates are the first 500 from a quasi-random sequence using base 3. Recall that the base must be a prime number. Since the two quasirandom sequences are not using the same base, the values are not (simply) correlated, so the points fill in the 2D space. Likewise, as each sequence is filling in the interval from [0, 1), and we are plotting them in pairs, it makes intuitive sense that they will fill in the 2D space as well, which is what the plot shows.</p>
<p class="indent">The moral of the story is that we can use quasirandom sequences, one base per dimension of the problem, to fill in some space in a way that seems random.</p>
<h4 class="h4" id="ch00lev2_11"><em><strong>Combining Deterministic and Truly Random Processes</strong></em></h4>
<p class="noindent">Truly random processes are the gold standard, but they tend to be relatively slow, at least given how quickly a computer works. A pseudorandom generator is a good substitute, but it’s only a truly random process wannabe. Why not merge the two? This section will examine two approaches to combining truly random processes with pseudorandom generators. I call these <em>hybrid processes</em>. We’ll discuss <span class="literal">/dev/urandom</span>, the Linux operating system’s approach, and <span class="literal">RDRAND</span>, a CPU-based hybrid processor instruction supported by many newer Intel and AMD processors.</p>
<p class="indent">Both <span class="literal">/dev/urandom</span>, henceforth <span class="literal">urandom</span>, and <span class="literal">RDRAND</span> operate in the same way: they use a slow source of true randomness to reseed a cryptographically secure pseudorandom number generator. We learned earlier in the chapter that pseudorandom generators have seeds, something that sets the initial state of the generator. Set the seed the same way, and the sequence of values generated repeats. The hybrid approaches frequently reseed the generator with the intention of altering the sequence to the point where, even if an adversary were to figure out what the pseudorandom generator was doing, it wouldn’t matter in practice because the seed would be random, making any knowledge of the state of the generator useless after a short time interval.</p>
<p class="indent"><span epub:type="pagebreak" id="page_27"/>I slipped a new phrase into the previous paragraph: <em>cryptographically secure</em>. Our experiments are not worried about adversaries and cryptography (well, mostly); we’re only concerned that the pseudorandom generator is “pretty good” in the sense that our experiments perform well using the generator.</p>
<p class="indent">A cryptographically secure pseudorandom generator is a high-quality pseudorandom generator with the following properties:</p>
<ul>
<li class="noindent">An attacker’s knowledge of the generator’s state at time <em>t<sub>c</sub></em> offers no ability for the attacker to know anything about the state of the generator at any previous time <em>t</em> &lt; <em>t<sub>c</sub></em>.</li>
<li class="noindent">For any output bit <em>i</em>, there is no polynomial-time algorithm operating on all previously generated bits, 0, . . . , <em>i</em> – 1, that can predict <em>i</em> with better than 50 percent accuracy.</li>
</ul>
<p class="noindent">The second property is known as the <em>next-bit</em> test. It’s what marks the cryptographically secure generator as a high-quality generator.</p>
<p class="indent">The phrase <em>polynomial-time algorithm</em> comes from the study of algorithms and how they perform regarding time and space. Polynomial-time algorithms are the nice ones; they are the algorithms that might finish sometime before the heat death of the universe. The classic bubble sort algorithm is a polynomial-time algorithm because its runtime scales as the square of the number of elements to sort; that is, sorting <em>n</em> elements takes time on the order of <em>n</em><sup>2</sup>, a polynomial. Any algorithm whose time or space resources are bounded by a polynomial is also a polynomial-time algorithm. For example, the Quicksort algorithm scales as <em>n</em> log <em>n</em> for <em>n</em> items. That’s a function easily bounded by a polynomial, like <em>n</em><sup>2</sup>, so Quicksort is also a polynomial-time algorithm. An algorithm that scales as 2<em><sup>n</sup></em> is not a polynomial-time algorithm because 2<em><sup>n</sup></em> is an exponential, not a polynomial. Exponential algorithms are bad because they quickly become intractable, which is precisely what people concerned about attacks on their pseudorandom number generators want.</p>
<p class="indent">Algorithmically, a hybrid process uses a strong pseudorandom generator to deliver random values on demand while periodically reseeding said generator from a truly random source. Let’s explore the <span class="literal">urandom</span> and <span class="literal">RDRAND</span> approaches to this problem.</p>
<h5 class="h5"><strong>Reading Random Bytes from urandom</strong></h5>
<p class="noindent">Unix systems treat many things that are not files as if they were files, and <span class="literal">urandom</span> is no exception. To acquire random bytes from <span class="literal">urandom</span>, we need to treat it like a file. For example, let’s run Python, open <span class="literal">urandom</span>, and dump 60 million bytes from it to a disk file so <span class="literal">ent</span> can evaluate them:</p>
<pre class="pre">&gt; <span class="codestrong1">python3</span>
&gt;&gt;&gt; <span class="codestrong1">b = open("/dev/urandom", "rb").read(60000000)</span>
&gt;&gt;&gt; <span class="codestrong1">open("ttt.bin", "wb").write(b)</span>
60000000</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_28"/>The output file is <em>ttt.bin</em>, a throwaway name. When I ran the code, <span class="literal">ent</span> reported the following:</p>
<pre class="pre">entropy: 7.999997
chi2   : 23.16
mean   : 127.4981
pi     : 3.141671600 (0.00)
corr   : 0.000024</pre>
<p class="noindent">As expected, <span class="literal">urandom</span> performs well and will work for our purposes as a randomness engine.</p>
<p class="indent">The Linux kernel maintains an <em>entropy pool</em>, a collection of bytes derived from system operations that it uses to update the seed of the pseudorandom generator every 300 seconds, according to kernel file <em>random.c</em>, which is available at <em><a href="https://github.com/torvalds/linux/blob/master/drivers/char/random.c">https://github.com/torvalds/linux/blob/master/drivers/char/random.c</a></em>. The same file also reveals the kernel’s entropy sources, if curiosity strikes.</p>
<p class="indent">We can monitor the size of the entropy pool by reading the contents of the file <em>entropy_avail</em>:</p>
<pre class="pre">&gt; <span class="codestrong1">cat /proc/sys/kernel/random/entropy_avail</span>
3693</pre>
<p class="noindent">In this case, we’re told that at the particular moment I executed the command, there were 3,693 bytes in the entropy pool, meaning that 3,693 bytes were available to reseed the generator when the reseed interval expired.</p>
<p class="indent">Linux uses the ChaCha20 pseudorandom number generator. It’s a reasonably new generator that performs exceptionally well overall, even when subjected to more intensive test suites. The details don’t concern us here, only that <span class="literal">urandom</span> delivers the goods.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>If you spend much time reviewing resources related to</em> <span class="codeitalic">/dev/urandom</span>, <em>you’ll run across its cousin,</em> <span class="codeitalic">/dev/random</span><em>. The latter device was intended for small amounts of high-quality random data, and it will block until enough entropy is available. This is contrary to our requirements, so we’re ignoring</em> <span class="codeitalic">/dev/random</span> <em>entirely. Indeed, it was announced in March 2022 that future versions of the Linux kernel will make</em> <span class="codeitalic">/dev/random</span> <em>nothing more than another name for</em> <span class="codeitalic">/dev/urandom</span>.</p>
</div>
<h5 class="h5"><strong>Using the RDRAND Instruction</strong></h5>
<p class="noindent">The RDRAND instruction, if available for your CPU, provides access to high-quality random numbers using much the same approach as the Linux kernel and <span class="literal">urandom</span>. The key differences are that the entropy source is part and parcel of the CPU itself and the pseudorandom generator is updated at an interval no longer than after returning 1,022 random values. That is, <span class="literal">RDRAND</span> is reseeded based on the number of samples returned, not on a fixed time interval like <span class="literal">urandom</span>.</p>
<p class="indent">The pseudorandom number generator used is not ChaCha20 but CTR_DRBG, a generator developed by the National Institute of Standards and Technology (NIST), part of the United States Department of Commerce. The close association between NIST and the US government has caused <span epub:type="pagebreak" id="page_29"/>some to distrust <span class="literal">RDRAND</span>’s output, but as we are not concerned with cryptographic security, <span class="literal">RDRAND</span> is fair game.</p>
<p class="indent">Unlike <span class="literal">urandom</span>, which is accessed as if it were a binary file and therefore available immediately to all programming languages, <span class="literal">RDRAND</span> is a CPU instruction, so we need to access it via lower-level code or by installing a Python library that uses the instruction for us. As it happens, the <span class="literal">rdrand</span> library will do nicely:</p>
<pre class="pre">&gt; <span class="codestrong1">pip install rdrand</span>
&gt; <span class="codestrong1">python3</span>
&gt;&gt;&gt; <span class="codestrong1">from rdrand import RdRandom</span>
&gt;&gt;&gt; <span class="codestrong1">RdRandom().random()</span>
0.2047133384122450
&gt;&gt;&gt; <span class="codestrong1">b = rdrand.rdrand_get_bytes(60000000)</span>
&gt;&gt;&gt; <span class="codestrong1">open("ttt.bin", "wb").write(b)</span>
60000000</pre>
<p class="noindent">The class <span class="literal">RdRandom</span> returns an interface to <span class="literal">RDRAND</span> and supports the <span class="literal">random</span> method to return a random float. Alternatively, the <span class="literal">rdrand_get_bytes</span> function returns the requested number of random bytes, here 60 million, so we can compare <span class="literal">ent</span>’s report with the output of <span class="literal">urandom</span>.</p>
<p class="indent">Passing the bytes in <em>ttt.bin</em> to <span class="literal">ent</span> gives us the following results:</p>
<pre class="pre">entropy: 7.999997
chi2   : 54.51
mean   : 127.5036
pi     : 3.141309600 (0.01)
corr   : -0.000159</pre>
<p class="noindent">For our purposes, these results are indistinguishable from those of <span class="literal">urandom</span>; therefore, we’ll rely on <span class="literal">RDRAND</span> from time to time as well.</p>
<p class="indent">If you’re looking to access <span class="literal">RDRAND</span> from C, the code in <em>drng.c</em> will serve as a guide. Note the <span class="literal">gcc</span> compile instructions at the top of the file.</p>
<p class="indent">In the next section, I’ll detail the <span class="literal">RE</span> class, a Python wrapper for different randomness sources, and the randomness engine we’ll use consistently for all the book’s experiments.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>For other ways to generate pseudorandom numbers, see my book</em> Random Numbers and Computers <em>(Springer, 2018). It includes a thorough treatment of all things related to pseudorandom number generation, including code for specific algorithms.</em></p>
</div>
<h3 class="h3" id="ch00lev1_10"><strong>The Book’s Randomness Engine</strong></h3>
<p class="noindent">In this section, we build the <span class="literal">RE</span> class, the randomness engine that will power most of our experiments throughout the remainder of the book. If you’d prefer to jump right in, read through <em>RE.py</em>, and then come back here to fill in any details you missed.</p>
<p class="indent"><span epub:type="pagebreak" id="page_30"/>We want to design a class that will accomplish the following:</p>
<ul>
<li class="noindent">Provide a vector of a specified number of uniform random samples</li>
<li class="noindent">Generate pseudorandom output using PCG64, MT19937, or MINSTD</li>
<li class="noindent">Generate hybrid random output using <span class="literal">urandom</span> or <span class="literal">RDRAND</span></li>
<li class="noindent">Generate quasirandom output for any specified prime base</li>
<li class="noindent">Produce floats or integers in any range, [<em>a</em>, <em>b</em>)</li>
<li class="noindent">Produce bytes ([0, 255]) or bits ([0, 1])</li>
<li class="noindent">Allow a seed value to generate the same output repeatedly</li>
<li class="noindent">Substitute a file of bytes as the randomness source</li>
</ul>
<p class="indent">Let’s review <span class="literal">RE</span>’s source code before taking it for a brief test drive.</p>
<h4 class="h4" id="ch00lev2_12"><em><strong>The RE Class</strong></em></h4>
<p class="noindent">The <span class="literal">RE</span> class is configured via its constructor and provides only one method meant for public use: <span class="literal">random</span>. This method takes a single argument, an integer specifying the number of samples to return as a NumPy vector.</p>
<p class="indent">The <span class="literal">RE</span> class’s private methods implement the possible randomness sources, shown in <a href="ch01.xhtml#ch01tab07">Table 1-7</a>.</p>
<p class="tabcap" id="ch01tab07"><strong>Table 1-7:</strong> <span class="literal">RE</span>’s Methods</p>
<table class="table-h">
<colgroup>
<col style="width:40%"/>
<col style="width:60%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><strong>Method</strong></th>
<th class="tab_th"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1"><span class="literal">Fetch</span></td>
<td class="bg1">Sample from a disk file</td>
</tr>
<tr>
<td class="bg"><span class="literal">MINSTD</span></td>
<td class="bg">Sample from MINSTD</td>
</tr>
<tr>
<td class="bg1"><span class="literal">Urandom</span></td>
<td class="bg1">Read from <span class="literal">/dev/urandom</span></td>
</tr>
<tr>
<td class="bg"><span class="literal">RDRAND</span></td>
<td class="bg">Read from <span class="literal">RDRAND</span></td>
</tr>
<tr>
<td class="bg1"><span class="literal">Quasirandom</span></td>
<td class="bg1">Sample from the Halton sequence</td>
</tr>
<tr>
<td class="bg"><span class="literal">NumPyGen</span></td>
<td class="bg">Sample from PCG64 or MT19937</td>
</tr>
</tbody>
</table>
<p class="noindent">Each private method accepts a single argument, the number of samples to return. Also, aside from <span class="literal">Fetch</span>, each private method returns a floating-point vector of samples in the range [0, 1).</p>
<h5 class="h5"><strong>NumPyGen and MINSTD</strong></h5>
<p class="noindent">Let’s look first at <span class="literal">NumPyGen</span> and <span class="literal">MINSTD</span>, as they are straightforward. To save space, I’ve removed comments and doc strings. They are present in <em>RE.py</em>:</p>
<pre class="pre">def NumPyGen(self, N):
    return self.g.random(N)

def MINSTD(self, N):
    v = np.zeros(N)
    for i in range(N):
        <span epub:type="pagebreak" id="page_31"/>self.seed = (48271 * self.seed) % 2147483647
        v[i] = self.seed * 4.656612875245797e-10
    return v</pre>
<p class="indent">Let’s begin with <span class="literal">NumPyGen</span>, as it’s about as simple as you can get. The <span class="literal">RE</span> class constructor, which we’ll discuss later, creates a NumPy generator, if that’s the source desired, and stores it in the <span class="literal">g</span> member variable. Don’t give me that look; we’re experimenting, and <span class="literal">g</span> is a perfectly good variable name.</p>
<p class="indent">NumPy already knows how to return vectors of floating-point numbers, so all that remains is to tell NumPy we want <span class="literal">N</span> of them, which we immediately return to the caller, <span class="literal">random</span>, which we’ll also discuss later on.</p>
<p class="indent">Similarly, <span class="literal">MINSTD</span>’s job is also to return a vector of floating-point samples. We first create a vector to hold the samples and then loop, applying the LCG equation</p>
<p class="center"><em>x</em><sub><em>i</em>+1</sub> = (<em>ax<sub>i</sub></em>) mod <em>m</em></p>
<p class="noindent">with <em>a</em> = 48,271 and <em>m</em> = 2<sup>31</sup> – 1 = 2,147,483,647. To convert <em>x</em><sub><em>i</em>+1</sub> to a float in [0, 1), we divide by <em>m</em> or, as here, multiply by 1/<em>m</em>.</p>
<p class="indent">There are a few things to notice. First, the <span class="literal">seed</span> member variable <em>is x</em> if the MINSTD generator is selected. That’s why it’s updated. Second, the MINSTD equation is iterative; to get <em>x</em><sub><em>i</em>+1</sub> we need <em>x<sub>i</sub></em> as pseudorandom values must be generated sequentially (usually). In practice, this means that our implementation of MINSTD is a bit slow, but that’s fine for our experiments.</p>
<p class="indent">Remember that MINSTD deals with 32-bit integers, meaning floating-point numbers returned by <span class="literal">MINSTD</span> are only as precise as 32-bit floats. If you know C, this means it returns <span class="literal">float</span>, not <span class="literal">double</span>. A Python <span class="literal">float</span> is a C <span class="literal">double</span>, and likewise uses 64 bits (52 for the mantissa or significand). For most of what we do in this book, the loss of precision isn’t important, but you should be aware of it if you wish to use <span class="literal">RE</span> in your projects.</p>
<h5 class="h5"><strong>RDRAND</strong></h5>
<p class="noindent">The <span class="literal">RDRAND</span> method needs to use the CPU’s RDRAND instruction, which it does via the <span class="literal">rdrand</span> module. When the <span class="literal">RE</span> class is imported, Python tries to load <span class="literal">rdrand</span> (see the top of <em>RE.py</em>). If present, the <span class="literal">RDRAND</span> method uses the library to access samples. If the library isn’t present, <span class="literal">RDRAND</span> still works but falls back to NumPy’s PCG64 generator and issues a suitable warning message.</p>
<p class="indent">Let’s look at <span class="literal">RDRAND</span>, ignoring the part where <span class="literal">rdrand</span> isn’t available:</p>
<pre class="pre">def RDRAND(self, N): 
    v = np.zeros(N)
    rng = rdrand.RdRandom()
    for i in range(N):
        v[i] = rng.random()
    return v</pre>
<p class="indent">The <span class="literal">rdrand</span> module supports a handful of methods, but we’re restricting ourselves to <span class="literal">random</span>, which returns a single float in [0, 1), using all 52 bits of the mantissa. However, as <span class="literal">random</span> is returning a single number, we need a <span epub:type="pagebreak" id="page_32"/>loop, so <span class="literal">RDRAND</span> isn’t particularly fast. If you want bytes from <span class="literal">rdrand</span>, you’ll be better served by using the module’s <span class="literal">rdrand_get_bytes</span> function directly.</p>
<h5 class="h5"><strong>Urandom</strong></h5>
<p class="noindent">The <span class="literal">RE</span> class uses <span class="literal">/dev/urandom</span> as follows:</p>
<pre class="pre">def Urandom(self, N):
    with open("/dev/urandom", "rb") as f:
        b = bytearray(f.read(4*N))
    return np.frombuffer(b, dtype="uint32") / (1&lt;&lt;32)</pre>
<p class="noindent">The code first reads four times as many bytes as requested into a Python byte array. Then, it tells NumPy to treat the byte array as a buffer and read 32-bit unsigned integers from it. The resulting array is divided by 2<sup>32</sup> to change it into a vector in [0, 1). Like <span class="literal">MINSTD</span>, we’re treating <span class="literal">urandom</span> as a source of 32-bit floats.</p>
<h5 class="h5"><strong>Quasirandom</strong></h5>
<p class="noindent">Quasirandom numbers are a bit different, as we saw earlier in the chapter, but the implementation is similar to the other sources:</p>
<pre class="pre">def Quasirandom(self, N):
    v = []
    while (len(v) &lt; N):
        v.append(Halton(self.qnum, self.base))
        self.qnum += 1
    return np.array(v)</pre>
<p class="noindent">The output vector (<span class="literal">v</span>) is constructed sample by sample. The <span class="literal">while</span> loop calls <span class="literal">Halton</span>, a function contained within <span class="literal">Quasirandom</span>. This function returns a specific number in the Halton sequence for the given prime base. The next number to use is in member variable <span class="literal">qnum</span>. We’ll return to <span class="literal">qnum</span> later when discussing the <span class="literal">RE</span> constructor.</p>
<h5 class="h5"><strong>Random</strong></h5>
<p class="noindent">The <span class="literal">RE</span> class’s only public method is <span class="literal">random</span>:</p>
<pre class="pre">def random(self, N=1):
    if (not self.disk):
        v = self.generators[self.kind](N)
        if (self.mode == "float"):
            v = (self.high - self.low)*v + self.low
        elif (self.mode == "int"):
            v = ((self.high - self.low)*v).astype("int64") + self.low
        elif (self.mode == "byte"):
            v = np.floor(256*v + 0.5).astype("uint8")
        else:
            v = np.floor(v + 0.5).astype("uint8")
    <span epub:type="pagebreak" id="page_33"/>else:
        v = self.Fetch(N)
    return v[0] if (N == 1) else v</pre>
<p class="indent">If we’re not using a disk file, then the process is the same, regardless of the randomness source: first, generate the requested number of floating-point samples in [0, 1), and then modify them to be in the desired range and of the desired type.</p>
<p class="indent">As all randomness sources accept the same argument and return the same [0, 1) vector, we store references to the particular methods in the dictionary <span class="literal">generators</span>. Then, to get <span class="literal">v</span>, we need only call the appropriate method indexing by <span class="literal">kind</span> and passing the number of samples (<span class="literal">N</span>), which defaults to 1.</p>
<p class="indent">With <span class="literal">v</span> in hand, we modify it according to the selected configuration. If we want floating-point, we multiply by <span class="literal">high</span> and add <span class="literal">low</span>, which default to 1 and 0, respectively. This returns a float in the range [low, high).</p>
<p class="indent">For integers, we first map <span class="literal">v</span> to [0, high) before adding <span class="literal">low</span> to give an integer in the range [low, high). The upper limit is not present in both cases, thereby following Python convention.</p>
<p class="indent">To get bytes, multiply by 256 and round. Finally, to get bits, round <span class="literal">v</span>.</p>
<h5 class="h5"><strong>Fetch</strong></h5>
<p class="noindent">What about <span class="literal">Fetch</span>, you ask? It’s a mix of <span class="literal">Urandom</span> and <span class="literal">random</span>:</p>
<pre class="pre">def Fetch(self, N=1):
    if (self.mode == "byte"):
        nbytes = N
    else:
        nbytes = 4*N

    b = []
    n = nbytes
    while (len(b) &lt; nbytes):
        t = self.file.read(n)
        if (len(t) &lt; n):
            n = n - len(t)
            self.file.close()
            self.file = open(self.kind, "rb")
        b += t

    if (self.mode == "byte"):
        v = np.array(b, dtype="uint8")
    else:
        v = np.frombuffer(bytearray(b), dtype="uint32")
        v = v / (1 &lt;&lt; 32)
        if (self.mode == "float"):
            v = (self.high - self.low)*v + self.low
        elif (self.mode == "int"):
            v = ((self.high - self.low)*v).astype("int64") + self.low
        <span epub:type="pagebreak" id="page_34"/>elif (self.mode == "byte"):
            v = np.floor(256*v + 0.5).astype("uint8")
        else:
            v = np.floor(v + 0.5).astype("uint8")
    return v</pre>
<p class="indent">The code is split into three paragraphs. The first calculates the number of bytes to read from the disk file (<span class="literal">nbytes</span>). As with <span class="literal">Urandom</span>, we’re restricting ourselves to 32-bit floats.</p>
<p class="indent">The second paragraph reads the bytes from disk and stores them in <span class="literal">b</span>. If we run out of file, we start again from the beginning. Remember this to ensure you don’t ask for far more samples than the file can supply. Think of the size of the file divided by four as the period of the generator.</p>
<p class="indent">The third paragraph massages the data accordingly. If we want bytes, we’re done; we simply convert the list <span class="literal">b</span> into a NumPy vector and return it. Otherwise, we first treat the bytes as a buffer and read them as unsigned 32-bit integers, which we divide by 2<sup>32</sup> to make <span class="literal">v</span> in the range [0, 1). We then convert the data to the final output format as in the <span class="literal">random</span> method. There is some code duplicated between <span class="literal">random</span> and <span class="literal">Fetch</span>, but pedagogically, the clarity this provides is worth it.</p>
<h5 class="h5"><strong>The Constructor</strong></h5>
<p class="noindent">The <span class="literal">RE</span> class constructor configures the randomness engine. The first part defines defaults and builds the dictionary of private methods:</p>
<pre class="pre">def __init__(self, mode="float", kind="pcg64", seed=None, low=0, high=1, base=2):
    self.generators = {
        "pcg64"  : self.NumPyGen,
        "mt19937": self.NumPyGen,
        "minstd" : self.MINSTD, 
        "quasi"  : self.Quasirandom,
        "urandom": self.Urandom,
        "rdrand" : self.RDRAND,
    }

    self.mode = mode  
    self.kind = kind  
    self.seed = seed  
    self.low  = low   
    self.high = high  
    self.base = base  
    self.disk = False</pre>
<p class="indent">The <span class="literal">mode</span> defines what <span class="literal">RE</span> returns. It’s a string: <span class="literal">float</span>, <span class="literal">int</span>, <span class="literal">byte</span>, or <span class="literal">bit</span>. Use <span class="literal">kind</span> to define the source. Possible values are the keys of <span class="literal">generators</span> or a pathname to a disk file. Case matters here.</p>
<p class="indent"><span epub:type="pagebreak" id="page_35"/>Use <span class="literal">low</span> and <span class="literal">high</span> to set the output range, ignored for bytes and bits. Remember, the output does not include <span class="literal">high</span>. To get deterministic sequences, set the <span class="literal">seed</span>. Finally, use <span class="literal">base</span> if working with a quasirandom sequence.</p>
<p class="indent">The second part handles source-specific things:</p>
<pre class="pre">if (self.kind == "pcg64"):
    self.g = np.random.Generator(np.random.PCG64(seed))
elif (self.kind == "mt19937"):
    self.g = np.random.Generator(np.random.MT19937(seed))
elif (self.kind == "minstd"):
    if (seed == None):
        self.seed = np.random.randint(1,93123544)
elif (self.kind == "quasi"):
    if (seed == None):
        self.qnum = 0
    elif (seed &lt; 0):
        self.qnum = np.random.randint(0,10000)
    else:
        self.qnum = seed
elif (self.kind == "urandom") or (self.kind == "rdrand"):
    pass
else:
    self.disk = True
    self.file = open(self.kind, "rb")</pre>
<p class="indent">I leave parsing the second part to you. I’ll just point out that for quasi-random sequences, the seed value is used to set the initial Halton sequence value, which is random if a negative seed is given. Use a negative seed to spice up the deterministic Halton sequence with a bit of randomness.</p>
<p class="indent">With that, we’re finished with the implementation of <span class="literal">RE</span>. Now let’s learn how to use it.</p>
<h4 class="h4" id="ch00lev2_13"><em><strong>RE Class Examples</strong></em></h4>
<p class="noindent">The following examples illustrate how we’ll use the <span class="literal">RE</span> class.</p>
<p class="indent">The first example imports <span class="literal">RE</span> and defines a generator using all the defaults—PCG64 with floating-point output in [0, 1):</p>
<pre class="pre">&gt;&gt;&gt; <span class="codestrong1">from RE import *</span> 
&gt;&gt;&gt; <span class="codestrong1">g = RE()</span>
&gt;&gt;&gt; <span class="codestrong1">g.random(5)</span>
array([0.44018704, 0.98320526, 0.61820454, 0.3124574 , 0.32110503])
&gt;&gt;&gt; <span class="codestrong1">g.random(5)</span>
array([0.47792361, 0.67769858, 0.50001674, 0.35449271, 0.92454641])</pre>
<span epub:type="pagebreak" id="page_36"/>
<p class="indent">The second example creates an instance of the MT19937 generator and uses it to return five floating-point values in [–3, 5):</p>
<pre class="pre">&gt;&gt;&gt; <span class="codestrong1">RE(kind='mt19937', low=-3, high=5).random(5)</span>
array([ 4.51484908,  2.31892577,  0.98488816, -1.36846592,  1.70944267])</pre>
<p class="indent">Next, we use <span class="literal">urandom</span> to return integers in [–3, 5) and use <span class="literal">RDRAND</span> to sample bytes:</p>
<pre class="pre">&gt;&gt;&gt; <span class="codestrong1">RE(kind='urandom', low=-3, high=5, mode='int').random(5)</span>
array([2, 2, 4, 3, 2])
&gt;&gt;&gt; <span class="codestrong1">RE(kind='rdrand', mode='byte').random(9)</span>
array([ 67, 173, 207, 230,  10, 127, 241,  21, 213], dtype=uint8)</pre>
<p class="indent">Here’s an example that specifies a seed value to return the same sequence each time:</p>
<pre class="pre">&gt;&gt;&gt; <span class="codestrong1">RE(kind='minstd', seed=5, mode='bit').random(9)</span>
array([0, 0, 0, 0, 1, 1, 1, 1, 0], dtype=uint8)
&gt;&gt;&gt; <span class="codestrong1">RE(kind='minstd', seed=5, mode='bit').random(9)</span>
array([0, 0, 0, 0, 1, 1, 1, 1, 0], dtype=uint8)</pre>
<p class="indent">For a quasirandom sequence, if no seed is given, the sequence begins at zero each time. If the seed is less than zero, the starting position is set randomly:</p>
<pre class="pre">&gt;&gt;&gt; <span class="codestrong1">RE(kind='quasi', base=2).random(6)</span>
array([0.   , 0.5  , 0.25 , 0.75 , 0.125, 0.625])
&gt;&gt;&gt; <span class="codestrong1">RE(kind='quasi', base=2).random(6)</span>
array([0.   , 0.5  , 0.25 , 0.75 , 0.125, 0.625])
&gt;&gt;&gt; <span class="codestrong1">RE(kind='quasi', base=2, seed=-1).random(6)</span>
array([0.3458252, 0.8458252, 0.2208252, 0.7208252, 0.4708252, 0.9708252])
&gt;&gt;&gt; <span class="codestrong1">RE(kind='quasi', base=2, seed=-1).random(6)</span>
array([0.74029541, 0.49029541, 0.99029541, 0.01373291, 0.51373291,
       0.26373291])</pre>
<p class="indent">This example sets <span class="literal">kind</span> to a filename to sample from a disk file, here called <em>hotbits.bin</em>:</p>
<pre class="pre">&gt;&gt;&gt; <span class="codestrong1">RE(kind='hotbits.bin').random(5)</span>
array([0.58051941, 0.79079893, 0.91321132, 0.26857162, 0.49829243])</pre>
<p class="indent">Now that we know how to use <span class="literal">RE</span>, let’s start experimenting.</p>
<span epub:type="pagebreak" id="page_37"/>
<h3 class="h3" id="ch00lev1_11"><strong>Summary</strong></h3>
<p class="noindent">This chapter focused on generating randomness, that is, on random processes. First, we explored the relationship between probability and randomness and learned that random processes sample from probability distributions, either continuous or discrete. We learned that there is generally no concrete answer to the question, How do we know if the output of a process is random? However, there are ways to test sequences to enhance our belief one way or the other. In practice, we declared the output of <span class="literal">ent</span> as our standard, since our experiments do not need state-of-the-art random processes.</p>
<p class="indent">Next, we discussed truly random processes. We began with classical approaches like coin flips and dice rolls, and then shifted our attention to physical processes such as random fluctuations in an analog signal, radio frequency noise due to atmospheric effects, and the decay of radioactive elements.</p>
<p class="indent">Pseudorandom and quasirandom sequences are random process mimics. Though they pretend to be the output of truly random processes, they are not. However, they do shadow truly random processes with sufficient fidelity to make them the primary drivers for our experiments. After learning the basics of pseudorandom and quasirandom generators, we covered hybrid generators, the marriage of pseudorandomness and truly random processes. Hybrid generators provide cryptographically secure sequences.</p>
<p class="indent">The chapter concluded with a walkthrough of the design and code for the <span class="literal">RE</span> class, the randomness engine that will power all of our experiments.<span epub:type="pagebreak" id="page_38"/></p>
</body></html>