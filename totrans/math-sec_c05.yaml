- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Identifying Threats with Social Network Analysis
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Social network analysis (SNA)* is a subset of graph theory that describes
    complex human interactions mathematically; it can be used in any research where
    human interaction is a factor. Security researchers use SNA for everything from
    predicting the spread of malicious content to identifying potential insider threats.
    We’ll do our own SNA in this chapter: we’ll build a humble social network graph
    from Mastodon posts, then use it to understand influence and information exchange
    among users. Specifically, we’ll be looking at a real social network effect known
    as the *small-world phenomenon*. Then we’ll look at how to build a graph from
    posts, answer a few research questions, and end with a proof-of-concept project,
    where you’ll be able to capture data from your own Mastodon timelines.'
  prefs: []
  type: TYPE_NORMAL
- en: The Small-World Phenomenon
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stanley Milgram’s *small-world experiments* demonstrated the influence of group
    conformity on human decision-making. The aim of the experiments was to examine
    the average path length for social networks in the United States, where the *path
    length* is the number of people it takes to get a letter from one person in the
    network to another, seemingly disconnected person. Milgram typically chose individuals
    in the US cities of Omaha, Nebraska, and Wichita, Kansas, to be the starting points;
    someone in Boston, Massachusetts, was the typical end point. Upon receiving the
    invitation to participate, the person designated as the original letter sender
    (the starting point) was asked whether they personally knew the randomly selected
    final recipient (the end point). If so, the starting point was to forward the
    letter directly to the end point. In the more likely scenario—the starting point
    doesn’t know the end point—the starting point was asked to think of a friend or
    relative who was more likely to know the end point. People along the path could
    forward the letter to anyone they knew who might be able to get the letter closer
    to the end point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Technically speaking, a social network exhibits the small-world phenomenon
    if any two individuals in the network are likely to be connected through a small
    number of intermediate acquaintances. Milgram’s research showed that our society
    is a strongly connected network: any two members of the network are likely to
    be connected through three to six intermediate acquaintances (this is popularly
    known as *six degrees of separation*, a specific case of the small-world phenomenon).
    The mechanism at play in the small-world phenomenon is called *preferential attachment*,
    where a person is more likely to form a connection to someone who already has
    a lot of connections. Put simply, you are statistically more likely to meet a
    new person who goes out and meets a lot of new people than to meet a shut-in with
    only a few social interactions. These types of networks, it turns out, are abundant
    in nature, having been observed everywhere from animal social structures to the
    human brain. Clearly, it’s worth our time as security analysts to understand it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal for analyzing our data set of fictional posts and users is to answer
    the following research questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How much information gets propagated?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What cliques exist in this network?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who are the three most influential users?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who are the three most influenced users?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who could introduce the most new connections?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over the rest of this chapter, we’ll cover each topic in turn, see how we can
    reinterpret the previous theory to gain insight into social network user interactions,
    and explore new graph theory topics, like residual information and node ancestry.
  prefs: []
  type: TYPE_NORMAL
- en: Graphing Social Network Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To turn our social network data into a graph, first we need to structure it
    into a searchable table format. We’ll be using the pandas library, which gives
    us access to functions and data structures that help us organize our data to prepare
    it for graphing.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s look at the raw JSON data. The file *fake_posts.json*, included
    with the book’s supplemental materials, contains 28,034 post-like objects formatted
    in the JSON schema shown in [Listing 5-1](#listing5-1).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-1: A mock API response using an example of the Mastodon schema'
  prefs: []
  type: TYPE_NORMAL
- en: The `id` field ❶ holds a numeric ID that the API assigns to an individual record—the
    post—when the post is created. The `content` field ❷ contains the data being added
    to the network—that is, the text that makes up the post. Some posts are originals
    and the rest are replies to posts, or replies to replies, and so on. The `reblog_count`
    field ❺ tallies how many times a post object received a reply. An object representing
    a reblog will contain the field names that start with `in_reply_to_` ❸. The `account`
    field ❹ is a nested JSON object that identifies the post creator.
  prefs: []
  type: TYPE_NORMAL
- en: Structuring the Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll be retaining a lot more data from the post objects than we did from packets
    in [Chapter 4](c04.xhtml), and the post object structure is nested, so first we
    need to load the data file into a pandas `DataFrame` object. A `DataFrame` object
    is the pandas version of row and column data storage for tabular data. It’s similar
    in structure to a database (and even supports some of the same operations, like
    filtering and joining data). Using `DataFrame` gives us a more convenient syntax
    for sorting and selecting relevant post objects, and it highlights the power of
    combining analytical libraries. By combining tools (in this case, pandas and NetworkX),
    you can choose the right tool for a particular job instead of trying to make a
    library do something it wasn’t designed for.
  prefs: []
  type: TYPE_NORMAL
- en: The code in [Listing 5-2](#listing5-2) defines a `DataFrame` object from the
    example data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-2: Creating a pandas `DataFrame` object from example JSON data'
  prefs: []
  type: TYPE_NORMAL
- en: After importing the required libraries ❶, we define a helper function called
    `user_to_series` ❷, which I’ll discuss in depth in a moment; at a high level,
    this function converts each JSON user object into a row suitable for use in a
    pandas `DataFrame`. We load *fake_posts.json* in the typical fashion using `with
    open` ❸, remove any trailing whitespace characters with the `strip` function,
    and split the file data into rows using the remaining `"\n"` characters. The pandas
    library can create a `DataFrame` from a list of JSON objects, so we convert each
    string row into a JSON object using `json.loads` ❹ and collect the objects in
    the `series_data` list ❺.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, when the JSON contains nested objects, like the `account` field,
    pandas doesn’t know how to unpack them. We need to turn the nested fields into
    a flat pandas object using the pandas functions `apply` and `concat` ❻ to apply
    the `user_to_series` function to each row in the data, creating a flat pandas
    `Series`. You can think of a *series* as similar to a row in database parlance—it
    groups all of the data relevant to a single entry.
  prefs: []
  type: TYPE_NORMAL
- en: The `pd.concat` pandas function appends these new features to the current `DataFrame`
    for all rows. The `axis=1` parameter tells pandas to use the series as *features*
    (columns in database parlance), which results in the `DataFrame` having a column
    matching each piece of data in the user field (such as username and ID). Each
    row then represents the user, and each column holds the value of that field for
    that particular user.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly we remove the original `account` feature, which is no longer needed,
    using `DataFrame.drop` ❼. Once we’ve loaded the initial data set and applied all
    the column processing, we can print out the structure of the data by calling `post_df.info`.
    [Listing 5-3](#listing5-3) shows the structure resulting from [Listing 5-2](#listing5-2),
    which you can see in the Jupyter notebook *Mastodon_network.ipynb* in the supplemental
    materials.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-3: Post data structure in pandas'
  prefs: []
  type: TYPE_NORMAL
- en: The data structure tells us a few important things. First, the `RangeIndex`
    property tells us how many rows of data are currently in the `DataFrame` object.
    In this case we have loaded 28,034 post records, indexed from `0` to `28033`.
    Next, we can see that there’s no longer an `account` column in the list, which
    means our drop operation in [Listing 5-2](#listing5-2) successfully modified the
    `DataFrame`. The number to the right of the column name represents how many rows
    in the data have a non-null value in that column. We can see most of our columns
    have values in every row because the non-null count matches the index count. In
    contrast, the columns starting with `in_reply_to_*` have non-null values in 10,302
    of the 28,034 rows. This is because these values are present only on posts that
    are responses. We’ll take advantage of this difference between original posts
    and replies later.
  prefs: []
  type: TYPE_NORMAL
- en: To the right of the value count is the type of data stored in the column. If
    you don’t explicitly define types for the data as you import it, pandas will do
    its best to logically interpret the types. Unfortunately, it’s really only good
    at finding integer and float types. For the rest of the columns, you can see it
    has assigned the generic type `object`. This is the pandas way of saying it really
    doesn’t know what to make of the data in the column. It may be of an unorderable
    type (like the strings stored in the `user_name` column) or there may be two or
    more data types in the same column (such as the column `in_reply_to_screen_name`,
    where some rows have an integer value and others have a null value). Before you
    begin any analysis, it’s important to understand the structure of the underlying
    data. You’ll become familiar with the different data types available and when
    to use each, but for now we don’t need to change anything, so we’ll move on to
    the last two rows of the output. The `dtypes` property just gives a summary of
    the data types in the column for convenience. We can see that one column was determined
    to be a floating-point number, three were determined to be integers, and the rest
    pandas left as generic `object` types.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the memory usage line estimates the amount of memory used to store
    the entire `DataFrame` object. You can use this value to get a rough idea of data
    storage requirements for your application, but there are some caveats here. Depending
    on your configuration, pandas can calculate this number in one of two ways. By
    default, pandas simply multiplies the bytes required to store a value of each
    column’s data type by the number of rows in the `DataFrame`. For example, an `int64`
    value takes up 8 bytes, so the `id` column takes up approximately 8 × 28,034 =
    224,272 bytes (a little over 224KB). By repeating this for each column and summing
    the results, pandas quickly approximates memory usage. The problem is that some
    data types (the `object` type, for instance) don’t have a maximum size, so pandas
    can only guess the minimum space assigned to these types. That’s why there’s a
    `+` symbol after the memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the Social Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the `post_df` object defined, you can analyze the data structure and choose
    which fields to use in your graph definitions. Let’s define a node ID *u* as a
    unique user account using the network. The `user_id` and `user_screen_name` have
    a 1:1 relationship, so either is a good candidate for the node ID. The `user_screen_name`
    field makes the graphs more aesthetically pleasing, but the `user_id` might be
    better for an automated system—for example, one that uses the network analysis
    results to look up user profile information by ID. We’ll be using the `user_screen_name`
    field so the graphs are more engaging and memorable; it beats staring at a bunch
    of randomly generated IDs.
  prefs: []
  type: TYPE_NORMAL
- en: For edges, we’ll look at when two users interacted over a post, shown in [Listing
    5-4](#listing5-4).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-4: Representing the post data as a directed graph'
  prefs: []
  type: TYPE_NORMAL
- en: The `in_reply_to_*` fields allow us to see when a post is in response to an
    earlier post (ostensibly from another user). When user B replies to a post from
    user A, we’ll consider this an edge between them, *e*[*(a→b)*]. I’ll discuss more
    about edges and interpreting them as we go along.
  prefs: []
  type: TYPE_NORMAL
- en: First we create a directed graph ❶ from the previously defined `DataFrame` object.
  prefs: []
  type: TYPE_NORMAL
- en: We loop over each index in the `post_df` object ❷ and use the `DataFrame.loc`
    function to retrieve each row individually ❸. We add edges to the graph whenever
    one user reblogs another user’s message ❹. The user who created the original post
    (the source node) is held in the `in_reply_to_account_id` field, and the user
    who’s responding (the terminal node) is held in the `user_screen_name` field.
    We then include the length of the text as a specially named version of edge weight
    called `capacity` ❺. This is a very simplistic measure of information contained
    in a post, as we’ll discuss more shortly. Finally, we can print out the length
    of the list of graph nodes to verify we’ve added 85 post objects to our graph.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-1](#figure5-1) shows a 3D representation of the graph generated from
    [Listing 5-4](#listing5-4).'
  prefs: []
  type: TYPE_NORMAL
- en: Each dot is a node representing a different user on the network, and each dashed
    line is an edge representing a post interaction between two users. Posts without
    replies don’t create edges in the graph, and so aren’t visualized here. Even though
    there’s a lot of data and the graph looks like a mess at first glance, there are
    some takeaways. For example, you can already see this is a highly connected network.
    Looking at the nodes around the periphery, you can see that most users have a
    lot of edges leading to various other users, which means at some point they interacted
    through a post. Also note that some of the nodes have a lot more edges than others.
    Just as we did with the computer network in the previous chapter, let’s begin
    to untangle this cloud of connectivity to see if we can make any interesting observations
    related to our research.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502567c05/f05001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-1: A 3D visualization of the social network graph'
  prefs: []
  type: TYPE_NORMAL
- en: Network Analysis Insights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With our graph built, we can turn our attention to our research questions, starting
    with how much information gets propagated within the network.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating Information Propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Calculating the amount of information something contains is an age-old problem
    with a lot of deep mathematical research behind it. Most truly useful methods
    deal with a concept called *information entropy* and dive into measuring the probability
    of some value (such as a phrase) existing by random occurrence. These measures
    are often complex to describe mathematically and would require a whole other discussion
    around linguistics and Markov chains. Instead, I’ve opted for the crude substitute
    of text length. Essentially, each post is treated as one unit of data and the
    post’s total information is exchanged with each interaction. We consider information
    propagated when a user replies to a post.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use a different method for the information exchange rate than the one
    we used in NetworkX (see “[Examining How Machines Interact on the Network](c04.xhtml#h2-502567c04-0005)”
    in [Chapter 4](c04.xhtml)). Instead, we’ll consider the *residual information
    (RI)* score, the difference between the amount of information added to a network
    and the amount consumed from it. For example, you could consider the residual
    information of your local library as the difference between all the books it has
    and all the books people in the area have read. It’s very likely that there are
    some esoteric volumes that sit idle, waiting for the day someone will need them.
    The same can be said of a social network like Mastodon. When a user creates a
    new post, they add *potential information* to the network—that is, information
    waiting to be discovered by other users. When a user replies to an existing post,
    potential information converts to *kinetic information* through information exchange:
    information transfers from one user to another through the act of reading and
    responding to the original message. In this case, information flows from the origin
    user to the terminal user via a directed edge *e*, so the edge set *p* can also
    be viewed as the set of kinetic exchanges.'
  prefs: []
  type: TYPE_NORMAL
- en: You can then reframe the question “How much information gets propagated?” as
    “How much potential information is required before some is likely to become kinetic?”
    or simply, “How many posts does it take before someone else is likely to read
    and respond?” One way we can answer this question is by calculating the ratio
    of original posts without replies (*o*) to original posts with replies (*p*);
    this gives us the RI score. For now, let’s say a whole post is one unit of information,
    so, for each edge, one unit of information is exchanged from *u* to *v*. This
    is a *balanced exchange*, where all (and only) the information in the post is
    passed. If the node receiving the information could receive only half of it at
    a time, it would be an *unbalanced exchange* because the sender can send more
    than the receiver can handle.
  prefs: []
  type: TYPE_NORMAL
- en: Using these definitions, you can look at the overall tendency for information
    to spread through the entire network by comparing the ratio of potential information
    to kinetic information exchanged. The formula *RI* = *|p|* / *|o|* describes the
    amount of potential information left in the network after all the exchanges have
    occurred. The result tells you approximately how much information must be added
    to the network before some of it is likely to be consumed by another user (by
    reading and replying). If every post on the network were responded to, you’d get
    an RI score of 0 / *n* = 0\. When there is zero residual information on the network,
    you need to add one piece of information for it to be consumed by another user.
    A network with no replies (all original messages) has an RI score of *n* / 0 =
    NaN, which indicates there is *only* residual information in the network, meaning
    no known amount of potential information will become kinetic. If there are twice
    as many original posts as there are response posts, the ratio is 2:1—for every
    two posts created, one post would get a reply. In another network with an RI score
    of 6 (a 6:1 ratio), only one in six posts get a reply, meaning the information
    flow is more resistant to propagating.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 5-5](#listing5-5) calculates the example network’s RI score using
    the `DataFrame` object `post_df`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-5: Applying the RI algorithm to the example data'
  prefs: []
  type: TYPE_NORMAL
- en: To measure the amount of potential and kinetic information in the network, we
    collect original posts (those that don’t have a value for `in_reply_to_id`) into
    `o_posts` ❶ and reblogs into `r_posts`. We separate original posts that didn’t
    receive any replies (*p*) into `o_no_r` ❸, and those that did (*o*) into `o_posts`,
    by gathering the IDs of posts with responses ❷ from the list of replies and creating
    a new list that excludes `replied_to` posts. The posts in `o_no_r` represent the
    potential information remaining in the network after the exchanges have all occurred.
    Finally, we take the ratio of the lengths of `o_no_r` and `o_posts` to get the
    RI score ❹. The result should be about `2.6358` for the sample data, indicating
    slightly fewer than three original posts are created for every one reply.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying Cliques and Most Influential Users
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A key aspect of network analysis is detecting smaller communities, or cliques,
    nested inside the larger network. Recall from [Chapter 3](c03.xhtml) that a clique
    is a group of nodes that are all directly connected to one another. In the case
    of our social network, this would represent a group of users who are all familiar
    with each other and have interacted previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s find some cliques, starting by cleaning up the data set and displaying
    the graph. Cliques are meaningful only for nodes with connections to other nodes,
    so first we need to clean up the data to include only those posts with replies.
    We can drop posts without replies from the `DataFrame` like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: All posts whose `in_reply_to_id` field is populated will be grouped in the `r_posts`
    object. We already discussed the second research question, “What cliques exist
    in this network?” from a theoretical perspective in [Chapter 3](c03.xhtml), so
    let’s apply that knowledge to understand the underlying structure of this network.
    A clique is a subset of nodes *u* wherein all the nodes of *u* are directly connected
    to one another, so if we assume that users read replies to their posts, we can
    defensibly loosen our directed graph to an undirected graph for the purposes of
    identifying these cliques. [Listing 5-6](#listing5-6) converts the graph and then
    finds the cliques as a list. We’ll continue our analysis using the directed graph
    in combination with the clique list from the undirected graph.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-6: Converting to an undirected graph to find cliques'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cliques in the network are interesting because they provide a picture of which
    users interact. Larger cliques usually represent users with some common association;
    they can reveal the formation of alliances and even predict fractures. Cliques
    by themselves may also be interesting: they tell you who knows whom, for example.
    However, it’s when you start to analyze the members of different cliques that
    you really gain insight. You might identify the leaders of the cliques to see
    who has influence or status over the rest of the network. That’s exactly what
    we’ll do: we’ll take what we’ve learned about the underlying cliques in the network
    to find which groups contain the most influential users in our Mastodon-like network.'
  prefs: []
  type: TYPE_NORMAL
- en: The out-degree of a node in this case indicates the number of times other users
    have replied to a post the original node authored. A node with a high out-degree
    could be viewed as “more popular” since those posts tend to trigger more responses.
    By identifying the nodes who are close to this popular node, we can zero in on
    the underlying influencer. [Listing 5-7](#listing5-7) finds the node with the
    highest out-degree in the directed graph, then finds the cliques containing this
    node.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-7: Finding all maximal cliques for the highest out-degree node'
  prefs: []
  type: TYPE_NORMAL
- en: First, we get the out-degree for all the nodes in the directed graph ❶. When
    analyzing relationships, you may sometimes want to quantify the strength of the
    connection between nodes along with the rest of the data. For example, if you
    know two users in the network are married, you may want to weight the edges between
    them higher than an edge between two people who are coworkers. In [Listing 5-3](#listing5-3),
    we captured the length of the text as a crude measure of the amount of data exchanged.
    We can use this information now to rate the quality of communications between
    users. To account for the quality of edges as well as the number, we replace the
    simple out-degree measure with a weighted out-degree measure like Dijkstra’s algorithm
    (as I mentioned in [Chapter 4](c04.xhtml), you do so by explicitly passing the
    `weight` parameter to the shortest path algorithm). After sorting the nodes in
    ascending order by out-degree count, we select the last item, the user who is
    the top source of posts that get responses, as the target node, and then use a
    list comprehension ❷ to extract the cliques that contain the target node.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-2](#figure5-2) shows the subgraph created by selecting the first
    of these cliques ❸.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502567c05/f05002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-2: The clique subgraph for the user most responded to'
  prefs: []
  type: TYPE_NORMAL
- en: The popular user, `dannyhoover`, has an outbound edge to each node in the graph.
    The users `michaelcruz` and `falvarez` reply to the largest number of other clique
    members. You can infer that `dannyhoover` is likely to be more influential (for
    these clique members) than either `michaelcruz` or `falvarez`. That isn’t to say
    that those two users aren’t influential in other contexts. Remember, when working
    with subgraphs, the information you derive is always with regard to the subgraph,
    not the graph as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: For the third research question, “Who are the three most influential users?”
    we just extend the code in [Listing 5-7](#listing5-7) to consider the top three
    source nodes. Influential users are those who add potential information that’s
    more likely to initiate a kinetic exchange. As an exercise, try to determine if
    the top three influential nodes are in the same clique. What can you possibly
    infer from the result?
  prefs: []
  type: TYPE_NORMAL
- en: Finding the Most Influenced Users
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next question posed explores the inverse relationship in the graph; that
    is, “Who are the three most influenced users?” is related to nodes with the highest
    in-degree. If you consider our definition of influence for this network, an influencer
    is someone who creates an original post that’s likely to get a response from one
    or more users. In contrast, a highly *influenced* user is one who responds to
    a lot of other users’ original posts. Luckily, the code is very similar to [Listing
    5-7](#listing5-7). Simply swapping `G.in_degree` for `G.out_degree` produces a
    graph similar to the one in [Figure 5-3](#figure5-3).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502567c05/f05003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-3: Finding the most influenced user'
  prefs: []
  type: TYPE_NORMAL
- en: The user `juliekennedy` is responsible for a large portion of the kinetic information
    exchange in the network, which means they reply to the most users. Given our assumption
    that the person responding to a post has been influenced somewhat (at least enough
    to create a response), we can conclude that the user `juliekennedy` has been influenced
    by the largest number of users. Of course, you’re free to (and probably should)
    debate the validity of this assumption. We’re dealing with an area of security
    where you must be prepared to defend the assumptions you build into your analysis.
    When analyzing something as complex as human interaction, keep in mind there are
    limits to the accuracy and validity of the claims we can make.
  prefs: []
  type: TYPE_NORMAL
- en: Using Topic-Based Information Exchange
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Going a little off-track on our research questions, we can answer the two previous
    questions of influence for more specific post topics using *topic-based information
    exchange*, in which we consider the most influential and influenced users within
    a certain context or topic. For instance, we might consider the most influential
    heart surgeon or the most influential hacker. By examining influence and popularity
    with a contextual example, we can gain more insight into the interactions we’ve
    recorded. Simply put, we can answer “What are these user interactions about?”
    We’ll find the most influential and influenced users for particular topics, such
    as environment and politics, but you can extend the same principle just as easily
    to search for users discussing current events or other topics of interest.
  prefs: []
  type: TYPE_NORMAL
- en: For topic-based information exchange, we use the *Hyperlink-Induced Topic Search*,
    or *HITS* (also known as *Hubs and Authorities*), an algorithm for analyzing the
    link relationships in a directed graph.^([1](b01.xhtml#c05-endnote-001)) Originally
    designed for internet search engines to score web pages on their relevance to
    a given topic, HITS has been adapted to many other types of link analysis. In
    terms of security and social network analysis, HITS can give useful context to
    the concept of generic influence measure, like information exchange ratio (IER).
    For example, security researchers used Twitter to track information related to
    a terrorist attack in Mumbai^([2](b01.xhtml#c05-endnote-002)) by examining topics
    related to the attack and determining which users seemed to have the most authoritative
    understanding of the events.
  prefs: []
  type: TYPE_NORMAL
- en: 'The intuition behind the original algorithm is fairly simple: certain sites,
    known as *hubs*, serve as large website directories. Pages are sorted by relevance
    to a queried topic. A good hub is one that points to many other pages across many
    subjects. If multiple hubs point to the same source page for a topic, that page
    is considered to be an authority on the subject. In other words, an authoritative
    node represents one that is linked to by many different hubs. The higher the hub
    scores, the more authoritative the node. The more authoritative nodes a hub connects
    to, the higher its hub score becomes. Modern search engines are excellent examples
    of hubs. These sites aren’t authoritative on any one topic they catalog, but they
    can lead users to other sites that *are* authoritative.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our network, a hub would be a user whose post on a subject is reblogged
    by a large number of authoritative users. On the other side of the information
    flow are the authority nodes, which equate to users who reblog the information
    from several quality information hubs. NetworkX relies on the SciPy library under
    the hood to convert the graph into a *sparse adjacency matrix* (a list where every
    possible connection in the graph is recorded as either present in the data or
    not). In turn, SciPy relies on NumPy to handle the matrix math. Unfortunately,
    this dependency chain can be fragile. Depending on how you install the packages,
    you might get an attribute error like `module ''scipy.sparse'' has no attribute
    ''coo_array''` when running the *Mastodon_network.ipynb* file. I was able to temporarily
    resolve this by installing NetworkX version 2.6.3 using the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The HITS algorithm is performed iteratively over a subset of relevant nodes,
    typically returned from some search algorithm. With each iteration, the algorithm
    recalculates the two real values representing the hub score and authority score
    for each node in the subset. Since the hub score for a good hub should increase
    with each new iteration, the score it lends to each authority will also increase,
    and vice versa. The final output is two scores for every node in the subset.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 5-8](#listing5-8) shows a method to find hubs and authorities relating
    to posts containing the word *environment*, using the `DataFrame` object from
    [Listing 5-2](#listing5-2) once again.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-8: Building a topic-based subgraph and running the HITS algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: We begin by casting the post text to lowercase ❶ (so we can perform case-insensitive
    matching), and then use the built-in pandas `contains` function for locating rows
    based on text content to retrieve all posts with the related root word ❷. This
    will also match environment*al*, environment*alist*, and so on. We use each row’s
    post ID to extract the set of responses to these posts of interest ❸. We loop
    over each of the reply rows and create a directed edge, which indicates the flow
    of influence for the related topic, in the resulting subgraph ❹.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we use the resulting subgraph to calculate the HITS hub and authority
    scores ❺. The `max_iter` parameter passed to the `networkx.hits` function (part
    of the NetworkX core library) controls the maximum value of iterations the algorithm
    will run in cases where the code doesn’t converge on a solution (see the NetworkX
    documentation for a description of how the HITS algorithm reaches convergence).
    The `tol` parameter controls the error tolerance to check for convergence. If
    the algorithm fails to converge on an answer within the tolerance and max iterations,
    a `PowerIterationFailedConvergence` exception will be raised.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm starts from the assumption that all nodes have a hub score and
    authority score of 1\. At each subsequent step, it computes two update rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Update authority scores**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update each node’s authority score to be equal to the sum of the hub scores
    of each node that points to it. That is, a node is given a higher authority score
    by reblogging messages of users recognized as information hubs. This is represented
    by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502567c05/m05001.png)'
  prefs: []
  type: TYPE_IMG
- en: where *n* is the number of incoming references to *u*, and *v* is the node at
    the opposite end of the *i*th edge.
  prefs: []
  type: TYPE_NORMAL
- en: '**Update hub scores**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update each node’s hub score to be equal to the sum of the authority scores
    of each node that it points to. In our example, a node is given a high hub score
    by writing posts that are reblogged by nodes considered to be authorities on the
    subject. This is represented by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502567c05/m05002.png)'
  prefs: []
  type: TYPE_IMG
- en: where *n* is the number of outgoing references from *u*, and *v* is the node
    at the opposite end of the *i*th edge.
  prefs: []
  type: TYPE_NORMAL
- en: You can now reframe the second and third research questions in terms of a given
    subject. For instance, “Who are the top three hubs for the topic of environment?”
    and “Who are the top three authorities for the topic of politics?” The topic-based
    subgraphs in [Figure 5-4](#figure5-4) show the results from our sample data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502567c05/f05004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-4: Topic subgraph examples for environment and politics'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than labeling the nodes, I used the `nx.spring_layout` function to visually
    graph the influence structure for the two topics. According to the documentation,
  prefs: []
  type: TYPE_NORMAL
- en: The [spring layout] algorithm simulates a force-directed representation of the
    network treating edges as springs holding nodes close, while treating nodes as
    repelling objects, sometimes called an anti-gravity force. Simulation continues
    until the positions are close to an equilibrium.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This has the effect of pushing highly connected nodes more toward the center
    depending on the relative connectivity of the rest of the nodes. Nodes near the
    center have exerted influence on more users, so the other nodes have pushed farther
    away. You can see that the graph for politics on the right of [Figure 5-4](#figure5-4)
    has a larger number of small clustered influences near the edges of the graph,
    with only a few nodes showing more influence than the others. The environment
    graph on the left, however, shows a distinctly influential user near the center
    and then a few smaller clusters of local influence around the edges. When using
    the `spring_layout` function, keep in mind that the initial positions are randomized
    so the resulting graph is stochastic (random). Rerunning the code will likely
    result in a different visual layout, but the most influential nodes will always
    have pushed the other nodes farther away than less influential nodes.
  prefs: []
  type: TYPE_NORMAL
- en: After running the HITS algorithm, you should find that the top three hubs for
    environment (in descending order of hub score) are `williamclarke`, `victoria73`,
    and `nromero`. The top three authorities for politics (also in descending order
    of authority score) are `wernerbrianna`, `trivera`, and `susanjohnson`. Remember
    that the scores produced by the HITS algorithm are relevant only to the topic
    subset. A node with a high authority score for “pet food” wouldn’t score the same
    on “programming.”
  prefs: []
  type: TYPE_NORMAL
- en: At the start of the chapter I mentioned how social network connections and influence
    could be used to predict the spread of malicious content; this is your first real
    method for doing so. A lot of malware is spread through social network message
    attachments. Once you identify a malicious message on your network (and extract
    some useful topic information), you can leverage the HITS algorithm to predict
    which users are more likely to respond to the message. By doing so, you can deal
    with the risks in descending order of importance. A real-world example of this
    occurred as I was revising this chapter. During the height of the COVID-19 pandemic
    fear, attackers used an infected version of a tracking map to trick concerned
    users into visiting a malicious website. Once this story broke ([https://krebsonsecurity.com/2020/03/live-coronavirus-map-used-to-spread-malware](https://krebsonsecurity.com/2020/03/live-coronavirus-map-used-to-spread-malware)),
    security teams used the HITS algorithm to track which, if any, of their users
    might have been impacted.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing Network Organization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final question we want to answer—“Who could introduce the most new connections?”—is
    a bit more complex but important nonetheless. Researchers and analysts use this
    type of information when analyzing the organization of networks from street gangs
    to military battalions—anywhere individuals may not directly interact but share
    some common oversight “higher up the ladder.” For example, a soldier in unit A
    may send information about enemy troop movement to the unit commander, who in
    turn forwards the information to the base commander. The base commander is in
    communication with several different unit commanders at any given time and may
    send the message to another unit commander in unit B, who then moves to intercept
    the enemy. In the US, this chain of command is an implementation of a node ancestry
    that can be traced from the office of the president (as commander in chief) all
    the way to each individual soldier in boot camp. By examining which nodes can
    facilitate connections between large numbers of currently disconnected nodes,
    you can begin to understand each person’s importance in the hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-5](#figure5-5) shows a tree structure for an example that’s probably
    more familiar to you, a company organization chart.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502567c05/f05005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-5: An example tree from an organization chart'
  prefs: []
  type: TYPE_NORMAL
- en: The root of this tree is the CEO at the top, below whom are three managers who
    all report directly to him. Below each manager are the subordinates that make
    up their team. Understanding the influence within social structures is vital to
    planning (or circumventing) security controls intended for interaction with humans,
    such as social engineering; social engineers use the concept intuitively to gain
    legitimacy with other employees. Simply put, if you can convince an influential
    person to introduce you, you can bypass most resistance. Of course, you wouldn’t
    want to directly call the CEO of a large company if the branch manager is capable
    of making the introduction you need. The first common node between yourself and
    the person you’d like to be introduced to is the *lowest common ancestor (LCA)*.
  prefs: []
  type: TYPE_NORMAL
- en: To determine the LCA, we first need to define *node ancestry* as it relates
    to trees (rather than genealogy). In graph theory, a tree is a special type of
    graph structure in which any two nodes are connected by exactly one path ([https://mathworld.wolfram.com/Tree.html](https://mathworld.wolfram.com/Tree.html)).
    The node at the start of the tree is the root node; offspring nodes are called
    branch nodes unless an offspring branch has no branch nodes of its own (a dead
    end), in which case it’s called a leaf node.
  prefs: []
  type: TYPE_NORMAL
- en: 'By definition, a simple graph has no directionality and no cycles. A *polytree*
    extends the concept of a simple graph to include directionality, making a *directed
    acyclic graph (DAG)*. This seemingly simple change imparts a lot of interesting
    properties. For example, a DAG has a *topological ordering*: the nodes are ordered
    so the root node has a lower value than the leaf nodes. DAGs are one of the most
    studied of all graph structures because they appear so frequently in nature. From
    the literal branching of trees and plants, the veins in your body, and rivers
    to the structure of most computer programs, DAGs can represent a huge number of
    natural and artificial systems. In our case, using a DAG to represent the relationship
    between nodes will allow us to encode a hierarchy of membership in the social
    network.'
  prefs: []
  type: TYPE_NORMAL
- en: Node ancestry for arbitrary polytrees is similar in concept and structure to
    that of a family tree. However, the order relies on the topological sorting of
    DAGs, rather than being strictly chronological. The most influential users are
    those nodes with some amount of out-degree and no in-degree (users, like `dannyhoover`,
    who have more influence than other users) and these form the roots for distinct
    trees. Each node they influence becomes a branch in the tree. For each branch
    node, the out-degree edges are again added as branches. Branching continues until
    all nodes are placed. This leads to leaf nodes with an out-degree of zero and
    some amount of in-degree (the users most influenced by other members of the network).
    Ordering the nodes in this manner gives you an idea of the flow of influence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally speaking, an ancestor of node *u* is any other node *v* such that
    a directed path exists in the graph from *v* to *u* or, written more algebraically:'
  prefs: []
  type: TYPE_NORMAL
- en: Ancestry ( u ) = ( u ← v ) ∈ E
  prefs: []
  type: TYPE_NORMAL
- en: 'The *common ancestors* for two nodes (*u*,*v*) are any nodes *x* that have
    a directed path to both *u* and *v* in the set of edges. This can be written as
    the intersection of these two subsets of edges:'
  prefs: []
  type: TYPE_NORMAL
- en: CommAnc( u ∧ v ) = ( x → u ∈ E ) ∪ ( x → v ∈ E )
  prefs: []
  type: TYPE_NORMAL
- en: The LCA of two nodes (*u*,*v*) is the common ancestor with the shortest path
    distance to both nodes, which is also the ancestor with the maximum path length
    from the root of the graph. For example, you and your cousin share some of the
    same great-grandparents. However, you also share some of the same grandparents.
    While both your great-grandparents and your grandparents are your ancestors, since
    your grandparents are closer to your generation than your great-grandparents are,
    they would be your LCA. [Figure 5-6](#figure5-6) shows two examples of ancestry
    on the same tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502567c05/f05006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-6: A general ancestry illustration'
  prefs: []
  type: TYPE_NORMAL
- en: In each tree the shaded node with the dashed outline is the LCA of the other
    two shaded nodes. On the left, the LCA for the nodes *D* and *E* is the root of
    the tree, *A*. On the right, although node *A* is still a common ancestor, node
    *B* is farther from the root and therefore the LCA. Thinking about this in terms
    of information security, the LCA of two nodes is the closest potential pivot point
    between them. If a user at node *G* wanted to be introduced to the user at node
    *E*, they could ask the user at node *B* to introduce them. In [Listing 5-9](#listing5-9)
    we tally the occurrence of each node as the LCA of other node pairs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-9: Counting LCA occurrences for all nodes'
  prefs: []
  type: TYPE_NORMAL
- en: First, we generate the list of ancestors using the NetworkX function `nx.all_pairs_lowest_common_ancestor`
    ❶, which returns a dictionary where the key is a pair of nodes from the graph,
    and the value is the LCA node for that pair of nodes. With the `ancestors` list
    populated, we then use a `for` loop to assign the pair of nodes to the variable
    `p` and the resulting ancestor to the `lca` variable, in order to count how many
    connections `lca` can bridge. We ignore pairs of nodes with an edge between them,
    since one of the nodes is the direct ancestor of the other ❷. For example, the
    pair of nodes *B* and *E* from [Figure 5-6](#figure5-6) can be ignored, even though
    the NetworkX function produces the LCA of the pair. For each pair of nodes without
    a direct edge between them, we check if their `lca` is in the `pred_count` dictionary,
    which counts the number of times a node is the LCA for two other nodes. If the
    LCA node is already in the dictionary, increment the count by 1 ❸. Otherwise,
    create a new entry with a value of `1` ❹. Running this code will print the top
    five users along with the number of connections they can potentially bridge, as
    shown in [Listing 5-10](#listing5-10).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-10: The results of the LCA analysis'
  prefs: []
  type: TYPE_NORMAL
- en: The root user, `dannyhoover`, is tied for first and can potentially bridge 444
    new connections within the network. Since we already think this user is very influential,
    that may come as no surprise. Their position at the root of the tree also means
    they’re the last possible LCA for all pairs of nodes, if no other ancestor can
    be found, so this result may not be as interesting as the second and third place.
    The fact that the user `georgejohnson` got the same exact score as `dannyhoover`
    is interesting and may point to two structures in the data worth investigating.
  prefs: []
  type: TYPE_NORMAL
- en: The user `judith20` can bridge 336 connections. As an exercise, examine how
    this user fits into the structure of the tree. Who influences their activity (inbound
    edges) and who do they influence (outbound edges)? What measures of centrality
    do they score most highly on?
  prefs: []
  type: TYPE_NORMAL
- en: 'The Proof of Concept: Social Network Analysis'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The proof-of-concept code for this chapter, found in the *social_network/post_graph.py*
    file in the book’s resources, allows you to capture the post data from your personal
    timeline into JSON data you can analyze using the methods shown here.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need to register for an account on whichever Mastodon instance you choose
    (I’m using defcon.social). You’ll then need to register an application for your
    own set of API credentials ([https://docs.joinmastodon.org/client/token](https://docs.joinmastodon.org/client/token)).
    Registering an application gets you an API token and API token secret that identifies
    a specific application under your account and grants access to authorized functions
    (such as liking posts and following users). Depending on the Mastodon instance
    you choose, you may be required to answer some questions to qualify for different
    use cases; otherwise, you simply need to define the scope of the access token
    as you create it. A lot of Mastodon instances are friendly to researchers, as
    long as you plan to protect the privacy of individuals’ data.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll be given a unique API key that identifies your API account to the Mastodon
    instance, paired with an API secret that should be protected like other cryptographic
    keys.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve registered, you can use the API via the Python Mastodon library
    to scrape your own timelines. Refer to the Mastodon library documentation ([https://mastodonpy.readthedocs.io/en/stable](https://mastodonpy.readthedocs.io/en/stable))
    and the Mastodon API documentation ([https://docs.joinmastodon.org/api](https://docs.joinmastodon.org/api))
    to get a sense of the data that’s available and how to access it using this library.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 5-11](#listing5-11) shows the proof-of-concept code.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-11: Capturing Mastodon public timeline data to a CSV file'
  prefs: []
  type: TYPE_NORMAL
- en: First, we import the `mastodon` library ❶. Once we have the API credentials,
    we modify the template file with the access token and base URL ❷ and run it from
    our terminal. The code uses these credentials to create an authenticated API object
    ❸, used to retrieve the timeline data ❹, which is conveniently delivered as a
    dictionary suitable for JSON encoding. We loop over these results and write them
    into the output CSV file ❺.
  prefs: []
  type: TYPE_NORMAL
- en: Now you can use code similar to Listings 5-2 and 5-4 to read the data back into
    a pandas `DataFrame`, then mold it into significant features and finally a relevant
    directed (or undirected) graph using NetworkX. You can also bypass writing to
    an intermediate file by combining this code with a data processing pipeline to
    analyze status information in near real time. We’ll discuss processing pipelines
    in [Part III](p03.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: The Darker Side of Social Network Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hopefully now you have an idea of how quickly and easily someone can build a
    map of someone else’s social life. The important thing to remember is, like maps,
    social network graphs require interpretation. When we interpret social network
    information, we’re invariably viewing the data through our own social biases.
    The core of the issue is that we’re trying to reduce a highly complex, multifaceted
    problem, such as the motivation behind people’s interactions, into a tightly controlled
    and well-defined mathematical model. To do so, we have to apply heuristics we
    choose based on our own social experience. For example, I mentioned earlier that
    you might want to weight interactions between married couples higher than those
    between coworkers. This shows one of my heuristic biases, which comes from my
    experiences, education, and understanding but doesn’t necessarily reflect the
    reality of everyone’s situation. You’ll need to make many such assumptions when
    building a SNA model, and it’s important to understand when, where, and how much
    you’re allowing your own biases to impact the analysis. This is one of the primary
    reasons I recommend doing SNA with a team. Peer review, especially from peers
    of diverse backgrounds, is one of the best counters to the problems inherited
    from a single-perspective interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: The other reason I recommend caution is that SNA raises moral and ethical questions.
    It is perhaps the dark arts of applied mathematics in security, primarily because
    there can be very real and dangerous consequences when it is abused. SNA has been
    used by tyrannical governments to attack dissidents, threaten whistleblowers,
    and manipulate the people of a society. Unfortunately, not all ethically questionable
    uses of SNA are as easy to spot. There are tools and sites designed to make it
    even easier to collect someone’s publicly available (and sometimes private) information.
    We live in a world that constantly struggles to balance privacy and openness.
    The small-world experiment can be used to link movies to Kevin Bacon or to link
    each one of us to any number of criminal figures and organizations. As an analyst,
    you’re responsible for understanding what’s ethically and morally appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although this chapter demonstrates the concept of social network analysis using
    Mastodon as an example, none of these concepts are inherently tied to the Mastodon
    platform. The US government and university researchers have been working on different
    technologies to leverage the information obtained from analyzing the reply network
    structure of discussions in dark web forums to understand the extent to which
    dark web information can be useful for predicting real-world cyberattacks.^([3](b01.xhtml#c05-endnote-003))
    In his paper “Tracking, Destabilizing and Disrupting Dark Networks with Social
    Networks Analysis,” written for the US Navy, Sean Everton covers SNA as a means
    to develop strategies for tracking and disrupting criminal and terrorist networks.^([4](b01.xhtml#c05-endnote-004))
    The paper serves as both a tactical and strategic introduction, so I highly recommend
    reading it.
  prefs: []
  type: TYPE_NORMAL
- en: As you extend your own SNA to work in the wild, you’ll want to reference the
    API documentation for whatever social network you’re using. If no API exists (or
    the platform starts charging exorbitant rates), you may have to resort to old-fashioned
    web-scraping techniques to gather the data you need. Such tasks are outside the
    scope of this book, but there are many excellent materials for doing so.
  prefs: []
  type: TYPE_NORMAL
- en: So far, everything we’ve used graph analysis for has been focused on the past.
    You can think of this as *descriptive* security analysis because it aims to classify
    things as they are now (or as they were when the data was captured). *Preventative*
    security analysis, however, seeks to analyze what might occur in the future so
    that hopefully we can step in and prevent a security incident from occurring in
    the first place. To achieve this, we’ll use one of my favorite simulation algorithms,
    Monte Carlo simulations—the subject of the next chapter.
  prefs: []
  type: TYPE_NORMAL
