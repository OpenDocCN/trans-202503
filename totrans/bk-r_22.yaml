- en: '**18**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**HYPOTHESIS TESTING**'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, you’ll build on your experience with confidence intervals and
    sampling distributions to make more formal statements about the value of a true,
    unknown parameter of interest. For this, you’ll learn about frequentist *hypothesis
    testing*, where a probability from a relevant sampling distribution is used as
    evidence against some claim about the true value. When a probability is used in
    this way, it is referred to as a *p*-value. In this chapter, I talk about interpreting
    results for relatively basic statistics, but you can apply the same concepts to
    statistics arising from more complicated methods (such as regression modeling
    in [Chapter 19](ch19.xhtml#ch19)).
  prefs: []
  type: TYPE_NORMAL
- en: '**18.1 Components of a Hypothesis Test**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To give you an example of hypothesis testing, suppose I told you that 7 percent
    of a certain population was allergic to peanuts. You then randomly selected 20
    individuals from that population and found that 18 of them were allergic to peanuts.
    Assuming your sample was unbiased and truly reflective of the population, what
    would you then think about my claim that the true proportion of allergic individuals
    is 7 percent?
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, you would doubt the correctness of my claim. In other words, there
    is such a small probability of observing 18 or more successes out of 20 trials
    for a set success rate of 0.07 that you can state that you have statistical evidence
    against the claim that the true rate is 0.07\. Indeed, when defining *X* as the
    number of allergic individuals out of 20 by assuming *X* ∼ BIN(20,0.07), evaluating
    Pr(*X* ≥ 18) gives you the precise *p*-value, which is tiny.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This *p*-value represents the probability of observing the results in your sample,
    *X* = 18, or a more extreme outcome (*X* = 19 or *X* = 20), if the chance of success
    was truly 7 percent.
  prefs: []
  type: TYPE_NORMAL
- en: Before looking at specific hypothesis tests and their implementation in R, this
    section will introduce terminology that you’ll come across often in the reporting
    of such tests.
  prefs: []
  type: TYPE_NORMAL
- en: '***18.1.1 Hypotheses***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As the name would suggest, in hypothesis testing, formally stating a claim and
    the subsequent hypothesis test is done with a *null* and an *alternative* hypothesis.
    The null hypothesis is interpreted as the *baseline* or *no-change* hypothesis
    and is the claim that is assumed to be true. The alternative hypothesis is the
    conjecture that you’re testing for, against the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, null and alternative hypotheses are denoted H[0] and H[A], respectively,
    and they are written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[0] : . . .'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[A] : . . .'
  prefs: []
  type: TYPE_NORMAL
- en: The null hypothesis is often (but not always) defined as an equality, =, to
    a null value. Conversely, the alternative hypothesis (the situation you’re testing
    for) is often defined in terms of an inequality to the null value.
  prefs: []
  type: TYPE_NORMAL
- en: • When H[A] is defined in terms of a less-than statement, with <, it is *one-sided*;
    this is also called a *lower-tailed test*.
  prefs: []
  type: TYPE_NORMAL
- en: • When H[A] is defined in terms of a greater-than statement, with >, it is *one-sided*;
    this is also called an *upper-tailed test*.
  prefs: []
  type: TYPE_NORMAL
- en: • When H[A] is merely defined in terms of a different-to statement, with ≠,
    it is *two-sided*; this is also called a *two-tailed test*.
  prefs: []
  type: TYPE_NORMAL
- en: These test variants are entirely situation specific and depend upon the problem
    at hand.
  prefs: []
  type: TYPE_NORMAL
- en: '***18.1.2 Test Statistic***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once the hypotheses are formed, sample data are collected, and statistics are
    calculated according to the parameters detailed in the hypotheses. The *test statistic*
    is the statistic that’s compared to the appropriate standardized sampling distribution
    to yield the *p*-value.
  prefs: []
  type: TYPE_NORMAL
- en: A test statistic is typically a standardized or rescaled version of the sample
    statistic of interest. The distribution and extremity (that is, distance from
    zero) of the test statistic are the sole drivers of the smallness of the *p*-value
    (which indicates the strength of the evidence against the null hypothesis—see
    [Section 18.1.3](ch18.xhtml#ch18lev2sec155)). Specifically, the test statistic
    is determined by both the difference between the original sample statistic and
    the null value and the standard error of the sample statistic.
  prefs: []
  type: TYPE_NORMAL
- en: '***18.1.3 p-value***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *p*-value is the probability value that’s used to quantify the amount of
    evidence, if any, against the null hypothesis. More formally, the *p*-value is
    found to be the probability of observing the test statistic, or something more
    extreme, assuming the null hypothesis is true.
  prefs: []
  type: TYPE_NORMAL
- en: 'The exact nature of calculating a *p*-value is dictated by the type of statistics
    being tested and the nature of H[A]. In reference to this, you’ll see the following
    terms:'
  prefs: []
  type: TYPE_NORMAL
- en: • A lower-tailed test implies the *p*-value is a left-hand tail probability
    from the sampling distribution of interest.
  prefs: []
  type: TYPE_NORMAL
- en: • For an upper-tailed test, the *p*-value is a right-hand tail probability.
  prefs: []
  type: TYPE_NORMAL
- en: • For a two-sided test, the *p*-value is the sum of a left-hand tail probability
    and right-hand tail probability. When the sampling distribution is symmetric (for
    example, normal or *t*, as in all examples coming up in [Sections 18.2](ch18.xhtml#ch18lev1sec55)
    and [18.3](ch18.xhtml#ch18lev1sec56)), this is equivalent to two times the area
    in one of those tails.
  prefs: []
  type: TYPE_NORMAL
- en: Put simply, the more extreme the test statistic, the smaller the *p*-value.
    The smaller the *p*-value, the greater the amount of statistical evidence against
    the assumed truth of H[0].
  prefs: []
  type: TYPE_NORMAL
- en: '***18.1.4 Significance Level***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For every hypothesis test, a *significance level*, denoted *α*, is assumed.
    This is used to qualify the result of the test. The significance level defines
    a cutoff point, at which you decide whether there is sufficient evidence to view
    H[0] as incorrect and favor H[A] instead.
  prefs: []
  type: TYPE_NORMAL
- en: • If the *p*-value is greater than or equal to *α*, then you conclude there
    is insufficient evidence against the null hypothesis, and therefore you *retain*
    H[0] when compared to H[A].
  prefs: []
  type: TYPE_NORMAL
- en: • If the *p*-value is less than *α*, then the result of the test is *statistically
    significant*. This implies there is sufficient evidence against the null hypothesis,
    and therefore you *reject* H[0] in favor of H[A].
  prefs: []
  type: TYPE_NORMAL
- en: Common or conventional values of *α* are *α* = 0.1, *α* = 0.05, and *α* = 0.01.
  prefs: []
  type: TYPE_NORMAL
- en: '***18.1.5 Criticisms of Hypothesis Testing***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The terminology just presented becomes easier to understand once you look at
    some examples in the upcoming sections. However, even at this early stage, it’s
    important to recognize that hypothesis testing is susceptible to justifiable criticism.
    The end result of any hypothesis test is to either retain or reject the null hypothesis,
    a decision that is solely dependent upon the rather arbitrary choice of significance
    level *α*; this is most often simply set at one of the conventionally used values.
  prefs: []
  type: TYPE_NORMAL
- en: Before you begin looking at examples, it is also important to note that a *p*-value
    never provides “proof” of either H[0] or H[A] being truly correct. It can only
    ever quantify evidence against the null hypothesis, which one rejects given a
    sufficiently small *p*-value < *α*. In other words, rejecting a null hypothesis
    is not the same as disproving it. Rejecting H[0] merely implies that the sample
    data suggest H[A] ought to be preferred, and the *p*-value merely indicates the
    strength of this preference.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, there has been a push against emphasizing these aspects of
    statistical inference in some introductory statistics courses owing at least in
    part to the overuse, and even misuse, of *p*-values in some areas of applied research.
    A particularly good article by Sterne and Smith ([2001](ref.xhtml#ref63)) discusses
    the role of, and problems surrounding, hypothesis testing from the point of view
    of medical research. Another good reference is Reinhart ([2015](ref.xhtml#ref53)),
    which discusses common misinterpretations of *p*-values in statistics.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, probabilistic inference with respect to sampling distributions
    is, and will always remain, a cornerstone of frequentist statistical practice.
    The best way to improve the use and interpretation of statistical tests and modeling
    is with a sound introduction to the relevant ideas and methods so that, from the
    outset, you understand statistical significance and what it can and cannot tell
    you.
  prefs: []
  type: TYPE_NORMAL
- en: '**18.2 Testing Means**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The validity of hypothesis tests involving sample means is dependent upon the
    same assumptions and conditions mentioned in [Section 17.1.1](ch17.xhtml#ch17lev2sec146).
    In particular, throughout this section you should assume that the central limit
    theorem holds, and if the sample sizes are small (in other words, roughly less
    than 30), the raw data are normally distributed. You’ll also focus on examples
    where the sample standard deviation *s* is used to estimate the true standard
    deviation, *σ*[*X*], because this is the most common situation you’ll encounter
    in practice. Again, mirroring [Section 17.1.1](ch17.xhtml#ch17lev2sec146), this
    means you need to use the *t*-distribution instead of the normal distribution
    when calculating the critical values and *p*-values.
  prefs: []
  type: TYPE_NORMAL
- en: '***18.2.1 Single Mean***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As you’ve already met the standard error formula, ![image](../images/f0379-01.jpg),
    and the R functionality needed to obtain quantiles and probabilities from the
    *t*-distribution (`qt` and `pt`), the only new concepts to introduce here are
    related to the definition of the hypotheses themselves and the interpretation
    of the result.
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculation: One-Sample t-Test**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s dive straight into an example—a one-sample *t*-test. Recall the problem
    in [Section 16.2.2](ch16.xhtml#ch16lev2sec142) where a manufacturer of a snack
    was interested in the mean net weight of contents in an advertised 80-gram pack.
    Say that a consumer calls in with a complaint—over time they have bought and precisely
    weighed the contents of 44 randomly selected 80-gram packs from different stores
    and recorded the weights as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The customer claims that they’ve been shortchanged because their data cannot
    have arisen from a distribution with mean *μ* = 80, so the true mean weight must
    be less than 80\. To investigate this claim, the manufacturer conducts a hypothesis
    test using a significance level of *α* = 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the hypotheses must be defined, with a null value of 80 grams. Remember,
    the alternative hypothesis is “what you’re testing for”; in this case, H[A] is
    that *μ* is smaller than 80\. The null hypothesis, interpreted as “no change,”
    will be defined as *μ* = 80: that the true mean is in fact 80 grams. These hypotheses
    are formalized like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e18-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Second, the mean and standard deviation must be estimated from the sample.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The question your hypotheses seek to answer is this: given the estimated standard
    deviation, what’s the probability of observing a sample mean (when *n* = 44) of
    78.91 grams or less if the true mean is 80 grams? To answer this, you need to
    calculate the relevant test statistic.'
  prefs: []
  type: TYPE_NORMAL
- en: Formally, the test statistic *T* in a hypothesis test for a single mean with
    respect to a null value of *μ*[0] is given as
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e18-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: based on a sample of size *n*, a sample mean of *x̄*, and a sample standard
    deviation of *s* (the denominator is the estimated standard error of the mean).
    Assuming the relevant conditions have been met, *T* follows a *t*-distribution
    with *ν* = *n* − 1 degrees of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: 'In R, the following provides you with the standard error of the sample mean
    for the snacks data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, *T* can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the test statistic is used to obtain the *p*-value. Recall that the
    *p*-value is the probability that you observe *T* or something more extreme. The
    nature of “more extreme” is determined by the alternative hypothesis H[A], which,
    as a less-than statement, directs you to find a left-hand, lower-tail probability
    as the *p*-value. In other words, the *p*-value is provided as the area under
    the sampling distribution (a *t*-distribution with 43 df in the current example)
    to the left of a vertical line at *T*. From [Section 16.2.3](ch16.xhtml#ch16lev2sec143),
    this is easily done, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Your result states that if the H[0] were true, there would be only a little
    more than a 1 percent chance that you’d observe the customer’s sample mean of
    *x̄* = 78.91, or less, as a random phenomenon. Since this *p*-value is smaller
    than the predefined significance level of *α* = 0.05, the manufacturer concludes
    that there is sufficient evidence to reject the null hypothesis in favor of the
    alternative, suggesting the true value of *μ* is in fact less than 80 grams.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if you find the corresponding 95 percent CI for the single sample
    mean, as described in [Section 17.2.1](ch17.xhtml#ch17lev2sec149) and given by
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: it does *not* include the null value of 80, mirroring the result of the hypothesis
    test at the 0.05 level.
  prefs: []
  type: TYPE_NORMAL
- en: '**R Function: t.test**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The result of the one-sample *t*-test can also be found with the built-in `t.test`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The function takes the raw data vector as `x`, the null value for the mean
    as `mu`, and the direction of the test (in other words, how to find the *p*-value
    under the appropriate *t*-curve) as `alternative`. The `alternative` argument
    has three available options: `"less"` for H[A] with <; `"greater"` for H[A] with
    >; and `"two.sided"` for H[A] with ≠. The default value of *α* is 0.05\. If you
    want a different significance level than 0.05, this must be provided to `t.test`
    as 1 − *α*, passed to the argument `conf.level`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the value of *T* is reported in the output of `t.test`, as are the
    degrees of freedom and the *p*-value. You also get a 95 percent “interval,” but
    its values of `-Inf` and `79.68517` do not match the interval calculated just
    a moment ago. The manually calculated interval is in fact a two-sided interval—a
    bounded interval formed by using an error component that’s equal on both sides.
  prefs: []
  type: TYPE_NORMAL
- en: The CI in the `t.test` output, on the other hand, takes instruction from the
    `alternative` argument. It provides a *one-sided confidence bound*. For a lower-tailed
    test, it provides an upper bound on the statistic such that the entire lower-tail
    area of the sampling distribution of interest is 0.95, as opposed to a *central*
    area as the traditional two-sided interval does. One-sided bounds are less frequently
    used than the fully bounded two-sided interval, which can be obtained (as the
    component `conf.int`) from a relevant call to `t.test` by setting `alternative="two.sided"`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This result matches your manually computed version from earlier. Note also that
    the corresponding confidence level, 1 − *α*, is stored alongside this component
    as an attribute (refer to [Section 6.2.1](ch06.xhtml#ch06lev2sec59)).
  prefs: []
  type: TYPE_NORMAL
- en: In examining the result for the snack example, with a *p*-value of around 0.011,
    remember to be careful when interpreting hypothesis tests. With *α* set at 0.05
    for this particular test, H[0] is rejected. But what if the test were carried
    out with *α* = 0.01? The *p*-value is greater than 0.01, so in that case, H[0]
    would be retained, for no reason other than the arbitrary movement of the value
    of *α*. In these situations, it’s helpful to comment on the perceived strength
    of the evidence against the null hypothesis. For the current example, you could
    reasonably state that there exists some evidence to support H[A] but that this
    evidence is not especially strong.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 18.1**'
  prefs: []
  type: TYPE_NORMAL
- en: Adult domestic cats of a certain breed are said to have an average weight of
    3.5 kilograms. A feline enthusiast disagrees and collects a sample of 73 weights
    of cats of this breed. From her sample, she calculates a mean of 3.97 kilograms
    and a standard deviation of 2.21 kilograms. Perform a hypothesis test to test
    her claim that the true mean weight *μ* is *not* 3.5 kilograms by setting up the
    appropriate hypothesis, carrying out the analysis, and interpreting the *p*-value
    (assume the significance level is *α* = 0.05).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose it was previously believed that the mean magnitude of seismic events
    off the coast of Fiji is 4.3 on the Richter scale. Use the data in the `mag` variable
    of the ready-to-use `quakes` data set, providing 1,000 sampled seismic events
    in that area, to test the claim that the true mean magnitude is in fact *greater*
    than 4.3\. Set up appropriate hypotheses, use `t.test` (conduct the test at a
    significance level of *α* = 0.01), and draw a conclusion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Manually compute a two-sided confidence interval for the true mean of (b).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '***18.2.2 Two Means***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Often, testing a single sample mean isn’t enough to answer the question you’re
    interested in. In many settings, a researcher wants to directly compare the means
    of two distinct groups of measurements, which boils down to a hypothesis test
    for the true difference between two means; call them **μ[1]** and **μ[2]**.
  prefs: []
  type: TYPE_NORMAL
- en: The way in which two groups of data relate to each other affects the specific
    form of standard error for the difference between two sample means and therefore
    the test statistic itself. The actual comparison of the two means, however, is
    often of the same nature—the typical null hypothesis is usually defined as **μ[1]**
    and **μ[2]** being equal. In other words, the null value of the difference between
    the two means is often zero.
  prefs: []
  type: TYPE_NORMAL
- en: '**Unpaired/Independent Samples: Unpooled Variances**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The most general case is where the two sets of measurements are based on two
    independent, separate groups (also referred to as *unpaired* samples). You compute
    the sample means and sample standard deviations of both data sets, define the
    hypotheses of interest, and then calculate the test statistic.
  prefs: []
  type: TYPE_NORMAL
- en: When you cannot assume the variances of the two populations are equal, then
    you perform the *unpooled* version of the two-sample *t*-test; this will be discussed
    first. If, however, you can safely assume equal variances, then you can perform
    a *pooled* two-sample *t*-test, which improves the precision of the results. You’ll
    look at the pooled version of the test in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: For an unpooled example, return to the 80-gram snack packet example from [Section
    18.2.1](ch18.xhtml#ch18lev2sec158). After collecting a sample of 44 packs from
    the original manufacturer (label this sample size *n*[1]), the disgruntled consumer
    goes out and collects *n[2]* = 31 randomly selected 80-gram packs from a rival
    snack manufacturer. This second set of measurements is stored as `snacks2`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: From [Section 18.2.1](ch18.xhtml#ch18lev2sec158), you already know the mean
    and standard deviation of the first sample of size *n[1]* = 44—these are stored
    as `snack.mean` (around 78.91) and `snack.sd` (around 3.06), respectively—think
    of these as *x̄[1]* and *s*[1]. Compute the same quantities, *x̄[2]* and *s*[2],
    respectively, for the new data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let the true mean of the original sample be denoted with **μ[1]** and the true
    mean of the new sample from the rival company packs be denoted with **μ[2]**.
    You’re now interested in testing whether there is statistical evidence to support
    the claim that **μ[2]** is greater than **μ[1]**. This suggests the hypotheses
    of H[0] : **μ[1]** = **μ[2]** and H[A] : **μ[1]** < **μ[2]**, which can be written
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[0] : **μ[2]** − **μ[1]** = 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[A] : **μ[2]** − **μ[1]** > 0'
  prefs: []
  type: TYPE_NORMAL
- en: That is, the difference between the true mean of the rival company packs and
    the original manufacturer’s packs, when the original is subtracted from the rival,
    is bigger than zero. The “no-change” scenario, the null hypothesis, is that the
    two means are the same, so their difference is truly zero.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve constructed the hypotheses, let’s look at how to actually test
    them. The difference between two means is the quantity of interest. For two independent
    samples arising from populations with true means **μ[1]** and **μ[2]**, sample
    means *x̄[1]* and *x̄*[2], and sample standard deviations *s[1]* and *s*[2], respectively
    (and that meet the relevant conditions for the validity of the *t*-distribution),
    the standardized test statistic *T* for testing the difference between **μ[2]**
    and **μ[1]**, in that order, is given as
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e18-3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: whose distribution is approximated by a *t*-distribution with *ν* degrees of
    freedom, where
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e18-4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In (18.3), *μ*[0] is the null value of interest—typically zero in tests concerned
    with “difference” statistics. This term would therefore disappear from the numerator
    of the test statistic. The denominator of *T* is the standard error of the difference
    between two means in this setting.
  prefs: []
  type: TYPE_NORMAL
- en: The └ · ┘ on the right of [Equation (18.4)](ch18.xhtml#ch18eq4) denotes a *floor*
    operation—rounding strictly down to the nearest integer.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*This two-sample* t*-test, conducted using [Equation (18.3)](ch18.xhtml#ch18eq3),
    is also called* Welch’s *t-*test*. This refers to use of [Equation (18.4)](ch18.xhtml#ch18eq4),
    called the* Welch-Satterthwaite equation*. Crucially, it assumes that the two
    samples have different true variances, which is why it’s called the* unpooled
    variance *version of the test.*'
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to be consistent when defining your two sets of parameters and
    when constructing the hypotheses. In this example, since the test aims to find
    evidence for **μ[2]** being greater than **μ[1]**, a difference of **μ[2]**−**μ[1]**
    > 0 forms H[A] (a greater-than, upper-tailed test), and this order of subtraction
    is mirrored when calculating *T*. The same test could be carried out if you defined
    the difference the other way around. In that case, your alternative hypothesis
    would suggest a lower-tailed test because if you’re testing for **μ[2]** being
    bigger than **μ[1]**, H[A] would correctly be written as **μ[1]** − **μ[2]** <
    0\. Again, this would modify the order of subtraction in the numerator of [Equation
    (18.3)](ch18.xhtml#ch18eq3) accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same care must apply to the use of `t.test` for two-sample comparisons.
    The two samples must be supplied as the arguments `x` and `y`, but the function
    interprets `x` as greater than `y` when doing an upper-tailed test and interprets
    `x` as less than `y` when doing a lower-tailed test. Therefore, when performing
    the test with `alternative="greater"` for the snack pack example, it’s `snacks2`
    that must be supplied to `x`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: With a small *p*-value of 0.008706, you’d conclude that there is sufficient
    evidence to reject H[0] in favor of H[A] (indeed, the *p*-value is certainly smaller
    than the stipulated *α* = 0.1 significance level as implied by `conf.level=0.9`).
    The evidence suggests that the mean net weight of snacks from the rival manufacturer’s
    80-gram packs is greater than the mean net weight for the original manufacturer.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the output from `t.test` has reported a df value of 60.091, which
    is the unfloored result of (18.4). You also receive a one-sided confidence bound
    (based on the aforementioned confidence level), triggered by the one-sided nature
    of this test. Again, the more common two-sided 90 percent interval is also useful;
    knowing that *ν* = └ 60.091 ┘ = 60 and using the statistic and the standard error
    of interest (numerator and denominator of [Equation (18.3)](ch18.xhtml#ch18eq3),
    respectively), you can calculate it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, you’ve used the previously stored sample statistics `snack.mean`, `snack.sd`
    (the mean and standard deviation of the 44 raw measurements from the original
    manufacturer’s sample), `snack2.mean`, and `snack2.sd` (the same quantities for
    the 31 observations corresponding to the rival manufacturer). Note that the CI
    takes the same form as detailed by [Equation (17.2)](ch17.xhtml#ch17eq2) on [page
    378](ch17.xhtml#page_378) and that to provide the correct 1 − *α* central area,
    the `q`-function for the appropriate *t*-distribution requires 1 − *α*/2 as its
    supplied probability value. You can interpret this as being “90 percent confident
    that the true difference in mean net weight between the rival and the original
    manufacturer (in that order) is somewhere between 0.395 and 2.098 grams.” The
    fact that zero isn’t included in the interval, and that the interval is wholly
    positive, supports the conclusion from the hypothesis test.
  prefs: []
  type: TYPE_NORMAL
- en: '**Unpaired/Independent Samples: Pooled Variance**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the unpooled variance example just passed, there was no assumption that the
    variances of the two populations whose means were being compared were equal. This
    is an important note to make because it leads to the use of (18.3) for the test
    statistic calculation and (18.4) for the associated degrees of freedom in the
    corresponding *t*-distribution. However, if you *can* assume equivalence of variances,
    the precision of the test is improved—you use a different formula for the standard
    error of the difference and for calculating the associated df.
  prefs: []
  type: TYPE_NORMAL
- en: Again, the quantity of interest is the difference between two means, written
    as **μ[2]** − **μ[1]**. Assume you have two independent samples of sizes *n[1]*
    and *n[2]* arising from populations with true means **μ[1]** and **μ[2]**, sample
    means *x̄[1]* and *x̄*[2], and sample standard deviations *s[1]* and *s*[2], respectively,
    and assume that the relevant conditions for the validity of the *t*-distribution
    have been met. Additionally, assume that the true variances of the samples, ![image](../images/f0396-01.jpg)
    and ![image](../images/f0396-02.jpg), are equal such that ![image](../images/f0396-03.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*There is a simple rule of thumb to check the validity of the “equal variance”
    assumption. If the ratio of the larger sample standard deviation to the smaller
    sample standard deviation is less than* 2*, you can assume equal variances. For
    example, if s*[1] > *s*[2]*, then if* ![image](../images/f0396-07.jpg) < 2, *you
    can use the pooled variance test statistic that follows.*'
  prefs: []
  type: TYPE_NORMAL
- en: The standardized test statistic *T* for this scenario is given as
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e18-5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: whose distribution is a *t*-distribution with *ν* = *n[1]* + *n[2]* − 2 degrees
    of freedom, where
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e18-6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: is the *pooled estimate of the variance* of all the raw measurements. This is
    substituted in place of *s[1]* and *s[2]* in the denominator of [Equation (18.3)](ch18.xhtml#ch18eq3),
    resulting in [Equation (18.5)](ch18.xhtml#ch18eq5).
  prefs: []
  type: TYPE_NORMAL
- en: All other aspects of the two-sample *t*-test remain as earlier, including the
    construction of appropriate hypotheses, the typical null value of *μ*[0], and
    the calculation and interpretation of the *p*-value.
  prefs: []
  type: TYPE_NORMAL
- en: For the comparison of the two means in the snack pack example, you’d find it
    difficult to justify using the pooled version of the *t*-test. Applying the rule
    of thumb, the two estimated standard deviations (*s*[1] ≊ 3.06 and *s[2]* ≊ 1.21
    for the original and rival manufacturer’s samples, respectively) have a large-to-small
    ratio that is greater than 2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Though this is rather informal, if the assumption cannot reasonably be made,
    it’s best to stick with the unpooled version of the test.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this, let’s consider a new example. The intelligence quotient
    (IQ) is a quantity commonly used to measure how clever a person is. IQ scores
    are reasonably assumed to be normally distributed, and the average IQ of the population
    is said to be 100\. Say that you’re interested in assessing whether there is a
    difference in mean IQ scores between men and women, suggesting the following hypotheses
    where you have *n[men]* = 12 and *n*[women] = 20:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[0] : *μ[men]* − *μ*women = 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[A] : *μ[men]* − *μ*women ≠ 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'You randomly sample the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As usual, let’s calculate the basic statistics required.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'These give the sample averages *x̄[men]* and *x̄*[women], as well as their
    respective sample standard deviations *s[men]* and *s*[women]. Enter the following
    to quickly check the ratio of the standard deviations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the ratio of the larger sample standard deviation to the smaller
    is less than 2, so you could assume equal variances in carrying out the hypothesis
    test.
  prefs: []
  type: TYPE_NORMAL
- en: The `t.test` command also enables the pooled two-sample *t*-test as per [Equations
    (18.5)](ch18.xhtml#ch18eq5) and [(18.6)](ch18.xhtml#ch18eq6). To execute it, you
    provide the optional argument `var.equal=TRUE` (as opposed to the default `var.equal=FALSE`,
    which triggers Welch’s *t*-test).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note also that H[A] for this example implies a two-tailed test, hence the provision
    of `alternative="two.sided"`.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting *p*-value of this test, 0.3559, is certainly larger than the conventional
    cutoff level of 0.05\. Thus, your conclusion here is that there is no evidence
    to reject H[0]—there is insufficient evidence to support a true difference in
    the mean IQ scores of men compared to women.
  prefs: []
  type: TYPE_NORMAL
- en: '**Paired/Dependent Samples**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Finally, we’ll look comparing two means in *paired* data. This setting is distinctly
    different from that of both unpaired *t*-tests because it concerns the way the
    data have been collected. The issue concerns *dependence* between pairs of observations
    across the two groups of interest—previously, the measurements in each group have
    been defined as independent. This notion has important consequences for how the
    test can be carried out.
  prefs: []
  type: TYPE_NORMAL
- en: Paired data occur if the measurements forming the two sets of observations are
    recorded on the same individual or if they are related in some other important
    or obvious way. A classic example of this is “before” and “after” observations,
    such as two measurements made on each person before and after some kind of intervention
    treatment. These situations still focus on the difference between the mean outcomes
    in each group, but rather than working with the two data sets separately, a paired
    *t*-test works with a single mean—the true mean of the individual paired differences
    *μ[d]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, consider a company interested in the efficacy of a drug designed
    to reduce resting heart rates in beats per minute (bpm). The resting heart rates
    of 16 individuals are measured. The individuals are then administered a course
    of the treatment, and their resting heart rates are again measured. The data are
    provided in the two vectors `rate.before` and `rate.after` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: It quickly becomes clear why any test comparing these two groups must take dependence
    into account. Heart rate is affected by an individual’s age, build, and level
    of physical fitness. An unfit individual older than 60 is likely to have a higher
    baseline resting heart rate than a fit 20-year-old, and if both are given the
    same drug to lower their heart rate, their final heart rates are still likely
    to reflect the baselines. Any true effect of the drug therefore has the potential
    to be hidden if you approached the analysis using either of the unpaired *t*-tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome this problem, the paired two-sample *t*-test considers the difference
    between each pair of values. Labeling one set of *n* measurements as *x*[1], ...,
    *x*[*n*] and the other set of *n* observations as *y*[1], ..., *y*[*n*], the difference,
    *d*, is defined as *d*[*i*] = *y*[*i*] − *x*[*i*]; *i* = 1, ..., *n*. In R, you
    can easily compute the pairwise differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code calculates the sample mean ![image](../images/d.jpg) and
    standard deviation *s[d]* of these differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You want to see how much the heart rate is reduced by, so the test at hand
    will be concerned with the following hypotheses:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[0] : *μ[d]* = 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[A] : *μ[d]* < 0'
  prefs: []
  type: TYPE_NORMAL
- en: Given the order or subtraction used to obtain the differences, detection of
    a successful reduction in heart rate will be represented by an “after” mean that
    is smaller than the “before” mean.
  prefs: []
  type: TYPE_NORMAL
- en: Expressing all this mathematically, the value of interest is the true mean difference,
    *μ[d]*, between two means of dependent pairs of measurements. There are two sets
    of *n* measurements, *x*[1], ..., *x*[*n*] and *y*[1], ..., *y*[*n*], with pairwise
    differences *d*[1], ..., *d*[*n*]. The relevant conditions for the validity of
    the *t*-distribution must be met; in this case, if the number of pairs *n* is
    less than 30, then you must be able to assume the raw data are normally distributed.
    The test statistic *T* is given as
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e18-7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ![image](../images/d.jpg) is the mean of the pairwise differences, *s[d]*
    is the sample standard deviation of the pairwise differences, and *μ*[0] is the
    null value (usually zero). The statistic *T* follows a *t*-distribution with *n*
    − 1 df.
  prefs: []
  type: TYPE_NORMAL
- en: The form of [Equation (18.7)](ch18.xhtml#ch18eq7) is actually the same as the
    form of the test statistic in (18.2), once the sample statistics for the individual
    paired differences have been calculated. Furthermore, it’s important to note that
    *n* represents the total number of *pairs*, not the total number of individual
    observations.
  prefs: []
  type: TYPE_NORMAL
- en: For the current example hypotheses, you can find the test statistic and *p*-value
    with `rate.dbar` and `rate.sd`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: These results suggest evidence to reject H[0]. In `t.test`, the optional logical
    argument `paired` must be set to `TRUE`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note that the order you supply your data vectors to the `x` and `y` arguments
    follows the same rules as for the unpaired tests, given the desired value of `alternative`.
    The same *p*-value as was calculated manually is confirmed through the use of
    `t.test`, and since this is less than an assumed conventional significance level
    of *α* = 0.05, a valid conclusion would be to state that there is statistical
    evidence that the medication does reduce the mean resting heart rate. You could
    go on to say you’re 95 percent confident that the true mean difference in heart
    rate after taking the course of medication lies somewhere between
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*On some occasions, such as when your data strongly indicate non-normality,
    you may not be comfortable assuming the validity of the CLT (refer back to [Section
    17.1.1](ch17.xhtml#ch17lev2sec146)). An alternative approach to the tests discussed
    here is to employ a* nonparametric *technique that relaxes these distributional
    requirements. In the two-sample case, you could employ the* Mann-Whitney U test
    *(also known as the* Wilcoxon rank-sum test*). This is a hypothesis test that
    compares two medians, as opposed to two means. You can use the R function* `wilcox.test`
    *to access this methodology; its help page provides useful commentary and references
    on the particulars of the technique.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 18.2**'
  prefs: []
  type: TYPE_NORMAL
- en: In the package `MASS` you’ll find the data set `anorexia`, which contains data
    on pre- and post-treatment weights (in pounds) of 72 young women suffering from
    the disease, obtained from Hand et al. ([1994](ref.xhtml#ref28)). One group of
    women is the control group (in other words, no intervention), and the other two
    groups are the cognitive behavioral program and family support intervention program
    groups. Load the library and ensure you can access the data frame and understand
    its contents. Let *μ[d]* denote the mean difference in weight, computed as (*post-weight*
    − *pre-weight*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of which treatment group the participants fall into, conduct and
    conclude an appropriate hypothesis test with *α* = 0.05 for the entire set of
    weights for the following hypotheses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H[0] : *μ[d]* = 0'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'H[A] : *μ[d]* > 0'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, conduct three separate hypothesis tests using the same defined hypotheses,
    based on which treatment group the participants fall into. What do you notice?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another ready-to-use data set in R is `PlantGrowth` ([Dobson, 1983](ref.xhtml#ref19)),
    which records a continuous measure of the yields of a certain plant, looking at
    the potential effect of two supplements administered during growth to increase
    the yield when compared to a control group with no supplement.
  prefs: []
  type: TYPE_NORMAL
- en: Set up hypotheses to test whether the mean yield for the control group is less
    than the mean yield from a plant given either of the treatments. Determine whether
    this test should proceed using a pooled estimate of the variance or whether Welch’s
    *t*-test would be more appropriate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conduct the test and make a conclusion (assuming normality of the raw observations).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As discussed, there is a rule of thumb for deciding whether to use a pooled
    estimate of the variance in an unpaired *t*-test.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your task is to write a *wrapper* function that calls `t.test` after deciding
    whether it should be executed with `var.equal=FALSE` according to the rule of
    thumb. Use the following guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '– Your function should take four defined arguments: `x` and `y` with no defaults,
    to be treated in the same way as the same arguments in `t.test`; and `var.equal`
    and `paired`, with defaults that are the same as the defaults of `t.test`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – An ellipsis ([Section 9.2.5](ch09.xhtml#ch09lev2sec86)) should be included
    to represent any additional arguments to be passed to `t.test`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – Upon execution, the function should determine whether `paired=FALSE`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '* If `paired` is `TRUE`, then there is no need to proceed with the check of
    a pooled variance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '* If `paired` is `FALSE`, then the function should determine the value for
    `var.equal` automatically by using the rule of thumb.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – If the value of `var.equal` was set automatically, you can assume it will
    override any value of this argument initially supplied by the user.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – Then, call `t.test` appropriately.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Try your new function on all three examples in the text of [Section 18.2.2](ch18.xhtml#ch18lev2sec159),
    ensuring you reach identical results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**18.3 Testing Proportions**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A focus on means is especially common in statistical modeling and hypothesis
    testing, and therefore you must also consider sample proportions, interpreted
    as the mean of a series of *n* binary trials, in which the results are success
    (1) or failure (0). This section focuses on the parametric tests of proportions,
    which assume normality of the target sampling distributions (otherwise referred
    to as *Z*-tests).
  prefs: []
  type: TYPE_NORMAL
- en: The general rules regarding the setup and interpretation of hypothesis tests
    for sample proportions remain the same as for sample means. In this introduction
    to *Z*-tests, you can consider these as tests regarding the true value of a single
    proportion or the difference between two proportions.
  prefs: []
  type: TYPE_NORMAL
- en: '***18.3.1 Single Proportion***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Section 17.1.2](ch17.xhtml#ch17lev2sec147) introduced the sampling distribution
    of a single sample proportion to be normally distributed, with a mean centered
    on the true proportion *π* and with a standard error of ![image](../images/f0402-01.jpg).
    Provided the trials are independent and that *n* isn’t “too small” and *π* isn’t
    “too close” to 0 or 1, those formulas are applicable here.'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*A rule of thumb to check the latter condition on n and* *π* *simply involves
    checking that n* ![image](../images/p.jpg) *and n*(1 − ![image](../images/p.jpg))
    *are both greater than 5, where* ![image](../images/p.jpg) *is the sample estimate
    of* *π.*'
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that the standard error in the case of hypothesis tests involving
    proportions is itself dependent upon *π*. This is important—remember that any
    hypothesis test assumes satisfaction of H[0] in the relevant calculations. In
    dealing with proportions, that means when computing the test statistic, the standard
    error must make use of the null value *π*[0] rather than the estimated sample
    proportion ![image](../images/p.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ll clarify this in an example. Suppose an individual fond of a particular
    fast-food chain notices that he tends to have an upset stomach within a certain
    amount of time after having his usual lunch. He comes across the website of a
    blogger who believes that the chance of getting an upset stomach shortly after
    eating that particular food is 20 percent. The individual is curious to determine
    whether his true rate of stomach upset *π* is any different from the blogger’s
    quoted value and, over time, visits these fast-food outlets for lunch on *n* =
    29 separate occasions, recording the success (`TRUE`) or failure (`FALSE`) of
    experiencing an upset stomach. This suggests the following pair of hypotheses:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[0] : *π* = 0.2'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[A] : *π* ≠ 0.2'
  prefs: []
  type: TYPE_NORMAL
- en: These may be tested according to the general rules discussed in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculation: One-Sample Z-Test**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In testing for the true value of some proportion of success, *π*, let ![image](../images/p.jpg)
    be the sample proportion over *n* trials, and let the null value be denoted with
    *π*[0]. You find the test statistic with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e18-8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Provided the aforementioned conditions on the size of *n* and the value of *π*
    can be assumed, *Z* ∼ N(0,1).
  prefs: []
  type: TYPE_NORMAL
- en: The denominator of [Equation (18.8)](ch18.xhtml#ch18eq8), the standard error
    of the proportion, is calculated with respect to the null value *π*[0], not ![image](../images/p.jpg).
    As mentioned just a moment ago, this is to satisfy the assumption of “truth” of
    H[0] as the test is carried out, so it allows interpretation of the resulting
    *p*-value as usual. The standard normal distribution is used to find the *p*-value
    with respect to *Z*; the direction underneath this curve is governed by the nature
    of H[A] just as before.
  prefs: []
  type: TYPE_NORMAL
- en: Getting back to the fast-food example, suppose these are the observed data,
    where `1` is recorded for an upset stomach and is `0` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of successes and probability of success in this sample are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'A quick check indicates that as per the rule of thumb, the test is reasonable
    to carry out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Following [Equation (18.8)](ch18.xhtml#ch18eq8), the test statistic *Z* for
    this example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The alternative hypothesis is two-sided, so you compute the corresponding *p*-value
    as a two-tailed area under the standard normal curve. With a positive test statistic,
    this can be evaluated by doubling the upper-tailed area from *Z*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Assume a conventional *α* level of 0.05\. The high *p*-value given as 0.307
    suggests the results in the sample of size 29 are not unusual enough, under the
    assumption that the null hypothesis is true, to reject H[0]. There is insufficient
    evidence to suggest that the proportion of instances of an upset stomach that
    this individual experiences is any different from 0.2 as noted by the blogger.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can support this conclusion with a confidence interval. At the level of
    95 percent, you calculate the CI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This interval easily includes the null value of 0.2.
  prefs: []
  type: TYPE_NORMAL
- en: '**R Function: prop.test**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Once more, R rescues you from tedious step-by-step calculation. The ready-to-use
    `prop.test` function allows you to perform, among other things, a single sample
    proportion test. The function actually performs the test in a slightly different
    way, using the chi-squared distribution (which will be explored more in [Section
    18.4](ch18.xhtml#ch18lev1sec57)). However, the test is equivalent, and the resulting
    *p*-value from `prop.test` is identical to the one reached using the *Z-*based
    test.
  prefs: []
  type: TYPE_NORMAL
- en: To the `prop.test` function, as used for a single sample test of a proportion,
    you provide the number of successes observed as `x`, the total number of trials
    as `n`, and the null value as `p`. The two further arguments, `alternative` (defining
    the nature of H[A]) and `conf.level` (defining 1 − *α*), are identical to the
    same arguments in `t.test` and have defaults of `"two.sided"` and `0.95`, respectively.
    Lastly, it is recommended to explicitly set the optional argument `correct=FALSE`
    if your data satisfy the *n* ![image](../images/p.jpg) and *n*(1 − ![image](../images/p.jpg))
    rule of thumb.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the current example, you perform the test with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The *p*-value is the same as you got earlier. Note, however, that the reported
    CI is not quite the same (the normal-based interval, dependent upon the CLT).
    The CI produced by `prop.test` is referred to as the *Wilson score interval*,
    which takes into account the direct association that a “probability of success”
    has with the binomial distribution. For simplicity, you’ll continue to work with
    normal-based intervals when performing hypothesis tests involving proportions
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Note also that, just like `t.test`, any one-sided test performed with `prop.test`
    will provide only a single-limit confidence bound; you’ll see this in the following
    example.
  prefs: []
  type: TYPE_NORMAL
- en: '***18.3.2 Two Proportions***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With a basic extension to the previous procedure, by way of a modification to
    the standard error, you can compare *two* estimated proportions from independent
    populations. As with the difference between two means, you’re often testing whether
    the two proportions are the same and thus have a difference of zero. Therefore,
    the typical null value is zero.
  prefs: []
  type: TYPE_NORMAL
- en: For an example, consider a group of students taking a statistics exam. In this
    group are *n[1]* = 233 students majoring in psychology, of whom *x[1]* = 180 pass,
    and *n[2]* = 197 students majoring in geography, of whom 175 pass. Suppose it
    is claimed that the geography students have a higher pass rate in statistics than
    the psychology students.
  prefs: []
  type: TYPE_NORMAL
- en: 'Representing the true pass rates for psychology students as *π*[1] and geography
    students as *π*[2], this claim can be statistically tested using a pair of hypotheses
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[0] : *π*[2] − *π*[1] = 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[A] : *π*[2] − *π*[1] > 0'
  prefs: []
  type: TYPE_NORMAL
- en: Just as with a comparison between two means, it’s important to keep the order
    of differencing consistent throughout the test calculations. This example shows
    an upper-tailed test.
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculation: Two-Sample Z-Test**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In testing for the true difference between two proportions mathematically,
    *π*[1] and *π*[2], let ![image](../images/p.jpg)[1] = *x*[1]/*n*[1] be the sample
    proportion for *x[1]* successes in *n[1]* trials corresponding to *π*[1], and
    the same quantities as ![image](../images/p.jpg)[2] = *x*[2]/*n*[2] for *π*[2].
    With a null value of the difference denoted *π*[0], the test statistic is given
    by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e18-9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Provided you can assume to apply the aforementioned conditions for a proportion
    with respect to *n*[1], *n[2]* and *π*[1], *π*[2], you can treat *Z* ∼ N(0,1).
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a new quantity, *p*^∗, present in the denominator of (18.9). This
    is a *pooled* proportion, given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e18-10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As noted, in this kind of test it is common for the null value, the true difference
    in proportions, to be set to zero (in other words, *π*[0] = 0).
  prefs: []
  type: TYPE_NORMAL
- en: The denominator of [Equation (18.9)](ch18.xhtml#ch18eq9) is itself the standard
    error of the difference between two proportions as used in a hypothesis test.
    The need to use *p*^∗ lies once more in the fact that H[0] is assumed to be true.
    Using ![image](../images/p1.jpg) and ![image](../images/p2.jpg) separately in
    the denominator of (18.9), in the form of ![image](../images/f0406-01.jpg) (the
    standard error of the difference between two proportions outside the confines
    of a hypothesis test), would violate the assumed “truth” of H[0].
  prefs: []
  type: TYPE_NORMAL
- en: 'So, returning to the statistics exam taken by the psychology and geography
    students, you can evaluate the required quantities as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The results indicate sample pass rates of around 77.2 percent for the psychology
    students and 88.8 percent for the geography students; this is a difference of
    roughly 11.6 percent. From examining the values of ![image](../images/p1.jpg),
    *n[1]* and ![image](../images/p2.jpg), *n*[2], you can see that the rule of thumb
    is satisfied for this test; again, assume a standard significance level of *α*
    = 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pooled proportion *p*^∗, following (18.10), is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'With that you calculate the test statistic *Z* as per [Equation (18.9)](ch18.xhtml#ch18eq9)
    with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In light of the hypotheses, you find the corresponding *p*-value as a right-hand,
    upper-tail area from *Z* underneath the standard normal curve as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: You observe a *p*-value that’s substantially smaller than *α*, so the formal
    decision is of course to reject the null hypothesis in favor of the alternative.
    The sample data provide sufficient evidence against H[0] such that you can conclude
    that evidence exists to support the pass rate for geography students being higher
    than the pass rate for psychology students.
  prefs: []
  type: TYPE_NORMAL
- en: '**R Function: prop.test**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Once more, R allows you to perform the test with one line of code using `prop.test`.
    For comparisons of two proportions, you pass the number of successes in each group
    as a vector of length 2 to `x` and the respective sample sizes as another vector
    of length 2 to `n`. Note that the order of the entries must reflect the order
    of `alternative` if this is one-sided (in other words, here, the proportion that
    is to be tested as “greater” corresponds to the first elements of `x` and `n`).
    Once more, `correct` is set to `FALSE`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The *p*-value is identical to the one generated by the previous series of calculations,
    suggesting a rejection of H[0]. Since `prop.test` was called as a one-sided test,
    the confidence interval returned provides a single bound. To provide a two-sided
    CI for the true difference, it makes sense, considering the outcome of the test,
    to construct this using the separate ![image](../images/p1.jpg) and ![image](../images/p2.jpg)
    instead of using the denominator of (18.9) specifically (which assumes truth of
    H[0]). The “separate-estimate” version of the standard error of the difference
    between two proportions was given earlier (in the text beneath [Equation (18.10)](ch18.xhtml#ch18eq10)),
    and a 95 percent CI is therefore calculated with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: With that, you’re 95 percent confident that the true difference between the
    proportion of geography students passing the exam and the proportion of psychology
    students passing the exam lies somewhere between 0.046 and 0.185\. Naturally,
    the interval also reflects the result of the hypothesis test—it doesn’t include
    the null value of zero and is wholly positive.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 18.3**'
  prefs: []
  type: TYPE_NORMAL
- en: An advertisement for a skin cream claims nine out of ten women who use it would
    recommend it to a friend. A skeptical salesperson in a department store believes
    the true proportion of women users who’d recommend it, *π*, is much smaller than
    0.9\. She follows up with 89 random customers who had purchased the skin cream
    and asks if they would recommend it to others, to which 71 answer yes.
  prefs: []
  type: TYPE_NORMAL
- en: Set up an appropriate pair of hypotheses for this test and determine whether
    it will be valid to carry out using the normal distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the test statistic and the *p*-value and state your conclusion for the
    test using a significance level of *α* = 0.1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using your estimated sample proportion, construct a two-sided 90 percent confidence
    interval for the true proportion of women who would recommend the skin cream.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The political leaders of a particular country are curious as to the proportion
    of citizens in two of its states that support the decriminalization of marijuana.
    A small pilot survey taken by officials reveals that 97 out of 445 randomly sampled
    voting-age citizens residing in state 1 support the decriminalization and that
    90 out of 419 voting-age citizens residing in state 2 support the same notion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Letting *π*[1] denote the true proportion of citizens in support of decriminalization
    in state 1, and *π*[2] the same measure in state 2, conduct and conclude a hypothesis
    test under a significance level of *α* = 0.05 with reference to the following
    hypotheses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H[0] : *π*[2] − *π*[1] = 0'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'H[A] : *π*[2] − *π*[1] ≠ 0'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute and interpret a corresponding CI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Though there is standard, ready-to-use R functionality for the *t*-test, at
    the time of this writing, there is no similar function for the *Z*-test (in other
    words, the normal-based test of proportions described here) except in contributed
    packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your task is to write a relatively simple R function, `Z.test`, that can perform
    a one- or two-sample *Z*-test, using the following guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '– The function should take the following arguments: `p1` and `n1` (no default)
    to pose as the estimated proportion and sample size; `p2` and `n2` (both defaulting
    to `NULL`) that contain the second sample proportion and sample size in the event
    of a two-sample test; `p0` (no default) as the null value; and `alternative` (default
    `"two.sided"`) and `conf.level` (default `0.95`), to be used in the same way as
    in `t.test`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – When conducting a two-sample test, it should be `p1` that is tested as being
    smaller or larger than `p2` when `alternative="less"` or `alternative="greater"`,
    the same as in the use of `x` and `y` in `t.test`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – The function should perform a one-sample *Z*-test using `p1`, `n1`, and `p0`
    if either `p2` or `n2` (or both) is `NULL`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – The function should contain a check for the rule of thumb to ensure the validity
    of the normal distribution in both one- and two-sample settings. If this is violated,
    the function should still complete but should issue an appropriate warning message
    (see [Section 12.1.1](ch12.xhtml#ch12lev2sec106)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – All that need be returned is a list containing the members `Z` (test statistic),
    `P` (appropriate *p*-value—this can be determined by `alternative`; for a two-sided
    test, determining whether `Z` is positive or not can help), and `CI` (two-sided
    CI with respect to `conf.level`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Replicate the two examples in the text of [Sections 18.3.1](ch18.xhtml#ch18lev2sec160)
    and [18.3.2](ch18.xhtml#ch18lev2sec161) using `Z.test`; ensure you reach identical
    results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call `Z.test(p1=0.11,n1=10,p0=0.1)` to try your warning message in the one-sample
    setting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**18.4 Testing Categorical Variables**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The normal-based *Z*-test is particular to data that are binary in nature. To
    statistically test claims regarding more general categorical variables, with more
    than two distinct levels, you use the ubiquitous *chi-squared test*. Pronounced
    *kai*, “chi” refers to the Greek symbol χ and is sometimes noted in shorthand
    as the χ² *test*.
  prefs: []
  type: TYPE_NORMAL
- en: There are two common variants of the chi-squared test. The first—a chi-squared
    test of distribution, also called a *goodness of fit (GOF)* test—is used when
    assessing the frequencies in the levels of a single categorical variable. The
    second—a chi-squared test of *independence*—is employed when you’re investigating
    the relationship between frequencies in the levels of two such variables.
  prefs: []
  type: TYPE_NORMAL
- en: '***18.4.1 Single Categorical Variable***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Like the *Z*-test, the one-dimensional chi-squared test is also concerned with
    comparing proportions but in a setting where there are more than two proportions.
    A chi-squared test is used when you have *k* levels (or categories) of a categorical
    variable and want to hypothesize about their relative frequencies to find out
    what proportion of *n* observations fall into each defined category. In the following
    examples, it must be assumed that the categories are *mutually exclusive* (in
    other words, an observation cannot take more than one of the possible categories)
    and *exhaustive* (in other words, the *k* categories cover all possible outcomes).
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ll illustrate how hypotheses are constructed and introduce the relevant ideas
    and methods with the following example. Suppose a researcher in sociology is interested
    in the dispersion of rates of facial hair in men of his local city and whether
    they are uniformly represented in the male population. He defines a categorical
    variable with three levels: clean shaven (1), beard only *or* moustache only (2),
    and beard *and* moustache (3). He collects data on 53 randomly selected men and
    finds the following outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the research question asks whether the proportions in each category are
    equally represented. Let *π*[1], *π*[2], and *π*[3] represent the true proportion
    of men in the city who fall into groups 1, 2, and 3, respectively. You therefore
    seek to test these hypotheses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0411-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For this test, use a standard significance level of 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: The appearance of the alternative hypothesis is a little different from what
    you’ve seen so far but is an accurate reflection of the interpretation of a chi-squared
    goodness of fit test. In these types of problems, H[0] is always that the proportions
    in each group are equal to the stated values, and H[A] is that the data, as a
    whole, do not match the proportions defined in the null. The test is conducted
    assuming the null hypothesis is true, and evidence against the no-change, baseline
    setting will be represented as a small *p*-value.
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculation: Chi-Squared Test of Distribution**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The quantities of interest are the proportion of *n* observations in each of
    *k* categories, *π*[1], ..., *π*[k], for a single mutually exclusive and exhaustive
    categorical variable. The null hypothesis defines hypothesized null values for
    each proportion; label these respectively as *π*[0][(][1][)], ..., *π*[0][(][k][)].
    The test statistic χ² is given as
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e18-11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *O[i]* is the *observed* count and *E[i]* is the *expected* count in the
    *i*th category; *i* = 1, ..., *k*. The *O[i]* are obtained directly from the raw
    data, and the expected counts, *E[i]* = *nπ*[0]*[(][i][)]*, are merely the product
    of the overall sample size *n* with the respective null proportion for each category.
    The result of χ² follows a *chi-squared distribution* (explained further momentarily)
    with *ν* = *k* − 1 degrees of freedom. You usually consider the test to be valid
    based on an informal rule of thumb stating that at least 80 percent of the expected
    counts *E[i]* should be at least 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this type of chi-squared test, it is important to note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: • The term *goodness of fit* refers to the proximity of the observed data to
    the distribution hypothesized in H[0].
  prefs: []
  type: TYPE_NORMAL
- en: • Positive extremity of the result of (18.11) provides evidence against H[0].
    As such, the corresponding *p-value is always computed as an upper-tail area*.
  prefs: []
  type: TYPE_NORMAL
- en: • As in the current example, a test for uniformity simplifies the null hypothesis
    slightly by having equivalent null proportions *π*[0] = *π*[0][(][1][)] = ...
    = *π*[0]*[(][k][)]*.
  prefs: []
  type: TYPE_NORMAL
- en: • A rejected H[0] doesn’t tell you about the true values of *π[i]*. It merely
    suggests that they do not follow H[0] specifically.
  prefs: []
  type: TYPE_NORMAL
- en: The chi-squared distribution relies on specification of a degree of freedom,
    much like the *t*-distribution. Unlike a *t* curve, however, a chi-squared curve
    is unidirectional in nature, being defined for non-negative values and with a
    positive (right-hand) horizontal asymptote (tail going to zero).
  prefs: []
  type: TYPE_NORMAL
- en: It’s this unidirectional distribution that leads to *p*-values being defined
    as upper-tail areas only; decisions like one- or two-tailed areas have no relevance
    in these types of chi-squared tests. To get an idea of what the density functions
    actually look like, [Figure 18-1](ch18.xhtml#ch18fig1) shows three particular
    curves defined with *ν* = 1, *ν* = 5, and *ν* = 10 degrees of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f18-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 18-1: Three instances of the chi-squared density function using differing
    degrees of freedom values. Note the positive domain of the function and the “flattening”
    and “right-extending” behavior as* *ν* *is increased.*'
  prefs: []
  type: TYPE_NORMAL
- en: This image was produced using the relevant `d`-function, `dchisq`, with *ν*
    passed to the argument `df`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The current facial hair example is a test for the uniformity of the distribution
    of frequencies in the three categories. You can obtain the observed counts and
    corresponding proportions with `table`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: For computation of the test statistic χ², you have the observed counts *O[i]*
    in `hairy.tab`. The expected count *E[i]* is a straightforward arithmetic calculation
    of the total number of observations multiplied by the null proportion 1/3 (the
    result stored as `expected`), giving you the same value for each category.
  prefs: []
  type: TYPE_NORMAL
- en: These, as well as the contribution of each category to the test statistic, are
    nicely presented in a matrix constructed with `cbind` ([Section 3.1.2](ch03.xhtml#ch03lev2sec25)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note that all the expected counts are comfortably greater than 5, which satisfies
    the informal rule of thumb mentioned earlier. In terms of R coding, note also
    that the single number `expected` is implicitly recycled to match the length of
    the other vectors supplied to `cbind` and that you’ve used the `dimnames` attribute
    (refer to [Section 6.2.1](ch06.xhtml#ch06lev2sec59)) to annotate the rows and
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: The test statistic, as per (18.11), is given as the sum of the (*O[i]* − *E[i]*)²/*E*[i]
    contributions in the fourth column of `hairy.matrix`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The corresponding *p*-value is the appropriate upper-tail area from the chi-squared
    distribution with *ν* = 3 − 1 = 2 degrees of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This small *p*-value provides evidence to suggest that the true frequencies
    in the defined categories of male facial hair are not uniformly distributed in
    a 1/3,1/3,1/3 fashion. Remember that the test result doesn’t give you the true
    proportions but only suggests that they do not follow those in H[0].
  prefs: []
  type: TYPE_NORMAL
- en: '**R Function: chisq.test**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Like `t.test` and `prop.test`, R provides a quick-use function for performing
    a chi-squared GOF test. The `chisq.test` function takes the vector of observed
    frequencies as its first argument `x`. For the facial hair example, this simple
    line therefore provides the same results as found previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the function performs a test for uniformity, taking the number
    of categories as the length of the vector supplied to `x`. However, suppose that
    the researcher collecting the facial hair data realizes that he was doing so in
    November, a month during which many men grow mustaches in support of “Mo-vember”
    to raise awareness of men’s health. This changes thoughts on the true rates in
    terms of his clean-shaven (1), beard-only *or* moustache-only (2), and beard *and*
    moustache (3) categories. He now wants to test the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[0] : *π*[0][(][1][)] = 0.25; *π*[0][(][2][)] = 0.5; *π*[0][(][3][)] = 0.25'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[A] : H[0] is incorrect.'
  prefs: []
  type: TYPE_NORMAL
- en: If a GOF test of uniformity is not desired, when the “true” rates across the
    categories are not all the same, the `chisq.test` function requires you to supply
    the null proportions as a vector of the same length as `x` to the `p` argument.
    Naturally, each entry in `p` must correspond to the categories tabulated in `x`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: With a very high *p*-value, there is no evidence to reject H[0] in this scenario.
    In other words, there is no evidence to suggest that the proportions hypothesized
    in H[0] are incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: '***18.4.2 Two Categorical Variables***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The chi-squared test can also apply to the situation in which you have *two*
    mutually exclusive and exhaustive categorical variables at hand—call them variable
    *A* and variable *B*. It is used to detect whether there might be some influential
    relationship (in other words, *dependence*) between *A* and *B* by looking at
    the way in which the distribution of frequencies change together with respect
    to their categories. If there is no relationship, the distribution of frequencies
    in variable *A* will have nothing to do with the distribution of frequencies in
    variable *B*. As such, this particular variant of the chi-squared test is called
    a *test of independence* and is always performed with the following hypotheses:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H[0] : Variables *A* and *B* are independent.'
  prefs: []
  type: TYPE_NORMAL
- en: (*or*, There is no relationship between *A* and *B*.)
  prefs: []
  type: TYPE_NORMAL
- en: 'H[A] : Variables *A* and *B* are *not* independent.'
  prefs: []
  type: TYPE_NORMAL
- en: (*or*, There *is* a relationship between *A* and *B*.)
  prefs: []
  type: TYPE_NORMAL
- en: To carry out the test, therefore, you compare the observed data to the counts
    you’d expect to see if the distributions were completely unrelated (satisfying
    the assumption that H[0] is true). An overall large departure from the expected
    frequencies will result in a small *p*-value and thus provide evidence against
    the null.
  prefs: []
  type: TYPE_NORMAL
- en: So, how are such data best presented? For two categorical variables, a two-dimensional
    structure is appropriate; in R, this is a standard matrix. For example, suppose
    some dermatologists at a certain clinical practice are interested in their successes
    in treating a common skin affliction. Their records show *N* = 355 patients have
    been treated at their clinic using one of four possible treatments—a course of
    tablets, a series of injections, a laser treatment, and an herbal-based remedy.
    The level of success in curing the affliction is also recorded—none, partial success,
    and full success. The data are given in the constructed matrix `skin`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: A two-dimensional table presenting frequencies in this fashion is called a *contingency
    table*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculation: Chi-Squared Test of Independence**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To compute the test statistic, presume data are presented as a *k[r]* × *k[c]*
    contingency table, in other words, a matrix of counts, based on two categorical
    variables (both mutually exclusive and exhaustive). The focus of the test is the
    way in which the frequencies of *N* observations between the *k[r]* levels of
    the “row” variable and the *k[c]* levels of the “column” variable are jointly
    distributed. The test statistic χ² is given with
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e18-12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *O*[[]*[i][,] [j]*] is the observed count and *E*[[]*[i][,] [j]*] is the
    expected count at row position *i* and column position *j*. Each *E*[[]*[i][,]
    [j]*] is found as the sum total of row *i* multiplied by the sum total of column
    *j*, all divided by *N*.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e18-13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The result, χ², follows a chi-squared distribution with *ν* = (*k[r]* − 1) ×
    (*k[c]* − 1) degrees of freedom. Again, the *p*-value is always an upper-tailed
    area, and you can consider the test valid with the satisfaction of the condition
    that at least 80 percent of the *E*[[]*[i][,] [j]*] are at least 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this calculation, it’s important to note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: • It’s not necessary to assume that *k[r]* = *k[c]*.
  prefs: []
  type: TYPE_NORMAL
- en: • The functionality of [Equation (18.12)](ch18.xhtml#ch18eq12) is the same as
    that of (18.11)—an overall sum involving the squared differences between the observed
    and expected values of each cell.
  prefs: []
  type: TYPE_NORMAL
- en: • The double-sum in (18.12) just represents the total sum over all the cells,
    in the sense that you can compute the total sample size *N* with ![image](../images/f0416-03.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: • A rejected H[0] doesn’t tell you about the nature of *how* the frequencies
    depend on one another, just that there is evidence to suggest that some kind of
    dependency between the two categorical variables exists.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing with the example, the dermatologists want to determine whether their
    records suggest there is statistical evidence to indicate some relationship between
    the type of treatment and the level of success in curing the skin affliction.
    For convenience, store the total number of categories *k[r]* and *k[c]* for the
    row and column variables, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: You have the *O*[[]*[i][,] [j]*] in `skin`, so now you must now compute the
    *E*[[]*[i][,] [j]*]. In light of [Equation (18.13)](ch18.xhtml#ch18eq13), which
    deals with row and column sums, you can evaluate these using the built-in `rowSums`
    and `colSums` functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'These results indicate the totals in each group, regardless of the other variable.
    To get the expected counts for all cells of the matrix, [Equation (18.13)](ch18.xhtml#ch18eq13)
    requires each row sum to be multiplied by each column sum once. You could write
    a `for` loop, but this would be inefficient and rather inelegant. It is better
    to use `rep` with the optional `each` argument (refer to [Section 2.3.2](ch02.xhtml#ch02lev2sec21)).
    By repeating each element of the column totals (level of success) four times,
    you can then use vector-oriented behavior to multiply that repeated vector by
    the shorter vector produced by `rowSums`. You can then call `sum(skin)` to divide
    this by *N* and rearrange it into a matrix. The following lines show how this
    example works step-by-step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Note that all the expected values are greater than 5, as preferred.
  prefs: []
  type: TYPE_NORMAL
- en: It’s best to construct a single object to hold the results of the different
    stages of calculations leading to the test statistic, as you did for the one-dimensional
    example. Since each stage is a matrix, you can bind the relevant matrices together
    with `cbind` and produce an array of the appropriate dimensions (refer to [Section
    3.4](ch03.xhtml#ch03lev1sec15) for a refresher).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The final steps are easy—the test statistic given by (18.12) is just the grand
    total of all elements of the matrix that is the third layer of `skin.array`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding *p*-value for this test of independence is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Recall that the relevant degrees of freedom are defined as *ν* = (*k[r]* − 1)
    × (*k[c]* − 1).
  prefs: []
  type: TYPE_NORMAL
- en: The extremely small *p*-value provides strong evidence against the null hypothesis.
    The appropriate conclusion would be to reject H[0] and state that there does appear
    to be a relationship between the type of treatment for the skin affliction and
    the level of success in curing it.
  prefs: []
  type: TYPE_NORMAL
- en: '**R Function: chisq.test**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Yet once more, no section in this chapter would be complete without showcasing
    the built-in functionality R possesses for these fundamental procedures. The default
    behavior of `chisq.test`, when supplied a matrix as `x`, is to perform a chi-squared
    test of independence with respect to the row and column frequencies—just as performed
    manually here for the skin affliction example. The following result easily confirms
    your previous calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '**Exercise 18.4**'
  prefs: []
  type: TYPE_NORMAL
- en: '`HairEyeColor` is a ready-to-use data set in R that you haven’t yet come across.
    This 4 × 4 × 2 array provides frequencies of hair and eye colors of 592 statistics
    students, split by sex ([Snee, 1974](ref.xhtml#ref61)).'
  prefs: []
  type: TYPE_NORMAL
- en: Perform and interpret, at a significance level of *α* = 0.01, a chi-squared
    test of independence for hair against eye color for all students, regardless of
    their sex.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In [Exercise 8.1](ch08.xhtml#ch8exc1) on [page 161](ch08.xhtml#page_161), you
    accessed the `Duncan` data set of the contributed package `car`, which contains
    markers of job prestige collected in 1950\. Install the package if you haven’t
    already and load the data frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first column of `Duncan` is the variable `type`, recording the type of
    job as a factor with three levels: `prof` (professional or managerial), `bc` (blue
    collar), and `wc` (white collar). Construct appropriate hypotheses and perform
    a chi-squared GOF test to determine whether the three job types are equally represented
    in the data set.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interpret the resulting *p*-value with respect to a significance level of *α*
    = 0.05.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What conclusion would you reach if you used a significance level of *α* = 0.01?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**18.5 Errors and Power**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In discussing all these forms of statistical hypothesis testing, there has
    been one common thread: the interpretation of a *p*-value and what it tells you
    about your problem in terms of the hypotheses. Frequentist statistical hypothesis
    testing is ubiquitous in many fields of research, so it is important to at least
    briefly explore directly related concepts.'
  prefs: []
  type: TYPE_NORMAL
- en: '***18.5.1 Hypothesis Test Errors***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Hypothesis testing is performed with the objective of obtaining a *p*-value
    in order to quantify evidence against the null statement H[0]. This is rejected
    in favor of the alternative, H[A], if the *p*-value is itself less than a predefined
    significance level *α*, which is conventionally 0.05 or 0.01\. As touched upon,
    this approach is justifiably criticized since the choice of *α* is essentially
    arbitrary; a decision to reject or retain H[0] can change depending solely upon
    the *α* value.
  prefs: []
  type: TYPE_NORMAL
- en: Consider for the moment, given a specific test, what the *correct* outcome is.
    If H[0] is really true, then you’d want to retain it. If H[A] is really true,
    you’d want to reject the null. This “truth,” one way or another, is impossible
    to know in practice. That being said, it’s useful to consider in a theoretical
    sense just how good (or bad) a given hypothesis test is at yielding a result that
    leads to the correct conclusion.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to test the validity of your rejection or retention of the null
    hypothesis, you must be able to identify two kinds of errors:'
  prefs: []
  type: TYPE_NORMAL
- en: • A *Type I error* occurs when you incorrectly reject a true H[0]. In any given
    hypothesis test, the probability of a Type I error is equivalent to the significance
    level *α*.
  prefs: []
  type: TYPE_NORMAL
- en: • A *Type II error* occurs when you incorrectly retain a false H[0] (in other
    words, fail to accept a true H[A]). Since this depends upon what the true H[A]
    actually is, the probability of committing such an error, labeled *β*, is not
    usually known in practice.
  prefs: []
  type: TYPE_NORMAL
- en: '***18.5.2 Type I Errors***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If your *p*-value is less than *α*, you reject the null statement. If the null
    is really true, though, the *α* directly defines the probability that you *incorrectly*
    reject it. This is referred to as a *Type I error*.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 18-2](ch18.xhtml#ch18fig2) provides a conceptual illustration of a
    Type I error probability for a supposed hypothesis test of a sample mean, where
    the hypotheses are set up as H[0] : *μ* = *μ*[0] and H[A] : *μ* > *μ*[0].'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f18-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 18-2: A conceptual diagram of the Type I error probability* *α*'
  prefs: []
  type: TYPE_NORMAL
- en: The null hypothesis distribution is centered on the null value *μ*[0]; the alternative
    hypothesis distribution is centered to its right at some mean *μ*[A] in [Figure
    18-2](ch18.xhtml#ch18fig2). As you can see, if the null hypothesis is really true,
    then the probability it is incorrectly rejected for this test will be equal to
    the significance level *α*, located in the upper tail of the null distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Simulating Type I Errors**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To demonstrate the Type I error rate via *numerical simulation* (here, this
    refers to randomly generating hypothetical data samples), you can write code that
    does the equivalent of repeating a hypothesis test under known conditions. So
    that you can use this code multiple times, in the R script editor define the following
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The `typeI.tester` function is designed to generate `ITERATIONS` samples from
    a particular normal distribution. With each sample, you’ll perform an upper-tailed
    test of the mean (refer to [Section 18.2.1](ch18.xhtml#ch18lev2sec158)) in the
    spirit of [Figure 18-2](ch18.xhtml#ch18fig2), assuming the hypotheses of H[0]
    : *μ* = *μ*[0] and H[A] : *μ* > *μ*[0].'
  prefs: []
  type: TYPE_NORMAL
- en: You can decrease `ITERATIONS` to generate fewer entire samples, and this will
    speed up computation time but will result in simulated rates that are more variable.
    Each entire sample of size `n` of hypothetical raw measurements is generated using
    `rnorm` with the mean equal to the `mu0` argument (and standard deviation equal
    to the `sigma` argument). The desired significance level is set by `alpha`. In
    the `for` loop, the sample mean and sample standard deviation are calculated for
    each generated sample.
  prefs: []
  type: TYPE_NORMAL
- en: Were each sample subjected to a “real” hypothesis test, the *p*-value would
    be taken from the right-hand area of the *t*-distribution with `n-1` degrees of
    freedom (using `pt`), with respect to the standardized test statistic given earlier
    in [Equation (18.2)](ch18.xhtml#ch18eq2).
  prefs: []
  type: TYPE_NORMAL
- en: The calculated *p*-value, at each iteration, is stored in a predefined vector
    `pvals`. The logical vector `pvals<alpha` therefore contains corresponding `TRUE`/`FALSE`
    values; the former logical value flags rejection of the null hypothesis, and the
    latter flags retention. The Type I error rate is determined by calling `mean`
    on that logical vector, which yields the proportion of `TRUE`s (in other words,
    the overall proportion of “null hypothesis rejections”) arising from the simulated
    samples. Remember, the samples are generated randomly, so your results are liable
    to change slightly each time you run the function.
  prefs: []
  type: TYPE_NORMAL
- en: This function works because, by definition of the problem, the samples that
    are being generated come from a distribution that truly has the mean set at the
    null value, in other words, *μ*[A] = *μ*[0]. Therefore, any statistical rejection
    of this statement, obtained with a *p*-value less than the significance level
    *α*, is clearly incorrect and is purely a result of random variation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To try this, import the function and execute it generating the default `ITERATIONS=10000`
    samples. Use the standard normal as the null (and “true” in this case!) distribution;
    make each sample of size 40 and set the significance level at the conventional
    *α* = 0.05\. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: This indicates that 10,000 × 0.0489 = 489 of the samples taken yielded a corresponding
    test statistic that provided a *p*-value, which would incorrectly result in rejection
    of H[0]. This simulated Type I error rate lies close to the preset `alpha=0.05`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s another example, this time for nonstandard normal data samples with
    *α* = 0.01:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Note that again, the numerically simulated rate of Type I error reflects the
    significance level.
  prefs: []
  type: TYPE_NORMAL
- en: These results are not difficult to understand theoretically—if the true distribution
    does indeed have a mean equal to the null value, you’ll naturally observe those
    “extreme” test statistic values in practice at a rate equal to *α*. The catch,
    of course, is that in practice the true distribution is unknown, highlighting
    once more the fact that a rejection of any H[0] can never be interpreted as proof
    of the truth of H[A]. It might simply be that the sample you observed followed
    the null hypothesis but produced an extreme test statistic value by chance, however
    small that chance might be.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bonferroni Correction**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The fact that Type I errors naturally occur because of random variation is particularly
    important and leads us to consider the *multiple testing problem*. If you’re conducting
    many hypothesis tests, you should be cautious in simply reporting the “number
    of statistically significant outcomes”—as you increase the number of hypothesis
    tests, you increase the chance of receiving an erroneous result. In, say, 20 tests
    conducted under *α* = 0.05, on average one will be a so-called false positive;
    if you conduct 40 or 60 tests, you are inevitably more likely to find more false
    positives.
  prefs: []
  type: TYPE_NORMAL
- en: When several hypothesis tests are conducted, you can curb the multiple testing
    problem with respect to committing a Type I error by using the *Bonferroni correction*.
    The Bonferroni correction suggests that when performing a total of *N* independent
    hypothesis tests, each under a significance level of *α*, you should instead use
    *α*[B] = *α*/*N* for any interpretation of statistical significance. Be aware,
    however, that this correction to the level of significance represents the simplest
    solution to the multiple testing problem and can be criticized for its conservative
    nature, which is potentially problematic when *N* is large.
  prefs: []
  type: TYPE_NORMAL
- en: The Bonferroni and other corrective measures were developed in an attempt to
    formalize remedies to making a Type I error in multiple tests. In general, though,
    it suffices to be aware of the possibility that H[0] may be true, even if the
    *p*-value is considered small.
  prefs: []
  type: TYPE_NORMAL
- en: '***18.5.3 Type II Errors***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The issues with Type I errors might suggest that it’s desirable to perform a
    hypothesis test with a smaller *α* value. Unfortunately, it’s not quite so simple;
    reducing the significance level for any given test leads directly to an increase
    in the chance of committing a Type II error.
  prefs: []
  type: TYPE_NORMAL
- en: A Type II error refers to incorrect retention of the null hypothesis—in other
    words, obtaining a *p*-value greater than the significance level when it’s the
    alternative hypothesis that’s actually true. For the same scenario you’ve been
    looking at so far (an upper-tailed test for a single sample mean), [Figure 18-3](ch18.xhtml#ch18fig3)
    illustrates the probability of a Type II error, shaded and denoted *β*.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f18-03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 18-3: A conceptual diagram of the Type II error probability *β**'
  prefs: []
  type: TYPE_NORMAL
- en: It’s not as easy to find *β* as it is to find the probability of making a Type
    I error because *β* depends, among other things, on what the true value of *μ*[A]
    is (which in general you won’t know). If *μ*[A] is closer to the hypothesized
    null value of *μ*[0], you can imagine the alternative distribution in [Figure
    18-3](ch18.xhtml#ch18fig3) translating (shifting) to the left, resulting in an
    increase in *β*. Similarly, staying with [Figure 18-3](ch18.xhtml#ch18fig3), imagine
    decreasing the significance level *α*. Doing so means the vertical dashed line
    (denoting the corresponding critical value) moves to the right, also increasing
    the shaded area of *β*. Intuitively, this makes sense—the closer the true alternative
    value is to the null and/or the smaller the significance level, the harder H[A]
    is to detect by rejection of H[0].
  prefs: []
  type: TYPE_NORMAL
- en: 'As noted, *β* usually can’t be calculated in practice because of the need to
    know what the true distribution actually is. This quantity is, however, useful
    in giving you an idea of how prone a test is to the incorrect retention of a null
    hypothesis under particular conditions. Suppose, for example, you’re performing
    a one-sample *t*-test for H[0] : *μ* = *μ*[0] and H[A] : *μ* > *μ*[0] with *μ*[0]
    = 0 but that the (true) alternative distribution of the raw measurements has mean
    *μ*[A] = 0.5 and standard deviation *σ* = 1\. Given a random sample of size *n*
    = 30 and using *α* = 0.05, what is the probability of committing a Type II error
    in any given hypothesis test (using the same standard deviation for the null distribution)?
    To answer this, look again at [Figure 18-3](ch18.xhtml#ch18fig3); you need the
    critical value marked off by the significance level (the dashed vertical line).
    If you assume *σ* is known, then the sampling distribution of interest will be
    normal with mean *μ*[0] = 0 and a standard error of ![image](../images/f0425-01a.jpg)
    (see [Section 17.1.1](ch17.xhtml#ch17lev2sec146)). Therefore, with an upper-tail
    area of 0.05, you can find the critical value with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'This represents the vertical dashed line in this specific setting (see [Section
    16.2.2](ch16.xhtml#ch16lev2sec142) for a refresher on use of `qnorm`). The Type
    II error in this example is found as the left-hand tail area under the alternative,
    “true” distribution, from that critical value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: From this, you can see that a hypothesis test under these conditions has roughly
    a 13.7 percent chance of incorrect retention of the null.
  prefs: []
  type: TYPE_NORMAL
- en: '**Simulating Type II Errors**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Simulation is especially useful here. In the editor, consider the function
    `typeII.tester` defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: This function is similar to `typeI.tester`. The null value, standard deviation
    of raw measurements, sample size, significance level, and number of iterations
    are all as before. Additionally, you now have `muA`, providing the “true” mean
    *μ*[A] under which to generate the samples. Again, at each iteration, a random
    sample of size `n` is generated, its mean and standard deviation are calculated,
    and the appropriate *p*-value for the test is computed using `pt` from the usual
    standardized test statistic with `df=n-1`. (Remember, since you’re estimating
    the true standard deviation of the measurements *σ* with the sample standard deviation
    *s*, it’s technically correct to use the *t*-distribution.) Following completion
    of the `for` loop, the proportion of *p*-values that were greater than or equal
    to the significance level `alpha` is returned.
  prefs: []
  type: TYPE_NORMAL
- en: After importing the function into the workspace, you can simulate *β* for this
    test.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: My result indicates something close to the theoretical *β* evaluated previously,
    albeit slightly larger because of the additional uncertainty that is naturally
    present when using a *t*-based sampling distribution instead of a normal. Again,
    each time you run `typeII.tester`, the results will vary slightly since everything
    is based on randomly generated hypothetical data samples.
  prefs: []
  type: TYPE_NORMAL
- en: Turning your attention to [Figure 18-3](ch18.xhtml#ch18fig3), you can see (in
    line with a comment made earlier) that if, in an effort to decrease the chance
    of a Type I error, you use *α* = 0.01 instead of 0.05, the vertical line moves
    to the right, thereby increasing the probability of a Type II error, with all
    other conditions being held constant.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '**Other Influences on the Type II Error Rate**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The significance level isn’t the only contributing factor in driving *β*. Keeping
    *α* at 0.01, this time see what happens if the standard deviation of the raw measurements
    is increased from *σ* = 1 to *σ* = 1.1 and then *σ* = 1.2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Increasing the variability of the measurements, without touching anything else
    in the scenario, also increases the chance of a Type II error. You can imagine
    the curves in [Figure 18-3](ch18.xhtml#ch18fig3) becoming flatter and more widely
    dispersed owing to a larger standard error of the mean, which would result in
    more probability weight in the left-hand tail marked off by the critical value.
    Conversely, if the variability of the raw measurements is smaller, then the sampling
    distributions of the sample mean will be taller and skinnier, meaning a reduction
    in *β*.
  prefs: []
  type: TYPE_NORMAL
- en: A smaller or larger sample size will have a similar impact. Located in the denominator
    of the standard error formula, a smaller *n* will result in a larger standard
    error and hence that flatter curve and an increased *β*; a larger sample size
    will have the opposite effect. If you remain with the latest values of *μ*[0]
    = 0, *μ*[A] = 0.5, *σ* = 1.2, and *α* = 0.01, note that reducing the sample size
    to 20 (from 30) results in an increased simulated Type II error rate compared
    with the most recent result of 0.5501, but increasing the sample size to 40 improves
    the rate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Finally, as noted at the beginning of the discussion, the specific value of
    *μ*[A] itself affects *β* just as you’d expect. Again, keeping the latest values
    for all other components, which resulted in my case in *β* = 0.4219, note that
    shifting the “true” mean closer to *μ*[0] by changing from *μ*[A] = 0.5 to *μ*[A]
    = 0.4 means the probability of committing a Type II error is increased; the opposite
    is true if the difference is increased to *μ*[A] = 0.6.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: To summarize, although these simulated rates have been applied to the specific
    situation in which the hypothesis test is an upper-tailed test for a single mean,
    the general concepts and ideas discussed here hold for any hypothesis test. It’s
    easy to establish that the Type I error rate matches the predefined significance
    level and so can be decreased by reducing *α*. In contrast, controlling the Type
    II error rate is a complex balancing act that can involve sample size, significance
    level, observation variability, and magnitude of the difference between the true
    value and the null. This problem is largely academic since the “truth” is typically
    unknown in practice. However, the Type II error rate’s direct relationship to
    statistical power often plays a critical role in preparing for data collection,
    especially when you’re considering sample size requirements, as you’ll see in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 18.5**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Write a new version of `typeI.tester` called `typeI.mean`. The new function
    should be able to simulate the Type I error rate for tests of a single mean in
    any direction (in other words, one-or two-sided). The new function should take
    an additional argument, `test`, which takes a character string `"less"`, `"greater"`,
    or `"two.sided"` depending on the type of desired test. You can achieve this by
    modifying `typeI.tester` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: – Instead of calculating and storing the *p*-values directly in the `for` loop,
    simply store the test statistic.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – When the loop is complete, set up stacked `if`-`else` statements that cater
    to each of the three types of test, calculating the *p*-value as appropriate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – For the two-sided test, remember that the *p*-value is defined as twice the
    area “more extreme” than the null. Computationally, this means you must use the
    upper-tail area if the test statistic is positive and the lower-tail area otherwise.
    If this area is less than half of *α* (since it is subsequently multiplied by
    2 in a “real” hypothesis test), then a rejection of the null should be flagged.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – If the value of `test` is not one of the three possibilities, the function
    should throw an appropriate error using `stop`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Experiment with your function using the first example setting in the text with
    *μ*[0] = 0, *σ* = 1, *n* = 40, and *α* = 0.05\. Call `typeI.mean` three times,
    using each of the three possible options for `test`. You should find that all
    simulated results sit close to 0.05.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat (i) using the second example setting in the text with *μ*[0] = −4, *σ*
    = 0.3, *n* = 60, and *α* = 0.01\. Again, you should find that all simulated results
    sit close to the value of *α*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify `typeII.tester` in the same way as you did `typeI.tester`; call the new
    function `typeII.mean`. Simulate the Type II error rates for the following hypothesis
    tests. As per the text, assume *μ*[A], *σ*, *α*, and *n* denote the true mean,
    standard deviation of raw observations, significance level, and sample size, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H[0] : *μ* = −3.2; H[A] : *μ* ≠ −3.2'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: with *μ*[A] = −3.3, *σ* = 0.1, *α* = 0.05, and *n* = 25.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'H[0] : *μ* = 8994; H[A] : *μ* < 8994'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: with *μ*[A] = 5600, *σ* = 3888, *α* = 0.01, and *n* = 9.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'H[0] : *μ* = 0.44; H[A] : *μ* > 0.44'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: with *μ*[A] = 0.4, *σ* = 2.4, *α* = 0.05, and *n* = 68.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '***18.5.4 Statistical Power***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For any hypothesis test, it is useful to consider its potential statistical
    power. *Power* is the probability of correctly rejecting a null hypothesis that
    is untrue. For a test that has a Type II error rate of *β*, the statistical power
    is found simply with 1 − *β*. It’s desirable for a test to have a power that’s
    as high as possible. The simple relationship with the Type II error probability
    means that all factors impacting the value of *β* also directly affect power.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the same one-sided H[0] : *μ* = *μ*[0] and H[A] : *μ* > *μ*[0] example
    discussed in the previous section, [Figure 18-4](ch18.xhtml#ch18fig4) shades the
    power of the test—the complement to the Type II error rate. By convention, a hypothesis
    test that has a power greater than 0.8 is considered *statistically powerful*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f18-04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 18-4: A conceptual diagram of statistical power 1 − *β**'
  prefs: []
  type: TYPE_NORMAL
- en: You can numerically evaluate power under specific testing conditions using simulation.
    For the previous discussion on Type II errors, you’re able to subtract all simulated
    results of *β* from 1 to evaluate the power of that particular test. For example,
    the power of detection of *μ*[A] = 0.6 when *μ*[0] = 0, taking samples of size
    *n* = 40 and with *σ* = 1.2 and *α* = 0.01, is simulated as 1 − 0.2287 = 0.7713
    (using my most recent result of *β* from earlier). This means there’s roughly
    a 77 percent chance of correctly detecting the true mean of 0.6 in a hypothesis
    test based on a sample of measurements generated under those conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers are often interested in the relationship between power and sample
    size (though it is important to bear in mind that this is only one of the influential
    ingredients in the determination of power). Before you begin to collect data to
    examine a particular hypothesis, you might have an idea of the potential true
    value of the parameter of interest from past research or pilot studies. This is
    useful in helping to determine your sample size, such as in helping to answer
    questions like “How big does my sample need to be in order to be able to conduct
    a statistically powerful test to correctly reject H[0], if the true mean is actually
    *μ*[A]?”
  prefs: []
  type: TYPE_NORMAL
- en: '**Simulating Power**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For the most recent testing conditions, with a sample size of *n* = 40, you’ve
    seen that there’s a power of around 0.77 of detecting *μ*[A] = 0.6\. For the purposes
    of this example, let’s say you want to find how much you should increase *n* by
    in order to conduct a statistically powerful test. To answer this, define the
    following function `power.tester` in the editor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: The `power.tester` function uses the `typeII.tester` function defined in [Section
    18.5.3](ch18.xhtml#ch18lev2sec166) to evaluate the power of a given upper-tailed
    hypothesis test of a single sample mean. It takes a vector of sample sizes supplied
    as the `nvec` argument (you pass all other arguments to `typeII.tester` using
    an ellipsis—refer to [Section 11.2.4](ch11.xhtml#ch11lev2sec102)). A `for` loop
    defined in `power.tester` cycles through the entries of `nvec` one at a time,
    simulates the power for each sample size, and stores them in a corresponding vector
    that’s returned to the user. Remember, through `typeII.tester`, this function
    is using random generation of hypothetical data samples, so there may be some
    fluctuation in the results you observe each time you run `power.tester`.
  prefs: []
  type: TYPE_NORMAL
- en: There can be a slight delay when evaluating the power for many individual sample
    sizes, so this function also provides a good opportunity to showcase a progress
    bar in a practical implementation (refer to [Section 12.2.1](ch12.xhtml#ch12lev2sec108)
    for details).
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up the following vector, which uses the colon operator (see [Section 2.3.2](ch02.xhtml#ch02lev2sec21))
    to construct a sequence of integers between 5 and 100 inclusive for the sample
    sizes to be examined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Importing the `power.tester` function, you can then simulate the power for each
    of these integers for this particular test (`ITERATIONS` is halved to `5000` to
    reduce the overall completion time).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the power of detection rises steadily as *n* increases; the conventional
    cutoff of 80 percent is visible in these results as lying between 0.7950 and 0.8050\.
    If you don’t want to identify the value visually, you can find which entry of
    `sample.sizes` corresponds to the 80 percent cutoff by first using `which` to
    identify the indexes of `pow` that are at least 0.8 and then returning the lowest
    value in that category with `min`. The identified index may then be specified
    in square brackets to `sample.sizes` to give you the value of *n* that corresponds
    to that simulated power (0.8050 in this case). These commands can be nested as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: The result indicates that if your sample size is at least 43, a hypothesis test
    under these particular conditions should be statistically powerful (based on the
    randomly simulated output in `pow` in this instance).
  prefs: []
  type: TYPE_NORMAL
- en: What if the significance level for this test were relaxed? Say you wanted to
    conduct the test (still upper-tailed under the condition of *μ*[0] = 0, *μ*[A]
    = 0.6, and *σ* = 1.2) using a significance level of *α* = 0.05 rather than 0.01\.
    If you look again at [Figure 18-4](ch18.xhtml#ch18fig4), this alteration means
    the vertical line (critical value) moves to the left, decreasing *β* and so increasing
    power. That would therefore suggest you’d require a smaller sample size than earlier,
    in other words, *n* < 43, in order to perform a statistically powerful test when
    *α* is increased.
  prefs: []
  type: TYPE_NORMAL
- en: 'To simulate this situation for the same range of sample sizes and store the
    resulting powers in `pow2`, examine the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: This result indicates a sample size of at least 27 is required, which is a noticeable
    reduction from the 43 noted if *α* = 0.01\. However, relaxing *α* means an increased
    risk of committing a Type I error!
  prefs: []
  type: TYPE_NORMAL
- en: '**Power Curves**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For comparison, you can plot your simulated powers as a kind of power curve
    using both `pow` and `pow2` with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: My particular image is given in [Figure 18-5](ch18.xhtml#ch18fig5). A horizontal
    line marks off the power of 0.8, and the vertical line marks the minimum sample
    size values identified and stored in `minimum.n` and `minimum.n2`. As a final
    touch, a legend is added to reference the *α* values of each curve.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f18-05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 18-5: Simulated power curves for the upper-tailed hypothesis test of
    a single sample mean*'
  prefs: []
  type: TYPE_NORMAL
- en: The curves themselves indicate exactly what you’d expect—the power of detection
    increases as the sample size is incremented. You can also note the flattening
    off occurring as the power rises ever closer to the “perfect” rate of 1, which
    is typical of a power curve. For *α* = 0.05, the curve sits almost consistently
    above the curve for *α* = 0.01, though the difference looks negligible as *n*
    rises above 75 or so.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding discussion of errors and power highlights the need for care in
    interpreting the results of even the most basic of statistical tests. A *p*-value
    is merely a probability, and as such, no matter how small it may be in any circumstance,
    it can never prove or disprove a claim on its own. Issues surrounding the quality
    of a hypothesis test (parametric or otherwise) should be considered, though this
    is arguably difficult in practice. Nevertheless, an awareness of Type I and Type
    II errors, as well as the concept of statistical power, is extremely useful in
    the implementation and appraisal of any formal statistical testing procedure.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 18.6**'
  prefs: []
  type: TYPE_NORMAL
- en: For this exercise you’ll need to have written `typeII.mean` from [Exercise 18.5](ch18.xhtml#ch18exc5)
    (b). Using this function, modify `power.tester` so that a new function, `power.mean`,
    calls `typeII.mean` instead of calling `typeII.tester`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Confirm that the power of the test given by H[0] : *μ* = 10; H[A] : *μ* ≠ 10,
    with *μ*[A] = 10.5, *σ* = 0.9, *α* = 0.01, and *n* = 50, is roughly 88 percent.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remember the hypothesis test in [Section 18.2.1](ch18.xhtml#ch18lev2sec158)
    for the mean net weight of an 80-gram pack of snacks, based on the *n* = 44 observations
    provided in the `snack` vector. The hypotheses were as follows:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H[0] : *μ* = 80'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'H[A] : *μ* < 80'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: If the true mean is *μ*[A] = 78.5 g and the true standard deviation of the weights
    is *σ* = 3.1 g, use `power.mean` to determine whether the test is statistically
    powerful, assuming *α* = 0.05\. Does your answer to this change if *α* = 0.01?
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Staying with the snacks hypothesis test, using the `sample.sizes` vector from
    the text, determine the minimum sample size required for a statistically powerful
    test using both *α* = 0.05 and *α* = 0.01\. Produce a plot showing the two power
    curves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Important Code in This Chapter**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| **Function/operator** | **Brief description** | **First occurrence** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `t.test` | One- and two-sample *t*-test | [Section 18.2.1](ch18.xhtml#ch18lev2sec158),
    [p. 391](ch18.xhtml#page_391) |'
  prefs: []
  type: TYPE_TB
- en: '| `prop.test` | One- and two-sample *Z*-test | [Section 18.3.1](ch18.xhtml#ch18lev2sec160),
    [p. 405](ch18.xhtml#page_405) |'
  prefs: []
  type: TYPE_TB
- en: '| `pchisq` | χ² cumulative problems | [Section 18.4.1](ch18.xhtml#ch18lev2sec162),
    [p. 414](ch18.xhtml#page_414) |'
  prefs: []
  type: TYPE_TB
- en: '| `chisq.test` | χ² test of distribution/independence | [Section 18.4.1](ch18.xhtml#ch18lev2sec162),
    [p. 414](ch18.xhtml#page_414) |'
  prefs: []
  type: TYPE_TB
- en: '| `rowSums` | Matrix row totals | [Section 18.4.2](ch18.xhtml#ch18lev2sec163),
    [p. 417](ch18.xhtml#page_417) |'
  prefs: []
  type: TYPE_TB
- en: '| `colSums` | Matrix column totals | [Section 18.4.2](ch18.xhtml#ch18lev2sec163),
    [p. 417](ch18.xhtml#page_417) |'
  prefs: []
  type: TYPE_TB
