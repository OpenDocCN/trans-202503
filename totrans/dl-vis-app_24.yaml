- en: '20'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Attention and Transformers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: In Chapter 19 we looked at how to use RNNs to handle sequential data. Though
    powerful, RNNs have a few drawbacks. Because all of the information about an input
    is represented in a single piece of state memory, or context vector, the networks
    inside each recurrent cell need to work hard to compress everything that’s needed
    into the available space. And no matter how large we make the state memory, we
    can always get an input that exceeds what the memory can hold, so something necessarily
    gets lost.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Another problem is that an RNN must be trained and used one word at a time.
    This can be a slow way to work, particularly with large databases.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: An alternative approach is based on a small network called an *attention network*,
    which doesn’t have a state memory and can be trained and used in parallel. Attention
    networks can be combined into larger structures called *transformers*, which are
    capable of serving as language models that can perform tasks like translation.
    The building blocks of transformers can be used in other architectures that provide
    even more powerful language models, including generators.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we start with a more powerful way to represent words rather
    than as single numbers, and then build our way up to attention and modern architectures
    that use transformer blocks to perform many NLP tasks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Embedding
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Chapter 19 we promised to improve our word descriptions beyond a single number.
    The value of this change is that it allows us to manipulate the representations
    of words in meaningful ways. For example, we can find a word that is like another
    word, or we can blend two words to find one that’s in between them. This concept
    is key to developing attention, and then transformers.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: The technique is called *word embedding* (or *token embedding* when we use it
    on the more general idea of a token). It’s a bit abstract, so let’s see the ideas
    first with a concrete example.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that you work as an animal wrangler on a movie with a tempestuous director.
    Today you’re filming a sequence where the human heroes are chased by some animals.
    The director asks you for a list of animals you can provide in sufficient numbers
    to produce a scary chase. You call your office, they prepare the list, and they
    even arrange those animals into a chart, where the horizontal axis represents
    each adult animal’s average top speed and the vertical axis represents its average
    weight, as in [Figure 20-1](#figure20-1).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![F20001](Images/F20001.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20-1: A collection of animals, organized roughly by land speed horizontally
    and adult weight vertically, though those axis labels aren’t shown (data from
    Reisner 2020)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: But due to a printer error, the chart your office sent you is missing the labels
    on the axes, so you have the chart with the animals laid out in 2D, but you don’t
    know what the axes mean.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: The director doesn’t even look at the chart. “Horses,” she says, “I want horses.
    They’re exactly what I want and will be perfect and nothing else will do.” So
    you bring in the horses, and they rehearse the scene.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 导演甚至没有看图表。“马，”她说，“我想要马。它们正是我想要的，完美无缺，别的都不行。”于是你带来了马，它们开始排练场景。
- en: Unfortunately, the director is unhappy. “No, no, no!” she says. “The horses
    are too twitchy and quick. They’re like foxes. Give me horses that are less fox-like.”
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，导演不满意。“不，不，不！”她说。“这些马太过敏捷和迅速，像狐狸一样。给我一些不那么像狐狸的马。”
- en: How on Earth can you satisfy this request? What does it even mean? Happily,
    you can do just as she asks with the chart, just by combining arrows.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你怎么可能满足这个要求呢？这到底是什么意思？幸运的是，你可以按照她的要求，利用图表通过组合箭头来实现。
- en: 'You only need to do two things with arrows: add them and subtract them. To
    add arrow B to arrow A, place the tail of B onto the head of A. The new arrow
    A + B starts at the tail of A, and ends at the head of B, as in the middle of
    [Figure 20-2](#figure20-2).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要做两件事：加箭头和减箭头。要将箭头B加到箭头A上，把B的尾部放到A的头部。新的箭头A + B从A的尾部开始，到B的头部结束，如[图20-2](#figure20-2)中间所示。
- en: '![F20002](Images/F20002.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![F20002](Images/F20002.png)'
- en: 'Figure 20-2: Arrow arithmetic. Left: Two arrows. Middle: The sum A + B. Right:
    The difference A – B.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-2：箭头运算。左：两个箭头。中：和A + B。右：差A – B。
- en: To subtract B from A, just flip B around by 180 degrees to make –B, and add
    together A and –B. The result, A – B, starts at the tail of A and ends at the
    head of –B, as in the right of [Figure 20-2](#figure20-2).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要从A中减去B，只需将B逆时针旋转180度，变成–B，然后将A和–B加在一起。结果，A – B，从A的尾部开始，到–B的头部结束，如[图20-2](#figure20-2)右侧所示。
- en: Now you can satisfy the director’s desire to remove the fox qualities from the
    horses. Start by drawing an arrow from the bottom left of the chart to the horse,
    and another to the fox, as in the left of [Figure 20-3](#figure20-3).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以满足导演移除马身上狐狸特征的要求。首先，从图表的左下角画一支箭头指向马，再画一支箭头指向狐狸，如[图20-3](#figure20-3)左侧所示。
- en: '![F20003](Images/F20003.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![F20003](Images/F20003.png)'
- en: 'Figure 20-3: Left: Arrows from the bottom left to the horse and fox. Right:
    Subtracting fox from horse gives us a giant sloth.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-3：左：从左下角到马和狐狸的箭头。右：从马中减去狐狸得到一只巨大的树懒。
- en: Now subtract foxes from horses, as requested, by subtracting the fox arrow from
    the horse arrow. Following the rules of [Figure 20-2](#figure20-2), that means
    flipping the fox arrow around and placing its tail at the head of the horse arrow.
    We get the right side of [Figure 20-3](#figure20-3).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在按照要求从马中减去狐狸，方法是将狐狸箭头从马箭头中减去。按照[图20-2](#figure20-2)的规则，这意味着将狐狸箭头翻转并将其尾部放在马箭头的头部。我们得到[图20-3](#figure20-3)的右侧。
- en: 'A giant sloth. Well, okay, it’s what the director wanted. We can even write
    this like a little bit of arithmetic: horse – fox = giant sloth (at least, according
    to our diagram).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一只巨大的树懒。好吧，导演想要的就是这个。我们甚至可以像做一些小算术一样写出这个：马 – 狐狸 = 巨大树懒（至少根据我们的图示来说）。
- en: The director throws her latte on the ground. “No no no! Sure, sloths would look
    great, but they hardly move! Make them fast! Give me sloths that are like roadrunners!”
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 导演把她的拿铁扔到地上。“不不不！当然，树懒看起来很好，但它们几乎不动！让它们更快！给我像路跑鸟一样快的树懒！”
- en: 'Now we know just how to satisfy this ridiculous demand: find the arrow from
    the bottom left to the roadrunner, as shown in the left of [Figure 20-4](#figure20-4),
    and add that to the head of the arrow pointing to the sloth, giving us a brown
    bear. That is, horse – fox + roadrunner = brown bear, as in the right of [Figure
    20-4](#figure20-4).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何满足这个荒谬的要求：找到从左下角到路跑鸟的箭头，如[图20-4](#figure20-4)左侧所示，并将其添加到指向树懒的箭头的头部，就得到了一只棕熊。也就是说，马
    – 狐狸 + 路跑鸟 = 棕熊，如[图20-4](#figure20-4)右侧所示。
- en: '![f20004](Images/f20004.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![f20004](Images/f20004.png)'
- en: 'Figure 20-4: Left: We can draw an arrow to the roadrunner. Right: Giant sloth
    + roadrunner = brown bear.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-4：左：我们可以画一个指向路跑鸟的箭头。右：巨大的树懒 + 路跑鸟 = 棕熊。
- en: You offer the director a group of brown bears (called a *sleuth* of bears).
    The director rolls her eyes dramatically. “Finally. Something that’s fast like
    horses, but not twitchy like foxes, and quick like roadrunners. It’s only what
    I asked for in the first place.” They shoot the chase scene with bears, and the
    movie later comes out to great acclaim.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你给导演提供了一组棕熊（称为*棕熊群*）。导演翻了个白眼。“终于，像马一样快，但不像狐狸那么敏捷，像路跑鸟一样迅速。这正是我最初要求的。”他们用熊拍摄了追逐场景，电影后来获得了广泛好评。
- en: There are two key elements to this story. The first is that the animals in our
    chart were laid out in a useful way, even though we didn’t know what that way
    was, or what the axes represented about the data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个故事有两个关键要素。第一个是，我们图表中的动物虽然我们不知道它们的排列方式或坐标轴代表什么数据，但它们的布局非常有用。
- en: The second key point is that we didn’t need the axis labels after all. We were
    able to navigate the chart just by adding and subtracting arrows pointing to elements
    on the chart itself. That is, we didn’t try to find a “slower horse.” Rather,
    we worked strictly with the animals themselves, and their various attributes came
    along implicitly. Removing the speediness of a fox from a big animal like a horse
    gave us a big, slow animal.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个关键点是，最终我们发现其实不需要坐标轴标签。我们仅通过添加和减去指向图表中元素的箭头，就能在图表中导航。也就是说，我们没有尝试找到一匹“较慢的马”。相反，我们直接与动物本身进行操作，它们的各种属性隐式地随之而来。将狐狸的快速特性移除后，结合像马这样的大动物，我们得到了一只又大又慢的动物。
- en: What does this have to do with processing language?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这和语言处理有什么关系呢？
- en: Embedding Words
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入词
- en: To apply what we’ve just seen to words, we’ll replace the animals with words.
    And instead of using only two axes, we’ll place our words in a space of hundreds
    of dimensions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们刚才看到的应用到词汇上，我们将动物替换为词语。并且，不仅仅使用两个坐标轴，我们将在一个拥有数百维的空间中放置这些词语。
- en: We do this with an algorithm that works out what each axis in this space should
    mean as it places every word at the appropriate point. Instead of assigning each
    word a single number, the algorithm assigns the word a whole list of numbers,
    representing its coordinates in a huge space.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个算法来实现这一点，算法会自动确定这个空间中每个坐标轴应该代表什么，并将每个词放置在适当的位置。算法并不是为每个词分配一个单一的数字，而是为词分配一个数字列表，表示它在这个庞大空间中的坐标。
- en: This algorithm is called an *embedder*, and we say that this process is one
    of *embedding* the words in the *embedding space*, thereby creating *word embeddings*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法被称为*嵌入器*，我们说这个过程是*将词嵌入*到*嵌入空间*中，从而创造出*词嵌入*。
- en: The embedder works out for itself how to construct the space and find the coordinates
    of each word so that it’s near similar words. For example, if it sees a lot of
    sentences that begin with I just drank some, then whatever noun comes next is
    interpreted as some kind of drink, and it is placed near other kinds of drinks.
    If it sees I just ate a red, then whatever comes next is interpreted as something
    that’s red and edible, and it is placed near other things that are red and near
    other things that are edible. The same thing is true of dozens or even hundreds
    of other relationships, both obvious and subtle. Because the space has so many
    dimensions and the axes can have arbitrarily complex meanings, words can belong
    simultaneously to many clusters based on seemingly unrelated characteristics.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入器会自动确定如何构造空间，并找出每个词的坐标，使得相似的词靠得更近。例如，如果它看到很多以“I just drank some”开头的句子，那么接下来出现的任何名词都会被理解为某种饮品，并被放置在其他饮品附近。如果它看到“I
    just ate a red”这样的句子，那么接下来出现的任何词都会被理解为红色且可食用的东西，并被放置在其他红色和其他可食用的物体附近。对于其他数十种甚至数百种关系，不管是显而易见的还是微妙的，情况也一样。由于空间有许多维度，并且坐标轴可以有任意复杂的含义，词汇可以同时属于许多不同的群体，基于看似不相关的特征。
- en: 'This idea is both abstract and powerful, so let’s illustrate it with some actual
    examples. We tried a few “word arithmetic” expressions using a pretrained embedding
    of 684,754 words saved in a space of 300 dimensions (spaCy authors 2020). Our
    first test was a famous one: king – man + woman (El Boukkouri 2018). The system
    returned queen as the most likely result, which makes sense: we can imagine that
    the embedder worked out some sense of nobility on one axis and gender on another.
    Other tests were close but not perfect. For example, lemon – yellow + green came
    back with ginger as the best match, but the expected lime wasn’t far back as the
    fifth-closest word. Similarly, trumpet – valves + slide returned saxophone as
    the most likely result, but the expected trombone was the first runner-up.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念既抽象又强大，所以让我们通过一些实际例子来说明它。我们尝试了一些“词汇算术”表达式，使用的是一个预训练的词嵌入，其中包含684,754个词，保存在一个300维的空间中（spaCy
    authors 2020）。我们的第一个测试是一个著名的例子：king – man + woman（El Boukkouri 2018）。系统返回了queen作为最可能的结果，这很有道理：我们可以想象，嵌入器在一个坐标轴上捕捉到了贵族的概念，在另一个坐标轴上捕捉到了性别的差异。其他测试结果也很接近，但并不完美。例如，lemon
    – yellow + green返回了ginger作为最佳匹配，但预期的lime也排在第五近的词汇。类似地，trumpet – valves + slide返回了saxophone作为最可能的结果，但预期的trombone排在了第一位的候选。
- en: The beauty of training an embedder in a space with hundreds (or even thousands)
    of dimensions is that it can use the space much more efficiently than any person
    probably would, enabling it to simultaneously represent an enormous number of
    relationships.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个嵌入模型在一个拥有数百（甚至数千）维度的空间中的美妙之处在于，它能够比任何人更高效地利用这个空间，从而使其能够同时表示大量的关系。
- en: The word arithmetic we just saw is a fun demonstration of embedding spaces,
    but it also enables us to meaningfully perform operations on words like comparing
    them, scaling them, and adding them, all of which are important to the algorithms
    in this chapter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到的词汇算式是嵌入空间的一个有趣示范，但它也使我们能够有意义地对词汇进行操作，比如比较、缩放和加法，这些操作对于本章的算法都非常重要。
- en: Once we have word embeddings, it’s easy to incorporate them into almost any
    network. Instead of assigning a single integer to each word, we assign the word
    embedding, which is a list of numbers. So instead of processing zero-dimensional
    tensors (single numbers), the system processes one-dimensional tensors (lists
    of numbers).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有了词嵌入，将它们整合到几乎任何网络中都变得容易。我们不是为每个词分配一个单独的整数，而是分配词嵌入，它是一个数字列表。因此，系统处理的不是零维张量（单一数字），而是一维张量（数字列表）。
- en: This neatly addresses the problem we saw in Chapter 19 where predictions that
    were close to the target but not exactly right gave us nonsense. Now we can tolerate
    a bit of imprecision, because similar words are embedded near one another. For
    example, we might give our language model the phrase The dragon approached and
    let out a mighty, expecting the next word to be roar. The algorithm might predict
    a tensor that’s near roar but not exactly on it, giving us bellow or blast instead.
    We probably wouldn’t get back something unrelated, like daffodil.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这巧妙地解决了我们在第 19 章看到的问题，即那些与目标接近但不完全正确的预测往往给我们带来无意义的结果。现在我们能够容忍一些不精确的情况，因为相似的词汇被嵌入在彼此接近的位置。例如，我们可能会给语言模型输入短语“巨龙逼近并发出一声响亮的”，期望下一个词是
    roar。算法可能会预测出一个接近 roar 的张量，但不完全相同，而给出 bellow 或 blast。我们大概率不会得到一个完全无关的词汇，如 daffodil。
- en: '[Figure 20-5](#figure20-5) shows six sets of four related words that we gave
    to a standard word embedder. The more the embeddings of any two words are like
    one another, the higher that pair of words scored, so the darker their intersection
    appears. The graph is symmetric around the diagonal from the upper left to the
    lower right, since the order in which we compare the words doesn’t matter.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 20-5](#figure20-5)展示了我们提供给标准词嵌入模型的六组四个相关词汇。任何两个词嵌入的相似度越高，这对词汇的得分就越高，因此它们的交集看起来越暗。图形在从左上到右下的对角线两侧对称，因为我们比较词汇的顺序并不重要。'
- en: '![F20005](Images/F20005.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![F20005](Images/F20005.png)'
- en: 'Figure 20-5: Comparing pairs of words by comparing the similarity of their
    embeddings'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-5：通过比较词嵌入的相似性来比较词对。
- en: We can see from [Figure 20-5](#figure20-5) that each word matches itself most
    strongly and also matches related words more strongly than unrelated words. Because
    we placed related words side by side, the graph shows their similarities as small
    blocks. There are a few curiosities, however. For example, why does fish match
    better than average with chocolate and coffee, and why does blue score well with
    caramel? These might be artifacts from the particular training data used for this
    embedder.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从[图 20-5](#figure20-5)看到，每个词与自身的匹配度最高，同时也比与不相关的词汇匹配得更强。由于我们将相关的词汇并排放置，图形显示它们的相似度为小块。然而，也有一些奇特之处。例如，为什么
    fish 与 chocolate 和 coffee 的匹配度比平均值更高？又为什么 blue 与 caramel 的得分较高？这些可能是由于这个嵌入模型使用的特定训练数据所导致的伪影。
- en: The coffee drinks and the flavors score well with one another, perhaps because
    people order coffee drinks with those flavored syrups. There’s also a hint of
    a relationship between the colors and the flavors.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 咖啡饮品和口味之间的得分相对较高，也许是因为人们会用这些口味的糖浆来调味咖啡饮品。颜色和口味之间也隐约存在一些关系。
- en: Many pretrained word embedders are widely available for free, and easily downloaded
    into almost any library. We can simply import them and immediately get the vector
    for any word. The GLoVe (Mikolov et al. 2013a; Mikolov et al. 2013b) and word2vec
    (Pennington, Socher, and Manning 2014) embeddings have been used in many projects.
    The more recent fastText (Facebook Open Source 2020) project offers embeddings
    in 157 languages.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 许多预训练的词嵌入器可以免费获得，并且可以轻松下载到几乎任何库中。我们可以简单地导入它们，立即获得任何单词的向量。GLoVe（Mikolov等，2013a；Mikolov等，2013b）和word2vec（Pennington、Socher和Manning，2014）嵌入在许多项目中得到应用。较新的fastText（Facebook开源，2020）项目提供了157种语言的嵌入向量。
- en: We can also embed entire sentences, so that we can compare them as a whole,
    rather than word by word (Cer et al. 2018). [Figure 20-6](#figure20-6) shows comparisons
    between embeddings for a dozen sentences (TensorFlow 2018). In this book, we focus
    on word embeddings rather than sentences.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以对整个句子进行嵌入，这样我们就可以整体比较句子，而不是逐词比较（Cer等，2018）。[图20-6](#figure20-6)展示了对12个句子（TensorFlow，2018）的嵌入比较。在本书中，我们将重点讨论词嵌入，而不是句子嵌入。
- en: '![F20006](Images/F20006.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![F20006](Images/F20006.png)'
- en: 'Figure 20-6: Comparing sentence embeddings. The larger the score, the more
    the sentences are considered like one another.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-6：比较句子嵌入。分数越大，表示句子之间的相似度越高。
- en: ELMo
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ELMo
- en: 'Word embeddings are a huge advance over assigning single integers to words.
    But even though word embeddings are powerful, the approach we described earlier
    to create them has a problem: nuance.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入比为单词分配单个整数要更为先进。然而，即便词嵌入功能强大，我们之前描述的创建词嵌入的方法仍然存在一个问题：细微差别。
- en: As we saw in Chapter 19, many languages have words with different meanings but
    are written and pronounced the same way. If we want to make sense of words, we
    need to distinguish these meanings. One way to do that is to give every meaning
    of a word its own embedding. So cupcake, which has one meaning, has one embedding.
    But train has two embeddings, one for when it’s a noun (as in, “I rode on a train”),
    and one for when it’s a verb (as in, “I like to train dogs”). These two meanings
    of train really are entirely different ideas that just happen to use the same
    sequence of letters.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第19章看到的，许多语言中有些单词的意义不同，但书写和发音是相同的。如果我们想理解这些单词，就需要区分它们的不同含义。做到这一点的一种方法是为每个单词的每个含义分配一个独特的嵌入向量。例如，cupcake
    只有一个意思，因此只有一个嵌入向量。但 train 则有两个嵌入向量，一个表示名词（例如，“我坐了一次火车”），另一个表示动词（例如，“我喜欢训练狗”）。这两个意思实际上是完全不同的概念，只是恰好使用了相同的字母序列。
- en: Such words present two challenges. First, we have to create unique embeddings
    for each meaning. Second, we have to select the correct embedding when such words
    are used as input. Solving these challenges requires that we take into account
    the context of every word. The first algorithm to do this in a big way was called
    *Embedding from Language Models*, but it’s better known by its friendly acronym
    *ELMo* (Peters et al. 2018), which is the name of a Muppet on the children’s television
    show *Sesame Street*. We say that ELMo produces *contextualized word embeddings*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这类词汇面临两个挑战。首先，我们必须为每个含义创建独特的嵌入向量。其次，当这些词作为输入时，我们必须选择正确的嵌入向量。解决这些问题需要我们考虑每个单词的上下文。第一个能够大规模处理这一问题的算法叫做*Embedding
    from Language Models*，但它更广为人知的是其友好的缩写*ELMo*（Peters等，2018），它的名字来自儿童电视节目《芝麻街》中的一个木偶角色Elmo。我们说ELMo生成了*上下文化的词嵌入*。
- en: ELMo’s architecture is similar to that of a pair of bi-RNNs, which we saw in
    [Figure 19-20](c19.xhtml#figure19-20), but the pieces are organized differently.
    In a standard bi-RNN, we couple two RNNs running in opposite directions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ELMo的架构类似于我们在[图19-20](c19.xhtml#figure19-20)中看到的一对双向RNN，但它的结构组织方式不同。在标准的双向RNN中，我们将两个朝相反方向运行的RNN相结合。
- en: ELMo changes this around. Although it uses two RNN networks that run forward
    and two that run backward, they are grouped by direction. Each of these groups
    is a two-layer-deep RNN, like the one we saw in [Figure 19-21](c19.xhtml#figure19-21).
    ELMo’s architecture is shown in [Figure 20-7](#figure20-7). It’s traditional to
    draw ELMo diagrams with a red color scheme, since Elmo on *Sesame Street* is a
    bright red character.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ELMo对这一点进行了改变。虽然它使用两个正向RNN网络和两个反向RNN网络，它们按方向分组。每个组都是一个两层深的RNN，就像我们在[图19-21](c19.xhtml#figure19-21)中看到的那样。ELMo的架构如[图20-7](#figure20-7)所示。通常，我们使用红色配色方案绘制ELMo的示意图，因为《芝麻街》中的Elmo是一个明亮的红色角色。
- en: '![F20007](Images/F20007.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![F20007](Images/F20007.png)'
- en: 'Figure 20-7: The structure of ELMo in unrolled form. The input text is at the
    bottom. The embedding of each input element is at the top.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-7：ELMo展开后的结构。输入文本位于底部。每个输入元素的嵌入位于顶部。
- en: This architecture means each input word is turned into two new tensors, one
    from the forward networks (labeled F1 and F2), that take into account the preceding
    words, and one from the backward networks (labeled B1 and B2) that consider the
    following words. By concatenating these results together, we get contextualized
    word embeddings informed by all the other words in the sentence.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构意味着每个输入单词都会转化为两个新的张量，一个来自前向网络（标记为F1和F2），考虑到前面的单词，另一个来自反向网络（标记为B1和B2），考虑到后面的单词。通过将这些结果连接在一起，我们得到了一个上下文化的单词嵌入，这些嵌入受到句子中所有其他单词的影响。
- en: Trained versions of ELMo are widely available for free downloads in a variety
    of sizes (Gluon 2020). Once we have a pretrained ELMo, it’s easy to use in any
    language model. We give our entire sentence to ELMo, and we get back a contextualized
    word embedding for each word, given its context.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 训练版的ELMo可以在多个大小的版本中免费提供下载（Gluon 2020）。一旦我们拥有了预训练的ELMo，它可以轻松地用于任何语言模型。我们将整个句子输入ELMo，得到一个上下文化的单词嵌入，基于其上下文。
- en: '[Figure 20-8](#figure20-8) shows four sentences that use the homonym train
    as a verb, and four that use train as a noun. We gave these to a standard ELMo
    model trained on a database of 1 billion words that places each word into a space
    of 1,024 dimensions (TensorFlow 2020a). We extracted ELMo’s embedding of the word
    train in each sentence, and compared its embedding to that of the word train in
    all the other sentences. Although the word is written in the identical way in
    each sentence, ELMo is able to identify the correct embedding based on the word’s
    context.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[图20-8](#figure20-8)展示了四个句子使用“train”作为动词，四个句子使用“train”作为名词的情况。我们将这些句子提供给了一个在10亿单词的数据库上训练的标准ELMo模型，该模型将每个单词放入一个1024维的空间中（TensorFlow
    2020a）。我们提取了ELMo在每个句子中“train”一词的嵌入，并将其与所有其他句子中“train”一词的嵌入进行比较。尽管该词在每个句子中的书写方式相同，ELMo能够根据该词的上下文识别出正确的嵌入。'
- en: '![F20008](Images/F20008.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![F20008](Images/F20008.png)'
- en: 'Figure 20-8: Comparing ELMo’s embeddings of *train* resulting from its use
    in different sentences. Darker colors mean more similar embeddings.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-8：比较ELMo在不同句子中使用*train*时的嵌入。较深的颜色表示嵌入更相似。
- en: We usually place embedding algorithms like ELMo on their own layer in a deep
    learning system. This is often the very first layer in a language processing network.
    Our icon for an embedding algorithm, shown in [Figure 20-9](#figure20-9), is meant
    to suggest taking the space of words and placing it inside the larger embedding
    space.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将像ELMo这样的嵌入算法放置在深度学习系统中的单独层级。这通常是语言处理网络中的第一层。我们为嵌入算法设计的图标，如[图20-9](#figure20-9)所示，旨在表示将单词的空间放入更大的嵌入空间中。
- en: '![F20009](Images/F20009.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![F20009](Images/F20009.png)'
- en: 'Figure 20-9: Our icon for an embedding layer'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-9：我们为嵌入层设计的图标
- en: ELMo and other algorithms like it, such as the *Universal Language Model Fine-Tuning*,
    or *ULMFiT* (Howard and Ruder 2018), are typically trained on general-purpose
    databases, such as books and documents from the web. When we need them for some
    specific downstream task, such as medical or legal applications, we usually fine-tune
    them with additional examples from those domains. The result is a set of embeddings
    that include the specialized language of those fields, clustered by their special
    meanings in that jargon.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ELMo和其他类似的算法，如*通用语言模型微调*（Universal Language Model Fine-Tuning，简称*ULMFiT*）（Howard
    和 Ruder 2018），通常在通用数据库上进行训练，例如来自网页的书籍和文档。当我们需要它们来处理某些特定的下游任务时，如医学或法律应用，我们通常会用来自这些领域的额外示例对其进行微调。结果是，一组嵌入包含了这些领域的专业语言，按其在该术语中的特殊含义进行聚类。
- en: We’ll use embeddings in the systems we will build later in this chapter. Those
    networks will rely on the mechanism of attention, so let’s look at that now.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章稍后的系统构建中使用嵌入。这些网络将依赖于注意力机制，因此让我们现在来看一下这个机制。
- en: Attention
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力
- en: In Chapter 19 we saw how to improve translation by taking into account all of
    the words in a sentence. But when we’re translating a particular word, not every
    word in the sentence is equally important, or even relevant.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在第19章中，我们看到如何通过考虑句子中的所有单词来改善翻译。但当我们翻译一个特定的单词时，句子中的每个单词并不是同等重要的，甚至有些单词可能与之无关。
- en: For example, suppose we’re translating the sentence I saw a big dog eat his
    dinner. When we’re translating dog, we probably don’t care about the word saw,
    but to translate the pronoun his correctly may require us to connect that to the
    two words big dog.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们在翻译句子“I saw a big dog eat his dinner”（我看到一只大狗吃晚餐）时。在翻译“dog”时，我们可能不关心单词“saw”，但要正确翻译代词“his”可能需要我们将其与“大狗”这两个词联系起来。
- en: If we can work out, for each word in the input, which other words can influence
    our translation, then we can focus just on those words and ignore the others.
    This would be a big savings in both memory and computation time. And if we can
    work this out in a way that doesn’t depend on processing the words serially, we
    can even do it in parallel.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够为输入中的每个单词找出哪些其他单词能够影响我们的翻译，那么我们就可以只关注那些单词，忽略其他单词。这将大大节省内存和计算时间。而且，如果我们能够以一种不依赖于串行处理单词的方式来实现这一点，我们甚至可以并行处理。
- en: The algorithm that does this job is called *attention*, or *self-attention*
    (Bahdanau, Cho, and Bengio 2016; Sutskever, Vinyals, and Le 2014; Cho et al. 2014).
    Attention lets us focus our resources on only the parts of the input that matter.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这一任务的算法叫做*注意力*或*自注意力*（Bahdanau, Cho, 和 Bengio 2016；Sutskever, Vinyals, 和 Le
    2014；Cho 等人 2014）。注意力机制使我们能够将资源集中在输入中重要的部分。
- en: Modern versions of attention are often based on a technique called *query, key,
    value*, or simply *QKV*. These terms come from the field of databases and can
    seem somewhat obscure in this context. So we’ll describe the concepts using a
    different set of terms and then connect them back to query, key, and value at
    the end.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现代的注意力机制通常基于一种名为*查询、键、值*（*QKV*）的技术。这些术语来自数据库领域，在这个背景下可能显得有些晦涩。因此，我们将使用一组不同的术语来描述这些概念，并最终将其与查询、键和值连接起来。
- en: A Motivating Analogy
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个激发思考的类比
- en: Let’s begin with an analogy. Suppose that you need to buy some paint, but all
    you’ve been told is that the color should be “light yellow with a bit of dark
    orange.”
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个类比开始。假设你需要购买一些油漆，但你得到的唯一信息是颜色应该是“浅黄色带有一点深橙色”。
- en: At the only paint store in town, the only clerk on duty is new to the paint
    department and isn’t personally familiar with the colors. You both presume you’ll
    need to mix together a few of their standard paints to get the color you want,
    but you don’t know which paints to choose or how much of each to use.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在镇上唯一的油漆店里，唯一在职的店员刚刚加入油漆部门，对颜色不太熟悉。你们都假设你需要将几种标准油漆混合起来以得到你想要的颜色，但你不知道该选择哪些油漆，也不知道每种油漆的用量。
- en: The clerk suggests that you compare your desired color description with the
    color names on each can of paint they carry. Some names will probably match better
    than others. The clerk puts a funnel on top of an empty can and suggests that
    you pour in some of each can of paint on the shelves, guided by how well that
    can’s name matches your description. That is, you’ll compare your desired description
    “light yellow with a bit of dark orange” with what’s printed on the label of each
    can, and the better the match, the more of that paint you’ll pour into the funnel.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 店员建议你将你想要的颜色描述与每个油漆罐上的颜色名称进行比较。有些名称可能比其他名称匹配得更好。店员在一个空油漆罐上放了一个漏斗，并建议你根据每个油漆罐名称与描述的匹配度，将不同的油漆倒入漏斗中。也就是说，你将把你想要的描述“浅黄色带有一点深橙色”与每个油漆罐标签上的内容进行比较，匹配度越好，你就倒入更多这种油漆。
- en: '[Figure 20-10](#figure20-10) shows the idea visually for six cans of paint.
    It shows their names and the quality of each name’s match with your desired color’s
    description. We got good matches on “Sunny Yellow” and “Orange Crush,” though
    a little bit of “Lunch with Teal” snuck in thanks to the match with the word “with.”'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 20-10](#figure20-10)通过六个油漆罐形象地展示了这个概念。它展示了油漆罐的名称以及每个名称与所需颜色描述的匹配度。我们得到了“阳光黄色”和“橙色粉碎”的不错匹配，尽管由于与“with”一词的匹配，稍微混入了一些“午餐配青绿色”。'
- en: '![F20010](Images/F20010.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![F20010](Images/F20010.png)'
- en: 'Figure 20-10: Given a color description (left), we combine some of each can
    based on how well its name matches the description (middle), to get a final result
    (right).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-10：根据颜色描述（左），我们根据每个油漆罐名称与描述的匹配程度（中）来混合不同的油漆，最终得到结果（右）。
- en: 'There are three things to focus on in this story. First, there’s your *request*:
    “light yellow with a bit of dark orange.” Second, there’s the *description* on
    each can of paint, like “Sunny Yellow” or “Mellow Blue.” Third, there’s the *content*
    of the paint that’s actually inside each can. In the story, you compared your
    request with each can’s description to find out how well they match. The better
    the match, the more of that can’s content you used in the final mixture.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个故事中，有三件事需要关注。首先是你的*请求*：“浅黄色带有一点深橙色”。第二是每罐油漆上的*描述*，例如“阳光黄”或“温柔蓝”。第三是每罐油漆的*内容*，即罐内实际的油漆。在故事中，你将自己的请求与每罐油漆的描述进行比较，以了解它们的匹配度。匹配度越高，最终混合物中使用的该罐油漆的内容就越多。
- en: That’s attention in a nutshell. Given a request, compare it to the description
    of each possible item and include some of the content of each item based on how
    well its description matches the request.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是注意力机制的核心。给定一个请求，将其与每个可能项目的描述进行比较，并根据描述与请求的匹配程度，选择每个项目的一部分内容。
- en: The authors of the first paper on attention compared this process to a common
    type of transaction used with a database. In database language, we look something
    up by sending a *query* to a database. In such a process, every object in the
    database has a descriptive *key*, which can be different than the actual *value*
    of the object. Note that here the word *value* refers to the contents of the object,
    whether it’s a single number or something more complicated, such as a string or
    tensor.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第一篇关于注意力机制的论文的作者将这一过程与数据库中常见的一种事务类型进行了比较。在数据库的术语中，我们通过向数据库发送*查询*来查找某个内容。在这样的过程中，数据库中的每个对象都有一个描述性的*键*，它可以与对象的实际*值*不同。请注意，这里所说的*值*指的是对象的内容，无论它是一个单一的数字，还是更复杂的东西，例如字符串或张量。
- en: The database system compares the query (or request) with each key (or description)
    and uses that score to determine how much of the object’s value (or content) to
    include in the final result. So our terms of request, description, and content
    correspond to query, key, and value, or, more commonly, QKV.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库系统将查询（或请求）与每个键（或描述）进行比较，并使用该得分来决定最终结果中包含多少对象的值（或内容）。因此，我们的请求、描述和内容对应于查询、键和值，或更常见的术语，QKV。
- en: Self-Attention
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自注意力
- en: '[Figure 20-11](#figure20-11) shows the fundamental operation of attention in
    abstracted form. Here we have five words of input. Each of the three colored boxes
    represents a small neural network that takes the numerical representation of a
    word and transforms it into something new (often, these networks are each just
    a single fully connected layer). In this example, the word dog is the one we want
    to translate. So a neural network (in red) transforms the tensor for dog and turns
    it into a new tensor representing the query, Q. As the figure shows, two more
    small neural networks translate the tensor for dinner into new tensors, corresponding
    to its key, K (from the blue network) and its value, V (from the green network).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[图20-11](#figure20-11)以抽象形式展示了注意力机制的基本操作。这里有五个输入词。每个彩色框表示一个小型神经网络，该网络接收一个词的数值表示并将其转化为新的东西（通常这些网络每个只是一个单一的全连接层）。在这个例子中，词“dog”是我们想要翻译的词。所以，一个神经网络（红色）将“dog”的张量转换为一个新的张量，代表查询Q。正如图中所示，另外两个小型神经网络将“dinner”的张量翻译成新的张量，分别对应其键K（来自蓝色网络）和其值V（来自绿色网络）。'
- en: '![f20011](Images/f20011.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![f20011](Images/f20011.png)'
- en: 'Figure 20-11: The core step of attention using *dog* for the query to determine
    the relevance of the word *dinner*. Each box represents a small neural network
    that transforms its input into a query, key, or value.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-11：使用*dog*作为查询来确定单词*dinner*相关性的注意力核心步骤。每个框代表一个小型神经网络，它将输入转化为查询、键或值。
- en: In practice, we compare the query for dog against the key of every word in the
    sentence, including dog itself. For this illustration, we limit our focus to the
    comparison with the word dinner.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 实际操作中，我们将“dog”的查询与句子中每个单词的键进行比较，包括“dog”本身。在这个例子中，我们只关注与“dinner”一词的比较。
- en: We compare the query and the key to determine how alike they are. We do this
    with a little scoring function that we’re indicating with the letter *S* in a
    circle. Without getting into the math, this function compares two tensors and
    produces a single number. The more that the two tensors are like one another,
    the larger that number. The scoring function is usually designed to produce a
    number between 0 and 1, with larger values indicating a better match.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较查询和键，以确定它们的相似度。我们用一个小的评分函数来完成这一操作，函数表示为一个圆圈中的字母*S*。不深入讨论数学部分，这个函数比较两个张量并产生一个单一的数字。两个张量越相似，得出的数字就越大。评分函数通常设计为产生一个介于0和1之间的数字，较大的值表示更好的匹配。
- en: We use the output from the scoring function to scale the tensor representing
    the value for dinner. The more the query and the key match, the larger the output
    of the scaling step, and the more the value of dinner will make it into the output.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用评分函数的输出对表示晚餐的值的张量进行缩放。查询和键的匹配度越高，缩放步骤的输出越大，晚餐的值会更多地影响最终输出。
- en: Let’s see what it looks like when we apply this fundamental step to all the
    words in the input simultaneously. We’ll continue to look at translating the word
    dog. The overall result is the sum of the individual scaled values of all the
    input words. [Figure 20-12](#figure20-12) shows how this looks.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当我们将这个基本步骤同时应用到输入中的所有单词时会是什么样子。我们将继续观察翻译单词dog的情况。总体结果是所有输入单词的单独缩放值的总和。[图20-12](#figure20-12)展示了这种情况。
- en: '![F20012](Images/F20012.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![F20012](Images/F20012.png)'
- en: 'Figure 20-12: Using attention to simultaneously determine the contribution
    of all five words in the sentence to the word *dog*. The QKV spatial and color
    coding matches [Figure 20-11](#figure20-11). All data flows upward in the figure.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-12：使用注意力机制同时确定句子中所有五个单词对单词*dog*的贡献。QKV的空间和颜色编码与[图20-11](#figure20-11)相匹配。图中的所有数据都朝上流动。
- en: There are a few things to note in [Figure 20-12](#figure20-12). First, only
    three neural networks are involved—one each to compute the query, key, and value
    tensors. We use the same “input to query” network (in red in the figure) to turn
    each input into its query, the same “input to key” network (in blue in the figure)
    to turn each input into its key, and the same “input to value” network (in green
    in the figure) to turn each input into its value. We only need to apply these
    transformations once to each word.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图20-12](#figure20-12)中有几点需要注意。首先，只有三个神经网络参与——分别用于计算查询、键和值张量。我们使用相同的“输入到查询”网络（图中的红色部分）将每个输入转化为其查询，使用相同的“输入到键”网络（图中的蓝色部分）将每个输入转化为其键，使用相同的“输入到值”网络（图中的绿色部分）将每个输入转化为其值。我们只需要对每个单词应用一次这些转换。
- en: Second, there’s a dashed line after the scores and before the scaling of the
    values. This represents a softmax step applied to the scores, followed by a division.
    These two operations keep the numbers coming out of the scores from getting too
    big or small. The softmax also exaggerates the influence of close matches.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，得分后和数值缩放前有一条虚线。这代表了对得分应用的softmax步骤，之后是一个除法操作。这两个操作可以防止得分的数值变得过大或过小。softmax还会夸大相似项的影响。
- en: Third, we sum up all the scaled values to get a new tensor for dog, including
    that from the value of dog itself. We often find that each word scores most highly
    with itself. This isn’t a bad thing, as in this case, the most important word
    for translating dog is indeed dog itself. But there are times when other words
    will matter more. Some examples include when word order changes, when a word has
    no direct translation and must rely on other words, or when we’re trying to resolve
    a pronoun.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们将所有的缩放值加起来，得到dog的新张量，其中包括dog本身的值。我们常发现每个单词与自己的得分最高。这并不是坏事，因为在这种情况下，翻译dog时最重要的单词确实是dog本身。但有时其他单词会更加重要。一些例子包括单词顺序改变、某个单词没有直接翻译且必须依赖其他单词，或者我们正在试图解析代词时。
- en: The fourth important point is that we apply the processing of [Figure 20-12](#figure20-12)
    to all the words in the input sentence simultaneously. That is, each word is considered
    the query, and the whole process executes independently for that word, as shown
    in [Figure 20-13](#figure20-13).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个重要点是我们将[图20-12](#figure20-12)中的处理同时应用到输入句子中的所有单词。也就是说，每个单词都被视为查询，整个过程独立地为该单词执行，如[图20-13](#figure20-13)所示。
- en: '![F20013](Images/F20013.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![F20013](Images/F20013.png)'
- en: 'Figure 20-13: Applying attention to the other four words in our sentence'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图20-13：将注意力应用于句子中的其他四个单词
- en: 'Our fifth and last point is just an explicit recap of something we’ve been
    noting all along: all of this processing in [Figure 20-12](#figure20-12) and [Figure
    20-13](#figure20-13) together can be done in parallel in just four steps, regardless
    of the length of the sentence. Step 1 transforms the inputs into query, key, and
    value tensors. Step 2 scores all the queries and keys against one another. Step
    3 uses the scores to scale the values, and step 4 adds up the scaled values to
    produce a new output for each input.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: None of these steps depend on how long the input is, so we can process long
    sentences in the same amount of time required by short ones, as long as we have
    the memory and computing power needed.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'We call the process of [Figure 20-12](#figure20-12) and [Figure 20-13](#figure20-13)
    *self-attention*, because the attention mechanism is using the same set of inputs
    for computing everything: the queries, keys, and values. That is, we’re finding
    how much the input should be paying attention to itself.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: When we place self-attention in a deep network, we put it onto its own *self-attention
    layer*, often simply called an *attention layer*. The input is a list of words
    in numerical form, and the output is the same.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: The engines that power attention are the scoring function and the neural networks
    that transform the inputs into queries, keys, and values. Let’s consider them
    briefly.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: The scoring function compares a query to a key, returning a value from 0 to
    1, where the more the two values are similar, the higher their score. So somehow,
    the inputs that we think of as being similar need to have similar values going
    into the scoring function. Now we can see the practical value of embeddings. Recall
    our discussion of *A Tale of Two Cities*, in Chapter 19 where we assigned each
    word a number given by its order in the text. That gave the words keep and flint
    numbers 1,003 and 1,004 respectively. If we just compared these numbers, they
    would get a high similarity score. For most sentences, this is not what we want.
    If we’re using the query value for the verb keep, we usually want it to be similar
    to the keys for synonyms like retain, hold,  and reserve, and not at all like
    the keys for unrelated words like flint, preposterous, or dinosaur. Embeddings
    are the means by which similar words (or words used in similar ways) are given
    similar representations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Doing any necessary fine-tuning to the embeddings is the job of the neural networks,
    which transform the input words into representations where they can be meaningfully
    compared in the context of the sentence they’re used in. The only reason we have
    any chance of that is that the words are already embedded in a space where similar
    words are near one another.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, it’s the job of the network that turns inputs into values to represent
    those values in a way that allow them to be usefully scaled and combined. Mixing
    two embedded words gives us a word that’s somewhere between them.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Q/KV Attention
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the self-attention network of [Figure 20-12](#figure20-12), the queries,
    keys, and values are all derived from the same inputs, which led to the name self-attention.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: A popular variation uses one source for the queries and another for the keys
    and values. This more closely matches our paint store analogy, where we came in
    with the query, and the store had the keys and values. We call this variation
    a *Q/KV* network, where the slash indicates that the queries come from one source,
    and the keys and values from another. This version is sometimes used when we add
    attention to a network like seq2seq, where the queries come from the encoder,
    and the keys and values from the decoder, so it’s sometimes also called an *encoder-decoder
    attention* layer. The structure is shown in [Figure 20-14](#figure20-14).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![F20014](Images/F20014.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20-14: A Q/KV layer is like self-attention as shown in [Figure 20-12](#figure20-12),
    except that the queries don’t come from the inputs.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Head Attention
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea of attention is to identify words that are alike and create a useful
    mix of them. But words can be considered alike based on many different metrics.
    We might consider nouns to be alike, or colors, or spatial ideas like up and down,
    or temporal ideas like yesterday and tomorrow. Which of these is the best choice?
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there is no one best answer. In fact, we often want to compare words
    using multiple criteria at once. For instance, when writing song lyrics, we may
    want to assign high scores to pairs of words that have similar meanings, similar
    sounds in their last syllable, the same number of syllables, and the same stress
    pattern in the syllables. When writing about sports, we might instead want to
    say that players on the same teams and with the same roles are like one another.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: We can score words along multiple criteria by simply running multiple independent
    attention networks simultaneously. Each network is called a *head*. By initializing
    each head independently, we hope that during training, each head will learn to
    compare the inputs according to criteria that are simultaneously useful and different
    from those used by the other layers. If we want, we can add additional processing
    to explicitly encourage different heads to attend to different aspects of the
    inputs. The idea is called *multi-head attention*, and we can apply it to both
    self-attention networks like [Figure 20-12](#figure20-12) and Q/KV networks like
    [Figure 20-14](#figure20-14).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Each head is a distinct attention network. The more heads we have, the more
    different aspects of the input they can focus on.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: A diagram for a multi-head attention layer is shown in [Figure 20-15](#figure20-15).
    As the figure shows, we usually combine the outputs of the heads into a list and
    run that through a single fully connected layer. This allows the entire multi-head
    network’s output to have the same shape as its input. This approach makes it easy
    to place multiple multi-head networks one after another.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-15展示了一个多头注意力层的示意图。如图所示，我们通常将各个头的输出组合成一个列表，并将其通过一个单一的全连接层。这使得整个多头网络的输出与其输入具有相同的形状。这种方法使得将多个多头网络串联在一起变得容易。
- en: '![F20015](Images/F20015.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![F20015](Images/F20015.png)'
- en: 'Figure 20-15: A multi-head attention layer. A box with a diamond inside is
    our icon for an attention layer.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-15：多头注意力层。一个带菱形的框是我们用来表示注意力层的图标。
- en: Attention is a general concept that we can apply in different forms to any kind
    of deep network. For example, in a CNN we can scale a filter’s outputs to emphasize
    the values produced in response to the most relevant locations in the input (Liu
    et al. 2018; H. Zhang et al. 2019).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力是一种通用概念，我们可以将其以不同形式应用于任何类型的深度网络。例如，在CNN中，我们可以缩放滤波器的输出，以强调响应输入中最相关位置的值（Liu等人，2018；H.
    Zhang等人，2019）。
- en: Layer Icons
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层图标
- en: '[Figure 20-16](#figure20-16) shows our icons for the different types of attention
    layers. Multi-head attention is drawn as a little 3D box, suggesting a stack of
    attention networks. For Q/KV attention, we place a short line inside the diamond
    to identify the Q inputs and bring in the K and V inputs on an adjacent side.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 20-16](#figure20-16)展示了我们为不同类型的注意力层设计的图标。多头注意力被绘制成一个小的3D框，暗示着一堆注意力网络。对于Q/KV注意力，我们在菱形内部放置一条短线，以标识Q输入，并将K和V输入放置在相邻的侧面。'
- en: '![F20016](Images/F20016.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![F20016](Images/F20016.png)'
- en: 'Figure 20-16: Attention layer icons. (a) Self-attention. (b) Multi-head self-attention.
    (c) Q/KV attention. (d) Multi-head Q/KV attention.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20-16：注意力层图标。（a）自注意力。（b）多头自注意力。（c）Q/KV注意力。（d）多头Q/KV注意力。
- en: Transformers
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer
- en: Now that we have embedding and attention, we’re ready to make good on our earlier
    promise to improve on RNNs.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了嵌入和注意力，准备好兑现我们之前承诺的改进RNN的目标。
- en: Our goal is to build a translator based not on RNNs, but on attention networks.
    The key idea is that the attention layers will learn how to transform our inputs
    into their translations, based on the relationships between words.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是构建一个基于注意力网络而非RNN的翻译器。关键思想是，注意力层将学习如何根据词与词之间的关系将我们的输入转换为其翻译。
- en: This approach first appeared in a paper with the great title, “Attention Is
    All You Need” (Vaswani et al. 2017). The authors called their attention-based
    model a *transformer* (an unfortunately ambiguous name, but it’s now firmly stuck
    in the language of the field). The transformer model works so well that we now
    have a new class of language models that not only can be trained in parallel,
    but also can outperform RNNs in a wide variety of tasks.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法首次出现在一篇标题为《Attention Is All You Need》（Vaswani等人，2017）的论文中。作者将他们基于注意力的模型称为*transformer*（这个名字不幸地有些模糊，但现在已经牢牢地成为该领域的术语）。transformer模型效果如此优秀，以至于我们现在拥有了一类新的语言模型，它们不仅可以并行训练，而且在广泛的任务中可以超越RNN。
- en: Transformers use three more ideas we haven’t discussed yet. Let’s cover them
    now, so when we get to the actual transformer architecture, it will be smooth
    sailing.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer使用了我们尚未讨论的三个新概念。让我们现在来介绍它们，这样当我们真正进入transformer架构时，就能顺利进行。
- en: Skip Connections
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跳跃连接
- en: The first new idea we cover is called a *residual connection* or *skip connection*
    (He et al. 2015). The inspiration is to reduce the amount of work that’s required
    of a deep network layer.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要介绍的第一个新概念被称为*残差连接*或*跳跃连接*（He等人，2015）。其灵感来自于减少深度网络层所需的工作量。
- en: Let’s start with an analogy. Suppose you’re painting a real, physical portrait
    using acrylic paints on canvas. After weeks of sittings, the portrait is done,
    and you send it to your subject for their approval. They say that they like it,
    but they regret having worn a particular ring on one finger, and wish they’d worn
    a different one that they like more. Can you change that?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个类比开始。假设你正在用丙烯画颜料在画布上画一幅真实的物理肖像画。在经过数周的坐姿之后，肖像完成了，你将它送给你的模特以供审批。他们说他们喜欢这幅画，但他们后悔自己在某个手指上戴了一个特定的戒指，应该戴另一个他们更喜欢的戒指。你能改变它吗？
- en: One way to proceed would be to invite your subject back to the studio and paint
    a whole new portrait from scratch on a blank canvas, only this time with the new
    ring on their finger. That would require a lot of time and effort. If they’d allow
    it, a more expeditious approach would be to take the portrait you have, and unobtrusively
    paint the new ring over the old one.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Now consider a layer in a deep network. A tensor comes in, and the layer does
    some processing to change that tensor. If the layer only needs to change the input
    by small amounts, or only in some places, then it would be wasteful to expend
    resources processing the parts of the tensor that don’t need to change. Just as
    with the painting, it would be much more efficient for the layer to compute only
    the changes it wants to make. Then it can combine those changes with the original
    input to produce its output.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: This idea works beautifully in deep learning networks. It lets us make layers
    that are smaller and faster, and it even improves the flow of gradients in backpropagation,
    which lets us efficiently train networks of dozens or even hundreds of layers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: The mechanism is shown on the left side of [Figure 20-17](#figure20-17). We
    feed an input tensor to some layer as usual, let it compute the changes, and then
    we add the layer’s output to its input tensor.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![F20017](Images/F20017.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20-17: Left: A skip connection, shown in red. Right: We can place a
    skip connection around multiple layers.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The extra line in the drawing that carries the input to the addition node is
    called a *skip connection*, or a *residual connection* because of its mathematical
    interpretation.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: We can place a skip connection around multiple layers in sequence, if we like,
    as on the right of [Figure 20-17](#figure20-17).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: The skip connection works because each layer is trying to reduce its own contribution
    to the final error, while participating in the network made up of all the other
    layers. The skip connection is part of the network, so the layer learns it doesn’t
    need to process the parts of the tensor that don’t change. This makes the layer’s
    job simpler, enabling it to be smaller and faster.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: We’ll see later that transformers use skip connections not just for efficiency
    and speed, but also because they allow the transformer to cleverly keep track
    of the location of each element in its input.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Norm-Add
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second idea on our road to transformers is really more of a conceptual and
    notational shorthand. In transformers, we usually apply a regularization step
    called *layer normalization*, or *layer norm*, to the outputs of a layer, as shown
    on the left in [Figure 20-18](#figure20-18) (Vaswani et al. 2017). Layer norm
    belongs to the class of regularization techniques that we saw in Chapter 15, such
    as dropout and batchnorm, which help control overfitting by keeping the values
    flowing through the network from getting too big or too small. The layer norm
    step learns to adjust the values coming out of a layer so that they approximate
    the shape of a Gaussian bump with a mean of 0 and standard deviation of 1.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![F20018](Images/F20018.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20-18: Left: A layer normalization followed by the addition step of
    a skip connection. Right: A combined icon for norm-add. This is just a visual
    and conceptual shorthand for the network on the left.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Performing layer norms is important in getting a transformer to work well, but
    there’s some flexibility about exactly where this step can be located. A popular
    approach places the layer norm just before the addition step of a skip connection,
    as in the left side of [Figure 20-18](#figure20-18). Since these two operations
    always come in pairs, it’s convenient to combine them into a single operation
    that we call *norm-add*. Our icon for norm-add is a combination of the layer norm
    and summation icons, and is shown on the right in [Figure 20-18](#figure20-18).
    This is just a visual shorthand for the two separate steps of layer norm followed
    by skip connection addition.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: People have experimented with other locations for the layer norm operation,
    such as before the layer (Vaswani et al. 2017), or after the addition node (TensorFlow
    2020b). These approaches differ in their details, but in practice, it seems that
    all of these choices are comparable. We’ll stick with the version in [Figure 20-18](#figure20-18)
    here.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Positional Encoding
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The third idea to cover before we get to transformers was designed to solve
    a problem that comes up as soon as we take RNNs out of our system: we lose track
    of where each word is located in the input sentence. This important information
    is inherent in the RNN structure, because the words come in one at a time, allowing
    the hidden state inside a recurrent cell to remember the order in which the words
    arrived.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: But as we’ve seen, attention mixes together the representations of multiple
    words. How can later stages know where each word belongs in the sentence?
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The answer is to insert each word’s position, or index, into the representation
    for the word itself. That way, as the word’s representations get processed, the
    position information naturally comes along for the ride. The generic name for
    this process is *positional encoding*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: A simple approach to positional encoding is to append a few bits to the end
    of each word to hold its location, as shown on the left of [Figure 20-19](#figure20-19).
    But at some point, we might get a sentence that requires more bits than we’ve
    made available, and then we’d be in trouble because we wouldn’t be able to assign
    each word a unique number for its location. And if we make the storage too big,
    it’s just wasted and slows everything down. This approach is also awkward to implement,
    since we then need to introduce some special mechanism for handling those bits
    (Thiruvengadam 2018).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![F20019](Images/F20019.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20-19: Tracking the location of each word in a sentence. Left: Appending
    an index to each word. Middle: Using a function F to turn each index into a vector,
    then adding it to the word’s representation. Right: Our icon for a positional
    embedding layer.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: A better answer is to use a mathematical function that creates a unique vector
    for each position in a sequence. Suppose that our word embeddings are 128 elements
    long. Then we give this function the index of each word (which can be as large
    as it needs to be), and the function gives us back a new 128-element vector that
    somehow describes that location. Basically it turns the index into a unique list
    of values. Our expectation is that the network will learn to associate each of
    these lists with the word’s position in the input.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Rather than appending this vector to the word’s representation, we add the two
    vectors together, as in the middle of [Figure 20-19](#figure20-19). Here we literally
    add the number in each element in the encoding to the corresponding number in
    the word’s embedding. The appeal of this approach is that we don’t need any extra
    bits or special processing. This form of positional encoding is called *positional
    embedding*, because of its similarity to the *word embedding* we saw earlier in
    algorithms like ELMo. The right side of the figure shows our icon for this process,
    which is drawn with a little sine wave because a popular choice for the embedding
    function is based on sine waves (derived from Vaswani et al. 2017).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: It may seem a bit weird to add position information to each word, rather than
    append it, as it changes the word’s representation. It also seems that the position
    information is liable to get lost as the attention network processes the values.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that the specific function that is frequently used to compute the
    positional embedding vector usually affects only a few bits at one end of the
    word’s vector (Vaswani et al. 2017; Kazemnejad 2019). Furthermore, it appears
    that transformers learn how to distinguish each word’s representation and position
    information during processing so they’re interpreted separately (TensorFlow 2019a).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: But why doesn’t the position embedding get lost altogether during processing?
    After all, attention changes its inputs using neural networks by turning them
    into QKV values and then mixing those values. Surely the positional information
    would be hopelessly scrambled and lost.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The clever solution to this problem is built into the architecture of the transformer
    itself. As we’ll see, the transformer network wraps up each operation (except
    the very last) in a skip connection. The embedding information never gets lost,
    because it gets added back in after every stage of processing. [Figure 20-20](#figure20-20)
    illustrates how positional embedding and norm-add skip connections are structurally
    similar. In short, each layer can change its input vector in any way it wants,
    and then the positional embedding gets added back in so that it’s available to
    the next layer.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![F20020](Images/F20020.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20-20: Left: Creating a position embedding and adding it to a word.
    Right: A norm-add operation implicitly adds a word’s embedding information back
    in after processing.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Assembling a Transformer
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now have all the pieces in place to build a transformer. We’ll continue to
    use word-level translation as our running example.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the name *transformer* refers to a wide variety
    of networks inspired by the architecture in the original transformer paper (Vaswani
    et al. 2017). In this discussion, we’ll stick to a generic version.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Our block diagram of a transformer is shown in [Figure 20-21](#figure20-21).
    The blocks marked *E* and *D* are repeated sequences of layers, or *blocks*, built
    around attention layers. We’ll look at both types of block in detail in a moment.
    The big picture is that an encoder stage (built from *encoder blocks*, marked
    with an *E*) accepts a sentence, and a decoder (build from *decoder blocks*, marked
    with a *D*) accepts information from the encoder and produces new output (the
    structure of this diagram is reminiscent in some ways of an unrolled seq2seq diagram,
    but there are no recurrent cells here).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![F20021](Images/F20021.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20-21: A block diagram of a transformer. An input is encoded and then
    decoded. The decoder’s output is fed back to its input autoregressively. The dashed
    lines stand for repeated elements.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Both the encoder and decoder begin with word embedding followed by positional
    embedding. The decoder has the usual fully connected layer and softmax at the
    end for predicting the next word. The decoder is autoregressive, so it appends
    each output word to the list of its outputs (shown by the box at the bottom of
    the figure), and that list becomes the decoder’s input for generating the next
    word. The decoder contains multi-head Q/KV attention networks, as in [Figure 20-14](#figure20-14),
    which receive their keys and values from the outputs of the encoder blocks, shown
    in the middle of [Figure 20-21](#figure20-21), where the encoder outputs are delivered
    to the decoder blocks. This illustrates why Q/KV attention is also called encoder-decoder
    attention.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look more closely at the blocks in [Figure 20-21](#figure20-21) starting
    with the encoder block, shown in [Figure 20-22](#figure20-22).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![F20022](Images/F20022.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20-22: The transformer’s encoder block. The first layer is self-attention.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The encoder block begins with a layer of multi-head self-attention, shown here
    with eight heads. Because this layer applies self-attention, the queries, keys,
    and values are all derived from the single set of inputs that arrive at the block.
    This multi-head attention is surrounded by a norm-add skip connection to help
    keep the numbers looking like a Gaussian and to retain the positional embeddings.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: This is followed by two layers that are usually referred to collectively as
    a *pointwise feed-forward layer* (another unfortunately vague name). Though the
    original transformers paper described these as a pair of modified fully connected
    layers (Vaswani et al. 2017), we can more conveniently think of them as two layers
    of 1×1 convolution (Chromiak 2017; Singhal 2020; A. Zhang et al. 2020). They learn
    how to adjust the output of the multi-head attention layer to remove redundancy
    and focus on just the information that will be of the most value to whatever processing
    comes next. The first convolution uses a ReLU activation function, while the second
    has no activation function. As usual, these two steps are wrapped in a norm-add
    skip connection.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at the decoder block, shown in [Figure 20-23](#figure20-23).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![F20023](Images/F20023.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20-23: The transformer’s decoder block. Note that the first attention
    layer is self-attention, whereas the second is Q/KV attention. The triangle on
    the left of the self-attention layer indicates that the layer uses masking.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, it looks a lot like the encoder block, with an extra step of
    attention. Let’s walk through the layers.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: We begin with a multi-head self-attention layer, just like the encoder block.
    The input to the layer is the words output so far by the transformer. If we’re
    just beginning, this sentence contains only the `[START]` token. Like any self-attention
    layer, the purpose here is to look at all of the input words and work out which
    ones are most strongly related to which others. As usual, this is wrapped in a
    skip connection with a norm-add node at the end. During training, we add an extra
    detail called *masking* to this self-attention step (indicated with a small triangle
    in [Figure 20-23](#figure20-23)), which we’ll come back to shortly.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: The self-attention layer is followed by a multi-head Q/KV attention layer. The
    query, or Q, vectors come from the output of the previous self-attention layer.
    The keys and values come from the concatenated outputs of all the encoder blocks.
    This layer also is wrapped in a skip connection with a norm-add node at the end.
    This stage uses the outputs of the previous attention network to choose among
    the keys coming from the encoder and then mix the values corresponding to those
    keys. Finally, we have a pair of 1×1 convolutions, following the same pattern
    as in the encoder block.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: We can now put the pieces together. [Figure 20-24](#figure20-24) shows the structure
    of a transformer model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![F20024](Images/F20024.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20-24: A complete transformer. The icons showing two stacked boxes represent
    two consecutive 1×1 convolutions. The dashed lines stand for repeated elements
    that are not drawn.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: We promised to return to a detail regarding the first attention layer in each
    decoder block. As we mentioned, one of the great values of the attention mechanism
    at the heart of the transformer is that it allows for a lot of parallelism. Whether
    an attention block is given five words or five hundred, it runs in the same amount
    of time.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we’re training the system to predict the next word in a sentence. We
    can provide it with the entire sentence and ask it to predict the first word,
    the second word, the third word, and so on, all in parallel.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: But there’s a problem here. Suppose the sentence is My dog loves taking long
    walks. We could give the system My dog loves taking long, and ask it to predict
    the sixth word, walks. But because we’re training in parallel, we want it to use
    this same input to predict each of the previous words, at the same time. That
    is, we also want it to predict the fifth word, long, from the input My dog loves
    taking long.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s too easy: the word long is right there! The system would find that all
    it has to do is return the fifth word, which is definitely not the same as learning
    how to predict it. We want to give the system My dog loves taking long as input,
    but for predicting the fifth word, it should only see My dog loves taking. We
    want to hide, or mask, the word long when we’re trying to predict it. Similarly,
    to predict the fourth word, it should only see My dog loves, to predict the third
    word it should only see My dog, and so on.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: In short, our transformer will run five parallel computations, each predicting
    a different word, but each computation should only be given the words that came
    before the one it’s supposed to predict.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: The mechanism to pull this off is called *masking*. We add an extra step to
    the first self-attention layer in the decoder block that masks, or hides, the
    words that each prediction step isn’t supposed to see. Thus the computation predicting
    the first word sees no input words, the computation predicting the second word
    only sees My, the one predicting the third word only sees My dog, and so on. Because
    of this extra step, the first attention layer in the decoder block is sometimes
    called a *masked multi-head self-attention* layer, which is a mouthful, so we
    often just refer to it as a *masked attention* layer.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Transformers in Action
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s see a transformer in action performing a translation. We trained a transformer
    following roughly the architecture of [Figure 20-24](#figure20-24) to translate
    from Portuguese to English (TensorFlow 2019b). We used a dataset of 50,000 training
    examples, which is small by today’s standards but good enough to demonstrate the
    ideas while also of a practical size to train from on a home computer (Kelly 2020).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: We gave our trained transformer the Portuguese question, você se sente da mesma
    maneira que eu? which Google Translate renders into English as do you feel the
    same that way I do? Our system produced the translation, do you see , do you get
    the same way i do ? This isn’t perfect, but given the small training database,
    it does a great job of capturing the spirit of the question. As always, more training
    data and training time would surely improve the results.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Heatmaps showing the attention paid to each input word by each output word,
    for each of the eight heads in the final Q/KV attention layer of the decoder,
    are shown in [Figure 20-25](#figure20-25). The brighter the cell, the more attention
    was paid. Note that some input words were broken up into multiple tokens by a
    preprocessor.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Transformers trained on larger datasets than this example, and for longer periods,
    can produce results that are as good or better than RNNs, and they can be trained
    in parallel. They don’t need recurrent cells with finite internal states that
    can run out of memory, nor do they need multiple neural networks to learn how
    to control those states. These are big advantages and explain why transformers
    have replaced RNNs in many applications.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![F20025](Images/F20025.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20-25: Heatmaps for each of the eight heads in the final Q/KV attention
    layer of the decoder during a translation of “*Você se sente da mesma maneira
    que eu?*” from Portuguese to English'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: One downside of transformers is that the memory required by the attention layers
    grows dramatically with the size of the input. There are ways to adjust the attention
    mechanism, and the transformer in general, to reduce these costs in different
    situations (Tay et al. 2020).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: BERT and GPT-2
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The full transformer model of [Figure 20-24](#figure20-24) consists of an encoder,
    which is designed to analyze the input text and create a series of context vectors
    that describe it, and a decoder, which uses that information to autoregressively
    generate a translation of the input.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The blocks making up the encoder and decoder are not specific to translation.
    Each is just one or more attention layers, followed by a pair of 1×1 convolutions.
    These blocks can be used as general-purpose processors for working out the relationship
    between elements of a sequence, and language in particular. Let’s look at two
    recent architectures that have used transformer blocks in ways that go way beyond
    translation.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: BERT
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s use transformer blocks to create a general-purpose language model. It
    can be used for any of the tasks we listed at the start of Chapter 19.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: The system is called *Bidirectional Encoder Representations from Transformers*,
    but it’s more commonly known by its acronym, *BERT* (Devlin et al. 2019) (another
    Muppet from *Sesame Street*, and a nodding reference to the ELMo system we saw
    earlier). The structure of BERT begins with a word embedder and a position embedder,
    followed by multiple transformer encoder blocks. The basic architecture is shown
    in in [Figure 20-26](#figure20-26) (in practice, other details help with training
    and performance, such as dropout layers). In this diagram, we’re showing the many
    inputs and outputs so that it’s clear that BERT is processing an entire sentence.
    For consistency and clarity, we’re only using a single line inside the blocks,
    but the parallel operations are still being carried out. It’s traditional to draw
    BERT diagrams with a yellow color scheme, since Bert on *Sesame Street* is a yellow
    character.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![F20026](Images/F20026.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20-26: The basic structure of BERT. The dashed lines stand for more
    encoder blocks that are not drawn.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: The original “large” version of BERT deserved its name, with 340 million weights,
    or parameters. The system was trained on Wikipedia and over 10,000 books (Zhu
    et al. 2015). Currently, 24 trained versions of the original BERT system are available
    freely online (Devlin et al. 2020), as well as a growing number of variations
    and improvements on the basic approach (Rajasekharan 2019).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: BERT was trained on two tasks. The first is called *next sentence prediction*,
    or *NSP*. In this technique, we give BERT two sentences at once (with a special
    token to separate them), and we ask it to determine if the second sentence reasonably
    follows the first. The second task presents the system with sentences where some
    of the words have been removed, and we ask it to fill in the blanks (language
    educators call this the *cloze task;* Taylor 1953). It’s the linguistic analog
    of the visual process called *closure*, describing the human tendency to fill
    in the blanks in images. Closure is illustrated in [Figure 20-27](#figure20-27).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![F20027](Images/F20027.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20-27: Demonstrating the principle of closure. Incomplete shapes like
    these are usually filled in by the human visual system to create objects.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: BERT is able to do well on these tasks because, compared to the RNN-based methods
    we saw before, BERT’s attention layers extract much more information from their
    inputs. Our first RNN models were *unidirectional*, reading inputs left to right.
    Then they became *bidirectional*, culminating in ELMo, which can be said to be
    *shallowly bidirectional*, where *shallow* refers to the architecture’s use of
    only two layers in each direction. Thanks to attention, BERT is able to determine
    the influence of every word on every other word, and by repeating the encoder
    block, it can do this many times in a row. BERT is sometimes called *deeply bidirectional*,
    but it might be more useful to think of it as *deeply dense*, since it considers
    every word simultaneously. The notion of direction really doesn’t apply when we’re
    using attention.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take BERT out for a spin. We’ll start with a pretrained model of 12 encoder
    blocks (McCormick and Ryan 2020). We’ll fine-tune it to determine if an input
    sentence is grammatical or not (Warstadt, Singh, and Bowman 2018; Warstadt, Singh,
    and Bowman 2019). This is basically a classification problem, producing a yes/no
    answer. Therefore, our downstream model should be a classifier of some kind. Let’s
    use a simple classifier consisting of a single fully connected layer. Our combined
    pair of models is shown in [Figure 20-28](#figure20-28).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![F20028](Images/F20028.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20-28: BERT with a small downstream classifier at the end. The dashed
    lines stand for the 10 additional, identical encoder blocks that are present,
    but not drawn.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: After four epochs of training, here are six results from the testing data. The
    first three are grammatical, and the second three are not. BERT produced the correct
    answer on all six.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Chris walks, Pat eats broccoli, and Sandy plays squash.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There was some particular dog who saved every family.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Susan frightens her.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The person confessed responsible.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cat slept soundly and furry.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The soundly and furry cat slept.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the test set of about 1,000 sentences, this little version of BERT got about
    82 percent of the examples correct. Some BERT variants have achieved more than
    88 percent right on this task (Wang et al. 2019; Wang et al. 2020).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try BERT out on another task, called *sentiment analysis*. We’ll classify
    short movie reviews as being either positive or negative in tone. The data comes
    from a database of almost 7,000 movie reviews called *SST2*, where each review
    has been labeled as positive or negative (Socher et al. 2013a; Socher et al. 2013b).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: For this run, we used a pretrained BERT model called DistillBERT (Sanh et al.
    2020; Alammar 2019) (the term *distilling* is often used when we carefully trim
    a trained neural network to make it smaller and faster without losing much performance).
    We’re again doing a classification task, so we can reuse the model of [Figure
    20-28](#figure20-28).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Here are six examples verbatim from the test data (there’s no indication of
    what movies they each refer to). DistillBERT properly classified the first three
    reviews as positive and the second three as negative (the reviews are all lowercase,
    and commas are treated as their own tokens).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: a beautiful , entertaining two hours
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this is a shrewd and effective film from a director who understands how to create
    and sustain a mood
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a thoroughly engaging , surprisingly touching british comedy
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the movie slides downhill as soon as macho action conventions assert themselves
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a zombie movie in every sense of the word mindless , lifeless , meandering ,
    loud , painful , obnoxious
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is that rare combination of bad writing , bad direction and bad acting the
    trifecta of badness
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of the 1,730 reviews in the test set, DistillBERT correctly predicted the sentiment
    of about 82 percent of them.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: To recap, models based on the BERT architecture are united by their use of a
    sequence of encoder blocks. They create an embedding of a sentence that captures
    enough information that downstream applications can perform a wide range of operations
    upon it. With an appropriate downstream model, BERT can be used to perform many
    of the NLP tasks we mentioned at the start of Chapter 19.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: If we’re willing to get clever, we can make BERT generate language, but it’s
    not easy (Mishra 2020; Mansimov et al. 2020). A better solution is to use decoder
    blocks, as we’ll see next.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve seen how transformers use a series of decoder blocks to generate words
    for a translation. We can also use a sequence of decoder blocks to generate new
    text.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Since we don’t have an encoder stage to receive KV values from, as in the full
    transformer of [Figure 20-24](#figure20-24), let’s remove the Q/KV multi-head
    attention layer from each decoder block, leaving us with just masked self-attention
    and a pair of 1×1 convolutions. The first system to do this in a big way was called
    the *Generative Pre-Training model 2*, or simply *GPT-2* (Radford et al. 2019).
    Its architecture is shown in [Figure 20-29](#figure20-29).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![F20029](Images/F20029.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20-29: A block diagram of GPT-2, made out of transformer decoder blocks
    without the Q/KV layer. The dashed lines stand for more repeated, identical decoder
    blocks. Note that because these are versions of decoder blocks, the first multi-head
    attention layer in each block is a masked attention layer.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Like BERT, we start with token embedding followed by positional embedding for
    each input word. The self-attention layer in each decoder block uses masking as
    before so that as we compute attention for any given word, we can only use information
    from that word and those that precede it.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The original GPT-2 model was released in several different sizes, the largest
    of which processed 512 tokens at a time through 48 decoder blocks with 12 heads
    in each, for a total of 1,542 million parameters. That’s 1.5 *billion* parameters.
    GPT-2 was trained on a dataset called *WebText*, which contained about eight million
    documents for a total of about 40GB of text (Radford et al. 2019).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: We typically use GPT-2 by starting with the pretrained model, and then we fine-tune
    it by providing an additional dataset to learn from, adjusting all of the weights
    in the process (Alammar 2018).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: We started each of our text generators in Chapter 19 with a seed, but that’s
    only one way to get them started. A simpler approach starts the system with general
    guidance and a prompt. This is called a *zero-shot* scenario, since the system
    has been given zero “shots,” or examples, for it to use as a model for new text.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, suppose we built a system to advise us on what to wear each day.
    A zero-shot scenario might start with the instruction, Describe today’s outfit,
    followed by the prompt, Today I should wear: The generator takes it from there.
    It has no examples or context to work from, so it might suggest a suit of armor,
    a spacesuit, or a bear skin.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can provide one or more examples, or shots. In a one-shot
    scenario, we might give the instruction Describe today’s outfit, followed by the
    example, Yesterday I wore a blue shirt and black pants, and conclude with the
    prompt, Today I should wear: The thinking is that the text that’s provided before
    the prompt can help guide the system into the kind of output we want. In this
    case, the bear skin would be less likely.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: If we give the system two or three shots, but not many more, we usually call
    it a *few-shot* scenario (these terms don’t have sharp cutoffs). People usually
    prefer generators that require as few shots as possible in order to provide the
    output we want.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see GPT-2 in action, using a medium-sized, pretrained GPT-2 model (von
    Platen 2020). We won’t do any fine tuning, so the system will generate text based
    only on its core training data. Let’s take a zero-shot approach, and give it no
    information except the starting prompt, I woke up this morning to the roar of
    a hippopotamus. Here’s a typical output, verbatim:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: I woke up this morning to the roar of a hippopotamus. I was in the middle of
    a long walk, and I saw a huge hippopotamus. I was so excited. I was so excited.
    I was so excited. I was so excited.
  id: totrans-253
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: At this point the system kept repeating I was so excited endlessly. That’s not
    the generator’s fault, it’s ours. The system is producing grammatical output,
    which is what we trained it to do. The problem with the output is that, despite
    its emphasis on excitement, it’s boring. The end of one sentence happened to lead
    back to the start of that same sentence, and we got locked in a loop. The system
    as it is now has no idea that such output is boring or undesirable.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: To make output more interesting, we can chip away at the problem, removing characteristics
    of the output we see as undesirable. Let’s look at just two such changes (Vijayakumar
    et al. 2018; Shao et al. 2017).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s do away with that repetition. We can penalize the system if it
    generates the same group of words repeatedly. This is called an *n-gram penalty*
    because a sequence of *n* words is called an *n-gram* (Paulus, Xiong, and Socher
    2017; Klein et al. 2017). Let’s take it easy on the system and only punish repeated
    pairs of words, or 2-grams.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Second, rather than always picking the word with the highest probability, let’s
    pick several of them and see where they lead. That is, for each word, we will
    generate more words following on from that. When we’ve followed several paths
    for a while, we will score each of them to determine which one has the highest
    probability of being generated. We use the first word of that branch as our predicted
    word, and throw away the rest of that branch and all of the other branches. Then
    we repeat the process for the next word, and the next, and so on. This is called
    a *beam search* (Russell and Norvig 2009). This should give us longer sequences
    that hang together. And instead of always choosing the very highest probability
    word, we’ll pick among several of the most probable words, according to their
    probabilities, which will help make the output less predictable. This is called
    increasing the *temperature* of the word selection process (a temperature of zero
    corresponds to always picking the most probable choice).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'With those two changes in place, let’s try generating text again using the
    same starting sentence. Here is one verbatim result:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: I woke up this morning to the roar of a hippopotamus. “Oh, my God,” I said.
    “I’m so sorry. I didn’t mean to hurt you. It’s just that I don’t know how to deal
    with it. You know what I mean? I’m not going to let you hurt me, okay? You’re
    my friend, and I love you so much. And I want you to know that you’re not alone
    in this. We’ve been together for a long time now, so it’s not like there’s anything
    we can do about it, but I can’t help but feel sorry for you, because I know you’ll
    never be alone with me again.
  id: totrans-259
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Wow. That’s remarkable. We could keep improving the output by using more processing
    techniques (Welleck et al. 2020), but this is already pretty great, particularly
    considering that we’re using a stripped-down GPT-2 model.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: For comparison, let’s revisit the same Sherlock Holmes stories from Chapter
    19\. We started with a small, pre-trained GPT-2, fine-tuned it on these stories
    (Woolf 2019), and then gave it the prompt, I asked Holmes to explain.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: We used a slightly warm temperature, so each time we ran the algorithm, we got
    a different result. Here’s just one, which we arbitrarily cut off after a few
    paragraphs.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: I asked Holmes to explain.
  id: totrans-263
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-264
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “It is very simple really,” he replied. “The short answer is, of course, that
    the law does not apply to these matters, and that the case should be left entirely
    in the discretion of the Crown Prosecution and the police. But the long answer
    is that the case clearly does not rest with us, and that it is very much in the
    power of the police to prevent it.
  id: totrans-265
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-266
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “You remember, Watson, that it was upon this very day that the young man, Mr.
    Latimer, picked up that long revolver which he used to brand me. The revolver
    which he used to brand me, is probably the best known weapon in the world. You
    remember, Watson, that I said at the time that it was better not to know than
    to hate the fear of it. Now, we have heard, and we must not let ourselves be drawn
    into a dilemma. The matter has been sufficiently complicated, and it is not necessary
    that the facts should be remarked upon in this fashion. The man who does not know
    is not himself a danger to himself. Let us take the man who does.
  id: totrans-267
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These results are grammatical and even refer to themselves. Compare this output
    to what we got from character-based autoregression with RNNs.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2 can do lots of other tasks well, such as running a version of the cloze
    test, predicting the next word of a phrase where essential information appears
    at least 50 tokens before, answering questions about text, summarizing documents,
    and translating from one language to another.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Generators Discussion
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GPT-2 shows that if we process 512 tokens at a time through 48 decoder layers
    with 12 attention heads in each, for a total of 1.5 billion parameters, we can
    produce some pretty good text. What if we scaled everything up? That is, we won’t
    modify the basic architecture at all, but just use a lot more of everything.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: This was the plan of the successor to GPT-2, which was called (surprise) *GPT-3*.
    The block diagram for GPT-3 looks generally like that of GPT-2 in [Figure 20-29](#figure20-29)
    (aside from some efficiency improvements). There’s just more of everything. A
    lot more. GPT-3 processes 2,048 tokens at a time, on 96 decoder layers, with 96
    attention heads in each layer, for a total of 175 billion parameters (Brown et
    al. 2020). 175 billion. Training this behemoth required an estimated 355 GPU years
    at an estimated cost of US$4.6 million (Alammar 2018).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 was trained using a database called the *Common Crawl* dataset (Common
    Crawl 2020). It started with about a trillion words from books and the web. After
    removing duplications and cleaning the database, the database still had about
    420 billion words (Raffel et al. 2020).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 is capable of creating lots of different kinds of data. It was made available
    to the public for a period as a kind of beta test, but it’s now a commercial product
    (Scott 2020). During the beta test, people used GPT-3 for many applications, such
    as writing code for web layouts, writing actual computer programs, taking imaginary
    employment interviews, rewriting legal text in plain language, writing new text
    that looks like legal language, and, of course, writing in creative genres like
    fiction and poetry (Huston 2020).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: All this power is a mixed bag. Fine-tuning such a system requires enormous resources,
    and it becomes harder and harder to fine tune, as that requires finding task-specific
    data that wasn’t in the original data.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: If bigger is better, would even bigger still be even better still? The researchers
    behind GPT-3 have estimated that we can extract everything we need to know about
    any text (at least from the point of view of NLP-type tasks) with a model that
    uses 1 trillion parameters trained on 1 trillion tokens (Kaplan et al. 2020).
    These numbers are rough predictions and could be far off, but it’s interesting
    to think that there could be a point at which a stack of decoder blocks (and some
    support mechanisms) could extract almost all the information we need from a piece
    of text. We’ll probably know the answer soon, as other huge firms with enormous
    resources are sure to produce their own gargantuan NLP systems trained on their
    own colossal databases. Training these vast systems is a game only the big and
    rich can play.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: On a light-hearted note, we can play an interactive text-based fantasy game
    online, driven by a GPT-3 implementation (Walton 2020). The system was trained
    on a variety of genres, ranging from fantasy and cyberpunk to spy stories. Perhaps
    the most fun way to play with this system is to treat the AI as an improv partner,
    agreeing with and expanding on whatever the system throws at us. Let the AI set
    the flow and go with it.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Generated text can often hold up well in short doses, but how well does it do
    when we look closer? A recent study asked many language generators, including
    GPT-3, to perform 57 tasks, based on topics from humanities like law and history,
    to social sciences like economics and psychology, and STEM subjects like physics
    and mathematics (Hendrycks et al. 2020). Most output never came near human performance.
    The systems fared especially poorly on important social issues like morality and
    law.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: This shouldn’t be a surprise. These systems are simply producing words based
    on their probabilities of belonging together. In a real and fundamental sense,
    they have no idea what they’re talking about.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: For all their power, text generators like those we’ve seen here have no common
    sense. Worse, they blindly reiterate the stereotypes and prejudices inherited
    wholesale from the gender, racial, social, political, age, and other biases in
    their training data. Text generators have no idea of accuracy, fairness, kindness,
    or honesty. They don’t know when they’re stating facts or making things up. They
    just generate words that follow the statistics of the training data, and perpetuate
    every prejudice and limitation to be found there.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Data Poisoning
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We saw in Chapter 17 that adversarial attacks can trick convolutional neural
    networks into generating incorrect results. Natural language processing algorithms
    are also susceptible to intentional attacks, called *data poisoning*.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind data poisoning is to manipulate the training data for an NLP
    system in such a way that the system produces a desired type of inaccurate result,
    perhaps consistently, or perhaps only in the presence of a triggering word or
    phrase. For example, one can insert sentences or phrases into the training data
    that suggest that strawberries are made of cement. If these new entries are not
    discovered, then if the system is later used to generate stocking orders for a
    supermarket or a building contractor, they may find that their inventories end
    up being consistently and mysteriously wrong.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: This is particularly concerning because, as we’ve seen, NLP systems are typically
    trained on massive databases of millions or billions of words, so nobody is carefully
    reviewing the database for misleading phrases. Even if one or more people carefully
    read the entire training set, the poisoning texts can be designed so that they
    never explicitly refer to their targets, making them essentially indetectable
    and their effects unpredictable.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Returning to our previous example, such phrases can convince a system that strawberries
    are made of cement, while never referring to fruit or building materials at all.
    This is called *concealed data poisoning*, and it can be fiendishly hard to detect
    and prevent (Wallace et al. 2020).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Another kind of attack is based on changing the training data in a seemingly
    benign way. Suppose we’re working with a system that classifies news headlines
    into different categories. Any given headline can be subtly rewritten so that
    the obvious meaning is not changed, but the story is incorrectly classified. For
    instance, the original headline, Turkey is put on track for EU membership, would
    be correctly classified under “World.” But if an editor rephrases this into the
    active voice—EU puts Turkey on track for full membership—this would now be misclassified
    as “Business” (Xu, Ramirez, and Veeramachaneni 2020).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Data poisoning is particularly nefarious for several reasons. First, it can
    be done by people who have no connection to the organizations building or training
    the NLP models. Since significant amounts of training data are usually drawn from
    public sources, such as the web, a poisoner only needs to publish the manipulative
    phrases in a public blog or other location where they’re likely to be scooped
    up and used. Second, data poisoning can be done well ahead of any specific system’s
    use, or indeed, even before it’s conceived of.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: There’s no knowing how much training data has already been poisoned and is simply
    awaiting activation, like the sleeper agents in *The Manchurian Candidate* (Frankenheimer
    1962). Finally, unlike adversarial attacks on CNNs, poisoned data compromises
    the NLP system from within, making its influence an inherent part of the trained
    model.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: When a compromised system is used to make important decisions, such as evaluating
    school admission essays, interpreting medical notes, monitoring social media for
    fraud and manipulation, or searching legal records, then data poisoning can produce
    errors that change the course of people’s lives. Before any NLP system is used
    in such sensitive applications, in addition to examining it for signs of bias
    and historical prejudice, we must also analyze it for data poisoning, and certify
    it as safe only if it is demonstrably not biased or poisoned. Unfortunately, no
    methods for robust detection or certification of any of these problems currently
    exist.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We started this chapter with word embedding, which assigns each word a vector
    in a high-dimensional space representing its use. We saw how ELMo lets us capture
    multiple meanings based on content.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the mechanism of attention, which lets us simultaneously find words
    in the input that seem related, and build combinations of versions of the vectors
    describing those words.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Then we looked at transformers, which do away with recurrent cells entirely
    and replace them with multiple attention networks. This change allows us to train
    in parallel, which is of huge practical value.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we saw how to use multiple transformer encoder blocks to build BERT,
    a system for high-quality encoding, and how to use multiple decoder blocks to
    build GPT-2, a high-quality text generator.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we’ll turn our attention to reinforcement learning, which
    offers a way to train neural networks by evaluating their guesses, rather than
    expecting them to predict a single correct answer.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
