- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Observability
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 可观察性
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: '*Observability* is an attribute of a system, rather than something you do.
    It is a system’s ability to be monitored, tracked, and analyzed. Any application
    worthy of production should be observable. Your main goal in observing a system
    is to discern what it is doing, internally. You do this by analyzing system outputs
    like metrics, traces, and logs. *Metrics* usually consist of data over time that
    provide key insights into an application’s health and/or performance. *Traces*
    track a request as it traverses different services, to provide a holistic view.
    *Logs* provide a historical audit trail of errors or events that can be used for
    troubleshooting. Once you collect this data, you need to monitor it and alert
    someone when there is unexpected behavior.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*可观察性* 是系统的属性，而不是您要做的事情。这是系统被监视、跟踪和分析的能力。任何值得投入生产的应用程序都应该具备可观察性。您在观察系统时的主要目标是分辨它内部在做什么。通过分析像度量、跟踪和日志等系统输出，您可以做到这一点。*度量*
    通常包括随时间变化的数据，提供关键的应用程序健康和/或性能见解。*跟踪* 跟踪请求在不同服务中的传递过程，以提供全面的视图。*日志* 提供错误或事件的历史审计轨迹，可用于故障排除。一旦收集了这些数据，您需要监视它，并在出现意外行为时通知相关人员。'
- en: It is not necessary to analyze metrics, traces, and logs from every application
    or piece of architecture. For example, tracing is key when you are running distributed
    microservices because it can shed light on the individual state of a given service
    and its interactions with other services. Your decisions about what, how, and
    how much to observe really will hinge on the level of architectural complexity
    you are dealing with. Since your application and infrastructure are relatively
    uncomplicated, you’ll observe your telnet-server application with metrics, monitoring,
    and alerting.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 并不需要分析每个应用程序或架构的度量、跟踪和日志。例如，在运行分布式微服务时，跟踪对于了解给定服务的个体状态及其与其他服务的交互至关重要。关于观察什么、如何观察以及观察多少的决定，真正取决于您处理的架构复杂性水平。由于您的应用程序和基础架构相对简单，您将通过度量、监控和警报来观察您的telnet-server应用程序。
- en: In this chapter, you’ll first install a monitoring stack inside the Kubernetes
    cluster you created in Chapter 7. Then, you’ll investigate common metric patterns
    you can use as a starting point for any service or application you may encounter.
    Finally, you’ll configure the monitoring stack to send an email notification when
    an alert is triggered. By the end of this chapter, you’ll have a solid understanding
    of how to install, monitor, and send notifications for any application inside
    Kubernetes.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将首先在您在第7章创建的Kubernetes集群内安装监控堆栈。然后，您将调查可以作为任何服务或应用程序的起点使用的常见度量模式。最后，您将配置监控堆栈，以在触发警报时发送电子邮件通知。通过本章结束时，您将对如何在Kubernetes内安装、监控和发送通知，为任何应用程序具有坚实的理解。
- en: Monitoring Overview
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控概述
- en: '*Monitoring* is any action that entails recording, analyzing, and alerting
    on predefined metrics to understand the current state of a system. To measure
    a system’s state, you need applications to publish metrics that can tell a story
    about what the system is doing at any given time. By setting thresholds around
    metrics, you can create a baseline of what you expect the application’s behavior
    to be. For example, a web application is expected to respond with an HTTP 200
    in most cases. When the application’s baseline is not in a range you expect, you’ll
    need to alert someone so they can bring the application back into line. Systems
    will fail, but robust monitoring and alerting can be the bridge to user satisfaction
    and on-call shifts that end with you getting a good night’s sleep.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*监控* 是任何涉及记录、分析和根据预定指标发出警报的操作，以了解系统当前状态的行为。为了衡量系统的状态，您需要应用程序发布指标，这些指标可以在任何时候讲述系统正在做什么的故事。通过设置指标周围的阈值，您可以创建应用程序行为的基准。例如，大多数情况下，Web应用程序预期会响应HTTP
    200。当应用程序的基准不在您期望的范围内时，您需要通知相关人员，以便他们将应用程序恢复到正常状态。系统会发生故障，但是强大的监控和警报可以成为用户满意和轮班结束后您能安心入眠的桥梁。'
- en: 'An observable system should do its best to answer two main questions: “What?”
    and “Why?” “What?” asks about a symptom of an application or service during a
    specific time frame, while “Why?” asks for the reasons behind the symptom. You
    can usually get the answer to “What?” by monitoring symptoms, while you can get
    the answer to “Why?” by other means, like logs and traces. Correlating the symptom
    with the cause can be the hardest part of monitoring and observability. This means
    your application’s resiliency is only as good as the data the application outputs.
    A common phrase to describe this concept is “Garbage in, garbage out.” If the
    metrics exported from an application are not targeted or relevant to how the user
    interacts with the service, detecting and diagnosing issues will be more difficult.
    Because of this, it’s more important to measure the application’s *critical path*,
    or its most-used parts, than every possible use case.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可观察的系统应该尽最大努力回答两个主要问题：“什么？”和“为什么？”“什么？”询问在特定时间段内应用或服务的症状，而“为什么？”则是询问症状背后的原因。你通常可以通过监控症状来回答“什么？”的问题，而可以通过其他手段（如日志和追踪）来回答“为什么？”的问题。将症状与原因关联起来可能是监控和可观察性中最难的部分。这意味着，你的应用的弹性只有在应用输出的数据足够好的情况下才会表现得好。一句常用的短语来描述这个概念是“垃圾进，垃圾出”。如果从应用导出的度量数据没有针对性或与用户如何与服务交互无关，那么检测和诊断问题就会变得更加困难。因此，衡量应用的*关键路径*（即最常用的部分）比衡量每个可能的用例更为重要。
- en: For instance, say you go to your doctor because you woke up with nausea and
    stomach cramps. The doctor asks you some basic questions and takes your temperature,
    heart rate, and blood pressure. While your temperature is a bit elevated, everything
    else falls within the normal range. After reviewing all the data, the doctor makes
    a judgment call about why you feel bad. Odds are, the doctor will be able to correctly
    diagnose your ailment (or at least find more clues about it to follow up on).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设你因为早上醒来感到恶心和胃部痉挛而去看医生。医生会问你一些基本问题，并测量你的体温、心率和血压。虽然你的体温稍微升高，但其他指标都在正常范围内。经过所有数据的审核后，医生会做出判断，找出你不舒服的原因。医生很可能能正确诊断你的病情（或至少能找到更多的线索来进一步追踪）。
- en: This process of medical diagnosis is the same process you’ll follow when diagnosing
    application issues. You’ll measure the symptoms and try to explain them with a
    diagnosis or a hypothesis. If you have enough relevant data points, it will be
    easier for you to correlate the symptoms with a cause. In the example above, if
    the doctor asked what you had eaten recently (another solid data point), they
    might have correlated your nausea and cramps with your unwise choice to eat gas
    station sushi at 3 AM.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这种医疗诊断过程与诊断应用问题的过程是相同的。你将测量症状并尝试通过诊断或假设来解释它们。如果你拥有足够相关的数据点，关联症状和原因会变得更容易。在上面的例子中，如果医生询问你最近吃了什么（这是另一个可靠的数据点），他们可能会将你的恶心和痉挛与凌晨
    3 点在加油站吃寿司的不明智选择联系起来。
- en: Finally, always consider the “What?” and “Why?” when designing metrics and monitoring
    solutions for your applications. Avoid metrics or alerts that do not provide value
    to your stakeholders. Engineers who get bombarded by nonactionable alerts tend
    to get tired and ignore them.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在为您的应用设计度量和监控解决方案时，始终考虑“什么？”和“为什么？”这两个问题。避免设置那些无法为利益相关者提供价值的度量或警报。那些不断被无效警报轰炸的工程师往往会感到疲倦，最终忽视这些警报。
- en: Monitoring the Sample Application
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控示例应用
- en: You’ll begin by monitoring the metrics that this book’s example telnet-server
    publishes. The telnet-server application has an HTTP endpoint that serves up metrics
    about the application. The metrics you’re interested in gathering for the application
    focus on user experiences, like connection errors and traffic. The stack for your
    telnet-server application will consist of three main monitoring applications and
    a traffic simulation application. You’ll use these applications to monitor, alert,
    and visualize the metrics instrumented by telnet-server.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你将开始监控本书示例中的 telnet-server 所发布的度量数据。telnet-server 应用有一个 HTTP 端点，可以提供关于该应用的度量信息。你感兴趣的度量数据主要集中在用户体验方面，如连接错误和流量。你的
    telnet-server 应用的技术栈将包括三个主要的监控应用和一个流量模拟应用。你将使用这些应用来监控、警报和可视化由 telnet-server 部署的度量数据。
- en: The monitoring applications are Prometheus, Alertmanager, and Grafana. They
    are commonly used in the Kubernetes ecosystem. *Prometheus* is a metric collection
    application that queries metric data with its powerful built-in query language.
    It can set alerts for those metrics as well. If a collected metric crosses a set
    threshold, Prometheus sends an alert to *Alertmanager*, which takes the alerts
    from Prometheus and decides where to route them based on some criteria that are
    user configurable. The routes are usually notifications. *Grafana* provides an
    easy-to-use interface to create and view dashboards and graphs from the data Prometheus
    provides. The traffic simulator, bbs-warrior, simulates the traffic an end user
    of the telnet-server application might generate. This lets you test your monitoring
    system, application metrics, and alerts. [Figure 9-1](#figure9-1) shows an overview
    of the example stack.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 监控应用程序包括 Prometheus、Alertmanager 和 Grafana。它们在 Kubernetes 生态系统中广泛使用。*Prometheus*
    是一款度量数据收集应用，使用其强大的内建查询语言查询度量数据。它还可以为这些度量设置警报。如果收集的度量超过设定的阈值，Prometheus 会将警报发送到
    *Alertmanager*，后者接收 Prometheus 的警报并根据一些用户可配置的标准决定将其路由到哪里。路由通常是通知。*Grafana* 提供了一个易于使用的界面，用于创建和查看
    Prometheus 提供的数据的仪表板和图表。流量模拟器 bbs-warrior 模拟了 telnet-server 应用的最终用户可能生成的流量。这可以让您测试监控系统、应用程序度量和警报。[图
    9-1](#figure9-1) 显示了示例堆栈的概览。
- en: '![Diagram showing the flow between Grafana, the telnet-server application,
    bbs-warrior, Prometheus, and Alertmanager to an email message](image_fi/502482c09/f09001.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![展示 Grafana、telnet-server 应用、bbs-warrior、Prometheus 和 Alertmanager 之间流向电子邮件消息的示意图](image_fi/502482c09/f09001.png)'
- en: 'Figure 9-1: Overview of our monitoring stack'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-1：我们的监控堆栈概览
- en: Installing the Monitoring Stack
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装监控堆栈
- en: 'To install these applications, you’ll use the provided Kubernetes manifest
    files. The manifest files for the monitoring stack and traffic simulator are in
    the repository ([https://github.com/bradleyd/devops_for_the_desperate/](https://github.com/bradleyd/devops_for_the_desperate/)),
    under the *monitoring* directory. Within that directory are four subdirectories:
    *alertmanager*, *bbs-warrior*, *grafana*, and *prometheus*. These make up the
    example monitoring stack. You’ll install Prometheus, Alertmanager, and Grafana
    in a new Kubernetes Namespace called `monitoring` by applying all the manifests
    in each of these directories.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装这些应用程序，您将使用提供的 Kubernetes 清单文件。监控堆栈和流量模拟器的清单文件位于仓库中（[https://github.com/bradleyd/devops_for_the_desperate/](https://github.com/bradleyd/devops_for_the_desperate/)），在*monitoring*目录下。在该目录内有四个子目录：*alertmanager*、*bbs-warrior*、*grafana*和*prometheus*。这些构成了示例监控堆栈。您将通过应用这些目录中的所有清单文件，将
    Prometheus、Alertmanager 和 Grafana 安装到一个新的 Kubernetes 命名空间 `monitoring` 中。
- en: 'In a terminal, enter the following command to install the monitoring stack
    and bbs-warrior:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中，输入以下命令来安装监控堆栈和 bbs-warrior：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The output shows that all the manifests for your monitoring stack and bbs-warrior
    were run without errors. The `-R` flag makes the `kubectl` command recursively
    go through all the application directories and their subdirectories under the
    *monitoring* directory. Without this flag, `kubectl` will skip any nested subdirectories,
    like *grafana/dashboards/*. Prometheus, Grafana, Alertmanager, and bbs-warrior
    should be up and running in a few moments.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示您的监控堆栈和 bbs-warrior 的所有清单文件都已无错误地运行。`-R` 标志使 `kubectl` 命令递归地遍历*monitoring*目录下的所有应用程序目录及其子目录。如果没有这个标志，`kubectl`
    将跳过任何嵌套的子目录，比如 *grafana/dashboards/*。Prometheus、Grafana、Alertmanager 和 bbs-warrior
    应该很快就会启动并运行。
- en: Verifying the Installation
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证安装
- en: If the monitoring stack installation was successful on your Kubernetes cluster,
    you should be able to access Grafana’s, Alertmanager’s, and Prometheus’s web interfaces
    on your browser. In the provided Kubernetes manifest files, I have set the Kubernetes
    Service types for Prometheus, Grafana, and Alertmanager to `NodePort`. A Kubernetes
    *NodePort* Service allows you to connect to an application outside the Kubernetes
    cluster, so you should be able to access each application on the minikube IP address
    and a dynamic port. You should also be able to confirm that the bbs-warrior traffic
    simulator was installed and is running periodically.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在 Kubernetes 集群中成功安装了监控栈，你应该能在浏览器中访问 Grafana、Alertmanager 和 Prometheus 的 Web
    界面。在提供的 Kubernetes 清单文件中，我已将 Prometheus、Grafana 和 Alertmanager 的 Kubernetes 服务类型设置为
    `NodePort`。Kubernetes 的 *NodePort* 服务允许你连接到 Kubernetes 集群外的应用，因此你应该能够通过 minikube
    IP 地址和动态端口访问每个应用。你还应该能确认 bbs-warrior 流量模拟器已安装并定期运行。
- en: Grafana
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Grafana
- en: 'In a terminal, enter the following command to open Grafana:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中，输入以下命令以打开 Grafana：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Grafana lives in the `monitoring` Namespace, so this command uses the `-n` (Namespace)
    flag to show the `minikube service` command where to locate the Service. If you
    omit the `-n` flag, minikube will error, as there’s no Service named `grafana-service`
    in the default Namespace. You should now see Grafana open in your web browser,
    with the telnet-server dashboard loaded as the first page. If you don’t see the
    telnet-server dashboard, check the terminal where you ran the `minikube service`
    command for any errors. (You’ll need access to Grafana to follow along with the
    rest of this chapter.) We’ll discuss the graphs on the Grafana dashboard later;
    for now, you should ensure that Grafana is installed correctly and that you can
    open it in your browser.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana 位于 `monitoring` 命名空间中，因此此命令使用 `-n`（命名空间）标志来告诉 `minikube service` 命令服务的位置。如果省略
    `-n` 标志，minikube 会报错，因为在默认命名空间中没有名为 `grafana-service` 的服务。此时，你应该能在浏览器中看到 Grafana
    打开，并且加载了 telnet-server 仪表盘作为首页。如果没有看到 telnet-server 仪表盘，请检查你运行 `minikube service`
    命令的终端中是否有错误。（你需要访问 Grafana 才能继续完成本章内容。）稍后我们将讨论 Grafana 仪表盘上的图表；现在，确保 Grafana 已正确安装，并且你能够在浏览器中打开它。
- en: Alertmanager
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Alertmanager
- en: 'In a terminal, enter the same command you used to open Grafana in your browser,
    but replace the Service name with `alertmanager-service`, like this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中，输入与打开 Grafana 相同的命令，但将服务名称替换为 `alertmanager-service`，像这样：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The Alertmanager application should now be open in your browser. This page has
    a few navigation links, like Alerts, Silences, Status, and Help. The Alerts page
    displays current alerts and any metadata, like timestamps and severity associated
    with an alert. The Silences page shows any alerts that have been silenced. You
    can mute or silence an alert for a specific amount of time, which is helpful if
    an alert is being triggered and you don’t want to keep getting paged for it. The
    Status page shows information about Alertmanager, like its version, ready status,
    and current configuration. Alertmanager is configured via the *configmap.yaml*
    file in the *alertmanager/* directory. (You’ll edit this file later to enable
    notifications.) Finally, the Help page is a link to Alertmanager’s documentation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Alertmanager 应用应该已在浏览器中打开。此页面有几个导航链接，如 Alerts、Silences、Status 和 Help。Alerts
    页面显示当前的警报及其元数据，如时间戳和警报的严重性。Silences 页面显示所有已被静音的警报。你可以为特定时间段静音或静默警报，如果某个警报持续触发，而你又不希望不断接收到通知，这时非常有用。Status
    页面显示有关 Alertmanager 的信息，如版本、就绪状态和当前配置。Alertmanager 通过 *configmap.yaml* 文件在 *alertmanager/*
    目录中进行配置。（你稍后将编辑该文件以启用通知。）最后，Help 页面是一个指向 Alertmanager 文档的链接。
- en: Prometheus
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Prometheus
- en: 'In your terminal, enter the same command you just entered, but replace `grafana-service`
    with `prometheus-service` to open Prometheus:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的终端中，输入你刚才输入的相同命令，但将`grafana-service`替换为`prometheus-service`以打开 Prometheus：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Prometheus should open in your browser with a few links at the top of the page:
    Alerts, Graph, Status, and Help. The Alerts page displays all known alerts and
    their current state. The Graph page is the default page that allows you to run
    queries against the metric database. The Status page contains information about
    Prometheus’s health and configuration file. Prometheus, like Alertmanager, is
    controlled by the *configmap.yaml* file in the *prometheus* directory. This file
    controls what endpoints Prometheus scrapes for metrics, and it contains the alert
    rules for specific metrics. (We’ll explore the alert rules later.) The Help page
    is a link to Prometheus’s official documentation. For now, you are just confirming
    that Prometheus is running. Leave Prometheus open, as you’ll need it in the next
    section.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 应该会在你的浏览器中打开，并在页面顶部显示几个链接：Alerts、Graph、Status 和 Help。Alerts 页面显示所有已知的警报及其当前状态。Graph
    页面是默认页面，允许你对度量数据库执行查询。Status 页面包含有关 Prometheus 健康状况和配置文件的信息。Prometheus，像 Alertmanager
    一样，是由 *configmap.yaml* 文件控制的，位于 *prometheus* 目录中。这个文件控制 Prometheus 用于抓取度量的端点，并且包含特定度量的警报规则。（稍后我们将探讨警报规则。）Help
    页面是 Prometheus 官方文档的链接。目前，你只需要确认 Prometheus 正在运行。保持 Prometheus 打开，因为接下来的部分你将需要它。
- en: bbs-warrior
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: bbs-warrior
- en: The bbs-warrior application is a Kubernetes CronJob that runs every minute and
    creates a random number of connections and errors to the telnet-server application.
    It also sends a random number of BBS commands (like `date` and `help`) to the
    telnet-server, to mimic typical user activity. About a minute after you install
    bbs-warrior, it should start generating random traffic. This simulation should
    last only a few seconds.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: bbs-warrior 应用是一个 Kubernetes CronJob，每分钟运行一次，并向 telnet-server 应用创建随机数量的连接和错误。它还会向
    telnet-server 发送随机数量的 BBS 命令（如 `date` 和 `help`），以模拟典型的用户活动。在你安装 bbs-warrior 后约一分钟，它应该开始生成随机流量。这个模拟应该只持续几秒钟。
- en: 'To make sure bbs-warrior is active and installed correctly in your Kubernetes
    cluster, enter the following command in a terminal:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保 bbs-warrior 在你的 Kubernetes 集群中正确安装并处于活动状态，请在终端中输入以下命令：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `-l` (label) flag narrows down the results when searching for CronJobs.
    The output shows that the CronJob was installed over a minute ago (`60s`, under
    the `AGE` column) and that it last ran `25` seconds ago (under the `LAST` `SCHEDULE`
    column). If it were actively running, the `ACTIVE` column would be set to `1`
    rather than `0`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`-l`（标签）标志在搜索 CronJobs 时可以缩小结果范围。输出显示 CronJob 是在一分钟前安装的（在 `AGE` 列下为`60s`），并且它最后一次运行是在
    `25` 秒前（在 `LAST` `SCHEDULE` 列下）。如果它正在积极运行，`ACTIVE` 列会显示为 `1` 而不是 `0`。'
- en: 'You now know that the CronJob ran, but you should make sure it completed successfully.
    To do that, you’ll list the Pod with the label `bbs-warrior` in the default Namespace
    and look for `Completed` in the `STATUS` column. In the same terminal you used
    above, enter the following command:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道 CronJob 已经运行，但你应该确保它成功完成。为此，你将列出在默认命名空间中带有标签 `bbs-warrior` 的 Pod，并在 `STATUS`
    列中查找 `Completed`。在你之前使用的终端中，输入以下命令：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output shows that the `bbs-warrior` CronJob completed successfully about
    `60` seconds ago. If the CronJob has a status different from `Completed`, check
    the Pod’s logs for errors like you did in Chapter 7.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示 `bbs-warrior` CronJob 在大约 `60` 秒前成功完成。如果 CronJob 的状态不是 `Completed`，请像在第
    7 章中那样检查 Pod 的日志以查找错误。
- en: Metrics
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 度量指标
- en: You’ve installed and verified your monitoring stack, so now, you should focus
    on what you are monitoring for your telnet-server. Since you want to tailor your
    metrics to user happiness, you should use a common pattern to align all your applications.
    This is always a good approach when instrumenting your services, because allowing
    applications to do their own unique version of metrics makes triaging (and thus,
    on-call shifts) very difficult.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经安装并验证了你的监控栈，现在，你应该专注于监控你的 telnet-server。因为你想将度量指标与用户的满意度对齐，所以你应该使用一个通用的模式来统一所有应用程序。这始终是仪表化服务时的好方法，因为允许应用程序执行其独特的度量版本会使故障排除（因此也影响值班）变得非常困难。
- en: For this example, you’ll explore a common metric pattern called *Golden* *Signals.*
    This provides a subset of metrics to track, like errors and traffic, plus a common
    language for you and your peers to use to discuss what healthy looks like.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，你将探索一个常见的度量模式，叫做 *Golden* *Signals*。它提供了一组度量指标来跟踪，例如错误和流量，并为你和你的同事提供一种共同的语言，用来讨论什么样的状态是健康的。
- en: Golden Signals
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 黄金信号
- en: The Golden Signals (a term first coined by Google) are four metrics that help
    us understand the health of a microservice. The Golden Signals are latency, traffic,
    errors, and saturation. *Latency* is the time it takes for a service to process
    a request. *Traffic* is how many requests an application is receiving. *Errors*
    refers to the number of errors an application is reporting (such as a web server
    reporting 500s). *Saturation* is how full a service is. For a saturation signal,
    you could measure CPU usage to determine how much headroom is left on the system
    before the application or host becomes slow or unresponsive. You will use this
    pattern often when measuring applications. If you are ever in a situation where
    you don’t know what to monitor, start with the Golden Signals. They’ll provide
    ample information about your application’s health.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 黄金信号（这一术语最早由谷歌提出）是帮助我们理解微服务健康状况的四个指标。黄金信号包括延迟、流量、错误和饱和度。*延迟*是服务处理请求所需的时间。*流量*是应用程序接收到的请求数量。*错误*是应用程序报告的错误数量（例如，Web服务器报告500错误）。*饱和度*是服务的负载情况。对于饱和度信号，你可以通过测量CPU使用率来判断系统还剩余多少空间，直到应用程序或主机变得缓慢或无响应。你在衡量应用程序时将经常使用这个模式。如果你遇到不知道该监控什么的情况，可以从黄金信号开始。它们将提供关于应用程序健康状况的充分信息。
- en: A *microservice* typically is an application loosely coupled to other services
    in your platform. It is designed to focus only on one or two aspects of your overall
    domain. In this chapter, the telnet-server application will serve as the microservice
    whose health you will measure.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*微服务*通常是一个与平台上其他服务松散耦合的应用程序。它的设计目标是仅关注你整体领域中的一两个方面。在本章中，telnet-server应用程序将作为你衡量健康状况的微服务。'
- en: Adjusting the Monitoring Pattern
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整监控模式
- en: Chances are your application will not fit neatly into a predefined monitoring
    pattern like the Golden Signals. Use your best judgment about what matters. For
    example, I decided not to track latency when instrumenting the telnet-server application
    even though the pattern lists it. Users of such an application typically wouldn’t
    connect, run a command, and then quit. You could track latency of the commands,
    or you could add tracing for each command workflow. However, that would be overkill
    for this sample application and beyond the scope of this book. Your commands are
    for demonstration purposes only, so focusing on traffic, errors, and saturation
    signals will provide an overall idea of the application’s health from a user’s
    point of view.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你的应用程序可能无法完美适应像黄金信号这样的预定义监控模式。请根据你的判断决定监控哪些内容。例如，我决定在为telnet-server应用程序进行仪表化时不追踪延迟，尽管模式中列出了它。使用这种应用程序的用户通常不会连接后运行一个命令然后退出。你可以追踪命令的延迟，或者为每个命令工作流程添加追踪。然而，这对于这个示例应用程序来说有些过头，也超出了本书的范围。你的命令仅用于演示目的，因此关注流量、错误和饱和度信号，从用户的角度来看，会提供应用程序健康状况的总体了解。
- en: The telnet-server Dashboard
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Telnet-server 仪表板
- en: 'Let’s review the traffic, saturation, and error signals on your Grafana dashboard.
    In the browser where you first opened Grafana, the telnet-server dashboard has
    three graphs for the Golden Signals and two collapsed graph rows titled System
    and Application (see [Figure 9-2](#figure9-2)). You’ll focus on the Golden Signals
    graphs, which are as follows: Connections per second, Saturation, and Errors per
    second.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下Grafana仪表板上的流量、饱和度和错误信号。在你第一次打开Grafana的浏览器中，telnet-server仪表板有三个黄金信号图表，以及两个折叠的图表行，分别是“系统”和“应用程序”（见[图9-2](#figure9-2)）。你将重点关注黄金信号图表，具体如下：每秒连接数、饱和度和每秒错误数。
- en: The first graph, Connections per second (in the top left), provides the traffic
    Golden Signal. In this case, you measure how many connections per second you are
    receiving in a two-minute time frame. The telnet-server application increases
    a metric counter each time a connection is made, providing a good idea of how
    many people connect to the application. Many connections could pose an issue with
    performance or reliability. In this example, the x-axis shoots up over 4.0 connections
    per second for both telnet-server Pods. Your graphs will show different results
    from mine since bbs-warrior generates the traffic randomly; the goal is to make
    sure the graphs are being populated.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个图表，连接数（左上角），提供了流量黄金信号。在这种情况下，你测量在两分钟的时间框架内每秒接收到的连接数。每当有连接建立时，telnet-server
    应用会增加一个指标计数器，这能清楚地显示有多少人连接到该应用。过多的连接可能会导致性能或可靠性问题。在这个例子中，x 轴在两个 telnet-server
    Pods 中急剧上升，超过 4.0 每秒连接数。你的图表可能与我的不同，因为 bbs-warrior 会随机生成流量；目标是确保图表数据正在填充。
- en: '![Screenshot of three graphs showing Connections per second (for the system
    and application, in the top left, going up and down), Saturation (which is flat
    at 0s, in the top right), and Errors per second (going up and down, in the bottom
    left)](image_fi/502482c09/f09002.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![显示三个图表的截图，其中连接数（系统和应用，左上角，波动）、饱和度（右上角，平稳在 0s）和每秒错误数（波动，左下角）](image_fi/502482c09/f09002.png)'
- en: 'Figure 9-2: The telnet-server Grafana dashboard'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-2：telnet-server Grafana 仪表板
- en: The Saturation graph (top right) represents the saturation Golden Signal. For
    saturation, you measure the amount of time Kubernetes throttles your telnet-server
    container’s CPU. You set a CPU resource limit of 500 millicpu for the telnet-server
    container in Chapter 7. Therefore, if the telnet-server container uses more than
    the maximum limit, Kubernetes will throttle it, possibly making the telnet-server
    slow to respond to commands or connections. This throttle could potentially cause
    poor performance or a service interruption. In the Saturation graph shown in [Figure
    9-2](#figure9-2), the x-axis is flat at 0 microseconds for both Pods. The line
    of the graph will rise if CPU throttling occurs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 饱和度图（右上角）表示饱和度黄金信号。在饱和度的衡量中，你需要测量 Kubernetes 限制你的 telnet-server 容器 CPU 的时间。你在第七章中为
    telnet-server 容器设置了 500 毫 CPU 的 CPU 资源限制。因此，如果 telnet-server 容器使用的 CPU 超过了最大限制，Kubernetes
    会对其进行限制，这可能会导致 telnet-server 对命令或连接的响应变慢。这种限制可能会导致性能下降或服务中断。在 [图 9-2](#figure9-2)
    中显示的饱和度图中，两个 Pod 的 x 轴都平稳地显示在 0 微秒。如果发生 CPU 限制，图线会升高。
- en: The Errors per second (bottom left) graph maps to the error Golden Signal. For
    this metric, you track the connection errors per second that you receive in a
    two-minute time frame. These errors are incremented when a client fails to connect
    properly or if the connection is killed unexpectedly. A high error rate could
    indicate a code or infrastructure issue that you need to address. In the graph
    shown in [Figure 9-2](#figure9-2), the errors-per-second rate is spiking to 0.4
    for both pods.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 每秒错误数（左下角）图表对应错误黄金信号。对于这个指标，你需要跟踪在两分钟时间段内收到的每秒连接错误。这些错误会在客户端无法正确连接或连接意外中断时递增。高错误率可能表示你需要解决的代码或基础设施问题。在
    [图 9-2](#figure9-2) 中显示的图表中，错误每秒的错误率在两个 pod 上都急剧上升到 0.4。
- en: 'The collapsed two rows at the bottom of this dashboard contain some miscellaneous
    graphs not covered in this chapter, but you should explore them on your own. The
    telnet-server dashboard System row contains two graphs: one for memory, and one
    for CPU usage by the telnet-server Pods. The Application row contains four graphs:
    Total Connections, Active Connections, Connection Error Total, and Unknown Command
    Total.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本仪表板底部折叠的两行包含一些本章未涵盖的杂项图表，但你应该自行探索它们。telnet-server 仪表板的系统行包含两个图表：一个是内存使用情况，另一个是
    telnet-server Pods 的 CPU 使用情况。应用行包含四个图表：总连接数、活动连接数、连接错误总数和未知命令总数。
- en: The telnet-server dashboard is in the *grafana/dashboards/telnet-server.yaml*
    file. This file is a Kubernetes ConfigMap resource that contains the JSON configuration
    that Grafana requires to create the dashboard and graphs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: telnet-server 仪表板位于 *grafana/dashboards/telnet-server.yaml* 文件中。该文件是一个 Kubernetes
    ConfigMap 资源，包含 Grafana 创建仪表板和图表所需的 JSON 配置。
- en: 'PromQL: A Primer'
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PromQL：简介
- en: '*PromQL* is a query language built into the Prometheus application. You use
    it to query and manipulate metric data. Think of PromQL as a distant cousin of
    SQL. It has some built-in functions (like `average` and `sum`) to make querying
    data easier plus conditional logic (like `>` or `=`). We won’t explore this query
    language in depth here except to show how I queried telnet-server’s Golden Signal
    metrics to populate your graphs and alerts.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*PromQL* 是 Prometheus 应用内置的查询语言。你用它来查询和操作指标数据。可以将 PromQL 看作是 SQL 的远亲。它有一些内置函数（如
    `average` 和 `sum`），可以简化数据查询，还支持条件逻辑（如 `>` 或 `=`）。我们在这里不会深入探讨这个查询语言，只会展示如何查询 telnet-server
    的黄金信号指标，以填充你的图表和警报。'
- en: 'For example, here is the query you enter to generate the Errors-per-second
    graph:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这是你输入的查询，用于生成每秒错误数的图表：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The name of the metric is `telnet_server_connection_errors_total`. This metric
    measures the total amount of `connection errors` a user may encounter. The query
    uses Prometheus’ `rate()` function, which calculates the per-second connection
    error average over a specified time interval. You limit the time frame for which
    this query fetches data to two minutes using square brackets `[2m]`. The result
    will show the two running telnet-server Pods you installed in Chapter 7. The curly
    brackets (`{}`) allow you to refine the query, using labels as matchers. Here,
    you specify that you want data only for the `telnet_server_connection_errors`
    metric with the `job="kubernetes-pods"` label.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标的名称是`telnet_server_connection_errors_total`。该指标用于衡量用户可能遇到的`连接错误`的总数。查询使用了
    Prometheus 的 `rate()` 函数，该函数计算在指定时间间隔内每秒的连接错误平均值。你使用方括号`[2m]`限制查询获取数据的时间范围为两分钟。结果将显示你在第7章中安装的两个运行中的
    telnet-server Pods。花括号（`{}`）允许你通过使用标签作为匹配项来细化查询。在这里，你指定只获取带有 `job="kubernetes-pods"`
    标签的 `telnet_server_connection_errors` 指标的数据。
- en: 'When creating an alert rule in Prometheus, you can enter the same query as
    above to drive the alert. However, this time, you should wrap the results from
    the `rate()` function in a `sum``()` function. You’ll do this because you want
    to know the overall error rate for both Pods. The alert rule should look like
    the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Prometheus 中创建警报规则时，你可以输入与上述相同的查询来触发警报。然而，这一次，你应该将 `rate()` 函数的结果包装在 `sum()`
    函数中。你这样做是因为你希望了解两个 Pod 的总体错误率。警报规则应该如下所示：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'At the end of the query, you add greater-than (`>`) conditional logic with
    a number: `2`. This basically means that if the error rate is greater than two
    per second, this alert query evaluates to true. (Later in this chapter, we’ll
    discuss what happens when alert rules are true.)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询的最后，你添加了大于符号（`>`）的条件逻辑，并指定了一个数字：`2`。这基本意味着，如果每秒的错误率大于2，则此警报查询将评估为真。（在本章稍后部分，我们将讨论当警报规则为真时会发生什么。）
- en: If you want to review or tinker with any of these metrics, see the Graph page
    in the Prometheus web interface. [Figure 9-3](#figure9-3) shows the `telnet_server_connection_errors_total`
    query being run.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看或调整这些指标中的任何一个，可以访问 Prometheus Web 界面中的图形页面。[图9-3](#figure9-3)展示了正在运行的
    `telnet_server_connection_errors_total` 查询。
- en: '![Screenshot showing a query in the Prometheus dashboard](image_fi/502482c09/f09003Keyline.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![显示在 Prometheus 仪表盘中查询的截图](image_fi/502482c09/f09003Keyline.png)'
- en: 'Figure 9-3: Running a query in Prometheus’s web interface'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-3：在 Prometheus 的 Web 界面中运行查询
- en: The query returns connection error data for both Pods. To learn more about PromQL,
    visit [https://prometheus.io/docs/prometheus/latest/querying/basics/](https://prometheus.io/docs/prometheus/latest/querying/basics/)
    for more examples and information*.*
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 查询返回了两个 Pods 的连接错误数据。要了解更多关于 PromQL 的信息，请访问 [https://prometheus.io/docs/prometheus/latest/querying/basics/](https://prometheus.io/docs/prometheus/latest/querying/basics/)，以获取更多示例和信息*。*
- en: Alerts
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 警报
- en: Metrics and graphs only constitute half of a monitoring solution. When your
    application decides to take a stroll off a cliff (and it will), someone or something
    needs to know about it. If a Pod dies in a Deployment, Kubernetes just replaces
    it with a new one. But if the Pod keeps restarting, someone needs to address it,
    and that’s where the alerts and notifications come into play.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 指标和图表仅构成监控解决方案的一半。当你的应用决定走下悬崖时（它肯定会），就需要有人或某些东西来监控它。如果一个 Pod 在部署中死掉，Kubernetes
    会用新的 Pod 来替换它。但如果 Pod 一直重启，就需要有人处理这个问题，这就是警报和通知发挥作用的地方。
- en: 'What constitutes a good alert? Besides an alert for each of your application’s
    Golden Signals, you may need an alert around a key metric to monitor. When this
    does happen, keep in mind a couple of guidelines to follow when creating alerts:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是一个好的告警？除了针对每个应用程序的 Golden Signals 设置告警外，你可能还需要围绕某个关键指标设置告警进行监控。当这种情况发生时，创建告警时要遵循以下几个指南：
- en: Do not set thresholds too low. Setting alert thresholds too low can cause the
    alerts to repeatedly fire and then clear if you have spiky metrics. This behavior
    is known as *flapping*, and it can be quite normal. The system should not issue
    alerts for flapping metrics every few minutes, because on-call engineers get stressed
    out when they repeatedly get a notification and then find the alarm has already
    cleared.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不要将阈值设置得过低。将告警阈值设置得过低可能导致告警频繁触发，然后如果你的指标波动较大，告警又会被清除。这种行为被称为 *告警波动*，它是很常见的现象。系统不应该为波动的指标每隔几分钟就发出告警，因为值班工程师在收到通知后会感到很有压力，而当他们赶到时，发现告警已经被清除。
- en: Avoid creating alerts that are not actionable. Don’t create alerts for a service
    when nothing can be done to remedy it. I call these alerts *status alerts*. Nothing
    is more frustrating to an on-call engineer than being woken up in the middle of
    the night only to babysit an alert that requires no action.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 避免创建不可操作的告警。不要为无法采取措施解决的服务创建告警。我把这些告警称为 *状态告警*。没有什么比半夜被叫醒来看守一个不需要采取行动的告警更让值班工程师沮丧的了。
- en: For this book, I have provided three alerts called `HighErrorRatePerSecond`,
    `HighConnectionRatePerSecond`, and `HighCPUThrottleRate` (more on these later).
    These alerts are located in the *prometheus.rules* section inside Prometheus’s
    configuration file (*configmap.yaml*). Prometheus uses alert rules to decide whether
    a metric is in an undesired state. An *alert rule* has information like the alert
    name, PromQL query, threshold, and labels. For your example, I have gone against
    my own alert-creation advice and set the provided thresholds extremely low, allowing
    bbs-warrior to trigger the alerts easily. Nothing beats a live example when learning
    about real-time metrics and alerts!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本书，我提供了三个告警，分别是 `HighErrorRatePerSecond`、`HighConnectionRatePerSecond` 和 `HighCPUThrottleRate`（稍后会详细介绍）。这些告警位于
    Prometheus 配置文件（*configmap.yaml*）中的 *prometheus.rules* 部分。Prometheus 使用告警规则来决定某个指标是否处于不希望的状态。一个
    *告警规则* 包含告警名称、PromQL 查询、阈值和标签等信息。对于你的示例，我违反了自己设定告警的建议，故意将阈值设置得非常低，使得 bbs-warrior
    很容易触发这些告警。在学习实时指标和告警时，活生生的例子最为有效！
- en: Reviewing Golden Signal Alerts in Prometheus
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 审查 Prometheus 中的 Golden Signal 告警
- en: You can view alerts in either Prometheus’s or Alertmanager’s web interfaces.
    The difference is that Alertmanger displays only alerts that are being triggered,
    whereas Prometheus will show all alerts, whether they are firing or not. You want
    to view all the alerts, so you’ll use Prometheus for this example. However, you
    should visit Alertmanager’s interface as well when an alert is being triggered.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 Prometheus 或 Alertmanager 的 Web 界面中查看告警。不同之处在于，Alertmanager 仅显示正在触发的告警，而
    Prometheus 会显示所有告警，无论它们是否正在触发。你需要查看所有告警，所以在这个示例中你将使用 Prometheus。然而，当某个告警正在触发时，你也应该访问
    Alertmanager 的界面。
- en: 'In the browser where Prometheus was originally opened, click the **Alerts**
    link in the top-left navigation bar. You should see the three provided telnet-server
    Golden Signals alerts: `HighErrorRatePerSecond`, `HighConnectionRatePerSecond`,
    and `HighCPUThrottleRate`. These alerts were created when you installed Prometheus
    earlier. The Alerts page should look like [Figure 9-4](#figure9-4).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初打开 Prometheus 的浏览器中，点击左上角导航栏中的 **Alerts** 链接。你应该能看到三个提供的 telnet-server Golden
    Signals 告警：`HighErrorRatePerSecond`、`HighConnectionRatePerSecond` 和 `HighCPUThrottleRate`。这些告警是在你之前安装
    Prometheus 时创建的。告警页面应该像 [图 9-4](#figure9-4) 一样显示。
- en: '![Screenshot showing the alerts in the Prometheus dashboard. The green alert
    at the top for HighErrorRatePerSecond shows 0 active. The red alert for HighConnectionRatePerSecond
    shows 1 active, and beneath that is the code breakdown with more information.
    At the bottom is the green alert for HighCPUThrottleRate, which shows 0 active.](image_fi/502482c09/f09004Keyline.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![截图显示了 Prometheus 仪表板中的告警。顶部的绿色告警 HighErrorRatePerSecond 显示为 0 个活动告警。红色告警
    HighConnectionRatePerSecond 显示为 1 个活动告警，下面是包含更多信息的代码分解。底部是绿色告警 HighCPUThrottleRate，显示为
    0 个活动告警。](image_fi/502482c09/f09004Keyline.png)'
- en: 'Figure 9-4: Prometheus alerts for telnet-server'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-4：Prometheus 对 telnet-server 的告警
- en: 'Each alert will be in one of three states: Pending (yellow), Inactive (green),
    or Firing (red). In [Figure 9-4](#figure9-4), the `HighConnectionRatePerSecond`
    alert is Firing. The other two alerts, `HighCPUThrottleRate` and `HighErrorRatePerSecond`,
    are Inactive since they are not being triggered. Your Alerts page will be different
    from mine because of bbs-warrior’s randomness. If your page doesn’t show any alerts
    in a Firing state, wait a few minutes until more traffic is generated. Then refresh
    the browser page. In all my testing for this chapter, I always had at least one
    alert transition to a Firing state.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 每个警报将处于三种状态之一：待处理（黄色）、非活动（绿色）或触发（红色）。在[图9-4](#figure9-4)中，`HighConnectionRatePerSecond`警报处于触发状态。其他两个警报，`HighCPUThrottleRate`和`HighErrorRatePerSecond`，由于未被触发，处于非活动状态。由于bbs-warrior的随机性，你的警报页面可能与我的不同。如果你的页面没有显示任何触发状态的警报，可以等几分钟生成更多流量，然后刷新浏览器页面。在我为本章进行的所有测试中，我总是至少有一条警报过渡到触发状态。
- en: The `HighErrorRatePerSecond` alert is concerned with the number of connection
    errors received per second. If the rate of connection errors in a two-minute window
    is greater than 2, the alert is in a Firing state. On my local Kubernetes setup,
    the alert is currently in the Inactive state.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`HighErrorRatePerSecond`警报关注每秒接收到的连接错误数量。如果两分钟窗口内连接错误的速率超过2次，警报将进入触发状态。在我的本地Kubernetes设置中，此警报当前处于非活动状态。'
- en: The next alert, `HighConnectionRatePerSecond`, detects whether the connection
    rate is greater than 2 per second in a two-minute time frame. Currently, this
    alert is in the Firing state. [Figure 9-4](#figure9-4) shows that the current
    value for my connection rate is more than 9.1 connections per second, which is
    well beyond the set threshold of 2\. I have expanded the alert in the browser
    to show the provided metadata in a key-value layout that an alert provides. In
    the labels section for all three alerts, I have set a label called `severity`
    with a value of `Critical` so it’s easier to distinguish between noncritical alerts
    and ones that need immediate attention. You’ll use this label to route important
    alerts in Alertmanager later. The annotations section includes a description,
    a summary, and a link to a *runbook*, which is a blueprint that provides unfamiliar
    engineers with the what, why, and how for a service. Having this information is
    crucial when sending out an alert notification, because it gives the person on
    call an idea of what to look for when troubleshooting.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下一条警报，`HighConnectionRatePerSecond`，用于检测在两分钟时间框架内连接速率是否超过每秒2次。目前，这条警报处于触发状态。[图9-4](#figure9-4)显示，当前我的连接速率超过每秒9.1次，远远超过设定的2的阈值。我已经在浏览器中展开了这条警报，以显示警报提供的元数据，采用键值对布局。在所有三个警报的标签部分，我设置了一个名为`severity`的标签，其值为`Critical`，这样可以更容易区分非关键警报和需要立即处理的警报。稍后，你将在Alertmanager中使用这个标签来路由重要的警报。注释部分包括描述、摘要和指向*运行手册*的链接，运行手册是为不熟悉的工程师提供服务的“蓝图”，解释了为何、如何以及如何做这项服务。提供这些信息对于发送警报通知至关重要，因为它能让值班人员在故障排除时了解应关注的内容。
- en: The last alert, `HighCPUThrottleRate`, detects high CPU saturation. If the CPU
    is being throttled by Kubernetes for more than 300 microseconds in a two-minute
    window, you’ll transition to a Firing state. This alert is currently inactive,
    but normally, I’d suggest a minimum five-minute window when tracking CPU throttling.
    This is because smaller time windows can make you more susceptible to alerting
    on a temporarily spiky workload.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一条警报，`HighCPUThrottleRate`，用于检测CPU饱和度是否过高。如果CPU在两分钟的时间窗口内被Kubernetes限流超过300微秒，系统将切换到触发状态。此警报当前处于非活动状态，但通常情况下，我建议在追踪CPU限流时使用至少五分钟的时间窗口。因为较小的时间窗口可能会让你更容易在短暂的工作负载峰值期间触发警报。
- en: Routing and Notifications
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 路由和通知
- en: You’ve verified that the metrics and alerts are visible and active, so now,
    you should set up Alertmanager to send out email notifications. Once an alert
    is in the Firing state, Prometheus sends it to Alertmanager for routing and notification.
    Notifications can be sent via text messages, push notifications, or email. Alertmanager
    calls these notification methods *receivers*. Routing is used to match on alerts
    and send them to a specific receiver. A common pattern is to route alerts based
    on specific labels. Alert labels are set in the Prometheus *configmap.yaml* file.
    You’ll use this pattern later, when you enable notifications.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经验证了指标和警报可见并处于活动状态，现在，你应该配置Alertmanager以发送电子邮件通知。一旦警报处于触发状态，Prometheus会将其发送到Alertmanager进行路由和通知。通知可以通过短信、推送通知或电子邮件发送。Alertmanager将这些通知方式称为*接收器*。路由用于匹配警报并将其发送到特定的接收器。一种常见的模式是根据特定标签路由警报。警报标签在Prometheus的*configmap.yaml*文件中设置。稍后在启用通知时，你将使用这种模式。
- en: The provided Alertmanager configuration is located in the *alertmanager/configmap.yaml*
    file. It is set up to match on all alerts with a `severity` label set to `Critical`
    and route them to a *none receiver*, which is basically a black hole that won’t
    notify anyone when there’s an alert. This means that to see whether an alert is
    being triggered, you would need to visit the web page on either Alertmanager or
    Prometheus. This setup isn’t ideal, as refreshing the web browser every few minutes
    would become tedious, so you’ll route any alert to the `email` receiver if the
    alert has a `severity` label set to `Critical`. If you’re following along, this
    step is completely optional, but it shows you how to configure receivers in Alertmanager.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的Alertmanager配置位于*alertmanager/configmap.yaml*文件中。它设置为匹配所有`severity`标签为`Critical`的警报，并将它们路由到*none接收器*，即一个不会在警报发生时通知任何人的黑洞。这意味着，要查看警报是否被触发，你需要访问Alertmanager或Prometheus上的网页。这个设置并不理想，因为每隔几分钟刷新一次网页会变得很麻烦，所以你将把任何`severity`标签设置为`Critical`的警报路由到`email`接收器。如果你在跟着操作，这一步完全是可选的，但它向你展示了如何在Alertmanager中配置接收器。
- en: Enabling Email Notifications
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 启用电子邮件通知
- en: To route an alert to the `email` receiver, you need to edit Alertmanager’s configuration.
    I have stubbed out a template for the `email` receiver and `route` block in the
    *configmap.yaml* file. The email example is based on a Gmail account, but you
    can alter it to accommodate any email provider. See [https://www.prometheus.io/docs/alerting/latest/configuration/#email_config/](https://www.prometheus.io/docs/alerting/latest/configuration/#email_config/)
    for more details.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要将警报路由到`email`接收器，你需要编辑Alertmanager的配置。我已经在*configmap.yaml*文件中为`email`接收器和`route`块提供了一个模板。这个电子邮件示例是基于Gmail账户的，但你可以根据需要修改为任何电子邮件提供商。更多详情请参见[https://www.prometheus.io/docs/alerting/latest/configuration/#email_config/](https://www.prometheus.io/docs/alerting/latest/configuration/#email_config/)。
- en: 'Open Alertmanager’s *configmap.yaml* file in your favorite editor; it should
    look like this:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 打开Alertmanager的*configmap.yaml*文件，用你喜欢的编辑器打开，它应该是这样的：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, you have two receivers named `email` 1 and `none` 2. The `none` receiver
    won’t send alerts anywhere, but when uncommented, the `email` receiver will send
    alerts to a Gmail account. Uncomment the `email` receiver lines and then replace
    with an email account you can use for testing.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你有两个接收器，分别命名为`email` 1和`none` 2。`none`接收器不会将警报发送到任何地方，但当取消注释时，`email`接收器将把警报发送到一个Gmail账户。取消注释`email`接收器的相关行，然后用一个你可以用于测试的电子邮件账户替换它。
- en: 'After configuring your email settings, change the `receiver` 3 under the `routes`
    section to `email`. This configures Alertmanager to route any alert to the `email`
    receiver if the alert has a `severity` label set to `Critical`. The receiver line
    4 should now look like this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 配置好电子邮件设置后，将`routes`部分下的`receiver` 3更改为`email`。这将配置Alertmanager，在警报的`severity`标签设置为`Critical`时，将任何警报路由到`email`接收器。接收器行4现在应该如下所示：
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You’ll still have your default or catch-all receiver 3 set to `none`, so any
    alert that does not match your `severity` label rule will be sent there. Save
    this file, as you are done modifying it.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你仍然会有默认的或用于捕获所有警报的接收器3，设置为`none`，因此任何不符合`severity`标签规则的警报都会发送到这里。保存此文件，因为你已经完成了对它的修改。
- en: Applying Alertmanager’s Configuration Changes
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 应用Alertmanager的配置更改
- en: 'Next, you’ll update Alertmanager’s ConfigMap inside the Kubernetes cluster.
    Since the local file contains changes that don’t exist on the cluster, enter the
    following in a terminal:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将更新Kubernetes集群中的Alertmanager ConfigMap。由于本地文件包含集群中不存在的更改，请在终端中输入以下内容：
- en: '[PRE10]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The next step is to tell Kubernetes to restart the Alertmanager Deployment
    so it can pick up the new configuration changes. In the same terminal, enter the
    following command to restart Alertmanager:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是告诉Kubernetes重启Alertmanager部署，以便它能够获取新的配置更改。在同一个终端中，输入以下命令来重启Alertmanager：
- en: '[PRE11]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The Alertmanager Pod should restart after a few moments. If you have any alerts
    in the Firing state, you should start receiving email in your inbox. Depending
    on Alertmanager and your email provider, the notifications may take some time
    to appear.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Alertmanager Pod 应该会在几秒钟后重启。如果你有任何处于触发状态的警报，你应该开始在收件箱中收到邮件通知。根据Alertmanager和你的邮件提供商，通知可能需要一些时间才能出现。
- en: 'If you do not receive any notification emails, check for a couple of common
    issues. First, make sure the *configmap.yaml* file does not have any typos or
    indentation errors. It is very easy to misalign a YAML file. Second, make sure
    the email settings you entered match what is required by your email provider.
    Look in Alertmanager’s logs to find these and other common issues. Enter the following
    `kubectl` command to view the logs for any errors:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有收到任何通知邮件，请检查几个常见问题。首先，确保*configmap.yaml*文件没有任何拼写错误或缩进问题。YAML文件很容易对齐错误。其次，确保你输入的邮件设置符合邮件提供商的要求。查看Alertmanager的日志，以查找这些和其他常见问题。输入以下`kubectl`命令来查看日志中的错误：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If you need to disable the notifications for any reason, set the routes receiver
    back to `none` in the *configmap.yaml* file, apply the manifest changes, and restart.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要出于某种原因禁用通知，请将路由接收器设置回`none`，修改*configmap.yaml*文件，应用清单更改，并重启。
- en: You now have alerts and notifications configured for telnet-server’s Golden
    Signals.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经为telnet-server的黄金信号配置了警报和通知。
- en: Summary
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Metrics and alerts are foundational pieces when monitoring an application. They
    provide insight into the health and performance of your service. In this chapter,
    you learned about the Golden Signals monitoring pattern and how to install a modern
    monitoring stack inside a Kubernetes cluster using Prometheus, Alertmanager, and
    Grafana. Finally, you learned how to configure Alertmanager to send email notifications
    for critical alerts.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 指标和警报是监控应用程序的基础。它们提供了服务健康状况和性能的洞察。在本章中，你了解了黄金信号监控模式，以及如何在Kubernetes集群中使用Prometheus、Alertmanager和Grafana安装现代监控栈。最后，你学会了如何配置Alertmanager以发送关键警报的邮件通知。
- en: In the next chapter, we’ll shift gears and discuss common troubleshooting scenarios
    you will find on a host or network, plus the tools you can use to diagnose them.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将转变话题，讨论在主机或网络上常见的故障排除场景，以及你可以使用的诊断工具。
