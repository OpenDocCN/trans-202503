- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Observability
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Observability* is an attribute of a system, rather than something you do.
    It is a system’s ability to be monitored, tracked, and analyzed. Any application
    worthy of production should be observable. Your main goal in observing a system
    is to discern what it is doing, internally. You do this by analyzing system outputs
    like metrics, traces, and logs. *Metrics* usually consist of data over time that
    provide key insights into an application’s health and/or performance. *Traces*
    track a request as it traverses different services, to provide a holistic view.
    *Logs* provide a historical audit trail of errors or events that can be used for
    troubleshooting. Once you collect this data, you need to monitor it and alert
    someone when there is unexpected behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: It is not necessary to analyze metrics, traces, and logs from every application
    or piece of architecture. For example, tracing is key when you are running distributed
    microservices because it can shed light on the individual state of a given service
    and its interactions with other services. Your decisions about what, how, and
    how much to observe really will hinge on the level of architectural complexity
    you are dealing with. Since your application and infrastructure are relatively
    uncomplicated, you’ll observe your telnet-server application with metrics, monitoring,
    and alerting.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll first install a monitoring stack inside the Kubernetes
    cluster you created in Chapter 7. Then, you’ll investigate common metric patterns
    you can use as a starting point for any service or application you may encounter.
    Finally, you’ll configure the monitoring stack to send an email notification when
    an alert is triggered. By the end of this chapter, you’ll have a solid understanding
    of how to install, monitor, and send notifications for any application inside
    Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Monitoring* is any action that entails recording, analyzing, and alerting
    on predefined metrics to understand the current state of a system. To measure
    a system’s state, you need applications to publish metrics that can tell a story
    about what the system is doing at any given time. By setting thresholds around
    metrics, you can create a baseline of what you expect the application’s behavior
    to be. For example, a web application is expected to respond with an HTTP 200
    in most cases. When the application’s baseline is not in a range you expect, you’ll
    need to alert someone so they can bring the application back into line. Systems
    will fail, but robust monitoring and alerting can be the bridge to user satisfaction
    and on-call shifts that end with you getting a good night’s sleep.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An observable system should do its best to answer two main questions: “What?”
    and “Why?” “What?” asks about a symptom of an application or service during a
    specific time frame, while “Why?” asks for the reasons behind the symptom. You
    can usually get the answer to “What?” by monitoring symptoms, while you can get
    the answer to “Why?” by other means, like logs and traces. Correlating the symptom
    with the cause can be the hardest part of monitoring and observability. This means
    your application’s resiliency is only as good as the data the application outputs.
    A common phrase to describe this concept is “Garbage in, garbage out.” If the
    metrics exported from an application are not targeted or relevant to how the user
    interacts with the service, detecting and diagnosing issues will be more difficult.
    Because of this, it’s more important to measure the application’s *critical path*,
    or its most-used parts, than every possible use case.'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, say you go to your doctor because you woke up with nausea and
    stomach cramps. The doctor asks you some basic questions and takes your temperature,
    heart rate, and blood pressure. While your temperature is a bit elevated, everything
    else falls within the normal range. After reviewing all the data, the doctor makes
    a judgment call about why you feel bad. Odds are, the doctor will be able to correctly
    diagnose your ailment (or at least find more clues about it to follow up on).
  prefs: []
  type: TYPE_NORMAL
- en: This process of medical diagnosis is the same process you’ll follow when diagnosing
    application issues. You’ll measure the symptoms and try to explain them with a
    diagnosis or a hypothesis. If you have enough relevant data points, it will be
    easier for you to correlate the symptoms with a cause. In the example above, if
    the doctor asked what you had eaten recently (another solid data point), they
    might have correlated your nausea and cramps with your unwise choice to eat gas
    station sushi at 3 AM.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, always consider the “What?” and “Why?” when designing metrics and monitoring
    solutions for your applications. Avoid metrics or alerts that do not provide value
    to your stakeholders. Engineers who get bombarded by nonactionable alerts tend
    to get tired and ignore them.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring the Sample Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ll begin by monitoring the metrics that this book’s example telnet-server
    publishes. The telnet-server application has an HTTP endpoint that serves up metrics
    about the application. The metrics you’re interested in gathering for the application
    focus on user experiences, like connection errors and traffic. The stack for your
    telnet-server application will consist of three main monitoring applications and
    a traffic simulation application. You’ll use these applications to monitor, alert,
    and visualize the metrics instrumented by telnet-server.
  prefs: []
  type: TYPE_NORMAL
- en: The monitoring applications are Prometheus, Alertmanager, and Grafana. They
    are commonly used in the Kubernetes ecosystem. *Prometheus* is a metric collection
    application that queries metric data with its powerful built-in query language.
    It can set alerts for those metrics as well. If a collected metric crosses a set
    threshold, Prometheus sends an alert to *Alertmanager*, which takes the alerts
    from Prometheus and decides where to route them based on some criteria that are
    user configurable. The routes are usually notifications. *Grafana* provides an
    easy-to-use interface to create and view dashboards and graphs from the data Prometheus
    provides. The traffic simulator, bbs-warrior, simulates the traffic an end user
    of the telnet-server application might generate. This lets you test your monitoring
    system, application metrics, and alerts. [Figure 9-1](#figure9-1) shows an overview
    of the example stack.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing the flow between Grafana, the telnet-server application,
    bbs-warrior, Prometheus, and Alertmanager to an email message](image_fi/502482c09/f09001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-1: Overview of our monitoring stack'
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Monitoring Stack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To install these applications, you’ll use the provided Kubernetes manifest
    files. The manifest files for the monitoring stack and traffic simulator are in
    the repository ([https://github.com/bradleyd/devops_for_the_desperate/](https://github.com/bradleyd/devops_for_the_desperate/)),
    under the *monitoring* directory. Within that directory are four subdirectories:
    *alertmanager*, *bbs-warrior*, *grafana*, and *prometheus*. These make up the
    example monitoring stack. You’ll install Prometheus, Alertmanager, and Grafana
    in a new Kubernetes Namespace called `monitoring` by applying all the manifests
    in each of these directories.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a terminal, enter the following command to install the monitoring stack
    and bbs-warrior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that all the manifests for your monitoring stack and bbs-warrior
    were run without errors. The `-R` flag makes the `kubectl` command recursively
    go through all the application directories and their subdirectories under the
    *monitoring* directory. Without this flag, `kubectl` will skip any nested subdirectories,
    like *grafana/dashboards/*. Prometheus, Grafana, Alertmanager, and bbs-warrior
    should be up and running in a few moments.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the Installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the monitoring stack installation was successful on your Kubernetes cluster,
    you should be able to access Grafana’s, Alertmanager’s, and Prometheus’s web interfaces
    on your browser. In the provided Kubernetes manifest files, I have set the Kubernetes
    Service types for Prometheus, Grafana, and Alertmanager to `NodePort`. A Kubernetes
    *NodePort* Service allows you to connect to an application outside the Kubernetes
    cluster, so you should be able to access each application on the minikube IP address
    and a dynamic port. You should also be able to confirm that the bbs-warrior traffic
    simulator was installed and is running periodically.
  prefs: []
  type: TYPE_NORMAL
- en: Grafana
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In a terminal, enter the following command to open Grafana:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Grafana lives in the `monitoring` Namespace, so this command uses the `-n` (Namespace)
    flag to show the `minikube service` command where to locate the Service. If you
    omit the `-n` flag, minikube will error, as there’s no Service named `grafana-service`
    in the default Namespace. You should now see Grafana open in your web browser,
    with the telnet-server dashboard loaded as the first page. If you don’t see the
    telnet-server dashboard, check the terminal where you ran the `minikube service`
    command for any errors. (You’ll need access to Grafana to follow along with the
    rest of this chapter.) We’ll discuss the graphs on the Grafana dashboard later;
    for now, you should ensure that Grafana is installed correctly and that you can
    open it in your browser.
  prefs: []
  type: TYPE_NORMAL
- en: Alertmanager
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In a terminal, enter the same command you used to open Grafana in your browser,
    but replace the Service name with `alertmanager-service`, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The Alertmanager application should now be open in your browser. This page has
    a few navigation links, like Alerts, Silences, Status, and Help. The Alerts page
    displays current alerts and any metadata, like timestamps and severity associated
    with an alert. The Silences page shows any alerts that have been silenced. You
    can mute or silence an alert for a specific amount of time, which is helpful if
    an alert is being triggered and you don’t want to keep getting paged for it. The
    Status page shows information about Alertmanager, like its version, ready status,
    and current configuration. Alertmanager is configured via the *configmap.yaml*
    file in the *alertmanager/* directory. (You’ll edit this file later to enable
    notifications.) Finally, the Help page is a link to Alertmanager’s documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In your terminal, enter the same command you just entered, but replace `grafana-service`
    with `prometheus-service` to open Prometheus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Prometheus should open in your browser with a few links at the top of the page:
    Alerts, Graph, Status, and Help. The Alerts page displays all known alerts and
    their current state. The Graph page is the default page that allows you to run
    queries against the metric database. The Status page contains information about
    Prometheus’s health and configuration file. Prometheus, like Alertmanager, is
    controlled by the *configmap.yaml* file in the *prometheus* directory. This file
    controls what endpoints Prometheus scrapes for metrics, and it contains the alert
    rules for specific metrics. (We’ll explore the alert rules later.) The Help page
    is a link to Prometheus’s official documentation. For now, you are just confirming
    that Prometheus is running. Leave Prometheus open, as you’ll need it in the next
    section.'
  prefs: []
  type: TYPE_NORMAL
- en: bbs-warrior
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The bbs-warrior application is a Kubernetes CronJob that runs every minute and
    creates a random number of connections and errors to the telnet-server application.
    It also sends a random number of BBS commands (like `date` and `help`) to the
    telnet-server, to mimic typical user activity. About a minute after you install
    bbs-warrior, it should start generating random traffic. This simulation should
    last only a few seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sure bbs-warrior is active and installed correctly in your Kubernetes
    cluster, enter the following command in a terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `-l` (label) flag narrows down the results when searching for CronJobs.
    The output shows that the CronJob was installed over a minute ago (`60s`, under
    the `AGE` column) and that it last ran `25` seconds ago (under the `LAST` `SCHEDULE`
    column). If it were actively running, the `ACTIVE` column would be set to `1`
    rather than `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You now know that the CronJob ran, but you should make sure it completed successfully.
    To do that, you’ll list the Pod with the label `bbs-warrior` in the default Namespace
    and look for `Completed` in the `STATUS` column. In the same terminal you used
    above, enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that the `bbs-warrior` CronJob completed successfully about
    `60` seconds ago. If the CronJob has a status different from `Completed`, check
    the Pod’s logs for errors like you did in Chapter 7.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ve installed and verified your monitoring stack, so now, you should focus
    on what you are monitoring for your telnet-server. Since you want to tailor your
    metrics to user happiness, you should use a common pattern to align all your applications.
    This is always a good approach when instrumenting your services, because allowing
    applications to do their own unique version of metrics makes triaging (and thus,
    on-call shifts) very difficult.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, you’ll explore a common metric pattern called *Golden* *Signals.*
    This provides a subset of metrics to track, like errors and traffic, plus a common
    language for you and your peers to use to discuss what healthy looks like.
  prefs: []
  type: TYPE_NORMAL
- en: Golden Signals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Golden Signals (a term first coined by Google) are four metrics that help
    us understand the health of a microservice. The Golden Signals are latency, traffic,
    errors, and saturation. *Latency* is the time it takes for a service to process
    a request. *Traffic* is how many requests an application is receiving. *Errors*
    refers to the number of errors an application is reporting (such as a web server
    reporting 500s). *Saturation* is how full a service is. For a saturation signal,
    you could measure CPU usage to determine how much headroom is left on the system
    before the application or host becomes slow or unresponsive. You will use this
    pattern often when measuring applications. If you are ever in a situation where
    you don’t know what to monitor, start with the Golden Signals. They’ll provide
    ample information about your application’s health.
  prefs: []
  type: TYPE_NORMAL
- en: A *microservice* typically is an application loosely coupled to other services
    in your platform. It is designed to focus only on one or two aspects of your overall
    domain. In this chapter, the telnet-server application will serve as the microservice
    whose health you will measure.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting the Monitoring Pattern
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Chances are your application will not fit neatly into a predefined monitoring
    pattern like the Golden Signals. Use your best judgment about what matters. For
    example, I decided not to track latency when instrumenting the telnet-server application
    even though the pattern lists it. Users of such an application typically wouldn’t
    connect, run a command, and then quit. You could track latency of the commands,
    or you could add tracing for each command workflow. However, that would be overkill
    for this sample application and beyond the scope of this book. Your commands are
    for demonstration purposes only, so focusing on traffic, errors, and saturation
    signals will provide an overall idea of the application’s health from a user’s
    point of view.
  prefs: []
  type: TYPE_NORMAL
- en: The telnet-server Dashboard
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s review the traffic, saturation, and error signals on your Grafana dashboard.
    In the browser where you first opened Grafana, the telnet-server dashboard has
    three graphs for the Golden Signals and two collapsed graph rows titled System
    and Application (see [Figure 9-2](#figure9-2)). You’ll focus on the Golden Signals
    graphs, which are as follows: Connections per second, Saturation, and Errors per
    second.'
  prefs: []
  type: TYPE_NORMAL
- en: The first graph, Connections per second (in the top left), provides the traffic
    Golden Signal. In this case, you measure how many connections per second you are
    receiving in a two-minute time frame. The telnet-server application increases
    a metric counter each time a connection is made, providing a good idea of how
    many people connect to the application. Many connections could pose an issue with
    performance or reliability. In this example, the x-axis shoots up over 4.0 connections
    per second for both telnet-server Pods. Your graphs will show different results
    from mine since bbs-warrior generates the traffic randomly; the goal is to make
    sure the graphs are being populated.
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of three graphs showing Connections per second (for the system
    and application, in the top left, going up and down), Saturation (which is flat
    at 0s, in the top right), and Errors per second (going up and down, in the bottom
    left)](image_fi/502482c09/f09002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-2: The telnet-server Grafana dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: The Saturation graph (top right) represents the saturation Golden Signal. For
    saturation, you measure the amount of time Kubernetes throttles your telnet-server
    container’s CPU. You set a CPU resource limit of 500 millicpu for the telnet-server
    container in Chapter 7. Therefore, if the telnet-server container uses more than
    the maximum limit, Kubernetes will throttle it, possibly making the telnet-server
    slow to respond to commands or connections. This throttle could potentially cause
    poor performance or a service interruption. In the Saturation graph shown in [Figure
    9-2](#figure9-2), the x-axis is flat at 0 microseconds for both Pods. The line
    of the graph will rise if CPU throttling occurs.
  prefs: []
  type: TYPE_NORMAL
- en: The Errors per second (bottom left) graph maps to the error Golden Signal. For
    this metric, you track the connection errors per second that you receive in a
    two-minute time frame. These errors are incremented when a client fails to connect
    properly or if the connection is killed unexpectedly. A high error rate could
    indicate a code or infrastructure issue that you need to address. In the graph
    shown in [Figure 9-2](#figure9-2), the errors-per-second rate is spiking to 0.4
    for both pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The collapsed two rows at the bottom of this dashboard contain some miscellaneous
    graphs not covered in this chapter, but you should explore them on your own. The
    telnet-server dashboard System row contains two graphs: one for memory, and one
    for CPU usage by the telnet-server Pods. The Application row contains four graphs:
    Total Connections, Active Connections, Connection Error Total, and Unknown Command
    Total.'
  prefs: []
  type: TYPE_NORMAL
- en: The telnet-server dashboard is in the *grafana/dashboards/telnet-server.yaml*
    file. This file is a Kubernetes ConfigMap resource that contains the JSON configuration
    that Grafana requires to create the dashboard and graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'PromQL: A Primer'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*PromQL* is a query language built into the Prometheus application. You use
    it to query and manipulate metric data. Think of PromQL as a distant cousin of
    SQL. It has some built-in functions (like `average` and `sum`) to make querying
    data easier plus conditional logic (like `>` or `=`). We won’t explore this query
    language in depth here except to show how I queried telnet-server’s Golden Signal
    metrics to populate your graphs and alerts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here is the query you enter to generate the Errors-per-second
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The name of the metric is `telnet_server_connection_errors_total`. This metric
    measures the total amount of `connection errors` a user may encounter. The query
    uses Prometheus’ `rate()` function, which calculates the per-second connection
    error average over a specified time interval. You limit the time frame for which
    this query fetches data to two minutes using square brackets `[2m]`. The result
    will show the two running telnet-server Pods you installed in Chapter 7. The curly
    brackets (`{}`) allow you to refine the query, using labels as matchers. Here,
    you specify that you want data only for the `telnet_server_connection_errors`
    metric with the `job="kubernetes-pods"` label.
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating an alert rule in Prometheus, you can enter the same query as
    above to drive the alert. However, this time, you should wrap the results from
    the `rate()` function in a `sum``()` function. You’ll do this because you want
    to know the overall error rate for both Pods. The alert rule should look like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the query, you add greater-than (`>`) conditional logic with
    a number: `2`. This basically means that if the error rate is greater than two
    per second, this alert query evaluates to true. (Later in this chapter, we’ll
    discuss what happens when alert rules are true.)'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to review or tinker with any of these metrics, see the Graph page
    in the Prometheus web interface. [Figure 9-3](#figure9-3) shows the `telnet_server_connection_errors_total`
    query being run.
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot showing a query in the Prometheus dashboard](image_fi/502482c09/f09003Keyline.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-3: Running a query in Prometheus’s web interface'
  prefs: []
  type: TYPE_NORMAL
- en: The query returns connection error data for both Pods. To learn more about PromQL,
    visit [https://prometheus.io/docs/prometheus/latest/querying/basics/](https://prometheus.io/docs/prometheus/latest/querying/basics/)
    for more examples and information*.*
  prefs: []
  type: TYPE_NORMAL
- en: Alerts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Metrics and graphs only constitute half of a monitoring solution. When your
    application decides to take a stroll off a cliff (and it will), someone or something
    needs to know about it. If a Pod dies in a Deployment, Kubernetes just replaces
    it with a new one. But if the Pod keeps restarting, someone needs to address it,
    and that’s where the alerts and notifications come into play.
  prefs: []
  type: TYPE_NORMAL
- en: 'What constitutes a good alert? Besides an alert for each of your application’s
    Golden Signals, you may need an alert around a key metric to monitor. When this
    does happen, keep in mind a couple of guidelines to follow when creating alerts:'
  prefs: []
  type: TYPE_NORMAL
- en: Do not set thresholds too low. Setting alert thresholds too low can cause the
    alerts to repeatedly fire and then clear if you have spiky metrics. This behavior
    is known as *flapping*, and it can be quite normal. The system should not issue
    alerts for flapping metrics every few minutes, because on-call engineers get stressed
    out when they repeatedly get a notification and then find the alarm has already
    cleared.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Avoid creating alerts that are not actionable. Don’t create alerts for a service
    when nothing can be done to remedy it. I call these alerts *status alerts*. Nothing
    is more frustrating to an on-call engineer than being woken up in the middle of
    the night only to babysit an alert that requires no action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For this book, I have provided three alerts called `HighErrorRatePerSecond`,
    `HighConnectionRatePerSecond`, and `HighCPUThrottleRate` (more on these later).
    These alerts are located in the *prometheus.rules* section inside Prometheus’s
    configuration file (*configmap.yaml*). Prometheus uses alert rules to decide whether
    a metric is in an undesired state. An *alert rule* has information like the alert
    name, PromQL query, threshold, and labels. For your example, I have gone against
    my own alert-creation advice and set the provided thresholds extremely low, allowing
    bbs-warrior to trigger the alerts easily. Nothing beats a live example when learning
    about real-time metrics and alerts!
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing Golden Signal Alerts in Prometheus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can view alerts in either Prometheus’s or Alertmanager’s web interfaces.
    The difference is that Alertmanger displays only alerts that are being triggered,
    whereas Prometheus will show all alerts, whether they are firing or not. You want
    to view all the alerts, so you’ll use Prometheus for this example. However, you
    should visit Alertmanager’s interface as well when an alert is being triggered.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the browser where Prometheus was originally opened, click the **Alerts**
    link in the top-left navigation bar. You should see the three provided telnet-server
    Golden Signals alerts: `HighErrorRatePerSecond`, `HighConnectionRatePerSecond`,
    and `HighCPUThrottleRate`. These alerts were created when you installed Prometheus
    earlier. The Alerts page should look like [Figure 9-4](#figure9-4).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot showing the alerts in the Prometheus dashboard. The green alert
    at the top for HighErrorRatePerSecond shows 0 active. The red alert for HighConnectionRatePerSecond
    shows 1 active, and beneath that is the code breakdown with more information.
    At the bottom is the green alert for HighCPUThrottleRate, which shows 0 active.](image_fi/502482c09/f09004Keyline.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-4: Prometheus alerts for telnet-server'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each alert will be in one of three states: Pending (yellow), Inactive (green),
    or Firing (red). In [Figure 9-4](#figure9-4), the `HighConnectionRatePerSecond`
    alert is Firing. The other two alerts, `HighCPUThrottleRate` and `HighErrorRatePerSecond`,
    are Inactive since they are not being triggered. Your Alerts page will be different
    from mine because of bbs-warrior’s randomness. If your page doesn’t show any alerts
    in a Firing state, wait a few minutes until more traffic is generated. Then refresh
    the browser page. In all my testing for this chapter, I always had at least one
    alert transition to a Firing state.'
  prefs: []
  type: TYPE_NORMAL
- en: The `HighErrorRatePerSecond` alert is concerned with the number of connection
    errors received per second. If the rate of connection errors in a two-minute window
    is greater than 2, the alert is in a Firing state. On my local Kubernetes setup,
    the alert is currently in the Inactive state.
  prefs: []
  type: TYPE_NORMAL
- en: The next alert, `HighConnectionRatePerSecond`, detects whether the connection
    rate is greater than 2 per second in a two-minute time frame. Currently, this
    alert is in the Firing state. [Figure 9-4](#figure9-4) shows that the current
    value for my connection rate is more than 9.1 connections per second, which is
    well beyond the set threshold of 2\. I have expanded the alert in the browser
    to show the provided metadata in a key-value layout that an alert provides. In
    the labels section for all three alerts, I have set a label called `severity`
    with a value of `Critical` so it’s easier to distinguish between noncritical alerts
    and ones that need immediate attention. You’ll use this label to route important
    alerts in Alertmanager later. The annotations section includes a description,
    a summary, and a link to a *runbook*, which is a blueprint that provides unfamiliar
    engineers with the what, why, and how for a service. Having this information is
    crucial when sending out an alert notification, because it gives the person on
    call an idea of what to look for when troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: The last alert, `HighCPUThrottleRate`, detects high CPU saturation. If the CPU
    is being throttled by Kubernetes for more than 300 microseconds in a two-minute
    window, you’ll transition to a Firing state. This alert is currently inactive,
    but normally, I’d suggest a minimum five-minute window when tracking CPU throttling.
    This is because smaller time windows can make you more susceptible to alerting
    on a temporarily spiky workload.
  prefs: []
  type: TYPE_NORMAL
- en: Routing and Notifications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You’ve verified that the metrics and alerts are visible and active, so now,
    you should set up Alertmanager to send out email notifications. Once an alert
    is in the Firing state, Prometheus sends it to Alertmanager for routing and notification.
    Notifications can be sent via text messages, push notifications, or email. Alertmanager
    calls these notification methods *receivers*. Routing is used to match on alerts
    and send them to a specific receiver. A common pattern is to route alerts based
    on specific labels. Alert labels are set in the Prometheus *configmap.yaml* file.
    You’ll use this pattern later, when you enable notifications.
  prefs: []
  type: TYPE_NORMAL
- en: The provided Alertmanager configuration is located in the *alertmanager/configmap.yaml*
    file. It is set up to match on all alerts with a `severity` label set to `Critical`
    and route them to a *none receiver*, which is basically a black hole that won’t
    notify anyone when there’s an alert. This means that to see whether an alert is
    being triggered, you would need to visit the web page on either Alertmanager or
    Prometheus. This setup isn’t ideal, as refreshing the web browser every few minutes
    would become tedious, so you’ll route any alert to the `email` receiver if the
    alert has a `severity` label set to `Critical`. If you’re following along, this
    step is completely optional, but it shows you how to configure receivers in Alertmanager.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling Email Notifications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To route an alert to the `email` receiver, you need to edit Alertmanager’s configuration.
    I have stubbed out a template for the `email` receiver and `route` block in the
    *configmap.yaml* file. The email example is based on a Gmail account, but you
    can alter it to accommodate any email provider. See [https://www.prometheus.io/docs/alerting/latest/configuration/#email_config/](https://www.prometheus.io/docs/alerting/latest/configuration/#email_config/)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open Alertmanager’s *configmap.yaml* file in your favorite editor; it should
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, you have two receivers named `email` 1 and `none` 2. The `none` receiver
    won’t send alerts anywhere, but when uncommented, the `email` receiver will send
    alerts to a Gmail account. Uncomment the `email` receiver lines and then replace
    with an email account you can use for testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'After configuring your email settings, change the `receiver` 3 under the `routes`
    section to `email`. This configures Alertmanager to route any alert to the `email`
    receiver if the alert has a `severity` label set to `Critical`. The receiver line
    4 should now look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You’ll still have your default or catch-all receiver 3 set to `none`, so any
    alert that does not match your `severity` label rule will be sent there. Save
    this file, as you are done modifying it.
  prefs: []
  type: TYPE_NORMAL
- en: Applying Alertmanager’s Configuration Changes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Next, you’ll update Alertmanager’s ConfigMap inside the Kubernetes cluster.
    Since the local file contains changes that don’t exist on the cluster, enter the
    following in a terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to tell Kubernetes to restart the Alertmanager Deployment
    so it can pick up the new configuration changes. In the same terminal, enter the
    following command to restart Alertmanager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The Alertmanager Pod should restart after a few moments. If you have any alerts
    in the Firing state, you should start receiving email in your inbox. Depending
    on Alertmanager and your email provider, the notifications may take some time
    to appear.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you do not receive any notification emails, check for a couple of common
    issues. First, make sure the *configmap.yaml* file does not have any typos or
    indentation errors. It is very easy to misalign a YAML file. Second, make sure
    the email settings you entered match what is required by your email provider.
    Look in Alertmanager’s logs to find these and other common issues. Enter the following
    `kubectl` command to view the logs for any errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If you need to disable the notifications for any reason, set the routes receiver
    back to `none` in the *configmap.yaml* file, apply the manifest changes, and restart.
  prefs: []
  type: TYPE_NORMAL
- en: You now have alerts and notifications configured for telnet-server’s Golden
    Signals.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Metrics and alerts are foundational pieces when monitoring an application. They
    provide insight into the health and performance of your service. In this chapter,
    you learned about the Golden Signals monitoring pattern and how to install a modern
    monitoring stack inside a Kubernetes cluster using Prometheus, Alertmanager, and
    Grafana. Finally, you learned how to configure Alertmanager to send email notifications
    for critical alerts.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll shift gears and discuss common troubleshooting scenarios
    you will find on a host or network, plus the tools you can use to diagnose them.
  prefs: []
  type: TYPE_NORMAL
