<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" epub:prefix="index: http://www.index.com/" lang="en" xml:lang="en">
	<head>
		<title>Chapter 5: Web Hacking Reconnaissance</title>
		<link href="NSTemplate_v1.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:f533d35e-262b-449b-a78e-85650cfe86cd" name="Adept.expected.resource"/>
	</head>
	<body epub:type="bodymatter chapter">
		<section>
			<header>
				<h1 class="chapter"><span class="ChapterNumber"><span epub:type="pagebreak" id="Page_61" title="61"/>5</span><br/><span class="ChapterTitle">Web Hacking Reconnaissance</span></h1>
			</header>
			<figure class="opener">
				<img alt="" src="image_fi/book_art/chapterart.png"/>
			</figure>
			<p class="ChapterIntro">The first step to attacking any target is conducting <em>reconnaissance</em>, or simply put, gathering information about the target. Reconnaissance is important because it’s how you figure out an application’s attack surface. To look for bugs most efficiently, you need to discover all the possible ways of attacking a target before deciding on the most effective approach.</p>
			<p>If an application doesn’t use PHP, for instance, there’s no reason to test it for PHP vulnerabilities, and if the organization doesn’t use Amazon Web Services (AWS), you shouldn’t waste time trying to crack its buckets. By understanding how a target works, you can set up a solid foundation for finding vulnerabilities. Recon skills are what separate a good hacker from an ineffective one.</p>
			<p><span epub:type="pagebreak" id="Page_62" title="62"/>In this chapter, I’ll introduce the most useful recon techniques for a bug bounty hunter. Then I’ll walk you through the basics of writing bash scripts to automate recon tasks and make them more efficient. <em>Bash</em> is a shell interpreter available on macOS and Linux systems. Though this chapter assumes you’re using a Linux system, you should be able to install many of these tools on other operating systems as well. You need to install some of the tools we discuss in this chapter before using them. I have included links to all the tools at the end of the chapter.</p>
			<p>Before you go on, please verify that you’re allowed to perform intrusive recon on your target before you attempt any techniques that actively engage with it. In particular, activities like port scanning, spidering, and directory brute-forcing can generate a lot of unwanted traffic on a site and may not be welcomed by the organization.</p>
			<h2 id="h1-501546c05-0001">Manually Walking Through the Target</h2>
			<p>Before we dive into anything else, it will help to first manually walk through the application to learn more about it. Try to uncover every feature in the application that users can access by browsing through every page and clicking every link. Access the functionalities that you don’t usually use.</p>
			<p>For example, if you’re hacking Facebook, try to create an event, play a game, and use the payment functionality if you’ve never done so before. Sign up for an account at every privilege level to reveal all of the application’s features. For example, on Slack, you can create owners, admins, and members of a workspace. Also create users who are members of different channels under the same workspace. This way, you can see what the application looks like to different users.</p>
			<p>
				This should give you a rough idea of what the <em>attack surface</em> (all of the different points at which an attacker can attempt to exploit the application) looks like, where the data entry points are, and how different users interact with each other. Then you can start a more in-depth recon process: finding out the technology and structure of an application.</p>
			<h2 id="h1-501546c05-0002">Google Dorking</h2>
			<p>When hunting for bugs, you’ll often need to research the details of a vulnerability. If you’re exploiting a potential cross-site scripting (XSS) vulnerability, you might want to find a particular payload you saw on GitHub. Advanced search-engine skills will help you find the resources you need quickly and accurately.</p>
			<p>
				In fact, advanced Google searches are a powerful technique that hackers often use to perform recon. Hackers call this <em>Google dorking</em>. For the average Joe, Google is just a text search tool for finding images, videos, and web pages. But for the hacker, Google can be a means of discovering valuable information such as hidden admin portals, unlocked password files, and leaked authentication keys.</p>
			<p><span epub:type="pagebreak" id="Page_63" title="63"/>Google’s search engine has its own built-in query language that helps you filter your searches. Here are some of the most useful operators that can be used with any Google search:</p>
			<p class="ListHead"><b>site</b></p>
			<ol class="none">
				<li>
					Tells Google to show you results from a certain site only. This will help you quickly find the most reputable source on the topic that you are researching. For example, if you wanted to search for the syntax of Python’s <code>print()</code> function, you could limit your results to the official Python documentation with this search: <code>print site:python.org</code>.</li>
			</ol>
			<p class="ListHead"><b>inurl</b></p>
			<ol class="none">
				<li>
					Searches for pages with a URL that match the search string. It’s a powerful way to search for vulnerable pages on a particular website. Let’s say you’ve read a blog post about how the existence of a page called <em>/course/jumpto.php</em> on a website could indicate that it’s vulnerable to remote code execution. You can check if the vulnerability exists on your target by searching<em> </em><code>inurl:"/course/jumpto.php" site:example.com</code>.</li>
			</ol>
			<p class="ListHead"><b>intitle</b></p>
			<ol class="none">
				<li>
					Finds specific strings in a page’s title. This is useful because it allows you to find pages that contain a particular type of content. For example, file-listing pages on web servers often have <em>index of</em> in their titles. You can use this query to search for directory pages on a website:<em> </em><code>intitle:"index of" site:example.com</code><em>.</em></li>
			</ol>
			<p class="ListHead"><b>link</b></p>
			<ol class="none">
				<li>
					Searches for web pages that contain links to a specified URL. You can use this to find documentation about obscure technologies or vulnerabilities. For example, let’s say you’re researching the uncommon regular expression denial-of-service (ReDoS) vulnerability. You’ll easily pull up its definition online but might have a hard time finding examples. The <code>link</code> operator can discover pages that reference the vulnerability’s Wikipedia page to locate discussions of the same topic:<em> </em><code>link:"https://en.wikipedia.org/wiki/ReDoS"</code>.</li>
			</ol>
			<p class="ListHead"><b>filetype</b></p>
			<ol class="none">
				<li>
					Searches for pages with a specific file extension. This is an incredible tool for hacking; hackers often use it to locate files on their target sites that might be sensitive, such as log and password files. For example, this query searches for log files, which often have the <em>.log</em> file extension, on the target site:<em> </em><code>filetype:log site:example.com</code>.</li>
			</ol>
			<p class="ListHead"><b>Wildcard (<code>*</code>)</b></p>
			<ol class="none">
				<li>
					You can use the wildcard operator (<code>*</code>) within searches to mean <em>any character or series of characters</em>. For example, the following query will return any string that starts with <em>how to hack</em> and ends with <em>using Google</em>. It will <span epub:type="pagebreak" id="Page_64" title="64"/>match with strings like <em>how to hack websites using Google</em>, <em>how to hack applications using Google</em>, and so on:<em> </em><code>"how to hack * using Google"</code>.</li>
			</ol>
			<p class="ListHead"><b>Quotes (<code>"</code> <code>"</code>)</b></p>
			<ol class="none">
				<li>
					Adding quotation marks around your search terms forces an exact match. For example, this query will search for pages that contain the phrase <em>how to hack</em>: <code>"how to hack"</code>. And this query will search for pages with the terms <em>how</em>, <em>to</em>, and <em>hack</em>, although not necessarily together: <code>how to hack</code>.</li>
			</ol>
			<p class="ListHead"><b>Or (<code>|</code>)</b></p>
			<ol class="none">
				<li>
					The or operator is denoted with the pipe character (<code>|</code>) and can be used to search for one search term or the other, or both at the same time. The pipe character must be surrounded by spaces. For example, this query will search for <em>how to hack</em> on either Reddit or Stack Overflow:<em> </em><code>"how to hack" site:(reddit.com | stackoverflow.com)</code>. And this query will search for web pages that mention either <em>SQL Injection</em> or <em>SQLi</em>: <code>(SQL Injection | SQLi)</code>. <em>SQLi</em> is an acronym often used to refer to SQL injection attacks, which we’ll talk about in <span class="xref" itemid="xref_target_Chapter 11">Chapter 11</span>.</li>
			</ol>
			<p class="ListHead"><b>Minus (<code>-</code>)</b></p>
			<ol class="none">
				<li>
					The minus operator (<code>-</code>) excludes certain search results. For example, let’s say you’re interested in learning about websites that discuss hacking, but not those that discuss hacking PHP. This query will search for pages that contain <em>how to hack websites</em> but not <em>php</em>:<em> </em><code>"how to hack websites" -php</code>.</li>
			</ol>
			<p>
				You can use advanced search engine options in many more ways to make your work more efficient. You can even search for the term <em>Google search operators</em> to discover more. These operators can be more useful than you’d expect. For example, look for all of a company’s subdomains by searching as follows:</p>
			<pre><code>site:*.example.com</code></pre>
			<p>
				You can also look for special endpoints that can lead to vulnerabilities. <em>Kibana</em> is a data visualization tool that displays server operation data such as server logs, debug messages, and server status. A compromised Kibana instance can allow attackers to collect extensive information about a site’s operation. Many Kibana dashboards run under the path <em>app/kibana</em>, so this query will reveal whether the target has a Kibana dashboard. You can then try to access the dashboard to see if it’s unprotected:</p>
			<pre><code>site:example.com inurl:app/kibana</code></pre>
			<p>
				Google can find company resources hosted by a third party online, such as Amazon S3 buckets (we’ll talk about these in more detail in <span class="xref" itemid="xref_target_“Third-Party Hosting” on page 74">“Third-Party Hosting” on page 74</span>):</p>
			<pre><code>site:s3.amazonaws.com <var>COMPANY_NAME</var></code></pre>
			<p><span epub:type="pagebreak" id="Page_65" title="65"/>Look for special extensions that could indicate a sensitive file. In addition to <em>.log</em>, which often indicates log files, search for <em>.php</em>, <em>cfm</em>, <em>asp</em>, <em>.jsp</em>, and <em>.pl</em>, the extensions often used for script files:</p>
			<pre><code>site:example.com ext:php
site:example.com ext:log</code></pre>
			<p>
				Finally, you can also combine search terms for a more accurate search. For example, this query searches the site <em>example.com</em> for text files that contain <em>password</em>:</p>
			<pre><code>site:example.com ext:txt password</code></pre>
			<p>
				In addition to constructing your own queries, check out the Google Hacking Database (<a class="LinkURL" href="https://www.exploit-db.com/google-hacking-database/">https://www.exploit-db.com/google-hacking-database/</a>), a website that hackers and security practitioners use to share Google search queries for finding security-related information. It contains many search queries that could be helpful to you during the recon process. For example, you can find queries that look for files containing passwords, common URLs of admin portals, or pages built using vulnerable software.</p>
			<p>While you are performing recon using Google search, keep in mind that if you’re sending a lot of search queries, Google will start requiring CAPTCHA challenges for visitors from your network before they can perform more searches. This could be annoying to others on your network, so I don’t recommend Google dorking on a corporate or shared network.</p>
			<h2 id="h1-501546c05-0003">Scope Discovery</h2>
			<p>
				Let’s now dive into recon itself. First, always verify the target’s scope. A program’s <em>scope</em> on its policy page specifies which subdomains, products, and applications you’re allowed to attack. Carefully verify which of the company’s assets are in scope to avoid overstepping boundaries during the recon and hacking process. For example, if <em>example.com</em>’s policy specifies that <em>dev.example.com</em> and <em>test.example.com</em> are out of scope, you shouldn’t perform any recon or attacks on those subdomains.</p>
			<p>Once you’ve verified this, discover what’s actually in the scope. Which domains, subdomains, and IP addresses can you attack? What company assets is the organization hosting on these machines?</p>
			<h3 id="h2-501546c05-0001">WHOIS and Reverse WHOIS</h3>
			<p>
				When companies or individuals register a domain name, they need to supply identifying information, such as their mailing address, phone number, and email address, to a domain registrar. Anyone can then query this information by using the <code>whois</code> command, which searches for the registrant and owner information of each known domain. You might be able to find the associated contact information, such as an email, name, address, or phone number:</p>
			<pre><code>$ <b>whois</b> <var>facebook.com</var></code></pre>
			<p><span epub:type="pagebreak" id="Page_66" title="66"/>This information is not always available, as some organizations and individuals use a service called <em>domain privacy</em>, in which a third-party service provider replaces the user’s information with that of a forwarding service.</p>
			<p>
				You could then conduct a <em>reverse WHOIS</em> search, searching a database by using an organization name, a phone number, or an email address to find domains registered with it. This way, you can find all the domains that belong to the same owner. Reverse WHOIS is extremely useful for finding obscure or internal domains not otherwise disclosed to the public. Use a public reverse WHOIS tool like ViewDNS.info (<a class="LinkURL" href="https://viewdns.info/reversewhois/">https://viewdns.info/reversewhois/</a>) to conduct this search. WHOIS and reverse WHOIS will give you a good set of top-level domains to work with.</p>
			<h3 id="h2-501546c05-0002">IP Addresses</h3>
			<p>
				Another way of discovering your target’s top-level domains is to locate IP addresses. Find the IP address of a domain you know by running the <code>nslookup</code> command. You can see here that <em>facebook.com</em> is located at 157.240.2.35:</p>
			<pre><code>$ <b>nslookup facebook.com</b>
Server: 192.168.0.1
Address: 192.168.0.1#53
Non-authoritative answer:
Name: facebook.com
Address: 157.240.2.35</code></pre>
			<p>
				Once you’ve found the IP address of the known domain, perform a reverse IP lookup. <em>Reverse IP</em> searches look for domains hosted on the same server, given an IP or domain. You can also use ViewDNS.info for this.</p>
			<p>
				Also run the <code>whois</code> command on an IP address, and then see if the target has a dedicated IP range by checking the <code>NetRange</code> field. An <em>IP range</em> is a block of IP addresses that all belong to the same organization. If the organization has a dedicated IP range, any IP you find in that range belongs to that organization:</p>
			<pre><code>$ whois 157.240.2.35<b>NetRange:       157.240.0.0 - 157.240.255.255</b>
CIDR:           157.240.0.0/16
NetName:        THEFA-3
NetHandle:      NET-157-240-0-0-1
Parent:         NET157 (NET-157-0-0-0-0)
NetType:        Direct Assignment
OriginAS:
Organization:   Facebook, Inc. (THEFA-3)
RegDate:        2015-05-14
Updated:        2015-05-14
Ref:            https://rdap.arin.net/registry/ip/157.240.0.0
OrgName:        Facebook, Inc.
OrgId:          THEFA-3
Address:        1601 Willow Rd.
City:           Menlo Park
StateProv:      CA<span epub:type="pagebreak" id="Page_67" title="67"/>PostalCode:     94025
Country:        US
RegDate:        2004-08-11
Updated:        2012-04-17
Ref:            https://rdap.arin.net/registry/entity/THEFA-3
OrgAbuseHandle: OPERA82-ARIN
OrgAbuseName:   Operations
OrgAbusePhone:  +1-650-543-4800
OrgAbuseEmail:  noc@fb.com
OrgAbuseRef:    https://rdap.arin.net/registry/entity/OPERA82-ARIN
OrgTechHandle: OPERA82-ARIN
OrgTechName:   Operations
OrgTechPhone:  +1-650-543-4800
OrgTechEmail:  noc@fb.com
OrgTechRef:    https://rdap.arin.net/registry/entity/OPERA82-ARIN</code></pre>
			<p>
				Another way of finding IP addresses in scope is by looking at autonomous systems, which are routable networks within the public internet. <em>Autonomous system numbers (</em><em>ASNs</em><em>)</em> identify the owners of these networks. By checking if two IP addresses share an ASN, you can determine whether the IPs belong to the same owner.</p>
			<p>To figure out if a company owns a dedicated IP range, run several IP-to-ASN translations to see if the IP addresses map to a single ASN. If many addresses within a range belong to the same ASN, the organization might have a dedicated IP range. From the following output, we can deduce that any IP within the 157.240.2.21 to 157.240.2.34 range probably belongs to Facebook:</p>
			<pre><code>$ <b>whois -h whois.cymru.com 157.240.2.20</b>
AS      | IP               | AS Name
32934   | 157.240.2.20     | FACEBOOK, US
$ <b>whois -h whois.cymru.com 157.240.2.27</b>
AS      | IP               | AS Name
32934   | 157.240.2.27     | FACEBOOK, US
$ <b>whois -h whois.cymru.com 157.240.2.35</b>
AS      | IP               | AS Name
32934   | 157.240.2.35     | FACEBOOK, US</code></pre>
			<p>
				The <code>-h</code> flag in the <code>whois</code> command sets the WHOIS server to retrieve information from, and <em>whois.cymru.com</em> is a database that translates IPs to ASNs. If the company has a dedicated IP range and doesn’t mark those addresses as out of scope, you could plan to attack every IP in that range.</p>
			<h3 id="h2-501546c05-0003">Certificate Parsing</h3>
			<p>
				Another way of finding hosts is to take advantage of the Secure Sockets Layer (SSL) certificates used to encrypt web traffic. An SSL certificate’s <em>Subject Alternative Name</em> field lets certificate owners specify additional hostnames that use the same certificate, so you can find those hostnames by parsing this field. Use online databases like crt.sh, Censys, and Cert Spotter to find certificates for a domain.</p>
			<p><span epub:type="pagebreak" id="Page_68" title="68"/>For example, by running a certificate search using crt.sh for <em>facebook.com</em>, we can find Facebook’s SSL certificate. You’ll see that that many other domain names belonging to Facebook are listed:</p>
			<pre><code>X509v3 Subject Alternative Name: DNS:*.facebook.com DNS:*.facebook.net DNS:*.fbcdn.net DNS:*.fbsbx.com DNS:*.messenger.com DNS:facebook.com DNS:messenger.com DNS:*.m.facebook.com DNS:*.xx.fbcdn.net DNS:*.xy.fbcdn.net DNS:*.xz.fbcdn.net</code></pre>
			<p>
				The crt.sh website also has a useful utility that lets you retrieve the information in JSON format, rather than HTML, for easier parsing. Just add the URL parameter <code>output=json</code> to the request URL: <em>https://crt.sh/?q=facebook.com&amp;output=json.</em></p>
			<h3 id="h2-501546c05-0004">Subdomain Enumeration</h3>
			<p>After finding as many domains on the target as possible, locate as many subdomains on those domains as you can. Each subdomain represents a new angle for attacking the network. The best way to enumerate subdomains is to use automation.</p>
			<p>
				Tools like Sublist3r, SubBrute, Amass, and Gobuster can enumerate subdomains automatically with a variety of wordlists and strategies. For example, Sublist3r works by querying search engines and online subdomain databases, while SubBrute is a brute-forcing tool that guesses possible subdomains until it finds real ones. Amass uses a combination of DNS zone transfers, certificate parsing, search engines, and subdomain databases to find subdomains. You can build a tool that combines the results of multiple tools to achieve the best results. We’ll discuss how to do this in <span class="xref" itemid="xref_target_“Writing Your Own Recon Scripts” on page 80">“Writing Your Own Recon Scripts” on page 80</span>.</p>
			<p>
				To use many subdomain enumeration tools, you need to feed the program a wordlist of terms likely to appear in subdomains. You can find some good wordlists made by other hackers online. Daniel Miessler’s SecLists at <a class="LinkURL" href="https://github.com/danielmiessler/SecLists/">https://github.com/danielmiessler/SecLists/</a><em> </em>is a pretty extensive one. You can also use a wordlist generation tool like Commonspeak2 (<a class="LinkURL" href="https://github.com/assetnote/commonspeak2">https://github.com/assetnote/commonspeak2</a>/) to generate wordlists based on the most current internet data. Finally, you can combine several wordlists found online or that you generated yourself for the most comprehensive results. Here’s a simple command to remove duplicate items from a set of two wordlists:</p>
			<pre><code>sort -u wordlist1.txt wordlist2.txt</code></pre>
			<p><span epub:type="pagebreak" id="Page_69" title="69"/>The <code>sort</code> command line tool sorts the lines of text files. When given multiple files, it will sort all files and write the output to the terminal. The <code>-u</code> option tells <code>sort</code> to return only unique items in the sorted list.</p>
			<p>
				Gobuster is a tool for brute-forcing to discover subdomains, directories, and files on target web servers. Its DNS mode is used for subdomain brute-forcing. In this mode, you can use the flag <code>-d</code> to specify the domain you want to brute-force and <code>-w</code> to specify the wordlist you want to use:</p>
			<pre><code>gobuster dns -d <var>target_domain</var> -w <var>wordlist</var> </code></pre>
			<p>
				Once you’ve found a good number of subdomains, you can discover more by identifying patterns. For example, if you find two subdomains of <em>example.com</em> named <em>1.example.com</em> and <em>3.example.com</em>, you can guess that <em>2.example.com</em> is probably also a valid subdomain. A good tool for automating this process is Altdns (<a class="LinkURL" href="https://github.com/infosec-au/altdns/">https://github.com/infosec-au/altdns/</a>), which discovers subdomains with names that are permutations of other subdomain names.</p>
			<p>
				In addition, you can find more subdomains based on your knowledge about the company’s technology stack. For example, if you’ve already learned that <em>example.com</em> uses Jenkins, you can check if <em>jenkins.example.com</em> is a valid subdomain.</p>
			<p>
				Also look for subdomains of subdomains. After you’ve found, say, <em>dev.example.com</em>, you might find subdomains like <em>1.dev.example.com</em>. You can find subdomains of subdomains by running enumeration tools recursively: add the results of your first run to your Known Domains list and run the tool again.</p>
			<h3 id="h2-501546c05-0005">Service Enumeration</h3>
			<p>Next, enumerate the services hosted on the machines you’ve found. Since services often run on default ports, a good way to find them is by port-scanning the machine with either active or passive scanning.</p>
			<p>
				In <em>active scanning</em>, you directly engage with the server. Active scanning tools send requests to connect to the target machine’s ports to look for open ones. You can use tools like Nmap or Masscan for active scanning. For example, this simple Nmap command reveals the open ports on <em>scanme.nmap.org</em>:</p>
			<pre><code>$ <b>nmap scanme.nmap.org</b>
Nmap scan report for scanme.nmap.org (45.33.32.156)
Host is up (0.086s latency).
Other addresses for scanme.nmap.org (not scanned): 2600:3c01::f03c:91ff:fe18:bb2f
Not shown: 993 closed ports
PORT STATE SERVICE
22/tcp open ssh
25/tcp filtered smtp
80/tcp open http
135/tcp filtered msrpc
445/tcp filtered microsoft-ds
9929/tcp open nping-echo
31337/tcp open Elite
Nmap done: 1 IP address (1 host up) scanned in 230.83 seconds</code></pre>
			<p><span epub:type="pagebreak" id="Page_70" title="70"/>On the other hand, in <em>passive scanning</em>, you use third-party resources to learn about a machine’s ports without interacting with the server. Passive scanning is stealthier and helps attackers avoid detection. To find services on a machine without actively scanning it, you can use <em>Shodan</em>, a search engine that lets the user find machines connected to the internet.</p>
			<p>
				With Shodan, you can discover the presence of webcams, web servers, or even power plants based on criteria such as hostnames or IP addresses. For example, if you run a Shodan search on <em>scanme.nmap.org</em>’s IP address, 45.33.32.156, you get the result in <a href="#figure5-1" id="figureanchor5-1">Figure 5-1</a>. You can see that the search yields different data than our port scan, and provides additional information about the server.</p>
			<figure>
				<img alt="f05001" class="keyline" src="image_fi/501546c05/f05001.png"/>
				<figcaption>
					<p><a id="figure5-1">Figure 5-1</a>: The Shodan results page of <em>scanme.nmap.org</em></p>
				</figcaption>
			</figure>
			<p>Alternatives to Shodan include Censys and Project Sonar. Combine the information you gather from different databases for the best results. With these databases, you might also find your target’s IP addresses, certificates, and software versions.</p>
			<h3 id="h2-501546c05-0006">Directory Brute-Forcing</h3>
			<p>The next thing you can do to discover more of the site’s attack surface is brute-force the directories of the web servers you’ve found. Finding directories on servers is valuable, because through them, you might discover hidden admin panels, configuration files, password files, outdated functionalities, database copies, and source code files. Directory brute-forcing can sometimes allow you to directly take over a server!</p>
			<p>
				Even if you can’t find any immediate exploits, directory information often tells you about the structure and technology of an application. For example, a pathname that includes <em>phpmyadmin</em> usually means that the application is built with PHP.</p>
			<p>
				You can use Dirsearch or Gobuster for directory brute-forcing. These tools use wordlists to construct URLs, and then request these URLs from a web server. If the server responds with a status code in the 200 range, the directory or file exists. This means you can browse to the page and see what <span epub:type="pagebreak" id="Page_71" title="71"/>the application is hosting there. A status code of 404 means that the directory or file doesn’t exist, while 403 means it exists but is protected. Examine 403 pages carefully to see if you can bypass the protection to access the content.</p>
			<p>
				Here’s an example of running a Dirsearch command. The <code>-u</code> flag specifies the hostname, and the <code>-e</code> flag specifies the file extension to use when constructing URLs:</p>
			<pre><code>$ <b>./dirsearch.py -u scanme.nmap.org -e php</b>
Extensions: php | HTTP method: get | Threads: 10 | Wordlist size: 6023
Error Log: /tools/dirsearch/logs/errors.log
Target: scanme.nmap.org
[12:31:11] Starting:
[12:31:13] 403 -  290B  - /.htusers
[12:31:15] 301 -  316B  - /.svn  -&gt;  http://scanme.nmap.org/.svn/
[12:31:15] 403 -  287B  - /.svn/
[12:31:15] 403 -  298B  - /.svn/all-wcprops
[12:31:15] 403 -  294B  - /.svn/entries
[12:31:15] 403 -  297B  - /.svn/prop-base/
[12:31:15] 403 -  296B  - /.svn/pristine/
[12:31:15] 403 -  291B  - /.svn/tmp/
[12:31:15] 403 -  315B  - /.svn/text-base/index.php.svn-base
[12:31:15] 403 -  293B  - /.svn/props/
[12:31:15] 403 -  297B  - /.svn/text-base/
[12:31:40] 301 -  318B  - /images  -&gt;  http://scanme.nmap.org/images/
[12:31:40] 200 -   7KB  - /index
[12:31:40] 200 -   7KB  - /index.html
[12:31:53] 403 -  295B  - /server-status
[12:31:53] 403 -  296B  - /server-status/
[12:31:54] 301 -  318B  - /shared  -&gt;  http://scanme.nmap.org/shared/
Task Completed </code></pre>
			<p>
				Gobuster’s Dir mode is used to find additional content on a specific domain or subdomain. This includes hidden directories and files. In this mode, you can use the <code>-u</code> flag to specify the domain or subdomain you want to brute-force and <code>-w</code> to specify the wordlist you want to use:</p>
			<pre><code>gobuster dir -u <var>target_url</var> -w <var>wordlist</var></code></pre>
			<p>
				Manually visiting all the pages you’ve found through brute-forcing can be time-consuming. Instead, use a screenshot tool like EyeWitness (<a class="LinkURL" href="https://github.com/FortyNorthSecurity/EyeWitness/">https://github.com/FortyNorthSecurity/EyeWitness/</a>) or Snapper (<a class="LinkURL" href="https://github.com/dxa4481/Snapper/">https://github.com/dxa4481/Snapper/</a>) to automatically verify that a page is hosted on each location. EyeWitness accepts a list of URLs and takes screenshots of each page. In a photo gallery app, you can quickly skim these to find the interesting-looking ones. Keep an eye out for hidden services, such as developer or admin panels, directory listing pages, analytics pages, and pages that look outdated and ill-maintained. These are all common places for vulnerabilities to manifest.</p>
			<h3 id="h2-501546c05-0007">Spidering the Site</h3>
			<p>
				Another way of discovering directories and paths is through <em>web spidering</em>, or web crawling, a process used to identify all pages on a site. A web spider tool <span epub:type="pagebreak" id="Page_72" title="72"/>starts with a page to visit. It then identifies all the URLs embedded on the page and visits them. By recursively visiting all URLs found on all pages of a site, the web spider can uncover many hidden endpoints in an application.</p>
			<p>
				OWASP Zed Attack Proxy (ZAP) at <a class="LinkURL" href="https://www.zaproxy.org/">https://www.zaproxy.org/</a> has a built-in web spider you can use (<a href="#figure5-2" id="figureanchor5-2">Figure 5-2</a>). This open source security tool includes a scanner, proxy, and many other features. Burp Suite has an equivalent tool called the <em>crawler</em>, but I prefer ZAP’s spider.</p>
			<figure>
				<img alt="f05002" class="keyline" src="image_fi/501546c05/f05002.png"/>
				<figcaption>
					<p><a id="figure5-2">Figure 5-2</a>: The startup page of OWASP ZAP</p>
				</figcaption>
			</figure>
			<p>
				Access its spider tool by opening ZAP and choosing <b>Tools</b><span class="MenuArrow">▶</span><b>Spider</b> (<a href="#figure5-3" id="figureanchor5-3">Figure 5-3</a>).</p>
			<figure>
				<img alt="f05003" class="keyline" src="image_fi/501546c05/f05003.png"/>
				<figcaption>
					<p><a id="figure5-3">Figure 5-3</a>: You can find the Spider tool via Tools<span class="MenuArrow">▶</span>Spider.</p>
				</figcaption>
			</figure>
			<p><span epub:type="pagebreak" id="Page_73" title="73"/>You should see a window for specifying the starting URL (<a href="#figure5-4" id="figureanchor5-4">Figure 5-4</a>).</p>
			<figure>
				<img alt="f05004" class="keyline" src="image_fi/501546c05/f05004.png"/>
				<figcaption>
					<p><a id="figure5-4">Figure 5-4</a>: You can specify the target URL to scan.</p>
				</figcaption>
			</figure>
			<p>
				Click <b>Start Scan</b>. You should see URLs pop up in the bottom window (<a href="#figure5-5" id="figureanchor5-5">Figure 5-5</a>).</p>
			<figure>
				<img alt="f05005" class="keyline" src="image_fi/501546c05/f05005.png"/>
				<figcaption>
					<p><a id="figure5-5">Figure 5-5</a>: The scan results show up at the bottom pane of the OWASP ZAP window.</p>
				</figcaption>
			</figure>
			<p>
				You should also see a site tree appear on the left side of your ZAP window (<a href="#figure5-6" id="figureanchor5-6">Figure 5-6</a>). This shows you the files and directories found on the target server in an organized format.</p>
			<figure>
				<img alt="f05006" class="keyline" src="image_fi/501546c05/f05006.png"/>
				<figcaption>
					<p><a id="figure5-6">Figure 5-6</a>: The site tree in the left window shows you the files and directories found on the target server.</p>
				</figcaption>
			</figure>
			<h3 id="h2-501546c05-0008"><span epub:type="pagebreak" id="Page_74" title="74"/>Third-Party Hosting</h3>
			<p>
				Take a look at the company’s third-party hosting footprint. For example, look for the organization’s S3 buckets. <em>S3</em>, which stands for <em>Simple Storage Service</em>, is Amazon’s online storage product. Organizations can pay to store resources in <em>buckets</em> to serve in their web applications, or they can use S3 buckets as a backup or storage location. If an organization uses Amazon S3, its S3 buckets can contain hidden endpoints, logs, credentials, user information, source code, and other information that might be useful to you.</p>
			<p>
				How do you find an organization’s buckets? One way is through Google dorking, as mentioned earlier. Most buckets use the URL format <em>BUCKET.s3.amazonaws.com</em> or <em>s3.amazonaws.com/BUCKET</em>, so the following search terms are likely to find results:</p>
			<pre><code>site:s3.amazonaws.com <var>COMPANY_NAME</var>
site:amazonaws.com <var>COMPANY_NAME</var></code></pre>
			<p>
				If the company uses custom URLs for its S3 buckets, try more flexible search terms instead. Companies often still place keywords like <em>aws</em> and <em>s3</em> in their custom bucket URLs, so try these searches:</p>
			<pre><code>amazonaws s3 <var>COMPANY_NAME</var>
amazonaws bucket <var>COMPANY_NAME</var>
amazonaws <var>COMPANY_NAME</var>
s3 <var>COMPANY_NAME</var></code></pre>
			<p>
				Another way of finding buckets is to search a company’s public GitHub repositories for S3 URLs. Try searching these repositories for the term <em>s3</em>. We’ll talk about using GitHub for recon in <span class="xref" itemid="xref_target_“GitHub Recon” on the following page">“GitHub Recon” on the following page</span>.</p>
			<p><em>GrayhatWarfare</em> (<a class="LinkURL" href="https://buckets.grayhatwarfare.com/">https://buckets.grayhatwarfare.com/</a>) is an online search engine you can use to find publicly exposed S3 buckets (<a href="#figure5-7" id="figureanchor5-7">Figure 5-7</a>). It allows you to search for a bucket by using a keyword. Supply keywords related to your target, such as the application, project, or organization name, to find relevant buckets.</p>
			<figure>
				<img alt="f05007" class="keyline" src="image_fi/501546c05/f05007.png"/>
				<figcaption>
					<p><a id="figure5-7">Figure 5-7</a>: The GrayhatWarfare home page</p>
				</figcaption>
			</figure>
			<p>
				Finally, you can try to brute-force buckets by using keywords. <em>Lazys3</em> (<a class="LinkURL" href="https://github.com/nahamsec/lazys3/">https://github.com/nahamsec/lazys3/</a>) is a tool that helps you do this. It relies on a wordlist to guess buckets that are permutations of common <span epub:type="pagebreak" id="Page_75" title="75"/>bucket names. Another good tool is <em>Bucket Stream</em> (<a class="LinkURL" href="https://github.com/eth0izzle/bucket-stream/">https://github.com/eth0izzle/bucket-stream/</a>), which parses certificates belonging to an organization and finds S3 buckets based on permutations of the domain names found on the certificates. Bucket Stream also automatically checks whether the bucket is accessible, so it saves you time.</p>
			<p>Once you’ve found a couple of buckets that belong to the target organization, use the AWS command line tool to see if you can access one. Install the tool by using the following command:</p>
			<pre><code>pip install awscli</code></pre>
			<p>
				Then configure it to work with AWS by following Amazon’s documentation at <a class="LinkURL" href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html">https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html</a>. Now you should be able to access buckets directly from your terminal via the <code>aws s3</code> command. Try listing the contents of the bucket you found:</p>
			<pre><code>aws s3 ls s3://<var>BUCKET_NAME</var>/</code></pre>
			<p>If this works, see if you can read the contents of any interesting files by copying files to your local machine:</p>
			<pre><code>aws s3 cp s3://<var>BUCKET_NAME</var>/<var>FILE_NAME/path/to/local/directory</var></code></pre>
			<p>
				Gather any useful information leaked via the bucket and use it for future exploitation! If the organization reveals information such as active API keys or personal information, you should report this right away. Exposed S3 buckets alone are often considered a vulnerability. You can also try to upload new files to the bucket or delete files from it. If you can mess with its contents, you might be able to tamper with the web application’s operations or corrupt company data. For example, this command will copy your local file named <em>TEST_FILE</em> into the target’s S3 bucket:</p>
			<pre><code>aws s3 cp TEST_FILE s3://<var>BUCKET_NAME</var>/</code></pre>
			<p>
				And this command will remove the <em>TEST_FILE</em> that you just uploaded:</p>
			<pre><code>aws s3 rm s3://<var>BUCKET_NAME</var>/TEST_FILE</code></pre>
			<p>These commands are a harmless way to prove that you have write access to a bucket without actually tampering with the target company’s files.</p>
			<p>Always upload and remove your own test files. Don’t risk deleting important company resources during your testing unless you’re willing to entertain a costly lawsuit.</p>
			<h3 id="h2-501546c05-0009">GitHub Recon</h3>
			<p>Search an organization’s GitHub repositories for sensitive data that has been accidentally committed, or information that could lead to the discovery of a vulnerability.</p>
			<p>
				Start by finding the GitHub usernames relevant to your target. You should be able to locate these by searching the organization’s name or <span epub:type="pagebreak" id="Page_76" title="76"/>product names via GitHub’s search bar, or by checking the GitHub accounts of known employees.</p>
			<p>When you’ve found usernames to audit, visit their pages. Find repositories related to the projects you’re testing and record them, along with the usernames of the organization’s top contributors, which can help you find more relevant repositories.</p>
			<p>
				Then dive into the code. For each repository, pay special attention to the Issues and Commits sections. These sections are full of potential info leaks: they could point attackers to unresolved bugs, problematic code, and the most recent code fixes and security patches. Recent code changes that haven’t stood the test of time are more likely to contain bugs. Look at any protection mechanisms implemented to see if you can bypass them. You can also search the Code section for potentially vulnerable code snippets. Once you’ve found a file of interest, check the Blame and History sections at the top-right corner of the file’s page to see how it was developed (<a href="#figure5-8" id="figureanchor5-8">Figure 5-8</a>).</p>
			<figure>
				<img alt="f05008" class="keyline" src="image_fi/501546c05/f05008.png"/>
				<figcaption>
					<p><a id="figure5-8">Figure 5-8</a>: The History and Blame sections</p>
				</figcaption>
			</figure>
			<p>
				We’ll dive deeper into reviewing source code in <span class="xref" itemid="xref_target_Chapter 22">Chapter 22</span>, but during the recon phase, look for hardcoded secrets such as API keys, encryption keys, and database passwords. Search the organization’s repositories for terms like <em>key</em>, <em>secret</em>, and <em>password</em> to locate hardcoded user credentials that you can use to access internal systems. After you’ve found leaked credentials, you can use KeyHacks (<a class="LinkURL" href="https://github.com/streaak/keyhacks/">https://github.com/streaak/keyhacks/</a>) to check if the credentials are valid and learn how to use them to access the target’s services.</p>
			<p>You should also search for sensitive functionalities in the project. See if any of the source code deals with important functions such as authentication, password reset, state-changing actions, or private info reads. Pay attention to code that deals with user input, such as HTTP request parameters, HTTP headers, HTTP request paths, database entries, file reads, and file uploads, because they provide potential entry points for attackers to exploit the application’s vulnerabilities. Look for any configuration files, as they allow you to gather more information about your infrastructure. Also, search for old endpoints and S3 bucket URLs that you can attack. Record these files for further review in the future.</p>
			<p>Outdated dependencies and the unchecked use of dangerous functions are also a huge source of bugs. Pay attention to dependencies and imports being used and go through the versions list to see if they’re outdated. Record any outdated dependencies. You can use this information later to look for publicly disclosed vulnerabilities that would work on your target.</p>
			<p><span epub:type="pagebreak" id="Page_77" title="77"/>Tools like Gitrob and TruffleHog can automate the GitHub recon process. <em>Gitrob</em> (<a class="LinkURL" href="https://github.com/michenriksen/gitrob/">https://github.com/michenriksen/gitrob/</a>) locates potentially sensitive files pushed to public repositories on GitHub. <em>TruffleHog</em> (<a class="LinkURL" href="https://github.com/trufflesecurity/truffleHog/">https://github.com/trufflesecurity/truffleHog/</a>) specializes in finding secrets in repositories by conducting regex searches and scanning for high-entropy strings.</p>
			<h2 id="h1-501546c05-0004">Other Sneaky OSINT Techniques</h2>
			<p>
				Many of the strategies I discussed so far are all examples of <em>open source intelligence (OSINT)</em>, or the practice of gathering intel from public sources of information. This section details other OSINT sources you might use to extract valuable information.</p>
			<p>First, check the company’s job posts for engineering positions. Engineering job listings often reveal the technologies the company uses. For example, take a look at an ad like this one:</p>
			<p class="ListHead"><b>Full Stack Engineer</b></p>
			<ol class="none">
				<li>Minimum Qualifications:</li>
				<li>Proficiency in Python and C/C++</li>
				<li>Linux experience</li>
				<li>Experience with Flask, Django, and Node.js</li>
				<li>Experience with Amazon Web Services, especially EC2, ECS, S3, and RDS</li>
			</ol>
			<p>From reading this, you know the company uses Flask, Django, and Node.js to build its web applications. The engineers also probably use Python, C, and C++ on the backend with a Linux machine. Finally, they use AWS to outsource their operations and file storage.</p>
			<p>If you can’t find relevant job posts, search for employees’ profiles on LinkedIn, and read employees’ personal blogs or their engineering questions on forums like Stack Overflow and Quora. The expertise of a company’s top employees often reflects the technology used in development.</p>
			<p>Another source of information is the employees’ Google calendars. People’s work calendars often contain meeting notes, slides, and sometimes even login credentials. If an employee shares their calendars with the public by accident, you could gain access to these. The organization or its employees’ social media pages might also leak valuable information. For example, hackers have actually discovered sets of valid credentials on Post-it Notes visible in the background of office selfies!</p>
			<p>If the company has an engineering mailing list, sign up for it to gain insight into the company’s technology and development process. Also check the company’s SlideShare or Pastebin accounts. Sometimes, when organizations present at conferences or have internal meetings, they upload slides to SlideShare for reference. You might be able to find information about the technology stack and security challenges faced by the company.</p>
			<p>
				Pastebin (<a class="LinkURL" href="https://pastebin.com/">https://pastebin.com/</a>) is a website for pasting and storing text online for a short time. People use it to share text across machines or with others. Engineers sometimes use it to share source code or server logs with their colleagues for viewing or collaboration, so it could be a great source of <span epub:type="pagebreak" id="Page_78" title="78"/>information. You might also find uploaded credentials and development comments. Go to Pastebin, search for the target’s organization name, and see what happens! You can also use automated tools like PasteHunter (<a class="LinkURL" href="https://github.com/kevthehermit/PasteHunter/">https://github.com/kevthehermit/PasteHunter/</a>) to scan for publicly pasted data.</p>
			<p>
				Lastly, consult archive websites like the Wayback Machine (<a class="LinkURL" href="https://archive.org/web/">https://archive.org/web/</a>), a digital record of internet content (<a href="#figure5-9" id="figureanchor5-9">Figure 5-9</a>). It records a site’s content at various points in time. Using the Wayback Machine, you can find old endpoints, directory listings, forgotten subdomains, URLs, and files that are outdated but still in use. Tomnomnom’s tool Waybackurls (<a class="LinkURL" href="https://github.com/tomnomnom/waybackurls/">https://github.com/tomnomnom/waybackurls/</a>) can automatically extract endpoints and URLs from the Wayback Machine.</p>
			<figure>
				<img alt="f05009" class="keyline" src="image_fi/501546c05/f05009.png"/>
				<figcaption>
					<p><a id="figure5-9">Figure 5-9</a>: The Wayback Machine archives the internet and allows you to see pages that have been removed by a website.</p>
				</figcaption>
			</figure>
			<h2 id="h1-501546c05-0005">Tech Stack Fingerprinting</h2>
			<p>
				Fingerprinting techniques can help you understand the target application even better. <em>Fingerprinting </em>is identifying the software brands and versions that a machine or an application uses. This information allows you to perform targeted attacks on the application, because you can search for any known misconfigurations and publicly disclosed vulnerabilities related to a particular version. For example, if you know the server is using an old version of Apache that could be impacted by a disclosed vulnerability, you can immediately attempt to attack the server using it.</p>
			<p>
				The security community classifies known vulnerabilities as <em>Common Vulnerabilities and Exposures (</em><em>CVEs)</em> and gives each CVE a number for reference. Search for them on the CVE database (<a class="LinkURL" href="https://cve.mitre.org/cve/search_cve_list.html">https://cve.mitre.org/cve/search_cve_list.html</a><em>).</em></p>
			<p>
				The simplest way of fingerprinting an application is to engage with the application directly. First, run Nmap on a machine with the <code>-sV</code> flag on to enable version detection on the port scan. Here, you can see that Nmap attempted to fingerprint some software running on the target host for us:</p>
			<pre><code>$ <b>nmap scanme.nmap.org -sV</b>
Starting Nmap 7.60 ( https://nmap.org )
Nmap scan report for scanme.nmap.org (45.33.32.156)<span epub:type="pagebreak" id="Page_79" title="79"/>Host is up (0.065s latency).
Other addresses for scanme.nmap.org (not scanned): 2600:3c01::f03c:91ff:fe18:bb2f
Not shown: 992 closed ports
PORT STATE SERVICE VERSION
22/tcp open ssh <b>OpenSSH 6.6.1p1 Ubuntu 2ubuntu2.13 (Ubuntu Linux; protocol 2.0)</b>
25/tcp filtered smtp
80/tcp open http <b>Apache httpd 2.4.7 ((Ubuntu))</b>
135/tcp filtered msrpc
139/tcp filtered netbios-ssn
445/tcp filtered microsoft-ds
9929/tcp open nping-echo <b>Nping echo</b>
31337/tcp open tcpwrapped
Service Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel
Service detection performed. Please report any incorrect results at https://nmap.org/submit/.
Nmap done: 1 IP address (1 host up) scanned in 9.19 seconds</code></pre>
			<p>Next, in Burp, send an HTTP request to the server to check the HTTP headers used to gain insight into the tech stack. A server might leak many pieces of information useful for fingerprinting its technology:</p>
			<pre><code>Server: Apache/2.0.6 (Ubuntu)
X-Powered-By: PHP/5.0.1
X-Generator: Drupal 8
X-Drupal-Dynamic-Cache: UNCACHEABLE
Set-Cookie: PHPSESSID=abcde;</code></pre>
			<p>
				HTTP headers like <code>Server</code> and <code>X-Powered-By</code> are good indicators of technologies. The <code>Server</code> header often reveals the software versions running on the server. <code>X-Powered-By</code> reveals the server or scripting language used. Also, certain headers are used only by specific technologies. For example, only Drupal uses <code>X-Generator</code> and <code>X-Drupal-Dynamic-Cache</code>. Technology-specific cookies such as <code>PHPSESSID</code> are also clues; if a server sends back a cookie named <code>PHPSESSID</code>, it’s probably developed using PHP.</p>
			<p>
				The HTML source code of web pages can also provide clues. Many web frameworks or other technologies will embed a signature in source code. Right-click a page, select <b>View Source Code</b>, and press <span class="KeyCaps">CTRL</span>-F to search for phrases like <em>powered by</em>, <em>built with</em>, and <em>running</em>. For instance, you might find <code>Powered by: WordPress 3.3.2</code> written in the source.</p>
			<p>
				Check technology-specific file extensions, filenames, folders, and directories. For example, a file named <em>phpmyadmin</em> at the root directory, like <em>https://example.com/phpmyadmin</em>, means the application runs PHP. A directory named <em>jinja2</em> that contains templates means the site probably uses Django and Jinja2. You can find more information about a specific technology’s filesystem signatures by visiting its individual documentation.</p>
			<p>
				Several applications can automate this process. <em>Wappalyzer</em> (<a class="LinkURL" href="https://www.wappalyzer.com/">https://www.wappalyzer.com/</a>) is a browser extension that identifies content management systems, frameworks, and programming languages used on a site. <em>BuiltWith</em> (<a class="LinkURL" href="https://builtwith.com/">https://builtwith.com/</a>) is a website that shows you which web technologies a site is built with. <em>StackShare</em> (<a class="LinkURL" href="https://stackshare.io/">https://stackshare.io/</a>) is an online platform that allows developers to share the tech they use. You can use it to find out if the organization’s developers have posted their tech stack. Finally, <span epub:type="pagebreak" id="Page_80" title="80"/>Retire.js is a tool that detects outdated JavaScript libraries and Node.js packages. You can use it to check for outdated technologies on a site.</p>
			<h2 id="h1-501546c05-0006">Writing Your Own Recon Scripts</h2>
			<p>You’ve probably realized by now that good recon is an extensive process. But it doesn’t have to be time-consuming or hard to manage. We’ve already discussed several tools that use the power of automation to make the process easier.</p>
			<p>
				Sometimes you may find it handy to write your own scripts. A <em>script</em> is a list of commands designed to be executed by a program. They’re used to automate tasks such as data analysis, web-page generation, and system administration. For us bug bounty hunters, scripting is a way of quickly and efficiently performing recon, testing, and exploitation. For example, you could write a script to scan a target for new subdomains, or enumerate potentially sensitive files and directories on a server. Once you’ve learned how to script, the possibilities are endless.</p>
			<p>
				This section covers bash scripts in particular—what they are and why you should use them. You’ll learn how to use bash to simplify your recon process and even write your own tools. I’ll assume that you have basic knowledge of how programming languages work, including variables, conditionals, loops, and functions, so if you’re not familiar with these concepts, please take an introduction to coding class at Codecademy (<a class="LinkURL" href="https://www.codecademy.com/">https://www.codecademy.com/</a>) or read a programming book.</p>
			<p>Bash scripts, or any type of shell script, are useful for managing complexities and automating recurrent tasks. If your commands involve multiple input parameters, or if the input of one command depends on the output of another, entering it all manually could get complicated quickly and increase the chance of a programming mistake. On the other hand, you might have a list of commands that you want to execute many, many times. Scripts are useful here, as they save you the trouble of typing the same commands over and over again. Just run the script each time and be done with it.</p>
			<h3 id="h2-501546c05-0010">Understanding Bash Scripting Basics</h3>
			<p>
				Let’s write our first script. Open any text editor to follow along. The first line of every shell script you write should be the <em>shebang</em><em> line</em>. It starts with a hash mark (<code>#</code>) and an exclamation mark (<code>!</code>), and it declares the interpreter to use for the script. This allows the plaintext file to be executed like a binary. We’ll use it to indicate that we’re using bash.</p>
			<p>Let’s say we want to write a script that executes two commands; it should run Nmap and then Dirsearch on a target. We can put the commands in the script like this:</p>
			<pre><code>#!/bin/bash
nmap scanme.nmap.org
/PATH/TO/dirsearch.py -u scanme.nmap.org -e php</code></pre>
			<p><span epub:type="pagebreak" id="Page_81" title="81"/>This script isn’t very useful; it can scan only one site, <em>scanme.nmap.org</em>. Instead, we should let users provide input arguments to the bash script so they can choose the site to scan. In bash syntax, <code>$1</code> represents the first argument passed in, <code>$2</code> is the second argument, and so on. Also, <code>$@</code> represents all arguments passed in, while <code>$#</code> represents the total number of arguments. Let’s allow users to specify their targets with the first input argument, assigned to the variable <code>$1</code>:</p>
			<pre><code>#!/bin/bash
nmap $1
/PATH/TO/dirsearch.py -u $1 -e php</code></pre>
			<p>Now the commands will execute for whatever domain the user passes in as the first argument.</p>
			<p>
				Notice that the third line of the script includes <em>/PATH/TO/dirsearch.py</em>. You should replace <em>/PATH/TO/</em> with the absolute path of the directory where you stored the Dirsearch script. If you don’t specify its location, your computer will try to look for it in the current directory, and unless you stored the Dirsearch file in the same directory as your shell script, bash won’t find it.</p>
			<p>
				Another way of making sure that your script can find the commands to use is through the <code>PATH</code> variable, an environmental variable in Unix systems that specifies where executable binaries are found. If you run this command to add Dirsearch’s directory to your <code>PATH</code>, you can run the tool from anywhere without needing to specify its absolute path:</p>
			<pre><code>export PATH="PATH_TO_DIRSEARCH:$PATH"</code></pre>
			<p>After executing this command, you should be able to use Dirsearch directly:</p>
			<pre><code>#!/bin/bash
nmap $1
dirsearch.py -u $1 -e php</code></pre>
			<p>
				Note that you will have to run the <code>export</code> command again after you restart your terminal for your <code>PATH</code> to contain the path to Dirsearch. If you don’t want to export <code>PATH</code> over and over again, you can add the <code>export</code> command to your <em>~/</em><em>.bash_profile</em><em> </em>file, a file that stores your bash preferences and configuration. You can do this by opening <em>~/.bash_profile </em>with your favorite text editor and adding the <code>export</code> command to the bottom of the file.</p>
			<p>
				The script is complete! Save it in your current directory with the filename <em>recon.sh</em>. The <em>.sh</em> extension is the conventional extension for shell scripts. Make sure your terminal’s working directory is the same as the one where you’ve stored your script by running the command <code class="bold">cd /</code><var class="bold">location</var><code class="bold">/</code><var class="bold">of</var><code class="bold">/</code><var class="bold">your</var><code class="bold">/</code><var class="bold">script</var>. Execute the script in the terminal with this command:</p>
			<pre><code>$ ./recon.sh</code></pre>
			<p>You might see a message like this:</p>
			<pre><code>permission denied: ./recon.sh</code></pre>
			<p><span epub:type="pagebreak" id="Page_82" title="82"/>This is because the current user doesn’t have permission to execute the script. For security purposes, most files aren’t executable by default. You can correct this behavior by adding executing rights for everyone by running this command in the terminal:</p>
			<pre><code>$ chmod +x recon.sh</code></pre>
			<p>
				The <code>chmod</code> command edits the permissions for a file, and <code>+x</code> indicates that we want to add the permission to execute for all users. If you’d like to grant executing rights for the owner of the script only, use this command instead:</p>
			<pre><code>$ chmod 700 recon.sh</code></pre>
			<p>
				Now run the script as we did before. Try passing in <em>scanme.nmap.org</em> as the first argument. You should see the output of the Nmap and Dirsearch printed out:</p>
			<pre><code>$ <b>./recon.sh scanme.nmap.org</b>
Starting Nmap 7.60 ( https://nmap.org )
Nmap scan report for scanme.nmap.org (45.33.32.156)
Host is up (0.062s latency).
Other addresses for scanme.nmap.org (not scanned): 2600:3c01::f03c:91ff:fe18:bb2f
Not shown: 992 closed ports
PORT      STATE    SERVICE
22/tcp    open     ssh
25/tcp    filtered smtp
80/tcp    open     http
135/tcp   filtered msrpc
139/tcp   filtered netbios-ssn
445/tcp   filtered microsoft-ds
9929/tcp  open     nping-echo
31337/tcp open     Elite
Nmap done: 1 IP address (1 host up) scanned in 2.16 seconds
Extensions: php | HTTP method: get | Threads: 10 | Wordlist size: 6023
Error Log: /Users/vickieli/tools/dirsearch/logs/errors.log
Target: scanme.nmap.org
[11:14:30] Starting:
[11:14:32] 403 -  295B  - /.htaccessOLD2
[11:14:32] 403 -  294B  - /.htaccessOLD
[11:14:33] 301 -  316B  - /.svn  -&gt;  http://scanme.nmap.org/.svn/
[11:14:33] 403 -  298B  - /.svn/all-wcprops
[11:14:33] 403 -  294B  - /.svn/entries
[11:14:33] 403 -  297B  - /.svn/prop-base/
[11:14:33] 403 -  296B  - /.svn/pristine/
[11:14:33] 403 -  315B  - /.svn/text-base/index.php.svn-base
[11:14:33] 403 -  297B  - /.svn/text-base/
[11:14:33] 403 -  293B  - /.svn/props/
[11:14:33] 403 -  291B  - /.svn/tmp/
[11:14:55] 301 -  318B  - /images  -&gt;  http://scanme.nmap.org/images/ 
[11:14:56] 200 -    7KB - /index
[11:14:56] 200 -    7KB - /index.html<span epub:type="pagebreak" id="Page_83" title="83"/>[11:15:08] 403 -  296B  - /server-status/
[11:15:08] 403 -  295B  - /server-status
[11:15:08] 301 -  318B  - /shared  -&gt;  http://scanme.nmap.org/shared/
Task Completed</code></pre>
			<h3 id="h2-501546c05-0011">Saving Tool Output to a File</h3>
			<p>
				To analyze the recon results later, you may want to save your scripts’ output in a separate file. This is where input and output redirection come into play. <em>Input redirection</em><em> </em>is using the content of a file, or the output of another program, as the input to your script. <em>Output redirection</em> is redirecting the output of a program to another location, such as to a file or another program. Here are some of the most useful redirection operators:</p>
			<ol class="none">
				<li><span class="RunInHead"><var>PROGRAM</var><span class="LiteralBold"><code> &gt; </code></span><var>FILENAME</var> </span>  Writes the program’s output into the file with that name. (It will clear any content from the file first. It will also create the file if the file does not already exist.)</li>
				<li><span class="RunInHead"><var>PROGRAM</var><span class="LiteralBold"><code> &gt;&gt; </code></span><var>FILENAME</var> </span>  Appends the output of the program to the end of the file, without clearing the file’s original content.</li>
				<li><span class="RunInHead"><var>PROGRAM</var><span class="LiteralBold"><code> &lt; </code></span><var>FILENAME</var> </span>  Reads from the file and uses its content as the program input.</li>
				<li><span class="RunInHead"><var>PROGRAM1</var><span class="LiteralBold"><code> | </code></span><var>PROGRAM2</var> </span>  Uses the output of <var>PROGRAM1</var> as the input to <var>PROGRAM2</var>.</li>
			</ol>
			<p>We could, for example, write the results of the Nmap and Dirsearch scans into different files:</p>
			<pre><code>#!/bin/bash
echo "Creating directory $1_recon." <span aria-label="annotation1" class="CodeAnnotationCode">1</span>
mkdir $1_recon <span aria-label="annotation2" class="CodeAnnotationCode">2</span>
nmap $1 &gt; $1_recon/nmap <span aria-label="annotation3" class="CodeAnnotationCode">3</span>
echo "The results of nmap scan are stored in $1_recon/nmap."
/PATH/TO/dirsearch.py -u $1 -e php <span aria-label="annotation4" class="CodeAnnotationCode">4</span> --simple-report=$1_recon/dirsearch
echo "The results of dirsearch scan are stored in $1_recon/dirsearch."</code></pre>
			<p>
				The <code>echo</code> command <span aria-label="annotation1" class="CodeAnnotation">1</span> prints a message to the terminal. Next, <code>mkdir</code> creates a directory with the name <em>DOMAIN_recon</em> <span aria-label="annotation2" class="CodeAnnotation">2</span>. We store the results of <code>nmap</code> into a file named <em>nmap</em> in the newly created directory <span aria-label="annotation3" class="CodeAnnotation">3</span>. Dirsearch’s <code>simple-report</code> flag <span aria-label="annotation4" class="CodeAnnotation">4</span> generates a report in the designated location. We store the results of Dirsearch to a file named <em>dirsearch</em> in the new directory.</p>
			<p>
				You can make your script more manageable by introducing variables to reference files, names, and values. Variables in bash can be assigned using the following syntax: <var>VARIABLE_NAME</var><code>=</code><var>VARIABLE_VALUE</var>. Note that there should be no spaces around the equal sign. The syntax for referencing variables is <code>$</code><var>VARIABLE_NAME</var>. Let’s implement these into the script:</p>
			<pre><code>#!/bin/bash
PATH_TO_DIRSEARCH="/Users/vickieli/tools/dirsearch"
DOMAIN=$1
DIRECTORY=${DOMAIN}_recon <span aria-label="annotation1" class="CodeAnnotationCode">1</span>
echo "Creating directory $DIRECTORY."
mkdir $DIRECTORY<span epub:type="pagebreak" id="Page_84" title="84"/>nmap $DOMAIN &gt; $DIRECTORY/nmap
echo "The results of nmap scan are stored in $DIRECTORY/nmap."
$PATH_TO_DIRSEARCH/dirsearch.py -u $DOMAIN -e php –simple-report=$DIRECTORY/dirsearch <span aria-label="annotation2" class="CodeAnnotationCode">2</span>
echo "The results of dirsearch scan are stored in $DIRECTORY/dirsearch."</code></pre>
			<p>
				We use <code>${DOMAIN}_recon</code> instead of <code>$DOMAIN_recon</code> <span aria-label="annotation1" class="CodeAnnotation">1</span> because, otherwise, bash would recognize the entirety of <code>DOMAIN_recon</code> as the variable name. The curly brackets tell bash that <code>DOMAIN</code> is the variable name, and <code>_recon</code> is the plaintext we’re appending to it. Notice that we also stored the path to Dirsearch in a variable to make it easy to change in the future <span aria-label="annotation2" class="CodeAnnotation">2</span>.</p>
			<p>Using redirection, you can now write shell scripts that run many tools in a single command and save their outputs in separate files.</p>
			<h3 id="h2-501546c05-0012">Adding the Date of the Scan to the Output</h3>
			<p>Let’s say you want to add the current date to your script’s output, or select which scans to run, instead of always running both Nmap and Dirsearch. If you want to write tools with more functionalities like this, you have to understand some advanced shell scripting concepts.</p>
			<p>
				For example, a useful one is <em>command substitution</em>, or operating on the output of a command. Using <code>$()</code> tells Unix to execute the command surrounded by the parentheses and assign its output to the value of a variable. Let’s practice using this syntax:</p>
			<pre><code>#!/bin/bash
PATH_TO_DIRSEARCH="/Users/vickieli/tools/dirsearch"
TODAY=$(date) <span aria-label="annotation1" class="CodeAnnotationCode">1</span>
echo "This scan was created on $TODAY" <span aria-label="annotation2" class="CodeAnnotationCode">2</span>
DOMAIN=$1
DIRECTORY=${DOMAIN}_recon
echo "Creating directory $DIRECTORY."
mkdir $DIRECTORY
nmap $DOMAIN &gt; $DIRECTORY/nmap
echo "The results of nmap scan are stored in $DIRECTORY/nmap."
$PATH_TO_DIRSEARCH/dirsearch.py -u $DOMAIN -e php --simple-report=$DIRECTORY/dirsearch
echo "The results of dirsearch scan are stored in $DIRECTORY/dirsearch."</code></pre>
			<p>
				At <span aria-label="annotation1" class="CodeAnnotation">1</span>, we assign the output of the <code>date</code> command to the variable <code>TODAY</code>. The <code>date</code> command displays the current date and time. This lets us output a message indicating the day on which we performed the scan <span aria-label="annotation2" class="CodeAnnotation">2</span>.</p>
			<h3 id="h2-501546c05-0013">Adding Options to Choose the Tools to Run</h3>
			<p>
				Now, to selectively run only certain tools, you need to use conditionals. In bash, the syntax of an <code>if</code> statement is as follows. Note that the conditional statement ends with the <code>fi</code> keyword, which is <code>if</code> backward:</p>
			<pre><code>if [ <var>condition 1</var> ]
then # Do if condition 1 is satisfied
elif [ <var>condition 2</var> ]
then<span epub:type="pagebreak" id="Page_85" title="85"/>  # Do if condition 2 is satisfied, and condition 1 is not satisfied
else # Do something else if neither condition is satisfied
fi</code></pre>
			<p>
				Let’s say that we want users to be able to specify the scan <code>MODE</code>, as such:</p>
			<pre><code>$ ./recon.sh scanmme.nmap.org MODE</code></pre>
			<p>We can implement this functionality like this:</p>
			<pre><code>#!/bin/bash
PATH_TO_DIRSEARCH="/Users/vickieli/tools/dirsearch"
TODAY=$(date)
echo "This scan was created on $TODAY"
DIRECTORY=${DOMAIN}_recon
echo "Creating directory $DIRECTORY."
mkdir $DIRECTORY 
if [ $2 == "nmap-only" ] <span aria-label="annotation1" class="CodeAnnotationCode">1</span>
then nmap $DOMAIN &gt; $DIRECTORY/nmap <span aria-label="annotation2" class="CodeAnnotationCode">2</span> echo "The results of nmap scan are stored in $DIRECTORY/nmap."
elif [ $2 == "dirsearch-only" ] <span aria-label="annotation3" class="CodeAnnotationCode">3</span>
then   $PATH_TO_DIRSEARCH/dirsearch.py -u $DOMAIN -e php –simple-report=$DIRECTORY/dirsearch <span aria-label="annotation4" class="CodeAnnotationCode">4</span> echo "The results of dirsearch scan are stored in $DIRECTORY/dirsearch."
else <span aria-label="annotation5" class="CodeAnnotationCode">5</span> nmap $DOMAIN &gt; $DIRECTORY/nmap <span aria-label="annotation6" class="CodeAnnotationCode">6</span> echo "The results of nmap scan are stored in $DIRECTORY/nmap." $PATH_TO_DIRSEARCH/dirsearch.py -u $DOMAIN -e php --simple-report=$DIRECTORY/dirsearch echo "The results of dirsearch scan are stored in $DIRECTORY/dirsearch."
fi</code></pre>
			<p>
				If the user specifies <code>nmap-only</code> <span aria-label="annotation1" class="CodeAnnotation">1</span>, we run <code>nmap</code> only and store the results to a file named <em>nmap</em> <span aria-label="annotation2" class="CodeAnnotation">2</span>. If the user specifies <code>dirsearch-only</code> <span aria-label="annotation3" class="CodeAnnotation">3</span>, we execute and store the results of Dirsearch only <span aria-label="annotation4" class="CodeAnnotation">4</span>. If the user specifies neither <span aria-label="annotation5" class="CodeAnnotation">5</span>, we run both scans <span aria-label="annotation6" class="CodeAnnotation">6</span>.</p>
			<p>Now you can make your tool run only the Nmap or Dirsearch commands by specifying one of these in the command:</p>
			<pre><code>$ ./recon.sh scanme.nmap.org nmap-only
$ ./recon.sh scanme.nmap.org dirsearch-only</code></pre>
			<h3 id="h2-501546c05-0014">Running Additional Tools</h3>
			<p>What if you want the option of retrieving information from the crt.sh tool, as well? For example, you want to switch between these three modes or run all three recon tools at once:</p>
			<pre><code>$ ./recon.sh scanme.nmap.org nmap-only
$ ./recon.sh scanme.nmap.org dirsearch-only
$ ./recon.sh scanme.nmap.org crt-only</code></pre>
			<p><span epub:type="pagebreak" id="Page_86" title="86"/>We could rewrite the <code>if-else</code> statements to work with three options: first, we check if <code>MODE</code> is <code>nmap-only</code>. Then we check if <code>MODE</code> is <code>dirsearch-only</code>, and finally if <code>MODE</code> is <code>crt-only</code>. But that’s a lot of <code>if-else</code> statements, making the code complicated.</p>
			<p>
				Instead, let’s use bash’s <code>case</code> statements, which allow you to match several values against one variable without going through a long list of <code>if-else</code> statements. The syntax of <code>case</code> statements looks like this. Note that the statement ends with <code>esac</code>, or <code>case</code> backward:</p>
			<pre><code>case $<var>VARIABLE_NAME</var> in case1) <var>Do something</var> ;; case2) <var>Do something</var> ;;  caseN) <var>Do something</var> ;; *)        Default case, this case is executed if no other case matches. ;; 
esac</code></pre>
			<p>
				We can improve our script by implementing the functionality with <code>case</code> statements instead of multiple <code>if-else</code> statements:</p>
			<pre><code>#!/bin/bash
PATH_TO_DIRSEARCH="/Users/vickieli/tools/dirsearch"
TODAY=$(date)
echo "This scan was created on $TODAY"
DOMAIN=$1
DIRECTORY=${DOMAIN}_recon
echo "Creating directory $DIRECTORY."
mkdir $DIRECTORY
case $2 in nmap-only) nmap $DOMAIN &gt; $DIRECTORY/nmap echo "The results of nmap scan are stored in $DIRECTORY/nmap." ;; dirsearch-only) $PATH_TO_DIRSEARCH/dirsearch.py -u $DOMAIN -e php --simple-report=$DIRECTORY/dirsearch echo "The results of dirsearch scan are stored in $DIRECTORY/dirsearch." ;;          crt-only)        curl "https://crt.sh/?q=$DOMAIN&amp;output=json" -o $DIRECTORY/crt <span aria-label="annotation1" class="CodeAnnotationCode">1</span> echo "The results of cert parsing is stored in $DIRECTORY/crt." ;; *) nmap $DOMAIN &gt; $DIRECTORY/nmap echo "The results of nmap scan are stored in $DIRECTORY/nmap." $PATH_TO_DIRSEARCH/dirsearch.py -u $DOMAIN -e php --simple-report=$DIRECTORY/dirsearch echo "The results of dirsearch scan are stored in $DIRECTORY/dirsearch."<span epub:type="pagebreak" id="Page_87" title="87"/>    curl "https://crt.sh/?q=$DOMAIN&amp;output=json" -o $DIRECTORY/crt echo "The results of cert parsing is stored in $DIRECTORY/crt." ;;
esac</code></pre>
			<p>
				The <code>curl</code> command <span aria-label="annotation1" class="CodeAnnotation">1</span> downloads the content of a page. We use it here to download data from crt.sh. And <code>curl</code>’s <code>-o</code> option lets you specify an output file. But notice that our code has a lot of repetition! The sections of code that run each type of scan repeat twice. Let’s try to reduce the repetition by using functions. The syntax of a bash function looks like this:</p>
			<pre><code><var>FUNCTION_NAME()</var>
{ <var>DO_SOMETHING</var>
}</code></pre>
			<p>After you’ve declared a function, you can call it like any other shell command within the script. Let’s add functions to the script:</p>
			<pre><code>#!/bin/bash
PATH_TO_DIRSEARCH="/Users/vickieli/tools/dirsearch"
TODAY=$(date)
echo "This scan was created on $TODAY"
DOMAIN=$1
DIRECTORY=${DOMAIN}_recon
echo "Creating directory $DIRECTORY."
mkdir $DIRECTORY
nmap_scan() <span aria-label="annotation1" class="CodeAnnotationCode">1</span>
{ nmap $DOMAIN &gt; $DIRECTORY/nmap echo "The results of nmap scan are stored in $DIRECTORY/nmap."
}
dirsearch_scan() <span aria-label="annotation2" class="CodeAnnotationCode">2</span>
{ $PATH_TO_DIRSEARCH/dirsearch.py -u $DOMAIN -e php --simple-report=$DIRECTORY/dirsearch echo "The results of dirsearch scan are stored in $DIRECTORY/dirsearch."
}
crt_scan() <span aria-label="annotation3" class="CodeAnnotationCode">3</span>
{ curl "https://crt.sh/?q=$DOMAIN&amp;output=json" -o $DIRECTORY/crt echo "The results of cert parsing is stored in $DIRECTORY/crt."
}
case $2 in <span aria-label="annotation4" class="CodeAnnotationCode">4</span> nmap-only) nmap_scan ;; dirsearch-only) dirsearch_scan ;;          crt-only)        crt_scan ;; *)        nmap_scan<span epub:type="pagebreak" id="Page_88" title="88"/>    dirsearch_scan crt_scan ;; 
esac</code></pre>
			<p>
				You can see that we’ve simplified our code. We created three functions, <code>nmap_scan</code> <span aria-label="annotation1" class="CodeAnnotation">1</span>, <code>dirsearch_scan</code> <span aria-label="annotation2" class="CodeAnnotation">2</span>, and <code>crt_scan</code> <span aria-label="annotation3" class="CodeAnnotation">3</span>. We put the <code>scan</code> and <code>echo</code> commands in these functions so we can call them repeatedly without writing the same code over and over <span aria-label="annotation4" class="CodeAnnotation">4</span>. This simplification might not seem like much here, but reusing code with functions will save you a lot of headaches when you write more complex programs.</p>
			<p>
				Keep in mind that all bash variables are <em>global</em> except for input parameters like <code>$1</code>, <code>$2</code>, and <code>$3</code>. This means that variables like <code>$DOMAIN</code>, <code>$DIRECTORY</code>, and <code>$PATH_TO_DIRSEARCH</code> become available throughout the script after we’ve declared them, even if they’re declared within functions. On the other hand, parameter values like <code>$1</code>, <code>$2</code>, and <code>$3</code> can refer only to the values the function is called with, so you can’t use a script’s input arguments within a function, like this:</p>
			<pre><code>nmap_scan()
{ nmap $1 &gt; $DIRECTORY/nmap  echo "The results of nmap scan are stored in $DIRECTORY/nmap."
}
nmap_scan</code></pre>
			<p>
				Here, the <code>$1</code> in the function refers to the first argument that <code>nmap_scan</code> was called with, not the argument our <em>recon.sh</em> script was called with. Since <code>nmap_scan</code> wasn’t called with any arguments, <code>$1</code> is blank.</p>
			<h3 id="h2-501546c05-0015">Parsing the Results</h3>
			<p>Now we have a tool that performs three types of scans and stores the results into files. But after the scans, we’d still have to manually read and make sense of complex output files. Is there a way to speed up this process too?</p>
			<p>
				Let’s say you want to search for a certain piece of information in the output files. You can use <em>Global Regular Expression Print</em><em> (</em><var>grep</var><em>) </em>to do that. This command line utility is used to perform searches in text, files, and command outputs. A simple <code>grep</code> command looks like this:</p>
			<pre><code>grep password file.txt</code></pre>
			<p>
				This tells <code>grep</code> to search for the string <code>password</code> in the file <em>file.txt</em>, then print the matching lines in standard output. For example, we can quickly search the Nmap output file to see if the target has port 80 open:</p>
			<pre><code>$ grep 80 TARGET_DIRECTORY/nmap
80/tcp open http</code></pre>
			<p>
				You can also make your search more flexible by using regular expressions in your search string. A <em>regular expression</em>, or <em>regex</em>, is a special string <span epub:type="pagebreak" id="Page_89" title="89"/>that describes a search pattern. It can help you display only specific parts of the output. For example, you may have noticed that the output of the Nmap command looks like this:</p>
			<pre><code>Starting Nmap 7.60 ( https://nmap.org )
Nmap scan report for scanme.nmap.org (45.33.32.156)
Host is up (0.065s latency).
Other addresses for scanme.nmap.org (not scanned): 2600:3c01::f03c:91ff:fe18:bb2f
Not shown: 992 closed ports
PORT STATE SERVICE
22/tcp open ssh
25/tcp filtered smtp
80/tcp open http
135/tcp filtered msrpc
139/tcp filtered netbios-ssn
445/tcp filtered microsoft-ds
9929/tcp open nping-echo
31337/tcp open Elite
Nmap done: 1 IP address (1 host up) scanned in 2.43 seconds</code></pre>
			<p>You might want to trim the irrelevant messages from the file so it looks more like this:</p>
			<pre><code>PORT STATE SERVICE
22/tcp open ssh
25/tcp filtered smtp
80/tcp open http
135/tcp filtered msrpc
139/tcp filtered netbios-ssn
445/tcp filtered microsoft-ds
9929/tcp open nping-echo
31337/tcp open Elite</code></pre>
			<p>Use this command to filter out the messages at the start and end of Nmap’s output and keep only the essential part of the report:</p>
			<pre><code>grep -E "^\S+\s+\S+\s+\S+$" DIRECTORY/nmap &gt; DIRECTORY/nmap_cleaned</code></pre>
			<p>
				The <code>-E</code> flag tells <code>grep</code> you’re using a regex. A regex consists of two parts: constants and operators. <em>Constants</em> are sets of strings, while <em>operators</em> are symbols that denote operations over these strings. These two elements together make regex a powerful tool of pattern matching. Here’s a quick overview of regex operators that represent characters:</p>
			<ol class="none">
				<li><code>\d</code> matches any digit.</li>
				<li><code>\w</code> matches any character.</li>
				<li><code>\s</code> matches any whitespace, and <code>\S</code> matches any non-whitespace.</li>
				<li><code>. </code>matches with any single character.</li>
				<li><code>\</code> escapes a special character.</li>
				<li><code>^</code> matches the start of the string or line.</li>
				<li><code>$</code> matches the end of the string or line.</li>
			</ol>
			<p><span epub:type="pagebreak" id="Page_90" title="90"/>Several operators also specify the number of characters to match:</p>
			<ol class="none">
				<li><code>*</code> matches the preceding character zero or more times.</li>
				<li><code>+</code> matches the preceding character one or more times.</li>
				<li><code>{3}</code> matches the preceding character three times.</li>
				<li><code>{1, 3}</code> matches the preceding character one to three times.</li>
				<li><code>{1, }</code> matches the preceding character one or more times.</li>
				<li><code>[</code><var>abc</var><code>]</code> matches one of the characters within the brackets.</li>
				<li><code>[</code><var>a</var><code>-</code><var>z</var><code>]</code> matches one of the characters within the range of <var>a</var> to <var>z</var>.</li>
				<li><code>(</code><var>a</var><code>|</code><var>b</var><code>|</code><var>c</var><code>)</code> matches either <var>a</var> or <var>b</var> or <var>c</var>.</li>
			</ol>
			<p>
				Let’s take another look at our regex expression here. Remember how <code>\s</code> matches any whitespace, and <code>\S</code> matches any non-whitespace? This means <code>\s+</code> would match any whitespace one or more characters long, and <code>\S+</code> would match any non-whitespace one or more characters long. This regex pattern specifies that we should extract lines that contain three strings separated by two whitespaces:</p>
			<pre><code>"^\S+\s+\S+\s+\S+$"</code></pre>
			<p>The filtered output will look like this:</p>
			<pre><code>PORT STATE SERVICE
22/tcp open ssh
25/tcp filtered smtp
80/tcp open http
135/tcp filtered msrpc
139/tcp filtered netbios-ssn
445/tcp filtered microsoft-ds
9929/tcp open nping-echo
31337/tcp open Elite</code></pre>
			<p>To account for extra whitespaces that might be in the command output, let’s add two more optional spaces around our search string:</p>
			<pre><code>"^\s*\S+\s+\S+\s+\S+\s*$"</code></pre>
			<p>
				You can use many more advanced regex features to perform more sophisticated matching. However, this simple set of operators serves well for our purposes. For a complete guide to regex syntax, read RexEgg’s cheat sheet (<a class="LinkURL" href="https://www.rexegg.com/regex-quickstart.html">https://www.rexegg.com/regex-quickstart.html</a>).</p>
			<h3 id="h2-501546c05-0016">Building a Master Report</h3>
			<p>
				What if you want to produce a master report from all three output files? You need to parse the JSON file from crt.sh. You can do this with <code>jq</code>, a command line utility that processes JSON. If we examine the JSON output file from crt.sh, we can see that we need to extract the <code>name_value</code> field of each certificate item to extract domain names. This command does just that:</p>
			<pre><code>$ jq -r ".[] | .name_value" $DOMAIN/crt</code></pre>
			<p><span epub:type="pagebreak" id="Page_91" title="91"/>The <code>-r</code> flag tells <code>jq</code> to write the output directly to standard output rather than format it as JSON strings. The <code>.[]</code> iterates through the array within the JSON file, and <code>.name_value</code> extracts the <code>name_value</code> field of each item. Finally, <code>$DOMAIN/crt</code> is the input file to the <code>jq</code> command. To learn more about how <code>jq</code> works, read its manual (<a class="LinkURL" href="https://stedolan.github.io/jq/manual/">https://stedolan.github.io/jq/manual/</a>).</p>
			<p>To combine all output files into a master report, write a script like this:</p>
			<pre><code>#!/bin/bash
PATH_TO_DIRSEARCH="/Users/vickieli/tools/dirsearch"
DOMAIN=$1
DIRECTORY=${DOMAIN}_recon
echo "Creating directory $DIRECTORY."
mkdir $DIRECTORY
nmap_scan()
{ nmap $DOMAIN &gt; $DIRECTORY/nmap echo "The results of nmap scan are stored in $DIRECTORY/nmap."
}
dirsearch_scan()
{ $PATH_TO_DIRSEARCH/dirsearch.py -u $DOMAIN -e php --simple-report=$DIRECTORY/dirsearch echo "The results of dirsearch scan are stored in $DIRECTORY/dirsearch."
}
crt_scan()
{ curl "https://crt.sh/?q=$DOMAIN&amp;output=json" -o $DIRECTORY/crt echo "The results of cert parsing is stored in $DIRECTORY/crt."
}
case $2 in nmap-only) nmap_scan ;; dirsearch-only) dirsearch_scan ;; crt-only) crt_scan ;; *) nmap_scan dirsearch_scan crt_scan ;; 
esac
echo "Generating recon report from output files..."
TODAY=$(date)
echo "This scan was created on $TODAY" &gt; $DIRECTORY/report <span aria-label="annotation1" class="CodeAnnotationCode">1</span>
echo "Results for Nmap:" &gt;&gt; $DIRECTORY/report
grep -E "^\s*\S+\s+\S+\s+\S+\s*$" $DIRECTORY/nmap &gt;&gt; $DIRECTORY/report <span aria-label="annotation2" class="CodeAnnotationCode">2</span>
echo "Results for Dirsearch:" &gt;&gt; $DIRECTORY/report
cat $DIRECTORY/dirsearch &gt;&gt; $DIRECTORY/report <span aria-label="annotation3" class="CodeAnnotationCode">3</span>
echo "Results for crt.sh:" &gt;&gt; $DIRECTORY/report
jq -r ".[] | .name_value" $DIRECTORY/crt &gt;&gt; $DIRECTORY/report <span aria-label="annotation4" class="CodeAnnotationCode">4</span></code></pre>
			<p><span epub:type="pagebreak" id="Page_92" title="92"/>First, we create a new file named <em>report</em> and write today’s date into it <span aria-label="annotation1" class="CodeAnnotation">1</span> to keep track of when the report was generated. We then append the results of the <code>nmap</code> and <code>dirsearch</code> commands into the report file <span aria-label="annotation2" class="CodeAnnotation">2</span>. The <code>cat</code> command prints the contents of a file to standard output, but we can also use it to redirect the content of the file into another file <span aria-label="annotation3" class="CodeAnnotation">3</span>. Finally, we extract domain names from the crt.sh report and append it to the end of the report file <span aria-label="annotation4" class="CodeAnnotation">4</span>.</p>
			<h3 id="h2-501546c05-0017">Scanning Multiple Domains</h3>
			<p>
				What if we want to scan multiple domains at once? When reconning a target, we might start with several of the organization’s domain names. For example, we know that Facebook owns both <em>facebook.com</em> and <em>fbcdn.net</em>. But our current script allows us to scan only one domain at a time. We need to write a tool that can scan multiple domains with a single command, like this:</p>
			<pre><code>./recon.sh facebook.com fbcdn.net nmap-only</code></pre>
			<p>
				When we scan multiple domains like this, we need a way to distinguish which arguments specify the scan <code>MODE</code> and which specify target domains. As you’ve already seen from the tools I introduced, most tools allow users to modify the behavior of a tool by using command line <em>option</em><em>s</em> or <em>flags</em>, such as <code>-u</code> and <code>--simple-report</code>.</p>
			<p>
				The <code>getopts</code> tool parses options from the command line by using single-character flags. Its syntax is as follows, where <var>OPTSTRING</var> specifies the option letters that <code>getopts</code> should recognize. For example, if it should recognize the options <code>-m</code> and <code>-i</code>, you should specify <code>mi</code>. If you want an option to contain argument values, the letter should be followed by a colon, like this: <code>m:i</code>. The <var>NAME</var> argument specifies the variable name that stores the option letter.</p>
			<pre><code>getopts <var>OPTSTRING</var> <var>NAME</var></code></pre>
			<p>
				To implement our multiple-domain scan functionality, we can let users use an <code>-m</code> flag to specify the scan mode and assume that all other arguments are domains. Here, we tell <code>getopts</code> to recognize an option if the option flag is <code>-m</code> and that this option should contain an input value. The <code>getopts</code> tool also automatically stores the value of any options into the <code>$OPTARG</code> variable. We can store that value into a variable named <code>MODE:</code></p>
			<pre><code>getopts "m:" OPTION
MODE=$OPTARG</code></pre>
			<p>
				Now if you run the shell script with an <code>-m</code> flag, the script will know that you’re specifying a scan <code>MODE</code>! Note that <code>getopts</code> stops parsing arguments when it encounters an argument that doesn’t start with the <code>-</code> character, so you’ll need to place the scan mode before the domain arguments when you run the script:</p>
			<pre><code>./recon.sh -m nmap-only facebook.com fbcdn.net</code></pre>
			<p><span epub:type="pagebreak" id="Page_93" title="93"/>Next, we’ll need a way to read every domain argument and perform scans on them. Let’s use loops! Bash has two types of loops: the <code>for</code> loop and the <code>while</code> loop. The <code>for</code> loop works better for our purposes, as we already know the number of values we are looping through. In general, you should use <code>for</code> loops when you already have a list of values to iterate through. You should use <code>while</code> loops when you’re not sure how many values to loop through but want to specify the condition in which the execution should stop.</p>
			<p>
				Here’s the syntax of a <code>for</code> loop in bash. For every item in <var>LIST_OF_VALUES</var>, bash will execute the code between <code>do</code> and <code>done</code> once:</p>
			<pre><code>for i in <var>LIST_OF_VALUES</var>
do <var>DO SOMETHING</var>
done</code></pre>
			<p>
				Now let’s implement our functionality by using a <code>for</code> loop:</p>
			<pre><code><span aria-label="annotation1" class="CodeAnnotationHang">1</span> for i in "${@:$OPTIND:$#}"
do # Do the scans for $i
done</code></pre>
			<p>
				We create an array <span aria-label="annotation1" class="CodeAnnotation">1</span> that contains every command line argument, besides the ones that are already parsed by <code>getopts</code>, which stores the index of the first argument after the options it parses into a variable named <code>$OPTIND</code>. The characters <code>$@</code> represent the array containing all input arguments, while <code>$#</code> is the number of command line arguments passed in. <code>"${@:OPTIND:}"</code> slices the array so that it removes the <code>MODE</code> argument, like <code>nmap-only</code>, making sure that we iterate through only the domains part of our input. Array slicing is a way of extracting a subset of items from an array. In bash, you can slice arrays by using this syntax (note that the quotes around the command are necessary):</p>
			<pre><code>"${INPUT_ARRAY:START_INDEX:END_INDEX}"</code></pre>
			<p>
				The <code>$i</code> variable represents the current item in the argument array. We can then wrap the loop around the code:</p>
			<pre><code>#!/bin/bash
PATH_TO_DIRSEARCH="/Users/vickieli/tools/dirsearch"
nmap_scan()
{ nmap $DOMAIN &gt; $DIRECTORY/nmap echo "The results of nmap scan are stored in $DIRECTORY/nmap."
}
dirsearch_scan()
{ $PATH_TO_DIRSEARCH/dirsearch.py -u $DOMAIN -e php --simple-report=$DIRECTORY/dirsearch echo "The results of dirsearch scan are stored in $DIRECTORY/dirsearch."
}
crt_scan()
{<span epub:type="pagebreak" id="Page_94" title="94"/>  curl "https://crt.sh/?q=$DOMAIN&amp;output=json" -o $DIRECTORY/crt echo "The results of cert parsing is stored in $DIRECTORY/crt."
}
getopts "m:" OPTION
MODE=$OPTARG
for i in "${@:$OPTIND:$#}" <span aria-label="annotation1" class="CodeAnnotationCode">1</span>
do DOMAIN=$i DIRECTORY=${DOMAIN}_recon echo "Creating directory $DIRECTORY." mkdir $DIRECTORY case $MODE in nmap-only) nmap_scan ;; dirsearch-only) dirsearch_scan ;; crt-only) crt_scan ;; *) nmap_scan dirsearch_scan crt_scan ;;      esac echo "Generating recon report for $DOMAIN..." TODAY=$(date) echo "This scan was created on $TODAY" &gt; $DIRECTORY/report if [ -f $DIRECTORY/nmap ];then <span aria-label="annotation2" class="CodeAnnotationCode">2</span> echo "Results for Nmap:" &gt;&gt; $DIRECTORY/report grep -E "^\s*\S+\s+\S+\s+\S+\s*$" $DIRECTORY/nmap &gt;&gt; $DIRECTORY/report fi if [ -f $DIRECTORY/dirsearch ];then <span aria-label="annotation3" class="CodeAnnotationCode">3</span> echo "Results for Dirsearch:" &gt;&gt; $DIRECTORY/report cat $DIRECTORY/dirsearch &gt;&gt; $DIRECTORY/report fi if [ -f $DIRECTORY/crt ];then <span aria-label="annotation4" class="CodeAnnotationCode">4</span> echo "Results for crt.sh:" &gt;&gt; $DIRECTORY/report jq -r ".[] | .name_value" $DIRECTORY/crt &gt;&gt; $DIRECTORY/report fi done <span aria-label="annotation5" class="CodeAnnotationCode">5</span></code></pre>
			<p>
				The <code>for</code> loop starts with the <code>for</code> keyword <span aria-label="annotation1" class="CodeAnnotation">1</span> and ends with the <code>done</code> keyword <span aria-label="annotation5" class="CodeAnnotation">5</span>. Notice that we also added a few lines in the report section to see if we need to generate each type of report. We check whether the output file of an Nmap scan, a Dirsearch scan, or a crt.sh scan exist so we can determine if we need to generate a report for that scan type <span aria-label="annotation2" class="CodeAnnotation">2</span> <span aria-label="annotation3" class="CodeAnnotation">3</span> <span aria-label="annotation4" class="CodeAnnotation">4</span>.</p>
			<p><span epub:type="pagebreak" id="Page_95" title="95"/>The brackets around a condition mean that we’re passing the contents into a<code> </code><code>test</code> command: <code>[ -f $DIRECTORY/nmap ]</code> is equivalent to <code>test -f $DIRECTORY/nmap</code>.</p>
			<p>
				The <code>test</code> command evaluates a conditional and outputs either <code>true</code> or <code>false</code>. The <code>-f</code> flag tests whether a file exists. But you can test for more conditions! Let’s go through some useful test conditions. The <code>-eq</code> and -<code>ne</code> flags test for equality and inequality, respectively. This returns <code>true</code> if <code>$3</code> is equal to <code>1</code>:</p>
			<pre><code>if [ $3 -eq 1 ]</code></pre>
			<p>
				This returns <code>true</code> if <code>$3</code> is not equal to <code>1</code>:</p>
			<pre><code>if [ $3 -ne 1 ]</code></pre>
			<p>
				The <code>-gt</code>, <code>-ge</code>, <code>-lt</code>, and <code>le</code> flags test for greater than, greater than or equal to, less than, and less than or equal to, respectively:</p>
			<pre><code>if [ $3 -gt 1 ]
if [ $3 -ge 1 ]
if [ $3 -lt 1 ]
if [ $3 -le 1 ]</code></pre>
			<p>
				The <code>-z</code> and <code>-n</code> flags test whether a string is empty. These conditions are both true:</p>
			<pre><code>if [ -z "" ]
if [ -n "abc" ]</code></pre>
			<p>
				The <code>-d</code>, <code>-f</code>, <code>-r</code>, <code>-w</code>, and <code>-x</code> flags check for directory and file statuses. You can use them to check the existence and permissions of a file before your shell script operates on them. For instance, this command returns <code>true</code> if <em>/bin</em> is a directory that exists:</p>
			<pre><code>if [ -d /bin]</code></pre>
			<p>
				This one returns <code>true</code> if <em>/bin/bash</em> is a file that exists:</p>
			<pre><code>if [ -f /bin/bash ]</code></pre>
			<p>
				And this one returns <code>true</code> if <em>/bin/bash</em> is a readable file:</p>
			<pre><code>if [ -r /bin/bash ]</code></pre>
			<p class="BodyContinued">or a writable file:</p>
			<pre><code>if [ -w /bin/bash ]</code></pre>
			<p class="BodyContinued">or an executable file:</p>
			<pre><code>if [ -x /bin/bash ]</code></pre>
			<p><span epub:type="pagebreak" id="Page_96" title="96"/>You can also use <code>&amp;&amp;</code> and <code>||</code> to combine test expressions. This command returns <code>true</code> if both expressions are true:</p>
			<pre><code>if [ $3 -gt 1 ] &amp;&amp; [ $3 -lt 3 ]</code></pre>
			<p>
				And this one returns <code>true</code> if at least one of them is true:</p>
			<pre><code>if [ $3 -gt 1 ] || [ $3 -lt 0 ]</code></pre>
			<p>
				You can find more comparison flags in the <code>test</code> command’s manual by running <code>man test</code>. (If you aren’t sure about the commands you’re using, you can always enter <code>man</code> followed by the command name in the terminal to access the command’s manual file.)</p>
			<h3 id="h2-501546c05-0018">Writing a Function Library</h3>
			<p>
				As your codebase gets larger, you should consider writing a <em>function library</em> to reuse code. We can store all the commonly used functions in a separate file called <em>scan.lib</em>. That way, we can call these functions as needed for future recon tasks:</p>
			<pre><code>#!/bin/bash
nmap_scan()
{ nmap $DOMAIN &gt; $DIRECTORY/nmap echo "The results of nmap scan are stored in $DIRECTORY/nmap."
}
dirsearch_scan()
{ $PATH_TO_DIRSEARCH/dirsearch.py -u $DOMAIN -e php --simple-report=$DIRECTORY/dirsearch echo "The results of dirsearch scan are stored in $DIRECTORY/dirsearch."
}
crt_scan()
{ curl "https://crt.sh/?q=$DOMAIN&amp;output=json" -o $DIRECTORY/crt echo "The results of cert parsing is stored in $DIRECTORY/crt."
}</code></pre>
			<p>
				In another file, we can source<em> </em>the library file in order to use all of its functions and variables. We source a script via the <code>source</code> command, followed by the path to the script:</p>
			<pre><code>#!/bin/bash
source ./scan.lib
PATH_TO_DIRSEARCH="/Users/vickieli/tools/dirsearch"
getopts "m:" OPTION
MODE=$OPTARG
for i in "${@:$OPTIND:$#}"
do DOMAIN=$i DIRECTORY=${DOMAIN}_recon echo "Creating directory $DIRECTORY." mkdir $DIRECTORY<span epub:type="pagebreak" id="Page_97" title="97"/>  case $MODE in nmap-only) nmap_scan ;; dirsearch-only) dirsearch_scan ;;  crt-only)  crt_scan ;; *)        nmap_scan dirsearch_scan crt_scan ;;  esac echo "Generating recon report for $DOMAIN..." TODAY=$(date) echo "This scan was created on $TODAY" &gt; $DIRECTORY/report if [ -f $DIRECTORY/nmap ];then echo "Results for Nmap:" &gt;&gt; $DIRECTORY/report grep -E "^\s*\S+\s+\S+\s+\S+\s*$" $DIRECTORY/nmap &gt;&gt; $DIRECTORY/report fi if [ -f $DIRECTORY/dirsearch ];then echo "Results for Dirsearch:" &gt;&gt; $DIRECTORY/report cat $DIRECTORY/dirsearch &gt;&gt; $DIRECTORY/report fi if [ -f $DIRECTORY/crt ];then echo "Results for crt.sh:" &gt;&gt; $DIRECTORY/report jq -r ".[] | .name_value" $DIRECTORY/crt &gt;&gt; $DIRECTORY/report fi
done</code></pre>
			<p>Using a library can be super useful when you’re building multiple tools that require the same functionalities. For example, you might build multiple networking tools that all require DNS resolution. In this case, you can simply write the functionality once and use it in all of your tools.</p>
			<h3 id="h2-501546c05-0019">Building Interactive Programs</h3>
			<p>
				What if you want to build an interactive program that takes user input during execution? Let’s say that if users enter the command line option, <code>-i</code>, you want the program to enter an interactive mode that allows you to specify domains to scan as you go:</p>
			<pre><code>./recon.sh -i -m nmap-only</code></pre>
			<p>
				For that, you can use <code>read</code>. This command reads user input and stores the input string into a variable:</p>
			<pre><code>echo "Please enter a domain!"
read $DOMAIN</code></pre>
			<p><span epub:type="pagebreak" id="Page_98" title="98"/>These commands will prompt the user to enter a domain, then store the input inside a variable named <code>$DOMAIN</code>.</p>
			<p>
				To prompt a user repeatedly, we need to use a <code>while</code> loop, which will keep printing the prompt asking for an input domain until the user exits the program. Here’s the syntax of a <code>while</code> loop. As long as the <var>CONDITION</var> is true, the <code>while</code> loop will execute the code between <code>do</code> and <code>done</code> repeatedly:</p>
			<pre><code>while <var>CONDITION</var>
do <var>DO SOMETHING</var>
done</code></pre>
			<p>
				We can use a <code>while</code> loop to repeatedly prompt the user for domains until the user enters <code>quit</code>:</p>
			<pre><code>while [ $INPUT != "quit" ];do echo "Please enter a domain!" read INPUT  if [ $INPUT != "quit" ];then scan_domain $INPUT report_domain $INPUT fi
done</code></pre>
			<p>
				We also need a way for users to actually invoke the <code>-i</code><em> option, and our </em><code>getopts</code><em> command isn’t currently doing that. We can use a </em><code>while</code><em> loop to parse options by using </em><code>getopts</code><em> repeatedly: </em></p>
			<pre><code>while getopts "m:i" OPTION; do case $OPTION in m)   MODE=$OPTARG ;;  i)   INTERACTIVE=true ;; esac
done</code></pre>
			<p><em>Here, we specify a </em><code>while</code><em> loop that gets command line options repeatedly. If the option flag is </em><code>-m</code><em>, we set the </em><code>MODE</code><em> variable to the scan mode that the user has specified. If the option flag is </em><code>-i</code><em>, we set the </em><code>$INTERACTIVE</code><em> variable to </em><code>true</code>. Then, later in the script, we can decide whether to invoke the interactive mode by checking the value of the <code>$INTERACTIVE</code><em> variable. Putting it all together, we get our final script:</em></p>
			<pre><code>#!/bin/bash
source ./scan.lib
while getopts "m:i" OPTION; do case $OPTION in m)   MODE=$OPTARG<span epub:type="pagebreak" id="Page_99" title="99"/>      ;;  i)   INTERACTIVE=true ;; esac
done
scan_domain(){ DOMAIN=$1 DIRECTORY=${DOMAIN}_recon echo "Creating directory $DIRECTORY." mkdir $DIRECTORY case $MODE in nmap-only) nmap_scan ;; dirsearch-only) dirsearch_scan ;; crt-only) crt_scan ;; *)        nmap_scan dirsearch_scan crt_scan ;;      esac
}
report_domain(){ DOMAIN=$1 DIRECTORY=${DOMAIN}_recon echo "Generating recon report for $DOMAIN..." TODAY=$(date) echo "This scan was created on $TODAY" &gt; $DIRECTORY/report if [ -f $DIRECTORY/nmap ];then echo "Results for Nmap:" &gt;&gt; $DIRECTORY/report grep -E "^\s*\S+\s+\S+\s+\S+\s*$" $DIRECTORY/nmap &gt;&gt; $DIRECTORY/report fi if [ -f $DIRECTORY/dirsearch ];then echo "Results for Dirsearch:" &gt;&gt; $DIRECTORY/report cat $DIRECTORY/dirsearch &gt;&gt; $DIRECTORY/report fi if [ -f $DIRECTORY/crt ];then echo "Results for crt.sh:" &gt;&gt; $DIRECTORY/report jq -r ".[] | .name_value" $DIRECTORY/crt &gt;&gt; $DIRECTORY/report fi
}
if [ $INTERACTIVE ];then <span aria-label="annotation1" class="CodeAnnotationCode">1</span> INPUT="BLANK" while [ $INPUT != "quit" ];do <span aria-label="annotation2" class="CodeAnnotationCode">2</span> echo "Please enter a domain!" read INPUT  if [ $INPUT != "quit" ];then <span aria-label="annotation3" class="CodeAnnotationCode">3</span> scan_domain $INPUT<span epub:type="pagebreak" id="Page_100" title="100"/>      report_domain $INPUT fi done
else for i in "${@:$OPTIND:$#}";do scan_domain $i report_domain $i  done
fi</code></pre>
			<p>
				In this program, we first check if the user has selected the interactive mode by specifying the <code>-i</code><em> option </em><span aria-label="annotation1" class="CodeAnnotation">1</span><em>. We then repeatedly prompt the user for a domain by using a </em><code>while</code><em> loop </em><span aria-label="annotation2" class="CodeAnnotation">2</span><em>. If the user input is not the keyword </em><code>quit</code><em>, we assume that they entered a target domain, so we scan and produce a report for that domain. The </em><code>while</code><em> loop will continue to run and ask the user for domains until the user enters </em><code>quit</code><em>, which will cause the </em><code>while</code><em> loop to exit and the program to terminate </em><span aria-label="annotation3" class="CodeAnnotation">3</span><em>.</em></p>
			<p>Interactive tools can help your workflow operate more smoothly. For example, you can build testing tools that will let you choose how to proceed based on preliminary results.</p>
			<h3 id="h2-501546c05-0020">Using Special Variables and Characters</h3>
			<p>You’re now equipped with enough bash knowledge to build many versatile tools. This section offers more tips that concern the particularities of shell scripts.</p>
			<p>
				In Unix, commands return <code>0</code> on success and a positive integer on failure. The variable <code>$?</code> contains the exit value of the last command executed. You can use these to test for execution successes and failures:</p>
			<pre><code>#!/bin/sh
chmod 777 script.sh
if [ "$?" -ne "0" ]; then echo "Chmod failed. You might not have permissions to do that!"
fi</code></pre>
			<p>
				Another special variable is <code>$$</code>, which contains the current process’s ID. This is useful when you need to create temporary files for the script. If you have multiple instances of the same script or program running at the same time, each might need its own temporary files. In this case, you can create temporary files named <em>/tmp/script_name_$$ </em>for every one of them.</p>
			<p>Remember that we talked about variable scopes in shell scripts earlier in this chapter? Variables that aren’t input parameters are global to the entire script. If you want other programs to use the variable as well, you need to export the variable:</p>
			<pre><code>export <var>VARIABLE_NAME</var>=<var>VARIABLE_VALUE</var></code></pre>
			<p>
				Let’s say that in one of your scripts you set the variable <code>VAR</code>:</p>
			<pre><code>VAR="hello!"</code></pre>
			<p><span epub:type="pagebreak" id="Page_101" title="101"/>If you don’t export it or source it in another script, the value gets destroyed after the script exits. But if you export <code>VAR</code> in the first script and run that script before running a second script, the second script will be able to read <code>VAR</code>’s value.</p>
			<p>
				You should also be aware of special characters in bash. In Unix, the wildcard character <code>*</code> stands for <em>all</em>. For example, this command will print out all the filenames in the current directory that have the file extension <em>.txt</em>:</p>
			<pre><code>$ ls *.txt</code></pre>
			<p>
				Backticks (<code>`</code>) indicate command substitution. You can use both backticks and the <code>$()</code> command substitution syntax mentioned earlier for the same purpose. This <code>echo</code> command will print the output of the <code>whoami</code> command:</p>
			<pre><code>echo `whoami`</code></pre>
			<p>
				Most special characters, like the wildcard character or the single quote, aren’t interpreted as special when they are placed in double quotes. Instead, they’re treated as part of a string. For example, this command will echo the string <code>"abc '*' 123"</code>:</p>
			<pre><code>$ echo "abc '*' 123"</code></pre>
			<p>
				Another important special character is the backslash (<code>\</code>), the escape character in bash. It tells bash that a certain character should be interpreted literally, and not as a special character.</p>
			<p>Certain special characters, like double quotes, dollar sign, backticks, and backslashes remain special even within double quotes, so if you want bash to treat them literally, you have to escape them by using a backslash:</p>
			<pre><code>$ echo "\" is a double quote. \$ is a dollar sign. \` is a backtick. \\ is a backslash."</code></pre>
			<p>This command will echo:</p>
			<pre><code>" is a double quote. $ is a dollar sign. ` is a backtick. \ is a backslash.</code></pre>
			<p>You can also use a backslash before a newline to indicate that the line of code has not ended. For example, this command</p>
			<pre><code>chmod 777 \
script.sh</code></pre>
			<p class="BodyContinued">is the same as this one:</p>
			<pre><code>chmod 777 script.sh</code></pre>
			<p>Congratulations! You can now write bash scripts. Bash scripting may seem scary at first, but once you’ve mastered it, it will be a powerful addition to your hacking arsenal. You’ll be able to perform better recon, conduct more efficient testing, and have a more structured hacking workflow.</p>
			<p><span epub:type="pagebreak" id="Page_102" title="102"/>If you plan on implementing a lot of automation, it’s a good idea to start organizing your scripts from the start. Set up a directory of scripts and sort your scripts by their functionality. This will become the start of developing your own hacking methodology. When you’ve collected a handful of scripts that you use on a regular basis, you can use scripts to run them automatically. For example, you might categorize your scripts into recon scripts, fuzzing scripts, automated reporting, and so on. This way, every time you find a script or tool you like, you can quickly incorporate it into your workflow in an organized fashion.</p>
			<h3 id="h2-501546c05-0021">Scheduling Automatic Scans</h3>
			<p>Now let’s take your automation to the next level by building an alert system that will let us know if something interesting turns up in our scans. This saves us from having to run the commands manually and comb through the results over and over again.</p>
			<p>
				We can use cron jobs to schedule our scans. <em>Cron</em> is a job scheduler on Unix-based operating systems. It allows you to schedule jobs to run periodically. For example, you can run a script that checks for new endpoints on a particular site every day at the same time. Or you can run a scanner that checks for vulnerabilities on the same target every day. This way, you can monitor for changes in an application’s behavior and find ways to exploit it.</p>
			<p>
				You can configure Cron’s behavior by editing files called <em>crontabs</em>. Unix keeps different copies of crontabs for each user. Edit your own user’s crontab by running the following:</p>
			<pre><code>crontab -e</code></pre>
			<p>All crontabs follow this same syntax:</p>
			<pre><code>A B C D E <var>command_to_be_executed</var>
A: Minute (0 – 59)
B: Hour (0 – 23)
C: Day (1 – 31)
D: Month (1 – 12)
E: Weekday (0 – 7) (Sunday is 0 or 7, Monday is 1...)</code></pre>
			<p>Each line specifies a command to be run and the time at which it should run, using five numbers. The first number, from 0 to 59, specifies the minute when the command should run. The second number specifies the hour, and ranges from 0 to 23. The third and fourth numbers are the day and month the command should run. And the last number is the weekday when the command should run, which ranges from 0 to 7. Both 0 and 7 mean that the command should run on Sundays; 1 means the command should run on Mondays; and so on.</p>
			<p>
				For example, you can add this line to your crontab to run your recon script every day at 9:30 <span class="SmallCaps">PM</span>:</p>
			<pre><code>30 21 * * * ./scan.sh</code></pre>
			<p><span epub:type="pagebreak" id="Page_103" title="103"/>You can also batch-run the scripts within directories. The <code>run-parts</code> command in crontabs tells Cron to run all the scripts stored in a directory. For example, you can store all your recon tools in a directory and scan your targets periodically. The following line tells Cron to run all scripts in my security directory every day at 9:30 <span class="SmallCaps">PM</span>:</p>
			<pre><code>30 21 * * * run-parts /Users/vickie/scripts/security</code></pre>
			<p>
				Next, <code>git diff</code> is a command that outputs the difference between two files. You need to install the Git program to use it. You can use <code>git diff</code> to compare scan results at different times, which quickly lets you see if the target has changed since you last scanned it:</p>
			<pre><code>git diff <var>SCAN_1</var> <var>SCAN_2</var></code></pre>
			<p>This will help you identify any new domains, subdomains, endpoints, and other new assets of a target. You could write a script like this to notify you of new changes on a target every day:</p>
			<pre><code>#!/bin/bash
DOMAIN=$1
DIRECTORY=${DOMAIN}_recon
echo "Checking for new changes about the target: $DOMAIN.\n Found these new things."
git diff &lt;SCAN AT TIME 1&gt; &lt;SCAN AT TIME 2&gt;</code></pre>
			<p>And schedule it with Cron:</p>
			<pre><code>30 21 * * * ./scan_diff.sh facebook.com</code></pre>
			<p>
				These automation techniques have helped me quickly find new JavaScript files, endpoints, and functionalities on targets. I especially like to use this technique to discover subdomain takeover vulnerabilities automatically. We’ll talk about subdomain takeovers in <span class="xref" itemid="xref_target_Chapter 20">Chapter 20</span>.</p>
			<p>
				Alternatively, you can use GitHub to track changes. Set up a repository to store your scan results at <a class="LinkURL" href="https://github.com/new/">https://github.com/new/</a>. GitHub has a Notification feature that will tell you when significant events on a repository occur. It’s located at Settings<span class="MenuArrow">▶</span>Notifications on each repository’s page. Provide GitHub with an email address that it will use to notify you about changes. Then, in the directory where you store scan results, run these commands to initiate <code>git</code> inside the directory:</p>
			<pre><code>git init
git remote add origin https://<var>PATH_TO_THE_REPOSITORY</var></code></pre>
			<p>Lastly, use Cron to scan the target and upload the files to GitHub periodically:</p>
			<pre><code>30 21 * * * ./recon.sh facebook.com
40 21 * * * git add *; git commit -m "new scan"; git push -u origin master</code></pre>
			<p>GitHub will then send you an email about the files that changed during the new scan.</p>
			<h2 id="h1-501546c05-0007"><span epub:type="pagebreak" id="Page_104" title="104"/>A Note on Recon APIs</h2>
			<p>
				Many of the tools mentioned in this chapter have APIs that allow you to integrate their services into your applications and scripts. We’ll talk about APIs more in <span class="xref" itemid="xref_target_Chapter 24">Chapter 24</span>, but for now, you can think of APIs as endpoints you can use to query a service’s database. Using these APIs, you can query recon tools from your script and add the results to your recon report without visiting their sites manually.</p>
			<p>
				For example, Shodan has an API (<a class="LinkURL" href="https://developer.shodan.io/">https://developer.shodan.io/</a><em>)</em> that allows you to query its database. You can access a host’s scan results by accessing this URL: <em>https://api.shodan.io/shodan/host/{ip}?key={YOUR_API_KEY}</em>. You could configure your bash script to send requests to this URL and parse the results<em>. </em>LinkedIn also has an API (<a class="LinkURL" href="https://www.linkedin.com/developers/">https://www.linkedin.com/developers/</a>) that lets you query its database. For example, you can use this URL to access information about a user on LinkedIn: <em>https://api.linkedin.com/v2/people/{PERSON ID}. </em>The Censys API (<a class="LinkURL" href="https://censys.io/api">https://censys.io/api</a>) allows you to access certificates by querying the endpoint <em>https://censys.io/api/v1</em>.</p>
			<p>
				Other tools mentioned in this chapter, like BuiltWith, Google search, and GitHub search, all have their own API services. These APIs can help you discover assets and content more efficiently by integrating third-party tools into your recon script. Note that most API services require you to create an account on their website to obtain an <em>API key</em>, which is how most API services authenticate their users. You can find information about how to obtain the API keys of popular recon services at <a class="LinkURL" href="https://github.com/lanmaster53/recon-ng-marketplace/wiki/API-Keys/">https://github.com/lanmaster53/recon-ng-marketplace/wiki/API-Keys/</a>.</p>
			<h2 id="h1-501546c05-0008">Start Hacking!</h2>
			<p>Now that you’ve conducted extensive reconnaissance, what should you do with the data you’ve collected? Plan your attacks by using the information you’ve gathered! Prioritize your tests based on the functionality of the application and its technology.</p>
			<p>
				For example, if you find a feature that processes credit card numbers, you could first look for vulnerabilities that might leak the credit card numbers, such as IDORs (<span class="xref" itemid="xref_target_Chapter 10">Chapter 10</span>). Focus on sensitive features such as credit cards and passwords, because these features are more likely to contain critical vulnerabilities. During your recon, you should be able to get a good idea of what the company cares about and the sensitive data it’s protecting. Go after those specific pieces of information throughout your bug-hunting process to maximize the business impact of the issues you discover. You can also focus your search on bugs or vulnerabilities that affect that particular tech stack you uncovered, or on elements of the source code you were able to find.</p>
			<p>And don’t forget, recon isn’t a one-time activity. You should continue to monitor your targets for changes. Organizations modify their system, technologies, and codebase constantly, so continuous recon will ensure that you always know what the attack surface looks like. Using a combination of bash, scheduling tools, and alerting tools, build a recon engine that does most of the work for you.</p>
			<h2 id="h1-501546c05-0009"><span epub:type="pagebreak" id="Page_105" title="105"/>Tools Mentioned in This Chapter</h2>
			<p>In this chapter, I introduced many tools you can use in your recon process. Many more good tools are out there. The ones mentioned here are merely my personal preferences. I’ve included them here in chronological order for your reference.</p>
			<p>Be sure to learn about how these tools work before you use them! Understanding the software you use allows you to customize it to fit your workflow.</p>
			<h3 id="h2-501546c05-0022">Scope Discovery</h3>
			<ol class="none">
				<li>WHOIS looks for the owner of a domain or IP.</li>
				<li>
					ViewDNS.info reverse WHOIS (<a class="LinkURL" href="https://viewdns.info/reversewhois/">https://viewdns.info/reversewhois/</a>) is a tool that searches for reverse WHOIS data by using a keyword.</li>
				<li><code>nslookup</code> queries internet name servers for IP information about a host.</li>
				<li>
					ViewDNS reverse IP (<a class="LinkURL" href="https://viewdns.info/reverseip/">https://viewdns.info/reverseip/</a>) looks for domains hosted on the same server, given an IP or domain.</li>
				<li>
					crt.sh (<a class="LinkURL" href="https://crt.sh/">https://crt.sh/</a>), Censys (<a class="LinkURL" href="https://censys.io/">https://censys.io/</a>), and Cert Spotter (<a class="LinkURL" href="https://sslmate.com/certspotter/">https://sslmate.com/certspotter/</a>) are platforms you can use to find certificate information about a domain.</li>
				<li>
					Sublist3r (<a class="LinkURL" href="https://github.com/aboul3la/Sublist3r/">https://github.com/aboul3la/Sublist3r/</a>), SubBrute (<a class="LinkURL" href="https://github.com/TheRook/subbrute/">https://github.com/TheRook/subbrute/</a>), Amass (<a class="LinkURL" href="https://github.com/OWASP/Amass/">https://github.com/OWASP/Amass/</a>), and Gobuster (<a class="LinkURL" href="https://github.com/OJ/gobuster/">https://github.com/OJ/gobuster/</a>) enumerate subdomains.</li>
				<li>
					Daniel Miessler’s SecLists (<a class="LinkURL" href="https://github.com/danielmiessler/SecLists/">https://github.com/danielmiessler/SecLists/</a>) is a list of keywords that can be used during various phases of recon and hacking. For example, it contains lists that can be used to brute-force subdomains and filepaths.</li>
				<li>
					Commonspeak2 (<a class="LinkURL" href="https://github.com/assetnote/commonspeak2/">https://github.com/assetnote/commonspeak2/</a>) generates lists that can be used to brute-force subdomains and filepaths using publicly available data.</li>
				<li>
					Altdns (<a class="LinkURL" href="https://github.com/infosec-au/altdns">https://github.com/infosec-au/altdns</a>) brute-forces subdomains by using permutations of common subdomain names.</li>
				<li>
					Nmap (<a class="LinkURL" href="https://nmap.org/">https://nmap.org/</a>) and Masscan (<a class="LinkURL" href="https://github.com/robertdavidgraham/masscan/">https://github.com/robertdavidgraham/masscan/</a>) scan the target for open ports.</li>
				<li>
					Shodan (<a class="LinkURL" href="https://www.shodan.io/">https://www.shodan.io/</a>), Censys (<a class="LinkURL" href="https://censys.io/">https://censys.io/</a>), and Project Sonar (<a class="LinkURL" href="https://www.rapid7.com/research/project-sonar/">https://www.rapid7.com/research/project-sonar/</a>) can be used to find services on targets without actively scanning them.</li>
				<li>
					Dirsearch (<a class="LinkURL" href="https://github.com/maurosoria/dirsearch/">https://github.com/maurosoria/dirsearch/</a>) and Gobuster (<a class="LinkURL" href="https://github.com/OJ/gobuster">https://github.com/OJ/gobuster</a>) are directory brute-forcers used to find hidden filepaths.</li>
				<li>
					EyeWitness (<a class="LinkURL" href="https://github.com/FortyNorthSecurity/EyeWitness/">https://github.com/FortyNorthSecurity/EyeWitness/</a>) and Snapper (<a class="LinkURL" href="https://github.com/dxa4481/Snapper/">https://github.com/dxa4481/Snapper/</a>) grab screenshots of a list of URLs. They can be used to quickly scan for interesting pages among a list of enumerated paths.</li>
				<li><span epub:type="pagebreak" id="Page_106" title="106"/>OWASP ZAP (<a class="LinkURL" href="https://owasp.org/www-project-zap/">https://owasp.org/www-project-zap/</a>) is a security tool that includes a scanner, proxy, and much more. Its web spider can be used to discover content on a web server.</li>
				<li>
					GrayhatWarfare (<a class="LinkURL" href="https://buckets.grayhatwarfare.com/">https://buckets.grayhatwarfare.com/</a>) is an online search engine you can use to find public Amazon S3 buckets.</li>
				<li>
					Lazys3 (<a class="LinkURL" href="https://github.com/nahamsec/lazys3/">https://github.com/nahamsec/lazys3/</a>) and Bucket Stream (<a class="LinkURL" href="https://github.com/eth0izzle/bucket-stream/">https://github.com/eth0izzle/bucket-stream/</a>) brute-force buckets by using keywords.</li>
			</ol>
			<h3 id="h2-501546c05-0023">OSINT</h3>
			<ol class="none">
				<li>
					The Google Hacking Database (<a class="LinkURL" href="https://www.exploit-db.com/google-hacking-database/">https://www.exploit-db.com/google-hacking-database/</a>) contains useful Google search terms that frequently reveal vulnerabilities or sensitive files.</li>
				<li>
					KeyHacks (<a class="LinkURL" href="https://github.com/streaak/keyhacks/">https://github.com/streaak/keyhacks/</a>) helps you determine whether a set of credentials is valid and learn how to use them to access the target’s services.</li>
				<li>
					Gitrob (<a class="LinkURL" href="https://github.com/michenriksen/gitrob/">https://github.com/michenriksen/gitrob/</a>) finds potentially sensitive files that are pushed to public repositories on GitHub.</li>
				<li>
					TruffleHog (<a class="LinkURL" href="https://github.com/trufflesecurity/truffleHog/">https://github.com/trufflesecurity/truffleHog/</a>) specializes in finding secrets in public GitHub repositories by searching for string patterns and high-entropy strings.</li>
				<li>
					PasteHunter (<a class="LinkURL" href="https://github.com/kevthehermit/PasteHunter/">https://github.com/kevthehermit/PasteHunter/</a>) scans online paste sites for sensitive information.</li>
				<li>
					Wayback Machine (<a class="LinkURL" href="https://archive.org/web/">https://archive.org/web/</a>) is a digital archive of internet content. You can use it to find old versions of sites and their files.</li>
				<li>
					Waybackurls (<a class="LinkURL" href="https://github.com/tomnomnom/waybackurls/">https://github.com/tomnomnom/waybackurls/</a>) fetches URLs from the Wayback Machine.</li>
			</ol>
			<h3 id="h2-501546c05-0024">Tech Stack Fingerprinting</h3>
			<ol class="none">
				<li>
					The CVE database (<a class="LinkURL" href="https://cve.mitre.org/cve/search_cve_list.html">https://cve.mitre.org/cve/search_cve_list.html</a>) contains publicly disclosed vulnerabilities. You can use its website to search for vulnerabilities that might affect your target.</li>
				<li>
					Wappalyzer (<a class="LinkURL" href="https://www.wappalyzer.com/">https://www.wappalyzer.com/</a>) identifies content management systems, frameworks, and programming languages used on a site.</li>
				<li>
					BuiltWith (<a class="LinkURL" href="https://builtwith.com/">https://builtwith.com/</a>) is a website that shows you which web technologies a website is built with.</li>
				<li>
					StackShare (<a class="LinkURL" href="https://stackshare.io/">https://stackshare.io/</a>) is an online platform that allows developers to share the tech they use. You can use it to collect information about your target.</li>
				<li>
					Retire.js (<a class="LinkURL" href="https://retirejs.github.io/retire.js/">https://retirejs.github.io/retire.js/</a>) detects outdated JavaScript libraries and Node.js packages.</li>
			</ol>
			<h3 id="h2-501546c05-0025"><span epub:type="pagebreak" id="Page_107" title="107"/>Automation</h3>
			<ol class="none">
				<li>
					Git (<a class="LinkURL" href="https://git-scm.com/">https://git-scm.com/</a>) is an open sourced version-control system. You can use its <code>git diff</code> command to keep track of file changes.</li>
			</ol>
			<p>
				You should now have a solid understanding of how to conduct reconnaissance on a target. Remember to keep extensive notes throughout your recon process, as the information you collect can really balloon over time. Once you have a solid understanding of how to conduct recon on a target, you can try to leverage recon platforms like Nuclei (<a class="LinkURL" href="https://github.com/projectdiscovery/nuclei/">https://github.com/projectdiscovery/nuclei/</a>) or Intrigue Core (<a class="LinkURL" href="https://github.com/intrigueio/intrigue-core/">https://github.com/intrigueio/intrigue-core/</a>) to make your recon process more efficient. But when you’re starting out, I recommend that you do recon manually with individual tools or write your own automated recon scripts to learn about the process.</p>
		</section>
	</body>
</html>