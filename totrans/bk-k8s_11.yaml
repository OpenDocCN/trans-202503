- en: '9'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '9'
- en: SERVICE AND INGRESS NETWORKS
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 服务和入口网络
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: A decent amount of complexity was involved in creating a cluster-wide network
    so that all of our Pods could communicate with one another. At the same time,
    we still don’t have all of the networking functionality we need to build scalable,
    resilient applications. We need networking that supports load balancing our application
    components across multiple instances and provides the ability to send traffic
    to new Pod instances as existing instances fail or need to be upgraded. Additionally,
    the Pod network is designed to be private, meaning that it is directly reachable
    only from within the cluster. We need additional traffic routing so that external
    users can reach our application components running in containers.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个集群级别的网络，使得所有Pod可以互相通信，涉及了相当复杂的操作。同时，我们仍然没有获得构建可扩展、弹性应用所需的所有网络功能。我们需要支持将应用组件跨多个实例进行负载均衡的网络，并且提供将流量发送到新的Pod实例的能力，以应对现有实例的故障或升级需求。此外，Pod网络设计为私有网络，意味着它仅能从集群内部直接访问。我们需要额外的流量路由功能，以便外部用户可以访问我们在容器中运行的应用组件。
- en: In this chapter, we’ll look at Service and Ingress networking. Kubernetes Service
    networking provides an entire additional networking layer on top of Pod networking,
    including dynamic discovery and load balancing. We’ll see how this networking
    layer works and how we can use it to expose our application components to the
    rest of the cluster as scalable, resilient services. We’ll then look at how Ingress
    configuration provides traffic routing for these Services to expose them to external
    users.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论服务和入口网络。Kubernetes的服务网络提供了一个额外的网络层，位于Pod网络之上，包括动态发现和负载均衡。我们将看到这个网络层如何工作，以及如何利用它将我们的应用组件暴露给集群中的其他部分，作为可扩展和有弹性的服务。然后，我们将探讨入口配置如何为这些服务提供流量路由，将它们暴露给外部用户。
- en: Services
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务
- en: 'Putting together Deployments and overlay networking, we have the ability to
    create multiple identical container instances with a unique IP address for each.
    Let’s create an NGINX Deployment to illustrate:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 将部署和覆盖网络结合起来，我们可以创建多个相同的容器实例，每个实例都有一个唯一的IP地址。让我们创建一个NGINX部署来说明：
- en: '*nginx-deploy.yaml*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*nginx-deploy.yaml*'
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is similar to Deployments we’ve seen previously. In this case we’re asking
    Kubernetes to maintain five Pods for us, each running an NGINX web server.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们之前看到的部署类似。在这种情况下，我们要求Kubernetes为我们维护五个Pod，每个Pod运行一个NGINX web服务器。
- en: '**NOTE**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*本书的示例仓库位于* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples)。*有关设置的详细信息，请参见“运行示例”部分，[第xx页](ch00.xhtml#ch00lev1sec2)。*'
- en: 'The automated scripts have already placed this file in */opt*, so we can apply
    it to the cluster:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化脚本已经将此文件放置在*/opt*目录下，因此我们可以将其应用到集群中：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After these Pods are running, we can check that they’ve been distributed across
    the cluster and each one has an IP address:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些Pod启动后，我们可以检查它们是否已分布到集群中，并且每个Pod都有一个IP地址：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If these containers were merely clients of some server, that might be all we
    need to do. For example, if our application architecture was driven by sending
    and receiving messages, as long as these containers could connect to the messaging
    server, they’d be able to function as required. However, because these containers
    act as servers, clients need to be able to find them and connect.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些容器只是某个服务器的客户端，那可能就是我们需要做的全部了。例如，如果我们的应用架构是通过发送和接收消息来驱动的，只要这些容器能够连接到消息服务器，它们就能按要求工作。然而，因为这些容器充当服务器的角色，客户端需要能够找到它们并建立连接。
- en: 'As it is, our separate NGINX instances aren’t very practical for clients to
    use. Sure, it’s possible to connect to any one of these NGINX server Pods directly.
    For example, we can communicate with the first one in the list using its IP address:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 就目前而言，我们的独立NGINX实例对客户端来说并不太实用。当然，直接连接到这些NGINX服务器Pod中的任何一个都是可能的。例如，我们可以通过其IP地址与列表中的第一个进行通信：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Unfortunately, just choosing one instance is not going to provide load balancing
    or failover. Additionally, we don’t have any way of knowing ahead of time what
    the Pod IP address is going to be, and every time we make any changes to the Deployment,
    the Pods will be re-created and get new IP addresses.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，单独选择一个实例并不能提供负载均衡或故障转移功能。此外，我们无法提前知道 Pod 的 IP 地址，而且每次对 Deployment 进行更改时，Pods
    会被重新创建并获得新的 IP 地址。
- en: The solution to this situation needs to have two main features. First, we need
    to have a well-known name that clients can use to find a server. Second, we need
    a consistent IP address so that when a client has identified a server, it can
    continue to use the same address for connections even as Pod instances come and
    go. This is exactly what Kubernetes provides with a *Service*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这种情况需要具备两个主要特性。首先，我们需要一个客户端可以用来查找服务器的众所周知的名称。其次，我们需要一个一致的 IP 地址，这样当客户端识别到一个服务器时，即使
    Pod 实例来来去去，也可以继续使用相同的地址进行连接。这正是 Kubernetes 通过 *Service* 提供的功能。
- en: Creating a Service
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建 Service
- en: Let’s create a Service for our NGINX Deployment and see what that gets us. [Listing
    9-1](ch09.xhtml#ch09list1) presents the resource YAML file.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为我们的 NGINX Deployment 创建一个 Service，看看这能带来什么。[清单 9-1](ch09.xhtml#ch09list1)
    提供了资源的 YAML 文件。
- en: '*nginx-service.yaml*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*nginx-service.yaml*'
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Listing 9-1: NGINX Service*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 9-1：NGINX Service*'
- en: 'First, a Service has a `selector` much like a Deployment. This selector is
    used in the same way: to identify the Pods that will be associated with the Service.
    However, unlike a Deployment, a Service does not manage its Pods in any way; it
    simply routes traffic to them.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Service 具有一个 `selector`，与 Deployment 类似。这个选择器以相同的方式使用：用来识别将与 Service 关联的
    Pods。然而，与 Deployment 不同，Service 不以任何方式管理它的 Pods；它只是将流量路由到它们。
- en: The traffic routing is based on the ports we identify in the `ports` field.
    Because the NGINX server is listening on port 80, we need to specify that as the
    `targetPort`. We can use any `port` we want, but it’s simplest to keep it the
    same, especially as 80 is the default port for HTTP.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 流量路由是基于我们在 `ports` 字段中指定的端口。由于 NGINX 服务器监听的是端口 80，我们需要将其指定为 `targetPort`。我们可以使用任何我们想要的
    `port`，但最简单的做法是保持一致，特别是因为 80 是 HTTP 的默认端口。
- en: 'Let’s apply this Service to the cluster:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个 Service 应用到集群中：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can now see that the Service was created:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以看到已经创建了 Service：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This `nginx` Service has the default type of `ClusterIP`. Kubernetes has automatically
    assigned a cluster IP address for this Service. The IP address is in an entirely
    different address space from that of our Pods.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `nginx` Service 默认类型为 `ClusterIP`。Kubernetes 已经为该 Service 自动分配了一个集群 IP 地址。该
    IP 地址与我们的 Pods 的地址空间完全不同。
- en: Using the selector, this Service will identify our NGINX server Pods and automatically
    start load balancing traffic to them. As Pods matching the selector come and go,
    the Service will automatically update its load balancing accordingly. As long
    as the Service exists, it will keep the same IP address, so clients have a consistent
    way of finding our NGINX server instances.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用选择器，这个 Service 会识别我们的 NGINX 服务器 Pods，并自动开始将流量负载均衡到它们。当匹配选择器的 Pods 来来去去时，Service
    会自动更新其负载均衡。只要 Service 存在，它就会保持相同的 IP 地址，这样客户端就能持续通过一致的方式找到我们的 NGINX 服务器实例。
- en: 'Let’s verify that we can reach an NGINX server through the Service:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证是否能够通过 Service 访问 NGINX 服务器：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can see that the Service has correctly identified all five NGINX Pods:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，Service 已经正确地识别了所有五个 NGINX Pods：
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `Endpoints` field shows that the Service is currently routing traffic to
    all five NGINX Pods. As a client, we don’t need to know which Pod was used to
    handle our request. We interact solely with the Service IP address and allow the
    Service to choose an instance for us.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`Endpoints` 字段显示 Service 当前正在将流量路由到所有五个 NGINX Pods。作为客户端，我们不需要知道是哪个 Pod 处理了我们的请求。我们只与
    Service IP 地址交互，允许 Service 为我们选择一个实例。'
- en: Of course, for this example, we had to look up the IP address of the Service.
    To make it easier on clients, we still should provide a well-known name.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在这个示例中，我们必须查找 Service 的 IP 地址。为了方便客户端，我们仍然应该提供一个众所周知的名称。
- en: Service DNS
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Service DNS
- en: Kubernetes provides a well-known name for each Service through a DNS (Domain
    Name System) server that is dynamically updated with the name and IP address of
    every Service in the cluster. Each Pod is configured with this DNS server such
    that a Pod can use the name of the Service to connect to an instance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes通过DNS（域名系统）服务器为每个服务提供一个众所周知的名称，该服务器会动态更新集群中每个服务的名称和IP地址。每个Pod都配置了这个DNS服务器，这样Pod就可以使用服务的名称来连接到一个实例。
- en: 'Let’s create a Pod that we can use to try this out:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个Pod，以便我们可以尝试这个操作：
- en: '*pod.yaml*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*pod.yaml*'
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We’re using `alpine` rather than `busybox` as the image for this Pod because
    we’ll want to use some DNS commands that require us to install a more full-featured
    DNS client.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`alpine`而不是`busybox`作为这个Pod的镜像，因为我们需要使用一些DNS命令，这些命令要求我们安装一个功能更强大的DNS客户端。
- en: '**NOTE**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*BusyBox makes a great debug image for Kubernetes clusters because it’s tiny
    and has many useful commands. However, in the interest of keeping BusyBox tiny,
    it’s typical for the commands to include only the most popular options. Alpine
    makes a great alternative for debugging. The default Alpine image uses BusyBox
    to provide many of its initial commands, but it’s possible to replace them with
    a full-featured alternative by just installing the appropriate package.*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*BusyBox是一个非常适合Kubernetes集群调试的镜像，因为它非常小并且包含许多有用的命令。然而，为了保持BusyBox的小巧，通常这些命令只包含最常用的选项。Alpine是一个非常好的调试替代品。默认的Alpine镜像使用BusyBox来提供其许多初始命令，但通过安装适当的软件包，也可以将其替换为功能更完整的替代品。*'
- en: 'Next, create the Pod:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建Pod：
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: After it’s running, let’s use it to connect to our NGINX Service, as demonstrated
    in [Listing 9-2](ch09.xhtml#ch09list2).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 它启动后，让我们使用它连接到我们的NGINX服务，如[示例9-2](ch09.xhtml#ch09list2)中所示。
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*Listing 9-2: Connect to NGINX Service*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例9-2：连接到NGINX服务*'
- en: 'We were able to use the name of the Service, `nginx`, and that name resolved
    to the Service IP address. This worked because our Pod is configured to talk to
    the DNS server that’s built in to the cluster:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够使用服务的名称`nginx`，并且该名称解析为服务的IP地址。之所以能这样工作，是因为我们的Pod已经配置为与集群内置的DNS服务器通信：
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We print out the file */etc/resolv.conf* inside the container because this is
    the file that is used to configure DNS.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打印容器内的文件*/etc/resolv.conf*，因为这是用来配置DNS的文件。
- en: 'The name server `10.96.0.10` referenced is itself a Kubernetes Service, but
    it’s in the `kube-system` Namespace, so we need to look there for it:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 引用的名称服务器`10.96.0.10`本身就是一个Kubernetes服务，但它位于`kube-system`命名空间中，因此我们需要在该命名空间中查找它：
- en: '[PRE13]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `kube-dns` Service connects to a DNS server Deployment called CoreDNS that
    listens for changes to Services in the Kubernetes cluster. CoreDNS updates the
    DNS server configuration as required to stay up to date with the current cluster
    configuration.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-dns`服务连接到一个名为CoreDNS的DNS服务器部署，该服务器监听Kubernetes集群中服务的变化。CoreDNS根据需要更新DNS服务器配置，以保持与当前集群配置同步。'
- en: Name Resolution and Namespaces
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 名称解析和命名空间
- en: DNS names in a Kubernetes cluster are based on the Namespace as well as the
    cluster domain. Because our Pod is in the `default` Namespace, it has been configured
    with a search path of `default.svc.cluster.local` as the first entry in the list,
    so it will search the `default` Namespace first when looking for Services. This
    is why we were able to use the bare Service name `nginx` to find the `nginx` Service—that
    Service is also in the `default` Namespace.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群中的DNS名称是基于命名空间以及集群域的。由于我们的Pod位于`default`命名空间，因此它的搜索路径已被配置为`default.svc.cluster.local`，这是列表中的第一个条目，因此在查找服务时，它将首先搜索`default`命名空间。这就是为什么我们能够使用裸服务名称`nginx`来找到`nginx`服务的原因——该服务也位于`default`命名空间中。
- en: 'We could have also found the same Service using the fully qualified name:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用完全限定的名称找到相同的服务：
- en: '[PRE14]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Understanding this interaction between Namespaces and Service lookup is important.
    One common deployment pattern for a Kubernetes cluster is to deploy the same application
    multiple times to different Namespaces and use simple hostnames for application
    components to communicate with one another. This pattern is often used to deploy
    a “development” and “production” version of an application to the same cluster.
    If we’re planning to use this pattern, we need to be sure that we stick to bare
    hostnames when our application components try to find one another; otherwise,
    we could end up communicating with the wrong version of our application.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 理解命名空间和服务查找之间的相互作用非常重要。Kubernetes集群的一个常见部署模式是将同一个应用程序多次部署到不同的命名空间，并使用简单的主机名使应用程序组件相互通信。这个模式通常用于将应用程序的“开发”版本和“生产”版本部署到同一个集群中。如果我们打算使用这种模式，我们需要确保在应用程序组件尝试相互发现时，坚持使用纯粹的主机名；否则，我们可能会与应用程序的错误版本进行通信。
- en: Another important configuration item in */etc/resolv.conf* is the `ndots` entry.
    The `ndots` entry tells the hostname resolver that when it sees a hostname with
    four or fewer dots, it should try appending the various search domains *prior*
    to performing an absolute search without any domain appended. This is critical
    to make sure that we try to find services inside the cluster before reaching outside
    the cluster.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在*/etc/resolv.conf*中另一个重要的配置项是`ndots`条目。`ndots`条目告诉主机名解析器，当它看到一个包含四个或更少点的主机名时，它应该先尝试附加各种搜索域，而不是在没有附加任何域名的情况下直接执行绝对查找。这对于确保我们在访问集群外部之前，尝试查找集群内的服务至关重要。
- en: As a result, when we used the name `nginx` in [Listing 9-2](ch09.xhtml#ch09list2),
    the DNS resolver within our container immediately tried `nginx.default.svc.cluster.local`
    and found the correct Service.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，当我们在[清单 9-2](ch09.xhtml#ch09list2)中使用名称`nginx`时，我们容器内的DNS解析器立即尝试了`nginx.default.svc.cluster.local`并找到了正确的服务。
- en: 'To make sure this is clear, let’s try one more example: looking up a Service
    in another Namespace. The `kube-system` Namespace has a `metrics-server` Service.
    To find it, let’s use the standard host lookup `dig` command in our Pod.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保这一点清晰明了，我们再看一个例子：查找另一个命名空间中的服务。`kube-system`命名空间中有一个`metrics-server`服务。为了查找它，我们可以在Pod中使用标准的主机查找命令`dig`。
- en: 'Our Pod is using Alpine Linux, so we need to install the `bind-tools` package
    to get access to `dig`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Pod使用的是Alpine Linux，因此我们需要安装`bind-tools`包来获取`dig`工具：
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, let’s try looking up `metrics-server` using the bare name first:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们首先使用纯主机名尝试查找`metrics-server`：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We add the `+search` flag onto the command to tell `dig` to use the search path
    information from */etc/resolv.conf*. However, even with that flag, we don’t find
    the Service, because our Pod is in the `default` Namespace, so the search path
    doesn’t lead `dig` to look in the `kube-system` Namespace.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在命令中添加了`+search`标志，告诉`dig`使用来自*/etc/resolv.conf*的搜索路径信息。然而，即便有了这个标志，我们仍然无法找到服务，因为我们的Pod位于`default`命名空间，因此搜索路径没有让`dig`去查找`kube-system`命名空间。
- en: 'Let’s try again, this time specifying the correct Namespace:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试一次，这次指定正确的命名空间：
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This lookup works, and we are able to get the IP address for the `metrics-server`
    Service. It works because the search path includes `svc.cluster.local` as its
    second entry. After initially trying `metrics-server.kube-system.default.svc.cluster.local`,
    which doesn’t work, `dig` then tries `metrics-server.kube-system.svc.cluster.local`,
    which does.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查找成功了，我们能够获得`metrics-server`服务的IP地址。之所以成功，是因为搜索路径的第二个条目包括了`svc.cluster.local`。在最初尝试了`metrics-server.kube-system.default.svc.cluster.local`（失败）之后，`dig`接着尝试了`metrics-server.kube-system.svc.cluster.local`，这次成功了。
- en: Traffic Routing
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 流量路由
- en: We’ve seen how to create and use Services, but we haven’t yet looked at how
    the actual traffic routing works. It turns out that Service network traffic works
    in a way that’s completely different from the overlay networks we saw in [Chapter
    8](ch08.xhtml#ch08), which can lead to some confusion.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何创建和使用服务，但还没有了解实际的流量路由是如何工作的。事实证明，服务的网络流量工作方式与我们在[第8章](ch08.xhtml#ch08)中看到的覆盖网络完全不同，这可能会导致一些混淆。
- en: 'For example, because we can use `wget` to reach an NGINX server instance using
    the `nginx` Service name, we might expect to be able to use `ping`, as well, but
    that doesn’t work:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，虽然我们可以使用`wget`通过`nginx`服务名访问NGINX服务器实例，但我们可能会期待同样能够使用`ping`，然而这并不起作用：
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Name resolution worked as expected, so `ping` knew what destination IP address
    to use for its ICMP packets. But there was no reply from that IP address. We could
    look at every host and container network interface in our cluster and never find
    an interface that carries the Service IP address of `10.100.221.220`. So how is
    our HTTP traffic getting through to an NGINX Service instance?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 名称解析按预期工作，因此 `ping` 知道应该使用哪个目标 IP 地址来发送 ICMP 包。但是，从该 IP 地址没有收到回应。我们可以查看集群中每个主机和容器的网络接口，却永远找不到承载
    `10.100.221.220` 这个 Service IP 地址的接口。那么，为什么我们的 HTTP 流量能够顺利到达 NGINX Service 实例呢？
- en: On every node in our cluster, there is a component called `kube-proxy` that
    configures traffic routing for Services. `kube-proxy` is run as a DaemonSet in
    the `kube-system` Namespace. Each `kube-proxy` instance watches for changes to
    Services in the cluster and configures the Linux firewall to route traffic.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们集群的每个节点上，都有一个名为 `kube-proxy` 的组件，它配置 Service 的流量路由。`kube-proxy` 作为一个 DaemonSet
    在 `kube-system` 命名空间中运行。每个 `kube-proxy` 实例都会监视集群中 Service 的变化，并配置 Linux 防火墙以路由流量。
- en: 'We can use `iptables` commands to look at the firewall configuration to see
    how `kube-proxy` has configured traffic routing for our `nginx` Service:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `iptables` 命令查看防火墙配置，看看 `kube-proxy` 如何为我们的 `nginx` Service 配置流量路由：
- en: '[PRE19]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `iptables-save` command backs up all of the current Linux firewall rules,
    so it’s useful for printing out all rules. The `grep` command searches for the
    comment string that `kube-proxy` applies to the Service rules it creates. In this
    example, `kube-proxy` has created two rules for the Service as a whole. The first
    rule ➊ looks for traffic destined for our Service that is *not* coming from the
    Pod network. This traffic must be marked for Network Address Translation (NAT)
    masquerade so that the source of any reply traffic will be rewritten to be the
    Service IP address rather than the actual Pod that handles the request. The second
    rule ➋ sends all traffic destined for the Service to a separate rule chain that
    will send it to a Pod instance. Note that in both cases, the rules only match
    for TCP traffic that is destined for port 80.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`iptables-save` 命令备份当前所有 Linux 防火墙规则，因此它对于打印所有规则非常有用。`grep` 命令用于搜索 `kube-proxy`
    应用于它所创建的 Service 规则的注释字符串。在这个示例中，`kube-proxy` 为整个 Service 创建了两条规则。第一条规则 ➊ 查找目标是我们的
    Service 且*不是*来自 Pod 网络的流量。这个流量必须标记为网络地址转换（NAT）伪装，以便任何响应流量的源地址会被重写为 Service IP
    地址，而不是实际处理请求的 Pod。第二条规则 ➋ 将所有目标是 Service 的流量发送到一个独立的规则链，这个规则链会将流量转发到一个 Pod 实例。注意，在这两种情况下，规则只会匹配目标端口为
    80 的 TCP 流量。'
- en: 'We can examine this separate rule chain to see how the actual routing to individual
    Pods works. Be sure to replace the name of the rule chain in this command with
    the one shown in the previous output:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查这个独立的规则链，看看实际是如何将流量路由到各个 Pod 实例的。确保在此命令中替换规则链的名称为之前输出中显示的名称：
- en: '[PRE20]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The output shows five rules, corresponding to each of the five NGINX Pod instances
    the Service’s selector matched. The five rules together provide random load balancing
    across all the instances so that each one has an equal chance of being selected
    for new connections.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了五条规则，分别对应于 Service 的选择器匹配的五个 NGINX Pod 实例。这五条规则共同提供了跨所有实例的随机负载均衡，确保每个实例都有相同的机会被选中处理新的连接。
- en: It may seem strange that the `probability` figure increases for each rule. This
    is necessary because the rules are evaluated sequentially. For the first rule,
    we want a 20 percent chance of choosing the first instance. However, if we don’t
    select the first instance, only four instances are left, so we want a 25 percent
    chance of choosing the second one. The same logic applies until we get to the
    last instance, which we always want to choose if we’ve skipped all the others.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 可能看起来有些奇怪的是，每条规则的 `probability` 数值会递增。这是必要的，因为规则是顺序评估的。对于第一条规则，我们希望有 20% 的概率选择第一个实例。然而，如果我们没有选择第一个实例，剩下的只有四个实例，因此我们希望有
    25% 的概率选择第二个实例。相同的逻辑适用于所有后续实例，直到最后一个实例，我们希望在跳过了其他所有实例之后总是选择它。
- en: 'Let’s quickly verify that these rules go to the expected destination (again,
    be sure to replace the name of the rule chain in this command):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速验证这些规则是否到达预期的目标地点（再次提醒，请确保在此命令中替换规则链的名称）：
- en: '[PRE21]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This output shows two rules. The first is the other half of the NAT masquerade
    configuration, as we mark all packets that leave our Pod instance so that they
    can have their source address rewritten to appear to come from the Service. The
    second rule is the one that actually routes Service traffic to a specific Pod
    as it performs a rewrite of the destination address so that a packet originally
    destined for the Service IP is now destined for a Pod. From there, the overlay
    networking takes over to actually send the packet to the correct container.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出展示了两条规则。第一条是 NAT 伪装配置的另一半，我们标记了所有离开 Pod 实例的包，以便它们的源地址可以被重写，看起来是来自 Service。第二条规则实际上是将流量路由到特定
    Pod 的规则，它执行目标地址的重写，使得原本应该发送到 Service IP 的包现在发送到 Pod。之后，覆盖网络接管，实际上将数据包发送到正确的容器。
- en: With this understanding of how Service traffic is actually routed, it makes
    sense that our ICMP packets didn’t make it through. The firewall rule that `kube-proxy`
    created applies only to TCP traffic destined for port 80\. As a result, there
    was no firewall rule to rewrite our ICMP packets and therefore no way for them
    to make it to a networking stack that could reply to them. Similarly, if we have
    a container that’s listening on multiple ports, we will be able to connect to
    any of those ports by directly using the Pod IP address, but the Service IP address
    will route traffic only if we explicitly declare that port in the Service specification.
    It can be a significant source of confusion when deploying an application where
    the Pod starts up as expected and listens for traffic, but a misconfiguration
    of the Service means that the traffic is not being routed to all of the correct
    destination ports.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解 Service 流量是如何被路由的，我们可以理解为什么 ICMP 包没有通过。`kube-proxy` 创建的防火墙规则仅适用于目标端口为 80
    的 TCP 流量。因此，没有防火墙规则来重写我们的 ICMP 包，因此它们无法到达能够回复它们的网络栈。类似地，如果我们有一个监听多个端口的容器，我们将能够直接使用
    Pod 的 IP 地址连接到这些端口，但 Service IP 地址只会在我们明确声明该端口的 Service 规范时路由流量。这在部署应用程序时可能会引起混淆，Pod
    按预期启动并监听流量，但 Service 配置错误导致流量无法路由到所有正确的目标端口。
- en: External Networking
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 外部网络
- en: We now have enough layers of networking to meet all of our internal cluster
    communication needs. Each Pod has its own IP address and has connectivity to other
    Pods as well as the control plane, and with Service networking we have automatic
    load balancing and failover based on running multiple Pod instances with a Service.
    However, we’re still missing the ability for external users to access services
    running in our cluster.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经具备了足够的网络层来满足所有内部集群通信需求。每个 Pod 都有自己的 IP 地址，并且可以连接到其他 Pod 以及控制平面，利用 Service
    网络我们可以实现基于运行多个 Pod 实例的负载均衡和故障转移。然而，我们仍然缺少外部用户访问我们集群中服务的能力。
- en: To provide access for external users, we can no longer rely solely on the cluster-specific
    IP address ranges that we use for Pods and Services, given that external networks
    don’t recognize those address ranges. Instead, we’ll need a way to allocate externally
    routable IP addresses to our Services, either by explicitly associating an IP
    address with a Service or by using an *ingress controller* that listens to external
    traffic and routes it to Services.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供外部用户的访问，我们不能再仅依赖于集群特定的 IP 地址范围，因为外部网络无法识别这些地址范围。相反，我们需要一种方法将外部可路由的 IP 地址分配给我们的服务，可以通过显式地将
    IP 地址与服务关联，或者使用 *ingress 控制器* 来监听外部流量并将其路由到服务。
- en: External Services
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 外部服务
- en: 'The `nginx` Service we created earlier was a `ClusterIP` Service, the default
    Service type. Kubernetes supports multiple Service types, including Service types
    that are made for Services that need to be exposed externally:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前创建的 `nginx` Service 是一个 `ClusterIP` Service，它是默认的 Service 类型。Kubernetes
    支持多种 Service 类型，包括为需要公开的服务而设计的类型：
- en: None Also known as a *headless* Service, it’s used to enable tracking of selected
    Pods but without an IP address or any network routing behavior.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: None 也称为 *无头* 服务，用于启用对选定 Pod 的跟踪，但没有 IP 地址或任何网络路由行为。
- en: ClusterIP The default Service type that provides tracking of selected Pods,
    a cluster IP address that is routed internally, and a well-known name in the cluster
    DNS.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ClusterIP 默认的 Service 类型，提供对选定 Pod 的跟踪，一个集群内路由的集群 IP 地址，以及在集群 DNS 中的一个知名名称。
- en: NodePort Extends `ClusterIP` and also provides a port on all nodes in the cluster
    that is routed to the Service.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: NodePort 扩展了 `ClusterIP`，并为集群中的所有节点提供了一个端口，该端口路由到该服务。
- en: LoadBalancer Extends `NodePort` and also uses an underlying cloud provider to
    obtain an IP address that is externally reachable.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: LoadBalancer 扩展了 `NodePort`，并使用底层云提供商来获取一个外部可访问的 IP 地址。
- en: ExternalName Aliases a well-known Service name in the cluster DNS to some external
    DNS name. Used to make external resources appear to be in-cluster Services.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ExternalName 在集群 DNS 中为一个知名服务名称设置别名，指向某个外部 DNS 名称。用于使外部资源看起来像集群内的服务。
- en: Of these Service types, the `NodePort` and `LoadBalancer` types are most useful
    for exposing Services outside the cluster. The `LoadBalancer` type seems the most
    straightforward, as it simply adds an external IP to the Service. However, it
    requires integration with an underlying cloud environment to create the external
    IP address when the Service is created, to route traffic from that IP address
    to the cluster’s nodes, and to create a DNS entry outside the cluster that enables
    external users to find the Service as a host on some pre-registered domain that
    we already own, rather than a `cluster.local` domain that works only within the
    cluster.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些服务类型中，`NodePort` 和 `LoadBalancer` 类型最适合将服务暴露到集群外部。`LoadBalancer` 类型似乎最直接，因为它只是为服务添加一个外部
    IP 地址。然而，它需要与底层云环境集成，以便在创建服务时创建外部 IP 地址，将流量从该 IP 地址路由到集群的节点，并在集群外创建 DNS 记录，使外部用户能够在我们已经拥有的预注册域名下找到该服务，而不是仅在集群内有效的
    `cluster.local` 域名。
- en: For this reason, a `LoadBalancer` Service is most useful for cases in which
    we know what cloud environment we’re using and we’re creating Services that will
    live for a long time. For HTTP traffic, we can get most of the benefit of a `LoadBalancer`
    Service by using a `NodePort` Service together with an ingress controller, with
    the added feature of better support for dynamically deploying new applications
    with new Services.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`LoadBalancer` 服务对于我们知道所使用的云环境，并且我们创建的服务会长期存在的情况最为有用。对于 HTTP 流量，我们可以通过将 `NodePort`
    服务与入口控制器一起使用，获得 `LoadBalancer` 服务的大部分好处，并且还可以更好地支持动态部署新的应用程序和服务。
- en: 'Before moving on to an ingress controller, let’s turn our existing `nginx`
    Service into a `NodePort` Service so that we can look at the effect. We can do
    this using a patch file:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续讨论入口控制器之前，让我们将现有的 `nginx` 服务转换为 `NodePort` 服务，这样我们就可以查看效果。我们可以通过使用补丁文件来实现这一点：
- en: '*nginx-nodeport.yaml*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*nginx-nodeport.yaml*'
- en: '[PRE22]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: A patch file allows us to update only the specific fields we care about. In
    this case, we are updating only the type of the Service. For this to work, we
    just need to specify that one changed field in its correct position in the hierarchy,
    which allows Kubernetes to know what field to modify. We don’t need to change
    the selector or ports for our Service, only the type, so the patch is very simple.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 补丁文件允许我们仅更新我们关心的特定字段。在这种情况下，我们只更新服务的类型。为了使其生效，我们只需要在正确的位置指定一个更改的字段，这样 Kubernetes
    就能知道该修改哪个字段。我们不需要更改服务的选择器或端口，只需要更改类型，因此补丁非常简单。
- en: 'Let’s use the patch:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个补丁：
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: For this command, we must specify the resource to be patched and a patch file
    to be used. The result is identical to if we had edited the YAML resource file
    for the Service and then used `kubectl apply` again.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个命令，我们必须指定要补丁的资源和要使用的补丁文件。其结果与我们编辑服务的 YAML 资源文件并再次使用 `kubectl apply` 相同。
- en: 'The Service now looks a little different:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 服务现在看起来有点不同：
- en: '[PRE24]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: A `NodePort` Service provides all the behavior of a `ClusterIP` Service, so
    we still have a cluster IP associated with our `nginx` Service. The Service even
    kept the same cluster IP. The only change is the `PORT` field now shows that the
    Service port 80 is attached to node port 31326.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`NodePort` 服务提供了 `ClusterIP` 服务的所有行为，因此我们仍然拥有与我们的 `nginx` 服务关联的集群 IP。服务甚至保留了相同的集群
    IP。唯一的变化是 `PORT` 字段现在显示服务端口 80 被附加到节点端口 31326。'
- en: 'The `kube-proxy` Service on every cluster node is listening on this port (be
    sure to use the correct node port for your Service):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 每个集群节点上的 `kube-proxy` 服务正在监听这个端口（请确保使用适合你服务的正确节点端口）：
- en: '[PRE25]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'As a result, we can still use the `nginx` Service name inside our Pod, but
    we can also use the NodePort from the host:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，我们仍然可以在我们的 Pod 内部使用 `nginx` 服务名，但也可以使用来自主机的 NodePort：
- en: '[PRE26]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Because `kube-proxy` is listening on all network interfaces, we’ve successfully
    exposed this Service to external users.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`kube-proxy`监听所有网络接口，我们已经成功地将这个服务暴露给外部用户。
- en: Ingress Services
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Ingress服务
- en: Although we’ve successfully exposed our NGINX Service outside the cluster, we
    still don’t provide a great user experience for external users. To use the `NodePort`
    Service, external users will need to know the IP address of at least one of our
    cluster nodes, and they’ll need to know the exact port on which each Service is
    listening. That port could change if the Service is deleted and re-created. We
    could partially address this by telling Kubernetes which port to use for the `NodePort`,
    but we don’t want to do this with any arbitrary Service because multiple Services
    may choose the same port.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已成功将NGINX服务暴露到集群外部，但仍然没有为外部用户提供良好的用户体验。要使用`NodePort`服务，外部用户需要知道至少一个集群节点的IP地址，并且需要知道每个服务监听的精确端口。如果该服务被删除并重新创建，则该端口可能会发生变化。我们可以通过告诉Kubernetes使用哪个端口来部分解决这个问题，但我们不想对任何任意服务这么做，因为多个服务可能会选择相同的端口。
- en: What we really need is a single external entry point to our cluster that keeps
    track of multiple services that are available and uses rules to route traffic
    to them. This way, we can do all of our routing configuration inside the cluster
    so that Services can come and go dynamically. At the same time, we can have a
    single well-known entry point for our cluster that all external users can use.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正需要的是一个单一的外部入口点，用于跟踪可用的多个服务，并使用规则将流量路由到它们。这样，我们就可以在集群内部完成所有的路由配置，以便服务可以动态地来来去去。同时，我们可以为我们的集群提供一个单一的、所有外部用户都能使用的入口点。
- en: 'For HTTP traffic, Kubernetes provides exactly this capability, calling it an
    *Ingress*. To configure our cluster to route external HTTP traffic to Services,
    we need to define the set of Ingress resources that specify the routing and to
    deploy the ingress controller that receives and routes the traffic. We already
    installed our ingress controller when we set up our cluster:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于HTTP流量，Kubernetes提供了正是这种能力，称之为*Ingress*。为了配置我们的集群将外部HTTP流量路由到服务，我们需要定义一组Ingress资源，指定路由规则，并部署接收和路由流量的入口控制器。当我们设置集群时，我们已经安装了入口控制器：
- en: '[PRE27]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Our ingress controller includes a Deployment and a Service. As the Service is
    of type `NodePort`, we know that `kube-proxy` is listening to ports 80 and 443
    on all of our cluster’s nodes, ready to route traffic to the associated Pod.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的入口控制器包括一个部署和一个服务。由于该服务类型为`NodePort`，我们知道`kube-proxy`正在集群所有节点的80和443端口上监听，准备将流量路由到相关的Pod。
- en: As the name implies, our ingress controller is actually an instance of an NGINX
    web server; however, in this case NGINX is solely acting as an HTTP reverse proxy
    rather than serving any web content of its own. The ingress controller listens
    for changes to Ingress resources in the cluster and reconfigures NGINX to connect
    to backend servers based on the rules that are defined. These rules use host or
    path information from the HTTP request to select a Service for the request.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，我们的入口控制器实际上是一个NGINX web服务器的实例；然而，在这种情况下，NGINX仅作为HTTP反向代理，而不提供任何自己的网页内容。入口控制器监听集群中Ingress资源的变化，并根据定义的规则重新配置NGINX，以连接到后端服务器。这些规则使用HTTP请求中的主机或路径信息来选择为该请求提供服务的服务。
- en: 'Let’s create an Ingress resource to route traffic to the `nginx` Service we
    defined in [Listing 9-1](ch09.xhtml#ch09list1). Here’s the resource we’ll create:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个Ingress资源，将流量路由到我们在[示例9-1](ch09.xhtml#ch09list1)中定义的`nginx`服务。以下是我们将创建的资源：
- en: '*nginx-ingress.yaml*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*nginx-ingress.yaml*'
- en: '[PRE28]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This resource instructs the ingress controller to look at the HTTP `Host` header.
    If it sees `web01` as the `Host` header, it then tries to match against a path
    in the `paths` we specified. In this case, all paths will match the path `/` prefix,
    so all traffic will be routed to the `nginx` Service.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个资源指示入口控制器查看HTTP `Host`头部。如果它看到`web01`作为`Host`头部，它就会尝试与我们指定的`paths`中的路径进行匹配。在这种情况下，所有路径都会匹配`/`前缀路径，因此所有流量都会路由到`nginx`服务。
- en: 'Before we apply this to the cluster, let’s confirm what happens if we try to
    use a hostname that the ingress controller doesn’t recognize. We’ll use the high-availability
    IP address that’s associated with our cluster, as the cluster’s load balancer
    will forward that to one of the instances:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在将其应用到集群之前，让我们确认一下如果我们尝试使用一个入口控制器无法识别的主机名会发生什么。我们将使用与集群相关联的高可用性IP地址，因为集群的负载均衡器会将其转发到其中一个实例：
- en: '[PRE29]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `-H "Host:web01"` flag in the `curl` command tells `curl` to use the value
    `host01` as the `Host` header in the HTTP request. This is necessary given that
    we don’t have a DNS server in our example cluster that can turn `web01` into our
    cluster’s IP address.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl` 命令中的 `-H "Host:web01"` 标志告诉 `curl` 在 HTTP 请求中使用 `host01` 作为 `Host` 头部的值。考虑到我们示例集群中没有能够将
    `web01` 转换为集群 IP 地址的 DNS 服务器，这是必要的。'
- en: As we can see, the NGINX server that’s acting as the ingress controller is configured
    to reply with a `404 Not Found` error message whenever it gets a request that
    doesn’t match any configured Ingress resource. In this case, because we haven’t
    created any Ingress resources yet, any request will get this response.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，作为 Ingress 控制器的 NGINX 服务器被配置为每当收到不匹配任何已配置的 Ingress 资源的请求时，都会回复一个 `404
    Not Found` 错误信息。在这种情况下，因为我们还没有创建任何 Ingress 资源，所以任何请求都会得到这个响应。
- en: 'Let’s apply the `web01` Ingress resource to the cluster:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 `web01` Ingress 资源应用到集群中：
- en: '[PRE30]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now that the Ingress resource exists, as [Listing 9-3](ch09.xhtml#ch09list3)
    illustrates, HTTP port 80 requests on both the cluster high-availability IP and
    individual hosts are routed to the `nginx` Service:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在既然已存在 Ingress 资源，如[清单 9-3](ch09.xhtml#ch09list3)所示，集群的高可用性 IP 和各个主机上的 HTTP
    80 端口请求都会被路由到 `nginx` 服务：
- en: '[PRE31]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '*Listing 9-3: NGINX via Ingress*'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 9-3：通过 Ingress 配置 NGINX*'
- en: The output in both cases is the same, showing that traffic is being routed to
    the `nginx` Service.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 两种情况的输出是相同的，显示流量正被路由到 `nginx` 服务。
- en: In the `web01-ingress` resource, we were able to use the bare name of the `nginx`
    Service. The Service name lookup is based on where the Ingress resource is located.
    Because we created the Ingress resource in the default Namespace, that is where
    it looks first for Services.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `web01-ingress` 资源中，我们能够使用 `nginx` 服务的裸名称。服务名称的查找是基于 Ingress 资源所在的位置。由于我们在默认的命名空间中创建了
    Ingress 资源，因此它会首先在该命名空间中查找服务。
- en: Putting this all together, we now have a high-availability solution to route
    traffic from external users to HTTP servers in our cluster. This combines our
    cluster’s high-availability IP address `192.168.61.10` with an ingress controller
    exposed as a `NodePort` Service on port 80 of all our cluster’s nodes. The ingress
    controller can be dynamically configured to expose additional Services by creating
    new Ingress resources.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 将这一切结合起来，我们现在有了一个高可用性解决方案，将外部用户的流量路由到集群中的 HTTP 服务器。这将集群的高可用性 IP 地址 `192.168.61.10`
    与暴露为 `NodePort` 服务的 Ingress 控制器结合在一起，该服务位于集群所有节点的 80 端口。Ingress 控制器可以通过创建新的 Ingress
    资源动态配置以暴露其他服务。
- en: Ingress in Production
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生产环境中的 Ingress
- en: The `curl` command in [Listing 9-3](ch09.xhtml#ch09list3) still looks a little
    strange, as we’re required to override the HTTP `Host` header manually. We need
    to perform a few additional steps to use Ingress resources to expose services
    in a production cluster.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 9-3](ch09.xhtml#ch09list3) 中的 `curl` 命令看起来仍然有点奇怪，因为我们需要手动覆盖 HTTP `Host`
    头。为了在生产集群中使用 Ingress 资源暴露服务，我们还需要执行一些额外的步骤。'
- en: First, we need our cluster to have an externally routable IP address together
    with a well-known name that is registered in DNS. The best way to do that is with
    a wildcard DNS scheme so that all hosts in a given domain are all routed to the
    cluster’s external IP. For example, if we own the domain `cluster.example.com`,
    we could create a DNS entry so that `*.cluster.example.com` routes to the cluster’s
    external IP address.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要让集群拥有一个可外部路由的 IP 地址，并且这个地址需要有一个在 DNS 中注册的知名名称。做到这一点的最佳方法是使用通配符 DNS 方案，使得给定域名下的所有主机都路由到集群的外部
    IP。例如，如果我们拥有 `cluster.example.com` 域名，我们可以创建一个 DNS 条目，使得 `*.cluster.example.com`
    路由到集群的外部 IP 地址。
- en: This approach still works with larger clusters that span multiple networks.
    We just need to have multiple IP addresses associated with the DNS entry, possibly
    using location-aware DNS servers that route clients to the closest service.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在跨多个网络的大型集群中仍然有效。我们只需要为 DNS 条目关联多个 IP 地址，可能还需要使用基于位置的 DNS 服务器，将客户端路由到最接近的服务。
- en: Next, we need to create an SSL certificate for our ingress controller that includes
    our wildcard DNS as a Subject Alternative Name (SAN). This will allow our ingress
    controller to provide a secure HTTP connection for external users no matter what
    specific service hostname they are using.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要为我们的 Ingress 控制器创建一个 SSL 证书，该证书包含我们的通配符 DNS 作为主题备用名称（SAN）。这将使得我们的 Ingress
    控制器能够为外部用户提供安全的 HTTP 连接，无论他们使用的是哪个特定的服务主机名。
- en: Finally, when we define our Services, we need to specify the fully qualified
    domain name for the `host` field. For the preceding example, we would specify
    `web01.cluster.example.com` rather than just `web01`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当我们定义我们的 Service 时，需要为 `host` 字段指定完全限定的域名。对于上述示例，我们应该指定 `web01.cluster.example.com`，而不是仅仅使用
    `web01`。
- en: After we’ve performed these additional steps, any external user would be able
    to connect via HTTPS to the fully qualified hostname of our Service, such as `https://web01.cluster.example.com`.
    This hostname would resolve to our cluster’s external IP address, and the load
    balancer would route it to one of the cluster’s nodes. At that point, our ingress
    controller, listening on the standard port of 443, would offer its wildcard certificate,
    which would match what the client expects. As soon as the secure connection is
    established, the ingress controller would inspect the HTTP `Host` header and proxy
    a connection to the correct Service, sending back the HTTP response to the client.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成这些额外步骤之后，任何外部用户都可以通过 HTTPS 连接到我们 Service 的完全限定主机名，例如 `https://web01.cluster.example.com`。这个主机名会解析到我们集群的外部
    IP 地址，负载均衡器会将流量路由到集群的某个节点。此时，我们的 Ingress 控制器会监听标准端口 443，提供其通配符证书，该证书与客户端的期望匹配。安全连接建立后，Ingress
    控制器会检查 HTTP `Host` 头，并将连接代理到正确的 Service，随后将 HTTP 响应返回给客户端。
- en: The advantage of this approach is that after we have it set up, we can deploy
    a new Ingress resource at any time to expose a Service externally, and as long
    as we choose a unique hostname, it won’t collide with any other exposed Service.
    After the initial setup, all of the configuration is maintained within the cluster
    itself, and we still have a highly available configuration for all of our Services.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是，一旦我们完成设置，就可以随时部署新的 Ingress 资源来将 Service 暴露到外部，只要我们选择一个唯一的主机名，就不会与任何其他暴露的
    Service 冲突。在初始设置完成后，所有配置都保存在集群内部，我们仍然为所有 Service 保持高度可用的配置。
- en: Final Thoughts
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最后思考
- en: 'Routing network traffic in a Kubernetes cluster might involve a great deal
    of complexity, but the end result is straightforward: we can deploy our application
    components to a cluster, with automatic scaling and failover, and external users
    can access our application using a well-known name without having to know how
    the application is deployed or how many container instances we’re using to meet
    demand. If we build our application to be resilient, our application containers
    can upgrade to new versions or restart in response to failure without users even
    being aware of the change.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 集群中路由网络流量可能涉及相当复杂的操作，但最终结果是直接的：我们可以将应用程序组件部署到集群中，并实现自动扩展和故障转移，外部用户可以使用一个广为人知的名称访问我们的应用，而不需要知道应用程序是如何部署的，或者我们使用了多少个容器实例来满足需求。如果我们将应用程序构建得具有弹性，那么我们的应用程序容器可以在不影响用户的情况下升级到新版本或因故障重启。
- en: Of course, if we’re going to build application components that are resilient,
    it’s important to know what can go wrong in deploying containers. In the next
    chapter, we’ll look at some common issues with deploying containers to a Kubernetes
    cluster and how to debug them.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果我们要构建具有弹性的应用程序组件，那么了解容器部署过程中可能出现的问题非常重要。在下一章中，我们将讨论一些在 Kubernetes 集群中部署容器时常见的问题以及如何调试这些问题。
