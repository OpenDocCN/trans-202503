- en: '9'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SERVICE AND INGRESS NETWORKS
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A decent amount of complexity was involved in creating a cluster-wide network
    so that all of our Pods could communicate with one another. At the same time,
    we still don’t have all of the networking functionality we need to build scalable,
    resilient applications. We need networking that supports load balancing our application
    components across multiple instances and provides the ability to send traffic
    to new Pod instances as existing instances fail or need to be upgraded. Additionally,
    the Pod network is designed to be private, meaning that it is directly reachable
    only from within the cluster. We need additional traffic routing so that external
    users can reach our application components running in containers.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll look at Service and Ingress networking. Kubernetes Service
    networking provides an entire additional networking layer on top of Pod networking,
    including dynamic discovery and load balancing. We’ll see how this networking
    layer works and how we can use it to expose our application components to the
    rest of the cluster as scalable, resilient services. We’ll then look at how Ingress
    configuration provides traffic routing for these Services to expose them to external
    users.
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Putting together Deployments and overlay networking, we have the ability to
    create multiple identical container instances with a unique IP address for each.
    Let’s create an NGINX Deployment to illustrate:'
  prefs: []
  type: TYPE_NORMAL
- en: '*nginx-deploy.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is similar to Deployments we’ve seen previously. In this case we’re asking
    Kubernetes to maintain five Pods for us, each running an NGINX web server.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The automated scripts have already placed this file in */opt*, so we can apply
    it to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After these Pods are running, we can check that they’ve been distributed across
    the cluster and each one has an IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If these containers were merely clients of some server, that might be all we
    need to do. For example, if our application architecture was driven by sending
    and receiving messages, as long as these containers could connect to the messaging
    server, they’d be able to function as required. However, because these containers
    act as servers, clients need to be able to find them and connect.
  prefs: []
  type: TYPE_NORMAL
- en: 'As it is, our separate NGINX instances aren’t very practical for clients to
    use. Sure, it’s possible to connect to any one of these NGINX server Pods directly.
    For example, we can communicate with the first one in the list using its IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, just choosing one instance is not going to provide load balancing
    or failover. Additionally, we don’t have any way of knowing ahead of time what
    the Pod IP address is going to be, and every time we make any changes to the Deployment,
    the Pods will be re-created and get new IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this situation needs to have two main features. First, we need
    to have a well-known name that clients can use to find a server. Second, we need
    a consistent IP address so that when a client has identified a server, it can
    continue to use the same address for connections even as Pod instances come and
    go. This is exactly what Kubernetes provides with a *Service*.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Service
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s create a Service for our NGINX Deployment and see what that gets us. [Listing
    9-1](ch09.xhtml#ch09list1) presents the resource YAML file.
  prefs: []
  type: TYPE_NORMAL
- en: '*nginx-service.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 9-1: NGINX Service*'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, a Service has a `selector` much like a Deployment. This selector is
    used in the same way: to identify the Pods that will be associated with the Service.
    However, unlike a Deployment, a Service does not manage its Pods in any way; it
    simply routes traffic to them.'
  prefs: []
  type: TYPE_NORMAL
- en: The traffic routing is based on the ports we identify in the `ports` field.
    Because the NGINX server is listening on port 80, we need to specify that as the
    `targetPort`. We can use any `port` we want, but it’s simplest to keep it the
    same, especially as 80 is the default port for HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply this Service to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now see that the Service was created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This `nginx` Service has the default type of `ClusterIP`. Kubernetes has automatically
    assigned a cluster IP address for this Service. The IP address is in an entirely
    different address space from that of our Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Using the selector, this Service will identify our NGINX server Pods and automatically
    start load balancing traffic to them. As Pods matching the selector come and go,
    the Service will automatically update its load balancing accordingly. As long
    as the Service exists, it will keep the same IP address, so clients have a consistent
    way of finding our NGINX server instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s verify that we can reach an NGINX server through the Service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the Service has correctly identified all five NGINX Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `Endpoints` field shows that the Service is currently routing traffic to
    all five NGINX Pods. As a client, we don’t need to know which Pod was used to
    handle our request. We interact solely with the Service IP address and allow the
    Service to choose an instance for us.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, for this example, we had to look up the IP address of the Service.
    To make it easier on clients, we still should provide a well-known name.
  prefs: []
  type: TYPE_NORMAL
- en: Service DNS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Kubernetes provides a well-known name for each Service through a DNS (Domain
    Name System) server that is dynamically updated with the name and IP address of
    every Service in the cluster. Each Pod is configured with this DNS server such
    that a Pod can use the name of the Service to connect to an instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a Pod that we can use to try this out:'
  prefs: []
  type: TYPE_NORMAL
- en: '*pod.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We’re using `alpine` rather than `busybox` as the image for this Pod because
    we’ll want to use some DNS commands that require us to install a more full-featured
    DNS client.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*BusyBox makes a great debug image for Kubernetes clusters because it’s tiny
    and has many useful commands. However, in the interest of keeping BusyBox tiny,
    it’s typical for the commands to include only the most popular options. Alpine
    makes a great alternative for debugging. The default Alpine image uses BusyBox
    to provide many of its initial commands, but it’s possible to replace them with
    a full-featured alternative by just installing the appropriate package.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, create the Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: After it’s running, let’s use it to connect to our NGINX Service, as demonstrated
    in [Listing 9-2](ch09.xhtml#ch09list2).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 9-2: Connect to NGINX Service*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We were able to use the name of the Service, `nginx`, and that name resolved
    to the Service IP address. This worked because our Pod is configured to talk to
    the DNS server that’s built in to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We print out the file */etc/resolv.conf* inside the container because this is
    the file that is used to configure DNS.
  prefs: []
  type: TYPE_NORMAL
- en: 'The name server `10.96.0.10` referenced is itself a Kubernetes Service, but
    it’s in the `kube-system` Namespace, so we need to look there for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `kube-dns` Service connects to a DNS server Deployment called CoreDNS that
    listens for changes to Services in the Kubernetes cluster. CoreDNS updates the
    DNS server configuration as required to stay up to date with the current cluster
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Name Resolution and Namespaces
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DNS names in a Kubernetes cluster are based on the Namespace as well as the
    cluster domain. Because our Pod is in the `default` Namespace, it has been configured
    with a search path of `default.svc.cluster.local` as the first entry in the list,
    so it will search the `default` Namespace first when looking for Services. This
    is why we were able to use the bare Service name `nginx` to find the `nginx` Service—that
    Service is also in the `default` Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could have also found the same Service using the fully qualified name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Understanding this interaction between Namespaces and Service lookup is important.
    One common deployment pattern for a Kubernetes cluster is to deploy the same application
    multiple times to different Namespaces and use simple hostnames for application
    components to communicate with one another. This pattern is often used to deploy
    a “development” and “production” version of an application to the same cluster.
    If we’re planning to use this pattern, we need to be sure that we stick to bare
    hostnames when our application components try to find one another; otherwise,
    we could end up communicating with the wrong version of our application.
  prefs: []
  type: TYPE_NORMAL
- en: Another important configuration item in */etc/resolv.conf* is the `ndots` entry.
    The `ndots` entry tells the hostname resolver that when it sees a hostname with
    four or fewer dots, it should try appending the various search domains *prior*
    to performing an absolute search without any domain appended. This is critical
    to make sure that we try to find services inside the cluster before reaching outside
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, when we used the name `nginx` in [Listing 9-2](ch09.xhtml#ch09list2),
    the DNS resolver within our container immediately tried `nginx.default.svc.cluster.local`
    and found the correct Service.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sure this is clear, let’s try one more example: looking up a Service
    in another Namespace. The `kube-system` Namespace has a `metrics-server` Service.
    To find it, let’s use the standard host lookup `dig` command in our Pod.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our Pod is using Alpine Linux, so we need to install the `bind-tools` package
    to get access to `dig`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s try looking up `metrics-server` using the bare name first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We add the `+search` flag onto the command to tell `dig` to use the search path
    information from */etc/resolv.conf*. However, even with that flag, we don’t find
    the Service, because our Pod is in the `default` Namespace, so the search path
    doesn’t lead `dig` to look in the `kube-system` Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try again, this time specifying the correct Namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This lookup works, and we are able to get the IP address for the `metrics-server`
    Service. It works because the search path includes `svc.cluster.local` as its
    second entry. After initially trying `metrics-server.kube-system.default.svc.cluster.local`,
    which doesn’t work, `dig` then tries `metrics-server.kube-system.svc.cluster.local`,
    which does.
  prefs: []
  type: TYPE_NORMAL
- en: Traffic Routing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ve seen how to create and use Services, but we haven’t yet looked at how
    the actual traffic routing works. It turns out that Service network traffic works
    in a way that’s completely different from the overlay networks we saw in [Chapter
    8](ch08.xhtml#ch08), which can lead to some confusion.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, because we can use `wget` to reach an NGINX server instance using
    the `nginx` Service name, we might expect to be able to use `ping`, as well, but
    that doesn’t work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Name resolution worked as expected, so `ping` knew what destination IP address
    to use for its ICMP packets. But there was no reply from that IP address. We could
    look at every host and container network interface in our cluster and never find
    an interface that carries the Service IP address of `10.100.221.220`. So how is
    our HTTP traffic getting through to an NGINX Service instance?
  prefs: []
  type: TYPE_NORMAL
- en: On every node in our cluster, there is a component called `kube-proxy` that
    configures traffic routing for Services. `kube-proxy` is run as a DaemonSet in
    the `kube-system` Namespace. Each `kube-proxy` instance watches for changes to
    Services in the cluster and configures the Linux firewall to route traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `iptables` commands to look at the firewall configuration to see
    how `kube-proxy` has configured traffic routing for our `nginx` Service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `iptables-save` command backs up all of the current Linux firewall rules,
    so it’s useful for printing out all rules. The `grep` command searches for the
    comment string that `kube-proxy` applies to the Service rules it creates. In this
    example, `kube-proxy` has created two rules for the Service as a whole. The first
    rule ➊ looks for traffic destined for our Service that is *not* coming from the
    Pod network. This traffic must be marked for Network Address Translation (NAT)
    masquerade so that the source of any reply traffic will be rewritten to be the
    Service IP address rather than the actual Pod that handles the request. The second
    rule ➋ sends all traffic destined for the Service to a separate rule chain that
    will send it to a Pod instance. Note that in both cases, the rules only match
    for TCP traffic that is destined for port 80.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can examine this separate rule chain to see how the actual routing to individual
    Pods works. Be sure to replace the name of the rule chain in this command with
    the one shown in the previous output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The output shows five rules, corresponding to each of the five NGINX Pod instances
    the Service’s selector matched. The five rules together provide random load balancing
    across all the instances so that each one has an equal chance of being selected
    for new connections.
  prefs: []
  type: TYPE_NORMAL
- en: It may seem strange that the `probability` figure increases for each rule. This
    is necessary because the rules are evaluated sequentially. For the first rule,
    we want a 20 percent chance of choosing the first instance. However, if we don’t
    select the first instance, only four instances are left, so we want a 25 percent
    chance of choosing the second one. The same logic applies until we get to the
    last instance, which we always want to choose if we’ve skipped all the others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s quickly verify that these rules go to the expected destination (again,
    be sure to replace the name of the rule chain in this command):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This output shows two rules. The first is the other half of the NAT masquerade
    configuration, as we mark all packets that leave our Pod instance so that they
    can have their source address rewritten to appear to come from the Service. The
    second rule is the one that actually routes Service traffic to a specific Pod
    as it performs a rewrite of the destination address so that a packet originally
    destined for the Service IP is now destined for a Pod. From there, the overlay
    networking takes over to actually send the packet to the correct container.
  prefs: []
  type: TYPE_NORMAL
- en: With this understanding of how Service traffic is actually routed, it makes
    sense that our ICMP packets didn’t make it through. The firewall rule that `kube-proxy`
    created applies only to TCP traffic destined for port 80\. As a result, there
    was no firewall rule to rewrite our ICMP packets and therefore no way for them
    to make it to a networking stack that could reply to them. Similarly, if we have
    a container that’s listening on multiple ports, we will be able to connect to
    any of those ports by directly using the Pod IP address, but the Service IP address
    will route traffic only if we explicitly declare that port in the Service specification.
    It can be a significant source of confusion when deploying an application where
    the Pod starts up as expected and listens for traffic, but a misconfiguration
    of the Service means that the traffic is not being routed to all of the correct
    destination ports.
  prefs: []
  type: TYPE_NORMAL
- en: External Networking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now have enough layers of networking to meet all of our internal cluster
    communication needs. Each Pod has its own IP address and has connectivity to other
    Pods as well as the control plane, and with Service networking we have automatic
    load balancing and failover based on running multiple Pod instances with a Service.
    However, we’re still missing the ability for external users to access services
    running in our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: To provide access for external users, we can no longer rely solely on the cluster-specific
    IP address ranges that we use for Pods and Services, given that external networks
    don’t recognize those address ranges. Instead, we’ll need a way to allocate externally
    routable IP addresses to our Services, either by explicitly associating an IP
    address with a Service or by using an *ingress controller* that listens to external
    traffic and routes it to Services.
  prefs: []
  type: TYPE_NORMAL
- en: External Services
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `nginx` Service we created earlier was a `ClusterIP` Service, the default
    Service type. Kubernetes supports multiple Service types, including Service types
    that are made for Services that need to be exposed externally:'
  prefs: []
  type: TYPE_NORMAL
- en: None Also known as a *headless* Service, it’s used to enable tracking of selected
    Pods but without an IP address or any network routing behavior.
  prefs: []
  type: TYPE_NORMAL
- en: ClusterIP The default Service type that provides tracking of selected Pods,
    a cluster IP address that is routed internally, and a well-known name in the cluster
    DNS.
  prefs: []
  type: TYPE_NORMAL
- en: NodePort Extends `ClusterIP` and also provides a port on all nodes in the cluster
    that is routed to the Service.
  prefs: []
  type: TYPE_NORMAL
- en: LoadBalancer Extends `NodePort` and also uses an underlying cloud provider to
    obtain an IP address that is externally reachable.
  prefs: []
  type: TYPE_NORMAL
- en: ExternalName Aliases a well-known Service name in the cluster DNS to some external
    DNS name. Used to make external resources appear to be in-cluster Services.
  prefs: []
  type: TYPE_NORMAL
- en: Of these Service types, the `NodePort` and `LoadBalancer` types are most useful
    for exposing Services outside the cluster. The `LoadBalancer` type seems the most
    straightforward, as it simply adds an external IP to the Service. However, it
    requires integration with an underlying cloud environment to create the external
    IP address when the Service is created, to route traffic from that IP address
    to the cluster’s nodes, and to create a DNS entry outside the cluster that enables
    external users to find the Service as a host on some pre-registered domain that
    we already own, rather than a `cluster.local` domain that works only within the
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, a `LoadBalancer` Service is most useful for cases in which
    we know what cloud environment we’re using and we’re creating Services that will
    live for a long time. For HTTP traffic, we can get most of the benefit of a `LoadBalancer`
    Service by using a `NodePort` Service together with an ingress controller, with
    the added feature of better support for dynamically deploying new applications
    with new Services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on to an ingress controller, let’s turn our existing `nginx`
    Service into a `NodePort` Service so that we can look at the effect. We can do
    this using a patch file:'
  prefs: []
  type: TYPE_NORMAL
- en: '*nginx-nodeport.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: A patch file allows us to update only the specific fields we care about. In
    this case, we are updating only the type of the Service. For this to work, we
    just need to specify that one changed field in its correct position in the hierarchy,
    which allows Kubernetes to know what field to modify. We don’t need to change
    the selector or ports for our Service, only the type, so the patch is very simple.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the patch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: For this command, we must specify the resource to be patched and a patch file
    to be used. The result is identical to if we had edited the YAML resource file
    for the Service and then used `kubectl apply` again.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Service now looks a little different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: A `NodePort` Service provides all the behavior of a `ClusterIP` Service, so
    we still have a cluster IP associated with our `nginx` Service. The Service even
    kept the same cluster IP. The only change is the `PORT` field now shows that the
    Service port 80 is attached to node port 31326.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `kube-proxy` Service on every cluster node is listening on this port (be
    sure to use the correct node port for your Service):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, we can still use the `nginx` Service name inside our Pod, but
    we can also use the NodePort from the host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Because `kube-proxy` is listening on all network interfaces, we’ve successfully
    exposed this Service to external users.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress Services
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although we’ve successfully exposed our NGINX Service outside the cluster, we
    still don’t provide a great user experience for external users. To use the `NodePort`
    Service, external users will need to know the IP address of at least one of our
    cluster nodes, and they’ll need to know the exact port on which each Service is
    listening. That port could change if the Service is deleted and re-created. We
    could partially address this by telling Kubernetes which port to use for the `NodePort`,
    but we don’t want to do this with any arbitrary Service because multiple Services
    may choose the same port.
  prefs: []
  type: TYPE_NORMAL
- en: What we really need is a single external entry point to our cluster that keeps
    track of multiple services that are available and uses rules to route traffic
    to them. This way, we can do all of our routing configuration inside the cluster
    so that Services can come and go dynamically. At the same time, we can have a
    single well-known entry point for our cluster that all external users can use.
  prefs: []
  type: TYPE_NORMAL
- en: 'For HTTP traffic, Kubernetes provides exactly this capability, calling it an
    *Ingress*. To configure our cluster to route external HTTP traffic to Services,
    we need to define the set of Ingress resources that specify the routing and to
    deploy the ingress controller that receives and routes the traffic. We already
    installed our ingress controller when we set up our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Our ingress controller includes a Deployment and a Service. As the Service is
    of type `NodePort`, we know that `kube-proxy` is listening to ports 80 and 443
    on all of our cluster’s nodes, ready to route traffic to the associated Pod.
  prefs: []
  type: TYPE_NORMAL
- en: As the name implies, our ingress controller is actually an instance of an NGINX
    web server; however, in this case NGINX is solely acting as an HTTP reverse proxy
    rather than serving any web content of its own. The ingress controller listens
    for changes to Ingress resources in the cluster and reconfigures NGINX to connect
    to backend servers based on the rules that are defined. These rules use host or
    path information from the HTTP request to select a Service for the request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create an Ingress resource to route traffic to the `nginx` Service we
    defined in [Listing 9-1](ch09.xhtml#ch09list1). Here’s the resource we’ll create:'
  prefs: []
  type: TYPE_NORMAL
- en: '*nginx-ingress.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This resource instructs the ingress controller to look at the HTTP `Host` header.
    If it sees `web01` as the `Host` header, it then tries to match against a path
    in the `paths` we specified. In this case, all paths will match the path `/` prefix,
    so all traffic will be routed to the `nginx` Service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we apply this to the cluster, let’s confirm what happens if we try to
    use a hostname that the ingress controller doesn’t recognize. We’ll use the high-availability
    IP address that’s associated with our cluster, as the cluster’s load balancer
    will forward that to one of the instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The `-H "Host:web01"` flag in the `curl` command tells `curl` to use the value
    `host01` as the `Host` header in the HTTP request. This is necessary given that
    we don’t have a DNS server in our example cluster that can turn `web01` into our
    cluster’s IP address.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the NGINX server that’s acting as the ingress controller is configured
    to reply with a `404 Not Found` error message whenever it gets a request that
    doesn’t match any configured Ingress resource. In this case, because we haven’t
    created any Ingress resources yet, any request will get this response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply the `web01` Ingress resource to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the Ingress resource exists, as [Listing 9-3](ch09.xhtml#ch09list3)
    illustrates, HTTP port 80 requests on both the cluster high-availability IP and
    individual hosts are routed to the `nginx` Service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 9-3: NGINX via Ingress*'
  prefs: []
  type: TYPE_NORMAL
- en: The output in both cases is the same, showing that traffic is being routed to
    the `nginx` Service.
  prefs: []
  type: TYPE_NORMAL
- en: In the `web01-ingress` resource, we were able to use the bare name of the `nginx`
    Service. The Service name lookup is based on where the Ingress resource is located.
    Because we created the Ingress resource in the default Namespace, that is where
    it looks first for Services.
  prefs: []
  type: TYPE_NORMAL
- en: Putting this all together, we now have a high-availability solution to route
    traffic from external users to HTTP servers in our cluster. This combines our
    cluster’s high-availability IP address `192.168.61.10` with an ingress controller
    exposed as a `NodePort` Service on port 80 of all our cluster’s nodes. The ingress
    controller can be dynamically configured to expose additional Services by creating
    new Ingress resources.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress in Production
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `curl` command in [Listing 9-3](ch09.xhtml#ch09list3) still looks a little
    strange, as we’re required to override the HTTP `Host` header manually. We need
    to perform a few additional steps to use Ingress resources to expose services
    in a production cluster.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need our cluster to have an externally routable IP address together
    with a well-known name that is registered in DNS. The best way to do that is with
    a wildcard DNS scheme so that all hosts in a given domain are all routed to the
    cluster’s external IP. For example, if we own the domain `cluster.example.com`,
    we could create a DNS entry so that `*.cluster.example.com` routes to the cluster’s
    external IP address.
  prefs: []
  type: TYPE_NORMAL
- en: This approach still works with larger clusters that span multiple networks.
    We just need to have multiple IP addresses associated with the DNS entry, possibly
    using location-aware DNS servers that route clients to the closest service.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to create an SSL certificate for our ingress controller that includes
    our wildcard DNS as a Subject Alternative Name (SAN). This will allow our ingress
    controller to provide a secure HTTP connection for external users no matter what
    specific service hostname they are using.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when we define our Services, we need to specify the fully qualified
    domain name for the `host` field. For the preceding example, we would specify
    `web01.cluster.example.com` rather than just `web01`.
  prefs: []
  type: TYPE_NORMAL
- en: After we’ve performed these additional steps, any external user would be able
    to connect via HTTPS to the fully qualified hostname of our Service, such as `https://web01.cluster.example.com`.
    This hostname would resolve to our cluster’s external IP address, and the load
    balancer would route it to one of the cluster’s nodes. At that point, our ingress
    controller, listening on the standard port of 443, would offer its wildcard certificate,
    which would match what the client expects. As soon as the secure connection is
    established, the ingress controller would inspect the HTTP `Host` header and proxy
    a connection to the correct Service, sending back the HTTP response to the client.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of this approach is that after we have it set up, we can deploy
    a new Ingress resource at any time to expose a Service externally, and as long
    as we choose a unique hostname, it won’t collide with any other exposed Service.
    After the initial setup, all of the configuration is maintained within the cluster
    itself, and we still have a highly available configuration for all of our Services.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Routing network traffic in a Kubernetes cluster might involve a great deal
    of complexity, but the end result is straightforward: we can deploy our application
    components to a cluster, with automatic scaling and failover, and external users
    can access our application using a well-known name without having to know how
    the application is deployed or how many container instances we’re using to meet
    demand. If we build our application to be resilient, our application containers
    can upgrade to new versions or restart in response to failure without users even
    being aware of the change.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if we’re going to build application components that are resilient,
    it’s important to know what can go wrong in deploying containers. In the next
    chapter, we’ll look at some common issues with deploying containers to a Kubernetes
    cluster and how to debug them.
  prefs: []
  type: TYPE_NORMAL
