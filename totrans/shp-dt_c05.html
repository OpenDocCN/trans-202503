<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" epub:prefix="index: http://www.index.com/" lang="en" xml:lang="en">
<head>
<title>Chapter 5: Geometry in Data Science</title>
<link href="NSTemplate_v1.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:ea5eeba6-9dea-4463-bfe4-b91d6c6b5861" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter chapter">
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_95" title="95"/><a class="XrefDestination" id="5"/><span class="XrefDestination" id="xref-503083c05-001"/>5</span><br/>
<span class="ChapterTitle"><a class="XrefDestination" id="GeometryinDataScience"/><span class="XrefDestination" id="xref-503083c05-002"/>Geometry in Data Science</span></h1>
</header>
<figure class="opener">
<img alt="" src="image_fi/book_art/chapterart.png"/>
</figure>
<p class="ChapterIntro">In this chapter, we’ll explore several tools from geometry: we’ll look at distance metrics and their use in <em>k</em>-nearest neighbor algorithms; we’ll discuss manifold learning algorithms that map high-dimensional data to potentially curved lower-dimensional manifolds; and we’ll see how to apply fractal geometry to stock market data. The motivation for this chapter follows, among other things, from the <em>manifold hypothesis</em>, which posits that real-world data often has a natural dimensionality lower than the dimensionality of the dataset collected. In other words, a dataset that has 20 variables (that is, a dimensionality of 20) might have a better representation in a 12-dimensional space or an 8-dimensional space. Given the curse of dimensionality, representing data in lower-dimensional spaces is ideal (particularly when the original dimensionality of a dataset is large, as in genomics or proteomics data). Choosing the right distance measurements needed to create these representations has important implications for solution quality.</p>
<h2 id="h1-503083c05-0001"><span epub:type="pagebreak" id="Page_96" title="96"/><a class="XrefDestination" id="IntroductiontoDistanceMetricsinData"/><span class="XrefDestination" id="xref-503083c05-003"/>Introduction to Distance Metrics in Data</h2>
<p class="BodyFirst">Many machine learning algorithms depend on distance metrics, which provide a measure between points or objects in a space or manifold. Changes in choice of distance metric can impact machine learning performance dramatically, as we’ll see later in this chapter. <em>Distance metrics </em>provide a measure between points or objects in a space or manifold. This can be relatively straightforward like using a ruler to measure the distance between two points on a flat sheet of paper, as demonstrated in <a href="#figure5-1" id="figureanchor5-1">Figure 5-1</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05001r.png"/>
<figcaption><p><a id="figure5-1">Figure 5-1</a>: A plot of two points on a sheet of paper and the line connecting them</p></figcaption>
</figure>
<p>However, measuring the distance between two points on a sphere using a ruler will surely be a bit more complicated.</p>
<p>If you used a piece of string to limn out the shortest path connecting the two points on the sphere, as in <a href="#figure5-2" id="figureanchor5-2">Figure 5-2</a>, you could mark the distance on the string and then use a ruler to measure that distance on the straightened-out string. This is akin to what is done with distances on manifolds, where <em>geodesics</em> (shortest paths between two points relative to the curved manifold) are lifted into the <em>tangent space </em>(a zero-curvature space defined by tangent lines, tangent planes, and higher-dimensional tangents) to measure distances.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05002.png"/>
<figcaption><p><a id="figure5-2">Figure 5-2</a>: A plot of two points on a sphere, along with the geodesic connecting them</p></figcaption>
</figure>
<p>We’ll explore tangent spaces and their applications in machine learning in more depth in <span class="xref" itemid="xref_target_Chapter 6"><a href="c06.xhtml">Chapter 6</a></span>, but for now, you can think of lifting the string to a large sheet of paper and measuring its length with a ruler to measure distance outside of the curved space, where it’s more difficult to establish a standard measurement. While geodesics and tangent spaces look counterintuitive, they follow from our knowledge of tangents in Euclidean geometry and derivatives in calculus.</p>
<p><span epub:type="pagebreak" id="Page_97" title="97"/>However, there are other situations in which distances between two points are a bit more complicated. Consider walking from one house to another in the neighborhood, as shown in <a href="#figure5-3" id="figureanchor5-3">Figure 5-3</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05003.png"/>
<figcaption><p><a id="figure5-3">Figure 5-3</a>: A plot of houses in a neighborhood, where one is walking between two houses</p></figcaption>
</figure>
<p>Unless one is able to walk through neighboring houses without running into exterior and interior walls (not to mention disgruntled neighbors!), it’s not possible to draw a straight line or geodesic between the houses that gives a direct route, as you can see in <a href="#figure5-4" id="figureanchor5-4">Figure 5-4</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05004.png"/>
<figcaption><p><a id="figure5-4">Figure 5-4</a>: A plot of houses in a neighborhood, where one attempts a straight line between houses</p></figcaption>
</figure>
<p>Instead, it’s a lot more practical to take the sidewalks (<a href="#figure5-5" id="figureanchor5-5">Figure 5-5</a>).</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05005.png"/>
<figcaption><p><a id="figure5-5">Figure 5-5</a>: A plot of houses in a neighborhood, where one walks on the sidewalks between houses</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_98" title="98"/>Distance is often discrete, rather than continuous, or lies on a manifold with curvature. Understanding the geometry of the data space in which the data points live can give a good indication of what distance metric is appropriate for the data. In the following section, we’ll go over some common distance metrics in machine learning, and then, in the sections after that, we’ll apply these distances to <em>k</em>-NN algorithms and dimensionality reduction algorithms.</p>
<h2 id="h1-503083c05-0002"><a class="XrefDestination" id="MetricMenagerie"/><span class="XrefDestination" id="xref-503083c05-004"/>Common Distance Metrics</h2>
<p class="BodyFirst">Given the nuances of measuring distance, it’s important to understand some of the more common distance metrics used in machine learning, including one we briefly encountered in <span class="xref" itemid="xref_target_Chapter 4"><a href="c04.xhtml">Chapter 4</a></span> (Wasserstein distance, used to compare persistent homology results). There are an infinite number of distance metrics, and some distance metrics have parameters that can give rise to an infinite number of variations. Thus, we cannot cover all possible distance metrics one could encounter in machine learning. We’ve left out some that are useful in recommender systems, such as cosine distance, as they are uncommon metrics within topological data analysis or network analysis applications. We’ll explore some of the more common ones; if you’re interested in going further, we suggest you explore the field of <em>metric geometry</em>.</p>
<h3 id="h2-503083c05-0001"><a class="XrefDestination" id="SimulatingOurData"/><span class="XrefDestination" id="xref-503083c05-005"/>Simulating a Small Dataset</h3>
<p class="BodyFirst">Before we start exploring common distance metrics, let’s simulate some data with <a href="#listing5-1" id="listinganchor5-1">Listing 5-1</a>.</p>
<pre><code>#create data
a&lt;-rbinom(5,4,0.2)
b&lt;-rbinom(5,1,0.5)
c&lt;-rbinom(5,2,0.1)
mydata&lt;-as.data.frame(cbind(a,b,c))

#create plot
library(scatterplot3d)
scatterplot3d(a,b,c,main="Scatterplot of 3-Dimensional Data")</code></pre>
<p class="CodeListingCaption"><a id="listing5-1">Listing 5-1</a>: A script that simulates and plots a small dataset</p>
<p>This script creates a dataset with three variables and plots points in a three-dimensional space. This should give a plot with points lying on the three axes (<a href="#figure5-6" id="figureanchor5-6">Figure 5-6</a>).</p>
<span epub:type="pagebreak" id="Page_99" title="99"/><figure>
<img alt="" class="" src="image_fi/503083c05/f05006.png"/>
<figcaption><p><a id="figure5-6">Figure 5-6</a>: A plot of five points, all lying on axes defined by variables <em>a</em>, <em>b</em>, and <em>c</em></p></figcaption>
</figure>
<p>This dataset includes the points shown in <a href="#listing5-2" id="listinganchor5-2">Listing 5-2</a>, which we will use to calculate distances between points.</p>
<pre><code>&gt; <b>mydata</b>
  a b c
1 2 1 0
2 0 0 1
3 1 0 0
4 0 0 0
5 3 0 0</code></pre>
<p class="CodeListingCaption"><a id="listing5-2">Listing 5-2</a>: A matrix of the five points in the simulated dataset with random variables <em>a</em>, <em>b</em>, and <em>c</em></p>
<p>Now that we have a dataset generated, let’s look at some standard distance metrics that can be used to measure the distance between pairs of points in the dataset. R comes with a handy package, called the <em>stats</em> package (which comes with the base R installation), for calculating some of the common distance metrics used on data through the <code>dist()</code> function.</p>
<h3 id="h2-503083c05-0002"><a class="XrefDestination" id="UsingNorm-BasedDistanceMetrics"/><span class="XrefDestination" id="xref-503083c05-006"/>Using Norm-Based Distance Metrics</h3>
<p class="BodyFirst">The first distances we’ll consider are related. The <em>norm </em>of a function or vector is a measurement of the “length” of that function or vector. The norm involves summing distance differences to a power and then applying that power’s root to the result. For the <em>Euclidean distance</em> between points, for example, the squares of differences are summed before taking the square root of the result. For single vectors (that is, a single data point), the norm <span epub:type="pagebreak" id="Page_100" title="100"/>will be a weighted distance from the origin, where the axes mutually intersect. You can think of this as the length of a straight line from the origin to the point being measured. Going back to the scatterplot of our points, this might be drawn like in <a href="#figure5-7" id="figureanchor5-7">Figure 5-7</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05007.png"/>
<figcaption><p><a id="figure5-7">Figure 5-7</a>: A plot of the five points with a straight line pointing to one of the points in the set</p></figcaption>
</figure>
<p>The most common norm used to measure metric distance between points is probably the Euclidean distance mentioned earlier, given by the L<sup>2</sup>-norm, defined as the square root of squared distance between points where L is a placeholder for the vector (or vectors) and the exponent is the power of the norm (here, 2). This is the distance typically taught in high school geometry classes, and it is also referred to as the <em>Pythagorean distance</em>. We saw it in <a href="#figure5-4">Figure 5-4</a>, which showed a straight line of shortest distance between the houses (traveling as the bird flies above the houses). Statisticians typically use the square of the Euclidean distance metric when calculating squared errors in regression algorithms; for reasons we won’t delve into here, using the square of Euclidean distance is very natural.</p>
<p>Related to the L<sup>2</sup>-norm is the L<sup>1</sup>-norm, or <em>Manhattan distance</em>. Manhattan distance calculations are much like the neighborhood example given in <a href="#figure5-5">Figure 5-5</a>. Manhattan distance is defined as the sum of point differences along each axis, with the axes’ point differences summed into a final tally of axis distances. Let’s say we have two points (0, 1) and (1, 0), which might represent whether a patient has a gene mutation in either gene of interest within a disease model. The Manhattan distance is (0 + 1) + (1 + 0), or the sum of point differences across all vector axes. In this example, we find the Manhattan distance is 2.</p>
<p>This metric is useful when working with count data or other discrete data formats, such as the example dataset generated earlier in this section. <a href="#figure5-5">Figure 5-5</a> demonstrates this type of distance calculation, where the person needs to walk on the streets along the north-south and east-west axes. <span epub:type="pagebreak" id="Page_101" title="101"/>Manhattan distance and L<sup>1</sup>-norms often come up in applications of Lasso and elastic net regression, where it is used to set beta coefficients to 0 if they are within a certain distance of the origin, thereby performing variable selection and creating a sparse model. This is useful in situations where the dimensionality of the independent variable set is high (such as in genomic data).</p>
<p>A generalization of both the L<sup>1</sup>-norm and L<sup>2</sup>-norm is the <em>Minkowski distance</em>, which generalizes norms from L<sup>3</sup>-norms to L<sup>∞</sup>-norms. The L<sup>∞</sup>-norm is another special instance of norm-based distances, dubbed the <em>Chebyshev distance</em>. Mathematically, Chebyshev distance is the maximum distance between points along any axis. It is often used in problems involving planned movements of machinery or autonomous systems (like drones).</p>
<p>As the dimension of the norm increases, the Minkowski distance values typically decrease and stabilize. Thus, Minkowski distances with high-dimensional norms can act as distance smoothers that rein in strange or unusually large distance calculations found with the Manhattan or Euclidean distance calculations. Minkowski distance does impose a few conditions, including that the zero vector has a length of zero, that application of a positive scalar multiple to a vector does not change the vector’s direction, and that the shortest distance between two points is a straight line (known as the <em>triangle inequality condition</em>). In the <code>dist()</code> function of R, the dimension of the norm is given by the parameter <code>p</code>, with <code>p=1</code> corresponding to Manhattan distance, <code>p=2</code> corresponding to Euclidean distance, and so on.</p>
<p>A special extension of Manhattan distance is the <em>Canberra distance</em>, which is a weighted sum of the L<sup>1</sup>-norm. Technically, Canberra distance is computed by finding the absolute value of the distance between a pair of points divided that by the sum of the pair of points’ absolute values, which then is summed across point pairs. It can be a useful distance metric when dealing with outliers, intrusion detection, or mixed types of predictors (continuous and discrete measures). The example point in <a href="#figure5-7">Figure 5-7</a> likely isn’t a statistical outlier, but it certainly lies in a different part of the data space than the other simulated points.</p>
<p>Let’s run these distances and compare the results on the dataset we simulated earlier in this section; add the following to the code in <a href="#listing5-1">Listing 5-1</a>:</p>
<pre><code>#run distance metrics on example dataset
d1&lt;-dist(mydata,"euclidean",upper=T,diag=T)
d2&lt;-dist(mydata,"manhattan",upper=T,diag=T)
d3&lt;-dist(mydata,"canberra",upper=T,diag=T)
d4&lt;-dist(mydata,"minkowski",p=1,upper=T,diag=T)
d5&lt;-dist(mydata,"minkowski",p=2,upper=T,diag=T)
d6&lt;-dist(mydata,"minkowski",p=10,upper=T,diag=T)</code></pre>
<p>This code computes distance metrics for Euclidean, Manhattan, Canberra, and Minkowski distances applied to our example dataset. Looking at Euclidean distance measurements between pairs of points in the simulated dataset, shown in <a href="#table5-1" id="tableanchor5-1">Table 5-1</a>, we see values with many decimal points for many pairs of points, owing to the square root involved in calculating the Euclidean distance.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table5-1">Table 5-1</a>: Euclidean Distance Calculations Between Pairs of Points in <a href="#listing5-2">Listing 5-2</a>’s Matrix<span epub:type="pagebreak" id="Page_102" title="102"/></p></figcaption>
<table border="1" id="table-503083c05-0001">
<thead>
<tr>
<td><b>Euclidean</b></td>
<td><b>1</b></td>
<td><b>2</b></td>
<td><b>3</b></td>
<td><b>4</b></td>
<td><b>5</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><b>1</b></td>
<td>0</td>
<td>2.44949</td>
<td>1.414214</td>
<td>2.236068</td>
<td>1.414214</td>
</tr>
<tr>
<td><b>2</b></td>
<td>2.44949</td>
<td>0</td>
<td>1.414214</td>
<td>1</td>
<td>3.162278</td>
</tr>
<tr>
<td><b>3</b></td>
<td>1.414214</td>
<td>1.414214</td>
<td>0</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td><b>4</b></td>
<td>2.236068</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td><b>5</b></td>
<td>1.414214</td>
<td>3.162278</td>
<td>2</td>
<td>3</td>
<td>0</td>
</tr>
</tbody>
</table>
</figure>
<p>Moving on to Manhattan distance (<a href="#table5-2" id="tableanchor5-2">Table 5-2</a>), the distances between pairs of points become whole numbers, as the calculation involves discrete steps along each axis separating the points.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table5-2">Table 5-2</a>: Manhattan Distance Calculations Between Pairs of Points in <a href="#listing5-2">Listing 5-2</a>’s Matrix</p></figcaption>
<table border="1" id="table-503083c05-0002">
<thead>
<tr>
<td><b>Manhattan</b></td>
<td><b>1</b></td>
<td><b>2</b></td>
<td><b>3</b></td>
<td><b>4</b></td>
<td><b>5</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><b>1</b></td>
<td>0</td>
<td>4</td>
<td>2</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td><b>2</b></td>
<td>4</td>
<td>0</td>
<td>2</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td><b>3</b></td>
<td>2</td>
<td>2</td>
<td>0</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td><b>4</b></td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td><b>5</b></td>
<td>2</td>
<td>4</td>
<td>2</td>
<td>3</td>
<td>0</td>
</tr>
</tbody>
</table>
</figure>
<p>As expected, the Minkowski distance calculations match the Manhattan distance for <code>p=1</code> and Euclidean distance for <code>p=2</code>. In <a href="#table5-3" id="tableanchor5-3">Table 5-3</a>, you can see Minkowski distances with <code>p=1</code>.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table5-3">Table 5-3</a>: Minkowski Distance Calculations Between Pairs of Points in <a href="#listing5-2">Listing 5-2</a>’s Matrix</p></figcaption>
<table border="1" id="table-503083c05-0003">
<thead>
<tr>
<td><b>Minkowski <code>p=1</code></b></td>
<td><b>1</b></td>
<td><b>2</b></td>
<td><b>3</b></td>
<td><b>4</b></td>
<td><b>5</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><b>1</b></td>
<td>0</td>
<td>4</td>
<td>2</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td><b>2</b></td>
<td>4</td>
<td>0</td>
<td>2</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td><b>3</b></td>
<td>2</td>
<td>2</td>
<td>0</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td><b>4</b></td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td><b>5</b></td>
<td>2</td>
<td>4</td>
<td>2</td>
<td>3</td>
<td>0</td>
</tr>
</tbody>
</table>
</figure>
<p>The Canberra distance gives some similar and overlapping values with Manhattan distance. However, some distances are different (particularly pairs involving points 2 or 3), owing to the weighted parts of the distance calculation, as shown in <a href="#table5-4" id="tableanchor5-4">Table 5-4</a>.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table5-4">Table 5-4</a>: Canberra Distance Calculations Between Pairs of Points in <a href="#listing5-2">Listing 5-2</a>’s Matrix<span epub:type="pagebreak" id="Page_103" title="103"/></p></figcaption>
<table border="1" id="table-503083c05-0004">
<thead>
<tr>
<td><b>Canberra</b></td>
<td><b>1</b></td>
<td><b>2</b></td>
<td><b>3</b></td>
<td><b>4</b></td>
<td><b>5</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><b>1</b></td>
<td>0</td>
<td>3</td>
<td>2</td>
<td>3</td>
<td>1.8</td>
</tr>
<tr>
<td><b>2</b></td>
<td>3</td>
<td>0</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td><b>3</b></td>
<td>2</td>
<td>3</td>
<td>0</td>
<td>3</td>
<td>1.5</td>
</tr>
<tr>
<td><b>4</b></td>
<td>3</td>
<td>3</td>
<td>3</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td><b>5</b></td>
<td>1.8</td>
<td>3</td>
<td>1.5</td>
<td>3</td>
<td>0</td>
</tr>
</tbody>
</table>
</figure>
<p>For some points in <a href="#listing5-2">Listing 5-2</a>’s distance matrix calculations, these three distances give the same distance score for a pair of points (such as for points 4 and 5). However, some of the distances are quite different when we increase the value of <code>p</code> (such as points 1 and 2). If we’re using the distance metrics in a support vector machine classifier, we might end up with a very different line cutting our data into groups—or very different error rates.</p>
<p>There are other ways to modify or extend norm-based distances. One popular modification is like the Canberra distance: <em>Mahalanobis distance</em> applies a weighting scheme to Euclidean distance calculations before taking the square root of the result, such that Euclidean distance is weighted by the covariance matrix. If the covariance matrix is simply the identity matrix, Mahalanobis distance will collapse to Euclidean distance. If the covariance matrix is diagonal, the result is a standardized Euclidean distance. Thus, Mahalanobis distance provides a type of “centered” distance metric that can identify leverage points and outliers within a data sample. It’s often used in clustering and discriminant analyses, as outliers and leverage points can skew results.</p>
<p>There’s a simple way to calculate Mahalanobis distance in R: the <code>mahalanobis()</code> function. Let’s add to our script again:</p>
<pre><code>#run Mahalanobis distance metrics
#first use the covariance to center the data
d7&lt;-mahalanobis(mydata,center=F,cov=cov(mydata))

#then center to one of the points of the data, in this case point 1
d8&lt;-mahalanobis(mydata,center=c(2,1,0),cov=cov(mydata))

#then use the column means to center the data
d9&lt;-mahalanobis(mydata,center=colMeans(mydata),cov=cov(mydata))</code></pre>
<p>This code will calculate Mahalanobis distance with various centering strategies, yielding three different measures of leverage/weighted standard distance from a defined reference, detailed in <a href="#table5-5" id="tableanchor5-5">Table 5-5</a>.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table5-5">Table 5-5</a>: Mahalanobis Distance Results for the Individual Points from <a href="#figure5-7">Figure 5-7</a>’s Matrix<span epub:type="pagebreak" id="Page_104" title="104"/></p></figcaption>
<table border="1" id="table-503083c05-0005">
<thead>
<tr>
<td><b>Mahalanobis</b></td>
<td><b>1</b></td>
<td><b>2</b></td>
<td><b>3</b></td>
<td><b>4</b></td>
<td><b>5</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><b>Covariance only</b></td>
<td>6.857143</td>
<td>6.857143</td>
<td>0.857143</td>
<td>0</td>
<td>7.714286</td>
</tr>
<tr>
<td><b>Point 1</b></td>
<td>0</td>
<td>8</td>
<td>5.428571</td>
<td>6.857143</td>
<td>7.714286</td>
</tr>
<tr>
<td><b>Column means</b></td>
<td>3.2</td>
<td>3.2</td>
<td>0.628571</td>
<td>2.057143</td>
<td>2.914286</td>
</tr>
</tbody>
</table>
</figure>
<p>By using each point as a center, you can complete a full distance matrix similar to how the <code>dist()</code> function creates the distance matrix. You would simply loop through the individual points and append rows to a data frame.</p>
<p>A few interesting observations come out of the Mahalanobis distance calculations. When only covariance is used, the origin becomes the reference point for calculating distances, and point 4, which is located at the origin, has a Mahalanobis distance of 0. However, when column means are used to center the data, this point jumps to a much farther away value. This suggests that point 4 is quite far away from the column means, though it is perfectly centered at the origin. Another interesting trend involves point 3, which is quite close to both the origin and the centered column means, which come out to (1.2, 0.2, 0.2) in this dataset. Point 3 is located at (1, 0, 0), which is both near the origin and near this centered column mean. The other points are relatively far from both the origin and the centered column means.</p>
<p>We can add these column means to our plot of this data and visualize a bit of how Mahalanobis distance works by adding to our script again:</p>
<pre><code>#add point to dataset created earlier in this section
colmean&lt;-c(1.2,0.2,0.2)
mydata&lt;-rbind(mydata,colmean)

#create plot
library(scatterplot3d)
scatterplot3d(mydata[,1],mydata[,2],mydata[,3],
main="Scatterplot of 3-Dimensional Data")</code></pre>
<p>This code adds the column mean point to the original dataset to examine where the “middle” of the data should be located in the three-dimensional space; the code should yield a plot similar <a href="#figure5-8" id="figureanchor5-8">Figure 5-8</a>.</p>
<p>Examining <a href="#figure5-8">Figure 5-8</a> and comparing it to <a href="#figure5-6">Figure 5-6</a>, we can see that a point has been placed off the axes that does seem to occupy a central location among the five points. Finding the central location of a dataset helps in several data science tasks, including finding stealth outliers (outliers without extreme values for any one variable but far from most points in the multivariate dataset) and calculating multivariate statistics. Some points are closer to this central location than others, as our Mahalanobis results suggest; these are points 3 and 4 in our dataset, which are relatively close to the origin.</p>
<span epub:type="pagebreak" id="Page_105" title="105"/><figure>
<img alt="" class="" src="image_fi/503083c05/f05008.png"/>
<figcaption><p><a id="figure5-8">Figure 5-8</a>: Mahalanobis distance with centering at column means for the individual points from <a href="#listing5-2">Listing 5-2</a>’s matrix plotted with the column mean point shown off the axes</p></figcaption>
</figure>
<p>These distance differences come up in many machine learning applications, and we’ll see how distance impacts machine learning performance and results when we apply these distances within a <em>k</em>-nearest neighbors problem later in this chapter. Performance can vary dramatically with a different choice of metric, and using the wrong metric can mislead model results and interpretation.</p>
<h3 id="h2-503083c05-0003"><a class="XrefDestination" id="ComparingDiagrams,Shapes,andProbabilityDistributions"/><span class="XrefDestination" id="xref-503083c05-007"/>Comparing Diagrams, Shapes, and Probability Distributions</h3>
<p class="BodyFirst">Norm-based distance metrics are not the only class of metrics possible in machine learning. As we saw in <span class="xref" itemid="xref_target_Chapter 4"><a href="c04.xhtml">Chapter 4</a></span>, it’s possible to calculate distance between objects other than points, such as persistence diagrams. Roughly speaking, these types of metrics measure differences in probability distributions. We’ve already briefly used one of these, the Wasserstein distance, to compare persistence diagram distributions. Let’s take a closer look now.</p>
<h4 id="h3-503083c05-0001"><a class="XrefDestination" id="WassersteinDistance"/><span class="XrefDestination" id="xref-503083c05-008"/>Wasserstein Distance</h4>
<p class="BodyFirst">In loose terms, <em>Wasserstein distance</em> compares the piles of probability weights stacked in two distributions. It’s often dubbed “earth-mover distance,” as the Wasserstein distance measures the cost and effort needed to move probability piles of one distribution to turn it into the comparison distribution. For more mathematically sophisticated readers, the <em>p</em>th Wasserstein distance can be calculated by taking the expected value of the joint distribution marginals to the <em>p</em>th power, finding the infimum over all join probability distributions of those random variables, and taking the <em>p</em>th root of the result. However, the details of this are beyond what is expected <span epub:type="pagebreak" id="Page_106" title="106"/>of readers, and we’ll stick with the intuition of earth-mover distance as we explore this metric. Let’s visualize two distributions of dirt piles to build some intuition behind this metric (<a href="#figure5-9" id="figureanchor5-9">Figure 5-9</a>).</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05009.png"/>
<figcaption><p><a id="figure5-9">Figure 5-9</a>: Two distributions of dirt piles akin to the type of probability density functions that could be compared using Wasserstein distance metrics</p></figcaption>
</figure>
<p>Pile 1 of <a href="#figure5-9">Figure 5-9</a> has a large stack of dirt toward the left side that would need to be shoveled to the right piles if we were to transform Pile 1’s distribution of dirt to Pile 2’s. Our dirt mover will have to move quite a bit of dirt to transform Pile 1 into Pile 2. However, if our Pile 2 had a distribution of dirt piles closer to that of Pile 1, as in <a href="#figure5-10" id="figureanchor5-10">Figure 5-10</a>, the amount of work to turn Pile 1 into Pile 2 would be less for our dirt mover.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05010.png"/>
<figcaption><p><a id="figure5-10">Figure 5-10</a>: Two distributions of dirt piles akin to the type of probability density functions that could be compared using Wasserstein distance metrics, which measure the amount of work needed to be done by our dirt mover to transform Pile 1 into Pile 2</p></figcaption>
</figure>
<p>To get from Distribution 1 to Distribution 2, you can think of someone shoveling the dirt. The amount of dirt shoveled corresponds to the Wasserstein distance. Probability density functions that are very similar will have a smaller Wasserstein distance; those that are very dissimilar will have a larger Wasserstein distance.</p>
<p><span epub:type="pagebreak" id="Page_107" title="107"/>Thus, Wasserstein distance can be a nice metric to use in comparing probability distributions—comparing theoretical distributions to sample distributions to see if they match, comparing multiple sample distributions from the same or different populations to see if they match, or even understanding if it’s possible to use a simpler probability distribution in a machine learning function that approximates the underlying data such that calculations within a machine learning algorithm will be easier to compute.</p>
<p>Although we won’t go into it here, some distributions—including count data, yes/no data, and even machine learning output structures like dendrograms—can be compared through other metrics. For now, let’s look at another way to compare probability distributions, this time with discrete distributions.</p>
<h4 id="h3-503083c05-0002"><a class="XrefDestination" id="Entropy"/><span class="XrefDestination" id="xref-503083c05-009"/>Entropy</h4>
<p class="BodyFirst">One common class of distance metrics involves a property dubbed <em>entropy</em>. Information entropy, defined by the negative logarithm of the probability density function, measures the amount of nonrandomness at each point of the probability density function. By understanding how much information is contained in a distribution at each value, it’s possible to compare differences in information across distributions. This can be a handy tool for comparing complicated probability functions or output from machine learning algorithms, as well as deriving nonparametric statistical tests.</p>
<p>Binomial distributions come up often in data science. We might think that a random customer has no preference between two new interface designs (50 percent preferring A and 50 percent preferring B in an A/B test). We could estimate the chance that 10 or 20 or 10,000 customers prefer A over B and compare it to samples of actual customers providing us feedback. One assumption might be that we have different customer populations, including one that is very small. Of course, in practice, we don’t know the actual preference distributions of our customer populations and may not have enough data to compare the distributions mathematically via a proportions test. Leveraging a metric can help us derive a test.</p>
<p>To understand this a bit more intuitively, let’s simulate two binomial probability distributions with the code in <a href="#listing5-3" id="listinganchor5-3">Listing 5-3</a>.</p>
<pre><code>#create samples from two different binomial probability distributions
a&lt;-rbinom(1000,5,0.1)
b&lt;-rbinom(1000,5,0.4)

#create plot of probability density
plot(density(b),ylim=c(0,2),main="Comparison of Probability Distributions")
lines(density(a),col="blue")</code></pre>
<p class="CodeListingCaption"><a id="listing5-3">Listing 5-3</a>: A script that simulates different binomial probability distributions</p>
<p><span epub:type="pagebreak" id="Page_108" title="108"/><a href="#figure5-11" id="figureanchor5-11">Figure 5-11</a> shows that these binomial distributions have very different density functions, with information stored in different parts of the distribution.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05011.png"/>
<figcaption><p><a id="figure5-11">Figure 5-11</a>: Two binomial distributions’ density functions plotted for comparison</p></figcaption>
</figure>
<p>The black curve distribution, distribution <em>b</em><b>,</b> includes a wider spread of information over more values than distribution <em>a</em>, which concentrates its information nearer to zero (lighter gray curve). Entropy-based metrics can be used to quantify this difference in information storage between distributions<b> </b><em>a</em> and <em>b</em>. R provides many tools and packages for quantifying and comparing information entropy. Let’s explore a bit further.</p>
<p>The philentropy package in R contains 46 different metrics for comparing probability distributions, including many metrics based on entropy. One of the more popular entropy metrics is the <em>Kullback–Leibler divergence</em>, which measures the relative entropy of two probability distributions. Technically speaking, the Kullback–Leibler divergence measures the expectation (sum for discrete distributions or integral for continuous distributions) of the logarithmic differences between two probability distributions. As such, it’s a measurement of information gain or loss. This allows us to convert information entropy into a distribution comparison tool, which is useful when we’re trying to compare differences between unknown or complicated probability distributions that might not be amenable to the usual statistical tools.</p>
<p>Let’s develop our intuition by returning to our two binomial distributions, <em>a</em><b> </b>and <em>b</em>. We’ll calculate the Kullback–Leibler divergence between the two distributions by adding the following to <a href="#listing5-3">Listing 5-3</a>:</p>
<pre><code>#load package
library(philentropy)

#calculate Kullback-Leibler divergence
kullback_leibler_distance(P=a,Q=b,testNA=T,unit="log2",epsilon=1e-05)</code></pre>
<p><span epub:type="pagebreak" id="Page_109" title="109"/>This code calculates the Kullback–Leibler divergence for the two binomial distribution samples generated in <a href="#listing5-3">Listing 5-3</a>. In this set of simulated data, the Kullback–Leibler divergence is 398.5428; another simulation of these distributions might yield a different divergence measurement. However, using nonparametric tests, we can compare this divergence value with the random error component of one of our distributions to see whether there is a statistically significant difference of entropy between distributions <em>a</em><b> </b>and <em>b</em>. We can add to our script to create a nonparametric statistical test using entropy differences:</p>
<pre><code>#create a nonparametric test
#create a vector to hold results from the simulation loop
test&lt;-rep(NA,1000)

#loop to draw from one of the binomial distributions to generate
#a null distribution for one of our samples
for (i in 1:1000){
  new&lt;-rbinom(1000,5,0.1)
  test[i]&lt;-kullback_leibler_distance(P=a,Q=new,testNA=T,unit="log10",epsilon=1e-05)
}

#obtain the cut-off score for 95% confidence intervals, corresponding
#to values above/below which a sample would be considered statistically
#different than the null distribution
quantile(test,c(0.025,0.0975))</code></pre>
<p>The confidence intervals from this test suggest confidence intervals of 1,427–1,475, which suggests that our distributions are significantly different. This is expected, as distributions <em>a</em><b> </b>and <em>b</em><b> </b>have very different values and ranges. Plotting the last distribution simulated (<a href="#figure5-12" id="figureanchor5-12">Figure 5-12</a>) shows that the new distribution is a much better match to <em>a</em><b> </b>than <em>b</em> is.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05012.png"/>
<figcaption><p><a id="figure5-12">Figure 5-12</a>: Three binomial distributions’ density functions plotted for comparison, with two coming from samples of the same distribution</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_110" title="110"/>Using the Kullback–Leibler divergence, we’ve determined that <em>a</em><b> </b>and <em>b</em><b> </b>are different populations statistically. If we saw a confidence range including 104.2, we’d conclude that <em>a</em><b> </b>and <em>b</em><b> </b>likely come from the same population distribution. Statistical tests such as proportions tests exist to compare binomial distributions in practice, but some discrete distributions or sample sizes don’t have easy statistical comparisons (such as comparisons of predicted class distributions coming out of two convolutional neural network classifiers).</p>
<h4 id="h3-503083c05-0003"><a class="XrefDestination" id="ComparisonofShapes"/><span class="XrefDestination" id="xref-503083c05-010"/>Comparison of Shapes</h4>
<p class="BodyFirst">As we saw in <span class="xref" itemid="xref_target_Chapter 4"><a href="c04.xhtml">Chapter 4</a></span>, sets of points and shapes, such as persistence diagrams, can be important data structures, and these objects can be measured and compared as well. The next three measures will deal with this situation in more depth. Let’s start with an example of two circles with differing radii, as shown in <a href="#figure5-13" id="figureanchor5-13">Figure 5-13</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05013r.png"/>
<figcaption><p><a id="figure5-13">Figure 5-13</a>: Two circles of differing radii</p></figcaption>
</figure>
<p>Now, let’s imagine these two circles are paths in a park, and someone is walking their dog on a leash, with the dog following the outer path and the owner following the inner path. <a href="#figure5-14" id="figureanchor5-14">Figure 5-14</a> shows a visual.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05014r.png"/>
<figcaption><p><a id="figure5-14">Figure 5-14</a>: A dog and owner connected by a leash walking on different paths at a park</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_111" title="111"/>At some points, the owner and her dog are close together, and a small leash suffices to connect them. However, as they move counterclockwise, the distance between the owner and their dog increases, necessitating more leash to connect them; you can see this in <a href="#figure5-15" id="figureanchor5-15">Figure 5-15</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05015r.png"/>
<figcaption><p><a id="figure5-15">Figure 5-15</a>: A dog and owner connected by a longer leash walking on different paths at a park</p></figcaption>
</figure>
<p>One historically important metric that compares distances between points on different shapes is <em>Fréchet distance</em>. The version of Fréchet distance that we’ll consider here applies to discrete measurements (usually taken to be polygons). A grid graph is constructed from the polygons, and minmax paths are computed to find the maximum distance that two paths may be from each other. Many assumptions can be placed on the paths themselves and the synchronization of movement along those paths; the strictest requirements give rise to what’s called a <em>homotopic</em> Fréchet distance, which has applications in many robotics problems. We’ll return to homotopy applications in <span class="xref" itemid="xref_target_Chapter 8"><a href="c08.xhtml">Chapter 8</a></span>.</p>
<p>For now, in more lay terms, Fréchet distance is known as the dog-walking distance, and it has many uses in analytics. It can be used to measure not only the maximum distance between points on curves or shapes but the total distance between points on shapes or curves. Many R packages include functions to calculate one of the extant versions of Fréchet distance, including the TSdist time-series package, which is used in the following example. In this package, two time series are generated from ARMA(3, 2) distributions, with Series 3 containing 100 time points and Series 4 containing 120 time points. Time series are important in tracking disease progression in groups of patients, tracking stock market changes over time, and tracking buyer behavior over time.</p>
<p>Let’s load these package-generated time series and plot them to visualize potential differences using the code in <a href="#listing5-4" id="listinganchor5-4">Listing 5-4</a>.</p>
<pre><code>#load package and time series contained in the TSdist package
library(TSdist)
data(example.series4)
data(example.series3)
my1&lt;-example.series4
my2&lt;-example.series3

<span epub:type="pagebreak" id="Page_112" title="112"/>#plot both time series
plot(my1,main="Time Series Plots of Series 4")
plot(my2,main="Time Series Plots of Series 3")</code></pre>
<p class="CodeListingCaption"><a id="listing5-4">Listing 5-4</a>: A script that loads and examines two time-series datasets</p>
<p><a href="#figure5-16" id="figureanchor5-16">Figure 5-16</a> shows two time series with distinct highs and lows over time.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05016.png"/>
<figcaption><p><a id="figure5-16">Figure 5-16</a>: Plots of the two example time series</p></figcaption>
</figure>
<p>Notice the overlap between the time series isn’t perfect, and we’d expect our comparisons to show some differences between these time series. With Fréchet distance, it’s possible to measure both the maximum/minimum deviation (maximum/minimum leash length) between the time series and the sum of deviations across the full comparison set. We’ll examine both of these for Series 3 and Series 4 by adding the following code to our script:</p>
<pre><code>#calculate Frechet distance
dis1&lt;-FrechetDistance(my1,my2,FrechetSumOrMax="sum")
dis2&lt;-FrechetDistance(my1,my2,FrechetSumOrMax="min")
dis3&lt;-FrechetDistance(my1,my2,FrechetSumOrMax="max")</code></pre>
<p>This code calculates the minimum, maximum, and sum of Fréchet distances between the time series, which should yield a value of 402.0 for the sum of distances between the time-series curves, a value of 13.7 for the maximum distance between points on the time series curves, and a value of 0.03 for the minimum distance between points on the time series curves. This suggests that the time series have approximately the same values at some comparison points and values that are very different at other points. The sum of distances between the time-series curves will converge to the integral taken with continuous time across the series; this calculation can give a good tool for calculating areas between functions using discrete and fairly quick approximations.</p>
<p><span epub:type="pagebreak" id="Page_113" title="113"/>There are other ways to compare shapes besides Fréchet distance, though, and these are sometimes preferable. Let’s return to our two circles again and move them so that they are intersecting, such as in <a href="#figure5-17" id="figureanchor5-17">Figure 5-17</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05017r.png"/>
<figcaption><p><a id="figure5-17">Figure 5-17</a>: Plot of two intersecting circles</p></figcaption>
</figure>
<p>We can think of points on each circle, much like we did to measure Fréchet distance. Let’s consider a point on circle B and its closest point on circle A (shown in <a href="#figure5-17">Figure 5-17</a>). Each point on circle B will have a closest point on circle A, and these form a collection of closest points. The points chosen in <a href="#figure5-17">Figure 5-17</a> are a special set of points. They are the farthest apart of any points in our set of closest points. This means that the maximum we’d have to travel to hop from one circle to the other occurs with those two points. This distance is called the <em>Hausdorff distance</em>, and it is found in a lot of applications in early computer vision and image matching tasks. These days, it is mainly used for sequence matching, graph matching, and other discrete object matching tasks.</p>
<p>However, one of the limitations of Hausdorff distance is that the sets being compared must exist in the same metric spaces. Thus, while we can compare points on circles, we cannot directly compare points on a circle to those on a sphere with Hausdorff distance or points on a Euclidean plane to points on a positively curved sphere. The two objects being compared must be in the same metric space.</p>
<p>Fortunately, a solution to this conundrum exists. We can simply measure the farthest shortest distance from two metric spaces when those metric spaces are mapped to a single metric space while preserving the original distances between points within each space (called an <em>isometric embedding</em>). So, we could project the sphere and its points to tangent space to compare with other Euclidean spaces. Or we could embed two objects in a higher-dimensional space similar to what is done in kernel applications. This extension of Hausdorff distance is dubbed <em>Gromov–Hausdorff distance</em>.</p>
<p>Let’s build some intuition around this metric. Say we have a triangle and a tetrahedron, as in <a href="#figure5-18" id="figureanchor5-18">Figure 5-18</a>.</p>
<span epub:type="pagebreak" id="Page_114" title="114"/><figure>
<img alt="" class="" src="image_fi/503083c05/f05018r.png"/>
<figcaption><p><a id="figure5-18">Figure 5-18</a>: A triangle and tetrahedron, which exist in different-dimensional Euclidean spaces</p></figcaption>
</figure>
<p>One solution to this problem depicted in <a href="#figure5-18">Figure 5-18</a> is to simply bring the triangle into three-dimensional Euclidean space and calculate the distances between the objects in three-dimensional Euclidean space. Perhaps part of the triangle overlaps with the tetrahedron when it is embedded in three-dimensional space, as shown in <a href="#figure5-19" id="figureanchor5-19">Figure 5-19</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05019r.png"/>
<figcaption><p><a id="figure5-19">Figure 5-19</a>: A triangle and tetrahedron, both mapped into three-dimensional Euclidean space</p></figcaption>
</figure>
<p>It’s now possible for us to compute the farthest point sets of the closest ones for these two objects, likely occurring at one of the far tips of the triangle or tetrahedron in this example.</p>
<p>R has a nice package for computing Gromov–Hausdorff distances (gromovlab), so we can easily implement this distance metric in R. Let’s first simulate a small sample from a two-dimensional disc with the code in <a href="#listing5-5" id="listinganchor5-5">Listing 5-5</a>.</p>
<pre><code>#create two-dimensional disc sample
a&lt;-runif(100,min=-1,max=1)
b&lt;-runif(100,min=-1,max=1)

#create circle from uniform distribution and restrict to points within the
#circle
d&lt;-a^2+b^2
<span epub:type="pagebreak" id="Page_115" title="115"/>w&lt;-which(d&gt;1)
mydata&lt;-cbind(a,b)
mydata&lt;-mydata[-w,-w]

#plot sample
plot(mydata,main="2-Dimensional Disc Sample",ylab="y",xlab="x")</code></pre>
<p class="CodeListingCaption"><a id="listing5-5">Listing 5-5</a>: A script that samples from a two-dimensional disc to examine Gromov–Hausdorff distance</p>
<p>This should give a rough disc shape when plotted; take a look at <a href="#figure5-20" id="figureanchor5-20">Figure 5-20</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05020.png"/>
<figcaption><p><a id="figure5-20">Figure 5-20</a>: A sample taken from a two-dimensional disc</p></figcaption>
</figure>
<p>Now, let’s add to <a href="#listing5-5">Listing 5-5</a> to simulate a sample of the same size from the same uniform distribution that was used to generate our two-dimensional disc sample:</p>
<pre><code>#create a uniform sample from a line segment
x&lt;-sort(runif(dim(as.data.frame(mydata))[[1]],min=-1,max=1))</code></pre>
<p>The code we’ve added samples one of the line segments to give a one-dimensional space. This gives us two Euclidean spaces of differing dimension. Now, we can compute the distances between points in each sample’s native space (two-dimensional Euclidean space for the disc, one-dimensional Euclidean space for the line). From there, we can compute the Gromov–Hausdorff distance between our samples by adding the following code:</p>
<pre><code>#load the package and calculate the distance matrices for use in calculations
library(gromovlab)
m1&lt;-dist(as.matrix(mydata))
m2&lt;-dist(as.matrix(x))

<span epub:type="pagebreak" id="Page_116" title="116"/>#calculate distance metric and compare distances with Gromov-Hausdorff
gromovdist(m1,m2,"lp",p=2)</code></pre>
<p>This code allows us to compare distance matrices between points in the samples. For this random sample, the Gromov–Hausdorff distance is 5.8. We could simulate a nonparametric test based on our metric as we did in <a href="#listing5-3">Listing 5-3</a> to help us determine if the embeddings of our disc and our line are the same statistically. Changing the metric parameters may change the significant differences between embeddings or the quality of an embedding, as we saw earlier in this chapter when we compared Canberra, Manhattan, and Euclidean distances. Interested readers are encouraged to play around with the embedding parameters, set up their own nonparametric tests, and see how the results vary for Gromov–Hausdorff distances for our disc and line sample.</p>
<p>The <code>lp</code> parameter allows one to use the norm-based metrics examined earlier in this chapter. For this particular comparison, we’ve used the Euclidean norm, as both samples lie in Euclidean spaces and the distance matrices ingested are defined by the Euclidean norm. Other norms, such as the Manhattan or Chebyshev, are possible and perhaps preferable for other problems, and the package is equipped to handle graphs and trees, as well as distance matrices. One thing to note about this particular package is that the algorithm searches through all possible isometric embeddings, so the compute time and memory needed may be large for some problems.</p>
<h2 id="h1-503083c05-0003"><a class="XrefDestination" id="K-NearestNeighborswithMetricGeometry"/><span class="XrefDestination" id="xref-503083c05-011"/><em>K</em>-Nearest Neighbors with Metric Geometry</h2>
<p class="BodyFirst">Metric geometry shows up in many algorithms, including <em>k</em>-nearest neighbor (<em>k</em>-NN) analyses, which classify observations based on the classifications of objects near them. One way to understand this method is to consider a high school cafeteria with different cliques of students, as shown in <a href="#figure5-21" id="figureanchor5-21">Figure 5-21</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05021.png"/>
<figcaption><p><a id="figure5-21">Figure 5-21</a>: A high school cafeteria with three distinct student cliques</p></figcaption>
</figure>
<p>In the cafeteria shown in <a href="#figure5-21">Figure 5-21</a>, three student cliques exist: a dark gray clique, a gray clique, and a light gray clique. Students tend to stay near their group of friends, as exhibited by students A, B, and C. These students <span epub:type="pagebreak" id="Page_117" title="117"/>are surrounded by their friends, and classifying them using an arbitrary number of students standing nearest them according to a distance metric (like Euclidean distance or number of floor tiles between students) would give a pretty accurate classification into student cliques.</p>
<p>However, there are a few students, such as students D and E, who are located near other cliques or among all three cliques. Student E might be part of the popular clique (bottom center) and also part of the varsity athlete clique (top left), and student D might be popular, a varsity athlete, and a math team member (top right). Depending on how many students located near students D and E are considered in classifying them into a clique, they may belong to their main clique or be incorrectly reassigned to a new clique, of which they may fit but not consider their main clique. For instance, the closest 10 students may assign student E correctly to the popular group, while the closest 2 students would not.</p>
<p>Thus, <em>k</em>-NN methods rely on both a neighborhood size (in this instance, the number of students nearest the student of interest) and a distance metric defining which students are closest to the student of interest. Let’s look at little more closely at how distance metric can impact <em>k</em>-NN classification accuracy with five nearest neighbors in a simulated dataset with three variables impacting classification and three noise variables, given in <a href="#listing5-6" id="listinganchor5-6">Listing 5-6</a>, which uses the knnGarden package (and includes many of the distances covered in the simulated data analyzed with norm-based distance metrics earlier in this chapter). You’ll first need to download the package (<a class="LinkURL" href="https://cran.r-project.org/web/packages/knnGarden/index.html">https://cran.r-project.org/web/packages/knnGarden/index.html</a>) and install it locally.</p>
<pre><code>#install package (and devtools if not installed)
#your local computer might save the .tar file in a different path than ours
library(devtools)
install_local("~/Downloads/knnGarden.tar")

#create data
a&lt;-rbinom(500,4,0.2)
b&lt;-rbinom(500,1,0.5)
c&lt;-rbinom(500,2,0.1)
d&lt;-rbinom(500,2,0.2)
e&lt;-rbinom(500,1,0.3)
f&lt;-rbinom(500,1,0.8)
class&lt;-a+e-d-rbinom(500,2,0.3)
class[class&gt;=0]&lt;-1
class[class&lt;0]&lt;-0
mydata&lt;-as.data.frame(cbind(a,b,c,d,e,f,class))

#partition data into training and test sets (60% train, 40% test)
s&lt;-sample(1:500,300)
train&lt;-mydata[s,]
test&lt;-mydata[-s,]

#create KNN models with different distances and five nearest neighbors
library(knnGarden)

<span epub:type="pagebreak" id="Page_118" title="118"/>#Euclidean
ke&lt;-knnVCN(TrnX=train[,-7],OrigTrnG=train[,7],TstX=test[,-7],
K=5,method="euclidean")
accke&lt;-length(which(ke==test[,7]))/length(test[,7])

#Canberra
kc&lt;-knnVCN(TrnX=train[,-7],OrigTrnG=train[,7],TstX=test[,-7],
K=5,method="canberra")
acckc&lt;-length(which(kc==test[,7]))/length(test[,7])

#Manhattan
km&lt;-knnVCN(TrnX=train[,-7],OrigTrnG=train[,7],TstX=test[,-7],
K=5,method="manhattan")
acckm&lt;-length(which(km==test[,7]))/length(test[,7])</code></pre>
<p class="CodeListingCaption"><a id="listing5-6">Listing 5-6</a>: A script that generates and classifies a sample through <em>k</em>-NN classification with varying distance metric and five neighbors</p>
<p><a href="#listing5-6">Listing 5-6</a> creates a sample and then runs the <em>k</em>-NN algorithm to classify points based on different distance metrics, including Manhattan, Euclidean, and Canberra distances. In this particular simulation, all of our distances yield similar accuracies (Euclidean distance of 81 percent, Manhattan distance of 81 percent, and Canberra distance of 82 percent). We can consider a larger neighborhood by modifying <a href="#listing5-6">Listing 5-6</a> to include 20 nearest neighbors.</p>
<pre><code>#create KNN models with different distance metrics and 20 nearest neighbors

#Euclidean
ke&lt;-knnVCN(TrnX=train[,-7],OrigTrnG=train[,7],TstX=test[,-7],
<b>K=20</b>,method="euclidean")
accke&lt;-length(which(ke==test[,7]))/length(test[,7])

#Canberra
kc&lt;-knnVCN(TrnX=train[,-7],OrigTrnG=train[,7],TstX=test[,-7],
<b>K=20</b>,method="canberra")
acckc&lt;-length(which(kc==test[,7]))/length(test[,7])

#Manhattan
km&lt;-knnVCN(TrnX=train[,-7],OrigTrnG=train[,7],TstX=test[,-7],
<b>K=20</b>,method="manhattan")
acckm&lt;-length(which(km==test[,7]))/length(test[,7])</code></pre>
<p>This script modifies the functions that calculate the <em>k</em>-NN model, with the changes marked in bold; we changed the parameter to <code>K=20</code>. With this particular simulated dataset, there are dramatic differences in classification accuracy when 20 nearest neighbors are considered. Euclidean and Manhattan distances give a slightly worse accuracy of 78.5 percent, and Canberra distance gives a much worse accuracy of 57 percent. Neighborhood size matters quite a bit in accuracy for Canberra distance, but it plays a lesser role for Euclidean and Manhattan distances. Generally speaking, using larger numbers of nearest neighbors smooths the data, similarly to how a weighted average might. These results suggest that, for our Canberra distance, adding <span epub:type="pagebreak" id="Page_119" title="119"/>a larger number of nearest neighbors might be smoothing the data too much. However, our Manhattan and Euclidean distance runs don’t show this smoothing effect and retain their original high performance. As our example shows, the choice of distance metric can matter a lot in algorithm performance—or it can matter little. Distance metrics thus function like other parameter choices in algorithm design.</p>
<p><em>k</em>-NN models are among the most closely tied to metric geometry and neighborhoods, though many other methods rely on distance metrics or neighborhood size. There are many recent papers that suggest a multiscale approach to algorithm neighborhood definition can improve algorithm performance, including applications in <em>k</em>-NN regression, deep learning image classification, and persistent graph and simplex algorithms (including persistent homology), and this nascent field has grown in recent years.</p>
<p>One branch of machine learning where the choice of distance metric matters a lot is in dimensionality reduction, where we’re mapping a high-dimensional dataset to a lower-dimensional space. For instance, imagine we have a genomic dataset for a group of patients including 300 gene loci of interest. That’s a bit too much to visualize for a stakeholder on a PowerPoint slide. However, if we find a good mapping to two-dimensional space, we can add a scatterplot of our data to the slide deck in a way that is much easier for humans to process.</p>
<h2 id="h1-503083c05-0004"><a class="XrefDestination" id="ManifoldLearning"/><span class="XrefDestination" id="xref-503083c05-012"/>Manifold Learning</h2>
<p class="BodyFirst">Many dimensionality reduction algorithms also involve distance metrics and <em>k</em>-NN calculations. One of the most common dimensionality reduction algorithms, <em>principal component analysis (PCA)</em>, helps wrangle high-dimensional data into lower-dimensional spaces using a linear mapping between the original high-dimensional space and a lower-dimensional target space. Essentially, PCA finds the ideal set of linear bases to account for the most variance (packing in most of the relevant information related to our data) with the fewest linear bases possible; this allows us to drop many of the data space’s bases that don’t contain much relevant information. This helps us visualize data that lives in more than three dimensions; it also decorrelates predictors being fed into a model.</p>
<p>However, as noted, PCA assumes that data lives in a geometrically flat space and is mapped to a lower-dimensional flat space. As we’ve seen, this isn’t always the case, and Euclidean metrics can give different distance results than other distance metrics. Recently, many attempts to relax different assumptions and generalize dimensionality reduction to manifolds have provided a new class of dimensionality reduction techniques, called <em>manifold learning</em>. Manifold learning allows for mappings to lower-dimensional spaces that might be curved and generalizations of PCA to include metrics other than Euclidean distance. A <em>manifold</em> is a space that is locally Euclidean, with Euclidean space being one example of a manifold, so some people refer to <em>manifold learning </em>as an umbrella to this more general framework.</p>
<h3 id="h2-503083c05-0004"><span epub:type="pagebreak" id="Page_120" title="120"/><a class="XrefDestination" id="UsingMultidimensionalScaling"/><span class="XrefDestination" id="xref-503083c05-013"/>Using Multidimensional Scaling</h3>
<p class="BodyFirst">One of the older manifold learning algorithms is <em>multidimensional scaling</em> <em>(MDS)</em>. MDS considers embeddings of points into a Euclidean space such that distances between points, which can be Euclidean distances, are preserved as best as possible; this is done through the minimization of a user-defined cost function. Defining distances and cost functions via Euclidean distance yields the same results as PCA. However, there is no need to limit oneself to Euclidean distance with MDS, and many other metrics might be more suitable for a given problem. Let’s explore this further with a small dataset and different distance matrices as input to our MDS algorithm; take a look at the code in <a href="#listing5-7" id="listinganchor5-7">Listing 5-7</a>.</p>
<pre><code>#create data
a&lt;-rbinom(100,4,0.2)
b&lt;-rbinom(100,1,0.5)
c&lt;-rbinom(100,2,0.1)
d&lt;-rbinom(100,2,0.2)
e&lt;-rbinom(100,1,0.3)
f&lt;-rbinom(100,1,0.8)
mydata&lt;-as.data.frame(cbind(a,b,c,d,e,f))

#create distance matrices using different distance metrics
m1&lt;-dist(mydata,upper=T,diag=T)
m2&lt;-dist(mydata,"minkowski",p=10,upper=T,diag=T) 
m3&lt;-dist(mydata,"manhattan",upper=T,diag=T)</code></pre>
<p class="CodeListingCaption"><a id="listing5-7">Listing 5-7</a>: A script that generates an example dataset and calculates distance matrices from the dataset</p>
<p>Now that we have generated some data and have calculated three different distance metrics, let’s see how the choice of distance metric impacts MDS embeddings. Let’s compute the MDS embeddings and plot the results by adding to <a href="#listing5-7">Listing 5-7</a>.</p>
<pre><code>#reduce dimensionality with MDS to two dimensions
c1&lt;-cmdscale(m1,k=2)
c2&lt;-cmdscale(m2,k=2)
c3&lt;-cmdscale(m3,k=2)

#plot results
plot(c1,xlab="Coordinate 1",ylab="Coordinate 2",
main="Euclidean Distance MDS Results")
plot(c2,xlab="Coordinate 1",ylab="Coordinate 2",
main="Minkowski p=10 Distance MDS Results")
plot(c3,xlab="Coordinate 1",ylab="Coordinate 2",
main="Manhattan Distance MDS Results")</code></pre>
<p>Our addition to <a href="#listing5-7">Listing 5-7</a> should give plots that look different from each other. In this example, the plots (shown in <a href="#figure5-22" id="figureanchor5-22">Figure 5-22</a>) do vary dramatically depending on the metric used, suggesting that different distance metrics result in embeddings to different spaces.</p>
<span epub:type="pagebreak" id="Page_121" title="121"/><figure>
<img alt="" class="" src="image_fi/503083c05/f05022.png"/>
<figcaption><p><a id="figure5-22">Figure 5-22</a>: A side-by-side view of MDS results, which vary by distance metric chosen</p></figcaption>
</figure>
<p>The plot results in <a href="#figure5-22">Figure 5-22</a> suggest that Minkowski distance yields quite different results than Euclidean or Manhattan distances; many points are bunched together in the Minkowski-type MDS result, which suggests it may not distinguish between pairs of points as well as the other metrics. However, the differences between Euclidean and Manhattan distance MDS results are less dramatic, with points spread out a lot more than in the case of our Minkowski distance.</p>
<h3 id="h2-503083c05-0005"><a class="XrefDestination" id="ExtendingMultidimensionalScalingwithIsomap"/><span class="XrefDestination" id="xref-503083c05-014"/>Extending Multidimensional Scaling with Isomap</h3>
<p class="BodyFirst">Some manifold learning algorithms extend MDS to other types of spaces and distance calculations. <em>Isomap</em> extends MDS by replacing the distance matrix with one of geodesic distances between points calculated from a neighborhood graph. This replacement of distance calculations with geodesic distances allows for the use of distances that naturally exist on spaces that are not flat, such as spheres (for instance, geographic information system data) or organs in a human body examined through MRIs. Most commonly, distances are estimated by examining a point’s nearest neighbors. This gives Isomap a neighborhood flavor and a way to investigate the role of scaling through the variance of the nearest neighbor parameter.</p>
<p>Let’s explore this modification by adding to <a href="#listing5-7">Listing 5-7</a>, which simulated a dataset and explored MDS. We’ll use Euclidean distance as a dissimilarity measure, though other distance metrics can be used much as they were with MDS. To understand the role of neighborhood size, we’ll create neighborhoods of 5, 10, and 20 nearest neighbors:</p>
<pre><code>#create Isomap projections of the data generated in <a href="#listing5-6">Listing 5-6</a>
library(vegan)

i1&lt;-scores(isomap(dist(mydata),ndim=2,k=5))
i2&lt;-scores(isomap(dist(mydata),ndim=2,k=10))
i3&lt;-scores(isomap(dist(mydata),ndim=2,k=20))

#plot results
plot(i1,xlab="Coordinate 1",ylab="Coordinate 2",main="K=5 Isomap Results")
<span epub:type="pagebreak" id="Page_122" title="122"/>plot(i2,xlab="Coordinate 1",ylab="Coordinate 2",main="K=10 Isomap Results")
plot(i3,xlab="Coordinate 1",ylab="Coordinate 2",main="K=20 Isomap Results")</code></pre>
<p>This snippet of code applies Isomap to the generated dataset in <a href="#listing5-7">Listing 5-7</a> using Euclidean distance. Other distance metrics can be used and may give different results, as shown in the MDS analyses. The results of the Isomap analyses suggest that neighborhood size doesn’t play a large role in determining results for this dataset, as shown by the scales for each coordinate in the <a href="#figure5-23" id="figureanchor5-23">Figure 5-23</a> plots.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05023.png"/>
<figcaption><p><a id="figure5-23">Figure 5-23</a>: A side-by-side view of Isomap results, which vary by number of nearest neighbors</p></figcaption>
</figure>
<p>MDS and Isomap aim to preserve distance between points regardless of how far apart the points lie on the data manifold, resulting in global preservation of distance. Other global manifold learning algorithms, which preserve distances between points that are not in the same neighborhood, exist. If you’re interested, you can explore global algorithms such as kernel PCA, autoencoders, and diffusion mapping.</p>
<h3 id="h2-503083c05-0006"><a class="XrefDestination" id="CapturingLocalPropertieswithLocallyLinearEmbedding"/><span class="XrefDestination" id="xref-503083c05-015"/>Capturing Local Properties with Locally Linear Embedding</h3>
<p class="BodyFirst">Sometimes global properties of the manifold aren’t as important as local properties. In fact, from the classical definition of a manifold, local properties might sometimes be more interesting. For instance, when we’re looking for nearest neighbors to a point, points that are very far way geometrically probably won’t be nearest neighbors of that point, but points that are nearby could be nearest neighbors with information that needs to be preserved in a mapping between higher-dimensional and lower-dimensional spaces. Local manifold learning algorithms aim to preserve the local properties with less focus on preserving global properties in the mapping between spaces.</p>
<p><em>Locally linear embedding</em> <em>(LLE)</em> is one such local manifold learning algorithm, and it is one of the more often used manifold learning algorithms. Roughly speaking, LLE starts with a nearest neighbor graph and then proceeds to create sets of weights for each point given its nearest neighbors. From there, the algorithm calculates the mapping according to a cost function and the preservation of the nearest neighbor weight sets for each point. <span epub:type="pagebreak" id="Page_123" title="123"/>This allows it to preserve important geometric information in the data that exists between points near each other on the manifold.</p>
<p>Returning to our code in <a href="#listing5-7">Listing 5-7</a>, let’s add to our code and explore LLE mapping to a two-dimensional space with varying numbers of neighbors. For this package, you’ll need to download the package (<a class="LinkURL" href="https://mran.microsoft.com/snapshot/2016-08-05/web/packages/TDAmapper/README.html">https://mran.microsoft.com/snapshot/2016-08-05/web/packages/TDAmapper/README.html</a>) and locally install it:</p>
<pre><code>#install package
library(devtools)
install_local("~/Downloads/lle.tar")

#create LLE projections of the data generated in <a href="#listing5-6">Listing 5-6</a>
library(lle)

l1&lt;-lle(mydata,m=2,k=5)
l2&lt;-lle(mydata,m=2,k=10)
l3&lt;-lle(mydata,m=2,k=20)

#plot results
plot(l1$Y,xlab="Coordinate 1",ylab="Coordinate 2",main="K=5 LLE Results")
plot(l2$Y,xlab="Coordinate 1",ylab="Coordinate 2",main="K=10 LLE Results")
plot(l3$Y,xlab="Coordinate 1",ylab="Coordinate 2",main="K=20 LLE Results")</code></pre>
<p>This piece of code applies the LLE algorithm to our dataset, varying the number of nearest neighbors considered in the algorithm calculations. Let’s examine the plots from this dataset to understand the role of nearest neighbors in this local algorithm (<a href="#figure5-24" id="figureanchor5-24">Figure 5-24</a>).</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05024.png"/>
<figcaption><p><a id="figure5-24">Figure 5-24</a>: A side-by-side view of LLE results, which vary by number of nearest neighbors</p></figcaption>
</figure>
<p>As shown in <a href="#figure5-24">Figure 5-24</a>, neighborhood size greatly impacts LLE results and the spread of points in the new two-dimensional space. Given that the number of nearest neighbors impacts the size of the neighborhood preserved, higher values result in less-local versions of LLE, converting the algorithm into more of a global-type manifold learning algorithm. Good separation seems to occur at <code>K=20</code>, which is less local than <code>K=5</code> but still a fairly small neighborhood for a dataset with 100 points. A fully global algorithm exists if we set K to 100, giving a two-dimensional plot with good separation and spread of points across the new space; you can see this in <a href="#figure5-25" id="figureanchor5-25">Figure 5-25</a>.</p>
<span epub:type="pagebreak" id="Page_124" title="124"/><figure>
<img alt="" class="" src="image_fi/503083c05/f05025.png"/>
<figcaption><p><a id="figure5-25">Figure 5-25</a>: A plot of LLE results using the entire sample as nearest neighbors</p></figcaption>
</figure>
<p>Other local manifold learning algorithms exist, and some of these allow for a scaling parameters like LLE’s neighborhood size. If you’re interested, you can explore Laplacian eigenmaps, Hessian LLE, and local tangent space analysis.</p>
<h3 id="h2-503083c05-0007"><a class="XrefDestination" id="Visualizingwitht-DistributedStochasticNeighborEmbedding"/><span class="XrefDestination" id="xref-503083c05-016"/>Visualizing with t-Distributed Stochastic Neighbor Embedding</h3>
<p class="BodyFirst">We’ve now seen how local algorithms can capture global properties through neighborhood size definition. Some manifold learning algorithms exist that explicitly capture both local and global properties. One of the more well-known algorithms is a visualization tool called <em>t-distributed stochastic neighbor embedding</em> <em>(t-SNE)</em>. The algorithm has two main stages: creating probability distributions over points in the high-dimensional space and then matching these distributions to ones in a lower-dimensional space by minimizing the Kullback–Leibler divergence over the two sets of distributions. Thus, rather than starting with a distance calculation between points, this algorithm focuses on matching distribution distances to find the optimal space.</p>
<p>Instead of defining a neighborhood by <em>k</em>-nearest neighbors to a point, t-SNE defines a neighborhood by the kernel’s bandwidth over the data; this yields a parameter called <em>perplexity</em>, which can also be varied to understand the role of neighborhood size. Let’s return to the data generated in <a href="#listing5-7">Listing 5-7</a> and see how this works in practice. Add the following code:</p>
<pre><code>#create t-SNE projections of the data generated in <a href="#listing5-7">Listing 5-7</a>
library(Rtsne)
library(dimRed)

t1&lt;-getDimRedData(embed(mydata,"tSNE",ndim=2,perplexity=5))
t2&lt;-getDimRedData(embed(mydata,"tSNE",ndim=2,perplexity=15))
t3&lt;-getDimRedData(embed(mydata,"tSNE",ndim=2,perplexity=25))

#plot results
plot(as.data.frame(t1),xlab="Coordinate 1",ylab="Coordinate2",
<span epub:type="pagebreak" id="Page_125" title="125"/>main="Perplexity=5 t-SNE Results")
plot(as.data.frame(t2),xlab="Coordinate 1",ylab="Coordinate2",
main="Perplexity=15 t-SNE Results")
plot(as.data.frame(t3),xlab="Coordinate 1",ylab="Coordinate2",
main="Perplexity=25 t-SNE Results")</code></pre>
<p>This piece of code runs t-SNE on the dataset generated with <a href="#listing5-7">Listing 5-7</a>, varying the perplexity parameter. The plots should produce something like <a href="#figure5-26" id="figureanchor5-26">Figure 5-26</a>, which shows more clumping in the lower-perplexity trial than in the trials with higher perplexity.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05026.png"/>
<figcaption><p><a id="figure5-26">Figure 5-26</a>: A side-by-side plot of t-SNE results with differing perplexity settings</p></figcaption>
</figure>
<p>The plots for perplexity of 15 and 25 look fairly similar, and as we increase perplexity, the range of the coordinates in the lower-dimensional space drops. There may be projects where more spread in the data is useful for subsequent analyses or visualizing possible trends; other projects may yield better results with tighter data.</p>
<p>In summary, the distance metrics in this chapter pop up regularly in machine learning applications. Manifold learning, in particular, can involve different choices of metric, neighborhood size, and type of space onto which the data space is mapped. Many good textbooks and papers exist that cover these algorithms and others like them in more detail. However, we hope that you’ve gained an overview of dimensionality reduction methods—particularly those that are intimately connected to metric geometry.</p>
<p>Before moving on, let’s consider one final use of metric geometry.</p>
<h2 id="h1-503083c05-0005"><a class="XrefDestination" id="Fractals"/><span class="XrefDestination" id="xref-503083c05-017"/>Fractals</h2>
<p class="BodyFirst">Another tool connected to metric geometry involves a type of self-similarity in geometry objects called <em>fractals</em>. Essentially, fractals have a pattern within the same pattern within the same pattern within the same pattern, and so on. <a href="#figure5-27" id="figureanchor5-27">Figure 5-27</a> has an example.</p>
<span epub:type="pagebreak" id="Page_126" title="126"/><figure>
<img alt="" class="" src="image_fi/503083c05/f05027.png"/>
<figcaption><p><a id="figure5-27">Figure 5-27</a>: An example of a fractal. Note the self-similarity of the patterns at different scales.</p></figcaption>
</figure>
<p>Fractals occur often in natural and man-made systems. For instance, coastlines, blood vessels, music scales, epidemic spread in confined spaces, stock market behavior, and word frequency and ranking all have self-similarity properties at different scales. Being able to measure fractal dimension allows us to better understand the degree of self-similarity of these phenomena. There are many fractal dimension estimators these days, but most rely on measuring variations in the area under a fractal curve through some sort of iterative approach that compares neighboring point sets’ areas.</p>
<p>Going back to the fractal in <a href="#figure5-27">Figure 5-27</a>, we could consider adding boxes to find the area under each iterative curve and then comparing the relative values across scales considered, as in <a href="#figure5-28" id="figureanchor5-28">Figure 5-28</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05028.png"/>
<figcaption><p><a id="figure5-28">Figure 5-28</a>: An example of measuring area under a series of fractal curves at scale</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_127" title="127"/>Now that we have some intuition around fractals, let’s consider an application of fractal dimension metrics. Stock markets are known to exhibit some degree of self-similar behavior over periods of time. Understanding market volatility is a major aspect of investing wisely, and one method used to predict coming market reversal points, such as crashes, is changing self-similarity. The closing prices of the Dow Jones Industrial Average (one of the American stock market indices), or DJIA, are widely available for free download. Here, we’ll consider simulated daily closing prices like DJIA data from the period of June 2019 to May 2020, during which the COVID freefall happened. <a href="#figure5-29" id="figureanchor5-29">Figure 5-29</a> shows a chart of closing prices over that time period.</p>
<figure>
<img alt="" class="" src="image_fi/503083c05/f05029new.png"/>
<figcaption><p><a id="figure5-29">Figure 5-29</a>: A plot of simulated DJIA closing prices from June 2019 to May 2020. Note the big drops starting in late February 2020, when COVID became a global issue.</p></figcaption>
</figure>
<p>If we were predicting future market behavior, we’d want to employ fractal analyses with tools from time-series data analysis, which are outside the scope of this book. However, we can get a feel for changes in self-similarity month by month easily by parsing the data into monthly series and calculating each monthly series’ fractal dimension. From there, we can examine how fractal dimension correlates with other measures of volatility, such as the range of closing prices within a month; we should see a positive correlation. <a href="#listing5-8" id="listinganchor5-8">Listing 5-8</a> loads the data, parses it, calculates fractal dimension, calculates closing price range, and runs a correlation test between fractal dimension and range.</p>
<pre><code>#load and parse stock market data
stocks&lt;-read.csv("Example_Stock_Data.csv")
June2019&lt;-stocks[stocks$Month=="June",]
July2019&lt;-stocks[stocks$Month=="July",]
August2019&lt;-stocks[stocks$Month=="August",]
September2019&lt;-stocks[stocks$Month=="September",]
October2019&lt;-stocks[stocks$Month=="October",]
November2019&lt;-stocks[stocks$Month=="November",]
December2019&lt;-stocks[stocks$Month=="December",]
January2020&lt;-stocks[stocks$Month=="January",]
February2020&lt;-stocks[stocks$Month=="February",]
<span epub:type="pagebreak" id="Page_128" title="128"/>March2020&lt;-stocks[stocks$Month=="March",]
April2020&lt;-stocks[stocks$Month=="April",]
May2020&lt;-stocks[stocks$Month=="May",]

#calculate fractal dimension for each series
library(fractaldim)
junedim&lt;-fd.estimate(June2019[,2],methods="hallwood")$fd
julydim&lt;-fd.estimate(July2019[,2],methods="hallwood")$fd
augustdim&lt;-fd.estimate(August2019[,2],methods="hallwood")$fd
septemberdim&lt;-fd.estimate(September2019[,2],methods="hallwood")$fd
octoberdim&lt;-fd.estimate(October2019[,2],methods="hallwood")$fd
novemberdim&lt;-fd.estimate(November2019[,2],methods="hallwood")$fd
decemberdim&lt;-fd.estimate(December2019[,2],methods="hallwood")$fd
januarydim&lt;-fd.estimate(January2020[,2],methods="hallwood")$fd
februarydim&lt;-fd.estimate(February2020[,2],methods="hallwood")$fd
marchdim&lt;-fd.estimate(March2020[,2],methods="hallwood")$fd
aprildim&lt;-fd.estimate(April2020[,2],methods="hallwood")$fd
maydim&lt;-fd.estimate(May2020[,2],methods="hallwood")$fd

#combine fractal dimension results into a vector
monthlyfd&lt;-c(junedim,julydim,augustdim,septemberdim,octoberdim,novemberdim,
decemberdim,januarydim,februarydim,marchdim,aprildim,maydim)

#examine monthly stock price range
monthlymax&lt;-c(max(June2019[,2]),max(July2019[,2]),max(August2019[,2]),
max(September2019[,2]),max(October2019[,2]),max(November2019[,2]),
max(December2019[,2]),max(January2020[,2]),max(February2020[,2]),
max(March2020[,2]),max(April2020[,2]),max(May2020[,2]))

monthlymin&lt;-c(min(June2019[,2]),min(July2019[,2]),min(August2019[,2]),
min(September2019[,2]),min(October2019[,2]),min(November2019[,2]),
min(December2019[,2]),min(January2020[,2]),min(February2020[,2]),
min(March2020[,2]),min(April2020[,2]),min(May2020[,2]))

monthlyrange&lt;-monthlymax-monthlymin

#examine relationship between monthly fractal dimension and monthly range
cor.test(monthlyfd,monthlyrange,"greater")</code></pre>
<p class="CodeListingCaption"><a id="listing5-8">Listing 5-8</a>: A script that loads the simulated DJIA closing data, calculates fractal dimension and range of closing prices, and runs a correlation test to determine the relationship between fractal dimension and closing price range</p>
<p>You should find a correlation of around 0.55, or a moderate relationship between closing price fractal dimension and closing price range, that is around the 0.05 significance level on the correlation test. Self-similarity does seem positively tied to one measure of market volatility. The fractal dimension varies by month, with some months’ dimensionality being close to 1 and others’ dimensionality being quite a bit higher. Impressively, the fractal dimension shoots up to 2 for March 2020.</p>
<p><span epub:type="pagebreak" id="Page_129" title="129"/>Given that we only have 12 months’ worth of data going into our test, it’s worth noting that we still find evidence for a positive relationship between fractal dimension and range of closing prices. Interested readers with their own stock market data are encouraged to optimize the time frame windows and potential window overlap chosen to calculate the series of fractal dimensions on their own data, as well as investigate the correlations with other geometric tools used in stock market change point detection, such as Forman–Ricci curvature and persistent homology.</p>
<h2 id="h1-503083c05-0006"><a class="XrefDestination" id="Summary"/><span class="XrefDestination" id="xref-503083c05-018"/>Summary</h2>
<p class="BodyFirst">In this chapter, we’ve investigated metric geometry and its application in several important machine learning algorithms, including the <em>k</em>-NN algorithm and several manifold learning algorithms. We’ve witnessed how the choice of distance metric (and other algorithm parameters) can dramatically impact performance. We’ve also examined fractals and their relationship to stock market volatility. Measuring distances between points and distributions crops up in many areas of machine learning and impacts quality of machine learning results. <span class="xref" itemid="xref_target_Chapter 5"><a href="c05.xhtml">Chapter 5</a></span> barely scratches the surface of extant tools from metric geometry. You may want to consult the papers referenced in the R packages used in this chapter, as well as current machine learning publications in distance metrics.</p>
</section>
</body>
</html>