<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="133" id="Page_133"/>6</span><br/>
<span class="ChapterTitle">Information Theory</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">In this chapter we look at the basics of <em>information theory</em>. This is a relatively new field of study, introduced to the world in 1948 in a groundbreaking paper, which laid the foundation for technologies from modern computers and satellites to cell phones and the internet (Shannon 1948). The goal of the original theory was to find the most efficient way to communicate a message electronically. But the ideas of that theory are deep, broad, and profound. They give us tools for measuring how much we know about anything by converting it to a digital form that we can study and manipulate.</p>
<p>Terms and ideas from information theory form part of the bedrock of deep learning. For example, the measurements provided by information theory are useful when we evaluate the performance of deep networks. In this chapter, we take a fast tour through some of the basics of information theory, while staying free of abstract mathematical notation. </p>
<p><span epub:type="pagebreak" title="134" id="Page_134"/>Let’s begin with the word <em>information</em>, one of those words that has both an everyday meaning and a specialized, scientific meaning. In this case, the meanings share a lot conceptual overlap, but while the popular meaning is broad and open to personal interpretation, the scientific meaning is precise and defined mathematically. Let’s start out by building up to the scientific definition of information, and ultimately work our way up to an important measurement that lets us compare two probability distributions. </p>
<h2 id="h1-500723c06-0001">Surprise and Context</h2>
<p class="BodyFirst">When we receive a communication of any kind, something moved from one place to another, whether it was an electrical pulse, some photons of light, or the sound of someone’s voice. Speaking broadly, we could say that a <em>sender</em> somehow transfers some kind of communication to a <em>receiver</em>. Let’s introduce some more specialized vocabulary.</p>
<h3 id="h2-500723c06-0001">Understanding Surprise</h3>
<p class="BodyFirst">In this chapter, we sometimes use the term <em>surprise</em> to represent how unexpected a sender’s communication is to a receiver. Surprise isn’t a formal term. In fact, one of our goals in this chapter is to find more formal names for surprise and attach specific meanings and measures to them.</p>
<p>Let’s suppose that we’re on the receiving end of a message. We want to describe how surprised we are by the communication we receive. Being able to do so is useful because, as we’ll see, the greater the surprise, the greater the amount of information that was delivered. </p>
<p>Suppose we get an unexpected text message from an unknown number. We open it up and the first word is <span class="CustomCharStyle">Thanks</span>. How surprised are we? Surely we are at least a little surprised, because so far, we don’t know who the message is from or what it’s about. But receiving a text thanking us for something does happen, so it’s not unheard of.</p>
<p>Let’s make up an imaginary and completely subjective surprise scale, where 0 means something is completely expected, and 100 means it’s a total surprise, as in <a href="#figure6-1" id="figureanchor6-1">Figure 6-1</a>.</p>
<figure>
<img src="Images/f06001.png" alt="f06001" width="450" height="115"/>
<figcaption><p><a id="figure6-1">Figure 6-1</a>: The surprise scale, expressed as a value from 0 to 100</p></figcaption>
</figure>
<p>On this scale, the word <span class="CustomCharStyle">Thanks</span> at the start of an unexpected text message might rank a 20. Now suppose that the first word in our message isn’t <span class="CustomCharStyle">Thanks</span>, but instead is <span class="CustomCharStyle">Hippopotamus</span>. Unless we’re working with those animals or are otherwise involved with them, that’s likely to be a rather surprising <span epub:type="pagebreak" title="135" id="Page_135"/>first word of a message. Let’s rank this word at an 80 on the surprise scale, as in <a href="#figure6-2" id="figureanchor6-2">Figure 6-2</a>.</p>
<figure>
<img src="Images/f06002.png" alt="f06002" width="453" height="103"/>
<figcaption><p><a id="figure6-2">Figure 6-2</a>: Placing messages on our surprise scale</p></figcaption>
</figure>
<p>Although <span class="CustomCharStyle">hippopotamus</span> might be a big surprise at the start of a message, it might not be surprising later on. The difference is context.</p>
<h3 id="h2-500723c06-0002">Unpacking Context</h3>
<p class="BodyFirst">For our purposes, we can think of <em>context</em> as the environment of the message. Since we’re focusing on the meaning of each message, rather than the physical way it’s communicated, the context represents the shared knowledge between the sender and receiver, which gives the message meaning.</p>
<p>When the message is a piece of language, this shared knowledge must include the words used, since a message of <span class="CustomCharStyle">Kxnfq rnggw</span> would carry no meaning. We can extend that shared knowledge to include grammar, current interpretations of emoticons and abbreviations, shared cultural influences, and so on. This is all called <em>global context</em>. It’s the general knowledge that we bring to any message, even before we’ve read it. In terms of our Bayes’ Rule discussion of Chapter 4, some of this global context is captured in our <em>prior</em>, since that is how we represent our understanding of the environment and what we expect to learn from it. </p>
<p>In contrast to the global context, there is also <em>local context</em>. That’s the environment composed of the elements of the message itself. In a text message, the local context for any given word is the other words in that message. </p>
<p>Let’s imagine that we’re reading a message for the first time, so each word’s local context is made up only of the words that preceded it. We can use the context to get a handle on surprise. If <span class="CustomCharStyle">Hippopotamus</span> is the first word of our message, then there is no local context yet, only the global. And if we don’t work with hippopotamuses on a regular basis, that word is likely very surprising. But if the message begins with, <span class="CustomCharStyle">Let’s go down to the river area at the zoo and maybe see a big gray</span>, then in that context, the word <span class="CustomCharStyle">hippopotamus</span> isn’t very surprising.</p>
<p>We can describe the amount of surprise carried by a specific word in our global context by assigning it a surprise value, as we did in <a href="#figure6-1">Figure 6-1</a>. Suppose that we assign a surprise value to every word in the dictionary (a tedious job, but certainly possible). If we scale these numbers so that they all add up to 1, we’ve created a probability mass function (or pmf), as we discussed in Chapter 2. That means we can draw a random variable from that pmf to get a word, with the most surprising words coming along more frequently than the less surprising words. A more common approach is to <span epub:type="pagebreak" title="136" id="Page_136"/>set up the pmf to represent how common a word is, which is roughly the opposite of surprise. With that setup, we’d expect to draw the least surprising, or more common, words more frequently than uncommon words. </p>
<p>We’ll use this idea later in the chapter to devise a scheme for transmitting the content of a message in an efficient manner.</p>
<h2 id="h1-500723c06-0002">Measuring Information</h2>
<p class="BodyFirst">In this chapter, we’re going to talk quite a lot about <em>bits</em>. In popular language, a bit is usually thought of as a little package of data, often labeled either 0 or 1. For instance, when we talk about internet speed in “bits per second,” we might picture the bits as leaves flowing down a river, and we count them as they go by.</p>
<p>This is a convenient idea, but in technical language, a bit is not a thing, like a leaf, but a unit, like a gallon or a gram. That is, it isn’t a piece of stuff but a way to talk about how much stuff we have. A bit is a container that holds just enough storage for what we currently think is the fundamental, indivisible, smallest possible chunk of information. </p>
<p>Speaking of bits as units in this way is technically correct, but it’s inconvenient. And most of the time, we can speak casually without any confusion, like when we say, “My net connection is 8,000 bits per second,” rather than, “My net connection is able to transmit 8,000 bits worth of information per second.” We’ll use the more casual language in most of this book, but it’s worthwhile to know the technical definition, because it does pop up from time to time in papers and documentation where the distinction is important. </p>
<p>We can measure the amount of information in a text message with a formula that tells us how many bits are needed to represent that message. We won’t get into the math, but we’ll describe what’s going on. The formula takes two inputs. The first is the text of the message. The second is a pmf that describes the surprise inherent in each word the message can contain (let’s just call this a <em>probability distribution</em> for the rest of this chapter). When we take the text of the message and the probability distribution together, we can produce a number that tells us how many bits of information the message carries.</p>
<p>The formula was designed so that the values it produces for each word (or, more generally, each <em>event</em>) have four key properties. We’ll illustrate each one using a context in which we work in an office, and not on a river.</p>
<ol class="decimal">
<li value="1">Likely events have low information. <span class="CustomCharStyle">Stapler</span> has low information.</li>
<li value="2">Unlikely events have high information. <span class="CustomCharStyle">Crocodile</span> has high information.</li>
<li value="3">Likely events have less information than unlikely events. <span class="CustomCharStyle">Stapler</span> conveys less information than <span class="CustomCharStyle">crocodile</span>. </li>
<li value="4">Finally, the total information due to two <em>unrelated </em>events is the sum of their individual information values found separately. </li>
</ol>
<p><span epub:type="pagebreak" title="137" id="Page_137"/>The first three properties relate single objects to their information. The oddball in the group is property 4, so let’s look at it more carefully. </p>
<p>In normal conversation, it’s rare for two consecutive words to be completely unrelated. But suppose someone asked us for a “kumquat daffodil.” Those words are just about completely unrelated, so property 4 says that we could find the information in that phrase by adding the information communicated by each word independently.</p>
<p>In normal conversation, the words that lead up to any given word often narrow the possibilities of what it could be. If someone says, “Today I ate a big,” then words like “sandwich” and “pizza” arriving next carry less surprise than “bathtub” or “sailboat.” When words are expected, they produce less surprise than when they’re not. By contrast, suppose we’re sending a device’s serial number, which is essentially an arbitrary sequence of letters and perhaps numbers, like “C02NV91EFY14.” If the characters really have no relation to each other, then adding the surprise due to each character gives us the overall surprise in the entire message representing the serial number.</p>
<p>By combining the surprise of two unrelated words into the sum of their individual surprise values, we go from measuring the surprise, or information, in each of those words to the surprise in their combination. We can keep combining words this way into ever-larger groups until we’ve considered the entire message. Though we haven’t gone into the math, we have reached a formal definition of <em>information</em>: it’s a number produced from a formula that uses one or more events (such as words), and a probability distribution to describe how surprising each event would be to us. From those two inputs the algorithm provides a number for each event, and guarantees that those numbers satisfy the four properties we just listed. We call each word’s number its <em>entropy</em>, telling us how many bits are needed to communicate it.</p>
<h2 id="h1-500723c06-0003">Adaptive Codes</h2>
<p class="BodyFirst">The amount of information carried by each event is influenced by the size of the probability function we hand to our formula. In other words, the number of possible words we might communicate affects the amount of information carried by each word we send.</p>
<p>Suppose we want to transmit the contents of a book from one place to another. We might list all the unique words in that book and then assign a number to each word, starting perhaps with 0 for <span class="CustomCharStyle">the</span>, then 1 for <span class="CustomCharStyle">and</span>, and so on. Then, if our recipient also has a copy of that word list, we can send the book just by sending the number for each word, starting with the first word in the book. The Dr. Seuss book <em>Green Eggs and Ham </em>contains only 50 different words (Seuss 1960). To represent a number between 0 and 49, we need six bits of information per word. By contrast, Robert Louis Stevenson’s book <em>Treasure Island </em>contains about 10,700 unique words (Stevenson 1883). We’d have to use 14 bits per word to uniquely identify each word in that book.</p>
<p><span epub:type="pagebreak" title="138" id="Page_138"/>Although we could use one giant word list of all English words to send these books, it’s more efficient to tailor our list to each book’s individual vocabulary, including only the words we actually need. In other words, we can improve our efficiency by <em>adapting</em> our transmission of information to what’s being communicated.</p>
<p>Let’s take that idea and run with it.</p>
<h3 id="h2-500723c06-0003">Speaking Morse </h3>
<p class="BodyFirst">A great example of adaptation is Morse code. In Morse code, each typographical character has an associated pattern of dots and dashes, separated by spaces, as shown in <a href="#figure6-3" id="figureanchor6-3">Figure 6-3</a>.</p>
<figure>
<img src="Images/f06003.png" alt="f06003" width="450" height="217"/>
<figcaption><p><a id="figure6-3">Figure 6-3</a>: Each character in Morse code has an associated pattern of dots, dashes, and spaces.</p></figcaption>
</figure>
<p>Morse code is traditionally sent by using a telegraph key to enable or disable transmission of a clear tone. A dot is a short burst of sound. The length of time we hold down the key to send a dot is represented by a unit called the <em>dit</em>. A dash is held for the duration of three dits. We leave one dit of silence between symbols, a silence of three dits between letters, and a silence of seven dits between words. These are of course ideal measures. In practice, many people can recognize the personal rhythm, called the <em>fist</em>, of each of their friends and colleagues (Longden 1987).</p>
<p>Morse code contains three types of symbols: dots, dashes, and dot-sized spaces. Let’s suppose we want to send the message “nice dog” in Morse code. <a href="#figure6-4" id="figureanchor6-4">Figure 6-4</a> shows the sequence of short tones (dots), long tones (dashes), and dot-sized spaces.</p>
<figure>
<img src="Images/f06004.png" alt="f06004" width="597" height="53"/>
<figcaption><p><a id="figure6-4">Figure 6-4</a>: The three symbols of Morse code: dots (solid circles), dashes (solid boxes), and silent spaces (empty circles).</p></figcaption>
</figure>
<p>We typically talk about Morse code strictly in terms of dots and dashes, which are called the <em>symbols</em>. The assigned set of symbols for any letter is that letter’s <em>pattern</em>. The length of time it takes to send a message depends <span epub:type="pagebreak" title="139" id="Page_139"/>on the specific patterns assigned to the letters that make up the message’s content. For example, even though the letters Q and H both have four symbols, Q requires 13 dits to send (3 for each of the 3 dashes, 1 for the dot, and 1 for each of the 3 spaces), while we need only 7 dits to send the letter H (4 dots, and 1 for each of the 3 spaces).</p>
<p>Let’s compare the patterns of the different characters. When we look at <a href="#figure6-3">Figure 6-3</a>, it might not be clear to us that there’s any principle behind how the various patterns are assigned. But a beautiful idea is there waiting to be uncovered. <a href="#figure6-5" id="figureanchor6-5">Figure 6-5</a> shows a list of the 26 Roman letters, sorted by their typical frequency in English (Wikipedia 2020). The most frequently used letter, E, leads the list.</p>
<figure>
<img src="Images/f06005.png" alt="f06005" width="617" height="34"/>
<figcaption><p><a id="figure6-5">Figure 6-5</a>: The Roman letters sorted by their frequency of use in English</p></figcaption>
</figure>
<p>Now look back at the patterns in <a href="#figure6-3">Figure 6-3</a>. The most frequent letter, E, is just a single dot. The next most frequent letter, T, is just a single dash. Those are the only two possible patterns with just one symbol, so now we move on to two symbols. The letter A is a dot followed by a dash. O is next, and it breaks the pattern because it’s too long: three dashes. Let’s come back to that later. Returning to our list, the I is two dots, the N is a dash and a dot. The last two-letter pattern is M, with two dashes, but that’s pretty far down the list from where we’ve gotten so far. Why is O too long and M too short? Morse code is almost following our letter-frequency table, but not quite.</p>
<p>The explanation starts with Samuel Morse, who only defined patterns for the numbers 0 through 9 in his original code. Letters and punctuation were added to the code by Alfred Vail, who designed those patterns in about 1844 (Bellizzi 11). Vail didn’t have an easy way to look up letter frequencies, but he knew he should follow them, according to Vail’s assistant, William Baxter. Baxter said,</p>
<blockquote class="review">
<p class="Blockquote">His general plan was to employ the simplest and shortest combinations to represent the most frequently recurring letters of the English alphabet, and the remainder for the more infrequent ones. For instance, he found upon investigation that the letter e occurs much more frequently than any other letter, and accordingly he assigned to it the shortest symbol, a single dot (•). On the other hand, j, which occurs infrequently, is expressed by dash-dot-dash-dot (– • – •) (Pope 1887)<sup class="FootnoteReference"><a id="c06-footnoteref-1" href="#c06-footnote-1">1</a></sup></p></blockquote> 
<p><span epub:type="pagebreak" title="140" id="Page_140"/>Vail figured that he could estimate the letter frequency table for English text by visiting his local newspaper in Morristown, New Jersey, where they were still setting stories by hand. In those days, typesetters built up a page one letter at a time. For each letter, they would choose an appropriate <em>slug</em>, or a metal bar with a letter embossed on one end, and place it into a large tray. Vail reasoned that the most popular characters would have the greatest number of slugs on hand, so he counted up the number of slugs in each letter’s bin. Those popularity counts were his proxy for letter frequency in English (McEwen 1997). Given how small this sample was, he did a pretty great job, despite imperfections like apparently thinking that M was more frequent than O. </p>
<p>To see how well our frequency chart (and Morse code) lines up with some actual text, <a href="#figure6-6" id="figureanchor6-6">Figure 6-6</a> shows the frequencies for the letters from <em>Treasure Island</em> (Stevenson 1883). For this chart, we counted only the letters, which we turned into lowercase before counting. We also excluded numbers, spaces, and punctuation.</p>
<figure>
<img src="Images/f06006.png" alt="f06006" width="694" height="497"/>
<figcaption><p><a id="figure6-6">Figure 6-6</a>: The number of times each letter appears in <em>Treasure Island</em> by Robert Louis Stevenson. Uppercase letters were counted as lowercase. </p></figcaption>
</figure>
<p>The order of the characters in <a href="#figure6-6">Figure 6-6</a> isn’t a perfect match to our letter frequency chart in <a href="#figure6-5">Figure 6-5</a>, but it’s close. <a href="#figure6-6">Figure 6-6</a> looks like a probability distribution over the letters A through Z. To make it an <em>actual</em> probability distribution, we have to scale it so that the sum of all the entries is 1. The result is shown in <a href="#figure6-7" id="figureanchor6-7">Figure 6-7</a>.</p>
<span epub:type="pagebreak" title="141" id="Page_141"/><figure>
<img src="Images/f06007.png" alt="f06007" width="694" height="507"/>
<figcaption><p><a id="figure6-7">Figure 6-7</a>: The probability distribution function (pdf) for characters in <em>Treasure Island</em></p></figcaption>
</figure>
<p>Now let’s use our probability distribution of letters to improve the efficiency of sending <em>Treasure Island</em> via Morse code.</p>
<h3 id="h2-500723c06-0004">Customizing Morse Code</h3>
<p class="BodyFirst">To motivate our improvements to sending <em>Treasure Island</em> via Morse code, let’s first take a step backward, and start with an imaginary version of Morse code where Mr. Vail didn’t bother to journey down to the newspaper office. Instead, let’s say he wanted to assign the same number of dot-and-dash symbols to each character. With four symbols he could only label 16 characters, but with five symbols he could label 32 characters.</p>
<p><a href="#figure6-8" id="figureanchor6-8">Figure 6-8</a> shows how we might arbitrarily assign such a five-symbol pattern to each character. To keep things simple, we made the timing of every dot and dash the same by using different tones for the two symbols. So every dot (shown here as a black dot) is a high tone lasting one dit, and every dash (shown as a red square) is a low tone lasting one dit. The result is that every character takes nine dits of time to send (five for the dots and dashes, now high and low tones, and four for the silences between them). This is an example of a <em>constant-length code</em>, also called <em>a fixed-length code</em>.</p>
<p>In <a href="#figure6-8">Figure 6-8</a> we didn’t create a character for the space, following in the footsteps of the original Morse code, where it was assumed that we could figure out where the spaces ought to go by looking at the message. Sticking to that spirit, we’ll ignore space characters for the rest of this discussion.</p>
<span epub:type="pagebreak" title="142" id="Page_142"/><figure>
<img src="Images/f06008.png" alt="f06008" width="531" height="254"/>
<figcaption><p><a id="figure6-8">Figure 6-8</a>: Assigning five symbols to each character gives us a constant-length code. Black circles are high tones; red squares are low tones. They all last one dit.</p></figcaption>
</figure>
<p>The first two words in the text of <em>Treasure Island </em>are the name “Squire Trelawney.” Since every character in our two-tone version of Morse code requires 9 dits, this phrase of 15 letters (remember that we’re ignoring the space) requires 9 × 15 = 135 dits of time to send. Adding in the 14 silences between letters, which take 3 × 14 = 42 bits, we find the fixed-length message takes 135 + 42 = 177 dits of time, as shown in <a href="#figure6-9" id="figureanchor6-9">Figure 6-9</a>. </p>
<figure>
<img src="Images/f06009.png" alt="f06009" width="679" height="206"/>
<figcaption><p><a id="figure6-9">Figure 6-9</a>: The first two words of <em>Treasure Island</em>, using our constant-length code</p></figcaption>
</figure>
<p>Now compare this to actual Morse code where, for the most part, the most common letters have fewer symbols than the uncommon letters. <a href="#figure6-10" id="figureanchor6-10">Figure 6-10</a> shows this. We’ll continue sending dots and dashes using different tones that last one dit each.</p>
<figure>
<img src="Images/f06010.png" alt="f06010" width="844" height="115"/>
<figcaption><p><a id="figure6-10">Figure 6-10</a>: The first two words of <em>Treasure Island</em>, using Morse code</p></figcaption>
</figure>
<p>If we count up the elements (remembering that dots and dashes now take just one dit each), we find that the <a href="#figure6-10">Figure 6-10</a> version requires 101 dits of time, about half as long as the fixed-length code (101 / 177 ≈ 0.57). That savings comes from adapting our code to the content we are sending. We call any code that tries to improve efficiency by matching up short patterns <span epub:type="pagebreak" title="143" id="Page_143"/>with high-probability events a <em>variable-bitrate code</em>, or more simply, an <em>adaptive code</em>. Even in this simple example, our adaptive code is almost twice as efficient as the constant-length code, cutting our communication time nearly in half.</p>
<p>Let’s look at the whole text of <em>Treasure Island</em>, which contains about 338,000 characters (excluding spaces, punctuation, etc.). The adaptive code would take only about 42 percent of the time required by the fixed-length code. We can send the book in less than half the time required by a nonadaptive code.</p>
<p>We can do even better if, instead of using standard Morse code, which is adapted to English writing in general, we tune the distribution of symbols to more closely match their actual percentages in the text of the specific book we’re sending. Of course, we’d have to share our clever encoding with our recipient, but if we’re sending a long message, that extra piece of communication is dwarfed by the message itself. Let’s take that step, and imagine a custom Treasure Island code that is perfectly adapted to the contents of <em>Treasure Island </em>specifically. We should expect even more savings. </p>
<p>Let’s rephrase this using the language of probability. An adaptive code creates a pattern for each value in a probability distribution. The value with the highest probability receives the shortest possible code. Then we work our way through the values, from the highest probability to the lowest, assigning patterns that are always as short as possible without repeating. That means each new pattern is as long as, or longer than, the pattern assigned to the previous value. That’s just what Mr. Vail did in 1844, guided by the number of letters he found in the typesetter’s bins of his local newspaper.</p>
<p>Now we can look at any message we want to communicate, identify each character, and compare it to the probability distribution that tells us how likely that character was in the first place. This tells us how much information, in bits, is carried by that character. Thanks to the fourth property in our description of the formula for computing information, the total number of bits required to represent the message (ignoring context for the moment), is just the sum of the individual numbers of bits required by each character.</p>
<p>We can also perform this process for our message before we send it. That tells us just how much information we’re about to communicate to our recipient.</p>
<h2 id="h1-500723c06-0004">Entropy</h2>
<p class="BodyFirst">We’ve discussed <em>surprise</em>, which refers to things we didn’t expect. A related idea is <em>uncertainty</em>, which refers to those times when we know all the things that might happen, but we’re not sure which one will actually occur. For instance, when we roll a fair six-sided die, we know that each of the six faces has an equal probability of coming up, but until we roll it and look, we are uncertain which face will be on top. A more formal term for this uncertainty is <em>entropy</em>.</p>
<p><span epub:type="pagebreak" title="144" id="Page_144"/>We can assign a number to the uncertainty, or entropy, of an outcome. This number often depends on how many outcomes are possible. For example, flipping a coin can have only 2 outcomes, but rolling a six-sided die can have 6 outcomes, and picking a letter from the alphabet can have 26 outcomes. The uncertainty of these three results, or their entropy, is a number that increases in size, from the coin to the die to the alphabet, because the number of outcomes is increasing in each case. That makes each specific result more uncertain.</p>
<p>In those three examples, the probability of each outcome is the same (1/2 for each side of the coin, 1/6 for each die face, and 1/26 for each letter). But what if the probabilities of the outcomes are different? The formula for computing the entropy explicitly takes these different probabilities into account. Essentially, it considers all the possible outcomes in a distribution and puts a number to the uncertainty describing which outcome is actually produced when we sample the distribution. </p>
<p>It turns out that the uncertainty of a specific event occurring is the same as the number of bits required to send a message with a perfectly adapted code. Conceptually, a text message is a set of words drawn from a vocabulary, which is no different than the values of a die rolled multiple times. We use the term <em>entropy</em> for both values: the uncertainty of an event, or the number of bits required to communicate that event (and remove the uncertainty). </p>
<p>Entropy is useful in machine learning because it lets us compare two probability distributions. This is a key step in learning. For example, consider a classifier. We might have a picture that we have manually decided is 80 percent likely to be a dog, but 10 percent likely to be a wolf, 3 percent likely to be a fox, and a few other smaller probabilities for other animals. We’d like the system’s predictions to match those labels. In other words, we want to compare our manual distribution with the system’s predicted distribution and use any differences to improve our system. We can invent lots of ways to compare distributions, but the one that works best in both theory and practice is based on entropy. Let’s build our way up to that comparison, starting with finding the entropy for a single distribution. </p>
<p>Consider a distribution made up of words. If only one word is in our distribution, there is no uncertainty about what word we’ll get when we sample the distribution, and therefore the entropy is 0. If there are lots of words but they all have a probability of 0, except for a single word with a probability of 1, there’s still no uncertainty, so the entropy is again 0. When all of the words have the same probabilities, we have the maximum uncertainty, since no choice is any more probable than any other. In this case, our uncertainty, or entropy, is at a maximum. Though it might be convenient to say that maximum entropy should be 1, or 100, the actual value is calculated by the formula. What we do know is that no other probability distribution will give us a larger entropy. </p>
<p>In the next section, we’ll see how to apply entropy to pairs of distributions.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="box">
<h2><span epub:type="pagebreak" title="145" id="Page_145"/>Where the Term Entropy Comes From </h2>
<p class="BoxBodyFirst">The term <em>entropy</em> has been used for decades in the field of thermodynamics, a branch of physics that is concerned with heat and temperature. In that field, entropy refers to “disorder.” Let’s see how the physics version of entropy compares to the information theory version.</p>
<p>In thermodynamics, we often think of whatever we’re studying as a <em>system</em>. This is just the collection of things we care about. Let’s imagine a system many people look forward to each morning. It consists of two mixed parts: coffee and milk. The physicist might ask, “Where is the coffee located in this this mixture?” The information theorist might ask, “If I spoon up some liquid, will I get coffee or milk?”</p>
<p>When we begin, the coffee is in a cup and the milk is in a small pitcher. To the physicist, this system has high order, or low disorder, since everything is in its own place. Because it has low disorder, it has low entropy. To the information theorist, dipping our spoon into the coffee cup is guaranteed to come up with coffee, so there is no uncertainty. Low uncertainty means low entropy. </p>
<p>Let’s pour the milk into the coffee, and stir lightly. Now when the physicist asks, “Where is the coffee in this mixture?” the answer is harder to state. The coffee is all mixed up with the milk. There is a lot of disorder here, so there’s high entropy. When the information theorist dips in a spoon, and we haven't stirred thoroughly, there’s no way to predict what ratio of coffee and milk will come up. There’s a lot of uncertainty, and thus again high entropy.</p>
<p>Disorder, uncertainty, and entropy are all different ways of referring to the same idea (Serrano 2017).</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1-500723c06-0005">Cross Entropy</h2>
<p class="BodyFirst">When we’re training a deep learning system, we’ll often want to have a measure that tells us to what degree two probability distributions are the same or different. The value we usually use is a quantity called the <em>cross entropy</em>, and it too is just a number. Recall that the entropy tells us how many bits we need to send a message using a code that is perfectly tuned to that message. The cross entropy tells us how many bits we need if we use some other, less perfect code. Generally, this is larger than the number of bits the perfect code needs (if the alternative code happens to be exactly as efficient as the ideal code, the cross entropy has its minimum value of 0). The cross entropy is a measurement that lets us compare two probability distributions numerically. Identical distributions have a cross entropy of 0, while increasingly different pairs of distributions have increasingly larger values of cross entropy.</p>
<p><span epub:type="pagebreak" title="146" id="Page_146"/>To get a feeling for the idea, let’s look at two novels, and build up a word-based adaptive code for each. Though our goal is to compare probability distributions, and we’re here talking about codes, it’s conceptually easy to go back and forth. Recall that by construction, smaller codes correspond to words with higher probabilities, while larger codes correspond to words with lower probabilities. </p>
<h3 id="h2-500723c06-0005">Two Adaptive Codes</h3>
<p class="BodyFirst">The novels <em>Treasure Island </em>and <em>The Adventures of Huckleberry Finn</em>, by Mark Twain, were both written in English at about the same time (Stevenson 1883; Twain 1885). <em>Treasure Island</em> has the larger vocabulary, using about 10,700 unique words, compared to about 7,400 unique words in <em>Huckleberry Finn</em>. Of course, they use very different sets of words, but there’s lots of overlap. Let’s look at the 25 most popular words in <em>Treasure Island</em>, shown in <a href="#figure6-11" id="figureanchor6-11">Figure 6-11</a>. For the purposes of counting words, we first converted all uppercase letters to lowercase. The single-letter pronoun “I” therefore appears in the charts as the lower-case “i.”</p>
<figure>
<img src="Images/f06011.png" alt="f06011" width="694" height="489"/>
<figcaption><p><a id="figure6-11">Figure 6-11</a>: The 25 most popular words in <em>Treasure Island</em>, sorted by number of appearances</p></figcaption>
</figure>
<p>Let’s compare these to the 25 most popular words in <em>Huckleberry Finn</em>, shown in <a href="#figure6-12" id="figureanchor6-12">Figure 6-12</a>.</p>
<p>Perhaps unsurprisingly, the most popular dozen words in both books are almost the same (though in different orders), but then things begin to diverge.</p>
<span epub:type="pagebreak" title="147" id="Page_147"/><figure>
<img src="Images/f06012.png" alt="f06012" width="694" height="488"/>
<figcaption><p><a id="figure6-12">Figure 6-12</a>: The 25 most popular words in <em>Huckleberry Finn</em>, sorted by number of appearances</p></figcaption>
</figure>
<p>Let’s suppose we want to transmit the text of both books, word by word. We could go to the English dictionary and assign every word a number starting with 1, then 2, then 3, and so on. But we know from our earlier Morse code example that we can send information more efficiently by using a code that’s adapted to the material being sent. Let’s create that kind of code, where the more frequently a word appears, the smaller its code number. So super-frequent words like <span class="CustomCharStyle">the</span> and <span class="CustomCharStyle">and</span> can be sent with short codes, while the rare words have longer codes that require us to send more bits (in <em>Treasure Island</em> about 2,780 words appear only once; in <em>Huckleberry Finn </em>about 2,280 words appear only once).</p>
<p>The vocabularies of the two books mostly overlap, but each book has words that don’t appear in the other. For instance, the word <span class="CustomCharStyle">yonder</span> appears 20 times <em>Huckleberry Finn</em>, but not even once in <em>Treasure Island. </em>And <span class="CustomCharStyle">schooner</span> is in <em>Treasure Island </em>28 times, but it’s nowhere to be found in <em>Huckleberry Finn</em>.</p>
<p>Because we want to be able to send either book with either code, let’s unify their vocabularies. For each word in <em>Huckleberry Finn</em> that isn’t in <em>Treasure Island</em>, we add one instance of that word when we make the Treasure Island code. We do the same thing for <em>Huckleberry Finn</em>. For example, we tack on one instance of <span class="CustomCharStyle">yonder</span> to the end of the book when we make the Treasure Island code so that we can use that code to send <em>Huckleberry Finn</em> if we wanted to.</p>
<p><span epub:type="pagebreak" title="148" id="Page_148"/>Let’s start with the words in <em>Treasure Island</em>. We’ll make an adaptive code for this text, starting with a tiny code for <span class="CustomCharStyle">the</span> and working our way up to huge codes for one-time-only words like <span class="CustomCharStyle">wretchedness</span>. Now we can send the whole book using that code and save time compared to any other code. </p>
<p>Now we’ll do the same thing for <em>Huckleberry Finn</em>, and make a code specifically for this text, giving the shortest code to <span class="CustomCharStyle">and</span> and leaving the big codes for one-time-only words like <span class="CustomCharStyle">dangerous</span> (shocking, but true: <span class="CustomCharStyle">dangerous</span> appears only once in <em>Huckleberry Finn</em>!). The Huckleberry Finn code now lets us send the contents of this book more quickly than any other code. </p>
<p>Note that these two codes are different. We’d expect that, because the two books have different vocabularies, and cover significantly different subject matter. </p>
<h3 id="h2-500723c06-0006">Using the Codes</h3>
<p class="BodyFirst">Now we have two codes, each of which can transmit either book. The Treasure Island code is tuned to how many times each word appears in <em>Treasure Island</em>, and the Huckleberry Finn code is tuned to <em>Huckleberry Finn</em>.</p>
<p>The <em>compression ratio</em> tells us how much savings we get from using an adaptive code versus a fixed-length code. If the ratio is exactly 1, then our adaptive code uses exactly as many bits as a nonadaptive code. If the ratio is 0.75, then the adaptive code sends only 3/4 the number of bits needed by the nonadaptive code. The smaller the compression ratio, the more bits we’re saving (some authors define this ratio with the numbers in the other order, so the larger the ratio, the better the compression).</p>
<p>Let’s try sending our two books word by word. The top bar of <a href="#figure6-13" id="figureanchor6-13">Figure 6-13</a> shows the compression ratio that we get from sending <em>Huckleberry Finn </em>with the code we built for it. We used an adaptive code called a <em>Huffman code</em>, but the results would be similar for most adaptive codes (Huffman 1952; Ferrier 2020).</p>
<figure>
<img src="Images/f06013.png" alt="f06013" width="694" height="219"/>
<figcaption><p><a id="figure6-13">Figure 6-13</a>: Top: The compression ratio from sending <em>Huckleberry Finn</em> using the code built from that book. Bottom: The compression from using the code built from <em>Treasure Island</em>.</p></figcaption>
</figure>
<p>This is pretty great. The adaptive code got a compression ratio of a little less than 0.5, meaning that to send <em>Huckleberry Finn </em>using this code would require a little less than half the number of bits required by a fixed-length <span epub:type="pagebreak" title="149" id="Page_149"/>code. If we send <em>Huckleberry Finn </em>using the code built from <em>Treasure Island</em>, we should expect that the compression won’t be as good, because our numbers in that code are not matched to the word frequencies we’re encoding. The bottom bar of <a href="#figure6-13">Figure 6-13</a> shows this result, with a compression ratio of around 0.54. That’s still pretty great, but not quite as efficient.</p>
<p>Let’s flip the situation around and see how <em>Treasure Island </em>does with a code built for it, and one built for <em>Huckleberry Finn</em>. The results are shown in <a href="#figure6-14" id="figureanchor6-14">Figure 6-14</a>.</p>
<figure>
<img src="Images/f06014.png" alt="f06014" width="694" height="219"/>
<figcaption><p><a id="figure6-14">Figure 6-14</a>: Top: The compression ratio from sending <em>Treasure Island</em> using the code built from <em>Huckleberry Finn</em>. Bottom: The compression ratio for sending <em>Treasure Island, </em>using the code for <em>Treasure Island</em>. </p></figcaption>
</figure>
<p>This time we find that <em>Treasure Island </em>compressed better than <em>Huckleberry Finn</em>, which makes sense because we used a code tuned to its word usage. In general, the fastest way to send any message is with a code that was built for the contents of that message. No other code can do better, and most will do worse.</p>
<p>We’ve seen that using the Treasure Island code to send <em>Huckleberry Finn</em> gives us worse compression. In other words, it requires more bits to send this book with a code that is imperfect for this message. This is because each code is based on its corresponding probability distribution, and those distributions are different.</p>
<p>The quantity we use to measure the difference between two probability distributions is <em>cross entropy</em>. </p>
<p>Note that the situation is not symmetrical. If we want to send words from <em>Treasure Island</em> using the Huckleberry Finn code, the cross entropy will be different from sending <em>Huckleberry Finn</em> with the Treasure Island code. We sometimes say that the cross entropy function is <em>asymmetrical</em> in its arguments, meaning that their order matters. </p>
<p>One way to conceptualize this is to picture that our space of probability distributions is like the ocean, with currents flowing in different directions in different places. The effort required to swim from some point A to another point B, sometimes fighting the currents and sometimes getting carried along by them, is generally different than the effort required to swim from B to A. In this metaphor, the cross entropy is measuring the amount of work, not the actual distance between the points. But as A and B <span epub:type="pagebreak" title="150" id="Page_150"/>get closer together, the work involved in swimming between them, in either direction, goes down.</p>
<h3 id="h2-500723c06-0007">Cross Entropy in Practice</h3>
<p class="BodyFirst">Let’s see cross entropy in action. We’ll use it just as we do when we’re training a photo classifier and need to compare two probability distributions. The first is the label that we manually created to describe what’s in the photo. The second is the set of probabilities that the system computes when we show it that photo. Our goal is to train the system so that its outputs match our labels. To do that, we need to know when the system gets it wrong and put a number to how wrong it is. That’s the cross entropy we get by comparing the label and the predictions. The larger the cross entropy, the larger the error.</p>
<p>In <a href="#figure6-15" id="figureanchor6-15">Figure 6-15</a> we have the output of an imaginary classifier that’s predicting the probabilities for a picture of a dog. In most real situations, all of the label values would be 0 except for the entry for <span class="CustomCharStyle">dog</span>, which would be 1. Here we’ve assigned arbitrary probabilities to each of the six labels to better show how the system tries to match the label distribution (we can imagine that the picture is blurry, so we’re not sure ourselves what animal it shows). </p>
<figure>
<img src="Images/f06015.png" alt="f06015" width="844" height="306"/>
<figcaption><p><a id="figure6-15">Figure 6-15</a>: Classifying a picture of a dog. Left: At the start of training. Right: After much training. The cross entropy is lower when the match is better.</p></figcaption>
</figure>
<p>The figure at the left comes from the start of training. The system’s predictions are a pretty poor match to our manual labels. If we run these numbers through the cross entropy formula, we get a cross entropy of about 1.9. On the right, we see the results after some training. Now the two distributions are much closer, and the cross entropy has dropped to about 1.6. </p>
<p>Most deep learning libraries offer built-in routines that compute the cross entropy for us in a single step. In <a href="#figure6-15">Figure 6-15</a> we had six categories. When there are only two categories, we can use a routine that’s specialized for that case. It’s often called the <em>binary cross entropy</em> function. </p>
<h2 id="h1-500723c06-0006"><span epub:type="pagebreak" title="151" id="Page_151"/>Kullback–Leibler Divergence</h2>
<p class="BodyFirst">Cross entropy is a great measure for comparing two distributions. By minimizing the cross entropy, we minimize the error between the classifier’s outputs and our label, improving our system.</p>
<p>We can make things just a little simpler conceptually with one more step. Let’s think of our word distributions as codes again. Recall that the entropy tells us how many bits are required to send a message with a perfect, tuned code. And the cross entropy tells us how many bits are required to send that message with an imperfect code. If we subtract the entropy from the cross entropy, we get the number of additional bits required by the imperfect code. The smaller we can get this number, the fewer additional bits we need, and the more the corresponding probability distributions are the same.</p>
<p>This extra number of bits required by an imperfect code (that is, the increase in entropy) goes by a large number of formidable names. The most popular is the <em>Kullback–Leibler divergence</em> or just <em>KL divergence</em>, named for the scientists who presented a formula for computing this value. Less frequently, it’s also referred to as <em>discrimination information</em>, <em>information divergence</em>, <em>directed divergence</em>, <em>relative entropy</em>, and <em>KLIC</em> (for <em>Kullback–Leibler information criterion</em>).</p>
<p>Like the cross entropy, the KL divergence is asymmetrical: the order of the arguments matters. The KL divergence for sending <em>Treasure Island </em>with the Huckleberry Finn code is written KL(Treasure Island||Huckleberry Finn). The two bars in the middle can be thought of as a single separator, like the more frequently seen comma. We can think of them as representing the phrase “sent using the code for.” If we run through the math, this value is about 0.287. We can think of this as telling us that we’re “paying” around 0.3 extra bits per word because we’re using the wrong code (Kurt 2017). The KL divergence for sending <em>Huckleberry Finn</em> with the<em> </em>Treasure Island code, or KL(Huckleberry Finn||Treasure Island), is much higher, at about 0.5. </p>
<p>The KL divergence tells us the number of additional bits we need in order to send our message with an imperfect code. Another way to think about this is that the KL divergence describes how much more information we need to turn our imperfectly adapted code into a perfect one. We can imagine this as a step of Bayes’ Rule, where we go from an approximate prior (the imperfect code) to a better posterior (the adapted code). In this case, the KL divergence is telling us just how much we learn from that idealized step of Bayes’ Rule (Thomas 2017).</p>
<p>We can train our systems either by minimizing the KL divergence, or the cross entropy, choosing whichever is more convenient. The KL divergence has nice mathematical properties and shows up in many mathematical and algorithmic discussions and even deep learning documentation. But in practice, the cross entropy is almost always faster to compute. Since minimizing either one has the same effect of improving our system, we usually see KL divergence in technical discussions, and cross entropy in deep learning programs.</p>
<h2 id="h1-500723c06-0007"><span epub:type="pagebreak" title="152" id="Page_152"/>Summary</h2>
<p class="BodyFirst">In this chapter we looked at some of the basic ideas behind information theory, and how we can use them to train a deep learning system. We use these ideas in machine learning by translating our codes into probability distributions. That just means identifying the code elements with the smallest code numbers as the most frequent elements, and as the size of the number goes up, the frequency goes down. Interpreted this way, we can calculate the cross entropy of a classifier by comparing the list of predicted probabilities it produces in response to an input with the list of probabilities we assigned by hand. Our goal in training is to make the two distributions as similar as possible, which we can also state as trying to minimize the cross entropy. </p>
<p>This wraps up the first part of the book. We’ve covered some fundamental ideas that have value far beyond deep learning. Statistics, probability, Bayes’ Rule, curves, and information theory all can help us make sense of a wide variety of problems and even things that come up in everyday life. They can help us improve our reasoning about events that happen in the world, and thus help us understand the past and prepare for the future.</p>
<p>With these fundamentals in our pocket, we’ll now turn to the basic tools of machine learning. </p>
<section class="footnotes">
<aside class="FootnoteEntry"><p><sup class="FootnoteReference"><a id="c06-footnote-1" href="#c06-footnoteref-1">1</a></sup> The quote refers to the letter J having the pattern dash-dot-dash-dot, but <a href="#figure6-3">Figure 6-3</a> associates that pattern with the letter C. The quote refers to J’s pattern in an early version of the code called American Morse, now rarely used (see <a href="https://en.wikipedia.org/wiki/American_Morse_code" class="LinkURL">https://en.wikipedia.org/wiki/American_Morse_code</a>).</p></aside>
</section>
</section>
</div></body></html>