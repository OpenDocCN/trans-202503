- en: Chapter 9. Logging, Monitoring, and Statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Logging, Monitoring, and Statistics](httpatomoreillycomsourcenostarchimages2127149.png.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Exercising control over a network—whether for your home networking needs or
    in a professional context—is likely to be a main objective for anyone who reads
    this book. One necessary element of keeping control is having access to all relevant
    information about what happens in your network. Fortunately for us, PF—like most
    components of Unix-like systems—is able to generate log data for network activity.
  prefs: []
  type: TYPE_NORMAL
- en: PF offers a wealth of options for setting the level of logging detail, processing
    log files, and extracting specific kinds of data. You can already do a lot with
    the tools that are in your base system, and several other tools are available
    via your package system to collect, study, and view log data in a number of useful
    ways. In this chapter, we take a closer look at PF logs in general and some of
    the tools you can use to extract and present information.
  prefs: []
  type: TYPE_NORMAL
- en: 'PF Logs: The Basics'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The information that PF logs and the level of logging detail are up to you,
    as determined by your rule set. Basic logging is simple: For each rule that you
    want to log data for, add the `log` keyword. When you load the rule set with `log`
    added to one or more rules, any packet that starts a connection matching the logging
    rule (blocked, passed, or matched) is copied to a `pflog` device. *The packet
    is logged as soon as it’s seen by PF and at the same time that the logging rule
    is evaluated.*'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*In complicated rule sets, a packet may go through several transformations
    due to `match` or `pass` rules, and criteria that matched a packet when it entered
    the host might not match after a transformation.*'
  prefs: []
  type: TYPE_NORMAL
- en: PF will also store certain additional data, such as the timestamp, interface,
    original source and destination IP addresses, whether the packet was blocked or
    passed, and the associated rule number from the loaded rule set.
  prefs: []
  type: TYPE_NORMAL
- en: PF log data is collected by the `pflogd` logging daemon, which starts by default
    when PF is enabled at system startup. The default location for storing the log
    data is */var/log/pflog*. The log is written in a binary format, usually called
    *packet capture format (pcap)*, that’s intended to be read and processed by `tcpdump`.
    We’ll discuss additional tools to extract and display information from your log
    file later. The log file format is a well-documented and widely supported binary
    format.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, here’s a basic log example. Start with the rules you want to
    log and add the `log` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Reload the rule set, and you should see the timestamp on your */var/log/ pflog*
    file change as the file starts growing. To see what’s being stored there, use
    `tcpdump` with the `-r` option to read the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'If logging has been going on for a while, entering the following on a command
    line can produce large amounts of output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, the following are just the first lines from a file several screens
    long, with almost all lines long enough to wrap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `tcpdump` program is very flexible, especially when it comes to output,
    and it offers a number of display choices. The format in this example follows
    from the options we fed to `tcpdump`. The program almost always displays the date
    and time the packet arrived (the `-ttt` option specifies this long format). Next,
    `tcpdump` lists the rule number in the loaded rule set, the interface on which
    the packet appeared, the source and target address and ports (the `-n` option
    tells `tcpdump` to display IP addresses, not hostnames), and the various packet
    properties.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*The rule numbers in your log files refer to the* loaded, in-memory *rule set.
    Your rule set goes through some automatic steps during the loading process, such
    as macro expansion and optimizations, which make it likely that the rule number
    as stored in the logs will not quite match what you’d find by counting from the
    top of your* pf.conf *file. If it isn’t immediately obvious to you which rule
    matched, use `pfctl -vvs rules` and study the output.*'
  prefs: []
  type: TYPE_NORMAL
- en: In our `tcpdump` output example, we see that the tenth rule (`rule 10`) in the
    loaded rule set seems to be a catchall that matches both IDENT requests and domain
    name lookups. This is the kind of output you’ll find invaluable when debugging,
    and it’s essential to have this kind of data available in order to stay on top
    of your network. With a little effort and careful reading of the `tcpdump` man
    pages, you should be able to extract useful information from your log data.
  prefs: []
  type: TYPE_NORMAL
- en: For a live display of the traffic you log, use `tcpdump` to read log information
    directly from the log device. To do so, use the `-i` option to specify which interface
    you want `tcpdump` to read from, as follows. (The `-l` option, which enables line
    buffering on the output, is useful if you want to look at what you’re capturing.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This sequence begins with a domain name lookup answer, followed by two packets
    from an open SSH connection, which tells us that the site’s administrator probably
    enabled `log (all)` on the matching rules (see [Logging All Packets: log (all)](ch09.html#logging_all_packets_log_left_parenthesis
    "Logging All Packets: log (all)")). The fourth packet belongs to a website connection,
    the fifth is part of an outgoing SMTP connection, and finally there’s another
    SSH packet. If you were to leave this command running, the displayed lines would
    eventually scroll off the top of your screen, but you could redirect the data
    to a file or to a separate program for further processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Sometimes you’ll be interested mainly in traffic between specific hosts or
    in traffic matching specific criteria. For these cases, `tcpdump`’s filtering
    features can be useful. See `man tcpdump` for details.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Logging the Packet’s Path Through Your Rule Set: log (matches)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Early versions of the PF logging code didn’t feature an easy way to track all
    rules that a packet would match during rule-set traversal. This omission became
    more evident than before when `match` rules were introduced in OpenBSD 4.6 and
    PF users were offered a more convenient and slightly easier way to subject packets
    and connections to transformations, such as address rewriting. `match` rules allow
    you to perform actions on a packet or connection independently of the eventual
    `pass` or `block` decision. The specified actions—such as `nat-to`, `rdr-to`,
    and a few others—are performed immediately. This can lead to situations in which
    a packet has been transformed by a `match` rule and it no longer matches criteria
    in a filtering rule that appears later in the rule set that it otherwise would
    have matched if the transformation hadn’t already occurred. One fairly basic example
    is a `match` rule that applies `nat-to` on the external interface, placed before
    any `pass` rules in the rule set. Once the `nat-to` action has been applied, any
    filtering criteria that would have matched the packet’s original source address
    will no longer match the packet.
  prefs: []
  type: TYPE_NORMAL
- en: This greater versatility made some rule sets harder to debug (typically those
    with several `match` rules that perform transformations), and it became clear
    that a new logging option was needed.
  prefs: []
  type: TYPE_NORMAL
- en: The PF developers had been eyeing the logging code for a rewrite for some time,
    and by the time the logging system was rewritten for the OpenBSD 4.9 release,
    the restructured code made it easy to introduce the log option `matches` to help
    debug such rule sets and to help track a packet’s path through rule sets where
    several sets of `match` or `pass` rules could transform the packet.
  prefs: []
  type: TYPE_NORMAL
- en: Adding `log (matches)` to a rule forces the logging of all matched rules once
    a packet matches a rule containing a `log (matches)` clause. Once such a match
    occurs, all subsequent rules will also be logged. As a result, you can use targeted
    `log (`**`matches`**`)` statements to trace a packet’s path through your loaded
    rule set, making it much easier to untangle complicated rule sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider this simple rule set with NAT. The `log (matches)` rule
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Our test host is a workstation in the local network with the IP address 192.168.103.44\.
    When the test host looks up a website somewhere on the Internet, the logged information
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The initial packet first matches `rule 3`, the `match log (matches)` rule quoted
    above the log fragment ➊. The next match is `rule 11` in our loaded rule set ➋,
    the initial `block all`, but the packet also matches `rule 17`, which lets it
    `pass in on em0` ➌. The next matching `rule 5` at ➍ is apparently a `match` rule
    that applies `nat-to` (note the changed source address). Finally, the packet passes
    `out on xl0` thanks to `rule 16` ➎, a matching `pass` rule.
  prefs: []
  type: TYPE_NORMAL
- en: This example really has only one transformation (the `nat-to`), but the `log
    (matches)` feature allows us to follow the connection’s initial packet through
    all matching rules in the rule set, including the source address substitution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logging All Packets: log (all)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For most debugging and lightweight monitoring purposes, logging the first packet
    in a connection provides enough information. However, sometimes you may want to
    log all packets that match certain rules. To do so, use the `(all)` logging option
    in the rules you want to monitor. After making this change to our minimal rule
    set, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This option makes the logs quite a bit more verbose. To illustrate just how
    much more data `log (all)` generates, we’ll use the following rule set fragment,
    which passes domain name lookups and network time synchronizations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'With these rules in place, here’s an example of what happens when a Russian
    name server sends a domain name request to a server in our network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We now have six entries instead of just one.
  prefs: []
  type: TYPE_NORMAL
- en: Even with all but `port domain` filtered out by `tcpdump`, adding `log (all)`
    to one or more rules considerably increases the amount of data in your logs. If
    you need to log all traffic but your gateway’s storage capacity is limited, you
    may find yourself shopping for additional storage, and the added I/O activity
    may in fact have a negative impact on performance. Also, recording and storing
    traffic logs with this level of detail is likely to have legal implications.
  prefs: []
  type: TYPE_NORMAL
- en: Log Responsibly!
  prefs: []
  type: TYPE_NORMAL
- en: Creating logs of any kind could have surprising consequences, including some
    legal implications. Once you start storing log data generated by your network
    traffic, you’re creating a store of information about your users. There may be
    good technical and business reasons to store logs for extended periods, but logging
    just enough data and storing it for just the right amount of time can be a fine
    art.
  prefs: []
  type: TYPE_NORMAL
- en: You probably have some idea of the practical issues related to generating log
    data, such as arranging for sufficient storage to retain enough log data long
    enough to be useful. The legal implications will vary according to your location.
    Some countries and territories have specific requirements for handling log data,
    along with restrictions on how that data may be used and how long logs can be
    retained. Others require service providers to retain traffic logs for a specific
    period of time, in some cases with a requirement to deliver any such data to law
    enforcement upon request. Make sure you understand the legal issues before you
    build a logging infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Logging to Several pflog Interfaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Versions of PF newer than OpenBSD 4.1 make it possible to direct your log data
    to more than one `pflog` interface. In OpenBSD 4.1, the `pflog` interface became
    a *cloneable* device, meaning that you can use `ifconfig` commands to create several
    `pflog` interfaces, in addition to the default `pflog0`. This makes it possible
    to record the log data for different parts of your rule set to separate `pflog`
    interfaces, and it makes it easier to process the resulting data separately if
    necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Moving from the default single `pflog0` interface to several `pflog` interfaces
    requires some changes to your setup that are subtle but effective. To log to several
    interfaces, make sure that all the log interfaces your rule set uses are created.
    You don’t need to create the devices before the rule set is loaded; if your rule
    set logs to a nonexistent interface, the log data is simply discarded.
  prefs: []
  type: TYPE_NORMAL
- en: 'When tuning your setup to use several `pflog` interfaces, you’ll most likely
    add the required interfaces from the command line, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the log device when you add the `log` keyword to your rule set, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For a more permanent configuration on OpenBSD, create a *hostname. pflog1* file
    containing only `up` and similar *hostname.pflogN* files for any additional logging
    interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'On FreeBSD, the configuration of the cloned `pflog` interfaces belongs in your
    *rc.conf* file in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As of this writing, cloning `pflog` interfaces on NetBSD isn’t an option.
  prefs: []
  type: TYPE_NORMAL
- en: As you saw in [Chapter 6](ch06.html "Chapter 6. Turning the Tables for Proactive
    Defense"), directing log information for different parts of your rule set to separate
    interfaces makes it possible to feed different parts of the log data PF produces
    to separate applications. This makes it easier to have programs like `spamlogd`
    process only the relevant information, while you feed other parts of your PF log
    data to other log-processing programs.
  prefs: []
  type: TYPE_NORMAL
- en: Logging to syslog, Local or Remote
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One way to avoid storing PF log data on the gateway itself is to instruct your
    gateway to log to another machine. If you already have a centralized logging infrastructure
    in place, this is a fairly logical thing to do, even if PF’s ordinary logging
    mechanisms weren’t really designed with traditional `syslog`-style logging in
    mind.
  prefs: []
  type: TYPE_NORMAL
- en: As any old BSD hand will tell you, the traditional `syslog` system log facility
    is a bit naive about managing the data it receives over UDP from other hosts,
    with denial-of-service attacks involving full disks one frequently mentioned danger.
    There’s also the ever-present risk that log information will be lost under high
    load on either individual systems or the network. Therefore, consider setting
    up remote logging *only* if all hosts involved communicate over a well-secured
    and properly dimensioned network. On most BSDs, `syslogd` isn’t set up by default
    to accept log data from other hosts. (See the `syslogd` man page for information
    about how to enable listening for log data from remote hosts if you plan to use
    remote `syslog` logging.)
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’d still like to do your PF logging via `syslog`, the following is a
    short recipe for how to accomplish this. In ordinary PF setups, `pflogd` copies
    the log data to the log file. When you want to store the log data on a remote
    system, you should disable `pflog`’s data accumulation by changing daemon’s startup
    options in *rc.conf.local* (on OpenBSD), like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'On FreeBSD and NetBSD, change the `pflog_flats=` setting line in *rc.conf.*
    Then kill the `pflogd` process. Next, make sure that the log data, now no longer
    collected by `pflogd`, is transmitted in a meaningful way to your log-processing
    system instead. This step has two parts: First, set up your system logger to transmit
    data to the log-processing system, and then use `tcpdump` with `logger` to convert
    the data and inject it into the `syslog` system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up `syslogd` to process the data, choose your *log facility*, *log level*,
    and *action* and put the resulting line in */etc/syslog.conf*. These concepts
    are very well explained in `man syslog.conf`, which is required reading if you
    want to understand system logs. The *action* part is usually a file in a local
    file-system. For example, if you’ve already set up the system logger at *`loghost.example.com`*
    to receive your data, choose log facility `local2` with log level `info` and enter
    this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Once you’ve made this change, restart `syslogd` to make it read the new settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, set `tcpdump` to convert the log data from the `pflog` device and feed
    it to `logger`, which will then send it to the system logger. Here, we reuse the
    `tcpdump` command from the basic examples earlier in this chapter, with some useful
    additions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `nohup` command makes sure the process keeps running even if it doesn’t
    have a controlling terminal or it’s put in the background (as we do here with
    the trailing `&`). The `-l` option to the `tcpdump` command specifies line-buffered
    output, which is useful for redirecting to other programs. The `logger` option
    adds the tag `pf` to identify the PF data in the stream and specifies log priority
    with the `-p` option as `local2.info`. The result is logged to the file you specify
    on the logging host, with entries that will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This log fragment shows mainly Web-browsing activities from a client in a NATed
    local network, as seen from the gateway’s perspective, with accompanying domain
    name lookups.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking Statistics for Each Rule with Labels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The sequential information you get from retrieving log data basically tracks
    packet movements over time. In other contexts, the sequence or history of connections
    is less important than aggregates, such as the number of packets or bytes that
    have matched a rule since the counters were last cleared.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of [Chapter 2](ch02.html "Chapter 2. PF Configuration Basics"),
    you saw how to use `pfctl -s info` to view the global aggregate counters, along
    with other data. For a more detailed breakdown of the data, track traffic totals
    on a per-rule basis with a slightly different form of `pfctl` command, such as
    `pfctl -vs rules`, to display statistics along with the rule, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The format of this output is easy to read, and it’s obviously designed for contexts
    in which you want to get an idea of what’s going on at a glance. If you specify
    even more verbose output with `pfctl -vvs rules`, you’ll see essentially the same
    display, with rule numbers added. On the other hand, the output from this command
    isn’t very well suited for feeding to a script or other program for further processing.
    To extract these statistics and a few more items in a script-friendly format—and
    to make your own decisions about which rules are worth tracking)—use rule *labels*.
  prefs: []
  type: TYPE_NORMAL
- en: Labels do more than identify rules for processing specific kinds of traffic;
    they also make it easier to extract the traffic statistics. By attaching labels
    to rules, you can store certain extra data about parts of your rule set. For example,
    you could use labeling to measure bandwidth use for accounting purposes.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we attach the labels `mail-in` and `mail-out` to our
    `pass` rules for incoming and outgoing mail traffic, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you’ve loaded the rule set with labels, check the data using `pfctl -vsl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This output contains the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ The label
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ➋ The number of times the rule has been evaluated
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ➌ The total number of packets passed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ➍ The total number of bytes passed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ➎ The number of packets passed in
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ➏ The number of bytes passed in
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ➐ The number of packets passed out
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ➒ The number of bytes passed out
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The format of this list makes it very well suited for parsing by scripts and
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: The labels accumulate data from the time the rule set is loaded until their
    counters are reset. And, in many contexts, it makes sense to set up a `cron` job
    that reads label values at fixed intervals and then puts those values into permanent
    storage.
  prefs: []
  type: TYPE_NORMAL
- en: If you choose to run the data collection at fixed intervals, consider collecting
    the data using `pfctl -vsl -z`. The `z` option resets the counters once `pfctl`
    has read them, with the result that your data collector will then fetch *periodic
    data*, accumulated since the command or the script was last run.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Rules with macros and lists expand to several distinct rules. If your rule
    set contains rules with lists and macros that have a label attached, the in-memory
    result will be a number of rules, each with a separate, identically named label
    attached to it. While this may lead to confusing `sudo pfctl -vsl` output, it
    shouldn’t be a problem as long as the application or script that receives the
    data can interpret the data correctly by adding up the totals for the identical
    labels.*'
  prefs: []
  type: TYPE_NORMAL
- en: If this type of data collection sounds useful to you, it’s also worth noting
    that recent PF versions offer the option of collecting traffic metadata as NetFlow
    or IPFIX data. See [Collecting NetFlow Data with pflow(4)](ch09.html#collecting_netflow_data_with_pflowleft_p
    "Collecting NetFlow Data with pflow(4)") for details.
  prefs: []
  type: TYPE_NORMAL
- en: Additional Tools for PF Logs and Statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One other important component of staying in control of your network is having
    the ability to keep an updated view of your system’s status. In this section,
    we’ll examine a selection of monitoring tools that you may find useful. All the
    tools presented here are available either in the base system or via the package
    system on OpenBSD and FreeBSD (and, with some exceptions, on NetBSD).
  prefs: []
  type: TYPE_NORMAL
- en: Keeping an Eye on Things with systat
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you’re interested in seeing an instant snapshot of the traffic passing through
    your systems right now, the `systat` program on OpenBSD offers several useful
    views. In [Chapter 7](ch07.html "Chapter 7. Traffic Shaping with Queues and Priorities"),
    we looked briefly at `systat queues` to see how traffic was assigned to queues
    in our traffic-shaping rule sets. Here, we’ll review some additional useful options.
  prefs: []
  type: TYPE_NORMAL
- en: The `systat` program is available on all BSD operating systems, in slightly
    different versions. On all systems, `systat` offers views of system statistics,
    with some minor variations in syntax and output. For example, the `queues` view
    is one of several `systat` views available in recent OpenBSD versions, but not
    in FreeBSD or NetBSD as of this writing.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more general view of the current state table than that offered by `queues`,
    try `systat states`, which gives a listing very similar to the `top(1)` process
    listing. Here’s an example of typical `systat states` output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If your states don’t fit on one screen, just page through the live display.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, `systat rules` displays a live view of packets, bytes, and other
    statistics for your loaded rule set, as in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `systat rules` view is especially useful because it offers a live view into
    the fully parsed and loaded rule set. For example, if your rule set behaves oddly,
    the `rules` view can point you in the right direction and show you the flow of
    packets.
  prefs: []
  type: TYPE_NORMAL
- en: The `systat` program also offers a view that presents the same data you’d get
    via `pfctl -s status` on the command line. The following example shows part of
    the output of `systat pf`. The `systat pf` view offers more information than will
    fit on most screens, but you can page through the live display of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `systat` program offers quite a few other views, including network-related
    ones, such as `netstat`, `vmstat` for virtual memory statistics, and `iostat`
    for input/output statistics by device. You can cycle through all `systat` views
    using the left and right cursor keys. (See `man systat` for full details.)
  prefs: []
  type: TYPE_NORMAL
- en: Keeping an Eye on Things with pftop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If your system doesn’t have a `systat` version with the PF-related views, you
    can still keep an eye on what’s passing into and out of your network in real time
    using Can Erkin Acar’s `pftop`. This command shows a running snapshot of your
    traffic. `pftop` isn’t included in the base system, but it’s available as a package—in
    ports on OpenBSD and FreeBSD as *sysutils/pftop*^([[44](#ftn.ch09fn01)]) and on
    NetBSD via `pkgsrc` as *sysutils/pftop*. Here’s an example of its output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: You can use `pftop` to sort your connections by a number of different criteria,
    including by PF rule, volume, age, and source and destination addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Graphing Your Traffic with pfstat
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once you have a system up and running and producing data, a graphical representation
    of traffic data is a useful way to view and analyze your data. One way to graph
    your PF data is with `pfstat`, a utility developed by Daniel Hartmeier to extract
    and present the statistical data that’s automatically generated by PF. The `pfstat`
    tool is available via the OpenBSD package system or as the port *net/pfstat*,
    via the FreeBSD ports system as *sysutils/pfstat*, and via NetBSD `pkgsrc` as
    *sysutils/pfstat*.
  prefs: []
  type: TYPE_NORMAL
- en: The `pfstat` program collects the data you specify in the configuration file
    and presents that data as JPG or PNG graphics files. The data source can be either
    PF running on the local system via the */dev/pf* device or data collected from
    a remote computer running the companion `pfstatd` daemon.
  prefs: []
  type: TYPE_NORMAL
- en: To set up `pfstat`, you simply decide which parts of your PF data you want to
    graph and how, and then you write the configuration file and start `cron` jobs
    to collect the data and generate your graphs. The program comes with a well-annotated
    sample configuration file and a useful man page. The sample configuration is a
    useful starting point for writing your own configuration file. For example, the
    following *pfstat.conf* fragment is very close to one you’ll find in the sample
    configuration:^([[45](#ftn.ch09fn02)])
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The configuration here starts off with three `collect` statements, where each
    of the data series is assigned a unique numeric identifier. Here, we capture the
    number of insertions, removals, and searches in the state table. Next up is the
    `image` definition, which specifies the data that is to be graphed. The `from`
    line specifies the period to display (`from 1 days to now` means that only data
    collected during the last 24 hours is to be displayed). `width` and `height` specify
    the graph size measured in number of pixels in each direction. The `graph` statements
    specify how the data series are displayed as well as the graph legends. Collecting
    state insertions, removals, and searches once a minute and then graphing the data
    collected over one day produces a graph roughly like the one in [Figure 9-1](ch09.html#state_table_statisticscomma_24-hour_time
    "Figure 9-1. State table statistics, 24-hour time scale").
  prefs: []
  type: TYPE_NORMAL
- en: '![State table statistics, 24-hour time scale](httpatomoreillycomsourcenostarchimages2127165.png.jpg)Figure 9-1. State
    table statistics, 24-hour time scale'
  prefs: []
  type: TYPE_NORMAL
- en: The graph can be tweaked to provide a more detailed view of the same data. For
    example, to see the data for the last hour in a slightly higher resolution, change
    the period to `from 1 hours to now` and the dimensions to `width 600 height 300`.
    The result is something like the graph in [Figure 9-2](ch09.html#state_table_statisticscomma_1-hour_time
    "Figure 9-2. State table statistics, 1-hour time scale").
  prefs: []
  type: TYPE_NORMAL
- en: '![State table statistics, 1-hour time scale](httpatomoreillycomsourcenostarchimages2127167.png)Figure 9-2. State
    table statistics, 1-hour time scale'
  prefs: []
  type: TYPE_NORMAL
- en: The `pfstat` home page at *[http://www.benzedrine.cx/pfstat.html](http://www.benzedrine.cx/pfstat.html)*
    contains several examples, with demonstrations in the form of live graphs of the
    data from the *benzedrine.cx* domain’s gateways. By reading the examples and tapping
    your own knowledge of your traffic, you should be able to create `pfstat` configurations
    that are well suited to your site’s needs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*In addition to `pfstat`, other system-monitoring packages offer at least some
    PF-monitoring features. One such package is the popular `symon` utility, which
    is usually configured with the `symon` data gatherer on all monitored systems
    and at least one host with `symux` and the optional `syweb` Web interface. Based
    on round-robin database tool (RRDtool), `symon` has a useful interface for recording
    PF data and offers a useful graphical interface for displaying PF statistics via
    the `syweb` Web interface. `symon` is available as a port or package on OpenBSD
    and FreeBSD as `sysutils/symon`, and the `syweb` Web interface is available as
    `www/syweb`.*'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting NetFlow Data with pflow(4)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*NetFlow* is a network data collection and analysis method that has spawned
    many supporting tools for recording and analyzing data about TCP/IP connections.
    NetFlow originated at Cisco and over time has become an essential feature in various
    network equipment as a tool for network management and analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The NetFlow data model defines a network *flow* as a unidirectional sequence
    of packets with the same source and destination IP address and protocol. For example,
    a TCP connection will appear in NetFlow data as two flows: one in each direction.'
  prefs: []
  type: TYPE_NORMAL
- en: PF data can be made available to NetFlow tools via the `pflow(4)` pseudo-interface
    that was introduced in OpenBSD 4.5 along with the `pflow` state option. Essentially,
    all the information you’d expect to find in a NetFlow-style flow record is easily
    derived from the data PF keeps in the state table, and the `pflow` interface offers
    a straightforward way to export PF state-table data in this processing-friendly
    and well-documented format. As with other logging, you enable NetFlow data collection
    in your PF rule set on a per-rule basis.
  prefs: []
  type: TYPE_NORMAL
- en: A complete NetFlow-based network-monitoring system consists of several distinct
    parts. The NetFlow data originates at one or more *sensors* that generate data
    about network traffic. The sensors forward data about the flows to a *collector*,
    which stores the data it receives. Finally, a *reporting* or *analysis* system
    lets you extract and process the data.^([[46](#ftn.ch09fn03)])
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up the NetFlow Sensor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The NetFlow sensor requires two components: one or more configured `pflow(4)`
    devices and at least one `pass` rule in your rule set with the `pflow` state option
    enabled. The `pflow` interfaces are created with two required parameters: the
    flow source IP address and flow destination’s IP address and port. Here’s an example
    of the `ifconfig` command for the */etc/hostname.pflow0* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'From the command line, use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In both cases, this command sets up the host to send NetFlow data with a flow
    source address 192.0.2.1 to a collector that should listen for NetFlow data at
    192.0.2.105, UDP port 3001.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*It’s possible to set up several `pflow` devices with separate flow destinations.
    It’s not currently possible, however, to specify on a per-rule basis which `pflow`
    device should receive the generated data.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'After enabling the `pflow` device, specify in */etc/pf.conf* which `pass` rules
    should provide NetFlow data to the sensor. For example, if your main concern is
    to collect data on your clients’ email traffic to IPv4 hosts, this rule would
    set up the necessary sensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: When `pflow` was first introduced to PF, the immediate reaction from early adopters
    was that more likely than not, they’d want to add the `pflow` option to most `pass`
    rules in their rule sets. This led PF developer Henning Brauer to introduce another
    useful PF feature—the ability to set *state defaults* that apply to all rules
    unless otherwise specified. For example, if you add the following line at the
    start of your rule set, all `pass` rules in the configuration will generate NetFlow
    data to be exported via the `pflow` device.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: With at least one `pflow` device configured and at least one rule in your *pf.conf*
    that generates data for export via the `pflow` device, you’re almost finished
    setting up the sensor. You may still need to add a rule, however, that allows
    the UDP data to flow from the IP address you specified as the flow data source
    to the collector’s IP address and target port at the flow destination. Once you’ve
    completed this last step, you should be ready to turn your attention to collecting
    the data for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: NetFlow Data Collecting, Reporting, and Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If your site has a NetFlow-based collection and analysis infrastructure in place,
    you may already have added the necessary configuration to feed the PF-originated
    data into the data collection and analysis system. If you haven’t yet set up a
    flow-analysis environment, there are a number of options available.
  prefs: []
  type: TYPE_NORMAL
- en: 'The OpenBSD packages system offers three NetFlow collector and analysis packages:
    `flow-tools`, `flowd`, and `nfdump`.^([[47](#ftn.ch09fn04)]) All three systems
    have a dedicated and competent developer and user community as well as various
    add-ons, including graphical Web interfaces. `flow-tools` is the main component
    in many sites’ flow-analysis setups. The `nfdump` fans point to the `nfsen` analysis
    package that integrates the `nfdump` tools in a powerful and flexible Web-based
    analysis frontend that will, among other things, display the command-line equivalent
    of your GUI selections. You’ll find the command-line display useful when you need
    to drill down further into the data than the selections in the GUI allow. You
    can copy the command displayed in the GUI and make any further adjustments you
    need on the `nfdump` command line in a shell session or script to extract the
    exact data you want.'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a Collector
  prefs: []
  type: TYPE_NORMAL
- en: The choice of collector is somewhat tied to the choice of analysis package.
    Perhaps because the collectors tend to store flow data in their own unique formats,
    most reporting and analysis backends are developed with a distinctive bias for
    one or the other collector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of your choice of NetFlow collector, the familiar logging caveats
    apply: Detailed traffic log information will require storage. In the case of NetFlow,
    each flow will generate a record of fairly fixed size, and anecdotal evidence
    indicates that even modest collection profiles on busy sites can generate gigabytes
    of NetFlow data per day. The amount of storage you’ll need is directly proportional
    to the number of connections and how long you keep the original NetFlow data.
    Finally, recording and storing traffic logs with this level of detail is likely
    to have legal implications.'
  prefs: []
  type: TYPE_NORMAL
- en: Collectors generally offer filtering features that let you discard data about
    specific hosts or networks or even discard some parts of the NetFlow records themselves,
    either globally or for data about specific hosts or networks.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate some basic NetFlow collection and how to extract a subset of the
    collected data for further analysis, we’ll use `flowd`, developed by long-time
    OpenBSD developer Damien Miller and available via the package systems (on OpenBSD
    as *net/flowd* and on FreeBSD as *net-mgmt/flowd*).
  prefs: []
  type: TYPE_NORMAL
- en: I’ve chosen to use `flowd` here mainly because it was developed to be small,
    simple, and secure. As you’ll see, `flowd` still manages to be quite useful and
    flexible. Flow data operations with other tools will differ in some details, but
    the underlying principles remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: When compared to other NetFlow collector suites, `flowd` is very compact, with
    only two executable programs—the collector daemon `flowd` and the flow-filtering
    and presentation program `flowd-reader`—as well as the supporting library and
    controlling configuration file. The documentation is adequate, if a bit terse,
    and the sample */etc/flowd.conf* file contains a generous number of comments.
    Based on the man pages and the comments in the sample configuration file, it shouldn’t
    take you long to create a useful collector configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'After stripping out any comment lines—using `grep -v \# /etc/flowd.conf` or
    similar—a very basic `flowd` configuration could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'While this configuration barely contains more information than the `pflow`
    interface’s configuration in the earlier description of setting up the sensor,
    it does include two important items:'
  prefs: []
  type: TYPE_NORMAL
- en: The `logfile` line tells us where the collected data is to be stored (and reveals
    that `flowd` tends to store all data in a single file).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final line tells us that `flowd` will store all fields in the data it receives
    from the designated flow source.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With this configuration in place, start up the `flowd` daemon, and almost immediately
    you should see the */var/log/flowd* file grow as network traffic passes through
    your gateway and flow records are collected. After a while, you should be able
    to look at the data using `flowd`’s companion program `flowd-reader`. For example,
    with all fields stored, the data for one name lookup from a host on the NATed
    local network looks like this in `flowd-reader`’s default view:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the lookup generates two flows: one in each direction.'
  prefs: []
  type: TYPE_NORMAL
- en: The first flow is identified mainly by the time it was received, followed by
    the protocol used (protocol 17 is UDP, as */etc/protocols* will tell you). The
    connection had both TCP and TOS flags unset, and the collector received the data
    from our gateway at 192.0.2.1\. The flow’s source address was 192.0.2.254, source
    port 55108, and the destination address was 192.0.2.1, source port 53, conventionally
    the DNS port. The flow consisted of 1 packet with a payload of 62 octets. The
    return flow was received by the collector at the same time, and we see that this
    flow has the source and destination reversed, with a slightly larger payload of
    129 octets. `flowd-reader`’s output format lends itself to parsing by regular
    expressions for postprocessing in reporting tools or plotting software.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might think that this data is all anyone would ever want to know about
    any particular set of network flows, but it’s possible to extract even more detailed
    information. For example, using the `flowd-reader -v` option for verbose output,
    you might see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The `gateway` field indicates that the sensor itself served as the gateway for
    this connection. You see a list of the interfaces involved (the `in_if` and `out_if`
    values), the sensor’s system uptime (`sys_uptime_ms`), and a host of other parameters—such
    as `AS` numbers (`src_AS` and `dst_AS`)—that may be useful for statistics or filtering
    purposes in various contexts. Once again, the output is ideally suited to filtering
    via regular expressions.
  prefs: []
  type: TYPE_NORMAL
- en: 'You don’t need to rely on external software for the initial filtering on the
    data you collect from your `pflow` sensor. `flowd` itself offers a range of filtering
    features that make it possible to store only the data you need. One approach is
    to put the filtering expressions in the *flowd.conf*, as in the following example
    (with the comments stripped to save space):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: You can choose to store only certain fields in the flow records. For example,
    in configurations where there’s only one collector or agent, the `agent` field
    serves no useful purpose and doesn’t need to be stored. In this configuration,
    we choose to store only the source and destination address and port, the number
    of packets, and the number of octets.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can limit the data you store even further. The macros `internalnet` and
    `unwired` expand to two NATed local networks, and the four `discard` lines following
    the macro definitions mean that `flowd` discards any data it receives about flows
    with either source or destination addresses in either of those local networks.
    The result is a more compact set of data, tailored to your specific needs, and
    you see only routable addresses and the address of the sensor gateway’s external
    interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Even with the verbose option, `flowd-reader`’s display reveals only what you
    explicitly specify in the filtering configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Fortunately, `flowd` doesn’t force you to make all your filtering decisions
    when your collector receives the flow data from the sensor. Using the `-f` flag,
    you can specify a separate file with filtering statements to extract specific
    data from a larger set of collected flow data. For example, to see HTTP traffic
    to your Web server, you could write a filter that stores only flows with your
    Web server’s address and TCP port 80 as the destination or flows with your Web
    server and TCP port 80 as the source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Assuming you stored the filter in *towebserver.flowdfilter*, you could then
    extract traffic matching your filtering criteria from */var/log/flowd*, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In addition to the filtering options demonstrated here, the `flowd` filtering
    functions take a number of other options. Some of those options will be familiar
    from other filtering contexts such as PF, including a range of network-oriented
    parameters; others are more oriented to extracting data on flows originating at
    specific dates or time periods and other storage-oriented parameters. The full
    story, as always, is found in `man flowd.conf`.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve extracted the data you need, you have several tools available for
    processing and presenting your data.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting NetFlow Data with pfflowd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For systems that don’t support NetFlow data export via `pflow`, NetFlow support
    is available via the `pfflowd` package. As we already saw in the previous section,
    PF state table data maps very well to the NetFlow data model, and `pfflowd` is
    intended to record state changes from the local system’s `pfsync` device. Once
    enabled, `pfflowd` acts as a NetFlow sensor that converts `pfsync` data to NetFlow
    format for transmission to a NetFlow collector on the network.
  prefs: []
  type: TYPE_NORMAL
- en: The `pfflowd` tool was written and is maintained by Damien Miller and is available
    from *[http://www.mindrot.org/projects/pfflowd/](http://www.mindrot.org/projects/pfflowd/)*
    as well as through the package systems on OpenBSD and FreeBSD as *net/pfflowd*.
    The lack of `pfsync` support on NetBSD means that `pfflowd` isn’t available on
    that platform as of this writing.
  prefs: []
  type: TYPE_NORMAL
- en: SNMP Tools and PF-Related SNMP MIBs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Simple Network Management Protocol (SNMP)* was designed to let network administrators
    collect and monitor key data about how their systems run and change configurations
    on multiple network nodes from a centralized system.^([[48](#ftn.ch09fn05)]) The
    SNMP protocol comes with a well-defined interface and a method for extending the
    *management information base (MIB)*, which defines the managed devices and objects.'
  prefs: []
  type: TYPE_NORMAL
- en: Both proprietary and open source network management and monitoring systems generally
    have SNMP support in one form or the other, and in some products, it’s a core
    feature. On the BSDs, SNMP support has generally come in the form of the `net-snmp`
    package, which provides the tools you need to retrieve SNMP data and to collect
    data for retrieval by management systems. The package is available on OpenBSD
    as *net/net-snmp*, on FreeBSD as *net-mgmt/net-snmp*, and on NetBSD as *net/net-snmp*.
    OpenBSD’s `snmpd` (written mainly by Reyk Floeter) debuted as part of the base
    system in OpenBSD 4.3 and implements all required SNMP functionality. (See `man
    snmpd` and `man snmpd.conf` for details.)
  prefs: []
  type: TYPE_NORMAL
- en: There are MIBs to make PF data available to SNMP monitoring. Joel Knight maintains
    the MIBs for retrieving data on PF, CARP, and OpenBSD kernel sensors, and he offers
    them for download from *[http://www.packetmischief.ca/openbsd/snmp/](http://www.packetmischief.ca/openbsd/snmp/)*.
    The site also offers patches to the `net-snmp` package to integrate the OpenBSD
    MIBs.
  prefs: []
  type: TYPE_NORMAL
- en: After installing the package and the extension, your SNMP-capable monitoring
    systems will be able to watch PF data in any detail you desire. (FreeBSD’s `bsnmpd`
    includes a PF module. See the `bsnmpd` man page for details.)
  prefs: []
  type: TYPE_NORMAL
- en: Log Data as the Basis for Effective Debugging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we walked through the basics of collecting, displaying, and
    interpreting data about a running system with PF enabled. Knowing how to find
    and use information about how your system behaves is useful for several purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping track of the status of a running system is useful in itself, but the
    ability to read and interpret log data is even more essential when testing your
    setup. Another prime use for log data is to track the effect of changes you make
    in the configuration, such as when tuning your system to give optimal performance.
    In the next chapter, we’ll focus on checking your configuration and tuning it
    for optimal performance, based on log data and other observations.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([[44](#ch09fn01)]) On OpenBSD, all `pftop` functionality is included in various
    `systat` views, as described in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: '^([[45](#ch09fn02)]) The color values listed in the configuration example would
    give you a graph with red, blue, and green lines. For the print version of this
    book, we changed the colors to grayscale values: `0 192 0` became `105 105 105`,
    `0 0 255` became `192 192 192`, and `255 0 0` became `0 0 0`.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([[46](#ch09fn03)]) For a more in-depth treatment of network analysis with
    NetFlow-based tools, see *Network Flow Analysis* by Michael W. Lucas (No Starch
    Press, 2010).
  prefs: []
  type: TYPE_NORMAL
- en: ^([[47](#ch09fn04)]) The actively maintained project home pages for `flow-tools`
    and `nfdump` are *[http://code.google.com/p/flow-tools/](http://code.google.com/p/flow-tools/)*
    and *[http://nfdump.sourceforge.net/](http://nfdump.sourceforge.net/)*. (The older
    versions should still be available from *[http://www.splintered.net/sw/flow-tools/](http://www.splintered.net/sw/flow-tools/).*)
    The `nfsen` Web frontend has a project page at *[http://nfsen.sourceforge.net/](http://nfsen.sourceforge.net/).*
    For the latest information about `flowd`, visit *[http://www.mindrot.org/flowd.html](http://www.mindrot.org/flowd.html).*
  prefs: []
  type: TYPE_NORMAL
- en: ^([[48](#ch09fn05)]) The protocol debuted with RFC 1067 in August 1988 and is
    now in its third major version as defined in RFCs 3411 through 3418.
  prefs: []
  type: TYPE_NORMAL
