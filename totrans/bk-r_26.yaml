- en: '**22**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**LINEAR MODEL SELECTION AND DIAGNOSTICS**'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'You’ve now spent a fair amount of time on many aspects of linear regression
    models. In this chapter, I’ll cover how formal R tools and techniques can be used
    to investigate two other, and no less important, aspects of regression: choosing
    an appropriate model for your analysis and assessing the validity of the assumptions
    you’ve made.'
  prefs: []
  type: TYPE_NORMAL
- en: '**22.1 Goodness-of-Fit vs. Complexity**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The overarching goal of fitting any statistical model is to faithfully represent
    the data and the relationships held within them. In general, fitting statistical
    models boils down to a balancing act between two things: goodness-of-fit and complexity.
    *Goodness-of-fit* refers to the goal of obtaining a model that best represents
    the relationships between the response and the predictor (or predictors). *Complexity*
    describes how complicated a model is; this is always tied to the number of terms
    in the model that require estimation—the inclusion of more predictors and additional
    functions (such as polynomial transformations and interactions) leads to a more
    complex model.'
  prefs: []
  type: TYPE_NORMAL
- en: '***22.1.1 Principle of Parsimony***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Statisticians refer to the balancing act between goodness-of-fit and complexity
    as the *principle of parsimony*, where the goal of the associated *model selection*
    is to find a model that’s as simple as possible (in other words, with relatively
    low complexity), without sacrificing too much goodness-of-fit. We’d say that a
    model that satisfies this notion is a *parsimonious* fit. You’ll often hear of
    researchers talking about choosing the “best” model—they’re actually referring
    to the idea of parsimony.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do you decide where to draw the line on such a balance? Naturally, statistical
    significance plays a role here—and model selection often simply comes down to
    assessing the significance of the effect of predictors or functions of predictors
    on the response. In an effort to impart some amount of objectivity to such a process,
    you can use systematic selection algorithms, such as those you’ll learn about
    in [Section 22.2](ch22.xhtml#ch22lev1sec73), to decide between multiple explanatory
    variables and any associated functions.
  prefs: []
  type: TYPE_NORMAL
- en: '***22.1.2 General Guidelines***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Performing any kind of model selection or comparing several models against one
    another involves decision making regarding the inclusion of available predictor
    variables. On this topic, there are several guidelines you should always follow.
  prefs: []
  type: TYPE_NORMAL
- en: • First, it’s important to remember that you can’t remove individual levels
    of a categorical predictor in a given model; this makes no sense. In other words,
    if one of the nonreference levels is statistically significant but all others
    are nonsignificant, you should treat the categorical variable, as a whole, as
    making a statistically significant contribution to the determination of the mean
    response. You should only really consider *entire* removal of that categorical
    predictor if *all* nonreference coefficients are associated with a lack of evidence
    (against being zero). This also holds for interactive terms involving categorical
    predictors.
  prefs: []
  type: TYPE_NORMAL
- en: • If an interaction is present in the fitted model, all lower-order interactions
    and main effects of the relevant predictors must remain in the model. This was
    touched upon in [Section 21.5.1](ch21.xhtml#ch21lev2sec202), when I discussed
    interpretation of interactive effects as augmentations of lower-order effects.
    As an example, you should only really consider removing the main effect of a predictor
    if there are no interaction terms present in the fitted model involving that predictor
    (even if that main effect has a high *p*-value).
  prefs: []
  type: TYPE_NORMAL
- en: • In models where you’ve used a polynomial transformation of a certain explanatory
    variable (refer to [Section 21.4.1](ch21.xhtml#ch21lev2sec199)), keep all lower-order
    polynomial terms in the model if the highest is deemed significant. A model containing
    an order 3 polynomial transformation in a predictor, for example, must also include
    the order 1 and order 2 transformations of that variable. This is because of the
    mathematical behavior of polynomial functions—only by explicitly separating out
    the linear, quadratic, and cubic (and so on) effects as distinct terms can you
    avoid confounding said effects with one another.
  prefs: []
  type: TYPE_NORMAL
- en: '**22.2 Model Selection Algorithms**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The job of a model selection algorithm is to sift through your available explanatory
    variables in some systematic fashion in order to establish which are best able
    to jointly describe the response, as opposed to fitting models by examining specific
    combinations of predictors in isolation, as you’ve done so far.
  prefs: []
  type: TYPE_NORMAL
- en: Model selection algorithms can be controversial. There are several different
    methods, and no single approach is universally appropriate for every regression
    model. Different selection algorithms can result in different final models, as
    you’ll see. In many cases, researchers will have additional information or knowledge
    about the problem that influences the decision—for example, that certain predictors
    must always be included or that it makes no sense to ever include them. This must
    be considered at the same time as other complications, such as the possibility
    of interactions or unobserved lurking variables influencing significant relationships
    and the need to ensure any fitted model is statistically valid (which you’ll look
    at in [Section 22.3](ch22.xhtml#ch22lev1sec74)).
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s helpful to keep in mind this famous quote from celebrated statistician
    George Box (1919–2013): “All models are wrong, but some are useful.”'
  prefs: []
  type: TYPE_NORMAL
- en: Any fitted model you produce can never be assumed to be the truth, but a model
    that’s fitted and checked carefully and thoroughly can reveal interesting features
    of the data and so have the potential to reveal associations and relationships
    by providing quantitative estimates thereof.
  prefs: []
  type: TYPE_NORMAL
- en: '***22.2.1 Nested Comparisons: The Partial F-Test***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The *partial F-test* is probably the most direct way to compare several different
    models. It looks at two or more *nested* models, where the smaller, less complex
    model is a reduced version of the bigger, more complex model. Formally, let’s
    say you’ve fitted two linear regressions models as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0529-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the reduced model, predicting *ŷ*[redu], has *p* predictors, plus one
    intercept. The full model, predicting *ŷ*[full], has *q* predictor terms. The
    notation implies that *q* > *p* and that, along with the standard inclusion of
    an intercept ![image](../images/b0.jpg), the full model involves all *p* predictors
    of the reduced model defined by *ŷ*[redu], as well as *q* − *p* additional terms.
    This emphasizes the fact that the model for *ŷ[redu]* is nested within *ŷ*[full].
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s important to note that increasing the number of predictors in a regression
    model will always improve *R*² and any other measures of goodness-of-fit. The
    real question, however, is whether that improvement in goodness-of-fit is large
    enough to make the additional complexity involved with including any additional
    predictor terms “worth it.” This is precisely the question that the partial *F*-test
    tries to answer in the context of nested regression models. Its goal is to test
    whether including those extra *q* − *p* terms, which produce the full model rather
    than the reduced model, provide a statistically significant improvement in goodness-of-fit.
    The partial *F*-test addresses these hypotheses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e22-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The calculation of the test statistic to address these hypotheses follows the
    same ideas behind the omnibus *F*-test automatically produced by R when summarizing
    a fitted linear model object (detailed in [Section 21.3.5](ch21.xhtml#ch21lev2sec197)).
    Denote the coefficient of determination for the reduced and full models with ![image](../images/r2redu.jpg)
    and ![image](../images/r2redu.jpg), respectively. If *n* refers to the sample
    size of the data used to fit both models, the test statistic given by
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e22-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: follows an *F* distribution with df[1] = *q* − *p*, df[2] = *n* − *q* degrees
    of freedom under the assumption of H[0] in (22.1). The *p*-value is found as the
    upper-tail area from ![image](../images/f.jpg) as usual; the smaller it is, the
    greater the evidence against the null hypothesis, which states that one or more
    of the additional parameters has no impact on the response variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the model objects `survmult` and `survmult2` from [Section 21.3.1](ch21.xhtml#ch21lev2sec193)
    as an example. The `survmult` model aims to predict mean student height from writing
    handspan and sex based on the `survey` data frame from the `MASS` package; `survmult2`
    adds smoking status to these predictors. If you need to, return to [Section 21.3.1](ch21.xhtml#ch21lev2sec193)
    to refit these two models. Printing the objects to the console screen previews
    the two fits and makes it easy to confirm that the smaller model is indeed nested
    within the larger model in terms of its explanatory variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once you’ve fitted your nested models, R can carry out partial *F*-tests using
    the `anova` function (partial *F*-tests fall within the suite of analysis of variance
    methodologies). To determine whether adding `Smoke` as a predictor provides any
    statistically significant improvement in fit, simply start with the reduced model
    and supply the model objects as arguments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The output provides the quantities associated with calculation of ![image](../images/r2redu.jpg)
    and ![image](../images/r2redu.jpg) and the test statistic ![image](../images/f.jpg)
    from (22.2), given in the resulting table as `F`, which is of the most interest.
    Using the values of *p* and *q* from printing `survmult` and `survmult2`, you
    should be able to confirm, for example, the values of df[1] and df[2] appearing
    in the second row of the table in the columns `Df` and `Res.Df`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The result of this particular test, obtained from a test statistic of ![image](../images/f0531-01.jpg)
    associated with df[1] = 3, df[2] = 201, is a high *p*-value of 0.823, suggesting
    no evidence against H[0]. This means that adding `Smoke` to the reduced model,
    which includes only the explanatory variables `Wr.Hnd` and `Sex`, offers no tangible
    improvement in fit when it comes to modeling student height. That conclusion isn’t
    surprising, given the nonsignificant *p*-values of all non-reference levels of
    `Smoke`, seen previously in [Section 21.3.1](ch21.xhtml#ch21lev2sec193).
  prefs: []
  type: TYPE_NORMAL
- en: This is how partial *F*-tests are used for model selection—in the current example,
    the reduced model would be the more parsimonious fit and preferred over the full
    model.
  prefs: []
  type: TYPE_NORMAL
- en: You can conduct comparisons among several nested models in a given call to `anova`,
    which can be useful for investigating things such as the inclusion of interactive
    terms or including polynomial transformations of predictors since there’s a natural
    hierarchy that requires you to retain any lower-order terms.
  prefs: []
  type: TYPE_NORMAL
- en: For an example, let’s use the `diabetes` data frame in the `faraway` package
    from [Section 21.5.2](ch21.xhtml#ch21lev2sec203), with the model fitted to predict
    cholesterol level (`chol`) against age (`age`) and body frame (`frame`) and the
    interaction between those two predictors. Before using the partial *F*-tests to
    compare nested variants, you need to ensure you’re using the same records for
    each model and that there aren’t missing values for any of the predictors that
    are then going to be unavailable to the “fuller” models (so the sample size is
    the same for each comparison). To do this, you just need to first define a version
    of `diabetes` that removes records with missing values in the predictors you’re
    using.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `faraway` package and use logical subsetting to identify and delete
    any individuals with a missing value for `age` *or* for `frame`. Define this new
    version of the `diabetes` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, fit the following four models using your new `diab` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The first model is just an intercept, the second adds `age` as a predictor,
    the third has `age` and `frame`, and the fourth includes the interaction. Nesting
    is evident, and you can now compare the significance of the improvements in goodness-of-fit
    as you increase the complexity of the model at each step.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you hadn’t deleted the records containing missing values in those predictors,
    you would’ve received an error telling you that the data sets for the four models
    were not equal sizes.
  prefs: []
  type: TYPE_NORMAL
- en: The results themselves suggest that including `age` provides a significant improvement
    to modeling `chol`; including a main effect for `frame` provides a further mild
    improvement; and there’s very weak evidence, if any, that including an interactive
    effect is beneficial to goodness-of-fit. From this, you might prefer to use `dia.mod3`,
    the main-effects-only model, as the most parsimonious representation of mean cholesterol
    out of these four models.
  prefs: []
  type: TYPE_NORMAL
- en: '***22.2.2 Forward Selection***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Partial *F*-tests are a natural way to investigate nested models but can be
    difficult to manage if you have many different models to fit when, for example,
    you have many predictor variables.
  prefs: []
  type: TYPE_NORMAL
- en: This is where *forward selection* (also referred to as *forward elimination*)
    comes in. The idea is to start with an intercept-only model and then perform a
    series of independent tests to determine which of your predictor variables significantly
    improves the goodness-of-fit. Then you update your model object by adding that
    term and execute the series of tests again for all remaining terms to determine
    which of those would further improve the fit. The process repeats until there
    aren’t any more terms that improve the fit in a statistically significant way.
    The ready-to-use R functions `add1` and `update` perform the series of tests and
    update your fitted regression model.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll use the `nuclear` data frame in the `boot` library from [Exercise 21.1](ch21.xhtml#ch21exc1)
    on [page 499](ch21.xhtml#page_499) and [Section 21.5.5](ch21.xhtml#ch21lev2sec206)
    as an example. The goal is to choose the most informative model for prediction
    of construction cost. Load `boot` and access the help file `?nuclear` to remind
    yourself of the variable definitions. First fit the model for construction cost
    with an overall intercept term only.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You know from earlier exploits that this particular model is rather inadequate
    for the reliable prediction of `cost`. So, consider the following line of code
    to start the forward selection (I’ve suppressed the output, which I’ll show separately
    and discuss in a moment):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The first argument to `add1` is always the model you’re aiming to update. The
    second argument, `scope`, is critical—you must supply a formula object defining
    the “fullest,” most complex model you’d consider fitting. For this you would typically
    use the `.~.` notation, in which the dots refer to the definition of the model
    in the first argument. Specifically, the dots stand for “what is already there.”
    In other words, through `scope` you’re telling `add1` that the fullest model you’d
    consider has `cost` as the response, an intercept, and main effects of all other
    predictors in the `nuclear` data frame (I’ll restrict the full model to main effects
    only for ease of demonstration). You don’t need to supply the data frame as an
    argument since those data are contained within the model object in the first argument.
    Lastly, you tell `add1` the test to perform. There are a handful of variants available
    (see `?add1`), but here you’ll stick with `test="F"` for partial *F*-tests.
  prefs: []
  type: TYPE_NORMAL
- en: Now, focus on the output that’s provided directly after the execution of `add1`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The output comprises a series of rows, starting with `<none>` (doing nothing
    to the current model). You receive the `Sum of Sq` and `RSS` values, directly
    related to calculating the test statistic. The differences in degrees of freedom
    are also reported. Another measure of parsimony, `AIC`, is also provided (you’ll
    look at that in more detail in [Section 22.2.4](ch22.xhtml#ch22lev2sec212)).
  prefs: []
  type: TYPE_NORMAL
- en: Most relevant are the test outcomes; with `test="F"`, each row corresponds to
    an independent partial *F*-test comparing the model in the first argument, as
    *ŷ*[redu], with the model that results from having added that row term only as
    *ŷ*[full]. Usually, therefore, you would update your model by adding only the
    term with the largest (and “most significant”) improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Here, you should be able to see that adding `date` as a predictor offers the
    largest significant improvement to modeling `cost`. So, let’s update `nuc.0` to
    include that term with the code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In `update` you provide the model you want to update as the first argument,
    and the second argument, `formula`, tells `update` how to update the model. Again
    using the `.~.` notation, the instruction is to update `nuc.0` by adding `date`
    as a predictor, resulting in a fitted model object of the same class of the first
    argument. Call a `summary` of the new model, `nuc.1`, to see this.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s keep going! Call `add1` again, but now pass `nuc.1` as your first
    argument.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that there’s now no row for adding in `date`; it’s already there in `nuc.1`.
    It seems the next most informative addition would be `cap`. Update `nuc.1` to
    that effect.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now keep going, testing, and updating. By calling `add1` on `nuc.2` (output
    not shown here), you’ll find that the next most significant addition is `pt` (by
    a small margin). Update to a new object named `nuc.3`, which includes the following
    term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Then test again, using `add1` on `nuc.3`. You’ll find weak evidence to additionally
    include a main effect for `ne`, so update with that inclusion to create `nuc.4`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: At this point, you may be reasonably certain there won’t be any more useful
    additions, but check with one final call to `add1` on the latest fit to be thorough.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Indeed it appears that none of the remaining covariates, if included in the
    model, would yield a statistically significant improvement in goodness-of-fit,
    so your final model will stay at `nuc.4`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This method may seem a little cumbersome, and it’s sometimes difficult to decide
    on the fullest model to be used as the `scope`, but it’s a remarkably good way
    to stay involved at every stage of the selection process so you can consider each
    addition carefully. Note, however, that there’s an element of subjectivity; it’s
    possible to arrive at different final models by choosing one addition over another,
    such as if you’d added `pt` instead of `date` (they had similar levels of significance
    in the very first call to `add1`).
  prefs: []
  type: TYPE_NORMAL
- en: '***22.2.3 Backward Selection***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After learning forward selection, understanding *backward selection* (or *elimination*)
    isn’t much of a stretch. As you might have guessed, where forward selection starts
    from a reduced model and works its way up to a final model by adding terms, backward
    selection starts with your fullest model and systematically drops terms. The R
    functions for this process are `drop1` to inspect the partial *F*-tests and `update`.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of forward versus backward model selection is usually made on a case-by-case
    basis. If your fullest model isn’t known or is difficult to define and fit, then
    forward selection is typically preferred. On the other hand, if you do have a
    natural and easily fitted fullest model, then backward selection can be more convenient
    to implement. Sometimes, researchers will perform both to see whether the final
    model they arrive at is different (a perfectly possible occurrence).
  prefs: []
  type: TYPE_NORMAL
- en: Revisit the `nuclear` example. First, define the fullest model as that which
    predicts `cost` by main effects of all available covariates (as you did in your
    use of `scope` in the forward selections).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: There are clearly several predictors that appear not to contribute significantly
    to the response, and these same results are evident the first time you use `drop1`
    to examine the impact on goodness-of-fit that would occur from dropping each variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: One handy feature of `drop1` is that its `scope` argument is optional. If you
    don’t include `scope`, it defaults to the intercept-only model as the “most-reduced”
    model, which is usually a reasonable choice.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving right into the deletion process, remind yourself of the interpretation
    of what you’re doing. Just as adding any term will always improve the goodness-of-fit
    in forward selection, deleting any term in backward selection will always worsen
    the goodness-of-fit. The real question is the perceived significance of these
    changes in fit quality. In the same way as earlier, where you wanted to add only
    those terms that offer a *statistically significant improvement* in goodness-of-fit,
    when dropping terms, you only want to remove those that *do not* result in a statistically
    significant *detriment* to goodness-of-fit. As such, backward selection is the
    complete reverse of forward selection in the way it’s carried out.
  prefs: []
  type: TYPE_NORMAL
- en: So, from the output of `drop1`, you want to choose the term to remove from the
    model that has the least significant effect of reducing the goodness of the fit.
    In other words, you’re looking for the term with the largest, nonsignificant *p*-value
    for its partial *F*-test—because dropping a term with a significantly small *p*-value
    would significantly worsen the predictive capability of the regression model.
  prefs: []
  type: TYPE_NORMAL
- en: In the current example, it seems the predictor `bw` has the single least significant
    effect on reducing the goodness-of-fit, so let’s start the update by removing
    that term from `nuc.0`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Use of `update` in this selection algorithm is the same as before; now, though,
    you use a `-` to signify the deletion of a term following the standard “what’s
    already there” `.~.` notation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process is then repeated using the latest model `nuc.1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: It would seem that `pt` is the next most sensible main effect to drop. Do so
    and name the resulting object `nuc.2`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now keep going, rechecking with a call to `drop1` (not shown), and you’ll find
    that the predictor `t1` reveals itself as another viable deletion. Update your
    model with that predictor deleted; name the model object `nuc.3`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Recheck the new `nuc.3` with `drop1`. You should now find the effect of `ct`
    remains nonsignificant, so delete that and update again, giving you a new `nuc.4`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Perform yet another check with `drop1`, this time on `nuc.4`. At this point,
    you might hesitate in removing any more predictors, with significance at varying
    strengths being associated with the effect of their deletion. Note, however, that
    for at least three of the remaining predictors, `t2`, `pr`, and `cum.n`, the statistical
    significance should probably be considered borderline at best—all of their *p*-values
    lie between the conventional cutoff levels of *β* = 0.01 and *β* = 0.05\. This
    again emphasizes the active role a researcher must play in model selection algorithms
    such as forward or backward elimination; whether you should delete any more variables
    from here is a difficult question to answer and is left up to your judgment.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s remain with `nuc.4` as the final model. Summarizing, you’re able to see
    the estimated regression parameters and the usual post-fit statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Immediately, you can see that your final model from forward selection in [Section
    22.2.2](ch22.xhtml#ch22lev2sec210) is different from the final model selected
    here, despite the fullest model being the same for both. How has that occurred?
  prefs: []
  type: TYPE_NORMAL
- en: The answer, simply put, is that the predictors present in a model affect each
    other. Remember that the estimated coefficients of present predictors easily change
    in value as you control for different variables. As the number of predictor terms
    increases, these relationships become more and more complex, so both the order
    and direction of the selection algorithm have the potential to lead you on different
    paths through the selection process and arrive at different final destinations,
    which is exactly what’s happened here.
  prefs: []
  type: TYPE_NORMAL
- en: As a perfect example of this, consider the main effect of `pt` in the `nuclear`
    data. In forward selection, `pt` was added because it offered the “most significant”
    improvement to the model `cost~date+cap`. In backward selection, `pt` was removed
    early, since it offered the least reduction in goodness-of-fit if taken from the
    model `cost~date+t1+t2+cap+pr+ne+ct+cum.n+pt`. What this means is that for the
    latter model, the contribution that `pt` might make in terms of predicting the
    outcome is already explained by the other present predictor terms. In the smaller
    model, that effect had not yet been explained, and so `pt` was an attractive addition.
  prefs: []
  type: TYPE_NORMAL
- en: All this serves to highlight the fickle nature of most selection algorithms,
    in spite of the systematic way they’re implemented. It’s important to acknowledge
    that a final model fit will probably vary between approaches and that you should
    view these selection methods more as helpful guidelines for finding the most parsimonious
    model and not as providing a universal, definitive solution.
  prefs: []
  type: TYPE_NORMAL
- en: '***22.2.4 Stepwise AIC Selection***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The application of a series of partial *F*-tests is the most common *test-based*
    model selection method, but it’s not the only tool a researcher has at their disposal.
    You can also locate parsimony by adopting a *criterion-based* approach. One of
    the most famous criterion measures is known as *Akaike’s* *Information Criterion
    (AIC)*. You’ll have noticed this value as one of the columns in the output of
    `add1` and `drop1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a given linear model, AIC is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e22-3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![image](../images/l.jpg) is a measure of goodness-of-fit named the *log-likelihood*,
    and *p* is the number of regression parameters in the model, excluding the overall
    intercept. The value of ![image](../images/l.jpg) is a direct outcome of the estimation
    procedure used to fit the model, though its exact calculation is beyond the scope
    of this text. The thing to know is that it takes on larger values for better-fitting
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation (22.3)](ch22.xhtml#ch22eq3) produces a value that rewards goodness-of-fit
    with the ![image](../images/f0542-01.jpg) but simultaneously penalizes complexity
    with the 2 × (*p* + 2). The negative sign associated with ![image](../images/l.jpg)
    coupled with the positive sign of the *p* + 2 means that smaller values of AIC
    refer to more parsimonious models.'
  prefs: []
  type: TYPE_NORMAL
- en: To find the AIC for a fitted linear model, you use the `AIC` or `extractAIC`
    functions on the object resulting from `lm`; take a look at the help files of
    these functions to see the technical differences between the two. The value of
    ![image](../images/l.jpg) (and therefore also the AIC) has no direct interpretation
    and is useful only when you compare it against the AIC of another model. You can
    base model selection on the AIC by identifying the fit with the lowest AIC value.
    This is the reason it’s directly reported in the output of `add1` and `drop1`—you
    could decide on which term to add or drop based on the change that results in
    a shift to the smallest AIC, instead of focusing exclusively on the *significance*
    of the change via the *F*-test.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go even further and combine the ideas of forward and backward selection.
    *Stepwise* model selection allows the option to either delete a present term *or*
    add a missing term and is typically implemented with respect to AIC. That is,
    a term is chosen to be added or deleted based on the one move out of all possible
    moves that yields the single biggest reduction in AIC. This affords you more flexibility
    in exploring candidate models on your way to the final model fit—determined as
    the model from which no addition or deletion would reduce the AIC value further.
  prefs: []
  type: TYPE_NORMAL
- en: It’s possible to implement stepwise AIC selection yourself using either `add1`
    or `drop1` at each stage, but fortunately R provides the built-in `step` function
    to do it for you. Take the `mtcars` data from the `MASS` package from the past
    couple of chapters. Let’s finally try to obtain a model for mean mileage that
    offers the opportunity to include every predictor that’s available.
  prefs: []
  type: TYPE_NORMAL
- en: First, take a look at the documentation in `?mtcars` and a scatterplot matrix
    of the data again to remind yourself of the variables and their format in the
    R data frame object. Then define the starting model (often called the *null* model)
    as the intercept-only model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Your starting model can be anything you like, provided it falls within the domain
    of the models described by your `scope` argument to be supplied to `step`. In
    this example, define `scope` as the fullest model to be considered—set this to
    be the overly complex model with a four-way interaction among `wt`, `hp`, `cyl`,
    and `disp` (and all relevant lower-order interactions and main effects, via the
    cross-factor operator), as well as main effects for `am`, `gear`, `drat`, `vs`,
    `qsec`, and `carb`. The two multilevel categorical variables, `cyl` and `gear`,
    are explicitly converted to factors to avoid them being treated as numeric (refer
    to [Section 20.5.4](ch20.xhtml#ch20lev2sec188)).
  prefs: []
  type: TYPE_NORMAL
- en: The potential for interactions in the final model will serve to highlight an
    especially important (and convenient) feature of `add1`, `drop1`, and `step`.
    These functions all respect the hierarchy imposed by interactions and main effects.
    That is, for `add1` (and `step`), an interactive term will not be provided as
    an option for addition unless all relevant lower-order effects are already present
    in the current fitted model; similarly, for `drop1` (and `step`), an interactive
    term or main effect will not be provided as an option for deletion unless all
    relevant higher-order effects are already gone from the current fitted model.
  prefs: []
  type: TYPE_NORMAL
- en: The `step` function itself returns a fitted model object and by default provides
    a comprehensive report of each stage of selection. Let’s call it now; for print
    reasons, some of the output has been snipped out, so you’re encouraged to bring
    this up on your own machine.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Each block of output displays the current model fit, its AIC value, and a table
    showing the possible moves (either adding `+`, deleting `-`, or doing nothing
    `<none>`). The AIC value that would result from each move alone is listed, and
    these potential single moves are ranked from smallest to largest AIC value.
  prefs: []
  type: TYPE_NORMAL
- en: As the algorithm proceeds, you see the `<none>` row creeping its way up the
    table. For example, in the first table, the value of the AIC for the intercept-only
    model is 115.94\. The biggest reduction in AIC would result from adding a main
    effect for `wt`; that move is made, and the effect of subsequent moves on the
    AIC is reassessed. Also note that the addition of the two-way interaction term
    between `wt` and `factor(cyl)` is considered only at the third step, after the
    main effects of those predictors have been added. That particular two-way interaction
    never ends up being included, though, because the main effect of `hp` is preferable
    at that third step, and subsequent interactions involving `hp` then offer a much
    better reduction in the AIC value in the fourth step. In fact, at the fifth step,
    actually *deleting* the main effect for `factor(cyl)` is deemed to reduce the
    AIC most, and so the tables for the sixth and seventh steps no longer include
    that `wt:factor(cyl)` term as an option. The sixth step suggests that adding the
    main effect for `qsec` offers a minor reduction in the AIC, so this is done. The
    seventh table signals the end of the algorithm because doing nothing offers the
    lowest AIC value and doing anything else would increase the AIC (shown through
    `<none>` taking pole position in that last table).
  prefs: []
  type: TYPE_NORMAL
- en: The final model is stored as the object `car.step`; by summarizing it, you’ll
    note that almost 90 percent of the variation in the response is explained by weight,
    horsepower, and their interaction, as well as the slightly curious main effect
    of `qsec` (which itself is not deemed statistically significant).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: From this, it seems it would be worthwhile to investigate this predictor further,
    to establish validity of the fitted model (see [Section 22.3](ch22.xhtml#ch22lev1sec74)),
    and perhaps to try transformations of the data (such as modeling GPM instead of
    MPG; see [Section 21.4.3](ch21.xhtml#ch21lev2sec201)) to see whether this effect
    persists in subsequent runs of the stepwise AIC algorithm. The presence of `qsec`
    in the final model illustrates the fact that the selection of the model wasn’t
    based solely on the significance of predictor contribution but on a criterion-based
    measure aiming for its own definition of parsimony.
  prefs: []
  type: TYPE_NORMAL
- en: The AIC is sometimes criticized for a tendency to err on the side of more complexity
    and higher *p*-values. To balance this, you can increase the penalizing effect
    of extra predictors by increasing the multiplicative contribution of the (*p*
    + 2) on the right of [Equation (22.3)](ch22.xhtml#ch22eq3); though the standard
    multiplicative factor of 2 is used in the majority of cases (in `step` you can
    use the optional argument `k` to change this). That being said, criterion-based
    measures are incredibly useful when you have models that aren’t nested (ruling
    out the partial *F*-test) and you want to compare them for quick identification
    of the one that, potentially, provides the most parsimonious representation of
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 22.1**'
  prefs: []
  type: TYPE_NORMAL
- en: In [Sections 22.2.2](ch22.xhtml#ch22lev2sec210) and [22.2.3](ch22.xhtml#ch22lev2sec211),
    you used forward and backward selection approaches to find a model for predicting
    the cost of the construction of nuclear power plants (based on the `nuclear` data
    frame in the `boot` package).
  prefs: []
  type: TYPE_NORMAL
- en: Using the same fullest model (in other words, main effects of all present predictors
    only), use stepwise AIC selection to find a suitable model for the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the final model found in (a) match either of the models resulting from
    the earlier use of forward and backward selection? How does it differ?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 21.2](ch21.xhtml#ch21exc2) on [page 512](ch21.xhtml#page_512) detailed
    Galileo’s ball data. Enter these as a data frame in your current R workspace if
    you haven’t already.'
  prefs: []
  type: TYPE_NORMAL
- en: Fit five linear models to these data with distance as the response—an intercept-only
    model and four separate polynomial models of increasing order 1 to 4 in height.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a table of partial *F*-tests to identify your favored model for distance
    traveled. Is your selection consistent with [Exercise 21.2](ch21.xhtml#ch21exc2)
    (b) and (c)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You first encountered the `diabetes` data frame in the contributed `faraway`
    package in [Section 21.5.2](ch21.xhtml#ch21lev2sec203), where you modeled the
    mean total cholesterol. Load the package and inspect the documentation in `?diabetes`
    to refresh your memory of the data set.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some missing values in `diabetes` that might interfere with model
    selection algorithms. Define a new version of the `diabetes` data frame that deletes
    all rows with a missing value in any of the following variables: `chol`, `age`,
    `gender`, `height`, `weight`, `frame`, `waist`, `hip`, `location`. Hint: Use `na.omit`
    or your knowledge of record extraction or deletion for a data frame. You can create
    the required vector of row numbers to be extracted or deleted using `which` and
    `is.na`, or you can try using the `complete.cases` function to obtain a logical
    flag vector—inspect its help file for details.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your data frame from (e) to fit two linear models with `chol` as the response.
    The null model object, named `dia.null`, should be an intercept-only model. The
    full model object, named `dia.full`, should be the overly complex model with a
    four-way interaction (and all lower-order terms) among `age`, `gender`, `weight`,
    and `frame`; a three-way interaction (and all lower-order terms) among `waist`,
    `height`, and `hip`; and a main effect for `location`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Starting from `dia.null` and using the same terms as in `dia.full` for `scope`,
    implement stepwise selection by AIC to choose a model for mean total cholesterol
    and then summarize.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use forward selection based on partial *F*-tests with a conventional significance
    level of *α* = 0.05 to choose a model, again starting from `dia.null`. Is the
    result here the same as the model arrived at in (g)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stepwise selection doesn’t have to start from the simplest model. Repeat (g),
    but this time, set `dia.full` to be the starting model (you don’t need to supply
    anything to `scope` if you’re starting from the most complex model). What is the
    final model selected via AIC if you start from `dia.full`? Is it different than
    the final model from (g)? Why is this or is this not the case, do you think?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Revisit the ubiquitous `mtcars` data frame from the `MASS` package.
  prefs: []
  type: TYPE_NORMAL
- en: In [Section 22.2.4](ch22.xhtml#ch22lev2sec212), you used stepwise AIC selection
    to model mean MPG. The selected model included a main effect for `qsec`. Rerun
    the same AIC selection process, but this time, do it in terms of GPM=1/MPG. Does
    this change the complexity of the final model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '***22.2.5 Other Selection Algorithms***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Any model selection algorithm will always aim to quantitatively define *parsimony*
    and suggest a model that optimizes that definition in light of the available data.
    There are alternatives to AIC, such as the *corrected AIC (AIC[c])* or the *Bayesian
    Information Criterion (BIC)*, both of which impose heavier penalties on complexity
    than the default AIC in (22.3).
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes it’s tempting to simply monitor *R*², the coefficient of determination,
    for a series of models. However, as mentioned in [Section 22.2.1](ch22.xhtml#ch22lev2sec209),
    this on its own is inadequate for choosing between models since it doesn’t penalize
    complexity and will generally always increase as you continue to add predictors,
    whether they have a statistically significant impact or not. The *adjusted R*²
    *statistic*, denoted ![image](../images/r2.jpg) and reported as `Adjusted R-squared`
    in `summary`, is a simple transformation of the original *R*² that does incorporate
    a penalty for complexity relative to the sample size *n*; calculated as
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0548-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *p* is the number of predictor terms (excluding the intercept). The algorithms
    based on tests and criteria are always preferable (since interpretation of ![image](../images/r2.jpg)
    can be difficult), but monitoring ![image](../images/r2.jpg) can be useful as
    a quick check between nested models—a higher value points to a preferred model.
  prefs: []
  type: TYPE_NORMAL
- en: For further reading, [Chapter 8](ch08.xhtml#ch08) of Faraway ([2005](ref.xhtml#ref21))
    provides some excellent commentary on the guideline-only nature of both test-
    and criterion-based model selection procedures. Regardless of which approach you
    employ, always remember that any final model reached by using these algorithms
    should still be subject to scrutiny.
  prefs: []
  type: TYPE_NORMAL
- en: '**22.3 Residual Diagnostics**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In previous chapters, you examined the practical aspects of multiple linear
    regression models, such as fitting and interpreting, dummy coding, transforming,
    and so on, but you haven’t yet looked at methods that are essential for determining
    the *validity* of your model. The final part of this chapter will introduce you
    to *model diagnostics*, the primary goal of which is to ensure that your regression
    model is valid and accurately represents the relationships in your data. For this,
    I’ll return focus to the theoretical assumptions underpinning the multiple linear
    regression model that were noted early in [Section 21.2.1](ch21.xhtml#ch21lev2sec190).
  prefs: []
  type: TYPE_NORMAL
- en: 'As a refresher, in general when fitting these models, remember to keep these
    four things in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Errors** The error term *ɛ*, which defines the departure of any observation
    from the fitted mean-outcome model, is assumed to be normally distributed with
    a mean of zero and a constant variance denoted with *σ*². The error associated
    with a given observation is also assumed to be independent of the error of any
    other observation. If a fitted model suggests violation of any of these assumptions,
    you’ll need to investigate further (usually involving refitting a variation of
    the model).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linearity** It is critical to be able to assume that the mean response as
    a function is linear in terms of the regression parameters *β*[0], *β*[1], . .
    ., *β*[*p*]. Though transformations of individual variables and the presence of
    interactions can relax the specific nature of the estimated trends somewhat, any
    diagnostic suggestion that a relationship is nonlinear (and hence not being captured
    by the fitted model at hand) must be investigated.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extreme or unusual observations** Always inspect extreme data points or data
    points that strongly influence the fitted model—for example, points that have
    been recorded incorrectly should be removed from the analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collinearity** Predictors highly correlated with one another can adversely
    affect an entire model, meaning it can be easy to misinterpret the effects of
    any included predictors. This should be avoided in any regression.'
  prefs: []
  type: TYPE_NORMAL
- en: You investigate the first three after fitting the model using diagnostic tools.
    Any violation of these assumptions diminishes the reliability of your model, sometimes
    severely. Collinearity and/or extreme observations can be discovered by basic
    statistical explorations (for example, viewing scatterplot matrices) of the raw
    data pre-fit, but any consequential effects are appraised post-fit.
  prefs: []
  type: TYPE_NORMAL
- en: There are some statistical tests you can perform to diagnose a statistical model,
    but commonly a diagnostic inspection boils down to interpretation of the results
    of graphical tools designed to target specific assumptions. Interpreting these
    plots can be quite difficult and only really becomes easier with experience. Here,
    I’ll provide an overview of these tools in R and describe some common things to
    look for. For a more detailed discussion, look to dedicated texts on regression
    methods such as Chatterjee et al. ([2000](ref.xhtml#ref12)), Faraway ([2005](ref.xhtml#ref21)),
    or Montgomery et al. ([2012](ref.xhtml#ref48)).
  prefs: []
  type: TYPE_NORMAL
- en: '***22.3.1 Inspecting and Interpreting Residuals***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you look back at the plot in [Figure 20-2](ch20.xhtml#ch20fig2) on [page
    456](ch20.xhtml#page_456), you’ll see a good demonstration of the importance of
    interpreting the results given by *ŷ* as a mean response value. Under the assumed
    model, any deviation of the raw observations from the fitted line is deemed to
    be the result of the (normally distributed) errors defined by the *ɛ* term in
    [Equation (20.1)](ch20.xhtml#ch20eq1) on [page 453](ch20.xhtml#page_453).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, in practice, you don’t have the true error values because you don’t
    know the true model of your data. For the *i*th response observation *y*[*i*]
    and its fitted value from the model, *ŷ*[*i*], you would typically assess diagnostic
    plots using the estimated residuals *e*[*i*] = *y*[*i*] − *ŷ*[*i*]. A call to
    `summary` even encourages a post-fit analysis of the residuals by providing you
    with a five-number summary of the *e*[*i*] above the table of estimated coefficients.
    This allows you to take a look at their values and do a preliminary numeric assessment
    of the symmetry of their distribution (as required by the assumption of normality—see
    [Section 22.3.2](ch22.xhtml#ch22lev2sec215)).
  prefs: []
  type: TYPE_NORMAL
- en: As well as a diagnostic inspection of the raw residuals *e*[*i*], some diagnostic
    checks can also be done using their *standardized* (or *Studentized*) values.
    The standardized residuals rescale the raw residuals *e*[*i*] to ensure they all
    have the same variance, which is important if you need to directly compare them
    to one another. Formally, this is achieved with the calculation ![image](../images/f0550-01.jpg),
    where ![image](../images/o.jpg) is the estimate of the residual standard error
    and *h*[*ii*] is the *leverage* of the *i*th observation (you’ll learn about leverage
    in [Section 22.3.4](ch22.xhtml#ch22lev2sec217)).
  prefs: []
  type: TYPE_NORMAL
- en: Arguably the most common graphical tool used for a post-fit analysis of the
    residuals is a simple scatterplot of the “observed-minus-fitted” raw residuals
    on the vertical axis against their corresponding fitted-model values from the
    regression. If the assumptions concerning *ɛ* are valid, then the *e*[*i*] should
    appear randomly scattered around zero (since the errors aren’t assumed to be related
    in any way to the value of the response). Any systematic pattern in the plot suggests
    the residuals don’t agree with the error assumptions—this could be because of
    nonlinear relationships in your data or the presence of dependent observations
    (in other words, your data points are correlated and therefore not independent
    of one another). The plot can also be used to detect *heteroscedasticity*—a nonconstant
    variance in the residuals—commonly seen as a “fanning out” of the residuals around
    0 as the fitted values increase.
  prefs: []
  type: TYPE_NORMAL
- en: Note once more that it’s important these theoretical assumptions are valid because
    they affect the validity of the estimates of the regression coefficients and the
    reliability of their standard errors (and so statistical significance)—in other
    words, the correctness of your interpretation of their impact on the response.
  prefs: []
  type: TYPE_NORMAL
- en: To give you a better idea of all this, consider the three images in [Figure
    22-1](ch22.xhtml#ch22fig1). These provide residuals versus fitted values plots
    for three hypothetical scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: The plot on the left is, more or less, what you’re looking for—the residuals
    appear randomly scattered around zero, and their spread around zero appears constant
    (*homoscedasticity*). In the middle plot, however, you can see systematic behavior
    in the residuals. Though the variability still seems to remain constant throughout
    the range of fitted values, the apparent trend suggests the current model isn’t
    explaining some of the relationship between response and predictor (or predictors).
    On the right, the residuals seem to be scattered randomly about zero again. However,
    the variability they exhibit isn’t constant. Among other things, this kind of
    heteroscedasticity will affect the reliability of your confidence and prediction
    intervals.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f22-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-1: Three impressions of a hypothetical residuals versus fitted diagnostic
    plot from a linear regression: random (left), systematic (middle), and heteroscedastic
    (right)*'
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to know that even if your graphical diagnostics don’t provide
    the well-behaved plot like the hypothetical example on the left of [Figure 22-1](ch22.xhtml#ch22fig1),
    this is not a reason to immediately give up on the analysis. These plots can form
    an integral part in finding an appropriate model for your data. You can often
    reduce nonlinearity by including additional predictors or interactions, changing
    the treatment of a categorical variable, or performing nonlinear transformations
    of certain continuous predictors. Heteroscedasticity, especially the kind in [Figure
    22-1](ch22.xhtml#ch22fig1) where the variability is higher for higher fitted values,
    is common in some fields of research. A first step to remedy this problem often
    involves a simple log transformation of the response followed by a reinspection
    of the diagnostics.
  prefs: []
  type: TYPE_NORMAL
- en: It’s time for an example. In [Section 22.2.4](ch22.xhtml#ch22lev2sec212), you
    used stepwise AIC selection to choose a model for MPG for the `mtcars` data, creating
    the object `car.step`. Let’s now diagnose that same fit to see whether there are
    any problems with the assumptions of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you apply the `plot` function directly to an `lm` object, it can conveniently
    produce six types of diagnostic plot of the fit. By default, four of these plots
    are produced in succession. Follow the signal to the user in the console, Hit
    <Return> to see next plot, to progress through them. In the examples that follow,
    however, you’ll select each plot individually using the optional `which` argument
    (specified by the integers `1` through `6`; see `?plot.lm` for the documentation).
    The residuals versus fitted plot is given with `which=1`; the following line produces
    the plot on the left in [Figure 22-2](ch22.xhtml#ch22fig2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, R adds a smoothed line to help the user interpret any trend,
    though this shouldn’t be used exclusively in any judgment. By default, the three
    most extreme points from zero are annotated (according to the `rownames` attribute
    of the data frame used in the call that fitted the model). The model formula itself
    is specified below the horizontal axis label.
  prefs: []
  type: TYPE_NORMAL
- en: From this, you see that the residuals versus fitted plot for the `car.step`
    offers little, if any, cause for concern. There isn’t much of a discernible trend,
    and you can take further comfort in the fact that the errors (*e*[*i*]) appear
    homoscedastic in their distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f22-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-2: Residuals versus fitted and scale-location diagnostic plots for
    the* `car.step` *model*'
  prefs: []
  type: TYPE_NORMAL
- en: The *scale-location* plot is similar to the residuals versus fitted plot, though
    instead of the raw *e*[*i*] on the vertical axis, the scale-location plot provides
    ![image](../images/f0552-01.jpg), that is, the square root of the absolute value
    (denoted by | · |; this renders all negative values positive) of the standardized
    residuals. These are plotted against the respective fitted values on the horizontal
    axis. By restricting attention to the magnitude of each residual in this way,
    the scale-location plot is used to reveal trends in the size of the departure
    of each data point from its fitted value, as the fitted values increase. This
    means such a plot can, for example, be more useful than the raw residuals versus
    fitted plot in detecting things such as heteroscedasticity. Just as with the original
    residuals versus fitted plot, you’re looking for a plot with no discernible pattern
    as an indication that no error assumptions have been violated.
  prefs: []
  type: TYPE_NORMAL
- en: The right plot of [Figure 22-2](ch22.xhtml#ch22fig2) shows the scale-location
    plot for `car.step`, selected with `which=3`. This plot also demonstrates the
    ability to remove the default smoothed trend line with the `add.smooth` argument
    and to control how many extreme points are labeled using the `id.n` argument.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As with the original residuals versus fitted plot, there doesn’t seem to be
    much to be concerned about in the scale-location plot for this `mtcars` model.
  prefs: []
  type: TYPE_NORMAL
- en: Return to Galileo’s ball-rolling data first laid out in [Exercise 21.2](ch21.xhtml#ch21exc2)
    on [page 512](ch21.xhtml#page_512). In their use in the following example, the
    response variable “distance traveled” is given as column `d`, and the explanatory
    variable “height” is column `h`, in the data frame `gal`. I’ll re-create some
    of the exercise to give you a couple of straightforward examples of cause for
    concern in residual diagnostic plots. Execute the following code to define the
    data frame of the seven observations and fit two regression models—the first linear
    in height and the second quadratic (refer to [Section 21.4.1](ch21.xhtml#ch21lev2sec199)
    for details on polynomial transformations).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, take a look at the three images in [Figure 22-3](ch22.xhtml#ch22fig3),
    created with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/f22-03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-3: Demonstrating residual diagnostics for Galileo’s ball-rolling
    data. Top left: The raw data with a simple linear trend corresponding to* `gal.mod1`
    *superimposed. Top right: Residuals versus fitted for the linear-trend-only model.
    Bottom: Residuals versus fitted for the quadratic model* `gal.mod2`.'
  prefs: []
  type: TYPE_NORMAL
- en: The top-left plot shows the data and provides the straight line of the simple
    linear model. Although this clearly captures the increasing trend, this plot suggests
    some curvature is also present. The diagnostic residuals versus fitted plot (top-right)
    shows that the linear-trend-only model is inadequate—the systematic pattern throws
    up a red flag concerning the assumptions surrounding the linear model errors.
    The bottom image shows the residuals versus fitted plot based on the quadratic
    version of the model in `gal.mod2`. Including a quadratic term in “height” removes
    this prominent curve in the residuals. However, these latest *e*[*i*] values still
    seem to exhibit systematic behavior in a wavelike form, perhaps suggesting you
    try a cubic model, which is difficult with such a small sample size.
  prefs: []
  type: TYPE_NORMAL
- en: '***22.3.2 Assessing Normality***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To assess the assumption that the error is normally distributed, you can use
    a normal QQ plot, as first discussed in [Section 16.2.2](ch16.xhtml#ch16lev2sec142).
    You select `which=2` when calling `plot` on an `lm` object to produce a normal
    quantile-quantile plot of the (standardized) residuals. Return to the `car.step`
    model object and enter the following line to produce [Figure 22-4](ch22.xhtml#ch22fig4).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/f22-04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-4: Normal QQ plot of the residuals from the* `car.step` *model*'
  prefs: []
  type: TYPE_NORMAL
- en: You interpret the QQ plot of the residuals in the same way as in [Section 16.2.2](ch16.xhtml#ch16lev2sec142).
    The gray diagonal line represents the true normal quantiles, and the plotted points
    are the corresponding numeric quantiles of the estimated regression errors. Normally
    distributed data should lie close to the straight line.
  prefs: []
  type: TYPE_NORMAL
- en: For the `car.step` regression model, the points generally seem to follow the
    path laid out by the theoretical normal quantiles. There is some deviation, which
    is to be expected, but no apparent major departure from normality.
  prefs: []
  type: TYPE_NORMAL
- en: There are also other ways to test for normality, such as the famous Shapiro-Wilk
    hypothesis test. The null hypothesis for the Shapiro-Wilk test is that the data
    are normally distributed, so a small *p*-value would suggest non-normality of
    your data (see [Royston, 1982](ref.xhtml#ref55), for technical details). To execute
    the procedure, use the `shapiro.test` function in R. By first extracting the standardized
    residuals of your fitted model with `rstandard`, you’ll see that this test applied
    to `car.step` offers up a large *p*-value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In other words, there’s no evidence (according to this test) that the residuals
    of `car.step` aren’t normal.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to assume normality of the error term supports the methodology used
    to produce reliable estimates of the regression coefficients. As long as your
    data are *approximately* normal, though, you shouldn’t be too concerned with mild
    indications of non-normality. Some transformations of your data, and an increase
    in your sample size, can reduce concerns about more severe indications of non-normal
    residuals.
  prefs: []
  type: TYPE_NORMAL
- en: '***22.3.3 Illustrating Outliers, Leverage, and Influence***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It’s always important to investigate any individual observations that appear
    unusual or extreme compared to the bulk of your observations. In general, an exploratory
    analysis of your data, perhaps involving summary statistics or scatterplot matrices,
    is a good idea since it can help you identify any such values—they have the potential
    to adversely affect your model fits. Before going further, it’s important to clarify
    some frequently used terms.
  prefs: []
  type: TYPE_NORMAL
- en: '**Outlier** This is a general term used to describe an unusual observation
    in the context of the data, as you saw in [Section 13.2.6](ch13.xhtml#ch13lev2sec121).
    In linear regression, an outlier usually has a large residual but is identified
    as an outlier only if it doesn’t conform to the trend of the fitted model. An
    outlier can, but doesn’t always, significantly alter the trends described by the
    fitted model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Leverage** This term refers to the extremity of the values of the present
    predictors. A high-leverage point is an observation with predictor values extreme
    enough to potentially significantly affect the slopes or trends in the fitted
    model. An outlier can have a high or low leverage.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Influence** An observation with high leverage that *does* affect the estimated
    trends is deemed influential. In other words, influence is judged only when the
    response value is taken into account alongside the corresponding predictor values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These definitions have some overlap, so a given observation can be described
    using a combination of these terms. Let’s look at some hypothetical examples.
    Create the following two vectors of ten supposed responses (`y`) and explanatory
    (`x`) values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'These will form the bulk of the data. Now, consider the following six objects,
    `p1x` to `p3y`, which will be used to store the predictor and response values
    for three additional observation points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: That is, point 1 is (1.2,14); point 2 is (5,19); and point 3 is (5,5).
  prefs: []
  type: TYPE_NORMAL
- en: Next, the following four uses of `lm` provide four simple linear model fits.
    The first is the regression of `y` on `x` only. The next three additionally include
    points 1, 2, and 3, separately, as an 11th observation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can use these objects to visually clarify the definitions of *outlier*,
    *leverage*, and *influence*, as shown in [Figure 22-5](ch22.xhtml#ch22fig5). Enter
    the following code to initialize the scatterplot with set axis limits of `x` and
    `y`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then use `points`, `abline`, and `text` to build the top-left plot of [Figure
    22-5](ch22.xhtml#ch22fig5), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Create the middle and right plots by replacing `p1x`, `p1y`, and `mod.1` with
    those corresponding to points 2 and 3 and altering the `labels` argument in `text`.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f22-05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-5: Three examples of the definitions of outlier, leverage, and influence
    in a linear regression context. In each plot, the solid line represents the model
    fitted to the original observations in* `x` *and* `y`*, and the dashed line represents
    the model fitted including the extra point, plotted with* ▪.'
  prefs: []
  type: TYPE_NORMAL
- en: In the top-left plot of [Figure 22-5](ch22.xhtml#ch22fig5), the additional point
    is an example of an outlier since it sits away from the bulk of the data *and*
    doesn’t conform to the trend suggested by the original observations. Despite this,
    it’s considered to have low leverage only because of its predictor value of 1.2
    (`p1x`), which isn’t deemed unusual compared to that of the other values of `x`.
    In fact, its proximity to the overall mean of the `x` values indicates that the
    effect of including it, incorporated in `mod.1`, is mainly a modification to the
    original intercept. You could even classify this as a low influence point—the
    overall change to the fitted model seems minimal.
  prefs: []
  type: TYPE_NORMAL
- en: In the top-right plot, you can see an example of an observation that would *not*
    be considered an outlier. Although point 2 does sit apart from the 10 original
    observations, it conforms quite well to the model fitted to only `x` and `y`,
    which is important in the context of regression. That being said, point 2 is considered
    a high leverage point since it sits at an extreme predictor value compared to
    all other values of `x` (in other words, it has the *potential* to dramatically
    alter the fit should its response value be different). As it stands, it’s a low
    influence point since the model fit itself is barely affected by its inclusion.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the bottom plot shows a clear example of an outlier in a high-leverage
    position that also has a high influence—it sits away from the original 10 observations
    and isn’t a clear part of the original trend; its extreme predictor value means
    high leverage; and its inclusion substantially alters the entire model by dragging
    down the slope and raising the intercept. These ideas remain the same in higher
    dimensions (that is, when you have several predictors) for multiple linear regression
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '***22.3.4 Calculating Leverage***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Leverage itself is calculated using the design matrix structure ***X*** defined
    in [Section 21.2.2](ch21.xhtml#ch21lev2sec191). Specifically, if you have *n*
    observations, then the leverage of the *i*th point (*i* = 1, . . . , *n*) is denoted
    *h*[*ii*]. These are the diagonal elements (*i*th row, *i*th column) of the *n*
    × *n* matrix *H* such that
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e22-4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In R, constructing the design matrix for the 10 illustrative predictor observations
    you defined in [Section 22.3.3](ch22.xhtml#ch22lev2sec216) as `x` is achieved
    in a straightforward fashion using knowledge of `cbind` (refer to [Section 3.1.2](ch03.xhtml#ch03lev2sec25)).
    *H* is subsequently calculated using the corresponding functions for matrix multiplication
    (`%*%`), matrix transposition (`t`), matrix inversion (`solve`), and diagonal
    element extraction (`diag`). Then you can plot the values *h*[*ii*] against the
    values of `x` themselves. The following code produces [Figure 22-6](ch22.xhtml#ch22fig6):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: You would typically use the built-in R function `hatvalues`, named after the
    style of the matrix algebra in [Equation (22.4)](ch22.xhtml#ch22eq4), to obtain
    the leverages (rather than manually constructing the design matrix ***X*** and
    doing the math yourself). Simply provide `hatvalues` with your fitted model object.
    You can confirm your earlier calculations by using the corresponding `lm` object
    fitted to the `x` and `y` data (`mod.0` created earlier).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/f22-06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-6: Plotting the leverage of the 10 illustrative predictor observations
    in* `x`'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at [Figure 22-6](ch22.xhtml#ch22fig6), the appearance of the leverages
    plotted against the corresponding predictor values themselves makes sense—leverage
    gets progressively higher as you move away from the mean of the predictor data
    in either direction. This is essentially the pattern you’ll see for any plot of
    the raw leverages.
  prefs: []
  type: TYPE_NORMAL
- en: '***22.3.5 Cook’s Distance***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Of course, leverage alone isn’t enough to determine the overall influence of
    each observation on a fitted model. For that, the response value must also be
    taken into account.
  prefs: []
  type: TYPE_NORMAL
- en: 'Arguably the most well-known measure of influence is *Cook’s distance*, which
    estimates the magnitude of the effect of deleting the *i*th value from the fitted
    model. Cook’s distance for observation *i* (denoted *D*[*i*]) is given with the
    following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e22-5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It turns out that this equation is a specific function of a point’s leverage
    and residual. Here, the value *ŷ*[*j*] is the predicted mean response of observation
    *j* for the model fitted with all *n* observations, and ![image](../images/f0560-01.jpg)
    represents the predicted mean response of observation *j* for the model fitted
    *without* the *i*th observation. As usual, the term *p* is the number of predictor
    regression parameters (excluding the intercept), and the value ![image](../images/o.jpg)
    is the estimate of the residual standard error.
  prefs: []
  type: TYPE_NORMAL
- en: Put simply, the larger the value of *D*[*i*], the larger the influence the *i*th
    observation has on the fitted model, meaning outlying observations in high-leverage
    positions will correspond to higher values of *D*[*i*]. The important question
    is, how big does *D*[*i*] have to be in order for point *i* to be considered influential?
    In practice, this is difficult to universally answer, and there’s no formal hypothesis
    test for it, but there are several rule-of-thumb cutoff values. One such rule
    states that if *D*[*i*] > 1, the point should be considered influential; another,
    more sensitive rule suggests *D*[*i*] > 4/*n* (see, for example, [Bollen and Jackman,
    1990](ref.xhtml#ref08); [Chatterjee et al., 2000](ref.xhtml#ref12)). It’s generally
    advised that you compare multiple Cook’s distances for a given fitted model rather
    than analyzing one single value, and that any point corresponding to a comparatively
    large *D*[*i*] might need further inspection.
  prefs: []
  type: TYPE_NORMAL
- en: Continue with the objects created in [Section 22.3.3](ch22.xhtml#ch22lev2sec216),
    with the 10 illustrative observations in `x` and `y`, as well as the additional
    point defined in `p1x` and `p1y`. The linear regression model fitted to those
    data was stored as the object `mod.1`. It’s a good exercise to write some code
    that calculates the Cook’s distance measures following (22.5).
  prefs: []
  type: TYPE_NORMAL
- en: 'To that end, enter the following code in the R editor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: First, create new objects `x1` and `y1` to hold all 11 observations. The objects
    `n`, `param`, and `sigma` extract the data set size, the total number of estimated
    regression parameters (two in this case), and the estimated residual standard
    error for the model originally fitted to all 11 data points. The latter two items,
    `param` and `sigma`, represent (*p* + 1) and ![image](../images/o.jpg) in [Equation
    (22.5)](ch22.xhtml#ch22eq5), respectively. The object `yhat.full` uses the `fitted`
    function on the object `mod.1` to provide the fitted mean response values, representing
    the *ŷ*[*j*] values in (22.5).
  prefs: []
  type: TYPE_NORMAL
- en: To store the Cook’s distances, a vector `cooks` of 11 positions is created (initialized
    to be filled with `NA`s) with `rep`. Now, to calculate each *D*[*i*] value, set
    a `for` loop (see [Chapter 10](ch10.xhtml#ch10)) to scroll through each index
    from `1` to `11`. The first step of the loop is to create two temporary vectors
    `temp.x` and `temp.y` to be `x1` and `y1` with the observation at index `i` removed.
    A new temporary linear model is fitted to `temp.y` based on `temp.x`; then `predict`
    finds the mean responses from `temp.model` for each of the 11 predictor values
    (in other words, including the one that was deleted). As such, the resulting vector
    `temp.fitted` represents the ![image](../images/f0560-01.jpg) values in [Equation
    (22.5)](ch22.xhtml#ch22eq5). Finally, `sum` and the product of `param` with `sigma^2`
    compute *D*[*i*], and the result is stored in `cooks[i]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After highlighting and executing the code, the resulting Cook’s distances are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Unsurprisingly, the largest value of these is the last one, at around 0.314\.
    This corresponds to the 11th observation in `x1` and `y1`, which is the additional
    point originally defined in `p1x` and `p1y`. The value 0.314 is less than 1 and
    less than 4/11 = 0.364, the cutoffs defined by the earlier rules of thumb. This
    ties in with the assessment of the top-left image in [Figure 22-5](ch22.xhtml#ch22fig5)—that
    the influence of the point `p1x` and `p1y` is minimal when compared to the influence
    of a point like `p3x` and `p3y` in the rightmost image.
  prefs: []
  type: TYPE_NORMAL
- en: Just as the `hatvalues` function computes the leverages for you, the builtin
    `cooks.distance` function does the same for the *D*[*i*]. You can confirm the
    previous values in `cooks`, which are based on `mod.1`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: R automatically calculates and provides Cook’s distances as a diagnostic plot
    of a fitted linear model object when you select `which=4` in the relevant usage
    of `plot`. The following code uses `mod.1`, `mod.2`, and `mod.3` from earlier
    to produce the three images in [Figure 22-7](ch22.xhtml#ch22fig7); these correspond
    to the three data sets in [Figure 22-5](ch22.xhtml#ch22fig5).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The *D*[*i*] displayed on the top left of [Figure 22-7](ch22.xhtml#ch22fig7)
    match the values you manually calculated earlier, stored in `cooks`. The influences
    of all the data points in the top-right plot remain relatively small, which reflects
    what you can see in the top-right plot of [Figure 22-5](ch22.xhtml#ch22fig5),
    where the additional point (`p2x`, `p2y`) doesn’t greatly affect the overall fit.
    In the bottom plot, `abline` superimposes two horizontal lines marking off the
    values 1 (highest line) and 4/11 = 0.364, both of which are clearly breached by
    the 11th point (`p3x`, `p3y`), just as you’d expect given the corresponding bottom
    image in [Figure 22-5](ch22.xhtml#ch22fig5).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f22-07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-7: Three illustrative examples of the Cook’s distance plots produced
    in R, based on* `mod.1` *(top left),* `mod.2` *(top right), and* `mod.3` *(bottom)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Turn your attention back to the `car.step` model, where you modeled MPG using
    the `mtcars` data set, with the final fit achieved using stepwise AIC selection
    in [Section 22.2.4](ch22.xhtml#ch22lev2sec212). You’ve already looked at the residuals
    versus fitted values and QQ plot in [Figures 22-2](ch22.xhtml#ch22fig2) and [22-4](ch22.xhtml#ch22fig4).
    [Figure 22-8](ch22.xhtml#ch22fig8) provides a plot of Cook’s distances for the
    model with these two lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The plot labels the three points with the highest *D*[*i*] by default; two of
    these breach the 4/*n* = 4/32 = 0.125 mark. In light of the fitted model based
    on various effects of car weight (`wt`), horsepower (`hp`), and quarter-mile time
    (`qsec`), the Chrysler Imperial and Toyota Corolla are deemed to be in high-leverage
    positions with residuals large enough to be designated as highly influential.
    It should also be noted that the Fiat 128, though it doesn’t quite breach the
    0.125 line, is still rather influential and was in fact also one of the extreme
    labeled points in the residual plots ([Figure 22-2](ch22.xhtml#ch22fig2)) and
    the QQ plot ([Figure 22-4](ch22.xhtml#ch22fig4)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f22-08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-8: Cook’s distances for the model in* `car.step`*; a dashed horizontal
    line marks off 4/*n *for the* `mtcars` *data frame*'
  prefs: []
  type: TYPE_NORMAL
- en: This could reasonably suggest investigating these highly influential observations
    further. Was everything recorded correctly? Has your model been selected carefully?
    Are there alternative options for the model, such as additional predictor terms
    or transformations? You could explore these options and continue to monitor a
    plot of the Cook’s distances (along with the other diagnostics).
  prefs: []
  type: TYPE_NORMAL
- en: Whatever your result, the presence of influential points doesn’t necessarily
    mean there is a serious problem with your model—this is more a tool to help you
    detect observations that are extreme in terms of their specific combination of
    the predictor values *and* that have a larger residual, suggesting their response
    value sits away from the trends predicted by the model itself. This is especially
    useful in multiple regression, when high dimensionality of the response-predictor
    data can make conventional visualization of the raw data in a single plot difficult.
  prefs: []
  type: TYPE_NORMAL
- en: '***22.3.6 Graphically Combining Residuals, Leverage, and Cook’s Distance***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The last two diagnostic plots from `plot` combine the standardized residual,
    the leverage, and the Cook’s distance for the *i*th observation. These combination
    plots are especially useful in allowing you to see whether it is the high leverage
    or large residual, or both, that contributes more to a high influence observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the data models `mod.1`, `mod.2`, and `mod.3`, enter the following code
    with `which=5` to produce the three images in the left column of [Figure 22-9](ch22.xhtml#ch22fig9):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/f22-09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-9: Combination diagnostic plot of standardized residuals against
    leverage (left column) and Cook’s distance against leverage (right column) for
    the three illustrative models* `mod.1` *(top),* `mod.2` *(middle), and* `mod.3`
    *(bottom)*'
  prefs: []
  type: TYPE_NORMAL
- en: These plots show leverage on the horizontal axis and the standardized residuals
    on the vertical axis for each observation. As a function of residual and leverage,
    the Cook’s distances can be plotted as *contours* on each of these scatterplots.
    These contours delineate the spatial areas of the plots that correspond to high
    influence (in the right extreme corners).
  prefs: []
  type: TYPE_NORMAL
- en: The closer a point falls to the horizontal line at zero, the smaller its residual.
    A point that lies more left than right has a smaller leverage. If a point lies
    far enough from the horizontal line given its leverage (*x*-axis) position, it
    will breach the contours marking off certain values of *D*[*i*] (defaulting to
    just 0.5 and 1), indicating a high influence. Indeed, you can see by the narrowing
    of the contours as you move from left to right on these plots that classification
    as a high-influence point is easier if a given observation is in a high-leverage
    position, which makes perfect sense. In the previous calls to `plot` with `which=5`,
    the optional `cook.levels` argument is used to include a contour for the rule-of-thumb
    value of 4/11 for these three examples.
  prefs: []
  type: TYPE_NORMAL
- en: The plot for `mod.1` shows that the added point (`p1x`, `p1y`) has a large residual,
    but it doesn’t breach the 4/11 contour because it’s in a low-leverage position.
    The plot for `mod.2` shows that the added point (`p2x`,`p2y`) is in a high-leverage
    position but isn’t influential because its residual is small. Lastly, the plot
    for `mod.3` clearly identifies the added point (`p3x`, `p3y`) as highly influential—with
    a large residual and high leverage, it’s in clear breach of the high-level contours.
    Looking back at all the previous plots of these three illustrative data sets,
    it’s easy to note that these three plots clearly reflect the nature of each of
    the individually added extra observations.
  prefs: []
  type: TYPE_NORMAL
- en: The final diagnostic plot, using `which=6`, displays the same information as
    the `which=5` combination diagnostic, but this time the vertical axis displays
    Cook’s distance, and the horizontal axis displays a transformation of the leverage,
    namely, *h*[*ii*]/(1 − *h*[*ii*] ). This transformation amplifies larger leverage
    points in terms of their horizontal position, an effect that, in part, indirectly
    displays itself as a “stretched-out” scale on the *x*-axis—useful if you’re particularly
    interested in the extremity of the observations with respect to the collection
    of predictor variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'As such, the contours now define standardized residuals as a function of the
    scaled leverage and Cook’s distance. The following three lines produce the three
    images in the rightmost column of [Figure 22-9](ch22.xhtml#ch22fig9):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Points positioned further to the right are high-leverage points; points positioned
    higher up are high-influence points. Looking down the right column of plots in
    [Figure 22-9](ch22.xhtml#ch22fig9), you can see that the three additional points
    are found where you would expect them to be, according to their characteristics
    in `mod.1`, `mod.2`, and `mod.3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a real-data example, return to the model stored in `car.step`. Enter the
    following code to produce the two combination diagnostic plots in [Figure 22-10](ch22.xhtml#ch22fig10):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/f22-10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-10: Combination diagnostic plot of standardized residuals against
    leverage (left) and Cook’s distance against leverage (right) for the* `car.step`
    *model*'
  prefs: []
  type: TYPE_NORMAL
- en: The two images in [Figure 22-10](ch22.xhtml#ch22fig10) show the Corolla and
    the Imperial as influential observations with *D*[*i*] values greater than the
    `4/nrow(mtcars)` rule-of-thumb cutoff. Interestingly, this plot reveals that the
    Imperial (which was shown to have the largest *D*[*i*] by far in [Figure 22-8](ch22.xhtml#ch22fig8))
    actually has a smaller residual than both the Corolla and the Fiat 128\. Its high
    influence is clearly because of its high-leverage position with respect to the
    predictor values of the variables present in `car.step`. The Fiat 128, on the
    other hand, has one of the largest residuals in the entire data set (which is
    why it was flagged in some of the earlier diagnostic plots) but just misses out
    on being labeled a high-influence observation because of its relatively low-leverage
    position (based purely on the rule-of-thumb cutoff).
  prefs: []
  type: TYPE_NORMAL
- en: Any linear regression model will have observations that influence the model
    more than others, and these plots aim to help you identify them. But deciding
    what to actually do with highly influential observations can be difficult and
    is application specific. Although it’s not ideal to have a single observation
    exerting heavy influence over the final estimated model, it’s also extremely unwise
    to remove these observations without careful thought since they might be pointing
    to other issues, such as deficiencies in your current fit or previously undetected
    trends.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 22.2**'
  prefs: []
  type: TYPE_NORMAL
- en: In [Section 22.2.2](ch22.xhtml#ch22lev2sec210), you used the `nuclear` data
    frame in the `boot` package to illustrate forward selection, where a model was
    selected for `cost` as a function of main effects of `date`, `cap`, `pt`, and
    `ne`.
  prefs: []
  type: TYPE_NORMAL
- en: Access the data frame; fit and summarize the model described earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inspect the raw residuals versus fitted values and a normal QQ plot of the residuals
    and comment on your interpretations—do the assumptions underpinning the error
    component of the linear model appear satisfied in this case?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the rule-of-thumb cutoff for influential observations based on the
    Cook’s distances. Produce a plot of the Cook’s distances and add a horizontal
    line corresponding to the cutoff. Comment on your findings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Produce a combination diagnostic plot of the standardized residuals against
    leverage. Set the Cook’s distance contours to include the cutoff value from (c)
    as well as the default contours. Interpret the plot—how are any individually influential
    points characterized?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on (c) and (d), you should be able to identify the record in `nuclear`
    exerting the largest influence on the fitted model. For the sake of argument,
    let’s assume the observation was recorded incorrectly. Refit the model from (a),
    this time omitting the offending row from the data frame. Summarize the model—which
    coefficients have changed the most? Produce the diagnostic plots from (b) for
    the new model and compare them to the ones from earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `faraway` package and access the `diabetes` data frame. In [Exercise
    22.1](ch22.xhtml#ch22exc1) (g), you used stepwise AIC selection to choose a model
    for `chol`.
  prefs: []
  type: TYPE_NORMAL
- en: Using `diabetes`, fit the multiple linear model identified in the earlier exercise,
    that is, with main effects and a two-way interaction between `age` and `frame`
    and a main effect for `waist`. By summarizing the fit, determine the number of
    records that contained missing values in `diabetes` that were deleted from the
    estimation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Produce the raw residuals versus fitted and QQ diagnostic plots for the model
    in (f). Comment on the validity of the error assumptions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Investigate influential points. Make use of the familiar rule-of-thumb cutoff
    (note you’ll need to subtract the number of missing values from the total size
    of the data frame to get the effective sample size for your model). In the combination
    plot of the standardized residuals against leverage, use one, three, and five
    times the cutoff as the Cook’s distance contours.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recall the discussion of reading in web-based files in [Section 8.2.3](ch08.xhtml#ch08lev2sec75).
    There, you called in a data frame containing data on the prices of 308 diamonds
    (in Singapore dollars), as well as weight (in carats—continuous), color (categorical—six
    levels from `D`, the least yellow and the reference level, to `I`, the most yellow),
    clarity (categorical—five levels with `IF`, essentially flawless and the reference
    level, `VVS1`, `VVS2`, `VS1`, and `VS2`, with the last being the least clear),
    and certification (categorical—three levels for different diamond certification
    bodies with levels `GIA` as the reference, `HRD` and `IGI`). Seek out the freely
    available article by Chu ([2001](ref.xhtml#ref13)) for more information on these
    data. With an Internet connection, run the following lines, which will read in
    the data as the object `diamonds` and name each variable column appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Using either base R graphics or `ggplot2`, to get a feel for the data, produce
    a scatterplot of the price on the *y*-axis and carat weight on the *x*-axis. Experiment
    with using plotting color to split the points according to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: – Diamond clarity
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – Diamond color
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – Diamond certification
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fit a multiple linear model with `Price` as the response and main effects for
    the other variables as the predictors. Summarize the model and produce the three
    diagnostic plots that tell you about the assumptions surrounding the error term.
    Comment on the plots—are you satisfied that this is an appropriate model for the
    diamond prices? Why or why not?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat (j) but use the log transformation of `Price`. Again, inspect and comment
    on the validity of the error assumptions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat (k), but in modeling the log-price, this time include an additional quadratic
    term for `Carat` (refer to [Section 21.4.1](ch21.xhtml#ch21lev2sec199) for details
    on polynomial transformations). How do the residual diagnostics look now?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**22.4 Collinearity**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This final aspect of fitting regression models isn’t technically classified
    as a diagnostic check but still has substantial potential to adversely affect
    the validity of any conclusions you draw from a fitted model and occurs frequently
    enough to warrant discussion here. *Collinearity* (also referred to as *multicollinearity*)
    is when two or more of the explanatory variables are highly correlated with each
    other.
  prefs: []
  type: TYPE_NORMAL
- en: '***22.4.1 Potential Warning Signs***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: High correlation between two predictors implies there will be some level of
    redundancy in terms of the information they contain when it comes to the response
    variable. It’s a problem since it can destabilize the ability to reliably fit
    a model and, as noted earlier, therefore be detrimental to any subsequent model-based
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following items serve as potential warnings of collinearity when you’re
    inspecting a model summary:'
  prefs: []
  type: TYPE_NORMAL
- en: • The omnibus *F*-test ([Section 21.3.5](ch21.xhtml#ch21lev2sec197)) result
    is statistically significant, but none of the individual *t*-test results for
    the regression parameters are significant.
  prefs: []
  type: TYPE_NORMAL
- en: • The sign of a given coefficient estimate contradicts what you would reasonably
    expect to see, for example, drinking more wine resulting in a lower blood alcohol
    level.
  prefs: []
  type: TYPE_NORMAL
- en: • Parameter estimates are associated with unusually high standard errors or
    vary wildly when the model is fitted to different random record subsets of the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: As the last point notes, collinearity tends to have more of a detrimental effect
    on the standard errors of the coefficients (and associated outcomes such as confidence
    intervals, significance tests, and prediction intervals) than it does on point
    predictions per se. In most cases, you can avoid collinearity simply by being
    careful. Be aware of the variables present and how the data have been collected.
    For example, ensure any given predictors you intend to include in the model don’t
    just represent a rescaled value of another included predictor. It’s also advisable
    to perform an exploratory analysis of your data, producing summary statistics
    and basic statistical plots. You can tabulate counts between categorical variables
    or look at estimated correlation coefficients between continuous variables, for
    example. In the latter case, as a rough guide, some statisticians suggest that
    a correlation of 0.8 or more could lead to potential problems.
  prefs: []
  type: TYPE_NORMAL
- en: '***22.4.2 Correlated Predictors: A Quick Example***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consider the `survey` data of the statistics students again, located in the
    `MASS` package. In most models you’ve looked at for these data, you’ve attempted
    to predict student height from certain explanatory variables, often including
    the handspan of the writing hand (`Wr.Hnd`). The help page `?survey` shows that
    data have also been collected on the nonwriting handspan (`NW.Hnd`). It’s reasonable
    to expect that these two variables will be highly correlated, which is precisely
    why I’ve avoided any use of `NW.Hnd` previously. Indeed, executing
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: reveals a high correlation coefficient, suggesting a strong positive linear
    association between the writing and nonwriting handspans of the students. In other
    words, these two variables should represent much the same information in any given
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you know from previously fitted models that writing handspan has a significant
    and positive impact on predicting mean student height. The following code quickly
    confirms this via a simple linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The high positive correlation between `Wr.Hnd` and `NW.Hnd` suggests that using
    `NW.Hnd` instead should have a similar effect.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: You can see from these results that this is certainly the case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look, however, at what happens if you try to model height based on both of
    these predictors at the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Since the effects of `Wr.Hnd` and `NW.Hnd` on `Height` are intermingled with
    one another, including both at the same time heavily masks any individual contribution
    to modeling the response. Statistical significance of the predictors is almost
    nonexistent; at the least, the effects are both associated with much, much higher
    *p*-values than in the individual single-predictor fits. That said, the omnibus
    *F*-test remains highly significant, giving an example of the first warning sign
    noted previously.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Code in This Chapter**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| **Function/operator** | **Brief description** | **First occurrence** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `anova` | Partial *F* -tests | [Section 22.2.1](ch22.xhtml#ch22lev2sec209),
    [p. 531](ch22.xhtml#page_531) |'
  prefs: []
  type: TYPE_TB
- en: '| `add1` | Review single-term additions | [Section 22.2.2](ch22.xhtml#ch22lev2sec210),
    [p. 533](ch22.xhtml#page_533) |'
  prefs: []
  type: TYPE_TB
- en: '| `update` | Make changes to fitted model | [Section 22.2.2](ch22.xhtml#ch22lev2sec210),
    [p. 534](ch22.xhtml#page_534) |'
  prefs: []
  type: TYPE_TB
- en: '| `drop1` | Review single-term deletions | [Section 22.2.3](ch22.xhtml#ch22lev2sec211),
    [p. 538](ch22.xhtml#page_538) |'
  prefs: []
  type: TYPE_TB
- en: '| `step` | Stepwise AIC model selection | [Section 22.2.4](ch22.xhtml#ch22lev2sec212),
    [p. 543](ch22.xhtml#page_543) |'
  prefs: []
  type: TYPE_TB
- en: '| `plot` (used on `lm` object) | Model diagnostics | [Section 22.3.1](ch22.xhtml#ch22lev2sec214),
    [p. 551](ch22.xhtml#page_551) |'
  prefs: []
  type: TYPE_TB
- en: '| `rstandard` | Extract standardized residuals | [Section 22.3.2](ch22.xhtml#ch22lev2sec215),
    [p. 555](ch22.xhtml#page_555) |'
  prefs: []
  type: TYPE_TB
- en: '| `shapiro.test` | Shapiro-Wilk test of normality | [Section 22.3.2](ch22.xhtml#ch22lev2sec215),
    [p. 555](ch22.xhtml#page_555) |'
  prefs: []
  type: TYPE_TB
- en: '| `hatvalues` | Calculate leverages | [Section 22.3.4](ch22.xhtml#ch22lev2sec217),
    [p. 558](ch22.xhtml#page_558) |'
  prefs: []
  type: TYPE_TB
- en: '| `cooks.distance` | Calculate Cook’s distances | [Section 22.3.5](ch22.xhtml#ch22lev2sec218),
    [p. 561](ch22.xhtml#page_561) |'
  prefs: []
  type: TYPE_TB
