- en: '**22**'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**22**'
- en: '**LINEAR MODEL SELECTION AND DIAGNOSTICS**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性模型选择与诊断**'
- en: '![image](../images/common-01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common-01.jpg)'
- en: 'You’ve now spent a fair amount of time on many aspects of linear regression
    models. In this chapter, I’ll cover how formal R tools and techniques can be used
    to investigate two other, and no less important, aspects of regression: choosing
    an appropriate model for your analysis and assessing the validity of the assumptions
    you’ve made.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经花费了相当多的时间研究线性回归模型的许多方面。在这一章中，我将介绍如何使用正式的R工具和技术来研究回归的另两个方面，这两个方面同样重要：为你的分析选择合适的模型，并评估你所做假设的有效性。
- en: '**22.1 Goodness-of-Fit vs. Complexity**'
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**22.1 拟合优度与复杂度**'
- en: 'The overarching goal of fitting any statistical model is to faithfully represent
    the data and the relationships held within them. In general, fitting statistical
    models boils down to a balancing act between two things: goodness-of-fit and complexity.
    *Goodness-of-fit* refers to the goal of obtaining a model that best represents
    the relationships between the response and the predictor (or predictors). *Complexity*
    describes how complicated a model is; this is always tied to the number of terms
    in the model that require estimation—the inclusion of more predictors and additional
    functions (such as polynomial transformations and interactions) leads to a more
    complex model.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合任何统计模型的总体目标是忠实地表示数据及其内部关系。一般而言，拟合统计模型归结为在两者之间取得平衡：拟合优度和复杂度。*拟合优度*指的是获得一个最能表示响应变量与预测变量（或多个预测变量）之间关系的模型的目标。*复杂度*描述了一个模型的复杂程度；它始终与模型中需要估计的项数相关——更多的预测变量和附加函数（如多项式变换和交互项）的加入会使模型更复杂。
- en: '***22.1.1 Principle of Parsimony***'
  id: totrans-6
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***22.1.1 简约原则***'
- en: Statisticians refer to the balancing act between goodness-of-fit and complexity
    as the *principle of parsimony*, where the goal of the associated *model selection*
    is to find a model that’s as simple as possible (in other words, with relatively
    low complexity), without sacrificing too much goodness-of-fit. We’d say that a
    model that satisfies this notion is a *parsimonious* fit. You’ll often hear of
    researchers talking about choosing the “best” model—they’re actually referring
    to the idea of parsimony.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学家将拟合优度和复杂度之间的平衡称为*简约原则*，其相关的*模型选择*目标是找到一个尽可能简单的模型（换句话说，具有相对较低的复杂度），同时又不牺牲太多的拟合优度。我们可以说，满足这一概念的模型是*简约的*拟合。你常常会听到研究人员谈论选择“最佳”模型——他们实际上是在指简约原则的概念。
- en: So, how do you decide where to draw the line on such a balance? Naturally, statistical
    significance plays a role here—and model selection often simply comes down to
    assessing the significance of the effect of predictors or functions of predictors
    on the response. In an effort to impart some amount of objectivity to such a process,
    you can use systematic selection algorithms, such as those you’ll learn about
    in [Section 22.2](ch22.xhtml#ch22lev1sec73), to decide between multiple explanatory
    variables and any associated functions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你如何决定在这种平衡上划定界限呢？自然，统计显著性在其中发挥着作用——而模型选择通常归结为评估预测变量或预测变量函数对响应的影响是否显著。为了使这一过程尽可能客观，你可以使用系统化的选择算法，例如在[第22.2节](ch22.xhtml#ch22lev1sec73)中学习到的算法，来决定多个解释变量及其相关函数之间的选择。
- en: '***22.1.2 General Guidelines***'
  id: totrans-9
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***22.1.2 一般指南***'
- en: Performing any kind of model selection or comparing several models against one
    another involves decision making regarding the inclusion of available predictor
    variables. On this topic, there are several guidelines you should always follow.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 执行任何类型的模型选择或将多个模型进行比较，都涉及到关于是否包含可用预测变量的决策。在这个话题上，有几个指南是你应当始终遵循的。
- en: • First, it’s important to remember that you can’t remove individual levels
    of a categorical predictor in a given model; this makes no sense. In other words,
    if one of the nonreference levels is statistically significant but all others
    are nonsignificant, you should treat the categorical variable, as a whole, as
    making a statistically significant contribution to the determination of the mean
    response. You should only really consider *entire* removal of that categorical
    predictor if *all* nonreference coefficients are associated with a lack of evidence
    (against being zero). This also holds for interactive terms involving categorical
    predictors.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: • 首先，重要的是要记住，你不能在给定的模型中删除分类预测变量的单独水平；这样做是没有意义的。换句话说，如果某个非参考水平具有统计显著性，而其他所有水平都不显著，那么你应该将该分类变量整体视为对平均响应的统计显著贡献。只有在所有非参考系数都与缺乏证据（反对为零）相关时，你才应考虑*完全*删除该分类预测变量。这同样适用于涉及分类预测变量的交互项。
- en: • If an interaction is present in the fitted model, all lower-order interactions
    and main effects of the relevant predictors must remain in the model. This was
    touched upon in [Section 21.5.1](ch21.xhtml#ch21lev2sec202), when I discussed
    interpretation of interactive effects as augmentations of lower-order effects.
    As an example, you should only really consider removing the main effect of a predictor
    if there are no interaction terms present in the fitted model involving that predictor
    (even if that main effect has a high *p*-value).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: • 如果拟合的模型中存在交互效应，则所有相关预测变量的低阶交互项和主效应必须保留在模型中。这一点在[第21.5.1节](ch21.xhtml#ch21lev2sec202)中有所提及，当时我讨论了交互效应作为低阶效应的扩展。例如，只有在拟合的模型中没有涉及该预测变量的交互项时，才应考虑删除该预测变量的主效应（即使该主效应的*p*-值很高）。
- en: • In models where you’ve used a polynomial transformation of a certain explanatory
    variable (refer to [Section 21.4.1](ch21.xhtml#ch21lev2sec199)), keep all lower-order
    polynomial terms in the model if the highest is deemed significant. A model containing
    an order 3 polynomial transformation in a predictor, for example, must also include
    the order 1 and order 2 transformations of that variable. This is because of the
    mathematical behavior of polynomial functions—only by explicitly separating out
    the linear, quadratic, and cubic (and so on) effects as distinct terms can you
    avoid confounding said effects with one another.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: • 在使用某一解释变量的多项式变换的模型中（参考[第21.4.1节](ch21.xhtml#ch21lev2sec199)），如果最高次幂被认为是显著的，则应保留模型中的所有低阶多项式项。例如，包含三次多项式变换的预测变量模型，必须同时包含该变量的一级和二级变换。这是由于多项式函数的数学特性——只有通过明确地将线性、二次和三次（以此类推）效应作为独立项分开，才能避免将这些效应相互混淆。
- en: '**22.2 Model Selection Algorithms**'
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**22.2 模型选择算法**'
- en: The job of a model selection algorithm is to sift through your available explanatory
    variables in some systematic fashion in order to establish which are best able
    to jointly describe the response, as opposed to fitting models by examining specific
    combinations of predictors in isolation, as you’ve done so far.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择算法的任务是以某种系统的方式筛选你可用的解释变量，以确定哪些变量能最好地联合描述响应，而不是像之前那样仅通过单独考察预测变量的特定组合来拟合模型。
- en: Model selection algorithms can be controversial. There are several different
    methods, and no single approach is universally appropriate for every regression
    model. Different selection algorithms can result in different final models, as
    you’ll see. In many cases, researchers will have additional information or knowledge
    about the problem that influences the decision—for example, that certain predictors
    must always be included or that it makes no sense to ever include them. This must
    be considered at the same time as other complications, such as the possibility
    of interactions or unobserved lurking variables influencing significant relationships
    and the need to ensure any fitted model is statistically valid (which you’ll look
    at in [Section 22.3](ch22.xhtml#ch22lev1sec74)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择算法可能存在争议。它有几种不同的方法，没有一种方法对于每个回归模型都是普遍适用的。不同的选择算法可能会得出不同的最终模型，正如你将看到的那样。在许多情况下，研究人员会有关于问题的额外信息或知识，这会影响决策——例如，某些预测变量必须始终包括，或者它们的包括根本没有意义。必须同时考虑这些因素，以及其他复杂情况，例如交互效应或未观察到的潜在变量对显著关系的影响，以及确保任何拟合模型在统计上是有效的（这一点你将在[第22.3节](ch22.xhtml#ch22lev1sec74)中看到）。
- en: 'It’s helpful to keep in mind this famous quote from celebrated statistician
    George Box (1919–2013): “All models are wrong, but some are useful.”'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 记住统计学家乔治·博克斯（George Box，1919–2013）的一句名言很有帮助：“所有模型都是错误的，但有些模型是有用的。”
- en: Any fitted model you produce can never be assumed to be the truth, but a model
    that’s fitted and checked carefully and thoroughly can reveal interesting features
    of the data and so have the potential to reveal associations and relationships
    by providing quantitative estimates thereof.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 任何你拟合的模型都不能被假设为真，但一个经过仔细检查的拟合模型可以揭示数据的有趣特征，从而有可能通过提供这些特征的定量估计，揭示关联性和关系。
- en: '***22.2.1 Nested Comparisons: The Partial F-Test***'
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***22.2.1 嵌套比较：部分F检验***'
- en: 'The *partial F-test* is probably the most direct way to compare several different
    models. It looks at two or more *nested* models, where the smaller, less complex
    model is a reduced version of the bigger, more complex model. Formally, let’s
    say you’ve fitted two linear regressions models as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 部分*F*检验可能是比较多个不同模型的最直接方式。它考察两个或多个*嵌套*模型，其中较小、较简单的模型是较大、较复杂模型的简化版本。正式地说，假设你拟合了两个线性回归模型，如下所示：
- en: '![image](../images/f0529-01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0529-01.jpg)'
- en: Here, the reduced model, predicting *ŷ*[redu], has *p* predictors, plus one
    intercept. The full model, predicting *ŷ*[full], has *q* predictor terms. The
    notation implies that *q* > *p* and that, along with the standard inclusion of
    an intercept ![image](../images/b0.jpg), the full model involves all *p* predictors
    of the reduced model defined by *ŷ*[redu], as well as *q* − *p* additional terms.
    This emphasizes the fact that the model for *ŷ[redu]* is nested within *ŷ*[full].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，简化模型*ŷ*[redu]有*p*个预测变量，加上一个截距项。完整模型*ŷ*[full]有*q*个预测变量项。这个符号表示*q* > *p*，并且，除了标准的截距项![image](../images/b0.jpg)外，完整模型包括了简化模型*ŷ*[redu]的所有*p*个预测变量，以及*q*
    − *p*个额外项。这强调了模型*ŷ[redu]*是嵌套在*ŷ*[full]*中的事实。
- en: 'It’s important to note that increasing the number of predictors in a regression
    model will always improve *R*² and any other measures of goodness-of-fit. The
    real question, however, is whether that improvement in goodness-of-fit is large
    enough to make the additional complexity involved with including any additional
    predictor terms “worth it.” This is precisely the question that the partial *F*-test
    tries to answer in the context of nested regression models. Its goal is to test
    whether including those extra *q* − *p* terms, which produce the full model rather
    than the reduced model, provide a statistically significant improvement in goodness-of-fit.
    The partial *F*-test addresses these hypotheses:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，增加回归模型中的预测变量数量总是会提高*R*²以及其他拟合优度的衡量标准。然而，真正的问题是，拟合优度的改善是否足够大，以至于包括任何额外的预测变量项所增加的复杂性“是值得的”。正是这个问题，部分*F*检验试图在嵌套回归模型的背景下回答。它的目的是测试是否包括那些额外的*q*
    − *p*项（这些项构成完整模型而不是简化模型）能显著改善拟合优度。部分*F*检验所解决的假设如下：
- en: '![image](../images/e22-1.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/e22-1.jpg)'
- en: The calculation of the test statistic to address these hypotheses follows the
    same ideas behind the omnibus *F*-test automatically produced by R when summarizing
    a fitted linear model object (detailed in [Section 21.3.5](ch21.xhtml#ch21lev2sec197)).
    Denote the coefficient of determination for the reduced and full models with ![image](../images/r2redu.jpg)
    and ![image](../images/r2redu.jpg), respectively. If *n* refers to the sample
    size of the data used to fit both models, the test statistic given by
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 计算测试统计量以检验这些假设的过程遵循与R在总结拟合的线性模型对象时自动生成的总体*F*检验背后的相同思想（详见[第21.3.5节](ch21.xhtml#ch21lev2sec197)）。将简化模型和完整模型的决定系数分别表示为
    ![image](../images/r2redu.jpg) 和 ![image](../images/r2redu.jpg)。如果*n*表示用来拟合这两个模型的数据样本大小，那么测试统计量为：
- en: '![image](../images/e22-2.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/e22-2.jpg)'
- en: follows an *F* distribution with df[1] = *q* − *p*, df[2] = *n* − *q* degrees
    of freedom under the assumption of H[0] in (22.1). The *p*-value is found as the
    upper-tail area from ![image](../images/f.jpg) as usual; the smaller it is, the
    greater the evidence against the null hypothesis, which states that one or more
    of the additional parameters has no impact on the response variable.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在假设(22.1)中的H[0]下，遵循*F*分布，df[1] = *q* − *p*，df[2] = *n* − *q*自由度。*p*-值通过通常的方式，从![image](../images/f.jpg)的上尾区域得出；它越小，越能反驳原假设，即认为一个或多个额外的参数对响应变量没有影响。
- en: 'Take the model objects `survmult` and `survmult2` from [Section 21.3.1](ch21.xhtml#ch21lev2sec193)
    as an example. The `survmult` model aims to predict mean student height from writing
    handspan and sex based on the `survey` data frame from the `MASS` package; `survmult2`
    adds smoking status to these predictors. If you need to, return to [Section 21.3.1](ch21.xhtml#ch21lev2sec193)
    to refit these two models. Printing the objects to the console screen previews
    the two fits and makes it easy to confirm that the smaller model is indeed nested
    within the larger model in terms of its explanatory variables:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以[第21.3.1节](ch21.xhtml#ch21lev2sec193)中的模型对象`survmult`和`survmult2`为例。`survmult`模型旨在基于`MASS`包中的`survey`数据框，通过写手跨和性别来预测学生的平均身高；`survmult2`则在这些预测变量中加入了吸烟状态。如果需要，可以回到[第21.3.1节](ch21.xhtml#ch21lev2sec193)重新拟合这两个模型。将这些对象打印到控制台，可以预览这两个拟合结果，并且容易确认较小的模型确实嵌套在较大的模型中，其解释变量是相同的：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once you’ve fitted your nested models, R can carry out partial *F*-tests using
    the `anova` function (partial *F*-tests fall within the suite of analysis of variance
    methodologies). To determine whether adding `Smoke` as a predictor provides any
    statistically significant improvement in fit, simply start with the reduced model
    and supply the model objects as arguments.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拟合了嵌套模型，R可以使用`anova`函数进行部分 *F* 检验（部分 *F* 检验属于方差分析方法的一部分）。要判断将`Smoke`作为预测变量是否能显著改善拟合效果，只需从简化模型开始，并将模型对象作为参数传入。
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The output provides the quantities associated with calculation of ![image](../images/r2redu.jpg)
    and ![image](../images/r2redu.jpg) and the test statistic ![image](../images/f.jpg)
    from (22.2), given in the resulting table as `F`, which is of the most interest.
    Using the values of *p* and *q* from printing `survmult` and `survmult2`, you
    should be able to confirm, for example, the values of df[1] and df[2] appearing
    in the second row of the table in the columns `Df` and `Res.Df`, respectively.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 输出提供了与计算![image](../images/r2redu.jpg) 和 ![image](../images/r2redu.jpg)以及检验统计量
    ![image](../images/f.jpg)（来自式（22.2））相关的量，这些内容在结果表中以`F`的形式给出，是最受关注的。通过打印`survmult`和`survmult2`时得到的*p*和*q*值，你应该能够确认，例如，表格第二行中`Df`和`Res.Df`列中出现的df[1]和df[2]的值。
- en: The result of this particular test, obtained from a test statistic of ![image](../images/f0531-01.jpg)
    associated with df[1] = 3, df[2] = 201, is a high *p*-value of 0.823, suggesting
    no evidence against H[0]. This means that adding `Smoke` to the reduced model,
    which includes only the explanatory variables `Wr.Hnd` and `Sex`, offers no tangible
    improvement in fit when it comes to modeling student height. That conclusion isn’t
    surprising, given the nonsignificant *p*-values of all non-reference levels of
    `Smoke`, seen previously in [Section 21.3.1](ch21.xhtml#ch21lev2sec193).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定测试的结果，来自与 df[1] = 3 和 df[2] = 201 相关的检验统计量 ![image](../images/f0531-01.jpg)，得到一个较高的*p*-值
    0.823，表明没有反对 H[0] 的证据。这意味着，将`Smoke`加入到包含解释变量`Wr.Hnd`和`Sex`的简化模型中，在建模学生身高时并没有显著改善拟合效果。这个结论并不令人惊讶，因为之前在[第21.3.1节](ch21.xhtml#ch21lev2sec193)中看到，所有非参考水平的`Smoke`都没有显著的*p*-值。
- en: This is how partial *F*-tests are used for model selection—in the current example,
    the reduced model would be the more parsimonious fit and preferred over the full
    model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是部分 *F* 检验在模型选择中的应用——在当前示例中，简化模型将是更简洁的拟合模型，并且更倾向于选择它而非完整模型。
- en: You can conduct comparisons among several nested models in a given call to `anova`,
    which can be useful for investigating things such as the inclusion of interactive
    terms or including polynomial transformations of predictors since there’s a natural
    hierarchy that requires you to retain any lower-order terms.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在一次`anova`调用中比较多个嵌套模型，这对于调查例如是否包含交互项或包含预测变量的多项式变换等情况非常有用，因为自然的层次结构要求你保留任何较低阶的项。
- en: For an example, let’s use the `diabetes` data frame in the `faraway` package
    from [Section 21.5.2](ch21.xhtml#ch21lev2sec203), with the model fitted to predict
    cholesterol level (`chol`) against age (`age`) and body frame (`frame`) and the
    interaction between those two predictors. Before using the partial *F*-tests to
    compare nested variants, you need to ensure you’re using the same records for
    each model and that there aren’t missing values for any of the predictors that
    are then going to be unavailable to the “fuller” models (so the sample size is
    the same for each comparison). To do this, you just need to first define a version
    of `diabetes` that removes records with missing values in the predictors you’re
    using.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用`faraway`包中的`diabetes`数据框，来自[第21.5.2节](ch21.xhtml#ch21lev2sec203)，该模型拟合用于预测胆固醇水平（`chol`）与年龄（`age`）和体型（`frame`）之间的关系，以及这两个预测变量的交互作用。在使用部分*F*检验比较嵌套模型之前，你需要确保每个模型使用的是相同的记录，并且在所有预测变量中没有缺失值，这样才能确保所有“更全面”的模型使用相同的样本量进行比较。为此，你只需要首先定义一个版本的`diabetes`，该版本删除了在你使用的预测变量中包含缺失值的记录。
- en: 'Load the `faraway` package and use logical subsetting to identify and delete
    any individuals with a missing value for `age` *or* for `frame`. Define this new
    version of the `diabetes` object:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 加载`faraway`包，并使用逻辑子集选择方法，找出并删除`age`*或*`frame`中存在缺失值的个体。定义这个新的`diabetes`对象版本：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, fit the following four models using your new `diab` object:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用新的`diab`对象拟合以下四个模型：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The first model is just an intercept, the second adds `age` as a predictor,
    the third has `age` and `frame`, and the fourth includes the interaction. Nesting
    is evident, and you can now compare the significance of the improvements in goodness-of-fit
    as you increase the complexity of the model at each step.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个模型仅包含截距，第二个模型加入了`age`作为预测变量，第三个模型包含了`age`和`frame`，第四个模型则包括了交互项。可以看到模型之间存在嵌套关系，随着每一步模型复杂度的增加，你可以比较拟合优度改善的显著性。
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If you hadn’t deleted the records containing missing values in those predictors,
    you would’ve received an error telling you that the data sets for the four models
    were not equal sizes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有删除那些包含缺失值的记录，你会收到一个错误提示，告诉你四个模型的数据集大小不一致。
- en: The results themselves suggest that including `age` provides a significant improvement
    to modeling `chol`; including a main effect for `frame` provides a further mild
    improvement; and there’s very weak evidence, if any, that including an interactive
    effect is beneficial to goodness-of-fit. From this, you might prefer to use `dia.mod3`,
    the main-effects-only model, as the most parsimonious representation of mean cholesterol
    out of these four models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 结果本身表明，包含`age`显著改善了`chol`的建模；包含`frame`的主效应进一步带来了轻微的改善；而包括交互效应对拟合优度的提升几乎没有证据，甚至可以说没有任何证据。从这些结果来看，你可能更倾向于使用`dia.mod3`，即仅包含主效应的模型，作为这四个模型中最简洁的均值胆固醇表现方式。
- en: '***22.2.2 Forward Selection***'
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***22.2.2 前向选择***'
- en: Partial *F*-tests are a natural way to investigate nested models but can be
    difficult to manage if you have many different models to fit when, for example,
    you have many predictor variables.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 部分*F*检验是检查嵌套模型的自然方式，但如果你有许多不同的模型需要拟合，尤其是当有许多预测变量时，管理起来可能会很困难。
- en: This is where *forward selection* (also referred to as *forward elimination*)
    comes in. The idea is to start with an intercept-only model and then perform a
    series of independent tests to determine which of your predictor variables significantly
    improves the goodness-of-fit. Then you update your model object by adding that
    term and execute the series of tests again for all remaining terms to determine
    which of those would further improve the fit. The process repeats until there
    aren’t any more terms that improve the fit in a statistically significant way.
    The ready-to-use R functions `add1` and `update` perform the series of tests and
    update your fitted regression model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是*前向选择*（也称为*前向消除*）的应用场景。其思路是从一个仅包含截距的模型开始，然后通过一系列独立的检验，确定哪些预测变量显著改善了拟合优度。接着，你会通过加入这些变量来更新模型对象，并对剩余的变量执行一系列检验，以确定哪些变量会进一步改善拟合。这个过程会一直重复，直到没有任何变量在统计学上显著提高拟合优度为止。R
    语言中的`add1`和`update`函数可以执行这些检验并更新你拟合的回归模型。
- en: You’ll use the `nuclear` data frame in the `boot` library from [Exercise 21.1](ch21.xhtml#ch21exc1)
    on [page 499](ch21.xhtml#page_499) and [Section 21.5.5](ch21.xhtml#ch21lev2sec206)
    as an example. The goal is to choose the most informative model for prediction
    of construction cost. Load `boot` and access the help file `?nuclear` to remind
    yourself of the variable definitions. First fit the model for construction cost
    with an overall intercept term only.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用`boot`库中的`nuclear`数据框作为示例，来自[第21.1节](ch21.xhtml#ch21exc1)的[第499页](ch21.xhtml#page_499)和[第21.5.5节](ch21.xhtml#ch21lev2sec206)。目标是选择最具信息量的模型来预测建筑成本。加载`boot`库并访问帮助文件`?nuclear`，以提醒自己变量定义。首先，仅用一个总截距项拟合建筑成本模型。
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You know from earlier exploits that this particular model is rather inadequate
    for the reliable prediction of `cost`. So, consider the following line of code
    to start the forward selection (I’ve suppressed the output, which I’ll show separately
    and discuss in a moment):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你从之前的实验中知道，这个特定的模型对于`cost`的可靠预测相当不足。所以，考虑以下代码行来开始前向选择（我已经抑制了输出，稍后会单独展示并讨论）：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first argument to `add1` is always the model you’re aiming to update. The
    second argument, `scope`, is critical—you must supply a formula object defining
    the “fullest,” most complex model you’d consider fitting. For this you would typically
    use the `.~.` notation, in which the dots refer to the definition of the model
    in the first argument. Specifically, the dots stand for “what is already there.”
    In other words, through `scope` you’re telling `add1` that the fullest model you’d
    consider has `cost` as the response, an intercept, and main effects of all other
    predictors in the `nuclear` data frame (I’ll restrict the full model to main effects
    only for ease of demonstration). You don’t need to supply the data frame as an
    argument since those data are contained within the model object in the first argument.
    Lastly, you tell `add1` the test to perform. There are a handful of variants available
    (see `?add1`), but here you’ll stick with `test="F"` for partial *F*-tests.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`add1`的第一个参数总是你要更新的模型。第二个参数`scope`非常重要——你必须提供一个公式对象，定义你考虑拟合的“最完整”模型。通常你会使用`.~.`符号，点号表示第一个参数中模型的定义。具体来说，点号代表“已经存在的部分”。换句话说，通过`scope`，你告诉`add1`，你考虑的最完整模型是以`cost`作为响应变量，包含一个截距项，以及`nuclear`数据框中所有其他预测变量的主效应（为了演示方便，我将完整模型限制为仅包含主效应）。你无需提供数据框作为参数，因为数据已经包含在第一个参数中的模型对象里。最后，你需要告诉`add1`要执行的测试。这里有一些可用的变体（参见`?add1`），但在此你将坚持使用`test="F"`进行部分*F*检验。'
- en: Now, focus on the output that’s provided directly after the execution of `add1`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关注`add1`执行后直接提供的输出。
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output comprises a series of rows, starting with `<none>` (doing nothing
    to the current model). You receive the `Sum of Sq` and `RSS` values, directly
    related to calculating the test statistic. The differences in degrees of freedom
    are also reported. Another measure of parsimony, `AIC`, is also provided (you’ll
    look at that in more detail in [Section 22.2.4](ch22.xhtml#ch22lev2sec212)).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 输出包含一系列行，从`<none>`开始（表示对当前模型没有任何修改）。你会得到`Sum of Sq`和`RSS`值，这些值与计算测试统计量直接相关。自由度的差异也会被报告。另一个简洁性的度量`AIC`也会提供（你将在[第22.2.4节](ch22.xhtml#ch22lev2sec212)中详细查看）。
- en: Most relevant are the test outcomes; with `test="F"`, each row corresponds to
    an independent partial *F*-test comparing the model in the first argument, as
    *ŷ*[redu], with the model that results from having added that row term only as
    *ŷ*[full]. Usually, therefore, you would update your model by adding only the
    term with the largest (and “most significant”) improvement.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最相关的是测试结果；使用`test="F"`时，每一行对应一个独立的部分*F*检验，比较第一个参数中的模型（*ŷ*[redu]）与仅添加该行项后的模型（*ŷ*[full]）。因此，通常你会通过仅添加具有最大（和“最显著”）改进的项来更新模型。
- en: Here, you should be able to see that adding `date` as a predictor offers the
    largest significant improvement to modeling `cost`. So, let’s update `nuc.0` to
    include that term with the code.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你应该能看到将`date`作为预测变量添加对建模`cost`提供了最大的显著改进。所以，接下来我们更新`nuc.0`，在代码中加入该项。
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In `update` you provide the model you want to update as the first argument,
    and the second argument, `formula`, tells `update` how to update the model. Again
    using the `.~.` notation, the instruction is to update `nuc.0` by adding `date`
    as a predictor, resulting in a fitted model object of the same class of the first
    argument. Call a `summary` of the new model, `nuc.1`, to see this.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在`update`中，你将想要更新的模型作为第一个参数，第二个参数`formula`告诉`update`如何更新该模型。再次使用`.~.`符号，指示通过将`date`添加为预测因子来更新`nuc.0`，从而生成一个与第一个参数同类的拟合模型对象。对新模型`nuc.1`调用`summary`以查看结果。
- en: So, let’s keep going! Call `add1` again, but now pass `nuc.1` as your first
    argument.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们继续吧！再次调用`add1`，但这次将`nuc.1`作为第一个参数。
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that there’s now no row for adding in `date`; it’s already there in `nuc.1`.
    It seems the next most informative addition would be `cap`. Update `nuc.1` to
    that effect.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，现在没有为`date`添加行；它已经在`nuc.1`中。似乎下一个最有信息量的添加项是`cap`。将`nuc.1`更新为此。
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now keep going, testing, and updating. By calling `add1` on `nuc.2` (output
    not shown here), you’ll find that the next most significant addition is `pt` (by
    a small margin). Update to a new object named `nuc.3`, which includes the following
    term:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在继续进行，测试并更新。通过对`nuc.2`调用`add1`（此处未显示输出），你会发现下一个最显著的添加项是`pt`（仅略微更为显著）。更新为一个新对象`nuc.3`，它包含以下项：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Then test again, using `add1` on `nuc.3`. You’ll find weak evidence to additionally
    include a main effect for `ne`, so update with that inclusion to create `nuc.4`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然后再次测试，使用`add1`对`nuc.3`进行测试。你会发现有弱证据表明可以额外加入`ne`的主效应，因此更新并包含它，生成`nuc.4`。
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: At this point, you may be reasonably certain there won’t be any more useful
    additions, but check with one final call to `add1` on the latest fit to be thorough.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可以相当确定不会有更多有用的添加项，但还是通过对最新拟合模型进行最后一次`add1`调用来彻底检查。
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Indeed it appears that none of the remaining covariates, if included in the
    model, would yield a statistically significant improvement in goodness-of-fit,
    so your final model will stay at `nuc.4`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 确实看来，如果将其纳入模型，剩余的协变量都不会显著提高拟合优度，因此你的最终模型将保持为`nuc.4`。
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This method may seem a little cumbersome, and it’s sometimes difficult to decide
    on the fullest model to be used as the `scope`, but it’s a remarkably good way
    to stay involved at every stage of the selection process so you can consider each
    addition carefully. Note, however, that there’s an element of subjectivity; it’s
    possible to arrive at different final models by choosing one addition over another,
    such as if you’d added `pt` instead of `date` (they had similar levels of significance
    in the very first call to `add1`).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可能看起来有点繁琐，有时很难决定用哪个最完整的模型作为`scope`，但它是一个非常好的方式，让你在每个选择阶段都能保持参与，从而仔细考虑每个添加项。然而，请注意，存在一定的主观性；通过选择一个项而非另一个，可能会得出不同的最终模型，比如你可能会添加`pt`而不是`date`（它们在第一次调用`add1`时具有相似的显著性水平）。
- en: '***22.2.3 Backward Selection***'
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***22.2.3 后向选择***'
- en: After learning forward selection, understanding *backward selection* (or *elimination*)
    isn’t much of a stretch. As you might have guessed, where forward selection starts
    from a reduced model and works its way up to a final model by adding terms, backward
    selection starts with your fullest model and systematically drops terms. The R
    functions for this process are `drop1` to inspect the partial *F*-tests and `update`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 学习了前向选择后，理解*后向选择*（或*淘汰法*）并不难。如你所猜测的那样，前向选择是从一个简化的模型开始，通过添加项逐步构建最终模型，而后向选择则从最完整的模型开始，系统地去除项。这个过程的R函数是`drop1`，用于检查部分*F*检验，以及`update`。
- en: The choice of forward versus backward model selection is usually made on a case-by-case
    basis. If your fullest model isn’t known or is difficult to define and fit, then
    forward selection is typically preferred. On the other hand, if you do have a
    natural and easily fitted fullest model, then backward selection can be more convenient
    to implement. Sometimes, researchers will perform both to see whether the final
    model they arrive at is different (a perfectly possible occurrence).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 前向选择与后向选择的选择通常是根据具体情况决定的。如果最完整的模型未知或难以定义和拟合，则通常倾向于使用前向选择。另一方面，如果你有一个自然且容易拟合的最完整模型，那么后向选择可能更方便实现。有时，研究人员会同时进行两者，以查看他们最终得到的模型是否有所不同（这是完全可能的情况）。
- en: Revisit the `nuclear` example. First, define the fullest model as that which
    predicts `cost` by main effects of all available covariates (as you did in your
    use of `scope` in the forward selections).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 再次回顾`nuclear`示例。首先，定义最完整的模型，即通过所有可用协变量的主效应来预测`cost`（就像你在前向选择中使用`scope`时所做的那样）。
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: There are clearly several predictors that appear not to contribute significantly
    to the response, and these same results are evident the first time you use `drop1`
    to examine the impact on goodness-of-fit that would occur from dropping each variable.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，有几个预测变量似乎对响应变量没有显著贡献，这些相同的结果在你第一次使用`drop1`检查每个变量删除后对拟合优度的影响时也很明显。
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: One handy feature of `drop1` is that its `scope` argument is optional. If you
    don’t include `scope`, it defaults to the intercept-only model as the “most-reduced”
    model, which is usually a reasonable choice.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`drop1`的一个方便功能是，它的`scope`参数是可选的。如果你不包含`scope`，它默认为仅截距模型作为“最简化”模型，通常这是一个合理的选择。'
- en: Before diving right into the deletion process, remind yourself of the interpretation
    of what you’re doing. Just as adding any term will always improve the goodness-of-fit
    in forward selection, deleting any term in backward selection will always worsen
    the goodness-of-fit. The real question is the perceived significance of these
    changes in fit quality. In the same way as earlier, where you wanted to add only
    those terms that offer a *statistically significant improvement* in goodness-of-fit,
    when dropping terms, you only want to remove those that *do not* result in a statistically
    significant *detriment* to goodness-of-fit. As such, backward selection is the
    complete reverse of forward selection in the way it’s carried out.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在直接进入删除过程之前，提醒自己你所做的事情的解释。就像在前向选择中添加任何项总是会改善拟合优度一样，在后向选择中删除任何项总是会使拟合优度变差。真正的问题是这些拟合质量变化的显著性。就像之前一样，你只希望添加那些在拟合优度上提供*统计显著改进*的项，在删除项时，你只希望删除那些*不会*导致拟合优度的*统计显著恶化*的项。因此，后向选择是前向选择的完全反向过程。
- en: So, from the output of `drop1`, you want to choose the term to remove from the
    model that has the least significant effect of reducing the goodness of the fit.
    In other words, you’re looking for the term with the largest, nonsignificant *p*-value
    for its partial *F*-test—because dropping a term with a significantly small *p*-value
    would significantly worsen the predictive capability of the regression model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，从`drop1`的输出中，你需要选择从模型中删除的项，它对拟合优度的影响最小。换句话说，你在寻找的是其部分*F*检验中*p*-值最大且不显著的项——因为删除*p*-值显著小的项会显著恶化回归模型的预测能力。
- en: In the current example, it seems the predictor `bw` has the single least significant
    effect on reducing the goodness-of-fit, so let’s start the update by removing
    that term from `nuc.0`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前示例中，似乎预测变量`bw`对拟合优度的减少影响最不显著，因此让我们从`nuc.0`中删除该项开始更新。
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Use of `update` in this selection algorithm is the same as before; now, though,
    you use a `-` to signify the deletion of a term following the standard “what’s
    already there” `.~.` notation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个选择算法中使用`update`和之前一样；不过现在，你使用`-`符号表示删除项，遵循标准的“已经存在的”`.~.`符号表示法。
- en: 'The process is then repeated using the latest model `nuc.1`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用最新的模型`nuc.1`重复此过程：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: It would seem that `pt` is the next most sensible main effect to drop. Do so
    and name the resulting object `nuc.2`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来`pt`是下一个最合理的主效应删除项。执行删除，并将结果对象命名为`nuc.2`。
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now keep going, rechecking with a call to `drop1` (not shown), and you’ll find
    that the predictor `t1` reveals itself as another viable deletion. Update your
    model with that predictor deleted; name the model object `nuc.3`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在继续前进，通过调用`drop1`进行重新检查（未显示），你会发现预测变量`t1`表现为另一个可以删除的项。更新你的模型并删除该预测变量；将模型对象命名为`nuc.3`。
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Recheck the new `nuc.3` with `drop1`. You should now find the effect of `ct`
    remains nonsignificant, so delete that and update again, giving you a new `nuc.4`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 重新检查新的`nuc.3`与`drop1`。你现在应该发现`ct`的效应仍然不显著，因此删除该项并再次更新，得到新的`nuc.4`。
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Perform yet another check with `drop1`, this time on `nuc.4`. At this point,
    you might hesitate in removing any more predictors, with significance at varying
    strengths being associated with the effect of their deletion. Note, however, that
    for at least three of the remaining predictors, `t2`, `pr`, and `cum.n`, the statistical
    significance should probably be considered borderline at best—all of their *p*-values
    lie between the conventional cutoff levels of *β* = 0.01 and *β* = 0.05\. This
    again emphasizes the active role a researcher must play in model selection algorithms
    such as forward or backward elimination; whether you should delete any more variables
    from here is a difficult question to answer and is left up to your judgment.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用`drop1`进行检查，这次检查的是`nuc.4`。此时，你可能会对再删除任何预测变量感到犹豫，因为删除这些变量会与它们的显著性强度相关。然而需要注意的是，对于剩余的至少三个预测变量——`t2`、`pr`和`cum.n`，它们的统计显著性可能最多只能算是边缘显著——它们的*p*-值都在传统的显著性水平*β*
    = 0.01和*β* = 0.05之间。这再次强调了研究人员在选择模型时必须发挥的主动作用，尤其是在前向或后向选择算法中；是否要删除更多的变量是一个难以回答的问题，最终取决于你的判断。
- en: Let’s remain with `nuc.4` as the final model. Summarizing, you’re able to see
    the estimated regression parameters and the usual post-fit statistics.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将`nuc.4`作为最终模型。总结一下，你可以看到估计的回归参数以及通常的拟合后统计数据。
- en: '[PRE22]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Immediately, you can see that your final model from forward selection in [Section
    22.2.2](ch22.xhtml#ch22lev2sec210) is different from the final model selected
    here, despite the fullest model being the same for both. How has that occurred?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 立刻你就可以看到，尽管两者的完整模型相同，但你在[第22.2.2节](ch22.xhtml#ch22lev2sec210)中从前向选择得到的最终模型与这里选出的最终模型不同。这是怎么发生的呢？
- en: The answer, simply put, is that the predictors present in a model affect each
    other. Remember that the estimated coefficients of present predictors easily change
    in value as you control for different variables. As the number of predictor terms
    increases, these relationships become more and more complex, so both the order
    and direction of the selection algorithm have the potential to lead you on different
    paths through the selection process and arrive at different final destinations,
    which is exactly what’s happened here.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，答案是模型中的预测变量相互影响。记住，当前预测变量的估计系数在你控制不同变量时很容易发生变化。随着预测变量项数的增加，这些关系变得越来越复杂，因此选择算法的顺序和方向可能会引导你走上不同的路径，并得出不同的最终结论，这正是这里发生的情况。
- en: As a perfect example of this, consider the main effect of `pt` in the `nuclear`
    data. In forward selection, `pt` was added because it offered the “most significant”
    improvement to the model `cost~date+cap`. In backward selection, `pt` was removed
    early, since it offered the least reduction in goodness-of-fit if taken from the
    model `cost~date+t1+t2+cap+pr+ne+ct+cum.n+pt`. What this means is that for the
    latter model, the contribution that `pt` might make in terms of predicting the
    outcome is already explained by the other present predictor terms. In the smaller
    model, that effect had not yet been explained, and so `pt` was an attractive addition.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个完美的例子，考虑`nuclear`数据中`pt`的主效应。在前向选择中，`pt`被加入，因为它对模型`cost~date+cap`的改进最为“显著”。在后向选择中，`pt`被早早移除，因为它在从模型`cost~date+t1+t2+cap+pr+ne+ct+cum.n+pt`中移除时，对拟合优度的减少最小。这意味着，对于后者模型，`pt`可能在预测结果方面的贡献，已经被其他现有的预测变量解释了。在较小的模型中，这一效应尚未被解释，因此`pt`是一个有吸引力的新增变量。
- en: All this serves to highlight the fickle nature of most selection algorithms,
    in spite of the systematic way they’re implemented. It’s important to acknowledge
    that a final model fit will probably vary between approaches and that you should
    view these selection methods more as helpful guidelines for finding the most parsimonious
    model and not as providing a universal, definitive solution.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这一切都突显了大多数选择算法的反复无常，尽管它们的实施方式是系统的。重要的是要认识到，最终模型的拟合可能会因不同方法而异，你应该将这些选择方法视为找到最简约模型的有用指南，而不是提供普适的、最终的解决方案。
- en: '***22.2.4 Stepwise AIC Selection***'
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***22.2.4 步进AIC选择法***'
- en: The application of a series of partial *F*-tests is the most common *test-based*
    model selection method, but it’s not the only tool a researcher has at their disposal.
    You can also locate parsimony by adopting a *criterion-based* approach. One of
    the most famous criterion measures is known as *Akaike’s* *Information Criterion
    (AIC)*. You’ll have noticed this value as one of the columns in the output of
    `add1` and `drop1`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'For a given linear model, AIC is calculated as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e22-3.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: Here, ![image](../images/l.jpg) is a measure of goodness-of-fit named the *log-likelihood*,
    and *p* is the number of regression parameters in the model, excluding the overall
    intercept. The value of ![image](../images/l.jpg) is a direct outcome of the estimation
    procedure used to fit the model, though its exact calculation is beyond the scope
    of this text. The thing to know is that it takes on larger values for better-fitting
    models.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation (22.3)](ch22.xhtml#ch22eq3) produces a value that rewards goodness-of-fit
    with the ![image](../images/f0542-01.jpg) but simultaneously penalizes complexity
    with the 2 × (*p* + 2). The negative sign associated with ![image](../images/l.jpg)
    coupled with the positive sign of the *p* + 2 means that smaller values of AIC
    refer to more parsimonious models.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: To find the AIC for a fitted linear model, you use the `AIC` or `extractAIC`
    functions on the object resulting from `lm`; take a look at the help files of
    these functions to see the technical differences between the two. The value of
    ![image](../images/l.jpg) (and therefore also the AIC) has no direct interpretation
    and is useful only when you compare it against the AIC of another model. You can
    base model selection on the AIC by identifying the fit with the lowest AIC value.
    This is the reason it’s directly reported in the output of `add1` and `drop1`—you
    could decide on which term to add or drop based on the change that results in
    a shift to the smallest AIC, instead of focusing exclusively on the *significance*
    of the change via the *F*-test.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go even further and combine the ideas of forward and backward selection.
    *Stepwise* model selection allows the option to either delete a present term *or*
    add a missing term and is typically implemented with respect to AIC. That is,
    a term is chosen to be added or deleted based on the one move out of all possible
    moves that yields the single biggest reduction in AIC. This affords you more flexibility
    in exploring candidate models on your way to the final model fit—determined as
    the model from which no addition or deletion would reduce the AIC value further.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: It’s possible to implement stepwise AIC selection yourself using either `add1`
    or `drop1` at each stage, but fortunately R provides the built-in `step` function
    to do it for you. Take the `mtcars` data from the `MASS` package from the past
    couple of chapters. Let’s finally try to obtain a model for mean mileage that
    offers the opportunity to include every predictor that’s available.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在每个阶段使用`add1`或`drop1`自己实现逐步AIC选择，但幸运的是，R提供了内建的`step`函数来为你完成这项工作。使用过去几章中的`MASS`包中的`mtcars`数据。让我们最终尝试获得一个关于平均油耗的模型，该模型提供了包括所有可用预测变量的机会。
- en: First, take a look at the documentation in `?mtcars` and a scatterplot matrix
    of the data again to remind yourself of the variables and their format in the
    R data frame object. Then define the starting model (often called the *null* model)
    as the intercept-only model.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，查看`?mtcars`中的文档，并再次查看数据的散点图矩阵，以帮助你回忆变量及其在R数据框对象中的格式。然后定义起始模型（通常称为*空*模型）为仅包含截距的模型。
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Your starting model can be anything you like, provided it falls within the domain
    of the models described by your `scope` argument to be supplied to `step`. In
    this example, define `scope` as the fullest model to be considered—set this to
    be the overly complex model with a four-way interaction among `wt`, `hp`, `cyl`,
    and `disp` (and all relevant lower-order interactions and main effects, via the
    cross-factor operator), as well as main effects for `am`, `gear`, `drat`, `vs`,
    `qsec`, and `carb`. The two multilevel categorical variables, `cyl` and `gear`,
    are explicitly converted to factors to avoid them being treated as numeric (refer
    to [Section 20.5.4](ch20.xhtml#ch20lev2sec188)).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你的起始模型可以是你喜欢的任何模型，只要它符合你为`step`提供的`scope`参数描述的模型范围。在这个例子中，定义`scope`为要考虑的最完整模型——设置为一个过于复杂的模型，包含`wt`、`hp`、`cyl`和`disp`之间的四阶交互作用（以及通过交叉因子操作符获得的所有相关低阶交互作用和主效应），同时包括`am`、`gear`、`drat`、`vs`、`qsec`和`carb`的主效应。这两个多级分类变量，`cyl`和`gear`，被显式转换为因子，以避免它们被当作数值型处理（参见[第20.5.4节](ch20.xhtml#ch20lev2sec188)）。
- en: The potential for interactions in the final model will serve to highlight an
    especially important (and convenient) feature of `add1`, `drop1`, and `step`.
    These functions all respect the hierarchy imposed by interactions and main effects.
    That is, for `add1` (and `step`), an interactive term will not be provided as
    an option for addition unless all relevant lower-order effects are already present
    in the current fitted model; similarly, for `drop1` (and `step`), an interactive
    term or main effect will not be provided as an option for deletion unless all
    relevant higher-order effects are already gone from the current fitted model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最终模型中的交互作用潜力将突出显示`add1`、`drop1`和`step`的一个特别重要（且方便）的特点。这些函数都会遵循交互作用和主效应所强加的层次结构。也就是说，对于`add1`（和`step`），除非所有相关的低阶效应已经出现在当前的拟合模型中，否则交互项不会作为添加选项提供；同样地，对于`drop1`（和`step`），除非所有相关的高阶效应已经从当前拟合模型中删除，否则交互项或主效应不会作为删除选项提供。
- en: The `step` function itself returns a fitted model object and by default provides
    a comprehensive report of each stage of selection. Let’s call it now; for print
    reasons, some of the output has been snipped out, so you’re encouraged to bring
    this up on your own machine.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`step`函数本身返回一个拟合的模型对象，并且默认会提供每个选择阶段的详细报告。现在让我们调用它；为了打印输出的方便，一些输出已经被剪切掉了，所以你可以在自己的机器上查看完整的输出。'
- en: '[PRE24]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Each block of output displays the current model fit, its AIC value, and a table
    showing the possible moves (either adding `+`, deleting `-`, or doing nothing
    `<none>`). The AIC value that would result from each move alone is listed, and
    these potential single moves are ranked from smallest to largest AIC value.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输出块展示了当前模型的拟合情况、其AIC值，以及一个显示可能操作的表格（包括添加`+`、删除`-`或不做任何操作`<none>`）。每个操作单独执行后的AIC值会列出，并且这些潜在的单一操作会按从小到大的AIC值进行排序。
- en: As the algorithm proceeds, you see the `<none>` row creeping its way up the
    table. For example, in the first table, the value of the AIC for the intercept-only
    model is 115.94\. The biggest reduction in AIC would result from adding a main
    effect for `wt`; that move is made, and the effect of subsequent moves on the
    AIC is reassessed. Also note that the addition of the two-way interaction term
    between `wt` and `factor(cyl)` is considered only at the third step, after the
    main effects of those predictors have been added. That particular two-way interaction
    never ends up being included, though, because the main effect of `hp` is preferable
    at that third step, and subsequent interactions involving `hp` then offer a much
    better reduction in the AIC value in the fourth step. In fact, at the fifth step,
    actually *deleting* the main effect for `factor(cyl)` is deemed to reduce the
    AIC most, and so the tables for the sixth and seventh steps no longer include
    that `wt:factor(cyl)` term as an option. The sixth step suggests that adding the
    main effect for `qsec` offers a minor reduction in the AIC, so this is done. The
    seventh table signals the end of the algorithm because doing nothing offers the
    lowest AIC value and doing anything else would increase the AIC (shown through
    `<none>` taking pole position in that last table).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The final model is stored as the object `car.step`; by summarizing it, you’ll
    note that almost 90 percent of the variation in the response is explained by weight,
    horsepower, and their interaction, as well as the slightly curious main effect
    of `qsec` (which itself is not deemed statistically significant).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: From this, it seems it would be worthwhile to investigate this predictor further,
    to establish validity of the fitted model (see [Section 22.3](ch22.xhtml#ch22lev1sec74)),
    and perhaps to try transformations of the data (such as modeling GPM instead of
    MPG; see [Section 21.4.3](ch21.xhtml#ch21lev2sec201)) to see whether this effect
    persists in subsequent runs of the stepwise AIC algorithm. The presence of `qsec`
    in the final model illustrates the fact that the selection of the model wasn’t
    based solely on the significance of predictor contribution but on a criterion-based
    measure aiming for its own definition of parsimony.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: The AIC is sometimes criticized for a tendency to err on the side of more complexity
    and higher *p*-values. To balance this, you can increase the penalizing effect
    of extra predictors by increasing the multiplicative contribution of the (*p*
    + 2) on the right of [Equation (22.3)](ch22.xhtml#ch22eq3); though the standard
    multiplicative factor of 2 is used in the majority of cases (in `step` you can
    use the optional argument `k` to change this). That being said, criterion-based
    measures are incredibly useful when you have models that aren’t nested (ruling
    out the partial *F*-test) and you want to compare them for quick identification
    of the one that, potentially, provides the most parsimonious representation of
    the data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: AIC有时因倾向于偏向更复杂的模型和更高的*p*值而受到批评。为平衡这一点，你可以通过增加右侧[方程式(22.3)](ch22.xhtml#ch22eq3)中(*p*
    + 2)的乘法贡献，来增强额外预测变量的惩罚效应；尽管在大多数情况下，标准的乘法因子为2（在`step`函数中，你可以使用可选参数`k`来更改此值）。话虽如此，基于准则的度量在你拥有非嵌套模型时（排除了部分*F*-检验）非常有用，这时你可以快速比较它们，找出可能提供最简洁数据表示的模型。
- en: '**Exercise 22.1**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**习题 22.1**'
- en: In [Sections 22.2.2](ch22.xhtml#ch22lev2sec210) and [22.2.3](ch22.xhtml#ch22lev2sec211),
    you used forward and backward selection approaches to find a model for predicting
    the cost of the construction of nuclear power plants (based on the `nuclear` data
    frame in the `boot` package).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第22.2.2节](ch22.xhtml#ch22lev2sec210)和[第22.2.3节](ch22.xhtml#ch22lev2sec211)中，你使用了前向选择和后向选择方法来寻找一个用于预测核电站建设成本的模型（基于`boot`包中的`nuclear`数据框）。
- en: Using the same fullest model (in other words, main effects of all present predictors
    only), use stepwise AIC selection to find a suitable model for the data.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用相同的最全模型（换句话说，只包含所有现有预测因子的主效应），使用逐步AIC选择来找到适合数据的模型。
- en: Does the final model found in (a) match either of the models resulting from
    the earlier use of forward and backward selection? How does it differ?
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 (a) 中找到的最终模型与之前使用前向选择和后向选择得到的模型是否匹配？它们有什么不同？
- en: '[Exercise 21.2](ch21.xhtml#ch21exc2) on [page 512](ch21.xhtml#page_512) detailed
    Galileo’s ball data. Enter these as a data frame in your current R workspace if
    you haven’t already.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[习题 21.2](ch21.xhtml#ch21exc2)在[第512页](ch21.xhtml#page_512)详细介绍了伽利略的球数据。如果你还没有这样做，将这些数据作为数据框输入到你当前的R工作空间中。'
- en: Fit five linear models to these data with distance as the response—an intercept-only
    model and four separate polynomial models of increasing order 1 to 4 in height.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对这些数据拟合五个线性模型，以距离作为响应变量——一个仅包含截距的模型和四个逐渐增加阶数1到4的多项式模型。
- en: Construct a table of partial *F*-tests to identify your favored model for distance
    traveled. Is your selection consistent with [Exercise 21.2](ch21.xhtml#ch21exc2)
    (b) and (c)?
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个部分*F*-检验表，以确定你偏好的距离旅行模型。你的选择是否与[习题21.2](ch21.xhtml#ch21exc2) (b) 和 (c) 一致？
- en: You first encountered the `diabetes` data frame in the contributed `faraway`
    package in [Section 21.5.2](ch21.xhtml#ch21lev2sec203), where you modeled the
    mean total cholesterol. Load the package and inspect the documentation in `?diabetes`
    to refresh your memory of the data set.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你第一次遇到`diabetes`数据框是在贡献的`faraway`包的[第21.5.2节](ch21.xhtml#ch21lev2sec203)中，在那里你建立了总胆固醇的均值模型。加载该包并通过`?diabetes`检查文档，以便回顾数据集的内容。
- en: 'There are some missing values in `diabetes` that might interfere with model
    selection algorithms. Define a new version of the `diabetes` data frame that deletes
    all rows with a missing value in any of the following variables: `chol`, `age`,
    `gender`, `height`, `weight`, `frame`, `waist`, `hip`, `location`. Hint: Use `na.omit`
    or your knowledge of record extraction or deletion for a data frame. You can create
    the required vector of row numbers to be extracted or deleted using `which` and
    `is.na`, or you can try using the `complete.cases` function to obtain a logical
    flag vector—inspect its help file for details.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`diabetes`数据框中有一些缺失值，这可能会干扰模型选择算法。定义一个新的`diabetes`数据框版本，删除以下变量中任何一个存在缺失值的行：`chol`、`age`、`gender`、`height`、`weight`、`frame`、`waist`、`hip`、`location`。提示：使用`na.omit`或者你对数据框中记录提取或删除的知识。你可以使用`which`和`is.na`创建需要提取或删除的行号向量，或者你可以尝试使用`complete.cases`函数来获得一个逻辑标志向量——查看其帮助文件以了解更多细节。
- en: Use your data frame from (e) to fit two linear models with `chol` as the response.
    The null model object, named `dia.null`, should be an intercept-only model. The
    full model object, named `dia.full`, should be the overly complex model with a
    four-way interaction (and all lower-order terms) among `age`, `gender`, `weight`,
    and `frame`; a three-way interaction (and all lower-order terms) among `waist`,
    `height`, and `hip`; and a main effect for `location`.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Starting from `dia.null` and using the same terms as in `dia.full` for `scope`,
    implement stepwise selection by AIC to choose a model for mean total cholesterol
    and then summarize.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use forward selection based on partial *F*-tests with a conventional significance
    level of *α* = 0.05 to choose a model, again starting from `dia.null`. Is the
    result here the same as the model arrived at in (g)?
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stepwise selection doesn’t have to start from the simplest model. Repeat (g),
    but this time, set `dia.full` to be the starting model (you don’t need to supply
    anything to `scope` if you’re starting from the most complex model). What is the
    final model selected via AIC if you start from `dia.full`? Is it different than
    the final model from (g)? Why is this or is this not the case, do you think?
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Revisit the ubiquitous `mtcars` data frame from the `MASS` package.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: In [Section 22.2.4](ch22.xhtml#ch22lev2sec212), you used stepwise AIC selection
    to model mean MPG. The selected model included a main effect for `qsec`. Rerun
    the same AIC selection process, but this time, do it in terms of GPM=1/MPG. Does
    this change the complexity of the final model?
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '***22.2.5 Other Selection Algorithms***'
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Any model selection algorithm will always aim to quantitatively define *parsimony*
    and suggest a model that optimizes that definition in light of the available data.
    There are alternatives to AIC, such as the *corrected AIC (AIC[c])* or the *Bayesian
    Information Criterion (BIC)*, both of which impose heavier penalties on complexity
    than the default AIC in (22.3).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes it’s tempting to simply monitor *R*², the coefficient of determination,
    for a series of models. However, as mentioned in [Section 22.2.1](ch22.xhtml#ch22lev2sec209),
    this on its own is inadequate for choosing between models since it doesn’t penalize
    complexity and will generally always increase as you continue to add predictors,
    whether they have a statistically significant impact or not. The *adjusted R*²
    *statistic*, denoted ![image](../images/r2.jpg) and reported as `Adjusted R-squared`
    in `summary`, is a simple transformation of the original *R*² that does incorporate
    a penalty for complexity relative to the sample size *n*; calculated as
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0548-01.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: where *p* is the number of predictor terms (excluding the intercept). The algorithms
    based on tests and criteria are always preferable (since interpretation of ![image](../images/r2.jpg)
    can be difficult), but monitoring ![image](../images/r2.jpg) can be useful as
    a quick check between nested models—a higher value points to a preferred model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: For further reading, [Chapter 8](ch08.xhtml#ch08) of Faraway ([2005](ref.xhtml#ref21))
    provides some excellent commentary on the guideline-only nature of both test-
    and criterion-based model selection procedures. Regardless of which approach you
    employ, always remember that any final model reached by using these algorithms
    should still be subject to scrutiny.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '**22.3 Residual Diagnostics**'
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In previous chapters, you examined the practical aspects of multiple linear
    regression models, such as fitting and interpreting, dummy coding, transforming,
    and so on, but you haven’t yet looked at methods that are essential for determining
    the *validity* of your model. The final part of this chapter will introduce you
    to *model diagnostics*, the primary goal of which is to ensure that your regression
    model is valid and accurately represents the relationships in your data. For this,
    I’ll return focus to the theoretical assumptions underpinning the multiple linear
    regression model that were noted early in [Section 21.2.1](ch21.xhtml#ch21lev2sec190).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'As a refresher, in general when fitting these models, remember to keep these
    four things in mind:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '**Errors** The error term *ɛ*, which defines the departure of any observation
    from the fitted mean-outcome model, is assumed to be normally distributed with
    a mean of zero and a constant variance denoted with *σ*². The error associated
    with a given observation is also assumed to be independent of the error of any
    other observation. If a fitted model suggests violation of any of these assumptions,
    you’ll need to investigate further (usually involving refitting a variation of
    the model).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '**Linearity** It is critical to be able to assume that the mean response as
    a function is linear in terms of the regression parameters *β*[0], *β*[1], . .
    ., *β*[*p*]. Though transformations of individual variables and the presence of
    interactions can relax the specific nature of the estimated trends somewhat, any
    diagnostic suggestion that a relationship is nonlinear (and hence not being captured
    by the fitted model at hand) must be investigated.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '**Extreme or unusual observations** Always inspect extreme data points or data
    points that strongly influence the fitted model—for example, points that have
    been recorded incorrectly should be removed from the analysis.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '**Collinearity** Predictors highly correlated with one another can adversely
    affect an entire model, meaning it can be easy to misinterpret the effects of
    any included predictors. This should be avoided in any regression.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: You investigate the first three after fitting the model using diagnostic tools.
    Any violation of these assumptions diminishes the reliability of your model, sometimes
    severely. Collinearity and/or extreme observations can be discovered by basic
    statistical explorations (for example, viewing scatterplot matrices) of the raw
    data pre-fit, but any consequential effects are appraised post-fit.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: There are some statistical tests you can perform to diagnose a statistical model,
    but commonly a diagnostic inspection boils down to interpretation of the results
    of graphical tools designed to target specific assumptions. Interpreting these
    plots can be quite difficult and only really becomes easier with experience. Here,
    I’ll provide an overview of these tools in R and describe some common things to
    look for. For a more detailed discussion, look to dedicated texts on regression
    methods such as Chatterjee et al. ([2000](ref.xhtml#ref12)), Faraway ([2005](ref.xhtml#ref21)),
    or Montgomery et al. ([2012](ref.xhtml#ref48)).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '***22.3.1 Inspecting and Interpreting Residuals***'
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you look back at the plot in [Figure 20-2](ch20.xhtml#ch20fig2) on [page
    456](ch20.xhtml#page_456), you’ll see a good demonstration of the importance of
    interpreting the results given by *ŷ* as a mean response value. Under the assumed
    model, any deviation of the raw observations from the fitted line is deemed to
    be the result of the (normally distributed) errors defined by the *ɛ* term in
    [Equation (20.1)](ch20.xhtml#ch20eq1) on [page 453](ch20.xhtml#page_453).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Of course, in practice, you don’t have the true error values because you don’t
    know the true model of your data. For the *i*th response observation *y*[*i*]
    and its fitted value from the model, *ŷ*[*i*], you would typically assess diagnostic
    plots using the estimated residuals *e*[*i*] = *y*[*i*] − *ŷ*[*i*]. A call to
    `summary` even encourages a post-fit analysis of the residuals by providing you
    with a five-number summary of the *e*[*i*] above the table of estimated coefficients.
    This allows you to take a look at their values and do a preliminary numeric assessment
    of the symmetry of their distribution (as required by the assumption of normality—see
    [Section 22.3.2](ch22.xhtml#ch22lev2sec215)).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: As well as a diagnostic inspection of the raw residuals *e*[*i*], some diagnostic
    checks can also be done using their *standardized* (or *Studentized*) values.
    The standardized residuals rescale the raw residuals *e*[*i*] to ensure they all
    have the same variance, which is important if you need to directly compare them
    to one another. Formally, this is achieved with the calculation ![image](../images/f0550-01.jpg),
    where ![image](../images/o.jpg) is the estimate of the residual standard error
    and *h*[*ii*] is the *leverage* of the *i*th observation (you’ll learn about leverage
    in [Section 22.3.4](ch22.xhtml#ch22lev2sec217)).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Arguably the most common graphical tool used for a post-fit analysis of the
    residuals is a simple scatterplot of the “observed-minus-fitted” raw residuals
    on the vertical axis against their corresponding fitted-model values from the
    regression. If the assumptions concerning *ɛ* are valid, then the *e*[*i*] should
    appear randomly scattered around zero (since the errors aren’t assumed to be related
    in any way to the value of the response). Any systematic pattern in the plot suggests
    the residuals don’t agree with the error assumptions—this could be because of
    nonlinear relationships in your data or the presence of dependent observations
    (in other words, your data points are correlated and therefore not independent
    of one another). The plot can also be used to detect *heteroscedasticity*—a nonconstant
    variance in the residuals—commonly seen as a “fanning out” of the residuals around
    0 as the fitted values increase.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Note once more that it’s important these theoretical assumptions are valid because
    they affect the validity of the estimates of the regression coefficients and the
    reliability of their standard errors (and so statistical significance)—in other
    words, the correctness of your interpretation of their impact on the response.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: To give you a better idea of all this, consider the three images in [Figure
    22-1](ch22.xhtml#ch22fig1). These provide residuals versus fitted values plots
    for three hypothetical scenarios.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: The plot on the left is, more or less, what you’re looking for—the residuals
    appear randomly scattered around zero, and their spread around zero appears constant
    (*homoscedasticity*). In the middle plot, however, you can see systematic behavior
    in the residuals. Though the variability still seems to remain constant throughout
    the range of fitted values, the apparent trend suggests the current model isn’t
    explaining some of the relationship between response and predictor (or predictors).
    On the right, the residuals seem to be scattered randomly about zero again. However,
    the variability they exhibit isn’t constant. Among other things, this kind of
    heteroscedasticity will affect the reliability of your confidence and prediction
    intervals.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f22-01.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-1: Three impressions of a hypothetical residuals versus fitted diagnostic
    plot from a linear regression: random (left), systematic (middle), and heteroscedastic
    (right)*'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to know that even if your graphical diagnostics don’t provide
    the well-behaved plot like the hypothetical example on the left of [Figure 22-1](ch22.xhtml#ch22fig1),
    this is not a reason to immediately give up on the analysis. These plots can form
    an integral part in finding an appropriate model for your data. You can often
    reduce nonlinearity by including additional predictors or interactions, changing
    the treatment of a categorical variable, or performing nonlinear transformations
    of certain continuous predictors. Heteroscedasticity, especially the kind in [Figure
    22-1](ch22.xhtml#ch22fig1) where the variability is higher for higher fitted values,
    is common in some fields of research. A first step to remedy this problem often
    involves a simple log transformation of the response followed by a reinspection
    of the diagnostics.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: It’s time for an example. In [Section 22.2.4](ch22.xhtml#ch22lev2sec212), you
    used stepwise AIC selection to choose a model for MPG for the `mtcars` data, creating
    the object `car.step`. Let’s now diagnose that same fit to see whether there are
    any problems with the assumptions of the model.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'When you apply the `plot` function directly to an `lm` object, it can conveniently
    produce six types of diagnostic plot of the fit. By default, four of these plots
    are produced in succession. Follow the signal to the user in the console, Hit
    <Return> to see next plot, to progress through them. In the examples that follow,
    however, you’ll select each plot individually using the optional `which` argument
    (specified by the integers `1` through `6`; see `?plot.lm` for the documentation).
    The residuals versus fitted plot is given with `which=1`; the following line produces
    the plot on the left in [Figure 22-2](ch22.xhtml#ch22fig2):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As you can see, R adds a smoothed line to help the user interpret any trend,
    though this shouldn’t be used exclusively in any judgment. By default, the three
    most extreme points from zero are annotated (according to the `rownames` attribute
    of the data frame used in the call that fitted the model). The model formula itself
    is specified below the horizontal axis label.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: From this, you see that the residuals versus fitted plot for the `car.step`
    offers little, if any, cause for concern. There isn’t much of a discernible trend,
    and you can take further comfort in the fact that the errors (*e*[*i*]) appear
    homoscedastic in their distribution.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f22-02.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-2: Residuals versus fitted and scale-location diagnostic plots for
    the* `car.step` *model*'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: The *scale-location* plot is similar to the residuals versus fitted plot, though
    instead of the raw *e*[*i*] on the vertical axis, the scale-location plot provides
    ![image](../images/f0552-01.jpg), that is, the square root of the absolute value
    (denoted by | · |; this renders all negative values positive) of the standardized
    residuals. These are plotted against the respective fitted values on the horizontal
    axis. By restricting attention to the magnitude of each residual in this way,
    the scale-location plot is used to reveal trends in the size of the departure
    of each data point from its fitted value, as the fitted values increase. This
    means such a plot can, for example, be more useful than the raw residuals versus
    fitted plot in detecting things such as heteroscedasticity. Just as with the original
    residuals versus fitted plot, you’re looking for a plot with no discernible pattern
    as an indication that no error assumptions have been violated.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: The right plot of [Figure 22-2](ch22.xhtml#ch22fig2) shows the scale-location
    plot for `car.step`, selected with `which=3`. This plot also demonstrates the
    ability to remove the default smoothed trend line with the `add.smooth` argument
    and to control how many extreme points are labeled using the `id.n` argument.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As with the original residuals versus fitted plot, there doesn’t seem to be
    much to be concerned about in the scale-location plot for this `mtcars` model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Return to Galileo’s ball-rolling data first laid out in [Exercise 21.2](ch21.xhtml#ch21exc2)
    on [page 512](ch21.xhtml#page_512). In their use in the following example, the
    response variable “distance traveled” is given as column `d`, and the explanatory
    variable “height” is column `h`, in the data frame `gal`. I’ll re-create some
    of the exercise to give you a couple of straightforward examples of cause for
    concern in residual diagnostic plots. Execute the following code to define the
    data frame of the seven observations and fit two regression models—the first linear
    in height and the second quadratic (refer to [Section 21.4.1](ch21.xhtml#ch21lev2sec199)
    for details on polynomial transformations).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, take a look at the three images in [Figure 22-3](ch22.xhtml#ch22fig3),
    created with the following code:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![image](../images/f22-03.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-3: Demonstrating residual diagnostics for Galileo’s ball-rolling
    data. Top left: The raw data with a simple linear trend corresponding to* `gal.mod1`
    *superimposed. Top right: Residuals versus fitted for the linear-trend-only model.
    Bottom: Residuals versus fitted for the quadratic model* `gal.mod2`.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: The top-left plot shows the data and provides the straight line of the simple
    linear model. Although this clearly captures the increasing trend, this plot suggests
    some curvature is also present. The diagnostic residuals versus fitted plot (top-right)
    shows that the linear-trend-only model is inadequate—the systematic pattern throws
    up a red flag concerning the assumptions surrounding the linear model errors.
    The bottom image shows the residuals versus fitted plot based on the quadratic
    version of the model in `gal.mod2`. Including a quadratic term in “height” removes
    this prominent curve in the residuals. However, these latest *e*[*i*] values still
    seem to exhibit systematic behavior in a wavelike form, perhaps suggesting you
    try a cubic model, which is difficult with such a small sample size.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '***22.3.2 Assessing Normality***'
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To assess the assumption that the error is normally distributed, you can use
    a normal QQ plot, as first discussed in [Section 16.2.2](ch16.xhtml#ch16lev2sec142).
    You select `which=2` when calling `plot` on an `lm` object to produce a normal
    quantile-quantile plot of the (standardized) residuals. Return to the `car.step`
    model object and enter the following line to produce [Figure 22-4](ch22.xhtml#ch22fig4).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![image](../images/f22-04.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-4: Normal QQ plot of the residuals from the* `car.step` *model*'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: You interpret the QQ plot of the residuals in the same way as in [Section 16.2.2](ch16.xhtml#ch16lev2sec142).
    The gray diagonal line represents the true normal quantiles, and the plotted points
    are the corresponding numeric quantiles of the estimated regression errors. Normally
    distributed data should lie close to the straight line.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: For the `car.step` regression model, the points generally seem to follow the
    path laid out by the theoretical normal quantiles. There is some deviation, which
    is to be expected, but no apparent major departure from normality.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: There are also other ways to test for normality, such as the famous Shapiro-Wilk
    hypothesis test. The null hypothesis for the Shapiro-Wilk test is that the data
    are normally distributed, so a small *p*-value would suggest non-normality of
    your data (see [Royston, 1982](ref.xhtml#ref55), for technical details). To execute
    the procedure, use the `shapiro.test` function in R. By first extracting the standardized
    residuals of your fitted model with `rstandard`, you’ll see that this test applied
    to `car.step` offers up a large *p*-value.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In other words, there’s no evidence (according to this test) that the residuals
    of `car.step` aren’t normal.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Being able to assume normality of the error term supports the methodology used
    to produce reliable estimates of the regression coefficients. As long as your
    data are *approximately* normal, though, you shouldn’t be too concerned with mild
    indications of non-normality. Some transformations of your data, and an increase
    in your sample size, can reduce concerns about more severe indications of non-normal
    residuals.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '***22.3.3 Illustrating Outliers, Leverage, and Influence***'
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It’s always important to investigate any individual observations that appear
    unusual or extreme compared to the bulk of your observations. In general, an exploratory
    analysis of your data, perhaps involving summary statistics or scatterplot matrices,
    is a good idea since it can help you identify any such values—they have the potential
    to adversely affect your model fits. Before going further, it’s important to clarify
    some frequently used terms.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '**Outlier** This is a general term used to describe an unusual observation
    in the context of the data, as you saw in [Section 13.2.6](ch13.xhtml#ch13lev2sec121).
    In linear regression, an outlier usually has a large residual but is identified
    as an outlier only if it doesn’t conform to the trend of the fitted model. An
    outlier can, but doesn’t always, significantly alter the trends described by the
    fitted model.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '**Leverage** This term refers to the extremity of the values of the present
    predictors. A high-leverage point is an observation with predictor values extreme
    enough to potentially significantly affect the slopes or trends in the fitted
    model. An outlier can have a high or low leverage.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '**Influence** An observation with high leverage that *does* affect the estimated
    trends is deemed influential. In other words, influence is judged only when the
    response value is taken into account alongside the corresponding predictor values.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'These definitions have some overlap, so a given observation can be described
    using a combination of these terms. Let’s look at some hypothetical examples.
    Create the following two vectors of ten supposed responses (`y`) and explanatory
    (`x`) values:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'These will form the bulk of the data. Now, consider the following six objects,
    `p1x` to `p3y`, which will be used to store the predictor and response values
    for three additional observation points:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: That is, point 1 is (1.2,14); point 2 is (5,19); and point 3 is (5,5).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Next, the following four uses of `lm` provide four simple linear model fits.
    The first is the regression of `y` on `x` only. The next three additionally include
    points 1, 2, and 3, separately, as an 11th observation.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, you can use these objects to visually clarify the definitions of *outlier*,
    *leverage*, and *influence*, as shown in [Figure 22-5](ch22.xhtml#ch22fig5). Enter
    the following code to initialize the scatterplot with set axis limits of `x` and
    `y`:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then use `points`, `abline`, and `text` to build the top-left plot of [Figure
    22-5](ch22.xhtml#ch22fig5), as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Create the middle and right plots by replacing `p1x`, `p1y`, and `mod.1` with
    those corresponding to points 2 and 3 and altering the `labels` argument in `text`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f22-05.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-5: Three examples of the definitions of outlier, leverage, and influence
    in a linear regression context. In each plot, the solid line represents the model
    fitted to the original observations in* `x` *and* `y`*, and the dashed line represents
    the model fitted including the extra point, plotted with* ▪.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: In the top-left plot of [Figure 22-5](ch22.xhtml#ch22fig5), the additional point
    is an example of an outlier since it sits away from the bulk of the data *and*
    doesn’t conform to the trend suggested by the original observations. Despite this,
    it’s considered to have low leverage only because of its predictor value of 1.2
    (`p1x`), which isn’t deemed unusual compared to that of the other values of `x`.
    In fact, its proximity to the overall mean of the `x` values indicates that the
    effect of including it, incorporated in `mod.1`, is mainly a modification to the
    original intercept. You could even classify this as a low influence point—the
    overall change to the fitted model seems minimal.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: In the top-right plot, you can see an example of an observation that would *not*
    be considered an outlier. Although point 2 does sit apart from the 10 original
    observations, it conforms quite well to the model fitted to only `x` and `y`,
    which is important in the context of regression. That being said, point 2 is considered
    a high leverage point since it sits at an extreme predictor value compared to
    all other values of `x` (in other words, it has the *potential* to dramatically
    alter the fit should its response value be different). As it stands, it’s a low
    influence point since the model fit itself is barely affected by its inclusion.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the bottom plot shows a clear example of an outlier in a high-leverage
    position that also has a high influence—it sits away from the original 10 observations
    and isn’t a clear part of the original trend; its extreme predictor value means
    high leverage; and its inclusion substantially alters the entire model by dragging
    down the slope and raising the intercept. These ideas remain the same in higher
    dimensions (that is, when you have several predictors) for multiple linear regression
    models.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '***22.3.4 Calculating Leverage***'
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Leverage itself is calculated using the design matrix structure ***X*** defined
    in [Section 21.2.2](ch21.xhtml#ch21lev2sec191). Specifically, if you have *n*
    observations, then the leverage of the *i*th point (*i* = 1, . . . , *n*) is denoted
    *h*[*ii*]. These are the diagonal elements (*i*th row, *i*th column) of the *n*
    × *n* matrix *H* such that
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e22-4.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: 'In R, constructing the design matrix for the 10 illustrative predictor observations
    you defined in [Section 22.3.3](ch22.xhtml#ch22lev2sec216) as `x` is achieved
    in a straightforward fashion using knowledge of `cbind` (refer to [Section 3.1.2](ch03.xhtml#ch03lev2sec25)).
    *H* is subsequently calculated using the corresponding functions for matrix multiplication
    (`%*%`), matrix transposition (`t`), matrix inversion (`solve`), and diagonal
    element extraction (`diag`). Then you can plot the values *h*[*ii*] against the
    values of `x` themselves. The following code produces [Figure 22-6](ch22.xhtml#ch22fig6):'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: You would typically use the built-in R function `hatvalues`, named after the
    style of the matrix algebra in [Equation (22.4)](ch22.xhtml#ch22eq4), to obtain
    the leverages (rather than manually constructing the design matrix ***X*** and
    doing the math yourself). Simply provide `hatvalues` with your fitted model object.
    You can confirm your earlier calculations by using the corresponding `lm` object
    fitted to the `x` and `y` data (`mod.0` created earlier).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![image](../images/f22-06.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-6: Plotting the leverage of the 10 illustrative predictor observations
    in* `x`'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Looking at [Figure 22-6](ch22.xhtml#ch22fig6), the appearance of the leverages
    plotted against the corresponding predictor values themselves makes sense—leverage
    gets progressively higher as you move away from the mean of the predictor data
    in either direction. This is essentially the pattern you’ll see for any plot of
    the raw leverages.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '***22.3.5 Cook’s Distance***'
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Of course, leverage alone isn’t enough to determine the overall influence of
    each observation on a fitted model. For that, the response value must also be
    taken into account.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'Arguably the most well-known measure of influence is *Cook’s distance*, which
    estimates the magnitude of the effect of deleting the *i*th value from the fitted
    model. Cook’s distance for observation *i* (denoted *D*[*i*]) is given with the
    following equation:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/e22-5.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: It turns out that this equation is a specific function of a point’s leverage
    and residual. Here, the value *ŷ*[*j*] is the predicted mean response of observation
    *j* for the model fitted with all *n* observations, and ![image](../images/f0560-01.jpg)
    represents the predicted mean response of observation *j* for the model fitted
    *without* the *i*th observation. As usual, the term *p* is the number of predictor
    regression parameters (excluding the intercept), and the value ![image](../images/o.jpg)
    is the estimate of the residual standard error.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Put simply, the larger the value of *D*[*i*], the larger the influence the *i*th
    observation has on the fitted model, meaning outlying observations in high-leverage
    positions will correspond to higher values of *D*[*i*]. The important question
    is, how big does *D*[*i*] have to be in order for point *i* to be considered influential?
    In practice, this is difficult to universally answer, and there’s no formal hypothesis
    test for it, but there are several rule-of-thumb cutoff values. One such rule
    states that if *D*[*i*] > 1, the point should be considered influential; another,
    more sensitive rule suggests *D*[*i*] > 4/*n* (see, for example, [Bollen and Jackman,
    1990](ref.xhtml#ref08); [Chatterjee et al., 2000](ref.xhtml#ref12)). It’s generally
    advised that you compare multiple Cook’s distances for a given fitted model rather
    than analyzing one single value, and that any point corresponding to a comparatively
    large *D*[*i*] might need further inspection.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Continue with the objects created in [Section 22.3.3](ch22.xhtml#ch22lev2sec216),
    with the 10 illustrative observations in `x` and `y`, as well as the additional
    point defined in `p1x` and `p1y`. The linear regression model fitted to those
    data was stored as the object `mod.1`. It’s a good exercise to write some code
    that calculates the Cook’s distance measures following (22.5).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'To that end, enter the following code in the R editor:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: First, create new objects `x1` and `y1` to hold all 11 observations. The objects
    `n`, `param`, and `sigma` extract the data set size, the total number of estimated
    regression parameters (two in this case), and the estimated residual standard
    error for the model originally fitted to all 11 data points. The latter two items,
    `param` and `sigma`, represent (*p* + 1) and ![image](../images/o.jpg) in [Equation
    (22.5)](ch22.xhtml#ch22eq5), respectively. The object `yhat.full` uses the `fitted`
    function on the object `mod.1` to provide the fitted mean response values, representing
    the *ŷ*[*j*] values in (22.5).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: To store the Cook’s distances, a vector `cooks` of 11 positions is created (initialized
    to be filled with `NA`s) with `rep`. Now, to calculate each *D*[*i*] value, set
    a `for` loop (see [Chapter 10](ch10.xhtml#ch10)) to scroll through each index
    from `1` to `11`. The first step of the loop is to create two temporary vectors
    `temp.x` and `temp.y` to be `x1` and `y1` with the observation at index `i` removed.
    A new temporary linear model is fitted to `temp.y` based on `temp.x`; then `predict`
    finds the mean responses from `temp.model` for each of the 11 predictor values
    (in other words, including the one that was deleted). As such, the resulting vector
    `temp.fitted` represents the ![image](../images/f0560-01.jpg) values in [Equation
    (22.5)](ch22.xhtml#ch22eq5). Finally, `sum` and the product of `param` with `sigma^2`
    compute *D*[*i*], and the result is stored in `cooks[i]`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'After highlighting and executing the code, the resulting Cook’s distances are
    as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Unsurprisingly, the largest value of these is the last one, at around 0.314\.
    This corresponds to the 11th observation in `x1` and `y1`, which is the additional
    point originally defined in `p1x` and `p1y`. The value 0.314 is less than 1 and
    less than 4/11 = 0.364, the cutoffs defined by the earlier rules of thumb. This
    ties in with the assessment of the top-left image in [Figure 22-5](ch22.xhtml#ch22fig5)—that
    the influence of the point `p1x` and `p1y` is minimal when compared to the influence
    of a point like `p3x` and `p3y` in the rightmost image.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Just as the `hatvalues` function computes the leverages for you, the builtin
    `cooks.distance` function does the same for the *D*[*i*]. You can confirm the
    previous values in `cooks`, which are based on `mod.1`.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: R automatically calculates and provides Cook’s distances as a diagnostic plot
    of a fitted linear model object when you select `which=4` in the relevant usage
    of `plot`. The following code uses `mod.1`, `mod.2`, and `mod.3` from earlier
    to produce the three images in [Figure 22-7](ch22.xhtml#ch22fig7); these correspond
    to the three data sets in [Figure 22-5](ch22.xhtml#ch22fig5).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The *D*[*i*] displayed on the top left of [Figure 22-7](ch22.xhtml#ch22fig7)
    match the values you manually calculated earlier, stored in `cooks`. The influences
    of all the data points in the top-right plot remain relatively small, which reflects
    what you can see in the top-right plot of [Figure 22-5](ch22.xhtml#ch22fig5),
    where the additional point (`p2x`, `p2y`) doesn’t greatly affect the overall fit.
    In the bottom plot, `abline` superimposes two horizontal lines marking off the
    values 1 (highest line) and 4/11 = 0.364, both of which are clearly breached by
    the 11th point (`p3x`, `p3y`), just as you’d expect given the corresponding bottom
    image in [Figure 22-5](ch22.xhtml#ch22fig5).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f22-07.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-7: Three illustrative examples of the Cook’s distance plots produced
    in R, based on* `mod.1` *(top left),* `mod.2` *(top right), and* `mod.3` *(bottom)*'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Turn your attention back to the `car.step` model, where you modeled MPG using
    the `mtcars` data set, with the final fit achieved using stepwise AIC selection
    in [Section 22.2.4](ch22.xhtml#ch22lev2sec212). You’ve already looked at the residuals
    versus fitted values and QQ plot in [Figures 22-2](ch22.xhtml#ch22fig2) and [22-4](ch22.xhtml#ch22fig4).
    [Figure 22-8](ch22.xhtml#ch22fig8) provides a plot of Cook’s distances for the
    model with these two lines:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The plot labels the three points with the highest *D*[*i*] by default; two of
    these breach the 4/*n* = 4/32 = 0.125 mark. In light of the fitted model based
    on various effects of car weight (`wt`), horsepower (`hp`), and quarter-mile time
    (`qsec`), the Chrysler Imperial and Toyota Corolla are deemed to be in high-leverage
    positions with residuals large enough to be designated as highly influential.
    It should also be noted that the Fiat 128, though it doesn’t quite breach the
    0.125 line, is still rather influential and was in fact also one of the extreme
    labeled points in the residual plots ([Figure 22-2](ch22.xhtml#ch22fig2)) and
    the QQ plot ([Figure 22-4](ch22.xhtml#ch22fig4)).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f22-08.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-8: Cook’s distances for the model in* `car.step`*; a dashed horizontal
    line marks off 4/*n *for the* `mtcars` *data frame*'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: This could reasonably suggest investigating these highly influential observations
    further. Was everything recorded correctly? Has your model been selected carefully?
    Are there alternative options for the model, such as additional predictor terms
    or transformations? You could explore these options and continue to monitor a
    plot of the Cook’s distances (along with the other diagnostics).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Whatever your result, the presence of influential points doesn’t necessarily
    mean there is a serious problem with your model—this is more a tool to help you
    detect observations that are extreme in terms of their specific combination of
    the predictor values *and* that have a larger residual, suggesting their response
    value sits away from the trends predicted by the model itself. This is especially
    useful in multiple regression, when high dimensionality of the response-predictor
    data can make conventional visualization of the raw data in a single plot difficult.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '***22.3.6 Graphically Combining Residuals, Leverage, and Cook’s Distance***'
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The last two diagnostic plots from `plot` combine the standardized residual,
    the leverage, and the Cook’s distance for the *i*th observation. These combination
    plots are especially useful in allowing you to see whether it is the high leverage
    or large residual, or both, that contributes more to a high influence observation.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the data models `mod.1`, `mod.2`, and `mod.3`, enter the following code
    with `which=5` to produce the three images in the left column of [Figure 22-9](ch22.xhtml#ch22fig9):'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![image](../images/f22-09.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-9: Combination diagnostic plot of standardized residuals against
    leverage (left column) and Cook’s distance against leverage (right column) for
    the three illustrative models* `mod.1` *(top),* `mod.2` *(middle), and* `mod.3`
    *(bottom)*'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: These plots show leverage on the horizontal axis and the standardized residuals
    on the vertical axis for each observation. As a function of residual and leverage,
    the Cook’s distances can be plotted as *contours* on each of these scatterplots.
    These contours delineate the spatial areas of the plots that correspond to high
    influence (in the right extreme corners).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: The closer a point falls to the horizontal line at zero, the smaller its residual.
    A point that lies more left than right has a smaller leverage. If a point lies
    far enough from the horizontal line given its leverage (*x*-axis) position, it
    will breach the contours marking off certain values of *D*[*i*] (defaulting to
    just 0.5 and 1), indicating a high influence. Indeed, you can see by the narrowing
    of the contours as you move from left to right on these plots that classification
    as a high-influence point is easier if a given observation is in a high-leverage
    position, which makes perfect sense. In the previous calls to `plot` with `which=5`,
    the optional `cook.levels` argument is used to include a contour for the rule-of-thumb
    value of 4/11 for these three examples.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: The plot for `mod.1` shows that the added point (`p1x`, `p1y`) has a large residual,
    but it doesn’t breach the 4/11 contour because it’s in a low-leverage position.
    The plot for `mod.2` shows that the added point (`p2x`,`p2y`) is in a high-leverage
    position but isn’t influential because its residual is small. Lastly, the plot
    for `mod.3` clearly identifies the added point (`p3x`, `p3y`) as highly influential—with
    a large residual and high leverage, it’s in clear breach of the high-level contours.
    Looking back at all the previous plots of these three illustrative data sets,
    it’s easy to note that these three plots clearly reflect the nature of each of
    the individually added extra observations.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: The final diagnostic plot, using `which=6`, displays the same information as
    the `which=5` combination diagnostic, but this time the vertical axis displays
    Cook’s distance, and the horizontal axis displays a transformation of the leverage,
    namely, *h*[*ii*]/(1 − *h*[*ii*] ). This transformation amplifies larger leverage
    points in terms of their horizontal position, an effect that, in part, indirectly
    displays itself as a “stretched-out” scale on the *x*-axis—useful if you’re particularly
    interested in the extremity of the observations with respect to the collection
    of predictor variables.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'As such, the contours now define standardized residuals as a function of the
    scaled leverage and Cook’s distance. The following three lines produce the three
    images in the rightmost column of [Figure 22-9](ch22.xhtml#ch22fig9):'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Points positioned further to the right are high-leverage points; points positioned
    higher up are high-influence points. Looking down the right column of plots in
    [Figure 22-9](ch22.xhtml#ch22fig9), you can see that the three additional points
    are found where you would expect them to be, according to their characteristics
    in `mod.1`, `mod.2`, and `mod.3`.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'For a real-data example, return to the model stored in `car.step`. Enter the
    following code to produce the two combination diagnostic plots in [Figure 22-10](ch22.xhtml#ch22fig10):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![image](../images/f22-10.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-10: Combination diagnostic plot of standardized residuals against
    leverage (left) and Cook’s distance against leverage (right) for the* `car.step`
    *model*'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: The two images in [Figure 22-10](ch22.xhtml#ch22fig10) show the Corolla and
    the Imperial as influential observations with *D*[*i*] values greater than the
    `4/nrow(mtcars)` rule-of-thumb cutoff. Interestingly, this plot reveals that the
    Imperial (which was shown to have the largest *D*[*i*] by far in [Figure 22-8](ch22.xhtml#ch22fig8))
    actually has a smaller residual than both the Corolla and the Fiat 128\. Its high
    influence is clearly because of its high-leverage position with respect to the
    predictor values of the variables present in `car.step`. The Fiat 128, on the
    other hand, has one of the largest residuals in the entire data set (which is
    why it was flagged in some of the earlier diagnostic plots) but just misses out
    on being labeled a high-influence observation because of its relatively low-leverage
    position (based purely on the rule-of-thumb cutoff).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Any linear regression model will have observations that influence the model
    more than others, and these plots aim to help you identify them. But deciding
    what to actually do with highly influential observations can be difficult and
    is application specific. Although it’s not ideal to have a single observation
    exerting heavy influence over the final estimated model, it’s also extremely unwise
    to remove these observations without careful thought since they might be pointing
    to other issues, such as deficiencies in your current fit or previously undetected
    trends.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 22.2**'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: In [Section 22.2.2](ch22.xhtml#ch22lev2sec210), you used the `nuclear` data
    frame in the `boot` package to illustrate forward selection, where a model was
    selected for `cost` as a function of main effects of `date`, `cap`, `pt`, and
    `ne`.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Access the data frame; fit and summarize the model described earlier.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inspect the raw residuals versus fitted values and a normal QQ plot of the residuals
    and comment on your interpretations—do the assumptions underpinning the error
    component of the linear model appear satisfied in this case?
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the rule-of-thumb cutoff for influential observations based on the
    Cook’s distances. Produce a plot of the Cook’s distances and add a horizontal
    line corresponding to the cutoff. Comment on your findings.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Produce a combination diagnostic plot of the standardized residuals against
    leverage. Set the Cook’s distance contours to include the cutoff value from (c)
    as well as the default contours. Interpret the plot—how are any individually influential
    points characterized?
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on (c) and (d), you should be able to identify the record in `nuclear`
    exerting the largest influence on the fitted model. For the sake of argument,
    let’s assume the observation was recorded incorrectly. Refit the model from (a),
    this time omitting the offending row from the data frame. Summarize the model—which
    coefficients have changed the most? Produce the diagnostic plots from (b) for
    the new model and compare them to the ones from earlier.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `faraway` package and access the `diabetes` data frame. In [Exercise
    22.1](ch22.xhtml#ch22exc1) (g), you used stepwise AIC selection to choose a model
    for `chol`.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Using `diabetes`, fit the multiple linear model identified in the earlier exercise,
    that is, with main effects and a two-way interaction between `age` and `frame`
    and a main effect for `waist`. By summarizing the fit, determine the number of
    records that contained missing values in `diabetes` that were deleted from the
    estimation.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Produce the raw residuals versus fitted and QQ diagnostic plots for the model
    in (f). Comment on the validity of the error assumptions.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Investigate influential points. Make use of the familiar rule-of-thumb cutoff
    (note you’ll need to subtract the number of missing values from the total size
    of the data frame to get the effective sample size for your model). In the combination
    plot of the standardized residuals against leverage, use one, three, and five
    times the cutoff as the Cook’s distance contours.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recall the discussion of reading in web-based files in [Section 8.2.3](ch08.xhtml#ch08lev2sec75).
    There, you called in a data frame containing data on the prices of 308 diamonds
    (in Singapore dollars), as well as weight (in carats—continuous), color (categorical—six
    levels from `D`, the least yellow and the reference level, to `I`, the most yellow),
    clarity (categorical—five levels with `IF`, essentially flawless and the reference
    level, `VVS1`, `VVS2`, `VS1`, and `VS2`, with the last being the least clear),
    and certification (categorical—three levels for different diamond certification
    bodies with levels `GIA` as the reference, `HRD` and `IGI`). Seek out the freely
    available article by Chu ([2001](ref.xhtml#ref13)) for more information on these
    data. With an Internet connection, run the following lines, which will read in
    the data as the object `diamonds` and name each variable column appropriately.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Using either base R graphics or `ggplot2`, to get a feel for the data, produce
    a scatterplot of the price on the *y*-axis and carat weight on the *x*-axis. Experiment
    with using plotting color to split the points according to the following:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: – Diamond clarity
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – Diamond color
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: – Diamond certification
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fit a multiple linear model with `Price` as the response and main effects for
    the other variables as the predictors. Summarize the model and produce the three
    diagnostic plots that tell you about the assumptions surrounding the error term.
    Comment on the plots—are you satisfied that this is an appropriate model for the
    diamond prices? Why or why not?
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat (j) but use the log transformation of `Price`. Again, inspect and comment
    on the validity of the error assumptions.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat (k), but in modeling the log-price, this time include an additional quadratic
    term for `Carat` (refer to [Section 21.4.1](ch21.xhtml#ch21lev2sec199) for details
    on polynomial transformations). How do the residual diagnostics look now?
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**22.4 Collinearity**'
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This final aspect of fitting regression models isn’t technically classified
    as a diagnostic check but still has substantial potential to adversely affect
    the validity of any conclusions you draw from a fitted model and occurs frequently
    enough to warrant discussion here. *Collinearity* (also referred to as *multicollinearity*)
    is when two or more of the explanatory variables are highly correlated with each
    other.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '***22.4.1 Potential Warning Signs***'
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: High correlation between two predictors implies there will be some level of
    redundancy in terms of the information they contain when it comes to the response
    variable. It’s a problem since it can destabilize the ability to reliably fit
    a model and, as noted earlier, therefore be detrimental to any subsequent model-based
    inference.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'The following items serve as potential warnings of collinearity when you’re
    inspecting a model summary:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: • The omnibus *F*-test ([Section 21.3.5](ch21.xhtml#ch21lev2sec197)) result
    is statistically significant, but none of the individual *t*-test results for
    the regression parameters are significant.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: • The sign of a given coefficient estimate contradicts what you would reasonably
    expect to see, for example, drinking more wine resulting in a lower blood alcohol
    level.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: • Parameter estimates are associated with unusually high standard errors or
    vary wildly when the model is fitted to different random record subsets of the
    data.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: As the last point notes, collinearity tends to have more of a detrimental effect
    on the standard errors of the coefficients (and associated outcomes such as confidence
    intervals, significance tests, and prediction intervals) than it does on point
    predictions per se. In most cases, you can avoid collinearity simply by being
    careful. Be aware of the variables present and how the data have been collected.
    For example, ensure any given predictors you intend to include in the model don’t
    just represent a rescaled value of another included predictor. It’s also advisable
    to perform an exploratory analysis of your data, producing summary statistics
    and basic statistical plots. You can tabulate counts between categorical variables
    or look at estimated correlation coefficients between continuous variables, for
    example. In the latter case, as a rough guide, some statisticians suggest that
    a correlation of 0.8 or more could lead to potential problems.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '***22.4.2 Correlated Predictors: A Quick Example***'
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consider the `survey` data of the statistics students again, located in the
    `MASS` package. In most models you’ve looked at for these data, you’ve attempted
    to predict student height from certain explanatory variables, often including
    the handspan of the writing hand (`Wr.Hnd`). The help page `?survey` shows that
    data have also been collected on the nonwriting handspan (`NW.Hnd`). It’s reasonable
    to expect that these two variables will be highly correlated, which is precisely
    why I’ve avoided any use of `NW.Hnd` previously. Indeed, executing
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: reveals a high correlation coefficient, suggesting a strong positive linear
    association between the writing and nonwriting handspans of the students. In other
    words, these two variables should represent much the same information in any given
    model.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Now, you know from previously fitted models that writing handspan has a significant
    and positive impact on predicting mean student height. The following code quickly
    confirms this via a simple linear regression.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The high positive correlation between `Wr.Hnd` and `NW.Hnd` suggests that using
    `NW.Hnd` instead should have a similar effect.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: You can see from these results that this is certainly the case.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'Look, however, at what happens if you try to model height based on both of
    these predictors at the same time:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Since the effects of `Wr.Hnd` and `NW.Hnd` on `Height` are intermingled with
    one another, including both at the same time heavily masks any individual contribution
    to modeling the response. Statistical significance of the predictors is almost
    nonexistent; at the least, the effects are both associated with much, much higher
    *p*-values than in the individual single-predictor fits. That said, the omnibus
    *F*-test remains highly significant, giving an example of the first warning sign
    noted previously.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Code in This Chapter**'
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| **Function/operator** | **Brief description** | **First occurrence** |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| `anova` | Partial *F* -tests | [Section 22.2.1](ch22.xhtml#ch22lev2sec209),
    [p. 531](ch22.xhtml#page_531) |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| `add1` | Review single-term additions | [Section 22.2.2](ch22.xhtml#ch22lev2sec210),
    [p. 533](ch22.xhtml#page_533) |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '| `update` | Make changes to fitted model | [Section 22.2.2](ch22.xhtml#ch22lev2sec210),
    [p. 534](ch22.xhtml#page_534) |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| `drop1` | Review single-term deletions | [Section 22.2.3](ch22.xhtml#ch22lev2sec211),
    [p. 538](ch22.xhtml#page_538) |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| `step` | Stepwise AIC model selection | [Section 22.2.4](ch22.xhtml#ch22lev2sec212),
    [p. 543](ch22.xhtml#page_543) |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '| `plot` (used on `lm` object) | Model diagnostics | [Section 22.3.1](ch22.xhtml#ch22lev2sec214),
    [p. 551](ch22.xhtml#page_551) |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '| `rstandard` | Extract standardized residuals | [Section 22.3.2](ch22.xhtml#ch22lev2sec215),
    [p. 555](ch22.xhtml#page_555) |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: '| `shapiro.test` | Shapiro-Wilk test of normality | [Section 22.3.2](ch22.xhtml#ch22lev2sec215),
    [p. 555](ch22.xhtml#page_555) |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '| `hatvalues` | Calculate leverages | [Section 22.3.4](ch22.xhtml#ch22lev2sec217),
    [p. 558](ch22.xhtml#page_558) |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
- en: '| `cooks.distance` | Calculate Cook’s distances | [Section 22.3.5](ch22.xhtml#ch22lev2sec218),
    [p. 561](ch22.xhtml#page_561) |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
