- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extracting Information by Grouping and Summarizing
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: Every dataset tells a story, and it’s the data analyst’s job to find it. In
    Chapter 3, you learned about interviewing data using `SELECT` statements by sorting
    columns, finding distinct values, and filtering results. You’ve also learned the
    fundamentals of SQL math, data types, table design, and joining tables. With these
    tools under your belt, you’re ready to glean more insights by using *grouping*
    and *aggregate functions* to summarize your data.
  prefs: []
  type: TYPE_NORMAL
- en: By summarizing data, we can identify useful information we wouldn’t see just
    by scanning the rows of a table. In this chapter, we’ll use the well-known institution
    of your local library as our example.
  prefs: []
  type: TYPE_NORMAL
- en: Libraries remain a vital part of communities worldwide, but the internet and
    advancements in library technology have changed how we use them. For example,
    ebooks and online access to digital materials now have a permanent place in libraries
    along with books and periodicals.
  prefs: []
  type: TYPE_NORMAL
- en: In the United States, the Institute of Museum and Library Services (IMLS) measures
    library activity as part of its annual Public Libraries Survey. The survey collects
    data from about 9,000 library administrative entities, defined by the survey as
    agencies that provide library services to a particular locality. Some agencies
    are county library systems, and others are part of school districts. Data on each
    agency includes the number of branches, staff, books, hours open per year, and
    so on. The IMLS has been collecting data each year since 1988 and includes all
    public library agencies in the 50 states plus the District of Columbia and US
    territories such as American Samoa. (Read more about the program at [https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey/](https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey/).)
  prefs: []
  type: TYPE_NORMAL
- en: For this exercise, we’ll assume the role of an analyst who just received a fresh
    copy of the library dataset to produce a report describing trends from the data.
    We’ll create three tables to hold data from the 2018, 2017, and 2016 surveys.
    (Often, it’s helpful to assess multiple years of data to discern trends.) Then
    we’ll summarize the more interesting data in each table and join the tables to
    see how measures changed over time.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Library Survey Tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s create the three library survey tables and import the data. We’ll use
    appropriate data types and constraints for each column and add indexes where appropriate.
    The code and three CSV files are available in the book’s resources.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the 2018 Library Data Table
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll start by creating the table for the 2018 library data. Using the `CREATE
    TABLE` statement, [Listing 9-1](#listing9-1) builds `pls_fy2018_libraries`, a
    table for the fiscal year 2018 Public Library System Data File from the Public
    Libraries Survey. The Public Library System Data File summarizes data at the agency
    level, counting activity at all agency outlets, which include central libraries,
    branch libraries, and bookmobiles. The annual survey generates two additional
    files we won’t use: one summarizes data at the state level, and the other has
    data on individual outlets. For this exercise, those files are redundant, but
    you can read about the data they contain at [https://www.imls.gov/sites/default/files/2018_pls_data_file_documentation.pdf](https://www.imls.gov/sites/default/files/2018_pls_data_file_documentation.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For convenience, I’ve created a naming scheme for the tables: `pls` refers
    to the survey title, `fy2018` is the fiscal year the data covers, and `libraries`
    is the name of the particular file from the survey. For simplicity, I’ve selected
    47 of the more relevant columns from the 166 in the original survey file to fill
    the `pls_fy2018_libraries` table, excluding data such as the codes that explain
    the source of individual responses. When a library didn’t provide data, the agency
    derived the data using other means, but we don’t need that information for this
    exercise.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 9-1](#listing9-1) is abbreviated for convenience, as indicated by
    the `--snip--` noted in the code, but the full version is included with the book’s
    resources.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-1: Creating and filling the 2018 Public Libraries Survey table'
  prefs: []
  type: TYPE_NORMAL
- en: After finding the code and data file for [Listing 9-1](#listing9-1), connect
    to your `analysis` database in pgAdmin and run it. Make sure you remember to change
    `C:\YourDirectory\` to the path where you saved the *pls_fy2018_libraries.csv*
    file.
  prefs: []
  type: TYPE_NORMAL
- en: First, the code makes the table via `CREATE TABLE`. We assign a primary key
    constraint to the column named `fscskey` 1, a unique code the data dictionary
    says is assigned to each library. Because it’s unique, present in each row, and
    unlikely to change, it can serve as a natural primary key.
  prefs: []
  type: TYPE_NORMAL
- en: The definition for each column includes the appropriate data type and `NOT NULL`
    constraints where the columns have no missing values. The `startdate` and `enddate`
    columns contain dates, but we’ve set their data type to `text` in the code; in
    the CSV file, those columns include nondate values, and our import will fail if
    we try to use a `date` data type. In Chapter 10, you’ll learn how to clean up
    cases like these. For now, those columns are fine as is.
  prefs: []
  type: TYPE_NORMAL
- en: After creating the table, the `COPY` statement 2 imports the data from a CSV
    file named *pls_fy2018_libraries.csv* using the file path you provide. We add
    an index 3 to the `libname` column to provide faster results when we search for
    a particular library.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the 2017 and 2016 Library Data Tables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creating the tables for the 2017 and 2016 library surveys follows similar steps.
    I’ve combined the code to create and fill both tables in [Listing 9-2](#listing9-2).
    Note again that the listing shown is truncated, but the full code is in the book’s
    resources at [https://nostarch.com/practical-sql-2nd-edition/](https://nostarch.com/practical-sql-2nd-edition/).
  prefs: []
  type: TYPE_NORMAL
- en: Update the file paths in the `COPY` statements for both imports and execute
    the code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-2: Creating and filling the 2017 and 2016 Public Libraries Survey
    tables'
  prefs: []
  type: TYPE_NORMAL
- en: We start by creating the two tables, and in both we again use `fscskey` 1 as
    the primary key. Next, we run `COPY` commands 2 to import the CSV files to the
    tables, and, finally, we create an index on the `libname` column 3 in both tables.
  prefs: []
  type: TYPE_NORMAL
- en: As you review the code, you’ll notice that the three tables have an identical
    structure. Most ongoing surveys will have a handful of year-to-year changes because
    the makers of the survey either think of new questions or modify existing ones,
    but the columns I’ve selected for these three tables are consistent. The documentation
    for the survey years is at [https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey/](https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey/).
    Now, let’s mine this data to discover its story.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Library Data Using Aggregate Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Aggregate functions combine values from multiple rows, perform an operation
    on those values, and return a single result. For example, you might return the
    average of values with the `avg()` aggregate function, as you learned in Chapter
    6. Some aggregate functions are part of the SQL standard, and others are specific
    to PostgreSQL and other database managers. Most of the aggregate functions used
    in this chapter are part of standard SQL (a full list of PostgreSQL aggregates
    is at [https://www.postgresql.org/docs/current/functions-aggregate.html](https://www.postgresql.org/docs/current/functions-aggregate.html)).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll work through the library data using aggregates on single
    and multiple columns and then explore how you can expand their use by grouping
    the results they return with values from additional columns.
  prefs: []
  type: TYPE_NORMAL
- en: Counting Rows and Values Using count()
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After importing a dataset, a sensible first step is to make sure the table has
    the expected number of rows. The IMLS documentation says the file we imported
    for the 2018 data has 9,261 rows; 2017 has 9,245; and 2016 has 9,252\. The difference
    likely reflects library openings, closings, or mergers. When we count the number
    of rows in those tables, the results should match those counts.
  prefs: []
  type: TYPE_NORMAL
- en: The `count()` aggregate function, which is part of the ANSI SQL standard, makes
    it easy to check the number of rows and perform other counting tasks. If we supply
    an asterisk as an input, such as `count(*)`, the asterisk acts as a wildcard,
    so the function returns the number of table rows regardless of whether they include
    `NULL` values. We do this in all three statements in [Listing 9-3](#listing9-3).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-3: Using `count(``)` for table row counts'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run each of the commands in [Listing 9-3](#listing9-3) one at a time to see
    the table row counts. For `pls_fy2018_libraries`, the result should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For `pls_fy2017_libraries`, you should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the result for `pls_fy2016_libraries` should be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: All three results match the number of rows we expected. This is a good first
    step because it will alert us to issues such as missing rows or a case where we
    might have imported the wrong file.
  prefs: []
  type: TYPE_NORMAL
- en: Counting Values Present in a Column
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If we supply a column name instead of an asterisk to `count()`, it will return
    the number of rows that are not `NULL`. For example, we can count the number of
    non-`NULL` values in the `phone` column of the `pls_fy2018_libraries` table using
    `count()` as in [Listing 9-4](#listing9-4).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-4: Using `count(``)` for the number of values in a column'
  prefs: []
  type: TYPE_NORMAL
- en: The result shows 9,261 rows have a value in `phone`, the same as the total rows
    we found earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This means every row in the `phone` column has a value. You may have suspected
    this already, given that the column has a `NOT NULL` constraint in the `CREATE
    TABLE` statement. But running this check is worthwhile because the absence of
    values might influence your decision on whether to proceed with analysis at all.
    To fully vet the data, checking with topical experts and digging deeper into the
    data is usually a good idea; I recommend seeking expert advice as part of a broader
    analysis methodology (for more on this topic, see Chapter 20).
  prefs: []
  type: TYPE_NORMAL
- en: Counting Distinct Values in a Column
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Chapter 3, I covered the `DISTINCT` keyword—part of the SQL standard—which
    with `SELECT` returns a list of unique values. We can use it to see unique values
    in a single column, or we can see unique combinations of values from multiple
    columns. We also can add `DISTINCT` to the `count()` function to return a count
    of distinct values from a column.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 9-5](#listing9-5) shows two queries. The first counts all values in
    the 2018 table’s `libname` column. The second does the same but includes `DISTINCT`
    in front of the column name. Run them both, one at a time.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-5: Using `count(``)` for the number of distinct values in a column'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first query returns a row count that matches the number of rows in the
    table that we found using [Listing 9-3](#listing9-3):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s good. We expect to have the library agency name listed in every row.
    But the second query returns a smaller number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Using `DISTINCT` to remove duplicates reduces the number of library names to
    the 8,478 that are unique. Closer inspection of the data shows that 526 library
    agencies in the 2018 survey shared their name with one or more other agencies.
    Ten library agencies are named `OXFORD PUBLIC LIBRARY`, each one in a city or
    town named Oxford in different states, including Alabama, Connecticut, Kansas,
    and Pennsylvania, among others. We’ll write a query to see combinations of distinct
    values in the “Aggregating Data Using GROUP BY” section.
  prefs: []
  type: TYPE_NORMAL
- en: Finding Maximum and Minimum Values Using max() and min()
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `max()` and `min()` functions give us the largest and smallest values in
    a column and are useful for a couple of reasons. First, they help us get a sense
    of the scope of the values reported. Second, the functions can reveal unexpected
    issues with data, as you’ll see now.
  prefs: []
  type: TYPE_NORMAL
- en: Both `max()` and `min()` work the same way, with the name of a column as input.
    [Listing 9-6](#listing9-6) uses `max()` and `min()` on the 2018 table, taking
    the `visits` column that records the number of annual visits to the library agency
    and all of its branches. Run the code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-6: Finding the most and fewest visits using `max(``)` and `min()`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The query returns the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Well, that’s interesting. The maximum value of more than 16.6 million is reasonable
    for a large city library system, but `-3` as the minimum? On the surface, that
    result seems like a mistake, but it turns out that the creators of the library
    survey are employing a common but potentially problematic convention in data collection
    by placing a negative number or some artificially high value in a column to indicate
    some condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, negative values in number columns indicate the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A value of `-1` indicates a “nonresponse” to that question.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A value of `-3` indicates “not applicable” and is used when a library agency
    has closed either temporarily or permanently.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’ll need to account for and exclude negative values as we explore the data,
    because summing a column and including the negative values will result in an incorrect
    total. We can do this using a `WHERE` clause to filter them. It’s a good reminder
    to always read the documentation for the data to get ahead of the issue instead
    of having to backtrack after spending a lot of time on deeper analysis!
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating Data Using GROUP BY
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you use the `GROUP BY` clause with aggregate functions, you can group results
    according to the values in one or more columns. This allows us to perform operations
    such as `sum()` or `count()` for every state in the table or for every type of
    library agency.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore how using `GROUP BY` with aggregate functions works. On its own,
    `GROUP BY`, which is also part of standard ANSI SQL, eliminates duplicate values
    from the results, similar to `DISTINCT`. [Listing 9-7](#listing9-7) shows the
    `GROUP BY` clause in action.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9-7: Using `GROUP BY` on the `stabr` column'
  prefs: []
  type: TYPE_NORMAL
- en: We add the `GROUP BY` clause 1 after the `FROM` clause and include the column
    name to group. In this case, we’re selecting `stabr`, which contains the state
    abbreviation, and grouping by that same column. We then use `ORDER BY` `stabr
    as well so that the grouped results are in alphabetical order. This will yield
    a result with unique state abbreviations from the 2018 table. Here’s a portion
    of the results:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
