- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Extracting Information by Grouping and Summarizing
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分组和汇总提取信息
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: Every dataset tells a story, and it’s the data analyst’s job to find it. In
    Chapter 3, you learned about interviewing data using `SELECT` statements by sorting
    columns, finding distinct values, and filtering results. You’ve also learned the
    fundamentals of SQL math, data types, table design, and joining tables. With these
    tools under your belt, you’re ready to glean more insights by using *grouping*
    and *aggregate functions* to summarize your data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集都在讲述一个故事，而数据分析师的工作就是找到这个故事。在第三章中，你学习了如何通过使用 `SELECT` 语句对列进行排序、查找不同的值以及筛选结果来“采访”数据。你还学到了
    SQL 数学、数据类型、表格设计和连接表格的基础知识。有了这些工具，你已经准备好通过使用 *分组* 和 *聚合函数* 来总结数据，获取更多的见解。
- en: By summarizing data, we can identify useful information we wouldn’t see just
    by scanning the rows of a table. In this chapter, we’ll use the well-known institution
    of your local library as our example.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 通过汇总数据，我们可以识别出仅通过扫描表格的行无法看到的有用信息。在本章中，我们将使用你当地图书馆这一大家熟悉的机构作为示例。
- en: Libraries remain a vital part of communities worldwide, but the internet and
    advancements in library technology have changed how we use them. For example,
    ebooks and online access to digital materials now have a permanent place in libraries
    along with books and periodicals.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图书馆仍然是全球社区的重要组成部分，但互联网和图书馆技术的进步已经改变了我们使用图书馆的方式。例如，电子书和在线访问数字资料现在与书籍和期刊一起，在图书馆中占据了永久位置。
- en: In the United States, the Institute of Museum and Library Services (IMLS) measures
    library activity as part of its annual Public Libraries Survey. The survey collects
    data from about 9,000 library administrative entities, defined by the survey as
    agencies that provide library services to a particular locality. Some agencies
    are county library systems, and others are part of school districts. Data on each
    agency includes the number of branches, staff, books, hours open per year, and
    so on. The IMLS has been collecting data each year since 1988 and includes all
    public library agencies in the 50 states plus the District of Columbia and US
    territories such as American Samoa. (Read more about the program at [https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey/](https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey/).)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在美国，博物馆与图书馆服务研究所（IMLS）将图书馆活动作为其年度公共图书馆调查的一部分进行测量。该调查收集来自大约 9,000 个图书馆行政机构的数据，调查定义这些机构为提供图书馆服务给特定地区的单位。一些机构是县级图书馆系统，其他一些则是学校区的一部分。每个机构的数据包括分馆数量、员工数、书籍数量、每年开放小时数等。自1988年以来，IMLS每年都会收集数据，并涵盖所有50个州的公共图书馆机构，包括哥伦比亚特区和美国领土，如美属萨摩亚。（阅读更多关于该项目的信息：[https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey/](https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey/)）
- en: For this exercise, we’ll assume the role of an analyst who just received a fresh
    copy of the library dataset to produce a report describing trends from the data.
    We’ll create three tables to hold data from the 2018, 2017, and 2016 surveys.
    (Often, it’s helpful to assess multiple years of data to discern trends.) Then
    we’ll summarize the more interesting data in each table and join the tables to
    see how measures changed over time.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本次练习，我们将假设自己是一个分析师，刚刚收到了一份新的图书馆数据集，目的是根据数据生成描述趋势的报告。我们将创建三个表格，分别保存2018年、2017年和2016年调查的数据。（通常，评估多个年份的数据有助于发现趋势。）接着，我们会对每个表格中更有趣的数据进行汇总，并将这些表格连接起来，看看各项指标如何随时间变化。
- en: Creating the Library Survey Tables
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建图书馆调查表
- en: Let’s create the three library survey tables and import the data. We’ll use
    appropriate data types and constraints for each column and add indexes where appropriate.
    The code and three CSV files are available in the book’s resources.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建三个图书馆调查表并导入数据。我们将为每个列使用适当的数据类型和约束，并在适当的位置添加索引。代码和三个 CSV 文件可以在本书的资源中找到。
- en: Creating the 2018 Library Data Table
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 2018 年图书馆数据表
- en: 'We’ll start by creating the table for the 2018 library data. Using the `CREATE
    TABLE` statement, [Listing 9-1](#listing9-1) builds `pls_fy2018_libraries`, a
    table for the fiscal year 2018 Public Library System Data File from the Public
    Libraries Survey. The Public Library System Data File summarizes data at the agency
    level, counting activity at all agency outlets, which include central libraries,
    branch libraries, and bookmobiles. The annual survey generates two additional
    files we won’t use: one summarizes data at the state level, and the other has
    data on individual outlets. For this exercise, those files are redundant, but
    you can read about the data they contain at [https://www.imls.gov/sites/default/files/2018_pls_data_file_documentation.pdf](https://www.imls.gov/sites/default/files/2018_pls_data_file_documentation.pdf).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从创建2018年图书馆数据的表开始。使用`CREATE TABLE`语句，[列表9-1](#listing9-1)构建了`pls_fy2018_libraries`，这是来自公共图书馆调查的2018财年公共图书馆系统数据文件。公共图书馆系统数据文件总结了在机构级别的数据，统计所有机构网点的活动，包括中央图书馆、分馆和流动图书馆。年度调查生成两个额外的文件我们不会使用：一个总结州级数据，另一个包含各个网点的数据。对于本练习来说，这些文件是冗余的，但你可以在[https://www.imls.gov/sites/default/files/2018_pls_data_file_documentation.pdf](https://www.imls.gov/sites/default/files/2018_pls_data_file_documentation.pdf)阅读它们包含的数据。
- en: 'For convenience, I’ve created a naming scheme for the tables: `pls` refers
    to the survey title, `fy2018` is the fiscal year the data covers, and `libraries`
    is the name of the particular file from the survey. For simplicity, I’ve selected
    47 of the more relevant columns from the 166 in the original survey file to fill
    the `pls_fy2018_libraries` table, excluding data such as the codes that explain
    the source of individual responses. When a library didn’t provide data, the agency
    derived the data using other means, but we don’t need that information for this
    exercise.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，我为表格创建了命名方案：`pls`代表调查标题，`fy2018`是数据覆盖的财年，`libraries`是调查中某个特定文件的名称。为了简便起见，我从原始调查文件中的166列中选择了47个更相关的列填充`pls_fy2018_libraries`表，排除了诸如解释个别响应来源的代码等数据。当某个图书馆没有提供数据时，机构使用其他方法推导数据，但我们在本次练习中不需要这些信息。
- en: '[Listing 9-1](#listing9-1) is abbreviated for convenience, as indicated by
    the `--snip--` noted in the code, but the full version is included with the book’s
    resources.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表9-1](#listing9-1)为了方便进行了缩写，代码中标注了`--snip--`，但完整版本已经包含在本书的资源中。'
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Listing 9-1: Creating and filling the 2018 Public Libraries Survey table'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9-1：创建并填充2018年公共图书馆调查表
- en: After finding the code and data file for [Listing 9-1](#listing9-1), connect
    to your `analysis` database in pgAdmin and run it. Make sure you remember to change
    `C:\YourDirectory\` to the path where you saved the *pls_fy2018_libraries.csv*
    file.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 找到[列表9-1](#listing9-1)的代码和数据文件后，连接到你的`analysis`数据库，在pgAdmin中运行它。确保记得将`C:\YourDirectory\`改为你保存*pls_fy2018_libraries.csv*文件的路径。
- en: First, the code makes the table via `CREATE TABLE`. We assign a primary key
    constraint to the column named `fscskey` 1, a unique code the data dictionary
    says is assigned to each library. Because it’s unique, present in each row, and
    unlikely to change, it can serve as a natural primary key.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，代码通过`CREATE TABLE`语句创建表。我们将一个名为`fscskey`的列赋予主键约束1，这是数据字典中为每个图书馆分配的唯一代码。因为它是唯一的、出现在每一行，并且不太可能改变，所以可以作为自然主键。
- en: The definition for each column includes the appropriate data type and `NOT NULL`
    constraints where the columns have no missing values. The `startdate` and `enddate`
    columns contain dates, but we’ve set their data type to `text` in the code; in
    the CSV file, those columns include nondate values, and our import will fail if
    we try to use a `date` data type. In Chapter 10, you’ll learn how to clean up
    cases like these. For now, those columns are fine as is.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 每个列的定义包括适当的数据类型和`NOT NULL`约束条件，其中列没有缺失值。`startdate`和`enddate`列包含日期，但我们在代码中将其数据类型设置为`text`；在CSV文件中，这些列包含非日期值，如果我们尝试使用`date`数据类型导入，导入将失败。在第10章，你将学习如何清理像这样的情况。目前，这些列的设置已经可以使用。
- en: After creating the table, the `COPY` statement 2 imports the data from a CSV
    file named *pls_fy2018_libraries.csv* using the file path you provide. We add
    an index 3 to the `libname` column to provide faster results when we search for
    a particular library.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 创建表后，`COPY`语句2从名为*pls_fy2018_libraries.csv*的CSV文件中导入数据，使用你提供的文件路径。我们为`libname`列添加了索引3，以便在搜索特定图书馆时提供更快的结果。
- en: Creating the 2017 and 2016 Library Data Tables
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建2017年和2016年图书馆数据表
- en: Creating the tables for the 2017 and 2016 library surveys follows similar steps.
    I’ve combined the code to create and fill both tables in [Listing 9-2](#listing9-2).
    Note again that the listing shown is truncated, but the full code is in the book’s
    resources at [https://nostarch.com/practical-sql-2nd-edition/](https://nostarch.com/practical-sql-2nd-edition/).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 创建2017年和2016年图书馆调查的表格遵循类似的步骤。我将创建和填充这两个表格的代码合并在了[清单9-2](#listing9-2)中。再次提醒，所示的清单是截断的，但完整的代码可以在本书的资源页面找到：[https://nostarch.com/practical-sql-2nd-edition/](https://nostarch.com/practical-sql-2nd-edition/)。
- en: Update the file paths in the `COPY` statements for both imports and execute
    the code.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 更新`COPY`语句中的文件路径以导入数据，并执行代码。
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Listing 9-2: Creating and filling the 2017 and 2016 Public Libraries Survey
    tables'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 清单9-2：创建并填充2017年和2016年公共图书馆调查表格
- en: We start by creating the two tables, and in both we again use `fscskey` 1 as
    the primary key. Next, we run `COPY` commands 2 to import the CSV files to the
    tables, and, finally, we create an index on the `libname` column 3 in both tables.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建两个表，在这两个表中，我们再次使用`fscskey` 1作为主键。接下来，我们运行`COPY`命令导入CSV文件到表中，最后，我们在两个表的`libname`列上创建索引。
- en: As you review the code, you’ll notice that the three tables have an identical
    structure. Most ongoing surveys will have a handful of year-to-year changes because
    the makers of the survey either think of new questions or modify existing ones,
    but the columns I’ve selected for these three tables are consistent. The documentation
    for the survey years is at [https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey/](https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey/).
    Now, let’s mine this data to discover its story.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看代码时，你会注意到这三个表格具有相同的结构。大多数正在进行的调查每年都会有一些变化，因为调查的制定者要么提出新的问题，要么修改现有问题，但我为这三个表格选择的列是一致的。调查年份的文档可以在[https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey/](https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey/)找到。现在，让我们挖掘这些数据，发现它们的故事。
- en: Exploring the Library Data Using Aggregate Functions
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用聚合函数探索图书馆数据
- en: Aggregate functions combine values from multiple rows, perform an operation
    on those values, and return a single result. For example, you might return the
    average of values with the `avg()` aggregate function, as you learned in Chapter
    6. Some aggregate functions are part of the SQL standard, and others are specific
    to PostgreSQL and other database managers. Most of the aggregate functions used
    in this chapter are part of standard SQL (a full list of PostgreSQL aggregates
    is at [https://www.postgresql.org/docs/current/functions-aggregate.html](https://www.postgresql.org/docs/current/functions-aggregate.html)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合函数将来自多行的值组合在一起，对这些值执行操作，并返回单一的结果。例如，你可能会使用`avg()`聚合函数返回值的平均值，正如你在第6章中学到的那样。一些聚合函数是SQL标准的一部分，而其他一些则特定于PostgreSQL和其他数据库管理系统。本章中使用的大多数聚合函数都是SQL标准的一部分（PostgreSQL聚合函数的完整列表请参见：[https://www.postgresql.org/docs/current/functions-aggregate.html](https://www.postgresql.org/docs/current/functions-aggregate.html)）。
- en: In this section, we’ll work through the library data using aggregates on single
    and multiple columns and then explore how you can expand their use by grouping
    the results they return with values from additional columns.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过对单列和多列使用聚合函数来处理图书馆数据，然后探索如何通过将返回结果与其他列的值分组，来扩展这些聚合函数的应用。
- en: Counting Rows and Values Using count()
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用count()函数计数行数和数值
- en: After importing a dataset, a sensible first step is to make sure the table has
    the expected number of rows. The IMLS documentation says the file we imported
    for the 2018 data has 9,261 rows; 2017 has 9,245; and 2016 has 9,252\. The difference
    likely reflects library openings, closings, or mergers. When we count the number
    of rows in those tables, the results should match those counts.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入数据集后，合理的第一步是确保表格中有预期的行数。IMLS文档中说明我们导入的2018年数据文件有9,261行；2017年有9,245行；2016年有9,252行。这个差异很可能反映了图书馆的开设、关闭或合并。当我们计算这些表格中的行数时，结果应该与这些数字匹配。
- en: The `count()` aggregate function, which is part of the ANSI SQL standard, makes
    it easy to check the number of rows and perform other counting tasks. If we supply
    an asterisk as an input, such as `count(*)`, the asterisk acts as a wildcard,
    so the function returns the number of table rows regardless of whether they include
    `NULL` values. We do this in all three statements in [Listing 9-3](#listing9-3).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`count()`聚合函数是ANSI SQL标准的一部分，它使得统计行数和执行其他计数任务变得非常简单。如果我们提供一个星号作为输入，例如`count(*)`，那么星号将充当通配符，因此该函数返回表格行数，无论这些行是否包含`NULL`值。在[清单
    9-3](#listing9-3)中的三个语句都是这样做的。'
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Listing 9-3: Using `count(``)` for table row counts'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 9-3：使用`count(``)`统计表格行数
- en: 'Run each of the commands in [Listing 9-3](#listing9-3) one at a time to see
    the table row counts. For `pls_fy2018_libraries`, the result should be as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 分别运行[清单 9-3](#listing9-3)中的每个命令，以查看表格行数。对于`pls_fy2018_libraries`，结果应如下：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For `pls_fy2017_libraries`, you should see the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`pls_fy2017_libraries`，你应该看到以下结果：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, the result for `pls_fy2016_libraries` should be this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，`pls_fy2016_libraries`的结果应如下所示：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: All three results match the number of rows we expected. This is a good first
    step because it will alert us to issues such as missing rows or a case where we
    might have imported the wrong file.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 三个结果都与我们预期的行数相符。这是一个良好的第一步，因为它可以提醒我们是否存在缺失行或可能导入了错误的文件等问题。
- en: Counting Values Present in a Column
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 统计列中出现的值
- en: If we supply a column name instead of an asterisk to `count()`, it will return
    the number of rows that are not `NULL`. For example, we can count the number of
    non-`NULL` values in the `phone` column of the `pls_fy2018_libraries` table using
    `count()` as in [Listing 9-4](#listing9-4).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们向`count()`提供一个列名而不是星号，它将返回非`NULL`行的数量。例如，我们可以使用`count()`来统计`pls_fy2018_libraries`表格中`phone`列的非`NULL`值数量，如[清单
    9-4](#listing9-4)所示。
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Listing 9-4: Using `count(``)` for the number of values in a column'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 9-4：使用`count(``)`统计列中值的数量
- en: The result shows 9,261 rows have a value in `phone`, the same as the total rows
    we found earlier.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示有9,261行在`phone`列中有值，这与我们之前找到的总行数相同。
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This means every row in the `phone` column has a value. You may have suspected
    this already, given that the column has a `NOT NULL` constraint in the `CREATE
    TABLE` statement. But running this check is worthwhile because the absence of
    values might influence your decision on whether to proceed with analysis at all.
    To fully vet the data, checking with topical experts and digging deeper into the
    data is usually a good idea; I recommend seeking expert advice as part of a broader
    analysis methodology (for more on this topic, see Chapter 20).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着`phone`列中的每一行都有值。你可能已经猜到这一点，因为该列在`CREATE TABLE`语句中有`NOT NULL`约束。但进行这个检查是值得的，因为值的缺失可能会影响你是否继续进行分析的决定。为了全面验证数据，通常检查领域专家并深入挖掘数据是一个好主意；我建议将寻求专家意见作为更广泛分析方法的一部分（有关此主题的更多内容，请参见第20章）。
- en: Counting Distinct Values in a Column
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 统计列中不同值的数量
- en: In Chapter 3, I covered the `DISTINCT` keyword—part of the SQL standard—which
    with `SELECT` returns a list of unique values. We can use it to see unique values
    in a single column, or we can see unique combinations of values from multiple
    columns. We also can add `DISTINCT` to the `count()` function to return a count
    of distinct values from a column.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我讲解了`DISTINCT`关键字——这是SQL标准的一部分——它与`SELECT`一起使用时返回唯一值的列表。我们可以用它来查看单列中的唯一值，或者查看多列中值的唯一组合。我们也可以将`DISTINCT`添加到`count()`函数中，来返回某列中不同值的计数。
- en: '[Listing 9-5](#listing9-5) shows two queries. The first counts all values in
    the 2018 table’s `libname` column. The second does the same but includes `DISTINCT`
    in front of the column name. Run them both, one at a time.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 9-5](#listing9-5)展示了两个查询。第一个查询统计2018表格中`libname`列的所有值。第二个查询也做相同的事情，但在列名前加上了`DISTINCT`。请分别运行这两个查询。'
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Listing 9-5: Using `count(``)` for the number of distinct values in a column'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 9-5：使用`count(``)`统计列中不同值的数量
- en: 'The first query returns a row count that matches the number of rows in the
    table that we found using [Listing 9-3](#listing9-3):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个查询返回的行数与我们通过[清单 9-3](#listing9-3)找到的表格行数相匹配：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'That’s good. We expect to have the library agency name listed in every row.
    But the second query returns a smaller number:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好。我们预计每一行都会列出图书馆机构的名称。但是第二个查询返回的结果更小：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Using `DISTINCT` to remove duplicates reduces the number of library names to
    the 8,478 that are unique. Closer inspection of the data shows that 526 library
    agencies in the 2018 survey shared their name with one or more other agencies.
    Ten library agencies are named `OXFORD PUBLIC LIBRARY`, each one in a city or
    town named Oxford in different states, including Alabama, Connecticut, Kansas,
    and Pennsylvania, among others. We’ll write a query to see combinations of distinct
    values in the “Aggregating Data Using GROUP BY” section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `DISTINCT` 去除重复项，将图书馆名称的数量减少到唯一的8,478个。对数据的进一步检查显示，2018年调查中有526个图书馆机构与一个或多个其他机构共享名称。十个图书馆机构都被命名为
    `OXFORD PUBLIC LIBRARY`，每个都位于不同州的名为Oxford的城市或城镇，包括阿拉巴马州、康涅狄格州、堪萨斯州和宾夕法尼亚州等。我们将编写查询，以查看“使用
    `GROUP BY` 聚合数据”部分中的不同值组合。
- en: Finding Maximum and Minimum Values Using max() and min()
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 `max()` 和 `min()` 查找最大值和最小值
- en: The `max()` and `min()` functions give us the largest and smallest values in
    a column and are useful for a couple of reasons. First, they help us get a sense
    of the scope of the values reported. Second, the functions can reveal unexpected
    issues with data, as you’ll see now.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`max()` 和 `min()` 函数分别给我们返回一列中的最大值和最小值，这些函数有几个用途。首先，它们帮助我们了解报告值的范围。其次，这些函数能揭示数据中的意外问题，正如你现在将看到的。'
- en: Both `max()` and `min()` work the same way, with the name of a column as input.
    [Listing 9-6](#listing9-6) uses `max()` and `min()` on the 2018 table, taking
    the `visits` column that records the number of annual visits to the library agency
    and all of its branches. Run the code.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`max()` 和 `min()` 都是以列名作为输入，工作方式相同。 [列表 9-6](#listing9-6) 在 2018 表中使用 `max()`
    和 `min()`，它使用 `visits` 列，记录了每年对图书馆机构及其所有分支的访问次数。运行代码。'
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Listing 9-6: Finding the most and fewest visits using `max(``)` and `min()`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9-6：使用 `max()` 和 `min()` 查找最多和最少的访问量
- en: 'The query returns the following results:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 查询返回以下结果：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Well, that’s interesting. The maximum value of more than 16.6 million is reasonable
    for a large city library system, but `-3` as the minimum? On the surface, that
    result seems like a mistake, but it turns out that the creators of the library
    survey are employing a common but potentially problematic convention in data collection
    by placing a negative number or some artificially high value in a column to indicate
    some condition.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，这很有趣。超过1,660万的最大值对一个大城市的图书馆系统来说是合理的，但最小值为 `-3` 呢？表面上看，这个结果似乎是一个错误，但事实证明，图书馆调查的创建者采用了一种常见但可能存在问题的数据收集惯例，即在某列中放入负数或某个人为的高值，以表示某种特定条件。
- en: 'In this case, negative values in number columns indicate the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，数字列中的负值表示以下内容：
- en: A value of `-1` indicates a “nonresponse” to that question.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`-1` 的值表示对该问题的“未响应”。'
- en: A value of `-3` indicates “not applicable” and is used when a library agency
    has closed either temporarily or permanently.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`-3` 的值表示“不可应用”，当一个图书馆机构暂时或永久关闭时，会使用这个值。'
- en: We’ll need to account for and exclude negative values as we explore the data,
    because summing a column and including the negative values will result in an incorrect
    total. We can do this using a `WHERE` clause to filter them. It’s a good reminder
    to always read the documentation for the data to get ahead of the issue instead
    of having to backtrack after spending a lot of time on deeper analysis!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索数据时，我们需要考虑并排除负值，因为将负值包含在列的求和中会导致总和错误。我们可以使用 `WHERE` 子句来过滤这些负值。这也提醒我们，在深入分析前，始终先阅读数据的文档，以提前发现问题，而不是在投入大量时间分析后才后退修正！
- en: Aggregating Data Using GROUP BY
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 `GROUP BY` 聚合数据
- en: When you use the `GROUP BY` clause with aggregate functions, you can group results
    according to the values in one or more columns. This allows us to perform operations
    such as `sum()` or `count()` for every state in the table or for every type of
    library agency.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用带有聚合函数的 `GROUP BY` 子句时，可以根据一个或多个列中的值对结果进行分组。这使我们可以对表中的每个州或每种图书馆机构类型执行如 `sum()`
    或 `count()` 等操作。
- en: Let’s explore how using `GROUP BY` with aggregate functions works. On its own,
    `GROUP BY`, which is also part of standard ANSI SQL, eliminates duplicate values
    from the results, similar to `DISTINCT`. [Listing 9-7](#listing9-7) shows the
    `GROUP BY` clause in action.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨一下如何使用带有聚合函数的 `GROUP BY`。单独使用 `GROUP BY`（这也是标准 ANSI SQL 的一部分）会从结果中去除重复值，类似于
    `DISTINCT`。 [列表 9-7](#listing9-7) 显示了 `GROUP BY` 子句的实际应用。
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Listing 9-7: Using `GROUP BY` on the `stabr` column'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9-7：对 `stabr` 列使用 `GROUP BY`
- en: We add the `GROUP BY` clause 1 after the `FROM` clause and include the column
    name to group. In this case, we’re selecting `stabr`, which contains the state
    abbreviation, and grouping by that same column. We then use `ORDER BY` `stabr
    as well so that the grouped results are in alphabetical order. This will yield
    a result with unique state abbreviations from the 2018 table. Here’s a portion
    of the results:`
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`FROM`子句后添加`GROUP BY`子句1，并包括要分组的列名。在这种情况下，我们选择`stabr`，它包含州的缩写，并按该列进行分组。然后，我们使用`ORDER
    BY` `stabr`，这样分组后的结果将按字母顺序排列。这样就会得到2018年表格中唯一的州缩写。以下是部分结果：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
