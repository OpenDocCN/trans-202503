<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section epub:type="chapter" role="doc-chapter" aria-labelledby="ch9">&#13;
<span role="doc-pagebreak" epub:type="pagebreak" id="pg_177" aria-label="177"/>&#13;
<hgroup>&#13;
<h2 class="CHAPTER" id="ch9">&#13;
<span class="CN"><samp class="SANS_Futura_Std_Bold_Condensed_B_11">9</samp></span>&#13;
<span class="CT"><samp class="SANS_Dogma_OT_Bold_B_11">HARD PROBLEMS</samp></span>&#13;
</h2>&#13;
</hgroup>&#13;
<figure class="opener"><img class="opener" src="../images/opener.jpg" alt="" width="401" height="386"/></figure>&#13;
<p class="TNI1">Hard computational problems are the cornerstone of modern cryptography. These are problems for which even the best algorithm wouldn’t find a solution before the sun burns out.</p>&#13;
<p class="TX">In the 1970s, the rigorous study of hard problems gave rise to a new field of science called <i>computational complexity theory</i>, which dramatically impacted cryptography and many other fields, including economics, physics, and biology. In this chapter, you’ll learn the conceptual tools from complexity theory necessary to understand the foundations of cryptographic security. I’ll also introduce the hard problems behind public-key schemes, such as RSA encryption and Diffie–Hellman key agreement. I’ll touch on some deep concepts, but I’ll minimize the technical details and scratch only the surface. Still, I hope you’ll see the beauty in how cryptography leverages computational complexity theory to maximize security assurance.</p>&#13;
<section epub:type="division" aria-labelledby="sec1">&#13;
<span role="doc-pagebreak" epub:type="pagebreak" id="pg_178" aria-label="178"/>&#13;
<h3 class="H1" id="sec1"><span id="h1-62"/><samp class="SANS_Futura_Std_Bold_B_11">Computational Hardness</samp></h3>&#13;
<p class="TNI">A computational problem is a question that one can answer by doing enough computation—for example, “Is 217 a prime number?” or “How many <i>i</i>s are in <i>incomprehensibilities</i>?” The first question is a decision problem, because it can be answered with “yes” or “no,” while the second is a search problem.</p>&#13;
<p class="TX"><i>Computational hardness</i> is the property of computational problems for which there is no algorithm that will run in a reasonable amount of time. Such problems are also called <i>intractable</i>. Computational hardness is independent of the type of computing device used, be it a general-purpose central processing unit (CPU), a graphics processing unit (GPU), an integrated circuit, or a mechanical Turing machine. Indeed, one of the first findings of computational complexity theory is that all computing models are equivalent. If one computing device can solve a problem efficiently, any other device can efficiently solve it by porting the algorithm to the other device’s language—an exception is quantum computers, which we’ll discuss in <span class="Xref"><a href="chapter14.xhtml">Chapter 14</a></span>. Thus, I won’t need to specify the underlying computing device or hardware when discussing computational hardness; instead, we’ll just discuss algorithms.</p>&#13;
<p class="TX">To evaluate computational hardness, you first need a way to measure the complexity of an algorithm, or its running time. You then categorize running times as hard or easy.</p>&#13;
<section epub:type="division" aria-labelledby="sec2">&#13;
<h4 class="H2" id="sec2"><span id="h2-113"/><samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Running Time</samp></h4>&#13;
<p class="TNI">The <i>computational complexity</i> of an algorithm is the approximate number of operations it does, as a function of its input size. You can count the size in bits or in the number of elements taken as input. For example, take the algorithm in <a href="chapter9.xhtml#Lis9-1">Listing 9-1</a>, written in pseudocode. It searches for a value, <i>x</i>, within an array of <i>n</i> elements and then returns its index position, or –1, if <i>x</i> isn’t found.</p>&#13;
<span id="Lis9-1"/><pre><code>search(x, array, n) {&#13;
    for i from 0 to n - 1 {&#13;
        if (array[i] == x) {&#13;
            return i;&#13;
        }&#13;
    }&#13;
    return -1;&#13;
}</code></pre>&#13;
<p class="ListingCaption"><samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing 9-1: A simple search algorithm of complexity linear with respect to the array length</samp> <samp class="SANS_Futura_Std_Book_11">n</samp></p>&#13;
<p class="TX">This algorithm uses a <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> loop to find a specific value, <i>x</i>, in an array, iterating over values of the variable <i>i</i>, starting with 0. It checks whether the value of position <i>i</i> in <samp class="SANS_TheSansMonoCd_W5Regular_11">array</samp> is equal to the value of <i>x</i>. If so, it returns the position <i>i</i>. Otherwise, it increments <i>i</i> and tries the next position until it reaches <i>n</i> – 1, the last position in the array, at which point it returns –1.</p>&#13;
<p class="TX"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_179" aria-label="179"/>For this kind of algorithm, you count complexity as the number of iterations of the <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> loop: 1 in the best case (if <i>x</i> is equal to <samp class="SANS_TheSansMonoCd_W5Regular_11">array[0]</samp>), <i>n</i> in the worst case (if <i>x</i> is equal to <samp class="SANS_TheSansMonoCd_W5Regular_11">array[</samp><samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">n -</samp> <samp class="SANS_TheSansMonoCd_W5Regular_11">1</samp>] or if <i>x</i> isn’t found in <samp class="SANS_TheSansMonoCd_W5Regular_11">array</samp>), and <i>n</i>/2 on average if <i>x</i> is uniformly randomly distributed in one of the <i>n</i> cells of the array. With an array 10 times as large, the algorithm will be 10 times as slow. Complexity is therefore proportional to <i>n</i>, or “linear” in <i>n</i>. A complexity linear with respect to its input size is considered fast, as opposed to exponential complexities. In this example, although processing larger input values is slower, the computational cost won’t blow up exponentially but remains proportional to the table size.</p>&#13;
<p class="TX">However, many useful algorithms are slower than that and have a complexity higher than linear. The textbook example is sorting algorithms: given a list of <i>n</i> values in a random order, you need in the worst case <i>n</i> × log <i>n</i> basic operations to sort the list, which is sometimes called <i>linearithmic complexity</i>. Since <i>n</i> × log <i>n</i> grows faster than <i>n</i>, sorting speed slows down faster than proportionally to <i>n</i>. Yet such sorting algorithms remain in the realm of <i>practical</i> computation, or computation that one can carry out in a reasonable amount of time.</p>&#13;
<p class="TX">What’s usually <i>not</i> reasonable are complexities exponential in the input size. At some point, you’ll hit the ceiling of what’s feasible even for relatively small input lengths. Take the simplest example from cryptanalysis: the brute-force search for a secret key. Recall from <span class="Xref"><a href="chapter1.xhtml">Chapter 1</a></span> that given a plaintext <i>P</i> and a ciphertext <i>C</i> = <b>E</b>(<i>K</i>, <i>P</i>), it takes at most 2<i><sup>n</sup></i> attempts to recover an <i>n</i>-bit symmetric key because there are 2<i><sup>n</sup></i> possible keys—an example of a complexity that grows exponentially. A problem with <i>exponential complexity</i> is practically impossible to solve because as <i>n</i> grows, the effort rapidly becomes infeasible.</p>&#13;
<p class="TX">You may object that we’re comparing oranges and apples here: in the <samp class="SANS_TheSansMonoCd_W5Regular_11">search()</samp> function in <a href="chapter9.xhtml#Lis9-1">Listing 9-1</a>, we counted the number of <samp class="SANS_TheSansMonoCd_W5Regular_11">if (array[i]</samp> <samp class="SANS_TheSansMonoCd_W5Regular_11">==</samp> <samp class="SANS_TheSansMonoCd_W5Regular_11">x)</samp> operations, whereas key recovery counts the number of encryptions, each thousands of times slower than a single <samp class="SANS_TheSansMonoCd_W5Regular_11">==</samp> comparison. This seeming inconsistency can make a difference if you compare two algorithms with very similar complexities, but most of the time it won’t matter because the number of operations has a greater impact than the cost of an individual operation. Also, complexity estimates ignore <i>constant factors</i>: when we say that an algorithm takes time in the order of <i>n</i><sup>3</sup> operations (which is <i>cubic complexity</i>), it may actually take 41 × <i>n</i><sup>3</sup> operations, or even 12,345 × <i>n</i><sup>3</sup> operations—but again, as <i>n</i> grows, the constant factors lose significance to the point that you can ignore them. Complexity analysis is about theoretical hardness as a function of the input size; it doesn’t care about the exact number of CPU cycles it takes on your computer.</p>&#13;
<p class="TX">You can often use the <i>O</i>() notation (<i>big O</i>) to express complexities. For example, <i>O</i>(<i>n</i><sup>3</sup>) means that complexity grows no faster than <i>n</i><sup>3</sup>, ignoring potential constant factors. <i>O</i>() denotes the <i>upper bound</i> of an algorithm’s complexity. The notation <i>O</i>(1) means that an algorithm runs in <i>constant time</i> —that is, the running time doesn’t depend on the input length. For example, the algorithm that determines an integer’s parity by looking at its least significant bit (LSB) and returning “even” if it’s zero and “odd” <span role="doc-pagebreak" epub:type="pagebreak" id="pg_180" aria-label="180"/>otherwise will do the same thing at the same cost, whatever the integer’s length.</p>&#13;
<p class="TX">To see the difference between linear, quadratic, and exponential time complexities, look at how complexity grows for <i>O</i>(<i>n</i>) (linear) versus <i>O</i>(<i>n</i><sup>2</sup>) (quadratic) versus <i>O</i>(2<i><sup>n</sup></i>) (exponential) in <a href="chapter9.xhtml#fig9-1">Figure 9-1</a>.</p>&#13;
<figure class="IMG"><img id="fig9-1" class="img1" src="../images/fig9-1.jpg" alt="" width="1325" height="1006"/>&#13;
<figcaption><p class="CAP"><samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 9-1: The growth of exponential, quadratic, and linear complexities, from the fastest to the slowest growing</samp></p></figcaption>&#13;
</figure>&#13;
<p class="TX">Exponential complexity means the problem is practically impossible to solve, and linear complexity means the solution is feasible, whereas quadratic complexity is somewhere between the two.</p>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec3">&#13;
<h4 class="H2" id="sec3"><span id="h2-114"/><samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Polynomial vs. Superpolynomial Time</samp></h4>&#13;
<p class="TNI">The quadratic <i>O</i>(<i>n</i><sup>2</sup>) complexity (the middle curve in <a href="chapter9.xhtml#fig9-1">Figure 9-1</a>) is a special case of the broader class of polynomial complexities, or <i>O</i>(<i>n</i><i><sup>k</sup></i>), where <i>k</i> is some fixed number such as 3, 2.373, 7/10, or the square root of 17. Polynomial-time algorithms are eminently important in complexity theory and in cryptography because they’re the very definition of practically feasible. When an algorithm runs in <i>polynomial time</i>, or <i>polytime</i> for short, it completes in a decent amount of time even if the input is large. That’s why polynomial time is synonymous with “efficient” for complexity theorists and cryptographers.</p>&#13;
<p class="TX">In contrast, you can view algorithms running in <i>superpolynomial time</i>—that is, in <i>O</i>(<i>f</i>(<i>n</i>)), where <i>f</i>(<i>n</i>) is any function that grows faster than any polynomial—as impractical. I’m saying superpolynomial, and not just <span role="doc-pagebreak" epub:type="pagebreak" id="pg_181" aria-label="181"/>exponential, because there are complexities in between polynomial and the well-known exponential complexity <i>O</i>(2<i><sup>n</sup></i>), such as <i>O</i>(<i>n</i><sup>log(</sup><i><sup>n</sup></i><sup>)</sup>), as <a href="chapter9.xhtml#fig9-2">Figure 9-2</a> shows.</p>&#13;
<figure class="IMG"><img id="fig9-2" class="img1" src="../images/fig9-2.jpg" alt="" width="1296" height="985"/>&#13;
<figcaption><p class="CAP"><samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 9-2: The growth of the 2</samp> <samp class="SANS_Futura_Std_Book_SUP_11">n</samp><samp class="SANS_Futura_Std_Book_Oblique_I_11">,</samp> <samp class="SANS_Futura_Std_Book_11">n</samp><samp class="SANS_Futura_Std_Book_Oblique_I-SUP_11">log(</samp><samp class="SANS_Futura_Std_Book_SUP_11">n</samp><samp class="SANS_Futura_Std_Book_Oblique_I-SUP_11">)</samp><samp class="SANS_Futura_Std_Book_Oblique_I_11">, and</samp> <samp class="SANS_Futura_Std_Book_11">n</samp><samp class="SANS_Futura_Std_Book_Oblique_I-SUP_11">2</samp> <samp class="SANS_Futura_Std_Book_Oblique_I_11">functions, from the fastest to the slowest growing</samp></p></figcaption>&#13;
</figure>&#13;
<p class="TX"><i>O</i>(<i>n</i><sup>2</sup>) or <i>O</i>(<i>n</i><sup>3</sup>) may be efficient, but <i>O</i>(<i>n</i><sup>99,999,999,999</sup>) obviously isn’t. In other words, polytime is fast in practice as long as the exponent isn’t too large. Fortunately, all polynomial-time algorithms found to solve actual problems have small exponents. For example, <i>O</i>(<i>n</i><sup>2.373</sup>) is the time complexity of the best known algorithm for multiplying two <i>n</i> × <i>n</i> matrices in theory, as this algorithm is never used in practice. The 2002 breakthrough polytime deterministic algorithm for identifying <i>n</i>-bit prime numbers initially had a complexity <i>O</i>(<i>n</i><sup>12</sup>), but it was later improved to <i>O</i>(<i>n</i><sup>6</sup>). Polynomial time thus may not be the perfect definition of a practical time for an algorithm, but it’s the best we have.</p>&#13;
<p class="TX">By extension, you can consider a problem that can’t be solved by a polynomial-time algorithm impractical, or <i>hard</i>. As an example, for a straightforward key search, there’s no way to beat the <i>O</i>(2<i><sup>n</sup></i>) complexity unless the cipher is somehow broken.</p>&#13;
<blockquote>&#13;
<p class="Note"><span class="NoteHead"><samp class="SANS_Dogma_OT_Bold_B_15">NOTE</samp></span></p>&#13;
</blockquote>&#13;
<p class="NOTE-TXT"><i>Exponential complexity</i> <span class="note_Italic">O</span><i>(2</i><sup>n</sup><i>) is not the worst you can get. Some complexities grow even faster and thus characterize algorithms even slower to compute—for example, the complexity</i> <span class="note_Italic">O</span><i>(</i><span class="note_Italic">n</span><sup>n</sup><i>) or the</i> <span class="note_Italic">exponential factorial O</span><i>(</i><span class="note_Italic">n</span><i><sup>f(</sup></i><sup>n</sup> <i><sup>– 1)</sup></i><i>), where for any</i> <span class="note_Italic">x</span><i>, the function</i> <span class="note_Italic">f</span> <i>is here recursively defined as</i> <span class="note_Italic">f</span><i>(</i><span class="note_Italic">x</span><i>) =</i> <span class="note_Italic">x</span><i><sup>f(</sup></i><sup>x</sup> <i><sup>– 1)</sup></i><i>. In practice, you’ll never encounter algorithms with such preposterous complexities.</i></p>&#13;
<p class="TX"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_182" aria-label="182"/>You know that there’s no way to beat the <i>O</i>(2<i><sup>n</sup></i>) complexity of a brute-force key search (as long as the cipher is secure), but you won’t always know the fastest way to solve a computational problem in general. A large portion of the research in complexity theory is about proving complexity <i>bounds</i> on the running time of algorithms solving a given problem. To make their job easier, complexity theorists have categorized computational problems in different groups, or <i>classes</i>, according to the effort needed to solve them.</p>&#13;
</section>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec4">&#13;
<h3 class="H1" id="sec4"><span id="h1-63"/><samp class="SANS_Futura_Std_Bold_B_11">Complexity Classes</samp></h3>&#13;
<p class="TNI">In mathematics, a <i>class</i> is a group of objects with some similar attribute. For example, all computational problems solvable in time <i>O</i>(<i>n</i><sup>2</sup>), which complexity theorists simply denote <b>TIME</b>(<i>n</i><sup>2</sup>), are one class. Likewise, <b>TIME</b>(<i>n</i><sup>3</sup>) is the class of problems solvable in time <i>O</i>(<i>n</i><sup>3</sup>), <b>TIME</b>(2<i><sup>n</sup></i>) is the class of problems solvable in time <i>O</i>(2<i><sup>n</sup></i>), and so on. For the same reason that a supercomputer can compute whatever a laptop can compute, any problem solvable in <i>O</i>(<i>n</i><sup>2</sup>) is also solvable in <i>O</i>(<i>n</i><sup>3</sup>). Hence, any problem in the class <b>TIME</b>(<i>n</i><sup>2</sup>) also belongs to the class <b>TIME</b>(<i>n</i><sup>3</sup>), which also both belong to the class <b>TIME</b>(<i>n</i><sup>4</sup>), and so on. The union of all the classes <b>TIME</b>(<i>n</i><i><sup>k</sup></i>), for all constants <i>k</i>, is <b>P</b>, which stands for polynomial time.</p>&#13;
<p class="TX">If you’ve programmed a computer, you’ll know that seemingly fast algorithms may still crash your system by eating all its memory resources. When selecting an algorithm, you should consider not only its time complexity but also how much memory it uses, or its <i>space complexity</i>. This is especially important because a single memory access is usually orders of magnitudes slower than a basic arithmetic operation in a CPU.</p>&#13;
<p class="TX">Formally, you define an algorithm’s memory consumption as a function of its input length, <i>n</i>, in the same way you defined time complexity. The class of problems solvable using <i>f</i>(<i>n</i>) bits of memory is <b>SPACE</b>(<i>f</i>(<i>n</i>)). For example, <b>SPACE</b>(<i>n</i><sup>3</sup>) is the class of problems solvable using of the order of <i>n</i><sup>3</sup> bits of memory. Just as you had <b>P</b> as the union of all <b>TIME</b>(<i>n</i><i><sup>k</sup></i>), the union of all <b>SPACE</b>(<i>n</i><i><sup>k</sup></i>) problems is <b>PSPACE</b>.</p>&#13;
<p class="TX">While the lower the memory the better, a polynomial amount of memory doesn’t necessarily imply that an algorithm is practical. Take, for example, a brute-force key search, which takes negligible memory but is slow as hell. More generally, an algorithm can take forever, even if it uses just a few bytes of memory.</p>&#13;
<p class="TX">Any problem solvable in time <i>f</i>(<i>n</i>) needs at most <i>f</i>(<i>n</i>) memory, so <b>TIME</b>(<i>f</i>(<i>n</i>)) is included in <b>SPACE</b>(<i>f</i>(<i>n</i>)). In time <i>f</i>(<i>n</i>), you can write up to <i>f</i>(<i>n</i>) bits, and no more, because writing (or reading) 1 bit is assumed to take one unit of time; therefore, any problem in <b>TIME</b>(<i>f</i>(<i>n</i>)) can’t use more than <i>f</i>(<i>n</i>) space. As a consequence, <b>P</b> is a subset of <b>PSPACE</b>.</p>&#13;
<section epub:type="division" aria-labelledby="sec5">&#13;
<h4 class="H2" id="sec5"><span id="h2-115"/><samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Nondeterministic Polynomial Time</samp></h4>&#13;
<p class="TNI"><b>NP</b>, <i>nondeterministic</i> polynomial time, is the second most important complexity class, after the class <b>P</b> of all polynomial-time algorithms.</p>&#13;
<p class="TX"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_183" aria-label="183"/><b>NP</b> is the class of decision problems for which you can verify a solution in polynomial time—that is, efficiently—even though the solution may be hard to find. By <i>verified</i>, I mean that given a potential solution, you can run some polynomial-time algorithm that checks whether you’ve found an actual solution. For example, the problem of deciding whether there exists a key <i>K</i> such that <i>C</i> = <b>E</b>(<i>K</i>, <i>P</i>) given <i>P</i> and <i>C</i> for a symmetric cryptosystem <b>E</b> is in <b>NP</b>. This is because given a candidate key <i>K</i><sub>0</sub>, you can check that <i>K</i><sub>0</sub> is the correct key by verifying that <b>E</b>(<i>K</i><sub>0</sub>, <i>P</i>) equals <i>C</i>. You can’t find a potential key (the solution), if it exists, in polynomial time, but you can check whether a key is correct.</p>&#13;
<p class="TX">Now for a counterexample: What about known-ciphertext attacks? This time, you get only some <b>E</b>(<i>K</i>, <i>P</i>) values for random unknown plaintext <i>P</i>s. If you don’t know what the <i>P</i>s are, then there’s no way to verify whether a potential key, <i>K</i><sub>0</sub>, is the right one. In other words, the key-recovery problem under known-ciphertext attacks is not in <b>NP</b> (let alone in <b>P</b>), as you can’t express it as a decision problem.</p>&#13;
<p class="TX">Another example of a problem not in <b>NP</b> is that of verifying the <i>absence</i> of a solution to a problem. Verifying that a solution is correct boils down to computing some algorithm with the candidate solution as an input and then checking the return value. However, to verify that <i>no</i> solution exists, you may need to go through all possible inputs. If there’s an exponential number of inputs, you won’t be able to efficiently prove that no solution exists. The absence of a solution is hard to show for the hardest problems in the class <b>NP</b>—the so-called <b>NP</b>-complete problems.</p>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec6">&#13;
<h4 class="H2" id="sec6"><span id="h2-116"/><samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">NP-Complete Problems</samp></h4>&#13;
<p class="TNI"><b><i>NP</i></b><i>-complete</i> problems are the hardest decision problems in the class <b>NP</b>; you won’t know how to solve the worst-case instances of these problems in polynomial time. As complexity theorists discovered in the 1970s when they developed the theory of <b>NP</b>-completeness, <b>NP</b>’s hardest problems are all fundamentally equally hard. This was proven by showing that you can turn any efficient solution to any of the <b>NP</b>-complete problems into an efficient solution for any of the other <b>NP</b>-complete problems. In other words, if you can solve any <b>NP</b>-complete problem efficiently, you can solve all of them, as well as all problems in <b>NP</b>. How can this be?</p>&#13;
<p class="TX"><b>NP</b>-complete problems come in different guises, but they’re fundamentally similar from a mathematical perspective. In fact, you can reduce any <b>NP</b>-complete problem to any other <b>NP</b>-complete problem such that the capability to solve the second implies the capability to solve the first. Remember that <b>NP</b> contains decision problems, not search problems. You can efficiently transform an algorithm able to solve a search problem into an algorithm able to solve the corresponding decision problem, though the converse direction is not always possible. Fortunately, this is the case for NP-complete problems, which explains why people often mix up the two.</p>&#13;
<p class="TX"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_184" aria-label="184"/>Here are some examples of <b>NP</b>-complete problems:</p>&#13;
<p class="RunInPara"><b>The traveling salesman problem </b>Given a set of points on a map (such as cities) with the distances between each point from each other point and given a maximum distance <i>x</i>, decide whether there is a path that visits every point such that the total distance is smaller than <i>x</i>. (Note that you can find such a path with essentially the same complexity as the decision problem, but deciding whether a path is optimal is not in <b>NP</b>, because you can’t efficiently verify a solution’s correctness.)</p>&#13;
<p class="RunInPara"><b>The clique problem </b>Given a number, <i>x</i>, and a graph (a set of nodes connected by edges, as in <a href="chapter9.xhtml#fig9-3">Figure 9-3</a>), determine whether there’s a set of at most <i>x</i> nodes that are all connected to each other.</p>&#13;
<figure class="IMG"><img id="fig9-3" class="img5" src="../images/fig9-3.jpg" alt="" width="635" height="709"/>&#13;
<figcaption><p class="CAP"><samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 9-3: A graph containing a clique of four nodes. The general problem of finding a clique (set of nodes all connected to each other) of a given size in a graph is</samp> <samp class="SANS_Futura_Std_Bold_Oblique_BI_11">NP</samp><samp class="SANS_Futura_Std_Book_Oblique_I_11">-complete.</samp></p></figcaption>&#13;
</figure>&#13;
<p class="RunInPara"><b>The knapsack problem </b>Given two numbers, <i>x</i> and <i>y</i>, and a set of items, each of a known value and weight, decide if there is a group of items such that the total value is at least <i>x</i> and the total weight is at most <i>y.</i></p>&#13;
<p class="TX">You can find such <b>NP</b>-complete problems everywhere, from scheduling (given jobs of some priority and duration and one or more processors, assign jobs to the processors by respecting the priority while minimizing total execution time) to constraint-satisfaction (determine values that satisfy a set of mathematical constraints, such as logical equations). Even the task of winning certain video games was proven to be <b>NP</b>-hard (for famous games including <i>Tetris</i>, <i>Super Mario Bros.</i>, <i>Pokémon</i>, and <i>Candy Crush Saga</i>). For example, the article “Classic Nintendo Games are (Computationally) <span role="doc-pagebreak" epub:type="pagebreak" id="pg_185" aria-label="185"/>Hard” considers “the decision problem of reachability” to determine the possibility of reaching the goal point from a particular starting point (<i><a href="https://arxiv.org/abs/1203.1895">https://<wbr/>arxiv<wbr/>.org<wbr/>/abs<wbr/>/1203<wbr/>.1895</a></i>).</p>&#13;
<p class="TX">Some of these video game problems are at least as hard as <b>NP</b>-complete problems and are called <b>NP</b>-<i>hard</i>. A (not necessarily decisional) problem is <b>NP</b>-hard when it’s at least as hard as <b>NP</b>-complete (decision) problems and if any method to solve it can be efficiently used to solve <b>NP</b>-complete problems.</p>&#13;
<p class="TX"><b>NP</b>-complete problems must be decisional problems; that is, problems with a yes or no answer. Therefore, strictly speaking, problems of computing the “best” value of a solution cannot be <b>NP</b>-complete but may be <b>NP</b>-hard. For example, take the traveling salesman problem: the problem “Is there a path visiting all points with a distance less than <i>X</i>?” is <b>NP</b>-complete, whereas “Find the faster path visiting all points” is <b>NP</b>-hard.</p>&#13;
<p class="TX">Note that not all <i>instances</i> of <b>NP</b>-hard problems are actually hard to solve. You may be able to efficiently solve some instances because they’re small or have a specific structure. Take, for example, the graph in <a href="chapter9.xhtml#fig9-3">Figure 9-3</a>. You can quickly spot the clique, which is the top four connected nodes—even though the aforementioned clique-finding problem is <b>NP</b>-hard, there’s nothing hard here. Being <b>NP</b>-hard doesn’t mean that all instances of a given problem are hard but that as the problem size grows, some of them are. This is why cryptographers are more interested in problems that are hard on average, not just in the worst case.</p>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec7">&#13;
<h4 class="H2" id="sec7"><span id="h2-117"/><samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">The P vs. NP Problem</samp></h4>&#13;
<p class="TNI">If you could solve the hardest <b>NP</b> problems in polynomial time, then you could solve <i>all</i> <b>NP</b> problems in polynomial time, and therefore <b>NP</b> would equal <b>P</b>. Such an equality sounds preposterous: Aren’t there problems for which a solution is easy to verify but hard to find? For example, isn’t it obvious that exponential-time brute force is the fastest way to recover the key of a symmetric cipher, and therefore that the problem can’t be in <b>P</b>?</p>&#13;
<p class="TX">As crazy as it sounds, no one has proved that <b>P</b> is different from <b>NP</b>, despite a bounty of $1 million. The Clay Mathematics Institute will award this to anyone who proves that either <b>P</b> ≠ <b>NP</b> or <b>P</b> = <b>NP</b>. This problem, known as <b>P</b> vs. <b>NP</b>, was called “one of the deepest questions that human beings have ever asked” by renowned complexity theorist Scott Aaronson. Think about it: if <b>P</b> were equal to <b>NP</b>, then any easily checked solution would also be easy to find. All cryptography used in practice would be insecure because you could recover symmetric keys and invert hash functions efficiently.</p>&#13;
<p class="TX">But don’t panic: most complexity theorists believe <b>P</b> isn’t equal to <b>NP</b> and therefore that <b>P</b> is instead a strict subset of <b>NP</b>, as <a href="chapter9.xhtml#fig9-4">Figure 9-4</a> shows, where <b>NP</b>-complete problems are another subset of <b>NP</b> not overlapping with <b>P</b>. In other words, problems that look hard actually are hard. It’s just difficult to prove this mathematically. While proving that <b>P</b> = <b>NP</b> requires only a polynomial-time algorithm for an <b>NP</b>-complete problem, proving the nonexistence of such an algorithm is fundamentally harder. This didn’t <span role="doc-pagebreak" epub:type="pagebreak" id="pg_186" aria-label="186"/>stop mathematicians from coming up with simple proofs that, while usually obviously wrong, often make for funny reads; for an example, see “The P-versus-NP page” (<i><a href="https://www.win.tue.nl/~wscor/woeginger/P-versus-NP.htm">https://<wbr/>www<wbr/>.win<wbr/>.tue<wbr/>.nl<wbr/>/~wscor<wbr/>/woeginger<wbr/>/P<wbr/>-versus<wbr/>-NP<wbr/>.htm</a></i>).</p>&#13;
<figure class="IMG"><img id="fig9-4" class="img5" src="../images/fig9-4.jpg" alt="" width="548" height="402"/>&#13;
<figcaption><p class="CAP"><samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure 9-4: The classes</samp> <samp class="SANS_Futura_Std_Bold_Oblique_BI_11">NP</samp> <samp class="SANS_Futura_Std_Book_Oblique_I_11">and</samp> <samp class="SANS_Futura_Std_Bold_Oblique_BI_11">P</samp> <samp class="SANS_Futura_Std_Book_Oblique_I_11">and the set of</samp> <samp class="SANS_Futura_Std_Bold_Oblique_BI_11">NP</samp><samp class="SANS_Futura_Std_Book_Oblique_I_11">-complete problems</samp></p></figcaption>&#13;
</figure>&#13;
<p class="TX">If we’re almost sure that hard problems exist in <b>NP</b>, what about leveraging them to build strong, provably secure crypto? Imagine a proof that breaking some cipher is <b>NP</b>-hard and therefore that the cipher is unbreakable as long as <b>P</b> isn’t equal to <b>NP</b>. But reality is disappointing: the search versions of <b>NP</b>-complete problems have proved difficult to use for crypto purposes because the very structure that makes them hard in the worst case can make them easy in specific cases that sometimes occur in crypto. Instead, cryptography often relies on problems that are <i>probably not</i> <b>NP</b>-hard.</p>&#13;
</section>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec8">&#13;
<h3 class="H1" id="sec8"><span id="h1-64"/><samp class="SANS_Futura_Std_Bold_B_11">The Factoring Problem</samp></h3>&#13;
<p class="TNI">The factoring problem consists of finding the prime numbers <i>p</i> and <i>q</i> given a large number, <i>N</i> = <i>p</i> × <i>q</i>. The widely used RSA algorithms are based on the difficulty of factoring: RSA encryption and signature schemes are secure because factoring is a hard problem. Before we see how RSA leverages the factoring problem in <span class="Xref"><a href="chapter10.xhtml">Chapter 10</a></span>, I’d like to convince you that the decision version of this problem (“Does <i>N</i> have a factor smaller than <i>k</i> that is not equal to 1?”) is indeed hard yet probably not <b>NP</b>-complete.</p>&#13;
<p class="TX">First, some basic math. A <i>prime number</i> isn’t divisible by any other number but itself and 1. For example, the numbers 3, 7, and 11 are prime; the numbers 4 (that is, 2 × 2), 6 (2 × 3), and 12 (2 × 2 × 3) are not. A fundamental theorem of number theory says that you can write any integer number uniquely as a product of primes, a representation called the <i>factorization</i> of that number. For example, the factorization of 123,456 is 2<sup>6</sup> × 3 × 643, the factorization of 1,234,567 is 127 × 9,721, and so on. Any integer has a unique factorization, or a unique way to write it as a product of prime numbers. Polynomial-time primality testing algorithms allow us to efficiently test whether a given number is prime or a given factorization contains only prime numbers. Getting from a number to its prime factors, however, is another matter.</p>&#13;
<section epub:type="division" aria-labelledby="sec9">&#13;
<span role="doc-pagebreak" epub:type="pagebreak" id="pg_187" aria-label="187"/>&#13;
<h4 class="H2" id="sec9"><span id="h2-118"/><samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Factoring Large Numbers</samp></h4>&#13;
<p class="TNI">So how do you go from a number to its factorization—namely, its decomposition as a product of prime numbers? The most basic way to factor a number, <i>N</i>, is to try dividing it by all the numbers lower than it until you find a number, <i>x</i>, that divides <i>N</i>. Then attempt to divide <i>N</i> with the next number, <i>x</i> + 1, and so on. You’ll end up with a list of factors of <i>N</i>. What’s the time complexity of this? First, remember that we express complexities as a function of the input’s <i>length</i>. The bit length of the number <i>N</i> is <i>n</i> = log<sub>2</sub> <i>N</i>. By definition of the logarithm, this means that <i>N</i> = 2<i><sup>n</sup></i>. Because all the numbers less than <i>N</i>/2 are reasonable guesses for possible factors of <i>N</i>, there are about <i>N/</i>2 = 2<i><sup>n</sup></i>/2 values to try. The complexity of our naive factoring algorithm is therefore <i>O</i>(2<i><sup>n</sup></i>), ignoring the 1/2 coefficient in the <i>O</i>() notation.</p>&#13;
<p class="TX">While many numbers are easy to factor by first finding any small factors (2, 3, 5, and so on) and then iteratively factoring any other nonprime factors, here we’re interested in numbers of the form <i>N</i> = <i>p</i> × <i>q</i>, where <i>p</i> and <i>q</i> are large, as found in cryptography.</p>&#13;
<p class="TX">Let’s be a bit smarter: as we don’t need to test all numbers lower than <i>N</i>/2, but rather only the prime numbers, we can start by trying those smaller than the square root of <i>N</i>. If <i>N</i> isn’t a prime number, then it must have at least one factor lower than its square root √<i>N.</i> This is because if both of <i>N</i>’s factors <i>p</i> and <i>q</i> were greater than √<i>N</i>, then their product would be greater than √<i>N</i> × √<i>N</i> = <i>N</i>, which is impossible. For example, if <i>N</i> = 100, its factors <i>p</i> and <i>q</i> can’t both be greater than 10 because that would result in a product greater than 100. Either <i>p</i> or <i>q</i> has to be less than or equal to √<i>N</i>.</p>&#13;
<p class="TX">So what’s the complexity of testing only the primes less than √<i>N</i>? The <i>prime number theorem</i> states that there are approximately <i>N</i>/log <i>N</i> primes smaller than <i>N</i>. Hence, there are approximately √<i>N</i>/log √<i>N</i> primes smaller than √<i>N</i>. Expressing this value in terms of <i>n</i> = log<sub>2</sub> <i>N</i>, we get approximately 2<i><sup>n</sup></i><sup>/2 + 1</sup>/<i>n</i> possible prime factors and therefore a complexity of <i>O</i>(2<i><sup>n</sup></i><sup>/2</sup>/<i>n</i>), since √<i>N</i> = 2<i><sup>n</sup></i><sup>/2</sup> and 1/log √<i>N</i> = 1/(<i>n</i>/2) = 2/<i>n</i>. This is faster than testing all prime numbers, but it’s still painfully slow—on the order of 2<sup>120</sup> operations for a 256-bit number. That’s quite an impractical computational effort. But we can do much better.</p>&#13;
<p class="TX">The fastest factoring algorithm is the <i>general number field sieve (GNFS)</i>, which I won’t describe here because it requires the introduction of several advanced mathematical concepts. A rough estimate of GNFS’s complexity is exp(1.91 × <i>n</i><sup>1/3</sup> (log <i>n</i>)<sup>2/3</sup>), where <i>n</i> = log<sub>2</sub> <i>N</i> is the bit length of <i>N</i> and exp(. . .) is just a different notation for the exponential function <i>e</i> <i><sup>x</sup></i>, with <i>e</i> the exponential constant approximately equal to 2.718. However, it’s difficult to get an accurate estimate of GNFS’s actual complexity for a given number size. Therefore, we must rely on heuristic complexity estimates, which show how security increases with a longer <i>n</i>. For example:</p>&#13;
<ul class="ul">&#13;
<li class="BL">Factoring a <b>1,024-bit</b> number, which has two prime factors of approximately 500 bits each, takes on the order of 2<sup>70</sup> basic operations.</li>&#13;
<li class="BL">Factoring a <b>2,048-bit</b> number, which has two prime factors of approximately 1,000 bits each, takes on the order of 2<sup>90</sup> basic operations—about a million times slower than for a 1,024-bit number.</li>&#13;
</ul>&#13;
<p class="TX"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_188" aria-label="188"/>You can estimate that reaching 128-bit security requires at least 4,096 bits. Take these values with a grain of salt, as researchers don’t always agree on these estimates. The following experimental results reveal the actual cost of factoring:</p>&#13;
<ul class="ul">&#13;
<li class="BL">In 2005, after about 18 months of computation—and thanks to the power of a cluster of 80 processors, with a total effort equivalent to 75 years of computation on a single processor—a group of researchers factored a <b>663-bit</b> (200-decimal digit) number.</li>&#13;
<li class="BL">In 2009, after about two years and using several hundred processors, with a total effort equivalent to about 2,000 years of computation on a single processor, another group of researchers factored a <b>768-bit</b> (232-decimal digit) number.</li>&#13;
<li class="BL">In 2020, after a few months of computation, using tens of thousands of processors and a supercomputer, for a total effort equivalent to around 2,700 years of calculation on a single processor, another team factored an <b>829-bit</b> (250-decimal digit) number.</li>&#13;
</ul>&#13;
<p class="TX">As you can see, the numbers factored by researchers are shorter than those in real applications, which are at least 1,024-bit and often more than 2,048-bit. As I write this, no one has reported the factoring of a 1,024-bit number, but many speculate that well-funded organizations such as the NSA can do it.</p>&#13;
<p class="TX">In sum, you should view 1,024-bit RSA as insecure and use RSA with at least a 2,048-bit value, but preferably a 4,096-bit one to ensure higher security.</p>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec10">&#13;
<h4 class="H2" id="sec10"><span id="h2-119"/><samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Factoring Is Probably Not NP-Hard</samp></h4>&#13;
<p class="TNI">We don’t know how to factor large numbers efficiently, which suggests that the factoring problem doesn’t belong to <b>P</b>. However, the decision version of factoring is clearly in <b>NP</b>, because given a factorization, we can verify the solution by checking that all factors are prime numbers, thanks to the aforementioned primality testing algorithm, and that when multiplied together, the factors give the expected number. For example, to check that 3 × 5 is the factorization of 15, confirm that both 3 and 5 are prime and that 3 times 5 equals 15.</p>&#13;
<p class="TX">So we have a problem that is in <b>NP</b> and that looks hard, but is it as hard as the hardest <b>NP</b> problems? In other words, is the decision version of factoring <b>NP</b>-complete, and as a consequence, is factoring <b>NP</b>-hard? Spoiler alert: probably not.</p>&#13;
<p class="TX">There’s no mathematical proof that factoring isn’t <b>NP</b>-hard, but we have a few pieces of soft evidence. First, all known <b>NP</b>-hard problems in <b>NP</b> can have one solution, more than one solution, or no solution at all. In contrast, factoring always has exactly one solution. Also, the factoring problem has a mathematical structure that allows for the GNFS algorithm to significantly outperform a naive algorithm, a structure that <b>NP</b>-hard problems don’t have. Factoring would be easy with a <i>quantum computer</i>, a computing model that exploits quantum mechanical phenomena to run different kinds of <span role="doc-pagebreak" epub:type="pagebreak" id="pg_189" aria-label="189"/>algorithms and that would have the capability to factor large numbers efficiently (not because it’d run the algorithm faster but because it could run a quantum algorithm dedicated to factoring large numbers). Such a quantum computer doesn’t exist yet, though—and might never. Regardless, a quantum computer is believed to be useless in tackling <b>NP</b>-hard problems because it’d be no faster than a classical one (see <span class="Xref"><a href="chapter14.xhtml">Chapter 14</a></span>).</p>&#13;
<p class="TX">Factoring may be slightly easier than solving <b>NP</b>-hard problems in theory, but as far as cryptography is concerned, it’s hard enough, and even more reliable than <b>NP</b>-hard problems. Indeed, it’s easier to build cryptosystems on top of the factoring problem than search versions of <b>NP</b>-complete problems because it’s difficult to know exactly how hard it is to break a cryptosystem based on such problems—in other words, how many bits of security you’d get. As mentioned earlier, this relates to the fact that <b>NP</b> concerns worst-case hardness, and cryptographers are looking for average-case hardness.</p>&#13;
<p class="TX">The factoring problem is one of several problems you can use in cryptography as a <i>hardness assumption</i>, which is an assumption that some problem is computationally hard. You can use this assumption when proving that breaking a cryptosystem’s security is at least as hard as solving said problem. Another problem you can use as a hardness assumption, the <i>discrete logarithm problem (DLP)</i>, is actually a family of problems.</p>&#13;
</section>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec11">&#13;
<h3 class="H1" id="sec11"><span id="h1-65"/><samp class="SANS_Futura_Std_Bold_B_11">The Discrete Logarithm Problem</samp></h3>&#13;
<p class="TNI">The Discrete Logarithm Problem (DLP) predates the factoring problem in the official history of cryptography. Whereas RSA appeared in 1977, another cryptographic breakthrough, the Diffie–Hellman key agreement (see <span class="Xref"><a href="chapter11.xhtml">Chapter 11</a></span>), came about a year earlier, grounding its security on the hardness of the DLP. Like the factoring problem, the DLP deals with large numbers, but it’s less straightforward and requires more math than factoring. Let’s begin by introducing the mathematical notion of a group in the context of discrete logarithms.</p>&#13;
<section epub:type="division" aria-labelledby="sec12">&#13;
<h4 class="H2" id="sec12"><span id="h2-120"/><samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Groups</samp></h4>&#13;
<p class="TNI">In a mathematical context, a <i>group</i> is a set of elements (typically, numbers) that relate to each other according to certain well-defined rules. An example of a group is the set of nonzero integers modulo a prime number <i>p</i> (that is, numbers between 1 and <i>p</i> – 1) with modular multiplication being the group operation. We note such a group as (<b>Z</b><span class="ePub-I-SUB">p</span><sup>*</sup>, ×), or just <b>Z</b><span class="ePub-I-SUB">p</span><sup>*</sup> when it’s clear that the group operation is multiplication.</p>&#13;
<p class="TX">For <i>p</i> = 5, you get the group <b>Z</b><sub>5</sub><sup>*</sup> = {1, 2, 3, 4}. In the group <b>Z</b><sub>5</sub><sup>*</sup>, operations are carried out modulo 5; hence, you don’t have 3 × 4 = 12 but instead 3 × 4 = 2, because 12 mod 5 = 2. You can nonetheless use the same sign (×) that you use for normal integer multiplication. You can also use the exponent notation to denote a group element’s multiplication with itself mod <i>p</i>, a common operation in cryptography. For example, in the context of <b>Z</b><sub>5</sub><sup>*</sup>, 2<sup>3</sup> = 2 × 2 × 2 = 3 rather than 8, because 8 mod 5 is equal to 3.</p>&#13;
<p class="TX"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_190" aria-label="190"/>To be a group, a mathematical set and its operation must have the following characteristics, which are called <i>group axioms</i>:</p>&#13;
<p class="RunInPara"><b>Closure </b>For any two group elements <i>x</i> and <i>y</i>, <i>x</i> × <i>y</i> is in the group too. In <b>Z</b><sub>5</sub><sup>*</sup>, 2 × 3 = 1 (because 6 = 1 mod 5), 2 × 4 = 3, and so on.</p>&#13;
<p class="RunInPara"><b>Associativity </b>For any group elements <i>x</i>, <i>y</i>, <i>z</i>, (<i>x</i> × <i>y</i>) × <i>z</i> = <i>x</i> × (<i>y</i> × <i>z</i>). In <b>Z</b><sub>5</sub><sup>*</sup>, (2 × 3) × 4 = 1 × 4 = 2 × (3 × 4) = 2 × 2 = 4.</p>&#13;
<p class="RunInPara"><b>Identity existence </b>The group includes an element <i>e</i> such that <i>e</i> × <i>x</i> = <i>x</i> × <i>e</i> = <i>x</i>. Such an element is called the <i>identity</i>. In any <b>Z</b><span class="ePub-I-SUB">p</span><sup>*</sup>, the identity element is 1.</p>&#13;
<p class="RunInPara"><b>Inverse existence </b>For any <i>x</i> in the group, there’s a <i>y</i> such that <i>x</i> × <i>y</i> = <i>y</i> × <i>x</i> = <i>e</i>. In <b>Z</b><sub>5</sub><sup>*</sup>, the inverse of 2 is 3, and the inverse of 3 is 2, while 4 is its own inverse because 4 × 4 = 16 = 1 mod 5.</p>&#13;
<p class="TX">In addition, a group is <i>commutative</i>, or <i>abelian</i>, if <i>x</i> × <i>y</i> = <i>y</i> × <i>x</i> for any group elements <i>x</i> and <i>y</i>. That’s also true for any multiplicative group of integers <b>Z</b><span class="ePub-I-SUB">p</span><sup>*</sup>. In particular, <b>Z</b><sub>5</sub><sup>*</sup> is commutative: 3 × 4 = 4 × 3, 2 × 3 = 3 × 2, and so on.</p>&#13;
<p class="TX">A group is <i>cyclic</i> if there’s at least one element <i>g</i> such that its powers (<i>g</i><sup>1</sup>, <i>g</i><sup>2</sup>, <i>g</i><sup>3</sup>, and so on) mod <i>p</i> span all distinct group elements. The element <i>g</i> is then a <i>generator</i> of the group. <b>Z</b><sub>5</sub><sup>*</sup> is cyclic and has two generators, 2 and 3, because 2<sup>1</sup> = 2, 2<sup>2</sup> = 4, 2<sup>3</sup> = 3, 2<sup>4</sup> = 1, and 3<sup>1</sup> = 3, 3<sup>2</sup> = 4, 3<sup>3</sup> = 2, 3<sup>4</sup> = 1.</p>&#13;
<p class="TX">Note that I’m using multiplication as a group operator, but you can also get groups from other operators. For example, the most straightforward group is the set of all integers, positive and negative, with addition as a group operation. Let’s check that the group axioms hold with addition, in the preceding order: the number <i>x</i> + <i>y</i> is an integer if <i>x</i> and <i>y</i> are integers (closure); (<i>x</i> + <i>y</i>) + <i>z</i> = <i>x</i> + (<i>y</i> + <i>z</i>) for any <i>x</i>, <i>y</i>, and <i>z</i> (associativity); zero is the identity element; and the inverse of any number <i>x</i> in the group is –<i>x</i> because <i>x</i> + (–<i>x</i>) = 0 for any integer <i>x</i>. A big difference, though, is that this group of integers is of infinite size, whereas in crypto you’ll deal with only <i>finite groups</i>, or groups with a finite number of elements, for implementation reasons. Typically, you’ll use groups <b>Z</b><span class="ePub-I-SUB">p</span><sup>*</sup>, where <i>p</i> is <i>thousands</i> of bits long (that is, groups that contain on the order of 2<i><sup>m</sup></i> numbers if <i>p</i> is <i>m</i>-bit long).</p>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec13">&#13;
<h4 class="H2" id="sec13"><span id="h2-121"/><samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">The Hard Thing</samp></h4>&#13;
<p class="TNI">The discrete logarithm problem as initially used in cryptography consists of finding the <i>y</i> for which <i>g</i><i><sup>y</sup></i> = <i>x</i>, given a generator <i>g</i> within some group <b>Z</b><span class="ePub-I-SUB">p</span><sup>*</sup>, where <i>p</i> is a prime number, and given a group element <i>y</i>. We often express the DLP in additive rather than multiplicative notation, as in groups of elliptic curves. In this case, the problem is to find the multiplicative factor <i>k</i> such that <i>k</i> × <i>P</i> = <i>Q</i>, where you know the points <i>P</i> and <i>Q</i>. This is called the <i>elliptic curve DLP (ECDLP)</i>.</p>&#13;
<p class="TX">The DLP is <i>discrete</i> because you’re dealing with countable integers as opposed to uncountable real numbers, and it’s a <i>logarithm</i> because you’re looking for the logarithm of <i>x</i> in base <i>g</i>. For example, the logarithm of 256 in base two is 8 because 2<sup>8</sup> = 256.</p>&#13;
<p class="TX"><span role="doc-pagebreak" epub:type="pagebreak" id="pg_191" aria-label="191"/>Factoring is about as equally hard, thus as secure, as a discrete logarithm. In fact, algorithms to solve DLP bear similarities with those factoring integers, and you get about the same security level with <i>n</i>-bit hard-to-factor numbers as with discrete logarithms in an <i>n</i>-bit group. For the same reason as factoring, DLP isn’t <b>NP</b>-hard. (There are certain groups where the DLP is easier to solve, but these aren’t used in cryptography, at least not where DLP hardness is needed.)</p>&#13;
</section>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec14">&#13;
<h3 class="H1" id="sec14"><span id="h1-66"/><samp class="SANS_Futura_Std_Bold_B_11">How Things Can Go Wrong</samp></h3>&#13;
<p class="TNI">More than 40 years later, we still don’t know how to efficiently factor large numbers or solve discrete logarithms. In the absence of mathematical proof, it’s always possible to speculate that they’ll be broken one day. But we also don’t have proof that <b>P</b> ≠ <b>NP</b>, so you can speculate that <b>P</b> may be equal to <b>NP</b>; however, according to experts, that surprise is unlikely. Most public-key crypto deployed today relies on either factoring (RSA) or DLP (Diffie–Hellman, ElGamal, elliptic curve cryptography). Although math may not fail us, real-world concerns and human error can sneak in.</p>&#13;
<section epub:type="division" aria-labelledby="sec15">&#13;
<h4 class="H2" id="sec15"><span id="h2-122"/><samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">When Factoring Is Easy</samp></h4>&#13;
<p class="TNI">Factoring large numbers isn’t always hard. For example, take the following 1,024-bit number <i>N</i>:</p>&#13;
<figure class="DIS-IMG"><img class="img1" src="../images/pg191-1.jpg" alt="" width="1911" height="221"/></figure>&#13;
<p class="TX">For 1,024-bit numbers in RSA encryption or signature schemes where <i>N</i> = <i>pq</i>, we expect the best factoring algorithms to need around 2<sup>70</sup> operations, as we discussed earlier. But you can factor this sample number in seconds using SageMath, a piece of Python-based mathematical software. Using SageMath’s <samp class="SANS_TheSansMonoCd_W5Regular_11">factor()</samp> function on my 2023 MacBook, it took less than a second to find the following factorization:</p>&#13;
<figure class="DIS-IMG"><img class="img1" src="../images/pg191-2.jpg" alt="" width="1387" height="100"/></figure>&#13;
<p class="TX">Right, I cheated. This number isn’t of the form <i>N</i> = <i>pq</i> because it doesn’t have just two large prime factors but rather five, including very small ones, which makes it easy to factor. First, you identify the 2<sup>800</sup> × 641 × 6,700,417 part by trying small primes from a precomputed list of prime numbers, which leaves you with a 192-bit number that’s much easier to factor than a 1,024-bit number with two large factors.</p>&#13;
<p class="TX">Factoring can be easy not only when <i>n</i> has small prime factors but also when <i>N</i> or its factors <i>p</i> and <i>q</i> have particular forms—for example, when <span role="doc-pagebreak" epub:type="pagebreak" id="pg_192" aria-label="192"/><i>N</i> = <i>pq</i> with <i>p</i> and <i>q</i> both close to some 2<i><sup>b</sup></i>, when <i>N</i> = <i>pq</i> and some bits of <i>p</i> or <i>q</i> are known, or when <i>N</i> is of the form <i>N</i> = <i>p</i><i><sup>r</sup></i><i>q</i><i><sup>s</sup></i> and <i>r</i> is greater than log <i>p</i>. However, detailing the reasons for these weaknesses is too technical for this book.</p>&#13;
<p class="TX">The upshot is that the RSA encryption and signature algorithms (see <span class="Xref"><a href="chapter10.xhtml">Chapter 10</a></span>) need to work with a value of <i>N</i> = <i>pq</i>, where <i>p</i> and <i>q</i> are carefully chosen, to avoid easy factorization of <i>N</i>, which can result in a security disaster.</p>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec16">&#13;
<h4 class="H2" id="sec16"><span id="h2-123"/><samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Small Hard Problems Aren’t Hard</samp></h4>&#13;
<p class="TNI">Computationally hard problems, and even exponential-time algorithms, become practical when they’re small enough. A symmetric cipher may be secure in the sense that there’s no faster attack than the 2<i><sup>n</sup></i>-time brute force, but if the key length is <i>n</i> = 32, you’ll break the cipher in minutes. This sounds obvious, and you’d think that no one would be naive enough to use small keys, but in reality, there are plenty of reasons why this could happen. The following are two true stories.</p>&#13;
<p class="TX">Say you’re a developer who knows nothing about crypto but has some API to encrypt with RSA and has been told to encrypt with 128-bit security. What RSA key size would you pick? I’ve seen real cases of 128-bit RSA, or RSA based on a 128-bit number <i>N</i> = <i>pq</i>. However, although factoring is impractically hard for an <i>N</i> thousands of bits long, factoring a 128-bit number is easy. Using the SageMath software, the commands in <a href="chapter9.xhtml#Lis9-2">Listing 9-2</a> complete instantaneously.</p>&#13;
<span id="Lis9-2"/><pre><code>sage: p = random_prime(2**64)&#13;
sage: print(p)&#13;
6822485253121677229&#13;
sage: q = random_prime(2**64)&#13;
sage: print(q)&#13;
17596998848870549923&#13;
Sage: N = p*q&#13;
sage: factor(N)&#13;
6822485253121677229 * 17596998848870549923</code></pre>&#13;
<p class="ListingCaption"><samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing 9-2: Generating an RSA modulus by picking two random prime numbers and factoring it instantaneously</samp></p>&#13;
<p class="TX"><a href="chapter9.xhtml#Lis9-2">Listing 9-2</a> shows that you can easily factor a 128-bit number taken randomly as the product of two 64-bit prime numbers on a typical laptop. However, if I chose 1,024-bit prime numbers instead by using <samp class="SANS_TheSansMonoCd_W5Regular_11">p = random _prime(2**1024)</samp>, the command <samp class="SANS_TheSansMonoCd_W5Regular_11">factor(p*q)</samp> would never complete, at least not in my lifetime.</p>&#13;
<p class="TX">To be fair, the tools available don’t help prevent the naive use of insecurely short parameters. For example, the OpenSSL toolkit used to let you generate RSA keys as short as 31 bits without any warning; such short keys are totally insecure, as <a href="chapter9.xhtml#Lis9-3">Listing 9-3</a> shows. OpenSSL has since been fixed, and its version 1.1.1t (from February 2023) returns an error “key size too small” if you request a key shorter than 512 bits.</p>&#13;
<span id="Lis9-3"/><pre><code><span role="doc-pagebreak" epub:type="pagebreak" id="pg_193" aria-label="193"/>$ <b>openssl genrsa 31</b>&#13;
Generating RSA private key, 31 bit long modulus&#13;
.+++++++++++++++++++++++++++&#13;
.+++++++++++++++++++++++++++&#13;
e is 65537 (0x10001)&#13;
-----BEGIN RSA PRIVATE KEY-----&#13;
MCsCAQACBHHqFuUCAwEAAQIEP6zEJQIDANATAgMAjCcCAwCSBwICTGsCAhpp&#13;
-----END RSA PRIVATE KEY-----</code></pre>&#13;
<p class="ListingCaption"><samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing 9-3: Generating an insecure RSA private key using an older version of the OpenSSL toolkit</samp></p>&#13;
<p class="TX">When reviewing cryptography, you should check not only the type of algorithms used but also their parameters and the length of their secret values. However, as you’ll see in the following story, what’s secure enough today may be insecure tomorrow.</p>&#13;
<p class="TX">In 2015, researchers discovered that many HTTPS servers and email servers supported an older, insecure version of the Diffie–Hellman key agreement protocol. Namely, the underlying TLS implementation supported Diffie–Hellman within a group, <b>Z</b><span class="ePub-I-SUB">p</span><sup>*</sup>, defined by a prime number, <i>p</i>, of only 512 bits, where the discrete logarithm problem was no longer practically impossible to compute.</p>&#13;
<p class="TX">Not only did servers support a weak algorithm, but also attackers could force a benign client to use that algorithm by injecting malicious traffic within the client’s session. Even better for attackers, the largest part of the attack could be carried out once and recycled to attack multiple clients. After about a week of computations to attack a specific group, <b>Z</b><span class="ePub-I-SUB">p</span><sup>*</sup>, it took only 70 seconds to break individual sessions of different users.</p>&#13;
<p class="TX">A secure protocol is worthless if it’s undermined by a weakened algorithm, and a reliable algorithm is useless if sabotaged by weak parameters. In cryptography, you should always read the fine print.</p>&#13;
<p class="TX">For more details about this story, check the research article “Imperfect Forward Secrecy: How Diffie–Hellman Fails in Practice” (<i><a href="https://weakdh.org/imperfect-forward-secrecy-ccs15.pdf">https://<wbr/>weakdh<wbr/>.org<wbr/>/imperfect<wbr/>-forward<wbr/>-secrecy<wbr/>-ccs15<wbr/>.pdf</a></i>).</p>&#13;
</section>&#13;
</section>&#13;
<section epub:type="division" aria-labelledby="sec17">&#13;
<h3 class="H1" id="sec17"><span id="h1-67"/><samp class="SANS_Futura_Std_Bold_B_11">Further Reading</samp></h3>&#13;
<p class="TNI">I encourage you to look deeper into the foundational aspects of computation in the context of computability (what functions can be computed?) and complexity (at what cost?) and how they relate to cryptography. I’ve talked mostly about the classes <b>P</b> and <b>NP</b>, but there are many more classes and points of interest for cryptographers. I highly recommend the book <i>Quantum Computing Since Democritus</i> by Scott Aaronson (Cambridge University Press, 2013). It’s in large part about quantum computing, but its first chapters brilliantly introduce complexity theory and cryptography.</p>&#13;
<p class="TX">In the cryptography research literature, you’ll find other hard computational problems. I’ll mention them in later chapters, but here are <span role="doc-pagebreak" epub:type="pagebreak" id="pg_194" aria-label="194"/>some examples that illustrate the diversity of problems leveraged by cryptographers:</p>&#13;
<ul class="ul">&#13;
<li class="BL">The Diffie–Hellman problem (given <i>g</i><i><sup>x</sup></i> and <i>g</i> <i><sup>y</sup></i>, find <i>g</i><i><sup>xy</sup></i>) is a variant of the discrete logarithm problem and is widely used in key agreement protocols.</li>&#13;
<li class="BL">Lattice problems, such as the shortest vector problem (SVP) as well as the short integer solutions (SIS) and learning with errors (LWE) problems, can be <b>NP</b>-hard, depending on their parameters.</li>&#13;
<li class="BL">Coding problems rely on the hardness of decoding error-correcting codes with insufficient information and have been studied since the late 1970s. These can also be <b>NP</b>-hard.</li>&#13;
<li class="BL">Multivariate problems are about solving nonlinear systems of equations and are potentially <b>NP</b>-hard, but they’ve failed to provide reliable cryptosystems because hard versions are too big and slow, and practical versions were found to be insecure.</li>&#13;
</ul>&#13;
<p class="TX">In <span class="Xref"><a href="chapter10.xhtml">Chapter 10</a></span>, we’ll continue exploring hard problems, especially factoring and its main variant, the RSA problem.</p>&#13;
</section>&#13;
</section>&#13;
</div>
</div>
</body></html>