<html><head></head><body><div id="sbo-rt-content"><section>&#13;
<header>&#13;
<h1 class="chapter">&#13;
<span class="ChapterNumber"><span epub:type="pagebreak" title="183" id="Page_183"/>11</span><br/>&#13;
<span class="ChapterTitle">Statistical Functions in SQL</span></h1>&#13;
</header>&#13;
<figure class="opener">&#13;
<img src="Images/chapterart.png" alt="" width="200" height="200"/>&#13;
</figure>&#13;
<p class="ChapterIntro">In this chapter, we’ll explore SQL statistical functions along with guidelines for using them. A SQL database usually isn’t the first tool a data analyst chooses when they need to do more than calculate sums and averages. Typically, the software of choice is a full-featured statistics package, such as SPSS or SAS, the programming languages R or Python, or even Excel. But you don’t have to discount your database. Standard ANSI SQL, including PostgreSQL’s implementation, offers powerful stats functions and capabilities that reveal a lot about your data without having to export your dataset to another program.</p>&#13;
<p>Statistics is a vast subject worthy of its own book, so we’ll only skim the surface here. Nevertheless, you’ll learn how to apply high-level statistical concepts to help you derive meaning from your data using a new dataset from the US Census Bureau. You’ll also learn to use SQL to create rankings, calculate rates using data about business establishments, and smooth out time-series data using rolling averages and sums.</p>&#13;
<h2 id="h1-501065c11-0001"><span epub:type="pagebreak" title="184" id="Page_184"/>Creating a Census Stats Table</h2>&#13;
<p class="BodyFirst">Let’s return to one of my favorite data sources, the US Census Bureau. This time, you’ll use county data from the 2014–2018 American Community Survey (ACS) 5-Year Estimates, another product from the bureau.</p>&#13;
<p>Use the code in <a href="#listing11-1" id="listinganchor11-1">Listing 11-1</a> to create the table <code>acs_2014_2018_stats</code> and import the CSV file <em>acs_2014_2018_stats.csv</em>. The code and data are available with all the book’s resources via <a href="https://nostarch.com/practical-sql-2nd-edition/" class="LinkURL">https://nostarch.com/practical-sql-2nd-edition/</a>. Remember to change <var>C:\YourDirectory\</var> to the location of the CSV file.</p>&#13;
<pre><code>CREATE TABLE acs_2014_2018_stats (&#13;
  <span class="CodeAnnotationCode" aria-label="annotation1">1</span> geoid text CONSTRAINT geoid_key PRIMARY KEY,&#13;
    county text NOT NULL,&#13;
    st text NOT NULL,&#13;
  <span class="CodeAnnotationCode" aria-label="annotation2">2</span> pct_travel_60_min numeric(5,2),&#13;
    pct_bachelors_higher numeric(5,2),&#13;
    pct_masters_higher numeric(5,2),&#13;
    median_hh_income integer,&#13;
  <span class="CodeAnnotationCode" aria-label="annotation3">3</span> CHECK (pct_masters_higher &lt;= pct_bachelors_higher)&#13;
);&#13;
&#13;
COPY acs_2014_2018_stats&#13;
FROM '<var>C:\YourDirectory\</var>acs_2014_2018_stats.csv'&#13;
WITH (FORMAT CSV, HEADER);&#13;
&#13;
<span class="CodeAnnotationHang" aria-label="annotation4">4</span> SELECT * FROM acs_2014_2018_stats;</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing11-1">Listing 11-1</a>: Creating a 2014–2018 ACS 5-Year Estimates table and importing data</p>&#13;
<p>The <code>acs_2014_2018_stats</code> table has seven columns. The first three <span class="CodeAnnotation" aria-label="annotation1">1</span> include a unique <code>geoid</code> that serves as the primary key, the name of the <code>county</code>, and the state name <code>st</code>. Both <code>county</code> and <code>st</code> carry the <code>NOT NULL</code> constraint because each row should contain a value. The next four columns display certain percentages <span class="CodeAnnotation" aria-label="annotation2">2</span> I derived for each county from estimates in the ACS release, plus one more economic indicator:</p>&#13;
<p class="ListHead"><b><code class="bold">pct_travel_60_min</code></b></p>&#13;
<ol class="none">&#13;
<li>The percentage of workers ages 16 and older who commute more than 60 minutes to work.</li>&#13;
</ol>&#13;
<p class="ListHead"><b><code class="bold">pct_bachelors_higher</code></b></p>&#13;
<ol class="none">&#13;
<li>The percentage of people ages 25 and older whose level of education is a bachelor’s degree or higher. (In the United States, a bachelor’s degree is usually awarded upon completing a four-year college education.)</li>&#13;
</ol>&#13;
<p class="ListHead"><b><code class="bold">pct_masters_higher</code></b></p>&#13;
<ol class="none">&#13;
<li>The percentage of people ages 25 and older whose level of education is a master’s degree or higher. (In the United States, a master’s degree is the first advanced degree earned after completing a bachelor’s degree.)</li>&#13;
</ol>&#13;
<p class="ListHead"><b><span epub:type="pagebreak" title="185" id="Page_185"/><code class="bold">median_hh_income</code></b></p>&#13;
<ol class="none">&#13;
<li>The county’s median household income in 2018 inflation-adjusted dollars. As you learned in <span class="xref" itemid="xref_target_Chapter 6">Chapter 6</span>, a median value is the midpoint in an ordered set of numbers, where half the values are larger than the midpoint and half are smaller. Because averages can be skewed by a few very large or very small values, government reporting on economic data, such as income, tends to use medians.</li>&#13;
</ol>&#13;
<p>We include a <code>CHECK</code> constraint <span class="CodeAnnotation" aria-label="annotation3">3</span> to ensure that the figures for the bachelor’s degree are equal to or higher than those for the master’s degree, because in the United States, a bachelor’s degree is earned before or concurrently with a master’s degree. A county showing the opposite could indicate data imported incorrectly or a column mislabeled. Our data checks out: upon import, there are no errors showing a violation of the <code>CHECK</code> constraint.</p>&#13;
<p>We use the <code>SELECT</code> statement <span class="CodeAnnotation" aria-label="annotation4">4</span> to view all 3,142 rows imported, each corresponding to a county surveyed in this census release.</p>&#13;
<p>Next, we’ll use statistics functions in SQL to better understand the relationships among the percentages.</p>&#13;
<aside epub:type="sidebar">&#13;
<div class="top hr"><hr/></div>&#13;
<section class="box">&#13;
<h2>The US Census: Estimates vs. the Complete Count</h2>&#13;
<p class="BoxBodyFirst">Each US Census Bureau data product has its own methodology. The Decennial Census, the most well-known, is a full count of the US population conducted every 10 years using forms mailed to each household in the country and visits by census workers. One of its primary purposes is to determine the number of seats each state holds in the US House of Representatives. The census population estimates we’ve used build off the decennial count and use births, deaths, migration, and other factors to create population totals for the years between the decennial counts.</p>&#13;
<p>In contrast, the American Community Survey is an ongoing annual survey of about 3.5 million US households. It asks about topics including income, education, employment, ancestry, and housing. Private and public organizations use ACS data to track trends that drive decision-making. Currently, the US Census Bureau packages ACS data into two releases: a one-year dataset with estimates for geographies with populations of 65,000 or more, and a five-year dataset that includes all geographies. Because it’s a survey, ACS results are estimates and have a margin of error, which I’ve omitted for brevity but which you’ll see included in a full ACS dataset.</p>&#13;
<div class="bottom hr"><hr/></div>&#13;
</section>&#13;
</aside>&#13;
<h3 id="h2-501065c11-0001">Measuring Correlation with corr(Y, X)</h3>&#13;
<p class="BodyFirst"><em>Correlation</em> describes the statistical relationship between two variables, measuring the extent to which a change in one is associated with a change <span epub:type="pagebreak" title="186" id="Page_186"/>in the other. In this section, we’ll use the SQL <code>corr(</code><var>Y</var><code>, </code><var>X</var><code>)</code> function to measure what relationship exists, if any, between the percentage of people in a county who’ve attained a bachelor’s degree and the median household income in that county. We’ll also determine whether, according to our data, a better-educated population typically equates to higher income and, if it does, the strength of that relationship.</p>&#13;
<p>First, some background. The <em>Pearson correlation coefficient</em> (generally denoted as <em>r</em>) measures the strength and direction of a <em>linear relationship</em> between two variables. Variables that have a strong linear relationship cluster along a line when graphed on a scatterplot. The Pearson value of <em>r</em> falls between −1 and 1; either end of the range indicates a perfect correlation, whereas values near zero indicate a random distribution with little correlation. A positive <em>r</em> value indicates a <em>direct relationship</em>: as one variable increases, the other does too. When graphed, the data points representing each pair of values in a direct relationship would slope upward from left to right. A negative <em>r</em> value indicates an <em>inverse</em> <em>relationship</em>: as one variable increases, the other decreases. Dots representing an inverse relationship would slope downward from left to right on a scatterplot.</p>&#13;
<p><a href="#table11-1" id="tableanchor11-1">Table 11-1</a> provides general guidelines for interpreting positive and negative <em>r</em> values, although different statisticians may offer different interpretations.</p>&#13;
<figure>&#13;
<figcaption class="TableTitle"><p><a id="table11-1">Table 11-1</a>: Interpreting Correlation Coefficients</p></figcaption>&#13;
<table id="table-501065c11-0001" border="1">&#13;
<thead>&#13;
<tr>&#13;
<td><b>Correlation coefficient (+/−)</b></td>&#13;
<td><b>What it could mean</b></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td>0</td>&#13;
<td>No relationship</td>&#13;
</tr>&#13;
<tr>&#13;
<td>.01 to .29</td>&#13;
<td>Weak relationship</td>&#13;
</tr>&#13;
<tr>&#13;
<td>.3 to .59</td>&#13;
<td>Moderate relationship</td>&#13;
</tr>&#13;
<tr>&#13;
<td>.6 to .99</td>&#13;
<td>Strong to nearly perfect relationship</td>&#13;
</tr>&#13;
<tr>&#13;
<td>1</td>&#13;
<td>Perfect relationship</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
</figure>&#13;
<p>In standard ANSI SQL and PostgreSQL, we calculate the Pearson correlation coefficient using <code>corr(</code><var>Y</var><code>, </code><var>X</var><code>)</code>. It’s one of several <em>binary aggregate functions</em> in SQL and is so named because these functions accept two inputs. The input <var>Y</var> is the <em>dependent variable</em> whose variation depends on the value of another variable, and <var>X</var> is the <em>independent variable</em> whose value doesn’t depend on another variable.</p>&#13;
<aside epub:type="sidebar">&#13;
<div class="top hr"><hr/></div>&#13;
<section class="note">&#13;
<h2><span class="NoteHead">Note</span></h2>&#13;
<p>	Even though SQL specifies the <var>Y</var><code> </code>and <var>X</var> inputs for the <code>corr()</code> function, correlation calculations don’t distinguish between dependent and independent variables. Switching the order of inputs in <code>corr()</code> produces the same result. However, for convenience and readability, these examples order the input variables according to dependent and independent.</p>&#13;
<div class="bottom hr"><hr/></div>&#13;
</section>&#13;
</aside>&#13;
<p><span epub:type="pagebreak" title="187" id="Page_187"/>We’ll use <code>corr(</code><var>Y</var><code>, </code><var>X</var><code>)</code><code/> to discover the relationship between education level and income, with income as our dependent variable and education as our independent variable. Enter the code in <a href="#listing11-2" id="listinganchor11-2">Listing 11-2</a> to use <code>corr(</code><var>Y</var><code>, </code><var>X</var><code>)</code><code/> with <code>median_hh_income</code> and <code>pct_bachelors_higher</code> as inputs.</p>&#13;
<pre><code>SELECT corr(median_hh_income, pct_bachelors_higher)&#13;
    AS bachelors_income_r&#13;
FROM acs_2014_2018_stats;</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing11-2">Listing 11-2</a>: Using <code>corr(</code><var>Y</var><code>, </code><var>X</var><code>)</code> to measure the relationship between education and income</p>&#13;
<p>Run the query; your result should be an <em>r</em> value of just below 0.70 given as the floating-point <code>double precision</code> data type:</p>&#13;
<pre><code>bachelors_income_r&#13;
------------------&#13;
0.6999086502599159</code></pre>&#13;
<p>This positive <em>r</em> value indicates that as a county’s educational attainment increases, household income tends to increase. The relationship isn’t perfect, but the <em>r</em> value shows the relationship is fairly strong. We can visualize this pattern by plotting the variables on a scatterplot using Excel, as shown in <a href="#figure11-1" id="figureanchor11-1">Figure 11-1</a>. Each data point represents one US county; the data point’s position on the x-axis shows the percentage of the population ages 25 and older that has a bachelor’s degree or higher. The data point’s position on the y-axis represents the county’s median household income.</p>&#13;
<figure>&#13;
<img src="Images/f11001.png" alt="f11001" class="" width="689" height="467"/>&#13;
<figcaption><p><a id="figure11-1">Figure 11-1</a>: A scatterplot showing the relationship between education and income</p></figcaption>&#13;
</figure>&#13;
<p><span epub:type="pagebreak" title="188" id="Page_188"/>Notice that although most of the data points are grouped together in the bottom-left corner of the graph, they do generally slope upward from left to right. Also, the points spread out rather than strictly follow a straight line. If they were in a straight line sloping up from left to right, the <em>r</em> value would be 1, indicating a perfect positive linear relationship.</p>&#13;
<h3 id="h2-501065c11-0002">Checking Additional Correlations</h3>&#13;
<p class="BodyFirst">Now let’s calculate the correlation coefficients for the remaining variable pairs using the code in <a href="#listing11-3" id="listinganchor11-3">Listing 11-3</a>.</p>&#13;
<pre><code>SELECT&#13;
  <span class="CodeAnnotationCode" aria-label="annotation1">1</span> round(&#13;
      corr(median_hh_income, pct_bachelors_higher)::numeric, 2&#13;
      ) AS bachelors_income_r,&#13;
    round(&#13;
      corr(pct_travel_60_min, median_hh_income)::numeric, 2&#13;
      ) AS income_travel_r,&#13;
    round(&#13;
      corr(pct_travel_60_min, pct_bachelors_higher)::numeric, 2&#13;
      ) AS bachelors_travel_r&#13;
FROM acs_2014_2018_stats;</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing11-3">Listing 11-3</a>: Using <code>corr(</code><var>Y</var><code>, </code><var>X</var><code>)</code> on additional variables</p>&#13;
<p>This time we’ll round off the decimal values to make the output more readable by wrapping the <code>corr(</code><var>Y</var><code>, </code><var>X</var><code>)</code> function inside SQL’s <code>round()</code> function <span class="CodeAnnotation" aria-label="annotation1">1</span>, which takes two inputs: the <code>numeric</code> value to be rounded and an <code>integer</code> value indicating the number of decimal places to round the first value. If the second parameter is omitted, the value is rounded to the nearest whole integer. Because <code>corr(</code><var>Y</var><code>, </code><var>X</var><code>)</code><code/> returns a floating-point value by default, we cast it to the <code>numeric</code> type using the <code>::</code> notation you learned in <span class="xref" itemid="xref_target_Chapter 4">Chapter 4</span>. Here’s the output:</p>&#13;
<pre><code>bachelors_income_r    income_travel_r    bachelors_travel_r&#13;
------------------    ---------------    ------------------&#13;
              0.70               0.06                 -0.14</code></pre>&#13;
<p>The <code>bachelors_income_r</code> value is <code>0.70</code>, which is the same as our first run but rounded up to two decimal places. Compared to <code>bachelors_income_r</code>, the other two correlations are weak.</p>&#13;
<p>The <code>income_travel_r</code> value shows that the correlation between income and the percentage of those who commute more than an hour to work is practically zero. This indicates that a county’s median household income bears little connection to how long it takes people to get to work.</p>&#13;
<p>The <code>bachelors_travel_r</code> value shows that the correlation of bachelor’s degrees and lengthy commutes is also low at <code>-0.14</code>. The negative value indicates an inverse relationship: as education increases, the percentage of the population that travels more than an hour to work decreases. Although this is interesting, a correlation coefficient that is this close to zero indicates a weak relationship.</p>&#13;
<p><span epub:type="pagebreak" title="189" id="Page_189"/>When testing for correlation, we need to note some caveats. The first is that even a strong correlation does not imply causality. We can’t say that a change in one variable causes a change in the other, only that the changes move together. The second is that correlations should be subject to testing to determine whether they’re statistically significant. Those tests are beyond the scope of this book but worth studying on your own.</p>&#13;
<p>Nevertheless, the SQL <code>corr(</code><var>Y</var><code>, </code><var>X</var><code>)</code><code/> function is a handy tool for quickly checking correlations between variables.</p>&#13;
<h3 id="h2-501065c11-0003">Predicting Values with Regression Analysis</h3>&#13;
<p class="BodyFirst">Researchers also want to predict values using available data. For example, let’s say 30 percent of a county’s population has a bachelor’s degree or higher. Given the trend in our data, what would we expect that county’s median household income to be? Likewise, for each percent increase in education, how much increase, on average, would we expect in income?</p>&#13;
<p>We can answer both questions using <em>linear regression</em>. Simply put, the regression method finds the best linear equation, or straight line, that describes the relationship between an independent variable (such as education) and a dependent variable (such as income). We can then look at points along this line to predict values where we don’t have observations. Standard ANSI SQL and PostgreSQL include functions that perform linear regression.</p>&#13;
<p><a href="#figure11-2" id="figureanchor11-2">Figure 11-2</a> shows our previous scatterplot with a regression line added.</p>&#13;
<figure>&#13;
<img src="Images/f11002.png" alt="f11002" class="" width="682" height="467"/>&#13;
<figcaption><p><a id="figure11-2">Figure 11-2</a>: Scatterplot with least squares regression line showing the relationship between education and income</p></figcaption>&#13;
</figure>&#13;
<p><span epub:type="pagebreak" title="190" id="Page_190"/>The straight line running through the middle of all the data points is called the <em>least squares regression line</em>, which approximates the “best fit” for a straight line that best describes the relationship between the variables. The equation for the regression line is like the <em>slope-intercept</em> formula you might remember from high school math but written using differently named variables: <em>Y</em> = <em>bX</em> + <em>a</em>. Here are the formula’s components:</p>&#13;
<ol class="none">&#13;
<li><b>Y</b> is the predicted value, which is also the value on the y-axis, or dependent variable.</li>&#13;
<li><b>b</b> is the slope of the line, which can be positive or negative. It measures how many units the y-axis value will increase or decrease for each unit of the x-axis value.</li>&#13;
<li><b>X</b> represents a value on the x-axis, or independent variable.</li>&#13;
<li><b>a</b> is the y-intercept, the value at which the line crosses the y-axis when the <em>X</em> value is zero.</li>&#13;
</ol>&#13;
<p>Let’s apply this formula using SQL. Earlier, we questioned the expected median household income in a county where than 30 percent or more of the population had a bachelor’s degree. In our scatterplot, the percentage with bachelor’s degrees falls along the x-axis, represented by <em>X</em> in the calculation. Let’s plug that value into the regression line formula in place of <em>X</em>:</p>&#13;
<p class="Equation"><em>Y</em> = <em>b</em>(30) + <em>a</em></p>&#13;
<p>To calculate <em>Y</em>, which represents the predicted median household income, we need the line’s slope, <em>b</em>, and the y-intercept, <em>a</em>. To get these values, we’ll use the SQL functions <code>regr_slope(</code><var>Y</var><code>, </code><var>X</var><code>)</code> and <code>regr_intercept(</code><var>Y</var><code>, </code><var>X</var><code>)</code>,  as shown in <a href="#listing11-4" id="listinganchor11-4">Listing 11-4</a>.</p>&#13;
<pre><code>SELECT&#13;
    round(&#13;
        regr_slope(median_hh_income, pct_bachelors_higher)::numeric, 2&#13;
        ) AS slope,&#13;
    round(&#13;
        regr_intercept(median_hh_income, pct_bachelors_higher)::numeric, 2&#13;
        ) AS y_intercept&#13;
FROM acs_2014_2018_stats;</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing11-4">Listing 11-4</a>: Regression slope and intercept functions</p>&#13;
<p>Using the <code>median_hh_income</code> and <code>pct_bachelors_higher</code> variables as inputs for both functions, we’ll set the resulting value of the <code>regr_slope(</code><var>Y</var><code>, </code><var>X</var><code>)</code> function as <code>slope</code> and the output for the <code>regr_intercept(</code><var>Y</var><code>, </code><var>X</var><code>)</code> function as <code>y_intercept</code>.</p>&#13;
<p>Run the query; the result should show the following:</p>&#13;
<pre><code>slope      y_intercept&#13;
-------    -----------&#13;
1016.55       29651.42</code></pre>&#13;
<p>The <code>slope</code> value shows that for every one-unit increase in bachelor’s degree percentage, we can expect a county’s median household income will <span epub:type="pagebreak" title="191" id="Page_191"/>increase by $1,016.55. The <code>y_intercept</code> value shows that when the regression line crosses the y-axis, where the percentage with bachelor’s degrees is at 0, the y-axis value is 29,651.42. Now let’s plug both values into the equation to get our predicted value <em>Y</em>:</p>&#13;
<p class="Equation" style="text-align: left; margin-bottom:0em; margin-left: 25%;"><em>Y</em> = 1016.55(30) + 29651.42</p>&#13;
<p class="Equation" style="text-align: left; margin-top:0em; margin-left: 25%;"><em>Y</em> = 60147.92</p>&#13;
<p>Based on our calculation, in a county in which 30 percent of people age 25 and older have a bachelor’s degree or higher, we can expect a median household income to be about $60,148. Of course, our data includes counties whose median income falls above and below that predicted value, but we expect this to be the case because our data points in the scatterplot don’t line up perfectly along the regression line. Recall that the correlation coefficient we calculated was 0.70, indicating a strong but not perfect relationship between education and income. Other factors likely contributed to variations in income, such as the types of jobs available in each county.</p>&#13;
<h3 id="h2-501065c11-0004">Finding the Effect of an Independent Variable with r-Squared</h3>&#13;
<p class="BodyFirst">Beyond determining the direction and strength of the relationship between two variables, we can also calculate the extent that the variation in the <em>x</em> (independent) variable explains the variation in the <em>y</em> (dependent) variable. To do this we square the <em>r</em> value to find the <em>coefficient of determination</em>, better known as <em>r-squared</em>. An <em>r</em>-squared indicates the percentage of the variation that is explained by the independent variable, and is a value between zero and one. For example, if <em>r</em>-squared equals 0.1, we would say that the independent variable explains 10 percent of the variation in the dependent variable, or not much at all.</p>&#13;
<p>To find <em>r</em>-squared, we use the <code>regr_r2(</code><var>Y</var><code>, </code><var>X</var><code>) </code>function in SQL. Let’s apply it to our education and income variables using the code in <a href="#listing11-5" id="listinganchor11-5">Listing 11-5</a>.</p>&#13;
<pre><code>SELECT round(&#13;
        regr_r2(median_hh_income, pct_bachelors_higher)::numeric, 3&#13;
        ) AS r_squared&#13;
FROM acs_2014_2018_stats;</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing11-5">Listing 11-5</a>: Calculating the coefficient of determination, or <em>r</em>-squared</p>&#13;
<p>This time we’ll round off the output to the nearest thousandth place and alias the result to <code>r_squared</code>. The query should return the following result:</p>&#13;
<pre><code>r_squared&#13;
---------&#13;
    0.490</code></pre>&#13;
<p>The <em>r</em>-squared value of <code>0.490</code> indicates that about 49 percent of the variation in median household income among counties can be explained by the percentage of people with a bachelor’s degree or higher in that county. Any number of factors could explain the other 51 percent, and statisticians <span epub:type="pagebreak" title="192" id="Page_192"/>will typically test numerous combinations of variables to determine what they are.</p>&#13;
<p>Before you use these numbers in a headline or presentation, it’s worth revisiting the following points:</p>&#13;
<ul>&#13;
<li>Correlation doesn’t prove causality. For verification, do a Google search on “correlation and causality.” Many variables correlate well but have no meaning. (See <a href="https://www.tylervigen.com/spurious-correlations" class="LinkURL">https://www.tylervigen.com/spurious-correlations</a> for examples of correlations that don’t prove causality, including the correlation between divorce rate in Maine and margarine consumption.) Statisticians usually perform <em>significance testing</em> on the results to make sure values are not simply the result of randomness.</li>&#13;
<li>Statisticians also apply additional tests to data before accepting the results of a regression analysis, including whether the variables follow the standard bell curve distribution and meet other criteria for a valid result.</li>&#13;
</ul>&#13;
<p>Let’s explore two additional concepts before wrapping up our look at statistical functions.</p>&#13;
<h3 id="h2-501065c11-0005">Finding Variance and Standard Deviation</h3>&#13;
<p class="BodyFirst"><em>Variance</em> and <em>standard deviation</em> describe the degree to which a set of values varies from the average of those values. Variance, often used in finance, is the average of each number’s squared distance from the average. The more dispersion in a set of values, the greater the variance. A stock market trader can use variance to measure the volatility of a particular stock—how much its daily closing values tend to vary from the average. That could indicate how risky an investment the stock might be.</p>&#13;
<p>Standard deviation is the square root of the variance and is most useful for assessing data whose values form a normal distribution, usually visualized as a symmetrical <em>bell curve</em>. In a <em>normal distribution</em>, about two-thirds of values fall within one standard deviation of the average; 95 percent are within two standard deviations. The standard deviation of a set of values, therefore, helps us understand how close most of our values are to the average. For example, consider a study that found the average height of adult US women is about 65.5 inches with a standard deviation of 2.5 inches. Given that heights are normally distributed, that means about two-thirds of women are within 2.5 inches of the average, or 63 inches to 68 inches tall.</p>&#13;
<p>When calculating variance and standard deviation, note that they report different units. Standard deviation is expressed in the same units as the values, while variance is not—it reports a number that is larger than the units, on a scale of its own.</p>&#13;
<p>These are the functions for calculating variance:</p>&#13;
<ol class="none">&#13;
<li><span class="RunInHead"><span class="LiteralBold"><code>var_pop(numeric)</code></span></span>  Calculates the population variance of the input values. In this context, <em>population</em> refers to a dataset that contains all possible values, as opposed to a sample that just contains a portion of all possible values.</li>&#13;
<li><span epub:type="pagebreak" title="193" id="Page_193"/><span class="RunInHead"><span class="LiteralBold"><code>var_samp(numeric)</code></span></span>  Calculates the sample variance of the input values. Use this with data that is sampled from a population, as in a random sample survey.</li>&#13;
</ol>&#13;
<p>For calculating standard deviation, we use these:</p>&#13;
<ol class="none">&#13;
<li><span class="RunInHead"><span class="LiteralBold"><code>stddev_pop(numeric)</code></span></span>  Calculates the population standard deviation.</li>&#13;
<li><span class="RunInHead"><span class="LiteralBold"><code>stddev_samp(numeric)</code></span><span class="LiteralBold"><code/></span></span>  Calculates the sample standard deviation.</li>&#13;
</ol>&#13;
<p>With functions covering correlation, regression, and other descriptive statistics, you have a basic toolkit for obtaining a preliminary survey of your data before doing more rigorous analysis. All these topics are worth in-depth study to better understand when you might use them and what they measure. A classic, easy-to-understand resource I recommend is the book <em>Statistics</em> by David Freedman, Robert Pisani, and Roger Purves.</p>&#13;
<h2 id="h1-501065c11-0002">Creating Rankings with SQL</h2>&#13;
<p class="BodyFirst">Rankings make the news often. You’ll see them used anywhere from weekend box-office charts to sports teams’ league standings. With SQL you can create numbered rankings in your query results, which are useful for tasks such as tracking changes over several years. You can also simply use a ranking as a fact on its own in a report. Let’s explore how to create rankings using SQL.</p>&#13;
<h3 id="h2-501065c11-0006">Ranking with rank() and dense_rank()</h3>&#13;
<p class="BodyFirst">Standard ANSI SQL includes several ranking functions, but we’ll just focus on two: <code>rank()</code> and <code>dense_rank()</code>. Both are <em>window functions</em>, which are defined as functions that perform calculations across a set of rows relative to the current row. Unlike aggregate functions, which combine rows to calculate values, with window functions the query first generates a set of rows, and then the window function runs across the result set to calculate the value it will return.</p>&#13;
<p>The difference between <code>rank()</code> and <code>dense_rank()</code> is the way they handle the next rank value after a tie: <code>rank()</code> includes a gap in the rank order, but <code>dense_rank()</code> does not. This concept is easier to understand in action, so let’s look at an example. Consider a Wall Street analyst who covers the highly competitive widget manufacturing market. The analyst wants to rank companies by their annual output. The SQL statements in <a href="#listing11-6" id="listinganchor11-6">Listing 11-6</a> create and fill a table with this data and then rank the companies by widget output.</p>&#13;
<pre><code>CREATE TABLE widget_companies (&#13;
    id integer PRIMARY KEY GENERATED ALWAYS AS IDENTITY,&#13;
    company text NOT NULL,&#13;
    widget_output integer NOT NULL&#13;
);&#13;
&#13;
INSERT INTO widget_companies (company, widget_output)&#13;
<span epub:type="pagebreak" title="194" id="Page_194"/>VALUES&#13;
    ('Dom Widgets', 125000),&#13;
    ('Ariadne Widget Masters', 143000),&#13;
    ('Saito Widget Co.', 201000),&#13;
    ('Mal Inc.', 133000),&#13;
    ('Dream Widget Inc.', 196000),&#13;
    ('Miles Amalgamated', 620000),&#13;
    ('Arthur Industries', 244000),&#13;
    ('Fischer Worldwide', 201000);&#13;
&#13;
SELECT&#13;
    company,&#13;
    widget_output,&#13;
  <span class="CodeAnnotationCode" aria-label="annotation1">1</span> rank() OVER (ORDER BY widget_output DESC),&#13;
  <span class="CodeAnnotationCode" aria-label="annotation2">2</span> dense_rank() OVER (ORDER BY widget_output DESC)&#13;
FROM widget_companies&#13;
ORDER BY widget_output DESC;</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing11-6">Listing 11-6</a>: Using the <code>rank()</code> and <code>dense_rank()</code> window functions</p>&#13;
<p>Notice the syntax in the <code>SELECT</code> statement that includes <code>rank()</code> <span class="CodeAnnotation" aria-label="annotation1">1</span> and <code>dense_rank()</code> <span class="CodeAnnotation" aria-label="annotation2">2</span>. After the function names, we use the <code>OVER</code> clause and in parentheses place an expression that specifies the “window” of rows the function should operate on. The <em>window</em> is the set of rows relative to the current row, and in this case, we want both functions to work on all rows of the <code>widget_output</code> column, sorted in descending order. Here’s the output:</p>&#13;
<pre><code>company                       widget_output    rank    dense_rank&#13;
--------------------------    -------------    ----    ----------&#13;
Miles Amalgamated                 620000         1          1&#13;
Arthur Industries                 244000         2          2&#13;
Fischer Worldwide                 201000         3          3&#13;
Saito Widget Co.                  201000         3          3&#13;
Dream Widget Inc.                 196000         5          4&#13;
Ariadne Widget Masters            143000         6          5&#13;
Mal Inc.                          133000         7          6&#13;
Dom Widgets                       125000         8          7</code></pre>&#13;
<p>The columns produced by <code>rank()</code> and <code>dense_rank()</code> show each company’s ranking based on the <code>widget_output</code> value from highest to lowest, with Miles Amalgamated at number one. To see how <code>rank()</code> and <code>dense_rank()</code> differ, check the fifth-row listing, Dream Widget Inc.</p>&#13;
<p>With <code>rank()</code>, Dream Widget Inc. is the fifth-highest-ranking company. Because <code>rank()</code> allows a gap in the order when a tie occurs, Dream placing fifth tells us there are four companies with more output. In contrast, <code>dense_rank()</code> doesn’t allow a gap in the rank order so it places Dream Widget Inc. in fourth place. This reflects the fact that Dream has the fourth-highest widget output regardless of how many companies produced more.</p>&#13;
<p>Both ways of handling ties have merit, but in practice <code>rank()</code> is used most often. It’s also what I recommend using, because it more accurately reflects the total number of companies ranked, shown by the fact that Dream Widget Inc. has four companies ahead of it in total output, not three.</p>&#13;
<p><span epub:type="pagebreak" title="195" id="Page_195"/>Let’s look at a more complex ranking example.</p>&#13;
<h3 id="h2-501065c11-0007">Ranking Within Subgroups with PARTITION BY</h3>&#13;
<p class="BodyFirst">The ranking we just did was a simple overall ranking based on widget output. But sometimes you’ll want to produce ranks within groups of rows in a table. For example, you might want to rank government employees by salary within each department or rank movies by box-office earnings within each genre.</p>&#13;
<p>To use window functions in this way, we’ll add <code>PARTITION BY</code> to the <code>OVER</code> clause. A <code>PARTITION BY</code> clause divides table rows according to values in a column we specify.</p>&#13;
<p>Here’s an example using made-up data about grocery stores. Enter the code in <a href="#listing11-7" id="listinganchor11-7">Listing 11-7</a> to fill a table called <code>store_sales</code>.</p>&#13;
<pre><code>CREATE TABLE store_sales (&#13;
    store text NOT NULL,&#13;
    category text NOT NULL,&#13;
    unit_sales bigint NOT NULL,&#13;
    CONSTRAINT store_category_key PRIMARY KEY (store, category)&#13;
);&#13;
&#13;
INSERT INTO store_sales (store, category, unit_sales)&#13;
VALUES&#13;
    ('Broders', 'Cereal', 1104),&#13;
    ('Wallace', 'Ice Cream', 1863),&#13;
    ('Broders', 'Ice Cream', 2517),&#13;
    ('Cramers', 'Ice Cream', 2112),&#13;
    ('Broders', 'Beer', 641),&#13;
    ('Cramers', 'Cereal', 1003),&#13;
    ('Cramers', 'Beer', 640),&#13;
    ('Wallace', 'Cereal', 980),&#13;
    ('Wallace', 'Beer', 988);&#13;
&#13;
SELECT&#13;
    category,&#13;
    store,&#13;
    unit_sales,&#13;
  <span class="CodeAnnotationCode" aria-label="annotation1">1</span> rank() OVER (PARTITION BY category ORDER BY unit_sales DESC)&#13;
FROM store_sales&#13;
<span class="CodeAnnotationHang" aria-label="annotation2">2</span> ORDER BY category, rank() OVER (PARTITION BY category&#13;
        ORDER BY unit_sales DESC);</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing11-7">Listing 11-7</a>: Applying <code>rank()</code> within groups using <code>PARTITION BY</code></p>&#13;
<p>In the table, each row includes a store’s product category and sales for that category. The final <code>SELECT</code> statement creates a result set showing how each store’s sales ranks within each category. The new element is the addition of <code>PARTITION BY</code> in the <code>OVER</code> clause <span class="CodeAnnotation" aria-label="annotation1">1</span>. In effect, the clause tells the program to create rankings one category at a time, using the store’s unit sales in descending order.</p>&#13;
<p><span epub:type="pagebreak" title="196" id="Page_196"/>To display the results by category and rank, we add an <code>ORDER BY</code> clause <span class="CodeAnnotation" aria-label="annotation2">2</span> that includes the <code>category</code> column and the same <code>rank()</code> function syntax. Here’s the output:</p>&#13;
<pre><code>category     store      unit_sales    rank&#13;
---------    -------    ----------    ----&#13;
Beer         Wallace           988       1&#13;
Beer         Broders           641       2&#13;
Beer         Cramers           640       3&#13;
Cereal       Broders          1104       1&#13;
Cereal       Cramers          1003       2&#13;
Cereal       Wallace           980       3&#13;
Ice Cream    Broders          2517       1&#13;
Ice Cream    Cramers          2112       2&#13;
Ice Cream    Wallace          1863       3</code></pre>&#13;
<p>Rows for each category are ordered by category unit sales with the <code>rank</code> column displaying the ranking.</p>&#13;
<p>Using this table, we can see at a glance how each store ranks in a food category. For instance, Broders tops sales for cereal and ice cream, but Wallace wins in the beer category. You can apply this concept to many other scenarios: for each auto manufacturer, finding the vehicle with the most consumer complaints; figuring out which month had the most rainfall in each of the last 20 years; finding the team with the most wins against left-handed pitchers; and so on.</p>&#13;
<h2 id="h1-501065c11-0003">Calculating Rates for Meaningful Comparisons</h2>&#13;
<p class="BodyFirst">Rankings based on raw counts aren’t always meaningful; in fact, they can be misleading. Consider birth statistics: the US National Center for Health Statistics (NCHS) reported that in 2019, there were 377,599 babies born in the state of Texas and 46,826 born in the state of Utah. So, women in Texas are more likely to have babies, right? Not so fast. In 2019, Texas’ estimated population was nine times as much as Utah’s. Given that context, comparing the plain number of births in the two states isn’t very meaningful.</p>&#13;
<p>A more accurate way to compare these numbers is to convert them to rates. Analysts often calculate a rate per 1,000 people, or some multiple of that number, to allow an apples-to-apples comparison. For example, the fertility rate—the number of births per 1,000 women ages 15 to 44—was 62.5 for Texas in 2019 and 66.7 for Utah, according to the NCHS. So, despite the smaller number of births, on a per-1,000 rate, women in Utah actually had more children.</p>&#13;
<p>The math behind this is simple. Let’s say your town had 115 births and a population of 2,200 women ages 15 to 44. You can find the per-1,000 rate as follows:</p>&#13;
<p class="Equation">(115 / 2,200) × 1,000 = 52.3</p>&#13;
<p>In your town, there were 52.3 births per 1,000 women ages 15 to 44, which you can now compare to other places regardless of their size.</p>&#13;
<h3 id="h2-501065c11-0008"><span epub:type="pagebreak" title="197" id="Page_197"/>Finding Rates of Tourism-Related Businesses</h3>&#13;
<p class="BodyFirst">Let’s try calculating rates using SQL and census data. We’ll join two tables: the census population estimates you imported in <span class="xref" itemid="xref_target_Chapter 5">Chapter 5</span> plus data I compiled about tourism-related businesses from the census’ County Business Patterns program. (You can read about the program methodology at <a href="https://www.census.gov/programs-surveys/cbp/about.html" class="LinkURL">https://www.census.gov/programs-surveys/cbp/about.html</a>.)</p>&#13;
<p><a href="#listing11-8" id="listinganchor11-8">Listing 11-8</a> contains the code to create and fill the business patterns table. Remember to point the script to the location in which you’ve saved the CSV file <em>cbp_naics_72_establishments.csv</em>, which you can download from GitHub via the link at <a href="https://nostarch.com/practical-sql-2nd-edition/" class="LinkURL">https://nostarch.com/practical-sql-2nd-edition/</a>.</p>&#13;
<pre><code>CREATE TABLE cbp_naics_72_establishments (&#13;
    state_fips text,&#13;
    county_fips text,&#13;
    county text NOT NULL,&#13;
    st text NOT NULL,&#13;
    naics_2017 text NOT NULL,&#13;
    naics_2017_label text NOT NULL,&#13;
    year smallint NOT NULL,&#13;
    establishments integer NOT NULL,&#13;
    CONSTRAINT cbp_fips_key PRIMARY KEY (state_fips, county_fips)&#13;
);&#13;
&#13;
COPY cbp_naics_72_establishments&#13;
FROM '<var>C:\YourDirectory\</var>cbp_naics_72_establishments.csv'&#13;
WITH (FORMAT CSV, HEADER);&#13;
&#13;
SELECT *&#13;
FROM cbp_naics_72_establishments&#13;
ORDER BY state_fips, county_fips&#13;
LIMIT 5;</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing11-8">Listing 11-8</a>: Creating and filling a table for census county business pattern data</p>&#13;
<p>Once you’ve imported the data, run the final <code>SELECT</code> statement to view the first few rows of the table. Each row contains descriptive information about a county along with the number of business establishments that fall under code 72 of the North American Industry Classification System (NAICS). Code 72 covers “Accommodation and Food Services” establishments, mainly hotels, inns, bars, and restaurants. The number of those businesses in a county is a good proxy for the amount of tourist and recreation activity in the area.</p>&#13;
<p>Let’s find out which counties have the highest concentration of such businesses per 1,000 population, using the code in <a href="#listing11-9" id="listinganchor11-9">Listing 11-9</a>.</p>&#13;
<pre><code>SELECT&#13;
    cbp.county,&#13;
    cbp.st,&#13;
    cbp.establishments,&#13;
    pop.pop_est_2018,&#13;
  <span class="CodeAnnotationCode" aria-label="annotation1">1</span> round( (cbp.establishments::numeric / pop.pop_est_2018) * 1000, 1 )&#13;
        AS estabs_per_1000&#13;
<span epub:type="pagebreak" title="198" id="Page_198"/>FROM cbp_naics_72_establishments cbp JOIN us_counties_pop_est_2019 pop&#13;
    ON cbp.state_fips = pop.state_fips&#13;
    AND cbp.county_fips = pop.county_fips&#13;
<span class="CodeAnnotationHang" aria-label="annotation2">2</span> WHERE pop.pop_est_2018 &gt;= 50000&#13;
ORDER BY cbp.establishments::numeric / pop.pop_est_2018 DESC;</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing11-9">Listing 11-9</a>: Finding business rates per thousand population in counties with 50,000 or more people</p>&#13;
<p>Overall, this syntax should look familiar. In <span class="xref" itemid="xref_target_Chapter 5">Chapter 5</span>, you learned that when dividing an integer by an integer, one of the values must be a <code>numeric</code> or <code>decimal</code> for the result to include decimal places. We do that in the rate calculation <span class="CodeAnnotation" aria-label="annotation1">1</span> with PostgreSQL’s double-colon shorthand. Because we don’t need many decimal places, we wrap the statement in the <code>round()</code> function to round off the output to the nearest tenth. Then we give the calculated column an alias of <code>estabs_per_1000</code> for easy reference.</p>&#13;
<p>Also, we use a <code>WHERE</code> clause <span class="CodeAnnotation" aria-label="annotation2">2</span> to limit our results to counties with 50,000 or more people. That’s an arbitrary value that lets us see how rates compare within a group of more-populous, better-known counties. Here’s a portion of the results, sorted with highest rates at top:</p>&#13;
<pre><code>    county             st      establishments  pop_est_2018  estabs_per_1000&#13;
------------------ ----------- --------------- ------------- ---------------&#13;
Cape May County    New Jersey              925         92446            10.0&#13;
Worcester County   Maryland                453         51960             8.7&#13;
Monroe County      Florida                 540         74757             7.2&#13;
Warren County      New York                427         64215             6.6&#13;
New York County    New York              10428       1629055             6.4&#13;
Hancock County     Maine                   337         54734             6.2&#13;
Sevier County      Tennessee               570         97895             5.8&#13;
Eagle County       Colorado                309         54943             5.6&#13;
<var>--snip--</var></code></pre>&#13;
<p>The counties that have the highest rates make sense. Cape May County, New Jersey, is home to numerous beach resort towns on the Atlantic Ocean and Delaware Bay. Worcester County, Maryland, contains Ocean City and other beach attractions. And Monroe County, Florida, is best known for its vacation hotspot, the Florida Keys. Sense a pattern?</p>&#13;
<h2 id="h1-501065c11-0004">Smoothing Uneven Data</h2>&#13;
<p class="BodyFirst">A <em>rolling average</em> is an average calculated for each time period in a dataset, using a moving window of rows as input each time. Think of a hardware store: it might sell 20 hammers on Monday, 15 hammers on Tuesday, and just a few the rest of the week. The next week, hammer sales might spike on Friday. To find the big-picture story in such uneven data, we can smooth numbers by calculating the rolling average, sometimes called a <em>moving average</em>.</p>&#13;
<p><span epub:type="pagebreak" title="199" id="Page_199"/>Here are two weeks of hammer sales at that hypothetical hardware store:</p>&#13;
<pre><code>Date        Hammer sales  Seven-day average&#13;
----------  ------------  -----------------&#13;
2022-05-01       0&#13;
2022-05-02      20&#13;
2022-05-03      15&#13;
2022-05-04       3&#13;
2022-05-05       6&#13;
2022-05-06       1&#13;
<span class="CodeAnnotationHang" aria-label="annotation1">1</span> 2022-05-07       1             6.6&#13;
<span class="CodeAnnotationHang" aria-label="annotation2">2</span> 2022-05-08       2             6.9&#13;
2022-05-09      18             6.6&#13;
2022-05-10      13             6.3&#13;
2022-05-11       2             6.1&#13;
2022-05-12       4             5.9&#13;
2022-05-13      12             7.4&#13;
2022-05-14       2             7.6</code></pre>&#13;
<p>Let’s say that for every day we want to know the average sales over the last seven days (we can choose any period, but a week is an intuitive unit). Once we have seven days of data <span class="CodeAnnotation" aria-label="annotation1">1</span>, we calculate the average of sales over the seven-day period that includes the current day. The average of hammer sales from May 1 to May 7, 2022, is <code>6.6</code> per day.</p>&#13;
<p>The next day <span class="CodeAnnotation" aria-label="annotation2">2</span>, we again average sales over the most recent seven days, from May 2 to May 8, 2022. The result is <code>6.9</code> per day. As we continue each day, despite the ups and downs in the daily sales, the seven-day average remains fairly steady. Over a long period of time, we’ll be able to better discern a trend.</p>&#13;
<p>Let’s use the window function syntax again to perform this calculation using the code in <a href="#listing11-10" id="listinganchor11-10">Listing 11-10</a>. The code and data are available with all the book’s resources in GitHub, available via <a href="https://nostarch.com/practical-sql-2nd-edition/" class="LinkURL">https://nostarch.com/practical-sql-2nd-edition/</a>. Remember to change <var>C:\YourDirectory\</var> to the location of the CSV file.</p>&#13;
<pre><code><span class="CodeAnnotationHang" aria-label="annotation1">1</span> CREATE TABLE us_exports (&#13;
    year smallint,&#13;
    month smallint,&#13;
    citrus_export_value bigint,&#13;
    soybeans_export_value bigint&#13;
);&#13;
&#13;
<span class="CodeAnnotationHang" aria-label="annotation2">2</span> COPY us_exports&#13;
FROM '<var>C:\YourDirectory\</var>us_exports.csv'&#13;
WITH (FORMAT CSV, HEADER);&#13;
&#13;
<span class="CodeAnnotationHang" aria-label="annotation3">3</span> SELECT year, month, citrus_export_value&#13;
FROM us_exports&#13;
ORDER BY year, month;&#13;
&#13;
<span epub:type="pagebreak" title="200" id="Page_200"/><span class="CodeAnnotationHang" aria-label="annotation4">4</span> SELECT year, month, citrus_export_value,&#13;
    round(&#13;
       <span class="CodeAnnotationCode" aria-label="annotation5">5</span> avg(citrus_export_value)&#13;
            <span class="CodeAnnotationCode" aria-label="annotation6">6</span> OVER(ORDER BY year, month&#13;
                 <span class="CodeAnnotationCode" aria-label="annotation7">7</span> ROWS BETWEEN 11 PRECEDING AND CURRENT ROW), 0)&#13;
          AS twelve_month_avg&#13;
FROM us_exports&#13;
ORDER BY year, month;</code></pre>&#13;
<p class="CodeListingCaption"><a id="listing11-10">Listing 11-10</a>: Creating a rolling average for export data</p>&#13;
<p>We create a table <span class="CodeAnnotation" aria-label="annotation1">1</span> and use <code>COPY</code> <span class="CodeAnnotation" aria-label="annotation2">2</span> to insert data from <em>us_exports.csv</em>. This file contains data showing the monthly dollar value of US exports of citrus fruit and soybeans, two commodities whose sales are tied to the growing season. The data comes from the US Census Bureau’s international trade division at <a href="https://usatrade.census.gov/" class="LinkURL">https://usatrade.census.gov/</a>.</p>&#13;
<p>The first <code>SELECT</code> statement <span class="CodeAnnotation" aria-label="annotation3">3</span> lets you view the monthly citrus export data, which covers every month from 2002 through summer 2020. The last dozen rows should look like this:</p>&#13;
<pre><code>year month citrus_export_value&#13;
---- ----- -------------------&#13;
<var>--snip--</var>&#13;
2019     9            14012305&#13;
2019    10            26308151&#13;
2019    11            60885676&#13;
2019    12            84873954&#13;
2020     1           110924836&#13;
2020     2           171767821&#13;
2020     3           201231998&#13;
2020     4           122708243&#13;
2020     5            75644260&#13;
2020     6            36090558&#13;
2020     7            20561815&#13;
2020     8            15510692</code></pre>&#13;
<p>Notice the pattern: the value of citrus fruit exports is highest in winter months, when the growing season is paused in the northern hemisphere and countries need imports to meet demand. We’ll use the second <code>SELECT</code> statement <span class="CodeAnnotation" aria-label="annotation4">4</span> to compute a 12-month rolling average so we can see, for each month, the annual trend in exports.</p>&#13;
<p>In the <code>SELECT</code> values list, we place an <code>avg()</code> <span class="CodeAnnotation" aria-label="annotation5">5</span> function to calculate the average of the values in the <code>citrus_export_value</code> column. We follow the function with an <code>OVER</code> clause <span class="CodeAnnotation" aria-label="annotation6">6</span> that has two elements in parentheses: an <code>ORDER BY</code> clause that sorts the data for the period we plan to average, and the number of rows to average, using the keywords <code>ROWS BETWEEN 11 PRECEDING AND CURRENT ROW</code> <span class="CodeAnnotation" aria-label="annotation7">7</span>. This tells PostgreSQL to limit the window to the current row and the 11 rows before it—12 total.</p>&#13;
<p>We wrap the entire statement, from the <code>avg()</code> function through the <code>OVER</code> clause, in a <code>round()</code> function to limit the output to whole numbers. The last dozen rows of your query result should be as follows:</p>&#13;
<pre><code><span epub:type="pagebreak" title="201" id="Page_201"/>year month citrus_export_value twelve_month_avg&#13;
---- ----- ------------------- ----------------&#13;
<var>--snip--</var>&#13;
2019     9            14012305         74465440&#13;
2019    10            26308151         74756757&#13;
2019    11            60885676         74853312&#13;
2019    12            84873954         74871644&#13;
2020     1           110924836         75099275&#13;
2020     2           171767821         78874520&#13;
2020     3           201231998         79593712&#13;
2020     4           122708243         78278945&#13;
2020     5            75644260         77999174&#13;
2020     6            36090558         78045059&#13;
2020     7            20561815         78343206&#13;
2020     8            15510692         78376692</code></pre>&#13;
<p>Notice the 12-month average is far more consistent. If we want to see the trend, it’s helpful to graph the results using Excel or a stats program. <a href="#figure11-3" id="figureanchor11-3">Figure 11-3</a> shows the monthly totals from 2015 through August 2020 in bars, with the 12-month average as a line.</p>&#13;
<figure>&#13;
<img src="Images/f11003.png" alt="f11003" class="" width="692" height="412"/>&#13;
<figcaption><p><a id="figure11-3">Figure 11-3</a>: Monthly citrus fruit exports with 12-month rolling average</p></figcaption>&#13;
</figure>&#13;
<p>Based on the rolling average, citrus fruit exports were generally steady until 2019 and then trended down before recovering slightly in 2020. It’s difficult to discern that movement from the monthly data, but the rolling average makes it apparent.</p>&#13;
<p>The window function syntax offers multiple options for analysis. For example, instead of calculating a rolling average, you could substitute the <code>sum()</code> function to find the rolling total over a time period. If you calculated a seven-day rolling sum, you’d know the weekly total ending on any day in your dataset.</p>&#13;
<aside epub:type="sidebar">&#13;
<div class="top hr"><hr/></div>&#13;
<section class="note">&#13;
<span epub:type="pagebreak" title="202" id="Page_202"/><h2><span class="NoteHead">Note</span></h2>&#13;
<p>	Calculating rolling averages or sums works best when there are no breaks in the time periods in your data. A missing month, for example, will turn a 12-month sum into a 13-month sum because the window function pays attention to rows, not dates.</p>&#13;
<div class="bottom hr"><hr/></div>&#13;
</section>&#13;
</aside>&#13;
<p>SQL offers additional window functions. Check the official PostgreSQL documentation at <a href="https://www.postgresql.org/docs/current/tutorial-window.html" class="LinkURL">https://www.postgresql.org/docs/current/tutorial-window.html</a> for an overview of window functions, and check <a href="https://www.postgresql.org/docs/current/functions-window.html" class="LinkURL">https://www.postgresql.org/docs/current/functions-window.html</a> for a listing of window functions.</p>&#13;
<h2 id="h1-501065c11-0005">Wrapping Up</h2>&#13;
<p class="BodyFirst">Now your SQL analysis toolkit includes ways to find relationships among variables using statistical functions, create rankings from ordered data, smooth spiky data to find trends, and properly compare raw numbers by turning them into rates. That toolkit is starting to look impressive!</p>&#13;
<p>Next, we’ll dive deeper into date and time data, using SQL functions to extract the information we need.</p>&#13;
<aside epub:type="sidebar">&#13;
<div class="top hr"><hr/></div>&#13;
<section class="box">&#13;
<h2>Try It Yourself</h2>&#13;
<p class="BoxBodyFirst">Test your new skills with the following questions:</p>&#13;
<ol>&#13;
<li value="1">In <a href="#listing11-2">Listing 11-2</a>, the correlation coefficient, or <code>r</code> value, of the variables <code>pct_bachelors_higher</code> and <code>median_hh_income</code> was about 0.70. Write a query using the same dataset to show the correlation between <code>pct_masters_higher</code> and <code>median_hh_income</code>. Is the <code>r</code> value higher or lower? What might explain the difference?</li>&#13;
<li value="2">Using the exports data, create a 12-month rolling sum using the values in the column <code>soybeans_export_value</code> and the query pattern from <a href="#listing11-8">Listing 11-8</a>. Copy and paste the results from the pgAdmin output pane and graph the values using Excel. What trend do you see?</li>&#13;
<li value="3">As a bonus challenge, revisit the libraries data in the table <code>pls_fy2018_libraries</code> in <span class="xref" itemid="xref_target_Chapter 9">Chapter 9</span>. Rank library agencies based on the rate of visits per 1,000 population (column <code>popu_lsa</code>), and limit the query to agencies serving 250,000 people or more.</li>&#13;
</ol>&#13;
<div class="bottom hr"><hr/></div>&#13;
</section>&#13;
</aside>&#13;
</section>&#13;
</div></body></html>