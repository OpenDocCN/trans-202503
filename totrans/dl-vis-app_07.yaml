- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Information Theory
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 信息理论
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: In this chapter we look at the basics of *information theory*. This is a relatively
    new field of study, introduced to the world in 1948 in a groundbreaking paper,
    which laid the foundation for technologies from modern computers and satellites
    to cell phones and the internet (Shannon 1948). The goal of the original theory
    was to find the most efficient way to communicate a message electronically. But
    the ideas of that theory are deep, broad, and profound. They give us tools for
    measuring how much we know about anything by converting it to a digital form that
    we can study and manipulate.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨*信息理论*的基础知识。这是一个相对较新的研究领域，于1948年通过一篇开创性的论文向世界介绍，为现代计算机、卫星、手机和互联网等技术奠定了基础（Shannon
    1948）。原始理论的目标是找到最有效的方式来电子化传递信息。但该理论的思想深刻、广泛且深远。它为我们提供了测量我们了解某事物的程度的工具，通过将信息转换为我们可以研究和操作的数字形式。
- en: Terms and ideas from information theory form part of the bedrock of deep learning.
    For example, the measurements provided by information theory are useful when we
    evaluate the performance of deep networks. In this chapter, we take a fast tour
    through some of the basics of information theory, while staying free of abstract
    mathematical notation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 信息理论中的术语和概念构成了深度学习的基石。例如，信息理论提供的度量方法在评估深度网络的性能时非常有用。在本章中，我们将快速浏览信息理论的一些基本概念，同时避免使用抽象的数学符号。
- en: Let’s begin with the word *information*, one of those words that has both an
    everyday meaning and a specialized, scientific meaning. In this case, the meanings
    share a lot conceptual overlap, but while the popular meaning is broad and open
    to personal interpretation, the scientific meaning is precise and defined mathematically.
    Let’s start out by building up to the scientific definition of information, and
    ultimately work our way up to an important measurement that lets us compare two
    probability distributions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从单词*information*（信息）开始，这是一个既有日常意义，又有专业科学意义的词。在这种情况下，这两种意义有很多概念上的重叠，但日常意义广泛且开放，容易受到个人理解的影响，而科学意义则是精确的，并且通过数学定义的。我们从逐步构建信息的科学定义开始，最终我们将上升到一个重要的度量标准，允许我们比较两个概率分布。
- en: Surprise and Context
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 惊讶与语境
- en: When we receive a communication of any kind, something moved from one place
    to another, whether it was an electrical pulse, some photons of light, or the
    sound of someone’s voice. Speaking broadly, we could say that a *sender* somehow
    transfers some kind of communication to a *receiver*. Let’s introduce some more
    specialized vocabulary.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们收到任何形式的通信时，某些东西从一个地方移动到另一个地方，无论是电脉冲、一些光子，还是某人的声音。广义地说，我们可以说一个*发送者*通过某种方式将某种信息传递给*接收者*。接下来，我们将介绍一些更专业的词汇。
- en: Understanding Surprise
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解惊讶
- en: In this chapter, we sometimes use the term *surprise* to represent how unexpected
    a sender’s communication is to a receiver. Surprise isn’t a formal term. In fact,
    one of our goals in this chapter is to find more formal names for surprise and
    attach specific meanings and measures to them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们有时使用*惊讶*一词来表示发送者的通信对接收者来说有多么出乎意料。惊讶并不是一个正式术语。事实上，本章的目标之一是为惊讶找到更正式的名称，并附上具体的含义和度量标准。
- en: Let’s suppose that we’re on the receiving end of a message. We want to describe
    how surprised we are by the communication we receive. Being able to do so is useful
    because, as we’ll see, the greater the surprise, the greater the amount of information
    that was delivered.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们是接收者，收到一条消息。我们想描述收到这条信息后我们有多惊讶。能够这样描述很有用，因为正如我们所看到的，惊讶越大，传递的信息量就越大。
- en: Suppose we get an unexpected text message from an unknown number. We open it
    up and the first word is Thanks. How surprised are we? Surely we are at least
    a little surprised, because so far, we don’t know who the message is from or what
    it’s about. But receiving a text thanking us for something does happen, so it’s
    not unheard of.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们收到了一条来自未知号码的意外短信。我们打开短信，第一句话是“Thanks”（谢谢）。我们有多惊讶呢？肯定至少有些惊讶，因为到目前为止，我们不知道这条短信是来自谁，也不知道它的内容是什么。不过，收到感谢我们的短信确实是有可能的，所以这并不是什么闻所未闻的事。
- en: Let’s make up an imaginary and completely subjective surprise scale, where 0
    means something is completely expected, and 100 means it’s a total surprise, as
    in [Figure 6-1](#figure6-1).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来设定一个假想的、完全主观的惊讶量表，其中0表示完全预期的事件，100表示完全出乎意料的事件，就像在[图6-1](#figure6-1)中所示。
- en: '![f06001](Images/f06001.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![f06001](Images/f06001.png)'
- en: 'Figure 6-1: The surprise scale, expressed as a value from 0 to 100'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-1：惊讶度量表，表示从0到100的值
- en: On this scale, the word Thanks at the start of an unexpected text message might
    rank a 20\. Now suppose that the first word in our message isn’t Thanks, but instead
    is Hippopotamus. Unless we’re working with those animals or are otherwise involved
    with them, that’s likely to be a rather surprising first word of a message. Let’s
    rank this word at an 80 on the surprise scale, as in [Figure 6-2](#figure6-2).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个量表上，"Thanks"（谢谢）出现在一条意外的文本消息开头时，可能排在20的位置。现在假设我们消息的第一个词不是"Thanks"，而是"Hippopotamus"（河马）。除非我们在处理这些动物或与它们有其他关联，否则这可能是消息中的一个相当令人惊讶的词。我们将这个词的惊讶度定为80，正如图[6-2](#figure6-2)所示。
- en: '![f06002](Images/f06002.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![f06002](Images/f06002.png)'
- en: 'Figure 6-2: Placing messages on our surprise scale'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-2：将消息放置在我们的惊讶度量表上
- en: Although hippopotamus might be a big surprise at the start of a message, it
    might not be surprising later on. The difference is context.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管"hippopotamus"（河马）可能在消息开头给人一种很大的惊讶感，但在后续的语境中它可能就不那么令人惊讶了。区别就在于语境。
- en: Unpacking Context
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拆解语境
- en: For our purposes, we can think of *context* as the environment of the message.
    Since we’re focusing on the meaning of each message, rather than the physical
    way it’s communicated, the context represents the shared knowledge between the
    sender and receiver, which gives the message meaning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的来说，我们可以将*语境*视为信息的环境。由于我们关注的是每条消息的意义，而不是它的传递方式，语境代表了发送者和接收者之间的共享知识，正是这些知识赋予了消息意义。
- en: When the message is a piece of language, this shared knowledge must include
    the words used, since a message of Kxnfq rnggw would carry no meaning. We can
    extend that shared knowledge to include grammar, current interpretations of emoticons
    and abbreviations, shared cultural influences, and so on. This is all called *global
    context*. It’s the general knowledge that we bring to any message, even before
    we’ve read it. In terms of our Bayes’ Rule discussion of Chapter 4, some of this
    global context is captured in our *prior*, since that is how we represent our
    understanding of the environment and what we expect to learn from it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当信息是一段语言时，这种共享知识必须包括所使用的词汇，因为像Kxnfq rnggw这样的消息是没有意义的。我们可以将这种共享知识扩展到包括语法、表情符号和缩写的当前解释、共享的文化影响等等。这一切都被称为*全球语境*。它是我们在接收任何信息之前就已经带入的普遍知识。就像在第4章讨论的贝叶斯定理一样，部分全球语境被捕捉到我们的*先验*中，因为这就是我们表示对环境的理解以及我们期望从中学到什么的方式。
- en: In contrast to the global context, there is also *local context*. That’s the
    environment composed of the elements of the message itself. In a text message,
    the local context for any given word is the other words in that message.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与全球语境相对的是*局部语境*。那是由消息本身的元素构成的环境。在一条文本消息中，任何给定单词的局部语境就是该消息中的其他单词。
- en: Let’s imagine that we’re reading a message for the first time, so each word’s
    local context is made up only of the words that preceded it. We can use the context
    to get a handle on surprise. If Hippopotamus is the first word of our message,
    then there is no local context yet, only the global. And if we don’t work with
    hippopotamuses on a regular basis, that word is likely very surprising. But if
    the message begins with, Let’s go down to the river area at the zoo and maybe
    see a big gray, then in that context, the word hippopotamus isn’t very surprising.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们第一次阅读这条消息，因此每个单词的局部语境仅由它前面的单词构成。我们可以利用语境来判断惊讶感。如果"Hippopotamus"是我们消息中的第一个词，那么此时没有局部语境，只有全球语境。如果我们不经常接触河马，这个词可能非常令人惊讶。但是，如果消息的开头是"Let's
    go down to the river area at the zoo and maybe see a big gray"（让我们去动物园的河边，或许能看到一只大灰色的动物），那么在这个语境下，"hippopotamus"（河马）就不再令人惊讶了。
- en: We can describe the amount of surprise carried by a specific word in our global
    context by assigning it a surprise value, as we did in [Figure 6-1](#figure6-1).
    Suppose that we assign a surprise value to every word in the dictionary (a tedious
    job, but certainly possible). If we scale these numbers so that they all add up
    to 1, we’ve created a probability mass function (or pmf), as we discussed in Chapter
    2\. That means we can draw a random variable from that pmf to get a word, with
    the most surprising words coming along more frequently than the less surprising
    words. A more common approach is to set up the pmf to represent how common a word
    is, which is roughly the opposite of surprise. With that setup, we’d expect to
    draw the least surprising, or more common, words more frequently than uncommon
    words.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过为特定单词分配一个惊讶值，来描述在全球语境中该单词所携带的惊讶感，正如我们在[图6-1](#figure6-1)中所做的那样。假设我们为字典中的每个单词分配一个惊讶值（这是一项繁琐的工作，但肯定是可行的）。如果我们将这些数字缩放，使它们的总和为1，就创建了一个概率质量函数（或pmf），正如我们在第二章中讨论的那样。这意味着我们可以从这个pmf中抽取一个随机变量来得到一个单词，其中最令人惊讶的单词比那些不太令人惊讶的单词更常出现。一种更常见的方法是建立一个pmf来表示单词的常见程度，这大致是惊讶感的对立面。在这种设置下，我们预计会更频繁地抽到那些不太令人惊讶或更常见的单词，而不是那些不常见的单词。
- en: We’ll use this idea later in the chapter to devise a scheme for transmitting
    the content of a message in an efficient manner.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面使用这个思想，设计一种高效的方案来传输信息的内容。
- en: Measuring Information
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量信息
- en: In this chapter, we’re going to talk quite a lot about *bits*. In popular language,
    a bit is usually thought of as a little package of data, often labeled either
    0 or 1\. For instance, when we talk about internet speed in “bits per second,”
    we might picture the bits as leaves flowing down a river, and we count them as
    they go by.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将谈论很多关于*比特*的内容。在日常语言中，比特通常被认为是一个小的数据包，通常被标记为0或1。例如，当我们谈论“每秒比特数”的网络速度时，我们可能会把比特想象成河流中漂流的叶子，并在它们经过时进行计数。
- en: This is a convenient idea, but in technical language, a bit is not a thing,
    like a leaf, but a unit, like a gallon or a gram. That is, it isn’t a piece of
    stuff but a way to talk about how much stuff we have. A bit is a container that
    holds just enough storage for what we currently think is the fundamental, indivisible,
    smallest possible chunk of information.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个方便的概念，但从技术角度来说，比特并不是像树叶一样的物体，而是像加仑或克那样的单位。也就是说，它不是一种物质，而是一种衡量我们所拥有的物质多少的方式。比特是一个容器，刚好能够存储我们当前认为的基本、不可分割的最小信息单元。
- en: Speaking of bits as units in this way is technically correct, but it’s inconvenient.
    And most of the time, we can speak casually without any confusion, like when we
    say, “My net connection is 8,000 bits per second,” rather than, “My net connection
    is able to transmit 8,000 bits worth of information per second.” We’ll use the
    more casual language in most of this book, but it’s worthwhile to know the technical
    definition, because it does pop up from time to time in papers and documentation
    where the distinction is important.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从这种方式谈论比特作为单位在技术上是正确的，但这并不方便。大多数时候，我们可以不加澄清地使用通俗的说法，就像我们说“我的网速是每秒8,000比特”而不是“我的网连接每秒能够传输8,000比特的信息”。我们在本书的大部分内容中将使用更通俗的语言，但了解技术定义是值得的，因为它确实会出现在一些论文和文档中，而在那些地方区分开来是很重要的。
- en: We can measure the amount of information in a text message with a formula that
    tells us how many bits are needed to represent that message. We won’t get into
    the math, but we’ll describe what’s going on. The formula takes two inputs. The
    first is the text of the message. The second is a pmf that describes the surprise
    inherent in each word the message can contain (let’s just call this a *probability
    distribution* for the rest of this chapter). When we take the text of the message
    and the probability distribution together, we can produce a number that tells
    us how many bits of information the message carries.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个公式来衡量文本信息的量，这个公式告诉我们需要多少比特来表示该信息。我们不会深入探讨数学内容，但会描述其中的原理。这个公式有两个输入。第一个是信息的文本内容。第二个是描述信息中每个单词所包含的惊讶感的概率质量函数（在本章中，我们将其称为*概率分布*）。当我们将信息的文本内容和概率分布结合起来时，就可以得出一个数字，告诉我们这条信息所携带的比特数。
- en: The formula was designed so that the values it produces for each word (or, more
    generally, each *event*) have four key properties. We’ll illustrate each one using
    a context in which we work in an office, and not on a river.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式的设计是为了使它为每个单词（或者更一般地，每个*事件*）所生成的值具备四个关键属性。我们将在一个我们在办公室工作而不是在河流上的情境中说明每一个属性。
- en: Likely events have low information. Stapler has low information.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能发生的事件信息量较少。订书机的信息量较少。
- en: Unlikely events have high information. Crocodile has high information.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不太可能发生的事件包含更多信息。鳄鱼包含更多信息。
- en: Likely events have less information than unlikely events. Stapler conveys less
    information than crocodile.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能发生的事件比不太可能发生的事件包含的信息更少。订书机传递的信息比鳄鱼少。
- en: Finally, the total information due to two *unrelated* events is the sum of their
    individual information values found separately.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，两个*不相关*事件的总信息量是它们各自信息值的和。
- en: The first three properties relate single objects to their information. The oddball
    in the group is property 4, so let’s look at it more carefully.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 前三个属性将单个对象与其信息相关联。这个组中的特例是属性 4，所以我们需要更仔细地看一下它。
- en: In normal conversation, it’s rare for two consecutive words to be completely
    unrelated. But suppose someone asked us for a “kumquat daffodil.” Those words
    are just about completely unrelated, so property 4 says that we could find the
    information in that phrase by adding the information communicated by each word
    independently.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常的对话中，两个连续的词完全不相关是很少见的。但假设有人要求我们给出一个“金桔水仙花”。这些词几乎完全不相关，因此属性 4 说明我们可以通过将每个词独立传递的信息相加来找到该短语的信息。
- en: In normal conversation, the words that lead up to any given word often narrow
    the possibilities of what it could be. If someone says, “Today I ate a big,” then
    words like “sandwich” and “pizza” arriving next carry less surprise than “bathtub”
    or “sailboat.” When words are expected, they produce less surprise than when they’re
    not. By contrast, suppose we’re sending a device’s serial number, which is essentially
    an arbitrary sequence of letters and perhaps numbers, like “C02NV91EFY14.” If
    the characters really have no relation to each other, then adding the surprise
    due to each character gives us the overall surprise in the entire message representing
    the serial number.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常的对话中，引导到某个给定单词的词汇通常会缩小它可能是什么的范围。如果有人说：“今天我吃了一个大...”，那么接下来的“三明治”和“比萨”这类词比“浴缸”或“帆船”产生的惊讶感要少。当某些词被预期时，它们比没有预期时产生的惊讶感要少。相比之下，假设我们正在传送一个设备的序列号，它本质上是一个任意的字母序列，可能还包含数字，比如“C02NV91EFY14”。如果这些字符之间真的没有关联，那么通过加上每个字符所带来的惊讶感，我们就能得到整个序列号所代表的信息。
- en: 'By combining the surprise of two unrelated words into the sum of their individual
    surprise values, we go from measuring the surprise, or information, in each of
    those words to the surprise in their combination. We can keep combining words
    this way into ever-larger groups until we’ve considered the entire message. Though
    we haven’t gone into the math, we have reached a formal definition of *information*:
    it’s a number produced from a formula that uses one or more events (such as words),
    and a probability distribution to describe how surprising each event would be
    to us. From those two inputs the algorithm provides a number for each event, and
    guarantees that those numbers satisfy the four properties we just listed. We call
    each word’s number its *entropy*, telling us how many bits are needed to communicate
    it.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将两个不相关词汇的惊讶感合并为它们各自惊讶值的和，我们从衡量每个词汇中的惊讶感或信息，转向衡量它们组合中的惊讶感。我们可以这样不断将词汇组合成越来越大的组，直到我们考虑完整个信息。虽然我们没有涉及数学，但我们已经得出了*信息*的正式定义：它是一个通过公式生成的数字，该公式使用一个或多个事件（如单词）和一个概率分布，来描述每个事件对我们来说有多么令人惊讶。通过这两个输入，算法为每个事件提供一个数字，并确保这些数字满足我们刚才列出的四个属性。我们称每个单词的数字为其*熵*，它告诉我们传递该单词需要多少比特。
- en: Adaptive Codes
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自适应编码
- en: The amount of information carried by each event is influenced by the size of
    the probability function we hand to our formula. In other words, the number of
    possible words we might communicate affects the amount of information carried
    by each word we send.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 每个事件所携带的信息量受我们公式中所使用的概率函数大小的影响。换句话说，我们可能传递的词汇数量会影响每个词所携带的信息量。
- en: Suppose we want to transmit the contents of a book from one place to another.
    We might list all the unique words in that book and then assign a number to each
    word, starting perhaps with 0 for the, then 1 for and, and so on. Then, if our
    recipient also has a copy of that word list, we can send the book just by sending
    the number for each word, starting with the first word in the book. The Dr. Seuss
    book *Green Eggs and Ham* contains only 50 different words (Seuss 1960). To represent
    a number between 0 and 49, we need six bits of information per word. By contrast,
    Robert Louis Stevenson’s book *Treasure Island* contains about 10,700 unique words
    (Stevenson 1883). We’d have to use 14 bits per word to uniquely identify each
    word in that book.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Although we could use one giant word list of all English words to send these
    books, it’s more efficient to tailor our list to each book’s individual vocabulary,
    including only the words we actually need. In other words, we can improve our
    efficiency by *adapting* our transmission of information to what’s being communicated.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take that idea and run with it.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Speaking Morse
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A great example of adaptation is Morse code. In Morse code, each typographical
    character has an associated pattern of dots and dashes, separated by spaces, as
    shown in [Figure 6-3](#figure6-3).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![f06003](Images/f06003.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-3: Each character in Morse code has an associated pattern of dots,
    dashes, and spaces.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Morse code is traditionally sent by using a telegraph key to enable or disable
    transmission of a clear tone. A dot is a short burst of sound. The length of time
    we hold down the key to send a dot is represented by a unit called the *dit*.
    A dash is held for the duration of three dits. We leave one dit of silence between
    symbols, a silence of three dits between letters, and a silence of seven dits
    between words. These are of course ideal measures. In practice, many people can
    recognize the personal rhythm, called the *fist*, of each of their friends and
    colleagues (Longden 1987).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'Morse code contains three types of symbols: dots, dashes, and dot-sized spaces.
    Let’s suppose we want to send the message “nice dog” in Morse code. [Figure 6-4](#figure6-4)
    shows the sequence of short tones (dots), long tones (dashes), and dot-sized spaces.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![f06004](Images/f06004.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-4: The three symbols of Morse code: dots (solid circles), dashes (solid
    boxes), and silent spaces (empty circles).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: We typically talk about Morse code strictly in terms of dots and dashes, which
    are called the *symbols*. The assigned set of symbols for any letter is that letter’s
    *pattern*. The length of time it takes to send a message depends on the specific
    patterns assigned to the letters that make up the message’s content. For example,
    even though the letters Q and H both have four symbols, Q requires 13 dits to
    send (3 for each of the 3 dashes, 1 for the dot, and 1 for each of the 3 spaces),
    while we need only 7 dits to send the letter H (4 dots, and 1 for each of the
    3 spaces).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常严格地用点和划来讨论摩尔斯电码，这些被称为*符号*。分配给任何字母的符号集就是该字母的*模式*。发送信息所需的时间取决于组成信息内容的字母所分配的具体模式。例如，尽管字母Q和H都有四个符号，但发送Q需要13个点（每个3个划的3个点，1个点，和每个3个空格的1个点），而发送字母H只需要7个点（4个点，和每个3个空格的1个点）。
- en: Let’s compare the patterns of the different characters. When we look at [Figure
    6-3](#figure6-3), it might not be clear to us that there’s any principle behind
    how the various patterns are assigned. But a beautiful idea is there waiting to
    be uncovered. [Figure 6-5](#figure6-5) shows a list of the 26 Roman letters, sorted
    by their typical frequency in English (Wikipedia 2020). The most frequently used
    letter, E, leads the list.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较不同字符的模式。当我们查看[图6-3](#figure6-3)时，我们可能不清楚这些不同模式分配背后是否有任何原则。但一个美丽的思想正在等待被揭示。[图6-5](#figure6-5)显示了26个罗马字母的列表，按它们在英语中的典型使用频率排序（Wikipedia
    2020）。最常用的字母E排在列表的最前面。
- en: '![f06005](Images/f06005.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![f06005](Images/f06005.png)'
- en: 'Figure 6-5: The Roman letters sorted by their frequency of use in English'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-5：按在英语中使用频率排序的罗马字母
- en: 'Now look back at the patterns in [Figure 6-3](#figure6-3). The most frequent
    letter, E, is just a single dot. The next most frequent letter, T, is just a single
    dash. Those are the only two possible patterns with just one symbol, so now we
    move on to two symbols. The letter A is a dot followed by a dash. O is next, and
    it breaks the pattern because it’s too long: three dashes. Let’s come back to
    that later. Returning to our list, the I is two dots, the N is a dash and a dot.
    The last two-letter pattern is M, with two dashes, but that’s pretty far down
    the list from where we’ve gotten so far. Why is O too long and M too short? Morse
    code is almost following our letter-frequency table, but not quite.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回头看看[图6-3](#figure6-3)中的模式。最常见的字母E只是一个点。下一个最常见的字母T只是一个划。这是唯一两个只有一个符号的模式，因此我们继续往下看两个符号。字母A是一个点后跟一个划。接下来是O，它打破了这个模式，因为它太长：三个划。稍后再回来讨论。返回我们的列表，字母I是两个点，字母N是一个划和一个点。最后的两个字母模式是M，由两个划组成，但它在我们到达的位置上已经很远了。为什么O太长而M太短？摩尔斯电码几乎遵循我们的字母频率表，但还不完全是。
- en: The explanation starts with Samuel Morse, who only defined patterns for the
    numbers 0 through 9 in his original code. Letters and punctuation were added to
    the code by Alfred Vail, who designed those patterns in about 1844 (Bellizzi 11).
    Vail didn’t have an easy way to look up letter frequencies, but he knew he should
    follow them, according to Vail’s assistant, William Baxter. Baxter said,
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 解释从塞缪尔·摩尔斯开始，他在原始代码中仅定义了数字0到9的模式。字母和标点符号是由阿尔弗雷德·维尔添加到代码中的，他大约在1844年设计了这些模式（Bellizzi
    11）。维尔没有一个简单的方法来查找字母的出现频率，但根据维尔的助手威廉·巴克斯特的说法，他知道自己应该遵循这些频率。巴克斯特说，
- en: His general plan was to employ the simplest and shortest combinations to represent
    the most frequently recurring letters of the English alphabet, and the remainder
    for the more infrequent ones. For instance, he found upon investigation that the
    letter e occurs much more frequently than any other letter, and accordingly he
    assigned to it the shortest symbol, a single dot (•). On the other hand, j, which
    occurs infrequently, is expressed by dash-dot-dash-dot (– • – •) (Pope 1887)^([1](#c06-footnote-1))
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 他的总体计划是采用最简单和最短的组合来表示最常出现的英语字母，将其余的字母分配给较不常见的字母。例如，他在调查后发现字母e的出现频率远高于其他字母，因此他为其分配了最短的符号，一个点（•）。另一方面，j的出现频率较低，用划-点-划-点（–
    • – •）表示（Pope 1887）^([1](#c06-footnote-1))
- en: Vail figured that he could estimate the letter frequency table for English text
    by visiting his local newspaper in Morristown, New Jersey, where they were still
    setting stories by hand. In those days, typesetters built up a page one letter
    at a time. For each letter, they would choose an appropriate *slug*, or a metal
    bar with a letter embossed on one end, and place it into a large tray. Vail reasoned
    that the most popular characters would have the greatest number of slugs on hand,
    so he counted up the number of slugs in each letter’s bin. Those popularity counts
    were his proxy for letter frequency in English (McEwen 1997). Given how small
    this sample was, he did a pretty great job, despite imperfections like apparently
    thinking that M was more frequent than O.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Vail认为，他可以通过访问新泽西州莫里斯敦的本地报社来估算英文文本中的字母频率表，当时他们仍在手动排版。那时，排版工人是逐个字母地构建页面。对于每个字母，他们会选择一个合适的*铅字*，即一根金属条的一端上压印有字母，然后将其放入一个大托盘中。Vail推测，最常用的字符会有最多的铅字，因此他数了每个字母托盘中的铅字数量。这些流行度计数成为了他估算英语字母频率的替代指标（McEwen
    1997）。尽管这个样本很小，他还是做得相当好，尽管存在一些不准确的地方，比如他显然认为M比O更常见。
- en: To see how well our frequency chart (and Morse code) lines up with some actual
    text, [Figure 6-6](#figure6-6) shows the frequencies for the letters from *Treasure
    Island* (Stevenson 1883). For this chart, we counted only the letters, which we
    turned into lowercase before counting. We also excluded numbers, spaces, and punctuation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看我们的频率图（和摩尔斯电码）与一些实际文本的对比，[图6-6](#figure6-6)展示了**《宝岛》**中各个字母的频率（史蒂文森 1883）。在这张图中，我们只统计了字母，并在计数前将它们转换为小写字母。我们还排除了数字、空格和标点符号。
- en: '![f06006](Images/f06006.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![f06006](Images/f06006.png)'
- en: 'Figure 6-6: The number of times each letter appears in *Treasure Island* by
    Robert Louis Stevenson. Uppercase letters were counted as lowercase.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-6：每个字母在**罗伯特·路易斯·史蒂文森**的《**宝岛**》中出现的次数。大写字母被当作小写字母计算。
- en: The order of the characters in [Figure 6-6](#figure6-6) isn’t a perfect match
    to our letter frequency chart in [Figure 6-5](#figure6-5), but it’s close. [Figure
    6-6](#figure6-6) looks like a probability distribution over the letters A through
    Z. To make it an *actual* probability distribution, we have to scale it so that
    the sum of all the entries is 1\. The result is shown in [Figure 6-7](#figure6-7).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-6](#figure6-6)中的字符顺序与[图6-5](#figure6-5)中的字母频率图不完全匹配，但差距不大。[图6-6](#figure6-6)看起来像是从A到Z字母的概率分布。为了使其成为一个*实际的*概率分布，我们必须对其进行缩放，使得所有条目的总和为1。结果显示在[图6-7](#figure6-7)中。'
- en: '![f06007](Images/f06007.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![f06007](Images/f06007.png)'
- en: 'Figure 6-7: The probability distribution function (pdf) for characters in *Treasure
    Island*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-7：**《宝岛》**中字符的概率分布函数(pdf)
- en: Now let’s use our probability distribution of letters to improve the efficiency
    of sending *Treasure Island* via Morse code.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们利用字母的概率分布，来提高通过摩尔斯电码发送**《宝岛》**的效率。
- en: Customizing Morse Code
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义摩尔斯电码
- en: To motivate our improvements to sending *Treasure Island* via Morse code, let’s
    first take a step backward, and start with an imaginary version of Morse code
    where Mr. Vail didn’t bother to journey down to the newspaper office. Instead,
    let’s say he wanted to assign the same number of dot-and-dash symbols to each
    character. With four symbols he could only label 16 characters, but with five
    symbols he could label 32 characters.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了激励我们改进通过摩尔斯电码发送**《宝岛》**的方式，首先让我们退后一步，开始思考一个假设的摩尔斯电码版本，在这个版本中，Vail先生并没有下到报社去。而是，假设他想为每个字符分配相同数量的点和划符号。使用四个符号，他只能标记16个字符，但使用五个符号，他可以标记32个字符。
- en: '[Figure 6-8](#figure6-8) shows how we might arbitrarily assign such a five-symbol
    pattern to each character. To keep things simple, we made the timing of every
    dot and dash the same by using different tones for the two symbols. So every dot
    (shown here as a black dot) is a high tone lasting one dit, and every dash (shown
    as a red square) is a low tone lasting one dit. The result is that every character
    takes nine dits of time to send (five for the dots and dashes, now high and low
    tones, and four for the silences between them). This is an example of a *constant-length
    code*, also called *a fixed-length code*.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-8](#figure6-8)展示了我们如何任意地为每个字符分配这样的五符号模式。为了简化起见，我们通过使用不同的音调来表示两个符号，使得每个点和划的时间都相同。所以每个点（这里显示为黑色圆点）是一个持续一拍的高音，而每个划（显示为红色方块）是一个持续一拍的低音。结果是，每个字符需要发送九拍的时间（五拍用于点和划，现在分别是高音和低音，四拍是它们之间的间隔）。这是一个*定长编码*的例子，也叫做*固定长度编码*。'
- en: In [Figure 6-8](#figure6-8) we didn’t create a character for the space, following
    in the footsteps of the original Morse code, where it was assumed that we could
    figure out where the spaces ought to go by looking at the message. Sticking to
    that spirit, we’ll ignore space characters for the rest of this discussion.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![f06008](Images/f06008.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-8: Assigning five symbols to each character gives us a constant-length
    code. Black circles are high tones; red squares are low tones. They all last one
    dit.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: The first two words in the text of *Treasure Island* are the name “Squire Trelawney.”
    Since every character in our two-tone version of Morse code requires 9 dits, this
    phrase of 15 letters (remember that we’re ignoring the space) requires 9 × 15
    = 135 dits of time to send. Adding in the 14 silences between letters, which take
    3 × 14 = 42 bits, we find the fixed-length message takes 135 + 42 = 177 dits of
    time, as shown in [Figure 6-9](#figure6-9).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![f06009](Images/f06009.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-9: The first two words of *Treasure Island*, using our constant-length
    code'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Now compare this to actual Morse code where, for the most part, the most common
    letters have fewer symbols than the uncommon letters. [Figure 6-10](#figure6-10)
    shows this. We’ll continue sending dots and dashes using different tones that
    last one dit each.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![f06010](Images/f06010.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-10: The first two words of *Treasure Island*, using Morse code'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: If we count up the elements (remembering that dots and dashes now take just
    one dit each), we find that the [Figure 6-10](#figure6-10) version requires 101
    dits of time, about half as long as the fixed-length code (101 / 177 ≈ 0.57).
    That savings comes from adapting our code to the content we are sending. We call
    any code that tries to improve efficiency by matching up short patterns with high-probability
    events a *variable-bitrate code*, or more simply, an *adaptive code*. Even in
    this simple example, our adaptive code is almost twice as efficient as the constant-length
    code, cutting our communication time nearly in half.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the whole text of *Treasure Island*, which contains about 338,000
    characters (excluding spaces, punctuation, etc.). The adaptive code would take
    only about 42 percent of the time required by the fixed-length code. We can send
    the book in less than half the time required by a nonadaptive code.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: We can do even better if, instead of using standard Morse code, which is adapted
    to English writing in general, we tune the distribution of symbols to more closely
    match their actual percentages in the text of the specific book we’re sending.
    Of course, we’d have to share our clever encoding with our recipient, but if we’re
    sending a long message, that extra piece of communication is dwarfed by the message
    itself. Let’s take that step, and imagine a custom Treasure Island code that is
    perfectly adapted to the contents of *Treasure Island* specifically. We should
    expect even more savings.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Let’s rephrase this using the language of probability. An adaptive code creates
    a pattern for each value in a probability distribution. The value with the highest
    probability receives the shortest possible code. Then we work our way through
    the values, from the highest probability to the lowest, assigning patterns that
    are always as short as possible without repeating. That means each new pattern
    is as long as, or longer than, the pattern assigned to the previous value. That’s
    just what Mr. Vail did in 1844, guided by the number of letters he found in the
    typesetter’s bins of his local newspaper.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Now we can look at any message we want to communicate, identify each character,
    and compare it to the probability distribution that tells us how likely that character
    was in the first place. This tells us how much information, in bits, is carried
    by that character. Thanks to the fourth property in our description of the formula
    for computing information, the total number of bits required to represent the
    message (ignoring context for the moment), is just the sum of the individual numbers
    of bits required by each character.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: We can also perform this process for our message before we send it. That tells
    us just how much information we’re about to communicate to our recipient.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Entropy
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve discussed *surprise*, which refers to things we didn’t expect. A related
    idea is *uncertainty*, which refers to those times when we know all the things
    that might happen, but we’re not sure which one will actually occur. For instance,
    when we roll a fair six-sided die, we know that each of the six faces has an equal
    probability of coming up, but until we roll it and look, we are uncertain which
    face will be on top. A more formal term for this uncertainty is *entropy*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: We can assign a number to the uncertainty, or entropy, of an outcome. This number
    often depends on how many outcomes are possible. For example, flipping a coin
    can have only 2 outcomes, but rolling a six-sided die can have 6 outcomes, and
    picking a letter from the alphabet can have 26 outcomes. The uncertainty of these
    three results, or their entropy, is a number that increases in size, from the
    coin to the die to the alphabet, because the number of outcomes is increasing
    in each case. That makes each specific result more uncertain.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: In those three examples, the probability of each outcome is the same (1/2 for
    each side of the coin, 1/6 for each die face, and 1/26 for each letter). But what
    if the probabilities of the outcomes are different? The formula for computing
    the entropy explicitly takes these different probabilities into account. Essentially,
    it considers all the possible outcomes in a distribution and puts a number to
    the uncertainty describing which outcome is actually produced when we sample the
    distribution.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that the uncertainty of a specific event occurring is the same
    as the number of bits required to send a message with a perfectly adapted code.
    Conceptually, a text message is a set of words drawn from a vocabulary, which
    is no different than the values of a die rolled multiple times. We use the term
    *entropy* for both values: the uncertainty of an event, or the number of bits
    required to communicate that event (and remove the uncertainty).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，特定事件发生的不确定性与用一个完全适应的编码发送消息所需的比特数相同。从概念上讲，一条文本消息是从词汇表中抽取的一组单词，这与多次掷骰子的值没有什么不同。我们使用术语*熵*来表示这两种情况：事件的不确定性，或者传递该事件所需的比特数（从而消除不确定性）。
- en: Entropy is useful in machine learning because it lets us compare two probability
    distributions. This is a key step in learning. For example, consider a classifier.
    We might have a picture that we have manually decided is 80 percent likely to
    be a dog, but 10 percent likely to be a wolf, 3 percent likely to be a fox, and
    a few other smaller probabilities for other animals. We’d like the system’s predictions
    to match those labels. In other words, we want to compare our manual distribution
    with the system’s predicted distribution and use any differences to improve our
    system. We can invent lots of ways to compare distributions, but the one that
    works best in both theory and practice is based on entropy. Let’s build our way
    up to that comparison, starting with finding the entropy for a single distribution.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 熵在机器学习中非常有用，因为它允许我们比较两个概率分布。这是学习中的关键步骤。例如，考虑一个分类器。我们可能有一张图片，我们手动判定它有80%的可能性是狗，但有10%的可能性是狼，3%的可能性是狐狸，还有其他一些动物的较小概率。我们希望系统的预测与这些标签相匹配。换句话说，我们希望比较我们的手动分布与系统的预测分布，并利用任何差异来改进我们的系统。我们可以发明许多比较分布的方法，但在理论和实践中最有效的方法是基于熵的。让我们从求一个单一分布的熵开始，逐步建立起这种比较方法。
- en: Consider a distribution made up of words. If only one word is in our distribution,
    there is no uncertainty about what word we’ll get when we sample the distribution,
    and therefore the entropy is 0\. If there are lots of words but they all have
    a probability of 0, except for a single word with a probability of 1, there’s
    still no uncertainty, so the entropy is again 0\. When all of the words have the
    same probabilities, we have the maximum uncertainty, since no choice is any more
    probable than any other. In this case, our uncertainty, or entropy, is at a maximum.
    Though it might be convenient to say that maximum entropy should be 1, or 100,
    the actual value is calculated by the formula. What we do know is that no other
    probability distribution will give us a larger entropy.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑由单词组成的分布。如果我们的分布中只有一个单词，那么当我们从分布中抽取单词时就没有不确定性，因此熵为0。如果有很多单词，但它们的概率都是0，除了一个单词的概率是1，那么仍然没有不确定性，所以熵再次为0。当所有单词的概率相同，我们就有了最大的
    uncertainty，因为没有任何选择比其他选择更可能。在这种情况下，我们的不确定性或熵达到了最大值。虽然可以方便地说最大熵应该是1或100，但实际值是通过公式计算的。我们所知道的是，其他任何概率分布都不会给我们更大的熵。
- en: In the next section, we’ll see how to apply entropy to pairs of distributions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将看到如何将熵应用于分布对的比较。
- en: Cross Entropy
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉熵
- en: When we’re training a deep learning system, we’ll often want to have a measure
    that tells us to what degree two probability distributions are the same or different.
    The value we usually use is a quantity called the *cross entropy*, and it too
    is just a number. Recall that the entropy tells us how many bits we need to send
    a message using a code that is perfectly tuned to that message. The cross entropy
    tells us how many bits we need if we use some other, less perfect code. Generally,
    this is larger than the number of bits the perfect code needs (if the alternative
    code happens to be exactly as efficient as the ideal code, the cross entropy has
    its minimum value of 0). The cross entropy is a measurement that lets us compare
    two probability distributions numerically. Identical distributions have a cross
    entropy of 0, while increasingly different pairs of distributions have increasingly
    larger values of cross entropy.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度学习系统时，我们通常希望有一个度量标准，告诉我们两个概率分布在多大程度上相同或不同。我们通常使用的值是一种叫做*交叉熵*的量，它也只是一个数字。回想一下，熵告诉我们，使用一个完美匹配消息的编码需要多少位。交叉熵则告诉我们，如果我们使用某种其他的、不太完美的编码，我们需要多少位。通常情况下，交叉熵比完美编码所需的位数要大（如果替代编码恰好与理想编码一样高效，则交叉熵的最小值为
    0）。交叉熵是一个让我们数值化比较两个概率分布的测量工具。相同的分布具有 0 的交叉熵，而越来越不同的分布对的交叉熵值则会越来越大。
- en: To get a feeling for the idea, let’s look at two novels, and build up a word-based
    adaptive code for each. Though our goal is to compare probability distributions,
    and we’re here talking about codes, it’s conceptually easy to go back and forth.
    Recall that by construction, smaller codes correspond to words with higher probabilities,
    while larger codes correspond to words with lower probabilities.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个概念，让我们看两本小说，并为每本小说建立一个基于单词的自适应编码。虽然我们的目标是比较概率分布，而且我们现在讨论的是编码，但在概念上很容易来回切换。回想一下，通过构造，较小的编码对应于概率较高的单词，而较大的编码对应于概率较低的单词。
- en: Two Adaptive Codes
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 两种自适应编码
- en: The novels *Treasure Island* and *The Adventures of Huckleberry Finn*, by Mark
    Twain, were both written in English at about the same time (Stevenson 1883; Twain
    1885). *Treasure Island* has the larger vocabulary, using about 10,700 unique
    words, compared to about 7,400 unique words in *Huckleberry Finn*. Of course,
    they use very different sets of words, but there’s lots of overlap. Let’s look
    at the 25 most popular words in *Treasure Island*, shown in [Figure 6-11](#figure6-11).
    For the purposes of counting words, we first converted all uppercase letters to
    lowercase. The single-letter pronoun “I” therefore appears in the charts as the
    lower-case “i.”
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 马克·吐温的小说《金银岛》和《哈克贝里·费恩历险记》几乎在同一时期用英语写成（斯蒂文森 1883年；吐温 1885年）。《金银岛》的词汇量更大，使用了大约
    10,700 个独特的单词，而《哈克贝里·费恩历险记》则用了大约 7,400 个独特的单词。当然，这两本书使用的单词集完全不同，但也有很多重叠。让我们来看一下《金银岛》中最流行的
    25 个单词，如[图 6-11](#figure6-11)所示。在计算单词时，我们首先将所有的大写字母转换为小写字母。因此，单字母代词“I”在图表中显示为小写字母“i”。
- en: '![f06011](Images/f06011.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![f06011](Images/f06011.png)'
- en: 'Figure 6-11: The 25 most popular words in *Treasure Island*, sorted by number
    of appearances'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6-11：按出现频率排序的《金银岛》中最流行的 25 个单词
- en: Let’s compare these to the 25 most popular words in *Huckleberry Finn*, shown
    in [Figure 6-12](#figure6-12).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些与《哈克贝里·费恩历险记》中最流行的 25 个单词进行比较，如[图 6-12](#figure6-12)所示。
- en: Perhaps unsurprisingly, the most popular dozen words in both books are almost
    the same (though in different orders), but then things begin to diverge.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 也许并不令人惊讶，两本书中最流行的十二个单词几乎完全相同（尽管顺序不同），但随后就开始有所不同。
- en: '![f06012](Images/f06012.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![f06012](Images/f06012.png)'
- en: 'Figure 6-12: The 25 most popular words in *Huckleberry Finn*, sorted by number
    of appearances'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6-12：按出现频率排序的《哈克贝里·费恩历险记》中最流行的 25 个单词
- en: Let’s suppose we want to transmit the text of both books, word by word. We could
    go to the English dictionary and assign every word a number starting with 1, then
    2, then 3, and so on. But we know from our earlier Morse code example that we
    can send information more efficiently by using a code that’s adapted to the material
    being sent. Let’s create that kind of code, where the more frequently a word appears,
    the smaller its code number. So super-frequent words like the and and can be sent
    with short codes, while the rare words have longer codes that require us to send
    more bits (in *Treasure Island* about 2,780 words appear only once; in *Huckleberry
    Finn* about 2,280 words appear only once).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: The vocabularies of the two books mostly overlap, but each book has words that
    don’t appear in the other. For instance, the word yonder appears 20 times *Huckleberry
    Finn*, but not even once in *Treasure Island.* And schooner is in *Treasure Island*
    28 times, but it’s nowhere to be found in *Huckleberry Finn*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Because we want to be able to send either book with either code, let’s unify
    their vocabularies. For each word in *Huckleberry Finn* that isn’t in *Treasure
    Island*, we add one instance of that word when we make the Treasure Island code.
    We do the same thing for *Huckleberry Finn*. For example, we tack on one instance
    of yonder to the end of the book when we make the Treasure Island code so that
    we can use that code to send *Huckleberry Finn* if we wanted to.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with the words in *Treasure Island*. We’ll make an adaptive code
    for this text, starting with a tiny code for the and working our way up to huge
    codes for one-time-only words like wretchedness. Now we can send the whole book
    using that code and save time compared to any other code.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we’ll do the same thing for *Huckleberry Finn*, and make a code specifically
    for this text, giving the shortest code to and and leaving the big codes for one-time-only
    words like dangerous (shocking, but true: dangerous appears only once in *Huckleberry
    Finn*!). The Huckleberry Finn code now lets us send the contents of this book
    more quickly than any other code.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Note that these two codes are different. We’d expect that, because the two books
    have different vocabularies, and cover significantly different subject matter.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Using the Codes
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we have two codes, each of which can transmit either book. The Treasure
    Island code is tuned to how many times each word appears in *Treasure Island*,
    and the Huckleberry Finn code is tuned to *Huckleberry Finn*.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The *compression ratio* tells us how much savings we get from using an adaptive
    code versus a fixed-length code. If the ratio is exactly 1, then our adaptive
    code uses exactly as many bits as a nonadaptive code. If the ratio is 0.75, then
    the adaptive code sends only 3/4 the number of bits needed by the nonadaptive
    code. The smaller the compression ratio, the more bits we’re saving (some authors
    define this ratio with the numbers in the other order, so the larger the ratio,
    the better the compression).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try sending our two books word by word. The top bar of [Figure 6-13](#figure6-13)
    shows the compression ratio that we get from sending *Huckleberry Finn* with the
    code we built for it. We used an adaptive code called a *Huffman code*, but the
    results would be similar for most adaptive codes (Huffman 1952; Ferrier 2020).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![f06013](Images/f06013.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-13: Top: The compression ratio from sending *Huckleberry Finn* using
    the code built from that book. Bottom: The compression from using the code built
    from *Treasure Island*.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: This is pretty great. The adaptive code got a compression ratio of a little
    less than 0.5, meaning that to send *Huckleberry Finn* using this code would require
    a little less than half the number of bits required by a fixed-length code. If
    we send *Huckleberry Finn* using the code built from *Treasure Island*, we should
    expect that the compression won’t be as good, because our numbers in that code
    are not matched to the word frequencies we’re encoding. The bottom bar of [Figure
    6-13](#figure6-13) shows this result, with a compression ratio of around 0.54\.
    That’s still pretty great, but not quite as efficient.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Let’s flip the situation around and see how *Treasure Island* does with a code
    built for it, and one built for *Huckleberry Finn*. The results are shown in [Figure
    6-14](#figure6-14).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![f06014](Images/f06014.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-14: Top: The compression ratio from sending *Treasure Island* using
    the code built from *Huckleberry Finn*. Bottom: The compression ratio for sending
    *Treasure Island,* using the code for *Treasure Island*.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: This time we find that *Treasure Island* compressed better than *Huckleberry
    Finn*, which makes sense because we used a code tuned to its word usage. In general,
    the fastest way to send any message is with a code that was built for the contents
    of that message. No other code can do better, and most will do worse.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: We’ve seen that using the Treasure Island code to send *Huckleberry Finn* gives
    us worse compression. In other words, it requires more bits to send this book
    with a code that is imperfect for this message. This is because each code is based
    on its corresponding probability distribution, and those distributions are different.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: The quantity we use to measure the difference between two probability distributions
    is *cross entropy*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Note that the situation is not symmetrical. If we want to send words from *Treasure
    Island* using the Huckleberry Finn code, the cross entropy will be different from
    sending *Huckleberry Finn* with the Treasure Island code. We sometimes say that
    the cross entropy function is *asymmetrical* in its arguments, meaning that their
    order matters.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: One way to conceptualize this is to picture that our space of probability distributions
    is like the ocean, with currents flowing in different directions in different
    places. The effort required to swim from some point A to another point B, sometimes
    fighting the currents and sometimes getting carried along by them, is generally
    different than the effort required to swim from B to A. In this metaphor, the
    cross entropy is measuring the amount of work, not the actual distance between
    the points. But as A and B get closer together, the work involved in swimming
    between them, in either direction, goes down.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Cross Entropy in Practice
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s see cross entropy in action. We’ll use it just as we do when we’re training
    a photo classifier and need to compare two probability distributions. The first
    is the label that we manually created to describe what’s in the photo. The second
    is the set of probabilities that the system computes when we show it that photo.
    Our goal is to train the system so that its outputs match our labels. To do that,
    we need to know when the system gets it wrong and put a number to how wrong it
    is. That’s the cross entropy we get by comparing the label and the predictions.
    The larger the cross entropy, the larger the error.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 6-15](#figure6-15) we have the output of an imaginary classifier
    that’s predicting the probabilities for a picture of a dog. In most real situations,
    all of the label values would be 0 except for the entry for dog, which would be
    1\. Here we’ve assigned arbitrary probabilities to each of the six labels to better
    show how the system tries to match the label distribution (we can imagine that
    the picture is blurry, so we’re not sure ourselves what animal it shows).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![f06015](Images/f06015.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-15: Classifying a picture of a dog. Left: At the start of training.
    Right: After much training. The cross entropy is lower when the match is better.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: The figure at the left comes from the start of training. The system’s predictions
    are a pretty poor match to our manual labels. If we run these numbers through
    the cross entropy formula, we get a cross entropy of about 1.9\. On the right,
    we see the results after some training. Now the two distributions are much closer,
    and the cross entropy has dropped to about 1.6\.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Most deep learning libraries offer built-in routines that compute the cross
    entropy for us in a single step. In [Figure 6-15](#figure6-15) we had six categories.
    When there are only two categories, we can use a routine that’s specialized for
    that case. It’s often called the *binary cross entropy* function.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Kullback–Leibler Divergence
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cross entropy is a great measure for comparing two distributions. By minimizing
    the cross entropy, we minimize the error between the classifier’s outputs and
    our label, improving our system.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: We can make things just a little simpler conceptually with one more step. Let’s
    think of our word distributions as codes again. Recall that the entropy tells
    us how many bits are required to send a message with a perfect, tuned code. And
    the cross entropy tells us how many bits are required to send that message with
    an imperfect code. If we subtract the entropy from the cross entropy, we get the
    number of additional bits required by the imperfect code. The smaller we can get
    this number, the fewer additional bits we need, and the more the corresponding
    probability distributions are the same.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: This extra number of bits required by an imperfect code (that is, the increase
    in entropy) goes by a large number of formidable names. The most popular is the
    *Kullback–Leibler divergence* or just *KL divergence*, named for the scientists
    who presented a formula for computing this value. Less frequently, it’s also referred
    to as *discrimination information*, *information divergence*, *directed divergence*,
    *relative entropy*, and *KLIC* (for *Kullback–Leibler information criterion*).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the cross entropy, the KL divergence is asymmetrical: the order of the
    arguments matters. The KL divergence for sending *Treasure Island* with the Huckleberry
    Finn code is written KL(Treasure Island||Huckleberry Finn). The two bars in the
    middle can be thought of as a single separator, like the more frequently seen
    comma. We can think of them as representing the phrase “sent using the code for.”
    If we run through the math, this value is about 0.287\. We can think of this as
    telling us that we’re “paying” around 0.3 extra bits per word because we’re using
    the wrong code (Kurt 2017). The KL divergence for sending *Huckleberry Finn* with
    theTreasure Island code, or KL(Huckleberry Finn||Treasure Island), is much higher,
    at about 0.5\.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: The KL divergence tells us the number of additional bits we need in order to
    send our message with an imperfect code. Another way to think about this is that
    the KL divergence describes how much more information we need to turn our imperfectly
    adapted code into a perfect one. We can imagine this as a step of Bayes’ Rule,
    where we go from an approximate prior (the imperfect code) to a better posterior
    (the adapted code). In this case, the KL divergence is telling us just how much
    we learn from that idealized step of Bayes’ Rule (Thomas 2017).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: We can train our systems either by minimizing the KL divergence, or the cross
    entropy, choosing whichever is more convenient. The KL divergence has nice mathematical
    properties and shows up in many mathematical and algorithmic discussions and even
    deep learning documentation. But in practice, the cross entropy is almost always
    faster to compute. Since minimizing either one has the same effect of improving
    our system, we usually see KL divergence in technical discussions, and cross entropy
    in deep learning programs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter we looked at some of the basic ideas behind information theory,
    and how we can use them to train a deep learning system. We use these ideas in
    machine learning by translating our codes into probability distributions. That
    just means identifying the code elements with the smallest code numbers as the
    most frequent elements, and as the size of the number goes up, the frequency goes
    down. Interpreted this way, we can calculate the cross entropy of a classifier
    by comparing the list of predicted probabilities it produces in response to an
    input with the list of probabilities we assigned by hand. Our goal in training
    is to make the two distributions as similar as possible, which we can also state
    as trying to minimize the cross entropy.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: This wraps up the first part of the book. We’ve covered some fundamental ideas
    that have value far beyond deep learning. Statistics, probability, Bayes’ Rule,
    curves, and information theory all can help us make sense of a wide variety of
    problems and even things that come up in everyday life. They can help us improve
    our reasoning about events that happen in the world, and thus help us understand
    the past and prepare for the future.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: With these fundamentals in our pocket, we’ll now turn to the basic tools of
    machine learning.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
