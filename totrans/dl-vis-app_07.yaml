- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Information Theory
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter we look at the basics of *information theory*. This is a relatively
    new field of study, introduced to the world in 1948 in a groundbreaking paper,
    which laid the foundation for technologies from modern computers and satellites
    to cell phones and the internet (Shannon 1948). The goal of the original theory
    was to find the most efficient way to communicate a message electronically. But
    the ideas of that theory are deep, broad, and profound. They give us tools for
    measuring how much we know about anything by converting it to a digital form that
    we can study and manipulate.
  prefs: []
  type: TYPE_NORMAL
- en: Terms and ideas from information theory form part of the bedrock of deep learning.
    For example, the measurements provided by information theory are useful when we
    evaluate the performance of deep networks. In this chapter, we take a fast tour
    through some of the basics of information theory, while staying free of abstract
    mathematical notation.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin with the word *information*, one of those words that has both an
    everyday meaning and a specialized, scientific meaning. In this case, the meanings
    share a lot conceptual overlap, but while the popular meaning is broad and open
    to personal interpretation, the scientific meaning is precise and defined mathematically.
    Let’s start out by building up to the scientific definition of information, and
    ultimately work our way up to an important measurement that lets us compare two
    probability distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Surprise and Context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we receive a communication of any kind, something moved from one place
    to another, whether it was an electrical pulse, some photons of light, or the
    sound of someone’s voice. Speaking broadly, we could say that a *sender* somehow
    transfers some kind of communication to a *receiver*. Let’s introduce some more
    specialized vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Surprise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we sometimes use the term *surprise* to represent how unexpected
    a sender’s communication is to a receiver. Surprise isn’t a formal term. In fact,
    one of our goals in this chapter is to find more formal names for surprise and
    attach specific meanings and measures to them.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s suppose that we’re on the receiving end of a message. We want to describe
    how surprised we are by the communication we receive. Being able to do so is useful
    because, as we’ll see, the greater the surprise, the greater the amount of information
    that was delivered.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we get an unexpected text message from an unknown number. We open it
    up and the first word is Thanks. How surprised are we? Surely we are at least
    a little surprised, because so far, we don’t know who the message is from or what
    it’s about. But receiving a text thanking us for something does happen, so it’s
    not unheard of.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make up an imaginary and completely subjective surprise scale, where 0
    means something is completely expected, and 100 means it’s a total surprise, as
    in [Figure 6-1](#figure6-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![f06001](Images/f06001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-1: The surprise scale, expressed as a value from 0 to 100'
  prefs: []
  type: TYPE_NORMAL
- en: On this scale, the word Thanks at the start of an unexpected text message might
    rank a 20\. Now suppose that the first word in our message isn’t Thanks, but instead
    is Hippopotamus. Unless we’re working with those animals or are otherwise involved
    with them, that’s likely to be a rather surprising first word of a message. Let’s
    rank this word at an 80 on the surprise scale, as in [Figure 6-2](#figure6-2).
  prefs: []
  type: TYPE_NORMAL
- en: '![f06002](Images/f06002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-2: Placing messages on our surprise scale'
  prefs: []
  type: TYPE_NORMAL
- en: Although hippopotamus might be a big surprise at the start of a message, it
    might not be surprising later on. The difference is context.
  prefs: []
  type: TYPE_NORMAL
- en: Unpacking Context
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our purposes, we can think of *context* as the environment of the message.
    Since we’re focusing on the meaning of each message, rather than the physical
    way it’s communicated, the context represents the shared knowledge between the
    sender and receiver, which gives the message meaning.
  prefs: []
  type: TYPE_NORMAL
- en: When the message is a piece of language, this shared knowledge must include
    the words used, since a message of Kxnfq rnggw would carry no meaning. We can
    extend that shared knowledge to include grammar, current interpretations of emoticons
    and abbreviations, shared cultural influences, and so on. This is all called *global
    context*. It’s the general knowledge that we bring to any message, even before
    we’ve read it. In terms of our Bayes’ Rule discussion of Chapter 4, some of this
    global context is captured in our *prior*, since that is how we represent our
    understanding of the environment and what we expect to learn from it.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the global context, there is also *local context*. That’s the
    environment composed of the elements of the message itself. In a text message,
    the local context for any given word is the other words in that message.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine that we’re reading a message for the first time, so each word’s
    local context is made up only of the words that preceded it. We can use the context
    to get a handle on surprise. If Hippopotamus is the first word of our message,
    then there is no local context yet, only the global. And if we don’t work with
    hippopotamuses on a regular basis, that word is likely very surprising. But if
    the message begins with, Let’s go down to the river area at the zoo and maybe
    see a big gray, then in that context, the word hippopotamus isn’t very surprising.
  prefs: []
  type: TYPE_NORMAL
- en: We can describe the amount of surprise carried by a specific word in our global
    context by assigning it a surprise value, as we did in [Figure 6-1](#figure6-1).
    Suppose that we assign a surprise value to every word in the dictionary (a tedious
    job, but certainly possible). If we scale these numbers so that they all add up
    to 1, we’ve created a probability mass function (or pmf), as we discussed in Chapter
    2\. That means we can draw a random variable from that pmf to get a word, with
    the most surprising words coming along more frequently than the less surprising
    words. A more common approach is to set up the pmf to represent how common a word
    is, which is roughly the opposite of surprise. With that setup, we’d expect to
    draw the least surprising, or more common, words more frequently than uncommon
    words.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use this idea later in the chapter to devise a scheme for transmitting
    the content of a message in an efficient manner.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we’re going to talk quite a lot about *bits*. In popular language,
    a bit is usually thought of as a little package of data, often labeled either
    0 or 1\. For instance, when we talk about internet speed in “bits per second,”
    we might picture the bits as leaves flowing down a river, and we count them as
    they go by.
  prefs: []
  type: TYPE_NORMAL
- en: This is a convenient idea, but in technical language, a bit is not a thing,
    like a leaf, but a unit, like a gallon or a gram. That is, it isn’t a piece of
    stuff but a way to talk about how much stuff we have. A bit is a container that
    holds just enough storage for what we currently think is the fundamental, indivisible,
    smallest possible chunk of information.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of bits as units in this way is technically correct, but it’s inconvenient.
    And most of the time, we can speak casually without any confusion, like when we
    say, “My net connection is 8,000 bits per second,” rather than, “My net connection
    is able to transmit 8,000 bits worth of information per second.” We’ll use the
    more casual language in most of this book, but it’s worthwhile to know the technical
    definition, because it does pop up from time to time in papers and documentation
    where the distinction is important.
  prefs: []
  type: TYPE_NORMAL
- en: We can measure the amount of information in a text message with a formula that
    tells us how many bits are needed to represent that message. We won’t get into
    the math, but we’ll describe what’s going on. The formula takes two inputs. The
    first is the text of the message. The second is a pmf that describes the surprise
    inherent in each word the message can contain (let’s just call this a *probability
    distribution* for the rest of this chapter). When we take the text of the message
    and the probability distribution together, we can produce a number that tells
    us how many bits of information the message carries.
  prefs: []
  type: TYPE_NORMAL
- en: The formula was designed so that the values it produces for each word (or, more
    generally, each *event*) have four key properties. We’ll illustrate each one using
    a context in which we work in an office, and not on a river.
  prefs: []
  type: TYPE_NORMAL
- en: Likely events have low information. Stapler has low information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unlikely events have high information. Crocodile has high information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Likely events have less information than unlikely events. Stapler conveys less
    information than crocodile.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the total information due to two *unrelated* events is the sum of their
    individual information values found separately.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first three properties relate single objects to their information. The oddball
    in the group is property 4, so let’s look at it more carefully.
  prefs: []
  type: TYPE_NORMAL
- en: In normal conversation, it’s rare for two consecutive words to be completely
    unrelated. But suppose someone asked us for a “kumquat daffodil.” Those words
    are just about completely unrelated, so property 4 says that we could find the
    information in that phrase by adding the information communicated by each word
    independently.
  prefs: []
  type: TYPE_NORMAL
- en: In normal conversation, the words that lead up to any given word often narrow
    the possibilities of what it could be. If someone says, “Today I ate a big,” then
    words like “sandwich” and “pizza” arriving next carry less surprise than “bathtub”
    or “sailboat.” When words are expected, they produce less surprise than when they’re
    not. By contrast, suppose we’re sending a device’s serial number, which is essentially
    an arbitrary sequence of letters and perhaps numbers, like “C02NV91EFY14.” If
    the characters really have no relation to each other, then adding the surprise
    due to each character gives us the overall surprise in the entire message representing
    the serial number.
  prefs: []
  type: TYPE_NORMAL
- en: 'By combining the surprise of two unrelated words into the sum of their individual
    surprise values, we go from measuring the surprise, or information, in each of
    those words to the surprise in their combination. We can keep combining words
    this way into ever-larger groups until we’ve considered the entire message. Though
    we haven’t gone into the math, we have reached a formal definition of *information*:
    it’s a number produced from a formula that uses one or more events (such as words),
    and a probability distribution to describe how surprising each event would be
    to us. From those two inputs the algorithm provides a number for each event, and
    guarantees that those numbers satisfy the four properties we just listed. We call
    each word’s number its *entropy*, telling us how many bits are needed to communicate
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive Codes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The amount of information carried by each event is influenced by the size of
    the probability function we hand to our formula. In other words, the number of
    possible words we might communicate affects the amount of information carried
    by each word we send.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we want to transmit the contents of a book from one place to another.
    We might list all the unique words in that book and then assign a number to each
    word, starting perhaps with 0 for the, then 1 for and, and so on. Then, if our
    recipient also has a copy of that word list, we can send the book just by sending
    the number for each word, starting with the first word in the book. The Dr. Seuss
    book *Green Eggs and Ham* contains only 50 different words (Seuss 1960). To represent
    a number between 0 and 49, we need six bits of information per word. By contrast,
    Robert Louis Stevenson’s book *Treasure Island* contains about 10,700 unique words
    (Stevenson 1883). We’d have to use 14 bits per word to uniquely identify each
    word in that book.
  prefs: []
  type: TYPE_NORMAL
- en: Although we could use one giant word list of all English words to send these
    books, it’s more efficient to tailor our list to each book’s individual vocabulary,
    including only the words we actually need. In other words, we can improve our
    efficiency by *adapting* our transmission of information to what’s being communicated.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take that idea and run with it.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking Morse
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A great example of adaptation is Morse code. In Morse code, each typographical
    character has an associated pattern of dots and dashes, separated by spaces, as
    shown in [Figure 6-3](#figure6-3).
  prefs: []
  type: TYPE_NORMAL
- en: '![f06003](Images/f06003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-3: Each character in Morse code has an associated pattern of dots,
    dashes, and spaces.'
  prefs: []
  type: TYPE_NORMAL
- en: Morse code is traditionally sent by using a telegraph key to enable or disable
    transmission of a clear tone. A dot is a short burst of sound. The length of time
    we hold down the key to send a dot is represented by a unit called the *dit*.
    A dash is held for the duration of three dits. We leave one dit of silence between
    symbols, a silence of three dits between letters, and a silence of seven dits
    between words. These are of course ideal measures. In practice, many people can
    recognize the personal rhythm, called the *fist*, of each of their friends and
    colleagues (Longden 1987).
  prefs: []
  type: TYPE_NORMAL
- en: 'Morse code contains three types of symbols: dots, dashes, and dot-sized spaces.
    Let’s suppose we want to send the message “nice dog” in Morse code. [Figure 6-4](#figure6-4)
    shows the sequence of short tones (dots), long tones (dashes), and dot-sized spaces.'
  prefs: []
  type: TYPE_NORMAL
- en: '![f06004](Images/f06004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-4: The three symbols of Morse code: dots (solid circles), dashes (solid
    boxes), and silent spaces (empty circles).'
  prefs: []
  type: TYPE_NORMAL
- en: We typically talk about Morse code strictly in terms of dots and dashes, which
    are called the *symbols*. The assigned set of symbols for any letter is that letter’s
    *pattern*. The length of time it takes to send a message depends on the specific
    patterns assigned to the letters that make up the message’s content. For example,
    even though the letters Q and H both have four symbols, Q requires 13 dits to
    send (3 for each of the 3 dashes, 1 for the dot, and 1 for each of the 3 spaces),
    while we need only 7 dits to send the letter H (4 dots, and 1 for each of the
    3 spaces).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s compare the patterns of the different characters. When we look at [Figure
    6-3](#figure6-3), it might not be clear to us that there’s any principle behind
    how the various patterns are assigned. But a beautiful idea is there waiting to
    be uncovered. [Figure 6-5](#figure6-5) shows a list of the 26 Roman letters, sorted
    by their typical frequency in English (Wikipedia 2020). The most frequently used
    letter, E, leads the list.
  prefs: []
  type: TYPE_NORMAL
- en: '![f06005](Images/f06005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-5: The Roman letters sorted by their frequency of use in English'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now look back at the patterns in [Figure 6-3](#figure6-3). The most frequent
    letter, E, is just a single dot. The next most frequent letter, T, is just a single
    dash. Those are the only two possible patterns with just one symbol, so now we
    move on to two symbols. The letter A is a dot followed by a dash. O is next, and
    it breaks the pattern because it’s too long: three dashes. Let’s come back to
    that later. Returning to our list, the I is two dots, the N is a dash and a dot.
    The last two-letter pattern is M, with two dashes, but that’s pretty far down
    the list from where we’ve gotten so far. Why is O too long and M too short? Morse
    code is almost following our letter-frequency table, but not quite.'
  prefs: []
  type: TYPE_NORMAL
- en: The explanation starts with Samuel Morse, who only defined patterns for the
    numbers 0 through 9 in his original code. Letters and punctuation were added to
    the code by Alfred Vail, who designed those patterns in about 1844 (Bellizzi 11).
    Vail didn’t have an easy way to look up letter frequencies, but he knew he should
    follow them, according to Vail’s assistant, William Baxter. Baxter said,
  prefs: []
  type: TYPE_NORMAL
- en: His general plan was to employ the simplest and shortest combinations to represent
    the most frequently recurring letters of the English alphabet, and the remainder
    for the more infrequent ones. For instance, he found upon investigation that the
    letter e occurs much more frequently than any other letter, and accordingly he
    assigned to it the shortest symbol, a single dot (•). On the other hand, j, which
    occurs infrequently, is expressed by dash-dot-dash-dot (– • – •) (Pope 1887)^([1](#c06-footnote-1))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Vail figured that he could estimate the letter frequency table for English text
    by visiting his local newspaper in Morristown, New Jersey, where they were still
    setting stories by hand. In those days, typesetters built up a page one letter
    at a time. For each letter, they would choose an appropriate *slug*, or a metal
    bar with a letter embossed on one end, and place it into a large tray. Vail reasoned
    that the most popular characters would have the greatest number of slugs on hand,
    so he counted up the number of slugs in each letter’s bin. Those popularity counts
    were his proxy for letter frequency in English (McEwen 1997). Given how small
    this sample was, he did a pretty great job, despite imperfections like apparently
    thinking that M was more frequent than O.
  prefs: []
  type: TYPE_NORMAL
- en: To see how well our frequency chart (and Morse code) lines up with some actual
    text, [Figure 6-6](#figure6-6) shows the frequencies for the letters from *Treasure
    Island* (Stevenson 1883). For this chart, we counted only the letters, which we
    turned into lowercase before counting. We also excluded numbers, spaces, and punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: '![f06006](Images/f06006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-6: The number of times each letter appears in *Treasure Island* by
    Robert Louis Stevenson. Uppercase letters were counted as lowercase.'
  prefs: []
  type: TYPE_NORMAL
- en: The order of the characters in [Figure 6-6](#figure6-6) isn’t a perfect match
    to our letter frequency chart in [Figure 6-5](#figure6-5), but it’s close. [Figure
    6-6](#figure6-6) looks like a probability distribution over the letters A through
    Z. To make it an *actual* probability distribution, we have to scale it so that
    the sum of all the entries is 1\. The result is shown in [Figure 6-7](#figure6-7).
  prefs: []
  type: TYPE_NORMAL
- en: '![f06007](Images/f06007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-7: The probability distribution function (pdf) for characters in *Treasure
    Island*'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s use our probability distribution of letters to improve the efficiency
    of sending *Treasure Island* via Morse code.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing Morse Code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To motivate our improvements to sending *Treasure Island* via Morse code, let’s
    first take a step backward, and start with an imaginary version of Morse code
    where Mr. Vail didn’t bother to journey down to the newspaper office. Instead,
    let’s say he wanted to assign the same number of dot-and-dash symbols to each
    character. With four symbols he could only label 16 characters, but with five
    symbols he could label 32 characters.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-8](#figure6-8) shows how we might arbitrarily assign such a five-symbol
    pattern to each character. To keep things simple, we made the timing of every
    dot and dash the same by using different tones for the two symbols. So every dot
    (shown here as a black dot) is a high tone lasting one dit, and every dash (shown
    as a red square) is a low tone lasting one dit. The result is that every character
    takes nine dits of time to send (five for the dots and dashes, now high and low
    tones, and four for the silences between them). This is an example of a *constant-length
    code*, also called *a fixed-length code*.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 6-8](#figure6-8) we didn’t create a character for the space, following
    in the footsteps of the original Morse code, where it was assumed that we could
    figure out where the spaces ought to go by looking at the message. Sticking to
    that spirit, we’ll ignore space characters for the rest of this discussion.
  prefs: []
  type: TYPE_NORMAL
- en: '![f06008](Images/f06008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-8: Assigning five symbols to each character gives us a constant-length
    code. Black circles are high tones; red squares are low tones. They all last one
    dit.'
  prefs: []
  type: TYPE_NORMAL
- en: The first two words in the text of *Treasure Island* are the name “Squire Trelawney.”
    Since every character in our two-tone version of Morse code requires 9 dits, this
    phrase of 15 letters (remember that we’re ignoring the space) requires 9 × 15
    = 135 dits of time to send. Adding in the 14 silences between letters, which take
    3 × 14 = 42 bits, we find the fixed-length message takes 135 + 42 = 177 dits of
    time, as shown in [Figure 6-9](#figure6-9).
  prefs: []
  type: TYPE_NORMAL
- en: '![f06009](Images/f06009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-9: The first two words of *Treasure Island*, using our constant-length
    code'
  prefs: []
  type: TYPE_NORMAL
- en: Now compare this to actual Morse code where, for the most part, the most common
    letters have fewer symbols than the uncommon letters. [Figure 6-10](#figure6-10)
    shows this. We’ll continue sending dots and dashes using different tones that
    last one dit each.
  prefs: []
  type: TYPE_NORMAL
- en: '![f06010](Images/f06010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-10: The first two words of *Treasure Island*, using Morse code'
  prefs: []
  type: TYPE_NORMAL
- en: If we count up the elements (remembering that dots and dashes now take just
    one dit each), we find that the [Figure 6-10](#figure6-10) version requires 101
    dits of time, about half as long as the fixed-length code (101 / 177 ≈ 0.57).
    That savings comes from adapting our code to the content we are sending. We call
    any code that tries to improve efficiency by matching up short patterns with high-probability
    events a *variable-bitrate code*, or more simply, an *adaptive code*. Even in
    this simple example, our adaptive code is almost twice as efficient as the constant-length
    code, cutting our communication time nearly in half.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the whole text of *Treasure Island*, which contains about 338,000
    characters (excluding spaces, punctuation, etc.). The adaptive code would take
    only about 42 percent of the time required by the fixed-length code. We can send
    the book in less than half the time required by a nonadaptive code.
  prefs: []
  type: TYPE_NORMAL
- en: We can do even better if, instead of using standard Morse code, which is adapted
    to English writing in general, we tune the distribution of symbols to more closely
    match their actual percentages in the text of the specific book we’re sending.
    Of course, we’d have to share our clever encoding with our recipient, but if we’re
    sending a long message, that extra piece of communication is dwarfed by the message
    itself. Let’s take that step, and imagine a custom Treasure Island code that is
    perfectly adapted to the contents of *Treasure Island* specifically. We should
    expect even more savings.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s rephrase this using the language of probability. An adaptive code creates
    a pattern for each value in a probability distribution. The value with the highest
    probability receives the shortest possible code. Then we work our way through
    the values, from the highest probability to the lowest, assigning patterns that
    are always as short as possible without repeating. That means each new pattern
    is as long as, or longer than, the pattern assigned to the previous value. That’s
    just what Mr. Vail did in 1844, guided by the number of letters he found in the
    typesetter’s bins of his local newspaper.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can look at any message we want to communicate, identify each character,
    and compare it to the probability distribution that tells us how likely that character
    was in the first place. This tells us how much information, in bits, is carried
    by that character. Thanks to the fourth property in our description of the formula
    for computing information, the total number of bits required to represent the
    message (ignoring context for the moment), is just the sum of the individual numbers
    of bits required by each character.
  prefs: []
  type: TYPE_NORMAL
- en: We can also perform this process for our message before we send it. That tells
    us just how much information we’re about to communicate to our recipient.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve discussed *surprise*, which refers to things we didn’t expect. A related
    idea is *uncertainty*, which refers to those times when we know all the things
    that might happen, but we’re not sure which one will actually occur. For instance,
    when we roll a fair six-sided die, we know that each of the six faces has an equal
    probability of coming up, but until we roll it and look, we are uncertain which
    face will be on top. A more formal term for this uncertainty is *entropy*.
  prefs: []
  type: TYPE_NORMAL
- en: We can assign a number to the uncertainty, or entropy, of an outcome. This number
    often depends on how many outcomes are possible. For example, flipping a coin
    can have only 2 outcomes, but rolling a six-sided die can have 6 outcomes, and
    picking a letter from the alphabet can have 26 outcomes. The uncertainty of these
    three results, or their entropy, is a number that increases in size, from the
    coin to the die to the alphabet, because the number of outcomes is increasing
    in each case. That makes each specific result more uncertain.
  prefs: []
  type: TYPE_NORMAL
- en: In those three examples, the probability of each outcome is the same (1/2 for
    each side of the coin, 1/6 for each die face, and 1/26 for each letter). But what
    if the probabilities of the outcomes are different? The formula for computing
    the entropy explicitly takes these different probabilities into account. Essentially,
    it considers all the possible outcomes in a distribution and puts a number to
    the uncertainty describing which outcome is actually produced when we sample the
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that the uncertainty of a specific event occurring is the same
    as the number of bits required to send a message with a perfectly adapted code.
    Conceptually, a text message is a set of words drawn from a vocabulary, which
    is no different than the values of a die rolled multiple times. We use the term
    *entropy* for both values: the uncertainty of an event, or the number of bits
    required to communicate that event (and remove the uncertainty).'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy is useful in machine learning because it lets us compare two probability
    distributions. This is a key step in learning. For example, consider a classifier.
    We might have a picture that we have manually decided is 80 percent likely to
    be a dog, but 10 percent likely to be a wolf, 3 percent likely to be a fox, and
    a few other smaller probabilities for other animals. We’d like the system’s predictions
    to match those labels. In other words, we want to compare our manual distribution
    with the system’s predicted distribution and use any differences to improve our
    system. We can invent lots of ways to compare distributions, but the one that
    works best in both theory and practice is based on entropy. Let’s build our way
    up to that comparison, starting with finding the entropy for a single distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a distribution made up of words. If only one word is in our distribution,
    there is no uncertainty about what word we’ll get when we sample the distribution,
    and therefore the entropy is 0\. If there are lots of words but they all have
    a probability of 0, except for a single word with a probability of 1, there’s
    still no uncertainty, so the entropy is again 0\. When all of the words have the
    same probabilities, we have the maximum uncertainty, since no choice is any more
    probable than any other. In this case, our uncertainty, or entropy, is at a maximum.
    Though it might be convenient to say that maximum entropy should be 1, or 100,
    the actual value is calculated by the formula. What we do know is that no other
    probability distribution will give us a larger entropy.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll see how to apply entropy to pairs of distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Cross Entropy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we’re training a deep learning system, we’ll often want to have a measure
    that tells us to what degree two probability distributions are the same or different.
    The value we usually use is a quantity called the *cross entropy*, and it too
    is just a number. Recall that the entropy tells us how many bits we need to send
    a message using a code that is perfectly tuned to that message. The cross entropy
    tells us how many bits we need if we use some other, less perfect code. Generally,
    this is larger than the number of bits the perfect code needs (if the alternative
    code happens to be exactly as efficient as the ideal code, the cross entropy has
    its minimum value of 0). The cross entropy is a measurement that lets us compare
    two probability distributions numerically. Identical distributions have a cross
    entropy of 0, while increasingly different pairs of distributions have increasingly
    larger values of cross entropy.
  prefs: []
  type: TYPE_NORMAL
- en: To get a feeling for the idea, let’s look at two novels, and build up a word-based
    adaptive code for each. Though our goal is to compare probability distributions,
    and we’re here talking about codes, it’s conceptually easy to go back and forth.
    Recall that by construction, smaller codes correspond to words with higher probabilities,
    while larger codes correspond to words with lower probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Two Adaptive Codes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The novels *Treasure Island* and *The Adventures of Huckleberry Finn*, by Mark
    Twain, were both written in English at about the same time (Stevenson 1883; Twain
    1885). *Treasure Island* has the larger vocabulary, using about 10,700 unique
    words, compared to about 7,400 unique words in *Huckleberry Finn*. Of course,
    they use very different sets of words, but there’s lots of overlap. Let’s look
    at the 25 most popular words in *Treasure Island*, shown in [Figure 6-11](#figure6-11).
    For the purposes of counting words, we first converted all uppercase letters to
    lowercase. The single-letter pronoun “I” therefore appears in the charts as the
    lower-case “i.”
  prefs: []
  type: TYPE_NORMAL
- en: '![f06011](Images/f06011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-11: The 25 most popular words in *Treasure Island*, sorted by number
    of appearances'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s compare these to the 25 most popular words in *Huckleberry Finn*, shown
    in [Figure 6-12](#figure6-12).
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps unsurprisingly, the most popular dozen words in both books are almost
    the same (though in different orders), but then things begin to diverge.
  prefs: []
  type: TYPE_NORMAL
- en: '![f06012](Images/f06012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-12: The 25 most popular words in *Huckleberry Finn*, sorted by number
    of appearances'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s suppose we want to transmit the text of both books, word by word. We could
    go to the English dictionary and assign every word a number starting with 1, then
    2, then 3, and so on. But we know from our earlier Morse code example that we
    can send information more efficiently by using a code that’s adapted to the material
    being sent. Let’s create that kind of code, where the more frequently a word appears,
    the smaller its code number. So super-frequent words like the and and can be sent
    with short codes, while the rare words have longer codes that require us to send
    more bits (in *Treasure Island* about 2,780 words appear only once; in *Huckleberry
    Finn* about 2,280 words appear only once).
  prefs: []
  type: TYPE_NORMAL
- en: The vocabularies of the two books mostly overlap, but each book has words that
    don’t appear in the other. For instance, the word yonder appears 20 times *Huckleberry
    Finn*, but not even once in *Treasure Island.* And schooner is in *Treasure Island*
    28 times, but it’s nowhere to be found in *Huckleberry Finn*.
  prefs: []
  type: TYPE_NORMAL
- en: Because we want to be able to send either book with either code, let’s unify
    their vocabularies. For each word in *Huckleberry Finn* that isn’t in *Treasure
    Island*, we add one instance of that word when we make the Treasure Island code.
    We do the same thing for *Huckleberry Finn*. For example, we tack on one instance
    of yonder to the end of the book when we make the Treasure Island code so that
    we can use that code to send *Huckleberry Finn* if we wanted to.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with the words in *Treasure Island*. We’ll make an adaptive code
    for this text, starting with a tiny code for the and working our way up to huge
    codes for one-time-only words like wretchedness. Now we can send the whole book
    using that code and save time compared to any other code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we’ll do the same thing for *Huckleberry Finn*, and make a code specifically
    for this text, giving the shortest code to and and leaving the big codes for one-time-only
    words like dangerous (shocking, but true: dangerous appears only once in *Huckleberry
    Finn*!). The Huckleberry Finn code now lets us send the contents of this book
    more quickly than any other code.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that these two codes are different. We’d expect that, because the two books
    have different vocabularies, and cover significantly different subject matter.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Codes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we have two codes, each of which can transmit either book. The Treasure
    Island code is tuned to how many times each word appears in *Treasure Island*,
    and the Huckleberry Finn code is tuned to *Huckleberry Finn*.
  prefs: []
  type: TYPE_NORMAL
- en: The *compression ratio* tells us how much savings we get from using an adaptive
    code versus a fixed-length code. If the ratio is exactly 1, then our adaptive
    code uses exactly as many bits as a nonadaptive code. If the ratio is 0.75, then
    the adaptive code sends only 3/4 the number of bits needed by the nonadaptive
    code. The smaller the compression ratio, the more bits we’re saving (some authors
    define this ratio with the numbers in the other order, so the larger the ratio,
    the better the compression).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try sending our two books word by word. The top bar of [Figure 6-13](#figure6-13)
    shows the compression ratio that we get from sending *Huckleberry Finn* with the
    code we built for it. We used an adaptive code called a *Huffman code*, but the
    results would be similar for most adaptive codes (Huffman 1952; Ferrier 2020).
  prefs: []
  type: TYPE_NORMAL
- en: '![f06013](Images/f06013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-13: Top: The compression ratio from sending *Huckleberry Finn* using
    the code built from that book. Bottom: The compression from using the code built
    from *Treasure Island*.'
  prefs: []
  type: TYPE_NORMAL
- en: This is pretty great. The adaptive code got a compression ratio of a little
    less than 0.5, meaning that to send *Huckleberry Finn* using this code would require
    a little less than half the number of bits required by a fixed-length code. If
    we send *Huckleberry Finn* using the code built from *Treasure Island*, we should
    expect that the compression won’t be as good, because our numbers in that code
    are not matched to the word frequencies we’re encoding. The bottom bar of [Figure
    6-13](#figure6-13) shows this result, with a compression ratio of around 0.54\.
    That’s still pretty great, but not quite as efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s flip the situation around and see how *Treasure Island* does with a code
    built for it, and one built for *Huckleberry Finn*. The results are shown in [Figure
    6-14](#figure6-14).
  prefs: []
  type: TYPE_NORMAL
- en: '![f06014](Images/f06014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-14: Top: The compression ratio from sending *Treasure Island* using
    the code built from *Huckleberry Finn*. Bottom: The compression ratio for sending
    *Treasure Island,* using the code for *Treasure Island*.'
  prefs: []
  type: TYPE_NORMAL
- en: This time we find that *Treasure Island* compressed better than *Huckleberry
    Finn*, which makes sense because we used a code tuned to its word usage. In general,
    the fastest way to send any message is with a code that was built for the contents
    of that message. No other code can do better, and most will do worse.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve seen that using the Treasure Island code to send *Huckleberry Finn* gives
    us worse compression. In other words, it requires more bits to send this book
    with a code that is imperfect for this message. This is because each code is based
    on its corresponding probability distribution, and those distributions are different.
  prefs: []
  type: TYPE_NORMAL
- en: The quantity we use to measure the difference between two probability distributions
    is *cross entropy*.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the situation is not symmetrical. If we want to send words from *Treasure
    Island* using the Huckleberry Finn code, the cross entropy will be different from
    sending *Huckleberry Finn* with the Treasure Island code. We sometimes say that
    the cross entropy function is *asymmetrical* in its arguments, meaning that their
    order matters.
  prefs: []
  type: TYPE_NORMAL
- en: One way to conceptualize this is to picture that our space of probability distributions
    is like the ocean, with currents flowing in different directions in different
    places. The effort required to swim from some point A to another point B, sometimes
    fighting the currents and sometimes getting carried along by them, is generally
    different than the effort required to swim from B to A. In this metaphor, the
    cross entropy is measuring the amount of work, not the actual distance between
    the points. But as A and B get closer together, the work involved in swimming
    between them, in either direction, goes down.
  prefs: []
  type: TYPE_NORMAL
- en: Cross Entropy in Practice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s see cross entropy in action. We’ll use it just as we do when we’re training
    a photo classifier and need to compare two probability distributions. The first
    is the label that we manually created to describe what’s in the photo. The second
    is the set of probabilities that the system computes when we show it that photo.
    Our goal is to train the system so that its outputs match our labels. To do that,
    we need to know when the system gets it wrong and put a number to how wrong it
    is. That’s the cross entropy we get by comparing the label and the predictions.
    The larger the cross entropy, the larger the error.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 6-15](#figure6-15) we have the output of an imaginary classifier
    that’s predicting the probabilities for a picture of a dog. In most real situations,
    all of the label values would be 0 except for the entry for dog, which would be
    1\. Here we’ve assigned arbitrary probabilities to each of the six labels to better
    show how the system tries to match the label distribution (we can imagine that
    the picture is blurry, so we’re not sure ourselves what animal it shows).
  prefs: []
  type: TYPE_NORMAL
- en: '![f06015](Images/f06015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-15: Classifying a picture of a dog. Left: At the start of training.
    Right: After much training. The cross entropy is lower when the match is better.'
  prefs: []
  type: TYPE_NORMAL
- en: The figure at the left comes from the start of training. The system’s predictions
    are a pretty poor match to our manual labels. If we run these numbers through
    the cross entropy formula, we get a cross entropy of about 1.9\. On the right,
    we see the results after some training. Now the two distributions are much closer,
    and the cross entropy has dropped to about 1.6\.
  prefs: []
  type: TYPE_NORMAL
- en: Most deep learning libraries offer built-in routines that compute the cross
    entropy for us in a single step. In [Figure 6-15](#figure6-15) we had six categories.
    When there are only two categories, we can use a routine that’s specialized for
    that case. It’s often called the *binary cross entropy* function.
  prefs: []
  type: TYPE_NORMAL
- en: Kullback–Leibler Divergence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cross entropy is a great measure for comparing two distributions. By minimizing
    the cross entropy, we minimize the error between the classifier’s outputs and
    our label, improving our system.
  prefs: []
  type: TYPE_NORMAL
- en: We can make things just a little simpler conceptually with one more step. Let’s
    think of our word distributions as codes again. Recall that the entropy tells
    us how many bits are required to send a message with a perfect, tuned code. And
    the cross entropy tells us how many bits are required to send that message with
    an imperfect code. If we subtract the entropy from the cross entropy, we get the
    number of additional bits required by the imperfect code. The smaller we can get
    this number, the fewer additional bits we need, and the more the corresponding
    probability distributions are the same.
  prefs: []
  type: TYPE_NORMAL
- en: This extra number of bits required by an imperfect code (that is, the increase
    in entropy) goes by a large number of formidable names. The most popular is the
    *Kullback–Leibler divergence* or just *KL divergence*, named for the scientists
    who presented a formula for computing this value. Less frequently, it’s also referred
    to as *discrimination information*, *information divergence*, *directed divergence*,
    *relative entropy*, and *KLIC* (for *Kullback–Leibler information criterion*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the cross entropy, the KL divergence is asymmetrical: the order of the
    arguments matters. The KL divergence for sending *Treasure Island* with the Huckleberry
    Finn code is written KL(Treasure Island||Huckleberry Finn). The two bars in the
    middle can be thought of as a single separator, like the more frequently seen
    comma. We can think of them as representing the phrase “sent using the code for.”
    If we run through the math, this value is about 0.287\. We can think of this as
    telling us that we’re “paying” around 0.3 extra bits per word because we’re using
    the wrong code (Kurt 2017). The KL divergence for sending *Huckleberry Finn* with
    theTreasure Island code, or KL(Huckleberry Finn||Treasure Island), is much higher,
    at about 0.5\.'
  prefs: []
  type: TYPE_NORMAL
- en: The KL divergence tells us the number of additional bits we need in order to
    send our message with an imperfect code. Another way to think about this is that
    the KL divergence describes how much more information we need to turn our imperfectly
    adapted code into a perfect one. We can imagine this as a step of Bayes’ Rule,
    where we go from an approximate prior (the imperfect code) to a better posterior
    (the adapted code). In this case, the KL divergence is telling us just how much
    we learn from that idealized step of Bayes’ Rule (Thomas 2017).
  prefs: []
  type: TYPE_NORMAL
- en: We can train our systems either by minimizing the KL divergence, or the cross
    entropy, choosing whichever is more convenient. The KL divergence has nice mathematical
    properties and shows up in many mathematical and algorithmic discussions and even
    deep learning documentation. But in practice, the cross entropy is almost always
    faster to compute. Since minimizing either one has the same effect of improving
    our system, we usually see KL divergence in technical discussions, and cross entropy
    in deep learning programs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter we looked at some of the basic ideas behind information theory,
    and how we can use them to train a deep learning system. We use these ideas in
    machine learning by translating our codes into probability distributions. That
    just means identifying the code elements with the smallest code numbers as the
    most frequent elements, and as the size of the number goes up, the frequency goes
    down. Interpreted this way, we can calculate the cross entropy of a classifier
    by comparing the list of predicted probabilities it produces in response to an
    input with the list of probabilities we assigned by hand. Our goal in training
    is to make the two distributions as similar as possible, which we can also state
    as trying to minimize the cross entropy.
  prefs: []
  type: TYPE_NORMAL
- en: This wraps up the first part of the book. We’ve covered some fundamental ideas
    that have value far beyond deep learning. Statistics, probability, Bayes’ Rule,
    curves, and information theory all can help us make sense of a wide variety of
    problems and even things that come up in everyday life. They can help us improve
    our reasoning about events that happen in the world, and thus help us understand
    the past and prepare for the future.
  prefs: []
  type: TYPE_NORMAL
- en: With these fundamentals in our pocket, we’ll now turn to the basic tools of
    machine learning.
  prefs: []
  type: TYPE_NORMAL
