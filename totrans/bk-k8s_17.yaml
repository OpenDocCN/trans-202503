- en: '15'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '15'
- en: PERSISTENT STORAGE
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 持久存储
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: Scalability and rapid failover are big advantages of containerized applications,
    and it’s a lot easier to scale, update, and replace stateless containers that
    don’t have any persistent storage. As a result, we’ve mostly used Deployments
    to create one or more instances of Pods with only temporary storage.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性和快速故障切换是容器化应用的巨大优势，且扩展、更新和替换没有持久存储的无状态容器要容易得多。因此，我们通常使用部署（Deployments）来创建一个或多个仅具有临时存储的
    Pod 实例。
- en: However, even if we have an application architecture in which most of the components
    are stateless, we still need some amount of persistent storage for our application.
    At the same time, we don’t want to lose the ability to deploy a Pod to any node
    in the cluster, and we don’t want to lose the contents of our persistent storage
    if a container or a node fails.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使我们有一个大多数组件都是无状态的应用架构，我们仍然需要一些持久存储来支持我们的应用。同时，我们不想失去将 Pod 部署到集群中任何节点的能力，也不希望在容器或节点故障时丢失持久存储的内容。
- en: In this chapter, we’ll see how Kubernetes offers persistent storage on demand
    to Pods by using a plug-in architecture that allows any supported distributed
    storage engine to act as the backing store.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到 Kubernetes 如何通过使用插件架构按需为 Pods 提供持久存储，该架构允许任何支持的分布式存储引擎作为后端存储。
- en: Storage Classes
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储类
- en: The Kubernetes storage plug-in architecture is highly flexible; it recognizes
    that some clusters may not need storage at all, whereas others need multiple storage
    plug-ins to handle large amounts of data or low-latency storage. For this reason,
    `kubeadm` doesn’t set up storage immediately during cluster installation; it’s
    configured afterward by adding *StorageClass* resources to the cluster.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的存储插件架构高度灵活；它认识到一些集群可能根本不需要存储，而其他集群则需要多个存储插件来处理大量数据或低延迟存储。因此，`kubeadm`
    在集群安装时不会立即设置存储；它是在安装后通过向集群添加*StorageClass*资源来配置的。
- en: Each StorageClass identifies a particular storage plug-in that will provide
    the actual storage along with any additional required parameters. We can use multiple
    storage classes to define different plug-ins or parameters, or even multiple storage
    classes with the same plug-in but different parameters, allowing for separate
    classes of service for different purposes. For example, a cluster may provide
    in-memory, solid-state, and traditional spinning-disk media to give applications
    the opportunity to select the storage type that is most applicable for a given
    purpose. The cluster may offer smaller quotas for more expensive and lower-latency
    storage, while offering large quotas for slower storage that is more suitable
    for infrequently accessed data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 StorageClass 都标识一个特定的存储插件，该插件将提供实际存储以及任何其他所需的参数。我们可以使用多个存储类来定义不同的插件或参数，甚至使用相同插件但不同参数的多个存储类，以便为不同的用途提供独立的服务类别。例如，一个集群可能提供内存存储、固态硬盘存储和传统的旋转磁盘存储，让应用程序选择最适合特定目的的存储类型。该集群可能为更昂贵且低延迟的存储提供较小的配额，同时为更适合不常访问数据的慢速存储提供较大的配额。
- en: Kubernetes has a set of internal storage provisioners built in. This includes
    storage drivers for popular cloud providers such as Amazon Web Services, Microsoft
    Azure, and Google Container Engine. However, using any storage plug-in is easy
    as long as it has support for the Container Storage Interface (CSI), a published
    standard for interfacing with a storage provider.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 内置了一组内部存储提供者。这包括支持流行云服务提供商（如 Amazon Web Services、Microsoft Azure
    和 Google Container Engine）的存储驱动程序。然而，只要存储插件支持容器存储接口（CSI）这一已发布的标准，就可以轻松使用任何存储插件与存储提供商接口。
- en: Of course, to be compatible with CSI, the storage provider must include a minimum
    set of features that are essential for storage in a Kubernetes cluster. The most
    important of these are dynamic storage management (provisioning and deprovisioning)
    and dynamic storage attachment (mounting storage on any node in the cluster).
    Together, these two key features allow the cluster to allocate storage for any
    Pod that requests it, schedule that Pod on any node, and start a new Pod with
    the same storage on any node if the existing node fails or the Pod is replaced.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，为了与CSI兼容，存储提供者必须包含一些最低限度的功能，这些功能对于Kubernetes集群中的存储至关重要。最重要的功能包括动态存储管理（配置和解除配置）和动态存储附加（在集群中的任何节点上挂载存储）。这两个关键特性使得集群能够为任何请求存储的Pod分配存储，并在集群中的任何节点上调度该Pod，如果现有节点失败或Pod被替换，还能在任何节点上启动具有相同存储的新Pod。
- en: Storage Class Definition
  id: totrans-11
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 存储类定义
- en: Our Kubernetes cluster deployment in [Chapter 6](ch06.xhtml#ch06) included the
    Longhorn storage plug-in (see “Installing Storage” on [page 102](ch06.xhtml#ch00lev2sec46)).
    The automation scripts have installed it in the cluster for each following chapter.
    Part of this installation created a DaemonSet so that Longhorn components exist
    on every node. That DaemonSet kicked off a number of Longhorn components and then
    created a StorageClass resource to tell Kubernetes how to use Longhorn to provision
    storage for a Pod.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第6章](ch06.xhtml#ch06)中部署的Kubernetes集群包含了Longhorn存储插件（请参阅“安装存储”章节 [102页](ch06.xhtml#ch00lev2sec46)）。自动化脚本已将其安装到集群中，并为后续各章做好了准备。部分安装工作创建了一个DaemonSet，以确保Longhorn组件存在于每个节点上。该DaemonSet启动了多个Longhorn组件，并创建了一个StorageClass资源，告诉Kubernetes如何使用Longhorn为Pod配置存储。
- en: '**NOTE**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*本书的示例仓库在* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples)。*有关如何设置的详细信息，请参见“运行示例”章节
    [xx页](ch00.xhtml#ch00lev1sec2)。*'
- en: '[Listing 15-1](ch15.xhtml#ch15list1) shows the StorageClass that Longhorn created.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 15-1](ch15.xhtml#ch15list1)显示了Longhorn创建的StorageClass。'
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Listing 15-1: Longhorn StorageClass*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 15-1：Longhorn StorageClass*'
- en: The two most important fields show the name of the StorageClass and the provisioner.
    The name is used in resource specifications to identify that the Longhorn StorageClass
    should be used to provision the requested volume, whereas the provisioner is used
    internally by `kubelet` to communicate with the Longhorn CSI plug-in.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个最重要的字段显示了StorageClass的名称和供应者。名称用于资源规格中，标识应该使用Longhorn StorageClass来配置请求的卷，而供应者则是`kubelet`内部用来与Longhorn
    CSI插件通信的。
- en: CSI Plug-in Internals
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CSI插件内部实现
- en: Let’s look quickly at how `kubelet` finds and communicates with the Longhorn
    CSI plug-in before moving on to provisioning volumes and attaching them to Pods.
    Note that `kubelet` runs as a service directly on the cluster nodes; on the other
    hand, all of the Longhorn components are containerized. This means that the two
    need a little help to communicate in the form of a Unix socket that is created
    on the host filesystem and then mounted into the filesystem of the Longhorn containers.
    A Unix socket allows two processes to communicate by streaming data, similar to
    a network connection but without the network overhead.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续配置卷并将其附加到Pods之前，我们先快速了解一下`kubelet`是如何查找并与Longhorn CSI插件通信的。请注意，`kubelet`作为服务直接运行在集群节点上；另一方面，所有Longhorn组件都被容器化。这意味着二者需要通过在主机文件系统上创建的Unix套接字来帮助它们进行通信，然后将该套接字挂载到Longhorn容器的文件系统中。Unix套接字允许两个进程通过流式数据进行通信，类似于网络连接，但没有网络开销。
- en: 'To explore how this communication works, first we’ll list the Longhorn containers
    that are running on `host01`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探讨这种通信如何工作，首先我们将列出在`host01`上运行的Longhorn容器：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Longhorn creates containers with names that start with either `longhorn` or
    `csi`, so we use a regular expression with `crictl` to show only those containers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Longhorn创建的容器名称以`longhorn`或`csi`开头，因此我们使用正则表达式和`crictl`来仅显示这些容器。
- en: 'Let’s capture the container ID of the `csi-attacher` container and then inspect
    it to see what volume mounts it has:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取`csi-attacher`容器的容器ID，然后检查它，看看它挂载了哪些卷：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `crictl inspect` command returns a lot of data from the container, but we
    show only the relevant data in this example. We can see that this Longhorn component
    is instructed to connect to */csi/csi.sock* ➋, which is the mount point inside
    the container for the Unix socket that `kubelet` uses to communicate with the
    storage driver. We can also see that */csi* inside the container is */var/lib/kubelet/plugins/driver.longhorn.io*
    ➊. The location */var/lib/kubelet/plugins* is a standard location for `kubelet`
    to look for storage plug-ins, and of course, *driver.longhorn.io* is the value
    of the `provisioner` field, as defined in the Longhorn StorageClass in [Listing
    15-1](ch15.xhtml#ch15list1).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`crictl inspect` 命令返回了容器的很多数据，但在这个示例中我们只展示了相关数据。我们可以看到这个 Longhorn 组件被指示连接到
    */csi/csi.sock* ➋，这是容器内的 Unix 套接字挂载点，`kubelet` 用它与存储驱动进行通信。我们还可以看到容器内的 */csi*
    实际上是 */var/lib/kubelet/plugins/driver.longhorn.io* ➊。`/var/lib/kubelet/plugins`
    是 `kubelet` 查找存储插件的标准位置，当然，*driver.longhorn.io* 是 `provisioner` 字段的值，如 [Listing
    15-1](ch15.xhtml#ch15list1) 中的 Longhorn StorageClass 所定义。'
- en: 'If we look on the host, we can confirm that this Unix socket exists:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看主机，能够确认这个 Unix 套接字存在：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `s` as the first character indicates that this is a Unix socket.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一个字符的 `s` 表示这是一个 Unix 套接字。
- en: Persistent Volumes
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 持久卷
- en: Now that we’ve seen how `kubelet` communicates with an external storage driver,
    let’s look at how to request allocation of storage and then attach that storage
    to a Pod.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 `kubelet` 如何与外部存储驱动通信，让我们看看如何请求分配存储并将其附加到 Pod。
- en: Stateful Sets
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Stateful Sets
- en: The easiest way to get storage in a Pod is to use a StatefulSet (a resource
    described in [Chapter 7](ch07.xhtml#ch07)). Like a Deployment, a StatefulSet creates
    multiple Pods, which can be allocated to any node. However, a StatefulSet also
    creates persistent storage as well as a mapping between each Pod and its storage.
    If a Pod needs to be replaced, it is replaced with a new Pod with the same identifier
    and the same persistent storage.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pod 中获取存储的最简单方式是使用 StatefulSet（[第7章](ch07.xhtml#ch07) 中描述的一种资源）。像 Deployment
    一样，StatefulSet 会创建多个 Pod，这些 Pod 可以分配到任何节点。然而，StatefulSet 还会创建持久存储，以及每个 Pod 和其存储之间的映射。如果某个
    Pod 需要被替换，它将被替换为一个具有相同标识符和相同持久存储的新 Pod。
- en: '[Listing 15-2](ch15.xhtml#ch15list2) presents an example StatefulSet that creates
    two PostgreSQL Pods with persistent storage.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 15-2](ch15.xhtml#ch15list2) 展示了一个示例 StatefulSet，它创建了两个带有持久存储的 PostgreSQL
    Pods。'
- en: '*pgsql-set.yaml*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*pgsql-set.yaml*'
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Listing 15-2: PostgreSQL StatefulSet*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 15-2: PostgreSQL StatefulSet*'
- en: In addition to setting the password using an environment variable ➊, we also
    set `PGDATA` to */data/pgdata* ➋, which tells PostgreSQL where to store the files
    for the database. It aligns with the volume mount we also declare as part of the
    StatefulSet, as that persistent volume will be mounted at */data* ➌. The PostgreSQL
    container image documentation recommends configuring the database files to reside
    in a subdirectory beneath the mount point to avoid a potential issue with ownership
    of the data directory.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过环境变量设置密码 ➊，我们还将 `PGDATA` 设置为 */data/pgdata* ➋，这告诉 PostgreSQL 数据库文件应该存储的位置。这与我们作为
    StatefulSet 一部分声明的卷挂载相一致，因为那个持久卷将挂载到 */data* ➌。PostgreSQL 容器镜像文档建议将数据库文件配置在挂载点下的子目录中，以避免数据目录的所有权问题。
- en: Separate from the configuration for the PostgreSQL Pods, we supply the StatefulSet
    with the `volumeClaimTemplates` field. This field tells the StatefulSet how we
    want the persistent storage to be configured. It includes the name of the StorageClass
    and the requested size, and it also includes an `accessMode` of `ReadWriteOnce`,
    which we’ll explore later. The StatefulSet will use this specification to allocate
    independent storage for each Pod.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与 PostgreSQL Pod 的配置分开，我们为 StatefulSet 提供了 `volumeClaimTemplates` 字段。这个字段告诉
    StatefulSet 我们希望如何配置持久存储。它包括 StorageClass 的名称和请求的大小，还包括 `ReadWriteOnce` 的 `accessMode`，我们稍后将探讨。StatefulSet
    将使用此规范为每个 Pod 分配独立的存储。
- en: 'As mentioned in [Chapter 7](ch07.xhtml#ch07), this StatefulSet references a
    Service using the `serviceName` field, and this Service is used to create the
    domain name for the Pods. The Service is defined in the same file as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [第7章](ch07.xhtml#ch07) 中所提到的，这个 StatefulSet 通过 `serviceName` 字段引用了一个 Service，该
    Service 用来为 Pods 创建域名。Service 的定义在同一个文件中，具体如下：
- en: '*pgsql-set.yaml*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*pgsql-set.yaml*'
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Setting the `clusterIP` field to `None` makes this a *Headless Service*, which
    means that no IP address is allocated from the service IP range and none of the
    load balancing described in [Chapter 9](ch09.xhtml#ch09) is configured for this
    Service. This approach is typical for a StatefulSet. With a StatefulSet, each
    Pod has its own unique identity and unique storage. Because service load balancing
    just randomly chooses a destination, it is typically not useful with a StatefulSet.
    Instead, clients explicitly select a Pod instance as a destination.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `clusterIP` 字段设置为 `None` 会使其成为一个 *无头服务*，这意味着不会从服务 IP 范围分配 IP 地址，也不会为该 Service
    配置 [第 9 章](ch09.xhtml#ch09) 中描述的负载均衡。这个方法通常用于 StatefulSet。对于 StatefulSet，每个 Pod
    都有自己独特的身份和独特的存储。由于服务负载均衡是随机选择目标，因此通常在 StatefulSet 中无效。相反，客户端需要明确选择一个 Pod 实例作为目标。
- en: 'Let’s create the Service and StatefulSet:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建 Service 和 StatefulSet：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It will take some time to get the Pods up and running because they are created
    sequentially, one at a time. After they are running, we can see how they’ve been
    named:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 Pods 需要一些时间，因为它们是顺序创建的，一个接一个。它们启动后，我们可以看到它们的名称：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let’s examine the persistent storage from within the container:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在容器内检查持久化存储：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As requested, we see a Longhorn device that has been mounted at */data*. Kubernetes
    will keep this persistent storage even if the node fails or the Pod is upgraded.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如请求所示，我们看到一个已经挂载在 */data* 的 Longhorn 设备。即使节点失败或 Pod 升级，Kubernetes 仍会保留这个持久化存储。
- en: 'This StatefulSet has two more important resources to explore. First is the
    headless Service that we created:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 StatefulSet 还有两个重要的资源需要探索。第一个是我们创建的无头 Service：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `postgres` Service exists, but no cluster IP address is shown because we
    created it as a headless Service. However, it has created DNS entries for the
    associated Pods, so we can use it to connect to specific PostgreSQL Pods without
    knowing the Pod IP address.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`postgres` Service 存在，但没有显示集群 IP 地址，因为我们创建它时是一个无头服务。然而，它为关联的 Pods 创建了 DNS 记录，因此我们可以使用它来连接特定的
    PostgreSQL Pods，而无需知道 Pod 的 IP 地址。'
- en: 'We need to use the cluster DNS to do the lookup. The easiest way to do that
    is from within a container:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用集群 DNS 来进行查找。最简单的方法是从容器内进行：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This form of the `run` command stays in the foreground and gives us an interactive
    terminal. It also tells Kubernetes not to try to restart the container when we
    exit the shell.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这种形式的 `run` 命令保持在前台并为我们提供一个交互式终端。它还告诉 Kubernetes 在我们退出 shell 时不要尝试重启容器。
- en: 'From inside this container, we can refer to either of our PostgreSQL Pods by
    a well-known name:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个容器内部，我们可以通过一个众所周知的名称来引用我们的任何 PostgreSQL Pod：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The naming convention is identical to what we saw for Services in [Chapter 9](ch09.xhtml#ch09),
    but with an extra hostname prefix for the name of the Pod; in this case, either
    `postgres-0` or `postgres-1`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 命名约定与我们在 [第 9 章](ch09.xhtml#ch09) 中看到的 Service 相同，但多了一个主机名前缀来表示 Pod 的名称；在这种情况下，可能是
    `postgres-0` 或 `postgres-1`。
- en: 'The other important resource is the *PersistentVolumeClaim* that the StatefulSet
    created automatically. The PersistentVolumeClaim is what actually allocates storage
    using the Longhorn StorageClass:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的资源是 StatefulSet 自动创建的 *PersistentVolumeClaim*。PersistentVolumeClaim 实际上是通过
    Longhorn StorageClass 分配存储的：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We use the abbreviation `pvc` in lieu of its full name, `persistentvolumeclaim`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用缩写 `pvc` 来代替其全称 `persistentvolumeclaim`。
- en: 'The StatefulSet used the data in the `volumeClaimTemplates` field in [Listing
    15-2](ch15.xhtml#ch15list2) to create these two PersistentVolumeClaims. However,
    if we delete the StatefulSet, the PersistentVolumeClaims continue to exist:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet 使用了 [清单 15-2](ch15.xhtml#ch15list2) 中 `volumeClaimTemplates` 字段的数据来创建这两个
    PersistentVolumeClaims。然而，如果我们删除 StatefulSet，PersistentVolumeClaims 会继续存在：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This protects us from accidentally deleting our persistent storage. If we create
    the StatefulSet again and keep the same name in the volume claim template, our
    new Pods will get the same storage back.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以保护我们免于意外删除持久化存储。如果我们再次创建 StatefulSet 并在卷声明模板中保持相同的名称，我们的新 Pods 会重新获得相同的存储。
- en: '**HIGHLY AVAILABLE POSTGRESQL**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**高可用 PostgreSQL**'
- en: We’ve deployed two separate instances of PostgreSQL, each with its own independent
    persistent storage. However, that’s only the first step in deploying a highly
    available database. We would also need to configure one instance as primary and
    the other as backup, configure replication from the primary to the backup, and
    configure failover. We would also need to configure clients to talk to the primary
    and switch to a new primary when there’s a failure. Fortunately, we don’t need
    to do this configuration ourselves. In [Chapter 17](ch17.xhtml#ch17), we’ll see
    how to take advantage of the power of custom resources to deploy a Kubernetes
    Operator for PostgreSQL that automatically will handle all of this.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经部署了两个独立的 PostgreSQL 实例，每个实例都有自己的独立持久存储。然而，这只是部署高可用数据库的第一步。我们还需要将其中一个实例配置为主实例，另一个配置为备份实例，配置从主实例到备份实例的复制，以及配置故障切换。我们还需要配置客户端连接到主实例，并在发生故障时切换到新的主实例。幸运的是，我们无需自己进行这些配置。在
    [第 17 章](ch17.xhtml#ch17)中，我们将看到如何利用自定义资源的强大功能，部署一个 Kubernetes Operator 来自动处理所有这些任务。
- en: The StatefulSet is the best way to handle the case in which we need multiple
    instances of a container, each with its own independent storage. However, we can
    also use persistent volumes more directly, which gives us more control over how
    they’re mounted into our Pods.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet 是处理需要多个容器实例并且每个实例都需要独立存储的最佳方式。然而，我们也可以更直接地使用持久卷，这样能让我们对它们如何挂载到 Pod
    中有更多控制。
- en: Volumes and Claims
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 卷和声明
- en: 'Kubernetes has both a *PersistentVolume* and a PersistentVolumeClaim resource
    type. The PersistentVolumeClaim represents a request for allocated storage, whereas
    the PersistentVolume holds information on the allocated storage. For the most
    part, this distinction doesn’t matter, and we can just focus on the PersistentVolumeClaim.
    However, the difference is important in two cases:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 有两种资源类型：*PersistentVolume* 和 PersistentVolumeClaim。PersistentVolumeClaim
    表示对已分配存储的请求，而 PersistentVolume 则包含关于已分配存储的信息。在大多数情况下，这种区别并不重要，我们可以专注于 PersistentVolumeClaim。然而，在两种情况下，区别是很重要的：
- en: Administrators can create a PersistentVolume manually, and this PersistentVolume
    can be directly mounted into a Pod.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理员可以手动创建 PersistentVolume，并将这个 PersistentVolume 直接挂载到 Pod 中。
- en: If there is an issue allocating storage as specified in the PersistentVolumeClaim,
    the PersistentVolume will not be created.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在按照 PersistentVolumeClaim 中指定的方式分配存储时出现问题，PersistentVolume 将不会被创建。
- en: 'To illustrate, first we’ll start with a PersistentVolumeClaim that automatically
    allocates storage:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，我们首先从一个自动分配存储的 PersistentVolumeClaim 开始：
- en: '*pvc.yaml*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*pvc.yaml*'
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We named this PersistentVolumeClaim `nginx-storage` because that’s how we’ll
    use it in a moment. The PersistentVolumeClaim requests 100MiB of storage from
    the `longhorn` StorageClass. When we apply this PersistentVolumeClaim to the cluster,
    Kubernetes invokes the Longhorn storage driver and allocates the storage, creating
    a PersistentVolume in the process:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个 PersistentVolumeClaim 命名为 `nginx-storage`，因为我们接下来会用到它。这个 PersistentVolumeClaim
    请求从 `longhorn` 存储类中获取 100MiB 的存储。当我们将这个 PersistentVolumeClaim 应用到集群时，Kubernetes
    会调用 Longhorn 存储驱动并分配存储，过程中会创建一个 PersistentVolume：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The abbreviation `pv` is short for `persistentvolumes`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 缩写 `pv` 是 `persistentvolumes` 的简称。
- en: Even though no Pod is using the storage, it still shows a status of `Bound`
    because there is an active PersistentVolumeClaim for the storage.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有 Pod 在使用这个存储，它仍然显示为 `Bound` 状态，因为有一个活动的 PersistentVolumeClaim 绑定了这个存储。
- en: 'If we try to create a PersistentVolumeClaim without a matching storage class,
    the cluster won’t be able to create the corresponding PersistentVolume:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试创建一个没有匹配存储类的 PersistentVolumeClaim，集群将无法创建相应的 PersistentVolume：
- en: '*pvc-man.yaml*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*pvc-man.yaml*'
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Because there is no StorageClass called `manual`, Kubernetes can’t create this
    storage automatically:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因为没有名为 `manual` 的 StorageClass，Kubernetes 无法自动创建这个存储：
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Our PersistentVolumeClaim has a status of `Pending` and there is no corresponding
    PersistentVolume. However, as a cluster administrator, we can create this PersistentVolume
    manually:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 PersistentVolumeClaim 处于 `Pending` 状态，并且没有相应的 PersistentVolume。然而，作为集群管理员，我们可以手动创建这个
    PersistentVolume：
- en: '*pv.yaml*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*pv.yaml*'
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: When creating a PersistentVolume in this way, we need to specify the type of
    volume we want. In this case, by including the `csi` field, we identify this as
    a volume created by a CSI plug-in. We then specify the `driver` to use and provide
    a unique value for `volumeHandle`. After the PersistentVolume is created, Kubernetes
    directly invokes the Longhorn storage driver to allocate storage.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式创建 PersistentVolume 时，我们需要指定所需的卷类型。在这种情况下，通过包含 `csi` 字段，我们将其标识为由 CSI 插件创建的卷。然后，我们指定要使用的
    `driver` 并为 `volumeHandle` 提供唯一值。在 PersistentVolume 创建后，Kubernetes 会直接调用 Longhorn
    存储驱动程序来分配存储。
- en: 'We create the PersistentVolume with the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过以下方式创建 PersistentVolume：
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Because we specified a `claimRef` for this PersistentVolume, it will automatically
    move into the `Bound` state:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们为这个 PersistentVolume 指定了 `claimRef`，它将自动进入 `Bound` 状态：
- en: '[PRE20]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: It will take a few seconds, so the PersistentVolume may show up as `Available`
    briefly.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这将花费几秒钟，因此 PersistentVolume 可能会短暂地显示为 `Available`。
- en: 'The PersistentVolumeClaim also moves into the `Bound` state:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: PersistentVolumeClaim 也会进入 `Bound` 状态：
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: It is useful for an administrator to create a PersistentVolume manually for
    those rare cases when specialized storage is needed for an application. However,
    for most persistent storage, it is much better to automate storage allocation
    through a StorageClass and either a PersistentVolumeClaim or a StatefulSet.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于管理员来说，手动创建 PersistentVolume 在某些特殊情况下非常有用，尤其是当应用程序需要特定存储时。然而，对于大多数持久存储，最好通过
    StorageClass 和 PersistentVolumeClaim 或 StatefulSet 来自动化存储分配。
- en: Deployments
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Deployments
- en: 'Now that we’ve directly created a PersistentVolumeClaim and we have the associated
    volume, we can use it in a Deployment. To demonstrate this, we’ll show how we
    can use persistent storage to hold HTML files served by an NGINX web server:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经直接创建了 PersistentVolumeClaim 并且有了相关的卷，我们就可以在 Deployment 中使用它。为了演示这一点，我们将展示如何使用持久存储来保存由
    NGINX 网络服务器提供的 HTML 文件：
- en: '*nginx.yaml*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*nginx.yaml*'
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: It takes two steps to get the persistent storage mounted into our container.
    First, we declare a `volume` named `html` ➋ that references the PersistentVolumeClaim
    we created. This makes the storage available in the Pod. Next, we declare a `volumeMount`
    ➊ to specify where in the container’s filesystem this particular volume should
    appear. The advantage of having these two separate steps is that we can mount
    the same volume in multiple containers within the same Pod, which enables us to
    share data between processes using files even for cases in which the processes
    come from separate container images.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 将持久存储挂载到容器中需要两个步骤。首先，我们声明一个名为 `html` ➋ 的 `volume`，该卷引用我们创建的 PersistentVolumeClaim。这样，存储就可以在
    Pod 中使用。接下来，我们声明一个 `volumeMount` ➊ 来指定这个特定的卷应该出现在容器的文件系统中的位置。将这两个步骤分开的好处是，我们可以在同一个
    Pod 中的多个容器中挂载相同的卷，这使得我们能够在使用文件的情况下，即使这些进程来自不同的容器镜像，也能在进程之间共享数据。
- en: This capability allows for some interesting use cases. For example, suppose
    that we’re building a web application that includes some static content. We might
    deploy an NGINX web server to serve that content, as we’re doing here. At the
    same time, we also need a way to update the content. We might do that by having
    an additional container in the Pod that periodically checks for new content and
    updates a persistent volume that is shared with the NGINX container.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这一功能允许一些有趣的用例。例如，假设我们正在构建一个包含一些静态内容的 web 应用程序。我们可能会部署一个 NGINX 网络服务器来提供这些内容，正如我们在这里所做的那样。同时，我们还需要一种更新内容的方法。我们可以通过在
    Pod 中添加一个额外的容器，让它定期检查新内容，并更新与 NGINX 容器共享的持久卷。
- en: 'Let’s create the NGINX Deployment so that we can demonstrate that HTML files
    can be served from the persistent storage. The persistent storage will start empty,
    so at first there won’t be any web content to serve. Let’s see how NGINX behaves
    in that case:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建 NGINX Deployment，以便我们能够展示如何从持久存储中提供 HTML 文件。持久存储将会为空，因此最初不会有任何网络内容可供提供。让我们看看
    NGINX 在这种情况下会如何表现：
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As soon as the NGINX server is up and running, we need to grab its IP address
    so that we can make an HTTP request using `curl`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 NGINX 服务器启动并运行，我们需要获取它的 IP 地址，以便使用 `curl` 发出 HTTP 请求：
- en: '[PRE24]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: To grab the IP address in this case, we use the `jsonpath` output format for
    `kubectl` rather than use `jq` to filter JSON output; `jsonpath` has a very useful
    syntax for searching into a JSON object and pulling out a single uniquely named
    field (in this example, `podIP`). We could use a `jq` filter similar to what we
    did in [Chapter 8](ch08.xhtml#ch08), but the `jq` syntax for recursion is more
    complex.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，为了获取 IP 地址，我们使用 `kubectl` 的 `jsonpath` 输出格式，而不是使用 `jq` 来过滤 JSON 输出；`jsonpath`
    提供了一个非常有用的语法，可以在 JSON 对象中进行搜索并提取单个唯一命名的字段（在这个例子中是 `podIP`）。我们也可以使用类似于在[第 8 章](ch08.xhtml#ch08)中做的
    `jq` 过滤器，但 `jq` 的递归语法更为复杂。
- en: After we have the IP, we use `curl` to contact NGINX. As expected, we don’t
    see an HTML response, because our persistent storage is empty. However, we know
    that our volume mounted correctly because in this case we don’t even see the default
    NGINX welcome page.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 获取到 IP 地址后，我们使用 `curl` 来联系 NGINX。正如预期的那样，我们没有看到 HTML 响应，因为我们的持久存储是空的。然而，我们知道我们的卷已经正确挂载，因为在这种情况下，我们甚至没有看到默认的
    NGINX 欢迎页面。
- en: 'Let’s copy in an *index.html* file to give our NGINX server something to serve:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们复制一个 *index.html* 文件，以便给我们的 NGINX 服务器提供一些内容：
- en: '[PRE25]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'First, we capture the name of the Pod as randomly generated by the Deployment
    and then we use `kubectl cp` to copy in an HTML file. If we try running `curl`
    again, we’ll see a much better response:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们捕获由部署随机生成的 Pod 名称，然后使用 `kubectl cp` 将一个 HTML 文件复制进去。如果我们再次运行 `curl`，我们将看到一个更好的响应：
- en: '[PRE26]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Because this is persistent storage, this HTML content will remain available
    even if we delete the Deployment and create it again.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是持久存储，所以即使我们删除并重新创建部署，这些 HTML 内容仍然可用。
- en: 'However, we still have one significant problem to overcome. One of the primary
    reasons to have a Deployment is to be able to scale to multiple Pod instances.
    Scaling this Deployment makes a lot of sense, as we could have multiple Pod instances
    serving the same HTML content. Unfortunately, scaling won’t currently work:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们仍然有一个重要的问题需要解决。进行部署的主要原因之一是能够扩展到多个 Pod 实例。扩展这个部署是非常有意义的，因为我们可以有多个 Pod 实例来提供相同的
    HTML 内容。不幸的是，目前扩展无法正常工作：
- en: '[PRE27]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The Deployment appears to scale, but if we look at the Pods, we will see that
    we don’t really have multiple running instances:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 部署似乎已经扩展，但如果我们查看 Pod，我们会发现我们并没有真正拥有多个运行中的实例：
- en: '[PRE28]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The two new instances are stuck in `ContainerCreating`. Let’s examine one of
    those two Pods to see why:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个新实例卡在了 `ContainerCreating` 状态。让我们检查其中一个 Pod，看看原因：
- en: '[PRE29]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The first Pod we created has claimed the volume, and no other Pods can attach
    to it, so they are stuck in a `Pending` state. Even worse, this doesn’t just prevent
    scaling, it also prevents upgrading or making other configuration changes to the
    Deployment. If we update the Deployment configuration, Kubernetes will try to
    start a Pod using the new configuration before shutting down any old Pods. The
    new Pods can’t attach to the volume and therefore can’t start, so the old Pod
    will never be cleaned up and the configuration change will never take place.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的第一个 Pod 已经占用了该卷，其他 Pod 无法附加到它，因此它们卡在了 `Pending` 状态。更糟糕的是，这不仅阻止了扩展，还阻止了升级或对部署进行其他配置更改。如果我们更新部署配置，Kubernetes
    会尝试在关闭任何旧的 Pod 之前使用新配置启动一个 Pod。新的 Pod 无法附加到卷，因此无法启动，这样旧的 Pod 就永远不会被清理，配置更改也永远不会生效。
- en: We could force a Pod update in a couple ways. First, we could manually delete
    and re-create the Deployment anytime we made changes. Second, we could configure
    Kubernetes to delete the old Pod first by using a `Recreate` update strategy.
    We explore update strategy options in greater detail in [Chapter 20](ch20.xhtml#ch20).
    For now, it’s worth noting that this still would not allow us to scale the Deployment.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过几种方式强制更新 Pod。首先，每次我们做出更改时，可以手动删除并重新创建部署。其次，我们可以配置 Kubernetes 使用 `Recreate`
    更新策略，在删除旧的 Pod 之前先删除它。我们将在[第 20 章](ch20.xhtml#ch20)中更详细地探讨更新策略选项。目前值得注意的是，这仍然无法让我们扩展部署。
- en: If we want to fix this so that we can scale the Deployment, we’ll need to allow
    multiple Pods to attach to the volume at the same time. We can do this by changing
    the access mode for the persistent volume.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想修复这个问题，以便能够扩展部署，我们需要允许多个 Pod 同时附加到持久卷。我们可以通过更改持久卷的访问模式来实现这一点。
- en: Access Modes
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 访问模式
- en: Kubernetes is refusing to attach multiple Pods to the same persistent volume
    because we configured the PersistentVolumeClaim with an access mode of `ReadWriteOnce`.
    An alternate access mode, `ReadWriteMany`, will allow all of the NGINX server
    Pods to mount the storage simultaneously. Only some storage drivers support the
    `ReadWriteMany` access mode, because it requires the ability to manage simultaneous
    changes to files, including communicating changes dynamically to all of the nodes
    in the cluster.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 拒绝将多个 Pod 附加到同一个持久卷，因为我们将 PersistentVolumeClaim 配置为 `ReadWriteOnce`
    的访问模式。另一种访问模式 `ReadWriteMany` 将允许所有 NGINX 服务器 Pod 同时挂载存储。只有一些存储驱动程序支持 `ReadWriteMany`
    访问模式，因为它要求能够管理文件的同时更改，包括动态地将更改传递给集群中的所有节点。
- en: 'Longhorn does support `ReadWriteMany`, so creating a PersistentVolumeClaim
    with `ReadWriteMany` access mode is an easy change:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Longhorn 确实支持 `ReadWriteMany`，因此创建一个具有 `ReadWriteMany` 访问模式的 PersistentVolumeClaim
    是一个简单的变更：
- en: '*pvc-rwx.yaml*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*pvc-rwx.yaml*'
- en: '[PRE30]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Unfortunately, we can’t modify our existing PersistentVolumeClaim to change
    the access mode. And we can’t delete the PersistentVolumeClaim while the storage
    is in use by our Deployment. So we need to clean up everything and then deploy
    again:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们无法修改现有的 PersistentVolumeClaim 来更改访问模式。并且在存储仍然被我们的 Deployment 使用时，无法删除
    PersistentVolumeClaim。所以我们需要清理所有内容，然后重新部署：
- en: '[PRE31]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We specify `deploy/nginx` and `pvc/storage` as the resources to delete. This
    style of identifying the resources allows us to operate on two resources in the
    same command.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定 `deploy/nginx` 和 `pvc/storage` 作为要删除的资源。这种标识资源的方式允许我们在同一个命令中操作两个资源。
- en: 'After a minute or so, the new NGINX Pod will be running:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 大约一分钟后，新的 NGINX Pod 将开始运行：
- en: '[PRE32]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'At this point, we need to copy our HTML content over again because deleting
    the PersistentVolumeClaim deleted the previous storage:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 到这个时候，我们需要再次复制 HTML 内容，因为删除 PersistentVolumeClaim 会删除之前的存储：
- en: '[PRE33]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This time, when we scale our NGINX Deployment, the additional two Pods are
    able to mount the storage and start running:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，当我们扩展 NGINX 部署时，额外的两个 Pod 能够挂载存储并开始运行：
- en: '[PRE34]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'All three NGINX Pods are serving the same content, as we can see if we fetch
    the IP address for one of the new Pods and connect to it:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三个 NGINX Pod 都在提供相同的内容，如果我们获取其中一个新 Pod 的 IP 地址并连接到它，就能看到这一点：
- en: '[PRE35]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: At this point, we could use any NGINX Pod to update the HTML content and all
    Pods would serve the new content. We could even use a separate CronJob with an
    application component that updates the content dynamically, and NGINX would happily
    serve whatever files are in place.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以使用任何一个 NGINX Pod 来更新 HTML 内容，所有 Pod 都会提供新的内容。我们甚至可以使用一个单独的 CronJob，并配合一个动态更新内容的应用组件，NGINX
    会很高兴地提供任何当前的文件。
- en: Final Thoughts
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最后的想法
- en: Persistent storage is an essential requirement for building a fully functioning
    application. After a cluster administrator has configured one or more storage
    classes, it’s easy for application developers to dynamically request persistent
    storage as part of their application deployment. In most cases, the best way to
    do this is with a StatefulSet, as Kubernetes will automatically handle allocating
    independent storage for each Pod and will maintain a one-to-one relationship between
    Pod and storage during failover and upgrades.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 持久存储是构建一个完全功能的应用程序的基本需求。在集群管理员配置了一个或多个存储类之后，应用程序开发人员可以轻松地将持久存储作为其应用部署的一部分动态请求。在大多数情况下，最好的方法是使用
    StatefulSet，因为 Kubernetes 会自动为每个 Pod 分配独立的存储，并在故障转移和升级过程中保持 Pod 与存储之间的一对一关系。
- en: At the same time, there are other storage use cases, such as having multiple
    Pods access the same storage. We can easily handle those use cases by directly
    creating a PersistentVolumeClaim resource and then declaring it as a volume in
    a controller such as a Deployment or Job.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，还有其他存储使用场景，比如多个 Pod 访问相同的存储。我们可以通过直接创建一个 PersistentVolumeClaim 资源，然后在像
    Deployment 或 Job 这样的控制器中声明它作为一个卷，轻松处理这些场景。
- en: Although persistent storage is an effective way to make file content available
    to containers, Kubernetes has other powerful resource types that can store configuration
    data and pass it to containers as either environment variables or file content.
    In the next chapter, we’ll explore how to manage application configuration and
    secrets.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然持久存储是让文件内容对容器可用的有效方式，但 Kubernetes 还有其他强大的资源类型，可以存储配置数据并将其传递给容器，作为环境变量或文件内容。在下一章中，我们将探索如何管理应用程序配置和机密。
