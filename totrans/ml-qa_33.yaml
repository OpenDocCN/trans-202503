- en: '**28'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: THE K IN K-FOLD CROSS-VALIDATION**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*k*-fold cross-validation is a common choice for evaluating machine learning
    classifiers because it lets us use all training data to simulate how well a machine
    learning algorithm might perform on new data. What are the advantages and disadvantages
    of choosing a large *k*?'
  prefs: []
  type: TYPE_NORMAL
- en: We can think of *k*-fold cross-validation as a workaround for model evaluation
    when we have limited data. In machine learning model evaluation, we care about
    the generalization performance of our model, that is, how well it performs on
    new data. In *k*-fold cross-validation, we use the training data for model selection
    and evaluation by partitioning it into *k* validation rounds and folds. If we
    have *k* folds, we have *k* iterations, leading to *k* different models, as illustrated
    in [Figure 28-1](ch28.xhtml#ch28fig1).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/28fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 28-1: An example of* k *-fold cross-validation for model evaluation
    where* k *= 5*'
  prefs: []
  type: TYPE_NORMAL
- en: Using *k*-fold cross-validation, we usually evaluate the performance of a particular
    hyperparameter configuration by computing the average performance over the *k*
    models. This performance reflects or approximates the performance of a model trained
    on the complete training dataset after evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections cover the trade-offs of selecting values for *k* in *k*-fold
    cross-validation and address the challenges of large *k* values and their computational
    demands, especially in deep learning contexts. We then discuss the core purposes
    of *k* and how to choose an appropriate value based on specific modeling needs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Trade-offs in Selecting Values for k**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If *k* is too large, the training sets are too similar between the different
    rounds of cross-validation. The *k* models are thus very similar to the model
    we obtain by training on the whole training set. In this case, we can still leverage
    the advantage of *k*-fold cross-validation: evaluating the performance for the
    entire training set via the held-out validation fold in each round. (Here, we
    obtain the training set by concatenating all *k* – 1 training folds in a given
    iteration.) However, a disadvantage of a large *k* is that it is more challenging
    to analyze how the machine learning algorithm with the particular choice of hyperparameter
    setting behaves on different training datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Besides the issue of too-similar datasets, running *k*-fold cross-validation
    with a large value of *k* is also computationally more demanding. A larger *k*
    is more expensive since it increases both the number of iterations and the training
    set size at each iteration. This is especially problematic if we work with relatively
    large models that are expensive to train, such as contemporary deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: A common choice for *k* is typically 5 or 10, for practical and historical reasons.
    A study by Ron Kohavi (see “[References](ch28.xhtml#ch00lev143)” at the end of
    this chapter) found that *k* = 10 offers a good bias and variance trade-off for
    classical machine learning algorithms, such as decision trees and naive Bayes
    classifiers, on a handful of small datasets.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in 10-fold cross-validation, we use 9/10 (90 percent) of the data
    for training in each round, whereas in 5-fold cross-validation, we use only 4/5
    (80 percent) of the data, as shown in [Figure 28-2](ch28.xhtml#ch28fig2).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/28fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 28-2: A comparison of 5-fold and 10-fold cross-validation*'
  prefs: []
  type: TYPE_NORMAL
- en: However, this does not mean large training sets are bad, since they can reduce
    the pessimistic bias of the performance estimate (mostly a good thing) if we assume
    that the model training can benefit from more training data. (See [Figure 5-1](ch05.xhtml#ch5fig1)
    on [page 24](ch05.xhtml#ch5fig1) for an example of a learning curve.)
  prefs: []
  type: TYPE_NORMAL
- en: In practice, both a very small and a very large *k* may increase variance. For
    instance, a larger *k* makes the training folds more similar to each other since
    a smaller proportion is left for the held-out validation sets. Since the training
    folds are more similar, the models in each round will be more similar. In practice,
    we may observe that the variance of the held-out validation fold scores is more
    similar for larger values of *k*. On the other hand, when *k* is large, the validation
    sets are small, so they may contain more random noise or be more susceptible to
    quirks of the data, leading to more variation in the validation scores across
    the different folds. Even though the models themselves are more similar (since
    the training sets are more similar), the validation scores may be more sensitive
    to the particularities of the small validation sets, leading to higher variance
    in the overall cross-validation score.
  prefs: []
  type: TYPE_NORMAL
- en: '**Determining Appropriate Values for k**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When deciding upon an appropriate value of *k*, we are often guided by computational
    performance and conventions. However, it’s worthwhile to define the purpose and
    context of using *k*-fold cross-validation. For example, if we care primarily
    about approximating the predictive performance of the final model, using a large
    *k* makes sense. This way, the training folds are very similar to the combined
    training dataset, yet we still get to evaluate the model on all data points via
    the validation folds.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if we care to evaluate how sensitive a given hyperparameter
    configuration and training pipeline is to different training datasets, then choosing
    a smaller number for *k* makes more sense.
  prefs: []
  type: TYPE_NORMAL
- en: Since most practical scenarios consist of two steps—tuning hyperparameters and
    evaluating the performance of a model—we can also consider a two-step procedure.
    For instance, we can use a smaller *k* during hyperparameter tuning. This will
    help speed up the hyperparameter search and probe the hyperparameter configurations
    for robustness (in addition to the average performance, we can also consider the
    variance as a selection criterion). Then, after hyperparameter tuning and selection,
    we can increase the value of *k* to evaluate the model.
  prefs: []
  type: TYPE_NORMAL
- en: However, reusing the same dataset for model selection and evaluation introduces
    biases, and it is usually better to use a separate test set for model evaluation.
    Also, nested cross-validation may be preferred as an alternative to *k*-fold cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**28-1.** Suppose we want to provide a model with as much training data as
    possible. We consider using *leave-one-out cross-validation (LOOCV)*, a special
    case of *k*-fold cross-validation where *k* is equal to the number of training
    examples, such that the validation folds contain only a single data point. A colleague
    mentions that LOOCV is defective for discontinuous loss functions and performance
    measures such as classification accuracy. For instance, for a validation fold
    consisting of only one example, the accuracy is always either 0 (0 percent) or
    1 (99 percent). Is this really a problem?'
  prefs: []
  type: TYPE_NORMAL
- en: '**28-2.** This chapter discussed model selection and model evaluation as two
    use cases of *k*-fold cross-validation. Can you think of other use cases?'
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For a longer and more detailed explanation of why and how to use *k*-fold cross-validation,
    see my article: “Model Evaluation, Model Selection, and Algorithm Selection in
    Machine Learning” (2018), *[https://arxiv.org/abs/1811.12808](https://arxiv.org/abs/1811.12808)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper that popularized the recommendation of choosing *k* = 5 and *k* =
    10: Ron Kohavi, “A Study of Cross-Validation and Bootstrap for Accuracy Estimation
    and Model Selection” (1995), *[https://dl.acm.org/doi/10.5555/1643031.1643047](https://dl.acm.org/doi/10.5555/1643031.1643047)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
