## 第三十三章：线程：更多细节

本章提供了关于 POSIX 线程的各个方面的更多细节。我们讨论了线程与传统 UNIX API 的交互，特别是信号和进程控制原语（*fork()*, *exec()*, 和 *_exit()*）。我们还概述了 Linux 上可用的两种 POSIX 线程实现——LinuxThreads 和 NPTL，并指出这些实现在哪些方面偏离了 SUSv3 规范中的 Pthreads。

## 线程栈

每个线程都有自己的栈，其大小在创建线程时就已固定。在 Linux/x86-32 上，除主线程外，所有线程的默认栈大小为 2 MB。（在一些 64 位架构上，默认大小较大；例如，在 IA-64 上为 32 MB。）主线程有一个更大的栈空间用于栈增长（参考 图 29-1，以及 Pthreads API 背景细节）。

有时，改变线程栈的大小是有用的。*pthread_attr_setstacksize()* 函数设置一个线程属性（线程属性），它决定了使用线程属性对象创建的线程的栈大小。相关的 *pthread_attr_setstack()* 函数可用于控制栈的大小和位置，但设置栈的位置可能会降低应用程序的可移植性。手册页提供了这些函数的详细信息。

改变每个线程栈大小的一个原因是为分配大规模自动变量或进行深度嵌套函数调用（可能是由于递归）的线程提供更大的栈空间。或者，应用程序可能希望减少每个线程栈的大小，以便在一个进程内创建更多的线程。例如，在 x86-32 上，用户可访问的虚拟地址空间为 3 GB，默认的栈大小为 2 MB，这意味着我们最多可以创建大约 1500 个线程。（精确的最大值取决于文本段、数据段、共享库等占用的虚拟内存量。）可以通过调用 *sysconf(_SC_THREAD_STACK_MIN)* 来确定某个架构上可使用的最小栈大小。对于 Linux/x86-32 上的 NPTL 实现，该调用返回值为 16,384。

### 注意

在 NPTL 线程实现下，如果堆栈大小资源限制（`RLIMIT_STACK`）设置为非 *无限*，则在创建新线程时会使用该值作为默认堆栈大小。此限制必须在程序执行之前设置，通常通过在执行程序之前使用 *ulimit -s* shell 内置命令（在 C shell 中为 *limit stacksize*）来设置。仅在主程序中使用 *setrlimit()* 设置限制是不足够的，因为 NPTL 在运行时初始化时会在调用 *main()* 之前确定默认堆栈大小。

## 线程与信号

UNIX 信号模型是在 UNIX 进程模型的基础上设计的，并且比 Pthreads 的出现早了几十年。因此，信号模型与线程模型之间存在一些显著的冲突。这些冲突主要源于需要保持传统单线程进程的信号语义（即，传统程序的信号语义不应因 Pthreads 的引入而改变），同时开发一个可以在多线程进程中使用的信号模型。

信号模型与线程模型之间的差异意味着将信号与线程结合起来是复杂的，并且应尽可能避免。然而，有时我们不得不在多线程程序中处理信号。在本节中，我们讨论线程与信号之间的交互，并描述在处理信号的多线程程序中有用的各种函数。

### UNIX 信号模型如何映射到线程

为了理解 UNIX 信号如何映射到 Pthreads 模型，我们需要知道哪些信号模型的方面是进程范围的（即进程中的所有线程共享的），哪些方面是特定于进程中单个线程的。以下列表总结了关键点：

+   信号动作是进程范围的。如果任何未处理的信号，其默认动作是 *停止* 或 *终止*，并被发送到进程中的任何线程，那么该进程中的所有线程都将被停止或终止。

+   信号的处理方式是进程范围的；进程中的所有线程共享每个信号的相同处理方式。如果某个线程使用 *sigaction()* 设置了一个处理程序，例如 `SIGINT`，那么该处理程序可能会从任何接收到 `SIGINT` 信号的线程中调用。同样，如果某个线程将信号的处理方式设置为 *忽略*，那么所有线程都会忽略该信号。

+   信号可以被发送到整个进程或特定的线程。如果信号是线程特定的，则其生成方式如下：

    +   它是由特定硬件指令在该线程上下文中执行的直接结果生成的（即，硬件生成的信号中描述的硬件异常：`SIGBUS`、`SIGFPE`、`SIGILL` 和 `SIGSEGV`）；

    +   它是一个 `SIGPIPE` 信号，当线程尝试写入一个破损的管道时生成；或者

    +   它是通过 *pthread_kill()* 或 *pthread_sigqueue()* 发送的，这些是允许一个线程向同一进程内的另一个线程发送信号的函数（描述请见 向线程发送信号）。

    所有由其他机制生成的信号都是进程级信号。例如，通过 *kill()* 或 *sigqueue()* 从另一个进程发送的信号；用户输入生成信号的终端特殊字符时产生的 `SIGINT` 和 `SIGTSTP` 信号；以及为软件事件生成的信号，如终端窗口调整大小时的 `SIGWINCH` 或定时器到期时的信号（例如 `SIGALRM`）。

+   当信号传递到一个已经建立了信号处理程序的多线程进程时，内核会任意选择该进程中的一个线程来传递信号，并在该线程中调用处理程序。这种行为与维持传统信号语义是一致的。对于一个进程来说，响应单一信号多次执行信号处理操作是没有意义的。

+   信号屏蔽是按线程进行的。（不存在一个针对多线程进程中所有线程的进程级信号屏蔽。）线程可以通过 *pthread_sigmask()*（Pthreads API 中定义的一个新函数）独立地屏蔽或解除屏蔽不同的信号。通过操作每个线程的信号屏蔽，应用程序可以控制哪个线程（或哪些线程）可以处理针对整个进程的信号。

+   内核维护着针对整个进程和每个线程的待处理信号记录。调用 *sigpending()* 返回待处理的进程信号和调用线程的待处理信号的并集。在新创建的线程中，每个线程的待处理信号集合最初是空的。线程级信号只能传递给目标线程。如果该线程屏蔽了信号，则信号将保持待处理状态，直到线程解除屏蔽该信号（或终止）。

+   如果信号处理程序中断了对 *pthread_mutex_lock()* 的调用，那么该调用将始终自动重新启动。如果信号处理程序中断了对 *pthread_cond_wait()* 的调用，那么该调用要么自动重新启动（这就是 Linux 的行为），要么返回 0，表示发生了虚假唤醒（在这种情况下，设计良好的应用程序会重新检查相应的谓词并重新启动调用，详情请见 测试条件变量的谓词）。SUSv3 要求这两个函数按这里描述的方式行为。

+   替代信号堆栈是每个线程独立的（参考 sigaltstack()的描述")）。新创建的线程不会继承其创建者的替代信号堆栈。

### 注意

更精确地说，SUSv3 指定每个内核调度实体（KSE）都有一个单独的替代信号堆栈。在具有 1:1 线程实现的系统中（如 Linux），每个线程都有一个 KSE（参见线程实现模型）。

### 操作线程信号屏蔽字

当创建一个新线程时，它会继承创建它的线程的信号屏蔽字的副本。线程可以使用*pthread_sigmask()*来更改其信号屏蔽字，检索现有的屏蔽字，或同时执行这两者。

```
#include <signal.h>

int `pthread_sigmask`(int *how*, const sigset_t **set*, sigset_t **oldset*);
```

### 注意

成功时返回 0，出错时返回正数错误码

除了它操作线程信号屏蔽字这一事实，*pthread_sigmask()*的使用与*sigprocmask()*的使用相同（信号屏蔽字（阻塞信号传递））。

### 注意

SUSv3 指出，在多线程程序中使用*sigprocmask()*是未指定的。我们不能在多线程程序中便捷地使用*sigprocmask()*。在实践中，*sigprocmask()*和*pthread_sigmask()*在许多实现中是相同的，包括 Linux。

### 向线程发送信号

*pthread_kill()*函数将信号*sig*发送到同一进程中的另一个线程。目标线程由*thread*参数标识。

```
#include <signal.h>

int `pthread_kill`(pthread_t *thread*, int *sig*);
```

### 注意

成功时返回 0，出错时返回正数错误码

因为线程 ID 仅在进程内保证唯一（见线程 ID），所以我们不能使用*pthread_kill()*向另一个进程中的线程发送信号。

### 注意

*pthread_kill()*函数通过 Linux 特有的*tgkill(tgid, tid, sig)*系统调用实现，该调用将信号*sig*发送到由*tid*标识的线程（由*gettid()*返回的内核线程 ID）在由*tgid*标识的线程组内。有关详细信息，请参见*tgkill(2)*手册页。

Linux 特有的*pthread_sigqueue()*函数结合了*pthread_kill()*和*sigqueue()*的功能（队列中实时信号的数量限制）：它向同一进程中的另一个线程发送带有附加数据的信号。

```
#define _GNU_SOURCE@
#include <signal.h>

int `pthread_sigqueue`(pthread_t *thread*, int *sig*, const union sigval *value*);
```

### 注意

成功时返回 0，出错时返回正数错误码

与*pthread_kill()*类似，*sig*指定要发送的信号，*thread*标识目标线程。*value*参数指定信号的附加数据，并以与*sigqueue()*的等效参数相同的方式使用。

### 注意

*pthread_sigqueue()* 函数在 *glibc* 2.11 版本中添加，并需要内核的支持。这个支持由 *rt_tgsigqueueinfo()* 系统调用提供，该调用在 Linux 2.6.31 中引入。

### 理智地处理异步信号

在 第二十章 到 第二十二章 中，我们讨论了各种因素——如可重入问题、需要重新启动中断的系统调用以及避免竞争条件——这些问题使得通过信号处理程序处理异步生成的信号变得复杂。此外，Pthreads API 中的函数都不属于我们可以安全地从信号处理程序中调用的异步信号安全函数集合（可重入和异步信号安全函数）。因此，必须处理异步生成信号的多线程程序通常不应使用信号处理程序作为接收信号传递通知的机制。相反，推荐的方法是：

+   所有线程会阻塞进程可能接收到的所有异步信号。实现这一点最简单的方法是在创建任何其他线程之前，在主线程中阻塞这些信号。之后创建的每个线程将继承主线程的信号掩码副本。

+   创建一个专用线程，使用 *sigwaitinfo()*、*sigtimedwait()* 或 *sigwait()* 接受传入的信号。我们在 同步等待信号 中描述了 *sigwaitinfo()* 和 *sigtimedwait()*。下面我们将描述 *sigwait()*。

这种方法的优点是异步生成的信号以同步方式接收。当它接受到传入的信号时，专用线程可以安全地修改共享变量（在互斥锁控制下）并调用非异步信号安全函数。它还可以信号条件变量，并使用其他线程和进程通信与同步机制。

*sigwait()* 函数等待传递到 *set* 指向的信号集中的一个信号，接受该信号，并将其返回到 *sig* 中。

```
#include <signal.h>

int `sigwait`(const sigset_t **set*, int **sig*);
```

### 注意

成功时返回 0，出错时返回一个正的错误号码

*sigwait()* 的操作与 *sigwaitinfo()* 相同，不同之处在于：

+   *sigwait()* 不会返回描述信号的 *siginfo_t* 结构，而仅返回信号号码；并且

+   返回值与其他线程相关的函数一致（而不是传统 UNIX 系统调用返回的 0 或 -1）。

如果多个线程正在等待同一个信号并使用 *sigwait()*，当信号到达时，只有一个线程会实际接受到信号。哪个线程会接受信号是不确定的。

## 线程与进程控制

和信号机制一样，*exec()*, *fork()*, 和 *exit()* 在 Pthreads API 之前就已存在。在接下来的段落中，我们将提到一些关于在多线程程序中使用这些系统调用的细节。

#### 线程与 *exec()*

当任何线程调用 *exec()* 函数时，调用的程序会被完全替换。除调用 *exec()* 的线程外，所有线程都会立即消失。没有线程会执行线程特定数据的析构函数或调用清理处理程序。进程中所有的（进程私有）互斥锁和条件变量也会消失。在 *exec()* 之后，剩余线程的线程 ID 未指定。

#### 线程与 *fork()*

当一个多线程进程调用 *fork()* 时，只有调用线程会在子进程中被复制。（子进程中的线程 ID 与调用 *fork()* 的父进程中的线程 ID 相同。）子进程中所有其他线程都会消失；对于这些线程，不会执行线程特定数据的析构函数或清理处理程序。这可能会导致各种问题：

+   尽管只有调用线程会在子进程中被复制，但全局变量的状态以及所有 Pthreads 对象（如互斥锁和条件变量）都会在子进程中得到保留。（这是因为这些 Pthreads 对象分配在父进程的内存中，子进程会获得该内存的副本。）这可能会导致一些棘手的情况。例如，假设在执行 *fork()* 时，另一个线程已经锁定了一个互斥锁，并且正在部分更新一个全局数据结构。在这种情况下，子进程中的线程将无法解锁该互斥锁（因为它不是互斥锁的所有者），如果尝试获取互斥锁，则会阻塞。此外，子进程中全局数据结构的副本可能处于不一致的状态，因为正在更新它的线程在更新过程中消失了。

+   由于线程特定数据的析构函数和清理处理程序不会被调用，在多线程程序中执行 *fork()* 可能会导致子进程出现内存泄漏。此外，由其他线程创建的线程特定数据项可能无法在新子进程中的线程中访问，因为它没有指向这些数据项的指针。

由于这些问题，通常的建议是在多线程进程中，*fork()* 之后应立即执行 *exec()*。*exec()* 会导致子进程中的所有 Pthreads 对象消失，因为新程序会覆盖进程的内存。

对于必须使用 *fork()* 但不跟随 *exec()* 的程序，Pthreads API 提供了一种机制来定义 *fork handlers*。可以通过以下形式的 *pthread_atfork()* 调用来设置 fork 处理程序：

```
pthread_atfork(prepare_func, parent_func, child_func);
```

每次调用 *pthread_atfork()* 都会将 *prepare_func* 添加到一个函数列表中，这些函数将在调用 *fork()* 时，在创建新的子进程之前自动执行（按照注册顺序的逆序执行）。类似地，*parent_func* 和 *child_func* 会被添加到一个函数列表中，这些函数会在 *fork()* 返回之前，分别在父进程和子进程中自动调用（按照注册顺序执行）。

Fork 处理程序有时对于使用线程的库代码很有用。如果没有 fork 处理程序，库就无法处理那些天真地使用该库并调用 *fork()* 的应用程序，因它们没有意识到该库已经创建了一些线程。

由 *fork()* 创建的子进程会继承调用 *fork()* 的线程的 fork 处理程序。在 *exec()* 过程中，fork 处理程序不会被保留（它们不能被保留，因为处理程序的代码在 *exec()* 过程中会被覆盖）。

有关 fork 处理程序的更多细节，以及它们使用的示例，可以参考 [Butenhof, 1996]。

### 注意

在 Linux 上，如果一个使用 NPTL 线程库的程序调用了 *vfork()*，则不会调用 fork 处理程序。然而，在使用 LinuxThreads 的程序中，在这种情况下会调用 fork 处理程序。

#### 线程与 *exit()*

如果任何线程调用 *exit()* 或者主线程执行 `return`，所有线程会立即消失；不会执行任何线程特定的数据析构函数或清理处理程序。

## 线程实现模型

在这一节中，我们简要地讨论了三种不同的线程 API 实现模型。这为 Linux 实现 POSIX 线程 提供了有用的背景，后者我们将考虑 Linux 的线程实现。这些实现模型之间的区别在于线程如何映射到 *内核调度实体*（KSEs），这些实体是内核为其分配 CPU 和其他系统资源的单位。（在传统的 UNIX 实现中，线程出现之前，*内核调度实体* 这一术语与 *进程* 一词是同义的。）

#### 多对一（M:1）实现（用户级线程）

在 M:1 线程实现中，所有线程的创建、调度和同步（互斥锁、等待条件变量等）细节完全由用户空间的线程库处理。内核对于进程中多个线程的存在一无所知。

M:1 实现有一些优点。最大的优点是许多线程操作——例如，创建和终止线程、线程之间的上下文切换，以及互斥锁和条件变量操作——都很快速，因为不需要切换到内核模式。此外，由于不需要内核对线程库的支持，M:1 实现可以相对容易地从一个系统移植到另一个系统。

然而，M:1 实现存在一些严重的缺点：

+   当一个线程执行像*read()*这样的系统调用时，控制权从用户空间线程库转交给内核。这意味着，如果*read()*调用阻塞，进程中的所有线程都会被阻塞。

+   内核无法调度进程的线程。由于内核无法感知进程中多个线程的存在，它无法将不同的线程调度到多处理器硬件上的不同处理器上。也无法在不同进程之间有意义地为某个线程分配比其他进程中的线程更高的优先级，因为线程的调度完全由进程内处理。

#### 一对一（1:1）实现（内核级线程）

在 1:1 线程实现中，每个线程映射到一个独立的 KSE。内核单独处理每个线程的调度。线程同步操作通过系统调用实现。

1:1 实现消除了 M:1 实现所面临的缺点。一个阻塞的系统调用不会导致进程中的所有线程都被阻塞，内核可以将进程的线程调度到多处理器硬件上的不同 CPU。

然而，1:1 实现中的线程创建、上下文切换和同步等操作较慢，因为需要切换到内核模式。此外，对于包含大量线程的应用程序来说，维护每个线程的独立 KSE 所需的开销可能会对内核调度程序造成较大负担，进而降低整体系统性能。

尽管存在这些缺点，1:1 实现通常比 M:1 实现更受青睐。Linux 的两种线程实现——LinuxThreads 和 NPTL——都采用了 1:1 模型。

### 注意

在 NPTL 的开发过程中，进行了大量的工作来重写内核调度程序，并设计一种线程实现，能够高效地执行包含成千上万线程的多线程进程。随后的测试表明，这一目标已成功实现。

#### 多对多（M:N）实现（两级模型）

M:N 实现旨在结合 1:1 和 M:1 模型的优点，同时消除它们的缺点。

在 M:N 模型中，每个进程可以拥有多个关联的 KSE（内核线程实体），且每个 KSE 可以映射多个线程。这种设计使得内核可以将应用程序的线程分配到多个 CPU 上，同时避免了大量线程的应用程序可能面临的扩展问题。

M:N 模型的最大缺点是复杂性。线程调度的任务在内核和用户空间线程库之间共享，这两者必须合作并相互传递信息。在 M:N 实现中，根据 SUSv3 的要求管理信号也非常复杂。

### 注意

最初考虑过为 NPTL 线程实现采用 M:N 方案，但由于这种实现需要对内核进行广泛的修改，而这种修改可能并不必要，尤其是在 Linux 调度程序即使在处理大量 KSE 时也能很好地扩展的情况下，因此被 rejected。

## Linux 的 POSIX 线程实现

Linux 有两个主要的 Pthreads API 实现：

+   *LinuxThreads*：这是最初的 Linux 线程实现，由 Xavier Leroy 开发。

+   *NPTL（原生 POSIX 线程库）*：这是现代的 Linux 线程实现，由 Ulrich Drepper 和 Ingo Molnar 开发，作为 LinuxThreads 的继任者。NPTL 提供的性能优于 LinuxThreads，并且更严格遵循 SUSv3 对 Pthreads 的规范。NPTL 的支持需要对内核进行更改，这些更改出现在 Linux 2.6 中。

### 注意

曾经一度，LinuxThreads 的继任者被认为是另一种实现，名为下一代 POSIX 线程（NGPT），这是一种在 IBM 开发的线程实现。NGPT 使用 M:N 设计，性能明显优于 LinuxThreads。然而，NPTL 开发者决定追求新的实现。这种方法是有充分理由的——1:1 设计的 NPTL 被证明比 NGPT 性能更好。NPTL 发布后，NGPT 的开发被终止。

在接下来的章节中，我们将进一步讨论这两种实现的详细信息，并指出它们偏离 SUSv3 对 Pthreads 的要求的地方。

此时，值得强调的是，LinuxThreads 实现现在已经过时；在 *glibc* 2.4 及更高版本中不再支持。所有新的线程库开发仅在 NPTL 中进行。

### LinuxThreads

多年来，LinuxThreads 是 Linux 上主要的线程实现，它足以实现各种线程应用程序。LinuxThreads 实现的核心内容如下：

+   线程通过 *clone()* 调用创建，并指定以下标志：

    ```
    CLONE_VM | CLONE_FILES | CLONE_FS | CLONE_SIGHAND
    ```

    这意味着 LinuxThreads 线程共享虚拟内存、文件描述符、与文件系统相关的信息（如 umask、根目录和当前工作目录）以及信号处理。然而，线程之间不会共享进程 ID 和父进程 ID。

+   除了应用程序创建的线程外，LinuxThreads 还会创建一个额外的“管理”线程，负责处理线程的创建和终止。

+   该实现使用信号进行内部操作。对于支持实时信号的内核（Linux 2.2 及更高版本），使用前三个实时信号。对于旧版本的内核，则使用 `SIGUSR1` 和 `SIGUSR2`。应用程序不能使用这些信号。（使用信号会导致各种线程同步操作的高延迟。）

#### LinuxThreads 偏离指定行为

LinuxThreads 在若干方面不符合 SUSv3 关于 Pthreads 的规范。（LinuxThreads 的实现受到当时内核特性限制，在这些限制下，它是尽可能符合规范的。）以下列表总结了不符合的地方：

+   调用*getpid()*会返回进程中每个线程的不同值。调用*getppid()*则反映了除主线程外的每个线程都是由进程的管理线程创建的（即，*getppid()*返回管理线程的进程 ID）。其他线程中的*getppid()*调用应该返回与主线程中的*getppid()*调用相同的值。

+   如果一个线程使用*fork()*创建了一个子进程，那么其他任何线程都应该能够使用*wait()*（或类似方法）获取该子进程的终止状态。然而，事实并非如此；只有创建子进程的线程才能*wait()*它。

+   如果一个线程调用*exec()*，根据 SUSv3 的要求，所有其他线程都会被终止。然而，如果*exec()*是从主线程之外的任何线程调用的，那么结果进程将与调用线程具有相同的进程 ID——即，与主线程的进程 ID 不同。根据 SUSv3，进程 ID 应该与主线程的进程 ID 相同。

+   线程不共享凭据（用户和组 ID）。当一个多线程进程执行设置用户 ID 的程序时，这可能导致某些线程无法使用*pthread_kill()*向另一个线程发送信号，因为两个线程的凭据已发生变化，以至于发送线程不再有权限向目标线程发送信号（请参见图 20-2，在检查进程是否存在中）。此外，由于 LinuxThreads 实现内部使用信号，若线程更改其凭据，可能会导致各种 Pthreads 操作失败或挂起。

+   SUSv3 规范中关于线程与信号交互的各个方面并未得到遵守：

    +   使用*kill()*或*sigqueue()*发送到进程的信号应当被目标进程中的某个不阻塞该信号的线程接收并处理。然而，由于 LinuxThreads 线程具有不同的进程 ID，信号只能针对特定的线程。如果该线程正在阻塞信号，即便有其他线程未阻塞信号，该信号仍然会保持待处理状态。

    +   LinuxThreads 不支持针对整个进程的待处理信号的概念；仅支持每线程的待处理信号。

    +   如果信号被定向到包含多线程应用程序的进程组，那么该信号将由应用程序中的所有线程（即所有已建立信号处理程序的线程）处理，而不是由单个（任意）线程处理。例如，输入终端字符生成前台进程组的作业控制信号时，可能会生成这样的信号。

    +   备用信号栈设置（由 *sigaltstack()* 建立）是每个线程独立的。然而，由于新线程错误地从 *pthread_create()* 的调用者继承了备用信号栈设置，导致两个线程共享一个备用信号栈。SUSv3 规定，新线程应从没有定义备用信号栈开始。LinuxThreads 不符合此规范的后果是，如果两个线程恰好在相同时间处理不同信号，并且使用的是共享的备用信号栈，可能会导致混乱（例如，程序崩溃）。这个问题可能很难重现和调试，因为其发生依赖于两个信号同时处理的概率，这个事件可能较为罕见。

        ### 注意

        在使用 LinuxThreads 的程序中，新线程可以调用 *sigaltstack()* 来确保它使用与创建它的线程不同的备用信号栈（或根本不使用栈）。然而，便携式程序（以及创建线程的库函数）不会知道这样做，因为在其他实现中这并不是一个要求。此外，即使采用此技术，仍然可能会出现竞争条件：新线程可能会在有机会调用 *sigaltstack()* 之前，在备用栈上接收并处理信号。

+   线程之间不共享会话 ID 和进程组 ID。*setsid()* 和 *setpgid()* 系统调用不能用于改变多线程进程的会话或进程组成员身份。

+   使用 *fcntl()* 建立的记录锁不共享。同一类型的重叠锁请求不会合并。

+   线程不共享资源限制。SUSv3 指定资源限制是进程级别的属性。

+   *times()* 返回的 CPU 时间和 *getrusage()* 返回的资源使用信息是每个线程独立的。这些系统调用应返回进程级别的总和。

+   某些版本的 *ps(1)* 会将进程中的所有线程（包括管理线程）显示为具有不同进程 ID 的独立项。

+   线程不共享由 *setpriority()* 设置的 nice 值。

+   使用 *setitimer()* 创建的间隔定时器在线程之间不共享。

+   线程不共享 System V 信号量撤销（*semadj*）值。

#### LinuxThreads 的其他问题

除了上述偏离 SUSv3 的情况，LinuxThreads 实现还有以下问题：

+   如果管理线程被杀死，那么剩余的线程必须手动清理。

+   多线程程序的核心转储可能不会包含进程的所有线程（甚至可能不包括触发核心转储的线程）。

+   非标准的 *ioctl()* `TIOCNOTTY` 操作只有在主线程调用时，才能移除进程与控制终端的关联。

### NPTL

NPTL 旨在解决 LinuxThreads 的大多数不足之处，特别是：

+   NPTL 更加符合 SUSv3 对 Pthreads 的规范。

+   使用大量线程的应用程序在 NPTL 下的扩展性比在 LinuxThreads 下要好得多。

    ### 注意

    NPTL 允许应用程序创建大量线程。NPTL 的实现者能够运行创建 100,000 个线程的测试程序。而在 LinuxThreads 中，线程数量的实际限制为几千个。（诚然，极少数应用程序需要如此大量的线程。）

NPTL 的实现工作始于 2002 年，并在接下来的一年左右取得了进展。同时，Linux 内核也进行了各种修改，以适应 NPTL 的需求。Linux 2.6 内核中为支持 NPTL 所做的修改包括：

+   对线程组实现的改进（示例程序）；

+   添加了 futexes 作为一种同步机制（futexes 是一种通用机制，不仅仅为 NPTL 设计）；

+   新增系统调用 (*get_thread_area()* 和 *set_thread_area()*) 以支持线程局部存储；

+   支持线程化核心转储和多线程进程的调试；

+   为支持与 Pthreads 模型一致的信号管理，进行了修改；

+   新增 *exit_group()* 系统调用，用于终止进程中的所有线程（从 *glibc* 2.3 开始，*_exit()* —— 也就是 *exit()* 库函数 —— 被重定向为调用 *exit_group()* 的包装函数，而调用 *pthread_exit()* 则会调用内核中的真正 *_exit()* 系统调用，只终止调用线程）；

+   内核调度器的重写，允许高效地调度大量（即数千个）KSE。

+   改进了内核的进程终止代码的性能；

+   对 *clone()* 系统调用的扩展（The *clone()* System Call System Call")）。

NPTL 实现的要点如下：

+   线程是通过 *clone()* 调用创建的，并指定以下标志：

    ```
    CLONE_VM | CLONE_FILES | CLONE_FS | CLONE_SIGHAND |
    CLONE_THREAD | CLONE_SETTLS | CLONE_PARENT_SETTID |
    CLONE_CHILD_CLEARTID | CLONE_SYSVSEM
    ```

    NPTL 线程共享 LinuxThreads 线程共享的所有信息，甚至更多。`CLONE_THREAD` 标志表示线程被放入与其创建者相同的线程组，并共享相同的进程 ID 和父进程 ID。`CLONE_SYSVSEM` 标志表示线程与其创建者共享 System V 信号量撤销值。

    ### 注意

    当我们使用 *ps(1)* 列出一个在 NPTL 下运行的多线程进程时，我们只看到一行输出。要查看进程内线程的信息，我们可以使用 *ps -L* 选项。

+   该实现内部使用前两个实时信号。应用程序无法使用这些信号。

    ### 注意

    其中一个信号用于实现线程取消。另一个信号则作为确保进程中所有线程具有相同用户和组 ID 的技术的一部分。此技术是必需的，因为在内核层面，线程具有不同的用户和组凭证。因此，NPTL 实现会在每个系统调用的包装函数中执行一些工作，这些系统调用会更改用户和组 ID（如*setuid()*、*setresuid()* 等及其组相关调用），并导致进程中所有线程的 ID 都发生变化。

+   与 LinuxThreads 不同，NPTL 不使用管理线程。

#### NPTL 标准兼容性

这些变化意味着 NPTL 实现比 LinuxThreads 更加符合 SUSv3 标准。目前为止，以下不兼容问题仍然存在：

+   线程之间不会共享 nice 值。

在早期的 2.6.*x* 内核中，存在一些额外的 NPTL 不兼容问题：

+   在 2.6.16 之前的内核中，备用信号栈是按线程分配的，但新线程错误地继承了调用*pthread_create()*的线程设置的备用信号栈（通过*sigaltstack()*建立），因此两个线程共享了备用信号栈。

+   在 2.6.16 之前的内核中，只有线程组的领导者（即主线程）才能通过调用*setsid()*来启动一个新的会话。

+   在 2.6.16 之前的内核中，只有线程组的领导者才能使用*setpgid()*将宿主进程设置为进程组的领导者。

+   在 2.6.12 之前的内核中，使用*setitimer()*创建的间隔定时器不会在进程的线程之间共享。

+   在 2.6.10 之前的内核中，资源限制设置不会在进程的线程之间共享。

+   在 2.6.9 之前的内核中，*times()* 返回的 CPU 时间和 *getrusage()* 返回的资源使用信息是按线程计算的。

NPTL 旨在与 LinuxThreads 兼容 ABI。这意味着，针对提供 LinuxThreads 的 GNU C 库编译的程序无需重新链接即可使用 NPTL。然而，当程序在 NPTL 上运行时，某些行为可能会发生变化，主要是因为 NPTL 更加遵循 SUSv3 对 Pthreads 的规范。

### 选择哪种线程实现？

一些 Linux 发行版自带一个 GNU C 库，提供 LinuxThreads 和 NPTL，两者的默认选择由动态链接器根据系统运行的底层内核来决定。（这些发行版现在已经成为历史，因为从 2.4 版本开始，*glibc* 不再提供 LinuxThreads。）因此，我们有时需要回答以下问题：

+   在特定的 Linux 发行版中，哪种线程实现是可用的？

+   在提供 LinuxThreads 和 NPTL 的 Linux 发行版中，默认使用哪种实现，如何明确选择程序使用的实现？

#### 发现线程实现方式

我们可以使用一些技术来发现特定系统上可用的线程实现，或者在提供两种线程实现的系统上，发现默认使用的实现。

在提供 *glibc* 版本 2.3.2 或更高版本的系统上，我们可以使用以下命令来发现系统提供的线程实现，或者如果系统提供两种实现，则发现默认使用的实现：

```
$ `getconf GNU_LIBPTHREAD_VERSION`
```

在 NPTL 是唯一或默认实现的系统上，这将显示类似以下的字符串：

```
NPTL 2.3.4
```

### 注意

从 *glibc* 2.3.2 版本开始，程序可以通过使用 *confstr(3)* 获取 *glibc* 特定的 `_CS_GNU_LIBPTHREAD_VERSION` 配置变量的值，从而获得类似的信息。

在使用较旧的 GNU C 库的系统中，我们必须做更多的工作。首先，可以使用以下命令来显示运行程序时使用的 GNU C 库的路径名（这里我们以标准的 *ls* 程序为例，该程序位于 `/bin/ls`）：

```
$ `ldd /bin/ls | grep libc.so`
        libc.so.6 => /lib/tls/libc.so.6 (0x40050000)
```

### 注意

我们将在第 41.5 节中稍微介绍一下 *ldd*（列出动态依赖）程序。

GNU C 库的路径名会显示在 `=>` 后面。如果我们执行这个路径名作为命令，那么 *glibc* 会显示一系列关于它的信息。我们可以通过 *grep* 来筛选这些信息，以选择显示线程实现的那一行：

```
$ `/lib/tls/libc.so.6 | egrep -i 'threads|nptl'`
        Native POSIX Threads Library by Ulrich Drepper et al
```

我们在 *egrep* 正则表达式中包括 *nptl*，因为某些包含 NPTL 的 *glibc* 版本会显示如下字符串：

```
NPTL 0.61 by Ulrich Drepper
```

由于 *glibc* 的路径名可能因 Linux 发行版而异，我们可以使用 shell 命令替换，生成一个命令行来显示任何 Linux 系统上正在使用的线程实现，方法如下：

```
$ `$(ldd /bin/ls | grep libc.so | awk '{print $3}') | egrep -i 'threads|nptl'`
        Native POSIX Threads Library by Ulrich Drepper et al
```

#### 选择程序使用的线程实现方式

在提供 NPTL 和 LinuxThreads 的 Linux 系统中，有时需要显式地控制使用哪种线程实现。这个需求最常见的例子是，当我们有一个依赖于 LinuxThreads（可能是非标准行为）的旧程序时，我们希望强制程序使用该线程实现，而不是默认的 NPTL。

为此，我们可以使用一个动态链接器理解的特殊环境变量：`LD_ASSUME_KERNEL`。顾名思义，这个环境变量告诉动态链接器按某个特定的 Linux 内核版本运行。通过指定一个不支持 NPTL 的内核版本（例如，`2.2.5`），我们可以确保使用 LinuxThreads。因此，我们可以使用以下命令运行一个多线程程序并使用 LinuxThreads：

```
$ `LD_ASSUME_KERNEL=2.2.5 ./prog`
```

当我们将这个环境变量设置与前面描述的命令结合使用时，用来显示所使用的线程实现时，我们会看到类似以下内容：

```
$ `export LD_ASSUME_KERNEL=2.2.5`
$ `$(ldd /bin/ls | grep libc.so | awk '{print $3}') | egrep -i 'threads|nptl'`
        linuxthreads-0.10 by Xavier Leroy
```

### 注意

`LD_ASSUME_KERNEL`中可以指定的内核版本号范围有限。在多个同时提供 NPTL 和 LinuxThreads 的常见发行版中，指定版本号为 2.2.5 足以确保使用 LinuxThreads。有关此环境变量使用的更完整描述，请参见[`people.redhat.com/drepper/assumekernel.html`](http://people.redhat.com/drepper/assumekernel.html)。

## Pthreads API 的高级特性

Pthreads API 的一些高级功能包括以下内容：

+   *实时调度*：我们可以为线程设置实时调度策略和优先级。这类似于第 35.3 节中描述的进程实时调度系统调用。

+   *进程间共享互斥量和条件变量*：SUSv3 规定了一个选项，允许互斥量和条件变量在进程之间共享（而不仅仅是单一进程的线程之间共享）。在这种情况下，条件变量或互斥量必须位于进程之间共享的内存区域中。NPTL 支持此功能。

+   *高级线程同步原语*：这些设施包括屏障、读写锁和自旋锁。

所有这些功能的进一步细节可以在[Butenhof, 1996]中找到。

## 摘要

线程与信号配合不好；多线程应用程序设计应尽量避免使用信号。如果多线程应用程序必须处理异步信号，通常最干净的方式是阻塞所有线程中的信号，并让一个专门的线程使用*sigwait()*（或类似方法）接收传入的信号。然后，该线程可以安全地执行任务，如修改共享变量（在互斥量控制下）和调用非异步信号安全的函数。

Linux 上通常有两种线程实现：LinuxThreads 和 NPTL。LinuxThreads 在 Linux 上已有多年历史，但它在多个方面不符合 SUSv3 的要求，现在已经过时。更新的 NPTL 实现提供了更接近 SUSv3 的符合性和更优的性能，并且是现代 Linux 发行版中提供的实现。

#### 进一步的信息

请参阅 Summary 中列出的更多信息来源。

LinuxThreads 的作者在一个网页上记录了该实现，网址为[`pauillac.inria.fr/~xleroy/linuxthreads/`](http://pauillac.inria.fr/~xleroy/linuxthreads/)。NPTL 的实现由其开发者在一篇（现在有些过时的）论文中描述，该论文可以在线访问，地址为[`people.redhat.com/drepper/nptl-design.pdf`](http://people.redhat.com/drepper/nptl-design.pdf)。

## 练习

1.  编写一个程序来演示同一进程中的不同线程可以拥有不同的待处理信号集，如通过*sigpending()*返回的那样。你可以通过使用*pthread_kill()*向两个不同的线程发送不同的信号来实现这些信号的阻塞，然后让每个线程调用*sigpending()*并显示关于待处理信号的信息。（你可能会发现示例 20-4 中的函数很有用。）

1.  假设一个线程使用*fork()*创建了一个子进程。当子进程终止时，是否保证结果`SIGCHLD`信号会传递给调用*fork()*的线程（而不是进程中的其他线程）？
