- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensembles
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: Anyone can make mistakes, even algorithms. Sometimes we might be pretty sure
    that our algorithms are giving us good answers, but for any number of reasons,
    we might harbor a bit of doubt. How can we increase our confidence in what the
    computer tells us?
  prefs: []
  type: TYPE_NORMAL
- en: This isn’t a new problem. The Apollo spacecraft of the 1960s and ’70s relied
    on one type of computer in the command module, which orbited the moon, and a different
    type of computer in the lunar module, which landed there. These computers were
    a critical part of almost every maneuver, so it was essential that the astronauts
    could trust their outputs. The computers were built with integrated circuits,
    which were relatively new at the time. The astronauts trusted their lives to their
    software and hardware, but there was always room for doubt. How could they guard
    against errors or malfunctions that could end the mission or even prove fatal?
  prefs: []
  type: TYPE_NORMAL
- en: 'The designers of these computers addressed that problem with redundancy: every
    circuit board was duplicated, not once, but twice, producing three copies in all.
    All three systems always ran in synchrony, a technique called *triple modular
    redundancy*. The computers took the same inputs and computed their own independent
    results. The output of the group was decided by majority vote (Ceruzzi 2015).
    That way, if any one of the three systems got damaged, the right answer would
    still emerge.'
  prefs: []
  type: TYPE_NORMAL
- en: We can adopt and expand on this idea in machine learning. Like the Apollo engineers,
    we can make multiple learners and use them all simultaneously. In machine learning,
    groups of similar learners are called *ensembles*. And like the Apollo computers,
    the output of the ensemble is the most popular result from its members. But unlike
    Apollo’s identical software and hardware, we make each learner unique, usually
    by training it on slightly different data. This makes it unlikely that a mistake
    made by one learner will be made in exactly the same way by the others. In this
    way, the majority vote helps us weed out bad decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 11, we saw that decision trees easily overfit their training data,
    which can lead to errors when the system is deployed. In this chapter, we’ll see
    how to combine many such trees into an ensemble. The result is an algorithm that
    enjoys the simplicity and transparency of decision trees but greatly reduces their
    problems. Let’s begin with a brief discussion of how ensembles determine their
    final results.
  prefs: []
  type: TYPE_NORMAL
- en: Voting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Making decisions is hard for computers and humans alike. In some human societies,
    we deal with individual imperfections in decision-making by aggregating the opinions
    of many people. Laws are passed by senates, financial decisions are made by boards,
    and individual leaders are elected by popular vote. The thinking in all of these
    cases is that we can avoid errors in judgment that are unique to a single individual
    if we instead use the consensus of multiple, independent voters. Although this
    doesn’t guarantee good decisions, it can sometimes help avoid problems caused
    by any one person’s idiosyncrasies, biases, or bad judgment.
  prefs: []
  type: TYPE_NORMAL
- en: Machines have biases, too. When we use learning algorithms to make decisions,
    their predictions are based on the data that they trained on. If that data contained
    biases, omissions, underrepresentations, overrepresentations, or any other kind
    of systemic error, those errors are baked into the learner as well. This can have
    profound implications in the real world. For instance, when we use machine learning
    to evaluate loans for homes or businesses, to determine admissions to colleges,
    or to prescreen job applicants, any unfairness or bias in our training data causes
    similar unfairness and bias in the decisions the system makes, and bad decisions
    from the past are repeated in the present and propagated into the future.
  prefs: []
  type: TYPE_NORMAL
- en: One way to reduce the effects of these problems is to create multiple learners
    trained with different datasets. For example, we might train each system with
    a different training set from a different source. Since such data is often hard
    to come by in practice, often we train with different subsets drawn from a common
    pool of training data instead.
  prefs: []
  type: TYPE_NORMAL
- en: When we have trained a bunch of learners on these different datasets, we usually
    ask each one to evaluate each new input. Then we let the learners vote to determine
    a final result.
  prefs: []
  type: TYPE_NORMAL
- en: The typical way to do this is to use *plurality voting* (RangeVoting.org 2020).
    Put simply, each learner casts a single vote for its prediction, and whatever
    prediction receives the most votes is the winner (if there’s a tie, the computer
    can either randomly select one of the tied entries, or try another round of voting).
    Though plurality voting is not perfect, and there exist useful alternatives, it
    is simple, fast, and usually produces acceptable results in machine learning (NCSL
    2020).
  prefs: []
  type: TYPE_NORMAL
- en: A popular variation of plurality voting is *weighted plurality voting*. Here,
    every vote gets a certain *weight*, which is just a number that tells us how much
    influence that vote has on the result. Another variation is to ask each voter
    to identify their *confidence* in their decision, so more confident voters can
    have more impact than those that are less sure.
  prefs: []
  type: TYPE_NORMAL
- en: With those terms in place, let’s now dig into making an ensemble of decision
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: Ensembles of Decision Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A great way to build on the strengths of decision trees, while reducing their
    drawbacks, is to combine them into ensembles. To keep the following discussion
    specific, let’s focus on using decision trees for classification.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at three popular techniques for building decision tree ensembles
    that can significantly outperform their individual components.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ensemble technique called *bagging* is a portmanteau of *bootstrap aggregating*.
    As the name suggests, this technique is based on the bootstrap idea we saw in
    Chapter 2\. There, we saw how to use bootstrapping to estimate the quality of
    some statistical measure by evaluating lots of small subsets drawn from the starting
    data. In this case, let’s again create many small sets built from a training set,
    but now let’s use them to train a collection of decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with our original training set of samples, we can build multiple new
    sets, or *bootstraps*, by picking items from the original, using sampling with
    replacement. This means that it’s possible to pick the same sample more than once.
    [Figure 12-1](#figure12-1) shows the idea. Remember that each sample comes with
    its assigned class (shown by color) so we can train with it.
  prefs: []
  type: TYPE_NORMAL
- en: In the center of [Figure 12-1](#figure12-1), we have a starting set of eight
    samples, belonging to five classes. By selecting samples from this set, we can
    make many new sets, in this case of four samples each. This is the first step
    of bagging. Since we’re sampling with replacement, it’s possible that any given
    sample might appear multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s create a decision tree for every bootstrap and train it on that data.
    We call the collection of those trees an *ensemble*.
  prefs: []
  type: TYPE_NORMAL
- en: '![f12001](Images/f12001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-1: Creating three bootstraps (top and bottom) from a set of samples
    (center)'
  prefs: []
  type: TYPE_NORMAL
- en: When training is done and we’re evaluating a new sample, we give it to all the
    decision trees in the ensemble. Each tree produces one class prediction. We treat
    the predicted classes as votes in a plurality election, producing either a winner
    or a tie. Suppose that we have a small ensemble with just five trees. [Figure
    12-2](#figure12-2) shows the process of evaluating a new sample after deployment,
    assigning it to one of four lettered classes.
  prefs: []
  type: TYPE_NORMAL
- en: '![f12002](Images/f12002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-2: Using an ensemble to predict the class of a sample'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the left side of the figure. At the top, a new sample of unknown class
    arrives at our ensemble. The sample is given to every one of our decision trees
    (shown as triangles), and each one produces a predicted class, labeled A through
    D. Because each tree was trained on a different bootstrap, it’s a little different
    from all the others. On the right side of the figure, we run a plurality voting
    election with those predicted classes. In this example, the most popular class
    is B. That class wins, and is therefore the output of the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: 'We only need to specify two parameters to create this ensemble: how many samples
    we should use in each bootstrap, and how many trees we want to build. Analysis
    shows that adding more classifiers makes the ensemble’s predictions better, but
    at some point, adding more classifiers just makes it slower and the results stop
    improving. This is called *the law of diminishing returns in ensemble construction*.
    A good rule of thumb is to use about the same number of classifiers as classes
    of data (Bonab 2016), though we can use cross-validation to search for the best
    number of trees for any given dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we leave bagging, let’s consider a couple of techniques that build on
    the basic idea. The central idea of each is to add extra randomization to our
    trees during training.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we saw in Chapter 11, when it’s time to split a decision tree’s node in two,
    we can choose any feature (or set of features) to create the test that directs
    elements into one child or the other. If we choose to split based on just one
    feature, then we need to choose which feature we want to use and what value of
    that feature to test for. To compare different tests, we can use the measurements
    we saw in Chapter 11, such as information gain or the Gini impurity.
  prefs: []
  type: TYPE_NORMAL
- en: When building a decision tree, we often look for the best test by considering
    every feature. But we can also use a technique called *feature bagging.* Before
    looking for the best test at a node, we first choose a random subset of the features
    of the samples at that node, using selection without replacement. Now we’re ready
    to look for the best test, based only on those features. We don’t even consider
    splits based on the features we’re ignoring.
  prefs: []
  type: TYPE_NORMAL
- en: Later, when we decide to split another node, we again choose a brand-new subset
    of features and again determine our new split using only those. The idea is shown
    in [Figure 12-3](#figure12-3).
  prefs: []
  type: TYPE_NORMAL
- en: '![F12003](Images/F12003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-3: Determining which feature (f1 to f5) to use when splitting a node
    by feature bagging, shown for two nodes with five samples each'
  prefs: []
  type: TYPE_NORMAL
- en: On the left of [Figure 12-3](#figure12-3), we randomly select a set of three
    features from the five available, and search those for the best feature to split
    on. On the right we do it again, only we pick a different random set of three
    features, giving us a different test. The thinking here is that by randomly choosing
    only a few of the features, we can avoid making the same choice for this node
    in every tree we train, and thus we can increase the diversity of our decisions.
  prefs: []
  type: TYPE_NORMAL
- en: When we build ensembles this way, we call the result a *random forest*. The
    *random* part of the name refers to our random choice of features at each node,
    and the word *forest* refers to the resulting collection of decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a random forest, we need to provide the same two parameters that
    we used for bagging: the size of each bootstrap and the number of trees to build.
    We also have to specify what fraction of the features to consider at each node.
    We can express this as a percentage of the number of features in the node. Alternatively,
    many libraries offer a variety of algorithms that pick that percentage for us.'
  prefs: []
  type: TYPE_NORMAL
- en: Extra Trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s look at a second way to randomize our construction of trees when building
    an ensemble. Normally when we split a node, we consider each feature it contains
    (or a random subset of them if we’re building a random forest), and we find the
    value of that feature that best splits the samples at that node into two children.
    As we mentioned, we compare different possible tests using a measure like information
    gain.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of finding the best splitting point for each feature, let’s choose the
    splitting point randomly, based on the values that are in the node. The result
    of this change is an ensemble called *Extremely Randomized Trees*, or *Extra Trees*.
  prefs: []
  type: TYPE_NORMAL
- en: Although it may seem that this is destined to give us worse results for that
    tree, remember that decision trees are prone to overfitting. This random choice
    of the splitting point lets us trade off a little accuracy for reduced overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The techniques we just finished looking at are all specific to decision trees.
    Now let’s look at another method for building ensembles that is applicable to
    any kind of learner. This method is called *boosting* (Schapire 2012).
  prefs: []
  type: TYPE_NORMAL
- en: Boosting is a popular algorithm because it lets us combine a large number of
    small, fast, and inaccurate learners into a single accurate learner. To keep things
    concrete, let’s continue using decision trees as our example learners. We can
    make the discussion even simpler by focusing on binary classifiers, which assign
    every sample to one of only two classes. We’re going to build our ensemble out
    of simple classifiers that are just barely useful. To get going, let’s start with
    a thought experiment involving a completely useless classifier and then improve
    it just a little.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a dataset where the samples come from two classes. Also imagine a completely
    random binary classifier. Regardless of a sample’s features, the classifier assigns
    the sample to one of these two classes arbitrarily. If the samples are evenly
    split in the training set, we have a 50:50 chance of any sample being correctly
    labeled. We call this *random labeling*, because the odds of getting the right
    answer are up to chance.
  prefs: []
  type: TYPE_NORMAL
- en: Now suppose that we can tweak our binary classifier so that it does just barely
    better than chance. For example, [Figure 12-4](#figure12-4) shows a set of data
    of two classes, a binary classifier that is no better than chance, and a binary
    classifier that is just a tiny bit better than chance.
  prefs: []
  type: TYPE_NORMAL
- en: The learner in [Figure 12-4](#figure12-4)(b) is no better than chance, with
    half of each class getting incorrectly classified. This is a useless classifier.
    The learner in part (c) is just slightly less useless than the classifier in part
    (b) because the slight tilt in the boundary line means it does just a little better
    than the useless classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '![F12004](Images/F12004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-4: (a) Our training data. (b) A random classifier. (c) A terrible,
    but not quite random, classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: We call the classifier in [Figure 12-4](#figure12-4)(c) a *weak learner*. In
    this situation, a weak learner is any classifier that is even the slightest bit
    accurate. That is, it assigns the correct class more than 50 percent of the time,
    but perhaps only barely. The beauty of boosting is that we can use this weak learner
    as part of an ensemble that produces great results.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, a weak binary learner is just as useful to us even if it does *worse*
    than chance. That’s because we have only two classes. If a classifier is below
    chance (that is, it assigns the wrong class more frequently than the right one),
    then we can just swap the output classes, and then it’s doing better than chance,
    rather than worse. The conclusion is that as long as a binary learner isn’t completely
    random, we are able to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Weak classifiers are easy to make. Perhaps the most commonly used weak classifier
    is a decision tree that’s only one test deep. That is, the whole tree is made
    up of just a root node and its two children. This ridiculously small decision
    tree is often called a *decision stump*. Because it almost always does a better
    job than randomly assigning a class to each sample, it’s a fine example of a weak
    classifier. It’s small, fast, and a little better than random.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to a weak learner, a *strong learner* is a classifier that gets
    the correct label most of the time. The stronger the learner, the better its percentage
    of being right.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind boosting is to combine multiple weak classifiers into an ensemble
    that acts like a strong classifier. Note that our weakness condition is just a
    minimum threshold. We can combine lots of strong classifiers if we want to, though
    using weak ones is more common because they’re usually faster.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how boosting works with an example. [Figure 12-5](#figure12-5) shows
    a training set of samples that belong to two different classes.
  prefs: []
  type: TYPE_NORMAL
- en: What might be some good classifiers for this data? A fast and easy classifier
    just draws a straight line through the 2D dataset. We can see that no straight
    line is going to split up this data because the circular samples surround the
    square samples on three sides.
  prefs: []
  type: TYPE_NORMAL
- en: '![F12005](Images/F12005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-5: A collection of samples we’re going to classify using boosting'
  prefs: []
  type: TYPE_NORMAL
- en: Even though no single straight line can separate this data, we will see that
    multiple straight lines can, so let’s use straight lines as our weak classifiers.
    [Figure 12-6](#figure12-6) shows the boundary line for one such classifier. We’ll
    use A for the name of both the classifier and the boundary line it defines.
  prefs: []
  type: TYPE_NORMAL
- en: '![F12006](Images/F12006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-6: Placing one line called A into our samples cuts the two big clusters
    apart.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this classifier, everything on the side of A pointed to by the arrow is
    classified as square, and everything on the other side is classified as a circle.
    Using our measure of accuracy from Chapter 3, we find that the accuracy of this
    learner is given by (TP + TN) / (TP + TN + FP + FN) = (12 + 8) / (12 + 8 + 12
    + 2) = 20 / 34, or about 59 percent. That’s a nice example of a weak learner:
    it’s better than chance (50 percent), but not a lot better.'
  prefs: []
  type: TYPE_NORMAL
- en: To use boosting, we’ll want to add more lines (that is, additional weak learners)
    so that ultimately every region formed by the lines contains samples of a single
    class. After adding two more of these straight-line classifiers, we end up with
    [Figure 12-7](#figure12-7).
  prefs: []
  type: TYPE_NORMAL
- en: Line B has an accuracy of about 73 percent. C is terrible, with an accuracy
    of only about 12 percent. But as we noted before, that’s fine, because if we just
    swap the labels that C assigns (that is, just point the arrow in the other direction)
    it has an accuracy of about 88 percent!
  prefs: []
  type: TYPE_NORMAL
- en: The three lines, or boundaries, in [Figure 12-7](#figure12-7) together create
    seven nonoverlapping regions. The figure also shows that each region contains
    only one class of samples. By looking at these regions, we can see a way to determine
    the class of a sample using just the outputs of the three classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: '![F12007](Images/F12007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-7: Two more lines added to [Figure 12-6](#figure12-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s draw our three boundaries together. We will label each region with the
    learners that point toward it. The result is shown in [Figure 12-8](#figure12-8).
  prefs: []
  type: TYPE_NORMAL
- en: '![F12008](Images/F12008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-8: On the left, we show three lines named A, B, and C. On the right,
    each region is marked with the names of the learners that put that region on the
    positive side of their respective lines.'
  prefs: []
  type: TYPE_NORMAL
- en: When our classifiers get a new sample, normally they each return a class. Instead,
    let’s set them up to return a 1 if the sample is on the positive side of that
    classifier’s boundary (that is, the side pointed to by the arrow in [Figure 12-8](#figure12-8)),
    and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can add up all the contributions from all three learners in each cell.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider the region at the top center, marked C in [Figure 12-8](#figure12-8).
    It’s on the positive side of learner C, earning it a score of 1\. It’s on the
    negative side of both A and B, each of which thus contribute 0, so the sum of
    the three outputs is 1\. The region at the bottom, marked AB, gets 1 from learners
    A and B, and 0 from C, giving it a total of 2\. These scores are shown along with
    the other regions in [Figure 12-9](#figure12-9).
  prefs: []
  type: TYPE_NORMAL
- en: '![F12009](Images/F12009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-9: The composite score for each of the seven regions. Each letter
    in [Figure 12-8](#figure12-8) earns that region a 1.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve almost created our new classifier. There are two steps to go. First, we
    replace the 1 we arbitrarily assigned to each classifier’s output with a more
    useful value. Second, we find a threshold that turns the summed value in each
    region into a class.
  prefs: []
  type: TYPE_NORMAL
- en: Recall our discussion of weighted plurality voting from earlier in the chapter.
    If a region is on the positive side of a line, then that line is voting for that
    region. Rather than simply adding 1 from every classifier, we can assign each
    classifier its own voting weight. For example, if classifiers A, B, and C have
    voting weights 2, 3, and –4, and a point is on the positive side of A and C but
    not B, then A contributes 2, B contributes 0 (since the point is on the negative
    side of line B), and C contributes –4, for a total of 2 + 0 + –4 = –2.
  prefs: []
  type: TYPE_NORMAL
- en: The voting weight for each classifier is found for us by the boosting algorithm.
    Rather than going into those mechanics, let’s visualize the results of a specific
    set of weights for this dataset so we can see their effect.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 12-10](#figure12-10) we show the regions that are affected by the
    score for each learner. A dark region gets that learner’s value, while a light
    region does not (so the learner’s value in light regions is 0). Here we use the
    weights 1.0, 1.5, and −2 for A, B, and C, respectively. Recall that line C was
    pointing in the “wrong” direction. Giving a negative value to C’s weight has the
    effect of reversing the decisions from classifier C.
  prefs: []
  type: TYPE_NORMAL
- en: '![F12010](Images/F12010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-10: We assign a numerical value to each region that is classified
    as positive by each learner. The dark regions for each line get the weight associated
    with that line.'
  prefs: []
  type: TYPE_NORMAL
- en: The sums of all of these scores are shown in [Figure 12-11](#figure12-11). Blue
    regions have positive sums, and red regions have negative sums. These exactly
    correspond to where the circles and squares fell in our dataset. Any sample in
    a positive region is a square, and any sample in a negative region is a circle.
  prefs: []
  type: TYPE_NORMAL
- en: '![F12011](Images/F12011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-11: Adding up the scores from [Figure 12-10](#figure12-10)'
  prefs: []
  type: TYPE_NORMAL
- en: When a sample arrives, we send it to each classifier (that is, we test it against
    its corresponding line). For each classifier that finds the sample to be on the
    positive side of its line, we contribute its voting weight to a running sum. After
    adding up all the classifier outputs, we determine if the sample is positive or
    negative, which tells us which class the sample belongs to. We’ve correctly classified
    our data!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at another example. [Figure 12-12](#figure12-12) shows a new set
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: '![F12012](Images/F12012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-12: A set of data we’d like to classify using boosting'
  prefs: []
  type: TYPE_NORMAL
- en: For this data, let’s try using four learners. [Figure 12-13](#figure12-13) shows
    the four weak learners that a boosting algorithm might find in order to partition
    this data.
  prefs: []
  type: TYPE_NORMAL
- en: '![F12013](Images/F12013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-13: Four lines that let us classify the data of [Figure 12-12](#figure12-12)'
  prefs: []
  type: TYPE_NORMAL
- en: As before, the algorithm also assigns weights to these learners. Let’s illustrate
    the results using weights −8, 2, 3, and 4 for learners A, B, C, and D respectively.
    [Figure 12-14](#figure12-14) shows which regions have those weights added to their
    overall score. Light-colored regions implicitly receive a value of 0\.
  prefs: []
  type: TYPE_NORMAL
- en: '![F12014](Images/F12014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-14: The regions corresponding to each learner'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-15](#figure12-15) shows the sums of the contributions for each region.
    Again, a positive or negative sum distinguishes the two types of regions. We’ve
    found a way to combine four weak learners to correctly classify the points in
    [Figure 12-13](#figure12-13).'
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of boosting is that it takes classifiers that are simple and fast,
    but lousy, and by finding weights for them, it turns the ensemble into a single
    great classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '![F12015](Images/F12015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-15: The sums of the scores of each region from [Figure 12-14](#figure12-14).
    Positive regions are shown in blue, and they correctly classify the points in
    [Figure 12-13](#figure12-13).'
  prefs: []
  type: TYPE_NORMAL
- en: The only hyperparameter we need to provide is how many classifiers we want.
    In boosting, as in bagging, a good rule of thumb is to start with about as many
    classifiers as there are classes (Bonab 2016). That means that our earlier examples
    started on the high side, since we used three or four classifiers for only two
    classes. But as in so many things in machine learning, the best value is found
    by trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Boosting made its first appearance as part of an algorithm called *Adaboost*
    (Freund 1997; Schapire 2013). Although it can work with any learning algorithm,
    boosting has been particularly popular with decision trees. In fact, it works
    very well with the decision stumps we mentioned previously (these are trees that
    have only a root node and its two immediate children). We can think of the lines
    we used in Figures 12-7 and 12-13 as decision stumps since they have just one
    test: Is a sample on the side of the line pointed to by the arrow, or is it not?'
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that boosting is not a sure-fire way to improve all classification
    algorithms. The theory of boosting only covers binary classification, as in our
    earlier examples (Fumera 2008; Kak 2016). This is partly why boosting has been
    so popular and successful with decision tree classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensembles are collections of diversified learners. The general idea is that
    we gather up multiple learners of similar type, but trained on different data,
    and let them all evaluate an input. We then let them vote for the class each one
    determines, and the winner of the most votes is reported as the class for that
    input. The thinking is that any errors in the individual learners are essentially
    voted away by the class agreed upon by the others.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting is a way of using many weak learners as an ensemble to perform like
    a strong learner.
  prefs: []
  type: TYPE_NORMAL
- en: This discussion wraps up our discussion of machine learning techniques. Starting
    in Chapter 13, we look at the neural networks that power deep learning algorithms.
    We’ll see that the methods we introduced here are helpful in deep learning, because
    they help us understand our data and make the best choices of algorithms and networks
    to work with that data, and produce results that are useful to us.
  prefs: []
  type: TYPE_NORMAL
