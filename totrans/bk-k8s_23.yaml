- en: '20'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '20'
- en: APPLICATION RESILIENCY
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序弹性
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: Over the course of this book, we’ve seen how containers and Kubernetes enable
    scalable, resilient applications. Using containers, we can encapsulate application
    components so that processes are isolated from one another, have separate virtualized
    network stacks, and a separate filesystem. Each container can then be rapidly
    deployed without interfering with other containers. When we add Kubernetes as
    a container orchestration layer on top of the container runtime, we are able to
    include many separate hosts into a single cluster, dynamically scheduling containers
    across available cluster nodes with automatic scaling and failover, distributed
    networking, traffic routing, storage, and configuration.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的过程中，我们已经看到容器和 Kubernetes 如何实现可扩展的、具有弹性的应用程序。通过使用容器，我们可以将应用程序组件封装起来，使得进程相互隔离，拥有独立的虚拟化网络堆栈和独立的文件系统。然后，每个容器可以快速部署，而不会干扰其他容器。当我们在容器运行时之上添加
    Kubernetes 作为容器编排层时，我们能够将多个独立的主机合并为一个集群，动态调度容器到可用的集群节点，支持自动扩展和故障转移、分布式网络、流量路由、存储和配置。
- en: All of the container and Kubernetes features we’ve seen in this book work together
    to provide the necessary infrastructure to deploy scalable, resilient applications,
    but it’s up to us to configure our applications correctly to take advantage of
    what the infrastructure provides. In this chapter, we’ll take another look at
    the `todo` application we deployed in [Chapter 1](ch01.xhtml#ch01). This time,
    however, we’ll deploy it across multiple nodes in a Kubernetes cluster, eliminating
    single points of failure and taking advantage of the key features that Kubernetes
    has to offer. We’ll also explore how to monitor the performance of our Kubernetes
    cluster and our deployed application so that we can identify performance issues
    before they lead to downtime for our users.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们看到的所有容器和 Kubernetes 特性协同工作，为部署可扩展且具备弹性的应用程序提供了必要的基础设施，但要利用这些基础设施，我们需要正确配置我们的应用程序。在本章中，我们将再次回顾我们在[第1章](ch01.xhtml#ch01)中部署的
    `todo` 应用程序。不过这次，我们将其部署到 Kubernetes 集群中的多个节点上，从而消除单点故障，并利用 Kubernetes 提供的关键功能。我们还将探讨如何监控我们的
    Kubernetes 集群和已部署的应用程序的性能，以便在问题导致用户停机之前识别性能瓶颈。
- en: Example Application Stack
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例应用栈
- en: In [Chapter 1](ch01.xhtml#ch01), we deployed `todo` onto a Kubernetes cluster
    running `k3s` from Rancher. We already had some amount of scalability and failover
    available. The web layer was based on a Deployment, so we were able to scale the
    number of server instances with a single command. Our Kubernetes cluster was monitoring
    those instances so failed instances could be replaced. However, we still had some
    single points of failure. We had not yet introduced the idea of a highly available
    Kubernetes control plane, so we chose to run `k3s` only in a single-node configuration.
    Additionally, even though we used a Deployment for our PostgreSQL database, it
    was lacking in any of the necessary configuration for high availability. In this
    chapter, we’ll see the details necessary to correct those limitations, and we’ll
    also take advantage of the many other Kubernetes features we’ve learned.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.xhtml#ch01)中，我们将 `todo` 部署到运行 `k3s`（由 Rancher 提供）的 Kubernetes 集群上。我们已经具备了一定的可扩展性和故障转移功能。Web
    层基于 Deployment，因此我们可以通过单个命令来扩展服务器实例的数量。我们的 Kubernetes 集群会监控这些实例，以便在实例失败时进行替换。然而，我们仍然存在一些单点故障问题。我们尚未引入高可用
    Kubernetes 控制平面的概念，因此选择仅在单节点配置下运行 `k3s`。此外，尽管我们为 PostgreSQL 数据库使用了 Deployment，但它缺乏任何必要的高可用性配置。在本章中，我们将看到如何纠正这些限制，并利用我们所学到的其他
    Kubernetes 特性。
- en: Database
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据库
- en: Let’s begin by deploying a highly available PostgreSQL database. [Chapter 17](ch17.xhtml#ch17)
    demonstrated how the Kubernetes Operator design pattern uses CustomResourceDefinitions
    to extend the behavior of a cluster, making it easy to package and deploy advanced
    functionality. We’ll use the Postgres Operator we introduced in that chapter to
    deploy our database.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从部署一个高可用的 PostgreSQL 数据库开始。[第17章](ch17.xhtml#ch17)展示了 Kubernetes Operator
    设计模式如何使用自定义资源定义（CustomResourceDefinitions）来扩展集群的行为，使得打包和部署高级功能变得更加容易。我们将使用在那一章中介绍的
    Postgres Operator 来部署我们的数据库。
- en: '**NOTE**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up. This chapter uses a larger six-node cluster to provide room for the application
    and all the monitoring components that we’ll be deploying. See the *README.md*
    file for this chapter for more information.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*本书的示例代码库在* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples)。*有关设置的详细信息，请参见[第
    xx 页](ch00.xhtml#ch00lev1sec2)中的“运行示例”。本章使用了一个更大的六节点集群，以为应用程序和我们将要部署的所有监控组件提供空间。有关更多信息，请参阅本章的
    *README.md* 文件。*'
- en: The automation for this chapter has already deployed the Postgres Operator together
    with its configuration. You can inspect the Postgres Operator and its configuration
    by looking at the files in */etc/kubernetes/components*. The operator is running
    in the `todo` Namespace, where the `todo` application is also deployed. Many operators
    prefer to run in their own Namespace and operate across the cluster, but the Postgres
    Operator is designed to be deployed directly into the Namespace where the database
    will reside.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的自动化已部署了 Postgres Operator 及其配置。你可以通过查看 */etc/kubernetes/components* 目录中的文件来检查
    Postgres Operator 及其配置。该 Operator 正在 `todo` 命名空间中运行，`todo` 应用程序也已在该命名空间中部署。许多
    Operator 更倾向于在自己的命名空间中运行并跨集群操作，但 Postgres Operator 设计为直接部署到数据库所在的命名空间中。
- en: 'Because we’re using the Postgres Operator, we can create a highly available
    PostgreSQL database by applying a custom resource to the cluster:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们使用的是 Postgres Operator，所以可以通过向集群应用自定义资源来创建一个高可用的 PostgreSQL 数据库：
- en: '*database.yaml*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*database.yaml*'
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: All of the files shown in this walkthrough have been staged to the */etc/kubernetes/todo*
    directory so that you can explore them and experiment with changes. The `todo`
    application is automatically deployed, but it can take several minutes for all
    the components to reach a healthy state.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本文演示的所有文件都已被放置在 */etc/kubernetes/todo* 目录中，方便你进行探索并尝试修改。`todo` 应用程序已自动部署，但所有组件可能需要几分钟才能达到健康状态。
- en: The Postgres Operator has the job of creating the Secrets, StatefulSets, Services,
    and other core Kubernetes resources needed to deploy PostgreSQL. We’re only required
    to supply the configuration it should use. We start by identifying the name for
    this database, `todo-db` ➊, which will be used as the name of the primary Service
    that we’ll use to connect to the primary database instance, so we’ll see this
    name again in the application configuration.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Postgres Operator 的职责是创建 Secrets、StatefulSets、Services 以及部署 PostgreSQL 所需的其他核心
    Kubernetes 资源。我们只需要提供它应该使用的配置。我们首先确定数据库的名称为 `todo-db` ➊，这个名称将作为连接到主数据库实例的主要 Service
    的名称，因此我们将在应用程序配置中再次看到这个名称。
- en: We want a highly available database, so let’s specify three instances ➋. We
    also ask the Postgres Operator to create a `todo` user ➌ and to create a `todo`
    database with the `todo` user as the owner ➍. This way, our database is already
    set up and we only need to populate the tables to store the application data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个高可用的数据库，因此我们指定了三个实例 ➋。我们还要求 Postgres Operator 创建一个 `todo` 用户 ➌，并使用 `todo`
    用户作为所有者创建一个 `todo` 数据库 ➍。这样，我们的数据库就已经设置好了，我们只需要填充表格以存储应用程序数据。
- en: 'We can verify that the database is running in the cluster:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以验证数据库是否在集群中运行：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `todo-db` StatefulSet has three Pods, all of which are ready.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`todo-db` StatefulSet 有三个 Pod，所有 Pod 均已准备就绪。'
- en: 'Because the Postgres Operator is using a StatefulSet, as we saw in [Chapter
    15](ch15.xhtml#ch15), a PersistentVolumeClaim is allocated for the database instances
    as they are created:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Postgres Operator 使用 StatefulSet，如我们在[第15章](ch15.xhtml#ch15)中所见，数据库实例创建时会为其分配
    PersistentVolumeClaim：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: These PersistentVolumeClaims will be reused if one of the database instance
    Pods fails and must be re-created, and the Longhorn storage engine is distributing
    its storage across our entire cluster, so the database will retain the application
    data even if we have a node failure.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 PersistentVolumeClaims 会在数据库实例 Pod 出现故障并需要重新创建时被重新使用，且 Longhorn 存储引擎正在将存储分布在整个集群中，因此即使我们遇到节点故障，数据库仍能保持应用程序数据。
- en: 'Note that when we requested the Postgres Operator to create a `todo` user,
    we didn’t specify a password. For security, the Postgres Operator automatically
    generates a password. This password is placed into a Secret based on the name
    of the user and the name of the database. We can see the Secret created for the
    `todo` user:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当我们要求Postgres操作员创建`todo`用户时，并没有指定密码。出于安全考虑，Postgres操作员会自动生成密码。此密码会放入一个Secret中，名称基于用户和数据库的名称。我们可以看到为`todo`用户创建的Secret：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We’ll need to use this information to configure the application so that it can
    authenticate to the database.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用这些信息来配置应用，使其能够进行数据库身份验证。
- en: 'Before we look at the application configuration, let’s inspect the Service
    that the Postgres Operator created:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看应用配置之前，让我们检查一下Postgres操作员创建的服务：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is a `ClusterIP` Service, meaning that it is reachable from anywhere inside
    the cluster but is not externally exposed. That matches perfectly with what we
    want for our application, as our web service component is the only user-facing
    component and thus the only one that will be exposed outside the cluster.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个`ClusterIP`服务，这意味着它可以在集群内部的任何地方访问，但不会对外暴露。这完全符合我们应用的需求，因为我们的Web服务组件是唯一对外暴露的组件，因此是唯一会暴露到集群外的。
- en: Application Deployment
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 应用部署
- en: All of our application’s data is in the PostgreSQL database, so the web server
    layer is stateless. For this stateless component, we’ll use a Deployment and set
    up automatic scaling.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用的所有数据都存储在PostgreSQL数据库中，因此Web服务器层是无状态的。对于这个无状态组件，我们将使用部署并设置自动扩展。
- en: The Deployment has a lot of information, so let’s look at it step by step. To
    see the entire Deployment configuration and get a sense of how it all fits together,
    you can look at the file */etc/kubernetes/todo/application.yaml* on any of the
    cluster nodes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 部署包含大量信息，让我们一步一步来看。要查看整个部署配置并了解其如何组合在一起，您可以查看集群节点上的文件*/etc/kubernetes/todo/application.yaml*。
- en: 'The first section tells Kubernetes that we’re creating a Deployment:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分告诉Kubernetes我们正在创建一个部署：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This part is simple because we’re only specifying the metadata for the Deployment.
    Note that we don’t include the `namespace` in the metadata. Instead, we provide
    it to Kubernetes directly when we apply this Deployment to the cluster. This way,
    we can reuse the same Deployment YAML for development, test, and production versions
    of this application, keeping each in a separate Namespace to avoid conflict.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分很简单，因为我们只是指定了部署的元数据。请注意，我们没有在元数据中包含`namespace`。相反，当我们将此部署应用到集群时，会直接将其提供给Kubernetes。这样，我们可以在开发、测试和生产版本中重复使用相同的部署YAML，并将每个版本放在不同的命名空间中，以避免冲突。
- en: The `label` field is purely informational, though it also provides a way for
    us to query the cluster for all of the resources associated with this application
    by matching on the label.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`label`字段纯粹是信息性字段，不过它也为我们提供了一种方法，通过匹配标签查询集群中与此应用相关的所有资源。'
- en: 'The next part of the Deployment YAML specifies how the cluster should handle
    updates:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 部署YAML的下一部分指定了集群应如何处理更新：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `replicas` field tells Kubernetes how many instances to create initially.
    The autoscaling configuration will automatically adjust this.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`replicas`字段告诉Kubernetes要初始创建多少个实例。自动扩展配置会自动调整这个数量。'
- en: The `strategy` field allows us to configure this Deployment for updates without
    any application downtime. We can choose either `RollingUpdate` or `Recreate` as
    a strategy. With `Recreate`, when the Deployment changes, all of the existing
    Pods are terminated, and then the new Pods are created. With `RollingUpdate`,
    new Pods are immediately created, and old Pods are kept running to ensure that
    this application component can continue functioning while it is updated.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`strategy`字段允许我们配置此部署以便在没有应用停机的情况下进行更新。我们可以选择`RollingUpdate`或`Recreate`作为策略。使用`Recreate`时，当部署发生变化时，所有现有的Pods都会被终止，然后创建新的Pods。而使用`RollingUpdate`时，新的Pods会立即创建，旧的Pods会继续运行，以确保在更新过程中应用组件可以继续运行。'
- en: We can control how the rolling update operates using the `maxUnavailable` and
    `maxSurge` fields, which we can specify either as integer numbers or as a percentage
    of the current number of replicas. In this case, we specified 30 percent for `maxUnavailable`,
    so the Deployment will throttle the rolling update process to prevent us from
    falling below 70 percent of the current number of replicas. Additionally, because
    we set `maxSurge` at 50 percent, the Deployment will immediately start new Pods
    until the number of Pods that are running or in the creation process reaches 150
    percent of the current number of replicas.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`maxUnavailable`和`maxSurge`字段控制滚动更新的操作方式，这些字段可以指定为整数值或当前副本数量的百分比。在这种情况下，我们为`maxUnavailable`指定了30％，因此Deployment将限制滚动更新过程，以防止我们低于当前副本数的70％。此外，由于我们将`maxSurge`设置为50％，Deployment将在新的Pod启动时，直到正在运行或创建过程中的Pod数量达到当前副本数的150％。
- en: The `RollingUpdate` strategy is the default, and by default, both `maxSurge`
    and `maxUnavailable` are 25 percent. Most Deployments should use the `RollingUpdate`
    strategy unless it is absolutely necessary to use `Recreate`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`RollingUpdate`策略是默认策略，默认情况下，`maxSurge`和`maxUnavailable`均为25％。除非必须使用`Recreate`，否则大多数Deployment应该使用`RollingUpdate`策略。'
- en: 'The next part of the Deployment YAML links the Deployment to its Pods:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Deployment YAML的下一部分将Deployment与其Pod关联起来：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `selector` and the `labels` in the Pod `metadata` must match. As we saw
    in [Chapter 7](ch07.xhtml#ch07), the Deployment uses the `selector` to track its
    Pods.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Pod `metadata`中的`selector`和`labels`必须匹配。正如我们在[第7章](ch07.xhtml#ch07)中看到的，Deployment使用`selector`来跟踪其Pod。
- en: 'With this part, we’ve now begun defining the `template` for the Pods this Deployment
    creates. The rest of the Deployment YAML completes the Pod template, which consists
    entirely of configuration for the single container this Pod runs:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们现在开始定义此Deployment所创建Pod的`template`。Deployment YAML的其余部分完成了Pod模板，完全由此Pod运行的单个容器的配置组成：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The container name is mostly informational, though it is essential for Pods
    with multiple containers so that we can choose a container when we need to retrieve
    logs and use `exec` to run commands. The `image` tells Kubernetes what container
    image to retrieve in order to run this container.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 容器名称主要是信息性的，尽管它对于有多个容器的Pod来说是必需的，这样我们可以在需要检索日志和使用`exec`运行命令时选择一个容器。`image`告诉Kubernetes需要获取哪个容器镜像来运行该容器。
- en: 'The next section of the Pod template specifies the environment variables for
    this container:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Pod模板的下一部分指定了此容器的环境变量：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Some of the environment variables have static values; they’re expected to remain
    the same for all uses of this Deployment. The `PGHOST` environment variable matches
    the name of the PostgreSQL database. The Postgres Operator has created a Service
    with the name `todo-db` in the `todo` Namespace where these Pods will run, so
    the Pods are able to resolve this hostname to the Service IP address. Traffic
    destined for the Service IP address is then routed to the primary PostgreSQL instance
    using the `iptables` configuration we saw in [Chapter 9](ch09.xhtml#ch09).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一些环境变量具有静态值；它们预计在所有使用此Deployment的实例中保持不变。`PGHOST`环境变量与PostgreSQL数据库的名称匹配。Postgres
    Operator在`todo`命名空间中创建了一个名为`todo-db`的Service，在这些Pod运行的地方，因此Pod能够将此主机名解析为Service的IP地址。目标为Service
    IP地址的流量随后将通过我们在[第9章](ch09.xhtml#ch09)中看到的`iptables`配置路由到主PostgreSQL实例。
- en: The final two variables provide the credentials for the application to authenticate
    to the database. We’re using the ability to fetch configuration from a Secret
    and provide it as an environment variable to a container, similar to what we saw
    in [Chapter 16](ch16.xhtml#ch16). However, in this case, we need the environment
    variable to have a different name from the key name in the Secret, so we use a
    slightly different syntax that allows us to specify each variable name separately.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的两个变量提供了应用程序用于认证数据库的凭证。我们使用从Secret中获取配置并将其作为环境变量提供给容器的功能，类似于我们在[第16章](ch16.xhtml#ch16)中看到的。然而，在这种情况下，我们需要环境变量的名称与Secret中的键名称不同，因此我们使用了一种稍微不同的语法，允许我们分别指定每个变量的名称。
- en: 'Finally, we declare the resource requirements of this container and the port
    it exposes:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们声明了此容器的资源需求和它暴露的端口：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `ports` field in a Pod is purely informational; the actual traffic routing
    will be configured in the Service.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`ports`字段在Pod中纯粹是信息性的；实际的流量路由将在Service中配置。'
- en: Within the `resources` field, we set the `requests` and `limits` to be the same
    for this container. As we saw in [Chapter 19](ch19.xhtml#ch19), this means that
    Pod will be placed in the `Guaranteed` Quality of Service class. The web service
    component is stateless and easy to scale, so it makes sense to use a relatively
    low CPU limit, in this case, 50 millicores, or 5 percent of a core, and rely on
    the autoscaling to create new instances if the load becomes high.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在`resources`字段中，我们将`requests`和`limits`设置为该容器相同的值。正如我们在[第19章](ch19.xhtml#ch19)中看到的，这意味着Pod将被放置在`Guaranteed`服务质量类中。由于Web服务组件是无状态的且易于扩展，因此使用相对较低的CPU限制是合理的，在此情况下为50毫核心，即一个核心的5%，并依赖自动扩展来创建新的实例，以应对负载增大时的需求。
- en: Pod Autoscaling
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Pod 自动扩展
- en: 'To automatically scale the Deployment to match the current load, we use a HorizontalPodAutoscaler,
    as we saw in [Chapter 7](ch07.xhtml#ch07). Here’s the configuration for the autoscaler:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了自动扩展部署以匹配当前负载，我们使用了水平Pod自动扩展器（HorizontalPodAutoscaler），正如我们在[第7章](ch07.xhtml#ch07)中看到的那样。这是自动扩展器的配置：
- en: '*scaler.yaml*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*scaler.yaml*'
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As we did in our earlier example, we apply a label to this resource purely for
    informational purposes. Three key configuration items are necessary for this autoscaler.
    First, the `scaleTargetRef` specifies that we want to scale the `todo` Deployment.
    Because this autoscaler is deployed to the `todo` Namespace, it finds the correct
    Deployment to scale.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前的示例所示，我们为该资源应用标签，纯粹是为了信息目的。这个自动扩展器需要三个关键配置项。首先，`scaleTargetRef`指定了我们希望扩展`todo`部署。由于这个自动扩展器部署在`todo`命名空间中，它会找到正确的部署进行扩展。
- en: Second, we specify a range for `minReplicas` and `maxReplicas`. We choose `3`
    as the minimum number of replicas, as we want to make sure the application is
    resilient even if we have a Pod failure. For simplicity, we didn’t apply the anti-affinity
    configuration we saw in [Chapter 18](ch18.xhtml#ch18), but this may also be a
    good practice to avoid having all of the instances on a single node. We choose
    a maximum number of replicas based on the size of our cluster; for a production
    application, we would measure our application load and choose based on the highest
    load we expect to handle.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们指定`minReplicas`和`maxReplicas`的范围。我们选择`3`作为最小副本数，因为我们希望即使发生Pod故障，应用程序也能保持弹性。为了简化，我们没有应用我们在[第18章](ch18.xhtml#ch18)中看到的反亲和性配置，但这也是一种避免所有实例都部署在单个节点上的良好实践。我们根据集群的大小选择最大副本数；对于生产环境的应用程序，我们会根据预计处理的最大负载来选择副本数。
- en: Third, we need to specify the metric that the autoscaler will use to decide
    how many replicas are needed. We base this autoscaler on CPU utilization. If the
    average utilization across the Pods is greater than 50 percent of the Pod’s `requests`,
    the Deployment will be scaled up. We set the `requests` at 50 millicores, so this
    means that an average utilization greater than 25 millicores will cause the autoscaler
    to increase the number of replicas.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们需要指定自动扩展器将用来决定需要多少副本的指标。我们基于CPU利用率来配置这个自动扩展器。如果Pod的平均利用率超过Pod `requests`的50%，部署将会扩展。我们将`requests`设置为50毫核心，这意味着平均利用率超过25毫核心将导致自动扩展器增加副本数。
- en: To retrieve the average CPU utilization, the autoscaler relies on a cluster
    infrastructure component that retrieves metrics data from the `kubelet` service
    running on each node and exposes that metrics data via an API. For this chapter,
    we have some extra cluster monitoring functionality to demonstrate, so the automation
    has skipped the regular metrics server component we described in [Chapter 6](ch06.xhtml#ch06).
    We’ll deploy an alternative later in this chapter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取平均CPU利用率，自动扩展器依赖于一个集群基础设施组件，该组件从每个节点上运行的`kubelet`服务中获取度量数据，并通过API暴露这些度量数据。对于本章，我们有一些额外的集群监控功能要展示，所以自动化跳过了我们在[第6章](ch06.xhtml#ch06)中描述的常规度量服务器组件。我们将在本章稍后部署一个替代方案。
- en: Application Service
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 应用程序服务
- en: The final cluster resource for our application is the Service. [Listing 20-1](ch20.xhtml#ch20list1)
    presents the definition we’re using for this chapter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用程序的最终集群资源是服务。[列表20-1](ch20.xhtml#ch20list1)展示了我们在本章中使用的定义。
- en: '*service.yaml*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*service.yaml*'
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*Listing 20-1: Todo Service*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表20-1：Todo 服务*'
- en: We use the same `selector` that we saw in the Deployment to find the Pods that
    will receive traffic sent to this Service. As we saw in [Chapter 9](ch09.xhtml#ch09),
    the `ports` field of a Service is essential because `iptables` traffic routing
    rules are configured only for the ports we identify. In this case, we declare
    the `port` to be 5000 and don’t declare a `targetPort`, so this Service will send
    to port 5000 on the Pods, which matches the port on which our web server is listening.
    We also configure a `name` on this port, which will be important later when we
    configure monitoring.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与在部署中看到的相同的`selector`来查找将接收发送到该服务的流量的Pods。正如我们在[第9章](ch09.xhtml#ch09)中看到的，服务的`ports`字段至关重要，因为`iptables`流量路由规则仅为我们指定的端口配置。在这种情况下，我们声明`port`为5000，并未声明`targetPort`，因此此服务将流量发送到Pods的5000端口，这与我们的Web服务器监听的端口相匹配。我们还为这个端口配置了一个`name`，这在稍后配置监控时会很重要。
- en: 'For this chapter, we’re exposing our application Service using `NodePort`,
    which means that all of our cluster’s nodes will be configured to route traffic
    to the Service that is sent to the `nodePort` for any host interface. Thus, we
    can access port 5000 on any of our cluster’s nodes and we’ll be routed to our
    application:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们通过`NodePort`暴露了我们的应用程序服务，这意味着我们集群中的所有节点都将被配置为将流量路由到发送到任何主机接口的`nodePort`的服务。因此，我们可以访问集群中任何节点的5000端口，流量会被路由到我们的应用程序：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This Service traffic routing works on any host interface, so the `todo` application
    can be accessed from outside the cluster as well. The URL is different depending
    on whether you’re using the Vagrant or Amazon Web Services configuration, so the
    automation for this chapter includes a message with the URL to use.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个服务流量路由在任何主机接口上都能工作，因此`todo`应用程序也可以从集群外部访问。URL的不同取决于你使用的是Vagrant配置还是Amazon
    Web Services配置，因此本章的自动化包括一条消息，告知使用的URL。
- en: '**NODEPORT, NOT INGRESS**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**NODEPORT，而非INGRESS**'
- en: When we deployed `todo` in [Chapter 1](ch01.xhtml#ch01), we exposed the Service
    using an Ingress. The Ingress, as we saw in [Chapter 9](ch09.xhtml#ch09), consolidates
    multiple Services such that they can all be exposed outside the cluster without
    requiring each Service to have a separate externally routable IP address. We’ll
    expose a monitoring service later in this chapter, so we have multiple Services
    to expose outside the cluster. However, because we’re working with an example
    cluster on a private network, we don’t have the underlying network infrastructure
    available to use an Ingress to its full potential. By using a `NodePort` instead,
    we’re able to expose multiple Services outside the cluster in a way that works
    well with both the Vagrant and Amazon Web Services configurations.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在[第1章](ch01.xhtml#ch01)中部署`todo`时，我们使用Ingress暴露了服务。正如我们在[第9章](ch09.xhtml#ch09)中看到的，Ingress将多个服务整合在一起，使它们都可以在不要求每个服务有单独外部可路由IP地址的情况下暴露到集群外部。我们将在本章稍后暴露一个监控服务，因此我们需要暴露多个服务到集群外部。然而，由于我们正在使用私有网络上的示例集群，我们没有可用的底层网络基础设施来充分利用Ingress。通过改为使用`NodePort`，我们能够以一种既适用于Vagrant配置又适用于Amazon
    Web Services配置的方式，将多个服务暴露到集群外部。
- en: We’ve now looked all of the components in the `todo` application, using what
    we’ve learned in this book to eliminate single points of failure and maximize
    scalability.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经查看了`todo`应用程序中的所有组件，利用本书中学到的知识消除了单点故障并最大化了可扩展性。
- en: You can also explore the source code for the `todo` application at *[https://github.com/book-of-kubernetes/todo](https://github.com/book-of-kubernetes/todo)*,
    including the *Dockerfile* that’s used to build the application’s container image
    and the GitHub Actions that automatically build it and publish it to Docker Hub
    whenever the code changes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在* [https://github.com/book-of-kubernetes/todo](https://github.com/book-of-kubernetes/todo)
    *上查看`todo`应用程序的源代码，其中包括用来构建应用程序容器镜像的*Dockerfile*，以及每当代码发生更改时，自动构建并发布到Docker Hub的GitHub
    Actions。
- en: However, although our Kubernetes cluster will now do its best to keep this application
    running and performing well, we can do more to monitor both the `todo` application
    and the Kubernetes cluster.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管我们的Kubernetes集群现在会尽最大努力保持此应用程序的运行和性能，我们仍可以做更多工作来监控`todo`应用程序和Kubernetes集群。
- en: Application and Cluster Monitoring
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用程序和集群监控
- en: Proper application and cluster monitoring is essential for applications, for
    multiple reasons. First, our Kubernetes cluster will try to keep the applications
    running, but any hardware or cluster failures could leave an application in a
    non-working or degraded state. Without monitoring, we would be dependent on our
    users to tell us when the application is down or behaving badly, which is poor
    user experience. Second, if we do see failures or performance issues with our
    application, we’re going to need data to diagnose them or to try to identify a
    pattern in order to find a root cause. It’s a lot easier to build in monitoring
    ahead of time than to try to apply it after we’re already seeing problems. Finally,
    we may have problems with our cluster or application that occurs below the level
    at which users notice, but that indicates potential performance or stability issues.
    Integrating proper monitoring allows us to detect those kinds of issues before
    they become a bigger headache. It also allows us to measure an application over
    time to make sure that added features aren’t degrading its performance.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 适当的应用和集群监控对应用程序至关重要，原因有很多。首先，我们的Kubernetes集群将尽力保持应用程序运行，但任何硬件或集群故障都可能导致应用程序处于无法正常工作或降级的状态。如果没有监控，我们将依赖用户告诉我们应用程序何时出现故障或表现异常，这样的用户体验很差。其次，如果我们确实看到应用程序出现故障或性能问题，我们需要数据来诊断问题，或者试图识别某种模式以找到根本原因。提前构建监控要比在我们已经看到问题后再去应用它要容易得多。最后，我们可能会遇到一些集群或应用程序的问题，这些问题发生在用户未察觉的层面，但它们可能预示着潜在的性能或稳定性问题。集成适当的监控使我们能够在这些问题变得更严重之前发现它们。它还使我们能够随着时间的推移衡量应用程序，确保新增的功能不会降低其性能。
- en: Fortunately, although we do need to think about monitoring at the level of each
    of our application components, we don’t need to build a monitoring framework ourselves.
    Many mature monitoring tools are already designed to work in a Kubernetes cluster,
    so we can get up and running quickly. In this chapter, we’ll look at `kube-prometheus`,
    a complete stack of tools that we can deploy to our cluster and use to monitor
    both the cluster and the `todo` application.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，尽管我们确实需要在每个应用组件的层面上考虑监控，但我们不需要自己构建监控框架。许多成熟的监控工具已经设计好，可以在Kubernetes集群中工作，因此我们可以快速启动并运行。在本章中，我们将介绍`kube-prometheus`，这是一个完整的工具栈，我们可以将其部署到集群中，用于监控集群和`todo`应用程序。
- en: Prometheus Monitoring
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Prometheus 监控
- en: The core component of `kube-prometheus` is, as the name implies, the open source
    Prometheus monitoring software. Prometheus deploys as a server that periodically
    queries various metrics sources and accumulates the data it receives. It supports
    a query language that is optimized for “time series” data, which makes it easy
    to collect individual data points showing a system’s performance at a moment in
    time. It then aggregates those data points to get a picture of the system’s load,
    resource utilization, and responsiveness.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-prometheus`的核心组件是，顾名思义，开源的Prometheus监控软件。Prometheus作为服务器部署，定期查询各种指标源并累积它收到的数据。它支持一种优化为“时间序列”数据的查询语言，使得收集显示系统在某一时刻性能的单个数据点变得容易。然后，它将这些数据点汇总，绘制出系统负载、资源利用率和响应能力的图景。'
- en: For each component that exposes metrics, Prometheus expects to reach out to
    a URL and receive data in return in a standard format. It’s common to use the
    path */metrics* to expose metrics to Prometheus. Following this convention, the
    Kubernetes control plane components already expose metrics in the format that
    Prometheus is expecting.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个暴露指标的组件，Prometheus期望访问一个URL并返回标准格式的数据。通常使用路径*/metrics*来暴露指标给Prometheus。遵循这一约定，Kubernetes控制平面组件已经以Prometheus期望的格式暴露了指标。
- en: 'To illustrate, we can use `curl` to visit the */metrics* path on the API server
    to see the metrics that it provides. To do this, we’ll need to authenticate to
    the API server, so let’s use a script that collects a client certificate for authentication:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，我们可以使用`curl`访问API服务器上的*/metrics*路径，以查看它提供的指标。为了实现这一点，我们需要进行API服务器的身份验证，因此让我们使用一个脚本来收集客户端证书以进行身份验证：
- en: '*api-metrics.sh*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*api-metrics.sh*'
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Running this script returns a wealth of API server metrics:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此脚本会返回大量的API服务器指标：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This example illustrates only a few of the hundreds of metrics that are collected
    and exposed. Each line of this response provides one data point to Prometheus.
    We can include additional parameters for the metric in curly braces, allowing
    for more complex queries. For example, the API server data in the preceding example
    can be used to determine not only the total number of client requests served by
    the API server but also the raw number and percentage of requests that resulted
    in an error. Most systems are resilient to a few HTTP error responses, but a sudden
    increase in error responses is often a good indication of a more serious issue,
    so this is valuable in configuring a reporting threshold.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例仅说明了收集和暴露的数百个指标中的一小部分。此响应的每一行都提供一个数据点给Prometheus。我们可以在大括号中包含附加的指标参数，以便进行更复杂的查询。例如，前面示例中的API服务器数据可以用来确定API服务器处理的客户端请求总数，以及导致错误的请求的原始数量和百分比。大多数系统能应对少量的HTTP错误响应，但错误响应的突然增加通常是更严重问题的良好指示，因此在配置报告阈值时，这非常有价值。
- en: In addition to all of the data that the Kubernetes cluster is already providing
    to Prometheus, we can also configure our application to expose metrics. Our application
    is based on Node.js, so we do this using the `prom-client` library. As demonstrated
    in [Listing 20-2](ch20.xhtml#ch20list2), our `todo` application is exposing metrics
    at */metrics*, like the API server.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Kubernetes集群已经提供给Prometheus的所有数据外，我们还可以配置我们的应用程序来暴露指标。我们的应用程序基于Node.js，因此我们使用`prom-client`库来完成此操作。如[清单20-2](ch20.xhtml#ch20list2)所示，我们的`todo`应用程序在*/metrics*处暴露指标，类似于API服务器。
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '*Listing 20-2: Todo metrics*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单20-2：待办事项指标*'
- en: The response includes some default metrics that are relevant to all applications.
    It also includes some counters that are specific to the `todo` application and
    track API usage and responses over time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 响应包括一些与所有应用程序相关的默认指标。它还包括一些特定于`todo`应用程序的计数器，用于跟踪API的使用情况和响应时间。
- en: Deploying kube-prometheus
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 部署kube-prometheus
- en: At this point, our Kubernetes cluster and our application are ready to provide
    these metrics on demand, but we don’t yet have a Prometheus server running in
    the cluster to collect them. To fix this, we’ll deploy the complete `kube-prometheus`
    stack. This includes not only a Prometheus Operator that makes it easy to deploy
    and configure Prometheus but also other useful tools, such as Alertmanager, which
    can trigger notifications in response to cluster and application alerts, and Grafana,
    a dashboard tool that we’ll use to see the metrics we’re collecting.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们的Kubernetes集群和应用程序已经准备好根据需求提供这些指标，但我们还没有在集群中运行Prometheus服务器来收集它们。为了解决这个问题，我们将部署完整的`kube-prometheus`堆栈。它不仅包括一个Prometheus操作员，简化了Prometheus的部署和配置，还包括其他有用的工具，如Alertmanager，它可以响应集群和应用程序的警报触发通知，以及Grafana，这是一个我们将用来查看收集的指标的仪表盘工具。
- en: To deploy `kube-prometheus`, we’ll use a script that’s been installed in */opt*.
    This script downloads a current `kube-prometheus` release from GitHub and applies
    the manifests.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署`kube-prometheus`，我们将使用一个已安装在*/opt*中的脚本。这个脚本从GitHub下载当前的`kube-prometheus`版本并应用清单。
- en: 'Run the script as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下方式运行脚本：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: These manifests also include a Prometheus Adapter. The Prometheus Adapter implements
    the same Kubernetes metrics API as the `metrics-server` we deployed to the clusters
    throughout [Part II](part02.xhtml#part02), so it exposes CPU and memory data obtained
    from `kubelet`, enabling our HorizontalPodAutoscaler to track CPU utilization
    of our `todo` application. However, it also exposes that utilization data to Prometheus
    so that we can observe it in Grafana dashboards. For this reason, we use the Prometheus
    Adapter in this chapter in place of the regular `metrics-server`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这些清单还包括一个Prometheus适配器。Prometheus适配器实现了与我们在[第二部分](part02.xhtml#part02)中部署到集群的`metrics-server`相同的Kubernetes指标API，因此它暴露了从`kubelet`获取的CPU和内存数据，使我们的HorizontalPodAutoscaler能够跟踪`todo`应用程序的CPU利用率。然而，它还将这些利用率数据暴露给Prometheus，以便我们在Grafana仪表盘中查看它。正因为如此，在本章中我们使用Prometheus适配器来替代常规的`metrics-server`。
- en: 'We can see the Prometheus Adapter and the other components by listing Pods
    in the `monitoring` Namespace:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过列出`monitoring`命名空间中的Pods来查看Prometheus适配器和其他组件：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In addition to the Prometheus Adapter, we see Pods for Alertmanager, Grafana,
    and various `exporter` Pods, which collect metrics from the cluster infrastructure
    and expose it to Prometheus. We also see Pods for Prometheus itself and for the
    Prometheus Operator. The Prometheus Operator automatically updates Prometheus
    whenever we change the custom resources that the Prometheus Operator is monitoring.
    The most important of those custom resources is the Prometheus resource shown
    in [Listing 20-3](ch20.xhtml#ch20list3).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Prometheus 适配器外，我们还看到 Alertmanager、Grafana 和各种 `exporter` Pod，这些 Pod 从集群基础设施中收集指标并将其暴露给
    Prometheus。我们还看到了 Prometheus 本身和 Prometheus Operator 的 Pod。每当我们更改 Prometheus Operator
    所监控的自定义资源时，Prometheus Operator 会自动更新 Prometheus。最重要的自定义资源是[清单 20-3](ch20.xhtml#ch20list3)中所示的
    Prometheus 资源。
- en: '[PRE19]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*Listing 20-3: Prometheus configuration*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 20-3：Prometheus 配置*'
- en: The Prometheus custom resource allows us to configure which Namespaces will
    be watched for Services to monitor. The default configuration presented in [Listing
    20-3](ch20.xhtml#ch20list3) does not specify a value for the Service Monitor Namespace
    Selector or the Service Monitor Selector. For this reason, by default the Prometheus
    Operator will be looking for monitoring configuration in all Namespaces, with
    any metadata label.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 自定义资源允许我们配置哪些命名空间中的服务需要被监控。在[清单 20-3](ch20.xhtml#ch20list3)中展示的默认配置并没有为
    Service Monitor Namespace Selector 或 Service Monitor Selector 指定值。因此，默认情况下，Prometheus
    Operator 会在所有命名空间中查找监控配置，且没有任何元数据标签。
- en: To identify specific Services to monitor, the Prometheus Operator keeps an eye
    out for another custom resource, *ServiceMonitor*, as demonstrated in [Listing
    20-4](ch20.xhtml#ch20list4).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别要监控的特定服务，Prometheus Operator 会监视另一个自定义资源 *ServiceMonitor*，正如在[清单 20-4](ch20.xhtml#ch20list4)中所示。
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '*Listing 20-4: Default ServiceMonitors*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 20-4：默认 ServiceMonitors*'
- en: When we installed `kube-prometheus`, it configured multiple ServiceMonitor resources.
    As a result, our Prometheus instance is already watching the Kubernetes control
    plane components and the `kubelet` services running on our cluster nodes. Let’s
    see the targets from which Prometheus is scraping metrics and see how those metrics
    are used to populate dashboards in Grafana.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们安装 `kube-prometheus` 时，它配置了多个 ServiceMonitor 资源。因此，我们的 Prometheus 实例已经在监控
    Kubernetes 控制平面组件和在集群节点上运行的 `kubelet` 服务。让我们看看 Prometheus 从哪些目标中抓取指标，并查看这些指标是如何用于填充
    Grafana 中的仪表板的。
- en: Cluster Metrics
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 集群指标
- en: The installation script patched the Grafana and Prometheus Services in the `monitoring`
    Namespace to expose them as `NodePort` Services. The automation scripts print
    the URL you can use to access Prometheus. The initial page looks like [Figure
    20-1](ch20.xhtml#ch20fig1).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 安装脚本修改了 `monitoring` 命名空间中 Grafana 和 Prometheus 服务，将其暴露为 `NodePort` 服务。自动化脚本会打印出可以用来访问
    Prometheus 的 URL。初始页面如下所示[图 20-1](ch20.xhtml#ch20fig1)。
- en: '![Image](../images/f0337-01.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0337-01.jpg)'
- en: '*Figure 20-1: Prometheus initial page*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 20-1：Prometheus 初始页面*'
- en: Click the **Targets** item underneath the **Status** menu on the top menu bar
    to see which components in the cluster Prometheus is currently scraping. Click
    **Collapse All** to get a consolidated list, as shown in [Figure 20-2](ch20.xhtml#ch20fig2).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 点击顶部菜单栏中 **状态** 菜单下的 **目标** 项，查看 Prometheus 当前正在抓取集群中的哪些组件。点击 **折叠所有**，以获取汇总列表，如[图
    20-2](ch20.xhtml#ch20fig2)所示。
- en: '![Image](../images/f0337-02.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0337-02.jpg)'
- en: '*Figure 20-2: Prometheus targets*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 20-2：Prometheus 目标*'
- en: This list matches the list of ServiceMonitors we saw in [Listing 20-4](ch20.xhtml#ch20list4),
    showing us that Prometheus is scraping Services as configured by the Prometheus
    Operator.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表与我们在[清单 20-4](ch20.xhtml#ch20list4)中看到的 ServiceMonitors 列表匹配，向我们展示了 Prometheus
    正在按照 Prometheus Operator 配置的方式抓取服务。
- en: We can use the Prometheus web interface to query data directly, but Grafana
    has already been configured with some useful dashboards, so we can more easily
    see the data there. The automation scripts print the URL you can use to access
    Grafana. Log in using the default `admin` as the username and `admin` as the password.
    You will be prompted to change the password; you can just click *Skip*. At this
    point you should see the Grafana initial page, as shown in [Figure 20-3](ch20.xhtml#ch20fig3).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Prometheus Web 界面直接查询数据，但 Grafana 已经配置了一些有用的仪表板，因此我们可以更轻松地在其中查看数据。自动化脚本会打印出可以用来访问
    Grafana 的 URL。使用默认的 `admin` 作为用户名，`admin` 作为密码登录。系统会提示你更改密码；你可以直接点击 *跳过*。此时，你应该看到
    Grafana 的初始页面，如[图 20-3](ch20.xhtml#ch20fig3)所示。
- en: '![Image](../images/f0338-01.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0338-01.jpg)'
- en: '*Figure 20-3: Grafana initial page*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*图20-3：Grafana初始页面*'
- en: From this page, choose the **Browse** item under **Dashboards** in the menu.
    There are many dashboards in the *Default* folder. For example, by selecting **Default**
    and then selecting **Kubernetes** ▸ **Compute Resources** ▸ **Pod**, you can see
    a dashboard, depicted in [Figure 20-4](ch20.xhtml#ch20fig4), that shows CPU and
    memory usage over time for any Pod in the cluster.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个页面中，选择菜单中的**浏览**选项。在*默认*文件夹中有许多仪表盘。例如，通过选择**默认**，然后选择**Kubernetes** ▸ **计算资源**
    ▸ **Pod**，你可以看到一个仪表盘，如[图20-4](ch20.xhtml#ch20fig4)所示，展示了集群中任何Pod随时间变化的CPU和内存使用情况。
- en: '![Image](../images/f0338-02.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0338-02.jpg)'
- en: '*Figure 20-4: Pod compute resources*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*图20-4：Pod计算资源*'
- en: All of the `todo` database and application Pods are selectable in this dashboard
    by first selecting the `todo` Namespace, so we can already get valuable information
    about our application by using nothing more than the default monitoring configuration.
    This is possible because the Prometheus Adapter is pulling data from the `kubelet`
    services, which includes resource utilization for each of the running Pods. The
    Prometheus Adapter is then exposing a */metrics* endpoint for Prometheus to scrape
    and store, and Grafana is querying Prometheus to build the chart showing usage
    over time.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个仪表盘中，所有的`todo`数据库和应用Pod都可以选择，首先选择`todo`命名空间，这样我们就可以通过使用默认监控配置来获取关于我们应用的宝贵信息。这是可能的，因为Prometheus适配器正在从`kubelet`服务拉取数据，这些数据包括每个运行中的Pod的资源利用情况。然后，Prometheus适配器暴露了一个*/metrics*端点供Prometheus抓取和存储，而Grafana则查询Prometheus来构建显示随时间变化的使用情况图表。
- en: There are numerous other Grafana dashboards to explore in the default installation
    of `kube-prometheus`. Choose the *Browse* menu item again to select other dashboards
    and see what data is available.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在`kube-prometheus`的默认安装中，还有许多其他Grafana仪表盘可以探索。再次选择*浏览*菜单项，选择其他仪表盘，查看可用的数据。
- en: Adding Monitoring for Services
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 添加服务监控
- en: Although we are already getting useful metrics for our `todo` application, Prometheus
    is not yet scraping our application Pods to pull in the Node.js metrics we saw
    in [Listing 20-2](ch20.xhtml#ch20list2). To configure Prometheus to scrape our
    `todo` metrics, we’ll need to provide a new ServiceMonitor resource to the Prometheus
    Operator, informing it about our `todo` Service.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经获得了关于`todo`应用程序的有用指标，但Prometheus尚未抓取我们的应用Pod以提取我们在[清单20-2](ch20.xhtml#ch20list2)中看到的Node.js指标。为了配置Prometheus抓取`todo`的指标，我们需要向Prometheus
    Operator提供一个新的ServiceMonitor资源，告诉它有关我们`todo`服务的信息。
- en: In a production cluster, the team deploying an application like our `todo` application
    wouldn’t have the permissions to create or update resources in the `monitoring`
    Namespace. However, the Prometheus Operator looks for ServiceMonitor resources
    in all Namespaces by default, so we can create a ServiceMonitor in the `todo`
    Namespace instead.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产集群中，像我们`todo`应用程序这样的应用部署团队通常没有权限在`monitoring`命名空间中创建或更新资源。然而，Prometheus Operator
    默认会在所有命名空间中查找ServiceMonitor资源，因此我们可以在`todo`命名空间中创建一个ServiceMonitor。
- en: 'First, though, we need to give Prometheus permission to see the Pods and Services
    we’ve created in the `todo` Namespace. As this access control configuration needs
    to apply only in a single Namespace, we’ll do this by creating a Role and a RoleBinding.
    Here’s the Role configuration we’ll use:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，首先我们需要授权Prometheus查看我们在`todo`命名空间中创建的Pods和Services。由于此访问控制配置只需应用于单一命名空间，我们将通过创建一个Role和RoleBinding来实现。以下是我们将使用的Role配置：
- en: '*rbac.yaml*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*rbac.yaml*'
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We need to make sure we allow access to Services, Pods, and Endpoints, so we
    confirm that these are listed in the `resources` field. The Endpoint resource
    records the current Pods that are receiving traffic for a Service, which will
    be critical for Prometheus to identify all of the Pods it scrapes. Because Prometheus
    needs only read-only access, we specify only the `get`, `list`, and `watch` verbs.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确保允许访问Services、Pods和Endpoints，因此我们确认这些资源列在`resources`字段中。Endpoint资源记录了当前接收流量的Pod，这对Prometheus识别它抓取的所有Pod至关重要。由于Prometheus只需要只读权限，我们只指定`get`、`list`和`watch`操作符。
- en: 'After we have this Role, we need to bind it to the ServiceAccount that Prometheus
    is using. We do that with this RoleBinding:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这个角色后，我们需要将其绑定到Prometheus使用的ServiceAccount上。我们可以通过这个RoleBinding来完成：
- en: '*rbac.yaml*'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*rbac.yaml*'
- en: '[PRE22]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `roleRef` matches the Role we just declared in the preceding example, whereas
    the `subjects` field lists the ServiceAccount Prometheus is using, based on the
    information we saw in [Listing 20-3](ch20.xhtml#ch20list3).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`roleRef`与我们在前面的示例中声明的Role相匹配，而`subjects`字段列出了Prometheus正在使用的ServiceAccount，基于我们在[清单
    20-3](ch20.xhtml#ch20list3)中看到的信息。'
- en: 'Both of these YAML resources are in the same file, so we can apply them both
    to the cluster at once. We need to make sure we apply them to the `todo` Namespace,
    as that’s the Namespace where we want to enable access by Prometheus:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个YAML资源位于同一个文件中，因此我们可以将它们同时应用到集群中。我们需要确保将它们应用到`todo`命名空间，因为这是我们希望Prometheus访问的命名空间：
- en: '[PRE23]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now that we’ve granted permission to Prometheus to see our Pods and Services,
    we can create the ServiceMonitor. Here’s that definition:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已授权Prometheus访问我们的Pods和服务，我们可以创建ServiceMonitor了。以下是其定义：
- en: '*svc-mon.yaml*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*svc-mon.yaml*'
- en: '[PRE24]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'A ServiceMonitor uses a selector, similar to a Service or a Deployment. We
    previously applied the `app: todo` label to the Service, so the `matchLabels`
    field will cause Prometheus to pick up the Service. The `endpoints` field matches
    the name of the port we declared in the Service in [Listing 20-1](ch20.xhtml#ch20list1).
    Prometheus requires us to name the port in order to match it.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 'ServiceMonitor使用选择器，类似于Service或Deployment。我们之前将`app: todo`标签应用于服务，因此`matchLabels`字段会使Prometheus选择该服务。`endpoints`字段与我们在[清单
    20-1](ch20.xhtml#ch20list1)中声明的端口名称相匹配。Prometheus要求我们命名端口，以便进行匹配。'
- en: 'Let’s apply this ServiceMonitor to the cluster:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个ServiceMonitor应用到集群中：
- en: '[PRE25]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As before, we need to make sure we deploy this to the `todo` Namespace because
    Prometheus will be configured to look for Services with the appropriate label
    in the same Namespace as the ServiceMonitor.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们需要确保将其部署到`todo`命名空间，因为Prometheus将配置为查找与ServiceMonitor位于同一命名空间的具有适当标签的服务。
- en: Because the Prometheus Operator is watching for new ServiceMonitor resources,
    using the API we saw in [Chapter 17](ch17.xhtml#ch17), it picks up this new resource
    and immediately reconfigures Prometheus to start scraping the Service. Prometheus
    then takes a few minutes to register the new targets and start scraping them.
    If we go back to the Prometheus Targets page after this is complete, the new Service
    shows up, as illustrated in [Figure 20-5](ch20.xhtml#ch20fig5).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因为Prometheus Operator正在监视新的ServiceMonitor资源，使用我们在[第17章](ch17.xhtml#ch17)中看到的API，它会立即获取这个新的资源，并重新配置Prometheus以开始抓取该服务。然后，Prometheus需要几分钟时间来注册新的目标并开始抓取它们。如果我们在此完成后回到Prometheus的目标页面，新的服务就会出现，如[图
    20-5](ch20.xhtml#ch20fig5)所示。
- en: '![Image](../images/f0341-01.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0341-01.jpg)'
- en: '*Figure 20-5: Prometheus monitoring todo*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 20-5：Prometheus监控todo*'
- en: If we click the **show more** button next to the `todo` Service, we see its
    three Endpoints, shown in [Figure 20-6](ch20.xhtml#ch20fig6).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们点击`todo`服务旁边的**显示更多**按钮，我们会看到它的三个端点，如[图 20-6](ch20.xhtml#ch20fig6)所示。
- en: '![Image](../images/f0342-01.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0342-01.jpg)'
- en: '*Figure 20-6: Todo Endpoints*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 20-6：Todo 端点*'
- en: It may be surprising that we created a ServiceMonitor, specifying the `todo`
    Service as the target, and yet Prometheus is scraping Pods. However, it’s essential
    that Prometheus works this way. Because Prometheus is using a regular HTTP request
    to scrape metrics, and because Service traffic routing chooses a random Pod for
    every new connection, Prometheus would get metrics from a random Pod each time
    it did scraping. By reaching behind the Service to identify the Endpoints, Prometheus
    is able to scrape metrics from all the Service’s Pods, enabling aggregation of
    metrics for the entire application.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会让人惊讶的是，我们创建了一个ServiceMonitor，指定`todo`服务作为目标，但Prometheus却在抓取Pods。不过，这正是Prometheus必须这样工作的原因。因为Prometheus使用常规的HTTP请求来抓取指标，并且由于服务流量路由每次会随机选择一个Pod进行新连接，Prometheus每次抓取时都会从一个随机的Pod获取指标。通过绕过服务直接识别端点，Prometheus能够抓取所有服务Pod的指标，从而实现整个应用的指标聚合。
- en: We’ve successfully incorporated the Node.js and custom metrics for the `todo`
    application into Prometheus, in addition to the default resource utilization metrics
    already collected. Before we finish our look at application monitoring, let’s
    run a Prometheus query to demonstrate that the data is being pulled in. First,
    you should interact with the `todo` application using the URL printed out by the
    automation scripts. This will ensure that there are metrics to display and that
    enough time has passed for Prometheus to scrape that data. Next, open the Prometheus
    web interface again, or click **Prometheus** at the top of any Prometheus web
    page to go back to the main page. Then, type **api_success** into the query box
    and press ENTER. Custom `todo` metrics should appear, as illustrated in [Figure
    20-7](ch20.xhtml#ch20fig7).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功将Node.js和自定义指标集成到Prometheus中，除了已经收集的默认资源使用率指标。在我们结束对应用监控的介绍之前，先运行一个Prometheus查询，来演示数据是否已经被拉取。首先，你应该使用自动化脚本打印出的URL与`todo`应用程序进行交互。这将确保有足够的指标可以显示，并且已经有足够的时间让Prometheus抓取这些数据。接下来，再次打开Prometheus
    Web界面，或者点击任何Prometheus网页顶部的**Prometheus**，返回主页。然后，在查询框中输入**api_success**并按下ENTER。自定义的`todo`指标应该会显示出来，如[图20-7](ch20.xhtml#ch20fig7)所示。
- en: '![Image](../images/f0343-01.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0343-01.jpg)'
- en: '*Figure 20-7: Todo metric query*'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*图20-7：Todo指标查询*'
- en: We’re now able to monitor both the Kubernetes cluster and the `todo` application.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以监控Kubernetes集群和`todo`应用程序了。
- en: Final Thoughts
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最后的思考
- en: In this chapter, we’ve explored how the various features of containers and Kubernetes
    come together to enable us to deploy a scalable, resilient application. We’ve
    used everything we learned about containers—Deployments, Services, networking,
    persistent storage, Kubernetes Operators, and role-based access control—to not
    only deploy the `todo` application but also configure Prometheus monitoring of
    our cluster and our application.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了容器和Kubernetes的各种功能是如何结合在一起，使我们能够部署一个可扩展、具有弹性的应用程序。我们使用了关于容器的一切知识——部署（Deployments）、服务（Services）、网络、持久存储、Kubernetes运维管理器（Operators）和基于角色的访问控制（RBAC）——不仅部署了`todo`应用，还配置了我们集群和应用的Prometheus监控。
- en: Kubernetes is a complex platform with many different capabilities, and new capabilities
    are being added all the time. The purpose of this book is not only to show you
    the most important features you need to run an application on Kubernetes, but
    also to give you the tools to explore a Kubernetes cluster for troubleshooting
    and performance monitoring. As a result, you should be equipped to explore new
    features as they are added to Kubernetes and to conquer the challenges of deploying
    complex applications and getting them to perform well.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是一个功能复杂的平台，具有许多不同的能力，而且新的功能正在不断增加。本书的目的不仅仅是向你展示运行Kubernetes应用所需的最重要功能，还为你提供工具来探索Kubernetes集群，进行故障排除和性能监控。因此，你应该能够在Kubernetes添加新功能时，能够探索这些功能，并克服部署复杂应用程序并使其表现良好的挑战。
