<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch21"><span epub:type="pagebreak" id="page_143"/><strong><span class="big">21</span><br/>DATA-CENTRIC AI</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="noindent">What is data-centric AI, how does it compare to the conventional modeling paradigm, and how do we decide whether it’s the right fit for a project?</p>&#13;
<p class="indent">Data-centric AI is a paradigm or workflow in which we keep the model training procedure fixed and iterate over the dataset to improve the predictive performance of a model. The following sections define what data-centric AI means in more detail and compare it to conventional model-centric approaches.</p>&#13;
<h3 class="h3" id="ch00lev104"><strong>Data-Centric vs. Model-Centric AI</strong></h3>&#13;
<p class="noindent">In the context of data-centric AI, we can think of the conventional workflow, which is often part of academic publishing, as model-centric AI. However, in an academic research setting, we are typically interested in developing new methods (for example, neural network architectures or loss functions). Here, we consider existing benchmark datasets to compare the new method to previous approaches and determine whether it is an improvement over the status quo.</p>&#13;
<p class="indent"><a href="ch21.xhtml#ch21fig1">Figure 21-1</a> summarizes the difference between data-centric and model-centric workflows.<span epub:type="pagebreak" id="page_144"/></p>&#13;
<div class="image"><img id="ch21fig1" src="../images/21fig01.jpg" alt="Image" width="1106" height="658"/></div>&#13;
<p class="figcap"><em>Figure 21-1: Data-centric versus model-centric machine learning workflow</em></p>&#13;
<p class="indent">While <em>data-centric AI</em> is a relatively new term, the idea behind it is not. Many people I’ve spoken with say they used a data-centric approach in their projects before the term was coined. In my opinion, data-centric AI was created to make “caring about data quality” attractive again, as data collection and curation are often considered tedious or thankless. This is analogous to how the term <em>deep learning</em> made neural networks interesting again in the early 2010s.</p>&#13;
<p class="indent">Do we need to choose between data-centric and model-centric AI, or can we rely on both? In short, data-centric AI focuses on changing the data to improve performance, while model-centric approaches focus on modifying the model to improve performance. Ideally, we should use both in an applied setting where we want to get the best possible predictive performance. However, in a research setting or an exploratory stage of an applied project, working with too many variables simultaneously is messy. If we change both model and data at once, it’s hard to pinpoint which change is responsible for the improvement.</p>&#13;
<p class="indent">It is important to emphasize that data-centric AI is a paradigm and work-flow, not a particular technique. Data-centric AI therefore implicitly includes the following:</p>&#13;
<ul>&#13;
<li class="noindent">Analyses and modifications of training data, from outlier removal to missing data imputation</li>&#13;
<li class="noindent">Data synthesis and data augmentation techniques<span epub:type="pagebreak" id="page_145"/></li>&#13;
<li class="noindent">Data labeling and label-cleaning methods</li>&#13;
<li class="noindent">The classic active learning setting where a model suggests which data points to label</li>&#13;
</ul>&#13;
<p class="indent">We consider an approach <em>data centric</em> if we change only the data (using the methods listed here), not the other aspects of the modeling pipeline.</p>&#13;
<p class="indent">In machine learning and AI, we often use the phrase “garbage in, garbage out,” meaning that poor-quality data will result in a poor predictive model. In other words, we cannot expect a well-performing model from a low-quality dataset.</p>&#13;
<p class="indent">I’ve observed a common pattern in applied academic projects that attempt to use machine learning to replace an existing methodology. Often, researchers have only a small dataset of examples (say, hundreds of training examples). Labeling data is often expensive or considered boring and thus best avoided. In these cases, the researchers spend an unreasonable amount of time trying out different machine-learning algorithms and model tuning. To resolve this issue, investing additional time or resources in labeling additional data would be worthwhile.</p>&#13;
<p class="indent">The main advantage of data-centric AI is that it puts the data first so that if we invest resources to create a higher-quality dataset, all modeling approaches will benefit from it downstream.</p>&#13;
<h3 class="h3" id="ch00lev105"><strong>Recommendations</strong></h3>&#13;
<p class="noindent">Taking a data-centric approach is often a good idea in an applied project where we want to improve the predictive performance to solve a particular problem. In this context, it makes sense to start with a modeling baseline and improve the dataset since it’s often more worthwhile than trying out bigger, more expensive models.</p>&#13;
<p class="indent">If our task is to develop a new or better methodology, such as a new neural network architecture or loss function, a model-centric approach might be a better choice. Using an established benchmark dataset without changing it makes it easier to compare the new modeling approach to previous work. Increasing the model size usually improves performance, but so does the addition of training examples. Assuming small training sets (&lt; 2<em>k</em>) for classification, extractive question answering, and multiple-choice tasks, adding a hundred examples can result in the same performance gain as adding billions of parameters.</p>&#13;
<p class="indent">In a real-world project, alternating between data-centric and model-centric modes makes a lot of sense. Investing in data quality early on will benefit all models. Once a good dataset is available, we can begin to focus on model tuning to improve performance.<span epub:type="pagebreak" id="page_146"/></p>&#13;
<h3 class="h3" id="ch00lev106"><strong>Exercises</strong></h3>&#13;
<p class="number1"><strong>21-1.</strong> A recent trend is the increased use of predictive analytics in healthcare. For example, suppose a healthcare provider develops an AI system that analyzes patients’ electronic health records and provides recommendations for lifestyle changes or preventive measures. For this, the provider requires patients to monitor and share their health data (such as pulse and blood pressure) daily. Is this an example of data-centric AI?</p>&#13;
<p class="number1"><strong>21-2.</strong> Suppose we train a ResNet-34 convolutional neural network to classify images in the CIFAR-10 and ImageNet datasets. To reduce overfitting and improve classification accuracy, we experiment with data augmentation techniques such as image rotation and cropping. Is this approach data centric?</p>&#13;
<h3 class="h3" id="ch00lev107"><strong>References</strong></h3>&#13;
<ul>&#13;
<li class="noindent">An example of how adding more training data can benefit model performance more than an increase in model size: Yuval Kirstain et al., “A Few More Examples May Be Worth Billions of Parameters” (2021), <em><a href="https://arxiv.org/abs/2110.04374">https://arxiv.org/abs/2110.04374</a></em>.</li>&#13;
<li class="noindent">Cleanlab is an open source library that includes methods for improving labeling errors and data quality in computer vision and natural language processing contexts: <em><a href="https://github.com/cleanlab/cleanlab">https://github.com/cleanlab/cleanlab</a></em>.</li>&#13;
</ul>&#13;
</div>
</div>
</body></html>