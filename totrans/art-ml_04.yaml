- en: '**3'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BIAS, VARIANCE, OVERFITTING, AND CROSS-VALIDATION**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We now look in detail at a vital topic touched on in [Sections 1.7](ch01.xhtml#ch01lev7),
    [1.12.4](ch01.xhtml#ch01lev12sec4), and [2.2.5](ch02.xhtml#ch02lev2sec5)—overfitting.
    In this chapter, we’ll explain what bias and variance really mean in ML contexts
    and how they affect overfitting. We’ll then cover a popular approach to avoiding
    overfitting known as *cross-validation*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem of overfitting exemplifies the point made in the title of this
    book: ML is an art, not a science. There is no formulaic solution to various problems,
    especially overfitting. Professor Yaser Abu-Mostafa of Caltech, a prominent ML
    figure, once summed it up: “The ability to avoid overfitting is what separates
    professionals from amateurs in ML.”^([1](footnote.xhtml#ch3fn1)) And my Google
    query on “overfitting” yielded 6,560,000 results!'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t be intimidated. The professor is correct, but avoiding overfitting is
    not difficult, provided one has a good understanding of bias and variance. One
    uses this understanding and gains skill through experience.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Overfitting and Underfitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, what is all the fuss about overfitting? We’ve given a hint here and there
    in earlier chapters. Now let’s go into the topic in depth.
  prefs: []
  type: TYPE_NORMAL
- en: Recall our discussion in [Section 1.7](ch01.xhtml#ch01lev7) of the Bias-Variance
    Trade-off involved in choosing hyperparameter values, specifically the value *k*
    in k-NN. Once again, let’s take the bike sharing data ([Section 1.1](ch01.xhtml#ch01lev1))
    as our motivating example. As before, say we wish to predict ridership, such as
    for a day in which the temperature is 28 degrees. We will look at the days in
    our data with temperatures nearest to 28\. Our predicted value will be the average
    ridership among those days.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we take *k* = 5\. Even those outside the technology world might intuitively
    feel that a value of 5 for *k* is “too small a sample.” There is too much variability
    in ridership from one set of 5 days to another, even if their temperatures are
    near 28\. If we had a sample from a different set of 731 days than the one we
    have, we’d have a different set of 5 closest days to 28, with a different average
    ridership. With *k* = 50, a lot of high and low ridership values would largely
    cancel out during the averaging process, but not so with just *k* = 5\. This argues
    for choosing a larger value than 5 for *k*. This is a *variance* issue: choosing
    too small a value for *k* brings us too much sampling variability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if we use, say, the *k* = 25 days with temperatures closest
    to 28, we risk getting some days whose temperatures are rather far from 28\. Say,
    for instance, the 25th-closest day had a temperature of 35\. People do not want
    to ride bikes in such hot weather. If we include too many hot days in our prediction
    for the 28-degree day, we will have a tendency to underpredict the true ridership.
    In such a situation, *k* = 25 may be too large. That’s a *bias* issue: choosing
    too large a value of *k* may induce a systemic tendency to underpredict or overpredict.'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*We’ll repeatedly mention variance and bias in this chapter and in later ones.
    It’s important to keep in mind what quantity’s variance and bias is under discussion:
    predicted values. Say we are predicting ridership for a 28-degree day. The larger
    the value of* k *we use, the lesser the variability in our predicted value, but
    the greater the bias of that value.*'
  prefs: []
  type: TYPE_NORMAL
- en: Variance and bias are at odds with each other. For a given dataset, we can reduce
    one only at the expense of the other. This trade-off is central to choosing the
    values of hyperparameters, as well as choosing which features to use. Using too
    small a *k*—trying to reduce bias below what is possible on this data—is called
    *overfitting*. Using too large a *k*—an overly conservative one—is called *underfitting*.
    We hope to choose our hyperparameter values in the “sweet spot,” neither overfitting
    nor underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '***3.1.1 Intuition Regarding the Number of Features and Overfitting***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A similar statement holds for features: using too large a value for *p* (that
    is, the number of features) results in overfitting, while using too small a value
    gives us underfitting. Here is the intuition behind this.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall `mlb`, the dataset on Major League Baseball players (in [Section 1.8](ch01.xhtml#ch01lev8)).
    We might predict weight from height and age. But what if we were to omit height
    from our feature set? That would induce a bias. Roughly speaking, we’d be tacitly
    assuming everyone is of middling height, which would result in our tending to
    overpredict the weight of shorter players while underpredicting that of the taller
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, it turns out that the more predictors we use (in general,
    not just for this data), the higher the variance of our predicted values. To see
    this, say we are conducting a marketing study, predicting purchases of winter
    parkas, and wish to account for geography of customers. There are about 42,000
    ZIP codes (US postal codes). Say we use ZIP code as one of our features in predicting
    purchases. We would then have 42,000 dummy variables and would have other features
    such as age, gender, and income, or *p* > 42000\. If our data consists of, say,
    100,000 customers, we would have on average only 2 or 3 data points per ZIP code.
    Again, even nontechies would point out that this is far too small a sample, causing
    variance to rise. In other words, having too large a value of *p* increases variance.
    Once again, we see a tension between variance and bias.
  prefs: []
  type: TYPE_NORMAL
- en: '***3.1.2 Relation to Overall Dataset Size***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: But there’s more. In choosing a “good” value of *k* or *p*, we need to take
    into consideration *n*, the number of data points we have. Recall that in the
    bike sharing example, we had *n* = 731 (that is, only 731 days’ worth of data).
    Is that large enough to make good predictions? Why should that number matter?
    Actually, it relates directly to the Bias-Variance Trade-off. Here’s why.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our bike sharing example above, we worried that with *k* = 25 nearest neighbors,
    we might have some days among those 25 whose temperatures are rather far from
    28\. But if we had, say, 2,000 days instead of 731, the 25th-closest might still
    be pretty close to 28\. In other words:'
  prefs: []
  type: TYPE_NORMAL
- en: The larger *n* is, the larger we can make *k* while still avoiding overly large
    bias.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, consider the ZIP code issue mentioned above. With 100,000 customers,
    we would have on average only 2 or 3 data points per ZIP code. But what if our
    dataset consisted of 50 million customers? Then it may be useful to include the
    dummies for ZIP codes, as we may have a sufficient number of customers from most
    ZIP codes. Remember, *p* denotes the number of features, and this counts each
    dummy variable separately. Thus, inclusion of ZIP codes in our feature set would
    increase *p* by about 42,000.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words:'
  prefs: []
  type: TYPE_NORMAL
- en: The larger *n* is, the larger the value we can use for *p*—that is, the more
    features we can use while still avoiding overly large variance.
  prefs: []
  type: TYPE_NORMAL
- en: '***3.1.3 Well Then, What Are the Best Values of k and p?***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mind you, this still doesn’t tell us how to set a good “Goldilocks” value of
    *k*—not too small and not too large. The same holds for choosing *p* (that is,
    choosing the number of features to use); in fact, it’s an even more challenging
    problem, as it is a question of not only *how many* features to use but also *which
    ones*.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have stated so many times:'
  prefs: []
  type: TYPE_NORMAL
- en: This is a fact of life in machine learning. For most issues, there are no neat,
    magic-formula answers. Again, ML is an art, not a science. However, holdout methods
    are used in practice, and they generally work pretty well, especially as the analyst
    gains experience.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll present holdout methods in full detail later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, a rough rule of thumb, suggested by some mathematical theory, is to follow
    this limitation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch03equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That is, the number of nearest neighbors should be less than the square root
    of the number of data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'What about choosing *p*? As noted, a feature set is not “large” or “small”
    on its own. Instead, its size *p* must be viewed relative to the number of data
    points *n*. Overfitting can arise by using too many features for a given dataset
    size. In classical statistics, a rough—though in my experience, conservative—
    rule of thumb has been to follow another “square root of *n*” limitation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch03equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That is, the number of features should be less than the square root of the number
    of data points. Under this criterion, if our data frame has, say, 1,000 rows,
    it can support about 30 features. This is not a bad rough guide and is supported
    by theoretical results for parametric models.
  prefs: []
  type: TYPE_NORMAL
- en: However, in modern statistics and ML, it is now common to have—or at least start
    with—a value of *p* much larger than *n*. We will see this with certain methods
    used later in the book. We’ll stick with ![Image](../images/prootn.jpg) as a reasonable
    starting point. If our data satisfies that rule, we can feel safe. But if *p*
    is larger, we should not automatically consider it to be overly large.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Cross-Validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most common approach to choosing the value of hyperparameters or choosing
    feature sets is to minimize MAPE (numeric- *Y* case) or the overall misclassification
    error (OME, classification case). For k-NN and a numeric- *Y* setting, we may
    find MAPE for each of a range of candidate values of *k* and then choose the one
    producing minimal MAPE.
  prefs: []
  type: TYPE_NORMAL
- en: 'In deciding what value of *k* to use, we need to assess the predictive ability
    of various values of that hyperparameter. But in doing so, we need to make sure
    we are using a “fresh” dataset to predict. This motivates splitting the data into
    two sets: a training set and a holdout, or test, set.'
  prefs: []
  type: TYPE_NORMAL
- en: However, holdout sets are chosen randomly. This induces additional randomness,
    on top of the sampling variation we already have. We saw an example of this in
    [Section 1.12.3](ch01.xhtml#ch01lev12sec3). So, in choosing *k* in k-NN, for instance,
    one holdout set may indicate *k* = 5 as best, while another would favor *k* =
    12\. To be thorough, we should not rely on a single holdout set. This leads to
    the method of *K-fold cross-validation*, where we generate many holdout sets,
    averaging MAPE, OME, or other criterion over all those sets. Note that *k*, the
    number of neighbors, is different from *K*, the number of *folds*, or possible
    holdout sets.
  prefs: []
  type: TYPE_NORMAL
- en: '***3.2.1 K-Fold Cross-Validation***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To see how K-fold cross-validation works, consider the “leaving one out” method,
    in which we set a holdout set size of 1\. Say we wish to evaluate the predictive
    ability of *k* = 5\. For each of our *n* data points, we would take the holdout
    set to be that point and take the remaining *n* − 1 points as our training set;
    we then predict the holdout point. This gives us *n* predictions, and we calculate
    MAPE as the average absolute prediction error among those *n* predictions. In
    other words, we would proceed as in the following pseudocode for data frame `d`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We’d call this *n*-fold cross-validation. Alternatively, we could take our holdout
    sets to have size 2, say, by partitioning the set 1,2, . . . ,*n* into non-overlapping
    adjacent pairs. Now there are *n*/2 possible holdout sets (folds). For each fold,
    we apply k-NN to the remaining data and then predict the data in that fold. MAPE
    is then the average over the *n*/2 folds.
  prefs: []
  type: TYPE_NORMAL
- en: One might expect that *K* = *n* is best, since then MAPE will be based on the
    most trials. On the other hand, each trial will be based on predicting just 1
    data point, which is presumably less accurate. There also may be computational
    and theoretical issues that we won’t go into here. How should we then choose *K*?
  prefs: []
  type: TYPE_NORMAL
- en: Note that *K* is not a hyperparameter, as it is not a trait of k-NN. It is simply
    a matter of how to estimate MAPE reliably. But yes, it’s one more thing to think
    about. Many analysts recommend using a value of 5 or 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach is as follows, say, for holdout sets of size 2\. We simply
    choose many random holdout sets, as many as we have time for, as in the following
    pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, *r* is the number of holdout sets. The larger value we choose for *r*,
    the more accurate MAPE will be. It just depends on how much computation time we
    wish to expend. (The plurals, such as `predicted Ys`, allude to the fact that
    any holdout set has two Y values to predict.)
  prefs: []
  type: TYPE_NORMAL
- en: '***3.2.2 Using the replicMeans() Function***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can use the `regtools` function `replicMeans()` to implement K-means cross-validation.
    The function name is short for “replicate an action and then take the mean of
    the results.”
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, say we have some data frame `d` in which we are predicting a
    column `y`. Consider the effect of the following call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This says to run `cmd` 10 times and return the mean of the result. Since the
    command is to run `qeKNN()`, 10 runs will use 10 different holdout sets, yielding
    10 different values of `testAcc`. The end result will be that the function returns
    the average of those 10 values, which is exactly what we want.
  prefs: []
  type: TYPE_NORMAL
- en: '***3.2.3 Example: Programmer and Engineer Data***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here we will introduce a new dataset, `pef`, to be used at several points in
    the book, and illustrate cross-validation on this data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `pef` dataset is included in the `regtools` package, which in turn is included
    in `qeML`. It is drawn from the 2000 US census, showing data on programmers and
    engineers. Here is a glimpse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: So, data on a bit more than 20,000 workers is stored here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The education variable here needs some explanation. The census has codes for
    education of various levels, down to even none at all. But for this dataset, there
    won’t be many (if any) workers with, say, just a sixth-grade education. For that
    reason, the `educ` column here has been simplified to just three levels: master’s
    (code 14), PhD (16), and “other” (coded as `zzzOther` by the software, `regtools::toSubFactor()`).
    Most of the “other” workers have a bachelor’s degree, but even those with less
    have been lumped into this level.'
  prefs: []
  type: TYPE_NORMAL
- en: Why do this? The `qe*`-series functions convert any feature that is an R factor
    to dummy variables, and for some such functions, the output is displayed in terms
    of the dummies. So, consolidation as above compactifies output. Even running `head()`
    would give very wide output if all education levels were included and dummy variables
    were displayed.
  prefs: []
  type: TYPE_NORMAL
- en: Second, simplification of this nature may, in general, be needed to avoid overfitting—remember,
    each dummy variable counts separately in the feature count *p*—even though in
    this dataset we are well within the “![Images](../images/prootn.jpg)” rule of
    thumb.
  prefs: []
  type: TYPE_NORMAL
- en: For detailed information on this dataset, such as the various occupation codes,
    type ?pef at the R prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3.1 Improved Estimation of MAPE
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Suppose we wish to predict `wageinc`, wage income, in this `pef` dataset. Let’s
    take a first cut at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'On average, our predictions are off by about $25,300\. This is a rather large
    number, but as emphasized in [Section 2.4](ch02.xhtml#ch02lev4), we must always
    gauge prediction accuracy of a feature set compared to predicting *without* the
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: So, just predicting everyone to have the overall mean income would give us a
    much larger MAPE.
  prefs: []
  type: TYPE_NORMAL
- en: At any rate, our point here concerns not this particular dataset but the general
    accuracy of MAPE if the latter is based on just a single holdout set. We really
    need to look at multiple holdout sets using cross-validation. Let’s do that using
    `replicMeans()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: So, the indicated `qeKNN()` call was run 10 times, yielding 10 holdout sets,
    having an average value of about $25,633 for accuracy on the test set. This is
    somewhat larger than the $25,296 figure we had obtained earlier based on just
    one holdout set. Thus, we should treat this new figure as more reliable.
  prefs: []
  type: TYPE_NORMAL
- en: That $412 number is the *standard error*. Multiplying it by 1.96 gives us the
    margin of error. If we feel that is too large, we can call `replicMeans()` with,
    say, 100 replications (that is, 100 holdout sets).
  prefs: []
  type: TYPE_NORMAL
- en: We could then try other values of *k*, running `replicMeans()` for each one
    as above and then finally choosing the value that gives the best MAPE or OME.
    If we have more than a few such values, it would be easier to use the `qeML` function
    `qeFT()`, which will be presented in [Chapter 7](ch07.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: '***3.2.4 Triple Cross-Validation***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose we split our data into training and test sets, and then fit many different
    combinations of hyperparameters, choosing the combination that does best on the
    test set. Again we run into the problem of potential p-hacking, meaning that the
    accuracy rates reported in the test set may be overly optimistic.
  prefs: []
  type: TYPE_NORMAL
- en: One common solution is to partition the data into three subsets rather than
    two, with the intermediate one being termed the *validation set*. We fit the various
    combinations of hyperparameters to the training set and evaluate them on the validation
    set. After choosing the best combination, we then evaluate (only) that combination
    on the test set to obtain an accuracy estimate untainted by p-hacking.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Conclusions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In summary, the main concepts in this brief but vital chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: In choosing a hyperparameter such as k-NN’s *k*, and in choosing a feature set,
    variance and bias are at odds with each other. For a fixed dataset, a small *k*
    or large *p* increases variance while reducing bias, and vice versa.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a larger *n*, we can afford to take a larger value of *k* or *p*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, there is no hard-and-fast formula for the “Goldilocks” values
    of *k* and *p*. But there are some very rough rules of thumb, and careful use
    of holdout sets and cross-validation will serve us pretty well. As one gains experience,
    one also becomes more skilled at this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, use of holdout sets is the main remedy, including using multiple holdout
    sets if there is concern about accuracy of MAPE or OME on a single set.
  prefs: []
  type: TYPE_NORMAL
