- en: '**8**'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**8**'
- en: '**BUILDING MACHINE LEARNING DETECTORS**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建机器学习检测器**'
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: Today, thanks to high-quality open source software that handles the heavy mathematical
    lifting of implementing machine learning systems, anyone who knows basic Python
    and understands the key concepts can use machine learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，得益于高质量的开源软件，它处理了实现机器学习系统的繁重数学工作，任何了解基本Python并掌握关键概念的人都可以使用机器学习。
- en: In this chapter, I show you how to build machine learning malware detection
    systems using `scikit-learn`, the most popular—and the best, in my opinion—open
    source machine learning package available. This chapter contains a lot of sample
    code. The major code blocks are accessible in the directory *malware_data_science/ch8/code*,
    and the corresponding sample data is accessible in the directory *malware_data_science/ch8/data*
    in the code and data (and on the virtual machine) accompanying this book.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将向你展示如何使用`scikit-learn`（我认为最受欢迎且最好的开源机器学习包）构建机器学习恶意软件检测系统。本章包含了大量的示例代码。主要的代码块可以在目录*malware_data_science/ch8/code*中访问，相应的示例数据可以在本书附带的代码和数据（以及虚拟机）中的目录*malware_data_science/ch8/data*中找到。
- en: By following along with the text, examining the sample code, and trying out
    the provided examples, you should be comfortable building and evaluating your
    own machine learning systems by the end of the chapter. You also learn to build
    a general malware detector and use the necessary tools to build malware detectors
    for specific malware families. The skills you gain here will have a broad application,
    allowing you to apply machine learning to other security problems, such as detecting
    malicious emails or suspicious network streams.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通过跟随文本、检查示例代码，并尝试提供的示例，你应该能够在本章结束时自信地构建和评估自己的机器学习系统。你还将学会构建一个通用的恶意软件检测器，并使用必要的工具为特定恶意软件家族构建恶意软件检测器。你在这里获得的技能将有广泛的应用，使你能够将机器学习应用于其他安全问题，比如检测恶意邮件或可疑的网络流量。
- en: First, you learn the terminology and concepts you need to know before using
    `scikit-learn`. Then, you use `scikit-learn` to implement a basic decision tree
    detector based on the decision tree concepts you learned in [Chapter 6](ch06.xhtml#ch06).
    Next, you learn how to integrate feature extraction code with `scikit-learn` to
    build a real-world malware detector that uses real-world feature extraction and
    a random forest approach to detect malware. Finally, you learn how to use `scikit-learn`
    to evaluate machine learning systems with the sample random forest detector.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将学习在使用`scikit-learn`之前需要了解的术语和概念。接下来，你将使用`scikit-learn`实现一个基于你在[第六章](ch06.xhtml#ch06)中学到的决策树概念的基本决策树检测器。然后，你将学习如何将特征提取代码与`scikit-learn`集成，构建一个使用真实世界特征提取和随机森林方法检测恶意软件的实际恶意软件检测器。最后，你将学习如何使用`scikit-learn`评估机器学习系统，并通过示例的随机森林检测器进行验证。
- en: '**Terminology and Concepts**'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**术语与概念**'
- en: Let’s review some terminology first. The open source library `scikit-learn`
    (`sklearn` for short) has become popular in the machine learning community because
    it’s both powerful and easy to use. Many data scientists use the library within
    the computer security community and in other fields, and many rely on it as their
    main toolbox for performing machine learning tasks. Although `sklearn` is constantly
    being updated with new machine learning approaches, it provides a consistent programming
    interface that makes using these machine learning approaches simple.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们回顾一下术语。开源库`scikit-learn`（简称`sklearn`）在机器学习社区中变得非常流行，因为它既强大又易于使用。许多数据科学家在计算机安全社区以及其他领域使用该库，许多人将其作为执行机器学习任务的主要工具箱。尽管`sklearn`不断更新，引入新的机器学习方法，但它提供了一个一致的编程接口，使得使用这些机器学习方法变得简单。
- en: Like many machine learning frameworks, `sklearn` requires training data in *vector*
    form. Vectors are arrays of numbers where each index in the array corresponds
    to a single feature of the training example software binary. For example, if the
    two features of software binaries our machine learning detector uses are `is compressed`
    and `contains encrypted data`, then our feature vector for a training example
    binary could be `[0,1]`. Here, the first index in the vector would represent whether
    or not the binary is compressed, with the zero indicating “no,” and the second
    index would represent whether or not the binary contains encrypted data, with
    the one indicating “yes.”
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 像许多机器学习框架一样，`sklearn` 需要以 *向量* 形式提供训练数据。向量是数字数组，其中数组的每个索引对应训练示例软件二进制文件的单个特征。例如，如果我们的机器学习检测器使用的软件二进制文件的两个特征是
    `是否压缩` 和 `是否包含加密数据`，那么训练示例二进制文件的特征向量可能是 `[0,1]`。在这里，向量中的第一个索引表示二进制文件是否压缩，零表示“否”，第二个索引表示二进制文件是否包含加密数据，1
    表示“是”。
- en: Vectors can be awkward to work with because you have to remember what feature
    each index maps to. Fortunately, `sklearn` provides helper code that translates
    other data representations to vector form. For example, you can use `sklearn`’s
    `DictVectorizer` class to transform dictionary representations of your training
    data (for instance, `{"is compressed":1,"contains encrypted` `data":0}`) into
    the vector representation that `sklearn` operates on, like `[0,1]`. Later, you
    can use the `DictVectorizer` to recover the mapping between the vector’s indices
    and the original feature names.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 向量的使用可能会有些麻烦，因为你需要记住每个索引映射到的特征。幸运的是，`sklearn` 提供了辅助代码，可以将其他数据表示形式转换为向量形式。例如，你可以使用
    `sklearn` 的 `DictVectorizer` 类将训练数据的字典表示（例如 `{"is compressed":1,"contains encrypted
    data":0}`）转换为 `sklearn` 操作的向量表示形式，比如 `[0,1]`。之后，你可以使用 `DictVectorizer` 恢复向量索引与原始特征名称之间的映射关系。
- en: 'To train an `sklearn`-based detector, you need to pass in two separate objects
    to `sklearn`: feature vectors (as described previously) and a label vector. A
    *label vector* contains one number per training example, which corresponds, in
    our case, to whether or not the example is malware or benignware. For instance,
    if we pass three training examples to `sklearn`, and then pass the label vector
    `[0,1,0]`, we’re telling `sklearn` that the first sample is benignware, the second
    sample is malware, and the third is benignware. By convention, machine learning
    engineers use a capital `X` variable to represent the training data and a lowercase
    `y` variable to represent the labels. The difference in case reflects the convention
    in mathematics of capitalizing variables that represent matrices (which we can
    think of as arrays of vectors) and lowercasing variables that represent individual
    vectors. You’ll see this convention used in machine learning sample code online,
    and I use this convention for the remainder of this book to get you comfortable
    with it.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练基于 `sklearn` 的检测器，您需要将两个独立的对象传递给 `sklearn`：特征向量（如前所述）和标签向量。*标签向量* 每个训练示例包含一个数字，在我们的例子中，这个数字表示示例是恶意软件还是良性软件。例如，如果我们将三个训练示例传递给
    `sklearn`，然后传递标签向量 `[0,1,0]`，我们就在告诉 `sklearn` 第一个样本是良性软件，第二个样本是恶意软件，第三个样本是良性软件。根据惯例，机器学习工程师使用大写的
    `X` 变量表示训练数据，使用小写的 `y` 变量表示标签。大小写的区别反映了数学中大写变量表示矩阵（我们可以将其视为向量数组），小写变量表示单个向量的惯例。你将在在线机器学习示例代码中看到这种惯例，在本书的后续部分，我也会使用这种惯例，以帮助你熟悉它。
- en: The `sklearn` framework uses other terminology that you might find new as well.
    Instead of calling machine learning–based detectors “detectors,” `sklearn` calls
    them “classifiers.” In this context, the term *classifier* simply means a machine
    learning system that categorizes things into two or more categories. Therefore,
    a *detector* (the term I use throughout this book) is a special type of a classifier
    that places things into two categories, like malware and benignware. Also, instead
    of using the term *training*, `sklearn`’s documentation and API often use the
    term *fit*. For example, you’ll see a sentence like “fit a machine learning classifier
    using training examples,” which is the equivalent to saying “train a machine learning
    detector using training examples.”
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`框架使用了你可能会觉得陌生的其他术语。`sklearn`并不称机器学习基础的检测器为“检测器”，而是称其为“分类器”。在这个上下文中，术语*分类器*仅指一个将事物归类为两个或更多类别的机器学习系统。因此，*检测器*（这是我在本书中始终使用的术语）是分类器的一种特殊类型，将事物归入两个类别，比如恶意软件和良性软件。此外，`sklearn`的文档和API通常不使用*训练*这个术语，而是使用*fit*。例如，你会看到类似“使用训练示例拟合机器学习分类器”这样的句子，这相当于说“使用训练示例训练机器学习检测器”。'
- en: Finally, instead of using the term *detect* in the context of classifiers, `sklearn`
    uses the term *predict*. This term is used in `sklearn`’s framework, and in the
    machine learning community more generally, whenever a machine learning system
    is used to perform a task, whether to predict the value of a stock a week from
    now or detect whether an unknown binary is malware.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在分类器的上下文中，`sklearn`并未使用*detect*这个术语，而是使用了*predict*这个术语。这个术语在`sklearn`的框架中，以及在更广泛的机器学习社区中被使用，当一个机器学习系统用于执行某个任务时，无论是预测一周后股票的价值，还是检测某个未知二进制文件是否为恶意软件。
- en: '**Building a Toy Decision Tree–Based Detector**'
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**构建一个基于决策树的玩具检测器**'
- en: Now that you’re familiar with `sklearn`’s technical terminology, let’s create
    a simple decision tree along the lines of what we discussed in [Chapter 6](ch06.xhtml#ch06),
    using the `sklearn` framework. Recall that decision trees play a “20 questions”
    type of game in which they ask a series of questions about input vectors to arrive
    at a decision concerning whether these vectors are malicious or benign. We walk
    through building a decision tree classifier, step by step, and then explore an
    example of a complete program. [Listing 8-1](ch08.xhtml#ch08list1) shows how to
    import the requisite modules from `sklearn`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经熟悉了`sklearn`的技术术语，让我们创建一个简单的决策树，按照我们在[第六章](ch06.xhtml#ch06)中讨论的内容，使用`sklearn`框架来实现。回想一下，决策树就像是在玩“20个问题”的游戏，它会对输入向量提出一系列问题，以决定这些向量是恶意的还是良性的。我们将一步一步地构建一个决策树分类器，然后探讨一个完整程序的示例。[清单
    8-1](ch08.xhtml#ch08list1)展示了如何从`sklearn`导入所需的模块。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Listing 8-1: Importing* sklearn *modules*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-1：导入* sklearn *模块*'
- en: The first module we import, `tree`, is `sklearn`’s decision tree module. The
    second module, `feature_extraction`, is `sklearn`’s helper module from which we
    import the `DictVectorizer` class. The `DictVectorizer` class conveniently translates
    the training data provided in readable, dictionary form to the vector representation
    that `sklearn` requires to actually train machine learning detectors.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入的第一个模块是`tree`，这是`sklearn`的决策树模块。第二个模块是`feature_extraction`，它是`sklearn`的辅助模块，我们从中导入了`DictVectorizer`类。`DictVectorizer`类方便地将以可读字典形式提供的训练数据转换为`sklearn`所需的向量表示，以实际训练机器学习检测器。
- en: After we import the modules we need from `sklearn`, we instantiate the requisite
    `sklearn` classes, as shown in [Listing 8-2](ch08.xhtml#ch08list2).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们从`sklearn`导入所需模块之后，我们实例化了所需的`sklearn`类，如[清单 8-2](ch08.xhtml#ch08list2)所示。
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Listing 8-2: Initializing the decision tree classifier and vectorizer*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-2：初始化决策树分类器和向量化器*'
- en: The first class we instantiate, `DecisionTreeClassifier` ➊, represents our detector.
    Although `sklearn` provides a number of parameters that control exactly how our
    decision tree will work, here we don’t select any parameters so that we’re using
    `sklearn`’s default decision tree settings.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实例化的第一个类是`DecisionTreeClassifier` ➊，它代表了我们的检测器。尽管`sklearn`提供了许多参数来精确控制我们的决策树如何工作，但在这里我们没有选择任何参数，因此我们使用的是`sklearn`的默认决策树设置。
- en: The next class we instantiate is `DictVectorizer` ➋. We set `sparse` to `False` ➌
    in the constructor, telling `sklearn` that we do not want it to use sparse vectors,
    which save memory but are complicated to work with. Because `sklearn`’s decision
    tree module can’t use sparse vectors, we turn this feature off.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们实例化的类是`DictVectorizer` ➋。我们在构造函数中将`sparse`设置为`False` ➌，告诉`sklearn`我们不希望使用稀疏向量，尽管稀疏向量节省内存但操作复杂。由于`sklearn`的决策树模块不能使用稀疏向量，我们关闭了这个功能。
- en: Now that we have instantiated our classes, we can initialize some sample training
    data, as shown in [Listing 8-3](ch08.xhtml#ch08list3).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实例化了我们的类，可以初始化一些示例训练数据，如[列表 8-3](ch08.xhtml#ch08list3)所示。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*Listing 8-3: Declaring training and label vectors*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 8-3：声明训练向量和标签向量*'
- en: In this example, we initialize two structures—feature vectors and a label vector—that
    together comprise our training data. The feature vectors, assigned to the `training_examples`
    variable ➊, are given in dictionary form. As you can see, we’re using two simple
    features. The first is `packed`, which represents whether a given file is packed,
    and the second is `contains_encrypted`, which represents whether the file contains
    encrypted data. The label vector, which is assigned to the `ground_truth` variable
    ➋, represents whether each training example is malicious or benign. In this book,
    and in general among security data scientists, 0 always stands for benign and
    1 always stands for malicious. In this case, the label vector declares that the
    first four feature vectors are malicious and the second four are benign.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们初始化了两个结构——特征向量和标签向量——它们一起构成了我们的训练数据。特征向量被赋值给`training_examples`变量 ➊，以字典形式给出。如你所见，我们使用了两个简单的特征。第一个是`packed`，表示一个文件是否被打包，第二个是`contains_encrypted`，表示文件是否包含加密数据。标签向量被赋值给`ground_truth`变量
    ➋，表示每个训练示例是恶意的还是良性的。在本书中，并且通常在安全数据科学家中，0总是代表良性，1总是代表恶意。在这种情况下，标签向量声明前四个特征向量是恶意的，后四个是良性的。
- en: '***Training Your Decision Tree Classifier***'
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***训练你的决策树分类器***'
- en: Now that we’ve declared our training vectors and label vector, let’s train our
    decision tree model by calling the decision tree class instance’s `fit` method,
    as shown in [Listing 8-4](ch08.xhtml#ch08list4).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经声明了训练向量和标签向量，让我们通过调用决策树类实例的`fit`方法来训练我们的决策树模型，如[列表 8-4](ch08.xhtml#ch08list4)所示。
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Listing 8-4: Initializing the* vectorizer *class with training data*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 8-4：用训练数据初始化* 向量化器 *类*'
- en: The code in [Listing 8-4](ch08.xhtml#ch08list4) first initializes the `vectorizer`
    class that we initialized in [Listing 8-2](ch08.xhtml#ch08list2) by calling the
    `fit` method ➊. Here, the `fit` method tells `sklearn` to create a mapping between
    the `packed` feature and the `contains_encrypted` feature and vector array indices.
    Then we transform our dictionary-based feature vectors into numerical vector form
    by calling the `vectorizer` class’s `transform` method ➋. Recall that we assign
    our feature vectors to a variable called `X` and our label vector to a variable
    called `y`, which is the naming convention in the machine learning community.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8-4](ch08.xhtml#ch08list4)中的代码首先初始化了我们在[列表 8-2](ch08.xhtml#ch08list2)中初始化的`vectorizer`类，通过调用`fit`方法
    ➊。这里，`fit`方法告诉`sklearn`创建`packed`特征和`contains_encrypted`特征与向量数组索引之间的映射。然后，我们通过调用`vectorizer`类的`transform`方法
    ➋，将基于字典的特征向量转换为数值向量形式。回想一下，我们将特征向量赋值给一个名为`X`的变量，将标签向量赋值给一个名为`y`的变量，这是机器学习领域中的命名约定。'
- en: 'Now that we’re all set up with our training data, we can train our decision
    tree detector by calling the `fit` method on the decision tree classifier instances,
    like this:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的训练数据已经准备好，我们可以通过在决策树分类器实例上调用`fit`方法来训练我们的决策树检测器，如下所示：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see, training the `sklearn` detector is as simple as that. But behind
    the scenes, `sklearn` is going through the algorithmic process of identifying
    a good decision tree for accurately detecting whether new software is malicious
    or benign, along the lines of the algorithm we discussed in the previous chapter.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，训练`sklearn`检测器就这么简单。但在背后，`sklearn`正在通过算法过程来识别一个好的决策树，以便准确地检测新软件是恶意的还是良性的，这与我们在上一章讨论的算法思路相符。
- en: Now that we’ve trained the detector, let’s use the code in [Listing 8-5](ch08.xhtml#ch08list5)
    to detect whether a binary is malicious or benign.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练好了检测器，接下来使用[列表 8-5](ch08.xhtml#ch08list5)中的代码来检测一个二进制文件是恶意的还是良性的。
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Listing 8-5: Determining whether a binary is malicious*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 8-5：判断二进制文件是否恶意*'
- en: Here, we instantiate a dictionary-based feature vector for a hypothetical software
    binary ➊, translate it to numerical vector form using `vectorizer` ➋, which we
    declared earlier in our code, and then run the decision tree detector we built
    ➌ to determine whether or not the binary is malicious. You’ll see when we run
    the code that the classifier “thinks” that the new binary is malicious (because
    it gives a “1” as its output), and you’ll see why this is the case when we visualize
    our decision tree.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们为一个假设的软件二进制文件实例化一个基于字典的特征向量➊，使用我们在代码中之前声明的`vectorizer`➋将其转换为数值向量形式，然后运行我们构建的决策树检测器➌，以确定该二进制文件是否恶意。当我们运行代码时，你会看到分类器“认为”该新二进制文件是恶意的（因为它的输出为“1”），当我们可视化决策树时，你将看到为什么会这样。
- en: '***Visualizing the Decision Tree***'
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***决策树的可视化***'
- en: We can visualize the decision tree that `sklearn` has automatically created
    based on our training data, as shown in [Listing 8-6](ch08.xhtml#ch08list6).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如[清单 8-6](ch08.xhtml#ch08list6)所示，直观地展示`sklearn`基于我们的训练数据自动创建的决策树。
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*Listing 8-6: Creating an image file of the decision tree using GraphViz*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-6：使用GraphViz创建决策树图像文件*'
- en: Here, we open a file called *classifier.dot* ➊ to which we write a network representation
    of our decision tree using the `export_graphviz()` function that `sklearn`’s `tree`
    module provides. Then we call `tree.export_graphviz` ➋ to write a GraphViz *.dot*
    file to *classifier.dot*, which writes a network representation of the decision
    tree to disk. Finally, we use the GraphViz `dot` command line program to create
    an image file that visualizes the decision tree, in a form that corresponds to
    what you learned about decision trees in [Chapter 6](ch06.xhtml#ch06). When you
    run this, you should get an output image file called *classifier.png* that looks
    like [Figure 8-1](ch08.xhtml#ch08fig1).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们打开一个名为*classifier.dot*的文件➊，并使用`sklearn`的`tree`模块提供的`export_graphviz()`函数将决策树的网络表示写入该文件。然后，我们调用`tree.export_graphviz`➋将一个GraphViz的*.dot*文件写入*classifier.dot*，该文件将决策树的网络表示写入磁盘。最后，我们使用GraphViz的`dot`命令行程序创建一个图像文件，以可视化决策树，形式与[第6章](ch06.xhtml#ch06)中学习的决策树相符。当你运行这个程序时，你应该会得到一个名为*classifier.png*的输出图像文件，像[图8-1](ch08.xhtml#ch08fig1)一样。
- en: '![image](../images/f0132-01.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0132-01.jpg)'
- en: '*Figure 8-1: Decision tree visualization*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8-1：决策树可视化*'
- en: 'Although this decision tree visualization should be familiar from [Chapter
    6](ch06.xhtml#ch06), it contains some new vocabulary. The first line in each box
    contains the name of the feature about which the node asks a question (in machine
    learning parlance, we say that the node “splits on” this feature). For example,
    the first node splits on the feature “packed”: if a binary is not packed, we move
    along the left arrow; otherwise, we move along the right arrow.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个决策树的可视化在[第6章](ch06.xhtml#ch06)中应该很熟悉，但它包含了一些新的词汇。每个框的第一行包含该节点提问的特征名称（在机器学习术语中，我们说节点“根据”该特征进行分裂）。例如，第一个节点根据特征“packed”进行分裂：如果二进制文件没有被打包，我们沿左箭头走；否则，我们沿右箭头走。
- en: The second line of text in each box refers to that node’s *gini index*, which
    measures how much inequality there is between the malware and benignware training
    examples that match that node. The higher the gini index, the more skewed the
    samples that match that node are toward either benignware or malware. This means
    that a high gini index in each node is good, because the more the training examples
    skew toward either malware or benignware, the more sure we are about whether new
    test examples are malware or benignware. The third line in each box just gives
    the number of training examples that matched that node.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每个框中第二行的文本表示该节点的*基尼指数*，它衡量与该节点匹配的恶意软件和良性软件训练样本之间的不平等程度。基尼指数越高，匹配该节点的样本越倾向于恶意软件或良性软件。这意味着每个节点中较高的基尼指数是好的，因为训练样本越倾向于恶意软件或良性软件，我们就越确信新的测试样本是恶意的还是良性的。每个框中的第三行只给出了与该节点匹配的训练样本数量。
- en: You’ll notice that in the leaf nodes of the tree, the text in the box is different.
    These nodes don’t “ask a question;” instead, they provide an answer to the question
    “is this binary malicious or benign?” For example, in the leftmost leaf node,
    we have “value = [2\. 1.],” which means that two benign training examples matched
    this node (not packed and not encrypted) and one malware training example matched
    the node. That is, if we reach this node, we’d assign a probability of 33 percent
    to the binary being malware (1 malware sample / 3 total samples = 33 percent).
    The gini value in these boxes shows how much information is gained about whether
    the binary is malware or benignware when we split on the question directly leading
    up to these nodes. As you can see, it can be useful to inspect visualizations
    of decision trees generated by `sklearn` to understand how our decision trees
    are making detections.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，在决策树的叶节点中，框中的文本有所不同。这些节点不是“提问”，而是提供了“这个二进制文件是恶意的还是良性的？”这个问题的答案。例如，在最左边的叶节点中，我们看到“value
    = [2\. 1.]”，这意味着两个良性训练样本与此节点匹配（没有打包且没有加密），一个恶意软件训练样本与此节点匹配。也就是说，如果我们到达这个节点，我们会给这个二进制文件分配33%的概率是恶意软件（1个恶意样本
    / 3个样本总数 = 33%）。这些框中的基尼值显示了在我们根据直接引导到这些节点的问题进行拆分时，关于二进制文件是恶意软件还是良性软件所获得的信息量。如你所见，检查`sklearn`生成的决策树可视化图可以帮助我们理解决策树是如何做出检测的。
- en: '***Complete Sample Code***'
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***完整示例代码***'
- en: '[Listing 8-7](ch08.xhtml#ch08list7) shows the complete code for the decision
    tree workflow I have described thus far. This code should be easily legible to
    you now that we have worked through the code, piece by piece.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8-7](ch08.xhtml#ch08list7)展示了我迄今为止描述的决策树工作流的完整代码。既然我们已经逐步解析过这段代码，它现在应该对你来说非常易读。'
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*Listing 8-7: Complete decision tree workflow sample code*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 8-7：完整的决策树工作流示例代码*'
- en: The sample machine learning malware detector we just explored demonstrates how
    to get started with `sklearn`’s functionality, but it’s missing some essential
    features required for a real-world malware detector. Let’s now explore what a
    real-world malware detector entails.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚探讨的示例机器学习恶意软件检测器展示了如何开始使用`sklearn`的功能，但它缺少一些构建真实世界恶意软件检测器所需的基本特性。现在，让我们探讨一下真实世界恶意软件检测器需要具备哪些内容。
- en: '**Building Real-World Machine Learning Detectors with sklearn**'
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**构建真实世界的机器学习检测器（使用sklearn）**'
- en: To build a real-world detector, you need to use industrial-strength features
    of software binaries as well as write code to extract these features from software
    binaries. Industrial-strength features are those that reflect the content of binaries
    in all their complexity, which means we need to use hundreds or thousands of features.
    By “extracting” features I mean that you have to write code that identifies the
    presence of these features within binaries. You also need to use thousands of
    training examples and train a machine learning model at scale. Finally, you need
    to use `sklearn`’s more advanced detection approaches because the simple decision
    tree approaches we just explored don’t provide sufficient detection accuracy.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个真实世界的检测器，你需要使用软件二进制文件的工业级特性，并编写代码从软件二进制文件中提取这些特性。工业级特性是指能够反映二进制文件内容复杂性的特性，这意味着我们需要使用数百或数千个特性。所谓“提取”特性，我指的是你需要编写代码来识别这些特性在二进制文件中的存在。你还需要使用数千个训练样本，并在大规模上训练一个机器学习模型。最后，你需要使用`sklearn`的更高级的检测方法，因为我们刚刚探讨的简单决策树方法无法提供足够的检测精度。
- en: '***Real-World Feature Extraction***'
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***真实世界的特性提取***'
- en: The sample features I used previously, such as `is packed` and `contains encrypted
    data`, are simple toy examples, and these two features alone will never result
    in a working malware detector. As I mentioned previously, real-world malware detection
    systems use hundreds, thousands, or even millions of features. For example, a
    machine learning–based detector might use millions of character strings that occur
    in software binaries as features. Or it might use the values of software binary
    Portable Executable (PE) headers, the functions imported by a given binary, or
    some combination of all of these. Although we’ll work only with string features
    in this chapter, let’s take a moment to explore common categories of features
    used in machine learning–based malware detection, starting with the string features.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前使用的示例特征，如`is packed`和`contains encrypted data`，是简单的示例特征，这两个特征单独使用永远无法成为一个有效的恶意软件检测器。正如我之前提到的，现实世界中的恶意软件检测系统使用数百、数千甚至数百万个特征。例如，基于机器学习的检测器可能会使用软件二进制文件中的数百万个字符字符串作为特征。或者，它可能会使用软件二进制文件的可移植可执行文件（PE）头部的值、某个特定二进制文件导入的函数，或者这些特征的某种组合。尽管我们在本章中只讨论字符串特征，但让我们花一点时间探讨机器学习驱动的恶意软件检测中常用的特征类别，从字符串特征开始。
- en: '**String Features**'
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**字符串特征**'
- en: 'The string features of a software binary are all the contiguous strings of
    printable characters in the file that are at least some minimum length (in this
    book, this minimum is set to five characters). For example, suppose a binary file
    contains the following sequences of printable characters:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 软件二进制文件的字符串特征是文件中所有至少达到最小长度的可打印字符的连续字符串（在本书中，最小长度设定为五个字符）。例如，假设一个二进制文件包含以下可打印字符序列：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this case, the strings we can use as features would be `"PE executable"`
    and `"Malicious payload"` because these two strings have more than five characters
    in them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以用作特征的字符串是`"PE executable"`和`"Malicious payload"`，因为这两个字符串包含的字符超过了五个。
- en: 'To transform string features into a format that `sklearn` can understand, we
    need to put them into a Python dictionary. We do this by using the actual strings
    as dictionary keys and then setting their values to 1 to indicate that the binary
    in question contains that string. For example, the previous sample binary would
    get a feature vector of `{"PE executable": 1, "Malicious payload": 1}`. Of course,
    most software binaries have hundreds of printable strings in them, not just two,
    and these strings can contain rich information about what a program does.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '为了将字符串特征转换为`sklearn`能够理解的格式，我们需要将它们放入一个Python字典中。我们通过使用实际的字符串作为字典的键，并将它们的值设置为1，以表示该二进制文件包含该字符串。例如，前面提到的示例二进制文件将得到一个特征向量`{"PE
    executable": 1, "Malicious payload": 1}`。当然，大多数软件二进制文件中包含的可打印字符串不止两个，而是成百上千个，而且这些字符串可以包含关于程序行为的丰富信息。'
- en: In fact, string features work well with machine learning–based detection because
    they capture so much information about software binaries. If the binary is a packed
    malware sample, then it’s likely to have few informative strings, which in itself
    can be a giveaway that the file is malicious. On the other hand, if parts of the
    file’s resources section are not packed or obfuscated, then those strings reveal
    much about the file’s behavior. For example, if the binary program in question
    makes HTTP requests, it’s common to see strings such as `"GET %s"` in that file’s
    set of strings.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，字符串特征与基于机器学习的检测方法非常契合，因为它们能够捕获有关软件二进制文件的大量信息。如果二进制文件是一个被打包的恶意软件样本，那么它可能包含的信息性字符串很少，这本身就是文件恶意的一个线索。另一方面，如果文件的资源部分没有被打包或混淆，那么这些字符串就能揭示文件的行为。例如，如果相关二进制程序发出HTTP请求，通常可以在文件的字符串集中看到类似`"GET
    %s"`的字符串。
- en: String features have some limitations, however. For example, they don’t capture
    anything about the actual logic of a binary program, because they don’t include
    actual program code. So, although strings can be useful features even on packed
    binaries, they don’t reveal what packed binaries actually do. As a result, detectors
    based on string features are not ideal for detecting packed malware.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，字符串特征也有一些局限性。例如，它们无法捕捉到二进制程序的实际逻辑，因为它们不包含实际的程序代码。因此，尽管字符串特征在打包的二进制文件中仍然可能是有用的，但它们并不能揭示打包二进制文件的实际行为。因此，基于字符串特征的检测器并不适合用于检测打包恶意软件。
- en: '**Portable Executable (PE) Header Features**'
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**可移植可执行文件（PE）头部特征**'
- en: PE header features are extracted from the PE header metadata that resides in
    every Windows *.exe* and *.dll* file. For more information on the format of these
    headers, refer to [Chapter 1](ch01.xhtml#ch01). To extract PE features from static
    program binaries, you can use the code given in that chapter, and then encode
    file features in Python dictionary form, where the header field name is the dictionary
    key and the field value is the value corresponding to each key.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: PE头部特征是从每个Windows *.exe*和*.dll*文件中的PE头部元数据提取的。有关这些头部格式的更多信息，请参阅[第1章](ch01.xhtml#ch01)。要从静态程序二进制文件中提取PE特征，可以使用该章节中提供的代码，然后将文件特征编码为Python字典形式，其中头部字段名称是字典的键，字段值是对应于每个键的值。
- en: PE header features complement string features well. For example, whereas string
    features often do a good job of capturing the function calls and network transmissions
    made by a program, like the `"GET %s"` example, PE header features capture information
    like a program binary’s compile timestamp, the layout of its PE sections, and
    which of those sections are marked executable and how large they are on disk.
    They also capture the amount of memory a program allocates upon startup, and many
    other runtime characteristics of a program binary that string features don’t capture.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: PE头部特征与字符串特征互补。例如，尽管字符串特征通常能很好地捕捉程序的函数调用和网络传输，比如`"GET %s"`的例子，PE头部特征则捕捉程序二进制文件的编译时间戳、PE部分的布局，以及哪些部分被标记为可执行且它们在磁盘上的大小。它们还捕捉程序启动时分配的内存量，以及许多其他字符串特征无法捕捉的程序二进制的运行时特性。
- en: Even when you’re dealing with packed binaries, PE header features can still
    do a decent job of distinguishing packed malware from packed benignware. This
    is because although we cannot see packed binaries’ code because of obfuscation,
    we can still see how much space the code takes up on disk and how the binary is
    laid out on disk or compressed over a series of file sections. These are telling
    details that can help a machine learning system distinguish malware from benignware.
    On the downside, PE header features don’t capture the actual instructions a program
    executes when it is run, or the functions that it calls.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在处理被打包的二进制文件时，PE头部特征仍然能够有效区分恶意软件和良性软件。这是因为，尽管我们无法看到被混淆后的二进制代码，但我们仍然可以看到代码在磁盘上占用的空间，以及二进制文件如何在磁盘上布局，或如何在多个文件部分中被压缩。这些细节能够帮助机器学习系统区分恶意软件和良性软件。缺点是，PE头部特征并不能捕捉程序在运行时实际执行的指令或它调用的函数。
- en: '**Import Address Table (IAT) Features**'
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**导入地址表（IAT）特征**'
- en: The Import Address Table (IAT), which you learned about in [Chapter 1](ch01.xhtml#ch01),
    is also an important source of machine learning features. The IAT contains a list
    of functions and libraries that a software binary imports from external DLL files.
    As such, the IAT contains important information about program behavior that you
    can use to complement the PE header features described in the previous section.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 导入地址表（IAT），你在[第1章](ch01.xhtml#ch01)中学习到的，也是机器学习特征的重要来源。IAT包含了软件二进制文件从外部DLL文件导入的函数和库的列表。因此，IAT包含有关程序行为的重要信息，可以用来补充上一节中描述的PE头部特征。
- en: 'To use the IAT as a source of machine learning features, you need to represent
    each file as a dictionary of features, where the name of the imported library
    and function is the key, and the key maps to a 1, which indicates that the file
    in question contains that specific import (for example, the key `"KERNEL32.DLL:LoadLibraryA"`,
    where `KERNEL32.DLL` is the DLL and `LoadLibraryA` is the function call). The
    feature dictionary resulting from computing IAT features in this way for a sample
    would look like { `KERNEL32.DLL:LoadLibraryA: 1, ... }`, where we’d assign a 1
    to any keys observed in a binary.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '要将IAT作为机器学习特征的来源，你需要将每个文件表示为一个特征字典，其中导入的库和函数的名称是键，键的值为1，表示该文件包含特定的导入项（例如，键为`"KERNEL32.DLL:LoadLibraryA"`，其中`KERNEL32.DLL`是DLL，`LoadLibraryA`是函数调用）。通过这种方式计算IAT特征后，得到的特征字典可能是{
    `KERNEL32.DLL:LoadLibraryA: 1, ... }`，其中我们将1分配给二进制文件中出现的任何键。'
- en: In my experience building malware detectors, I have found that IAT features
    rarely work well on their own—although these features capture useful high-level
    information about program behavior, the malware often obfuscates the IAT to make
    itself look like benignware. Even when malware contains no obfuscation, it often
    imports the same DLL calls that benignware also imports, making it hard to distinguish
    between malware and benignware simply based on IAT information. Finally, when
    malware is packed (compressed or encrypted, such that the real malware code is
    only visible after the malware executes and uncompresses or unencrypts itself),
    the IAT only contains imports that the packer uses, not the imports that the malware
    uses. That said, when you use IAT features in conjunction with other features
    like PE header features and string features, they can boost system accuracy.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在我构建恶意软件检测器的经验中，我发现 IAT 特征单独使用时很少能发挥良好的效果——尽管这些特征能够捕捉到有关程序行为的有用高级信息，但恶意软件经常会混淆
    IAT，使其看起来像良性软件。即使恶意软件没有进行混淆，它通常也会导入与良性软件相同的 DLL 调用，这使得仅凭 IAT 信息很难区分恶意软件和良性软件。最后，当恶意软件被打包（被压缩或加密，直到恶意软件执行并解压或解密后才能看到真正的恶意代码）时，IAT
    只包含打包器使用的导入，而不是恶意软件使用的导入。也就是说，当你将 IAT 特征与其他特征，如 PE 头特征和字符串特征结合使用时，它们可以提高系统的准确性。
- en: '**N-grams**'
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**N-grams**'
- en: So far you’ve learned about machine learning features that don’t involve any
    concept of ordering. For example, we discussed string features to check whether
    or not a binary has a particular string, but not whether a particular string comes
    before or after another string in the layout of the binary on disk.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学习了不涉及顺序概念的机器学习特征。例如，我们讨论了字符串特征来检查二进制文件是否包含特定字符串，但没有讨论特定字符串在二进制文件布局中是否位于另一个字符串之前或之后。
- en: But sometimes order matters. For example, we might find that an important malware
    family imports only commonly used functions, but imports them in a very specific
    order, such that when we observe the functions in that order, we know we’re seeing
    that malware family and not benignware. To capture this kind of order information,
    you can use a machine learning concept called an N-gram.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 但有时顺序很重要。例如，我们可能会发现一个重要的恶意软件家族只导入常用的函数，但它们按非常特定的顺序导入，这样当我们观察这些函数的顺序时，就知道我们看到的是该恶意软件家族，而不是良性软件。为了捕捉这种顺序信息，你可以使用一个机器学习概念，叫做
    N-gram。
- en: 'N-grams sound more exotic than they are: they just involve laying out your
    features in the sequence in which they occur and then sliding a window of length
    *n* over the sequence, treating the sequence of features inside the window at
    each step as a single, aggregate feature. For example, if we had the sequence
    `["how", "now", "brown", "cow"]` and we wanted to extract N-gram features of length
    2 (*n* = 2) from this sequence, we would have `[("how","now"), ("now","brown"),
    ("brown","cow")]` as our features.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: N-gram 听起来比实际要复杂：它们只需要按照特征出现的顺序排列，然后将长度为 *n* 的窗口滑过序列，在每一步中将窗口内的特征序列视为一个整体特征。例如，如果我们有序列
    `["how", "now", "brown", "cow"]`，并且我们想从这个序列中提取长度为 2（*n* = 2）的 N-gram 特征，那么我们的特征就是
    `[("how","now"), ("now","brown"), ("brown","cow")]`。
- en: In the context of malware detection, some kinds of data are most naturally represented
    as N-gram features. For example, when you disassemble a binary into its constituent
    instructions, like `["inc", "dec", "sub", "mov"]`, it makes sense to then use
    the N-gram approach to capture these sequences of instructions because representing
    a sequence of instructions can be useful in detecting particular malware implementations.
    Alternatively, when you’re executing binaries to examine their dynamic behavior,
    you can use the N-gram approach to represent binaries’ sequences of API calls
    or high-level behaviors.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在恶意软件检测的背景下，有些数据最自然的表示方式是 N-gram 特征。例如，当你将一个二进制文件反汇编成其组成指令时，如`["inc", "dec",
    "sub", "mov"]`，使用 N-gram 方法来捕捉这些指令序列是很有意义的，因为表示指令序列对于检测特定的恶意软件实现非常有用。或者，当你执行二进制文件以检查其动态行为时，你可以使用
    N-gram 方法来表示二进制文件中 API 调用或高级行为的序列。
- en: I recommend experimenting with N-gram features in your machine learning–based
    malware detection systems whenever you’re working with data that occurs in some
    type of sequence. It often takes some trial and error to determine what number
    you should set *n* to, which determines the length of your N-grams. This trial
    and error involves varying the *n* value to see which value yields the best accuracy
    on your test data. Once you find the right number, N-grams can be powerful features
    for capturing the actual sequential behaviors of program binaries, thereby boosting
    system accuracy.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议在使用基于机器学习的恶意软件检测系统时，针对那些包含某种序列类型的数据，实验使用 N-gram 特征。通常需要通过一些试验和错误来确定 *n* 应设置为多少，这决定了你的
    N-gram 的长度。这个试验过程包括变动 *n* 值，看看哪个值能在你的测试数据上获得最佳的准确度。一旦找到了合适的值，N-gram 就可以成为捕捉程序二进制文件实际顺序行为的强大特征，从而提高系统的准确性。
- en: '***Why You Can’t Use All Possible Features***'
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***为什么不能使用所有可能的特征***'
- en: Now that you know the strengths and weaknesses of different categories of features,
    you may be wondering why you can’t use all these features at once to build the
    best possible detector. There are a few reasons using all possible features is
    not a good idea.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经了解了不同类别特征的优缺点，你可能会想知道为什么不能同时使用所有这些特征来构建最强的检测器。事实上，使用所有可能的特征并不是一个好主意，原因有几个。
- en: First, extracting all the features we just explored takes a long time, which
    compromises how quickly your system can scan files. More importantly, if you use
    too many features on machine learning algorithms, you can run into memory issues
    and your system can take too long to train. This is why when building your systems,
    I recommend trying different features and honing in on those that work well on
    the kinds of malware you’re trying to detect (and the kinds of benignware you’re
    trying to avoid generating false positives on).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，提取我们刚才探讨的所有特征需要很长时间，这会影响系统扫描文件的速度。更重要的是，如果你在机器学习算法中使用太多特征，你可能会遇到内存问题，且系统训练时间过长。因此，在构建系统时，我建议尝试不同的特征，并集中精力选择那些对你要检测的恶意软件有效的特征（以及对你想避免产生假阳性结果的良性软件）。
- en: Unfortunately, even if you do home in on one category of features, like string
    features, you’ll often have more features than most machine learning algorithms
    can handle. When using string features, you must have one feature for every unique
    string that occurs in your training data. For example, if training sample A contains
    the string `"hello world"`, and training sample B contains the string `"hello
    world!"`, then you’ll need to treat `"hello world"` and `"hello world!"` as two
    separate features. This means that when you’re dealing with thousands of training
    samples, you’ll quickly encounter thousands of unique strings, and your system
    will end up using that many features.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，即使你聚焦于某一类特征，比如字符串特征，你通常会遇到比大多数机器学习算法能够处理的更多的特征。使用字符串特征时，你必须为训练数据中出现的每一个独特字符串准备一个特征。例如，如果训练样本
    A 包含字符串 `"hello world"`，而训练样本 B 包含字符串 `"hello world!"`，那么你需要将 `"hello world"`
    和 `"hello world!"` 视为两个独立的特征。这意味着当你处理成千上万的训练样本时，你很快会遇到成千上万的独特字符串，系统最终将使用这么多的特征。
- en: '***Using the Hashing Trick to Compress Features***'
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用哈希技巧压缩特征***'
- en: 'To get around the problem of having too many features, you can use a popular
    and straightforward solution called the *hashing trick*, also known as *feature
    hashing*. The idea is as follows: suppose you have a million unique string features
    in your training set, but the machine learning algorithm and hardware you’re using
    can only deal with 4,000 unique features across the whole training set. You need
    some way of compressing a million features down to a feature vector that’s 4,000
    entries long.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决特征过多的问题，你可以使用一种流行且简单的解决方案，称为 *哈希技巧*，也叫 *特征哈希*。其原理如下：假设你的训练集中有一百万个独特的字符串特征，但你使用的机器学习算法和硬件只能处理全体训练集中
    4,000 个独特的特征。你需要某种方法将一百万个特征压缩成一个只有 4,000 个条目的特征向量。
- en: The hashing trick makes these million features fit within a feature space of
    4,000 by hashing each feature into one of 4,000 indices. Then you add the value
    of your original feature to the number at that index in your 4,000-dimensional
    feature vector. Of course, features often collide with this approach because their
    values are added together along the same dimension. This might affect system accuracy
    because the machine learning algorithm you’re using can’t “see” the value of individual
    features anymore. But in practice, this degradation in accuracy is often very
    small, and the benefit you get from the compressed representation of your features
    far outweighs this slight degradation that occurs because of the compression operation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希技巧通过将每个特征哈希到4,000个索引中的一个，使这些数百万个特征适应于一个4,000维的特征空间。然后，你将原始特征的值加到该索引位置的4,000维特征向量中的数字上。当然，由于特征值会在相同维度上相加，这种方法经常会导致特征碰撞。这可能会影响系统的准确性，因为你使用的机器学习算法再也无法“看到”单个特征的值。但在实践中，这种准确性下降通常非常小，而通过压缩特征表示所获得的好处远远超过了由于压缩操作带来的轻微准确性下降。
- en: '**Implementing the Hashing Trick**'
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**实现哈希技巧**'
- en: 'To make these ideas clearer, I walk you through sample code that implements
    the hashing trick. Here I show this code to illustrate how the algorithm works;
    later, we’ll use `sklearn`’s implementation of this function. Our sample code
    starts with a function declaration:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些概念更加清晰，我将通过示例代码来演示哈希技巧的实现。这里我展示这段代码以说明算法的工作原理；稍后，我们将使用`sklearn`对该功能的实现。我们的示例代码从一个函数声明开始：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `apply_hashing_trick()` function takes two parameters: the original feature
    dictionary and the size of the vector we store the smaller feature vector in after
    we apply the hashing trick.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply_hashing_trick()`函数接受两个参数：原始特征字典和应用哈希技巧后我们将特征存储在其中的较小特征向量的大小。'
- en: 'Next, we create the new feature array using the following code:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用以下代码创建新的特征数组：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `new_features` array stores the feature information after applying the hashing
    trick. Then we perform the key operations of the hashing trick inside a `for`
    loop, like in [Listing 8-8](ch08.xhtml#ch08list8).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`new_features`数组存储了应用哈希技巧后的特征信息。接着，我们在`for`循环中执行哈希技巧的关键操作，如[清单 8-8](ch08.xhtml#ch08list8)所示。'
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*Listing 8-8: Using a* for *loop to perform a hash operation*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-8：使用* `for` *循环执行哈希操作*'
- en: Here, we use a `for` loop to iterate over every feature in the feature dictionary
    ➊. To do this, first we hash the keys of the dictionary (in the case of string
    features, these would correspond to the software binaries’ individual strings)
    modulo `vector_size` such that the hash values are bounded between zero and `vector_size
    – 1` ➋. We store the result of this operation in the `array_index` variable.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`for`循环遍历特征字典中的每个特征 ➊。为此，首先我们对字典的键进行哈希（对于字符串特征，这些键对应软件二进制文件中的各个字符串），并对`vector_size`取模，使得哈希值介于0和`vector_size
    – 1`之间 ➋。我们将此操作的结果存储在`array_index`变量中。
- en: Still within the `for` loop, we increment the value of the `new_feature` array
    entry at index `array_index` by the value in our original feature array ➌. In
    the case of string features, where our feature values are set to 1 to indicate
    that the software binary has that particular string, we would increment this entry
    by one. In the case of PE header features, where features have a range of values
    (for example, corresponding to the amount of memory a PE section will take up),
    we would add the value of the feature to the entry.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然在`for`循环中，我们将原始特征数组中某个特征的值加到`new_feature`数组中对应`array_index`位置的值 ➌。对于字符串特征，当我们的特征值被设置为1以表示该软件二进制文件包含某个特定字符串时，我们会将该项的值加一。对于PE头特征，其中的特征值有一个范围（例如，表示PE段所占用的内存量），我们会将该特征值加到对应项中。
- en: 'Finally, outside of the `for` loop, we simply return the `new_features` dictionary,
    like this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在`for`循环之外，我们简单地返回`new_features`字典，如下所示：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: At this point, `sklearn` can operate on `new_features` using just thousands
    instead of millions of unique features.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`sklearn`可以使用仅包含数千个而非数百万个唯一特征的`new_features`进行操作。
- en: '**Complete Code for the Hashing Trick**'
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**哈希技巧的完整代码**'
- en: '[Listing 8-9](ch08.xhtml#ch08list9) shows the complete code for the hashing
    trick, which should now be familiar to you.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 8-9](ch08.xhtml#ch08list9)展示了哈希技巧的完整代码，现在你应该已经熟悉它了。'
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*Listing 8-9: Complete code for implementing the hashing trick*'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-9：实现哈希技巧的完整代码*'
- en: As you have seen, the feature hashing trick is easy to implement on your own,
    and doing so ensures that you understand how it works. However, you can also just
    use `sklearn`’s implementation, which is easy to use and more optimized.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，特征哈希技巧容易自己实现，这样做能够确保你理解其工作原理。然而，你也可以直接使用`sklearn`的实现，它既易于使用又经过了优化。
- en: '**Using sklearn’s FeatureHasher**'
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**使用sklearn的FeatureHasher**'
- en: 'To use `sklearn`’s built-in implementation instead of implementing your own
    hashing solution, you need to first import `sklearn`’s `FeatureHasher` class,
    like this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`sklearn`的内置实现而不是自己实现哈希解决方案，首先需要导入`sklearn`的`FeatureHasher`类，如下所示：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, instantiate the `FeatureHasher` class:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，实例化`FeatureHasher`类：
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: To do this, you declare `n_features` to be the size of the new array that results
    from applying the hashing trick.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，你需要声明`n_features`为应用哈希技巧后生成的新数组的大小。
- en: 'Then, to apply the hashing trick to some feature vectors, you simply run them
    through the `FeatureHasher` class’s `transform` method:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，要将哈希技巧应用于一些特征向量，只需将它们传入`FeatureHasher`类的`transform`方法：
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The result is effectively the same as our custom implementation of the feature
    hashing trick shown in [Listing 8-9](ch08.xhtml#ch08list9). The difference is
    that here we’re simply using `sklearn`’s implementation, since it’s easier to
    use a well-maintained machine learning library than our own code. The complete
    sample code is shown in [Listing 8-10](ch08.xhtml#ch08list10).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 结果实际上与我们自定义实现的特征哈希技巧相同，如[清单 8-9](ch08.xhtml#ch08list9)所示。不同之处在于这里我们仅仅使用了`sklearn`的实现，因为使用一个维护良好的机器学习库要比自己编写代码更为便捷。完整的示例代码见[清单
    8-10](ch08.xhtml#ch08list10)。
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '*Listing 8-10: Implementing* FeatureHasher'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-10：实现* FeatureHasher'
- en: There are a few things to note about feature hashing before we move on. First,
    as you may have guessed, feature hashing obfuscates the feature information you
    pass into a machine learning algorithm because it adds feature values together
    simply based on the fact that they hash to the same bin. This means that, in general,
    the fewer bins you use (or the more features you hash into some fixed numbers
    of bins), the worse your algorithm will perform. Surprisingly, machine learning
    algorithms can still work well even when using the hashing trick, and because
    we simply can’t deal with millions or billions of features on modern hardware,
    we usually have to use the feature hashing trick in security data science.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，有几点需要注意的特征哈希内容。首先，正如你可能猜到的那样，特征哈希通过简单地基于特征值哈希到相同的桶来混淆你传入机器学习算法的特征信息。这意味着，通常情况下，你使用的桶越少（或者将更多特征哈希到某些固定数量的桶中），算法的表现会越差。令人惊讶的是，即使使用了哈希技巧，机器学习算法仍然能够良好运作，因为我们根本无法在现代硬件上处理数百万甚至数十亿个特征，所以我们通常需要在安全数据科学中使用特征哈希技巧。
- en: 'Another limitation of the feature hashing trick is that it makes it difficult
    or impossible to recover the original features you hashed when analyzing the internals
    of your model. Take the example of decision trees: because we’re hashing arbitrary
    features into each entry of our feature vectors, we don’t know which of the features
    we added to a given entry is causing a decision tree algorithm to split on this
    entry, since any number of features could have caused the decision tree to think
    splitting on this entry was a good idea. Although this is a significant limitation,
    security data scientists live with it because of the huge benefits of the feature
    hashing trick in compressing millions of features down to a manageable number.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 特征哈希技巧的另一个限制是，在分析模型内部时，它使得恢复你哈希的原始特征变得困难或不可能。以决策树为例：因为我们将任意特征哈希到特征向量的每个条目中，所以我们不知道哪些特征导致决策树算法基于这个条目进行划分，因为任何特征都可能让决策树认为在这个条目上划分是个好主意。尽管这是一个显著的限制，但安全数据科学家依然接受这一点，因为特征哈希技巧在将数百万个特征压缩为一个可管理的数量时带来了巨大的好处。
- en: Now that we’ve gone over the building blocks required for building a real-world
    malware detector, let’s explore how to build an end-to-end machine learning malware
    detector.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了构建一个现实世界恶意软件检测器所需的基本构件，让我们来探索如何构建一个端到端的机器学习恶意软件检测器。
- en: '**Building an Industrial-Strength Detector**'
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**构建工业级检测器**'
- en: 'From a software requirements perspective, our real-world detector will need
    to do three things: extract features from software binaries for use in training
    and detection, train itself to detect malware using training data, and actually
    perform detection on new software binaries. Let’s walk through the code that does
    each of these things, which will show you how it all fits together.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 从软件需求的角度来看，我们的实际检测器需要完成三件事：从软件二进制文件中提取特征，以便用于训练和检测；使用训练数据自我训练以检测恶意软件；并且实际对新的软件二进制文件进行检测。让我们逐步查看执行这些操作的代码，这将向你展示它们是如何结合在一起的。
- en: You can access the code I use in this section at *malware_data_science/ch8/code/complete_detector.py*
    in the code accompanying this book, or at the same location in the virtual machine
    provided with this book. A one-line shell script, *malware_data_science/ch8/code/run_complete_detector.sh*,
    shows how to run the detector from the shell.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书附带的代码中，或在本书提供的虚拟机中的`malware_data_science/ch8/code/complete_detector.py`中访问我在本节中使用的代码。一个一行的Shell脚本`malware_data_science/ch8/code/run_complete_detector.sh`展示了如何从Shell中运行检测器。
- en: '***Extracting Features***'
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***提取特征***'
- en: To create our detector, the first thing we implement is code to extract features
    from training binaries (I skip over boilerplate code here and focus on the core
    functions of the program). Extracting features involves extracting the relevant
    data from training binaries, storing these features within a Python dictionary,
    and then, if we think our number of unique features will become prohibitively
    large, transforming them using `sklearn`’s implementation of the hashing trick.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建我们的检测器，首先实现的是从训练二进制文件中提取特征的代码（这里略过了模板代码，专注于程序的核心功能）。提取特征涉及从训练二进制文件中提取相关数据，将这些特征存储在Python字典中，然后，如果我们认为唯一特征的数量会变得过大，我们会使用`sklearn`实现的哈希技巧对它们进行转换。
- en: For simplicity’s sake, we use only string features and choose to use the hashing
    trick. [Listing 8-11](ch08.xhtml#ch08list11) shows how to do both.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们仅使用字符串特征并选择使用哈希技巧。[列表 8-11](ch08.xhtml#ch08list11)展示了如何同时进行这两者。
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '*Listing 8-11: Defining the* get_string_features *function*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 8-11：定义* get_string_features *函数*'
- en: Here, we declare a single function called `get_string_features` that takes the
    path to the target binary ➊ and an instance of `sklearn`’s feature hashing class
    ➋ as its arguments. Then we extract the target file’s strings using a regular
    expression, which parses out all printable strings of minimum length 5\. Then,
    we store the features in a Python dictionary ➌ for further processing by setting
    each string’s value to 1 in the dictionary, simply indicating that that feature
    is present in the binary.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们声明了一个名为`get_string_features`的函数，它以目标二进制文件的路径➊和`sklearn`的特征哈希类实例➋作为参数。然后，我们使用正则表达式提取目标文件的字符串，该正则表达式解析出所有最小长度为5的可打印字符串。接着，我们将特征存储在一个Python字典中➌，以便进一步处理，通过将每个字符串的值设置为1，简单地表示该特征在二进制文件中存在。
- en: Next, we hash the features using `sklearn`’s hashing trick implementation by
    calling `hasher`. Notice that we wrap the `string_features` dictionary in a Python
    list as we pass it into the `hasher` instance ➍ because `sklearn` requires that
    we pass in a list of dictionaries to be transformed, rather than a single dictionary.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`sklearn`的哈希技巧实现通过调用`hasher`来哈希特征。注意，在将`string_features`字典传递给`hasher`实例时，我们将其包装在一个Python列表中➍，因为`sklearn`要求我们传入一个字典列表进行转换，而不是单个字典。
- en: Because we passed in our feature dictionary as a list of dictionaries, features
    are returned as a list of arrays. Additionally, they are returned in *sparse*
    format, a compressed representation that can be useful for processing large matrices,
    which we won’t discuss in this book. We need to get our data back into a normal
    `numpy` vector.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将特征字典作为字典列表传入，所以返回的特征是一个数组列表。此外，返回的特征采用*稀疏*格式，这是一种压缩表示，适用于处理大矩阵，但我们在本书中不会讨论它。我们需要将数据转换回常规的`numpy`向量格式。
- en: To get the data back into normal format, we call `.todense()` and `.``asarray``()`,
    and then select the first array in the list of `hasher` results to recover our
    final feature vector. The last step in the function is simply to return the feature
    vector `hashed_features` ➎ to the caller.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将数据恢复为常规格式，我们调用`.todense()`和`.``asarray``()`，然后选择`hasher`结果列表中的第一个数组，以恢复我们的最终特征向量。函数的最后一步是简单地将特征向量`hashed_features`
    ➎返回给调用者。
- en: '***Training the Detector***'
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***训练检测器***'
- en: Because `sklearn` does most of the hard work of training machine learning systems,
    training a detector requires only a small amount of code once we’ve extracted
    machine learning features from our target binaries.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`sklearn`完成了大部分训练机器学习系统的繁重工作，训练一个检测器只需要少量代码，一旦我们从目标二进制文件中提取了机器学习特征。
- en: To train a detector, we first need to extract features from our training examples,
    and then instantiate the feature hasher and the `sklearn` machine learning detector
    we wish to use (in this case, we use a random forest classifier). Then we need
    to call `sklearn`’s `fit` method on the detector to train it on the examples’
    binaries. Finally, we save the detector and feature hasher to disk so we can use
    them when we want to scan files in the future.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个检测器，我们首先需要从训练示例中提取特征，然后实例化特征哈希器和我们希望使用的`sklearn`机器学习检测器（在本例中，我们使用随机森林分类器）。接着，我们需要调用`sklearn`的`fit`方法来训练检测器。最后，我们将检测器和特征哈希器保存到磁盘，以便将来扫描文件时使用。
- en: '[Listing 8-12](ch08.xhtml#ch08list12) shows the code for training the detector.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 8-12](ch08.xhtml#ch08list12)展示了训练检测器的代码。'
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*Listing 8-12: Programming* sklearn *to train the detector*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 8-12：编程* sklearn *以训练检测器*'
- en: 'Let’s start by declaring the `get_training_data()` function ➊, which extracts
    features from training examples we provide. The function has three arguments:
    a path to a directory containing examples of benign binary programs (`benign_path`),
    a path to a directory containing examples of malicious binary programs (`malicious_path`),
    and an instance of the `sklearn` `FeatureHasher` class used to do feature hashing
    (`hasher`).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从声明`get_training_data()`函数 ➊ 开始，该函数从我们提供的训练示例中提取特征。该函数有三个参数：包含良性二进制程序示例的目录路径（`benign_path`），包含恶意二进制程序示例的目录路径（`malicious_path`），以及用于执行特征哈希的`sklearn`的`FeatureHasher`类实例（`hasher`）。
- en: Next, we declare `get_training_paths()` ➋, a local helper function that gets
    us the list of absolute file paths for files occurring in a given directory. In
    the next two lines, we use `get_training_paths` to get us the lists of paths that
    occur in the malicious ➌ and benign ➍ training example directories.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们声明`get_training_paths()` ➋，这是一个本地辅助函数，用于获取给定目录中出现的文件的绝对路径列表。在接下来的两行中，我们使用`get_training_paths`来获取出现在恶意
    ➌ 和良性 ➍ 训练示例目录中的路径列表。
- en: Finally, we extract our features and create our label vector. We do this by
    calling the `get_string_features` function described in [Listing 8-11](ch08.xhtml#ch08list11)
    on every training example file path ➎. Notice that the label vector has a 1 for
    every malicious path and a 0 for every benign path, such that the numbers at the
    indices in the label vector correspond to the label of the feature vectors at
    those same indices in the `X` array. This is the form in which `sklearn` expects
    feature and label data, and it allows us to tell the library the label for each
    feature vector.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们提取了特征并创建了标签向量。我们通过在每个训练示例文件路径 ➎ 上调用[示例 8-11](ch08.xhtml#ch08list11)中描述的`get_string_features`函数来完成这一步。注意，标签向量中每个恶意路径对应的值是1，每个良性路径对应的值是0，因此标签向量中的索引值与`X`数组中相应索引的特征向量的标签相对应。这是`sklearn`期望的特征和标签数据格式，它使我们能够为每个特征向量指定标签。
- en: Now that we’ve finished extracting features and created our feature vector `X`
    and our label vector `y`, we’re ready to tell `sklearn` to train our detector
    using the feature vectors and the label vector.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了特征提取，并创建了我们的特征向量`X`和标签向量`y`，我们可以告诉`sklearn`使用特征向量和标签向量来训练我们的检测器。
- en: 'We do this using the `train_detector()` function ➏, which takes three arguments:
    the training example feature vectors (`X`), the label vector (`y`), and the instance
    of `sklearn`’s feature hasher (`hasher`). In the function body we instantiate
    `tree.RandomForestClassifier`, the `sklearn` detector. Then we pass `X` and `y`
    into the detector’s `fit` method to train it ➐, and then use the Python `pickle`
    module ➑ to save the detector and hasher for future use.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`train_detector()`函数 ➏ 来完成这一步，函数接收三个参数：训练示例的特征向量（`X`），标签向量（`y`），以及`sklearn`特征哈希器（`hasher`）的实例。在函数体内，我们实例化`tree.RandomForestClassifier`，这是`sklearn`的检测器。然后我们将`X`和`y`传递给检测器的`fit`方法来训练它
    ➐，接着使用Python的`pickle`模块 ➑ 来保存检测器和哈希器，以便将来使用。
- en: '***Running the Detector on New Binaries***'
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***在新二进制文件上运行检测器***'
- en: Now let’s go over how to use the saved detector we just trained to detect malware
    in new program binaries. [Listing 8-13](ch08.xhtml#ch08list13) shows how to write
    the `scan_file()` function to do this.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看如何使用我们刚刚训练的保存的检测器来检测新的程序二进制文件中的恶意软件。[列表 8-13](ch08.xhtml#ch08list13)
    展示了如何编写 `scan_file()` 函数来实现这一点。
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '*Listing 8-13: Running the detector on new binaries*'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 8-13：在新的二进制文件上运行检测器*'
- en: Here, we declare the `scan_file()` function to scan a file to determine whether
    it’s malicious or benign. Its only argument is the path to the binary that we
    are going to scan. The function’s first job is to load the saved detector and
    hasher from the `pickle` file to which they were saved ➊.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们声明了 `scan_file()` 函数，用于扫描文件以确定它是恶意的还是良性的。它的唯一参数是我们将要扫描的二进制文件的路径。该函数的第一项任务是加载从
    `pickle` 文件中保存的检测器和哈希器 ➊。
- en: Next, we extract features from the target file using the function `get_string_features`
    ➋ we defined in [Listing 8-11](ch08.xhtml#ch08list11).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用在 [列表 8-11](ch08.xhtml#ch08list11) 中定义的 `get_string_features` 函数 ➋ 提取目标文件的特征。
- en: Finally, we call the detector’s `predict` method to decide whether or not the
    file in question is malicious, given the features extracted. We do this using
    the `predict_proba` method ➌ of the `classifier` instance and selecting the second
    element of the array that it returns, which corresponds to the probability that
    the file is malicious. If this probability is above 0.5, or 50 percent ➍, we say
    the file is malicious; otherwise, we tell the user that it’s benign. We can change
    this decision threshold to something much higher to minimize false positives.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们调用检测器的 `predict` 方法，根据提取的特征来判断目标文件是否为恶意文件。我们通过使用 `classifier` 实例的 `predict_proba`
    方法 ➌，并选择返回数组的第二个元素，该元素对应文件为恶意的概率。如果该概率超过 0.5 或 50% ➍，我们认为文件是恶意的；否则，我们告诉用户文件是良性的。我们可以将此决策阈值调高，以减少误报。
- en: '***What We’ve Implemented So Far***'
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***我们到目前为止实现的内容***'
- en: '[Listing 8-14](ch08.xhtml#ch08list14) shows the code for this small-scale but
    realistic malware detector in its entirety. I hope that the code reads fluidly
    to you now that you’ve seen how each individual piece works.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8-14](ch08.xhtml#ch08list14) 展示了这个小规模但现实的恶意软件检测器的完整代码。我希望在你已经了解每个单独模块如何工作的基础上，代码能够流畅地读懂。'
- en: '[PRE21]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '*Listing 8-14: Basic machine learning malware detector code*'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 8-14：基础的机器学习恶意软件检测器代码*'
- en: Writing a machine learning–based malware detector is great, but evaluating and
    improving its performance is necessary if you’re going to deploy the detector
    with any confidence in its efficacy. Next, you learn different ways to evaluate
    the performance of your detector.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 编写基于机器学习的恶意软件检测器是很棒的，但如果你打算自信地部署该检测器，你需要评估并改进它的性能。接下来，你将学习评估检测器性能的不同方法。
- en: '**Evaluating Your Detector’s Performance**'
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**评估检测器的性能**'
- en: Conveniently, `sklearn` contains code that makes it easy to evaluate detection
    systems using measurements like ROC curves, which you learned about in [Chapter
    7](ch07.xhtml#ch07). The `sklearn` library also provides additional evaluation
    functionality specific to evaluating machine learning systems. For example, you
    can use `sklearn`’s functions for performing cross-validation, which is a powerful
    method for predicting how well your detector will work when you deploy it.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 方便的是，`sklearn` 提供了便于使用 ROC 曲线等度量标准来评估检测系统的代码，正如你在 [第 7 章](ch07.xhtml#ch07) 中学到的那样。`sklearn`
    库还提供了额外的评估功能，专门用于评估机器学习系统。例如，你可以使用 `sklearn` 的函数进行交叉验证，这是预测检测器在部署后效果的强大方法。
- en: In this section, you learn how to use `sklearn` to plot ROC curves that show
    your detector’s accuracy. You also learn about cross-validation and how to implement
    it with `sklearn`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何使用 `sklearn` 绘制 ROC 曲线，以展示检测器的准确性。你还将了解交叉验证，并学习如何在 `sklearn` 中实现它。
- en: '***Using ROC Curves to Evaluate Detector Efficacy***'
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用 ROC 曲线评估检测器效果***'
- en: Recall that Receiver Operating Characteristic (ROC) curves measure the changes
    in a detector’s true positive rate (the percentage of malware that it successfully
    detects) and false positive rate (the percentage of benignware that it falsely
    flags as malware) as you adjust its sensitivity.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得接收者操作特征（ROC）曲线衡量的是检测器的真正阳性率（它成功检测到的恶意软件的百分比）和假阳性率（它错误标记为恶意软件的良性软件的百分比），这些值会随着你调整敏感度而变化。
- en: The higher the sensitivity, the more false positives you will get but the greater
    your detection rate. The lower the sensitivity, the fewer false positives but
    also the fewer detections you’ll get. To compute a ROC curve you need a detector
    that can output a *threat score* such that the higher its value the more likely
    it is that a binary is malicious. Conveniently, `sklearn`’s implementations of
    decision trees, logistic regression, k-nearest neighbors, random forests, and
    other machine learning approaches covered in this book all provide the option
    of outputting a threat score that reflects whether a file is malware or benignware.
    Let’s explore how we can use ROC curves to determine a detector’s accuracy.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 灵敏度越高，假阳性越多，但检测率越高；灵敏度越低，假阳性越少，但检测到的数量也会减少。要计算ROC曲线，你需要一个能够输出*威胁评分*的检测器，评分越高，二进制文件被判定为恶意的可能性就越大。幸运的是，`sklearn`中实现的决策树、逻辑回归、k-近邻、随机森林和本书中介绍的其他机器学习方法都提供了输出威胁评分的选项，该评分反映了一个文件是恶意软件还是良性软件。让我们探讨如何利用ROC曲线来确定检测器的准确性。
- en: '***Computing ROC Curves***'
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***计算ROC曲线***'
- en: 'To compute a ROC curve for the machine learning detector we built in [Listing
    8-14](ch08.xhtml#ch08list14), we need to do two things: first, define an experimental
    setup, and second, implement the experiment using `sklearn`’s `metrics` module.
    For our basic experimental setup, we’ll split our training examples in half such
    that we use the first half for training and the second half for computing the
    ROC curve. This split simulates the problem of detecting zero-day malware. Basically,
    by splitting the data, we’re telling the program, “show me one set of benignware
    and malware that I’ll use to learn how to identify malware and benignware, and
    then show me the other set to test me on how well I learned the concept of malware
    and benignware.” Because the detector has never seen the malware (or benignware)
    in the test set, this evaluation setup is a simple way to predict how well the
    detector will do against truly new malware.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要为我们在[Listing 8-14](ch08.xhtml#ch08list14)中构建的机器学习检测器计算ROC曲线，我们需要做两件事：首先，定义一个实验设置，其次，使用`sklearn`的`metrics`模块实现该实验。对于我们的基本实验设置，我们将训练样本分成两半，一半用于训练，另一半用于计算ROC曲线。这种分割模拟了检测零日恶意软件的问题。基本上，通过分割数据，我们告诉程序：“给我展示一组恶意软件和良性软件的样本，我将用它们来学习如何识别恶意软件和良性软件，然后展示另一组给我测试，看看我学会了多好。”因为检测器从未见过测试集中的恶意软件（或良性软件），这个评估设置是预测检测器如何应对全新恶意软件的简单方法。
- en: 'Implementing this split with `sklearn` is straightforward. First, we add an
    option to the argument parser class of our detector program to signal that we
    want to evaluate the detector’s accuracy, like this:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sklearn`来实现这种分割是非常直接的。首先，我们在检测器程序的参数解析类中添加一个选项，以表明我们想要评估检测器的准确性，像这样：
- en: '[PRE22]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Then, in the part of the program where we process command line arguments, shown
    in [Listing 8-15](ch08.xhtml#ch08list15), we add another `elif` clause that handles
    the case that the user has added `-evaluate` to the command line arguments.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在程序中处理命令行参数的部分，见[Listing 8-15](ch08.xhtml#ch08list15)，我们添加了另一个`elif`语句来处理用户添加了`-evaluate`到命令行参数的情况。
- en: '[PRE23]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '*Listing 8-15: Running the detector on new binaries*'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 8-15: 在新二进制文件上运行检测器*'
- en: Let’s walk through this code in detail. First, we instantiate an `sklearn` feature
    hasher ➊, get the training data we require for our evaluation experiment ➋, and
    then call a function named `evaluate` ➌, which takes the training data (`X, y`)
    and the feature hasher instance (`hasher`) as its parameters and then imports
    three modules we need to perform the evaluation. We use the `random` module to
    randomly select which training examples to use for training the detector and which
    to use for testing it. We use the `metrics` module from `sklearn` to compute the
    ROC curve, and we use the `pyplot` module from `matplotlib` (the de facto standard
    Python library for data visualization) to visualize the ROC curve.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细分析一下这段代码。首先，我们实例化一个`sklearn`特征哈希器 ➊，获取我们评估实验所需的训练数据 ➋，然后调用一个名为`evaluate`的函数
    ➌，该函数接收训练数据（`X, y`）和特征哈希器实例（`hasher`）作为参数，然后导入进行评估所需的三个模块。我们使用`random`模块来随机选择用于训练检测器的训练示例以及用于测试的训练示例。我们使用`sklearn`中的`metrics`模块来计算ROC曲线，并使用`matplotlib`中的`pyplot`模块（这是Python中标准的数据可视化库）来可视化ROC曲线。
- en: '***Splitting Data into Training and Test Sets***'
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***将数据分割为训练集和测试集***'
- en: Now that we’ve randomly sorted the `X` and `y` arrays corresponding to our training
    data, we can split these arrays into equally sized training and test sets, as
    shown in [Listing 8-16](ch08.xhtml#ch08list16), which continues defining the `evaluate()`
    function begun in [Listing 8-15](ch08.xhtml#ch08list15).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经随机排序了与训练数据对应的`X`和`y`数组，可以将这些数组拆分成大小相等的训练集和测试集，如[列表 8-16](ch08.xhtml#ch08list16)所示，接着继续定义在[列表
    8-15](ch08.xhtml#ch08list15)中开始的`evaluate()`函数。
- en: '[PRE24]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '*Listing 8-16: Splitting the data into training and test sets*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 8-16：将数据分割成训练集和测试集*'
- en: First, we convert `X` and `y` into `numpy` arrays ➊, and then we create a list
    of indices corresponding to the number of elements in `X` and `y` ➋. Next, we
    randomly shuffle these indices ➌ and reorder `X` and `y` based on this new order
    ➍. This sets us up to randomly assign samples to either our training set or our
    test set, ensuring that we don’t split the samples simply by the order in which
    they occur in our experimental data directory. To complete the random split, we
    divide the arrays in half by finding the array index that evenly splits the dataset
    in half, rounding this point to the nearest integer using the `int()` function
    ➎, and then actually splitting the `X` and `y` arrays into training and test sets
    ➏.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将`X`和`y`转换为`numpy`数组 ➊，然后创建一个索引列表，表示`X`和`y`中元素的数量 ➋。接下来，我们随机打乱这些索引 ➌，并根据这个新顺序重新排列`X`和`y`
    ➍。这样，我们就为随机分配样本到训练集或测试集做好了准备，确保我们不是单纯按实验数据目录中的顺序来拆分样本。为了完成随机拆分，我们通过查找一个将数据集均分的数组索引，将该点四舍五入为最接近的整数，使用`int()`函数
    ➎，然后将`X`和`y`数组实际拆分为训练集和测试集 ➏。
- en: 'Now that we have our training and test sets, we can instantiate and train our
    decision tree detector using the training data using the following:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了训练集和测试集，可以使用以下方法，利用训练数据实例化并训练我们的决策树检测器：
- en: '[PRE25]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then we use the trained classifier to get scores for our test examples corresponding
    to the likelihood that these test examples are malicious:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用训练好的分类器，获取测试样本的得分，这些得分对应于这些测试样本是恶意的可能性：
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Here, we call the `predict_proba()` method on our classifier, which predicts
    the probability that our test examples are benignware or malware. Then, using
    `numpy` indexing magic, we pull out only the probabilities that the samples are
    malicious, as opposed to benign. Keep in mind that these probabilities are redundant
    (for example, if the probability an example is malicious is 0.99, then the probability
    it’s benign is 0.01, since probabilities add up to 1.00), so all we need is the
    malware probability here.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们调用`predict_proba()`方法对分类器进行预测，该方法预测我们的测试样本是良性软件还是恶意软件的概率。然后，使用`numpy`索引技巧，我们仅提取出样本是恶意的概率，而不是良性概率。请记住，这些概率是冗余的（例如，如果某个样本是恶意软件的概率是0.99，那么它是良性软件的概率就是0.01，因为概率总和为1.00），因此这里我们只需要恶意软件的概率。
- en: '***Computing the ROC Curve***'
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***计算 ROC 曲线***'
- en: 'Now that we’ve computed malware probabilities (which we can also refer to as
    “scores”) using our detector, it’s time to compute our ROC curve. We do this by
    first calling the `roc_curve` function within `sklearn`’s `metrics` module, like
    this:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经使用检测器计算了恶意软件概率（我们也可以称之为“得分”），是时候计算我们的ROC曲线了。我们通过首先调用`sklearn`的`metrics`模块中的`roc_curve`函数来完成这一步，如下所示：
- en: '[PRE27]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `roc_curve` function tests a variety of *decision thresholds,* or score
    thresholds above which we would deem a software binary to be malicious, and measures
    what the false positive rate and true positive rate of the detector would be if
    we were to use that detector.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`roc_curve`函数测试各种*决策阈值*，即我们认为软件二进制文件是恶意软件的得分阈值，并衡量如果我们使用该检测器时，假阳性率和真阳性率的变化。'
- en: 'You can see that the `roc_curve` function takes two arguments: the label vector
    for our test examples (`test_y`) and the `scores` array, which contains our detector’s
    judgment about how malicious it thinks each training example is. The function
    returns three related arrays: `fpr`, `tpr`, and `thresholds`. These arrays are
    all of equal length, such that the false positive rate, true positive rate, and
    decision threshold at each index correspond to one another.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，`roc_curve`函数接受两个参数：我们的测试样本的标签向量（`test_y`）和`score`数组，该数组包含了我们检测器对每个训练样本是否为恶意软件的判断。该函数返回三个相关数组：`fpr`（假阳性率），`tpr`（真阳性率），和`thresholds`（阈值）。这些数组长度相等，因此在每个索引处的假阳性率、真阳性率和决策阈值是相互对应的。
- en: 'Now we can use `matplotlib` to visualize the ROC curve we just calculated.
    We do this by calling the `plot` method on `matplotlib`’s `pyplot` module, as
    shown here:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`matplotlib`来可视化我们刚刚计算出的ROC曲线。我们通过调用`matplotlib`的`pyplot`模块中的`plot`方法来实现，代码如下所示：
- en: '[PRE28]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We call the `xlabel`, `ylabel`, and `title` methods to label the chart’s axes
    and title, and then the `show` method to make the chart window pop up.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用`xlabel`、`ylabel`和`title`方法来标记图表的坐标轴和标题，然后使用`show`方法让图表窗口弹出。
- en: The resulting ROC curve is shown in [Figure 8-2](ch08.xhtml#ch08fig2).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的ROC曲线如[图8-2](ch08.xhtml#ch08fig2)所示。
- en: '![image](../images/f0150-01.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0150-01.jpg)'
- en: '*Figure 8-2: Visualizing the detector’s ROC curve*'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8-2：可视化检测器的ROC曲线*'
- en: You can see from the plot in [Figure 8-2](ch08.xhtml#ch08fig2) that our detector
    performs well for such a basic example. At around a 1 percent false positive rate
    (10^(–2)), it can detect about 94 percent of the malware samples in the test set.
    We’re only training it on a few hundred training examples here; to get better
    accuracy we’d need to train it on tens of thousands, hundreds of thousands, or
    even millions of examples (alas, scaling machine learning to this degree is beyond
    the scope of this book).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图8-2](ch08.xhtml#ch08fig2)中的图表可以看出，我们的检测器在这样的基本示例中表现良好。在大约1%的假阳性率（10^(–2)）下，它能够检测出测试集中的约94%的恶意软件样本。我们这里只在几百个训练样本上进行训练；要提高准确性，我们需要在成千上万、几十万，甚至百万级的样本上进行训练（唉，扩展机器学习到如此规模超出了本书的范围）。
- en: '***Cross-Validation***'
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***交叉验证***'
- en: Although visualizing the ROC curve is useful, we can actually do better at predicting
    our detector’s real-world accuracy by performing many experiments on our training
    data, not just one. Recall that to perform our test, we split our training examples
    in half, training the detector on the first half and testing it on the second
    half. This is really an insufficient test of our detector. In the real world,
    we won’t be measured on our accuracy on this particular set of test examples but
    rather on our accuracy on new, previously unseen malware. To get a better sense
    of how we’ll perform once we deploy, we need to run more than just one experiment
    on one set of test data; we need to perform many experiments on many test sets
    and get a sense of the overall trend in accuracy.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可视化ROC曲线是有用的，但实际上我们可以通过对训练数据进行多次实验来更好地预测我们检测器在现实世界中的准确性，而不仅仅是进行一次实验。回想一下，我们为了进行测试，将训练样本分为两部分，在前一部分上训练检测器，在后一部分上测试检测器。这实际上是对检测器的不充分测试。在现实世界中，我们的准确性将不会仅仅在这一特定的测试集上进行衡量，而是针对新出现、以前未见过的恶意软件来衡量。为了更好地了解我们部署后的表现，我们需要在多个测试集上进行多次实验，并了解整体的准确性趋势。
- en: We can use *cross-validation* to do this. The basic idea behind cross-validation
    is to split our training examples into a number of folds (here I use three folds,
    but you can use more). For example, if you had 300 examples and decided to split
    them into three folds, the first 100 samples would go in the first fold, the second
    100 would go in the second fold, and the third 100 would go in the third fold.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用*交叉验证*来做到这一点。交叉验证的基本思想是将我们的训练样本分成若干个折叠（这里我使用了三折，你可以使用更多）。例如，如果你有300个样本并决定将它们分成三折，第一个100个样本会进入第一折，第二个100个样本进入第二折，第三个100个样本进入第三折。
- en: Then we run three tests. In the first test, we train the system on folds 2 and
    3 and test the system on fold 1\. On the second test, we repeat this process but
    train the system on folds 1 and 3 and test the system on fold 2\. On the third
    test, as you can probably predict by now, we train the system on folds 1 and 2
    and test the system on fold 3\. [Figure 8-3](ch08.xhtml#ch08fig3) illustrates
    this cross-validation process.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们进行三次测试。在第一次测试中，我们在第2折和第3折上训练系统，在第1折上测试系统。在第二次测试中，我们重复这一过程，但在第1折和第3折上训练系统，在第2折上测试系统。在第三次测试中，如你所料，我们在第1折和第2折上训练系统，在第3折上测试系统。[图8-3](ch08.xhtml#ch08fig3)展示了这一交叉验证过程。
- en: '![image](../images/f0151-01.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0151-01.jpg)'
- en: '*Figure 8-3: A visualization of a sample cross-validation process*'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8-3：一个示例交叉验证过程的可视化*'
- en: The `sklearn` library makes implementing cross-validation easy. To do this,
    let’s rewrite our `evaluate` function from [Listing 8-15](ch08.xhtml#ch08list15)
    as `cv_evaluate`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`库使得实现交叉验证变得简单。为了做到这一点，让我们将[清单8-15](ch08.xhtml#ch08list15)中的`evaluate`函数重写为`cv_evaluate`。'
- en: '[PRE29]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We start the `cv_evaluate()` function the same way we started our initial evaluation
    function, except that here we also import the `KFold` class from `sklearn`’s `cross_validation`
    module. *K-fold cross-validation*, or `KFold` for short, is synonymous with the
    type of cross-validation I just discussed and is the most common way to do cross-validation.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以与开始初步评估函数相同的方式启动`cv_evaluate()`函数，唯一不同的是这里我们还从`sklearn`的`cross_validation`模块中导入了`KFold`类。*K折交叉验证*，简称`KFold`，与我刚才讨论的交叉验证类型是同义的，也是最常用的交叉验证方法。
- en: 'Next, we convert our training data to `numpy` arrays so that we can use `numpy`’s
    enhanced array indexing on it:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将训练数据转换为`numpy`数组，以便我们可以对其使用`numpy`增强的数组索引：
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following code actually starts the cross-validation process:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码实际上启动了交叉验证过程：
- en: '[PRE31]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We first instantiate the `KFold` class, passing in the number of training examples
    we have as the first parameter and the number of folds we’d like to use as the
    second argument. The third argument, `shuffle=True` ➊, tells `sklearn` to randomly
    sort our training data before dividing it into three folds. The `KFold` instance
    is actually an iterator that gives a different training or test example split
    on each iteration. Within the `for` loop, we assign the training instances and
    test instances to the `training_X` and `training_y` arrays ➋ that contain the
    corresponding elements.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先实例化`KFold`类，将训练样本的数量作为第一个参数传入，并将希望使用的折数作为第二个参数传入。第三个参数`shuffle=True` ➊，告诉`sklearn`在将训练数据分成三折之前随机排序。`KFold`实例实际上是一个迭代器，在每次迭代时提供不同的训练或测试样本拆分。在`for`循环内，我们将训练实例和测试实例分配给包含相应元素的`training_X`和`training_y`数组
    ➋。
- en: 'After preparing the training and test data, we’re ready to instantiate and
    train the `RandomForestClassifier`, as you’ve learned to do previously in this
    chapter:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备好训练数据和测试数据后，我们就可以像本章前面学到的那样实例化并训练`RandomForestClassifier`：
- en: '[PRE32]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we compute a ROC curve for this particular fold and then plot a line
    that represents this ROC curve:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们为这个特定的折计算ROC曲线，然后绘制一条代表该ROC曲线的线：
- en: '[PRE33]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Note that we don’t call the `matplotlib` `show` method to display the chart
    just yet. We do this after all the folds have been evaluated and we’re ready to
    show all three lines at once. As we did in the previous section, we label our
    axes and give the plot a title, like this:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们暂时还没有调用`matplotlib`的`show`方法来显示图表。我们会在所有折评估完成并准备好一次性显示所有三条曲线时再做此操作。就像在上一节中一样，我们给坐标轴添加标签并为图表添加标题，像这样：
- en: '[PRE34]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The resulting ROC curve is shown in [Figure 8-4](ch08.xhtml#ch08fig4).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的ROC曲线显示在[图8-4](ch08.xhtml#ch08fig4)中。
- en: As you can see, our results were similar on every fold, but there is definitely
    some variation. Our detection rate (true positive rate) over the three runs averages
    about 90 percent at a 1 percent false positive rate. This estimate, which takes
    into account all three cross-validation experiments, is a more accurate estimate
    of our detector’s performance than we’d get if we just ran one experiment on our
    data; in that case, which samples we happened to use for training and testing
    would lead to a somewhat random outcome. By running more experiments, we can get
    a more robust sense of our solution’s efficacy.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们的结果在每一折上都类似，但确实存在一定的波动。在三次运行中，我们的检测率（真正率）在1%的假阳性率下平均约为90%。这个估计考虑了所有三次交叉验证实验，比我们只在数据上进行一次实验时得到的结果更准确；在后者的情况下，训练和测试样本的选择会导致一个有些随机的结果。通过进行更多的实验，我们可以更可靠地了解我们解决方案的有效性。
- en: '![image](../images/f0153-01.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0153-01.jpg)'
- en: '*Figure 8-4: Plotting the detector’s ROC curve using cross-validation*'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8-4：使用交叉验证绘制检测器的ROC曲线*'
- en: 'Note that these results are not great because we’re training on a very small
    amount of data: a few hundred malware and benignware samples. At my day job, where
    we train large-scale machine learning malware detection systems, we usually train
    on hundreds of millions of samples. You don’t need hundreds of millions of samples
    to train your own malware detector, but you’ll want to assemble datasets of at
    least tens of thousands of samples to start getting really good performance (for
    example, a 90 percent detection rate at a 0.1 percent false positive rate).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些结果并不理想，因为我们训练的数据量非常小：只有几百个恶意软件和良性软件样本。在我的日常工作中，我们训练大规模的机器学习恶意软件检测系统，通常使用数亿个样本。你不需要数亿个样本来训练自己的恶意软件检测器，但你应该至少准备几万的样本集，以便开始获得真正优秀的性能（例如，在0.1%的假阳性率下达到90%的检测率）。
- en: '**Next Steps**'
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**下一步**'
- en: So far, I covered how to use Python and `sklearn` to extract features from a
    training dataset of software binaries, and then train and evaluate a decision
    tree–based machine learning approach. To improve the system, you can use features
    other than or in addition to printable string features (for example, the PE header,
    instruction N-gram, or Import Address Table features discussed previously), or
    you could use a different machine learning algorithm.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我讲解了如何使用Python和`sklearn`从软件二进制文件的训练数据集中提取特征，然后训练和评估基于决策树的机器学习方法。为了改善系统，你可以使用除可打印字符串特征之外的其他特征（例如之前讨论的PE头、指令N-gram或导入地址表特征），或者你可以使用不同的机器学习算法。
- en: To make the detector more accurate, I recommend going beyond `sklearn`’s `RandomForestClassifier`
    (`sklearn.ensemble.RandomForestClassifier`) to try other classifiers. Recall from
    the previous chapter that *random forest detectors* are also based on decision
    trees, but instead of just one decision tree, they build many decision trees,
    randomizing the way they are built. To determine whether a new file is malware
    or benignware, each of these decision trees makes individual decisions, which
    we combine by summing them up and dividing them by the total number of trees to
    get the average result.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使检测器更加准确，我建议超越`sklearn`的`RandomForestClassifier`（`sklearn.ensemble.RandomForestClassifier`）尝试其他分类器。回想上一章，*随机森林检测器*也基于决策树，但它们不仅仅构建一个决策树，而是构建许多决策树，并随机化它们的构建方式。为了判断一个新文件是恶意软件还是良性软件，每棵决策树都会做出独立的判断，我们通过将它们的结果相加并除以树的总数来获得平均结果。
- en: 'You can also use other algorithms that `sklearn` provides, such as logistic
    regression. Using any of these algorithms can be as simple as doing a search and
    replace in the sample code discussed in this chapter. For example, in this chapter
    we instantiate and train our decision tree as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用`sklearn`提供的其他算法，例如逻辑回归。使用这些算法中的任何一种，只需在本章讨论的示例代码中进行简单的搜索和替换。例如，在本章中，我们通过以下方式实例化并训练我们的决策树：
- en: '[PRE35]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'But you can simply replace that code with this:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你可以简单地将那段代码替换为以下内容：
- en: '[PRE36]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This replacement yields a logistic regression detector instead of a decision
    tree–based detector. By computing a new cross validation–based evaluation of this
    Logistic Regression detector and comparing it to the results from [Figure 8-4](ch08.xhtml#ch08fig4),
    you could determine which works better.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这个替代方法会生成一个逻辑回归检测器，而不是基于决策树的检测器。通过对这个逻辑回归检测器进行基于交叉验证的评估，并将其与[图8-4](ch08.xhtml#ch08fig4)中的结果进行比较，你可以确定哪种方法效果更好。
- en: '**Summary**'
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this chapter, you learned the ins and outs of building machine learning–based
    malware detectors. Specifically, you learned how to extract features from software
    binaries for machine learning, how to compress these features using the hashing
    trick, and how to train machine learning–based malware detectors using these extracted
    features. You also learned how to plot ROC curves to examine the relationship
    between a detector’s detection threshold and its true and false positive rates.
    Finally, you learned about cross-validation, a more advanced evaluation concept,
    and other possible extensions to enhance the detector used in this chapter.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了构建基于机器学习的恶意软件检测器的方方面面。具体来说，你学习了如何从软件二进制文件中提取特征用于机器学习，如何使用哈希技巧压缩这些特征，以及如何使用这些提取的特征训练基于机器学习的恶意软件检测器。你还学习了如何绘制ROC曲线，以检查检测器的检测阈值与其真正和假阳性率之间的关系。最后，你了解了交叉验证这一更高级的评估概念，以及其他可能的扩展，用于增强本章中使用的检测器。
- en: This concludes this book’s discussion of machine learning–based malware detection
    using `sklearn`. We’ll cover another set of machine learning methods, known as
    deep learning methods or artificial neural networks in [Chapters 10](ch10.xhtml#ch10)
    and [11](ch11.xhtml#ch11). You now have the basic knowledge necessary to effectively
    use machine learning in the context of malware identification. I encourage you
    to read more about machine learning. Because computer security is in many ways
    a data analysis problem, machine learning is here to stay in the security industry
    and will continue to be useful not only in detecting malicious binaries but also
    in detecting malicious behavior in network traffic, system logs, and other contexts.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的讨论已经结束，内容涵盖了基于`sklearn`的机器学习恶意软件检测方法。我们将在[第10章](ch10.xhtml#ch10)和[第11章](ch11.xhtml#ch11)中介绍另一类机器学习方法，即深度学习方法或人工神经网络。现在你已经掌握了在恶意软件识别中有效使用机器学习所需的基础知识。我鼓励你进一步阅读有关机器学习的内容。由于计算机安全在许多方面是一个数据分析问题，机器学习将在安全行业中持续存在，并且不仅在检测恶意二进制文件时有用，还能在检测网络流量、系统日志及其他环境中的恶意行为时发挥作用。
- en: In the next chapter, we’ll take a deep dive into visualizing malware relationships,
    which can help us quickly understand the similarities and differences between
    large numbers of malware samples.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨恶意软件关系的可视化，这有助于我们快速理解大量恶意软件样本之间的相似性和差异性。
