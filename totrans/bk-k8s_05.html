<html><head></head><body>
<h2 class="h2" id="ch04"><span epub:type="pagebreak" id="page_51"/><span class="big">4</span><br/>NETWORK NAMESPACES</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">Understanding container networking is the biggest challenge in building modern applications based on containerized microservices. First, networking is complicated even without introducing containers. Multiple levels of abstraction are involved just in sending a simple <span class="literal">ping</span> from one physical server to another. Second, containers introduce additional complexity because each has its own set of virtual network devices to make it look like a separate machine. Not only that, but a container orchestration framework like Kubernetes then adds another layer of complexity by adding an “overlay” network through which containers can communicate even when they are running on different hosts.</p>&#13;
<p class="indent">In this chapter, we will look in detail at how container networking operates. We will look at a container’s virtual network devices, including how each network device is assigned a separate IP address that can reach the host. We’ll see how containers on the same host are connected to one another through a bridge device and how container devices are configured to <span epub:type="pagebreak" id="page_52"/>route traffic. Finally, we’ll examine how address translation is used to enable containers to connect to other hosts without exposing container networking internals on the host’s network.</p>&#13;
<h3 class="h3" id="ch00lev1sec17">Network Isolation</h3>&#13;
<p class="noindent">In <a href="ch02.xhtml#ch02">Chapter 2</a>, we discussed how isolation is important to system reliability because processes generally can’t affect something they cannot see. This is one important reason for network isolation in containers. Another reason is ease of configuration. To run a process that acts as a server, such as a web server, we need to choose one or more network interfaces on which that server will listen, and we need to choose a port number on which it will listen. We can’t have two processes listening on the same port on the same interface.</p>&#13;
<p class="indent">As a result, it’s common for a process that acts as a server to provide a way to configure which port it should use to listen for connections. However, that still requires us to know what other servers are out there and what ports they are using so that we can ensure there are no conflicts. That would be impossible with a container orchestration framework like Kubernetes because new processes can show up at any time, from different users, with a need to listen on any port number.</p>&#13;
<p class="indent">The way to get around this is to provide separate virtual network interfaces for each container. That way, a process in a container can choose any port it wants—it will be listening on a different network interface from a process in a different container. Let’s see a quick example.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on getting set up.</em></p>&#13;
</div>&#13;
<p class="indent">We’ll run two instances of an NGINX web server; each instance will listen on port 80. As before, we’ll use CRI-O and <span class="literal">crictl</span>, but we’ll use a script to cut down on the typing:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cd /opt</span>&#13;
root@host01:/opt# <span class="codestrong1">source nginx.sh</span>&#13;
...</pre>&#13;
<p class="indent">The <span class="literal">source</span> before <span class="literal">nginx.sh</span> is important; it ensures that the script is run in a way that makes the environment variables it sets available in our shell for future commands. Inside <em>nginx.sh</em> are the usual <span class="literal">crictl runp</span>, <span class="literal">crictl create</span>, and <span class="literal">crictl start</span> commands we’ve used in previous chapters. The YAML files are also very similar to examples we’ve seen before; the only difference is that we use a container image that has NGINX installed.</p>&#13;
<p class="indent">Let’s verify that we have two NGINX servers running:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">crictl ps</span>&#13;
CONTAINER      IMAGE            ... NAME    ...&#13;
<span epub:type="pagebreak" id="page_53"/>ae341010886ae  .../nginx:latest ... nginx2  ...&#13;
6a95800b16f15  .../nginx:latest ... nginx1  ...</pre>&#13;
<p class="indent">We can also verify that both NGINX servers are listening on port 80, the standard port for web servers:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">crictl exec $N1C_ID cat /proc/net/tcp</span>&#13;
  sl  local_address ...&#13;
   0: 00000000:0050 ...&#13;
root@host01:/opt# <span class="codestrong1">crictl exec $N2C_ID cat /proc/net/tcp</span>&#13;
  sl  local_address ...&#13;
   0: 00000000:0050 ...</pre>&#13;
<p class="indent">We look at the open port by printing <em>/proc/net/tcp</em> because we need to run this command inside the NGINX container, where we don’t have standard Linux commands like <span class="literal">netstat</span> or <span class="literal">ss</span>. As we saw in <a href="ch02.xhtml#ch02">Chapter 2</a>, in a container we have a separate <span class="literal">mnt</span> namespace providing a separate filesystem for each container, so only the executables available in that separate filesystem can be run in that namespace.</p>&#13;
<p class="indent">The port shown in both cases is <span class="literal">0050</span> in hexadecimal, which is port 80 in decimal. If these two processes were running together on the same system without network isolation, they wouldn’t both be able to listen on port 80, but in this case, the two NGINX instances have separate network interfaces. To explore this further, let’s start up a new BusyBox container:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">source busybox.sh</span>&#13;
...</pre>&#13;
<p class="indent">BusyBox is now running in addition to our two NGINX containers:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">crictl ps</span>&#13;
CONTAINER      IMAGE              ... NAME    ...&#13;
189dd26766d26  .../busybox:latest ... busybox ...&#13;
ae341010886ae  .../nginx:latest   ... nginx2  ...&#13;
6a95800b16f15  .../nginx:latest   ... nginx1  ...</pre>&#13;
<p class="indent">Let’s start a shell inside the container:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">crictl exec -ti $B1C_ID /bin/sh</span>&#13;
/ #</pre>&#13;
<p class="indent"><a href="ch04.xhtml#ch04list1">Listing 4-1</a> shows the container’s network devices and addresses.</p>&#13;
<pre>/ # <span class="codestrong1">ip addr</span>&#13;
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue ...&#13;
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00&#13;
    inet 127.0.0.1/8 scope host lo&#13;
        valid_lft forever preferred_lft forever&#13;
    inet6 ::1/128 scope host &#13;
        valid_lft forever preferred_lft forever&#13;
3: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue &#13;
    link/ether 9a:7c:73:2f:f7:1a brd ff:ff:ff:ff:ff:ff&#13;
    inet 10.85.0.4/16 brd 10.85.255.255 scope global eth0&#13;
        valid_lft forever preferred_lft forever&#13;
    inet6 fe80::987c:73ff:fe2f:f71a/64 scope link &#13;
        valid_lft forever preferred_lft forever</pre>&#13;
<p class="caption" id="ch04list1"><em>Listing 4-1: BusyBox network</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_54"/>Ignoring the standard loopback device, we see a network device with <span class="literal">10.85.0.4</span> for an IP address. This does not correspond at all with the IP address of the host, which is <span class="literal">192.168.61.11</span>; it is on a different network entirely. Because our container is on a separate network, we might not expect to be able to <span class="literal">ping</span> the underlying host system from inside the container, but it works, as <a href="ch04.xhtml#ch04list2">Listing 4-2</a> demonstrates.</p>&#13;
<pre>/ # <span class="codestrong1">ping -c 1 192.168.61.11</span>&#13;
PING 192.168.61.11 (192.168.61.11): 56 data bytes&#13;
64 bytes from 192.168.61.11: seq=0 ttl=64 time=7.471 ms&#13;
&#13;
--- 192.168.61.11 ping statistics ---&#13;
1 packets transmitted, 1 packets received, 0% packet loss&#13;
round-trip min/avg/max = 7.471/7.471/7.471 ms</pre>&#13;
<p class="caption" id="ch04list2"><em>Listing 4-2: BusyBox ping test</em></p>&#13;
<p class="indent">For traffic to get from our container to the host network, there must be an entry in the routing table to make that happen. As <a href="ch04.xhtml#ch04list3">Listing 4-3</a> illustrates, we can verify this by using the <span class="literal">ip</span> command.</p>&#13;
<pre>/ # <span class="codestrong1">ip route</span>&#13;
default via 10.85.0.1 dev eth0 &#13;
10.85.0.0/16 dev eth0 scope link  src 10.85.0.4</pre>&#13;
<p class="caption" id="ch04list3"><em>Listing 4-3: BusyBox routes</em></p>&#13;
<p class="indent">As expected, there is a default route. When we sent the <span class="literal">ping</span>, our BusyBox container reached out to <span class="literal">10.85.0.1</span>, which then had the ability to send the <span class="literal">ping</span> onward until it reached <span class="literal">192.168.61.11</span>.</p>&#13;
<p class="indent">We’ll leave all three containers running to explore them further, but let’s exit our BusyBox shell to get back to the host:</p>&#13;
<pre>/ # <span class="codestrong1">exit</span></pre>&#13;
<p class="indent">The view of the network from inside the container shows why our two NGINX servers are both able to listen on port 80. As mentioned earlier, only one process can listen on a port for a particular interface, but of course, if each NGINX server has a separate network interface, there is no conflict.</p>&#13;
<h3 class="h3" id="ch00lev1sec18"><span epub:type="pagebreak" id="page_55"/>Network Namespaces</h3>&#13;
<p class="noindent">CRI-O is using Linux network namespaces to create this isolation. We explored network namespaces briefly in <a href="ch02.xhtml#ch02">Chapter 2</a>; in this chapter, we’ll look at them in more detail.</p>&#13;
<p class="indent">First, let’s use the <span class="literal">lsns</span> command to list the network namespaces that CRI-O has created for our containers:</p>&#13;
<pre>root@host01:/opt# lsns -t net&#13;
        NS TYPE NPROCS   PID USER    NETNSID NSFS                   COMMAND&#13;
4026531992 net     114     1 root unassigned                        /sbin/init&#13;
4026532196 net       4  5801 root          0 /run/netns/ab8be6e6... /pause&#13;
4026532272 net       4  5937 root          1 /run/netns/8ffe0394... /pause&#13;
4026532334 net       2  6122 root          2 /run/netns/686d71d9... /pause</pre>&#13;
<p class="indent">In addition to the root network namespace that is used for all the processes that aren’t in a container, we see three network namespaces, one for each Pod we’ve created.</p>&#13;
<p class="indent">When we use CRI-O with <span class="literal">crictl</span>, the network namespace actually belongs to the Pod. The <span class="literal">pause</span> process that is listed here exists so that the namespaces can continue to exist even as containers come and go inside the Pod.</p>&#13;
<p class="indent">In the previous example, there are four network namespaces. The first one is the root namespace that was created when our host booted. The other three were created for each of the containers we have started so far: two NGINX containers and one BusyBox container.</p>&#13;
<h4 class="h4" id="ch00lev2sec28">Inspecting Network Namespaces</h4>&#13;
<p class="noindent">To learn about how network namespaces work and manipulate them, we’ll use the <span class="literal">ip netns</span> command to list network namespaces:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip netns list</span>&#13;
7c185da0-04e2-4321-b2eb-da18ceb5fcf6 (id: 2)&#13;
d26ca6c6-d524-4ae2-b9b7-5489c3db92ce (id: 1)&#13;
38bbb724-3420-46f0-bb50-9a150a9f0889 (id: 0)</pre>&#13;
<p class="indent">This command looks in a different configuration location to find network namespaces, so only the three container namespaces are listed.</p>&#13;
<p class="indent">We want to capture the network namespace for our BusyBox container. It’s one of the three listed, and we can guess that it is the one labeled <span class="literal">(id: 2)</span> because we created it last, but we can also use <span class="literal">crictl</span> and <span class="literal">jq</span> to extract the information we need:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">NETNS_PATH=$(crictl inspectp $B1P_ID |</span>&#13;
  <span class="codestrong1">jq -r '.info.runtimeSpec.linux.namespaces[]|select(.type=="network").path')</span>&#13;
root@host01:/opt# <span class="codestrong1">echo $NETNS_PATH</span>&#13;
/var/run/netns/7c185da0-04e2-4321-b2eb-da18ceb5fcf6&#13;
root@host01:/opt# <span class="codestrong1">NETNS=$(basename $NETNS_PATH)</span>&#13;
<span epub:type="pagebreak" id="page_56"/>root@host01:/opt# <span class="codestrong1">echo $NETNS</span>&#13;
7c185da0-04e2-4321-b2eb-da18ceb5fcf6</pre>&#13;
<p class="indent">If you run <span class="literal">crictl inspectp $B1P_ID</span> by itself, you’ll see a wealth of information about the BusyBox Pod. Out of all that information, we want only the information about the network namespace, so we use <span class="literal">jq</span> to extract that information in three steps. First, it reaches down into the JSON data to pull out all of the namespaces associated with this Pod. It then selects only the namespace that has a <span class="literal">type</span> field of <span class="literal">network</span>. Finally, it extracts the <span class="literal">path</span> field for that namespace and stores it in the environment variable <span class="literal">NETNS_PATH</span>.</p>&#13;
<p class="indent">The value that <span class="literal">crictl</span> returns is the full path to the network namespace under <em>/var/run</em>. For our upcoming commands, we want only the value of the namespace, so we use <span class="literal">basename</span> to strip off the path. Also, because this information will be a lot more usable if we assign it to an environment variable, we do that, and then we use <span class="literal">echo</span> to print the value so that we can confirm it all worked.</p>&#13;
<p class="indent">Of course, for interactive debugging, you can often just scroll through the entire contents of <span class="literal">crictl inspectp</span> (for Pods) and <span class="literal">crictl inspect</span> (for containers) and pick out the values you want. But this approach of extracting data with <span class="literal">jq</span> is very useful in scripting or in reducing the amount of output to scan through manually.</p>&#13;
<p class="indent">Now that we’ve extracted the network namespace for BusyBox from <span class="literal">crictl</span>, let’s see what processes are assigned to that namespace:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ps --pid $(ip netns pids $NETNS)</span>&#13;
PID TTY      STAT   TIME COMMAND&#13;
5800 ?        Ss     0:00 /pause&#13;
5839 ?        Ss     0:00 /bin/sleep 36000</pre>&#13;
<p class="indent">If we just ran <span class="literal">ip netns pids $NETNS</span>, we would get a list of the process IDs (PIDs), but no extra information. We take that output and send it to <span class="literal">ps --pid</span>, which makes it possible for us to see the name of the commands. As expected, we see a <span class="literal">pause</span> process and the <span class="literal">sleep</span> process that we specified when we ran the BusyBox container.</p>&#13;
<p class="indent">In the previous section, we used <span class="literal">crictl exec</span> to run a shell inside the container, which enabled us to see what network interfaces were available in that network namespace. Now that we know the ID of the network namespace, we can use <span class="literal">ip netns exec</span> to run commands individually from within a network namespace. Running <span class="literal">ip netns exec</span> is very powerful in that it is not limited to just networking commands, but could be any process such as a web server. However, note that this is not the same as fully running inside the container, because we are not entering any of the container’s other namespaces (for example, the <span class="literal">pid</span> namespace used for process isolation).</p>&#13;
<p class="indent">Next, let’s try the <span class="literal">ip addr</span> command from within the BusyBox network namespace:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip netns exec $NETNS ip addr</span>&#13;
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue ...&#13;
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00&#13;
    inet 127.0.0.1/8 scope host lo&#13;
        valid_lft forever preferred_lft forever&#13;
    inet6 ::1/128 scope host &#13;
        valid_lft forever preferred_lft forever&#13;
3: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue ...&#13;
    link/ether 9a:7c:73:2f:f7:1a brd ff:ff:ff:ff:ff:ff link-netnsid 0&#13;
    inet 10.85.0.4/16 brd 10.85.255.255 scope global eth0&#13;
        valid_lft forever preferred_lft forever&#13;
    inet6 fe80::987c:73ff:fe2f:f71a/64 scope link &#13;
        valid_lft forever preferred_lft forever</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_57"/>The list of network devices and IP addresses that we see here matches what we saw when we ran commands inside our BusyBox container in <a href="ch04.xhtml#ch04list1">Listing 4-1</a>. CRI-O is creating these network devices and placing them in the network namespace. (We will see how CRI-O was configured to perform container networking when we look at Kubernetes networking in <a href="ch08.xhtml#ch08">Chapter 8</a>.) For now, let’s look at how we can create our own devices and namespaces for network isolation. This will also show us how to debug container networking when something isn’t working correctly.</p>&#13;
<h4 class="h4" id="ch00lev2sec29">Creating Network Namespaces</h4>&#13;
<p class="noindent">We can create a network namespace with a single command:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip netns add myns</span></pre>&#13;
<p class="indent">This new namespace immediately shows up in the list:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip netns list</span>&#13;
myns&#13;
7c185da0-04e2-4321-b2eb-da18ceb5fcf6 (id: 2)&#13;
d26ca6c6-d524-4ae2-b9b7-5489c3db92ce (id: 1)&#13;
38bbb724-3420-46f0-bb50-9a150a9f0889 (id: 0)</pre>&#13;
<p class="indent">This namespace isn’t very useful yet; it has a loopback interface but nothing else:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip netns exec myns ip addr</span>&#13;
1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000&#13;
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</pre>&#13;
<p class="indent">In addition, even the loopback interface is down, so it couldn’t be used. Let’s quickly fix that:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip netns exec myns ip link set dev lo up</span>&#13;
root@host01:/opt# <span class="codestrong1">ip netns exec myns ip addr</span>&#13;
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue ...&#13;
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00&#13;
    inet 127.0.0.1/8 scope host lo&#13;
       valid_lft forever preferred_lft forever&#13;
    inet6 ::1/128 scope host &#13;
       valid_lft forever preferred_lft forever</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_58"/>The loopback interface is now up, and it has the typical IP address of <span class="literal">127.0.0.1</span>. A basic loopback <span class="literal">ping</span> will now work in this network namespace:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip netns exec myns ping -c 1 127.0.0.1</span>&#13;
PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.&#13;
64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.035 ms&#13;
&#13;
--- 127.0.0.1 ping statistics ---&#13;
1 packets transmitted, 1 received, 0% packet loss, time 0ms&#13;
rtt min/avg/max/mdev = 0.035/0.035/0.035/0.000 ms</pre>&#13;
<p class="indent">The ability to <span class="literal">ping</span> the loopback network interface is a useful first test for any networking stack, as it shows the ability to send and receive packets. So, we now have a basic working network stack in our new network namespace, but it still isn’t terribly useful because a loopback interface by itself can’t talk to anything else on our system. We need to add another network device in this network namespace in order to establish connectivity to the host and the rest of the network.</p>&#13;
<p class="indent">To do this, we’ll create a <em>virtual Ethernet</em> (veth) device. You can think of a veth as a virtual network cable. Like a network cable, it has two ends, and whatever goes in one end comes out the other end. For this reason, the term <em>veth pair</em> is often used.</p>&#13;
<p class="indent">We start with a command that creates the veth pair:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip link add myveth-host type veth \</span>&#13;
                  <span class="codestrong1">peer myveth-myns netns myns</span></pre>&#13;
<p class="indent">This command does three things:</p>&#13;
<ol>&#13;
<li class="noindent">Creates a veth device called <span class="literal">myveth-host</span></li>&#13;
<li class="noindent">Creates a veth device called <span class="literal">myveth-myns</span></li>&#13;
<li class="noindent">Places the device <span class="literal">myveth-myns</span> in the network namespace <span class="literal">myns</span></li>&#13;
</ol>&#13;
<p class="indent">The host side of the veth pair appears in the regular list of network devices on the host:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip addr</span>&#13;
...&#13;
8: myveth-host@if2: &lt;BROADCAST,MULTICAST&gt; mtu 1500 ... state DOWN ...&#13;
    link/ether fe:7a:5d:86:00:d9 brd ff:ff:ff:ff:ff:ff link-netns myns</pre>&#13;
<p class="indent">This output shows <span class="literal">myveth-host</span> and also that it is connected to a device in the network namespace <span class="literal">myns</span>.</p>&#13;
<p class="indent">If you run this command for yourself and look at the complete list of host network devices, you will notice additional <span class="literal">veth</span> devices connected to each of the container network namespaces. These were created by CRI-O when we deployed NGINX and BusyBox.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_59"/>Similarly, we can see that our <span class="literal">myns</span> network namespace has a new network interface:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip netns exec myns ip addr</span>&#13;
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue ...&#13;
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00&#13;
    inet 127.0.0.1/8 scope host lo&#13;
       valid_lft forever preferred_lft forever&#13;
    inet6 ::1/128 scope host &#13;
       valid_lft forever preferred_lft forever&#13;
2: myveth-myns@if8: &lt;BROADCAST,MULTICAST&gt; mtu 1500 ... state DOWN ...&#13;
    link/ether 26:0f:64:a8:37:1f brd ff:ff:ff:ff:ff:ff link-netnsid 0</pre>&#13;
<p class="indent">As before, this interface is currently down. We need to bring up both sides of the veth pair before we can start communicating. We also need to assign an IP address to the <span class="literal">myveth-myns</span> side to enable it to communicate:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip netns exec myns ip addr add 10.85.0.254/16 \</span>&#13;
                  <span class="codestrong1">dev myveth-myns</span>&#13;
root@host01:/opt# <span class="codestrong1">ip netns exec myns ip link set dev myveth-myns up</span>&#13;
root@host01:/opt# <span class="codestrong1">ip link set dev myveth-host up</span></pre>&#13;
<p class="indent">A quick check confirms that we’ve successfully configured an IP address and brought up the network:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip netns exec myns ip addr</span>&#13;
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue ...&#13;
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00&#13;
    inet 127.0.0.1/8 scope host lo&#13;
       valid_lft forever preferred_lft forever&#13;
    inet6 ::1/128 scope host &#13;
       valid_lft forever preferred_lft forever&#13;
2: myveth-myns@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; ... state UP ...&#13;
    link/ether 26:0f:64:a8:37:1f brd ff:ff:ff:ff:ff:ff link-netnsid 0&#13;
    inet 10.85.0.254/16 scope global myveth-myns&#13;
       valid_lft forever preferred_lft forever&#13;
    inet6 fe80::240f:64ff:fea8:371f/64 scope link &#13;
       valid_lft forever preferred_lft forever</pre>&#13;
<p class="indent">In addition to the loopback interface, we now see an additional interface with the IP address <span class="literal">10.85.0.254</span>. What happens if we try to <span class="literal">ping</span> this new IP address? It turns out we can indeed <span class="literal">ping</span> it, but only from within the network namespace:</p>&#13;
<pre>   root@host01:/opt# <span class="codestrong1">ip netns exec myns ping -c 1 10.85.0.254</span>&#13;
   PING 10.85.0.254 (10.85.0.254) 56(84) bytes of data.&#13;
   64 bytes from 10.85.0.254: icmp_seq=1 ttl=64 time=0.030 ms&#13;
&#13;
   --- 10.85.0.254 ping statistics ---&#13;
<span class="ent">➊</span> 1 packets transmitted, 1 received, 0% packet loss, time 0ms&#13;
   <span epub:type="pagebreak" id="page_60"/>rtt min/avg/max/mdev = 0.030/0.030/0.030/0.000 ms&#13;
   root@host01:/opt# <span class="codestrong1">ping -c 1 10.85.0.254</span>&#13;
   PING 10.85.0.254 (10.85.0.254) 56(84) bytes of data.&#13;
   From 10.85.0.1 icmp_seq=1 Destination Host Unreachable&#13;
&#13;
   --- 10.85.0.254 ping statistics ---&#13;
<span class="ent">➋</span> 1 packets transmitted, 0 received, +1 errors, 100% packet loss, time 0ms</pre>&#13;
<p class="indent">The first <span class="literal">ping</span> command, run using <span class="literal">ip netns exec</span> so that it runs within the network namespace, shows a successful response <span class="ent">➊</span>. However, the second <span class="literal">ping</span> command, run without <span class="literal">ip netns exec</span>, shows that no packets were received <span class="ent">➋</span>. The problem is that we have successfully created a network interface inside our network namespace, and we have the other end of the veth pair on our host network, but we haven’t connected up a corresponding network device on the host, so there’s no host network interface that can talk to the interface in the network namespace.</p>&#13;
<p class="indent">At the same time, when we ran a <span class="literal">ping</span> test from our BusyBox container in <a href="ch04.xhtml#ch04list2">Listing 4-2</a>, we were able to <span class="literal">ping</span> the host with no trouble. Clearly, there must be more configuration that CRI-O did for us when it created our containers. Let’s explore that in the next section.</p>&#13;
<h3 class="h3" id="ch00lev1sec19">Bridge Interfaces</h3>&#13;
<p class="noindent">The host side of the veth pair currently isn’t connected to anything, so it isn’t surprising that our network namespace can’t talk to the outside world yet. To fix that, let’s look at one of the veth pairs that CRI-O created:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip addr</span>&#13;
...&#13;
7: veth062abfa6@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; ... <span class="codeitalic1">master cni0</span> ...&#13;
    link/ether fe:6b:21:9b:d0:d2 brd ff:ff:ff:ff:ff:ff link-netns ...&#13;
    inet6 fe80::fc6b:21ff:fe9b:d0d2/64 scope link &#13;
       valid_lft forever preferred_lft forever&#13;
...</pre>&#13;
<p class="indent">Unlike the interface we created, this interface specifies <span class="literal">master cni0</span>, which shows that it belongs to a <em>network bridge</em>. A network bridge exists to connect multiple interfaces together. You can think of it as an Ethernet switch because it routes traffic from one network interface to another based on the media access control (MAC) address of the interfaces.</p>&#13;
<p class="indent">We can see the bridge <span class="literal">cni0</span> in the list of network devices on the host:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip addr</span>&#13;
...&#13;
4: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue ...&#13;
    link/ether 8e:0c:1c:7d:94:75 brd ff:ff:ff:ff:ff:ff&#13;
    inet 10.85.0.1/16 brd 10.85.255.255 scope global cni0&#13;
       valid_lft forever preferred_lft forever&#13;
    inet6 fe80::8c0c:1cff:fe7d:9475/64 scope link &#13;
<span epub:type="pagebreak" id="page_61"/>       valid_lft forever preferred_lft forever&#13;
...</pre>&#13;
<p class="indent">The bridge is a little smarter than a typical Ethernet switch in that it provides some firewall and routing capabilities. It also has an IP address of <span class="literal">10.85.0.1</span>. This IP address is the same as we saw with the default route for our BusyBox container in <a href="ch04.xhtml#ch04list3">Listing 4-3</a>, so we’ve started to solve the mystery of how our BusyBox container is able to talk to hosts outside of its own network.</p>&#13;
<h4 class="h4" id="ch00lev2sec30">Adding Interfaces to a Bridge</h4>&#13;
<p class="noindent">To inspect the bridge and add devices to it, we’ll use the <span class="literal">brctl</span> command. Let’s inspect the bridge first:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">brctl show</span>&#13;
bridge name     bridge id               STP enabled     interfaces&#13;
cni0            8000.8e0c1c7d9475       no              veth062abfa6&#13;
                                                        veth43ab68cd&#13;
                                                        vetha251c619</pre>&#13;
<p class="indent">The bridge <span class="literal">cni0</span> has three interfaces on it, corresponding to the host side of the veth pair for each of the three containers we have running (two NGINX and one BusyBox). Let’s take advantage of this existing bridge to set up network connectivity to the namespace we created:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">brctl addif cni0 myveth-host</span>&#13;
root@host01:/opt# <span class="codestrong1">brctl show</span>&#13;
bridge name     bridge id               STP enabled     interfaces&#13;
cni0            8000.8e0c1c7d9475       no              myveth-host&#13;
                                                        veth062abfa6&#13;
                                                        veth43ab68cd&#13;
                                                        vetha251c619</pre>&#13;
<p class="indent">The host side of our veth pair is now connected to the bridge, which means that we can now <span class="literal">ping</span> into the namespace from the host:</p>&#13;
<pre>   root@host01:/opt# <span class="codestrong1">ping -c 1 10.85.0.254</span>&#13;
   PING 10.85.0.254 (10.85.0.254) 56(84) bytes of data.&#13;
   64 bytes from 10.85.0.254: icmp_seq=1 ttl=64 time=0.194 ms&#13;
&#13;
   --- 10.85.0.254 ping statistics ---&#13;
<span class="ent">➊</span> 1 packets transmitted, 1 received, 0% packet loss, time 0ms&#13;
   rtt min/avg/max/mdev = 0.194/0.194/0.194/0.000 ms</pre>&#13;
<p class="indent">The fact that a packet was received <span class="ent">➊</span> shows that we set up a working connection. We should be pleased that it worked, but if we want to really understand this, we can’t be satisfied with saying that we can <span class="literal">ping</span> this interface “from the host.” We need to be more specific as to exactly how traffic is flowing.</p>&#13;
<h4 class="h4" id="ch00lev2sec31"><span epub:type="pagebreak" id="page_62"/>Tracing Traffic</h4>&#13;
<p class="noindent">Let’s actually trace this traffic to see what’s happening when we run the <span class="literal">ping</span> command. We will use <span class="literal">tcpdump</span> to print out the traffic. First, let’s start a <span class="literal">ping</span> command in the background to create some traffic to trace:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ping 10.85.0.254 &gt;/dev/null 2&gt;&amp;1 &amp;</span>&#13;
...</pre>&#13;
<p class="indent">We send the output to <em>/dev/null</em> so that it doesn’t clutter up our session. Now, let’s use <span class="literal">tcpdump</span> to see the traffic:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">timeout 1s tcpdump -i any -n icmp</span>&#13;
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode&#13;
listening on any, link-type LINUX_SLL (Linux cooked v1), ...&#13;
17:37:33.204863 IP 10.85.0.1 &gt; 10.85.0.254: ICMP echo request, ...&#13;
17:37:33.204894 IP 10.85.0.1 &gt; 10.85.0.254: ICMP echo request, ...&#13;
17:37:33.204936 IP 10.85.0.254 &gt; 10.85.0.1: ICMP echo reply, ...&#13;
17:37:33.204936 IP 10.85.0.254 &gt; 10.85.0.1: ICMP echo reply, ...&#13;
&#13;
4 packets captured&#13;
4 packets received by filter&#13;
0 packets dropped by kernel&#13;
root@host01:/opt# <span class="codestrong1">killall ping</span></pre>&#13;
<p class="indent">We use <span class="literal">timeout</span> to prevent <span class="literal">tcpdump</span> from running indefinitely, and we also use <span class="literal">killall</span> afterward to stop the <span class="literal">ping</span> command and discontinue it running in the background.</p>&#13;
<p class="indent">The output shows that the <span class="literal">ping</span> is originating from the bridge interface, which has IP address <span class="literal">10.85.0.1</span>. This is because of the host’s routing table:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip route</span>&#13;
...&#13;
10.85.0.0/16 dev cni0 proto kernel scope link src 10.85.0.1 &#13;
192.168.61.0/24 dev enp0s8 proto kernel scope link src 192.168.61.11</pre>&#13;
<p class="indent">When CRI-O created the bridge and configured its IP address, it also set up a route so that all traffic destined for the <span class="literal">10.85.0.0/16</span> network (that is, all traffic from <span class="literal">10.85.0.0</span> through <span class="literal">10.85.255.255</span>) would use <span class="literal">cni0</span>. This is enough information for the <span class="literal">ping</span> command to know where to send its packet, and the bridge handles the rest.</p>&#13;
<p class="indent">The fact that the <span class="literal">ping</span> is coming from the bridge interface of <span class="literal">10.85.0.1</span> rather than the host interface of <span class="literal">192.168.61.11</span> actually makes a big difference, as we can see if we try to run the <span class="literal">ping</span> the other way around. Let’s try to <span class="literal">ping</span> from within the namespace to the host network:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip netns exec myns ping -c 1 192.168.61.11</span>&#13;
ping: connect: Network is unreachable</pre>&#13;
<p class="indent">The issue here is that the interface in our network namespace doesn’t know how to reach the host network. The bridge is available and willing to <span epub:type="pagebreak" id="page_63"/>route traffic onto the host network, but we haven’t configured the necessary route to use it. Let’s do that now:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip netns exec myns ip route add default via 10.85.0.1</span></pre>&#13;
<p class="indent">And now the <span class="literal">ping</span> works:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip netns exec myns ping -c 1 192.168.61.11</span>&#13;
PING 192.168.61.11 (192.168.61.11) 56(84) bytes of data.&#13;
64 bytes from 192.168.61.11: icmp_seq=1 ttl=64 time=0.097 ms&#13;
&#13;
--- 192.168.61.11 ping statistics ---&#13;
1 packets transmitted, 1 received, 0% packet loss, time 0ms&#13;
rtt min/avg/max/mdev = 0.097/0.097/0.097/0.000 ms</pre>&#13;
<p class="indent">This illustrates an important rule to remember when debugging network problems: it’s very easy to jump to conclusions about what network traffic is really being sent and received. There is often no substitute for using tracing to see what the traffic really looks like.</p>&#13;
<div class="box5">&#13;
<p class="boxtitle-d"><strong>IP ADDRESSES ON THE HOST</strong></p>&#13;
<p class="noindents">This approach is not the only one that results in connectivity from the host into the network namespace. We also could have assigned an IP address directly to the host side of the veth pair. However, even though that would have enabled communication from the host into our network namespace, it wouldn’t provide a way for multiple network namespaces to communicate with one another. Using a bridge interface, as CRI-O does, enables the interconnection of all of the containers on a host, making them all appear to be on the same network.</p>&#13;
<p class="noindents">This also explains why we didn’t assign an IP address to the host side of the veth pair. When working with bridges, only the bridge interface gets an IP address. Interfaces added to the bridge do not.</p>&#13;
</div>&#13;
<p class="indent">With that last change, it would seem like we’ve matched the network configuration of our containers, but we are still missing the ability to communicate with the broader network outside of <span class="literal">host01</span>. We can demonstrate this by trying to <span class="literal">ping</span> from our network namespace to <span class="literal">host02</span>, which is on the same internal network as <span class="literal">host01</span> and has the IP address <span class="literal">192.168.61.12</span>. If we try a <span class="literal">ping</span> from our BusyBox container, it works:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">crictl exec $B1C_ID ping -c 1 192.168.61.12</span>&#13;
PING 192.168.61.12 (192.168.61.12): 56 data bytes&#13;
64 bytes from 192.168.61.12: seq=0 ttl=63 time=0.816 ms&#13;
&#13;
--- 192.168.61.12 ping statistics ---&#13;
1 packets transmitted, 1 packets received, 0% packet loss&#13;
round-trip min/avg/max = 0.816/0.816/0.816 ms</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_64"/>The <span class="literal">ping</span> output reports that a packet was received. However, if we try the same command using the network namespace we created, it doesn’t work:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip netns exec myns ping -c 1 192.168.61.12</span>&#13;
PING 192.168.61.12 (192.168.61.12) 56(84) bytes of data.&#13;
&#13;
--- 192.168.61.12 ping statistics ---&#13;
1 packets transmitted, 0 received, 100% packet loss, time 0ms</pre>&#13;
<p class="indent">This command reports that no packets were received.</p>&#13;
<p class="indent">Really, we ought to be surprised that the <span class="literal">ping</span> from our BusyBox container worked. After all, <span class="literal">host02</span> doesn’t know anything about the BusyBox container, or the <span class="literal">cni0</span> bridge interface, or the <span class="literal">10.85.0.0/16</span> network that the containers are in. How is it possible for <span class="literal">host02</span> to exchange a ping with our BusyBox container? To understand that, we need to look at network masquerade.</p>&#13;
<h3 class="h3" id="ch00lev1sec20">Masquerade</h3>&#13;
<p class="noindent"><em>Masquerade</em>, also known as Network Address Translation (NAT), is used every day in networking. For example, most home connections to the internet are provided with only a single IP address that is addressable from the internet, but many devices within the home network need an internet connection. It is the job of a router to make it appear that all traffic from that network is originating from a single IP address. It does this by rewriting the <em>source</em> IP address of outgoing traffic while tracking all outgoing connections so that it can rewrite the <em>destination</em> IP address of any replies.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The kind of NAT that we are talking about here is technically known as Source NAT (SNAT). Don’t get hung up on the name, though; for it to work correctly, any reply packets must have their destination rewritten. The term Source in this case just means that the source address is what’s rewritten when a new connection is initiated.</em></p>&#13;
</div>&#13;
<p class="indent">Masquerading sounds like just what we need to connect our containers running in the <span class="literal">10.85.0.0/16</span> network to the host network, <span class="literal">192.168.61.0/24</span>, and in fact it is exactly how it worked. When we sent a ping from our BusyBox container, the source IP address was rewritten such that the ping appeared to come from the <span class="literal">host01</span> IP <span class="literal">192.168.61.11</span>. When <span class="literal">host02</span> responded, it sent its reply to <span class="literal">192.168.61.11</span>, but the destination was rewritten so that it was actually sent to the BusyBox container.</p>&#13;
<p class="indent">Let’s trace the <span class="literal">ping</span> traffic all the way through to demonstrate:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">crictl exec $B1C_ID ping 192.168.61.12 &gt;/dev/null 2&gt;&amp;1 &amp;</span>&#13;
[1] 6335&#13;
root@host01:/opt# <span class="codestrong1">timeout 1s tcpdump -i any -n icmp</span>&#13;
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode&#13;
listening on any, link-type LINUX_SLL (Linux cooked v1)...&#13;
<span epub:type="pagebreak" id="page_65"/>18:53:44.310789 IP 10.85.0.4 <span class="ent">➊</span> &gt; 192.168.61.12: ICMP echo request, id 12, seq 17...&#13;
18:53:44.310789 IP 10.85.0.4 &gt; 192.168.61.12: ICMP echo request, id 12, seq 17...&#13;
18:53:44.310876 IP 192.168.61.11 <span class="ent">➋</span> &gt; 192.168.61.12: ICMP echo request, id 12, seq 17...&#13;
18:53:44.311619 IP 192.168.61.12 &gt; 192.168.61.11: ICMP echo reply, <span class="ent">➌</span> id 12, seq 17...&#13;
18:53:44.311648 IP 192.168.61.12 &gt; 10.85.0.4: <span class="ent">➍</span> ICMP echo reply, id 12, seq 17...&#13;
18:53:44.311656 IP 192.168.61.12 &gt; 10.85.0.4: ICMP echo reply, id 12, seq 17...&#13;
&#13;
6 packets captured&#13;
6 packets received by filter&#13;
0 packets dropped by kernel&#13;
root@host01:/opt# <span class="codestrong1">killall ping</span></pre>&#13;
<p class="indent">When the <span class="literal">ping</span> originates from within our BusyBox container, it has a source IP address of <span class="literal">10.85.0.4</span> <span class="ent">➊</span>. This address is rewritten, making the <span class="literal">ping</span> appear to be coming from the host IP <span class="literal">192.168.61.11</span> <span class="ent">➋</span>. Of course, <span class="literal">host02</span> knows how to respond to a <span class="literal">ping</span> coming from that address, so the <span class="literal">ping</span> is answered <span class="ent">➌</span>. At this point, the other half of the masquerade takes effect, and the destination is rewritten to <span class="literal">10.85.0.4</span> <span class="ent">➍</span>. The result is that the BusyBox container is able to send a packet to a separate host and get a reply.</p>&#13;
<p class="indent">To complete the setup for our network namespace, we need a similar rule to masquerade traffic coming from <span class="literal">10.85.0.254</span>. We can start by using <span class="literal">iptables</span> to look at the rules that CRI-O created when it configured the containers:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">iptables -t nat -n -L</span>&#13;
...&#13;
Chain POSTROUTING (policy ACCEPT)&#13;
target                        prot opt source    destination ...&#13;
CNI-f82910b3a7e28baf6aedc0d3  all  --  10.85.0.2 anywhere    ...&#13;
CNI-7f8aa3d8a4f621b186149f43  all  --  10.85.0.3 anywhere    ...&#13;
CNI-48ad69d30fe932fda9ea71d2  all  --  10.85.0.4 anywhere    ...&#13;
&#13;
Chain CNI-48ad69d30fe932fda9ea71d2 (1 references)&#13;
target     prot opt source               destination         &#13;
ACCEPT     all  --  anywhere             10.85.0.0/16 ...&#13;
MASQUERADE all  --  anywhere             !224.0.0.0/4 ...&#13;
...</pre>&#13;
<p class="indent">Masquerading starts when the connection is initiated; in this case, when traffic has a source address in the <span class="literal">10.85.0.0/16</span> network. For this reason, the <span class="literal">POSTROUTING</span> chain is used, because it sees all outgoing traffic. There is a rule in the <span class="literal">POSTROUTING</span> chain for each container; each rule invokes a <span class="literal">CNI</span> chain for that container.</p>&#13;
<p class="indent">For brevity, only one of the three <span class="literal">CNI</span> chains is shown. The other two are identical. The <span class="literal">CNI</span> chain first does an <span class="literal">ACCEPT</span> for all traffic that is local to the container network, so this traffic won’t be masqueraded. It then sets up masquerade for all traffic (except <span class="literal">224.0.0.0/4</span>, which is multicast traffic that cannot be masqueraded because there is no way to properly route replies).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_66"/>What’s missing from this configuration is a matching setup for traffic from <span class="literal">10.85.0.254</span>, the IP address we assigned to the interface in our network namespace. Let’s add that. First, create a new chain in the <span class="literal">nat</span> table:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">iptables -t nat -N chain-myns</span></pre>&#13;
<p class="indent">Next, add a rule to accept all traffic for the local network:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">iptables -t nat -A chain-myns -d 10.85.0.0/16 -j ACCEPT</span></pre>&#13;
<p class="indent">Now all remaining traffic (except multicast) should be masqueraded:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">iptables -t nat -A chain-myns \</span>&#13;
                  <span class="codestrong1">! -d 224.0.0.0/4 -j MASQUERADE</span></pre>&#13;
<p class="indent">And finally, tell <span class="literal">iptables</span> to use this chain for any traffic coming from <span class="literal">10.85.0.254</span>:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">iptables -t nat -A POSTROUTING -s 10.85.0.254 -j chain-myns</span></pre>&#13;
<p class="indent">We can verify that we did all that correctly by listing the rules again:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">iptables -t nat -n -L</span>&#13;
...&#13;
Chain POSTROUTING (policy ACCEPT)&#13;
target      prot opt source               destination&#13;
chain-myns  all  --  10.85.0.254          anywhere            &#13;
...&#13;
Chain chain-myns (1 references)&#13;
target      prot opt source               destination         &#13;
ACCEPT      all  --  anywhere             10.85.0.0/16        &#13;
MASQUERADE  all  --  anywhere             !224.0.0.0/4</pre>&#13;
<p class="indent">It looks like we have the configuration we need, as this configuration matches the way the virtual network devices were configured for the BusyBox container. To make sure, let’s try a <span class="literal">ping</span> to <span class="literal">host02</span> again:</p>&#13;
<pre>root@host01:/opt# <span class="codestrong1">ip netns exec myns ping -c 1 192.168.61.12</span>&#13;
PING 192.168.61.12 (192.168.61.12) 56(84) bytes of data.&#13;
64 bytes from 192.168.61.12: icmp_seq=1 ttl=63 time=0.843 ms&#13;
&#13;
--- 192.168.61.12 ping statistics ---&#13;
1 packets transmitted, 1 received, 0% packet loss, time 0ms&#13;
rtt min/avg/max/mdev = 0.843/0.843/0.843/0.000 ms</pre>&#13;
<p class="indent">Success! We’ve fully replicated the network isolation and connectivity that CRI-O is providing our containers.</p>&#13;
<h3 class="h3" id="ch00lev1sec21"><span epub:type="pagebreak" id="page_67"/>Final Thoughts</h3>&#13;
<p class="noindent">Container networking looks deceptively simple when running containers. Each container is provided with its own set of network devices, avoiding the need to worry about port conflicts and reducing the effect that one container can have on another. However, as we’ve seen in this chapter, this “simple” network isolation requires some complex configuration to enable not just isolation, but also connectivity to other containers and other networks. In <a href="part02.xhtml#part02">Part II</a>, after we properly introduce Kubernetes, we’ll return to container networking and show how the complexity only increases when we need to connect containers running on different hosts and load balance traffic across multiple container instances.</p>&#13;
<p class="indent">For now, we have one more key topic to address with containers before we can move on to Kubernetes. We need to understand how container storage works, including the container image that is used as the base filesystem when a new container is started as well as the temporary storage that a running container uses. In the next chapter, we’ll investigate how container storage makes application deployment easier and how a layered filesystem is used to save on storage and improve efficiency.<span epub:type="pagebreak" id="page_68"/></p>&#13;
</body></html>