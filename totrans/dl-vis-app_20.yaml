- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Convolutional Neural Networks
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: This chapter is all about a deep learning technique called *convolution*. Among
    its uses, convolution has become the standard method for classifying, manipulating,
    and generating images. Convolution is easy to use in deep learning because it
    can be easily encapsulated in a *convolution layer* (also called a *convolutional
    layer*). In this chapter, we look at the key ideas behind convolution and the
    related techniques we use to make convolution work in practice. We will see how
    to arrange a series of these operations to create a hierarchy of operations, which
    turns a series of simple operations into a powerful tool.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要介绍一种深度学习技术，叫做*卷积*。卷积的应用之一是它已成为分类、处理和生成图像的标准方法。卷积在深度学习中易于使用，因为它可以很方便地封装在一个*卷积层*（也叫*卷积神经层*）中。在本章中，我们将探讨卷积背后的关键思想以及我们在实际操作中使用的相关技术。我们将看到如何排列一系列这些操作，创建一个操作层次结构，将一系列简单的操作转化为强大的工具。
- en: In order to stay specific, in this chapter we focus our discussion of convolution
    on working with images. Models that use convolution have been spectacularly successful
    in this domain. For example, they excel at basic classification tasks like determining
    if an image is a leopard or a cheetah, or a planet or a marble. We can recognize
    the people in a photograph (Sun, Wang, and Tang 2014); detect and classify different
    types of skin cancers (Esteva et al. 2017); repair image damage like dust, scratches,
    and blur (Mao, Shen, and Yang 2016); and classify people’s age and gender from
    their photos (Levi and Hassner 2015). Convolution-based networks are also useful
    in many other applications, such as natural language processing (Britz 2015),
    where we can work out the structure of sentences (Kalchbrenner, Grefenstette,
    and Blunsom 2014) or classify sentences into different categories (Kim 2014).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持具体性，本章我们将卷积的讨论集中在图像处理上。使用卷积的模型在这个领域取得了惊人的成功。例如，它们在基本分类任务中表现优异，如判断一张图片是豹子还是猎豹，或者是行星还是大理石。我们可以识别照片中的人物（Sun,
    Wang, and Tang 2014）；检测并分类不同类型的皮肤癌（Esteva et al. 2017）；修复图像损坏，如灰尘、划痕和模糊（Mao, Shen,
    and Yang 2016）；并从照片中分类人的年龄和性别（Levi and Hassner 2015）。基于卷积的网络在许多其他应用中也很有用，例如自然语言处理（Britz
    2015），我们可以分析句子的结构（Kalchbrenner, Grefenstette, and Blunsom 2014）或将句子分类到不同类别中（Kim
    2014）。
- en: Introducing Convolution
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入卷积
- en: In deep learning, images are 3D tensors, with a height, width, and number of
    *channels*, or values per pixel. A grayscale image has only one value per pixel,
    and thus only one channel. A color image stored as RGB has three channels (with
    values for red, green, and blue). Sometimes people use the terms *depth* or *fiber
    size* to refer to the number of channels in a tensor. Unfortunately, *depth* is
    also used to refer to the number of layers in a deep network, and *fiber size*
    has not caught on widely. To avoid confusion, we always refer to the three dimensions
    of an image (and related 3D tensors) as height, width, and channels. Using our
    deep learning terminology, each image we provide to the network for processing
    is a sample. Each pixel in an image is a feature.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，图像是三维张量，具有高度、宽度和*通道数*，即每个像素的值。灰度图像每个像素只有一个值，因此只有一个通道。以RGB存储的彩色图像有三个通道（分别对应红色、绿色和蓝色的值）。有时人们使用*深度*或*光纤大小*来指代张量中的通道数。不幸的是，*深度*也用来指代深度网络中的层数，而*光纤大小*并没有广泛应用。为了避免混淆，我们总是将图像（三维张量的相关内容）的三个维度称为高度、宽度和通道数。按照我们的深度学习术语，每个提供给网络处理的图像都是一个样本。图像中的每个像素都是一个特征。
- en: When a tensor moves through a series of convolution layers, it often changes
    in width, height, and number of channels. If a tensor happens to have 1 or 3 channels,
    we can think of it as an image. But if a tensor has, say, 14 or 512 channels,
    it’s probably best not to think of it as an image any more. This suggests that
    we shouldn’t refer to individual elements of the tensor as *pixels*, which is
    an image-centric term. Instead, we call them *elements*. [Figure 16-1](#figure16-1)
    shows these terms visually.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个张量通过一系列卷积层时，它通常会在宽度、高度和通道数上发生变化。如果一个张量恰好有1个或3个通道，我们可以将其视为一张图像。但如果一个张量有14个或512个通道，最好就不再将其看作图像了。这意味着我们不应该将张量的单个元素称为*像素*，因为这是一个以图像为中心的术语。相反，我们称它们为*元素*。[图16-1](#figure16-1)直观地展示了这些术语。
- en: '![F16001](Images/F16001.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![F16001](Images/F16001.png)'
- en: 'Figure 16-1: Left: When our tensor has one or three channels, we can say that
    it’s made up of pixels. Right: For tensors with any number of channels, we call
    each slice through the channels an element.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-1：左侧：当我们的张量具有一个或三个通道时，我们可以说它是由像素构成的。右侧：对于具有任意数量通道的张量，我们称每个通道的切片为一个元素。
- en: A network in which the convolution layers play a central role is usually called
    a *convolutional neural network*, *convnet*, or *CNN*. Sometimes people also say
    *CNN network* (an example of “redundant acronym syndrome syndrome” [Memmott 2015]).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积层起核心作用的网络通常被称为*卷积神经网络*，*convnet*，或*CNN*。有时人们也会说*CNN网络*（这就是“冗余首字母缩略症综合症”[Memmott
    2015]的一个例子）。
- en: Detecting Yellow
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检测黄色
- en: 'To kick off our discussion of convolution, let’s consider processing a color
    image. Each pixel contains three numbers: one each for red, green, and blue. Suppose
    we want to create a grayscale output that has the same height and width as our
    color image, but where the amount of white in each pixel corresponds to the amount
    of yellow in its input pixel.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始讨论卷积，让我们考虑处理彩色图像。每个像素包含三个数字：分别表示红色、绿色和蓝色。假设我们想创建一个灰度输出，它的高度和宽度与我们的彩色图像相同，但每个像素的白色量与其输入像素中的黄色量对应。
- en: For simplicity, let’s assume our RGB values are numbers from 0 to 1\. Then a
    pixel that’s pure yellow has red and green values of 1, and a blue value of 0\.
    As the red and green values decrease, or the blue value increases, the pixel’s
    color shifts away from yellow.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，假设我们的RGB值是从0到1的数字。那么一个纯黄色的像素具有红色和绿色的值为1，蓝色的值为0。当红色和绿色的值减少，或者蓝色的值增加时，像素的颜色将偏离黄色。
- en: We want to combine each input pixel’s RGB values into a single number from 0
    to 1 that represents “yellowness,” which is the output pixel’s value. [Figure
    16-2](#figure16-2) shows one way to do this.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将每个输入像素的RGB值合并为一个从0到1的单一数字，表示“黄色度”，这是输出像素的值。[图16-2](#figure16-2)展示了实现这一目标的一种方式。
- en: '![F16002](Images/F16002.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![F16002](Images/F16002.png)'
- en: 'Figure 16-2: Representing our yellow detector as a simple neuron'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-2：将我们的黄色检测器表示为一个简单的神经元
- en: This sure looks familiar. It has the same structure as an artificial neuron.
    When we interpret [Figure 16-2](#figure16-2) as a neuron, +1, +1, and −1 are the
    three weights, and the numbers associated with the color values are the three
    inputs. [Figure 16-3](#figure16-3) shows how to apply this neuron to any pixel
    in an image.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很熟悉。它与人工神经元的结构相同。当我们将[图16-2](#figure16-2)解释为一个神经元时，+1、+1和−1是三个权重，与颜色值相关的数字是三个输入。[图16-3](#figure16-3)显示了如何将这个神经元应用于图像中的任何像素。
- en: '![F16003](Images/F16003.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![F16003](Images/F16003.png)'
- en: 'Figure 16-3: Applying our neuron in [Figure 16-2](#figure16-2) to a pixel in
    an image'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-3：将[图16-2](#figure16-2)中的神经元应用于图像中的一个像素
- en: We can apply this operation to every pixel in the input, creating a single output
    value for every pixel. The result is a new tensor with the same width and height
    as the input, but only one channel, as shown in [Figure 16-4](#figure16-4).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个操作应用于输入中的每个像素，为每个像素创建一个单独的输出值。结果是一个新的张量，宽度和高度与输入相同，但只有一个通道，如[图16-4](#figure16-4)所示。
- en: '![F16004](Images/F16004.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![F16004](Images/F16004.png)'
- en: 'Figure 16-4: Applying our neuron in [Figure 16-3](#figure16-3) to each pixel
    in the input produces an output tensor with the same width and height, but only
    one channel.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-4：将[图16-3](#figure16-3)中的神经元应用于输入中的每个像素，产生一个宽度和高度相同但只有一个通道的输出张量。
- en: We often imagine applying the neuron to the upper-left pixel, then moving it
    one step at a time to the right until we reach the end of the row, then repeating
    this for the next row, and the next, until we reach the bottom right pixel. We
    say that we’re *sweeping* the neuron over the input, or *scanning* the input.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常设想将神经元应用于左上角的像素，然后一次向右移动，直到到达行的末尾，然后对下一行重复这个过程，直到到达右下角的像素。我们说我们正在对输入进行*扫描*，或*扫描*输入。
- en: '[Figure 16-5](#figure16-5) shows the result of this process on a picture of
    a yellow frog. As we intended, the more yellow that’s present in each input pixel,
    the more white we see in its corresponding output. We say that the neuron is *identifying*
    or *detecting* yellow in the input.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-5](#figure16-5)显示了此过程在一只黄色青蛙图像上的结果。正如我们预期的那样，输入像素中黄色的含量越多，输出中对应的白色也越多。我们说神经元正在*识别*或*检测*输入中的黄色。'
- en: '![F16005](Images/F16005.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![F16005](Images/F16005.png)'
- en: 'Figure 16-5: An application of our yellow-finding operation. The image on the
    right runs from black to white, depending on the yellowness of the corresponding
    source pixel in the left image.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-5：我们的黄色检测操作的应用。右侧的图像根据左侧图像中对应源像素的黄色程度，从黑到白变化。
- en: Of course there’s nothing special about yellow. We can build a little neuron
    to detect any color. When we use a neuron in this way, we often say that it is
    *filtering* the input. In this context, the weights are sometimes collectively
    called the *filter values* or just the *filter*. Inheriting language from their
    mathematical roots, the weights are also called the *filter kernel* or just the
    *kernel*. It’s also common to refer to the entire neuron as a filter. Whether
    the word *filter* refers to a neuron, or specifically to its weights, is usually
    clear from context.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，黄色并没有什么特别之处。我们可以构建一个小神经元来检测任何颜色。当我们以这种方式使用神经元时，我们通常说它在对输入进行*过滤*。在这种情况下，权重有时被统称为*过滤值*，或者简称*过滤器*。继承自其数学根源的语言，权重也被称为*过滤核*，或简称*核*。通常也会把整个神经元称作过滤器。是否将*过滤器*一词指代神经元，或者特指其权重，通常可以从上下文中得知。
- en: This operation of sweeping the filter over the input corresponds to a mathematical
    operation called *convolution* (Oppenheim and Nawab 1996). We say that the right
    side of [Figure 16-5](#figure16-5) is the result of convolution of the color image
    with the yellow-detecting filter. We also say that we *convolve* the image with
    the filter. Sometimes we combine these terms and refer to a filter (whether an
    entire neuron, or just its weights) as a *convolution filter*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种将过滤器扫过输入图像的操作对应于一种数学操作，称为*卷积*（Oppenheim和Nawab 1996）。我们说[图16-5](#figure16-5)的右侧是对彩色图像和黄色检测过滤器进行卷积的结果。我们还可以说我们将图像与过滤器*卷积*。有时我们将这些术语合并，称一个过滤器（无论是整个神经元，还是仅其权重）为*卷积过滤器*。
- en: Weight Sharing
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重共享
- en: In the last section, we imagined sweeping our neuron over the input image, performing
    exactly the same operation at every pixel. If we want to go faster, we can create
    a huge grid of identical neurons and apply them to all the pixels simultaneously.
    In other words, we process the pixels in parallel.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们设想了将我们的神经元扫过输入图像，在每个像素上执行完全相同的操作。如果我们想加速这一过程，可以创建一个巨大的相同神经元网格，并同时应用于所有像素。换句话说，我们并行处理这些像素。
- en: In this approach, every neuron has identical weights. Rather than repeating
    the same weights in a separate piece of memory for every neuron, we can imagine
    that the weights are stored in some shared piece of memory, as in [Figure 16-6](#figure16-6).
    We say that the neurons are *weight sharing*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，每个神经元具有相同的权重。我们不需要在每个神经元的独立内存中重复相同的权重，而是可以想象这些权重存储在某个共享的内存中，如[图16-6](#figure16-6)所示。我们说神经元是*共享权重*的。
- en: '![F16006](Images/F16006.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![F16006](Images/F16006.png)'
- en: 'Figure 16-6: We can apply our neuron to every pixel in the input simultaneously.
    Each neuron uses the same weights, found in a piece of shared memory.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-6：我们可以将我们的神经元同时应用于输入中的每个像素。每个神经元使用相同的权重，这些权重存储在共享内存中。
- en: This lets us save on memory. In our yellow detector example, weight sharing
    also makes it easy to change the color we’re detecting. Rather than change the
    weights in thousands of neurons (or more), we just change the one set in the shared
    memory.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够节省内存。在我们的黄色检测器示例中，共享权重还使得更换我们要检测的颜色变得容易。我们无需改变成千上万个神经元（或更多）的权重，只需更改共享内存中的那一组权重。
- en: We can actually implement this scheme on a GPU, which is capable of performing
    many identical sequences of operations at once. Weight sharing lets us save on
    precious GPU memory, freeing it up for other uses.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以在GPU上实现这个方案，GPU能够同时执行许多相同的操作序列。共享权重使我们能够节省宝贵的GPU内存，从而将其释放用于其他用途。
- en: Larger Filters
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更大的过滤器
- en: So far, we’ve been sweeping our neuron over the image (or applying it in parallel
    using weight sharing), processing one pixel at a time, using only that pixel’s
    values for input. In many situations, it’s also useful to look at the pixels near
    the one we’re processing. Usually we consider a pixel’s eight immediate *neighbors*.
    That is, we use the values in a little three by three box that’s centered on the
    pixel.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在将神经元扫过图像（或使用共享权重并行应用它），一次处理一个像素，仅使用该像素的值作为输入。在许多情况下，查看我们正在处理的像素周围的像素也是有用的。通常我们会考虑一个像素的八个直接*邻居*。也就是说，我们使用一个以该像素为中心的三乘三的小框中的值。
- en: '[Figure 16-7](#figure16-7) shows three different operations we can apply using
    a three by three block of numbers in this way: blurring, detecting horizontal
    edges, and detecting vertical edges.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-7](#figure16-7)展示了我们可以通过这种方式使用三乘三数字块进行的三种不同操作：模糊处理、检测水平边缘和检测垂直边缘。'
- en: To compute each image, we center the block of weights over each pixel in turn
    and multiply each of the nine values under it by the corresponding weight. We
    add up the results and use their sum as the output value for that pixel. Let’s
    see how to implement this process with a neuron.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算每个图像，我们将权重块依次放置在每个像素上，并将其下方的九个值与相应的权重相乘。然后将结果加起来，并将它们的和作为该像素的输出值。让我们看看如何用神经元来实现这个过程。
- en: '![F16007](Images/F16007.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![F16007](Images/F16007.png)'
- en: 'Figure 16-7: Processing a grayscale image of the frog in [Figure 16-5](#figure16-5)
    by moving a three by three template of numbers over the image. From left to right,
    we blur the image, find horizontal edges, and find vertical edges.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-7：通过将三乘三数字模板在图像上移动来处理灰度图像中的青蛙（见[图16-5](#figure16-5)）。从左到右，我们对图像进行模糊处理，找到水平边缘，并找到垂直边缘。
- en: For simplicity, we’ll stick with a grayscale input for now. We can think of
    the blocks of numbers in [Figure 16-7](#figure16-7) as weights, or filter kernels.
    In this scenario, we have a grid of nine weights that we place over a grid of
    nine pixel values. Each pixel value is multiplied by its corresponding weight,
    the results are summed up and run through an activation function, and we have
    our output. [Figure 16-8](#figure16-8) shows the idea.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们暂时保持灰度输入。我们可以将[图16-7](#figure16-7)中的数字块视为权重或滤波器核。在这种情况下，我们有一个九个权重的网格，将其放置在九个像素值的网格上。每个像素值都与其相应的权重相乘，结果相加并通过激活函数，我们就得到了输出。[图16-8](#figure16-8)展示了这个概念。
- en: '![F16008](Images/F16008.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![F16008](Images/F16008.png)'
- en: 'Figure 16-8: Processing a grayscale input (red) with a three by three filter
    (blue)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-8：使用三乘三滤波器（蓝色）处理灰度输入（红色）
- en: This figure shows how to process a single pixel (shown in dark red). We center
    the filter over the intended pixel and multiply each of the nine values in the
    input with its corresponding filter value. We add up all nine results and pass
    that sum through an activation function.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图展示了如何处理单个像素（显示为深红色）。我们将滤波器集中在目标像素上，并将输入中的每个九个值与其相应的滤波器值相乘。我们将这九个结果相加，并将总和通过激活函数。
- en: The shape of the pixels that form a neuron’s input in this scheme is called
    that neuron’s *local receptive field*, or more simply its *footprint*. In [Figure
    16-8](#figure16-8), the neuron’s footprint is a square, three pixels on a side.
    In our yellow detector, the footprint was a single pixel. When a filter’s footprint
    is larger than a single pixel, we sometimes emphasize that quality by calling
    a *spatial filter*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，构成神经元输入的像素形状被称为该神经元的*局部感受野*，或者更简单地称为它的*足迹*。在[图16-8](#figure16-8)中，神经元的足迹是一个正方形，边长为三个像素。在我们的黄色探测器中，足迹是一个单独的像素。当滤波器的足迹大于单个像素时，我们有时会通过称之为*空间滤波器*来强调这一特性。
- en: Note that the neuron in [Figure 16-8](#figure16-8) is just like any other neuron.
    It receives nine numbers as inputs, multiplies each one by its corresponding weight,
    adds the results together, and passes that number through an activation function.
    It doesn’t know or care that these nine numbers are coming from a square region
    of the input, or even that they’re coming from an image.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[图16-8](#figure16-8)中的神经元就像其他任何神经元一样。它接收九个数字作为输入，将每个数字与相应的权重相乘，将结果加在一起，并通过激活函数传递这个数字。它并不在乎这九个数字来自输入的一个正方形区域，甚至不在乎它们来自于一张图像。
- en: We apply this three by three filter to an image by convolving it with the image,
    just as before, by sweeping it over each pixel in turn. For each input pixel,
    we imagine centering the three by three grid of weights over that pixel, applying
    the neuron, and creating a single output value, as in [Figure 16-9](#figure16-9).
    We say that the pixel we’re centering the filter over is the *anchor* (or the
    *reference point* or *zero point*).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过与图像进行卷积来应用这个三乘三滤波器，就像之前一样，依次将其扫描过每个像素。对于每个输入像素，我们想象将三乘三的权重网格放置在该像素上，应用神经元并创建一个单一的输出值，如[图16-9](#figure16-9)所示。我们称我们正在将滤波器集中在其上的像素为*锚点*（或*参考点*或*零点*）。
- en: '![F16009](Images/F16009.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![F16009](Images/F16009.png)'
- en: 'Figure 16-9: Applying a three by three filter (center) to a grayscale image
    (left), creating a new single-channel image (right)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-9：将三乘三滤波器（中心）应用于灰度图像（左），生成新的单通道图像（右）
- en: We can design our filters to have footprints of any size and shape we like.
    In practice, small sizes are most common, since they are faster to evaluate than
    larger footprints. We usually use small squares with an odd number of pixels on
    each side (often between one and nine). Such squares let us place the anchor in
    the center of the footprint. This keeps everything symmetrical and easier to understand.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以设计任何大小和形状的滤波器。实际上，小尺寸的滤波器最为常见，因为它们比大尺寸的滤波器更快速地进行计算。我们通常使用每边像素数为奇数的小方块（通常是1到9之间）。这样的方块使我们能够将锚点放在滤波器中心，这样可以保持对称，且更易于理解。
- en: Let’s put this into practice. [Figure 16-10](#figure16-10) shows the result
    of convolving a seven by seven input with a three by three filter. Note that if
    we were to center the filter over the input’s corners or edges, the filter’s footprint
    would extend beyond the input, and the neuron would require input values that
    aren’t present. We address this a little later. For now, let’s just limit ourselves
    to those locations where the filter sits entirely on top of the image. That means
    that the output image is only five by five.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把这个理论应用到实践中。[图16-10](#figure16-10)显示了将一个七乘七的输入图像与一个三乘三的滤波器进行卷积的结果。请注意，如果我们将滤波器置于输入图像的角落或边缘，滤波器的覆盖区域会超出输入图像，神经元将需要一些不存在的输入值。我们稍后会处理这个问题。现在，我们先限制讨论那些滤波器完全覆盖图像位置的情况。这样，输出图像的大小就是五乘五。
- en: We motivated our discussion by looking at spatial filters that can do things
    like blur an image or detect edges. But why are such things useful for deep learning?
    To answer this, let’s look at filters more closely.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过观察空间滤波器来引出讨论，这些滤波器可以实现像模糊图像或检测边缘这样的功能。那么，这些功能为什么对深度学习有用呢？为了回答这个问题，让我们更仔细地看一下滤波器。
- en: '![F16010](Images/F16010.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![F16010](Images/F16010.png)'
- en: 'Figure 16-10: To convolve an image with a filter, we move the filter across
    the image and apply it at each position. We’re skipping corners and edges for
    this figure.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-10：为了将滤波器与图像进行卷积，我们将滤波器在图像上移动，并在每个位置应用它。我们在这个图中略去了角落和边缘。
- en: Filters and Features
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 滤波器与特征
- en: Some biologists who study toads think that certain cells in the animal’s visual
    system are sensitive to specific types of visual patterns (Ewert et al. 1985).
    The theory is that a toad is looking for particular shapes corresponding to the
    creatures it likes to eat and to certain motions that those animals make. People
    used to think that a toad’s eyes absorbed all the light that struck them, sent
    that mass of information to the brain, and it was the brain’s job to sift among
    the results looking for food. The new hypothesis is that the cells in the eye
    are doing some early steps in this detection process (such as finding edges) all
    by themselves, and they only fire and pass on information to the brain if they
    “think” they’re looking at prey.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究蟾蜍的生物学家认为，动物视觉系统中的某些细胞对特定类型的视觉模式敏感（Ewert 等，1985年）。这个理论认为，蟾蜍在寻找与它喜欢吃的生物相关的特定形状，以及这些动物所做的某些动作。人们曾认为，蟾蜍的眼睛会吸收所有照射到它们的光，将这些信息传送到大脑，然后由大脑从结果中筛选寻找食物。而新的假设认为，眼睛中的细胞在这个检测过程中会自行完成一些早期步骤（如寻找边缘），只有当它们“认为”看到的是猎物时，才会发射信号并将信息传递给大脑。
- en: The theory has been extended to the human visual system, where it has led to
    the surprising hypothesis that some individual neurons are so precisely fine-tuned
    that they only fire in response to pictures of specific people. The original study
    that led to this suggestion showed people 87 different images, including people,
    animals, and landmarks. In one volunteer they found a specific neuron that only
    fired when the volunteer was shown a photo of the actress Jennifer Aniston (Quiroga
    2005). Even more curiously, that neuron only fired when Aniston was alone, and
    not when she was pictured together with other people, including famous actors.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该理论已扩展到人类视觉系统，进而提出了一个令人惊讶的假设：某些单一的神经元被精确地调节，只对特定人的图片作出反应。导致这一建议的最初研究展示了87张不同的图像，其中包括人类、动物和地标。在一位志愿者身上，他们发现了一个只在志愿者看到女演员詹妮弗·安妮斯顿的照片时才会激活的神经元（Quiroga
    2005年）。更有趣的是，这个神经元只会在安妮斯顿单独出现时激活，而在她与其他人（包括著名演员）合影时并不激活。
- en: The idea that our neurons are precision pattern-matching devices is not universally
    accepted, but we’re not doing real neuroscience and biology here. We’re just looking
    for inspiration. And this idea of letting neurons perform detection work seems
    like some pretty great inspiration.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的神经元是精确的模式匹配设备这一观点并未得到普遍接受，但我们这里并不是在进行真正的神经科学和生物学研究。我们只是寻找灵感。而让神经元执行检测工作的这一想法，似乎是一个相当棒的灵感。
- en: The connection to convolution is that we can use filters to simulate the cells
    in the toad’s eyes. Our filters also pick out specific patterns and then pass
    on their discoveries to later filters that look for even bigger patterns. Some
    of the terminology we use for this process echoes terms that we’ve seen before.
    Specifically, we’ve been using the word *feature* to refer to one of the values
    contained in a sample. But in this context, the word *feature* also refers to
    a particular structure in an input that the filter is trying to detect, like an
    edge, a feather, or scaly skin. We say that a filter is *looking for* a stripe
    feature, or eyeglasses, or a sports car. Continuing this usage, the filters themselves
    are sometimes called *feature detectors*. When a feature detector has been swept
    over an entire input, we say that its output is a *feature map* (the word *map*
    in this context comes from mathematical language). The feature map tells us, pixel
    by pixel, how well the image around that pixel matched what the filter was looking
    for.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与卷积的关联在于，我们可以使用滤波器来模拟蟾蜍眼睛中的细胞。我们的滤波器同样能够挑选出特定的模式，并将其发现传递给后续滤波器，后者寻找更大的模式。我们在这个过程中使用的一些术语，回响了我们之前见过的术语。具体来说，我们一直在使用*特征*这个词来指代样本中的一个值。但在这个上下文中，*特征*一词也指输入中的一个特定结构，滤波器试图检测到它，比如边缘、羽毛或鳞片皮肤。我们说一个滤波器在*寻找*条纹特征、眼镜或跑车。延续这一用法，滤波器本身有时被称为*特征检测器*。当特征检测器扫描完整个输入后，我们称其输出为*特征图*（此处的*图*一词来源于数学语言）。特征图告诉我们，逐像素地，图像中该像素周围的内容与滤波器所寻找的匹配程度。
- en: Let’s see how feature detection works. In [Figure 16-11](#figure16-11) we show
    the process of using a filter to find short, isolated vertical white stripes in
    a binary image.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看特征检测是如何工作的。在[图16-11](#figure16-11)中，我们展示了使用滤波器在二值图像中找到短的、孤立的垂直白色条纹的过程。
- en: '![F16011](Images/F16011.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![F16011](Images/F16011.png)'
- en: 'Figure 16-11: 2D pattern matching with convolution. (a) The filter. (b) The
    input. (c) The feature map, scaled to [0, 1] for display. (d) Feature map entries
    with value 3\. (e) Neighborhoods of (b) around the white pixels in (d).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-11：使用卷积进行二维模式匹配。（a）滤波器。（b）输入。（c）特征图，已缩放至[0, 1]以便显示。（d）特征图中值为3的条目。（e）（d）中白色像素周围的（b）邻域。
- en: '[Figure 16-11](#figure16-11)(a) shows a three by three filter with values −1
    (black) and 1 (white). [Figure 16-11](#figure16-11)(b) shows a noisy input image,
    consisting only of black and white pixels. [Figure 16-11](#figure16-11)(c) shows
    the result of applying the filter to each pixel in the input image (except for
    the outermost border). Here the values range from −6 to +3, which we scaled to
    [0, 1] for display. The larger the value in this image, the better the match between
    the filter and the pixel (and its neighborhood). A value of +3 means the filter
    matched the image perfectly at that pixel.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-11](#figure16-11)(a) 显示了一个三乘三的滤波器，值为−1（黑色）和1（白色）。[图16-11](#figure16-11)(b)
    显示了一个噪声输入图像，仅包含黑白像素。[图16-11](#figure16-11)(c) 显示了将滤波器应用于输入图像中每个像素的结果（外部边框除外）。这里的值从−6到+3，我们将其缩放至[0,
    1]以便显示。图像中值越大，滤波器与像素（及其邻域）之间的匹配度越好。值为+3表示该像素与滤波器完美匹配。'
- en: '[Figure 16-11](#figure16-11)(d) shows a thresholded version of [Figure 16-11](#figure16-11)(c),
    where pixels with a value of +3 are shown in white, and all others are black.
    Finally, [Figure 16-11](#figure16-11)(e) shows the noisy image of [Figure 16-11](#figure16-11)(b)
    with the three by three grid of pixels around the white pixels in [Figure 16-11](#figure16-11)(d)
    highlighted. We can see that the filter found those places in the image where
    the pixels matched the filter’s pattern.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-11](#figure16-11)(d) 显示了[图16-11](#figure16-11)(c)的阈值版本，其中值为+3的像素显示为白色，其他所有像素为黑色。最后，[图16-11](#figure16-11)(e)
    显示了[图16-11](#figure16-11)(b)的噪声图像，并突出显示了[图16-11](#figure16-11)(d)中白色像素周围的三乘三像素网格。我们可以看到，滤波器找到了图像中与滤波器模式匹配的那些位置。'
- en: Let’s see why this worked. In the top row of [Figure 16-12](#figure16-12) we
    show our filter and a three by three patch of the image, along with the pixel-by-pixel
    results.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看为什么这样有效。在[图16-12](#figure16-12)的顶部行中，我们展示了滤波器和图像的三乘三块区域，以及逐像素的结果。
- en: '![F16012](Images/F16012.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![F16012](Images/F16012.png)'
- en: 'Figure 16-12: Applying a filter to two image fragments. From left to right,
    each row shows the filter, an input, and the result. The final number is the sum
    of the rightmost three by three grid.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-12：将滤波器应用于两个图像片段。从左到右，每一行显示滤波器、输入和结果。最后一个数字是右侧三乘三网格的总和。
- en: Consider the pixels shown in the middle of the top row. The black pixels (shown
    in gray here), with a value of 0, don’t contribute to the output. The white pixels
    (shown in light yellow here), with a value of 1, get multiplied by either 1 or
    –1, depending on the filter value. In the top row of pixels, only one of the white
    pixels (the top center) is matched by a 1 in the filter. This gives a result of
    1 × 1 = 1\. The other three white pixels are matched up with −1, giving three
    results of −1 × 1 = −1\. Adding these gives us −3 + 1 = −2.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑顶部行中间显示的像素。黑色像素（这里显示为灰色），值为0，不对输出产生影响。白色像素（这里显示为浅黄色），值为1，根据滤波器的值乘以1或-1。在顶部行的像素中，只有一个白色像素（顶部中央）与滤波器中的1匹配。这给出了结果1
    × 1 = 1。其他三个白色像素与-1匹配，给出了三个结果-1 × 1 = -1。将这些加在一起，我们得到-3 + 1 = -2。
- en: In the lower row, our image matches the filter. All three weights of 1 on the
    filter are sitting on white pixels, and there are no other white pixels in the
    input. The result is a score of 3, indicating a perfect match.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在下方这一行中，我们的图像与滤波器匹配。滤波器上三个权重为1的部分正好位于白色像素上，输入图像中没有其他白色像素。结果是一个3的分数，表示完全匹配。
- en: '[Figure 16-13](#figure16-13) shows another filter, this time looking for diagonals.
    Let’s run it over the same image. This diagonal of three white pixels surrounded
    by black is present in two places.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-13](#figure16-13)展示了另一个滤波器，这次是寻找对角线。让我们在同一图像上运行它。这个由三个白色像素组成的对角线被黑色像素包围，它在两个地方出现。'
- en: '![F16013](Images/F16013.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![F16013](Images/F16013.png)'
- en: 'Figure 16-13: Another filter and its result on our random image. (a) The filter.
    (b) The input. (c) The feature map. (d) Feature map entries with value 3\. (e)
    Neighborhoods of (b) around the white pixels in (d).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-13：另一个滤波器及其在随机图像上的结果。(a) 滤波器。(b) 输入。(c) 特征图。(d) 特征图中值为3的条目。(e) (d)中白色像素周围的邻域。
- en: By sweeping a filter over the image and computing the output value at each pixel,
    we can hunt for lots of different simple patterns. In practice, our filter and
    pixel values are all real numbers (not just 0 and 1), so we can make much more
    complex patterns that find more complex features (Snavely 2013).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在图像上滑动滤波器并计算每个像素的输出值，我们可以寻找许多不同的简单模式。实际上，我们的滤波器和像素值都是实数（而不仅仅是0和1），因此我们可以制作出更复杂的模式，找到更复杂的特征（Snavely
    2013）。
- en: If we take the output of a set of filters and feed them to another set of filters,
    we can look for patterns of patterns. If we feed that second set of outputs to
    a third set of filters, we can look for patterns of patterns of patterns. This
    process lets us build up from, say, a collection of edges, to a set of shapes,
    such as ovals and rectangles, to ultimately matching a pattern corresponding to
    some specific object, such as a guitar or bicycle.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将一组滤波器的输出传递给另一组滤波器，我们可以寻找模式中的模式。如果我们将第二组输出传递给第三组滤波器，我们可以寻找模式中的模式中的模式。这个过程让我们能够从一组边缘开始，构建一组形状，如椭圆和矩形，最终匹配某个特定物体的模式，如吉他或自行车。
- en: Applying successive groups of filters in this way, in concert with another technique
    we will soon discuss called *pooling*, enormously expands the sorts of patterns
    that we can detect. The reason is that the filters operate *hierarchically*, where
    each filter’s patterns are combinations of the patterns found by earlier filters.
    Such a hierarchy allows us to look for features of great complexity, such as the
    face of a friend, the grain of a basketball, or the eye on the end of a peacock’s
    feather.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式应用连续的滤波器组，再结合我们很快将讨论的另一种技术——*池化*，极大地扩展了我们能够检测的模式种类。原因在于，滤波器以*层级*方式工作，其中每个滤波器的模式都是前一个滤波器找到的模式的组合。这样的层级结构让我们能够寻找复杂度较高的特征，如朋友的面孔、篮球的纹理，或是孔雀羽毛末端的眼睛。
- en: If we had to work out these filters by hand, classifying images would be impractical.
    What are the proper weights in a chain of eight filters that tell us if a picture
    shows a kitten or an airplane? How could we even go about working out that problem?
    And how would we know when we found the best filters? In Chapter 1 we discussed
    expert systems, in which people tried to do this kind of feature engineering by
    hand. It’s a formidable task for simple problems, and it grows in complexity so
    quickly that really interesting problems, such as distinguishing cats from airplanes,
    seem entirely out of reach.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们必须手动计算这些滤波器，图像分类将变得不切实际。在一连串八个滤波器中，如何确定合适的权重来告诉我们一张图片是小猫还是飞机？我们又该如何去解决这个问题呢？我们又怎么知道何时找到了最好的滤波器？在第一章中，我们讨论了专家系统，人们曾试图通过手动进行这种特征工程。对于简单问题来说，这是一个艰巨的任务，且复杂性增长非常迅速，真正有趣的问题，比如区分猫和飞机，似乎完全无法解决。
- en: The beauty of CNNs is that they carry out the goals of expert systems, but we
    don’t have to figure out the values of the filters by hand. The learning process
    that we’ve seen in previous chapters, involving measuring error, backpropagating
    the gradients, and then improving the weights, teaches a CNN to find the filters
    it needs. The learning process modifies the kernel of each filter (that is, the
    weights in each neuron), until the network is producing results that match our
    targets. In other words, training tunes the values in the filters until they find
    the features that enable it to come up with the right class for the object in
    the image. And this can happen for hundreds or even thousands of filters, all
    at once.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）的美妙之处在于，它们实现了专家系统的目标，但我们无需手动计算滤波器的值。我们在前几章中看到的学习过程，包括测量误差、反向传播梯度以及改进权重，教会了
    CNN 自行找到所需的滤波器。学习过程修改了每个滤波器的核心（即每个神经元中的权重），直到网络产生符合我们目标的结果。换句话说，训练过程调节滤波器中的值，直到它们找到能够帮助网络为图像中的物体分类的特征。这一过程可以在数百个甚至数千个滤波器中同时发生。
- en: This can seem like magic. Starting with random numbers, the system learns what
    patterns it needs to look for in order to distinguish a piano from an apricot
    from an elephant, and then it learns what numbers to put into the filter kernels
    in order to find those patterns.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来像是魔法。系统从随机数开始，学习需要寻找的模式，以区分钢琴、杏子和大象，然后学习如何将数字放入滤波器内核，以便找到这些模式。
- en: That this process can even come close in one situation is remarkable. The fact
    that it often produces highly accurate results in a vast range of applications
    is one of the great discoveries in deep learning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程在某些情况下能够接近完成，实属不易。它在广泛的应用中经常产生高度准确的结果，这是深度学习领域的伟大发现之一。
- en: Padding
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 填充
- en: Earlier, we promised to return to the issue of what happens when a convolution
    filter is centered over an element in a corner or on an edge of an input tensor.
    Let’s look at that now.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们承诺会回到卷积滤波器位于输入张量的角落或边缘时会发生什么问题。现在让我们来看一下。
- en: Suppose that we want to apply a 5 by 5 filter to a 10 by 10 input. If we’re
    somewhere in the middle of the tensor, as in [Figure 16-14](#figure16-14), then
    our job is easy. We pull out the 25 values from the input, and apply them to the
    convolution filter.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想对一个 10x10 的输入应用一个 5x5 的滤波器。如果我们位于张量的中间位置，如[图 16-14](#figure16-14)所示，那么我们的工作就很简单。我们从输入中提取出
    25 个值，并将它们应用到卷积滤波器上。
- en: '![F16014](Images/F16014.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![F16014](Images/F16014.png)'
- en: 'Figure 16-14: A five by five filter located somewhere in the middle of a tensor.
    The bright red pixel is the anchor, while the lighter ones make up the receptive
    field.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16-14：一个位于张量中部的 5x5 滤波器。鲜红色的像素是锚点，而较浅的像素组成了感受野。
- en: But what if we’re on, or near, an edge, as in [Figure 16-15](#figure16-15)?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果我们位于边缘上，或者接近边缘，如[图 16-15](#figure16-15)所示，会怎么样呢？
- en: '![F16015](Images/F16015.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![F16015](Images/F16015.png)'
- en: 'Figure 16-15: Near the edge, the filter’s receptive field can fall off the
    side of the input. What values do we use for these missing elements?'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16-15：在边缘附近，滤波器的感受野可能会超出输入的边界。我们该如何处理这些缺失的元素呢？
- en: The footprint of the filter is hanging off the edge of the input. There aren’t
    any input elements there. How do we compute an output value for the filter when
    it’s missing some of its inputs?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 滤波器的足迹悬挂在输入的边缘。那里没有输入元素。那我们如何计算滤波器的输出值，当它缺少一些输入时呢？
- en: We have a few choices. One is to disallow this case so we can only place the
    footprint where it is entirely within the input image. The result is an output
    that’s smaller in height and width. [Figure 16-16](#figure16-16) shows this idea.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: While simple, this is a lousy solution. We said that we often apply many filters
    in sequence. If we sacrificed one or more rings of elements each time, we would
    lose information with every step we take through the network.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![F16016](Images/F16016.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-16: We can avoid the “falling off the edge” problem by never letting
    our filter get that far. With a 5 by 5 filter, we can only center the filter over
    the elements marked here in blue, reducing our 10 by 10 input to a 6 by 6 output.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: A popular alternative is to use a technique called *padding,* which lets us
    create an output image of the same width and height as the input. The idea is
    that we add a border of extra elements around the outside of the input, as in
    [Figure 16-17](#figure16-17). All of these elements have the same value. If we
    place zeros in all the new elements, we call the technique *zero-padding*. In
    practice, we almost always use zeros, so people often refer to zero-padding as
    merely padding, with the understanding that if they mean to use any value other
    than zero, they say so explicitly.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![F16017](Images/F16017.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-17: A better way to solve the “falling off the edge” problem is to
    add padding, or extra elements (in light blue), around the border of the input.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: The thickness of the border depends on the size of the filter. We usually use
    just enough padding so that the filter can be centered on every element of the
    input. Every filter needs to have its input padded if we don’t want to lose information
    from the sides.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Most deep learning libraries automatically calculate the necessary amount of
    padding so that our output has the same width and height as our input, and apply
    it for us as a default.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Multidimensional Convolution
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far in this chapter, we’ve mostly been considering grayscale images with
    only one channel of color information. We know that most color images have three
    channels, representing the red, green, and blue components of each pixel. Let’s
    see how to handle those. Once we can work with images with three channels, we
    can work with tensors of any number of channels.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: To process an input with multiple channels, our filters (which can have any
    footprint) need to have an identical number of channels. That’s because each value
    in the input needs to have a corresponding value in the filter. For an RGB image,
    a filter needs three channels. So, a filter with a footprint of three by three
    needs to have three channels, for a total of 27 numbers, as shown in [Figure 16-18](#figure16-18).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![F16018](Images/F16018.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-18: A three-channel filter with a three by three footprint. We’ve
    colored the values to show which input channel’s values they will multiply.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: To apply this kernel to a three-channel color image, we proceed much as before,
    but now we think in terms of blocks (or tensors of three dimensions).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take the filter of [Figure 16-18](#figure16-18), with a three by three
    footprint and three channels, and use it to process an RGB image with three color
    channels. For each input pixel, we center the filter’s footprint over that pixel
    as before, and match up each of the 27 numbers in the image with the 27 numbers
    in the filter, as in [Figure 16-19](#figure16-19).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![F16019](Images/F16019.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-19: Convolving an RGB image with a three by three by three kernel.
    We can imagine that each channel is filtered by its own channel in the filter.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 16-19](#figure16-19), our input has three channels, so our filter
    has three channels as well. It may be helpful to think of the red, green, and
    blue channels as each getting filtered by their corresponding channel in the filter,
    as shown in [Figure 16-19](#figure16-19). In practice, we treat the input and
    the filter as three by three by three blocks, and each of the 27 input values
    get multiplied with its corresponding filter value.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'This idea generalizes to any number of channels. In order to make sure that
    every input value has a corresponding filter value, we can state the necessary
    property as a rule: every filter must have the same number of channels as the
    tensor it’s filtering.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Filters
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve been applying a single filter at a time, but that’s rare in practice.
    Usually we bundle up tens or hundreds of filters into one *convolution layer*
    and apply them all simultaneously (and independently) to that layer’s input.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: To see the general picture, imagine that we’ve been given a black-and-white
    image, and we want to look for several low-level features in the pixels, such
    as vertical stripes, horizontal stripes, isolated dots, and plus signs. We can
    create one filter for each of these features and run each one over the input independently.
    Each filter produces an output image with one channel. Combining the four outputs
    gives us one tensor with four channels. [Figure 16-20](#figure16-20) shows the
    idea.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![F16020](Images/F16020.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-20: We can run multiple filters (in color) over the same input (in
    gray). Each filter creates its own channel in the output. They are then combined
    to create a single element in the output tensor with four channels.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Instead of a grayscale image with one channel, or a color image with three channels,
    we now have an output tensor with four channels. If we used seven filters, then
    the output is a new image with seven channels. The key thing to note here is that
    the output tensor has one channel for each filter that’s applied.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, our filters can have any footprint, and we can apply as
    many of them as we like to any input image. [Figure 16-21](#figure16-21) shows
    this idea.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![F16021](Images/F16021.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-21: When we convolve filters with an input, each filter must have
    as many channels as the input. The output tensor has one channel for each filter.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-21：当我们将过滤器与输入进行卷积时，每个过滤器的通道数必须与输入的通道数相同。输出张量的通道数与过滤器的数量相同。
- en: The input tensor at the far left has seven channels. We’re applying four different
    filters, each with a three by three footprint, so each filter is a tensor of size
    three by three by seven. The output of each filter is a feature map of a single
    channel. The output tensor is what we get from stacking these four feature maps,
    so it has four channels.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最左边的输入张量有七个通道。我们应用四个不同的过滤器，每个过滤器的尺寸是3x3，因此每个过滤器的张量大小是3x3x7。每个过滤器的输出是一个单通道的特征图。输出张量是通过堆叠这四个特征图得到的，因此它有四个通道。
- en: Although in principle each filter we apply can have a different footprint, in
    practice we almost always use the same footprint for every filter in any given
    convolution layer. For example, in [Figure 16-21](#figure16-21) all the filters
    have a footprint of three by three.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然原则上我们应用的每个过滤器可以有不同的尺寸，但实际上，我们几乎总是为任何给定的卷积层使用相同的过滤器尺寸。例如，在[图16-21](#figure16-21)中，所有的过滤器的尺寸都是3x3。
- en: Let’s gather together the two numerical rules from the previous section and
    this one. First, every filter in a convolution layer must have the same number
    of channels as that layer’s input tensor. Second, a convolution layer’s output
    tensor will have as many channels as there are filters in the layer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把上一节和这一节中的两个数值规则汇总起来。首先，卷积层中的每个过滤器必须与该层的输入张量具有相同的通道数。其次，卷积层的输出张量将具有与该层中过滤器数量相等的通道数。
- en: Convolution Layers
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积层
- en: Let’s take a closer look at the mechanics of convolution layers. A convolution
    layer is simply a bunch of filters gathered together. They’re applied independently
    to the input tensor, as in [Figure 16-21](#figure16-21), and their outputs are
    combined to create a new output tensor. The input is not changed by this process.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看卷积层的机制。卷积层实际上是将多个过滤器组合在一起。它们独立地应用于输入张量，如[图16-21](#figure16-21)所示，然后它们的输出被组合起来，生成一个新的输出张量。输入在这个过程中没有发生变化。
- en: When we create a convolution layer in code, we typically tell our library how
    many filters we want, what their footprint should be, and other optional details
    like whether we want to use padding and what activation function we want to use—the
    library takes care of all the rest. Most importantly, training improves the kernel
    values in each filter, so that the filters learn the values that enable them to
    produce the best results.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在代码中创建一个卷积层时，我们通常会告诉库我们需要多少个过滤器、它们的尺寸应该是多少，以及其他可选的细节，比如是否使用填充以及我们希望使用什么激活函数——其余的由库来处理。最重要的是，训练过程中会改善每个过滤器的核值，使过滤器学习到能够产生最佳结果的值。
- en: When we draw a diagram of a deep learner, we usually label our convolution layers
    with how many filters are used, their footprints, and their activation function.
    Since it’s common to use the same padding all around the input, we often just
    provide a single value rather than two, with the understanding that it applies
    to both width and height.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们绘制深度学习模型的图示时，通常会标注卷积层使用了多少个过滤器、它们的尺寸以及它们的激活函数。由于在输入的周围常常使用相同的填充，我们通常只提供一个值，而不是两个，默认它适用于宽度和高度。
- en: Like the weights in fully connected layers, the values in a convolution layer’s
    filters start out with random values and are improved with training. Also like
    fully connected layers, if we’re careful about choosing these random initial values,
    training usually goes faster. Most libraries offer a variety of initialization
    methods. Generally speaking, the built-in defaults normally work fine, and we
    rarely need to explicitly choose an initialization algorithm.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 就像全连接层中的权重一样，卷积层中过滤器的值最初是随机的，并通过训练得到改善。同样，像全连接层一样，如果我们在选择这些随机初始值时小心一些，训练通常会更快。大多数库提供多种初始化方法。一般来说，内置的默认值通常能很好地工作，我们很少需要明确选择初始化算法。
- en: If we do want to pick a method, the He algorithm is a good first choice (He
    et al. 2015; Karpathy 2016). If that’s not available, or doesn’t work well in
    a given situation, Glorot is a good second choice (Glorot and Bengio 2010).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们确实想选择一种方法，He算法是一个不错的首选（He et al. 2015; Karpathy 2016）。如果该方法不可用，或者在特定情况下效果不好，Glorot算法是一个不错的第二选择（Glorot
    和 Bengio 2010）。
- en: Let’s look at a couple of special types of convolution that have their own names.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看几种有自己名字的特殊卷积类型。
- en: 1D Convolution
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一维卷积
- en: An interesting special case of sweeping a filter over an input is called *1D
    convolution*. Here we sweep over the input as usual in either height or width,
    but not the other (Snavely 2013). This is a popular technique when working with
    text, which can be represented as a grid where each element holds a single letter,
    and rows contain complete words (or a fixed number of letters) (Britz 2015).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的特殊情况是对输入进行滤波的过程，称为*1D 卷积*。在这里，我们像往常一样对输入进行扫描，但只沿高度或宽度方向，而不在另一个方向上进行扫描（Snavely
    2013）。当处理文本时，这是一种常见的技术，文本可以表示为一个网格，每个元素包含一个字母，行包含完整的单词（或固定数量的字母）（Britz 2015）。
- en: The basic idea is shown in [Figure 16-22](#figure16-22).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思路见[图 16-22](#figure16-22)。
- en: '![F16022](Images/F16022.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![F16022](Images/F16022.png)'
- en: 'Figure 16-22: An example of 1D convolution. The filter only moves downward.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16-22：1D 卷积示例。滤波器只向下移动。
- en: Here, we’ve created a filter that is the entire width of the input and two rows
    high. The first application of the filter processes everything in the first two
    rows. Then we move the filter down and process the next two rows. We don’t move
    the filter horizontally. The name *1D convolution* comes from this single direction,
    or dimension, of movement.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个宽度与输入相同且高度为两行的滤波器。滤波器的第一次应用处理前两行的所有内容。然后，我们将滤波器向下移动，处理接下来的两行。我们不会水平移动滤波器。*1D
    卷积* 这个名字来源于这种单一方向或维度的移动方式。
- en: 'As always, we can have multiple filters sliding down the grid. We can perform
    1D convolution on an input tensor of any number of dimensions, as long as the
    filter itself moves in just one dimension. There’s nothing otherwise special about
    1D convolution: it’s just a filter that only moves in one direction. The technique
    has its own name to emphasize the filter’s limited mobility.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们可以让多个滤波器在网格上滑动。我们可以对任何维度的输入张量执行 1D 卷积，只要滤波器本身仅在一个维度上移动。1D 卷积没有其他特别之处：它只是一个仅在一个方向上移动的滤波器。这个技巧有自己的名字，目的是强调滤波器的有限移动性。
- en: The name 1D convolution is almost the same as the name of another, quite different,
    technique. Let’s look at that now.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 1D 卷积的名称几乎与另一种完全不同的技术名称相同。让我们现在来看看这个技术。
- en: 1×1 Convolutions
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1×1 卷积
- en: 'Sometimes we want to reduce the number of channels in a tensor as it flows
    through a network. Often this is because we think that some of the channels contain
    redundant information. This isn’t uncommon. For example, suppose we have a classifier
    that identifies the dominant object in a photograph. The classifier might have
    a dozen or more filters that look for eyes of different sorts: human eyes, cat
    eyes, fish eyes, and so on. If our classifier is going to ultimately lump all
    living things together into one class called “living things,” then there’s no
    need to care about which kind of eye we find. It’s enough just to know that a
    particular region in the input image has an eye.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们希望在张量通过网络时减少通道的数量。通常这是因为我们认为某些通道包含冗余信息。这并不罕见。例如，假设我们有一个分类器，它识别照片中的主导物体。分类器可能有十几个滤波器，用于寻找不同种类的眼睛：人类眼睛、猫眼、鱼眼等等。如果我们的分类器最终会将所有生物合并成一个名为“生物”的类别，那么就没有必要关心我们发现的是哪种眼睛。只要知道输入图像中的某个区域有眼睛就足够了。
- en: Suppose that we have a layer containing filters that detect 12 different kinds
    of eyes. Then the output tensor from that layer will have at least 12 channels,
    one from each filter. If we only care about whether or not an eye is found, then
    it would be useful to modify that tensor by combining, or compressing, those 12
    channels into just 1 channel representing whether or not an eye is found at each
    location.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个层，其中包含检测 12 种不同类型眼睛的滤波器。那么该层的输出张量将至少有 12 个通道，每个滤波器一个通道。如果我们只关心是否找到眼睛，那么将这个张量通过合并或压缩这
    12 个通道为一个表示每个位置是否有眼睛的通道会很有用。
- en: This doesn’t require anything new. We want to process one input element at a
    time, so we create a filter with a footprint of one by one, like we saw in [Figure
    16-6](#figure16-6). We make sure that we have at least 11 fewer filters than there
    are input channels. The result is a tensor of the same width and height as the
    input, but the multiple eye channels get crunched together into just one channel.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这不需要任何新东西。我们希望一次处理一个输入元素，因此我们创建了一个尺寸为 1x1 的滤波器，正如我们在[图 16-6](#figure16-6)中看到的那样。我们确保滤波器的数量至少比输入通道数少
    11 个。结果是一个与输入相同宽度和高度的张量，但多个眼睛通道被压缩成一个通道。
- en: We don’t have to do anything explicit to make this happen. The network learns
    weights for the filters such that the network produces the correct output for
    each input. If that means combining all the channels for eyes, then the network
    learns to do that.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 16-23](#figure16-23) shows how to use these filters to compress a tensor
    with 300 channels into a new tensor of the same width and height, but with only
    175 channels.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![F16023](Images/F16023.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-23: Applying 1×1 convolution to perform feature reduction'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: The technique of using one by one filters has been given its own name. We say
    that we apply a *one by one filter*, often written as a *1×1 filter*, and use
    that to perform *1×1 convolution* (Lin, Chen, and Yan 2014).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 10 we talked about the value of preprocessing our input data in order
    to save processing time and memory. Rather than perform this processing once,
    before the data has entered our system, 1×1 convolution lets us apply this compression
    and restructuring of the data on the fly, inside of the network. If our network
    produces information that can be compressed or removed entirely, then 1×1 convolutions
    can find and then compress or remove that data. We can do this anywhere, even
    in the middle of a network.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: When the channels are correlated, 1×1 convolution is particularly effective
    (Canziani, Paszke, and Culurciello 2016; Culurciello 2017). This means that the
    filters on the previous layers have created results that are in sync with one
    another, so that when one goes up, we can predict by how much the others will
    go up or down. The better this correlation, the more likely it is that we can
    remove some of the channels and suffer little to no loss of information. The 1×1
    filters are perfect for this job.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: The term *1×1 convolution* is uncomfortably close to *1D convolution*, which
    we discussed in the last section. But these names refer to quite distinct techniques.
    When encountering either of these terms, it is worth taking a moment to make sure
    we have the correct idea in mind.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Changing Output Size
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve just seen how to change the number of channels in a tensor by using 1×1
    convolution. We can also change the width and height, which is useful for at least
    two reasons. The first is that if we can make the data flowing through our network
    smaller, we can use a simpler network and save time, computing resources, and
    energy. The second is that reducing the width and height can make some operations,
    like classification, more efficient and even more accurate. Let’s see why this
    is so.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In previous sections, we applied each filter to one pixel, or one region of
    pixels. The filter matches the feature it’s looking for if the underlying pixels
    match the filter’s values. But what if some of the elements of the feature are
    in slightly wrong places? Then the filter won’t match. There’s no way for the
    filter to look around and report a match if one or more pieces of the pattern
    it’s looking for are present but slightly out of position. This would be a real
    problem if we didn’t address it. For example, suppose we’re looking for a capital
    T on a page of text. Due to a minor mechanical error during printing, a column
    of pixels was displaced downward by one pixel.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们将每个滤波器应用于一个像素或一块像素区域。如果基础像素与滤波器的值匹配，滤波器就能找到它所寻找的特征。但是如果特征的某些元素稍微偏离了正确的位置呢？那么滤波器就无法匹配。如果图案中一个或多个部分存在，但稍微错位，滤波器就无法找到匹配的地方。如果我们不解决这个问题，那将是一个真正的麻烦。例如，假设我们在一页文本中寻找一个大写字母T。由于印刷过程中发生了轻微的机械故障，一列像素被向下错位了一个像素。
- en: We still want to find the T. The situation is illustrated in [Figure 16-24](#figure16-24).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然希望找到字母T。这个情况在[图16-24](#figure16-24)中有说明。
- en: '![F16024](Images/F16024.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![F16024](Images/F16024.png)'
- en: 'Figure 16-24: From left to right: A five by five filter looking for a letter
    T, a misprinted T, the filter on top of the image, and the filter’s resulting
    values. The filter would not report a match to the letter T.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-24：从左到右：一个五乘五的滤波器正在寻找字母T，一个印刷错误的T，滤波器覆盖在图像上方，以及滤波器的结果值。该滤波器无法报告找到字母T的匹配。
- en: We begin with a five by five filter that is looking for a T in the center. We
    illustrate this using blue for 1 and yellow for 0\.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个五乘五的滤波器开始，寻找位于中心的T字形。我们使用蓝色表示1，黄色表示0来进行说明。
- en: We’ve labeled this the “perfect filter,” a name that will make sense in a moment.
    To its right is the misprinted text we’re going to examine, labeled “perfect image.”
    To the right of that, we overlay the filter on the image. At the far right is
    the result. Only when the filter and the input are both blue will the output be
    blue. Since the filter’s upper-right element did not find the blue pixel it was
    expecting, the filter as a whole reports either no match, or a weak one.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个称为“完美滤波器”，稍后它的名称会更清晰。在其右侧是我们将要检查的印刷错误文本，标记为“完美图像”。再往右，我们将滤波器叠加到图像上。最右侧是结果。只有当滤波器和输入都是蓝色时，输出才会是蓝色。由于滤波器的右上角元素没有找到它预期的蓝色像素，因此整个滤波器报告了没有匹配，或者是一个较弱的匹配。
- en: If the upper-right element in the filter could look around and notice the blue
    pixel just below it, it could match the input. One way to make this happen is
    to let each filter element “see” more of the input. The most convenient way to
    do that mathematically is to make the filter a bit blurry.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果滤波器的右上角元素能够环顾四周并注意到它正下方的蓝色像素，它就能够匹配输入。实现这一点的一种方法是让每个滤波器元素“看到”更多的输入。最方便的数学方法是让滤波器稍微模糊一些。
- en: On the top row of [Figure 16-25](#figure16-25) we picked out one element of
    the filter and blurred it. If the filter finds a blue pixel anywhere in this larger,
    blurry region, it reports finding blue. If we do this for all the entries in the
    filter, we create a “blurry filter.” Thanks to this extended reach, the upper-right
    blue filter element now overlaps two blue pixels, and since the other blue elements
    also overlap blue pixels, the filter now reports a match.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图16-25](#figure16-25)的上排中，我们选取了滤波器的一个元素并使其模糊。如果滤波器在这个更大、更模糊的区域中找到了蓝色像素，它就会报告找到了蓝色。如果我们对滤波器中的所有条目都进行这样的操作，就会创建一个“模糊滤波器”。由于这个扩展的范围，右上角的蓝色滤波器元素现在覆盖了两个蓝色像素，并且由于其他蓝色元素也覆盖了蓝色像素，滤波器现在报告了一个匹配。
- en: '![F16025](Images/F16025.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![F16025](Images/F16025.png)'
- en: 'Figure 16-25: Top row: Replacing a filter element with a bigger, blurrier version.
    Bottom row: Applying the blur to every filter element gives us a blurry filter.
    Applying this to the image matches the misprinted T.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-25：上排：将一个滤波器元素替换为更大、更模糊的版本。下排：将模糊应用于每个滤波器元素，从而得到一个模糊滤波器。将其应用于图像时，可以匹配到印刷错误的T字形。
- en: Unfortunately, we can’t blur filters like this. If we modified our filter values
    by blurring them, our training process would go haywire, since we would be altering
    the very values we’re trying to learn. But there’s nothing stopping us from blurring
    the input! This is particularly easy to see if the input is a picture, but we
    can blur any tensor. So rather than applying a blurry filter to a perfect input,
    let’s flip that around and apply a perfect filter to a blurry input.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: The top row of [Figure 16-26](#figure16-26) shows a single pixel from the misprinted
    T, and the version of that pixel after it’s been blurred. After we apply this
    blurring to all the pixels, we can apply the perfect filter to this blurry image.
    Now every blue dot in the filter sees blue under it. Success!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![F16026](Images/F16026.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-26: Top row: The effect of blurring one pixel in the input. Bottom
    row: We apply the perfect filter to a blurred version of the image. This matches
    the misprinted T.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Taking this as our inspiration, we can come up with a technique to blur a tensor.
    We call the method *pooling*, or *downsampling.* Let’s see how pooling works numerically
    with a small tensor with a single channel. Suppose we start with a tensor that
    has a width and height of four, as shown in [Figure 16-27](#figure16-27)(a).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![F16027](Images/F16027.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-27: Pooling, or downsampling, a tensor. (a) Our input tensor. (b)
    Subdividing (a) into two by two blocks. (c) The result of average pooling. (d)
    The result of max pooling. (e) Our icon for a pooling layer.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Let’s subdivide the width and height of this tensor into two by two blocks,
    as in [Figure 16-27](#figure16-27)(b). To blur the input tensor, recall [Figure
    16-7](#figure16-7). We saw that by convolving with a filter whose contents are
    all 1’s, the image got blurry. Such a filter is called a *low-pass filter*, or
    more specifically, a *box filter.*
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: To apply a box filter to a tensor, we can use a two by two filter where every
    weight is a 1\. Applying this filter merely means adding up the four numbers in
    each two by two block. Because we don’t want our numbers to grow without bound,
    we divide the result by four to get the average value in that block. Since this
    average now stands in for the entire block, we save it just once. We do the same
    thing for the other three blocks. The result is a new tensor of size two by two,
    shown in [Figure 16-27](#figure16-27)(c). This technique is called *average pooling.*
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s a variation on this method: instead of computing the average value,
    we just use the largest value in each block. This is called *maximum pooling*
    (or more often, just *max pooling)*, and is shown in [Figure 16-27](#figure16-27)(d).
    It’s common to think of these pooling operations as being carried out by a little
    utility layer. In [Figure 16-27](#figure16-27)(e) we show our icon for such a
    *pooling layer*. Experience has shown that networks that use max pooling learn
    more quickly than those using average pooling, so when people speak of pooling
    with no other qualifiers, they usually mean max pooling.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: The power of pooling appears when we apply multiple convolution layers in succession.
    Just as with a filter and a blurred input, if the first filter’s values aren’t
    in quite the expected locations, pooling helps the second layer’s filter still
    find them. For example, suppose that we have two layers in succession, and Layer
    2 has a filter that is looking for a strong match from Layer 1, directly above
    a match of about half that value (maybe this is characteristic of a particular
    animal’s coloration). Nothing in the original 4 by 4 tensor in [Figure 16-27](#figure16-27)(a)
    fits that pattern. There’s a 20 over a 2, but the 2 isn’t close to being half
    of 20\. And there’s a 6 over 3, but 6 isn’t a very strong output. So Layer 2’s
    filter would fail to find what it was looking for. That’s too bad, because there
    is a 20 that’s close to being over a 9, which is what the filter wants to find.
    The problem is that the 20 and the 9 are not exactly vertical neighbors.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: But the max pooling version has the 20 over the 9\. The pooling operation is
    communicating to Layer 2 that there is a strong match of 20 somewhere in the upper
    right two by two block, and a match of 9 somewhere in the block directly below
    the 20\. That’s the pattern we’re looking for, and the filter will tell us that
    it found a match.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: We’ve discussed pooling for just one channel. When our tensors have multiple
    channels, we apply the same process to each channel. [Figure 16-28](#figure16-28)
    shows the idea.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: We start with an input tensor of height and width 6 and one channel, padded
    with a ring of zeros. The convolution layer applies three filters, each producing
    a feature map of six by six. The output of the convolution layer is a tensor of
    size six by six by three. The pooling layer then conceptually considers each channel
    of this tensor, and applies max pooling to it, reducing each feature map to three
    by three. Those feature maps are then combined as before to produce an output
    tensor of width and height 3, with three channels.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![F16028](Images/F16028.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-28: Pooling, or downsampling, with multiple filters'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: We’ve been using binary images and filters as examples. This means that a feature
    that straddles cell boundaries could be missed, or wind up in the wrong element
    in the pooled tensor. When we use real valued inputs and filter kernels, this
    problem is greatly reduced.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Pooling is a powerful operation that frees filters from requiring their inputs
    to be in precisely the right place. Mathematicians refer to a change in location
    as *translation* or *shift*, and if some operation is insensitive to a certain
    kind of change it’s called *invariant* with respect to that operation. Combining
    these, we sometimes say that pooling allows our convolutions to be *translationally
    invariant*, or *shift invariant* (Zhang 2019).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Pooling also has the bonus benefit of reducing the size of the tensors flowing
    through our network, which reduces both memory needs and execution time.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Striding
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve seen how useful pooling is in a convolutional network. Though pooling
    layers are common, we can save time by bundling the pooling step right into the
    convolution process. This combined operation is much faster than two distinct
    layers. The tensors resulting from the two procedures usually contain different
    values, but experience has shown that the faster, combined operation usually produces
    results that are just as useful as the slower, sequential operations.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: As we saw, during convolution we can imagine starting the filter in the upper-left
    pixel of the input image (let’s assume we have padding). The filter produces an
    output, then takes one step right, produces another output, moves another step
    right, and so on until it reaches the right edge of that row. Then it moves down
    one row and back to the left side, and the process repeats.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: But we don’t have to move in single steps. Suppose we move, or *stride*, more
    than one pixel to the right, or more than one pixel down, as we sweep our filter.
    Then our output will end up being smaller than the input. We usually use the word
    *stride* (and the related *striding*) only when we use steps greater than one
    in any dimension.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: To visualize striding, let’s see how the filter moves starting from the upper
    left. As the filter moves left to right, it produces a sequence of outputs, and
    those get placed one after the other, also left to right, in the output. When
    the filter moves down, the new outputs go on a new line of cells in the output.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Now suppose that instead of moving the filter to the right by one element on
    each horizontal step, we moved to the right by three elements. And perhaps on
    each vertical step we move down by two rows, rather than one. We still grow the
    output by one element for each output. The idea is shown in [Figure 16-29](#figure16-29).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![F16029](Images/F16029.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-29: Our input scanning can skip over input elements as it moves.
    Here we move three elements to the right on each horizontal step, and two elements
    down on each vertical step.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 16-29](#figure16-29) we used a stride of three horizontally, and
    a stride of two vertically. More often we specify a single stride value for both
    axes. A stride of two on both axes can be thought of as evaluating every other
    pixel both horizontally and vertically. This results in an output that has half
    the input dimensions as the input, which means the output has the same dimensions
    as striding by one and then pooling with two by two blocks. [Figure 16-30](#figure16-30)
    shows where the filter lands in the input for a couple of different pairs of strides.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![F16030](Images/F16030.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-30: Examples of striding. (a) A stride of two in both directions
    means centering the filter over every other pixel, both horizontally and vertically.
    (b) A stride of three in both directions means centering over every third pixel.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: When we move by one element on every step, a filter with a three by three footprint
    processes the same input elements multiple times. When we stride by larger amounts,
    our filter can still process some elements multiple times, as shown in [Figure
    16-31](#figure16-31).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![F16031](Images/F16031.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-31: This three by three filter is moving with a stride of two in
    each dimension, reading left to right, top to bottom. The gray elements show what’s
    been processed so far. The green elements are those that have already been used
    by the filter on previous evaluations but are being used again.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: There’s nothing wrong with reusing an input value repeatedly, but if we’re trying
    to save time, we might want to do as little computation as possible. Then we can
    use striding to prevent any input element from being used more than once. For
    instance, if we’re moving a three by three filter over an image, we might use
    a stride of three in both directions, so that no pixel gets used more than once,
    as in [Figure 16-32](#figure16-32).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![F16032](Images/F16032.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-32: Like [Figure 16-31](#figure16-31), only now we’re striding by
    three in each dimension. Every input element is processed exactly one time.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: The striding in [Figure 16-32](#figure16-32) produces an output tensor with
    a height and width that are each one-third of the input tensor’s height and width.
    Consider that in [Figure 16-32](#figure16-32) we processed a nine by six block
    of input elements with just six filter evaluations. By doing this, we created
    a three by two block of outputs with no explicit pooling. If we don’t stride,
    and then pool, we need many more filter evaluations to cover the same region,
    and then we need to run the pooling operation on the filter outputs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Strided convolutions are faster than convolution without striding followed by
    pooling for two reasons. First, we evaluate the filter fewer times, and second,
    we don’t have an explicit pooling step to compute. Like padding, striding can
    (and often is) carried out on any convolutional layer, not just the first.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: The filters learned from striding are usually different than those learned from
    convolution without striding followed by pooling. This means we can’t take a trained
    network and replace pairs of convolution and pooling with strided convolution
    (or vice versa) and expect things to still work properly. If we want to change
    our network’s architecture, we have to retrain it.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, training with strided convolution gives us final results that
    are roughly the same as those we get from convolution followed by pooling, delivered
    in less time. But sometimes the slower combination works better for a given dataset
    and architecture.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Transposed Convolution
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve seen how to reduce the size of the input, or *downsize* it, using either
    pooling or striding. We can also increase the size of the input, or *upsize* it.
    As with downsizing, when we upsize a tensor, we increase its width and height,
    but we don’t change the number of channels.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Just as with downsampling, we can upsize with a separate layer or build it into
    the convolution layer. A distinct upsampling layer usually just repeats the input
    tensor values as many times as we request. For example, if we upsample a tensor
    by two in both the width and height, each input element turns into a little two
    by two square. [Figure 16-33](#figure16-33) shows the idea.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![F16033](Images/F16033.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-33: Upsampling a tensor by two in each direction. Left: The input
    tensor. Each element of this tensor is repeated twice vertically and horizontally.
    Right: The output tensor. The number of channels is unchanged.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that we can combine downsampling with convolution by using striding.
    We can also combine upsampling with convolution. This combined step is called
    *transposed convolution*, *fractional striding*, *dilated convolution*, or *atrous
    convolution.* The word *transposed* comes from the mathematical operation of transposition,
    which we can use to write the equation for this operation. The word *atrous* is
    French for “with holes.” We’ll see where that term, and the others, come from
    in a moment. Note that some authors refer to the combination of upsampling and
    convolution as *deconvolution*, but it’s best to avoid that term, since it’s already
    in use and refers to a different idea (Zeiler et al. 2010). Following current
    practice, we’ll use the term *transposed convolution*.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how transposed convolution works to enlarge a tensor (Dumoulin and
    Visin 2016). Suppose that we have a starting image of width and height three by
    three (remember, the number of channels won’t be changing), and we’d like to process
    it with a three by three filter, but we’d like to end up with a five by five image.
    One approach is to pad the input with two rings of zeros, as in [Figure 16-34](#figure16-34).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![F16034](Images/F16034.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-34: Our original three by three input is shown in white in the outer
    grids, padded with two elements of zeros all around. The three by three filter
    now produces a five by five result, shown in the center.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: If we add more rings of zeros to the input, we get larger outputs, but they
    will produce rings of zeros around the central five by five core. That’s not very
    useful.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: An alternative way to enlarge the input is to spread it out before convolving
    by inserting padding both around and *between* the input elements. Let’s try this
    out. Let’s insert a single row and column of zeros between each element of our
    starting three by three image, and pad all of that with two rings of zeros around
    the outside, like before. The result is that our three by three input now has
    dimensions nine by nine, though a lot of those entries are zero. When we sweep
    our three by three filter over this grid, we get a seven by seven output, as shown
    in [Figure 16-35](#figure16-35).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Our original three by three image is shown in the outer grids with white pixels.
    We’ve inserted a row and column of zeros (blue) between each pixel, and then surrounded
    the whole thing with two rings of zeros. When we convolve our three by three filter
    (red) with this grid, we get a seven by seven result, shown in the center.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![F16035](Images/F16035.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-35: Transposed convolution, convolving a three by three filter into
    a seven by seven result'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 16-35](#figure16-35) suggests where the names *atrous* (French for
    “with holes”) *convolution* and *dilated convolution* come from. We can make our
    output even bigger by inserting another row and column between each original input
    element, as in [Figure 16-36](#figure16-36). Now our 3 by 3 input has become an
    11 by 11 input, and the output is 9 by 9.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![F16036](Images/F16036.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-36: The same setup as [Figure 16-35](#figure16-35), only now we have
    two rows and columns between our original input pixels, producing the nine by
    nine result in the center'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: We can’t push this technique any further without producing rows and columns
    of zeros in the output. The limit of two rows or columns of zeros is due to our
    filter having a footprint of three by three. If the filter was, say, five by five,
    we could use up to four rows and columns of zeros. This technique of inserting
    zeros can create little checkerboard-like artifacts in the output tensors. But
    library routines can usually avoid these if they take steps to handle the convolution
    and upsampling carefully (Odena, Dumoulin, and Olah 2018; Aitken et al. 2017).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: There is a connection between transposed convolution and striding. With some
    imagination, we can describe a transposed convolution process like that of [Figure
    16-36](#figure16-36) as using a stride of one-third in each dimension. We don’t
    mean that we literally move one-third of an element, but rather that we need to
    take three steps in the 11 by 11 grid to move the equivalent of one step in the
    original 3 by 3 input. This point of view explains why the method is sometimes
    called *fractional striding*.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Just as striding combines convolution with a downsampling (or pooling) step,
    transposed convolution (or fractional striding) combines convolution with an upsampling
    step. This results in faster execution time, which is always nice. A problem is
    that there is a limit to how much we can increase the input size. In practice,
    we commonly double the input dimensions, and use filters with a footprint of three
    by three, and transposed convolution supports that combination without introducing
    extraneous zeros in the output.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: As with striding, the output of transposed convolution is different than the
    output of upsampling followed by standard convolution, so if we’re given a trained
    network using upsampling followed by convolution, we can’t just replace those
    two layers with one transposed convolution layer and use the same filters.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Transposed convolution is becoming more common than upsampling followed by convolution
    because of the increased efficiency, and similarity of the results (Springenberg
    et al. 2015).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: We’ve covered a lot of basic tools, from different types of convolution to padding
    and changing the output size. In the next section, we put these all together to
    create a complete, but simple, convolutional network.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchies of Filters
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many real visual systems seem to be arranged *hierarchically* (Serre 2014).
    In broad terms, many biologists think of the processing in the visual system as
    taking place in a series of layers, with each successive layer working at a higher
    level of abstraction than the one before.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: We’ve taken inspiration from biology already in this chapter, and we can do
    it again now.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Let’s return to our discussion of the visual system of a toad. The first layer
    of cells to receive light may be looking for “bug-colored blobs,” the next may
    be looking for “combinations of blobs from the previous layer that form bug-like
    shapes,” the next may be looking for “combinations of bug-like shapes from the
    previous layer that look like a thorax with wings,” and so on, up to the top layer,
    which looks for “flies” (these features are completely imaginary, and only meant
    to illustrate the idea).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: This approach is nice conceptually because it lets us structure our analysis
    of an image in terms of a hierarchy of image features and the filters that look
    for them. It’s also nice for implementations because it’s a flexible and efficient
    way to analyze an image.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying Assumptions
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To illustrate the use of hierarchies, let’s solve a recognition problem with
    a convolutional network. To focus this discussion just on the concepts, we’ll
    make use of some simplifications. These simplifications in no way change the principles
    we’re demonstrating; they just make the pictures easier to draw and interpret.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we restrict ourselves to binary images: just black and white, with no
    shades of gray (though for clarity, we draw them with beige and green for 0 and
    1, respectively). In real applications, each channel in our input images is usually
    either an integer in the range [0, 255], or more commonly a real number in the
    range [0, 1].'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Second, our filters are also binary and look for exact matches in their inputs.
    In real networks, our filters use real numbers, and they match their inputs to
    different degrees, represented by different real numbers at their output.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Third, we hand-create all of our filters. In other words, we do our own feature
    engineering. When we looked at expert systems, we said that their biggest problem
    was that they required people to manually build features, and here we are, doing
    just that! We’re doing so just for this discussion, however. In practice, our
    filter values are learned by training. Since we’re not interested in the training
    step right now, we’ll use handmade filters (we can think of them as filters that
    resulted from training).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Fourth, we won’t use padding. This also is just to keep things simple.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Finally, our example uses tiny input images that are just 12 pixels on a side.
    This is large enough to demonstrate the ideas but small enough that we can draw
    everything clearly on the page.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: With these simplifications in place, we’re ready to get started.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Finding Face Masks
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s suppose that we work at a museum that has received a big collection of
    art, and it’s our job to organize it all. One of our tasks is to find all of the
    drawings of grid-based face masks that are close matches to the simple mask in
    [Figure 16-37](#figure16-37).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![F16037](Images/F16037.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-37: A simple binary mask on a 12 by 12 mesh'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we’re given the new mask in the middle of [Figure 16-37](#figure16-37).
    Let’s call this the *candidate*. We want to determine whether it’s roughly the
    “same” as the original mask, which we call the *reference*. We can just overlay
    the two masks and see if they match up, as in the right of [Figure 16-38](#figure16-38).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![F16038](Images/F16038.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-38: Testing for similarity. On the left is our original mask, or
    reference. In the middle is a new mask, or candidate. To see if they’re close
    to one another, we can overlay them, at the right.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: In this case, it’s a perfect match, which is easy to detect. But what if a candidate
    is slightly different than the reference, as in [Figure 16-39](#figure16-39)?
    Here one eye has moved down by one pixel.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![F16039](Images/F16039.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-39: Like [Figure 16-38](#figure16-38), only the candidate’s left
    eye has moved down by one pixel. The overlay is now imperfect.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that we still want to accept this candidate, since it has all the
    same features as the reference, and they’re mostly in the right places. But the
    overlay shows that they’re not identical, so a simple pixel-by-pixel comparison
    won’t do the job.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: In this simple example, we could come up with lots of ways to detect close matches,
    but let’s use convolution to determine that a candidate like the one in [Figure
    16-39](#figure16-39) is “like” the reference. As mentioned earlier, we’re going
    to hand-engineer our filters. To describe our hierarchy, it’s easiest to work
    backward, from the final step of convolution to the first.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by describing the reference mask. Then we can determine if a candidate
    shares its qualities. Let’s say that our reference is characterized by having
    one eye in each of the upper corners, a nose in the middle, and a mouth under
    the nose. That description applies to all of the masks we saw in Figures 16-38
    and 16-39.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'We can formalize this description with a three by three filter, as in the top-left
    grid of [Figure 16-40](#figure16-40). This will be one of our last filters: if
    we run a candidate through a series of convolutions, ultimately producing a three
    by three tensor (we’ll see how that happens shortly), then if that tensor matches
    this filter, we’ve found a successful match, and an acceptable candidate. The
    cells with an × in them mean “don’t care.” For instance, suppose a candidate has
    a tattoo on one cheek that falls into the × to the right of the nose. This doesn’t
    affect our decision, so we explicitly don’t care about what’s in that cell.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![F16040](Images/F16040.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-40: Filters for mask recognition. Top and bottom rows: Finding a
    mask facing forward, or in profile. Left column: Characterizing the reference.
    Middle: An exploded version of the tensor described by the grid at the left. Right:
    An X-ray view of the filter (see text).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Since our filters only contain the values 1 (green) and 0 (beige), we can’t
    make a filter like the upper left diagram of [Figure 16-40](#figure16-40) directly.
    Instead, since it’s looking for three different kinds of features, we need to
    redraw it as a filter with three channels, which we’ll apply to an input tensor
    with three channels. One input channel tells us all the locations where an eye
    was located in the input, the next tells us all the locations of a nose, and the
    last tells us all the locations of a mouth. Our upper-left diagram corresponds,
    then, to a three by three by three tensor, shown in the upper middle diagram,
    where we’ve staggered the channels so we can read each one.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: We drew the staggered version because if we drew that tensor as a solid block,
    we wouldn’t be able to see most of the values on the N (nose) and M (mouth) channels.
    The staggered version is useful, but it will get too complicated when we start
    comparing tensors in the following discussion. Instead, let’s draw an “X-ray view”
    of the tensor, as in the upper right. We imagine we’re looking through the channels
    of the tensor, and we mark each cell with the names of all the channels that have
    a 1 in that cell.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Since this filter is looking for a mask facing forward, we label it F. For fun,
    we can make another mask that’s looking for a face in profile, which we can call
    P. We won’t look at any candidates that would be matched by P, but we’re including
    it here to show the generality of this process. The layers to come, which operate
    before the filters of [Figure 16-40](#figure16-40), will tell us where they found
    an eye, nose, and mouth. We use that information in [Figure 16-40](#figure16-40)
    to recognize different arrangements of these facial features just by using different
    filters.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Finding Eyes, Noses, and Mouths
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s see how to turn a 12 by 12 candidate picture into the 3 by 3 grid required
    by the filters of [Figure 16-40](#figure16-40). We can do that with a series of
    convolutions, each followed by a pooling step. Since the filters of [Figure 16-40](#figure16-40)
    are trying to match eyes, a nose, and a mouth, we know that the convolution layer
    before these filters has to produce those features. So, let’s design filters that
    search for them.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 16-41](#figure16-41), we show three filters, each with a four by
    four footprint. They’re labeled E4, N4, and M4\. They look for an eye, a nose,
    and a mouth, respectively. The reason for placing the “4” at the end of each name
    will be clear in a moment.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '![F16041](Images/F16041.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-41: Three filters that detect an eye, nose, and mouth'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: We can jump right in and apply these three filters to any candidate image. Since
    the images are 12 by 12, and we’re not padding, the outputs will be 10 by 10\.
    If we pool those down to 3 by 3, we can then apply the filters of [Figure 16-40](#figure16-40)
    to the output of the filters in [Figure 16-41](#figure16-41) to determine if the
    candidate is a mask looking forward, or in profile, or neither.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: But applying four by four filters requires a lot of computation. Worse, if we
    want to look for another feature (like a winking eye), we have to build another
    four by four filter and also apply that to the whole image. We can make our system
    more flexible, and also faster, by introducing another layer of convolution before
    this one.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: What features can make up our E4, N4, and M4 filters of [Figure 16-41](#figure16-41)?
    If we think of each four by four filter as a grid of two by two blocks, then we
    need only four types of two by two blocks to make up all three filters. The top
    row of [Figure 16-42](#figure16-42) shows those four little blocks, and the rows
    below that show how they can be combined to make our eye, nose, and mouth filters.
    We’ve called these T, Q, L, and R for top, quartet, lower-left corner, and lower-right
    corner, respectively.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '![F16042](Images/F16042.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-42: Top row: The two by two filters T, Q, L, and R. Second row, left
    to right: Filter E4, breaking it into four smaller blocks and the tensor form
    of those blocks. The far right shows the X-ray view of the two by two by four
    filter E. Third and fourth rows: Filters N4 and M4.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Starting with the eye filter E4, we break the four by four filter into four
    two by two blocks. The third drawing in the E4 row shows the four channels that
    we expect as input, one each for T, Q, L, and R, drawn as a single tensor where
    we staggered the channels. To draw that tensor more conveniently, we use the X-ray
    convention we saw in [Figure 16-40](#figure16-40). This gives us a new filter,
    of size two by two by four. This is the filter we really want to use to detect
    eyes, so we drop the “4” and just call this E.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: The N and M filters are created by the same process of subdivision and assembly
    from T, Q, L, and R.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Now imagine running the little T, Q, L, and R filters over a candidate image.
    They’re looking for patterns of pixels. Then the E, N, and M filters look for
    specific arrangements of T, Q, L, and R patterns. And then the F and P filters
    look for specific arrangements of E, N, and M patterns. Thus, we have a series
    of convolution layers, with each output serving as the next layer’s input. [Figure
    16-43](#figure16-43) shows this graphically.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '![F16043](Images/F16043.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-43: Using three layers of convolution to analyze an input candidate'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our filters, we can start at the bottom and process an input.
    Along the way, we’ll see where to put the pooling layers.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Applying Our Filters
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s start at the bottom of [Figure 16-43](#figure16-43) and apply the filters
    of Layer 1\. [Figure 16-44](#figure16-44) shows the result of sweeping the T filter
    over the 12 by 12 candidate image. Because T is 2 by 2, it doesn’t have a center,
    so we arbitrarily place its anchor in its upper-left corner. Because we’re not
    padding, and the filter is 2 by 2, the output will be 11 by 11\. In [Figure 16-44](#figure16-44),
    each location where T finds an exact match is marked in light green; otherwise,
    it’s marked in pink. We’ll call this output the T-map.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we want to make sure that the E, N, and M filters that are looking for
    T matches still succeed even if the T matches aren’t exactly where our reference
    mask had them. As we saw in the previous section, the way to make filters robust
    to small displacements in their input is to use pooling. Let’s use the most common
    form of pooling: max pooling with two by two blocks.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![F16044](Images/F16044.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-44: Convolving the 12 by 12 input image with the 2 by 2 filter T
    produces the 11 by 11 output, or feature map, which we call the T-map.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 16-45](#figure16-45) shows max pooling applied to the T-map. For each
    two by two block, if there’s at least one green value in the block, the output
    is green (recall that green elements have a value of 1, and the red are 0). When
    the pooling blocks fall off the right and bottom sides of the input, we just ignore
    the missing entries and apply pooling to the values we actually have. We call
    the result of pooling the T-pool tensor.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '![F16045](Images/F16045.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-45: Applying two by two max pooling to the T-map to produce the T-pool
    tensor. Green means 1, and pink means 0\.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: The upper-left element of T-pool tells us if the T filter matched when placed
    on top of *any* of the four pixels in the upper left of the input. In this case,
    it did, so that element is turned green (that is, it’s assigned a value of 1).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Let’s repeat this process for the other three first-layer filters (Q, L, and
    R). The results are shown in the left part of [Figure 16-46](#figure16-46).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: The four T, Q, L, and R filters together produce a result with four feature
    maps, each six by six after pooling. Recall from [Figure 16-40](#figure16-40)
    that the E, N, and M filters are expecting a tensor with four channels. To combine
    these individual outputs into one tensor, we can just stack them up, as in the
    center of [Figure 16-46](#figure16-46). As usual, we then draw this as a 2D grid
    using our X-ray view convention. This gives us a tensor of four channels, which
    is just what Layer 2 is expecting as input.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![F16046](Images/F16046.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-46: Left: The result of applying all four first-level filters to
    our candidate and then pooling. Center: Stacking up the outputs into a single
    tensor. Right: Drawing the six by six by four tensor in X-ray view.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Now we can move up to the filters in Layer 2\. Let’s start with E, in [Figure
    16-47](#figure16-47).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '![F16047](Images/F16047.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-47: Applying the E filter. As before, from left to right, we have
    the input tensor, the E filter (both in our X-ray view), the result of applying
    that filter, the pooling grid, and the result of pooling.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 16-47](#figure16-47) shows our input tensor (the output of Layer 1)
    and the E filter, both in X-ray view. To their right, we see the E-map resulting
    from applying the E filter, the process of applying two by two pooling to the
    E-map, and finally the E-pool feature map. We can see already how the pooling
    process allows the next filter to match the locations of the eyes, even though
    one eye is not located where it was in the reference mask.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: We can follow the same process for the N and M filters, producing a new output
    tensor for the second layer, as shown in [Figure 16-48](#figure16-48).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Now we have a three by three tensor with three channels, just right for the
    filters we created for F and P back in [Figure 16-40](#figure16-40). We’re ready
    to move up another level to Layer 3.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '![F16048](Images/F16048.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-48: Computing outputs for the E, N, and M filters, then stacking
    them up into a tensor with three channels'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'This final step is easy: we just apply the F and P filters to their entire
    input, since their sizes are the same (that is, there’s no need to scan the filter
    over the image). The result is a tensor with shape one by one by two. If the element
    in the first channel in this tensor is green, then F matches, and the candidate
    should be accepted as a match to our reference. If it’s beige, the candidate’s
    not a match.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![F16049](Images/F16049.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-49: Applying the F and P filters to the output tensor of the second
    layer. In this layer, each filter is the same size as the input, so the layer
    produces an output tensor of size one by one by two.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: And we’re done! We used three layers of convolution to characterize a candidate
    image as being either like, or unlike, a reference image. We found that our candidate
    with one eye dropped down by one pixel was still close enough to our reference
    that we should accept it.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: We solved this problem by creating not just a sequence of convolutions, but
    a hierarchy. Each convolution used the results of the previous one. The first
    layer looked for patterns in the pixels, the second looked for patterns of those
    patterns, and the third looked for larger patterns still, corresponding to a face
    looking forward or in profile. Pooling enabled the network to recognize a candidate
    even though one important block of pixels was shifted a little.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 16-50](#figure16-50) shows our whole network at a glance. Since the
    only layers with neurons are convolution layers, we call this an *all-convolutional
    network.*'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '![F16050](Images/F16050.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-50: Our all-convolutional network for evaluating masks. We’re also
    showing the input, output, and intermediate tensors. The icons with nested boxes
    are convolution layers, the trapezoids are pooling layers.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 16-50](#figure16-50), the icons with a box in a box represent convolution,
    and the trapezoids represent pooling layers.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: If we want to match even more types of faces, we can just add more filters to
    the final layer. This lets us match any pattern of eyes, noses, and mouths that
    we want, with little additional cost. By reducing the size of the tensors in our
    network, pooling reduces the amount of computation we have to do. This means that
    not only is the network with pooling more robust than a version without pooling,
    it also consumes less memory and runs faster.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: There’s a sense in which our filters are getting more powerful as we work our
    way up the levels. For example, our eye filter E is processing a four by four
    region, though it’s only two by two itself, because each of its tensor elements
    is the result of a two by two convolution. In this way, the filters at higher
    levels in a hierarchy are able to look for large and complex features, even though
    they use only small (and therefore fast) filters.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Higher levels are able to combine the results of lower levels in multiple ways.
    Suppose we want to classify a variety of different birds in a photo. Low-level
    filters may look for feathers or beaks, while higher filters are able to combine
    different types of feathers or beaks to recognize different species of birds,
    all in a single pass through a photo. We sometimes say that using this technique
    of convolution and pooling to analyze an input is applying a *hierarchy of scales*.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This chapter was all about convolution: the method of taking a filter or kernel
    (that is, a neuron with a set of weights) and moving it over an input. Each time
    we apply the filter to the input, we produce a single value of output. The filter
    may use just a single input element in its calculation, or it may have a larger
    footprint and use the values of multiple input elements. If a filter has a footprint
    that is larger than one by one, there will be places in the input where the filter
    “spills” over the edge, requiring input data that isn’t there. If we don’t place
    the filter in such places, the output has a smaller width or height (or both)
    than the input. To avoid this, we commonly pad the input by surrounding it with
    enough rings of zeroes so that the filter can be placed over every input element.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: We can bundle up many filters into a single convolution layer. In such a layer,
    typically every filter has the same footprint and the same activation function.
    Every filter produces one channel per filter. The output of the layer has one
    channel per filter.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: If we want to change the width and height of a tensor, we can perform downsampling
    (to reduce either or both dimensions) or upsampling (to increase either or both
    dimensions). To downsample, we can use a pooling layer, which finds the average
    or maximum value in blocks from the input. To upsample, we can use an upsampling
    layer, which duplicates input elements. Either of these techniques may be combined
    with the convolution step itself. To downsample, we use striding, in which the
    filter is moved by more than one step horizontally, vertically, or both. To upsample,
    we use fractional striding, or transposed convolution, in which we insert rows
    and/or columns of zeroes between the input elements.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: We saw that by applying convolutions in a series of layers with downsampling,
    we are able to create a hierarchy of filters that work at different scales. This
    also means that the system enjoys the property of shift invariance, meaning that
    it’s able to find the patterns it seeks even if they’re not exactly where they’re
    expected to be.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll examine real convnets and look at their filters to
    see how they do their jobs.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
