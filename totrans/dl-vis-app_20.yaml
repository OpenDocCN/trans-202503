- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Convolutional Neural Networks
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: This chapter is all about a deep learning technique called *convolution*. Among
    its uses, convolution has become the standard method for classifying, manipulating,
    and generating images. Convolution is easy to use in deep learning because it
    can be easily encapsulated in a *convolution layer* (also called a *convolutional
    layer*). In this chapter, we look at the key ideas behind convolution and the
    related techniques we use to make convolution work in practice. We will see how
    to arrange a series of these operations to create a hierarchy of operations, which
    turns a series of simple operations into a powerful tool.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要介绍一种深度学习技术，叫做*卷积*。卷积的应用之一是它已成为分类、处理和生成图像的标准方法。卷积在深度学习中易于使用，因为它可以很方便地封装在一个*卷积层*（也叫*卷积神经层*）中。在本章中，我们将探讨卷积背后的关键思想以及我们在实际操作中使用的相关技术。我们将看到如何排列一系列这些操作，创建一个操作层次结构，将一系列简单的操作转化为强大的工具。
- en: In order to stay specific, in this chapter we focus our discussion of convolution
    on working with images. Models that use convolution have been spectacularly successful
    in this domain. For example, they excel at basic classification tasks like determining
    if an image is a leopard or a cheetah, or a planet or a marble. We can recognize
    the people in a photograph (Sun, Wang, and Tang 2014); detect and classify different
    types of skin cancers (Esteva et al. 2017); repair image damage like dust, scratches,
    and blur (Mao, Shen, and Yang 2016); and classify people’s age and gender from
    their photos (Levi and Hassner 2015). Convolution-based networks are also useful
    in many other applications, such as natural language processing (Britz 2015),
    where we can work out the structure of sentences (Kalchbrenner, Grefenstette,
    and Blunsom 2014) or classify sentences into different categories (Kim 2014).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持具体性，本章我们将卷积的讨论集中在图像处理上。使用卷积的模型在这个领域取得了惊人的成功。例如，它们在基本分类任务中表现优异，如判断一张图片是豹子还是猎豹，或者是行星还是大理石。我们可以识别照片中的人物（Sun,
    Wang, and Tang 2014）；检测并分类不同类型的皮肤癌（Esteva et al. 2017）；修复图像损坏，如灰尘、划痕和模糊（Mao, Shen,
    and Yang 2016）；并从照片中分类人的年龄和性别（Levi and Hassner 2015）。基于卷积的网络在许多其他应用中也很有用，例如自然语言处理（Britz
    2015），我们可以分析句子的结构（Kalchbrenner, Grefenstette, and Blunsom 2014）或将句子分类到不同类别中（Kim
    2014）。
- en: Introducing Convolution
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入卷积
- en: In deep learning, images are 3D tensors, with a height, width, and number of
    *channels*, or values per pixel. A grayscale image has only one value per pixel,
    and thus only one channel. A color image stored as RGB has three channels (with
    values for red, green, and blue). Sometimes people use the terms *depth* or *fiber
    size* to refer to the number of channels in a tensor. Unfortunately, *depth* is
    also used to refer to the number of layers in a deep network, and *fiber size*
    has not caught on widely. To avoid confusion, we always refer to the three dimensions
    of an image (and related 3D tensors) as height, width, and channels. Using our
    deep learning terminology, each image we provide to the network for processing
    is a sample. Each pixel in an image is a feature.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，图像是三维张量，具有高度、宽度和*通道数*，即每个像素的值。灰度图像每个像素只有一个值，因此只有一个通道。以RGB存储的彩色图像有三个通道（分别对应红色、绿色和蓝色的值）。有时人们使用*深度*或*光纤大小*来指代张量中的通道数。不幸的是，*深度*也用来指代深度网络中的层数，而*光纤大小*并没有广泛应用。为了避免混淆，我们总是将图像（三维张量的相关内容）的三个维度称为高度、宽度和通道数。按照我们的深度学习术语，每个提供给网络处理的图像都是一个样本。图像中的每个像素都是一个特征。
- en: When a tensor moves through a series of convolution layers, it often changes
    in width, height, and number of channels. If a tensor happens to have 1 or 3 channels,
    we can think of it as an image. But if a tensor has, say, 14 or 512 channels,
    it’s probably best not to think of it as an image any more. This suggests that
    we shouldn’t refer to individual elements of the tensor as *pixels*, which is
    an image-centric term. Instead, we call them *elements*. [Figure 16-1](#figure16-1)
    shows these terms visually.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个张量通过一系列卷积层时，它通常会在宽度、高度和通道数上发生变化。如果一个张量恰好有1个或3个通道，我们可以将其视为一张图像。但如果一个张量有14个或512个通道，最好就不再将其看作图像了。这意味着我们不应该将张量的单个元素称为*像素*，因为这是一个以图像为中心的术语。相反，我们称它们为*元素*。[图16-1](#figure16-1)直观地展示了这些术语。
- en: '![F16001](Images/F16001.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![F16001](Images/F16001.png)'
- en: 'Figure 16-1: Left: When our tensor has one or three channels, we can say that
    it’s made up of pixels. Right: For tensors with any number of channels, we call
    each slice through the channels an element.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-1：左侧：当我们的张量具有一个或三个通道时，我们可以说它是由像素构成的。右侧：对于具有任意数量通道的张量，我们称每个通道的切片为一个元素。
- en: A network in which the convolution layers play a central role is usually called
    a *convolutional neural network*, *convnet*, or *CNN*. Sometimes people also say
    *CNN network* (an example of “redundant acronym syndrome syndrome” [Memmott 2015]).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积层起核心作用的网络通常被称为*卷积神经网络*，*convnet*，或*CNN*。有时人们也会说*CNN网络*（这就是“冗余首字母缩略症综合症”[Memmott
    2015]的一个例子）。
- en: Detecting Yellow
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检测黄色
- en: 'To kick off our discussion of convolution, let’s consider processing a color
    image. Each pixel contains three numbers: one each for red, green, and blue. Suppose
    we want to create a grayscale output that has the same height and width as our
    color image, but where the amount of white in each pixel corresponds to the amount
    of yellow in its input pixel.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始讨论卷积，让我们考虑处理彩色图像。每个像素包含三个数字：分别表示红色、绿色和蓝色。假设我们想创建一个灰度输出，它的高度和宽度与我们的彩色图像相同，但每个像素的白色量与其输入像素中的黄色量对应。
- en: For simplicity, let’s assume our RGB values are numbers from 0 to 1\. Then a
    pixel that’s pure yellow has red and green values of 1, and a blue value of 0\.
    As the red and green values decrease, or the blue value increases, the pixel’s
    color shifts away from yellow.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，假设我们的RGB值是从0到1的数字。那么一个纯黄色的像素具有红色和绿色的值为1，蓝色的值为0。当红色和绿色的值减少，或者蓝色的值增加时，像素的颜色将偏离黄色。
- en: We want to combine each input pixel’s RGB values into a single number from 0
    to 1 that represents “yellowness,” which is the output pixel’s value. [Figure
    16-2](#figure16-2) shows one way to do this.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将每个输入像素的RGB值合并为一个从0到1的单一数字，表示“黄色度”，这是输出像素的值。[图16-2](#figure16-2)展示了实现这一目标的一种方式。
- en: '![F16002](Images/F16002.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![F16002](Images/F16002.png)'
- en: 'Figure 16-2: Representing our yellow detector as a simple neuron'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-2：将我们的黄色检测器表示为一个简单的神经元
- en: This sure looks familiar. It has the same structure as an artificial neuron.
    When we interpret [Figure 16-2](#figure16-2) as a neuron, +1, +1, and −1 are the
    three weights, and the numbers associated with the color values are the three
    inputs. [Figure 16-3](#figure16-3) shows how to apply this neuron to any pixel
    in an image.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很熟悉。它与人工神经元的结构相同。当我们将[图16-2](#figure16-2)解释为一个神经元时，+1、+1和−1是三个权重，与颜色值相关的数字是三个输入。[图16-3](#figure16-3)显示了如何将这个神经元应用于图像中的任何像素。
- en: '![F16003](Images/F16003.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![F16003](Images/F16003.png)'
- en: 'Figure 16-3: Applying our neuron in [Figure 16-2](#figure16-2) to a pixel in
    an image'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-3：将[图16-2](#figure16-2)中的神经元应用于图像中的一个像素
- en: We can apply this operation to every pixel in the input, creating a single output
    value for every pixel. The result is a new tensor with the same width and height
    as the input, but only one channel, as shown in [Figure 16-4](#figure16-4).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个操作应用于输入中的每个像素，为每个像素创建一个单独的输出值。结果是一个新的张量，宽度和高度与输入相同，但只有一个通道，如[图16-4](#figure16-4)所示。
- en: '![F16004](Images/F16004.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![F16004](Images/F16004.png)'
- en: 'Figure 16-4: Applying our neuron in [Figure 16-3](#figure16-3) to each pixel
    in the input produces an output tensor with the same width and height, but only
    one channel.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-4：将[图16-3](#figure16-3)中的神经元应用于输入中的每个像素，产生一个宽度和高度相同但只有一个通道的输出张量。
- en: We often imagine applying the neuron to the upper-left pixel, then moving it
    one step at a time to the right until we reach the end of the row, then repeating
    this for the next row, and the next, until we reach the bottom right pixel. We
    say that we’re *sweeping* the neuron over the input, or *scanning* the input.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常设想将神经元应用于左上角的像素，然后一次向右移动，直到到达行的末尾，然后对下一行重复这个过程，直到到达右下角的像素。我们说我们正在对输入进行*扫描*，或*扫描*输入。
- en: '[Figure 16-5](#figure16-5) shows the result of this process on a picture of
    a yellow frog. As we intended, the more yellow that’s present in each input pixel,
    the more white we see in its corresponding output. We say that the neuron is *identifying*
    or *detecting* yellow in the input.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-5](#figure16-5)显示了此过程在一只黄色青蛙图像上的结果。正如我们预期的那样，输入像素中黄色的含量越多，输出中对应的白色也越多。我们说神经元正在*识别*或*检测*输入中的黄色。'
- en: '![F16005](Images/F16005.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![F16005](Images/F16005.png)'
- en: 'Figure 16-5: An application of our yellow-finding operation. The image on the
    right runs from black to white, depending on the yellowness of the corresponding
    source pixel in the left image.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-5：我们的黄色检测操作的应用。右侧的图像根据左侧图像中对应源像素的黄色程度，从黑到白变化。
- en: Of course there’s nothing special about yellow. We can build a little neuron
    to detect any color. When we use a neuron in this way, we often say that it is
    *filtering* the input. In this context, the weights are sometimes collectively
    called the *filter values* or just the *filter*. Inheriting language from their
    mathematical roots, the weights are also called the *filter kernel* or just the
    *kernel*. It’s also common to refer to the entire neuron as a filter. Whether
    the word *filter* refers to a neuron, or specifically to its weights, is usually
    clear from context.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，黄色并没有什么特别之处。我们可以构建一个小神经元来检测任何颜色。当我们以这种方式使用神经元时，我们通常说它在对输入进行*过滤*。在这种情况下，权重有时被统称为*过滤值*，或者简称*过滤器*。继承自其数学根源的语言，权重也被称为*过滤核*，或简称*核*。通常也会把整个神经元称作过滤器。是否将*过滤器*一词指代神经元，或者特指其权重，通常可以从上下文中得知。
- en: This operation of sweeping the filter over the input corresponds to a mathematical
    operation called *convolution* (Oppenheim and Nawab 1996). We say that the right
    side of [Figure 16-5](#figure16-5) is the result of convolution of the color image
    with the yellow-detecting filter. We also say that we *convolve* the image with
    the filter. Sometimes we combine these terms and refer to a filter (whether an
    entire neuron, or just its weights) as a *convolution filter*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种将过滤器扫过输入图像的操作对应于一种数学操作，称为*卷积*（Oppenheim和Nawab 1996）。我们说[图16-5](#figure16-5)的右侧是对彩色图像和黄色检测过滤器进行卷积的结果。我们还可以说我们将图像与过滤器*卷积*。有时我们将这些术语合并，称一个过滤器（无论是整个神经元，还是仅其权重）为*卷积过滤器*。
- en: Weight Sharing
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重共享
- en: In the last section, we imagined sweeping our neuron over the input image, performing
    exactly the same operation at every pixel. If we want to go faster, we can create
    a huge grid of identical neurons and apply them to all the pixels simultaneously.
    In other words, we process the pixels in parallel.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们设想了将我们的神经元扫过输入图像，在每个像素上执行完全相同的操作。如果我们想加速这一过程，可以创建一个巨大的相同神经元网格，并同时应用于所有像素。换句话说，我们并行处理这些像素。
- en: In this approach, every neuron has identical weights. Rather than repeating
    the same weights in a separate piece of memory for every neuron, we can imagine
    that the weights are stored in some shared piece of memory, as in [Figure 16-6](#figure16-6).
    We say that the neurons are *weight sharing*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，每个神经元具有相同的权重。我们不需要在每个神经元的独立内存中重复相同的权重，而是可以想象这些权重存储在某个共享的内存中，如[图16-6](#figure16-6)所示。我们说神经元是*共享权重*的。
- en: '![F16006](Images/F16006.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![F16006](Images/F16006.png)'
- en: 'Figure 16-6: We can apply our neuron to every pixel in the input simultaneously.
    Each neuron uses the same weights, found in a piece of shared memory.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-6：我们可以将我们的神经元同时应用于输入中的每个像素。每个神经元使用相同的权重，这些权重存储在共享内存中。
- en: This lets us save on memory. In our yellow detector example, weight sharing
    also makes it easy to change the color we’re detecting. Rather than change the
    weights in thousands of neurons (or more), we just change the one set in the shared
    memory.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够节省内存。在我们的黄色检测器示例中，共享权重还使得更换我们要检测的颜色变得容易。我们无需改变成千上万个神经元（或更多）的权重，只需更改共享内存中的那一组权重。
- en: We can actually implement this scheme on a GPU, which is capable of performing
    many identical sequences of operations at once. Weight sharing lets us save on
    precious GPU memory, freeing it up for other uses.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以在GPU上实现这个方案，GPU能够同时执行许多相同的操作序列。共享权重使我们能够节省宝贵的GPU内存，从而将其释放用于其他用途。
- en: Larger Filters
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更大的过滤器
- en: So far, we’ve been sweeping our neuron over the image (or applying it in parallel
    using weight sharing), processing one pixel at a time, using only that pixel’s
    values for input. In many situations, it’s also useful to look at the pixels near
    the one we’re processing. Usually we consider a pixel’s eight immediate *neighbors*.
    That is, we use the values in a little three by three box that’s centered on the
    pixel.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在将神经元扫过图像（或使用共享权重并行应用它），一次处理一个像素，仅使用该像素的值作为输入。在许多情况下，查看我们正在处理的像素周围的像素也是有用的。通常我们会考虑一个像素的八个直接*邻居*。也就是说，我们使用一个以该像素为中心的三乘三的小框中的值。
- en: '[Figure 16-7](#figure16-7) shows three different operations we can apply using
    a three by three block of numbers in this way: blurring, detecting horizontal
    edges, and detecting vertical edges.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-7](#figure16-7)展示了我们可以通过这种方式使用三乘三数字块进行的三种不同操作：模糊处理、检测水平边缘和检测垂直边缘。'
- en: To compute each image, we center the block of weights over each pixel in turn
    and multiply each of the nine values under it by the corresponding weight. We
    add up the results and use their sum as the output value for that pixel. Let’s
    see how to implement this process with a neuron.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算每个图像，我们将权重块依次放置在每个像素上，并将其下方的九个值与相应的权重相乘。然后将结果加起来，并将它们的和作为该像素的输出值。让我们看看如何用神经元来实现这个过程。
- en: '![F16007](Images/F16007.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![F16007](Images/F16007.png)'
- en: 'Figure 16-7: Processing a grayscale image of the frog in [Figure 16-5](#figure16-5)
    by moving a three by three template of numbers over the image. From left to right,
    we blur the image, find horizontal edges, and find vertical edges.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-7：通过将三乘三数字模板在图像上移动来处理灰度图像中的青蛙（见[图16-5](#figure16-5)）。从左到右，我们对图像进行模糊处理，找到水平边缘，并找到垂直边缘。
- en: For simplicity, we’ll stick with a grayscale input for now. We can think of
    the blocks of numbers in [Figure 16-7](#figure16-7) as weights, or filter kernels.
    In this scenario, we have a grid of nine weights that we place over a grid of
    nine pixel values. Each pixel value is multiplied by its corresponding weight,
    the results are summed up and run through an activation function, and we have
    our output. [Figure 16-8](#figure16-8) shows the idea.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们暂时保持灰度输入。我们可以将[图16-7](#figure16-7)中的数字块视为权重或滤波器核。在这种情况下，我们有一个九个权重的网格，将其放置在九个像素值的网格上。每个像素值都与其相应的权重相乘，结果相加并通过激活函数，我们就得到了输出。[图16-8](#figure16-8)展示了这个概念。
- en: '![F16008](Images/F16008.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![F16008](Images/F16008.png)'
- en: 'Figure 16-8: Processing a grayscale input (red) with a three by three filter
    (blue)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-8：使用三乘三滤波器（蓝色）处理灰度输入（红色）
- en: This figure shows how to process a single pixel (shown in dark red). We center
    the filter over the intended pixel and multiply each of the nine values in the
    input with its corresponding filter value. We add up all nine results and pass
    that sum through an activation function.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图展示了如何处理单个像素（显示为深红色）。我们将滤波器集中在目标像素上，并将输入中的每个九个值与其相应的滤波器值相乘。我们将这九个结果相加，并将总和通过激活函数。
- en: The shape of the pixels that form a neuron’s input in this scheme is called
    that neuron’s *local receptive field*, or more simply its *footprint*. In [Figure
    16-8](#figure16-8), the neuron’s footprint is a square, three pixels on a side.
    In our yellow detector, the footprint was a single pixel. When a filter’s footprint
    is larger than a single pixel, we sometimes emphasize that quality by calling
    a *spatial filter*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，构成神经元输入的像素形状被称为该神经元的*局部感受野*，或者更简单地称为它的*足迹*。在[图16-8](#figure16-8)中，神经元的足迹是一个正方形，边长为三个像素。在我们的黄色探测器中，足迹是一个单独的像素。当滤波器的足迹大于单个像素时，我们有时会通过称之为*空间滤波器*来强调这一特性。
- en: Note that the neuron in [Figure 16-8](#figure16-8) is just like any other neuron.
    It receives nine numbers as inputs, multiplies each one by its corresponding weight,
    adds the results together, and passes that number through an activation function.
    It doesn’t know or care that these nine numbers are coming from a square region
    of the input, or even that they’re coming from an image.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[图16-8](#figure16-8)中的神经元就像其他任何神经元一样。它接收九个数字作为输入，将每个数字与相应的权重相乘，将结果加在一起，并通过激活函数传递这个数字。它并不在乎这九个数字来自输入的一个正方形区域，甚至不在乎它们来自于一张图像。
- en: We apply this three by three filter to an image by convolving it with the image,
    just as before, by sweeping it over each pixel in turn. For each input pixel,
    we imagine centering the three by three grid of weights over that pixel, applying
    the neuron, and creating a single output value, as in [Figure 16-9](#figure16-9).
    We say that the pixel we’re centering the filter over is the *anchor* (or the
    *reference point* or *zero point*).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过与图像进行卷积来应用这个三乘三滤波器，就像之前一样，依次将其扫描过每个像素。对于每个输入像素，我们想象将三乘三的权重网格放置在该像素上，应用神经元并创建一个单一的输出值，如[图16-9](#figure16-9)所示。我们称我们正在将滤波器集中在其上的像素为*锚点*（或*参考点*或*零点*）。
- en: '![F16009](Images/F16009.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![F16009](Images/F16009.png)'
- en: 'Figure 16-9: Applying a three by three filter (center) to a grayscale image
    (left), creating a new single-channel image (right)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-9：将三乘三滤波器（中心）应用于灰度图像（左），生成新的单通道图像（右）
- en: We can design our filters to have footprints of any size and shape we like.
    In practice, small sizes are most common, since they are faster to evaluate than
    larger footprints. We usually use small squares with an odd number of pixels on
    each side (often between one and nine). Such squares let us place the anchor in
    the center of the footprint. This keeps everything symmetrical and easier to understand.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以设计任何大小和形状的滤波器。实际上，小尺寸的滤波器最为常见，因为它们比大尺寸的滤波器更快速地进行计算。我们通常使用每边像素数为奇数的小方块（通常是1到9之间）。这样的方块使我们能够将锚点放在滤波器中心，这样可以保持对称，且更易于理解。
- en: Let’s put this into practice. [Figure 16-10](#figure16-10) shows the result
    of convolving a seven by seven input with a three by three filter. Note that if
    we were to center the filter over the input’s corners or edges, the filter’s footprint
    would extend beyond the input, and the neuron would require input values that
    aren’t present. We address this a little later. For now, let’s just limit ourselves
    to those locations where the filter sits entirely on top of the image. That means
    that the output image is only five by five.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把这个理论应用到实践中。[图16-10](#figure16-10)显示了将一个七乘七的输入图像与一个三乘三的滤波器进行卷积的结果。请注意，如果我们将滤波器置于输入图像的角落或边缘，滤波器的覆盖区域会超出输入图像，神经元将需要一些不存在的输入值。我们稍后会处理这个问题。现在，我们先限制讨论那些滤波器完全覆盖图像位置的情况。这样，输出图像的大小就是五乘五。
- en: We motivated our discussion by looking at spatial filters that can do things
    like blur an image or detect edges. But why are such things useful for deep learning?
    To answer this, let’s look at filters more closely.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过观察空间滤波器来引出讨论，这些滤波器可以实现像模糊图像或检测边缘这样的功能。那么，这些功能为什么对深度学习有用呢？为了回答这个问题，让我们更仔细地看一下滤波器。
- en: '![F16010](Images/F16010.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![F16010](Images/F16010.png)'
- en: 'Figure 16-10: To convolve an image with a filter, we move the filter across
    the image and apply it at each position. We’re skipping corners and edges for
    this figure.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-10：为了将滤波器与图像进行卷积，我们将滤波器在图像上移动，并在每个位置应用它。我们在这个图中略去了角落和边缘。
- en: Filters and Features
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 滤波器与特征
- en: Some biologists who study toads think that certain cells in the animal’s visual
    system are sensitive to specific types of visual patterns (Ewert et al. 1985).
    The theory is that a toad is looking for particular shapes corresponding to the
    creatures it likes to eat and to certain motions that those animals make. People
    used to think that a toad’s eyes absorbed all the light that struck them, sent
    that mass of information to the brain, and it was the brain’s job to sift among
    the results looking for food. The new hypothesis is that the cells in the eye
    are doing some early steps in this detection process (such as finding edges) all
    by themselves, and they only fire and pass on information to the brain if they
    “think” they’re looking at prey.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究蟾蜍的生物学家认为，动物视觉系统中的某些细胞对特定类型的视觉模式敏感（Ewert 等，1985年）。这个理论认为，蟾蜍在寻找与它喜欢吃的生物相关的特定形状，以及这些动物所做的某些动作。人们曾认为，蟾蜍的眼睛会吸收所有照射到它们的光，将这些信息传送到大脑，然后由大脑从结果中筛选寻找食物。而新的假设认为，眼睛中的细胞在这个检测过程中会自行完成一些早期步骤（如寻找边缘），只有当它们“认为”看到的是猎物时，才会发射信号并将信息传递给大脑。
- en: The theory has been extended to the human visual system, where it has led to
    the surprising hypothesis that some individual neurons are so precisely fine-tuned
    that they only fire in response to pictures of specific people. The original study
    that led to this suggestion showed people 87 different images, including people,
    animals, and landmarks. In one volunteer they found a specific neuron that only
    fired when the volunteer was shown a photo of the actress Jennifer Aniston (Quiroga
    2005). Even more curiously, that neuron only fired when Aniston was alone, and
    not when she was pictured together with other people, including famous actors.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该理论已扩展到人类视觉系统，进而提出了一个令人惊讶的假设：某些单一的神经元被精确地调节，只对特定人的图片作出反应。导致这一建议的最初研究展示了87张不同的图像，其中包括人类、动物和地标。在一位志愿者身上，他们发现了一个只在志愿者看到女演员詹妮弗·安妮斯顿的照片时才会激活的神经元（Quiroga
    2005年）。更有趣的是，这个神经元只会在安妮斯顿单独出现时激活，而在她与其他人（包括著名演员）合影时并不激活。
- en: The idea that our neurons are precision pattern-matching devices is not universally
    accepted, but we’re not doing real neuroscience and biology here. We’re just looking
    for inspiration. And this idea of letting neurons perform detection work seems
    like some pretty great inspiration.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的神经元是精确的模式匹配设备这一观点并未得到普遍接受，但我们这里并不是在进行真正的神经科学和生物学研究。我们只是寻找灵感。而让神经元执行检测工作的这一想法，似乎是一个相当棒的灵感。
- en: The connection to convolution is that we can use filters to simulate the cells
    in the toad’s eyes. Our filters also pick out specific patterns and then pass
    on their discoveries to later filters that look for even bigger patterns. Some
    of the terminology we use for this process echoes terms that we’ve seen before.
    Specifically, we’ve been using the word *feature* to refer to one of the values
    contained in a sample. But in this context, the word *feature* also refers to
    a particular structure in an input that the filter is trying to detect, like an
    edge, a feather, or scaly skin. We say that a filter is *looking for* a stripe
    feature, or eyeglasses, or a sports car. Continuing this usage, the filters themselves
    are sometimes called *feature detectors*. When a feature detector has been swept
    over an entire input, we say that its output is a *feature map* (the word *map*
    in this context comes from mathematical language). The feature map tells us, pixel
    by pixel, how well the image around that pixel matched what the filter was looking
    for.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与卷积的关联在于，我们可以使用滤波器来模拟蟾蜍眼睛中的细胞。我们的滤波器同样能够挑选出特定的模式，并将其发现传递给后续滤波器，后者寻找更大的模式。我们在这个过程中使用的一些术语，回响了我们之前见过的术语。具体来说，我们一直在使用*特征*这个词来指代样本中的一个值。但在这个上下文中，*特征*一词也指输入中的一个特定结构，滤波器试图检测到它，比如边缘、羽毛或鳞片皮肤。我们说一个滤波器在*寻找*条纹特征、眼镜或跑车。延续这一用法，滤波器本身有时被称为*特征检测器*。当特征检测器扫描完整个输入后，我们称其输出为*特征图*（此处的*图*一词来源于数学语言）。特征图告诉我们，逐像素地，图像中该像素周围的内容与滤波器所寻找的匹配程度。
- en: Let’s see how feature detection works. In [Figure 16-11](#figure16-11) we show
    the process of using a filter to find short, isolated vertical white stripes in
    a binary image.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看特征检测是如何工作的。在[图16-11](#figure16-11)中，我们展示了使用滤波器在二值图像中找到短的、孤立的垂直白色条纹的过程。
- en: '![F16011](Images/F16011.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![F16011](Images/F16011.png)'
- en: 'Figure 16-11: 2D pattern matching with convolution. (a) The filter. (b) The
    input. (c) The feature map, scaled to [0, 1] for display. (d) Feature map entries
    with value 3\. (e) Neighborhoods of (b) around the white pixels in (d).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-11：使用卷积进行二维模式匹配。（a）滤波器。（b）输入。（c）特征图，已缩放至[0, 1]以便显示。（d）特征图中值为3的条目。（e）（d）中白色像素周围的（b）邻域。
- en: '[Figure 16-11](#figure16-11)(a) shows a three by three filter with values −1
    (black) and 1 (white). [Figure 16-11](#figure16-11)(b) shows a noisy input image,
    consisting only of black and white pixels. [Figure 16-11](#figure16-11)(c) shows
    the result of applying the filter to each pixel in the input image (except for
    the outermost border). Here the values range from −6 to +3, which we scaled to
    [0, 1] for display. The larger the value in this image, the better the match between
    the filter and the pixel (and its neighborhood). A value of +3 means the filter
    matched the image perfectly at that pixel.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-11](#figure16-11)(a) 显示了一个三乘三的滤波器，值为−1（黑色）和1（白色）。[图16-11](#figure16-11)(b)
    显示了一个噪声输入图像，仅包含黑白像素。[图16-11](#figure16-11)(c) 显示了将滤波器应用于输入图像中每个像素的结果（外部边框除外）。这里的值从−6到+3，我们将其缩放至[0,
    1]以便显示。图像中值越大，滤波器与像素（及其邻域）之间的匹配度越好。值为+3表示该像素与滤波器完美匹配。'
- en: '[Figure 16-11](#figure16-11)(d) shows a thresholded version of [Figure 16-11](#figure16-11)(c),
    where pixels with a value of +3 are shown in white, and all others are black.
    Finally, [Figure 16-11](#figure16-11)(e) shows the noisy image of [Figure 16-11](#figure16-11)(b)
    with the three by three grid of pixels around the white pixels in [Figure 16-11](#figure16-11)(d)
    highlighted. We can see that the filter found those places in the image where
    the pixels matched the filter’s pattern.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-11](#figure16-11)(d) 显示了[图16-11](#figure16-11)(c)的阈值版本，其中值为+3的像素显示为白色，其他所有像素为黑色。最后，[图16-11](#figure16-11)(e)
    显示了[图16-11](#figure16-11)(b)的噪声图像，并突出显示了[图16-11](#figure16-11)(d)中白色像素周围的三乘三像素网格。我们可以看到，滤波器找到了图像中与滤波器模式匹配的那些位置。'
- en: Let’s see why this worked. In the top row of [Figure 16-12](#figure16-12) we
    show our filter and a three by three patch of the image, along with the pixel-by-pixel
    results.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看为什么这样有效。在[图16-12](#figure16-12)的顶部行中，我们展示了滤波器和图像的三乘三块区域，以及逐像素的结果。
- en: '![F16012](Images/F16012.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![F16012](Images/F16012.png)'
- en: 'Figure 16-12: Applying a filter to two image fragments. From left to right,
    each row shows the filter, an input, and the result. The final number is the sum
    of the rightmost three by three grid.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-12：将滤波器应用于两个图像片段。从左到右，每一行显示滤波器、输入和结果。最后一个数字是右侧三乘三网格的总和。
- en: Consider the pixels shown in the middle of the top row. The black pixels (shown
    in gray here), with a value of 0, don’t contribute to the output. The white pixels
    (shown in light yellow here), with a value of 1, get multiplied by either 1 or
    –1, depending on the filter value. In the top row of pixels, only one of the white
    pixels (the top center) is matched by a 1 in the filter. This gives a result of
    1 × 1 = 1\. The other three white pixels are matched up with −1, giving three
    results of −1 × 1 = −1\. Adding these gives us −3 + 1 = −2.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑顶部行中间显示的像素。黑色像素（这里显示为灰色），值为0，不对输出产生影响。白色像素（这里显示为浅黄色），值为1，根据滤波器的值乘以1或-1。在顶部行的像素中，只有一个白色像素（顶部中央）与滤波器中的1匹配。这给出了结果1
    × 1 = 1。其他三个白色像素与-1匹配，给出了三个结果-1 × 1 = -1。将这些加在一起，我们得到-3 + 1 = -2。
- en: In the lower row, our image matches the filter. All three weights of 1 on the
    filter are sitting on white pixels, and there are no other white pixels in the
    input. The result is a score of 3, indicating a perfect match.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在下方这一行中，我们的图像与滤波器匹配。滤波器上三个权重为1的部分正好位于白色像素上，输入图像中没有其他白色像素。结果是一个3的分数，表示完全匹配。
- en: '[Figure 16-13](#figure16-13) shows another filter, this time looking for diagonals.
    Let’s run it over the same image. This diagonal of three white pixels surrounded
    by black is present in two places.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-13](#figure16-13)展示了另一个滤波器，这次是寻找对角线。让我们在同一图像上运行它。这个由三个白色像素组成的对角线被黑色像素包围，它在两个地方出现。'
- en: '![F16013](Images/F16013.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![F16013](Images/F16013.png)'
- en: 'Figure 16-13: Another filter and its result on our random image. (a) The filter.
    (b) The input. (c) The feature map. (d) Feature map entries with value 3\. (e)
    Neighborhoods of (b) around the white pixels in (d).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-13：另一个滤波器及其在随机图像上的结果。(a) 滤波器。(b) 输入。(c) 特征图。(d) 特征图中值为3的条目。(e) (d)中白色像素周围的邻域。
- en: By sweeping a filter over the image and computing the output value at each pixel,
    we can hunt for lots of different simple patterns. In practice, our filter and
    pixel values are all real numbers (not just 0 and 1), so we can make much more
    complex patterns that find more complex features (Snavely 2013).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在图像上滑动滤波器并计算每个像素的输出值，我们可以寻找许多不同的简单模式。实际上，我们的滤波器和像素值都是实数（而不仅仅是0和1），因此我们可以制作出更复杂的模式，找到更复杂的特征（Snavely
    2013）。
- en: If we take the output of a set of filters and feed them to another set of filters,
    we can look for patterns of patterns. If we feed that second set of outputs to
    a third set of filters, we can look for patterns of patterns of patterns. This
    process lets us build up from, say, a collection of edges, to a set of shapes,
    such as ovals and rectangles, to ultimately matching a pattern corresponding to
    some specific object, such as a guitar or bicycle.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将一组滤波器的输出传递给另一组滤波器，我们可以寻找模式中的模式。如果我们将第二组输出传递给第三组滤波器，我们可以寻找模式中的模式中的模式。这个过程让我们能够从一组边缘开始，构建一组形状，如椭圆和矩形，最终匹配某个特定物体的模式，如吉他或自行车。
- en: Applying successive groups of filters in this way, in concert with another technique
    we will soon discuss called *pooling*, enormously expands the sorts of patterns
    that we can detect. The reason is that the filters operate *hierarchically*, where
    each filter’s patterns are combinations of the patterns found by earlier filters.
    Such a hierarchy allows us to look for features of great complexity, such as the
    face of a friend, the grain of a basketball, or the eye on the end of a peacock’s
    feather.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式应用连续的滤波器组，再结合我们很快将讨论的另一种技术——*池化*，极大地扩展了我们能够检测的模式种类。原因在于，滤波器以*层级*方式工作，其中每个滤波器的模式都是前一个滤波器找到的模式的组合。这样的层级结构让我们能够寻找复杂度较高的特征，如朋友的面孔、篮球的纹理，或是孔雀羽毛末端的眼睛。
- en: If we had to work out these filters by hand, classifying images would be impractical.
    What are the proper weights in a chain of eight filters that tell us if a picture
    shows a kitten or an airplane? How could we even go about working out that problem?
    And how would we know when we found the best filters? In Chapter 1 we discussed
    expert systems, in which people tried to do this kind of feature engineering by
    hand. It’s a formidable task for simple problems, and it grows in complexity so
    quickly that really interesting problems, such as distinguishing cats from airplanes,
    seem entirely out of reach.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们必须手动计算这些滤波器，图像分类将变得不切实际。在一连串八个滤波器中，如何确定合适的权重来告诉我们一张图片是小猫还是飞机？我们又该如何去解决这个问题呢？我们又怎么知道何时找到了最好的滤波器？在第一章中，我们讨论了专家系统，人们曾试图通过手动进行这种特征工程。对于简单问题来说，这是一个艰巨的任务，且复杂性增长非常迅速，真正有趣的问题，比如区分猫和飞机，似乎完全无法解决。
- en: The beauty of CNNs is that they carry out the goals of expert systems, but we
    don’t have to figure out the values of the filters by hand. The learning process
    that we’ve seen in previous chapters, involving measuring error, backpropagating
    the gradients, and then improving the weights, teaches a CNN to find the filters
    it needs. The learning process modifies the kernel of each filter (that is, the
    weights in each neuron), until the network is producing results that match our
    targets. In other words, training tunes the values in the filters until they find
    the features that enable it to come up with the right class for the object in
    the image. And this can happen for hundreds or even thousands of filters, all
    at once.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）的美妙之处在于，它们实现了专家系统的目标，但我们无需手动计算滤波器的值。我们在前几章中看到的学习过程，包括测量误差、反向传播梯度以及改进权重，教会了
    CNN 自行找到所需的滤波器。学习过程修改了每个滤波器的核心（即每个神经元中的权重），直到网络产生符合我们目标的结果。换句话说，训练过程调节滤波器中的值，直到它们找到能够帮助网络为图像中的物体分类的特征。这一过程可以在数百个甚至数千个滤波器中同时发生。
- en: This can seem like magic. Starting with random numbers, the system learns what
    patterns it needs to look for in order to distinguish a piano from an apricot
    from an elephant, and then it learns what numbers to put into the filter kernels
    in order to find those patterns.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来像是魔法。系统从随机数开始，学习需要寻找的模式，以区分钢琴、杏子和大象，然后学习如何将数字放入滤波器内核，以便找到这些模式。
- en: That this process can even come close in one situation is remarkable. The fact
    that it often produces highly accurate results in a vast range of applications
    is one of the great discoveries in deep learning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程在某些情况下能够接近完成，实属不易。它在广泛的应用中经常产生高度准确的结果，这是深度学习领域的伟大发现之一。
- en: Padding
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 填充
- en: Earlier, we promised to return to the issue of what happens when a convolution
    filter is centered over an element in a corner or on an edge of an input tensor.
    Let’s look at that now.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们承诺会回到卷积滤波器位于输入张量的角落或边缘时会发生什么问题。现在让我们来看一下。
- en: Suppose that we want to apply a 5 by 5 filter to a 10 by 10 input. If we’re
    somewhere in the middle of the tensor, as in [Figure 16-14](#figure16-14), then
    our job is easy. We pull out the 25 values from the input, and apply them to the
    convolution filter.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想对一个 10x10 的输入应用一个 5x5 的滤波器。如果我们位于张量的中间位置，如[图 16-14](#figure16-14)所示，那么我们的工作就很简单。我们从输入中提取出
    25 个值，并将它们应用到卷积滤波器上。
- en: '![F16014](Images/F16014.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![F16014](Images/F16014.png)'
- en: 'Figure 16-14: A five by five filter located somewhere in the middle of a tensor.
    The bright red pixel is the anchor, while the lighter ones make up the receptive
    field.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16-14：一个位于张量中部的 5x5 滤波器。鲜红色的像素是锚点，而较浅的像素组成了感受野。
- en: But what if we’re on, or near, an edge, as in [Figure 16-15](#figure16-15)?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果我们位于边缘上，或者接近边缘，如[图 16-15](#figure16-15)所示，会怎么样呢？
- en: '![F16015](Images/F16015.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![F16015](Images/F16015.png)'
- en: 'Figure 16-15: Near the edge, the filter’s receptive field can fall off the
    side of the input. What values do we use for these missing elements?'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16-15：在边缘附近，滤波器的感受野可能会超出输入的边界。我们该如何处理这些缺失的元素呢？
- en: The footprint of the filter is hanging off the edge of the input. There aren’t
    any input elements there. How do we compute an output value for the filter when
    it’s missing some of its inputs?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 滤波器的足迹悬挂在输入的边缘。那里没有输入元素。那我们如何计算滤波器的输出值，当它缺少一些输入时呢？
- en: We have a few choices. One is to disallow this case so we can only place the
    footprint where it is entirely within the input image. The result is an output
    that’s smaller in height and width. [Figure 16-16](#figure16-16) shows this idea.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有几种选择。一种是禁止这种情况，只能将足迹放置在完全位于输入图像内部的位置。结果是输出的高度和宽度会变小。[图16-16](#figure16-16)展示了这一想法。
- en: While simple, this is a lousy solution. We said that we often apply many filters
    in sequence. If we sacrificed one or more rings of elements each time, we would
    lose information with every step we take through the network.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管简单，但这是一个糟糕的解决方案。我们曾说过，我们通常会依次应用多个滤波器。如果每次都牺牲一个或多个元素环，那么我们在每一步通过网络时都会丧失信息。
- en: '![F16016](Images/F16016.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![F16016](Images/F16016.png)'
- en: 'Figure 16-16: We can avoid the “falling off the edge” problem by never letting
    our filter get that far. With a 5 by 5 filter, we can only center the filter over
    the elements marked here in blue, reducing our 10 by 10 input to a 6 by 6 output.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-16：我们可以通过不让滤波器跑到那么远来避免“掉出边缘”问题。使用5x5的滤波器时，我们只能将滤波器集中在这里标记为蓝色的元素上，将10x10的输入图像缩小为6x6的输出图像。
- en: A popular alternative is to use a technique called *padding,* which lets us
    create an output image of the same width and height as the input. The idea is
    that we add a border of extra elements around the outside of the input, as in
    [Figure 16-17](#figure16-17). All of these elements have the same value. If we
    place zeros in all the new elements, we call the technique *zero-padding*. In
    practice, we almost always use zeros, so people often refer to zero-padding as
    merely padding, with the understanding that if they mean to use any value other
    than zero, they say so explicitly.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的替代方法是使用一种称为*填充*的技术，这可以让我们创建一个与输入具有相同宽度和高度的输出图像。其思想是，在输入的外侧添加一个额外元素的边框，如[图16-17](#figure16-17)所示。所有这些元素都有相同的值。如果我们在所有新元素中放置零，这种技术被称为*零填充*。在实践中，我们几乎总是使用零，因此人们通常将零填充称为填充，理解为如果他们打算使用零以外的任何值，他们会明确说明。
- en: '![F16017](Images/F16017.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![F16017](Images/F16017.png)'
- en: 'Figure 16-17: A better way to solve the “falling off the edge” problem is to
    add padding, or extra elements (in light blue), around the border of the input.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-17：解决“掉出边缘”问题的更好方法是添加填充或额外的元素（浅蓝色），围绕输入的边界。
- en: The thickness of the border depends on the size of the filter. We usually use
    just enough padding so that the filter can be centered on every element of the
    input. Every filter needs to have its input padded if we don’t want to lose information
    from the sides.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 边界的厚度取决于滤波器的大小。我们通常只使用足够的填充，以便滤波器可以集中在输入的每个元素上。如果我们不希望从两侧丢失信息，每个滤波器都需要对其输入进行填充。
- en: Most deep learning libraries automatically calculate the necessary amount of
    padding so that our output has the same width and height as our input, and apply
    it for us as a default.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度学习库会自动计算所需的填充量，以确保我们的输出与输入具有相同的宽度和高度，并将其作为默认设置应用。
- en: Multidimensional Convolution
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多维卷积
- en: So far in this chapter, we’ve mostly been considering grayscale images with
    only one channel of color information. We know that most color images have three
    channels, representing the red, green, and blue components of each pixel. Let’s
    see how to handle those. Once we can work with images with three channels, we
    can work with tensors of any number of channels.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们主要考虑的是只有一个通道颜色信息的灰度图像。我们知道，大多数彩色图像有三个通道，表示每个像素的红、绿、蓝分量。让我们看看如何处理这些图像。一旦我们能够处理具有三个通道的图像，就能处理任何通道数的张量。
- en: To process an input with multiple channels, our filters (which can have any
    footprint) need to have an identical number of channels. That’s because each value
    in the input needs to have a corresponding value in the filter. For an RGB image,
    a filter needs three channels. So, a filter with a footprint of three by three
    needs to have three channels, for a total of 27 numbers, as shown in [Figure 16-18](#figure16-18).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理具有多个通道的输入，我们的滤波器（其足迹可以是任何形状）需要具有相同数量的通道。这是因为输入中的每个值都需要在滤波器中有一个相应的值。对于RGB图像，一个滤波器需要三个通道。因此，一个三乘三的滤波器需要三个通道，共27个数字，如[图16-18](#figure16-18)所示。
- en: '![F16018](Images/F16018.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![F16018](Images/F16018.png)'
- en: 'Figure 16-18: A three-channel filter with a three by three footprint. We’ve
    colored the values to show which input channel’s values they will multiply.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-18：一个具有三行三列足迹的三通道滤波器。我们已经对数值进行了着色，以显示它们将与哪个输入通道的数值相乘。
- en: To apply this kernel to a three-channel color image, we proceed much as before,
    but now we think in terms of blocks (or tensors of three dimensions).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take the filter of [Figure 16-18](#figure16-18), with a three by three
    footprint and three channels, and use it to process an RGB image with three color
    channels. For each input pixel, we center the filter’s footprint over that pixel
    as before, and match up each of the 27 numbers in the image with the 27 numbers
    in the filter, as in [Figure 16-19](#figure16-19).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![F16019](Images/F16019.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-19: Convolving an RGB image with a three by three by three kernel.
    We can imagine that each channel is filtered by its own channel in the filter.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 16-19](#figure16-19), our input has three channels, so our filter
    has three channels as well. It may be helpful to think of the red, green, and
    blue channels as each getting filtered by their corresponding channel in the filter,
    as shown in [Figure 16-19](#figure16-19). In practice, we treat the input and
    the filter as three by three by three blocks, and each of the 27 input values
    get multiplied with its corresponding filter value.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'This idea generalizes to any number of channels. In order to make sure that
    every input value has a corresponding filter value, we can state the necessary
    property as a rule: every filter must have the same number of channels as the
    tensor it’s filtering.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Filters
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve been applying a single filter at a time, but that’s rare in practice.
    Usually we bundle up tens or hundreds of filters into one *convolution layer*
    and apply them all simultaneously (and independently) to that layer’s input.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: To see the general picture, imagine that we’ve been given a black-and-white
    image, and we want to look for several low-level features in the pixels, such
    as vertical stripes, horizontal stripes, isolated dots, and plus signs. We can
    create one filter for each of these features and run each one over the input independently.
    Each filter produces an output image with one channel. Combining the four outputs
    gives us one tensor with four channels. [Figure 16-20](#figure16-20) shows the
    idea.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![F16020](Images/F16020.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-20: We can run multiple filters (in color) over the same input (in
    gray). Each filter creates its own channel in the output. They are then combined
    to create a single element in the output tensor with four channels.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Instead of a grayscale image with one channel, or a color image with three channels,
    we now have an output tensor with four channels. If we used seven filters, then
    the output is a new image with seven channels. The key thing to note here is that
    the output tensor has one channel for each filter that’s applied.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, our filters can have any footprint, and we can apply as
    many of them as we like to any input image. [Figure 16-21](#figure16-21) shows
    this idea.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![F16021](Images/F16021.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-21: When we convolve filters with an input, each filter must have
    as many channels as the input. The output tensor has one channel for each filter.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-21：当我们将过滤器与输入进行卷积时，每个过滤器的通道数必须与输入的通道数相同。输出张量的通道数与过滤器的数量相同。
- en: The input tensor at the far left has seven channels. We’re applying four different
    filters, each with a three by three footprint, so each filter is a tensor of size
    three by three by seven. The output of each filter is a feature map of a single
    channel. The output tensor is what we get from stacking these four feature maps,
    so it has four channels.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最左边的输入张量有七个通道。我们应用四个不同的过滤器，每个过滤器的尺寸是3x3，因此每个过滤器的张量大小是3x3x7。每个过滤器的输出是一个单通道的特征图。输出张量是通过堆叠这四个特征图得到的，因此它有四个通道。
- en: Although in principle each filter we apply can have a different footprint, in
    practice we almost always use the same footprint for every filter in any given
    convolution layer. For example, in [Figure 16-21](#figure16-21) all the filters
    have a footprint of three by three.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然原则上我们应用的每个过滤器可以有不同的尺寸，但实际上，我们几乎总是为任何给定的卷积层使用相同的过滤器尺寸。例如，在[图16-21](#figure16-21)中，所有的过滤器的尺寸都是3x3。
- en: Let’s gather together the two numerical rules from the previous section and
    this one. First, every filter in a convolution layer must have the same number
    of channels as that layer’s input tensor. Second, a convolution layer’s output
    tensor will have as many channels as there are filters in the layer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把上一节和这一节中的两个数值规则汇总起来。首先，卷积层中的每个过滤器必须与该层的输入张量具有相同的通道数。其次，卷积层的输出张量将具有与该层中过滤器数量相等的通道数。
- en: Convolution Layers
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积层
- en: Let’s take a closer look at the mechanics of convolution layers. A convolution
    layer is simply a bunch of filters gathered together. They’re applied independently
    to the input tensor, as in [Figure 16-21](#figure16-21), and their outputs are
    combined to create a new output tensor. The input is not changed by this process.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看卷积层的机制。卷积层实际上是将多个过滤器组合在一起。它们独立地应用于输入张量，如[图16-21](#figure16-21)所示，然后它们的输出被组合起来，生成一个新的输出张量。输入在这个过程中没有发生变化。
- en: When we create a convolution layer in code, we typically tell our library how
    many filters we want, what their footprint should be, and other optional details
    like whether we want to use padding and what activation function we want to use—the
    library takes care of all the rest. Most importantly, training improves the kernel
    values in each filter, so that the filters learn the values that enable them to
    produce the best results.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在代码中创建一个卷积层时，我们通常会告诉库我们需要多少个过滤器、它们的尺寸应该是多少，以及其他可选的细节，比如是否使用填充以及我们希望使用什么激活函数——其余的由库来处理。最重要的是，训练过程中会改善每个过滤器的核值，使过滤器学习到能够产生最佳结果的值。
- en: When we draw a diagram of a deep learner, we usually label our convolution layers
    with how many filters are used, their footprints, and their activation function.
    Since it’s common to use the same padding all around the input, we often just
    provide a single value rather than two, with the understanding that it applies
    to both width and height.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们绘制深度学习模型的图示时，通常会标注卷积层使用了多少个过滤器、它们的尺寸以及它们的激活函数。由于在输入的周围常常使用相同的填充，我们通常只提供一个值，而不是两个，默认它适用于宽度和高度。
- en: Like the weights in fully connected layers, the values in a convolution layer’s
    filters start out with random values and are improved with training. Also like
    fully connected layers, if we’re careful about choosing these random initial values,
    training usually goes faster. Most libraries offer a variety of initialization
    methods. Generally speaking, the built-in defaults normally work fine, and we
    rarely need to explicitly choose an initialization algorithm.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 就像全连接层中的权重一样，卷积层中过滤器的值最初是随机的，并通过训练得到改善。同样，像全连接层一样，如果我们在选择这些随机初始值时小心一些，训练通常会更快。大多数库提供多种初始化方法。一般来说，内置的默认值通常能很好地工作，我们很少需要明确选择初始化算法。
- en: If we do want to pick a method, the He algorithm is a good first choice (He
    et al. 2015; Karpathy 2016). If that’s not available, or doesn’t work well in
    a given situation, Glorot is a good second choice (Glorot and Bengio 2010).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们确实想选择一种方法，He算法是一个不错的首选（He et al. 2015; Karpathy 2016）。如果该方法不可用，或者在特定情况下效果不好，Glorot算法是一个不错的第二选择（Glorot
    和 Bengio 2010）。
- en: Let’s look at a couple of special types of convolution that have their own names.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看几种有自己名字的特殊卷积类型。
- en: 1D Convolution
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一维卷积
- en: An interesting special case of sweeping a filter over an input is called *1D
    convolution*. Here we sweep over the input as usual in either height or width,
    but not the other (Snavely 2013). This is a popular technique when working with
    text, which can be represented as a grid where each element holds a single letter,
    and rows contain complete words (or a fixed number of letters) (Britz 2015).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的特殊情况是对输入进行滤波的过程，称为*1D 卷积*。在这里，我们像往常一样对输入进行扫描，但只沿高度或宽度方向，而不在另一个方向上进行扫描（Snavely
    2013）。当处理文本时，这是一种常见的技术，文本可以表示为一个网格，每个元素包含一个字母，行包含完整的单词（或固定数量的字母）（Britz 2015）。
- en: The basic idea is shown in [Figure 16-22](#figure16-22).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思路见[图 16-22](#figure16-22)。
- en: '![F16022](Images/F16022.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![F16022](Images/F16022.png)'
- en: 'Figure 16-22: An example of 1D convolution. The filter only moves downward.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16-22：1D 卷积示例。滤波器只向下移动。
- en: Here, we’ve created a filter that is the entire width of the input and two rows
    high. The first application of the filter processes everything in the first two
    rows. Then we move the filter down and process the next two rows. We don’t move
    the filter horizontally. The name *1D convolution* comes from this single direction,
    or dimension, of movement.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个宽度与输入相同且高度为两行的滤波器。滤波器的第一次应用处理前两行的所有内容。然后，我们将滤波器向下移动，处理接下来的两行。我们不会水平移动滤波器。*1D
    卷积* 这个名字来源于这种单一方向或维度的移动方式。
- en: 'As always, we can have multiple filters sliding down the grid. We can perform
    1D convolution on an input tensor of any number of dimensions, as long as the
    filter itself moves in just one dimension. There’s nothing otherwise special about
    1D convolution: it’s just a filter that only moves in one direction. The technique
    has its own name to emphasize the filter’s limited mobility.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们可以让多个滤波器在网格上滑动。我们可以对任何维度的输入张量执行 1D 卷积，只要滤波器本身仅在一个维度上移动。1D 卷积没有其他特别之处：它只是一个仅在一个方向上移动的滤波器。这个技巧有自己的名字，目的是强调滤波器的有限移动性。
- en: The name 1D convolution is almost the same as the name of another, quite different,
    technique. Let’s look at that now.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 1D 卷积的名称几乎与另一种完全不同的技术名称相同。让我们现在来看看这个技术。
- en: 1×1 Convolutions
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1×1 卷积
- en: 'Sometimes we want to reduce the number of channels in a tensor as it flows
    through a network. Often this is because we think that some of the channels contain
    redundant information. This isn’t uncommon. For example, suppose we have a classifier
    that identifies the dominant object in a photograph. The classifier might have
    a dozen or more filters that look for eyes of different sorts: human eyes, cat
    eyes, fish eyes, and so on. If our classifier is going to ultimately lump all
    living things together into one class called “living things,” then there’s no
    need to care about which kind of eye we find. It’s enough just to know that a
    particular region in the input image has an eye.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们希望在张量通过网络时减少通道的数量。通常这是因为我们认为某些通道包含冗余信息。这并不罕见。例如，假设我们有一个分类器，它识别照片中的主导物体。分类器可能有十几个滤波器，用于寻找不同种类的眼睛：人类眼睛、猫眼、鱼眼等等。如果我们的分类器最终会将所有生物合并成一个名为“生物”的类别，那么就没有必要关心我们发现的是哪种眼睛。只要知道输入图像中的某个区域有眼睛就足够了。
- en: Suppose that we have a layer containing filters that detect 12 different kinds
    of eyes. Then the output tensor from that layer will have at least 12 channels,
    one from each filter. If we only care about whether or not an eye is found, then
    it would be useful to modify that tensor by combining, or compressing, those 12
    channels into just 1 channel representing whether or not an eye is found at each
    location.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个层，其中包含检测 12 种不同类型眼睛的滤波器。那么该层的输出张量将至少有 12 个通道，每个滤波器一个通道。如果我们只关心是否找到眼睛，那么将这个张量通过合并或压缩这
    12 个通道为一个表示每个位置是否有眼睛的通道会很有用。
- en: This doesn’t require anything new. We want to process one input element at a
    time, so we create a filter with a footprint of one by one, like we saw in [Figure
    16-6](#figure16-6). We make sure that we have at least 11 fewer filters than there
    are input channels. The result is a tensor of the same width and height as the
    input, but the multiple eye channels get crunched together into just one channel.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这不需要任何新东西。我们希望一次处理一个输入元素，因此我们创建了一个尺寸为 1x1 的滤波器，正如我们在[图 16-6](#figure16-6)中看到的那样。我们确保滤波器的数量至少比输入通道数少
    11 个。结果是一个与输入相同宽度和高度的张量，但多个眼睛通道被压缩成一个通道。
- en: We don’t have to do anything explicit to make this happen. The network learns
    weights for the filters such that the network produces the correct output for
    each input. If that means combining all the channels for eyes, then the network
    learns to do that.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 16-23](#figure16-23) shows how to use these filters to compress a tensor
    with 300 channels into a new tensor of the same width and height, but with only
    175 channels.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![F16023](Images/F16023.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-23: Applying 1×1 convolution to perform feature reduction'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: The technique of using one by one filters has been given its own name. We say
    that we apply a *one by one filter*, often written as a *1×1 filter*, and use
    that to perform *1×1 convolution* (Lin, Chen, and Yan 2014).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 10 we talked about the value of preprocessing our input data in order
    to save processing time and memory. Rather than perform this processing once,
    before the data has entered our system, 1×1 convolution lets us apply this compression
    and restructuring of the data on the fly, inside of the network. If our network
    produces information that can be compressed or removed entirely, then 1×1 convolutions
    can find and then compress or remove that data. We can do this anywhere, even
    in the middle of a network.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: When the channels are correlated, 1×1 convolution is particularly effective
    (Canziani, Paszke, and Culurciello 2016; Culurciello 2017). This means that the
    filters on the previous layers have created results that are in sync with one
    another, so that when one goes up, we can predict by how much the others will
    go up or down. The better this correlation, the more likely it is that we can
    remove some of the channels and suffer little to no loss of information. The 1×1
    filters are perfect for this job.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: The term *1×1 convolution* is uncomfortably close to *1D convolution*, which
    we discussed in the last section. But these names refer to quite distinct techniques.
    When encountering either of these terms, it is worth taking a moment to make sure
    we have the correct idea in mind.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Changing Output Size
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve just seen how to change the number of channels in a tensor by using 1×1
    convolution. We can also change the width and height, which is useful for at least
    two reasons. The first is that if we can make the data flowing through our network
    smaller, we can use a simpler network and save time, computing resources, and
    energy. The second is that reducing the width and height can make some operations,
    like classification, more efficient and even more accurate. Let’s see why this
    is so.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In previous sections, we applied each filter to one pixel, or one region of
    pixels. The filter matches the feature it’s looking for if the underlying pixels
    match the filter’s values. But what if some of the elements of the feature are
    in slightly wrong places? Then the filter won’t match. There’s no way for the
    filter to look around and report a match if one or more pieces of the pattern
    it’s looking for are present but slightly out of position. This would be a real
    problem if we didn’t address it. For example, suppose we’re looking for a capital
    T on a page of text. Due to a minor mechanical error during printing, a column
    of pixels was displaced downward by one pixel.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们将每个滤波器应用于一个像素或一块像素区域。如果基础像素与滤波器的值匹配，滤波器就能找到它所寻找的特征。但是如果特征的某些元素稍微偏离了正确的位置呢？那么滤波器就无法匹配。如果图案中一个或多个部分存在，但稍微错位，滤波器就无法找到匹配的地方。如果我们不解决这个问题，那将是一个真正的麻烦。例如，假设我们在一页文本中寻找一个大写字母T。由于印刷过程中发生了轻微的机械故障，一列像素被向下错位了一个像素。
- en: We still want to find the T. The situation is illustrated in [Figure 16-24](#figure16-24).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然希望找到字母T。这个情况在[图16-24](#figure16-24)中有说明。
- en: '![F16024](Images/F16024.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![F16024](Images/F16024.png)'
- en: 'Figure 16-24: From left to right: A five by five filter looking for a letter
    T, a misprinted T, the filter on top of the image, and the filter’s resulting
    values. The filter would not report a match to the letter T.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-24：从左到右：一个五乘五的滤波器正在寻找字母T，一个印刷错误的T，滤波器覆盖在图像上方，以及滤波器的结果值。该滤波器无法报告找到字母T的匹配。
- en: We begin with a five by five filter that is looking for a T in the center. We
    illustrate this using blue for 1 and yellow for 0\.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个五乘五的滤波器开始，寻找位于中心的T字形。我们使用蓝色表示1，黄色表示0来进行说明。
- en: We’ve labeled this the “perfect filter,” a name that will make sense in a moment.
    To its right is the misprinted text we’re going to examine, labeled “perfect image.”
    To the right of that, we overlay the filter on the image. At the far right is
    the result. Only when the filter and the input are both blue will the output be
    blue. Since the filter’s upper-right element did not find the blue pixel it was
    expecting, the filter as a whole reports either no match, or a weak one.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个称为“完美滤波器”，稍后它的名称会更清晰。在其右侧是我们将要检查的印刷错误文本，标记为“完美图像”。再往右，我们将滤波器叠加到图像上。最右侧是结果。只有当滤波器和输入都是蓝色时，输出才会是蓝色。由于滤波器的右上角元素没有找到它预期的蓝色像素，因此整个滤波器报告了没有匹配，或者是一个较弱的匹配。
- en: If the upper-right element in the filter could look around and notice the blue
    pixel just below it, it could match the input. One way to make this happen is
    to let each filter element “see” more of the input. The most convenient way to
    do that mathematically is to make the filter a bit blurry.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果滤波器的右上角元素能够环顾四周并注意到它正下方的蓝色像素，它就能够匹配输入。实现这一点的一种方法是让每个滤波器元素“看到”更多的输入。最方便的数学方法是让滤波器稍微模糊一些。
- en: On the top row of [Figure 16-25](#figure16-25) we picked out one element of
    the filter and blurred it. If the filter finds a blue pixel anywhere in this larger,
    blurry region, it reports finding blue. If we do this for all the entries in the
    filter, we create a “blurry filter.” Thanks to this extended reach, the upper-right
    blue filter element now overlaps two blue pixels, and since the other blue elements
    also overlap blue pixels, the filter now reports a match.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图16-25](#figure16-25)的上排中，我们选取了滤波器的一个元素并使其模糊。如果滤波器在这个更大、更模糊的区域中找到了蓝色像素，它就会报告找到了蓝色。如果我们对滤波器中的所有条目都进行这样的操作，就会创建一个“模糊滤波器”。由于这个扩展的范围，右上角的蓝色滤波器元素现在覆盖了两个蓝色像素，并且由于其他蓝色元素也覆盖了蓝色像素，滤波器现在报告了一个匹配。
- en: '![F16025](Images/F16025.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![F16025](Images/F16025.png)'
- en: 'Figure 16-25: Top row: Replacing a filter element with a bigger, blurrier version.
    Bottom row: Applying the blur to every filter element gives us a blurry filter.
    Applying this to the image matches the misprinted T.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-25：上排：将一个滤波器元素替换为更大、更模糊的版本。下排：将模糊应用于每个滤波器元素，从而得到一个模糊滤波器。将其应用于图像时，可以匹配到印刷错误的T字形。
- en: Unfortunately, we can’t blur filters like this. If we modified our filter values
    by blurring them, our training process would go haywire, since we would be altering
    the very values we’re trying to learn. But there’s nothing stopping us from blurring
    the input! This is particularly easy to see if the input is a picture, but we
    can blur any tensor. So rather than applying a blurry filter to a perfect input,
    let’s flip that around and apply a perfect filter to a blurry input.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: The top row of [Figure 16-26](#figure16-26) shows a single pixel from the misprinted
    T, and the version of that pixel after it’s been blurred. After we apply this
    blurring to all the pixels, we can apply the perfect filter to this blurry image.
    Now every blue dot in the filter sees blue under it. Success!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![F16026](Images/F16026.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-26: Top row: The effect of blurring one pixel in the input. Bottom
    row: We apply the perfect filter to a blurred version of the image. This matches
    the misprinted T.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Taking this as our inspiration, we can come up with a technique to blur a tensor.
    We call the method *pooling*, or *downsampling.* Let’s see how pooling works numerically
    with a small tensor with a single channel. Suppose we start with a tensor that
    has a width and height of four, as shown in [Figure 16-27](#figure16-27)(a).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![F16027](Images/F16027.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-27: Pooling, or downsampling, a tensor. (a) Our input tensor. (b)
    Subdividing (a) into two by two blocks. (c) The result of average pooling. (d)
    The result of max pooling. (e) Our icon for a pooling layer.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Let’s subdivide the width and height of this tensor into two by two blocks,
    as in [Figure 16-27](#figure16-27)(b). To blur the input tensor, recall [Figure
    16-7](#figure16-7). We saw that by convolving with a filter whose contents are
    all 1’s, the image got blurry. Such a filter is called a *low-pass filter*, or
    more specifically, a *box filter.*
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: To apply a box filter to a tensor, we can use a two by two filter where every
    weight is a 1\. Applying this filter merely means adding up the four numbers in
    each two by two block. Because we don’t want our numbers to grow without bound,
    we divide the result by four to get the average value in that block. Since this
    average now stands in for the entire block, we save it just once. We do the same
    thing for the other three blocks. The result is a new tensor of size two by two,
    shown in [Figure 16-27](#figure16-27)(c). This technique is called *average pooling.*
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s a variation on this method: instead of computing the average value,
    we just use the largest value in each block. This is called *maximum pooling*
    (or more often, just *max pooling)*, and is shown in [Figure 16-27](#figure16-27)(d).
    It’s common to think of these pooling operations as being carried out by a little
    utility layer. In [Figure 16-27](#figure16-27)(e) we show our icon for such a
    *pooling layer*. Experience has shown that networks that use max pooling learn
    more quickly than those using average pooling, so when people speak of pooling
    with no other qualifiers, they usually mean max pooling.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法有一个变体：我们不计算平均值，而是直接使用每个块中的最大值。这称为*最大池化*（或更常见的，简称为*max pooling*），如[图16-27](#figure16-27)(d)所示。通常我们将这些池化操作视为由一个小的辅助层执行。在[图16-27](#figure16-27)(e)中，我们展示了这样的*池化层*的图标。经验表明，使用最大池化的网络学习速度比使用平均池化的网络更快，因此当人们提到池化而没有其他限定时，他们通常指的是最大池化。
- en: The power of pooling appears when we apply multiple convolution layers in succession.
    Just as with a filter and a blurred input, if the first filter’s values aren’t
    in quite the expected locations, pooling helps the second layer’s filter still
    find them. For example, suppose that we have two layers in succession, and Layer
    2 has a filter that is looking for a strong match from Layer 1, directly above
    a match of about half that value (maybe this is characteristic of a particular
    animal’s coloration). Nothing in the original 4 by 4 tensor in [Figure 16-27](#figure16-27)(a)
    fits that pattern. There’s a 20 over a 2, but the 2 isn’t close to being half
    of 20\. And there’s a 6 over 3, but 6 isn’t a very strong output. So Layer 2’s
    filter would fail to find what it was looking for. That’s too bad, because there
    is a 20 that’s close to being over a 9, which is what the filter wants to find.
    The problem is that the 20 and the 9 are not exactly vertical neighbors.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 池化的强大之处在于我们连续应用多个卷积层时。就像在滤波器和模糊输入的情况一样，如果第一个滤波器的值不在预期的位置，池化帮助第二层的滤波器仍然能够找到它们。例如，假设我们有两个连续的层，第二层的滤波器正在寻找第一层的强匹配，直接位于约一半值的匹配之上（也许这是某种动物的颜色特征）。在[图16-27](#figure16-27)(a)中的原始四乘四张量没有符合该模式的内容。存在一个20对2，但2并不是20的一半。而且有一个6对3，但6不是一个非常强的输出。所以第二层的滤波器会找不到它想要的匹配。那太遗憾了，因为确实有一个20，接近位于9上方，这正是滤波器想要找到的匹配。问题是20和9不是完全垂直的邻居。
- en: But the max pooling version has the 20 over the 9\. The pooling operation is
    communicating to Layer 2 that there is a strong match of 20 somewhere in the upper
    right two by two block, and a match of 9 somewhere in the block directly below
    the 20\. That’s the pattern we’re looking for, and the filter will tell us that
    it found a match.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，最大池化版本中有20在9之上。池化操作正在向第二层传达，在右上角的两乘二块中有一个强匹配20，并且在20正下方的块中有一个匹配9。这就是我们想要的模式，滤波器会告诉我们它找到了一个匹配。
- en: We’ve discussed pooling for just one channel. When our tensors have multiple
    channels, we apply the same process to each channel. [Figure 16-28](#figure16-28)
    shows the idea.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了单通道的池化。当我们的张量具有多个通道时，我们会对每个通道应用相同的过程。[图16-28](#figure16-28)展示了这个概念。
- en: We start with an input tensor of height and width 6 and one channel, padded
    with a ring of zeros. The convolution layer applies three filters, each producing
    a feature map of six by six. The output of the convolution layer is a tensor of
    size six by six by three. The pooling layer then conceptually considers each channel
    of this tensor, and applies max pooling to it, reducing each feature map to three
    by three. Those feature maps are then combined as before to produce an output
    tensor of width and height 3, with three channels.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个高度和宽度为6，通道数为1的输入张量开始，并用零环进行填充。卷积层应用三个滤波器，每个滤波器产生一个六乘六的特征图。卷积层的输出是一个尺寸为六乘六乘三的张量。然后，池化层在概念上考虑该张量的每个通道，并对其应用最大池化操作，将每个特征图缩小为三乘三。这些特征图随后像之前一样组合，生成一个宽度和高度为3、具有三个通道的输出张量。
- en: '![F16028](Images/F16028.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![F16028](Images/F16028.png)'
- en: 'Figure 16-28: Pooling, or downsampling, with multiple filters'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-28：使用多个滤波器的池化或下采样
- en: We’ve been using binary images and filters as examples. This means that a feature
    that straddles cell boundaries could be missed, or wind up in the wrong element
    in the pooled tensor. When we use real valued inputs and filter kernels, this
    problem is greatly reduced.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在使用二值图像和滤波器作为示例。这意味着，跨越单元边界的特征可能会被忽略，或者在池化后的张量中出现在错误的位置。当我们使用实值输入和滤波核时，这个问题会大大减少。
- en: Pooling is a powerful operation that frees filters from requiring their inputs
    to be in precisely the right place. Mathematicians refer to a change in location
    as *translation* or *shift*, and if some operation is insensitive to a certain
    kind of change it’s called *invariant* with respect to that operation. Combining
    these, we sometimes say that pooling allows our convolutions to be *translationally
    invariant*, or *shift invariant* (Zhang 2019).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 池化是一种强大的操作，它使得滤波器不再要求其输入位置必须完全正确。数学家称这种位置变化为*平移*或*偏移*，如果某个操作对某种变化不敏感，就称其为对该操作*不变*。结合这些，我们有时会说池化使我们的卷积操作具备*平移不变性*，或*偏移不变性*（Zhang
    2019）。
- en: Pooling also has the bonus benefit of reducing the size of the tensors flowing
    through our network, which reduces both memory needs and execution time.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 池化操作还具有一个额外的好处，就是减少了通过网络流动的张量的大小，这减少了内存需求和执行时间。
- en: Striding
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跨步
- en: We’ve seen how useful pooling is in a convolutional network. Though pooling
    layers are common, we can save time by bundling the pooling step right into the
    convolution process. This combined operation is much faster than two distinct
    layers. The tensors resulting from the two procedures usually contain different
    values, but experience has shown that the faster, combined operation usually produces
    results that are just as useful as the slower, sequential operations.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到池化在卷积网络中的重要性。尽管池化层很常见，但我们可以通过将池化步骤直接集成到卷积过程中来节省时间。这个结合操作比两个独立的层更快。通过这两种过程得到的张量通常包含不同的值，但经验表明，快速的结合操作通常能产生与慢速的顺序操作一样有用的结果。
- en: As we saw, during convolution we can imagine starting the filter in the upper-left
    pixel of the input image (let’s assume we have padding). The filter produces an
    output, then takes one step right, produces another output, moves another step
    right, and so on until it reaches the right edge of that row. Then it moves down
    one row and back to the left side, and the process repeats.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在卷积过程中，我们可以假设滤器从输入图像的左上角像素开始（假设我们有填充）。滤器产生一个输出，然后向右移动一步，产生另一个输出，再向右移动一步，依此类推，直到到达该行的右边缘。然后它向下一行移动，返回到左侧，过程重复进行。
- en: But we don’t have to move in single steps. Suppose we move, or *stride*, more
    than one pixel to the right, or more than one pixel down, as we sweep our filter.
    Then our output will end up being smaller than the input. We usually use the word
    *stride* (and the related *striding*) only when we use steps greater than one
    in any dimension.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不必每次都按单步移动。假设我们在扫过滤器时，向右移动或*跨步*超过一个像素，或者向下移动超过一个像素，那么我们的输出将会比输入更小。我们通常只有在每个维度的步长大于一时，才会使用*跨步*（以及相关的*跨步操作*）这个词。
- en: To visualize striding, let’s see how the filter moves starting from the upper
    left. As the filter moves left to right, it produces a sequence of outputs, and
    those get placed one after the other, also left to right, in the output. When
    the filter moves down, the new outputs go on a new line of cells in the output.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化跨步操作，让我们看看滤器从左上角开始的移动过程。当滤器从左到右移动时，它会生成一系列输出，这些输出会依次排列在输出中，同样是从左到右。当滤器向下移动时，新的输出会放置在输出的新一行单元格中。
- en: Now suppose that instead of moving the filter to the right by one element on
    each horizontal step, we moved to the right by three elements. And perhaps on
    each vertical step we move down by two rows, rather than one. We still grow the
    output by one element for each output. The idea is shown in [Figure 16-29](#figure16-29).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们不是每次水平移动滤器一个元素，而是每次向右移动三个元素。也许每次垂直移动时，我们会向下移动两行，而不是一行。我们仍然为每个输出增长一个元素。这个概念如[图16-29](#figure16-29)所示。
- en: '![F16029](Images/F16029.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![F16029](Images/F16029.png)'
- en: 'Figure 16-29: Our input scanning can skip over input elements as it moves.
    Here we move three elements to the right on each horizontal step, and two elements
    down on each vertical step.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-29：我们的输入扫描在移动时可以跳过输入元素。这里我们在每个水平步长上移动三个元素，每个垂直步长上移动两个元素。
- en: In [Figure 16-29](#figure16-29) we used a stride of three horizontally, and
    a stride of two vertically. More often we specify a single stride value for both
    axes. A stride of two on both axes can be thought of as evaluating every other
    pixel both horizontally and vertically. This results in an output that has half
    the input dimensions as the input, which means the output has the same dimensions
    as striding by one and then pooling with two by two blocks. [Figure 16-30](#figure16-30)
    shows where the filter lands in the input for a couple of different pairs of strides.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图16-29](#figure16-29)中，我们在水平方向上使用了步幅三，在垂直方向上使用了步幅二。更常见的是我们会为两个轴指定一个单一的步幅值。在两个轴上都使用步幅二可以看作是每隔一个像素进行评估，无论是水平还是垂直。这将导致输出的尺寸是输入尺寸的一半，这意味着输出的尺寸与先使用步幅为一然后进行二乘二块池化操作后的尺寸相同。[图16-30](#figure16-30)显示了滤波器在输入中对于不同步幅对的位置。
- en: '![F16030](Images/F16030.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![F16030](Images/F16030.png)'
- en: 'Figure 16-30: Examples of striding. (a) A stride of two in both directions
    means centering the filter over every other pixel, both horizontally and vertically.
    (b) A stride of three in both directions means centering over every third pixel.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-30：步幅示例。（a）在两个方向上使用步幅二意味着将滤波器集中在每隔一个像素的位置，既在水平方向也在垂直方向。（b）在两个方向上使用步幅三意味着将滤波器集中在每隔三个像素的位置。
- en: When we move by one element on every step, a filter with a three by three footprint
    processes the same input elements multiple times. When we stride by larger amounts,
    our filter can still process some elements multiple times, as shown in [Figure
    16-31](#figure16-31).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们每一步移动一个元素时，一个三乘三的滤波器会多次处理相同的输入元素。当我们使用较大的步幅时，我们的滤波器仍然可以多次处理某些元素，如[图16-31](#figure16-31)所示。
- en: '![F16031](Images/F16031.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![F16031](Images/F16031.png)'
- en: 'Figure 16-31: This three by three filter is moving with a stride of two in
    each dimension, reading left to right, top to bottom. The gray elements show what’s
    been processed so far. The green elements are those that have already been used
    by the filter on previous evaluations but are being used again.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-31：这个三乘三的滤波器在每个维度上以步幅为二进行移动，从左到右，从上到下读取。灰色的元素表示已经处理过的部分。绿色的元素是那些已经在之前的评估中被滤波器使用过，但这次会再次使用的部分。
- en: There’s nothing wrong with reusing an input value repeatedly, but if we’re trying
    to save time, we might want to do as little computation as possible. Then we can
    use striding to prevent any input element from being used more than once. For
    instance, if we’re moving a three by three filter over an image, we might use
    a stride of three in both directions, so that no pixel gets used more than once,
    as in [Figure 16-32](#figure16-32).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 重复使用一个输入值没有问题，但如果我们想节省时间，可能希望尽量减少计算量。这时，我们可以使用步幅来防止任何输入元素被重复使用。例如，如果我们在图像上移动一个三乘三的滤波器，我们可能会在两个方向上都使用步幅为三，这样就不会有任何像素被重复使用，正如[图16-32](#figure16-32)所示。
- en: '![F16032](Images/F16032.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![F16032](Images/F16032.png)'
- en: 'Figure 16-32: Like [Figure 16-31](#figure16-31), only now we’re striding by
    three in each dimension. Every input element is processed exactly one time.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-32：与[图16-31](#figure16-31)类似，只是现在我们在每个维度上都使用了步幅三。每个输入元素都只处理一次。
- en: The striding in [Figure 16-32](#figure16-32) produces an output tensor with
    a height and width that are each one-third of the input tensor’s height and width.
    Consider that in [Figure 16-32](#figure16-32) we processed a nine by six block
    of input elements with just six filter evaluations. By doing this, we created
    a three by two block of outputs with no explicit pooling. If we don’t stride,
    and then pool, we need many more filter evaluations to cover the same region,
    and then we need to run the pooling operation on the filter outputs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-32](#figure16-32)中的步幅操作生成的输出张量在高度和宽度上分别为输入张量高度和宽度的三分之一。考虑到在[图16-32](#figure16-32)中，我们通过仅进行六次滤波器评估就处理了一个九乘六的输入元素块。通过这种方式，我们创建了一个三乘二的输出块，且没有明确的池化操作。如果我们不进行步幅操作然后再进行池化，我们需要更多的滤波器评估来覆盖相同的区域，之后还需要在滤波器的输出上执行池化操作。'
- en: Strided convolutions are faster than convolution without striding followed by
    pooling for two reasons. First, we evaluate the filter fewer times, and second,
    we don’t have an explicit pooling step to compute. Like padding, striding can
    (and often is) carried out on any convolutional layer, not just the first.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 与没有步幅的卷积加池化操作相比，步幅卷积有两个原因更快。首先，我们评估滤波器的次数更少，其次，我们没有一个明确的池化步骤需要计算。像填充一样，步幅可以（并且通常会）应用于任何卷积层，而不仅仅是第一个卷积层。
- en: The filters learned from striding are usually different than those learned from
    convolution without striding followed by pooling. This means we can’t take a trained
    network and replace pairs of convolution and pooling with strided convolution
    (or vice versa) and expect things to still work properly. If we want to change
    our network’s architecture, we have to retrain it.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从步长学习到的滤波器通常与从没有步长的卷积后接池化学习到的滤波器不同。这意味着我们不能直接将训练好的网络中的卷积和池化对替换为步长卷积（或反之），然后期望它们仍然能正常工作。如果我们想改变网络的架构，就必须重新训练它。
- en: Most of the time, training with strided convolution gives us final results that
    are roughly the same as those we get from convolution followed by pooling, delivered
    in less time. But sometimes the slower combination works better for a given dataset
    and architecture.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，使用步长卷积进行训练会给我们带来与卷积加池化相似的最终结果，而且所需时间更短。但有时，对于特定的数据集和架构，较慢的组合方法反而效果更好。
- en: Transposed Convolution
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转置卷积
- en: We’ve seen how to reduce the size of the input, or *downsize* it, using either
    pooling or striding. We can also increase the size of the input, or *upsize* it.
    As with downsizing, when we upsize a tensor, we increase its width and height,
    but we don’t change the number of channels.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何通过池化或步长来减小输入的大小，或者说*下采样*它。我们也可以增加输入的大小，或者说*上采样*它。与下采样一样，当我们上采样一个张量时，我们增加它的宽度和高度，但不会改变通道数。
- en: Just as with downsampling, we can upsize with a separate layer or build it into
    the convolution layer. A distinct upsampling layer usually just repeats the input
    tensor values as many times as we request. For example, if we upsample a tensor
    by two in both the width and height, each input element turns into a little two
    by two square. [Figure 16-33](#figure16-33) shows the idea.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 与下采样一样，我们可以通过单独的层进行上采样，或者将其构建到卷积层中。一个独立的上采样层通常只是将输入张量的值重复我们要求的次数。例如，如果我们在宽度和高度上都将张量上采样两倍，每个输入元素就变成一个小的
    2x2 正方形。[图16-33](#figure16-33)展示了这一概念。
- en: '![F16033](Images/F16033.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![F16033](Images/F16033.png)'
- en: 'Figure 16-33: Upsampling a tensor by two in each direction. Left: The input
    tensor. Each element of this tensor is repeated twice vertically and horizontally.
    Right: The output tensor. The number of channels is unchanged.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-33：在每个方向上将张量上采样两倍。左图：输入张量。这个张量的每个元素都在垂直和水平方向上重复了两次。右图：输出张量。通道数不变。
- en: We have seen that we can combine downsampling with convolution by using striding.
    We can also combine upsampling with convolution. This combined step is called
    *transposed convolution*, *fractional striding*, *dilated convolution*, or *atrous
    convolution.* The word *transposed* comes from the mathematical operation of transposition,
    which we can use to write the equation for this operation. The word *atrous* is
    French for “with holes.” We’ll see where that term, and the others, come from
    in a moment. Note that some authors refer to the combination of upsampling and
    convolution as *deconvolution*, but it’s best to avoid that term, since it’s already
    in use and refers to a different idea (Zeiler et al. 2010). Following current
    practice, we’ll use the term *transposed convolution*.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，可以通过使用步长将下采样与卷积结合起来。我们也可以将上采样与卷积结合。这个结合步骤称为*转置卷积*、*分数步长*、*膨胀卷积*或*孔洞卷积*。*转置*一词来源于数学中的转置运算，我们可以用它来写出这个操作的方程式。*孔洞*（*atrous*）是法语中“带孔”的意思。稍后我们将看到这些术语的来源。需要注意的是，一些作者将上采样和卷积的结合称为*反卷积*，但最好避免使用这个术语，因为它已经被用于不同的概念（Zeiler
    等人 2010）。按照当前的做法，我们将使用*转置卷积*这一术语。
- en: Let’s see how transposed convolution works to enlarge a tensor (Dumoulin and
    Visin 2016). Suppose that we have a starting image of width and height three by
    three (remember, the number of channels won’t be changing), and we’d like to process
    it with a three by three filter, but we’d like to end up with a five by five image.
    One approach is to pad the input with two rings of zeros, as in [Figure 16-34](#figure16-34).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下转置卷积是如何工作的，以扩大一个张量的尺寸（Dumoulin 和 Visin 2016）。假设我们有一个宽度和高度都是 3x3 的初始图像（记住，通道数不会改变），并且我们希望使用一个
    3x3 的滤波器处理它，但希望最终得到一个 5x5 的图像。一个方法是用两圈零进行填充，如[图16-34](#figure16-34)所示。
- en: '![F16034](Images/F16034.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![F16034](Images/F16034.png)'
- en: 'Figure 16-34: Our original three by three input is shown in white in the outer
    grids, padded with two elements of zeros all around. The three by three filter
    now produces a five by five result, shown in the center.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-34：我们的原始三乘三输入在外部网格中以白色显示，四周用两圈零填充。三乘三滤波器现在产生一个五乘五的结果，显示在中心。
- en: If we add more rings of zeros to the input, we get larger outputs, but they
    will produce rings of zeros around the central five by five core. That’s not very
    useful.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在输入中添加更多的零环，我们会得到更大的输出，但它们会在中央五乘五的核心周围产生零环。这并不是特别有用。
- en: An alternative way to enlarge the input is to spread it out before convolving
    by inserting padding both around and *between* the input elements. Let’s try this
    out. Let’s insert a single row and column of zeros between each element of our
    starting three by three image, and pad all of that with two rings of zeros around
    the outside, like before. The result is that our three by three input now has
    dimensions nine by nine, though a lot of those entries are zero. When we sweep
    our three by three filter over this grid, we get a seven by seven output, as shown
    in [Figure 16-35](#figure16-35).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种放大输入的方法是在卷积之前通过在输入元素之间和周围插入填充来扩展它。让我们试一下这个方法。我们在起始的三乘三图像中的每个元素之间插入一行一列零，并且像之前一样在外围用两圈零进行填充。结果是，我们的三乘三输入现在变成了九乘九，尽管其中有很多条目是零。当我们用三乘三的滤波器扫描这个网格时，我们得到一个七乘七的输出，如[图16-35](#figure16-35)所示。
- en: Our original three by three image is shown in the outer grids with white pixels.
    We’ve inserted a row and column of zeros (blue) between each pixel, and then surrounded
    the whole thing with two rings of zeros. When we convolve our three by three filter
    (red) with this grid, we get a seven by seven result, shown in the center.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的原始三乘三图像显示在外部网格中，白色像素表示。我们在每个像素之间插入了一行和一列零（蓝色），然后用两圈零将整个图像包围。当我们将三乘三的滤波器（红色）与这个网格进行卷积时，我们得到了一个七乘七的结果，显示在中心。
- en: '![F16035](Images/F16035.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![F16035](Images/F16035.png)'
- en: 'Figure 16-35: Transposed convolution, convolving a three by three filter into
    a seven by seven result'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-35：转置卷积，将一个三乘三的滤波器卷积到七乘七的结果中
- en: '[Figure 16-35](#figure16-35) suggests where the names *atrous* (French for
    “with holes”) *convolution* and *dilated convolution* come from. We can make our
    output even bigger by inserting another row and column between each original input
    element, as in [Figure 16-36](#figure16-36). Now our 3 by 3 input has become an
    11 by 11 input, and the output is 9 by 9.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-35](#figure16-35) 显示了 *atrous*（法语意思是“带孔”）*卷积* 和 *扩张卷积* 这一命名的来源。通过在每个原始输入元素之间插入另一个行列，我们可以使输出变得更大，如
    [图16-36](#figure16-36) 所示。现在我们的三乘三输入变成了十一乘十一的输入，输出则变成九乘九。'
- en: '![F16036](Images/F16036.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![F16036](Images/F16036.png)'
- en: 'Figure 16-36: The same setup as [Figure 16-35](#figure16-35), only now we have
    two rows and columns between our original input pixels, producing the nine by
    nine result in the center'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-36：与[图16-35](#figure16-35)相同的设置，只不过现在我们在原始输入像素之间插入了两行两列，产生了中心的九乘九结果
- en: We can’t push this technique any further without producing rows and columns
    of zeros in the output. The limit of two rows or columns of zeros is due to our
    filter having a footprint of three by three. If the filter was, say, five by five,
    we could use up to four rows and columns of zeros. This technique of inserting
    zeros can create little checkerboard-like artifacts in the output tensors. But
    library routines can usually avoid these if they take steps to handle the convolution
    and upsampling carefully (Odena, Dumoulin, and Olah 2018; Aitken et al. 2017).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有在输出中产生零的行列，我们无法再进一步推展这一技术。零的两行或两列限制是由于我们的滤波器具有三乘三的占地面积。如果滤波器是五乘五的，我们就可以使用最多四行或列的零。插入零的技术可能会在输出张量中产生一些类似棋盘的伪影。但库函数通常可以通过仔细处理卷积和上采样来避免这些问题（Odena、Dumoulin和Olah
    2018；Aitken等人 2017）。
- en: There is a connection between transposed convolution and striding. With some
    imagination, we can describe a transposed convolution process like that of [Figure
    16-36](#figure16-36) as using a stride of one-third in each dimension. We don’t
    mean that we literally move one-third of an element, but rather that we need to
    take three steps in the 11 by 11 grid to move the equivalent of one step in the
    original 3 by 3 input. This point of view explains why the method is sometimes
    called *fractional striding*.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 反卷积与步幅之间存在联系。通过一些想象，我们可以将[图16-36](#figure16-36)中的反卷积过程描述为每个维度上使用三分之一的步幅。我们并不是说我们字面上移动三分之一的元素，而是指我们需要在11×11的网格中走三步，才能相当于在原始3×3的输入中走一步。这种视角解释了为什么这种方法有时被称为*分数步幅*。
- en: Just as striding combines convolution with a downsampling (or pooling) step,
    transposed convolution (or fractional striding) combines convolution with an upsampling
    step. This results in faster execution time, which is always nice. A problem is
    that there is a limit to how much we can increase the input size. In practice,
    we commonly double the input dimensions, and use filters with a footprint of three
    by three, and transposed convolution supports that combination without introducing
    extraneous zeros in the output.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 就像步幅结合卷积与下采样（或池化）步骤一样，反卷积（或分数步幅）结合了卷积与上采样步骤。这带来了更快的执行时间，这总是令人愉快的。一个问题是我们可以增加输入尺寸的限制。在实践中，我们通常将输入维度加倍，并使用三乘三的滤波器，反卷积支持这种组合而不会在输出中引入多余的零。
- en: As with striding, the output of transposed convolution is different than the
    output of upsampling followed by standard convolution, so if we’re given a trained
    network using upsampling followed by convolution, we can’t just replace those
    two layers with one transposed convolution layer and use the same filters.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 与步幅一样，反卷积的输出与上采样加标准卷积的输出不同，因此，如果我们得到一个使用上采样加卷积的训练网络，我们不能仅仅将这两层替换为一个反卷积层并使用相同的滤波器。
- en: Transposed convolution is becoming more common than upsampling followed by convolution
    because of the increased efficiency, and similarity of the results (Springenberg
    et al. 2015).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 反卷积因其效率更高且与结果相似（Springenberg 等，2015），而比上采样加卷积更为常见。
- en: We’ve covered a lot of basic tools, from different types of convolution to padding
    and changing the output size. In the next section, we put these all together to
    create a complete, but simple, convolutional network.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了很多基本工具，从不同类型的卷积到填充和改变输出大小。在下一节中，我们将把这些工具结合起来，创建一个完整但简单的卷积网络。
- en: Hierarchies of Filters
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滤波器的层次结构
- en: Many real visual systems seem to be arranged *hierarchically* (Serre 2014).
    In broad terms, many biologists think of the processing in the visual system as
    taking place in a series of layers, with each successive layer working at a higher
    level of abstraction than the one before.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 许多真实的视觉系统似乎是*层次化的*（Serre 2014）。从广义上讲，许多生物学家认为视觉系统的处理是在一系列层次中进行的，每一层都比前一层处理更高层次的抽象。
- en: We’ve taken inspiration from biology already in this chapter, and we can do
    it again now.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们已经从生物学中汲取了灵感，现在我们可以再次这样做。
- en: Let’s return to our discussion of the visual system of a toad. The first layer
    of cells to receive light may be looking for “bug-colored blobs,” the next may
    be looking for “combinations of blobs from the previous layer that form bug-like
    shapes,” the next may be looking for “combinations of bug-like shapes from the
    previous layer that look like a thorax with wings,” and so on, up to the top layer,
    which looks for “flies” (these features are completely imaginary, and only meant
    to illustrate the idea).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到讨论蟾蜍视觉系统的话题。接收光的第一层细胞可能在寻找“虫子颜色的斑点”，下一层可能在寻找“来自前一层的斑点组合，形成类似虫子的形状”，接下来的层次可能在寻找“前一层形成的虫子形状组合，看起来像是有翅膀的胸部”，依此类推，直到最上层，它在寻找“苍蝇”（这些特征完全是虚构的，只是为了说明这个想法）。
- en: This approach is nice conceptually because it lets us structure our analysis
    of an image in terms of a hierarchy of image features and the filters that look
    for them. It’s also nice for implementations because it’s a flexible and efficient
    way to analyze an image.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在概念上很不错，因为它让我们能够按照图像特征的层次结构和寻找这些特征的滤波器来构建对图像的分析。它在实现上也很有优势，因为这是分析图像的一种灵活且高效的方式。
- en: Simplifying Assumptions
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简化假设
- en: To illustrate the use of hierarchies, let’s solve a recognition problem with
    a convolutional network. To focus this discussion just on the concepts, we’ll
    make use of some simplifications. These simplifications in no way change the principles
    we’re demonstrating; they just make the pictures easier to draw and interpret.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明层次结构的使用，让我们用卷积网络来解决一个识别问题。为了将讨论重点放在概念上，我们将使用一些简化。这些简化并不会改变我们要展示的原理；它们只是让图像更容易绘制和解读。
- en: 'First, we restrict ourselves to binary images: just black and white, with no
    shades of gray (though for clarity, we draw them with beige and green for 0 and
    1, respectively). In real applications, each channel in our input images is usually
    either an integer in the range [0, 255], or more commonly a real number in the
    range [0, 1].'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将自己限制在二进制图像上：只有黑白，没有灰色阴影（虽然为了清晰起见，我们分别用米色和绿色表示 0 和 1）。在实际应用中，我们输入图像中的每个通道通常是一个范围在
    [0, 255] 之间的整数，或者更常见的是一个范围在 [0, 1] 之间的实数。
- en: Second, our filters are also binary and look for exact matches in their inputs.
    In real networks, our filters use real numbers, and they match their inputs to
    different degrees, represented by different real numbers at their output.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们的滤波器也是二进制的，并且寻找输入中的精确匹配。在实际的网络中，我们的滤波器使用实数，并且它们将输入与不同程度的匹配，输出为不同的实数。
- en: Third, we hand-create all of our filters. In other words, we do our own feature
    engineering. When we looked at expert systems, we said that their biggest problem
    was that they required people to manually build features, and here we are, doing
    just that! We’re doing so just for this discussion, however. In practice, our
    filter values are learned by training. Since we’re not interested in the training
    step right now, we’ll use handmade filters (we can think of them as filters that
    resulted from training).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们手动创建所有的滤波器。换句话说，我们自己做特征工程。当我们讨论专家系统时，我们提到它们的最大问题是需要人工构建特征，而我们现在就是在做这件事！不过，这只是为了本次讨论。实际上，我们的滤波器值是通过训练学习得来的。由于我们目前不关注训练步骤，所以我们将使用手工制作的滤波器（我们可以将其视为通过训练得到的滤波器）。
- en: Fourth, we won’t use padding. This also is just to keep things simple.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，我们不会使用填充。这也是为了保持简单。
- en: Finally, our example uses tiny input images that are just 12 pixels on a side.
    This is large enough to demonstrate the ideas but small enough that we can draw
    everything clearly on the page.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的示例使用的是只有 12 像素边长的小输入图像。这个大小足以展示这些概念，但又足够小，可以让我们在页面上清晰地绘制所有内容。
- en: With these simplifications in place, we’re ready to get started.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些简化措施到位后，我们准备好开始了。
- en: Finding Face Masks
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查找面具图
- en: Let’s suppose that we work at a museum that has received a big collection of
    art, and it’s our job to organize it all. One of our tasks is to find all of the
    drawings of grid-based face masks that are close matches to the simple mask in
    [Figure 16-37](#figure16-37).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在一个博物馆工作，博物馆收到了大量的艺术品，我们的任务是将其整理好。我们的其中一项任务是找到所有与[图 16-37](#figure16-37)中的简单掩码相近的网格化面具画作。
- en: '![F16037](Images/F16037.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![F16037](Images/F16037.png)'
- en: 'Figure 16-37: A simple binary mask on a 12 by 12 mesh'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16-37：在一个 12x12 网格上的简单二进制掩码
- en: Suppose we’re given the new mask in the middle of [Figure 16-37](#figure16-37).
    Let’s call this the *candidate*. We want to determine whether it’s roughly the
    “same” as the original mask, which we call the *reference*. We can just overlay
    the two masks and see if they match up, as in the right of [Figure 16-38](#figure16-38).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们得到了中间的这个新掩码，如[图 16-37](#figure16-37)所示。我们将其称为*候选图*。我们想要确定它是否大致与原始掩码“相同”，即我们称之为*参考图*。我们可以将这两个掩码叠加在一起，看看它们是否匹配，如[图
    16-38](#figure16-38)右侧所示。
- en: '![F16038](Images/F16038.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![F16038](Images/F16038.png)'
- en: 'Figure 16-38: Testing for similarity. On the left is our original mask, or
    reference. In the middle is a new mask, or candidate. To see if they’re close
    to one another, we can overlay them, at the right.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16-38：测试相似性。左边是我们的原始掩码或参考图。中间是一个新的掩码或候选图。为了检查它们是否接近，我们可以将它们叠加在右边。
- en: In this case, it’s a perfect match, which is easy to detect. But what if a candidate
    is slightly different than the reference, as in [Figure 16-39](#figure16-39)?
    Here one eye has moved down by one pixel.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，它是一个完美匹配，很容易检测出来。但是如果一个候选图与参考图略有不同，像[图 16-39](#figure16-39)所示呢？这里其中一只眼睛向下移动了一个像素。
- en: '![F16039](Images/F16039.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![F16039](Images/F16039.png)'
- en: 'Figure 16-39: Like [Figure 16-38](#figure16-38), only the candidate’s left
    eye has moved down by one pixel. The overlay is now imperfect.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-39：与[图16-38](#figure16-38)类似，只不过候选者的左眼向下移动了一个像素。覆盖图现在不完美。
- en: Let’s say that we still want to accept this candidate, since it has all the
    same features as the reference, and they’re mostly in the right places. But the
    overlay shows that they’re not identical, so a simple pixel-by-pixel comparison
    won’t do the job.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们仍然希望接受这个候选者，因为它具有与参考面具相同的所有特征，并且这些特征大部分都在正确的位置。但覆盖图显示它们并不完全相同，因此简单的逐像素比较无法完成任务。
- en: In this simple example, we could come up with lots of ways to detect close matches,
    but let’s use convolution to determine that a candidate like the one in [Figure
    16-39](#figure16-39) is “like” the reference. As mentioned earlier, we’re going
    to hand-engineer our filters. To describe our hierarchy, it’s easiest to work
    backward, from the final step of convolution to the first.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的例子中，我们可以想出很多方法来检测相似的匹配，但让我们使用卷积来确定像[图16-39](#figure16-39)中的候选者是否“像”参考面具。如前所述，我们将手动设计我们的滤波器。为了描述我们的层次结构，最简单的方法是从最后一步卷积开始，倒推到第一步。
- en: Let’s begin by describing the reference mask. Then we can determine if a candidate
    shares its qualities. Let’s say that our reference is characterized by having
    one eye in each of the upper corners, a nose in the middle, and a mouth under
    the nose. That description applies to all of the masks we saw in Figures 16-38
    and 16-39.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从描述参考面具开始。然后，我们可以判断候选者是否具有相同的特征。假设我们的参考面具的特征是：每个上角都有一只眼睛，鼻子在中间，嘴巴在鼻子下方。这个描述适用于我们在图16-38和图16-39中看到的所有面具。
- en: 'We can formalize this description with a three by three filter, as in the top-left
    grid of [Figure 16-40](#figure16-40). This will be one of our last filters: if
    we run a candidate through a series of convolutions, ultimately producing a three
    by three tensor (we’ll see how that happens shortly), then if that tensor matches
    this filter, we’ve found a successful match, and an acceptable candidate. The
    cells with an × in them mean “don’t care.” For instance, suppose a candidate has
    a tattoo on one cheek that falls into the × to the right of the nose. This doesn’t
    affect our decision, so we explicitly don’t care about what’s in that cell.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用一个三乘三的滤波器来形式化这个描述，如[图16-40](#figure16-40)左上角的网格所示。这将是我们最后几个滤波器之一：如果我们通过一系列卷积运算处理一个候选者，最终得到一个三乘三的张量（稍后我们将看到如何得到这个张量），那么如果这个张量与这个滤波器匹配，我们就找到了一个成功的匹配和一个可接受的候选者。带有×的单元格表示“不关心”。例如，假设候选者的一个面颊上有一个纹身，落在鼻子右侧的×区域内。这不会影响我们的判断，因此我们明确表示不关心该单元格中的内容。
- en: '![F16040](Images/F16040.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![F16040](Images/F16040.png)'
- en: 'Figure 16-40: Filters for mask recognition. Top and bottom rows: Finding a
    mask facing forward, or in profile. Left column: Characterizing the reference.
    Middle: An exploded version of the tensor described by the grid at the left. Right:
    An X-ray view of the filter (see text).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-40：面具识别的滤波器。上下行：寻找正面或侧面的面具。左列：特征化参考面具。中间：左侧网格所描述的张量的爆炸版。右侧：滤波器的X光视图（见正文）。
- en: Since our filters only contain the values 1 (green) and 0 (beige), we can’t
    make a filter like the upper left diagram of [Figure 16-40](#figure16-40) directly.
    Instead, since it’s looking for three different kinds of features, we need to
    redraw it as a filter with three channels, which we’ll apply to an input tensor
    with three channels. One input channel tells us all the locations where an eye
    was located in the input, the next tells us all the locations of a nose, and the
    last tells us all the locations of a mouth. Our upper-left diagram corresponds,
    then, to a three by three by three tensor, shown in the upper middle diagram,
    where we’ve staggered the channels so we can read each one.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的滤波器只包含1（绿色）和0（米色）这两种值，我们无法直接做出像[图16-40](#figure16-40)左上方示意图那样的滤波器。相反，由于它需要寻找三种不同类型的特征，我们需要将其重新绘制为一个具有三通道的滤波器，并将其应用于具有三通道的输入张量。一个输入通道告诉我们眼睛在输入中的所有位置，下一个告诉我们鼻子的所有位置，最后一个告诉我们嘴巴的所有位置。因此，我们的左上方示意图对应一个三乘三乘三的张量，如上中图所示，我们已将通道错开，以便可以分别读取每个通道。
- en: We drew the staggered version because if we drew that tensor as a solid block,
    we wouldn’t be able to see most of the values on the N (nose) and M (mouth) channels.
    The staggered version is useful, but it will get too complicated when we start
    comparing tensors in the following discussion. Instead, let’s draw an “X-ray view”
    of the tensor, as in the upper right. We imagine we’re looking through the channels
    of the tensor, and we mark each cell with the names of all the channels that have
    a 1 in that cell.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们画出了错开的版本，因为如果我们将该张量画成一个实心块，我们将无法看到N（鼻子）和M（嘴巴）通道上的大部分值。错开的版本很有用，但在我们开始比较张量的后续讨论时会变得非常复杂。相反，让我们画出张量的“X射线视图”，如右上角所示。我们假设我们正在透视张量的各个通道，并在每个单元格中标记所有在该单元格中有1的通道名称。
- en: Since this filter is looking for a mask facing forward, we label it F. For fun,
    we can make another mask that’s looking for a face in profile, which we can call
    P. We won’t look at any candidates that would be matched by P, but we’re including
    it here to show the generality of this process. The layers to come, which operate
    before the filters of [Figure 16-40](#figure16-40), will tell us where they found
    an eye, nose, and mouth. We use that information in [Figure 16-40](#figure16-40)
    to recognize different arrangements of these facial features just by using different
    filters.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个过滤器用于寻找面向前方的面具，我们将其标记为F。为了有趣，我们还可以制作一个寻找侧面面具的过滤器，我们称之为P。我们不会查看任何由P匹配的候选图像，但我们在这里包括它，是为了展示这一过程的普遍性。接下来的层将在[图16-40](#figure16-40)中的过滤器之前操作，它们将告诉我们在哪里找到了眼睛、鼻子和嘴巴。我们利用这些信息，在[图16-40](#figure16-40)中通过使用不同的过滤器来识别这些面部特征的不同排列。
- en: Finding Eyes, Noses, and Mouths
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 寻找眼睛、鼻子和嘴巴
- en: Let’s see how to turn a 12 by 12 candidate picture into the 3 by 3 grid required
    by the filters of [Figure 16-40](#figure16-40). We can do that with a series of
    convolutions, each followed by a pooling step. Since the filters of [Figure 16-40](#figure16-40)
    are trying to match eyes, a nose, and a mouth, we know that the convolution layer
    before these filters has to produce those features. So, let’s design filters that
    search for them.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何将一个12×12的候选图像转换为[图16-40](#figure16-40)过滤器所要求的3×3网格。我们可以通过一系列卷积操作来实现，每个卷积后跟随一个池化步骤。由于[图16-40](#figure16-40)中的过滤器是用来匹配眼睛、鼻子和嘴巴的，我们知道，在这些过滤器之前的卷积层必须产生这些特征。所以，让我们设计用于搜索这些特征的过滤器。
- en: In [Figure 16-41](#figure16-41), we show three filters, each with a four by
    four footprint. They’re labeled E4, N4, and M4\. They look for an eye, a nose,
    and a mouth, respectively. The reason for placing the “4” at the end of each name
    will be clear in a moment.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图16-41](#figure16-41)中，我们展示了三个过滤器，每个过滤器的尺寸为4×4。它们分别标记为E4、N4和M4。它们分别用于检测眼睛、鼻子和嘴巴。稍后，为什么每个名字后面加上“4”的原因将会变得清晰。
- en: '![F16041](Images/F16041.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![F16041](Images/F16041.png)'
- en: 'Figure 16-41: Three filters that detect an eye, nose, and mouth'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-41：三个用于检测眼睛、鼻子和嘴巴的过滤器
- en: We can jump right in and apply these three filters to any candidate image. Since
    the images are 12 by 12, and we’re not padding, the outputs will be 10 by 10\.
    If we pool those down to 3 by 3, we can then apply the filters of [Figure 16-40](#figure16-40)
    to the output of the filters in [Figure 16-41](#figure16-41) to determine if the
    candidate is a mask looking forward, or in profile, or neither.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接开始，将这三个过滤器应用于任何候选图像。由于图像的尺寸为12×12，且我们不进行填充，输出的尺寸将为10×10。如果我们将这些输出池化为3×3，我们就可以将[图16-40](#figure16-40)中的过滤器应用于[图16-41](#figure16-41)中过滤器的输出，从而确定该候选图像是否是面向前方的面具、侧面的面具，或者都不是。
- en: But applying four by four filters requires a lot of computation. Worse, if we
    want to look for another feature (like a winking eye), we have to build another
    four by four filter and also apply that to the whole image. We can make our system
    more flexible, and also faster, by introducing another layer of convolution before
    this one.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 但应用4×4的过滤器需要大量的计算。更糟糕的是，如果我们想要寻找另一个特征（比如眨眼），我们必须构建另一个4×4的过滤器，并将其应用于整个图像。我们可以通过在此之前引入另一层卷积，使我们的系统更加灵活，同时也更快。
- en: What features can make up our E4, N4, and M4 filters of [Figure 16-41](#figure16-41)?
    If we think of each four by four filter as a grid of two by two blocks, then we
    need only four types of two by two blocks to make up all three filters. The top
    row of [Figure 16-42](#figure16-42) shows those four little blocks, and the rows
    below that show how they can be combined to make our eye, nose, and mouth filters.
    We’ve called these T, Q, L, and R for top, quartet, lower-left corner, and lower-right
    corner, respectively.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 E4、N4 和 M4 滤波器在 [图 16-41](#figure16-41) 中可以由哪些特征组成？如果我们将每个 4x4 滤波器看作是由 2x2
    块组成的网格，那么我们只需要四种 2x2 块就可以组成所有三个滤波器。[图 16-42](#figure16-42) 的顶行显示了这四个小块，下面的行则展示了它们如何组合成我们的眼睛、鼻子和嘴巴滤波器。我们分别将它们命名为
    T、Q、L 和 R，代表上方、四分之一、左下角和右下角。
- en: '![F16042](Images/F16042.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![F16042](Images/F16042.png)'
- en: 'Figure 16-42: Top row: The two by two filters T, Q, L, and R. Second row, left
    to right: Filter E4, breaking it into four smaller blocks and the tensor form
    of those blocks. The far right shows the X-ray view of the two by two by four
    filter E. Third and fourth rows: Filters N4 and M4.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16-42：顶行：2x2 滤波器 T、Q、L 和 R。第二行，从左到右：滤波器 E4，将其分解为四个小块及其张量形式。最右边显示了 2x2x4 滤波器
    E 的 X 射线视图。第三和第四行：滤波器 N4 和 M4。
- en: Starting with the eye filter E4, we break the four by four filter into four
    two by two blocks. The third drawing in the E4 row shows the four channels that
    we expect as input, one each for T, Q, L, and R, drawn as a single tensor where
    we staggered the channels. To draw that tensor more conveniently, we use the X-ray
    convention we saw in [Figure 16-40](#figure16-40). This gives us a new filter,
    of size two by two by four. This is the filter we really want to use to detect
    eyes, so we drop the “4” and just call this E.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 从眼睛滤波器 E4 开始，我们将 4x4 滤波器分解为四个 2x2 块。E4 行中的第三个图显示了我们期望作为输入的四个通道，每个通道分别对应 T、Q、L
    和 R，绘制为一个单一的张量，其中我们错开了通道。为了更方便地绘制该张量，我们使用了在 [图 16-40](#figure16-40) 中看到的 X 射线约定。这样我们就得到了一个新的滤波器，大小为
    2x2x4。这就是我们真正想用来检测眼睛的滤波器，所以我们去掉了“4”，直接称其为 E。
- en: The N and M filters are created by the same process of subdivision and assembly
    from T, Q, L, and R.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: N 和 M 滤波器是通过从 T、Q、L 和 R 进行细分和组装的相同过程创建的。
- en: Now imagine running the little T, Q, L, and R filters over a candidate image.
    They’re looking for patterns of pixels. Then the E, N, and M filters look for
    specific arrangements of T, Q, L, and R patterns. And then the F and P filters
    look for specific arrangements of E, N, and M patterns. Thus, we have a series
    of convolution layers, with each output serving as the next layer’s input. [Figure
    16-43](#figure16-43) shows this graphically.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象将小的 T、Q、L 和 R 滤波器应用于候选图像。它们在寻找像素的模式。接着，E、N 和 M 滤波器寻找 T、Q、L 和 R 模式的特定排列。然后
    F 和 P 滤波器寻找 E、N 和 M 模式的特定排列。因此，我们有一系列卷积层，每个输出都作为下一层的输入。[图 16-43](#figure16-43)
    以图形方式展示了这一过程。
- en: '![F16043](Images/F16043.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![F16043](Images/F16043.png)'
- en: 'Figure 16-43: Using three layers of convolution to analyze an input candidate'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16-43：使用三层卷积分析输入候选图像
- en: Now that we have our filters, we can start at the bottom and process an input.
    Along the way, we’ll see where to put the pooling layers.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了滤波器，可以从底部开始处理输入。在此过程中，我们将看到应该放置池化层的位置。
- en: Applying Our Filters
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用我们的滤波器
- en: Let’s start at the bottom of [Figure 16-43](#figure16-43) and apply the filters
    of Layer 1\. [Figure 16-44](#figure16-44) shows the result of sweeping the T filter
    over the 12 by 12 candidate image. Because T is 2 by 2, it doesn’t have a center,
    so we arbitrarily place its anchor in its upper-left corner. Because we’re not
    padding, and the filter is 2 by 2, the output will be 11 by 11\. In [Figure 16-44](#figure16-44),
    each location where T finds an exact match is marked in light green; otherwise,
    it’s marked in pink. We’ll call this output the T-map.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 [图 16-43](#figure16-43) 的底部开始，应用第一层的滤波器。 [图 16-44](#figure16-44) 显示了将 T
    滤波器应用于 12x12 候选图像的结果。由于 T 是 2x2 的，它没有中心，因此我们将其锚点任意放置在左上角。因为我们没有填充，且滤波器为 2x2，所以输出将是
    11x11。在 [图 16-44](#figure16-44) 中，T 找到完全匹配的每个位置都标记为浅绿色；否则，标记为粉红色。我们将此输出称为 T 图。
- en: 'Now we want to make sure that the E, N, and M filters that are looking for
    T matches still succeed even if the T matches aren’t exactly where our reference
    mask had them. As we saw in the previous section, the way to make filters robust
    to small displacements in their input is to use pooling. Let’s use the most common
    form of pooling: max pooling with two by two blocks.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想确保，即使 T 匹配的位置与参考掩码中的位置不完全相符，E、N 和 M 滤波器依然能够成功找到 T 匹配。正如我们在上一节中所看到的，使滤波器对输入中的小位移具有鲁棒性的方法是使用池化。让我们使用最常见的池化形式：二乘二块的最大池化。
- en: '![F16044](Images/F16044.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![F16044](Images/F16044.png)'
- en: 'Figure 16-44: Convolving the 12 by 12 input image with the 2 by 2 filter T
    produces the 11 by 11 output, or feature map, which we call the T-map.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16-44：将 12 乘 12 的输入图像与 2 乘 2 的 T 滤波器进行卷积，产生 11 乘 11 的输出，或称为特征图，我们称之为 T 图。
- en: '[Figure 16-45](#figure16-45) shows max pooling applied to the T-map. For each
    two by two block, if there’s at least one green value in the block, the output
    is green (recall that green elements have a value of 1, and the red are 0). When
    the pooling blocks fall off the right and bottom sides of the input, we just ignore
    the missing entries and apply pooling to the values we actually have. We call
    the result of pooling the T-pool tensor.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 16-45](#figure16-45) 显示了对 T 图进行最大池化的应用。对于每个二乘二的块，如果该块中至少有一个绿色值，则输出为绿色（回想一下，绿色元素的值为
    1，红色元素的值为 0）。当池化块落在输入的右侧和底部时，我们只需忽略缺失的条目，并对实际存在的值进行池化。我们将池化的结果称为 T-pool 张量。'
- en: '![F16045](Images/F16045.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![F16045](Images/F16045.png)'
- en: 'Figure 16-45: Applying two by two max pooling to the T-map to produce the T-pool
    tensor. Green means 1, and pink means 0\.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16-45：对 T 图应用二乘二最大池化，生成 T-pool 张量。绿色代表 1，粉色代表 0。
- en: The upper-left element of T-pool tells us if the T filter matched when placed
    on top of *any* of the four pixels in the upper left of the input. In this case,
    it did, so that element is turned green (that is, it’s assigned a value of 1).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: T-pool 的左上角元素告诉我们，当 T 滤波器放置在输入图像的左上角任意四个像素上时，是否匹配。在这种情况下，它确实匹配，因此该元素被标记为绿色（即，赋值为
    1）。
- en: Let’s repeat this process for the other three first-layer filters (Q, L, and
    R). The results are shown in the left part of [Figure 16-46](#figure16-46).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对其他三个第一层滤波器（Q、L 和 R）重复这个过程。结果显示在 [图 16-46](#figure16-46) 的左侧部分。
- en: The four T, Q, L, and R filters together produce a result with four feature
    maps, each six by six after pooling. Recall from [Figure 16-40](#figure16-40)
    that the E, N, and M filters are expecting a tensor with four channels. To combine
    these individual outputs into one tensor, we can just stack them up, as in the
    center of [Figure 16-46](#figure16-46). As usual, we then draw this as a 2D grid
    using our X-ray view convention. This gives us a tensor of four channels, which
    is just what Layer 2 is expecting as input.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 四个滤波器 T、Q、L 和 R 一起生成一个包含四个特征图的结果，每个特征图在池化后为六乘六。回想一下 [图 16-40](#figure16-40)，E、N
    和 M 滤波器期望输入一个四通道的张量。为了将这些单独的输出合并成一个张量，我们可以像 [图 16-46](#figure16-46) 中间那样将它们堆叠起来。和往常一样，我们随后使用
    X 射线视图的约定将其绘制为二维网格。这为我们提供了一个四通道的张量，正是第二层所期望的输入。
- en: '![F16046](Images/F16046.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![F16046](Images/F16046.png)'
- en: 'Figure 16-46: Left: The result of applying all four first-level filters to
    our candidate and then pooling. Center: Stacking up the outputs into a single
    tensor. Right: Drawing the six by six by four tensor in X-ray view.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16-46：左：对候选图像应用所有四个第一层滤波器并池化后的结果。中：将输出堆叠成一个单一张量。右：以 X 射线视图绘制六乘六乘四的张量。
- en: Now we can move up to the filters in Layer 2\. Let’s start with E, in [Figure
    16-47](#figure16-47).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以转到第二层的滤波器。我们从 E 滤波器开始，见 [图 16-47](#figure16-47)。
- en: '![F16047](Images/F16047.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![F16047](Images/F16047.png)'
- en: 'Figure 16-47: Applying the E filter. As before, from left to right, we have
    the input tensor, the E filter (both in our X-ray view), the result of applying
    that filter, the pooling grid, and the result of pooling.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16-47：应用 E 滤波器。如同之前，从左到右，我们依次看到输入张量、E 滤波器（都以 X 射线视图显示）、应用该滤波器的结果、池化网格和池化后的结果。
- en: '[Figure 16-47](#figure16-47) shows our input tensor (the output of Layer 1)
    and the E filter, both in X-ray view. To their right, we see the E-map resulting
    from applying the E filter, the process of applying two by two pooling to the
    E-map, and finally the E-pool feature map. We can see already how the pooling
    process allows the next filter to match the locations of the eyes, even though
    one eye is not located where it was in the reference mask.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-47](#figure16-47)展示了我们的输入张量（第一层的输出）和E滤波器，都是X光视图。在它们的右侧，我们可以看到应用E滤波器后的E图，接着是对E图应用2x2池化操作的过程，最后是E池特征图。我们已经可以看到，池化过程使得下一个滤波器能够匹配眼睛的位置，即使一个眼睛并没有出现在参考掩膜中的原始位置。'
- en: We can follow the same process for the N and M filters, producing a new output
    tensor for the second layer, as shown in [Figure 16-48](#figure16-48).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对N和M滤波器应用相同的处理过程，生成第二层的输出张量，如[图16-48](#figure16-48)所示。
- en: Now we have a three by three tensor with three channels, just right for the
    filters we created for F and P back in [Figure 16-40](#figure16-40). We’re ready
    to move up another level to Layer 3.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个三乘三的张量，包含三个通道，正好适合我们在[图16-40](#figure16-40)中为F和P创建的滤波器。我们准备好进入下一层，即第三层。
- en: '![F16048](Images/F16048.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![F16048](Images/F16048.png)'
- en: 'Figure 16-48: Computing outputs for the E, N, and M filters, then stacking
    them up into a tensor with three channels'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-48：计算E、N和M滤波器的输出，然后将它们堆叠成一个具有三个通道的张量
- en: 'This final step is easy: we just apply the F and P filters to their entire
    input, since their sizes are the same (that is, there’s no need to scan the filter
    over the image). The result is a tensor with shape one by one by two. If the element
    in the first channel in this tensor is green, then F matches, and the candidate
    should be accepted as a match to our reference. If it’s beige, the candidate’s
    not a match.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步很简单：我们只需将F和P滤波器应用到整个输入上，因为它们的尺寸相同（也就是说，不需要对图像进行滤波器扫描）。结果是一个形状为一乘一乘二的张量。如果这个张量中第一通道的元素是绿色，那么F匹配，候选图像应该被接受为与我们的参考图像匹配。如果是米色，那么候选图像不匹配。
- en: '![F16049](Images/F16049.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![F16049](Images/F16049.png)'
- en: 'Figure 16-49: Applying the F and P filters to the output tensor of the second
    layer. In this layer, each filter is the same size as the input, so the layer
    produces an output tensor of size one by one by two.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-49：将F和P滤波器应用于第二层的输出张量。在这一层中，每个滤波器与输入的大小相同，因此该层生成的输出张量的大小为一乘一乘二。
- en: And we’re done! We used three layers of convolution to characterize a candidate
    image as being either like, or unlike, a reference image. We found that our candidate
    with one eye dropped down by one pixel was still close enough to our reference
    that we should accept it.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了！我们使用了三层卷积来将候选图像与参考图像进行比较，判断它们是否相似或不相似。我们发现，候选图像中一个眼睛下移了一个像素，但仍足够接近参考图像，因此我们应该接受它。
- en: We solved this problem by creating not just a sequence of convolutions, but
    a hierarchy. Each convolution used the results of the previous one. The first
    layer looked for patterns in the pixels, the second looked for patterns of those
    patterns, and the third looked for larger patterns still, corresponding to a face
    looking forward or in profile. Pooling enabled the network to recognize a candidate
    even though one important block of pixels was shifted a little.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过创建一个层级结构来解决这个问题，而不仅仅是一个卷积序列。每一层卷积都使用了前一层的结果。第一层寻找像素中的模式，第二层寻找这些模式的模式，而第三层则寻找更大的模式，代表着正面或侧面的脸部。池化使得网络能够识别出一个候选图像，即使其中一个重要的像素块稍微发生了偏移。
- en: '[Figure 16-50](#figure16-50) shows our whole network at a glance. Since the
    only layers with neurons are convolution layers, we call this an *all-convolutional
    network.*'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[图16-50](#figure16-50)展示了我们的整个网络。由于只有卷积层包含神经元，因此我们称之为*全卷积网络*。'
- en: '![F16050](Images/F16050.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![F16050](Images/F16050.png)'
- en: 'Figure 16-50: Our all-convolutional network for evaluating masks. We’re also
    showing the input, output, and intermediate tensors. The icons with nested boxes
    are convolution layers, the trapezoids are pooling layers.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图16-50：我们的全卷积网络用于评估掩膜。我们还展示了输入、输出和中间张量。带有嵌套框的图标表示卷积层，梯形图标表示池化层。
- en: In [Figure 16-50](#figure16-50), the icons with a box in a box represent convolution,
    and the trapezoids represent pooling layers.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图16-50](#figure16-50)中，带有框中框图标表示卷积层，梯形图标表示池化层。
- en: If we want to match even more types of faces, we can just add more filters to
    the final layer. This lets us match any pattern of eyes, noses, and mouths that
    we want, with little additional cost. By reducing the size of the tensors in our
    network, pooling reduces the amount of computation we have to do. This means that
    not only is the network with pooling more robust than a version without pooling,
    it also consumes less memory and runs faster.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要匹配更多类型的面部特征，只需在最终层添加更多的过滤器。这样，我们就可以匹配任何我们想要的眼睛、鼻子和嘴巴的模式，且几乎没有额外的成本。通过减少网络中张量的大小，池化操作减少了我们需要进行的计算量。这意味着，使用池化的网络不仅比没有池化的版本更加稳健，还消耗更少的内存并且运行更快。
- en: There’s a sense in which our filters are getting more powerful as we work our
    way up the levels. For example, our eye filter E is processing a four by four
    region, though it’s only two by two itself, because each of its tensor elements
    is the result of a two by two convolution. In this way, the filters at higher
    levels in a hierarchy are able to look for large and complex features, even though
    they use only small (and therefore fast) filters.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种感觉是，当我们逐步向上工作时，我们的过滤器变得越来越强大。例如，我们的眼睛过滤器E处理的是一个4×4的区域，尽管它本身只有2×2，因为它的每个张量元素都是由一个2×2的卷积产生的。通过这种方式，层次结构中更高层次的过滤器能够寻找大型和复杂的特征，即使它们只使用小的（因此更快的）过滤器。
- en: Higher levels are able to combine the results of lower levels in multiple ways.
    Suppose we want to classify a variety of different birds in a photo. Low-level
    filters may look for feathers or beaks, while higher filters are able to combine
    different types of feathers or beaks to recognize different species of birds,
    all in a single pass through a photo. We sometimes say that using this technique
    of convolution and pooling to analyze an input is applying a *hierarchy of scales*.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 更高层次的网络能够以多种方式组合低层次的结果。假设我们要在照片中分类不同种类的鸟类。低层的过滤器可能会寻找羽毛或喙，而高层的过滤器则能够结合不同类型的羽毛或喙来识别不同种类的鸟类，所有这些都在一次通过照片的过程中完成。我们有时会说，使用卷积和池化技术来分析输入是应用了一个*层次化的尺度*。
- en: Summary
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'This chapter was all about convolution: the method of taking a filter or kernel
    (that is, a neuron with a set of weights) and moving it over an input. Each time
    we apply the filter to the input, we produce a single value of output. The filter
    may use just a single input element in its calculation, or it may have a larger
    footprint and use the values of multiple input elements. If a filter has a footprint
    that is larger than one by one, there will be places in the input where the filter
    “spills” over the edge, requiring input data that isn’t there. If we don’t place
    the filter in such places, the output has a smaller width or height (or both)
    than the input. To avoid this, we commonly pad the input by surrounding it with
    enough rings of zeroes so that the filter can be placed over every input element.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讲述的内容完全是关于卷积的：即将一个过滤器或内核（也就是一组权重的神经元）应用到输入数据上的方法。每次我们将过滤器应用到输入时，会生成一个单一的输出值。过滤器可能只使用一个输入元素来进行计算，也可能有一个更大的区域并使用多个输入元素的值。如果一个过滤器的大小大于1×1，那么在输入的某些位置，过滤器会“溢出”到边缘，这时会需要没有的数据。如果我们不将过滤器放置在这些地方，输出的宽度或高度（或者两者）会比输入的小。为了避免这种情况，我们通常通过在输入周围加上一圈零来进行填充，使得过滤器可以覆盖每个输入元素。
- en: We can bundle up many filters into a single convolution layer. In such a layer,
    typically every filter has the same footprint and the same activation function.
    Every filter produces one channel per filter. The output of the layer has one
    channel per filter.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将许多过滤器打包成一个卷积层。在这样的层中，通常每个过滤器都有相同的大小和激活函数。每个过滤器会生成一个通道。该层的输出会有每个过滤器对应的通道。
- en: If we want to change the width and height of a tensor, we can perform downsampling
    (to reduce either or both dimensions) or upsampling (to increase either or both
    dimensions). To downsample, we can use a pooling layer, which finds the average
    or maximum value in blocks from the input. To upsample, we can use an upsampling
    layer, which duplicates input elements. Either of these techniques may be combined
    with the convolution step itself. To downsample, we use striding, in which the
    filter is moved by more than one step horizontally, vertically, or both. To upsample,
    we use fractional striding, or transposed convolution, in which we insert rows
    and/or columns of zeroes between the input elements.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想改变张量的宽度和高度，我们可以进行降采样（减少一个或两个维度）或升采样（增加一个或两个维度）。为了降采样，我们可以使用池化层，它会在输入块中找到平均值或最大值。为了升采样，我们可以使用升采样层，它会复制输入元素。以上任一技术都可以与卷积步骤结合使用。为了降采样，我们使用步幅，其中滤波器会在水平方向、垂直方向或两者上移动超过一个步骤。为了升采样，我们使用分数步幅或转置卷积，在此过程中我们在输入元素之间插入零行和/或零列。
- en: We saw that by applying convolutions in a series of layers with downsampling,
    we are able to create a hierarchy of filters that work at different scales. This
    also means that the system enjoys the property of shift invariance, meaning that
    it’s able to find the patterns it seeks even if they’re not exactly where they’re
    expected to be.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，通过在一系列层中应用卷积并进行降采样，我们能够创建一个在不同尺度上工作的滤波器层次结构。这也意味着该系统具备平移不变性，即使模式的位置不完全符合预期，它仍能找到所需的模式。
- en: In the next chapter, we’ll examine real convnets and look at their filters to
    see how they do their jobs.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将检查实际的卷积神经网络，并查看它们的滤波器，以了解它们是如何完成工作的。
