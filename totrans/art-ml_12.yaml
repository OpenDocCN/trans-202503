- en: '**9'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**9'
- en: 'CUTTING THINGS DOWN TO SIZE: REGULARIZATION**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 切割事物以适应大小：正则化**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: 'A number of modern statistical methods “shrink” their classical counterparts.
    This is true for ML methods as well. In particular, the principle may be applied
    in:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现代统计方法将其经典对手进行“缩小”。这对于机器学习方法也适用。特别是，这一原则可以应用于：
- en: Boosting (covered in [Section 6.3.8](ch06.xhtml#ch06lev3sec8))
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升方法（在[第6.3.8节](ch06.xhtml#ch06lev3sec8)中讲解）
- en: Linear models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性模型
- en: Support vector machines
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: In this chapter, we’ll see why that may be advantageous and apply it to the
    linear model case. This will also lay the foundation for material in future chapters
    on support vector machines and neural networks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到为什么这样做可能是有利的，并将其应用于线性模型的情况。这也为后续章节中关于支持向量机和神经网络的内容打下基础。
- en: 9.1 Motivation
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1 动机
- en: Suppose we have sample data on human height, weight, and age. We denote the
    population means of these quantities by *μ*[*ht*], *μ*[*wt*] and *μ*[*age*]. We
    estimate them from our sample data as the corresponding sample means, ![Image](../images/unch09equ01.jpg)
    and ![Image](../images/unch09equ02.jpg).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有关于人类身高、体重和年龄的样本数据。我们将这些量的人群均值分别表示为*μ*[*ht*]、*μ*[*wt*]和*μ*[*age*]。我们从样本数据中估算它们，得到相应的样本均值，![Image](../images/unch09equ01.jpg)和![Image](../images/unch09equ02.jpg)。
- en: We then add just a bit more notation, grouping these quantities into vectors
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们添加一些符号，将这些量组合成向量
- en: '![Image](../images/ch09equ01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch09equ01.jpg)'
- en: and
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 并且
- en: '![Image](../images/ch09equ02.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch09equ02.jpg)'
- en: 'Amazingly, *James−Stein theory* says the best estimate of *μ* might NOT be
    ![Image](../images/unch09equ03.jpg). It might be a shrunken-down version of ![Image](../images/unch09equ03.jpg),
    say, 0.9![Image](../images/unch09equ03.jpg):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 神奇的是，*James−Stein理论*表示，*μ*的最佳估计值可能并不是![Image](../images/unch09equ03.jpg)。它可能是![Image](../images/unch09equ03.jpg)的一个缩小版，比如，0.9×![Image](../images/unch09equ03.jpg)：
- en: '![Image](../images/ch09equ03.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch09equ03.jpg)'
- en: And the higher the dimension (3 here), the more shrinking needs to be done.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，维度越高（这里是3），需要进行的缩小就越多。
- en: 'The intuition is this: for many samples, there are a few data points that are
    extreme on the fringes of the distribution. These points skew our estimators in
    the direction of being too large. So, it is optimal to shrink the estimators.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 直觉是这样的：对于许多样本，有一些数据点在分布的边缘极端。这些点会使我们的估计值偏向过大。所以，最优的做法是缩小估计值。
- en: 'Note that, usually, different components of a vector will be shrunken by different
    amounts. Instead of [Equation 9.3](ch09.xhtml#ch09equ03), the best estimator might
    be:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，通常，向量的不同分量会被不同程度地缩小。与[方程9.3](ch09.xhtml#ch09equ03)不同，最佳估计量可能是：
- en: '![Image](../images/ch09equ04.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch09equ04.jpg)'
- en: In this example, the second component actually expanded rather than shrank.
    Shrinking refers to the overall size of the vector (defined in the next section)
    and not the individual components.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，第二个分量实际上扩展了而不是缩小。缩小是指向量的整体大小（将在下一节定义），而不是单个分量。
- en: How much shrinking should be done? In practice, this is typically decided by
    our usual approach of cross-validation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 应该进行多少缩小？在实际操作中，这通常通过我们常用的交叉验证方法来决定。
- en: Putting aside the mathematical theory—it’s quite deep—the implication for us
    in this book is that, for instance, the least squares estimator ![Image](../images/betacap1.jpg)
    of the population coefficient vector *β* in the linear model is often too large
    and should be shrunken. Most interesting, *this turns out to be a possible remedy
    for overfitting*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 抛开数学理论不谈——它相当深奥——对我们本书的意义在于，例如，线性模型中人口系数向量*β*的最小二乘估计量![Image](../images/betacap1.jpg)通常过大，应当进行缩小。最有趣的是，*这恰好是解决过拟合的一个可能方法*。
- en: 9.2 Size of a Vector
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2 向量的大小
- en: Is the vector (15.2,3.0,−6.8) “large”? What do we mean by its size, anyway?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 向量(15.2,3.0,−6.8)“大”吗？我们到底是什么意思它的大小呢？
- en: There are two main measures, called *ℓ*[1] and *ℓ*[2], that are denoted by the
    “norm” notation, || || (two pairs of vertical bars). So the two norms are denoted
    || ||[1] and || ||[2]. For the example above, the *ℓ*[1] norm is
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个主要的度量，称为*ℓ*[1]和*ℓ*[2]，它们通过“范数”符号表示，即|| ||（两对竖线）。所以这两个范数分别表示为|| ||[1]和|| ||[2]。对于上面的例子，*ℓ*[1]范数是
- en: '![Image](../images/ch09equ05.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch09equ05.jpg)'
- en: 'that is, the sum of the absolute values of the vector elements. Here is the
    *ℓ*[2] case:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 即，向量元素的绝对值之和。这里是*ℓ*[2]的情况：
- en: '![Image](../images/ch09equ06.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch09equ06.jpg)'
- en: This is the square root of the sums of squares of the vector elements. (Readers
    who remember their school geometry may notice that in 2 dimensions, this is simply
    the length of the diagonal of a right triangle—the famous Pythagorean theorem.)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是向量元素平方和的平方根。（记得几何的读者可能会注意到，在二维情况下，这仅仅是直角三角形斜边的长度——著名的毕达哥拉斯定理。）
- en: 9.3 Ridge Regression and the LASSO
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3 岭回归与LASSO
- en: For years, James−Stein theory was mainly a mathematical curiosity suitable for
    theoretical research but not affecting mainstream data analysis. There was some
    usage of *ridge regression*, to be introduced below, but even that was limited.
    The big change came from the development of the *Least Absolute Shrinkage and
    Selection Operator (LASSO)* and its adoption by the ML community.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，詹姆斯-斯坦因理论主要是一个数学上的好奇，适合理论研究，但对主流数据分析没有影响。虽然曾有人使用过*岭回归*，下面将介绍，但即便如此，也仅限于有限的使用。重大变化来自*最小绝对收缩与选择算子（LASSO）*的开发及其在机器学习社区中的采用。
- en: '***9.3.1 How They Work***'
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***9.3.1 它们是如何工作的***'
- en: 'Recall the basics of the least squares method for linear models, say, for the
    case of one feature: we choose ![Image](../images/unch09equ07.jpg) to minimize
    the sum of squared prediction errors, as in [Equation 8.10](ch08.xhtml#ch08equ10).
    For convenience, here is a copy of that expression:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下最小二乘法在线性模型中的基本概念，例如，假设有一个特征：我们选择![](../images/unch09equ07.jpg)来最小化平方预测误差的和，如[方程8.10](ch08.xhtml#ch08equ10)所示。为了方便，这里是该表达式的副本：
- en: '![Image](../images/ch09equ07.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/ch09equ07.jpg)'
- en: 'The idea of ridge regression was to “put a damper” on that by adding vector
    size limitation. We now minimize [Equation 9.7](ch09.xhtml#ch09equ07), *subject
    to the following constraint*:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归的思想是通过添加向量大小限制来“加上阻尼”。我们现在最小化[方程9.7](ch09.xhtml#ch09equ07)，*满足以下约束*：
- en: '![Image](../images/ch09equ08.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/ch09equ08.jpg)'
- en: Here *η* > 0 is a hyperparameter set by the user, say, via cross-validation.
    The minimizing values of ![Image](../images/unch08equ10.jpg) and ![Image](../images/unch08equ09.jpg)
    are the ridge coefficients.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*η* > 0是一个由用户设置的超参数，例如通过交叉验证设置。最小化的![](../images/unch08equ10.jpg)和![](../images/unch08equ09.jpg)的值即为岭回归系数。
- en: Here is the intuition behind such an approach. We are basically saying we wish
    to minimize the sum of squares as before *but* without allowing ![Image](../images/unch09equ08.jpg)
    to get too large. It’s a compromise between, on the one hand, predicting the *Y**[i]*
    well and, on the other, limiting the size of ![Image](../images/unch09equ08.jpg).
    (We hope that the shrinking will improve our prediction of future cases.) The
    hyperparameter *η* controls that trade-off.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是这种方法背后的直觉。我们基本上是说，我们希望最小化平方和，就像以前一样，*但是*不允许![](../images/unch09equ08.jpg)变得太大。这是在一方面良好预测*Y**[i]*和另一方面限制![](../images/unch09equ08.jpg)大小之间的折中。（我们希望这种收缩会提高我们对未来情况的预测。）超参数*η*控制这种权衡。
- en: 'It can be shown that this constrained minimization problem is equivalent to
    choosing ![Image](../images/unch08equ10.jpg) and ![Image](../images/unch08equ09.jpg)
    to minimize the quantity:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明，这个约束最小化问题等价于选择![](../images/unch08equ10.jpg)和![](../images/unch08equ09.jpg)，使得最小化以下量：
- en: '![Image](../images/ch09equ09.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/ch09equ09.jpg)'
- en: Here *λ* > 0 is a hyperparameter that takes the place of *η*, which, again,
    is typically set via cross-validation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*λ* > 0是一个超参数，替代了*η*，并且通常通过交叉验证来设置。
- en: 'This formulation ([Equation 9.9](ch09.xhtml#ch09equ09)) is actually the standard
    definition of ridge regression. The *η* version is easier to explain in terms
    of the James−Stein context, but this *λ* formulation should also make intuitive
    sense: that last term “penalizes” us in our minimizing the sum of squares. The
    larger we set *λ*, the greater the penalty, forcing us to limit the size of ![Image](../images/unch09equ08.jpg).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式（[方程9.9](ch09.xhtml#ch09equ09)）实际上是岭回归的标准定义。*η*版本在詹姆斯-斯坦因的背景下更容易解释，但这个*λ*的公式也应该是直观的：“惩罚”项使我们在最小化平方和时受到限制。我们设置*λ*的值越大，惩罚越大，从而迫使我们限制![](../images/unch09equ08.jpg)的大小。
- en: The LASSO version is almost the same as ridge but with an *ℓ*[1] “damper” term
    rather than *ℓ*[2]. It finds the values of ![Image](../images/unch08equ10.jpg)
    and ![Image](../images/unch08equ09.jpg) that minimize
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: LASSO版本几乎与岭回归相同，只不过使用了*ℓ*[1]“阻尼”项，而不是*ℓ*[2]。它找到的![](../images/unch08equ10.jpg)和![](../images/unch08equ09.jpg)的值使得
- en: '![Image](../images/ch09equ10.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/ch09equ10.jpg)'
- en: In terms of *η*, for LASSO we minimize
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在*η*的情况下，对于LASSO，我们进行最小化
- en: '![Image](../images/ch09equ11.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/ch09equ11.jpg)'
- en: 'subject to:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 受限于：
- en: '![Image](../images/ch09equ12.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch09equ12.jpg)'
- en: '***9.3.2 The Bias-Variance Trade-off, Avoiding Overfitting***'
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***9.3.2 偏差-方差权衡，避免过拟合***'
- en: 'A major reason that the idea of shrinkage—often called *regularization*—has
    had such an impact on statistics and ML is that it is a tool to avoid overfitting.
    Here are the issues:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 收缩思想——通常称为*正则化*——对统计学和机器学习产生如此巨大影响的一个主要原因是，它是避免过拟合的工具。这里有几个问题：
- en: On the one hand, we want to make the prediction sum of squares as small as possible,
    which can be shown to eliminate bias.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一方面，我们希望尽可能使预测的平方和最小，这可以证明消除了偏差。
- en: On the other hand, recall from [Section 8.8](ch08.xhtml#ch08lev8) that the sum
    of squares can be overly optimistic and thus smaller than we would get in predicting
    new cases in the future. A small value for that sum of squares may come with a
    large variance, due in part to the influence of extreme data points, as discussed
    earlier. Shrinkage reduces variance— a smaller quantity varies less than a larger
    one—thus partially neutralizing the pernicious effects of the extreme points.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，回想一下[第8.8节](ch08.xhtml#ch08lev8)中提到的，平方和可能过于乐观，因此可能小于我们在预测未来新案例时得到的结果。平方和的一个小值可能伴随着较大的方差，部分原因是极端数据点的影响，如前所述。收缩可以减少方差——较小的量变化较少——从而在一定程度上中和极端点的有害影响。
- en: So the hyperparameter *λ* is used to control where we want to be in that Bias-Variance
    Trade-off. Overfitting occurs when we are on the wrong side of that trade-off.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，超参数*λ*用于控制我们在偏差-方差权衡中希望处于哪个位置。过拟合发生在我们位于该权衡的错误一侧时。
- en: '*The bottom line*: shrinkage reduces variance, and if this can be done without
    increasing bias much, it’s a win.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*结论*：收缩减少了方差，如果可以在不显著增加偏差的情况下做到这一点，那么就是一个胜利。'
- en: Again, regularization is used not only in the linear model, the case studied
    in this chapter, but also in support vector machines, neural nets, and so on.
    It can even be used in principal component analysis.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，正则化不仅在本章讨论的线性模型中使用，也在支持向量机、神经网络等中使用。它甚至可以应用于主成分分析。
- en: '***9.3.3 Relation Between*** *λ**, n, and p***'
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***9.3.3 *λ*, n和p之间的关系***'
- en: Again, the Bias-Variance Trade-off notion plays a central role here, with implications
    for dataset size. The larger *n* is (that is, the larger the sample size), the
    smaller the variance in ![Image](../images/betacap1.jpg), which means the lesser
    the need to shrink.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，偏差-方差权衡概念在这里扮演着核心角色，且对数据集大小有影响。*n*（即样本量）越大，![Image](../images/betacap1.jpg)的方差越小，这意味着收缩的需求越小。
- en: In other words, for large datasets, we may not need regularization. But recall
    from [Chapter 3](ch03.xhtml) that “large *n*” here is meant both in absolute terms
    and relative to *p*—for example, by the ![Image](../images/unch08equ08.jpg) criterion
    following [Equation 3.2](ch03.xhtml#ch03equ02). So, a very large dataset may still
    need regularization if there are numerous features.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，对于大型数据集，我们可能不需要正则化。但回想一下[第3章](ch03.xhtml)中提到的，“大*n*”既是绝对意义上的大，也相对于*p*而言——例如，按照[方程3.2](ch03.xhtml#ch03equ02)中的标准。所以，如果有大量特征，即使是非常大的数据集，也可能仍然需要正则化。
- en: In any event, the surest way to settle whether shrinkage is needed in a particular
    setting is to try it, once again, with cross-validation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，确定是否在特定情境中需要收缩的最可靠方法是再次通过交叉验证来尝试。
- en: '***9.3.4 Comparison, Ridge vs. LASSO***'
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***9.3.4 比较，岭回归与LASSO***'
- en: The advantage of ridge regression is that its calculation is simple. There is
    an explicit, closed-form solution—that is, it is noniterative; the LASSO requires
    iterative computation (though it does not have convergence problems).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归的优势在于其计算简单。有一个明确的闭式解——也就是说，它是非迭代的；而LASSO则需要迭代计算（尽管它没有收敛问题）。
- en: But the success of the LASSO is due to its providing a *sparse* solution, meaning
    that often many of the elements of ![Image](../images/betacap1.jpg) are 0s. The
    smaller we set *η*, the more 0s we have. We then discard the features having ![Image](../images/unch08equ07.jpg)
    = 0, thereby achieving dimension reduction. Note that, of course, the resulting
    nonzero ![Image](../images/unch08equ07.jpg) values are different from the corresponding
    OLS values.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 但LASSO成功的原因在于它提供了一个*稀疏*解，这意味着通常许多![Image](../images/betacap1.jpg)的元素是0。我们将*η*设得越小，0的数量就越多。然后，我们丢弃那些![Image](../images/unch08equ07.jpg)
    = 0的特征，从而实现降维。需要注意的是，当然，最终非零的![Image](../images/unch08equ07.jpg)值与相应的OLS值是不同的。
- en: 9.4 Software
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4 软件
- en: 'Once again, we will use a `qe*`-series function, `qeLASSO()`, with the following
    call form:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用一个`qe*`系列的函数，`qeLASSO()`，并使用以下调用形式：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The function wraps `cv.glmnet()` in the `glmnet` package. That package allows
    the user to specify either ridge regression or LASSO via the argument `alpha`,
    setting that value to 0 or 1, respectively; the default is LASSO. One can also
    set `alpha` to an intermediate value, combining the two approaches, something
    termed the *elastic net*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数在`glmnet`包中包装了`cv.glmnet()`。该包允许用户通过参数`alpha`指定岭回归或LASSO，分别将该值设置为0或1；默认值是LASSO。用户还可以将`alpha`设置为中间值，结合这两种方法，这被称为*弹性网*。
- en: The `cv.glmnet()` algorithm will start with a huge value of *λ* and then progressively
    reduce *λ*. This corresponds to starting with a very tiny value of *η* and progressively
    increasing it. Since a very tiny value of *η* means that no features are allowed,
    progressively increasing it means we start adding features. It is all arranged
    so that we add one feature at a time. The algorithm computes MSPE or OME at each
    step, using its own built-in cross-validation. The return value of the `qeLASSO()`
    wrapper is actually the object returned by `cv.glmnet()`, with a few additional
    components, such as `testAcc`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`cv.glmnet()`算法将从一个非常大的*λ*值开始，然后逐渐减少*λ*。这相当于从一个非常小的*η*值开始，然后逐步增加它。由于一个非常小的*η*值意味着不允许任何特征，逐步增加它意味着我们开始添加特征。整个过程是按顺序添加每个特征的。算法在每个步骤上计算MSPE或OME，并使用其内置的交叉验证。`qeLASSO()`包装器的返回值实际上是`cv.glmnet()`返回的对象，带有一些附加组件，如`testAcc`。'
- en: That object will include one set of results for each value of *λ* run by the
    code. So, there will be one ![Image](../images/betacap1.jpg) vector for each *λ*.
    However, when we do subsequent prediction, the code uses the specific value of
    *λ* that had the smallest mean cross-validated prediction error.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该对象将包含每个*λ*值运行时的一组结果。因此，每个*λ*值将有一个 ![Image](../images/betacap1.jpg) 向量。然而，当我们进行后续预测时，代码使用的是具有最小平均交叉验证预测误差的特定*λ*值。
- en: '9.5 Example: NYC Taxi Data'
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5 示例：纽约市出租车数据
- en: Let’s return to the New York City taxi data from [Section 5.3](ch05.xhtml#ch05lev3).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到[第5.3节](ch05.xhtml#ch05lev3)中的纽约市出租车数据。
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We see that the features definitely are helpful in prediction, yielding a large
    reduction in MAPE relative to just using the overall mean for prediction.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到这些特征在预测中确实很有帮助，相较于仅使用整体均值进行预测，它大大降低了MAPE。
- en: Recall that the LASSO typically yields a sparse ![Image](../images/betacap1.jpg),
    meaning that most of the coefficients are 0s. In this way, the LASSO can be used
    for dimension reduction, in addition to being used as a predictive model in its
    own right. Let’s explore this for the taxi data by inspecting the `coefs` component
    of the output.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，LASSO通常会产生一个稀疏的 ![Image](../images/betacap1.jpg)，这意味着大多数系数都是0。通过这种方式，LASSO不仅可以用于预测模型本身，还可以用于降维。让我们通过检查输出中的`coefs`组件来探索一下出租车数据。
- en: Note first that, as usual, the features that are R factors are converted to
    dummy variables. How many are there?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意，像往常一样，作为R因子的特征会转换为虚拟变量。那么有多少个呢？
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Considering that the original dataset had only 5 features, 475 is quite a lot!
    But remember, two of our features were the pickup and dropoff locations, of which
    there are hundreds, and thus hundreds of dummies.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到原始数据集只有5个特征，475个特征其实已经很多了！但请记住，我们的两个特征是接送地点，这些地点有数百个，因此有数百个虚拟变量。
- en: Well, which coefficients are nonzero?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，哪些系数是非零的呢？
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Only 11 coefficients are nonzero, including pickup location 132 and dropoff
    location 1\. That’s impressive dimension reduction.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 只有11个系数是非零的，包括接送地点132和1。那是相当出色的降维效果。
- en: '9.6 Example: Airbnb Data'
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.6 示例：Airbnb数据
- en: Let’s revisit the Airbnb dataset analyzed in [Section 8.4.3](ch08.xhtml#ch08lev4sec3),
    where we are predicting monthly rent.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视在[第8.4.3节](ch08.xhtml#ch08lev4sec3)中分析的Airbnb数据集，我们正在预测月租。
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `qeLASSO()` function wraps `cv.glmnet()`. The latter has a generic `plot()`
    function, which we can access here:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`qeLASSO()`函数包装了`cv.glmnet()`。后者有一个通用的`plot()`函数，我们可以在这里访问：'
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The plot, shown in [Figure 9-1](ch09.xhtml#ch09fig01), displays the classic
    Bias-Variance Tradeoff, which is essentially U-shaped. As *λ* increases to the
    right (lower horizontal axis; the log is used), the number of nonzero coefficients
    (upper horizontal axis) decreases. At first, this produces reductions in MSPE.
    However, after we hit about 26 nonzero coefficients, this quantity rises. In bias-variance
    terms, increasing *λ* brought large reductions in variance with little increase
    in bias. But after hitting 26 features, the bias became the dominant factor.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图9-1](ch09.xhtml#ch09fig01)所示的图表展示了经典的偏差-方差权衡，其形状本质上是U形的。当*λ*增加时（右侧的横坐标轴；使用了对数），非零系数的数量（上方横坐标轴）减少。起初，这会减少MSPE。然而，在大约26个非零系数时，这个数量开始上升。从偏差-方差的角度来看，增加*λ*显著降低了方差，而偏差几乎没有增加。但在达到26个特征后，偏差成为主导因素。
- en: At any rate, using 26 features, corresponding to *λ* ≈ *e*⁴ = 53.9, seems best,
    yielding a very substantial improvement in prediction accuracy. (Standard errors
    are also shown in the vertical bars extending above and below the curve.)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，使用26个特征，相当于*λ* ≈ *e*⁴ = 53.9，似乎是最好的选择，能够显著提高预测精度。（标准误差也显示在曲线上下的垂直条中。）
- en: '![Image](../images/ch09fig01.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/ch09fig01.jpg)'
- en: '*Figure 9-1: MSPE, Airbnb data*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9-1：MSPE，Airbnb数据*'
- en: Let’s try a prediction, say, taking row 18 from our data and changing the security
    deposit to $360 and the rating to 92\. What would be our predicted value for the
    rent?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试预测，比如从我们的数据中取第18行，并将保证金改为360美元，评分改为92。那么我们预测的租金值是多少？
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How much did our shrinkage approach change the coefficients compared to the
    OLS output in [Section 8.4.4](ch08.xhtml#ch08lev4sec4)? Well, for example, the
    estimated average premium for living in ZIP code 94123 was $1,639.61 with the
    OLS model. What is it now, using LASSO?
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的收缩方法与[第8.4.4节](ch08.xhtml#ch08lev4sec4)中OLS输出相比，改变了多少系数？例如，在OLS模型中，居住在94123邮政编码的估计平均溢价为1,639.61美元。那么，使用LASSO后是多少呢？
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Ah, so it did shrink. On the other hand, LASSO shrinks the vector, not necessarily
    individual elements, which could even grow a bit. Of course, many elements were
    indeed shrunken all the way to 0.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 啊，结果确实进行了收缩。另一方面，LASSO收缩的是向量，而不一定是单独的元素，某些元素甚至可能稍微增加。当然，许多元素确实被收缩到了0。
- en: 'Recall how the process works: it begins with no features in the model at all,
    which corresponds to a huge value of *λ*. At each step, *λ* is reduced, possibly
    resulting in our acquiring a new feature. We can also view the order in which
    the features were brought into the model:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下这个过程是如何工作的：它一开始模型中没有任何特征，这对应着一个非常大的*λ*值。在每一步中，*λ*会被减少，这可能导致我们获取一个新的特征。我们还可以查看特征引入模型的顺序：
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Not too surprisingly, the first feature chosen by the process was the number
    of bedrooms. But perhaps less intuitively, the process’s second choice was a dummy
    variable regarding guests. Our example above, a dummy for ZIP code 94123, came
    in at the 17th step. One might view this ordering as a report on the importance
    of each selected feature.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 不太令人惊讶的是，过程选择的第一个特征是卧室数量。但也许不太直观的是，过程选择的第二个特征是关于客人的虚拟变量。我们上面举的例子，94123邮政编码的虚拟变量，在第17步时被选择。可以将这个顺序视为对每个选定特征重要性的报告。
- en: '**NOTE**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Since our emphasis in this book is on prediction from data rather than description
    of data, we have not discussed the issue of feature importance before now. We
    only present it here as an aid to understanding how LASSO works. However, it is
    available in some of the packages used in this book. For instance, see the* importance()
    *function in the* randomForests *package.*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*由于本书的重点是从数据中进行预测，而不是对数据进行描述，因此我们之前没有讨论特征重要性的问题。我们这里只是将其作为理解LASSO如何工作的辅助工具。不过，某些在本书中使用的软件包中提供了这个功能。例如，参见*randomForests*包中的*importance()*函数。*'
- en: '9.7 Example: African Soil Data'
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.7 示例：非洲土壤数据
- en: As noted in [Section 6.2.4](ch06.xhtml#ch06lev2sec4), the importance of the
    African soil dataset is that it has *p* > *n*, with the number of features being
    almost triple the number of data points. This is considered a very difficult situation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第6.2.4节](ch06.xhtml#ch06lev2sec4)所述，非洲土壤数据集的重要性在于它具有*p* > *n*，特征的数量几乎是数据点数量的三倍。这被认为是一个非常困难的情况。
- en: Remember, to many analysts, the very essence of LASSO is dimension reduction,
    so it will be very interesting to see how LASSO does on this data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，对于许多分析师来说，LASSO的精髓就在于降维，所以看到LASSO如何处理这些数据将是非常有趣的。
- en: '***9.7.1 LASSO Analysis***'
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***9.7.1 LASSO分析***'
- en: 'Again, we will predict soil acidity, pH:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们将预测土壤酸度，pH值：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `nzero` component of the output tells us how many features the process
    has chosen at each step:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的`nzero`组件告诉我们每一步过程中选择了多少特征：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And the `lambda` component gives the corresponding *λ* values:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`lambda`组件给出了相应的*λ*值：'
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The corresponding graph is shown in [Figure 9-2](ch09.xhtml#ch09fig02).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的图形显示在[图9-2](ch09.xhtml#ch09fig02)中。
- en: '![Image](../images/ch09fig02.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch09fig02.jpg)'
- en: '*Figure 9-2: African soil data*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9-2：非洲土壤数据*'
- en: Here we have a rather incomplete result. The smallest MSPE came from the smallest
    *λ* value tried by the software (0.003426429), but the curve seems to suggest
    that even smaller values would do better. So, we might rerun with a custom set
    of *λ* values, rather than using the default value sequence.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们得到的是一个相当不完整的结果。最小的MSPE来自软件尝试的最小*λ*值（0.003426429），但曲线似乎暗示即使更小的值也会表现得更好。因此，我们可能会使用一组自定义的*λ*值重新运行，而不是使用默认的值序列。
- en: Nevertheless, even if we choose to settle for *λ* = 0.003426429, that value
    would be pretty good. LASSO retained 156 features out of the original 3,578\.
    That’s quite a dimension reduction.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，即使我们选择了*λ* = 0.003426429，这个值也已经相当不错了。LASSO从原始的3,578个特征中保留了156个。这是一个相当大的降维。
- en: '9.8 Optional Section: The Famous LASSO Picture'
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.8 可选部分：著名的LASSO图
- en: This section has a bit more mathematical content, and it can be safely skipped,
    as it is not used in the sequel. However, readers who are curious as to why the
    LASSO retains some of the original features but excludes others may find this
    section helpful.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含更多的数学内容，可以安全跳过，因为它在后续部分并不使用。然而，对为什么LASSO保留了一些原始特征并排除其他特征感兴趣的读者，可能会发现这一部分内容有帮助。
- en: As mentioned, a key property of the LASSO is that it usually provides a *sparse*
    solution for ![Image](../images/betacap1.jpg), meaning that many of the ![Image](../images/unch08equ07.jpg)
    values are 0\. In other words, many features are discarded, thus providing a means
    of dimension reduction. [Figure 9-3](ch09.xhtml#ch09fig03) shows why. Here is
    how it works.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，LASSO的一个关键特性是它通常提供*稀疏*的解决方案，表示许多![Image](../images/unch08equ07.jpg)值为0。换句话说，许多特征被丢弃，从而提供了一种降维的方法。[图9-3](ch09.xhtml#ch09fig03)展示了原因。其工作原理如下：
- en: '![Image](../images/ch09fig03.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch09fig03.jpg)'
- en: '*Figure 9-3: Feature subsetting nature of the LASSO*'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9-3：LASSO的特征子集性质*'
- en: '[Figure 9-3](ch09.xhtml#ch09fig03) is for the case of *p* = 2 predictors, whose
    coefficients are *b*[1] and *b*[2]. (For simplicity, we assume there is no constant
    term *b*[0].) Let *U* and *V* denote the corresponding features. Write *b* = (*b*[1],
    *b*[2]) for the vector of the *b**[i]*.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-3](ch09.xhtml#ch09fig03)展示的是*p* = 2个预测变量的情况，它们的系数分别是*b*[1]和*b*[2]。（为简便起见，我们假设没有常数项*b*[0]。）让*U*和*V*分别表示相应的特征。将*b*
    = (*b*[1], *b*[2])表示为*b*[i]*的向量。'
- en: 'Without shrinkage, we would choose *b* to minimize the sum of squared errors:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有收缩，我们会选择*b*来最小化平方误差的和：
- en: '![Image](../images/ch09equ13.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch09equ13.jpg)'
- en: The horizontal and vertical axes are for *b*[1] and *b*[2], as shown. The key
    point is that for any value that we set in [Equation 9.13](ch09.xhtml#ch09equ13)
    for SSE, the points (*b*[1], *b*[2]) that solve the resulting equation form an
    ellipse. The value of (*b*[1], *b*[2]) computed by the LASSO is just one point
    in the given ellipse; lots of other (*b*[1], *b*[2]) values yield the same SSE.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 横轴和纵轴分别表示*b*[1]和*b*[2]，如图所示。关键点是，对于我们在[方程9.13](ch09.xhtml#ch09equ13)中为SSE设定的任何值，解出方程的(*b*[1],
    *b*[2])点都会形成一个椭圆。LASSO计算出的(*b*[1], *b*[2])值只是给定椭圆中的一个点；许多其他(*b*[1], *b*[2])值也能得到相同的SSE。
- en: As we vary the SSE value, we get various concentric ellipses, two of which are
    shown in [Figure 9-3](ch09.xhtml#ch09fig03). Larger values of SSE correspond to
    larger ellipses.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们改变SSE值时，我们会得到各种同心椭圆，其中两个在[图9-3](ch09.xhtml#ch09fig03)中展示。较大的SSE值对应于较大的椭圆。
- en: Now, what happens when we give the LASSO algorithm a value of *λ* or *η*? As
    noted earlier, either quantity can be used, but it will be easier to assume the
    latter. So, what will the LASSO algorithm do when we give it a value of *η*?
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们给LASSO算法一个*λ*或*η*的值时，会发生什么呢？如前所述，可以使用任一数量，但我们假设使用后者会更容易。那么，当我们给LASSO算法一个*η*的值时，它会做什么呢？
- en: 'The algorithm will minimize SSE, subject to the constraint:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法将最小化SSE，同时满足约束条件：
- en: '![Image](../images/ch09equ14.jpg)'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Image](../images/ch09equ14.jpg)'
- en: Let’s denote that minimum value of SSE by SSE*[alg]*, and denote the corresponding
    (*b*[1], *b*[2]) value by (*b*[1], *b*[2])*[alg]*.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们用SSE*[alg]*表示SSE的最小值，并用对应的(*b*[1], *b*[2])值表示(*b*[1], *b*[2])*[alg]*。
- en: On the one hand, the point (*b*[1], *b*[2])*[alg]* will be on the ellipse associated
    with SSE*[alg]*.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一方面，点 (*b*[1], *b*[2])*[alg]* 将位于与 SSE*[alg]* 相关的椭圆上。
- en: On the other hand, [Equation 9.14](ch09.xhtml#ch09equ14) says that (*b*[1],
    *b*[2])*[alg]* must be somewhere in the diamond in the picture, whose corners
    are at (*η*, 0), (0, *η*), and so on.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，[公式9.14](ch09.xhtml#ch09equ14)表明，(*b*[1], *b*[2])*[alg]* 必须位于图中的菱形内部，菱形的角落坐标为(*η*,
    0)、(0, *η*)，依此类推。
- en: So, (*b*[1], *b*[2])*[alg]* must lie on an ellipse that intersects with the
    diamond.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，(*b*[1], *b*[2])*[alg]* 必须位于与菱形相交的椭圆上。
- en: But remember, we want SSE to be as small as possible, subject to [Equation 9.14](ch09.xhtml#ch09equ14).
    Recall, too, that smaller SSE values correspond to smaller ellipses. So the ellipse
    for SSE*[alg]* must *just barely touch the diamond*, as seen in the outer ellipse
    in [Figure 9-3](ch09.xhtml#ch09fig03).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但请记住，我们希望 SSE 尽可能小，同时满足[公式9.14](ch09.xhtml#ch09equ14)。还要记住，较小的 SSE 值对应着较小的椭圆。因此，SSE*[alg]*
    的椭圆必须*刚好接触到菱形*，如[图9-3](ch09.xhtml#ch09fig03)中的外椭圆所示。
- en: In the figure, the “just barely touch” point is at one of the corners of the
    diamond. And each of the corners has either *b*[1] or *b*[2] equal to 0—sparsity!
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图中，“刚好接触”点位于菱形的一个角落。每个角落的 *b*[1] 或 *b*[2] 都等于 0——这就是稀疏性！
- en: 'Is that sparsity some kind of coincidence? No! Here’s why: depending on the
    relative values of our input data (*U**[i]*, *V**[i]*), the ellipses in the picture
    will have different orientations. The ones in the picture are pointing approximately
    “northwest and southeast.” But it is clear from inspection that most orientations
    will result in the touch point being at one of the corners and hence a sparse
    solution.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那种稀疏性是巧合吗？不是！原因如下：根据输入数据的相对值(*U**[i]*, *V**[i]*)，图中的椭圆会有不同的方向。图中的椭圆大约指向“西北和东南”。但通过检查可以明显看出，大多数方向都会导致接触点位于菱形的一个角落，从而产生稀疏解。
- en: Thus the LASSO will usually be sparse, which is the major reason for its popularity.
    And what about ridge regression? In that case, the diamond becomes a circle, so
    there is no sparseness property.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LASSO 通常会产生稀疏解，这也是它受欢迎的主要原因。那么岭回归呢？在这种情况下，菱形变成了一个圆形，因此没有稀疏性。
- en: 9.9 Coming Up
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.9 即将到来
- en: Next, we take an entirely different approach. With k-NN and decision trees,
    no linearity was used, and then this property was explicitly assumed. In [Part
    IV](part4.xhtml), we cover methods in which linearity is used, but only indirectly.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们采用一种完全不同的方法。使用k-NN和决策树时，没有使用线性关系，随后这个属性被显式假设。在[第四部分](part4.xhtml)中，我们介绍了在间接使用线性关系的情况下的方法。
