- en: '**9'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CUTTING THINGS DOWN TO SIZE: REGULARIZATION**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A number of modern statistical methods “shrink” their classical counterparts.
    This is true for ML methods as well. In particular, the principle may be applied
    in:'
  prefs: []
  type: TYPE_NORMAL
- en: Boosting (covered in [Section 6.3.8](ch06.xhtml#ch06lev3sec8))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we’ll see why that may be advantageous and apply it to the
    linear model case. This will also lay the foundation for material in future chapters
    on support vector machines and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Motivation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose we have sample data on human height, weight, and age. We denote the
    population means of these quantities by *μ*[*ht*], *μ*[*wt*] and *μ*[*age*]. We
    estimate them from our sample data as the corresponding sample means, ![Image](../images/unch09equ01.jpg)
    and ![Image](../images/unch09equ02.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: We then add just a bit more notation, grouping these quantities into vectors
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch09equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch09equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Amazingly, *James−Stein theory* says the best estimate of *μ* might NOT be
    ![Image](../images/unch09equ03.jpg). It might be a shrunken-down version of ![Image](../images/unch09equ03.jpg),
    say, 0.9![Image](../images/unch09equ03.jpg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch09equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: And the higher the dimension (3 here), the more shrinking needs to be done.
  prefs: []
  type: TYPE_NORMAL
- en: 'The intuition is this: for many samples, there are a few data points that are
    extreme on the fringes of the distribution. These points skew our estimators in
    the direction of being too large. So, it is optimal to shrink the estimators.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that, usually, different components of a vector will be shrunken by different
    amounts. Instead of [Equation 9.3](ch09.xhtml#ch09equ03), the best estimator might
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch09equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this example, the second component actually expanded rather than shrank.
    Shrinking refers to the overall size of the vector (defined in the next section)
    and not the individual components.
  prefs: []
  type: TYPE_NORMAL
- en: How much shrinking should be done? In practice, this is typically decided by
    our usual approach of cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Putting aside the mathematical theory—it’s quite deep—the implication for us
    in this book is that, for instance, the least squares estimator ![Image](../images/betacap1.jpg)
    of the population coefficient vector *β* in the linear model is often too large
    and should be shrunken. Most interesting, *this turns out to be a possible remedy
    for overfitting*.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Size of a Vector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Is the vector (15.2,3.0,−6.8) “large”? What do we mean by its size, anyway?
  prefs: []
  type: TYPE_NORMAL
- en: There are two main measures, called *ℓ*[1] and *ℓ*[2], that are denoted by the
    “norm” notation, || || (two pairs of vertical bars). So the two norms are denoted
    || ||[1] and || ||[2]. For the example above, the *ℓ*[1] norm is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch09equ05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'that is, the sum of the absolute values of the vector elements. Here is the
    *ℓ*[2] case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch09equ06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is the square root of the sums of squares of the vector elements. (Readers
    who remember their school geometry may notice that in 2 dimensions, this is simply
    the length of the diagonal of a right triangle—the famous Pythagorean theorem.)
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Ridge Regression and the LASSO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For years, James−Stein theory was mainly a mathematical curiosity suitable for
    theoretical research but not affecting mainstream data analysis. There was some
    usage of *ridge regression*, to be introduced below, but even that was limited.
    The big change came from the development of the *Least Absolute Shrinkage and
    Selection Operator (LASSO)* and its adoption by the ML community.
  prefs: []
  type: TYPE_NORMAL
- en: '***9.3.1 How They Work***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recall the basics of the least squares method for linear models, say, for the
    case of one feature: we choose ![Image](../images/unch09equ07.jpg) to minimize
    the sum of squared prediction errors, as in [Equation 8.10](ch08.xhtml#ch08equ10).
    For convenience, here is a copy of that expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch09equ07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The idea of ridge regression was to “put a damper” on that by adding vector
    size limitation. We now minimize [Equation 9.7](ch09.xhtml#ch09equ07), *subject
    to the following constraint*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch09equ08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here *η* > 0 is a hyperparameter set by the user, say, via cross-validation.
    The minimizing values of ![Image](../images/unch08equ10.jpg) and ![Image](../images/unch08equ09.jpg)
    are the ridge coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the intuition behind such an approach. We are basically saying we wish
    to minimize the sum of squares as before *but* without allowing ![Image](../images/unch09equ08.jpg)
    to get too large. It’s a compromise between, on the one hand, predicting the *Y**[i]*
    well and, on the other, limiting the size of ![Image](../images/unch09equ08.jpg).
    (We hope that the shrinking will improve our prediction of future cases.) The
    hyperparameter *η* controls that trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be shown that this constrained minimization problem is equivalent to
    choosing ![Image](../images/unch08equ10.jpg) and ![Image](../images/unch08equ09.jpg)
    to minimize the quantity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch09equ09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here *λ* > 0 is a hyperparameter that takes the place of *η*, which, again,
    is typically set via cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This formulation ([Equation 9.9](ch09.xhtml#ch09equ09)) is actually the standard
    definition of ridge regression. The *η* version is easier to explain in terms
    of the James−Stein context, but this *λ* formulation should also make intuitive
    sense: that last term “penalizes” us in our minimizing the sum of squares. The
    larger we set *λ*, the greater the penalty, forcing us to limit the size of ![Image](../images/unch09equ08.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: The LASSO version is almost the same as ridge but with an *ℓ*[1] “damper” term
    rather than *ℓ*[2]. It finds the values of ![Image](../images/unch08equ10.jpg)
    and ![Image](../images/unch08equ09.jpg) that minimize
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch09equ10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In terms of *η*, for LASSO we minimize
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch09equ11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'subject to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch09equ12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '***9.3.2 The Bias-Variance Trade-off, Avoiding Overfitting***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A major reason that the idea of shrinkage—often called *regularization*—has
    had such an impact on statistics and ML is that it is a tool to avoid overfitting.
    Here are the issues:'
  prefs: []
  type: TYPE_NORMAL
- en: On the one hand, we want to make the prediction sum of squares as small as possible,
    which can be shown to eliminate bias.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, recall from [Section 8.8](ch08.xhtml#ch08lev8) that the sum
    of squares can be overly optimistic and thus smaller than we would get in predicting
    new cases in the future. A small value for that sum of squares may come with a
    large variance, due in part to the influence of extreme data points, as discussed
    earlier. Shrinkage reduces variance— a smaller quantity varies less than a larger
    one—thus partially neutralizing the pernicious effects of the extreme points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So the hyperparameter *λ* is used to control where we want to be in that Bias-Variance
    Trade-off. Overfitting occurs when we are on the wrong side of that trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: '*The bottom line*: shrinkage reduces variance, and if this can be done without
    increasing bias much, it’s a win.'
  prefs: []
  type: TYPE_NORMAL
- en: Again, regularization is used not only in the linear model, the case studied
    in this chapter, but also in support vector machines, neural nets, and so on.
    It can even be used in principal component analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '***9.3.3 Relation Between*** *λ**, n, and p***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Again, the Bias-Variance Trade-off notion plays a central role here, with implications
    for dataset size. The larger *n* is (that is, the larger the sample size), the
    smaller the variance in ![Image](../images/betacap1.jpg), which means the lesser
    the need to shrink.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, for large datasets, we may not need regularization. But recall
    from [Chapter 3](ch03.xhtml) that “large *n*” here is meant both in absolute terms
    and relative to *p*—for example, by the ![Image](../images/unch08equ08.jpg) criterion
    following [Equation 3.2](ch03.xhtml#ch03equ02). So, a very large dataset may still
    need regularization if there are numerous features.
  prefs: []
  type: TYPE_NORMAL
- en: In any event, the surest way to settle whether shrinkage is needed in a particular
    setting is to try it, once again, with cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: '***9.3.4 Comparison, Ridge vs. LASSO***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The advantage of ridge regression is that its calculation is simple. There is
    an explicit, closed-form solution—that is, it is noniterative; the LASSO requires
    iterative computation (though it does not have convergence problems).
  prefs: []
  type: TYPE_NORMAL
- en: But the success of the LASSO is due to its providing a *sparse* solution, meaning
    that often many of the elements of ![Image](../images/betacap1.jpg) are 0s. The
    smaller we set *η*, the more 0s we have. We then discard the features having ![Image](../images/unch08equ07.jpg)
    = 0, thereby achieving dimension reduction. Note that, of course, the resulting
    nonzero ![Image](../images/unch08equ07.jpg) values are different from the corresponding
    OLS values.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Software
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once again, we will use a `qe*`-series function, `qeLASSO()`, with the following
    call form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The function wraps `cv.glmnet()` in the `glmnet` package. That package allows
    the user to specify either ridge regression or LASSO via the argument `alpha`,
    setting that value to 0 or 1, respectively; the default is LASSO. One can also
    set `alpha` to an intermediate value, combining the two approaches, something
    termed the *elastic net*.
  prefs: []
  type: TYPE_NORMAL
- en: The `cv.glmnet()` algorithm will start with a huge value of *λ* and then progressively
    reduce *λ*. This corresponds to starting with a very tiny value of *η* and progressively
    increasing it. Since a very tiny value of *η* means that no features are allowed,
    progressively increasing it means we start adding features. It is all arranged
    so that we add one feature at a time. The algorithm computes MSPE or OME at each
    step, using its own built-in cross-validation. The return value of the `qeLASSO()`
    wrapper is actually the object returned by `cv.glmnet()`, with a few additional
    components, such as `testAcc`.
  prefs: []
  type: TYPE_NORMAL
- en: That object will include one set of results for each value of *λ* run by the
    code. So, there will be one ![Image](../images/betacap1.jpg) vector for each *λ*.
    However, when we do subsequent prediction, the code uses the specific value of
    *λ* that had the smallest mean cross-validated prediction error.
  prefs: []
  type: TYPE_NORMAL
- en: '9.5 Example: NYC Taxi Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s return to the New York City taxi data from [Section 5.3](ch05.xhtml#ch05lev3).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We see that the features definitely are helpful in prediction, yielding a large
    reduction in MAPE relative to just using the overall mean for prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the LASSO typically yields a sparse ![Image](../images/betacap1.jpg),
    meaning that most of the coefficients are 0s. In this way, the LASSO can be used
    for dimension reduction, in addition to being used as a predictive model in its
    own right. Let’s explore this for the taxi data by inspecting the `coefs` component
    of the output.
  prefs: []
  type: TYPE_NORMAL
- en: Note first that, as usual, the features that are R factors are converted to
    dummy variables. How many are there?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Considering that the original dataset had only 5 features, 475 is quite a lot!
    But remember, two of our features were the pickup and dropoff locations, of which
    there are hundreds, and thus hundreds of dummies.
  prefs: []
  type: TYPE_NORMAL
- en: Well, which coefficients are nonzero?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Only 11 coefficients are nonzero, including pickup location 132 and dropoff
    location 1\. That’s impressive dimension reduction.
  prefs: []
  type: TYPE_NORMAL
- en: '9.6 Example: Airbnb Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s revisit the Airbnb dataset analyzed in [Section 8.4.3](ch08.xhtml#ch08lev4sec3),
    where we are predicting monthly rent.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `qeLASSO()` function wraps `cv.glmnet()`. The latter has a generic `plot()`
    function, which we can access here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The plot, shown in [Figure 9-1](ch09.xhtml#ch09fig01), displays the classic
    Bias-Variance Tradeoff, which is essentially U-shaped. As *λ* increases to the
    right (lower horizontal axis; the log is used), the number of nonzero coefficients
    (upper horizontal axis) decreases. At first, this produces reductions in MSPE.
    However, after we hit about 26 nonzero coefficients, this quantity rises. In bias-variance
    terms, increasing *λ* brought large reductions in variance with little increase
    in bias. But after hitting 26 features, the bias became the dominant factor.
  prefs: []
  type: TYPE_NORMAL
- en: At any rate, using 26 features, corresponding to *λ* ≈ *e*⁴ = 53.9, seems best,
    yielding a very substantial improvement in prediction accuracy. (Standard errors
    are also shown in the vertical bars extending above and below the curve.)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch09fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-1: MSPE, Airbnb data*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try a prediction, say, taking row 18 from our data and changing the security
    deposit to $360 and the rating to 92\. What would be our predicted value for the
    rent?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: How much did our shrinkage approach change the coefficients compared to the
    OLS output in [Section 8.4.4](ch08.xhtml#ch08lev4sec4)? Well, for example, the
    estimated average premium for living in ZIP code 94123 was $1,639.61 with the
    OLS model. What is it now, using LASSO?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Ah, so it did shrink. On the other hand, LASSO shrinks the vector, not necessarily
    individual elements, which could even grow a bit. Of course, many elements were
    indeed shrunken all the way to 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall how the process works: it begins with no features in the model at all,
    which corresponds to a huge value of *λ*. At each step, *λ* is reduced, possibly
    resulting in our acquiring a new feature. We can also view the order in which
    the features were brought into the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Not too surprisingly, the first feature chosen by the process was the number
    of bedrooms. But perhaps less intuitively, the process’s second choice was a dummy
    variable regarding guests. Our example above, a dummy for ZIP code 94123, came
    in at the 17th step. One might view this ordering as a report on the importance
    of each selected feature.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Since our emphasis in this book is on prediction from data rather than description
    of data, we have not discussed the issue of feature importance before now. We
    only present it here as an aid to understanding how LASSO works. However, it is
    available in some of the packages used in this book. For instance, see the* importance()
    *function in the* randomForests *package.*'
  prefs: []
  type: TYPE_NORMAL
- en: '9.7 Example: African Soil Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As noted in [Section 6.2.4](ch06.xhtml#ch06lev2sec4), the importance of the
    African soil dataset is that it has *p* > *n*, with the number of features being
    almost triple the number of data points. This is considered a very difficult situation.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, to many analysts, the very essence of LASSO is dimension reduction,
    so it will be very interesting to see how LASSO does on this data.
  prefs: []
  type: TYPE_NORMAL
- en: '***9.7.1 LASSO Analysis***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Again, we will predict soil acidity, pH:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `nzero` component of the output tells us how many features the process
    has chosen at each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And the `lambda` component gives the corresponding *λ* values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The corresponding graph is shown in [Figure 9-2](ch09.xhtml#ch09fig02).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch09fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-2: African soil data*'
  prefs: []
  type: TYPE_NORMAL
- en: Here we have a rather incomplete result. The smallest MSPE came from the smallest
    *λ* value tried by the software (0.003426429), but the curve seems to suggest
    that even smaller values would do better. So, we might rerun with a custom set
    of *λ* values, rather than using the default value sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, even if we choose to settle for *λ* = 0.003426429, that value
    would be pretty good. LASSO retained 156 features out of the original 3,578\.
    That’s quite a dimension reduction.
  prefs: []
  type: TYPE_NORMAL
- en: '9.8 Optional Section: The Famous LASSO Picture'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section has a bit more mathematical content, and it can be safely skipped,
    as it is not used in the sequel. However, readers who are curious as to why the
    LASSO retains some of the original features but excludes others may find this
    section helpful.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, a key property of the LASSO is that it usually provides a *sparse*
    solution for ![Image](../images/betacap1.jpg), meaning that many of the ![Image](../images/unch08equ07.jpg)
    values are 0\. In other words, many features are discarded, thus providing a means
    of dimension reduction. [Figure 9-3](ch09.xhtml#ch09fig03) shows why. Here is
    how it works.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch09fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-3: Feature subsetting nature of the LASSO*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-3](ch09.xhtml#ch09fig03) is for the case of *p* = 2 predictors, whose
    coefficients are *b*[1] and *b*[2]. (For simplicity, we assume there is no constant
    term *b*[0].) Let *U* and *V* denote the corresponding features. Write *b* = (*b*[1],
    *b*[2]) for the vector of the *b**[i]*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Without shrinkage, we would choose *b* to minimize the sum of squared errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch09equ13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The horizontal and vertical axes are for *b*[1] and *b*[2], as shown. The key
    point is that for any value that we set in [Equation 9.13](ch09.xhtml#ch09equ13)
    for SSE, the points (*b*[1], *b*[2]) that solve the resulting equation form an
    ellipse. The value of (*b*[1], *b*[2]) computed by the LASSO is just one point
    in the given ellipse; lots of other (*b*[1], *b*[2]) values yield the same SSE.
  prefs: []
  type: TYPE_NORMAL
- en: As we vary the SSE value, we get various concentric ellipses, two of which are
    shown in [Figure 9-3](ch09.xhtml#ch09fig03). Larger values of SSE correspond to
    larger ellipses.
  prefs: []
  type: TYPE_NORMAL
- en: Now, what happens when we give the LASSO algorithm a value of *λ* or *η*? As
    noted earlier, either quantity can be used, but it will be easier to assume the
    latter. So, what will the LASSO algorithm do when we give it a value of *η*?
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm will minimize SSE, subject to the constraint:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Image](../images/ch09equ14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Let’s denote that minimum value of SSE by SSE*[alg]*, and denote the corresponding
    (*b*[1], *b*[2]) value by (*b*[1], *b*[2])*[alg]*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the one hand, the point (*b*[1], *b*[2])*[alg]* will be on the ellipse associated
    with SSE*[alg]*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, [Equation 9.14](ch09.xhtml#ch09equ14) says that (*b*[1],
    *b*[2])*[alg]* must be somewhere in the diamond in the picture, whose corners
    are at (*η*, 0), (0, *η*), and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, (*b*[1], *b*[2])*[alg]* must lie on an ellipse that intersects with the
    diamond.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But remember, we want SSE to be as small as possible, subject to [Equation 9.14](ch09.xhtml#ch09equ14).
    Recall, too, that smaller SSE values correspond to smaller ellipses. So the ellipse
    for SSE*[alg]* must *just barely touch the diamond*, as seen in the outer ellipse
    in [Figure 9-3](ch09.xhtml#ch09fig03).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the figure, the “just barely touch” point is at one of the corners of the
    diamond. And each of the corners has either *b*[1] or *b*[2] equal to 0—sparsity!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Is that sparsity some kind of coincidence? No! Here’s why: depending on the
    relative values of our input data (*U**[i]*, *V**[i]*), the ellipses in the picture
    will have different orientations. The ones in the picture are pointing approximately
    “northwest and southeast.” But it is clear from inspection that most orientations
    will result in the touch point being at one of the corners and hence a sparse
    solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus the LASSO will usually be sparse, which is the major reason for its popularity.
    And what about ridge regression? In that case, the diamond becomes a circle, so
    there is no sparseness property.
  prefs: []
  type: TYPE_NORMAL
- en: 9.9 Coming Up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we take an entirely different approach. With k-NN and decision trees,
    no linearity was used, and then this property was explicitly assumed. In [Part
    IV](part4.xhtml), we cover methods in which linearity is used, but only indirectly.
  prefs: []
  type: TYPE_NORMAL
