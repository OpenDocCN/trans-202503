- en: '**18'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**18'
- en: USING AND FINE-TUNING PRETRAINED TRANSFORMERS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 使用和微调预训练变换器**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: What are the different ways to use and fine-tune pretrained large language models?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 使用和微调预训练的大型语言模型有哪些不同的方法？
- en: The three most common ways to use and fine-tune pretrained LLMs include a feature-based
    approach, in-context prompting, and updating a subset of the model parameters.
    First, most pretrained LLMs or language transformers can be utilized without the
    need for further fine-tuning. For instance, we can employ a feature-based method
    to train a new downstream model, such as a linear classifier, using embeddings
    generated by a pretrained transformer. Second, we can showcase examples of a new
    task within the input itself, which means we can directly exhibit the expected
    outcomes without requiring any updates or learning from the model. This concept
    is also known as *prompting*. Finally, it’s also possible to fine-tune all or
    just a small number of parameters to achieve the desired outcomes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 使用和微调预训练大型语言模型（LLMs）的三种最常见方法包括基于特征的方法、上下文提示和更新模型参数的子集。首先，大多数预训练的LLMs或语言变换器可以在不需要进一步微调的情况下使用。例如，我们可以使用基于特征的方法，通过预训练变换器生成的嵌入来训练一个新的下游模型，如线性分类器。第二，我们可以在输入本身展示一个新任务的示例，这意味着我们可以直接展示预期的结果，而无需模型进行任何更新或学习。这一概念也被称为*提示*。最后，我们也可以微调所有或仅少量的参数，以达到预期的结果。
- en: The following sections discuss these types of approaches in greater depth.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将更深入地讨论这些方法。
- en: '**Using Transformers for Classification Tasks**'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**使用变换器进行分类任务**'
- en: 'Let’s start with the conventional methods for utilizing pretrained transformers:
    training another model on feature embeddings, fine-tuning output layers, and fine-tuning
    all layers. We’ll discuss these in the context of classification. (We will revisit
    prompting later in the section “In-Context Learning, Indexing, and Prompt Tuning”
    on [page 116](ch18.xhtml#ch00lev91).)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从利用预训练变换器的传统方法开始：在特征嵌入上训练另一个模型、微调输出层和微调所有层。我们将在分类的背景下讨论这些方法。（稍后我们将在“上下文学习、索引和提示微调”部分的[第116页](ch18.xhtml#ch00lev91)中重新讨论提示。）
- en: In the feature-based approach, we load the pretrained model and keep it “frozen,”
    meaning we do not update any parameters of the pretrained model. Instead, we treat
    the model as a feature extractor that we apply to our new dataset. We then train
    a downstream model on these embeddings. This downstream model can be any model
    we like (random forests, XGBoost, and so on), but linear classifiers typically
    perform best. This is likely because pretrained transformers like BERT and GPT
    already extract high-quality, informative features from the input data. These
    feature embeddings often capture complex relationships and patterns, making it
    easy for a linear classifier to effectively separate the data into different classes.
    Furthermore, linear classifiers, such as logistic regression machines and support
    vector machines, tend to have strong regularization properties. These regularization
    properties help prevent overfitting when working with high-dimensional feature
    spaces generated by pretrained transformers. This feature-based approach is the
    most efficient method since it doesn’t require updating the transformer model
    at all. Finally, the embeddings can be precomputed for a given training dataset
    (since they don’t change) when training a classifier for multiple training epochs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于特征的方法中，我们加载预训练模型并保持其“冻结”，这意味着我们不更新预训练模型的任何参数。相反，我们将模型视为一个特征提取器，应用到我们的新数据集上。然后，我们在这些嵌入上训练一个下游模型。这个下游模型可以是我们喜欢的任何模型（如随机森林、XGBoost等），但线性分类器通常表现最佳。这可能是因为像BERT和GPT这样的预训练变换器已经从输入数据中提取了高质量、有用的特征。这些特征嵌入通常能够捕捉复杂的关系和模式，使得线性分类器能够有效地将数据分成不同的类别。此外，线性分类器，如逻辑回归机和支持向量机，通常具有较强的正则化特性。这些正则化特性有助于防止在使用由预训练变换器生成的高维特征空间时出现过拟合。这种基于特征的方法是最有效的，因为它完全不需要更新变换器模型。最后，嵌入可以为给定的训练数据集预先计算（因为它们不会改变），在多个训练周期中训练分类器时非常有用。
- en: '[Figure 18-1](ch18.xhtml#ch18fig1) illustrates how LLMs are typically created
    and adopted for downstream tasks using fine-tuning. Here, a pretrained model,
    trained on a general text corpus, is fine-tuned to perform tasks like German-to-English
    translation.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/18fig01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: '*Figure 18-1: The general fine-tuning workflow of large language models*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: The conventional methods for fine-tuning pretrained LLMs include updating only
    the output layers, a method we’ll refer to as *fine-tuning I*, and updating all
    layers, which we’ll call *fine-tuning II*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning I is similar to the feature-based approach described earlier, but
    it adds one or more output layers to the LLM itself. The backbone of the LLM remains
    frozen, and we update only the model parameters in these new layers. Since we
    don’t need to backpropagate through the whole network, this approach is relatively
    efficient regarding throughput and memory requirements.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: In fine-tuning II, we load the model and add one or more output layers, similarly
    to fine-tuning I. However, instead of backpropagating only through the last layers,
    we update *all* layers via backpropagation, making this the most expensive approach.
    While this method is computationally more expensive than the feature-based approach
    and fine-tuning I, it typically leads to better modeling or predictive performance.
    This is especially true for more specialized domain-specific datasets.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 18-2](ch18.xhtml#ch18fig2) summarizes the three approaches described
    in this section so far.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/18fig02.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: '*Figure 18-2: The three conventional approaches for utilizing pretrained LLMs*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the conceptual summary of the three fine-tuning methods described
    in this section, [Figure 18-2](ch18.xhtml#ch18fig2) also provides a rule-of-thumb
    guideline for these methods regarding training efficiency. Since fine-tuning II
    involves updating more layers and parameters than fine-tuning I, backpropagation
    is costlier for fine-tuning II. For similar reasons, fine-tuning II is costlier
    than a simpler feature-based approach.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '**In-Context Learning, Indexing, and Prompt Tuning**'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs like GPT-2 and GPT-3 popularized the concept of *in-context learning*,
    often called *zero-shot* or *few-shot learning* in this context, which is illustrated
    in [Figure 18-3](ch18.xhtml#ch18fig3).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/18fig03.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: '*Figure 18-3: Prompting an LLM for in-context learning*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: As [Figure 18-3](ch18.xhtml#ch18fig3) shows, in-context learning aims to provide
    context or examples of the task within the input or prompt, allowing the model
    to infer the desired behavior and generate appropriate responses. This approach
    takes advantage of the model’s ability to learn from vast amounts of data during
    pretraining, which includes diverse tasks and contexts.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '*The definition of few-shot learning, considered synonymous with in-context
    learning-based methods, differs from the conventional approach to few-shot learning
    discussed in [Chapter 3](ch03.xhtml).*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*少样本学习的定义，与基于上下文学习的方法同义，区别于[第3章](ch03.xhtml)中讨论的传统少样本学习方法。*'
- en: 'For example, suppose we want to use in-context learning for few-shot German–English
    translation using a large-scale pretrained language model like GPT-3\. To do so,
    we provide a few examples of German–English translations to help the model understand
    the desired task, as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们希望使用上下文学习进行少样本的德英翻译，使用像GPT-3这样的规模较大的预训练语言模型。为此，我们提供几个德英翻译的示例，以帮助模型理解所需任务，如下所示：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Generally, in-context learning does not perform as well as fine-tuning for certain
    tasks or specific datasets since it relies on the pretrained model’s ability to
    generalize from its training data without further adapting its parameters for
    the particular task at hand.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在上下文学习在某些任务或特定数据集上表现不如微调，因为它依赖于预训练模型从训练数据中泛化的能力，而不进一步调整模型的参数以适应当前任务。
- en: However, in-context learning has its advantages. It can be particularly useful
    when labeled data for fine-tuning is limited or unavailable. It also enables rapid
    experimentation with different tasks without fine-tuning the model parameters
    in cases where we don’t have direct access to the model or where we interact only
    with the model through a UI or API (for example, ChatGPT).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上下文学习也有其优势。当用于微调的标注数据有限或不可用时，它尤其有用。它还可以在没有微调模型参数的情况下快速实验不同的任务，特别是在我们无法直接访问模型，或者只通过用户界面（UI）或API与模型交互的情况下（例如，ChatGPT）。
- en: 'Related to in-context learning is the concept of *hard prompt tuning*, where
    *hard* refers to the non-differentiable nature of the input tokens. Where the
    previously described fine-tuning methods update the model parameters to better
    perform the task at hand, hard prompt tuning aims to optimize the prompt itself
    to achieve better performance. Prompt tuning does not modify the model parameters,
    but it may involve using a smaller labeled dataset to identify the best prompt
    formulation for the specific task. For example, to improve the prompts for the
    previous German–English translation task, we might try the following three prompting
    variations:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与上下文学习相关的概念是*硬提示调优*，其中*硬*指的是输入标记的不可微分特性。与之前描述的微调方法通过更新模型参数来更好地执行任务不同，硬提示调优旨在优化提示本身以达到更好的性能。提示调优并不修改模型的参数，但它可能涉及使用较小的标注数据集来识别针对特定任务的最佳提示词形式。例如，为了改善之前的德英翻译任务的提示，我们可能会尝试以下三种提示变体：
- en: '`"Translate the German sentence ''{german_sentence}'' into English: {english_translation}"`'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"将德语句子''{german_sentence}''翻译成英语：{english_translation}"`'
- en: '`"German: ''{german_sentence}'' | English: {english_translation}"`'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"德语：''{german_sentence}'' | 英语：{english_translation}"`'
- en: '`"From German to English: ''{german_sentence}'' -> {english_translation}"`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"从德语到英语：''{german_sentence}'' -> {english_translation}"`'
- en: Prompt tuning is a resource-efficient alternative to parameter fine-tuning.
    However, its performance is usually not as good as full model fine-tuning, as
    it does not update the model’s parameters for a specific task, potentially limiting
    its ability to adapt to task-specific nuances. Furthermore, prompt tuning can
    be labor intensive since it requires either human involvement comparing the quality
    of the different prompts or another similar method to do so. This is often known
    as *hard* prompting since, again, the input tokens are not differentiable. In
    addition, other methods exist that propose to use another LLM for automatic prompt
    generation and evaluation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 提示词调优是一种资源高效的替代方法，代替了参数微调。然而，提示词调优的性能通常不如完全模型微调，因为它并未更新模型的参数来针对特定任务进行调整，这可能会限制其适应任务特定细微差异的能力。此外，提示词调优可能需要大量人工干预，因为它需要人工对比不同提示词的质量，或使用类似的方法来完成这一任务。这通常被称为*硬*提示，因为输入的标记不可微分。除此之外，还有其他方法提议使用另一个大型语言模型（LLM）来自动生成和评估提示词。
- en: Yet another way to leverage a purely in-context learning-based approach is *indexing*,
    illustrated in [Figure 18-4](ch18.xhtml#ch18fig4).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种利用纯粹基于上下文学习的方法是*索引*，如[图18-4](ch18.xhtml#ch18fig4)所示。
- en: '![Image](../images/18fig04.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/18fig04.jpg)'
- en: '*Figure 18-4: LLM indexing to retrieve information from external documents*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*图18-4：LLM索引用于从外部文档中检索信息*'
- en: In the context of LLMs, we can think of indexing as a workaround based on in-context
    learning that allows us to turn LLMs into information retrieval systems to extract
    information from external resources and web-sites. In [Figure 18-4](ch18.xhtml#ch18fig4),
    an indexing module parses a document or website into smaller chunks, embedded
    into vectors that can be stored in a vector database. When a user submits a query,
    the indexing module computes the vector similarity between the embedded query
    and each vector stored in the database. Finally, the indexing module retrieves
    the top *k* most similar embeddings to synthesize the response.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameter-Efficient Fine-Tuning**'
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, many methods have been developed to adapt pretrained transformers
    more efficiently for new target tasks. These methods are commonly referred to
    as *parameter-efficient fine-tuning*, with the most popular methods at the time
    of writing summarized in [Figure 18-5](ch18.xhtml#ch18fig5).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/18fig05.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: '*Figure 18-5: The main categories of parameter-efficient fine-tuning techniques,
    with popular examples*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the hard prompting approach discussed in the previous section,
    *soft prompting* strategies optimize embedded versions of the prompts. While in
    hard prompt tuning we modify the discrete input tokens, in soft prompt tuning
    we utilize trainable parameter tensors instead.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind soft prompt tuning is to prepend a trainable parameter tensor
    (the “soft prompt”) to the embedded query tokens. The prepended tensor is then
    tuned to improve the modeling performance on a target data-set using gradient
    descent. In Python-like pseudocode, soft prompt tuning can be described as
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: where the `soft_prompt_tensor` has the same feature dimension as the embedded
    inputs produced by the embedding layer. Consequently, the modified input matrix
    has additional rows (as if it extended the original input sequence with additional
    tokens, making it longer).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'Another popular prompt tuning method is prefix tuning. *Prefix tuning* is similar
    to soft prompt tuning, except that in prefix tuning, we prepend trainable tensors
    (soft prompts) to each transformer block instead of only the embedded inputs,
    which can stabilize the training. The implementation of prefix tuning is illustrated
    in the following pseudocode:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*Listing 18-1: A transformer block modified for prefix tuning*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break [Listing 18-1](ch18.xhtml#ch18lis1) into three main parts: implementing
    the soft prompt, concatenating the soft prompt (prefix) with the input, and implementing
    the rest of the transformer block.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: First, the `soft_prompt`, a tensor, is processed through a set of fully connected
    layers ➊. Second, the transformed soft prompt is concatenated with the main input,
    `x` ➋. The dimension along which they are concatenated is denoted by `seq_len`,
    referring to the sequence length dimension. Third, the subsequent lines of code
    ➌ describe the standard operations in a transformer block, including self-attention,
    layer normalization, and feed-forward neural network layers, wrapped around residual
    connections.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，`soft_prompt`，一个张量，通过一组全连接层 ➊ 进行处理。其次，转换后的软提示与主输入 `x` ➋ 拼接。它们拼接的维度由 `seq_len`
    表示，指的是序列长度维度。第三，后续的代码行 ➌ 描述了变换器块中的标准操作，包括自注意力、层归一化和前馈神经网络层，这些都包裹在残差连接中。
- en: As shown in [Listing 18-1](ch18.xhtml#ch18lis1), prefix tuning modifies a transformer
    block by adding a trainable soft prompt. [Figure 18-6](ch18.xhtml#ch18fig6) further
    illustrates the difference between a regular transformer block and a prefix tuning
    transformer block.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如[代码示例 18-1](ch18.xhtml#ch18lis1)所示，前缀调优通过添加一个可训练的软提示来修改变换器块。[图 18-6](ch18.xhtml#ch18fig6)进一步说明了常规变换器块和前缀调优变换器块之间的区别。
- en: '![Image](../images/18fig06.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/18fig06.jpg)'
- en: '*Figure 18-6: A regular transformer compared with prefix tuning*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 18-6：常规变换器与前缀调优的比较*'
- en: Both soft prompt tuning and prefix tuning are considered parameter efficient
    since they require training only the prepended parameter tensors and not the LLM
    parameters themselves.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 软提示调优和前缀调优都被认为是参数高效的，因为它们只需要训练添加的参数张量，而不是大语言模型本身的参数。
- en: '*Adapter methods* are related to prefix tuning in that they add additional
    parameters to the transformer layers. In the original adapter method, additional
    fully connected layers were added after the multihead self-attention and existing
    fully connected layers in each transformer block, as illustrated in [Figure 18-7](ch18.xhtml#ch18fig7).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*适配器方法* 与前缀调优相关，因为它们在变换器层中添加了额外的参数。在原始适配器方法中，在每个变换器块的多头自注意力和现有全连接层之后添加了额外的全连接层，如[图
    18-7](ch18.xhtml#ch18fig7)所示。'
- en: '![Image](../images/18fig07.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/18fig07.jpg)'
- en: '*Figure 18-7: Comparison of a regular transformer block (left) and a transformer
    block with adapter layers*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 18-7：常规变换器块（左）与带适配器层的变换器块比较*'
- en: Only the new adapter layers are updated when training the LLM using the original
    adapter method, while the remaining transformer layers remain frozen. Since the
    adapter layers are usually small—the first fully connected layer in an adapter
    block projects its input into a low-dimensional representation, while the second
    layer projects it back into the original input dimension—this adapter method is
    usually considered parameter efficient.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用原始适配器方法训练大语言模型时，仅更新新的适配器层，而其余变换器层保持冻结。由于适配器层通常较小——适配器块中的第一个全连接层将其输入投影到低维表示中，而第二层则将其投影回原始输入维度——这种适配器方法通常被认为是参数高效的。
- en: 'In pseudocode, the original adapter method can be written as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在伪代码中，原始适配器方法可以写作如下：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Low-rank adaptation (LoRA)*, another popular parameter-efficient fine-tuning
    method worth considering, refers to reparameterizing pretrained LLM weights using
    low-rank transformations. LoRA is related to the concept of *low-rank transformation*,
    a technique to approximate a high-dimensional matrix or dataset using a lower-dimensional
    representation. The lower-dimensional representation (or *low-rank approximation*)
    is achieved by finding a combination of fewer dimensions that can effectively
    capture most of the information in the original data. Popular low-rank transformation
    techniques include principal component analysis and singular vector decomposition.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*低秩适配（LoRA）*，另一种值得考虑的流行参数高效微调方法，指的是通过低秩变换重新参数化预训练大语言模型（LLM）权重。LoRA 与 *低秩变换*
    的概念相关，低秩变换是一种使用低维表示来逼近高维矩阵或数据集的技术。低维表示（或 *低秩近似*）是通过找到一个较少维度的组合来有效捕获原始数据中的大部分信息。常见的低秩变换技术包括主成分分析和奇异值分解。'
- en: 'For example, suppose ∆*W* represents the parameter update for a weight matrix
    of the LLM with dimension ℝ*^(A×B)*. We can decompose the weight update matrix
    into two smaller matrices: ∆*W* = *W[A]W[B]*, where *W[A]∈* ℝ*^(A×h)* and *W[A]∈*
    ℝ*^(h×B)*. Here, we keep the original weight frozen and train only the new matrices
    *W[A]* and *W[B]*.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: How is this method parameter efficient if we introduce new weight matrices?
    These new matrices can be very small. For example, if *A* = 25 and *B* = 50, then
    the size of ∆*W* is 25 *×* 50 = 1,250\. If *h* = 5, then *W[A]* has 125 parameters,
    *W[B]* has 250 parameters, and the two matrices combined have only 125 + 250 =
    375 parameters in total.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'After learning the weight update matrix, we can then write the matrix multiplication
    of a fully connected layer, as shown in this pseudocode:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Listing 18-2: Matrix multiplication with LoRA*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: In [Listing 18-2](ch18.xhtml#ch18lis2), `scalar` is a scaling factor that adjusts
    the magnitude of the combined result (original model output plus low-rank adaptation).
    This balances the pretrained model’s knowledge and the new task-specific adaptation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: According to the original paper introducing the LoRA method, models using LoRA
    perform slightly better than models using the adapter method across several task-specific
    benchmarks. Often, LoRA performs even better than models fine-tuned using the
    fine-tuning II method described earlier.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '**Reinforcement Learning with Human Feedback**'
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous section focused on ways to make fine-tuning more efficient. Switching
    gears, how can we improve the modeling performance of LLMs via fine-tuning?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: The conventional way to adapt or fine-tune an LLM for a new target domain or
    task is to use a supervised approach with labeled target data. For instance, the
    fine-tuning II approach allows us to adapt a pretrained LLM and fine-tune it on
    a target task such as sentiment classification, using a dataset that contains
    texts with sentiment labels like *positive*, *neutral*, and *negative*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Supervised fine-tuning is a foundational step in training an LLM. An additional,
    more advanced step is *reinforcement learning with human feedback (RLHF)*, which
    can be used to further improve the model’s alignment with human preferences. For
    example, ChatGPT and its predecessor, Instruct-GPT, are two popular examples of
    pretrained LLMs (GPT-3) fine-tuned using RLHF.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: In RLHF, a pretrained model is fine-tuned using a combination of supervised
    learning and reinforcement learning. This approach was popularized by the original
    ChatGPT model, which was in turn based on Instruct-GPT. Human feedback is collected
    by having humans rank or rate different model outputs, providing a reward signal.
    The collected reward labels can be used to train a reward model that is then used
    to guide the LLMs’ adaptation to human preferences. The reward model is learned
    via supervised learning, typically using a pretrained LLM as the base model, and
    is then used to adapt the pretrained LLM to human preferences via additional fine-tuning.
    The training in this additional fine-tuning stage uses a flavor of reinforcement
    learning called *proximal policy optimization*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: RLHF uses a reward model instead of training the pretrained model on the human
    feedback directly because involving humans in the learning process would create
    a bottleneck since we cannot obtain feedback in real time.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '**Adapting Pretrained Language Models**'
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While fine-tuning all layers of a pretrained LLM remains the gold standard for
    adaption to new target tasks, several efficient alternatives exist for leveraging
    pretrained transformers. For instance, we can effectively apply LLMs to new tasks
    while minimizing computational costs and resources by utilizing feature-based
    methods, in-context learning, or parameter-efficient fine-tuning techniques.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: The three conventional methods—feature-based approach, fine-tuning I, and fine-tuning
    II—provide different computational efficiency and performance trade-offs. Parameter-efficient
    fine-tuning methods like soft prompt tuning, prefix tuning, and adapter methods
    further optimize the adaptation process, reducing the number of parameters to
    be updated. Meanwhile, RLHF presents an alternative approach to supervised fine-tuning,
    potentially improving modeling performance.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: In sum, the versatility and efficiency of pretrained LLMs continue to advance,
    offering new opportunities and strategies for effectively adapting these models
    to a wide array of tasks and domains. As research in this area progresses, we
    can expect further improvements and innovations in using pretrained language models.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**18-1.** When does it make more sense to use in-context learning rather than
    fine-tuning, and vice versa?'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '**18-2.** In prefix tuning, adapters, and LoRA, how can we ensure that the
    model preserves (and does not forget) the original knowledge?'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The paper introducing the GPT-2 model: Alec Radford et al., “Language Models
    Are Unsupervised Multitask Learners” (2019), *[https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)*.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper introducing the GPT-3 model: Tom B. Brown et al., “Language Models
    Are Few-Shot Learners” (2020), *[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)*.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The automatic prompt engineering method, which proposes using another LLM for
    automatic prompt generation and evaluation: Yongchao Zhou et al., “Large Language
    Models Are Human-Level Prompt Engineers” (2023), *[https://arxiv.org/abs/2211.01910](https://arxiv.org/abs/2211.01910)*.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LlamaIndex is an example of an indexing approach that leverages in-context
    learning: *[https://github.com/jerryjliu/llama_index](https://github.com/jerryjliu/llama_index)*.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DSPy is a popular open source library for retrieval augmentation and indexing:
    *[https://github.com/stanfordnlp/dsp](https://github.com/stanfordnlp/dsp)*.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A first instance of soft prompting: Brian Lester, Rami Al-Rfou, and Noah Constant,
    “The Power of Scale for Parameter-Efficient Prompt Tuning” (2021), *[https://arxiv.org/abs/2104.08691](https://arxiv.org/abs/2104.08691)*.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper that first described prefix tuning: Xiang Lisa Li and Percy Liang,
    “Prefix-Tuning: Optimizing Continuous Prompts for Generation” (2021), *[https://arxiv.org/abs/2101.00190](https://arxiv.org/abs/2101.00190)*.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper introducing the original adapter method: Neil Houlsby et al., “Parameter-Efficient
    Transfer Learning for NLP” (2019) *[https://arxiv.org/abs/1902.00751](https://arxiv.org/abs/1902.00751)*.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper introducing the LoRA method: Edward J. Hu et al., “LoRA: Low-Rank
    Adaptation of Large Language Models” (2021), *[https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)*.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A survey of more than 40 research papers covering parameter-efficient fine-tuning
    methods: Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky, “Scaling Down
    to Scale Up: A Guide to Parameter-Efficient Fine-Tuning” (2023), *[https://arxiv.org/abs/2303.15647](https://arxiv.org/abs/2303.15647)*.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The InstructGPT paper: Long Ouyang et al., “Training Language Models to Follow
    Instructions with Human Feedback” (2022), *[https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)*.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Proximal policy optimization, which is used for reinforcement learning with
    human feedback: John Schulman et al., “Proximal Policy Optimization Algorithms”
    (2017), *[https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)*.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
