- en: '**18'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: USING AND FINE-TUNING PRETRAINED TRANSFORMERS**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: What are the different ways to use and fine-tune pretrained large language models?
  prefs: []
  type: TYPE_NORMAL
- en: The three most common ways to use and fine-tune pretrained LLMs include a feature-based
    approach, in-context prompting, and updating a subset of the model parameters.
    First, most pretrained LLMs or language transformers can be utilized without the
    need for further fine-tuning. For instance, we can employ a feature-based method
    to train a new downstream model, such as a linear classifier, using embeddings
    generated by a pretrained transformer. Second, we can showcase examples of a new
    task within the input itself, which means we can directly exhibit the expected
    outcomes without requiring any updates or learning from the model. This concept
    is also known as *prompting*. Finally, it’s also possible to fine-tune all or
    just a small number of parameters to achieve the desired outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections discuss these types of approaches in greater depth.
  prefs: []
  type: TYPE_NORMAL
- en: '**Using Transformers for Classification Tasks**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start with the conventional methods for utilizing pretrained transformers:
    training another model on feature embeddings, fine-tuning output layers, and fine-tuning
    all layers. We’ll discuss these in the context of classification. (We will revisit
    prompting later in the section “In-Context Learning, Indexing, and Prompt Tuning”
    on [page 116](ch18.xhtml#ch00lev91).)'
  prefs: []
  type: TYPE_NORMAL
- en: In the feature-based approach, we load the pretrained model and keep it “frozen,”
    meaning we do not update any parameters of the pretrained model. Instead, we treat
    the model as a feature extractor that we apply to our new dataset. We then train
    a downstream model on these embeddings. This downstream model can be any model
    we like (random forests, XGBoost, and so on), but linear classifiers typically
    perform best. This is likely because pretrained transformers like BERT and GPT
    already extract high-quality, informative features from the input data. These
    feature embeddings often capture complex relationships and patterns, making it
    easy for a linear classifier to effectively separate the data into different classes.
    Furthermore, linear classifiers, such as logistic regression machines and support
    vector machines, tend to have strong regularization properties. These regularization
    properties help prevent overfitting when working with high-dimensional feature
    spaces generated by pretrained transformers. This feature-based approach is the
    most efficient method since it doesn’t require updating the transformer model
    at all. Finally, the embeddings can be precomputed for a given training dataset
    (since they don’t change) when training a classifier for multiple training epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 18-1](ch18.xhtml#ch18fig1) illustrates how LLMs are typically created
    and adopted for downstream tasks using fine-tuning. Here, a pretrained model,
    trained on a general text corpus, is fine-tuned to perform tasks like German-to-English
    translation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/18fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 18-1: The general fine-tuning workflow of large language models*'
  prefs: []
  type: TYPE_NORMAL
- en: The conventional methods for fine-tuning pretrained LLMs include updating only
    the output layers, a method we’ll refer to as *fine-tuning I*, and updating all
    layers, which we’ll call *fine-tuning II*.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning I is similar to the feature-based approach described earlier, but
    it adds one or more output layers to the LLM itself. The backbone of the LLM remains
    frozen, and we update only the model parameters in these new layers. Since we
    don’t need to backpropagate through the whole network, this approach is relatively
    efficient regarding throughput and memory requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In fine-tuning II, we load the model and add one or more output layers, similarly
    to fine-tuning I. However, instead of backpropagating only through the last layers,
    we update *all* layers via backpropagation, making this the most expensive approach.
    While this method is computationally more expensive than the feature-based approach
    and fine-tuning I, it typically leads to better modeling or predictive performance.
    This is especially true for more specialized domain-specific datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 18-2](ch18.xhtml#ch18fig2) summarizes the three approaches described
    in this section so far.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/18fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 18-2: The three conventional approaches for utilizing pretrained LLMs*'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the conceptual summary of the three fine-tuning methods described
    in this section, [Figure 18-2](ch18.xhtml#ch18fig2) also provides a rule-of-thumb
    guideline for these methods regarding training efficiency. Since fine-tuning II
    involves updating more layers and parameters than fine-tuning I, backpropagation
    is costlier for fine-tuning II. For similar reasons, fine-tuning II is costlier
    than a simpler feature-based approach.
  prefs: []
  type: TYPE_NORMAL
- en: '**In-Context Learning, Indexing, and Prompt Tuning**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs like GPT-2 and GPT-3 popularized the concept of *in-context learning*,
    often called *zero-shot* or *few-shot learning* in this context, which is illustrated
    in [Figure 18-3](ch18.xhtml#ch18fig3).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/18fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 18-3: Prompting an LLM for in-context learning*'
  prefs: []
  type: TYPE_NORMAL
- en: As [Figure 18-3](ch18.xhtml#ch18fig3) shows, in-context learning aims to provide
    context or examples of the task within the input or prompt, allowing the model
    to infer the desired behavior and generate appropriate responses. This approach
    takes advantage of the model’s ability to learn from vast amounts of data during
    pretraining, which includes diverse tasks and contexts.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The definition of few-shot learning, considered synonymous with in-context
    learning-based methods, differs from the conventional approach to few-shot learning
    discussed in [Chapter 3](ch03.xhtml).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose we want to use in-context learning for few-shot German–English
    translation using a large-scale pretrained language model like GPT-3\. To do so,
    we provide a few examples of German–English translations to help the model understand
    the desired task, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Generally, in-context learning does not perform as well as fine-tuning for certain
    tasks or specific datasets since it relies on the pretrained model’s ability to
    generalize from its training data without further adapting its parameters for
    the particular task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: However, in-context learning has its advantages. It can be particularly useful
    when labeled data for fine-tuning is limited or unavailable. It also enables rapid
    experimentation with different tasks without fine-tuning the model parameters
    in cases where we don’t have direct access to the model or where we interact only
    with the model through a UI or API (for example, ChatGPT).
  prefs: []
  type: TYPE_NORMAL
- en: 'Related to in-context learning is the concept of *hard prompt tuning*, where
    *hard* refers to the non-differentiable nature of the input tokens. Where the
    previously described fine-tuning methods update the model parameters to better
    perform the task at hand, hard prompt tuning aims to optimize the prompt itself
    to achieve better performance. Prompt tuning does not modify the model parameters,
    but it may involve using a smaller labeled dataset to identify the best prompt
    formulation for the specific task. For example, to improve the prompts for the
    previous German–English translation task, we might try the following three prompting
    variations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"Translate the German sentence ''{german_sentence}'' into English: {english_translation}"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"German: ''{german_sentence}'' | English: {english_translation}"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"From German to English: ''{german_sentence}'' -> {english_translation}"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt tuning is a resource-efficient alternative to parameter fine-tuning.
    However, its performance is usually not as good as full model fine-tuning, as
    it does not update the model’s parameters for a specific task, potentially limiting
    its ability to adapt to task-specific nuances. Furthermore, prompt tuning can
    be labor intensive since it requires either human involvement comparing the quality
    of the different prompts or another similar method to do so. This is often known
    as *hard* prompting since, again, the input tokens are not differentiable. In
    addition, other methods exist that propose to use another LLM for automatic prompt
    generation and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Yet another way to leverage a purely in-context learning-based approach is *indexing*,
    illustrated in [Figure 18-4](ch18.xhtml#ch18fig4).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/18fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 18-4: LLM indexing to retrieve information from external documents*'
  prefs: []
  type: TYPE_NORMAL
- en: In the context of LLMs, we can think of indexing as a workaround based on in-context
    learning that allows us to turn LLMs into information retrieval systems to extract
    information from external resources and web-sites. In [Figure 18-4](ch18.xhtml#ch18fig4),
    an indexing module parses a document or website into smaller chunks, embedded
    into vectors that can be stored in a vector database. When a user submits a query,
    the indexing module computes the vector similarity between the embedded query
    and each vector stored in the database. Finally, the indexing module retrieves
    the top *k* most similar embeddings to synthesize the response.
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameter-Efficient Fine-Tuning**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, many methods have been developed to adapt pretrained transformers
    more efficiently for new target tasks. These methods are commonly referred to
    as *parameter-efficient fine-tuning*, with the most popular methods at the time
    of writing summarized in [Figure 18-5](ch18.xhtml#ch18fig5).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/18fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 18-5: The main categories of parameter-efficient fine-tuning techniques,
    with popular examples*'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the hard prompting approach discussed in the previous section,
    *soft prompting* strategies optimize embedded versions of the prompts. While in
    hard prompt tuning we modify the discrete input tokens, in soft prompt tuning
    we utilize trainable parameter tensors instead.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind soft prompt tuning is to prepend a trainable parameter tensor
    (the “soft prompt”) to the embedded query tokens. The prepended tensor is then
    tuned to improve the modeling performance on a target data-set using gradient
    descent. In Python-like pseudocode, soft prompt tuning can be described as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: where the `soft_prompt_tensor` has the same feature dimension as the embedded
    inputs produced by the embedding layer. Consequently, the modified input matrix
    has additional rows (as if it extended the original input sequence with additional
    tokens, making it longer).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another popular prompt tuning method is prefix tuning. *Prefix tuning* is similar
    to soft prompt tuning, except that in prefix tuning, we prepend trainable tensors
    (soft prompts) to each transformer block instead of only the embedded inputs,
    which can stabilize the training. The implementation of prefix tuning is illustrated
    in the following pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 18-1: A transformer block modified for prefix tuning*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break [Listing 18-1](ch18.xhtml#ch18lis1) into three main parts: implementing
    the soft prompt, concatenating the soft prompt (prefix) with the input, and implementing
    the rest of the transformer block.'
  prefs: []
  type: TYPE_NORMAL
- en: First, the `soft_prompt`, a tensor, is processed through a set of fully connected
    layers ➊. Second, the transformed soft prompt is concatenated with the main input,
    `x` ➋. The dimension along which they are concatenated is denoted by `seq_len`,
    referring to the sequence length dimension. Third, the subsequent lines of code
    ➌ describe the standard operations in a transformer block, including self-attention,
    layer normalization, and feed-forward neural network layers, wrapped around residual
    connections.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [Listing 18-1](ch18.xhtml#ch18lis1), prefix tuning modifies a transformer
    block by adding a trainable soft prompt. [Figure 18-6](ch18.xhtml#ch18fig6) further
    illustrates the difference between a regular transformer block and a prefix tuning
    transformer block.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/18fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 18-6: A regular transformer compared with prefix tuning*'
  prefs: []
  type: TYPE_NORMAL
- en: Both soft prompt tuning and prefix tuning are considered parameter efficient
    since they require training only the prepended parameter tensors and not the LLM
    parameters themselves.
  prefs: []
  type: TYPE_NORMAL
- en: '*Adapter methods* are related to prefix tuning in that they add additional
    parameters to the transformer layers. In the original adapter method, additional
    fully connected layers were added after the multihead self-attention and existing
    fully connected layers in each transformer block, as illustrated in [Figure 18-7](ch18.xhtml#ch18fig7).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/18fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 18-7: Comparison of a regular transformer block (left) and a transformer
    block with adapter layers*'
  prefs: []
  type: TYPE_NORMAL
- en: Only the new adapter layers are updated when training the LLM using the original
    adapter method, while the remaining transformer layers remain frozen. Since the
    adapter layers are usually small—the first fully connected layer in an adapter
    block projects its input into a low-dimensional representation, while the second
    layer projects it back into the original input dimension—this adapter method is
    usually considered parameter efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'In pseudocode, the original adapter method can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Low-rank adaptation (LoRA)*, another popular parameter-efficient fine-tuning
    method worth considering, refers to reparameterizing pretrained LLM weights using
    low-rank transformations. LoRA is related to the concept of *low-rank transformation*,
    a technique to approximate a high-dimensional matrix or dataset using a lower-dimensional
    representation. The lower-dimensional representation (or *low-rank approximation*)
    is achieved by finding a combination of fewer dimensions that can effectively
    capture most of the information in the original data. Popular low-rank transformation
    techniques include principal component analysis and singular vector decomposition.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose ∆*W* represents the parameter update for a weight matrix
    of the LLM with dimension ℝ*^(A×B)*. We can decompose the weight update matrix
    into two smaller matrices: ∆*W* = *W[A]W[B]*, where *W[A]∈* ℝ*^(A×h)* and *W[A]∈*
    ℝ*^(h×B)*. Here, we keep the original weight frozen and train only the new matrices
    *W[A]* and *W[B]*.'
  prefs: []
  type: TYPE_NORMAL
- en: How is this method parameter efficient if we introduce new weight matrices?
    These new matrices can be very small. For example, if *A* = 25 and *B* = 50, then
    the size of ∆*W* is 25 *×* 50 = 1,250\. If *h* = 5, then *W[A]* has 125 parameters,
    *W[B]* has 250 parameters, and the two matrices combined have only 125 + 250 =
    375 parameters in total.
  prefs: []
  type: TYPE_NORMAL
- en: 'After learning the weight update matrix, we can then write the matrix multiplication
    of a fully connected layer, as shown in this pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 18-2: Matrix multiplication with LoRA*'
  prefs: []
  type: TYPE_NORMAL
- en: In [Listing 18-2](ch18.xhtml#ch18lis2), `scalar` is a scaling factor that adjusts
    the magnitude of the combined result (original model output plus low-rank adaptation).
    This balances the pretrained model’s knowledge and the new task-specific adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: According to the original paper introducing the LoRA method, models using LoRA
    perform slightly better than models using the adapter method across several task-specific
    benchmarks. Often, LoRA performs even better than models fine-tuned using the
    fine-tuning II method described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reinforcement Learning with Human Feedback**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous section focused on ways to make fine-tuning more efficient. Switching
    gears, how can we improve the modeling performance of LLMs via fine-tuning?
  prefs: []
  type: TYPE_NORMAL
- en: The conventional way to adapt or fine-tune an LLM for a new target domain or
    task is to use a supervised approach with labeled target data. For instance, the
    fine-tuning II approach allows us to adapt a pretrained LLM and fine-tune it on
    a target task such as sentiment classification, using a dataset that contains
    texts with sentiment labels like *positive*, *neutral*, and *negative*.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised fine-tuning is a foundational step in training an LLM. An additional,
    more advanced step is *reinforcement learning with human feedback (RLHF)*, which
    can be used to further improve the model’s alignment with human preferences. For
    example, ChatGPT and its predecessor, Instruct-GPT, are two popular examples of
    pretrained LLMs (GPT-3) fine-tuned using RLHF.
  prefs: []
  type: TYPE_NORMAL
- en: In RLHF, a pretrained model is fine-tuned using a combination of supervised
    learning and reinforcement learning. This approach was popularized by the original
    ChatGPT model, which was in turn based on Instruct-GPT. Human feedback is collected
    by having humans rank or rate different model outputs, providing a reward signal.
    The collected reward labels can be used to train a reward model that is then used
    to guide the LLMs’ adaptation to human preferences. The reward model is learned
    via supervised learning, typically using a pretrained LLM as the base model, and
    is then used to adapt the pretrained LLM to human preferences via additional fine-tuning.
    The training in this additional fine-tuning stage uses a flavor of reinforcement
    learning called *proximal policy optimization*.
  prefs: []
  type: TYPE_NORMAL
- en: RLHF uses a reward model instead of training the pretrained model on the human
    feedback directly because involving humans in the learning process would create
    a bottleneck since we cannot obtain feedback in real time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Adapting Pretrained Language Models**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While fine-tuning all layers of a pretrained LLM remains the gold standard for
    adaption to new target tasks, several efficient alternatives exist for leveraging
    pretrained transformers. For instance, we can effectively apply LLMs to new tasks
    while minimizing computational costs and resources by utilizing feature-based
    methods, in-context learning, or parameter-efficient fine-tuning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The three conventional methods—feature-based approach, fine-tuning I, and fine-tuning
    II—provide different computational efficiency and performance trade-offs. Parameter-efficient
    fine-tuning methods like soft prompt tuning, prefix tuning, and adapter methods
    further optimize the adaptation process, reducing the number of parameters to
    be updated. Meanwhile, RLHF presents an alternative approach to supervised fine-tuning,
    potentially improving modeling performance.
  prefs: []
  type: TYPE_NORMAL
- en: In sum, the versatility and efficiency of pretrained LLMs continue to advance,
    offering new opportunities and strategies for effectively adapting these models
    to a wide array of tasks and domains. As research in this area progresses, we
    can expect further improvements and innovations in using pretrained language models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**18-1.** When does it make more sense to use in-context learning rather than
    fine-tuning, and vice versa?'
  prefs: []
  type: TYPE_NORMAL
- en: '**18-2.** In prefix tuning, adapters, and LoRA, how can we ensure that the
    model preserves (and does not forget) the original knowledge?'
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The paper introducing the GPT-2 model: Alec Radford et al., “Language Models
    Are Unsupervised Multitask Learners” (2019), *[https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper introducing the GPT-3 model: Tom B. Brown et al., “Language Models
    Are Few-Shot Learners” (2020), *[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The automatic prompt engineering method, which proposes using another LLM for
    automatic prompt generation and evaluation: Yongchao Zhou et al., “Large Language
    Models Are Human-Level Prompt Engineers” (2023), *[https://arxiv.org/abs/2211.01910](https://arxiv.org/abs/2211.01910)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LlamaIndex is an example of an indexing approach that leverages in-context
    learning: *[https://github.com/jerryjliu/llama_index](https://github.com/jerryjliu/llama_index)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DSPy is a popular open source library for retrieval augmentation and indexing:
    *[https://github.com/stanfordnlp/dsp](https://github.com/stanfordnlp/dsp)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A first instance of soft prompting: Brian Lester, Rami Al-Rfou, and Noah Constant,
    “The Power of Scale for Parameter-Efficient Prompt Tuning” (2021), *[https://arxiv.org/abs/2104.08691](https://arxiv.org/abs/2104.08691)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper that first described prefix tuning: Xiang Lisa Li and Percy Liang,
    “Prefix-Tuning: Optimizing Continuous Prompts for Generation” (2021), *[https://arxiv.org/abs/2101.00190](https://arxiv.org/abs/2101.00190)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper introducing the original adapter method: Neil Houlsby et al., “Parameter-Efficient
    Transfer Learning for NLP” (2019) *[https://arxiv.org/abs/1902.00751](https://arxiv.org/abs/1902.00751)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper introducing the LoRA method: Edward J. Hu et al., “LoRA: Low-Rank
    Adaptation of Large Language Models” (2021), *[https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A survey of more than 40 research papers covering parameter-efficient fine-tuning
    methods: Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky, “Scaling Down
    to Scale Up: A Guide to Parameter-Efficient Fine-Tuning” (2023), *[https://arxiv.org/abs/2303.15647](https://arxiv.org/abs/2303.15647)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The InstructGPT paper: Long Ouyang et al., “Training Language Models to Follow
    Instructions with Human Feedback” (2022), *[https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Proximal policy optimization, which is used for reinforcement learning with
    human feedback: John Schulman et al., “Proximal Policy Optimization Algorithms”
    (2017), *[https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
