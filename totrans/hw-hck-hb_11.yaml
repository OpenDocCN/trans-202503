- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gettin’ Nerdy with It: Advanced Power Analysis'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: The previous two chapters, and power analysis literature in general, focused
    on theoretical understanding of the attacks and applying them in lab conditions.
    As people who have witnessed a plethora of such attacks, we can tell you that
    for the majority of actual targets, 10 percent of your time is spent getting the
    measurement set up in order; 10 percent of your time is running actual power analysis
    attacks, and the other 80 percent of your time is spent trying to figure out why
    the attacks are not showing any leakage. That is because your attack will show
    leakage only if you got every step from trace acquisition to trace analysis correct,
    and until you actually find leakage, it can be difficult to determine which step
    was wrong in the first place. In reality, power analysis requires patience, sprinkled
    with a lot of step analysis, a bunch of trial and error, and topped off with computing
    power. This chapter is more about the *art* of power analysis than the science.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, you’ll need some extra tools to overcome the various obstacles
    that a real-life target will throw at you. These obstacles will largely determine
    how difficult it will be to extract a secret from a device successfully. Some
    properties inherent in the target you’re testing will affect the signal and noise
    characteristics, as will properties like programmability, device complexity and
    clock speed, type of side channel, and countermeasures. When measuring a software
    implementation of AES on a microcontroller, you’ll probably be able to identify
    the individual encryption rounds from a single trace with one eye closed and a
    hand behind your back. When you’re measuring a hardware AES running at 800 MHz
    embedded in a full System-on-Chip (SoC), forget about ever seeing the encryption
    rounds in a single trace. Many parallel processes cause amplitude noise—never
    mind that the leakage signal is extremely small. The simplest AES implementations
    may break in less than 100 traces and 5 minutes of analysis, whereas the most
    complex attacks we’ve seen succeed have passed beyond a billion(!) traces and
    months of analysis—and, sometimes the attack still fails.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we’ll provide tools to apply in various situations and
    a general recipe for how to approach the entire power analysis topic. Equipped
    with these tools, it’s up to you to find out if, when, and how to apply them on
    your favorite target. As such, this chapter is a bit of a mixed bag. First, we
    discuss a number of more powerful attacks and provide references. Next, we dive
    into a number of ways to measure key extraction success and how to measure improvements
    in your setup. Then, we talk about measuring real devices, as opposed to some
    easy lab-based, full-control targets. After that, there is a section on trace
    analysis and processing, and, finally, we provide some additional references.
  prefs: []
  type: TYPE_NORMAL
- en: The Main Obstacles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Power analysis comes in various flavors. We’ll refer to *simple power analysis
    (SPA)*, *differential power analysis (DPA)*, and the *correlation power attack
    (CPA)* in this chapter, or simply to *power analysis* when a statement applies
    to all three.
  prefs: []
  type: TYPE_NORMAL
- en: 'The differences between theory and attacking actual devices are significant.
    You’ll meet your main obstacles when doing actual power analysis. These obstacles
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amplitude noise**'
  prefs: []
  type: TYPE_NORMAL
- en: This is the hiss you hear when listening to AM radio transmissions, the noise
    from all the other electrical components in your setup, or the random noise added
    as a countermeasure. Various parts of your measurement setup will cause it, but
    non-interesting-yet-parallel operations in the actual device will also end up
    in your measurement. You’ll encounter amplitude noise in all measurements you
    take, and it’s a problem to your power attack because it obscures the actual power
    variations due to data leakage. For CPA, it causes your correlation peak to decrease
    in amplitude.
  prefs: []
  type: TYPE_NORMAL
- en: '**Temporal noise (also known as misalignment)**'
  prefs: []
  type: TYPE_NORMAL
- en: Timing jitter caused by oscilloscope triggering or nonconstant time paths to
    your target operation result in the operation of interest appearing at different
    times with each trace. This jitter affects a correlation power attack because
    the attack assumes that the leakage always appears at the same time index. The
    jitter has the undesired effect of widening your correlation peak and decreasing
    its amplitude.
  prefs: []
  type: TYPE_NORMAL
- en: '**Side-channel countermeasures**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Yes, chip and device vendors also read this book. The unintentional noise sources
    just described can also be introduced by device designers intentionally to decrease
    the effectiveness of a power attack. Not only are noise sources introduced, but
    the leakage signals are decreased by using algorithms and chip designs such as
    masking and blinding (see Thomas S. Messerges’s “Securing the AES Finalists Against
    Power Analysis Attacks”), constant key rotation in a protocol (see Pankaj Rohatgi’s
    “Leakage Resistant Encryption and Decryption”), as well as constant power circuits
    (see Thomas Popp and Stefan Mangard’s “Masked Dual-Rail Pre-charge Logic: DPA-Resistance
    Without Routing Constraints”) and SCA-resistant cell libraries (see Kris Tiri
    and Ingrid Verbauwhede’s “A Logic Level Design Methodology for a Secure DPA Resistant
    ASIC or FPGA Implementation”).'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t despair, though. For each source of noise or countermeasure, a tool exists
    to recover at least some fraction of the leakage. As an attacker, your goal is
    to combine all these tools into a successful attack; as a defender, your goal
    is to present sufficient countermeasures that cause your attacker to run out of
    resources like skill, time, patience, computing power, and disk space.
  prefs: []
  type: TYPE_NORMAL
- en: More Powerful Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What we’ve described so far about power analysis are actually some of the more
    basic attacks in the field. A variety of more powerful attacks exist, and many
    are well beyond the scope of this chapter. Nevertheless, we don’t want to leave
    you on the wrong side of the Dunning-Kruger curve of actual knowledge versus perceived
    knowledge. We want to make sure you have sufficient knowledge to know that you
    don’t have all the knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Everything you have learned up to now has used a *leakage model*. This model
    made some basic assumptions—for example, that greater power being drawn can mean
    that more wires are set high. A more powerful method is the template attack (see
    Suresh Chari, Josyula R. Rao, and Pankaj Rohatgi’s “Template Attacks”). In a *template
    attack*, instead of assuming a leakage model, you measure it directly from a device
    for which you know the data (and key!) being processed. The knowledge of the data
    and key provides you with an indication of the power used for a range of known
    data values, which is encoded in a template for each value. A template of known
    data values helps you recognize the unknown data values on the same or similar
    device.
  prefs: []
  type: TYPE_NORMAL
- en: Making such a template model means you need a device you can completely control
    by setting your own key values and allowing the desired encryption to occur. The
    practicability of this approach varies because it may be difficult to reprogram
    your target device, or you may have only a single copy of the target that you
    can’t reprogram to generate templates. Other times, like with generic microcontrollers,
    you could access as many programmable devices as you need.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage of template attacks is that they operate on a more precise model
    than CPA and, therefore, can perform key retrieval in fewer traces, possibly revealing
    an entire encryption key with just a *single encryption operation*. Another advantage
    is that if the device you’re attacking is performing some nonstandard algorithm,
    a template attack doesn’t require you to have a model for the leakage. The downside
    of these more powerful attacks is the computational complexity and memory requirements,
    which are greater than a simple correlation with a Hamming weight. Therefore,
    choosing whether to use templates or other techniques, such as *linear regression*
    (see Julien Doget, Emmanuel Prouff, Matthieu Rivain, and François-Xavier Standaert’s
    “Univariate Side Channel Attacks and Leakage Modeling”), *mutual information analysis*
    (see Benedikt Gierlichs, Lejla Batina, Pim Tuyls, and Bart Preneel’s “Mutual Information
    Analysis”), *deep learning* (see Guilherme Perin, Baris Ege, and Jasper van Woudenberg’s,
    “Lowering the Bar: Deep Learning for Side-Channel Analysis”), or *differential
    cluster analysis* (see Lejla Batina, Benedikt Gierlichs, and Kerstin Lemke-Rust’s
    “Differential Cluster Analysis”), depends on what is required or available in
    your attack circumstances, such as having the least number of traces, the shortest
    wall clock time, the least computational complexity, lesser human analysis, and
    any number of other circumstances.'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of more practical tips, Victor Lomné, Emmanuel Prouff, and Thomas Roche
    wrote “Behind the Scene of Side Channel Attacks — Extended Version,” which contains
    many tips on various attacks. Specifically, *conditional leakage averaging* for
    CPA can save a lot of time. You can find an implementation of it and various other
    algorithms as part of Riscure’s open source Jlsca project at [https://github.com/Riscure/Jlsca/](https://github.com/Riscure/Jlsca/).
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, we’ll discuss further references.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Success
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How we measure success in life is a topic prone to philosophical ramblings.
    Fortunately, engineers and scientists have little time for ramblings, so here
    are a variety of methods that allow us to measure the success of side-channel
    analysis attacks. We’ll discuss several data types and graphs you are likely to
    run into during your further research.
  prefs: []
  type: TYPE_NORMAL
- en: Success Rate–Based Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the original metrics used in academia was based on the success rate of
    the attack. The most basic version of it might be to test how many traces are
    required for an attack that *completely recovers the encryption key*. This metric
    generally isn’t too useful. If you’re just doing a single trial, it might be that
    you got exceptionally lucky; usually it would take more traces than what you have
    reported.
  prefs: []
  type: TYPE_NORMAL
- en: To counter this unrealistic situation, we use plots of the success rate versus
    number of traces. We will first refer to the *global success rate (GSR)*, which
    provides the percentage of attacks that successfully recovered the complete key
    for a particular number of traces. [Figure 11-1](#figure11-1) shows a sample GSR
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![f11001](image_fi/278748c11/f11001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-1: Sample graph of global success rate for a leaky AES-256 target'
  prefs: []
  type: TYPE_NORMAL
- en: The graph in [Figure 11-1](#figure11-1) shows that if we had 40 traces recorded
    from the device, we would expect to recover the complete encryption key about
    80 percent of the time. We can find this metric simply by performing the experiment
    on the device many times, ideally with different encryption keys in case certain
    values of the key generate more leakage than other keys do.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than using the GSR, we might also plot the *partial success rate*. Here,
    *partial* means that we are considering each of the 16 bytes in the AES-128 key
    independently of the other bytes, which provides 16 values, each representing
    the probability of recovering the correct value for one particular byte, given
    a fixed number of traces.
  prefs: []
  type: TYPE_NORMAL
- en: The global success rate could be misleading because in some particular implementations,
    one of the key bytes might not leak. The GSR, thus, will always be zero, since
    the entire encryption key is never recovered, but plots of the partial success
    rate will reveal whether only one of the 16 bytes cannot be recovered. We could
    then brute-force that last byte within 1 second, whereas a zero GSR would not
    have revealed a real probability of recovering the key.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy-Based Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Entropy-based metrics are based on the principle that we can do some guessing
    to recover the key. The original AES-128 key would require, on average, 0.5 ×
    2^(128) guesses to recover the key without any prior knowledge. This number is
    so large, the key cannot be computed before the cluster brute-forcing the key
    will be melted and/or eaten by the sun as it transforms into a red giant (about
    5 billion years from now).
  prefs: []
  type: TYPE_NORMAL
- en: The outcome of a side-channel analysis attack provides more information than
    a simple “key is XYZ” or “key not found.” In fact, each key guess has a confidence
    level associated with it—the confidence that a key guess is correct relative to
    a particular analysis method. In CPA, this confidence value is the absolute value
    of the correlation of that particular key guess. The outcome of a CPA attack on
    one byte of an AES-128 key is therefore a ranked list of key guesses with confidence
    levels, with our best guess at the top and worst guess at the bottom.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that using a power analysis attack, we know the actual key byte is
    in the top three of each list. Then there are in total 3^(16) guesses to make
    for the key, which is about 43 million, so it can easily be done on a smartphone.
    We have, thus, reduced the entropy. The original key was a random collection of
    bits, but we now have some information about the most likely state of certain
    bits and can use this to speed up the brute-force attack.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest plot to represent this is the *partial guessing entropy (PGE)*.
    The PGE asks the following question: after you performed the attack with a certain
    number of traces, how many key guesses were incorrectly ranked as more likely
    than the correct key value? If you are doing key guesses for each byte, you will
    have a PGE value for each byte of the key; for AES-128, you will end up with 16
    PGE plots. PGE provides information about the reduction in key-search space being
    made by the side-channel attack. [Figure 11-2](#figure11-2) shows an example of
    such a plot.'
  prefs: []
  type: TYPE_NORMAL
- en: The graph in [Figure 11-2](#figure11-2) also averages all the 16 PGE plots to
    get an average PGE for the attack. The partial guessing entropy can be a little
    misleading, as we might not have an ideal way to combine guessing across all keys.
    For instance, if for one key byte the correct value is ranked first, and for a
    second key byte ranked third, we still need to take a worst-case assumption and
    brute-force all top three candidates. Such a brute-force attack very quickly becomes
    impossible, however, if the PGE is not even across all bytes.
  prefs: []
  type: TYPE_NORMAL
- en: '![f11002](image_fi/278748c11/f11002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-2: Partial guessing entropy'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms for ideally combining the output of the attack exist, and they can
    be used to generate a true total guessing entropy (see Nicholas Veyrat-Charvillon,
    Benoît Gérard, François-Xavier Standaert’s “Security Evaluations Beyond Computing
    Power”). The total guessing entropy provides exact details of the reduction of
    the guessing space of the key that resulted from running the attack algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation Peak Progression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another format is to plot the correlation of each key guess over a number of
    traces. This method is designed to show the progression of the amplitude of correlation
    peaks over time; see [Figure 11-3](#figure11-3) as an example. It shows for each
    key guess what the correlation peak is when we increase the number of traces.
    For wrong key guesses, this correlation will trend toward zero, whereas for the
    right key guess, it will trend toward the actual level of leakage.
  prefs: []
  type: TYPE_NORMAL
- en: '![f11003](image_fi/278748c11/f11003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-3: Plots of correlation peak vs. trace number show the correct guess.'
  prefs: []
  type: TYPE_NORMAL
- en: This graph removes information about at which point in time the maximum correlation
    peak occurred, but it now shows how that peak becomes differentiated from the
    “wrong guesses.” The point where the correct peak crosses over all the incorrect
    guesses is considered to be where the algorithm was broken. Plots of correlation
    output against the trace number show the correct key guess slowly evolving out
    of the noise of incorrect key guesses.
  prefs: []
  type: TYPE_NORMAL
- en: An advantage of the graph shown in [Figure 11-3](#figure11-3) is that it indicates
    the margin between the incorrect and the correct guess. If that margin is large,
    you can be more confident that the attack will be successful in general.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation Peak Height
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The success metrics described so far provide an idea of how close you are to
    key extraction, but they do not help much in debugging your setup or trace-processing
    approach. For those tasks, there is one simple approach: looking at the output
    traces from the attack algorithm, such as correlation traces for CPA (or t-traces
    for TVLA, which we discuss later). These output traces are one of the main ways
    to improve your setup or processing.'
  prefs: []
  type: TYPE_NORMAL
- en: The plot you make, such as in [Figure 11-4](#figure11-4), highlights all the
    correlation traces of incorrect key guesses in one color and the correct key guess
    in another color.
  prefs: []
  type: TYPE_NORMAL
- en: '![f11004](image_fi/278748c11/f11004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-4: Plot of the raw output from the attack algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-4](#figure11-4) shows that the correct key guess has the largest
    correlation peak, and it also provides the time index of this peak. This plot
    shows correlation as a function of time, where the correct key guess is highlighted
    in dark gray in the figure, and the incorrect guesses are light gray. Overlaying
    this plot with power traces can be useful for visualizing where the leakage happens.'
  prefs: []
  type: TYPE_NORMAL
- en: This type of plotting comes in very handy when you are optimizing your setup.
    Simply calculate the plot before and after you change one of your acquisition
    parameters or processing steps. If the peak gets stronger, you’ve improved your
    side-channel attack; if it decreases, it has gotten worse.
  prefs: []
  type: TYPE_NORMAL
- en: Measurements on Real Devices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When the time comes to measure a real device—not a simple experimental platform
    designed for side-channel analysis—you need to make some additional considerations.
    This section briefly outlines them.
  prefs: []
  type: TYPE_NORMAL
- en: Device Operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step in attacking a real device is operating it. The requirements
    for doing so depend on the attack you are performing, but we can give you some
    general guidance and hints on running crypto operations and choosing what inputs
    to send.
  prefs: []
  type: TYPE_NORMAL
- en: Initiating Encryption
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Real devices may not provide an “encrypt this block” function. Part of the work
    in side-channel analysis attacks is to determine exactly how to attack such devices.
    For example, if we’re attacking a bootloader that authenticates firmware before
    decrypting it, we cannot just send random input data to decrypt. However, for
    power analysis, often just knowing the ciphertext or the plaintext is sufficient.
    In this case, we can just feed the original firmware image, which will pass the
    authenticity check and will then be decrypted. Since we know the ciphertext of
    the firmware, we can still perform a power attack.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, many devices will have a challenge-response-based authentication
    function. These functions typically require you to respond to a random nonce value
    by encrypting it. The device will separately also encrypt the nonce. Now the device
    can verify whether the response from you was encrypted properly, thereby proving
    you share the same key as the device. If you send the device a random garbage
    value, the authentication check will ultimately fail. However, that failure is
    irrelevant; we have captured the nonce and the power signal of the device during
    encryption. If we collect a set of those signals, it could give us sufficient
    information for a power analysis attack. Proper implementations will include rate
    limiting or a fixed number of tries to avoid this attack.
  prefs: []
  type: TYPE_NORMAL
- en: Another problem when dealing with device communication will be timing the acquisition.
    As demonstrated previously, we don’t care about finding the exact moment the encryption
    happened, as the CPA attack will reveal this for us (assuming alignment, but we’ll
    talk about that later). We do need to get within the general vicinity of the correct
    timing (for example, by triggering our oscilloscope based on when we send the
    last packet of an encrypted block). We don’t know when the encryption occurs,
    but we do know that it clearly must occur sometime between sending that block
    and the device sending back a response message.
  prefs: []
  type: TYPE_NORMAL
- en: Triggering based on sniffing I/O lines will be more difficult. Often the easiest
    way is to implement a custom device that monitors the I/O lines for the relevant
    activity. You could program a microcontroller simply to read all data being sent
    and set an I/O pin high when it detects the desired byte(s), which in turn triggers
    the oscilloscope.
  prefs: []
  type: TYPE_NORMAL
- en: Starting and capturing the operation is mostly an engineering hurdle, but it’s
    important to make it as stable and jitter-free as possible. Jittery timing behavior
    results in timing noise and other issues down the line, which may make it impossible
    to do proper analysis of the traces later.
  prefs: []
  type: TYPE_NORMAL
- en: Repeating and Separating Operations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another trick to remember is that if you have programmatic control over your
    target, it helps to get many operations in a single trace. You can do this by
    making the number of times that the target operation is called within one trace
    an input variable in your protocol. The simplest trick is to put a loop around
    the call to the operation on the target itself. In some cases, you can have it
    loop at a lower level by, for instance, giving an AES-ECB encryption engine a
    large number of blocks to encrypt.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you perform acquisitions with an increasing number of calls to the target
    operation (for instance, by doubling it every trace), you’ll soon start to see
    an expansion where the crypto operations are being performed. This happens because
    although a single crypto operation may be an invisible blip, the more operations
    you do, the longer they will take. At some point, it becomes visible in your trace.
    You then can easily pinpoint the timing of the operation and calculate the average
    duration of a single operation.
  prefs: []
  type: TYPE_NORMAL
- en: It may also be worthwhile to experiment with a variable delay loop (or nop slide;
    *nop* means a no-operation, which effectively causes the processor to do nothing
    for a very specific amount of time) in between the operations. Once the previous
    trick has shown you the timing, you can use that information to separate the individual
    operation calls, which can actually help to detect leaks, because the leakage
    from one operation does not then bleed into successive operations.
  prefs: []
  type: TYPE_NORMAL
- en: From Random Inputs to Chosen Inputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Up to now, we’ve been inputting fully random data into our crypto algorithms,
    which provides good properties for the CPA calculation. Some specific attacks
    require chosen inputs, like certain attacks on AES (see Kai Schramm, Gregor Leander,
    Patrick Felke, and Christof Paar’s “A Collision-Attack on AES: Combining Side
    Channel- and Differential-Attack”) or for the intermediate round variant of test
    vector leakage assessment (TVLA) using Welch’s t-test (more details in the “Test
    Vector Leakage Assessment” section later in this chapter).'
  prefs: []
  type: TYPE_NORMAL
- en: Without going into the details of why (we will later), you can create a number
    of different sets during trace acquisition, such as measurements associated with
    constant or random input data, and various carefully chosen inputs.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll be doing various statistical analyses on these sets, so it’s crucially
    important that the only statistically relevant differences between your sets are
    caused by differences in your input data. In reality, trace acquisition campaigns
    that run for more than a few hours will have detectable changes in perhaps the
    average power level (see the “Analysis Techniques” section later in this chapter).
    If you measure set A at minute 0 and set B at minute 60, your statistics will
    surely show power differences between those sets. These power differences may
    appear to be insignificant until you discover that suspected leakage is in fact
    due to your air conditioning kicking in at minute 59 and cooling the target device,
    and not due to a leaky target. Whenever you do statistical analysis over several
    sets, you must make sure there is no accidental correlation with anything but
    the input data. This means that for each trace you measure, you must randomly
    select for which set you want to generate input. You also do *not* even want the
    target to know for which set you are doing a measurement; all it needs to know
    is the data on which to operate. If you send the target information regarding
    the set, it will show up in your traces. If you interleave the sets instead of
    choose them randomly, it will show up in your traces. These uninteresting correlations
    are extremely hard to debug, as they will show up as (false) leakage, so you should
    work hard at avoiding them. You are detecting extremely small changes in power,
    and a switch statement running on the target based on the trace set is going to
    overshadow any interesting leakage.
  prefs: []
  type: TYPE_NORMAL
- en: The Measurement Probe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To perform the side-channel attack, you need to measure your device’s power
    consumption. Taking this measurement was trivial when attacking a target board
    you designed, but it requires more creativity on real devices. We’ll discuss the
    two main methods: using a physical shunt resistor and using an electromagnetic
    probe.'
  prefs: []
  type: TYPE_NORMAL
- en: Inserting Shunt Resistors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If attempting to measure power on a “standard” board, you’ll need to make some
    modifications to the board for the power consumption measurements. This will differ
    from board to board, but as an example, see [Figure 11-5](#figure11-5), which
    shows how you can lift the leg of a thin quad flat pack (TQFP) package to insert
    a surface-mount resistor.
  prefs: []
  type: TYPE_NORMAL
- en: '![f11005](image_fi/278748c11/f11005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-5: Inserting a resistor into the leg of a TQFP package'
  prefs: []
  type: TYPE_NORMAL
- en: You then have to connect your oscilloscope probe to either side of the resistor,
    which allows you to measure the voltage drop across the resistor and thereby the
    current consumption of a specific voltage net.
  prefs: []
  type: TYPE_NORMAL
- en: Electromagnetic Probes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A more advanced alternative is to use an electromagnetic probe (also called
    an H-field probe, near-field probe, or magnetic field probe), which can be positioned
    above or close to the area of interest. The resulting analysis is called *electromagnetic
    analysis (EMA)*. EMA requires no modifications to the device under attack, as
    the probe can just be placed directly over the chip or above the decoupling capacitors
    around the chip. These probes are sold in sets known as *near-field probe sets*,
    and they typically include an amplifier.
  prefs: []
  type: TYPE_NORMAL
- en: The theory on why this works is simple. High-school physics teaches us that
    a current flowing through a wire creates a magnetic field around the wire. The
    righthand rule tells us that if we hold the wire such that our thumb is pointing
    in the direction of the current, the magnetic field lines would circle around
    the wire in the direction of our fingers. Now, any activity inside the chip is
    simply switching currents. Instead of measuring the switching current directly,
    we probe the switching magnetic field around it. This works on the principle that
    a switching magnetic field induces a current in a wire. We can measure that wire
    with a scope, which rather indirectly reflects the switching activity in the chip.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling Your Own Electromagnetic Probe
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As an alternative to buying a probe, you can build a simple probe yourself.
    Building your own EM probe is fun for the whole family, provided the family likes
    working with sharp objects, soldering irons, and chemicals. In addition to the
    probe, you’ll need to build a low-noise amplifier for increasing the strength
    of the signal your oscilloscope or other device is measuring.
  prefs: []
  type: TYPE_NORMAL
- en: The probe itself is built from a length of semi-flexible coaxial cable. You
    can purchase this from various sources (Digi-Key, eBay) by looking for “SMA to
    SMA cables,” such as Crystek Part Number CCSMA-MM-086-8, which is available from
    Digi-Key for around US$10\. Cutting this cable in half gives you two lengths of
    semi-flexible cable, each with an SMA connector on the one end (one of which is
    shown in [Figure 11-6](#figure11-6)).
  prefs: []
  type: TYPE_NORMAL
- en: '![f11006](image_fi/278748c11/f11006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-6: Home-built EM probes from a semi-flexible SMA cable'
  prefs: []
  type: TYPE_NORMAL
- en: Cut a slot 1 around the entire outer shield. Strip away a few millimeters from
    the end 2. Gently round this into a circle 3, gripping the slot with pliers to
    stop the internal conductor wire from kinking. To complete the basic probe, solder
    the circle shut 4, making sure that the internal conductor wire is included in
    the solder connection between the outer shields.
  prefs: []
  type: TYPE_NORMAL
- en: Because the outer shield is conductive, you might want to coat the surface with
    a nonconductive material, such as a rubber coating like Plasti Dip, or wrap it
    with self-fusing tape.
  prefs: []
  type: TYPE_NORMAL
- en: The signal picked up at the narrow gap in this probe will be tiny, so you’ll
    need an amplifier to see any signal on your oscilloscope. You can use a simple
    IC as the basis for a low-noise amplifier. It requires a clean 3.3 V power supply,
    so consider also building the voltage regulator onto the circuit board. If your
    oscilloscope isn’t sufficiently sensitive, you might even need to chain two amplifiers
    together to achieve enough gain. [Figure 11-7](#figure11-7) shows an example of
    a simple amplifier built around a $0.50 IC (part number BGA2801,115).
  prefs: []
  type: TYPE_NORMAL
- en: '![f11007](image_fi/278748c11/f11007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-7: Simple amplifier for an EM probe'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to build the amplifier yourself, see [Figure 11-8](#figure11-8)
    for the schematic.
  prefs: []
  type: TYPE_NORMAL
- en: 'The choice of side-channel measurement can significantly affect the signal
    and noise characteristics. There is generally low noise when directly measuring
    power drawn by a chip, as compared to, for instance, the noise in an electromagnetic
    measurement, or in an acoustic side channel (see Daniel Genkin, Adi Shamir, and
    Eran Tromer’s “RSA Key Extraction via Low-Bandwidth Acoustic Cryptanalysis”),
    or in a measurement of the chassis potential (see Daniel Genkin, Itamar Pipman,
    and Eran Tromer’s “Get Your Hands Off My Laptop: Physical Side-Channel Key-Extraction
    Attacks on PCs”). However, a direct measurement of power means that you measure
    all of the power consumption, including the power drawn by processes you’re not
    interested in. On an SoC, you may get a better signal with an EM measurement if
    your probe is carefully positioned over the physical location of the leakage.
    You may encounter countermeasures that minimize leakage in direct power measurement
    but do not limit it in the EM measurement, or vice versa. As a rule of thumb,
    try EM first on complex chips and SoCs, and try power first on smaller microcontrollers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![f11008](image_fi/278748c11/f11008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-8: Schematic for simple amplifier for an EM probe'
  prefs: []
  type: TYPE_NORMAL
- en: Determining Sensitive Nets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whether using a resistive shunt or an EM probe, we have to determine what part
    of the device must be measured. The objective is to measure power consumption
    of the logic circuit performing the sensitive operation—be it a hardware peripheral
    or the general-purpose core executing a software program.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of the resistive shunt, this means looking at power pins on the
    IC. Here you need to measure at one of the pins powering the internal cores, not
    at the pins that power the I/O pin drivers. Small microcontrollers might have
    a single power supply used for all parts of the microcontroller. Even these simple
    microcontrollers can have multiple power pins with the same name, so select one
    that’s most easily accessed. Be sure not to select a supply dedicated to the analog
    portion, such as the analog-to-digital converter power supply, as that will likely
    not power the components of interest.
  prefs: []
  type: TYPE_NORMAL
- en: More advanced devices might have four or more power supplies. For example, the
    memory, CPU, clock generator, and analog section could all be separate supplies.
    Again, you may need to do some experimentation, but almost certainly, the supply
    you want will be one of the supplies with the word *CPU* or *CORE* in the name.
    You can use the data you dug up with the help of Chapter 3 to identify the most
    likely targets.
  prefs: []
  type: TYPE_NORMAL
- en: If targeting a device using an EM probe, you’ll need to experiment to determine
    the correct orientation and location for the probe. It’s also worth placing the
    probe near the decoupling capacitors surrounding the target, as high currents
    will tend to flow through those parts. In this case, you would need to determine
    which decoupling capacitors are associated with the core components of the device,
    similar to determining which power supply to target.
  prefs: []
  type: TYPE_NORMAL
- en: Letting your target run encryptions while displaying live trace captures on
    a screen can be enlightening. As the probe moves, you’ll see the captured traces
    vary wildly. A good rule of thumb is to find a place where the field is weak before
    and after the crypto phase and strong while performing the crypto routine. It
    helps to display a trigger that “hugs” the operation as well. It doesn’t hurt
    to move the probe around manually to get a quick sense of the leakage over various
    parts of the chip.
  prefs: []
  type: TYPE_NORMAL
- en: Automated Probe Scanning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mounting the probe on an XY stage and automatically capturing traces over various
    positions on the chip allows more precise localization of interesting areas. [Figure
    11-9](#figure11-9) shows a sample setup.
  prefs: []
  type: TYPE_NORMAL
- en: You can use TVLA to get another nice visualization, as explained in the “Test
    Vector Leakage Assessment” section later in this chapter. TVLA measures leakage
    without doing a CPA attack, so if you visualize the TVLA outcome, you’ll see a
    plot of actual leakage over the area of the chip. The downside is that in order
    to calculate TVLA values, you need to have two full measurement sets for each
    spot on the chip, which increases the length of your trace acquisition campaign
    dramatically.
  prefs: []
  type: TYPE_NORMAL
- en: Probing more spots increases the chances that you find the *right* spot, but
    it decreases your efficiency. Scan at a spatial resolution that gives more continuous
    data gradients in the visualization to ensure that your XY scan step size is smaller
    than the sensitive area of your probe.
  prefs: []
  type: TYPE_NORMAL
- en: Scanning is of particular interest when combined with the technique described
    later in this chapter in the “Filtering for Visualization” section. If you know
    your target operation’s leakage frequency, you can visualize the signal strength
    at that frequency as a function of the position over your chip. This leads to
    pretty pictures such as the one in [Figure 11-10](#figure11-10), which shows an
    XY scan visualization of the leakage intensity over different areas on the chip
    in the 31-to-34 MHz band. These kinds of images can help localize areas of interest
    and can be done with as little as one trace per location.
  prefs: []
  type: TYPE_NORMAL
- en: '![f11009](image_fi/278748c11/f11009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-9: Example of a Riscure electromagnetic probe mounted on an XY stage'
  prefs: []
  type: TYPE_NORMAL
- en: '![f11010](image_fi/278748c11/f11010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-10: XY scan visualization of leakage areas from a chip'
  prefs: []
  type: TYPE_NORMAL
- en: Oscilloscope Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An oscilloscope is an ideal tool for capturing and presenting the leakage signals
    from a magnetic probe. You’ll have to set up your oscilloscope carefully to get
    good information. We discussed the various input types available for your oscilloscope
    in Chapter 2, along with the general advice on avoiding the use of probes that
    will introduce considerable noise on a very small signal. To reduce noise further,
    some sort of amplification is often required on the input to the oscilloscope
    for boosting the signal.
  prefs: []
  type: TYPE_NORMAL
- en: You can use a *differential amplifier* to do this, which amplifies only the
    *difference* between the two signal points. Beyond just boosting the signal, the
    differential amplifier removes noise present on both signal points (called *common-mode*
    noise). In real life, this means that noise generated by the power supply will
    mostly be removed, leaving only the voltage variation that is measured across
    your measurement resistor.
  prefs: []
  type: TYPE_NORMAL
- en: Oscilloscope manufacturers sell commercial *differential probes*, but they’re
    typically extremely expensive. As an alternative, you can simply build a differential
    amplifier using a commercial operational amplifier (or *op-amp*). A differential
    probe can measure power consumption across the resistor to reduce noise contribution.
    A sample open source design is available as part of the ChipWhisperer project,
    which uses the Analog Devices AD8129\. [Figure 11-11](#figure11-11) is a photo
    of this probe in use on a physical device.
  prefs: []
  type: TYPE_NORMAL
- en: '![f11011](image_fi/278748c11/f11011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-11: A differential probe in use on a target board'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 11-11](#figure11-11), the differential probe has a positive (+) and
    negative (–) pin. These pins are marked on the lower-right side on the black probe
    PCB silkscreen. Wires 2 and 1 connect the positive and negative pins, respectively,
    to two sides of the shunt resistor mounted onto the target PCB. The differential
    probe is used in this example because the power flowing into the shunt resistor
    is noisy, and we want to remove this common-mode noise.
  prefs: []
  type: TYPE_NORMAL
- en: The schematic for the differential probe is shown in [Figure 11-12](#figure11-12),
    in case you are curious about the details of its connection.
  prefs: []
  type: TYPE_NORMAL
- en: '![f11012](image_fi/278748c11/f11012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-12: Differential probe schematic'
  prefs: []
  type: TYPE_NORMAL
- en: Sampling Rate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So far, we assume you’ve magically been able to read your measurements into
    the computer. Previous chapters briefly explained that when setting up your oscilloscope,
    you need to select an appropriate sampling rate. The upper limit on this sampling
    rate is based on how much you paid for your oscilloscope; if you have enough money,
    you can buy 100 GS/s (giga-samples per second) or faster devices.
  prefs: []
  type: TYPE_NORMAL
- en: More is not always better. Longer traces mean lots of storage space and much
    longer processing times. You might want to sample at a very high rate and then
    *downsample* (that is, average consecutive samples) when storing your data, which
    will improve your waveforms considerably. First, downsampling results in a virtual
    increase in your scope’s quantization resolution. If your scope has an 8-bit ADC
    running at 100 MHz and you average every two samples, you effectively have a 9-bit
    scope running at 50 MHz. This is simply because if a sample value of 55 and a
    sample value of 56 are averaged, they produce 55.5\. The inclusion of these “half”
    values effectively adds 1 bit of resolution. Or, you can average over four consecutive
    samples to have an effective 10-bit scope at 25 MHz.
  prefs: []
  type: TYPE_NORMAL
- en: Second, sampling fast reduces time jitter in the measurements. A trigger event
    happens at some point during a sampling period, and the scope will start measuring
    only at the next sampling period. The fact that the trigger event happens asynchronously
    to the oscilloscope sampling clock means there is jitter between the trigger event
    and the next sampling period. This jitter manifests itself as a misalignment in
    traces.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the situation where the oscilloscope is sampling at a slower rate,
    like 25 MS/s, meaning that samples are being taken every 40ns. Whenever the trigger
    event occurs (that is, the start of the encryption), you’ll have some delay until
    the start of the next sample. This delay would be on average 20ns (half the sample
    period), since the time base of the oscilloscope is completely independent of
    the time base on the target device.
  prefs: []
  type: TYPE_NORMAL
- en: If you sample much faster (say, at 1 GS/s), that delay from the trigger to the
    start of the first sample will be only 0.5ns, or 40 times better! Once you record
    the data, you can then downsample it to reduce your memory requirements. The resulting
    waveform will have the same number of points as if you performed the capture at
    25 MS/s, but now the jitter is no more than 0.5ns, thus considerably improving
    the outcome of a side-channel attack (see Colin O’Flynn and Zhizhang Chen’s, “Synchronous
    Sampling and Clock Recovery of Internal Oscillators for Side Channel Analysis
    and Fault Injection”).
  prefs: []
  type: TYPE_NORMAL
- en: True downsampling from a *digital signal processing (DSP)* perspective uses
    a filter, and any downsampling routines built in to a DSP framework for your language
    of choice would support this. However, in practice, downsampling by averaging
    consecutive points, or even only keeping every 40th sample point, tends to maintain
    exploitable leakage.
  prefs: []
  type: TYPE_NORMAL
- en: Some oscilloscopes can perform this operation for you; some PicoScope devices
    have a downsample option that’s performed in hardware. Check your oscilloscope’s
    detailed programming manual to see whether this option exists.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you can use hardware that captures synchronously to the device clock.
    In Appendix A, we describe the ChipWhisperer hardware that’s designed specifically
    to perform this task. Some oscilloscopes will have a *reference in* capability,
    which usually allows the input of only up to a 10 MHz synchronization reference.
    This capability is less useful in real life, since it means you would have to
    feed your device from a 10 MHz clock (the same as the synchronization reference
    going to the scope) in order to achieve the synchronous sampling capability.
  prefs: []
  type: TYPE_NORMAL
- en: Trace Set Analysis and Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The assumption so far has been that you record power traces and then perform
    an analysis algorithm. Realistically, you’ll include an intermediate step: preprocessing
    the traces, which means performing some action on them *before* passing them on
    to the analysis algorithm (such as CPA). All these steps aim to decrease noise,
    and/or increase the level of the leakage signal. Your measurement setup and CPA
    scripts at this point should be *fire and forget*. Trace processing is largely
    a process of trial-and-error and relies on experimentation to find what works
    best on your target. In this section, we assume you’ve made a trace set of measurements
    but haven’t yet started CPA.'
  prefs: []
  type: TYPE_NORMAL
- en: Four main preprocessing techniques you might use include *normalizing/dropping*,
    *resynchronizing*, *filtering*, and *compression* (see the section “Processing
    Techniques” later in this chapter). To determine whether your preprocessing step
    is actually helping you, we’ll first describe some analysis techniques, such as
    calculating *average* and *standard deviations*, *filtering* (yes, again), *spectrum
    analysis*, *intermediate correlation*, *known-key CPA*, and *TVLA* (listed in
    the typical order you apply them). You won’t necessarily require them all, and
    when doing analysis on a simple, leaky experimental platform that you fully control,
    you’ll probably be able to ignore most of them completely. All of these techniques
    are *standard* digital signal processing (DSP) tools, applied in a power analysis
    context. Consult DSP literature for inspiration on more advanced techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The analysis techniques become more valuable as you transition away from an
    experimental platform and move to real-life measurements made under non-ideal
    situations. You’ll use a preprocessing technique and then check its result using
    an analysis technique. If you know the key, you can always check whether your
    attack improved by using known-key CPA or TVLA. If you don’t know the key, you
    rinse and repeat until you think you’re ready to do CPA. If it works, hooray;
    if not, you’ll have to backtrack to each step to figure out whether you should
    try something else. Unfortunately, it isn’t a hard science, but the analysis techniques
    described here can give you some starting points.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section describes some standard analysis techniques that provide a measure
    of how close you are to having a good enough signal for CPA. With CPA, you performed
    measurements using different input data. Many of the visualizations in the following
    section should first be performed with the same operation and with the same data,
    and then later you can use different information as you get closer to a CPA attack.
  prefs: []
  type: TYPE_NORMAL
- en: Averages and Standard Deviations over a Data Acquisition Campaign (per Trace)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s say you represent each trace as a single point—namely, the average of
    all samples in that trace. Recall *t*[*d,j*] , where *j* = 0,1,…,*T* – 1 is the
    time index in the trace, and *d* = 0,1,…,*D* – 1 is the trace number. Your calculation
    is
  prefs: []
  type: TYPE_NORMAL
- en: '![e11001](image_fi/278748c11/e11001.png)'
  prefs: []
  type: TYPE_IMG
- en: Plotting all these points shows changes in the average of the traces over time
    and can help you find anomalies in your trace acquisition campaign; see, for instance,
    [Figure 11-13](#figure11-13).
  prefs: []
  type: TYPE_NORMAL
- en: '![f11013](image_fi/278748c11/f11013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-13: Average value of all samples per trace, showing traces 58, 437,
    and 494 to be outliers'
  prefs: []
  type: TYPE_NORMAL
- en: One type of anomaly is a drifting average—for example, due to temperature changes
    (yes, you will see the air conditioning kick in) or due to a complete outlier
    caused, perhaps, by a missed trigger. You either want to correct these traces
    or drop them altogether. (See the “Normalizing Traces” section later in this chapter
    for details on what to do with this information.) The standard deviation will
    give you a different perspective on the same acquisition campaign. We recommend
    calculating them both, as the computational overhead is insignificant.
  prefs: []
  type: TYPE_NORMAL
- en: Averages and Standard Deviations over Operations (per Sample)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The other way of calculating an average is per sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![e11002](image_fi/278748c11/e11002.png)'
  prefs: []
  type: TYPE_IMG
- en: This average can help provide a clearer view of what the operation you are capturing
    actually looks like, because it reduces amplitude noise. [Figure 11-14](#figure11-14)
    shows a raw trace in the upper graph and a sample-averaged trace in the lower
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: The sample-averaged trace makes the process steps more obvious. However, its
    usefulness decreases with increasing temporal noise. A little misalignment is
    typically not an issue for visualization, as you lose only high-frequency signals,
    but the more misaligned the traces are, the lower the highest frequencies that
    you can see will be. A little misalignment can be bad for CPA if your leakage
    is only in the higher frequencies. You can use the average to judge misalignment
    visually by looking at the higher-frequency content.
  prefs: []
  type: TYPE_NORMAL
- en: '![f11014](image_fi/278748c11/f11014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-14: Raw trace (top) and sample-averaged trace (bottom)'
  prefs: []
  type: TYPE_NORMAL
- en: Another effective method is to calculate the standard deviation per sample.
    As a rule of thumb, the lower the standard deviation, the less misalignment you
    have, as shown in [Figure 11-15](#figure11-15). In this example, the time between
    300 and 460 samples has low standard deviation, indicating little misalignment.
  prefs: []
  type: TYPE_NORMAL
- en: Perfectly aligned traces with the same operations can still show differences
    for both the average and standard deviation, which is due to differences in data
    and therefore an indication of data leakage.
  prefs: []
  type: TYPE_NORMAL
- en: '![f11015](image_fi/278748c11/f11015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-15: Standard deviation over a trace set'
  prefs: []
  type: TYPE_NORMAL
- en: Filtering for Visualization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Frequency filtering can be used as a method for generating visual representations
    of the trace data. You can aggressively cancel certain frequencies (usually high
    frequencies) to get a better view of operations being performed, without having
    to calculate an average over an entire trace set. A simple low-pass filter can
    be implemented by taking a moving average over samples (see [Figure 11-16](#figure11-16)).
    A low-pass filter is a quick way to clean up a visual representation of trace
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '![f11016](image_fi/278748c11/f11016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-16: Raw trace (top) and low-pass filtered race (bottom)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use more precise and computationally complex filters (see the
    “Frequency Filtering” section later in this chapter), but doing so may be overkill
    for visualization purposes. This visualization step is only to provide an idea
    of what’s going on below the noise; it’s not a preprocessing step, as you’ll likely
    remove the leakage signal as well. An exception is for some simple power analysis
    type of attacks: visualization of secret-dependent operations, such as square/multiply
    in RSA, can break the private key!'
  prefs: []
  type: TYPE_NORMAL
- en: Spectrum Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'What you can’t see in the time domain may be visible in the frequency domain.
    If you don’t know what the frequency domain means, think about music and sound.
    If you record music, it captures the time domain information: the air pressure
    caused by sound waves through time. But when you listen to music, you hear the
    frequency domain: different pitches of sounds through time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two visualizations are typically useful: the *average spectrum*, which is the
    “pure” frequency domain without any representation for time, and the *average
    spectrogram*, which is a combination of frequency and time information. The spectrum
    shows the magnitude of each frequency in a single trace and is a one-dimensional
    signal. It is obtained by calculating the fast Fourier transform (FFT) of a trace.
    The spectrogram shows the progression over time of all frequencies for a single
    trace. Because it adds a time dimension, it is a two-dimensional signal. It is
    calculated by doing an FFT over small chunks of a trace.'
  prefs: []
  type: TYPE_NORMAL
- en: The average spectrum and average spectrogram represent the average of these
    signals over an entire trace set. When we say we look at the average, we mean
    we first calculate the signal for each individual trace and then average them
    all per sample.
  prefs: []
  type: TYPE_NORMAL
- en: The chip spectrum shown in [Figure 11-17](#figure11-17) has a clock around 35
    MHz, which can be seen from the frequency spikes every 35 MHz. There are smaller
    spikes every 17.5 MHz, indicating that there are repeating processes that take
    two clock cycles.
  prefs: []
  type: TYPE_NORMAL
- en: '![f11017](image_fi/278748c11/f11017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-17: Average spectrum over an entire trace set'
  prefs: []
  type: TYPE_NORMAL
- en: You can perform a few interesting analyses. The frequency spikes every 35 MHz
    are caused by *harmonics* of a square wave at 35 MHz; in other words, they’re
    caused by a digital signal that switches on and off at 35 MHz. Would you suggest
    that this is the clock? Correct. The spectrum can be used to identify one or more
    clock domains on a system.
  prefs: []
  type: TYPE_NORMAL
- en: This analysis can be particularly useful if your target (crypto) operation is
    running at a different clock frequency from that of other components. It gets
    even better when you do a differential analysis of two average spectra. Let’s
    say you know that some time section of your trace contains the target operation,
    and the rest of the trace does not. You now independently calculate the average
    spectrum for each of the two sections, and subtract one from the other; that is,
    you calculate the difference between these two averages. You’ll get a *differential
    spectrum*, showing exactly what frequencies are more (or less) active during the
    target operation, which can be a great starting point for frequency filtering
    (see the “Frequency Filtering”  section later in this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Another way to find the frequency of an operation is to do known-key CPA on
    the frequency domain of traces. Known-key CPA is explained in the equally named
    section later in this chapter, but in a nutshell, because you know the key, you
    can find how close an unknown-key CPA is to recovering a key. To find the frequency
    of an operation, first transform all traces using FFT, and then perform known-key
    CPA on the transformed traces. Now you may be able to see at what frequencies
    the leakage appears. You can do the same trick with TVLA. These methods don’t
    always work, and you may need (significantly) more traces to get a signal.
  prefs: []
  type: TYPE_NORMAL
- en: The nice thing about spectrum analysis is that it is relatively independent
    of timing and thus of misalignment, as we are not looking at the phase component
    of the signal. Instead of resynchronization of traces, you can actually do CPA
    on the spectrum, though the efficiency depends on the type of leakage (see “Correlation
    Power Analysis in the Frequency Domain” by O. Schimmel et al., presented at COSADE
    2010).
  prefs: []
  type: TYPE_NORMAL
- en: The spectrogram, which does contain timing information, can also help you identify
    *interesting* events. If you know when your target operation starts, you may be
    able to see certain frequencies appear or disappear. Alternatively, if you don’t
    know when the target operation starts, it can be helpful to note a point in time
    where the frequency pattern changes. See [Figure 11-18](#figure11-18), where the
    entire spectrum clearly changes at, for example, 5ms and 57ms.
  prefs: []
  type: TYPE_NORMAL
- en: The change in frequency characteristics of the signal could be due to a cryptographic
    engine being started. Unlike with spectrum analysis, you’re looking at time-based
    information, so this spectrogram method is more sensitive to timing noise.
  prefs: []
  type: TYPE_NORMAL
- en: '![f11018](image_fi/278748c11/f11018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-18: Spectrogram over a cryptographic operation (top) and the original
    trace (bottom)'
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate Correlations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You know now that you can use CPA to determine keys by calculating a correlation
    trace for each key hypothesis. You can use the correlation trace for other purposes
    as well: to detect other data values that are being processed by the target, for
    instance, where the plaintext or ciphertext are being used in an operation. In
    this section, we assume you actually know the data values you want to correlate
    against, so no hypothesis testing is required. The most immediate and interesting
    candidates are plaintext and ciphertext consumed and produced by a cipher algorithm.
    With known data values and a leakage model, you can correlate traces and find
    out if and when those data values leak.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume you have an AES encryption for which you know the plaintext of
    each execution, and you know that it leaks the Hamming weight (HW) of 8-bit values.
    You can now correlate the HW of each plaintext byte with your measurements and
    see when the algorithm consumes them; this is also known as *input correlation*.
    Depending on your trace acquisition window, you may see many moments of correlation:
    every bus transfer, buffer copy, or other processing of the plaintext may cause
    a spike. However, one of those spikes could be the actual input to the first `AddRoundKey`,
    soon after which you’ll want to attack the Substitute operation.'
  prefs: []
  type: TYPE_NORMAL
- en: Another trick is to calculate the correlation with the ciphertext; this is also
    known as *output correlation*. Although plaintext spikes can theoretically appear
    throughout your trace, ciphertext spikes *can appear only after the crypto has
    completed.* Therefore, the first spike of ciphertext indicates that the crypto
    must have happened before that spike. A good rule of thumb is to dig for crypto
    operations between the first ciphertext spike and the plaintext spike immediately
    before that.
  prefs: []
  type: TYPE_NORMAL
- en: Observing a spike in ciphertext correlation is a good thing. It’s an indication
    that you have sufficient traces, insignificant misalignment, and a leakage model
    that captures the ciphertext. Of course, not seeing a spike means you need to
    fix any of the above, and you may not necessarily know which one. The approach
    is usually trial and error. Note that with CPA, you are attacking crypto intermediates,
    and not plaintext or ciphertext. Correlation with plaintext or ciphertext is,
    therefore, merely an indication you have your processing right; the actual crypto
    intermediates may need a slightly different alignment, a different filter, or
    more traces.
  prefs: []
  type: TYPE_NORMAL
- en: The final correlation trick you can use if you know the key to a crypto execution
    is *intermediate* *correlation*. If you know the key, ciphertext or plaintext,
    and the type of crypto implementation, you can calculate all intermediate states
    of the cipher algorithm. For instance, you can correlate with the HW of each of
    the 8-bit outputs of `MixColumns` in AES, for every round. This way, you should
    see 16 spikes for each round, slightly delayed with respect to each other. This
    idea can be extended to correlating with the HW of an entire 128-bit AES round
    state at once, which works in parallel implementations of AES.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use this trick to brute-force the leakage model—for instance, by
    not only calculating the HW but also the Hamming distance (HD) and seeing which
    gives the highest spikes. The downside is that you need to know the key, but the
    upside is that if you see spikes here, you’re getting close to a successful CPA.
    (The reason you can’t conclude you’re there yet is because CPA cares about “correct
    spikes” versus “incorrect spikes,” and we’ve analyzed only “correct spikes” here.)
  prefs: []
  type: TYPE_NORMAL
- en: Known-Key CPA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *known-key CPA* technique combines the results of the CPA and partial guessing
    entropy principles addressed earlier in this chapter to tell whether you actually
    can extract a key. You calculate a full CPA and then use PGE to analyze (for each
    subkey) the rank of the correct key candidate versus the number of traces. Once
    you see subkeys structurally drop in rank, you know you are on to something.
  prefs: []
  type: TYPE_NORMAL
- en: 'Don’t get overexcited when just a few of your keys drop to very low ranks.
    Statistics can produce strange results. They may just as well go up again with
    a growing trace set. Only if most keys drop and stay low may you be on to something.
    We’ve also observed the opposite effect: 9 out of 10 key bytes at rank 1, whereas
    the last one takes forever to find. Again, statistics can produce strange results.
    Only when all subkeys are at a low rank do you enter the territory of being able
    to brute-force your way out.'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to intermediate correlation, this method actually tells you whether
    you can extract a key. However, the computational complexity is significantly
    larger; you need to calculate 256 correlation values for each key byte, instead
    of one correlation value in the case of intermediate correlation. As with intermediate
    correlation, not seeing spikes can be caused by insufficient traces, significant
    misalignment, or a bad leakage model. It may take trial-and-error to determine
    this.
  prefs: []
  type: TYPE_NORMAL
- en: Test Vector Leakage Assessment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Welch’s t-test* is a statistical test used to determine whether two sample
    sets have equal mean values. We’ll use this test to answer a simple question:
    if you have grouped power traces into two sets, are those sets statistically distinguishable?
    That is, if we have performed 100 encryption operations with key A and 100 encryption
    operations with key B, is there a detectable difference in the power traces? If
    the average power consumption of the device at a certain time in the trace differs
    for key A and key B, it might suggest that the device is leaking information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply this test to a certain point in time for each of two sets of power
    traces. The result is the probability that the two sets of power traces have equal
    means at that point in time, regardless of the standard deviation. We’ll intentionally
    create two trace sets, and in each set, the target processes different values.
    If these values give rise to changes in the average power level, we then know
    we have leakage. See the “Trace Set Analysis and Processing” earlier in this chapter
    for notes on acquiring multiple sets and to learn more about choosing the input
    data. We cannot emphasize this enough: if you generated two sets by running 100
    traces with key A and then sequentially after that 100 with key B, your traces
    are useless. The statistical test is almost certain to find a difference between
    them, since physical changes (such as temperature) are quite likely to occur between
    the times when each set was captured. Before acquisition of each trace, randomly
    decide on the PC (not the target) whether it will be with key A or key B. Ask
    us how we know.'
  prefs: []
  type: TYPE_NORMAL
- en: We can plot the value of Welch’s *t* over time and observe spikes where leakage
    is detected, similar to a correlation trace. The value of Welch’s *t* is calculated
    by
  prefs: []
  type: TYPE_NORMAL
- en: '![e11003](image_fi/278748c11/e11003.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![e11004](image_fi/278748c11/e11004.png) is the average sample value at
    time *j* for trace set *A*, *var()* is the sample variance, and *D*^(*A*) is the
    number of traces in trace set *A*. The higher *w*[*j*] is, the more likely it
    is that trace set *A* and trace set *B* actually are generated by a process with
    a different mean at time *j*. In our experience, for trace sets of at least a
    few hundred traces, absolute values for *w*[*j*] of say 10 and higher indicate
    that there most likely is leakage, and a CPA attack may succeed if *w*[*j*] is
    80 or higher. In other literature you’ll often see the value of 4.5, which in
    our experience has led to some false positives.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll give you a few sample sets for AES you can test so you get the idea of
    what we’re after here:'
  prefs: []
  type: TYPE_NORMAL
- en: Create one set with random input data and one set with constant input data.
    The idea is that if the target doesn’t leak, the power measurements inside the
    crypto algorithm should be statistically indistinguishable, even if the characteristics
    of the processed data clearly vary. Note that power measurements of the transporting
    of input data to the crypto engine will probably leak, which this test will detect.
    Obviously, differences in input data are not real leakage and cannot be exploited,
    so watch out for false *t* peaks caused by this “input leakage.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create one set where an intermediate data bit ***X*** has the value 0 and another
    set where ***X*** has the value 1. This example is of most interest when testing
    a bit in a middle round of AES, such as any AES state bit after the `SubBytes`
    or `MixColumns` operations in round 5\. With this test, there will be no false
    positives like “input leakage”; bits in round 5 of AES have effectively no correlation
    with the input or output bits of AES. If you want to test Hamming distance leakage,
    you can also calculate bit *X* as the XOR between, for instance, the input and
    output of an entire AES round. You should perform this test with a known key,
    but you can do it with fully random inputs. Since you don’t know which bit *X*
    actually leaks, you can calculate the statistics for all imaginable intermediate
    bits—for instance, for the 3 × 128 bits of state after `AddRoundKey`, `SubBytes`,
    and `MixColumns` (`ShiftRows` doesn’t flip bits) in round 5.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create one set where intermediate ***Y*** is ***A*** and another set where ***Y***
    is not ***A***. This is an extension of the previous idea. You can, for instance,
    test whether one byte of `SubBytes` output has a bias in the power measurements
    when its value is, for example, 0x80\. Again, you can calculate the t-test for
    any intermediate *Y* and value *A*, so you can run 16 × 256 tests for the `Substitute`
    output state in round 5.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create one set where the entire 128-bit round ***R*** state of AES has exactly
    ***N*** bits set to 1 and then create another random set. This one is clever.
    Let’s say we pick round *R* = 5, and we generate a 128-bit state with, say, *N*
    = 16 randomly selected bits set to 1\. This is a significant bias: under normal
    circumstances, on average, 64 bits are set to 1, and it’s highly unlikely for
    the biased state to appear. However, using the known key, we can calculate what
    plaintext would have generated that biased state under that key. Due to the properties
    of crypto, the bytes of these plaintexts will appear uniformly random. The same
    holds for the ciphertext. In fact, when calculating *t*, the only bias you may
    theoretically detect is actually in round *R*, because there shouldn’t be any
    other bias (except for some minor biasing of rounds *R* – 1 and *R* + 1). Therefore,
    you won’t get any *t* spikes caused by transfer of plaintext or ciphertext. Because
    you are biasing an entire round state, you may detect leakage with fewer traces
    than with the previous methods; therefore, it’s a great first way of detecting
    leakage before any CPA method can detect it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you can see, you can use the t-test to detect various types of leakage. Note
    that we have not specified an explicit power model, which makes the t-test a more
    generic leakage detector than CPA and friends. The biasing of an inner round especially
    amplifies leakage. The t-test is a great tool to determine the timing of leaks,
    the location of EM leaks, or for improving filters by tuning them for the highest
    value of *t*. One cool trick that can help if you have a lot of misalignment is
    first to do an FFT and then calculate *t* in the frequency domain to find out
    at what frequency your leakage is.
  prefs: []
  type: TYPE_NORMAL
- en: The downsides to t-tests are that you may need the key and that these tests
    don’t actually do key extraction. In other words, you’ll still need to use CPA
    and figure out a power model, and you may not succeed. Just like CPA, not seeing
    spikes means you may need to improve your trace processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because you aren’t actually recovering the key, it’s also easy for the t-test
    to produce false positives. These can occur because there is a statistical difference
    between the groups of traces unrelated to cryptographic leakage (for instance,
    due to not properly randomizing your acquisition campaign). In addition, the t-test
    will detect leakage related to the loading or unloading of data from the cryptographic
    core, which may be useless to attack. The t-test simply tells you that two groups
    have the same or different means, and *you* must correctly understand what that
    implies. It is, however, a really handy tool for tweaking your processing techniques:
    if the *t* value goes up, you’re heading in the right direction.'
  prefs: []
  type: TYPE_NORMAL
- en: Processing Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the “Analysis Techniques” section earlier in this chapter, we presented
    some standard methods that provide a measure of how close you are to having a
    good-enough signal for CPA. In this section, we’ll describe some techniques for
    processing trace sets. Some practical advice: check your results after each step
    and twice on Sunday. Otherwise, it’s too easy to make a misstep and lose the leakage
    signal forever. It’s more time-efficient to detect issues earlier rather than
    later when you need to debug your entire processing chain.'
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing Traces
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once you have acquired a trace set, it’s always helpful to calculate the average
    and standard deviation per trace, as explained in the “Averages and Standard Deviations
    over Operations (per Sample)” section earlier in this chapter. You’ll see two
    things: outliers that in only one trace will jump outside the “normal” range and
    a slow drift of the normal range due to environmental conditions as well as possible
    errors/bugs in your acquisition. To improve your trace set’s quality, you’ll want
    to drop traces that are outliers by only allowing a certain range of average/standard
    deviation values. After that, you can correct for drift by *normalizing* traces.
    A typical normalization strategy is to subtract the average per trace and divide
    all sample values by the standard deviation for that trace. The result is that
    each trace has an average sample value of 0 and a standard deviation of 1.'
  prefs: []
  type: TYPE_NORMAL
- en: Frequency Filtering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When capturing data with the oscilloscope, we can use analog filters on the
    input to the scope. These filters can also be computed digitally: a variety of
    environments provides libraries that easily allow you to pass traces through filters.
    Examples include scipy.signal for Python and SPUC for C++. Digital filters form
    the backbone of most digital signal processing work, so most programming languages
    have excellent filtering libraries.'
  prefs: []
  type: TYPE_NORMAL
- en: When doing *frequency* *filtering*, your aim is to take advantage of the fact
    that the leakage signal you are interested in, or some specific source of noise,
    may be present in a distinct part of the frequency spectrum. (The “Spectrum Analysis”
    section earlier in this chapter contains a description of how to analyze the spectrum
    for noise or signal.)
  prefs: []
  type: TYPE_NORMAL
- en: By either passing the signal or blocking the noise, you can improve the CPA’s
    effectiveness. You probably want to apply the same filter to the harmonics of
    the base signal; for instance, if your target clock is 4 MHz, it will probably
    help to keep 3.9–4.1, 7.9–8.1, 11.9–12.1 MHz, and so on. If your system has a
    switching regulator adding noise to your measurements, you might need a *high-pass*
    or *band-pass* filter to eliminate that noise. Often, *low-pass* filtering can
    help alleviate high-frequency noise present in these systems, but in some cases,
    your leakage signal is entirely in the high-frequency components, so high-pass
    filtering would rule out any chance of success! In other words, it requires some
    trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: For DPA, you most likely will be using (multi-)notch filters to pass or block
    base frequencies and their harmonics. A *finite impulse response (FIR)* or *infinite
    impulse response (IIR)* filter design for notch filtering can be complicated;
    you can always revert to the more computationally complex way of doing an FFT,
    and then block/pass arbitrary parts of the spectrum by setting the amplitude to
    0 and doing an inverse FFT.
  prefs: []
  type: TYPE_NORMAL
- en: Resynchronization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ideally, we would know when the encryption operation occurs, and we would trigger
    our oscilloscope to record that exact moment in time. Unfortunately, we might
    not have such a precise trigger but instead are triggering our oscilloscope based
    on a message being sent to the microcontroller. The amount of time that passes
    between the microcontroller receiving the message and performing the encryption
    isn’t constant, since it might not immediately act on the message.
  prefs: []
  type: TYPE_NORMAL
- en: This discrepancy means we need to resynchronize multiple traces. [Figure 11-19](#figure11-19)
    shows three traces before resynchronization (*misaligned traces*), and the same
    three traces after resynchronization (*aligned traces*).
  prefs: []
  type: TYPE_NORMAL
- en: The three traces on the top are not synchronized. By doing a *sum of absolute
    difference (SAD)* process on the three traces, the synchronized output shows a
    clear trace on the bottom.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the SAD method, you take a trace that forms your *reference trace*.
    This is the trace to which you’ll then align all others. From this reference trace,
    you select a group of points, usually some feature that appears in all the traces.
    Finally, you try to shift each trace such that the absolute difference between
    the two traces is minimized. This chapter comes with a small Jupyter notebook
    ([https://nostarch.com/hardwarehacking/](https://nostarch.com/hardwarehacking/))
    that implements the SAD and produces [Figure 11-19](#figure11-19).
  prefs: []
  type: TYPE_NORMAL
- en: '![f11019](image_fi/278748c11/f11019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-19: Synchronizing traces using the sum of absolute difference (SAD)
    method'
  prefs: []
  type: TYPE_NORMAL
- en: An alternative is to use the *circular convolution theorem*.The convolution
    between two signals is basically the point-wise multiplication of two signals
    at different shifts *n*. The value of *n* at which this multiplication has the
    lowest value is the “best fitting” shift for those signals. The naïve calculation
    is very expensive. Luckily, you can obtain a convolution by performing an FFT
    on both signals, multiplying the signals point-wise, and then doing an inverse
    FFT. This process will give you the result of the convolution between two signals
    for each shift value *n*, after which you just need to scan for the minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Several other simple resynchronization modules can be found in the ChipWhisperer
    software. Resynchronizing can become more advanced than simply applying a static
    shift. You might need to warp the traces in time or remove sections of a trace
    where an interrupt occurred in only a handful of traces. We don’t cover these
    details here, but see Jasper G. J. van Woudenberg, Marc F. Witteman, and Bram
    Bakker’s “Improving Differential Power Analysis by Elastic Alignment” for more
    details on *elastic alignment*.
  prefs: []
  type: TYPE_NORMAL
- en: Trace Compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Capturing long traces can take up a lot of disk and memory space. Using a high-speed
    oscilloscope sampling at GS/s or more, you’ll quickly discover that the size of
    your traces grows annoyingly large. Even worse, the analysis becomes very slow,
    since it is performed on every sample in succession.
  prefs: []
  type: TYPE_NORMAL
- en: If the real objective is to find some leakage information about each clock cycle,
    you might guess that you don’t need every single sample of every clock cycle.
    Rather, it’s often sufficient to keep one sample from each clock cycle. This is
    called *trace compression*, since you greatly reduce the number of sample points.
  prefs: []
  type: TYPE_NORMAL
- en: As previously mentioned in this chapter’s “Sampling Rate” section, you can perform
    trace compression by simply downsampling, but doing so won’t yield as much a savings
    as what true trace compression does.
  prefs: []
  type: TYPE_NORMAL
- en: True trace compression uses a function to determine the value by which to represent
    each clock cycle. It could be the minimum, maximum, or average value over an entire
    clock cycle or over only a part of the entire clock cycle. If your target device
    has a stable crystal oscillator, you can perform this trace compression by taking
    samples at a certain offset from the trigger, since the device and sample clock
    should both be stable. For non-stable clocks, you’ll need to do some clock recovery—for
    instance, by finding peaks indicating clock start. Once you have the clock, you
    may find that only the first *x* percent of a clock cycle contains most of the
    leakage, so you can disregard the rest.
  prefs: []
  type: TYPE_NORMAL
- en: When compressing EM probe measurements, take into account that the EM signal
    is the derivative of the power signal. So, for a single power spike, there will
    be a positive EM spike followed by a negative one. You don’t want to average the
    positive and negative parts of the captured waves; by their nature they cancel
    out! In this case, you just want to take the sum of the absolute sample values
    for that clock.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning Using Convolutional Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Staying relevant requires that a field like side-channel analysis must keep
    up with the machine learning (ML) trends. There are actually two seemingly fruitful
    ways to frame the side-channel problem in terms of machine learning: the first
    being side-channel analysis as a sequence of steps by an (intelligent) agent,
    and the second way being side-channel analysis as a classification problem. This
    research topic is still young at the time of writing, but it’s an important one.
    Side-channel analysis is becoming increasingly important, and there aren’t enough
    of us to keep up with market demands. Any automation such as machine learning
    is crucial.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the *agent* frame: agents observe their world, perform an action,
    and are punished/rewarded in relation to how their actions change the world. We
    could train an agent to decide what steps to take next, such as deciding whether
    to use alignment, filtering, or resampling based on how high a *t* spike is. The
    future will tell whether this is brilliant or foolish, as this topic is currently
    unstudied.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now consider the *classification problem*. Classification is the science of
    taking in an object and assigning it to a class. For instance, modern-day deep
    learning classifiers can take in an arbitrary image and, with high accuracy, detect
    whether a cat or a dog is in the image. The neural networks used to perform the
    classification are trained by presenting millions of pictures that are already
    labeled with “cat” or “dog.” Training means tuning the network parameters such
    that they detect features in the images representative of either cats or dogs.
    The interesting part about neural networks is that the tuning happens purely by
    observation; no expert needs to describe the features needed to detect “cat” or
    “dog.” (At the time of writing, experts are still needed to design the structure
    of the network and how the network is trained). Side-channel analysis is essentially
    a classification problem: we try to classify intermediate values from traces we
    are presented with. Knowing the intermediate values, we can calculate the key.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-20](#figure11-20) illustrates the process where a neural network
    is being trained to perform side-channel analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: '![f11020](image_fi/278748c11/f11020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-20: Training a neural network for side-channel analysis'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve replaced our lovely cats and dogs with a cute set of traces, which we
    individually label with the Hamming weight of the intermediate value we are targeting.
    For AES, this label could be the Hamming weight of a specific S-box output. This
    labeled set of traces will be the training set for the neural network, which will
    then, hopefully, learn how to determine the Hamming weight from a given trace.
    The outcome is a trained model that can be used for assigning probabilities over
    the Hamming weights for a new trace.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-21](#figure11-21) shows how a network’s classification can be used
    to obtain confidence values for intermediates (and thus keys).'
  prefs: []
  type: TYPE_NORMAL
- en: This diagram shows the neural network processing a single trace. The trace goes
    through the neural network, which results in a probability distribution over the
    Hamming weights. In this example, the most likely Hamming weight is 6 with a probability
    of 0.65.
  prefs: []
  type: TYPE_NORMAL
- en: We can train a neural network by presenting it with traces and known intermediate
    values, as shown in [Figure 11-20](#figure11-20), and thereafter let the network
    classify a trace with an unknown intermediate value, as shown in [Figure 11-21](#figure11-21),
    which in effect is an SPA method. Such an SPA analysis can be useful for ECC or
    RSA, where we need to classify chunks of traces that represent the calculation
    over one or a few key bits.
  prefs: []
  type: TYPE_NORMAL
- en: '![f11021](image_fi/278748c11/f11021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-21: Using the network’s classification to help with finding keys'
  prefs: []
  type: TYPE_NORMAL
- en: 'The DPA approach is to use the probability distribution (which is output of
    the neural network) for intermediate values, transform this probability distribution
    into confidence values over key bytes, and update these confidences for each observed
    trace. Here is where we diverge from usual neural network classification: we don’t
    care about classifying each trace perfectly, as long as on average we bias the
    confidence value for the relevant key byte. In other words, we don’t intend to
    identify a cat or dog perfectly in each picture, but we have a gazillion extremely
    noisy pictures of one animal, and we try to make out whether it is a cat.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Properly trained networks, specifically convolutional neural networks, detect
    objects irrespective of orientation, scale, irrelevant color changes, and some
    level of noise. So, hypothetically, these networks would be able to reduce human
    effort by analyzing traces that need filtering and alignment. In the 2018 Black
    Hat talk by Jasper, “Lowering the Bar: Deep Learning for Side Channel Analysis”
    (available on YouTube), he shows the work of his co-authors Guilherme Perin and
    Baris Ege.He demonstrates that neural networks are a viable approach for analyzing
    traces of asymmetric crypto and software implementations of symmetric ciphers
    where there is misalignment and some noise. It’s still an open question how well
    this extends to hardware implementation with harder countermeasures. One interesting
    result from the work was that it broke a second-order masked implementation by
    detecting first-order leakage with the network.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this work is to eliminate the need for a human analyst to interpret
    traces. We have not yet reached that goal, though we arguably made it easier by
    shifting the effort to network design, rather than the multidomain complexities
    of side-channel analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the introduction to this chapter, we mentioned it would be about the *art*
    of power analysis, as opposed to the *science* of power analysis. The science
    is the easy part—just trying to understand what the tools do. The art is in applying
    them at the right time in the right way or even designing your own tools. Achieving
    expertise in this art requires experience, which you’ll gain only through experimentation.
    For every skill level, there are interesting targets to play with. In our lab,
    we analyze multi-GHz SoCs, but that requires a team of people who’ve done this
    type of analysis professionally for a few years, and it may take a few months
    to start seeing any leakage. At the other end of the spectrum, in only a few hours,
    we are able to teach how to break the key on a simple microcontroller to people
    without experience. Whatever you play with, try to match it with your experience
    level.
  prefs: []
  type: TYPE_NORMAL
- en: Another great exercise is to build your own countermeasures. Grab a target you’re
    comfortable breaking that allows you to load your own code. Try to think what
    would really make it difficult for you as an attacker to break the implementation;
    one of the tricks to employ is to take one of the steps in your analysis and break
    the assumptions that step makes. A simple one is to randomize the timing of the
    algorithm, which breaks DPA and forces you to do alignment of the traces. This
    way, you improve your system’s security, you improve your attacker skills, and
    you give yourself something to do on the next weekend.
  prefs: []
  type: TYPE_NORMAL
