- en: '23'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '23'
- en: Creative Applications
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 创意应用
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: 'We’ve reached the end of the book. Before we go, let’s relax and have some
    fun. In this chapter we look at some creative ways to use neural networks to create
    art. We explore two image-based applications: *deep dreaming,* which turns images
    into wild, psychedelic art, and *neural style transfer,* which allows us to transform
    photographs into what appear to be paintings in the styles of different artists.
    At the very end, we take a quick dip into *text generation* and use deep learning
    to generate even more of this book.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经到了本书的最后。在我们结束之前，让我们放松一下，玩得开心一点。在这一章中，我们将探讨一些使用神经网络创造艺术的创意方式。我们将介绍两个基于图像的应用：*深度梦境*，它将图像转化为狂野的迷幻艺术，和*神经风格迁移*，它允许我们将照片转化为看起来像是不同艺术家风格的画作。在最后，我们简要介绍一下*文本生成*，并使用深度学习生成更多本书内容。
- en: Deep Dreaming
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度梦境
- en: In deep dreaming, we use some ideas that were invented to help us visualize
    filters in convolutional networks, but we use them to make art. The result is
    that we modify images to excite different filters, causing those images to explode
    in psychedelic patterns.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度梦境中，我们使用了一些发明出来的概念，这些概念最初是为了帮助我们可视化卷积网络中的滤波器，但我们用它们来创造艺术。最终的结果是，我们修改图像以激活不同的滤波器，使得图像爆发出迷幻的图案。
- en: Stimulating Filters
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 刺激滤波器
- en: In Chapter 17, we made images, or visualizations, of the filters in a convolutional
    neural network. Both deep dreaming and style transfer build on that visualization
    technique, so let’s look at it a little more closely. We can make our discussion
    specific by again using VGG16 as we did in Chapter 17 (Simonyan and Zisserman
    2020), though we could substitute just about any CNN image classifier. Our only
    interest here is in the convolution stages, so although we will use the whole
    network as described in Chapter 17, the drawings in this chapter show just the
    convolution and pooling layers, as shown in [Figure 23-1](#figure23-1).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在第17章中，我们创建了卷积神经网络中滤波器的图像或可视化。深度梦境和风格迁移都建立在这种可视化技术的基础上，因此让我们更仔细地看一下。我们可以通过再次使用VGG16来具体说明，就像在第17章中那样（Simonyan和Zisserman
    2020），尽管我们也可以替换为任何卷积神经网络图像分类器。我们唯一感兴趣的是卷积阶段，因此尽管我们会使用第17章中描述的整个网络，但本章中的图示只显示卷积层和池化层，如[图
    23-1](#figure23-1)所示。
- en: '![f23001](Images/f23001.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![f23001](Images/f23001.png)'
- en: 'Figure 23-1: A simplified diagram of VGG16, showing just the convolution and
    pooling layers'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23-1：VGG16的简化示意图，仅显示卷积层和池化层
- en: We’re leaving out the last few stages of VGG16 because their job is to help
    the network predict the proper class of the output. In this application, we don’t
    care about the network’s output. Our only interest here is running an image through
    the network so that the filters in the convolution layers will evaluate their
    inputs. Our goal is to modify a starting image so it excites some chosen layers
    as much as possible. For example, if a few pixels are darker in the middle, that
    may cause the filter that looks for eyes to respond a little bit. Our goal is
    to modify those pixels so that they excite that filter more and more, which means
    that they look more and more like eyes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们省略了VGG16的最后几个阶段，因为它们的作用是帮助网络预测输出的正确类别。在这个应用中，我们不关心网络的输出。我们唯一感兴趣的是将一张图片输入网络，以便卷积层中的滤波器对其输入进行评估。我们的目标是修改一张初始图片，使其尽可能激活某些选择的层。例如，如果中间的几个像素变得更暗，那可能会让寻找眼睛的滤波器稍微有些反应。我们的目标是修改这些像素，使它们激活那个滤波器越来越强烈，也就是说，它们看起来越来越像眼睛。
- en: We don’t need any new tools to do this. All we have to do is pick which filter
    outputs we want to maximize. We can pick just one filter, or several filters from
    different parts of the network. Our choice of which filters to use is entirely
    personal and artistic. Typically, we hunt around, trying out different filters,
    until we see our input images changing in a way that we like.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要任何新工具来完成这个。我们只需选择我们想要最大化的滤波器输出。我们可以选择一个滤波器，或者从网络的不同部分选择多个滤波器。我们选择使用哪些滤波器完全是个人的和艺术性的。通常，我们会四处尝试，测试不同的滤波器，直到看到我们的输入图像以我们喜欢的方式发生变化。
- en: Let’s see the steps. Suppose we pick one filter on each of three different layers,
    as in [Figure 23-2](#figure23-2). We start things off by providing the network
    with an image, which it processes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下步骤。假设我们从三个不同的层中各选一个滤波器，如[图 23-2](#figure23-2)所示。我们首先向网络提供一张图片，网络会对其进行处理。
- en: We take the feature map from the first filter we’ve chosen, add up all of its
    values, and determine how much influence this sum will have by multiplying it
    by a weight that we choose. Though we’re using the word *weight,* this isn’t a
    weight inside the network. It’s just a value we use to control the impact of each
    filter in the deep dream process. We sum up and weight the other filters we’ve
    chosen. Now we add up those results. This gives us a single number, telling us
    how strongly our chosen filters are responding to the input image, weighted by
    how much influence we want to give to each layer’s filters. We call this number
    the *multifilter loss,* or the *multilayer loss.*
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![f23002](Images/f23002.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23-2: The deep dream algorithm uses a loss built from multiple layers'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Now comes the tricky part: the multifilter loss becomes the network’s “error.”
    In previous chapters, we used the error to drive backprop, which computed gradients
    for all of the network’s weights, starting at the final layer and working our
    way backward to the first. Then we used those gradients to modify the network’s
    weights to minimize the error. But that’s not what we do here. Instead, we want
    the error (the filter responses) to be as big as we can make them. And we don’t
    want to do this by changing the network, since we’re not training it. We’re going
    to *freeze* the network, so its weights can’t change. Instead, we’re going to
    modify the colors of the pixels themselves.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: So, starting with this error, we use backprop as usual to find the gradients
    on all the weights in the network, but when we reach the first hidden layer, we
    take one more step backward to the input layer, which holds the pixels themselves.
    Then we use backprop as usual to find the gradients for the pixels. After all,
    changing the input pixels causes the values computed by the network to change,
    and thus causes a change to our error. Just as we can use backprop to learn how
    to change the network’s weights to reduce the error in a typical training setup,
    we can use the same backprop algorithm to find out how to change the pixel values
    to increase this error.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Now, as usual, we apply the optimization step. Since we’re not training and
    the network is frozen, we don’t touch the network weights. But we do use the gradients
    on the pixels to modify their color values so that they *maximize* the error,
    or more strongly stimulate our selected filters.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The result is that the pixel colors change just a little, in such a way that
    filters respond even more, creating a bigger error, which we use to find new gradients
    on the pixels, causing them to excite the filters even more, and around it goes,
    with the picture changing more and more each time we repeat the loop.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Since this is an artistic process, we usually watch the output after every update
    (or every few updates), and stop it when we like what we’re seeing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Running Deep Dreaming
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s put this algorithm to work, using the frog in [Figure 23-3](#figure23-3)
    as our starting point.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们让这个算法发挥作用，以[图 23-3](#figure23-3)中的青蛙为起点。
- en: '![f23003](Images/f23003.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![f23003](Images/f23003.png)'
- en: 'Figure 23-3: A calm and thoughtful frog'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23-3：一只平静且深思的青蛙
- en: '[Figure 23-4](#figure23-4) shows some “dreams” from our frog image, using some
    filters (and weights on them) that we chose by trial and error.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 23-4](#figure23-4) 显示了使用我们通过反复试验选择的一些滤镜（及其权重）从青蛙图像中生成的一些“梦想”。'
- en: '![f23004](Images/f23004.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![f23004](Images/f23004.png)'
- en: 'Figure 23-4: Some deep dream results from the starting frog image (in the upper-left
    corner)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23-4：从起始青蛙图像（左上角）开始的一些深度梦想结果
- en: We’re seeing lots of eyes in [Figure 23-4](#figure23-4) because some of our
    selected filters responded to eyes. If we had selected filters that responded
    to, say, horses and shoes, then we would expect to see lots of horses and shoes
    in our images.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 23-4](#figure23-4)中，我们看到很多眼睛，因为我们选择的部分滤镜响应了眼睛。如果我们选择了响应马和鞋子的滤镜，那么我们就会期望看到图像中出现大量的马和鞋子。
- en: '[Figure 23-5](#figure23-5) shows the results starting from an image of a dog.
    The changes to the image are mostly of a finer texture because the dog image is
    about 1,000 pixels on each side, more than four times the size of the images that
    the network trained on. The image in the lower-right used a version of the dog
    image that was scaled to the same size as the network’s training data, 224 by
    224\.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 23-5](#figure23-5) 显示了从一张狗的图像开始的结果。图像的变化主要体现在更细腻的纹理，因为这张狗的图像每边大约有 1000 像素，是网络训练时使用的图像的四倍多。右下角的图像使用了缩放到与网络训练数据相同大小（224
    x 224 像素）的狗图像版本。'
- en: '![f23005](Images/f23005.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![f23005](Images/f23005.png)'
- en: 'Figure 23-5: Deep dreaming about a dog'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23-5：关于狗的深度梦想
- en: The original name for this technique was *inceptionism* (Mordvintsev, Olah,
    and Tyka 2015) in honor of the movie *Inception,* but it has come to be more frequently
    known as *deep dreaming*. The name is a poetic suggestion that the network is
    “dreaming” about the original image, and the image we get back shows us where
    the network’s dream went. Deep dreaming has become popular not only because of
    the wild images it creates, but because it’s not hard to implement using modern
    deep learning libraries (Bonaccorso 2020).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术的原始名称是 *启示主义*（Mordvintsev, Olah, 和 Tyka 2015），以电影 *盗梦空间*（Inception）为灵感，但它现在更常被称为
    *深度梦想*。这个名字是一个富有诗意的暗示，表明网络正在“梦想”原始图像，我们得到的图像展示了网络的梦想去了哪里。深度梦想之所以流行，不仅因为它创造了奇异的图像，还因为使用现代深度学习库实现它并不困难（Bonaccorso
    2020）。
- en: Many variations on this basic algorithm have been explored (Tyka 2015), but
    they only scratch the surface. We can imagine schemes to automatically determine
    the weights on the layers or even apply weights to the individual filters on each
    layer. We can “mask” the activation maps before we add them up so that some areas
    (like the background) are ignored, or we can mask the updates to the pixels so
    that some pixels in the original image are not changed at all in response to one
    set of layer outputs, but are instead allowed to change a lot in response to some
    other set of layer outputs. We can even apply different combinations of layers
    and weights to different regions of the input image. The deep dreaming approach
    to making art has lots of room left for new discoveries.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基本算法的许多变种已被探索过（Tyka 2015），但它们只是略微触及了表面。我们可以设想一些方案，自动确定层上的权重，甚至对每层的单个滤镜应用权重。我们可以在叠加激活图之前对其进行“掩膜”，以忽略某些区域（如背景），或者我们可以对像素更新进行掩膜，这样原始图像中的某些像素在响应某一层的输出时完全不改变，而是在响应另一组层的输出时发生较大变化。我们甚至可以对输入图像的不同区域应用不同的层和权重组合。深度梦想艺术创作的方法仍有很大的探索空间。
- en: There’s no “right” or “best” way to do deep dreaming. It’s a creative exercise
    in which we follow our aesthetics, hunches, or wild guesses to hunt for images
    that appeal to us. It can be hard to predict what’s going to come out from any
    particular combination of network, layers, and weights, so the process rewards
    patience and a lot of experimenting.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 没有“正确”或“最佳”的深度梦想方法。这是一种创造性的练习，我们根据自己的审美、直觉或天马行空的猜测来寻找让我们感兴趣的图像。很难预测任何特定的网络、层和权重组合会产生什么效果，因此这个过程奖励耐心和大量的实验。
- en: Neural Style Transfer
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经风格迁移
- en: 'We can use a variation on the deep dreaming technique to do something remarkable:
    transfer one artist’s style onto another image. This process is called *neural
    style transfer*.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用深度梦境技术的一个变种来做一些了不起的事情：将一种艺术家的风格转移到另一张图像上。这个过程被称为*神经风格迁移*。
- en: Representing Style
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 表示风格
- en: Cultures often celebrate the idiosyncratic visual style of artists. Let’s focus
    on paintings. What characterizes the style of a painting? That’s a big question,
    because “style” can include someone’s world view, which influences choices as
    diverse as their subject matter, composition, materials, and tools. Let’s focus
    strictly on visual appearance. Even narrowed down this way, it’s hard to precisely
    identify what “style” means for a painting, but we might say that it refers to
    how colors and shapes are used to create forms, and the types and distributions
    of those forms across the canvas (Art Story Foundation 2020; Wikipedia 2020).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 各种文化常常庆祝艺术家独特的视觉风格。我们将重点讨论画作。那么，是什么特征定义了一幅画的风格呢？这是一个大问题，因为“风格”可以包括一个人的世界观，这种世界观会影响他们在主题、构图、材料和工具等方面的选择。让我们仅专注于视觉外观。即使这样限制范围，精确地定义画作的“风格”仍然很困难，但我们或许可以说，它指的是如何使用颜色和形状来创造形式，以及这些形式在画布上分布和类型的方式（Art
    Story Foundation 2020；Wikipedia 2020）。
- en: Rather than try to refine this description, let’s see if we can find something
    that seems like it’s in the ballpark, while also being something we can formalize
    in terms of the layers and filters of a deep convolutional network.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与其试图完善这个描述，不如看看我们是否能找到一个大致相符的东西，同时又能在深度卷积网络的层和过滤器的框架内进行形式化。
- en: Our goal in this section is to take a picture we want to modify, called the
    *base image*, and a second picture whose style we want to match, called the *style
    reference*. For example, our frog could be our base image, and any painting could
    be the style reference. We want to use these to create a new image, called the
    *generated image*, which has the content of the base image expressed in the style
    of the style reference.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目标是获取我们想要修改的图片，称为*基础图像*，以及我们希望匹配其风格的第二张图片，称为*风格参考图像*。例如，我们的青蛙可以是基础图像，任何一幅画作都可以是风格参考图像。我们希望使用这两者来创建一张新图像，称为*生成图像*，它将基础图像的内容以风格参考图像的风格进行表达。
- en: To get started, we will make an assertion that comes out of nowhere. We say
    that we can characterize the style of an image (such as a painting) by looking
    at the layer activations it produces and finding pairs of layers that activate
    in roughly the same way. This idea comes from a seminal paper published in 2015
    (Gatys, Ecker, and Bethge 2015). Without getting into the details, the process
    begins by running the style reference through a deep convolutional network. As
    with deep dreaming, we ignore its output, and instead focus on just the convolution
    filters.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始，我们将做出一个看似无根据的断言。我们说，图像（例如画作）的风格可以通过观察它产生的层激活来进行表征，并找到大致以相同方式激活的层对。这个想法来源于2015年发表的一篇开创性论文（Gatys,
    Ecker, 和 Bethge 2015）。不深入细节，过程从将风格参考图像输入深度卷积网络开始。与深度梦境一样，我们忽略它的输出，而专注于卷积过滤器。
- en: All of the activation maps in a given layer have the same size, so we can easily
    compare them to one another. Let’s start with the first activation map in a layer
    (that is, the first filter’s output). We can compare that to the activation map
    produced by the second filter and produce a score for the pair. If the two maps
    are very similar (that is, the filters are firing in the same places), we assign
    the pair a high score, and if the maps are very different, we give that pair a
    low score. We then compare the first map to the third map and compute their score,
    then compare the first and the fourth map, compute their score, and so on. Then
    we can start with the second map and compare it to every other map in the layer.
    We can organize the results in a grid, with as many cells on each side as there
    are filters in the layer. The value in each cell of the grid tells us the score
    for that pair of layers. This grid is called a *Gram matrix.* Let’s make one such
    Gram matrix for every layer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can restate our notion of style more formally: the style of an image
    is represented by the Gram matrices for all the layers. That is, each style produces
    its own particular form of Gram matrices.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see if this claim is true. [Figure 23-6](#figure23-6) shows a famous self-portrait
    by Pablo Picasso from 1907\. There’s a ton of style here, such as big blocks of
    color and thick dark lines. Let’s run this through VGG16 and save the Gram matrix
    at each layer. We’ll call those the *style matrices*, and save them as the representation
    of the style for this image.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![f23006](Images/f23006.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23-6: A 1907 self-portrait by Pablo Picasso'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: If the Gram matrices represent style, then we can use them to modify a starting
    image of random noise. We run the noisy input image through the network and compute
    its Gram matrices, which we call the *image matrices.* If the style matrices really
    do somehow represent the style of the Picasso image, then if we can change the
    colors of the pixels in the noisy image so that eventually the image matrices
    come close to the style matrices, the noisy image should take on the style of
    the painting.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Let’s do just that. We’ll run the noise through the network, compute the image
    matrix at each layer, and compare that to the style matrix we saved for that layer.
    We’ll add up the differences between these two matrices so the more different
    they are, the bigger the result. Then we’ll add together these differences for
    all the layers, and this is the error for our network. As with deep dreaming,
    we use this error to compute the gradients for the entire network, including the
    pixels at the start, but we only modify the colors of the pixels. Unlike deep
    dreaming, our goal now is to minimize the error, and thereby change the pixels
    so that their colors produce Gram matrices that are like those of the style reference.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 23-7](#figure23-7) shows the result of this process. For this visualization,
    we computed each layer’s error as the sum of the differences of all the matrices
    in all layers up to and including that layer.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[图23-7](#figure23-7)显示了这个过程的结果。对于这个可视化，我们计算了每一层的误差，即所有层的矩阵差异的总和，直到包括该层为止。'
- en: '![f23007](Images/f23007.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![f23007](Images/f23007.png)'
- en: 'Figure 23-7: The result of getting noise to match the Gram matrices in VGG16'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图23-7：使噪声匹配VGG16中的Gram矩阵的结果
- en: This is remarkable. By the time we get to the three convolution layers in Block
    3, we’re generating abstracts that are very similar to our original style reference
    in [Figure 23-6](#figure23-6). The splotches of color show similar gradual changes
    in color. There are dark lines between some regions of different colors, and we
    can even see brushstroke textures.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常值得注意。到达Block 3中的三个卷积层时，我们生成的抽象图像与我们原始的风格参考在[图23-6](#figure23-6)中非常相似。颜色的斑块展示了颜色的渐变变化。不同颜色区域之间有暗线，甚至可以看到画笔的纹理。
- en: The Gram matrices have indeed captured the style of Picasso’s painting. But
    why? The anticlimactic answer is that nobody really knows (Li et al. 2017). We
    have different ways to write down the mathematics of what the Gram matrices are
    measuring, but that doesn’t help us understand why this technique captures this
    elusive idea we call style. Neither the original paper on neural style transfer
    (Gatys, Ecker, and Bethge 2015), nor a somewhat more detailed follow-up (Gatys,
    Ecker, and Bethge 2016) explains how the authors hit on this idea or why it works
    so well.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Gram矩阵确实捕捉到了毕加索画作的风格。但为什么呢？令人失望的答案是没有人真正知道（Li等，2017）。我们有不同的方式来表达Gram矩阵所测量的数学内容，但这并没有帮助我们理解为什么这项技术能够捕捉到我们所说的“风格”这一难以捉摸的概念。神经风格迁移的原始论文（Gatys、Ecker和Bethge
    2015）以及后续的较为详细的论文（Gatys、Ecker和Bethge 2016）都没有解释作者是如何想到这个想法的，或为何它如此有效。
- en: Representing Content
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 表示内容
- en: In deep dreaming, we started with an image and manipulated it by changing its
    pixels. If we try the same thing with neural transfer and start with an image
    rather than noise, the image quickly gets lost. The effect of minimizing the differences
    between the Gram matrices causes big changes to the input image, moving it toward
    the style we want, but losing the content of the image in the process.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度梦境（deep dreaming）中，我们从一张图像开始，通过改变其像素来操控它。如果我们在神经风格迁移中做同样的事情，从图像而非噪声开始，图像很快就会丢失。最小化Gram矩阵之间差异的效果会对输入图像产生巨大变化，使其朝我们想要的风格靠拢，但在这个过程中丧失了图像的内容。
- en: A solution to this problem is to still start with noise, because it works so
    well (as shown by [Figure 23-7](#figure23-7)), but retain the essence of the original
    image by adding a second error term. In addition to imposing a *style loss* that
    punishes the input for being a poor match to the style reference (as measured
    by the difference in the Gram matrices), we also impose a *content loss* that
    punishes the input for being too much unlike the base image (the picture we want
    to stylize). By starting with noise and adding together these two error terms
    (usually with different emphasis), we cause the pixels in the noise to change
    so that they simultaneously more closely match the colors of the picture we want
    to modify and the style in which we want it to be shown.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是依然从噪声开始，因为它非常有效（如[图23-7](#figure23-7)所示），但通过添加第二个误差项来保留原始图像的本质。除了施加惩罚输入与风格参考匹配不良的*风格损失*（通过Gram矩阵的差异来衡量），我们还施加了惩罚输入与基图像（我们希望进行风格化的图片）差异过大的*内容损失*。通过从噪声开始并将这两个误差项加在一起（通常有不同的权重），我们使噪声中的像素发生变化，从而使它们同时更接近我们想要修改的图片的颜色和我们希望呈现的风格。
- en: Gathering the content loss is easy. We take our base image, like our frog in
    [Figure 23-3](#figure23-3), and run it through the network. Then we save the activation
    map of every filter. From then on, any time we feed a new image to the network,
    the content loss is just the difference between the filter responses for that
    input and the responses we got from our base image. After all, if all the filters
    respond to an input the same way as the starting image, then the input is the
    starting image (or something very close to it).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 收集内容损失非常简单。我们拿到基图像，比如[图23-3](#figure23-3)中的青蛙，并将其输入到网络中。然后我们保存每个滤波器的激活图。此后，每次我们将新图像输入到网络时，内容损失就是该输入的滤波器响应与我们从基图像中获得的响应之间的差异。毕竟，如果所有滤波器对输入的响应与起始图像相同，那么输入就是起始图像（或非常接近它）。
- en: Style and Content Together
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 风格和内容的结合
- en: To recap, we feed our style reference through the network and save the Gram
    matrix for every pair of filters after each layer. Next, we find a base picture
    we’d like to stylize, run that through the network, and save the feature maps
    produced by every filter.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们将风格参考输入到网络中，并在每一层之后保存每对滤波器的Gram矩阵。接下来，我们找到一个希望风格化的基础图像，将其输入网络，并保存每个滤波器生成的特征图。
- en: Using this saved data, we can create a stylized version of our picture. We start
    with noise and feed it to the network. The block diagram of the whole process
    is shown in [Figure 23-8](#figure23-8).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些保存的数据，我们可以创建一个风格化版本的图像。我们从噪声开始，并将其输入到网络中。整个过程的框图如[图 23-8](#figure23-8)所示。
- en: Let’s start with the content loss. In the light blue rounded-corner rectangle
    at the far left, we gather up the feature maps from the filters on the first convolution
    layer and compute the difference between these maps and the ones we saved from
    the base image (such as the frog). We do the same with the feature maps from the
    second convolution layer. We can do this for all the layers, but for this figure
    and the examples that follow we stopped after two (this is another personal choice,
    guided by experimentation). We add together all of these differences, or content
    losses, and scale their sum by some value that lets us control how much the content
    of the picture should influence the changes we ultimately make to the colors of
    the input image.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从内容损失开始。在最左侧的浅蓝色圆角矩形中，我们收集来自第一卷积层滤波器的特征图，并计算这些特征图与我们从基础图像（例如青蛙）中保存的特征图之间的差异。我们对第二卷积层的特征图也进行相同的处理。我们可以对所有层进行这种操作，但对于本图和后续示例，我们在第二层后停止（这是另一种个人选择，基于实验指导）。我们将所有这些差异或内容损失加在一起，并通过某个值对其总和进行缩放，以便控制图像内容对我们最终在输入图像的颜色上所做变化的影响程度。
- en: '![f23008](Images/f23008.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![f23008](Images/f23008.png)'
- en: 'Figure 23-8: A block diagram of neural style transfer'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23-8：神经风格迁移的框图
- en: Now we address the style. For each layer, in the light yellow rounded-corner
    rectangles, we compute the Gram matrix that tells us how much each filter’s output
    corresponds with every other filter’s output. We then compare those matrices with
    the style matrices we saved earlier. We add up all of these differences to get
    the style loss and scale them by some value that let us control how much influence
    the style has when we modify the pixel colors.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们处理风格问题。对于每一层，在浅黄色的圆角矩形中，我们计算Gram矩阵，告诉我们每个滤波器的输出与其他滤波器输出之间的对应关系。然后，我们将这些矩阵与我们之前保存的风格矩阵进行比较。我们将所有这些差异加起来，得到风格损失，并通过某个值对其进行缩放，以便控制风格对我们修改像素颜色时的影响程度。
- en: The sum of the content and style losses is our error. As with deep dreaming,
    we compute the gradients throughout the network and all the way back to the pixels.
    And again, we leave the weights in the network untouched. Unlike deep dreaming,
    we modify the values of the pixels to *minimize* this total error, because we
    want the input to match the content and style information we previously saved.
    The result is that the original noise slowly changes so that it is simultaneously
    more like the original image and also has the filter relationships of the style.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 内容损失和风格损失的总和就是我们的误差。与深度梦想一样，我们计算整个网络的梯度，并一直反向传播到像素。再次强调，我们保持网络中的权重不变。与深度梦想不同的是，我们修改像素值以*最小化*这个总误差，因为我们希望输入与我们之前保存的内容和风格信息相匹配。结果是，原始噪声会慢慢变化，使其既更像原始图像，又具有风格的滤波器关系。
- en: As with deep dreaming, implementing neural style transfer is straightforward
    with modern deep learning libraries (Chollet 2017; Majumdar 2020).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与深度梦想相似，使用现代深度学习库实现神经风格迁移是直接的（Chollet 2017；Majumdar 2020）。
- en: Running Style Transfer
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行风格迁移
- en: Let’s see how well this works in practice. We again used VGG16 as our network
    and followed the process summarized in [Figure 23-8](#figure23-8).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个方法在实践中效果如何。我们再次使用VGG16作为我们的网络，并按照[图 23-8](#figure23-8)中总结的过程进行。
- en: '[Figure 23-9](#figure23-9) shows nine images, each with a distinctive style.
    These are our style references.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 23-9](#figure23-9)展示了九张每张都有独特风格的图像。这些是我们的风格参考。'
- en: '![f23009](Images/f23009.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![f23009](Images/f23009.png)'
- en: 'Figure 23-9: Nine images with different styles, which serve as our style references.
    From left to right and top down, they are *Starry Night*, by Vincent van Gogh,
    *The Shipwreck of the Minotaur*, by J. M. W. Turner, *The Scream*, by Edvard Munch,
    *Seated Female Nude*, by Pablo Picasso, *Self-Portrait 1907*, by Pablo Picasso,
    *Nighthawks*, by Edward Hopper, *Sergeant Croce*, by the author, *Water Lilies,
    Yellow and Lilac*, by Claude Monet, and *Composition VII* by Wassily Kandinsky.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Let’s apply these styles to our old friend the frog. [Figure 23-10](#figure23-10)
    shows the results.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![f23010](Images/f23010.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23-10: Applying the nine styles in [Figure 23-9](#figure23-9) to a photograph
    of a frog (top)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Wow. That worked great. These images bear close examination, because they have
    a lot of detail. At a first glance, we can see that the color palette of each
    style reference has been transferred to the frog photo. But notice the textures
    and edges, and how blocks of color are shaped. These images are not just color
    shifted frogs, or some kind of overlay or blend of two images. Instead, these
    are high-quality, detailed images of the frog in the different styles. To see
    this more clearly, [Figure 23-11](#figure23-11) shows the same zoomed-in region
    from each frog.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: These images are significantly different, and all match the style they’re based
    on.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see another example. [Figure 23-12](#figure23-12) shows our styles applied
    to a photograph of a town.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![f23011](Images/f23011.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23-11: Details for the nine styled frogs of [Figure 23-10](#figure23-10)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: These images are all the more remarkable when we remember that every one of
    them started out as random noise. For each image, we weighted the content loss
    by 0.025 and the style loss by 1, so the style had 40 times more influence on
    the changes to the pixels than the content did. In these examples, a little bit
    of content went a long way.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: As Figures 23-10 through 23-12 show, the basic algorithm of neural style transfer
    produces terrific results. The technique has been extended and modified in many
    ways to improve the flexibility of the algorithm, the types of results it produces,
    and the range of control that artists can apply to create the results they want
    (Jing et al. 2018). It’s even been applied to video and spherical images that
    completely surround a viewer (Ruder, Dosovitskiy, and Brox 2018).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: As with deep dreaming, neural style transfer is a general algorithm that allows
    for a lot of variation and exploration. There are surely many interesting and
    beautiful artistic effects waiting to be discovered.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![f23012](Images/f23012.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23-12: Applying our nine styles of [Figure 23-9](#figure23-9) to a photograph
    of a town seen from above (top)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Generating More of This Book
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just for fun, we ran the text of the first edition of this book (except for
    this section) through an RNN that generates new text word by word, as discussed
    in Chapter 19\. The full text contained about 427,000 words, drawn from a vocabulary
    of about 10,300 words. To learn this text, we used a network built from two layers
    of LSTMs, with 128 cells each.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm develops its output autoregressively, by finding the next most
    likely word based on the text it has created so far, then the next most likely
    word, then the next, and so on, until we stop it. Generating text this way is
    like the game of creating messages by repeatedly choosing just one of the three
    or four words suggested by a cell phone when you are typing a text (Lowensohn
    2014).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Here are a few sentences manually selected from the output after 250 iterations.
    They are included here exactly as generated, including punctuation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: The responses of the samples in all the red circles share two numbers, like
    the bottom of the last step, when their numbers would influence the input with
    respect to its category.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gradient depends on the loss are little pixels on the wall.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at the code for different dogs in this syllogism.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s surprising how close these come to making sense!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Whole sentences are fun, but some of the most entertaining and poetic bits
    came out soon after the start of training, when the system was generating only
    fragments. Here are some manually selected excerpts after just 10 epochs, again
    presented exactly as they were produced:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Set of of apply, we + the information.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose us only parametric.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The usually quirk (alpha train had we than that to use them way up).
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are mostly incoherent, but from these synthetic phrases we can distill
    a grain of truth: one of the primary goals of this book has definitely been to
    “+ the information.”'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: RNNs are great, but transformer-based generators are even better. We fine-tuned
    a medium-sized instance of the GPT-2 generator on the current edition of this
    book (again, except for this section). Here are a few hand-picked fragments of
    output, selected for their creative range (the second set appear to be figure
    captions).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: This is the neural network that’s been hailed as the queen of artificial neurons.
    It’s no surprise that her name is Christine, but it does speak volumes about the
    state of the field.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can chain together several of these versions into a single tensor of a classifier
    that is essentially a jack-in-the-box.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s use a small utility to take a shortcut that will let us make word predictions
    on the fly, even if we aren’t the right person online or offline.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this view, a 1 in this range is a perfect integer, while a 0 in this range
    is a hyper-realized string of numbers. The approach we adopted in Chapter 6 is
    to treat all 0’s as incomplete, and all 1’s as incomplete, since we still have
    some information about the system we’re evaluating.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Figure 10-7](c10.xhtml#figure10-7): A grid of negative images that don’t have
    labels.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Figure 10-10](c10.xhtml#figure10-10): A deep learning system learns how to
    create and remove license plates from a dataset.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We started this chapter by looking at deep dreaming, a method for manipulating
    an image to stimulate chosen filters in a network and create wild, psychedelic
    images. Then we looked at neural style transfer. Using this technique, we slowly
    change an input of random noise to become simultaneously more like an input image
    and like a style reference, such as works by various painters. Finally, we used
    a little RNN and a transformer to produce new text from the manuscript of this
    book. It’s fun to make new text that seems familiar or reasonable on first glance!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This book has covered only the basic ideas of deep learning. The field is developing
    at a startling pace. Every year new breakthroughs seem to defy all expectations
    of what computers can recognize, analyze, predict, and synthesize.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Just keeping up with new work can be a full-time job. One way to stay up on
    new developments is to watch a website called the arXiv (pronounced “archive”)
    server at [https://arxiv.org/](https://arxiv.org/), or more specifically, the
    machine learning section at [https://arxiv.org/list/cs.LG/recent](https://arxiv.org/list/cs.LG/recent)*.*
    This site hosts preprints of new papers before they appear in journals and conferences.
    But even watching arXiv can be overwhelming, so many people use the arXiv Sanity
    Preserver at [http://www.arxiv-sanity.com](http://www.arxiv-sanity.com) and the
    Semantic Sanity project at [https://s2-sanity.apps.allenai.org/cold-start](https://s2-sanity.apps.allenai.org/cold-start).
    Both sites help filter the repository for just those papers involving specific
    keywords and ideas.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: In this book we’ve focused on the technical background of deep learning. It’s
    important to keep in mind that when we use these systems in ways that can affect
    people, we need to take into account much more than just algorithms (O’Neil 2016).
    Because of their efficiency, deep learning systems are being widely and rapidly
    deployed, often with little oversight or consideration of their impact on the
    societies they affect.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning systems are being used today to influence or determine job offers,
    school admissions, jail sentences, personal and business loans, and even the interpretation
    of medical tests. Deep learning systems control what people see in their news
    and social media feeds, choosing items not to build a healthy and well-informed
    society, but to increase the profits of the organizations delivering those feeds
    (Orlowski 2020). Deep learning systems in “smart speakers” and “smart displays”
    (which also contain microphones and cameras) listen and watch people in their
    homes without pause, sometimes sending their captured data up to a remote server
    for analysis. Cultures used to fear such constant surveillance, but now people
    pay for these devices and willingly put them in their previously private spaces,
    such as their homes and bedrooms. Deep learning is being used to single out potentially
    troublesome children in schools, evaluate the honesty of people answering questions
    at border crossings, and identify individuals at protests and other gatherings
    by recognizing their faces. Mistakes made by these algorithms can vary from annoying
    to fundamentally life-changing. Even when the results are not incorrect, these
    systems are increasingly making profound decisions that affect our public and
    private lives.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning systems are only as good as their training and their algorithms,
    and time after time, we find that biases, prejudices, and outright errors in the
    training data are perpetuated and even enforced by the resulting algorithms. Such
    systems fall far short of the kinds of accuracy and fairness we expect when dealing
    with humans and other living creatures. Our algorithms completely lack empathy
    and compassion. They have no sense of exceptional circumstances, and have no conception
    of the joy or suffering their decisions can cause. They cannot understand love,
    kindness, fear, hope, gratitude, sorrow, generosity, wisdom, or any of the other
    qualities that we value in one another. They cannot admire, cry, smile, grieve,
    celebrate, or regret.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning holds great promise to help us as individuals and societies. But
    any tool can be used as a weapon, benefiting the owners of that tool to the disadvantage
    of those affected. Machine learning systems are often effectively invisible, so
    any systematic errors can go undetected for long periods. Even once problems are
    uncovered, it can require enormous social and political action to hold accountable
    the organizations that benefit by selling or using these systems, and even more
    effort to bring about change.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Another peril of machine learning systems is their insatiable need for enormous
    amounts of training data. This creates a market for organizations that do nothing
    but collect, organize, and sell previously private information about people’s
    lives, from their friendships and family relationships to where and when they
    like to travel, what food they like to eat, what medications they’re taking, and
    what their DNA reveals about them. This data can be used to harass, intimidate,
    threaten, and harm individuals.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: The demand for massive quantities of data also means that as an organization
    grows larger (and often less accountable), the more data it can gather, and the
    more powerful its algorithms grow, making their decisions more influential, which
    means they can gather more data, and thereby solidify the organization’s power
    in a feedback loop. Every imperfection in such a system becomes magnified, and
    because of their scale, negative effects on individuals can go entirely unnoticed
    by the people running these systems. In general, the only competition to such
    organizations will come from other organizations of equivalent size, offering
    systems with their own enormous databases containing their own biases and errors,
    leading to the battles of the biased behemoths we’re familiar with today. This
    continuous and increasing concentration of power is a dangerous force in a free
    society when not subject to significant and strongly enforced control and regulation.
    Sadly, such controls are rarely to be seen today.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning has produced algorithms that make it possible to take any person’s
    appearance, from an entertainer to a politician, and produce images, audio, and
    video that seem to realistically portray that person saying or doing anything
    the person wielding the software desires. Societies have come to rely on captured
    audio, photographs, and video to enforce contracts, reconcile conflicts, exalt
    or shame a public person, influence elections, and serve as evidence in courts.
    The end of that era is very near, returning us to a time before reliable photographs,
    recordings, and video, when hearsay, memory, and opinion replace objective historical
    recordings. Without reliable audio and visual evidence documenting what someone
    actually said or did, the loudest, richest, or most persuasive voices in the room
    will determine many outcomes in public opinion, elections, and courts of law,
    because objective facts will be increasingly harder to find or trust.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is a fascinating field, and we’re just starting to understand
    how it will affect our culture and society. Learning algorithms will surely continue
    to grow in scope and impact. They have the chance to create enormous good by helping
    people be happier, enabling societies to be more fair and supportive, and creating
    healthier and more diverse personal, social, political, and physical environments.
    It’s important to strive for these positive outcomes, even when they curtail corporate
    profits or governmental control.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: We should remember to always use deep learning, like all of our tools, to bring
    out the best in humanity, and make the world a better place for everyone.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
