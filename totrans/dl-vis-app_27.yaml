- en: '23'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '23'
- en: Creative Applications
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 创意应用
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: 'We’ve reached the end of the book. Before we go, let’s relax and have some
    fun. In this chapter we look at some creative ways to use neural networks to create
    art. We explore two image-based applications: *deep dreaming,* which turns images
    into wild, psychedelic art, and *neural style transfer,* which allows us to transform
    photographs into what appear to be paintings in the styles of different artists.
    At the very end, we take a quick dip into *text generation* and use deep learning
    to generate even more of this book.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经到了本书的最后。在我们结束之前，让我们放松一下，玩得开心一点。在这一章中，我们将探讨一些使用神经网络创造艺术的创意方式。我们将介绍两个基于图像的应用：*深度梦境*，它将图像转化为狂野的迷幻艺术，和*神经风格迁移*，它允许我们将照片转化为看起来像是不同艺术家风格的画作。在最后，我们简要介绍一下*文本生成*，并使用深度学习生成更多本书内容。
- en: Deep Dreaming
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度梦境
- en: In deep dreaming, we use some ideas that were invented to help us visualize
    filters in convolutional networks, but we use them to make art. The result is
    that we modify images to excite different filters, causing those images to explode
    in psychedelic patterns.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度梦境中，我们使用了一些发明出来的概念，这些概念最初是为了帮助我们可视化卷积网络中的滤波器，但我们用它们来创造艺术。最终的结果是，我们修改图像以激活不同的滤波器，使得图像爆发出迷幻的图案。
- en: Stimulating Filters
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 刺激滤波器
- en: In Chapter 17, we made images, or visualizations, of the filters in a convolutional
    neural network. Both deep dreaming and style transfer build on that visualization
    technique, so let’s look at it a little more closely. We can make our discussion
    specific by again using VGG16 as we did in Chapter 17 (Simonyan and Zisserman
    2020), though we could substitute just about any CNN image classifier. Our only
    interest here is in the convolution stages, so although we will use the whole
    network as described in Chapter 17, the drawings in this chapter show just the
    convolution and pooling layers, as shown in [Figure 23-1](#figure23-1).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在第17章中，我们创建了卷积神经网络中滤波器的图像或可视化。深度梦境和风格迁移都建立在这种可视化技术的基础上，因此让我们更仔细地看一下。我们可以通过再次使用VGG16来具体说明，就像在第17章中那样（Simonyan和Zisserman
    2020），尽管我们也可以替换为任何卷积神经网络图像分类器。我们唯一感兴趣的是卷积阶段，因此尽管我们会使用第17章中描述的整个网络，但本章中的图示只显示卷积层和池化层，如[图
    23-1](#figure23-1)所示。
- en: '![f23001](Images/f23001.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![f23001](Images/f23001.png)'
- en: 'Figure 23-1: A simplified diagram of VGG16, showing just the convolution and
    pooling layers'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23-1：VGG16的简化示意图，仅显示卷积层和池化层
- en: We’re leaving out the last few stages of VGG16 because their job is to help
    the network predict the proper class of the output. In this application, we don’t
    care about the network’s output. Our only interest here is running an image through
    the network so that the filters in the convolution layers will evaluate their
    inputs. Our goal is to modify a starting image so it excites some chosen layers
    as much as possible. For example, if a few pixels are darker in the middle, that
    may cause the filter that looks for eyes to respond a little bit. Our goal is
    to modify those pixels so that they excite that filter more and more, which means
    that they look more and more like eyes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们省略了VGG16的最后几个阶段，因为它们的作用是帮助网络预测输出的正确类别。在这个应用中，我们不关心网络的输出。我们唯一感兴趣的是将一张图片输入网络，以便卷积层中的滤波器对其输入进行评估。我们的目标是修改一张初始图片，使其尽可能激活某些选择的层。例如，如果中间的几个像素变得更暗，那可能会让寻找眼睛的滤波器稍微有些反应。我们的目标是修改这些像素，使它们激活那个滤波器越来越强烈，也就是说，它们看起来越来越像眼睛。
- en: We don’t need any new tools to do this. All we have to do is pick which filter
    outputs we want to maximize. We can pick just one filter, or several filters from
    different parts of the network. Our choice of which filters to use is entirely
    personal and artistic. Typically, we hunt around, trying out different filters,
    until we see our input images changing in a way that we like.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要任何新工具来完成这个。我们只需选择我们想要最大化的滤波器输出。我们可以选择一个滤波器，或者从网络的不同部分选择多个滤波器。我们选择使用哪些滤波器完全是个人的和艺术性的。通常，我们会四处尝试，测试不同的滤波器，直到看到我们的输入图像以我们喜欢的方式发生变化。
- en: Let’s see the steps. Suppose we pick one filter on each of three different layers,
    as in [Figure 23-2](#figure23-2). We start things off by providing the network
    with an image, which it processes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下步骤。假设我们从三个不同的层中各选一个滤波器，如[图 23-2](#figure23-2)所示。我们首先向网络提供一张图片，网络会对其进行处理。
- en: We take the feature map from the first filter we’ve chosen, add up all of its
    values, and determine how much influence this sum will have by multiplying it
    by a weight that we choose. Though we’re using the word *weight,* this isn’t a
    weight inside the network. It’s just a value we use to control the impact of each
    filter in the deep dream process. We sum up and weight the other filters we’ve
    chosen. Now we add up those results. This gives us a single number, telling us
    how strongly our chosen filters are responding to the input image, weighted by
    how much influence we want to give to each layer’s filters. We call this number
    the *multifilter loss,* or the *multilayer loss.*
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从我们选择的第一个滤波器中提取特征图，将其所有值加起来，并通过乘以我们选择的权重来确定这个总和的影响力。尽管我们使用了“*权重*”这个词，但这并不是网络内部的权重。它只是一个我们用来控制每个滤波器在深度梦境过程中的影响的值。我们将其他选择的滤波器加总并加权。现在，我们将这些结果加起来。这给了我们一个单一的数字，告诉我们我们选择的滤波器对输入图像的响应强度，且该强度是根据我们希望赋予每一层滤波器的影响力来加权的。我们称这个数字为*多滤波器损失*，或者*多层损失*。
- en: '![f23002](Images/f23002.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![f23002](Images/f23002.png)'
- en: 'Figure 23-2: The deep dream algorithm uses a loss built from multiple layers'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图23-2：深度梦境算法使用由多层构建的损失
- en: 'Now comes the tricky part: the multifilter loss becomes the network’s “error.”
    In previous chapters, we used the error to drive backprop, which computed gradients
    for all of the network’s weights, starting at the final layer and working our
    way backward to the first. Then we used those gradients to modify the network’s
    weights to minimize the error. But that’s not what we do here. Instead, we want
    the error (the filter responses) to be as big as we can make them. And we don’t
    want to do this by changing the network, since we’re not training it. We’re going
    to *freeze* the network, so its weights can’t change. Instead, we’re going to
    modify the colors of the pixels themselves.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入棘手的部分：多滤波器损失变成了网络的“误差”。在前几章中，我们使用误差来驱动反向传播，计算网络所有权重的梯度，从最后一层开始，逐步向后推导到第一层。然后，我们使用这些梯度来修改网络的权重，以最小化误差。但在这里，我们不是这样做的。相反，我们希望误差（滤波器响应）尽可能大。而我们不想通过改变网络来实现这一点，因为我们并没有训练它。我们将*冻结*网络，使其权重无法改变。相反，我们将修改像素本身的颜色。
- en: So, starting with this error, we use backprop as usual to find the gradients
    on all the weights in the network, but when we reach the first hidden layer, we
    take one more step backward to the input layer, which holds the pixels themselves.
    Then we use backprop as usual to find the gradients for the pixels. After all,
    changing the input pixels causes the values computed by the network to change,
    and thus causes a change to our error. Just as we can use backprop to learn how
    to change the network’s weights to reduce the error in a typical training setup,
    we can use the same backprop algorithm to find out how to change the pixel values
    to increase this error.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，从这个误差开始，我们像往常一样使用反向传播来找到网络中所有权重的梯度，但当我们到达第一个隐藏层时，我们再向后退一步，来到输入层，这里包含了像素本身。然后，我们像往常一样使用反向传播来找到像素的梯度。毕竟，改变输入像素会导致网络计算的值发生变化，从而导致我们的误差发生变化。就像我们可以使用反向传播来学习如何改变网络的权重以减少误差，在典型的训练设置中一样，我们也可以使用相同的反向传播算法来找出如何改变像素值以增加这个误差。
- en: Now, as usual, we apply the optimization step. Since we’re not training and
    the network is frozen, we don’t touch the network weights. But we do use the gradients
    on the pixels to modify their color values so that they *maximize* the error,
    or more strongly stimulate our selected filters.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，像往常一样，我们应用优化步骤。由于我们没有进行训练，网络是被冻结的，我们不会触及网络的权重。但我们确实使用像素的梯度来修改它们的颜色值，以便它们*最大化*误差，或者更强烈地激发我们选择的滤波器。
- en: The result is that the pixel colors change just a little, in such a way that
    filters respond even more, creating a bigger error, which we use to find new gradients
    on the pixels, causing them to excite the filters even more, and around it goes,
    with the picture changing more and more each time we repeat the loop.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，像素的颜色会稍微改变，以使滤波器响应更强，从而产生更大的误差，我们使用这个误差来找到像素的新梯度，使它们更强烈地激发滤波器，如此循环往复，每次我们重复这个循环，图像都会发生越来越大的变化。
- en: Since this is an artistic process, we usually watch the output after every update
    (or every few updates), and stop it when we like what we’re seeing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个艺术过程，我们通常在每次更新后（或者每几次更新后）查看输出，并在看到自己喜欢的结果时停止。
- en: Running Deep Dreaming
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行深度梦境
- en: Let’s put this algorithm to work, using the frog in [Figure 23-3](#figure23-3)
    as our starting point.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们让这个算法发挥作用，以[图 23-3](#figure23-3)中的青蛙为起点。
- en: '![f23003](Images/f23003.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![f23003](Images/f23003.png)'
- en: 'Figure 23-3: A calm and thoughtful frog'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23-3：一只平静且深思的青蛙
- en: '[Figure 23-4](#figure23-4) shows some “dreams” from our frog image, using some
    filters (and weights on them) that we chose by trial and error.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 23-4](#figure23-4) 显示了使用我们通过反复试验选择的一些滤镜（及其权重）从青蛙图像中生成的一些“梦想”。'
- en: '![f23004](Images/f23004.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![f23004](Images/f23004.png)'
- en: 'Figure 23-4: Some deep dream results from the starting frog image (in the upper-left
    corner)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23-4：从起始青蛙图像（左上角）开始的一些深度梦想结果
- en: We’re seeing lots of eyes in [Figure 23-4](#figure23-4) because some of our
    selected filters responded to eyes. If we had selected filters that responded
    to, say, horses and shoes, then we would expect to see lots of horses and shoes
    in our images.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 23-4](#figure23-4)中，我们看到很多眼睛，因为我们选择的部分滤镜响应了眼睛。如果我们选择了响应马和鞋子的滤镜，那么我们就会期望看到图像中出现大量的马和鞋子。
- en: '[Figure 23-5](#figure23-5) shows the results starting from an image of a dog.
    The changes to the image are mostly of a finer texture because the dog image is
    about 1,000 pixels on each side, more than four times the size of the images that
    the network trained on. The image in the lower-right used a version of the dog
    image that was scaled to the same size as the network’s training data, 224 by
    224\.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 23-5](#figure23-5) 显示了从一张狗的图像开始的结果。图像的变化主要体现在更细腻的纹理，因为这张狗的图像每边大约有 1000 像素，是网络训练时使用的图像的四倍多。右下角的图像使用了缩放到与网络训练数据相同大小（224
    x 224 像素）的狗图像版本。'
- en: '![f23005](Images/f23005.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![f23005](Images/f23005.png)'
- en: 'Figure 23-5: Deep dreaming about a dog'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23-5：关于狗的深度梦想
- en: The original name for this technique was *inceptionism* (Mordvintsev, Olah,
    and Tyka 2015) in honor of the movie *Inception,* but it has come to be more frequently
    known as *deep dreaming*. The name is a poetic suggestion that the network is
    “dreaming” about the original image, and the image we get back shows us where
    the network’s dream went. Deep dreaming has become popular not only because of
    the wild images it creates, but because it’s not hard to implement using modern
    deep learning libraries (Bonaccorso 2020).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术的原始名称是 *启示主义*（Mordvintsev, Olah, 和 Tyka 2015），以电影 *盗梦空间*（Inception）为灵感，但它现在更常被称为
    *深度梦想*。这个名字是一个富有诗意的暗示，表明网络正在“梦想”原始图像，我们得到的图像展示了网络的梦想去了哪里。深度梦想之所以流行，不仅因为它创造了奇异的图像，还因为使用现代深度学习库实现它并不困难（Bonaccorso
    2020）。
- en: Many variations on this basic algorithm have been explored (Tyka 2015), but
    they only scratch the surface. We can imagine schemes to automatically determine
    the weights on the layers or even apply weights to the individual filters on each
    layer. We can “mask” the activation maps before we add them up so that some areas
    (like the background) are ignored, or we can mask the updates to the pixels so
    that some pixels in the original image are not changed at all in response to one
    set of layer outputs, but are instead allowed to change a lot in response to some
    other set of layer outputs. We can even apply different combinations of layers
    and weights to different regions of the input image. The deep dreaming approach
    to making art has lots of room left for new discoveries.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基本算法的许多变种已被探索过（Tyka 2015），但它们只是略微触及了表面。我们可以设想一些方案，自动确定层上的权重，甚至对每层的单个滤镜应用权重。我们可以在叠加激活图之前对其进行“掩膜”，以忽略某些区域（如背景），或者我们可以对像素更新进行掩膜，这样原始图像中的某些像素在响应某一层的输出时完全不改变，而是在响应另一组层的输出时发生较大变化。我们甚至可以对输入图像的不同区域应用不同的层和权重组合。深度梦想艺术创作的方法仍有很大的探索空间。
- en: There’s no “right” or “best” way to do deep dreaming. It’s a creative exercise
    in which we follow our aesthetics, hunches, or wild guesses to hunt for images
    that appeal to us. It can be hard to predict what’s going to come out from any
    particular combination of network, layers, and weights, so the process rewards
    patience and a lot of experimenting.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 没有“正确”或“最佳”的深度梦想方法。这是一种创造性的练习，我们根据自己的审美、直觉或天马行空的猜测来寻找让我们感兴趣的图像。很难预测任何特定的网络、层和权重组合会产生什么效果，因此这个过程奖励耐心和大量的实验。
- en: Neural Style Transfer
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经风格迁移
- en: 'We can use a variation on the deep dreaming technique to do something remarkable:
    transfer one artist’s style onto another image. This process is called *neural
    style transfer*.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用深度梦境技术的一个变种来做一些了不起的事情：将一种艺术家的风格转移到另一张图像上。这个过程被称为*神经风格迁移*。
- en: Representing Style
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 表示风格
- en: Cultures often celebrate the idiosyncratic visual style of artists. Let’s focus
    on paintings. What characterizes the style of a painting? That’s a big question,
    because “style” can include someone’s world view, which influences choices as
    diverse as their subject matter, composition, materials, and tools. Let’s focus
    strictly on visual appearance. Even narrowed down this way, it’s hard to precisely
    identify what “style” means for a painting, but we might say that it refers to
    how colors and shapes are used to create forms, and the types and distributions
    of those forms across the canvas (Art Story Foundation 2020; Wikipedia 2020).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 各种文化常常庆祝艺术家独特的视觉风格。我们将重点讨论画作。那么，是什么特征定义了一幅画的风格呢？这是一个大问题，因为“风格”可以包括一个人的世界观，这种世界观会影响他们在主题、构图、材料和工具等方面的选择。让我们仅专注于视觉外观。即使这样限制范围，精确地定义画作的“风格”仍然很困难，但我们或许可以说，它指的是如何使用颜色和形状来创造形式，以及这些形式在画布上分布和类型的方式（Art
    Story Foundation 2020；Wikipedia 2020）。
- en: Rather than try to refine this description, let’s see if we can find something
    that seems like it’s in the ballpark, while also being something we can formalize
    in terms of the layers and filters of a deep convolutional network.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与其试图完善这个描述，不如看看我们是否能找到一个大致相符的东西，同时又能在深度卷积网络的层和过滤器的框架内进行形式化。
- en: Our goal in this section is to take a picture we want to modify, called the
    *base image*, and a second picture whose style we want to match, called the *style
    reference*. For example, our frog could be our base image, and any painting could
    be the style reference. We want to use these to create a new image, called the
    *generated image*, which has the content of the base image expressed in the style
    of the style reference.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目标是获取我们想要修改的图片，称为*基础图像*，以及我们希望匹配其风格的第二张图片，称为*风格参考图像*。例如，我们的青蛙可以是基础图像，任何一幅画作都可以是风格参考图像。我们希望使用这两者来创建一张新图像，称为*生成图像*，它将基础图像的内容以风格参考图像的风格进行表达。
- en: To get started, we will make an assertion that comes out of nowhere. We say
    that we can characterize the style of an image (such as a painting) by looking
    at the layer activations it produces and finding pairs of layers that activate
    in roughly the same way. This idea comes from a seminal paper published in 2015
    (Gatys, Ecker, and Bethge 2015). Without getting into the details, the process
    begins by running the style reference through a deep convolutional network. As
    with deep dreaming, we ignore its output, and instead focus on just the convolution
    filters.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始，我们将做出一个看似无根据的断言。我们说，图像（例如画作）的风格可以通过观察它产生的层激活来进行表征，并找到大致以相同方式激活的层对。这个想法来源于2015年发表的一篇开创性论文（Gatys,
    Ecker, 和 Bethge 2015）。不深入细节，过程从将风格参考图像输入深度卷积网络开始。与深度梦境一样，我们忽略它的输出，而专注于卷积过滤器。
- en: All of the activation maps in a given layer have the same size, so we can easily
    compare them to one another. Let’s start with the first activation map in a layer
    (that is, the first filter’s output). We can compare that to the activation map
    produced by the second filter and produce a score for the pair. If the two maps
    are very similar (that is, the filters are firing in the same places), we assign
    the pair a high score, and if the maps are very different, we give that pair a
    low score. We then compare the first map to the third map and compute their score,
    then compare the first and the fourth map, compute their score, and so on. Then
    we can start with the second map and compare it to every other map in the layer.
    We can organize the results in a grid, with as many cells on each side as there
    are filters in the layer. The value in each cell of the grid tells us the score
    for that pair of layers. This grid is called a *Gram matrix.* Let’s make one such
    Gram matrix for every layer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can restate our notion of style more formally: the style of an image
    is represented by the Gram matrices for all the layers. That is, each style produces
    its own particular form of Gram matrices.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see if this claim is true. [Figure 23-6](#figure23-6) shows a famous self-portrait
    by Pablo Picasso from 1907\. There’s a ton of style here, such as big blocks of
    color and thick dark lines. Let’s run this through VGG16 and save the Gram matrix
    at each layer. We’ll call those the *style matrices*, and save them as the representation
    of the style for this image.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![f23006](Images/f23006.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23-6: A 1907 self-portrait by Pablo Picasso'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: If the Gram matrices represent style, then we can use them to modify a starting
    image of random noise. We run the noisy input image through the network and compute
    its Gram matrices, which we call the *image matrices.* If the style matrices really
    do somehow represent the style of the Picasso image, then if we can change the
    colors of the pixels in the noisy image so that eventually the image matrices
    come close to the style matrices, the noisy image should take on the style of
    the painting.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Let’s do just that. We’ll run the noise through the network, compute the image
    matrix at each layer, and compare that to the style matrix we saved for that layer.
    We’ll add up the differences between these two matrices so the more different
    they are, the bigger the result. Then we’ll add together these differences for
    all the layers, and this is the error for our network. As with deep dreaming,
    we use this error to compute the gradients for the entire network, including the
    pixels at the start, but we only modify the colors of the pixels. Unlike deep
    dreaming, our goal now is to minimize the error, and thereby change the pixels
    so that their colors produce Gram matrices that are like those of the style reference.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 23-7](#figure23-7) shows the result of this process. For this visualization,
    we computed each layer’s error as the sum of the differences of all the matrices
    in all layers up to and including that layer.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[图23-7](#figure23-7)显示了这个过程的结果。对于这个可视化，我们计算了每一层的误差，即所有层的矩阵差异的总和，直到包括该层为止。'
- en: '![f23007](Images/f23007.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![f23007](Images/f23007.png)'
- en: 'Figure 23-7: The result of getting noise to match the Gram matrices in VGG16'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图23-7：使噪声匹配VGG16中的Gram矩阵的结果
- en: This is remarkable. By the time we get to the three convolution layers in Block
    3, we’re generating abstracts that are very similar to our original style reference
    in [Figure 23-6](#figure23-6). The splotches of color show similar gradual changes
    in color. There are dark lines between some regions of different colors, and we
    can even see brushstroke textures.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常值得注意。到达Block 3中的三个卷积层时，我们生成的抽象图像与我们原始的风格参考在[图23-6](#figure23-6)中非常相似。颜色的斑块展示了颜色的渐变变化。不同颜色区域之间有暗线，甚至可以看到画笔的纹理。
- en: The Gram matrices have indeed captured the style of Picasso’s painting. But
    why? The anticlimactic answer is that nobody really knows (Li et al. 2017). We
    have different ways to write down the mathematics of what the Gram matrices are
    measuring, but that doesn’t help us understand why this technique captures this
    elusive idea we call style. Neither the original paper on neural style transfer
    (Gatys, Ecker, and Bethge 2015), nor a somewhat more detailed follow-up (Gatys,
    Ecker, and Bethge 2016) explains how the authors hit on this idea or why it works
    so well.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Gram矩阵确实捕捉到了毕加索画作的风格。但为什么呢？令人失望的答案是没有人真正知道（Li等，2017）。我们有不同的方式来表达Gram矩阵所测量的数学内容，但这并没有帮助我们理解为什么这项技术能够捕捉到我们所说的“风格”这一难以捉摸的概念。神经风格迁移的原始论文（Gatys、Ecker和Bethge
    2015）以及后续的较为详细的论文（Gatys、Ecker和Bethge 2016）都没有解释作者是如何想到这个想法的，或为何它如此有效。
- en: Representing Content
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 表示内容
- en: In deep dreaming, we started with an image and manipulated it by changing its
    pixels. If we try the same thing with neural transfer and start with an image
    rather than noise, the image quickly gets lost. The effect of minimizing the differences
    between the Gram matrices causes big changes to the input image, moving it toward
    the style we want, but losing the content of the image in the process.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度梦境（deep dreaming）中，我们从一张图像开始，通过改变其像素来操控它。如果我们在神经风格迁移中做同样的事情，从图像而非噪声开始，图像很快就会丢失。最小化Gram矩阵之间差异的效果会对输入图像产生巨大变化，使其朝我们想要的风格靠拢，但在这个过程中丧失了图像的内容。
- en: A solution to this problem is to still start with noise, because it works so
    well (as shown by [Figure 23-7](#figure23-7)), but retain the essence of the original
    image by adding a second error term. In addition to imposing a *style loss* that
    punishes the input for being a poor match to the style reference (as measured
    by the difference in the Gram matrices), we also impose a *content loss* that
    punishes the input for being too much unlike the base image (the picture we want
    to stylize). By starting with noise and adding together these two error terms
    (usually with different emphasis), we cause the pixels in the noise to change
    so that they simultaneously more closely match the colors of the picture we want
    to modify and the style in which we want it to be shown.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是依然从噪声开始，因为它非常有效（如[图23-7](#figure23-7)所示），但通过添加第二个误差项来保留原始图像的本质。除了施加惩罚输入与风格参考匹配不良的*风格损失*（通过Gram矩阵的差异来衡量），我们还施加了惩罚输入与基图像（我们希望进行风格化的图片）差异过大的*内容损失*。通过从噪声开始并将这两个误差项加在一起（通常有不同的权重），我们使噪声中的像素发生变化，从而使它们同时更接近我们想要修改的图片的颜色和我们希望呈现的风格。
- en: Gathering the content loss is easy. We take our base image, like our frog in
    [Figure 23-3](#figure23-3), and run it through the network. Then we save the activation
    map of every filter. From then on, any time we feed a new image to the network,
    the content loss is just the difference between the filter responses for that
    input and the responses we got from our base image. After all, if all the filters
    respond to an input the same way as the starting image, then the input is the
    starting image (or something very close to it).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 收集内容损失非常简单。我们拿到基图像，比如[图23-3](#figure23-3)中的青蛙，并将其输入到网络中。然后我们保存每个滤波器的激活图。此后，每次我们将新图像输入到网络时，内容损失就是该输入的滤波器响应与我们从基图像中获得的响应之间的差异。毕竟，如果所有滤波器对输入的响应与起始图像相同，那么输入就是起始图像（或非常接近它）。
- en: Style and Content Together
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 风格和内容的结合
- en: To recap, we feed our style reference through the network and save the Gram
    matrix for every pair of filters after each layer. Next, we find a base picture
    we’d like to stylize, run that through the network, and save the feature maps
    produced by every filter.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们将风格参考输入到网络中，并在每一层之后保存每对滤波器的Gram矩阵。接下来，我们找到一个希望风格化的基础图像，将其输入网络，并保存每个滤波器生成的特征图。
- en: Using this saved data, we can create a stylized version of our picture. We start
    with noise and feed it to the network. The block diagram of the whole process
    is shown in [Figure 23-8](#figure23-8).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些保存的数据，我们可以创建一个风格化版本的图像。我们从噪声开始，并将其输入到网络中。整个过程的框图如[图 23-8](#figure23-8)所示。
- en: Let’s start with the content loss. In the light blue rounded-corner rectangle
    at the far left, we gather up the feature maps from the filters on the first convolution
    layer and compute the difference between these maps and the ones we saved from
    the base image (such as the frog). We do the same with the feature maps from the
    second convolution layer. We can do this for all the layers, but for this figure
    and the examples that follow we stopped after two (this is another personal choice,
    guided by experimentation). We add together all of these differences, or content
    losses, and scale their sum by some value that lets us control how much the content
    of the picture should influence the changes we ultimately make to the colors of
    the input image.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从内容损失开始。在最左侧的浅蓝色圆角矩形中，我们收集来自第一卷积层滤波器的特征图，并计算这些特征图与我们从基础图像（例如青蛙）中保存的特征图之间的差异。我们对第二卷积层的特征图也进行相同的处理。我们可以对所有层进行这种操作，但对于本图和后续示例，我们在第二层后停止（这是另一种个人选择，基于实验指导）。我们将所有这些差异或内容损失加在一起，并通过某个值对其总和进行缩放，以便控制图像内容对我们最终在输入图像的颜色上所做变化的影响程度。
- en: '![f23008](Images/f23008.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![f23008](Images/f23008.png)'
- en: 'Figure 23-8: A block diagram of neural style transfer'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23-8：神经风格迁移的框图
- en: Now we address the style. For each layer, in the light yellow rounded-corner
    rectangles, we compute the Gram matrix that tells us how much each filter’s output
    corresponds with every other filter’s output. We then compare those matrices with
    the style matrices we saved earlier. We add up all of these differences to get
    the style loss and scale them by some value that let us control how much influence
    the style has when we modify the pixel colors.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们处理风格问题。对于每一层，在浅黄色的圆角矩形中，我们计算Gram矩阵，告诉我们每个滤波器的输出与其他滤波器输出之间的对应关系。然后，我们将这些矩阵与我们之前保存的风格矩阵进行比较。我们将所有这些差异加起来，得到风格损失，并通过某个值对其进行缩放，以便控制风格对我们修改像素颜色时的影响程度。
- en: The sum of the content and style losses is our error. As with deep dreaming,
    we compute the gradients throughout the network and all the way back to the pixels.
    And again, we leave the weights in the network untouched. Unlike deep dreaming,
    we modify the values of the pixels to *minimize* this total error, because we
    want the input to match the content and style information we previously saved.
    The result is that the original noise slowly changes so that it is simultaneously
    more like the original image and also has the filter relationships of the style.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 内容损失和风格损失的总和就是我们的误差。与深度梦想一样，我们计算整个网络的梯度，并一直反向传播到像素。再次强调，我们保持网络中的权重不变。与深度梦想不同的是，我们修改像素值以*最小化*这个总误差，因为我们希望输入与我们之前保存的内容和风格信息相匹配。结果是，原始噪声会慢慢变化，使其既更像原始图像，又具有风格的滤波器关系。
- en: As with deep dreaming, implementing neural style transfer is straightforward
    with modern deep learning libraries (Chollet 2017; Majumdar 2020).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与深度梦想相似，使用现代深度学习库实现神经风格迁移是直接的（Chollet 2017；Majumdar 2020）。
- en: Running Style Transfer
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行风格迁移
- en: Let’s see how well this works in practice. We again used VGG16 as our network
    and followed the process summarized in [Figure 23-8](#figure23-8).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个方法在实践中效果如何。我们再次使用VGG16作为我们的网络，并按照[图 23-8](#figure23-8)中总结的过程进行。
- en: '[Figure 23-9](#figure23-9) shows nine images, each with a distinctive style.
    These are our style references.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 23-9](#figure23-9)展示了九张每张都有独特风格的图像。这些是我们的风格参考。'
- en: '![f23009](Images/f23009.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![f23009](Images/f23009.png)'
- en: 'Figure 23-9: Nine images with different styles, which serve as our style references.
    From left to right and top down, they are *Starry Night*, by Vincent van Gogh,
    *The Shipwreck of the Minotaur*, by J. M. W. Turner, *The Scream*, by Edvard Munch,
    *Seated Female Nude*, by Pablo Picasso, *Self-Portrait 1907*, by Pablo Picasso,
    *Nighthawks*, by Edward Hopper, *Sergeant Croce*, by the author, *Water Lilies,
    Yellow and Lilac*, by Claude Monet, and *Composition VII* by Wassily Kandinsky.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图23-9：九张不同风格的图像，它们作为我们的风格参考。从左到右，从上到下，它们分别是**《星夜》**，由文森特·梵高绘制，**《米诺陶斯的沉船》**，由J.
    M. W. 塔纳绘制，**《呐喊》**，由爱德华·蒙克绘制，**《坐姿女性裸体》**，由巴勃罗·毕加索绘制，**《1907年自画像》**，由巴勃罗·毕加索绘制，**《夜鹰》**，由爱德华·霍普绘制，**《克罗奇中士》**，由作者绘制，**《睡莲：黄色与丁香》**，由克劳德·莫奈绘制，以及**《构成VII》**，由瓦西里·康定斯基绘制。
- en: Let’s apply these styles to our old friend the frog. [Figure 23-10](#figure23-10)
    shows the results.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些风格应用到我们老朋友——青蛙身上。[图23-10](#figure23-10)展示了结果。
- en: '![f23010](Images/f23010.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![f23010](Images/f23010.png)'
- en: 'Figure 23-10: Applying the nine styles in [Figure 23-9](#figure23-9) to a photograph
    of a frog (top)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图23-10：将[图23-9](#figure23-9)中的九种风格应用到青蛙的照片上（顶部）
- en: Wow. That worked great. These images bear close examination, because they have
    a lot of detail. At a first glance, we can see that the color palette of each
    style reference has been transferred to the frog photo. But notice the textures
    and edges, and how blocks of color are shaped. These images are not just color
    shifted frogs, or some kind of overlay or blend of two images. Instead, these
    are high-quality, detailed images of the frog in the different styles. To see
    this more clearly, [Figure 23-11](#figure23-11) shows the same zoomed-in region
    from each frog.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，这效果真棒。这些图像值得仔细观察，因为它们有很多细节。乍一看，我们可以看到每种风格参考的色彩调色板已经被转移到了青蛙照片中。但注意纹理和边缘，以及色块的形状。这些图像不仅仅是颜色变化的青蛙，或是两张图像的叠加或混合。相反，这些是不同风格下青蛙的高质量详细图像。为了更清晰地看到这一点，[图23-11](#figure23-11)展示了每只青蛙的相同放大区域。
- en: These images are significantly different, and all match the style they’re based
    on.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像有显著不同，而且每一张都与其所基于的风格相匹配。
- en: Let’s see another example. [Figure 23-12](#figure23-12) shows our styles applied
    to a photograph of a town.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看另一个例子。[图23-12](#figure23-12)展示了将我们的风格应用到一张城镇照片上。
- en: '![f23011](Images/f23011.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![f23011](Images/f23011.png)'
- en: 'Figure 23-11: Details for the nine styled frogs of [Figure 23-10](#figure23-10)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图23-11：[图23-10](#figure23-10)中九只风格化青蛙的详细信息
- en: These images are all the more remarkable when we remember that every one of
    them started out as random noise. For each image, we weighted the content loss
    by 0.025 and the style loss by 1, so the style had 40 times more influence on
    the changes to the pixels than the content did. In these examples, a little bit
    of content went a long way.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们记得这些图像最初都是从随机噪声开始时，它们显得尤为引人注目。对于每一张图像，我们将内容损失的权重设为0.025，风格损失的权重设为1，因此风格对像素变化的影响是内容的40倍。在这些示例中，少量的内容产生了很大的效果。
- en: As Figures 23-10 through 23-12 show, the basic algorithm of neural style transfer
    produces terrific results. The technique has been extended and modified in many
    ways to improve the flexibility of the algorithm, the types of results it produces,
    and the range of control that artists can apply to create the results they want
    (Jing et al. 2018). It’s even been applied to video and spherical images that
    completely surround a viewer (Ruder, Dosovitskiy, and Brox 2018).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如图23-10至23-12所示，神经风格迁移的基本算法产生了出色的结果。这项技术在许多方面得到了扩展和修改，以提高算法的灵活性、它产生的结果类型，以及艺术家可以应用的控制范围，以创造他们想要的效果（Jing
    et al. 2018）。它甚至已被应用于视频和完全包围观众的球形图像（Ruder, Dosovitskiy, and Brox 2018）。
- en: As with deep dreaming, neural style transfer is a general algorithm that allows
    for a lot of variation and exploration. There are surely many interesting and
    beautiful artistic effects waiting to be discovered.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与深度梦境类似，神经风格迁移是一个通用算法，允许进行大量的变化和探索。肯定还有许多有趣且美丽的艺术效果等待被发现。
- en: '![f23012](Images/f23012.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![f23012](Images/f23012.png)'
- en: 'Figure 23-12: Applying our nine styles of [Figure 23-9](#figure23-9) to a photograph
    of a town seen from above (top)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图23-12：将[图23-9](#figure23-9)中的九种风格应用到一张俯视的城镇照片（顶部）
- en: Generating More of This Book
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成更多本书内容
- en: Just for fun, we ran the text of the first edition of this book (except for
    this section) through an RNN that generates new text word by word, as discussed
    in Chapter 19\. The full text contained about 427,000 words, drawn from a vocabulary
    of about 10,300 words. To learn this text, we used a network built from two layers
    of LSTMs, with 128 cells each.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了好玩，我们将本书第一版的文本（这一节除外）通过一个RNN进行了处理，该RNN按字生成新文本，如第19章所讨论的。完整的文本大约包含427,000个单词，来自大约10,300个单词的词汇表。为了学习这些文本，我们使用了一个由两层LSTM组成的网络，每层128个单元。
- en: The algorithm develops its output autoregressively, by finding the next most
    likely word based on the text it has created so far, then the next most likely
    word, then the next, and so on, until we stop it. Generating text this way is
    like the game of creating messages by repeatedly choosing just one of the three
    or four words suggested by a cell phone when you are typing a text (Lowensohn
    2014).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法通过自回归地生成输出，根据它到目前为止生成的文本找到下一个最可能的单词，然后是下一个最可能的单词，再然后是下一个，以此类推，直到我们停止它。以这种方式生成文本就像是玩一个游戏，通过不断选择手机在你打字时建议的三到四个单词中的一个，来创建信息（Lowensohn
    2014）。
- en: Here are a few sentences manually selected from the output after 250 iterations.
    They are included here exactly as generated, including punctuation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在250次迭代后从输出中手动选择的一些句子。它们原封不动地包括在此，其中包括标点符号。
- en: The responses of the samples in all the red circles share two numbers, like
    the bottom of the last step, when their numbers would influence the input with
    respect to its category.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有红圈中的样本的响应共享两个数字，就像最后一步的底部，当它们的数字会影响输入与其类别的关系时。
- en: The gradient depends on the loss are little pixels on the wall.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度取决于损失，它们是墙上的一些小像素。
- en: Let’s look at the code for different dogs in this syllogism.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们看看这个三段论中不同狗的代码。
- en: It’s surprising how close these come to making sense!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，这些几乎是有意义的！
- en: 'Whole sentences are fun, but some of the most entertaining and poetic bits
    came out soon after the start of training, when the system was generating only
    fragments. Here are some manually selected excerpts after just 10 epochs, again
    presented exactly as they were produced:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 整个句子很有趣，但一些最具娱乐性和诗意的片段出现在训练开始不久，当时系统只生成片段。以下是仅经过10个周期后手动选择的一些摘录，再次按照它们生成的原样呈现：
- en: Set of of apply, we + the information.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组应用，我们+信息。
- en: Suppose us only parametric.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我们只有参数化。
- en: The usually quirk (alpha train had we than that to use them way up).
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常的怪异（我们比那更应该使用它们的方式上）。
- en: 'These are mostly incoherent, but from these synthetic phrases we can distill
    a grain of truth: one of the primary goals of this book has definitely been to
    “+ the information.”'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这些大多是无序的，但从这些合成短语中，我们可以提炼出一个事实：这本书的主要目标之一肯定是“+信息”。
- en: RNNs are great, but transformer-based generators are even better. We fine-tuned
    a medium-sized instance of the GPT-2 generator on the current edition of this
    book (again, except for this section). Here are a few hand-picked fragments of
    output, selected for their creative range (the second set appear to be figure
    captions).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）很棒，但基于转换器的生成器更好。我们对本书当前版本（除了这一节）使用了一个中等大小的GPT-2生成器进行微调。以下是一些精选的输出片段，挑选它们是因为它们的创意范围（第二组似乎是图表标题）。
- en: This is the neural network that’s been hailed as the queen of artificial neurons.
    It’s no surprise that her name is Christine, but it does speak volumes about the
    state of the field.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是被誉为人工神经元女王的神经网络。她的名字是Christine，这一点毫不奇怪，但这确实能充分说明该领域的现状。
- en: We can chain together several of these versions into a single tensor of a classifier
    that is essentially a jack-in-the-box.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将这些版本链接在一起，形成一个分类器的单一张量，它本质上是一个“惊喜盒子”。
- en: Let’s use a small utility to take a shortcut that will let us make word predictions
    on the fly, even if we aren’t the right person online or offline.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们使用一个小工具，采取捷径，使我们能够即时进行单词预测，即使我们不是在线或离线时合适的人。
- en: In this view, a 1 in this range is a perfect integer, while a 0 in this range
    is a hyper-realized string of numbers. The approach we adopted in Chapter 6 is
    to treat all 0’s as incomplete, and all 1’s as incomplete, since we still have
    some information about the system we’re evaluating.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个视图中，范围内的1是完美的整数，而范围内的0是一个超现实的数字字符串。我们在第6章采用的方法是将所有的0视为不完整，将所有的1视为不完整，因为我们仍然对我们正在评估的系统有一些信息。
- en: '[Figure 10-7](c10.xhtml#figure10-7): A grid of negative images that don’t have
    labels.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图 10-7](c10.xhtml#figure10-7)：一组没有标签的负图像。'
- en: '[Figure 10-10](c10.xhtml#figure10-10): A deep learning system learns how to
    create and remove license plates from a dataset.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图 10-10](c10.xhtml#figure10-10)：一个深度学习系统学习如何从数据集中创建和移除车牌。'
- en: Summary
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: We started this chapter by looking at deep dreaming, a method for manipulating
    an image to stimulate chosen filters in a network and create wild, psychedelic
    images. Then we looked at neural style transfer. Using this technique, we slowly
    change an input of random noise to become simultaneously more like an input image
    and like a style reference, such as works by various painters. Finally, we used
    a little RNN and a transformer to produce new text from the manuscript of this
    book. It’s fun to make new text that seems familiar or reasonable on first glance!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们研究了深度梦境技术，这是一种通过操控图像来激发网络中的特定滤镜，创造出狂野、迷幻图像的方法。接着，我们探讨了神经风格迁移技术。利用这项技术，我们逐渐将随机噪声输入转变为既像输入图像，又像风格参考图像的形式，比如各种画家的作品。最后，我们使用了一个小型的RNN和一个Transformer来从本书的手稿中生成新的文本。生成看似熟悉或合理的新文本是件有趣的事！
- en: Final Thoughts
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最后的思考
- en: This book has covered only the basic ideas of deep learning. The field is developing
    at a startling pace. Every year new breakthroughs seem to defy all expectations
    of what computers can recognize, analyze, predict, and synthesize.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 本书只涉及了深度学习的基本概念。这个领域正在以惊人的速度发展。每年新的突破似乎都打破了对计算机能够识别、分析、预测和合成的所有预期。
- en: Just keeping up with new work can be a full-time job. One way to stay up on
    new developments is to watch a website called the arXiv (pronounced “archive”)
    server at [https://arxiv.org/](https://arxiv.org/), or more specifically, the
    machine learning section at [https://arxiv.org/list/cs.LG/recent](https://arxiv.org/list/cs.LG/recent)*.*
    This site hosts preprints of new papers before they appear in journals and conferences.
    But even watching arXiv can be overwhelming, so many people use the arXiv Sanity
    Preserver at [http://www.arxiv-sanity.com](http://www.arxiv-sanity.com) and the
    Semantic Sanity project at [https://s2-sanity.apps.allenai.org/cold-start](https://s2-sanity.apps.allenai.org/cold-start).
    Both sites help filter the repository for just those papers involving specific
    keywords and ideas.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅跟上新工作的步伐就可能是一份全职工作。保持对新进展的关注的一个方法是访问一个名为arXiv（读作“archive”）的网站，网址是[https://arxiv.org/](https://arxiv.org/)，更具体地说，是访问机器学习板块：[https://arxiv.org/list/cs.LG/recent](https://arxiv.org/list/cs.LG/recent)*。这个网站发布新论文的预印本，在论文正式出现在期刊和会议之前。但即使是关注arXiv，也可能让人感到信息量过大，因此，许多人使用arXiv
    Sanity Preserver网站（[http://www.arxiv-sanity.com](http://www.arxiv-sanity.com)）和Semantic
    Sanity项目（[https://s2-sanity.apps.allenai.org/cold-start](https://s2-sanity.apps.allenai.org/cold-start)）。这两个网站帮助过滤文库，只展示涉及特定关键词和思想的论文。
- en: In this book we’ve focused on the technical background of deep learning. It’s
    important to keep in mind that when we use these systems in ways that can affect
    people, we need to take into account much more than just algorithms (O’Neil 2016).
    Because of their efficiency, deep learning systems are being widely and rapidly
    deployed, often with little oversight or consideration of their impact on the
    societies they affect.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 本书重点介绍了深度学习的技术背景。需要记住的是，当我们以可能影响人的方式使用这些系统时，我们需要考虑的远不止算法本身（O'Neil 2016）。由于深度学习系统的高效性，它们被广泛且迅速地部署，通常缺乏监管或对其对社会影响的考虑。
- en: Deep learning systems are being used today to influence or determine job offers,
    school admissions, jail sentences, personal and business loans, and even the interpretation
    of medical tests. Deep learning systems control what people see in their news
    and social media feeds, choosing items not to build a healthy and well-informed
    society, but to increase the profits of the organizations delivering those feeds
    (Orlowski 2020). Deep learning systems in “smart speakers” and “smart displays”
    (which also contain microphones and cameras) listen and watch people in their
    homes without pause, sometimes sending their captured data up to a remote server
    for analysis. Cultures used to fear such constant surveillance, but now people
    pay for these devices and willingly put them in their previously private spaces,
    such as their homes and bedrooms. Deep learning is being used to single out potentially
    troublesome children in schools, evaluate the honesty of people answering questions
    at border crossings, and identify individuals at protests and other gatherings
    by recognizing their faces. Mistakes made by these algorithms can vary from annoying
    to fundamentally life-changing. Even when the results are not incorrect, these
    systems are increasingly making profound decisions that affect our public and
    private lives.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统如今被广泛应用于影响或决定工作录用、学校录取、监禁判决、个人和商业贷款，甚至是医学检测的解读。深度学习系统控制着人们在新闻和社交媒体中的信息流，选择展示的内容不是为了建立一个健康且信息丰富的社会，而是为了增加提供这些信息流的组织的利润（Orlowski
    2020）。在“智能音响”和“智能显示器”中（这些设备也配有麦克风和摄像头），深度学习系统不断监听和监视人们在家中的活动，有时将捕捉到的数据发送到远程服务器进行分析。过去，文化上曾经害怕这种持续的监视，但现在人们愿意为这些设备付费，并将它们放置在以前属于私人空间的地方，如家中和卧室里。深度学习被用于在学校中挑出可能有问题的孩子，评估在边境检查时回答问题的人的诚实性，并通过识别面部特征来识别抗议活动和其他集会中的个体。这些算法所犯的错误可能从令人烦恼到彻底改变人生。即使结果不完全错误，这些系统也越来越多地做出深远的决定，影响着我们的公共和私人生活。
- en: Deep learning systems are only as good as their training and their algorithms,
    and time after time, we find that biases, prejudices, and outright errors in the
    training data are perpetuated and even enforced by the resulting algorithms. Such
    systems fall far short of the kinds of accuracy and fairness we expect when dealing
    with humans and other living creatures. Our algorithms completely lack empathy
    and compassion. They have no sense of exceptional circumstances, and have no conception
    of the joy or suffering their decisions can cause. They cannot understand love,
    kindness, fear, hope, gratitude, sorrow, generosity, wisdom, or any of the other
    qualities that we value in one another. They cannot admire, cry, smile, grieve,
    celebrate, or regret.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统的效果取决于其训练和算法的质量，一次又一次，我们发现训练数据中的偏见、成见和明显错误被算法继承并加以强化。此类系统远远无法达到我们在与人类和其他生物打交道时所期待的准确性和公平性。我们的算法完全缺乏同情心和怜悯心。它们没有对特殊情况的理解，也无法理解它们的决策可能带来的喜悦或痛苦。它们无法理解爱、善良、恐惧、希望、感恩、悲伤、慷慨、智慧，或我们彼此之间所珍视的其他品质。它们无法崇拜、哭泣、微笑、悲痛、庆祝或后悔。
- en: Deep learning holds great promise to help us as individuals and societies. But
    any tool can be used as a weapon, benefiting the owners of that tool to the disadvantage
    of those affected. Machine learning systems are often effectively invisible, so
    any systematic errors can go undetected for long periods. Even once problems are
    uncovered, it can require enormous social and political action to hold accountable
    the organizations that benefit by selling or using these systems, and even more
    effort to bring about change.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习为我们个人和社会提供了巨大的希望。但任何工具都可以被当作武器使用，利益仅仅归工具的拥有者，而对受影响的人则带来不利。机器学习系统往往是有效隐形的，因此任何系统性的错误可能会长期未被发现。即使问题被揭露，追究受益于这些系统的组织的责任也需要巨大的社会和政治行动，甚至更大的努力才能带来改变。
- en: Another peril of machine learning systems is their insatiable need for enormous
    amounts of training data. This creates a market for organizations that do nothing
    but collect, organize, and sell previously private information about people’s
    lives, from their friendships and family relationships to where and when they
    like to travel, what food they like to eat, what medications they’re taking, and
    what their DNA reveals about them. This data can be used to harass, intimidate,
    threaten, and harm individuals.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统的另一个危险在于它们对大量训练数据的贪婪需求。这为那些专门收集、整理和出售曾经私密的个人信息的组织创造了市场，这些信息涉及个人的友谊和家庭关系，以及他们喜欢去哪里和什么时候旅行，喜欢吃什么食物，正在服用什么药物，以及他们的DNA揭示了什么。这些数据可以用来骚扰、威胁、恐吓甚至伤害个人。
- en: The demand for massive quantities of data also means that as an organization
    grows larger (and often less accountable), the more data it can gather, and the
    more powerful its algorithms grow, making their decisions more influential, which
    means they can gather more data, and thereby solidify the organization’s power
    in a feedback loop. Every imperfection in such a system becomes magnified, and
    because of their scale, negative effects on individuals can go entirely unnoticed
    by the people running these systems. In general, the only competition to such
    organizations will come from other organizations of equivalent size, offering
    systems with their own enormous databases containing their own biases and errors,
    leading to the battles of the biased behemoths we’re familiar with today. This
    continuous and increasing concentration of power is a dangerous force in a free
    society when not subject to significant and strongly enforced control and regulation.
    Sadly, such controls are rarely to be seen today.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对大量数据的需求也意味着，随着一个组织的规模变大（且通常责任感变弱），它能收集的数据就越多，其算法也就越强大，决策的影响力也随之增加，这意味着它们可以收集更多的数据，从而在反馈循环中巩固组织的权力。这样系统中的每一个不完美都会被放大，并且由于其规模庞大，负面影响可能完全被运营这些系统的人忽视。通常，唯一能够与这些组织竞争的将是其他同样规模的组织，它们提供的系统包含着自己的巨大数据库，内含着各自的偏见和错误，最终导致我们今天所熟悉的偏见巨头之间的对抗。在没有强有力和严格执行的控制和监管的情况下，这种持续和日益集中的权力在自由社会中是一股危险的力量。不幸的是，今天这样的控制几乎无法看到。
- en: Deep learning has produced algorithms that make it possible to take any person’s
    appearance, from an entertainer to a politician, and produce images, audio, and
    video that seem to realistically portray that person saying or doing anything
    the person wielding the software desires. Societies have come to rely on captured
    audio, photographs, and video to enforce contracts, reconcile conflicts, exalt
    or shame a public person, influence elections, and serve as evidence in courts.
    The end of that era is very near, returning us to a time before reliable photographs,
    recordings, and video, when hearsay, memory, and opinion replace objective historical
    recordings. Without reliable audio and visual evidence documenting what someone
    actually said or did, the loudest, richest, or most persuasive voices in the room
    will determine many outcomes in public opinion, elections, and courts of law,
    because objective facts will be increasingly harder to find or trust.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已经产生了能够让任何人的外貌——从娱乐圈人士到政治家——都可以被用来制作出看似真实的图片、音频和视频，仿佛那个被操控的人说或做了任何操控软件的人所希望的事情。社会已经开始依赖录制的音频、照片和视频来执行合同、调解冲突、赞美或羞辱公众人物、影响选举，以及作为法庭证据。这个时代即将结束，我们将回到一个没有可靠照片、录音和视频的时代，那时流言、记忆和观点将取代客观的历史记录。如果没有可靠的音频和视觉证据来记录某人实际说了什么或做了什么，那么在场中最响亮、最富有或最有说服力的声音将决定公共舆论、选举和法庭中的许多结果，因为客观事实将越来越难以被找到或信任。
- en: Deep learning is a fascinating field, and we’re just starting to understand
    how it will affect our culture and society. Learning algorithms will surely continue
    to grow in scope and impact. They have the chance to create enormous good by helping
    people be happier, enabling societies to be more fair and supportive, and creating
    healthier and more diverse personal, social, political, and physical environments.
    It’s important to strive for these positive outcomes, even when they curtail corporate
    profits or governmental control.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一个令人着迷的领域，我们才刚刚开始理解它将如何影响我们的文化和社会。学习算法无疑会继续扩展其范围和影响力。它们有机会通过帮助人们更幸福、使社会更加公平和支持性强、创造更健康和多样化的个人、社会、政治和物理环境，带来巨大的好处。即使这些可能会削减企业利润或政府控制，我们仍然应该努力追求这些积极的成果。
- en: We should remember to always use deep learning, like all of our tools, to bring
    out the best in humanity, and make the world a better place for everyone.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应当记住，像使用我们所有工具一样，深度学习也应该用来激发人类的最佳潜力，让世界变得对每个人来说更加美好。
