<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="473" id="Page_473"/>17</span><br/>
<span class="ChapterTitle">Convnets in Practice</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">In the last chapter we discussed convolution, and we wrapped up with a simplified example of a convolutional network, or convnet.</p>
<p>In this chapter, we look at two real convnets designed for image classification. The first identifies grayscale handwritten digits, and the second identifies what object is dominant in a color photograph, choosing from 1,000 different categories.</p>
<h2 id="h1-500723c17-0001">Categorizing Handwritten Digits</h2>
<p class="BodyFirst">Categorizing handwritten digits is a famous problem in machine learning (LeCun et al. 1989), thanks to a freely available dataset called MNIST (pronounced em´-nist). It contains 60,000 hand-drawn digits from 0 to 9, each a grayscale picture rendered in white on a 28 by 28 black background, with a label identifying the digit. The drawings were collected from census takers and students. Our job is to identify the digit in each image.</p>
<p><span epub:type="pagebreak" title="474" id="Page_474"/>We will use a simple convnet designed for this job that is included with the Keras machine learning library (Chollet 2017). <a href="#figure17-1" id="figureanchor17-1">Figure 17-1</a> shows the architecture in our schematic form and in the traditional box-and-label form.</p>
<figure>
<img src="Images/F17001.png" alt="F17001" width="844" height="431"/>
<figcaption><p><a id="figure17-1">Figure 17-1</a>: A convnet for classifying MNIST digits. The input images are 28 by 28 by 1 channel. Two convolution layers are followed by pooling, dropout, and flatten, then a dense (or fully connected) layer, another dropout, and a final dense layer with 10 outputs followed by softmax. Top: Our schematic version. Bottom: Traditional box-and-label form.</p></figcaption>
</figure>
<p>The input to the net is the MNIST image, provided as a 3D tensor of shape 28 by 28 by 1 (the 1 refers to the single grayscale channel). Though there are two fully connected layers at the end, and various helper layers (such as dropout, flatten, and pooling), we still refer to this as a convolutional network, or convnet, because the convolution layers dominate the classification work. The first convolution layer runs 32 filters, each of size 3 by 3, over the input. Each filter’s output is run through a ReLU activation function before it leaves the layer.</p>
<p>By not specifying a stride, the filters will move by one element in each direction. We’re also not applying any padding. As we saw in <a href="c16.xhtml#figure16-10" id="figureanchor16-10">Figure 16-10</a>, this means that we lose a ring of elements after each convolution. That’s okay in this case because all MNIST images are supposed to have a border of four black pixels around the digit (not all images actually have this border, but most do).</p>
<p>The first layer’s input tensor is 28 by 28 by 1, so each filter in the first convolution layer is one channel deep. Because we have 32 filters, we don’t have any padding on the input, and the filters have a 3 by 3 footprint, the output of the first convolution layer is 26 by 26 by 32. The second <span epub:type="pagebreak" title="475" id="Page_475"/>convolution layer contains 64 filters with 3 by 3 footprints. The system knows that the input has 32 channels (because the previous layer had 32 filters), so each filter is created as a tensor of shape 3 by 3 by 32. Because we’re still not using padding, we again lose a ring around the outside of the input, producing an output tensor that’s 24 by 24 by 64.</p>
<p>We could have used striding to reduce the size of the output, but here we use an explicit max pooling layer with blocks of size 2 by 2. That means for every nonoverlapping 2 by 2 block in the input, the layer outputs just one value containing the largest value in the block. Thus, the output of this layer is a tensor of size 12 by 12 by 64 (the pooling doesn’t change the number of channels).</p>
<p>Next, we come to a dropout layer, represented by a diagonal slash. As we saw in Chapter 15, the dropout layer itself doesn’t actually do any processing. Instead, it instructs the system to apply dropout to the nearest preceding layer that contains neurons. The nearest layer preceding the dropout is pooling, but that has no neurons. As we continue to work backward, we find a convolution layer, which does have neurons. During training, the dropout algorithm is applied to this convolution layer (recall that dropout is only applied during training, and is otherwise ignored). Before each epoch of training, one-quarter of the neurons in this convolution layer are temporarily disabled. This should help hold off overfitting. By convention, we usually treat dropout as a layer, even though it does no computation. Note that since the dropout layer looks backward for the nearest layer with neurons, we could have placed it to the left of the pooling layer and nothing about the network would have changed. By convention, when we pool after convolution, we usually place those two layers together.</p>
<p>Now we leave the convolutional part of the network and prepare the values for output. We typically find these steps, or something like them, at the end of classification convnets. The output of the second convolution layer is a 3D tensor, but we want to feed that into a fully connected layer, which expects a list (or 1D tensor). A <em>flatten</em> layer, shown as two parallel lines, takes an input tensor of any number of dimensions and reorganizes it into a 1D tensor by placing all the elements together end-to-end. The list is made up starting with the first row in the tensor. We take the first element, and place its 64 values at the head of our list. Then we move to the second element, and place its 64 values at the end of the list. We continue doing this for every element in the row, and then we do it for the next row, and so on. <a href="#figure17-2" id="figureanchor17-2">Figure 17-2</a> shows the process. None of the values in the tensor are lost in this rearrangement. </p>
<span epub:type="pagebreak" title="476" id="Page_476"/><figure>
<img src="Images/F17002.png" alt="F17002" width="694" height="566"/>
<figcaption><p><a id="figure17-2">Figure 17-2</a>: The action of a flatten layer. Top: The input tensor. Middle: Turning each channel into a list. Bottom: Placing the lists one after the other to make one large list.</p></figcaption>
</figure>
<p>Returning to <a href="#figure17-1">Figure 17-1</a>, the flatten layer produces a list of 12 × 12 × 64 = 9,216 numbers. That list goes into a fully connected, or dense, layer of 128 neurons. That layer gets affected by dropout, where a quarter of the neurons are temporarily disconnected at the start of each batch during training.</p>
<p>The 128 outputs of this layer go into a final dense layer with 10 neurons. The 10 outputs of this layer go into a softmax step so that they’re converted to probabilities. The 10 numbers that come out of this last layer give us the network’s prediction of the probability that the input image belongs to each of the 10 possible classes, corresponding to the digits 0 through 9.</p>
<p>We trained this network for 12 epochs using the standard MNIST training data. Its accuracy on the training and validation data sets is shown in <a href="#figure17-3" id="figureanchor17-3">Figure 17-3</a>.</p>
<p>The curves show we’ve achieved about 99 percent accuracy on both the training and validation data sets. Since the curves aren’t diverging, we’ve successfully avoided overfitting.</p>
<span epub:type="pagebreak" title="477" id="Page_477"/><figure>
<img src="Images/f17003.png" alt="f17002" width="750" height="505"/>
<figcaption><p><a id="figure17-3">Figure 17-3</a>: The training performance of our convnet in <a href="#figure17-2">Figure 17-2</a>. We trained for 12 epochs, and since the training and validation curves are not diverging, we’ve successfully avoided overfitting, while reaching about 99 percent accuracy on both data sets.</p></figcaption>
</figure>
<p>Let’s look at some predictions. <a href="#figure17-4" id="figureanchor17-4">Figure 17-4</a> shows some images from the MNIST validation set, labeled by the digit that the network gave the largest probability to. On this little set of examples, it did a perfect job.</p>
<figure>
<img src="Images/f17004.png" alt="f17004" width="694" height="256"/>
<figcaption><p><a id="figure17-4">Figure 17-4</a>: These are 24 randomly chosen images from the MNIST validation set. Each image is labeled with the output of the network, showing the digit with the highest probability. The network classified all 24 of these digits correctly.</p></figcaption>
</figure>
<p>Just two convolution layers gave this system enough power to achieve 99 percent accuracy.</p>
<h2 id="h1-500723c17-0002"><span epub:type="pagebreak" title="478" id="Page_478"/>VGG16</h2>
<p class="BodyFirst">Let’s look at a bigger and more powerful convnet, called <em>VGG16</em>. It was trained to analyze color photographs and identify the dominant object in each photo by assigning probabilities to 1,000 different classes. </p>
<p>VGG16 was trained on a famous dataset that was used as part of a contest. The ILSVRC2014 competition was a public challenge in 2014. The goal was to build a neural network for classifying photos in a provided database of images (Russakovsky et al. 2015). The acronym ILSVRC stands for ImageNet Large Scale Visual Recognition Challenge, so the database of pictures is often called the ImageNet database. The ImageNet photo database is freely available online and is still widely used for training and testing new networks (newer, bigger versions of ImageNet are also available [Fei-Fei et al. 2020]).</p>
<p>The original ImageNet database contained 1.2 million images, each manually labeled with one of 1,000 labels, describing the object most prominent in the photo. The challenge actually included several subchallenges, each with its own winners (ImageNet 2020). The winner of one of the classification tasks was VGG16 (Simonyan and Zisserman 2014). VGG is an acronym for the Visual Geometry Group, who developed the system. The 16 refers to the network’s 16 computational layers (there are also some utility layers, such as dropout and flatten, that don’t do computation). </p>
<p>VGG16 broke records for accuracy when it won the contest, and even though years have passed, it remains popular. This is largely because it still does very well at classifying images (even compared to newer, more sophisticated systems), and it has a simple structure that’s easy to modify and experiment with. The authors have released all the weights and how they preprocessed the training data. Even better, every deep learning library makes it easy to create a fully trained instance of VGG16 in our own code. Thanks to all of these qualities, VGG16 is a frequent starting point for projects that involve image classification.</p>
<p>Let’s look at the VGG16 architecture. Most of the work is done by a series of convolution layers. Utility layers appear along the way, and some flattening and fully connected layers appear at the very end, as they did in <a href="#figure17-1">Figure 17-1</a>.</p>
<p>Before we feed any data to our model, we must preprocess it in the same way that the authors preprocessed their training data. That involves making sure that each channel has been adjusted by subtracting a specific value from all of its pixels (Simonyan and Zisserman 2014). To better discuss the shapes of the tensors flowing through the network, let’s assume each input image has a height and width of 224 to match the dimensions of the Imagenet data the network was trained on, and its colors have been correctly pre-processed. Once that’s done, we’re ready to feed our image to the network.</p>
<p>We will present the VGG16 architecture as a series of six groups of layers. These groups are strictly conceptual and are just a way of gathering together related layers for our discussion. The first few groups have the same structure: two or three layers of convolution followed by a pooling layer.</p>
<p>Group 1 is shown in <a href="#figure17-5" id="figureanchor17-5">Figure 17-5</a>.</p>
<span epub:type="pagebreak" title="479" id="Page_479"/><figure>
<img src="Images/f17005.png" alt="f17005" width="467" height="123"/>
<figcaption><p><a id="figure17-5">Figure 17-5</a>: Group 1 of VGG16. We convolve the input tensor with 64 filters each of size 3 by 3. Then we convolve again with 64 new filters. Finally, we use max pooling to reduce the output tensor’s height and width by half.</p></figcaption>
</figure>
<p>The convolutions both apply zero padding to their inputs so there’s no loss in width or height. The max pooling step uses nonoverlapping blocks of size 2 by 2.</p>
<p>All of the convolution layers in VGG16 use the default ReLU activation function.</p>
<p>We’ve seen how useful pooling is for helping our filters recognize patterns even if they’ve been displaced. For the same reasons that we used pooling when matching masks in Chapter 16, we apply pooling here, too.</p>
<p>The output of the group in <a href="#figure17-5">Figure 17-5</a> is a tensor of dimensions 112 by 112 by 64. The values of 112 come from the input dimensions of 224 by 224 that have been halved, and the 64 results from the 64 filters in the second convolution layer.</p>
<p>Group 2 is just like the first, only now we apply 128 filters in each convolution layer. <a href="#figure17-6" id="figureanchor17-6">Figure 17-6</a> shows the layers. The output of this group has size 56 by 56 by 128.</p>
<figure>
<img src="Images/f17006.png" alt="f17006" width="364" height="123"/>
<figcaption><p><a id="figure17-6">Figure 17-6</a>: Group 2 of VGG16 is just like the first block in <a href="#figure17-5">Figure 17-5</a>, except that we use 128 filters in each convolution layer rather than 64.</p></figcaption>
</figure>
<p>Group 3 continues the pattern of doubling the number of filters in each convolution layer, but it repeats the convolution step three times instead of twice. <a href="#figure17-7" id="figureanchor17-7">Figure 17-7</a> shows Group 3. The tensor after the max pooling step has size 28 by 28 by 256.</p>
<figure>
<img src="Images/f17007.png" alt="f17007" width="500" height="124"/>
<figcaption><p><a id="figure17-7">Figure 17-7</a>: Group 3 of VGG16 doubles the number of filters again to 256 and repeats the convolution step three times rather than two as before.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="480" id="Page_480"/>Groups 4 and 5 of the network are the same. Each group is built from three steps of convolution with 512 filters, followed by a max pooling layer. The structure of these layers is shown in <a href="#figure17-8" id="figureanchor17-8">Figure 17-8</a>. The tensor coming out of Group 4 has size 28 by 28 by 512, and the tensor after the max pooling layer in Group 5 has dimensions 14 by 14 by 512.</p>
<figure>
<img src="Images/f17008.png" alt="f17008" width="502" height="113"/>
<figcaption><p><a id="figure17-8">Figure 17-8</a>: Groups 4 and 5 of VGG16 are the same. They each have three convolution layers, followed by a two by two max pooling layer.</p></figcaption>
</figure>
<p>This ends the convolution part of the network, and now we come to the wrap-up. As with the MNIST classifier we saw in <a href="#figure17-1">Figure 17-1</a>, we first flatten the tensor coming out of Group 5. We then run it through two dense layers of 4,096 neurons, each using ReLU, and each followed by dropout with an aggressive setting of 50 percent. Finally, the output goes into a dense layer with 1,000 neurons. The results are fed to softmax, which produces our output of 1,000 probabilities, one for each class that VGG16 was trained to recognize. These final steps, which are typical for classification networks of this style, are shown in <a href="#figure17-9" id="figureanchor17-9">Figure 17-9</a>.</p>
<figure>
<img src="Images/f17009.png" alt="f17009" width="538" height="110"/>
<figcaption><p><a id="figure17-9">Figure 17-9</a>: The final steps of processing in VGG16. We flatten the image, then run it through two dense layers each using ReLU, followed by dropout, then through a dense layer with softmax. </p></figcaption>
</figure>
<p><a href="#figure17-10" id="figureanchor17-10">Figure 17-10</a> shows the whole architecture in one place.</p>
<figure>
<img src="Images/f17010.png" alt="f17010" width="841" height="321"/>
<figcaption><p><a id="figure17-10">Figure 17-10</a>: The VGG16 architecture in one place</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="481" id="Page_481"/>This network works very well. <a href="#figure17-11" id="figureanchor17-11">Figure 17-11</a> shows four pictures shot around Seattle on a phone’s camera. </p>
<figure>
<img src="Images/f17011.png" alt="f17011" width="705" height="613"/>
<figcaption><p><a id="figure17-11">Figure 17-11</a>: Four photos shot around Seattle on a sunny day. The convnet of <a href="#figure17-10">Figure 17-10</a> does a great job of identifying each image.</p></figcaption>
</figure>
<p>The convnet has never seen these images, but it does a great job with them. Even the ambiguous round object in the upper right is assigned sensible labels.</p>
<p>Let’s take a closer look at what’s going on inside VGG16 by looking at its filters.</p>
<h2 id="h1-500723c17-0003">Visualizing Filters, Part 1</h2>
<p class="BodyFirst">VGG16’s success in classifying is due to the filters that were learned by its convolution layers. It’s tempting to look at the filters and see what they’ve learned, but the filters themselves are big blocks of numbers, which are hard for us to interpret. Instead of trying to somehow make sense of a block of numbers, we can visualize our filters indirectly by creating images that trigger them. In other words, once we’ve selected a filter we want to visualize, we can find a picture that causes that filter to output its biggest value. That picture shows us what that filter is looking for.</p>
<p><span epub:type="pagebreak" title="482" id="Page_482"/>We can do this with a little trick based on gradient descent, the algorithm that we saw in Chapter 14 as part of backpropagation. We flip gradient descent around to create gradient <em>ascent</em>, which we use to climb up the gradient and increase the system’s error. Remember from Chapter 14 that during training, we use the system’s error to create gradients that we push backward through the network with backprop, enabling us to change the weights in order to reduce that error. For filter visualization, we’re going to ignore the network’s output and its error entirely. The only output we care about is the feature map that comes out of the particular filter (or neuron) we want to visualize. We know that when the filter sees information that it’s looking for, it produces a big output, so if we add up all of the output values of that filter for a given input image, it tells us how much of what the filter is looking for is in that image. We can use the sum of all the values in the feature map as a replacement for the network’s error.</p>
<p><a href="#figure17-12" id="figureanchor17-12">Figure 17-12</a> shows the idea.</p>
<figure>
<img src="Images/f17012.png" alt="f17012" width="833" height="299"/>
<figcaption><p><a id="figure17-12">Figure 17-12</a>: Visualizing a filter. The sum of all the values in the feature map serves as the network’s error.</p></figcaption>
</figure>
<p>We’re using VGG16, but for this visualization process we leave off the layers after the last convolution. We feed in a grid of random numbers and extract the filter map for the filter we want to visualize. That becomes our error. Now comes the tricky part: we use this error to compute the gradients, but we don’t adjust the weights at all. The network itself and all of its weights are <em>frozen</em>. We just keep computing the gradients and pushing them back until we reach the input layer, which holds the pixel values of the input image. The gradients that arrive at this layer tell us how to change those pixel values to decrease the error, which we know is the filter’s output. Since we want to stimulate the neuron as much as we can, we want the “error” to be as big as possible, so we change the pixel values to increase, rather than decrease, this error. That makes the picture stimulate our selected neuron a little more than it did before.</p>
<p>After doing this over and over, we will have adjusted our initially random pixel values so that they’re making the filter output the biggest values we can get it to produce. When we look at the input after it’s been modified <span epub:type="pagebreak" title="483" id="Page_483"/>in this way, we see a picture that makes that neuron produce a huge output, so the picture shows us what the filter is looking for (or at least gives us a general idea) (Zeiler and Fergus 2013). We will use this visualization process again in Chapter 23 when we look at the deep dreaming algorithm.</p>
<p>Because we start with random values in the input image, we get a different final image every time we run this algorithm. But each image we make is roughly like the others, since they’re all based on maximizing the output of the same filter.</p>
<p>Let’s look at some images produced by this method. <a href="#figure17-13" id="figureanchor17-13">Figure 17-13</a> shows pictures produced for the 64 filters in the second convolution layer in the first block, or group, of VGG16 (we use the label <code>block1_conv2</code> for this layer and similar names for the other layers we look at). In <a href="#figure17-13">Figure 17-13</a> and the others like it to come, we’ve enhanced the color saturation to make the results easier to interpret. </p>
<figure>
<img src="Images/f17013.png" alt="f17013" width="691" height="691"/>
<figcaption><p><a id="figure17-13">Figure 17-13</a>: Images that get the biggest response from each of the 64 filters in the <span class="LiteralInCaption"><code>block1_conv2</code></span> layer of VGG16</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="484" id="Page_484"/>It seems that a lot of these layers are looking for edges in different orientations. Some have values that are too subtle for us to interpret easily. </p>
<p>Let’s move forward to block 3, and look at the first 64 filters from the first convolution layer there. <a href="#figure17-14" id="figureanchor17-14">Figure 17-14</a> shows images that stimulate these filters the most.</p>
<figure>
<img src="Images/f17014.png" alt="f17014" width="692" height="692"/>
<figcaption><p><a id="figure17-14">Figure 17-14</a>: Images that get the biggest response from the first 64 filters in the <span class="LiteralInCaption"><code>block3_conv1</code></span> layer of VGG16</p></figcaption>
</figure>
<p>Now we’re talking! As we’d expect, the filters here are looking for more complex textures, combining the simpler patterns found by prior layers. Let’s move farther along and look at the first 64 filters from the first convolution layer of block 4, in <a href="#figure17-15" id="figureanchor17-15">Figure 17-15</a>.</p>
<span epub:type="pagebreak" title="485" id="Page_485"/><figure>
<img src="Images/f17015.png" alt="f17015" width="694" height="694"/>
<figcaption><p><a id="figure17-15">Figure 17-15</a>: Images that get the biggest response from the first 64 filters in the <span class="LiteralInCaption"><code>block4_conv1</code></span> layer of VGG16</p></figcaption>
</figure>
<p>These are fascinating glimpses into what VGG16 has learned. We can see some of the structures it has found to be useful in order to classify the object in an image. The filters seem to be hunting for patterns that involve a lot of different kinds of flowing and interlocking textures like those we’d find on animals and other surfaces in the world around us. </p>
<p>We can really see the value of the convolution hierarchy here. Each layer of convolution looks for patterns in the output of the previous layer, letting us work our way up from low-level details like stripes and edges to complex and rich geometrical structures.</p>
<p><span epub:type="pagebreak" title="486" id="Page_486"/>Just for fun, let’s look at close-ups of a few of these filters. <a href="#figure17-16" id="figureanchor17-16">Figure 17-16</a> shows larger views of nine patterns from the first few layers.</p>
<figure>
<img src="Images/f17016.png" alt="f17016" width="694" height="694"/>
<figcaption><p><a id="figure17-16">Figure 17-16</a>: Close-ups of some manually selected images that triggered the largest filter responses from the first few layers of VGG16</p></figcaption>
</figure>
<p><a href="#figure17-17" id="figureanchor17-17">Figure 17-17</a> shows patterns that triggered big responses from filters in the last few layers.</p>
<p>These patterns are exciting and beautiful. They also have an organic feeling about them, probably because the ImageNet database contains many images of animals.</p>
<span epub:type="pagebreak" title="487" id="Page_487"/><figure>
<img src="Images/f17017.png" alt="f17017" width="694" height="694"/>
<figcaption><p><a id="figure17-17">Figure 17-17</a>: Close-ups of some manually selected images that triggered the largest filter responses from the last few layers of VGG16</p></figcaption>
</figure>
<h2 id="h1-500723c17-0004">Visualizing Filters, Part 2</h2>
<p class="BodyFirst">Another way to visualize a filter is to run an image through VGG16, and look at the feature map produced by that filter. That is, we feed an image to VGG16 and let it run through the network, but as before, we ignore the network’s output. Instead, we extract the feature map for the filter we’re interested in, and draw it like a picture. This is possible because each feature map always has a single channel, so we can draw it as a grayscale image.</p>
<p>Let’s give it a spin. <a href="#figure17-18" id="figureanchor17-18">Figure 17-18</a> shows our input image of a drake, or male duck. This is the starting image for all of our visualizations in this section.</p>
<span epub:type="pagebreak" title="488" id="Page_488"/><figure>
<img src="Images/f17018.png" alt="f17018" width="468" height="468"/>
<figcaption><p><a id="figure17-18">Figure 17-18</a> The drake image that we use to visualize filter outputs</p></figcaption>
</figure>
<p>To get a feeling for things, <a href="#figure17-19" id="figureanchor17-19">Figure 17-19</a> shows the response from the very first filter on the very first convolution layer of the network. Since the output of a filter has just one channel, we can draw it in grayscale. We’ve chosen to instead use a heatmap from black to reds to yellow.</p>
<figure>
<img src="Images/f17019.png" alt="f17019" width="572" height="473"/>
<figcaption><p><a id="figure17-19">Figure 17-19</a>: The response of filter 0 in layer <span class="LiteralInCaption"><code>block1_conv1</code></span> in VGG16 to the duck image in <a href="#figure17-18">Figure 17-18</a></p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="489" id="Page_489"/>This filter is looking for edges. Consider the tail in the lower right. An edge that’s light on top and darker below gets a very large output from the filter, whereas an edge in the other direction gets a very low output. Less extreme changes cause smaller outputs, and regions of constant color have middling outputs.</p>
<p><a href="#figure17-20" id="figureanchor17-20">Figure 17-20</a> shows the responses from the first 32 filters in the first convolution layer of the first block.</p>
<figure>
<img src="Images/f17020.png" alt="f17020" width="694" height="379"/>
<figcaption><p><a id="figure17-20">Figure 17-20</a>: The responses of the first 32 filters in VGG convolution layer <span class="LiteralInCaption"><code>block1_conv1</code></span></p></figcaption>
</figure>
<p>A lot of these filters seem to be looking for edges, but others seem to be looking for particular features of the image. Let’s look at close-ups of 8 manually selected filters chosen from all 64 of the filters on this layer, shown in <a href="#figure17-21" id="figureanchor17-21">Figure 17-21</a>.</p>
<figure>
<img src="Images/f17021.png" alt="f17021" width="694" height="338"/>
<figcaption><p><a id="figure17-21">Figure 17-21</a>: Close-ups of eight manually chosen filter responses from VGG16’s first convolution layer, <span class="LiteralInCaption"><code>block1_</code></span><span class="LiteralInCaption"><code>conv1</code></span></p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="490" id="Page_490"/>The third image in the top row seems to be looking for the duck’s feet, or maybe it’s just interested in bright orange things. The left-most image in the bottom row looks like it’s searching for the waves and sand behind the duck, though the image to its right appears to be responding most to the blue waves. Some more experimentation with other inputs would help us nail down these interpretations, but it’s fun to see how much we can guess from a single image.</p>
<p>Let’s move farther into the network, out to the third block of convolution layers. The outputs here are smaller by a factor of four on each side than those coming out of the first block because they’ve gone through two pooling layers. We expect that they are looking for clusters of features. <a href="#figure17-22" id="figureanchor17-22">Figure 17-22</a> shows the responses for the first convolution layer in block 3.</p>
<figure>
<img src="Images/f17022.png" alt="f17022" width="694" height="380"/>
<figcaption><p><a id="figure17-22">Figure 17-22</a>: The responses of the first 32 filters in the VGG convolution layer <span class="LiteralInCaption"><code>block3_conv1</code></span></p></figcaption>
</figure>
<p>It’s interesting that a lot of edge finding still seems to be going on. This suggests that strong edges are an important cue for VGG16 as it works to figure out what an image is showing, even in the third set of convolutions. But lots of other regions are also bright.</p>
<p>Let’s jump all the way to the last block. <a href="#figure17-23" id="figureanchor17-23">Figure 17-23</a> shows the responses for the first 32 filters for the first convolution layer in block 5.</p>
<span epub:type="pagebreak" title="491" id="Page_491"/><figure>
<img src="Images/f17023.png" alt="f17023" width="694" height="378"/>
<figcaption><p><a id="figure17-23">Figure 17-23</a>: Filter responses for the first 32 filters in VGG convolution layer <span class="LiteralInCaption"><code>block5_conv1</code></span></p></figcaption>
</figure>
<p>As we’d expect, these images are even smaller, having passed through two more pooling layers that each reduce the size by a factor of two on each side. At this point, the duck is hardly visible because the system is combining features from the previous layers. Some of the filters are barely responding. They are probably responsible for finding high-level features that aren’t present in the duck image.</p>
<p>In Chapter 23 we’ll look at a couple of creative applications that use the filter responses in a convnet.</p>
<h2 id="h1-500723c17-0005">Adversaries</h2>
<p class="BodyFirst">Although VGG16 does very well at predicting the correct label for many images, we can change an image in ways so small that they’re undetectable to the human eye, but that fools the classifier into assigning the wrong label. In fact, this process can mess up the results of any convolution-based classifier.</p>
<p>The trick to fooling a convnet involves creating a new image called an <em>adversary</em>. This image is created from the starting image by adding an <em>adversarial perturbation</em> (or more simply, a <em>perturbation</em>). The perturbation is another image, the same size as the image we want to classify, typically with very small values. If we add the perturbation to our original image, the changes are usually so small that most people can’t detect any difference, even in the finest details. But if we ask VGG16 to classify the perturbed image, it gives us the wrong answer. Sometimes we can find a single perturbation that messes up the results for every image we give to a particular classifier, which we call a <em>universal perturbation</em> (Moosavi-Dezfooli et al. 2016).</p>
<p><span epub:type="pagebreak" title="492" id="Page_492"/>Let’s see this in action. On the left of <a href="#figure17-24" id="figureanchor17-24">Figure 17-24</a> we see an image of a tiger. All of the pixel values in this image are between 0 and 255. The system correctly classifies it as a tiger with about 80 percent confidence, with smaller confidences for related animals such as a tiger cat and a jaguar.</p>
<figure>
<img src="Images/f17024.png" alt="f17024" width="694" height="523"/>
<figcaption><p><a id="figure17-24">Figure 17-24</a>: An adversarial attack on an image. Left: The input and VGG16’s top five classes. Middle: The adversarial image, where the pixel values are in the range of about [–2, 2], but shown here scaled to the range [0, 255] so they can be seen. Right: The result of adding the image and the original (unscaled) adversary together, and the new top five classes.</p></figcaption>
</figure>
<p>In the middle of <a href="#figure17-24">Figure 17-24</a> we show an image computed by an algorithm designed to find adversaries. All of the values in this image are about in the range [–2, 2], but for this figure, we scaled the values to the range [0, 255] so they’d be easier to see. In the top right of <a href="#figure17-23">Figure 17-23</a> we show the result of adding the tiger and the adversary, so each of the original tiger’s pixels is changed by a value within the range [–2, 2]. To our eyes, the tiger seems unchanged. Even the thin whiskers look the same. Below that image are VGG16’s top five predictions for this new image. The system comes up with completely different predictions for the image, none of which come anywhere close to the correct class. Except for the low-probability class of brain coral, the system doesn’t even think this image is an animal.</p>
<p>The perturbation image in <a href="#figure17-24">Figure 17-24</a> may look random to our eyes, but it’s not. This picture was specifically computed to throw off VGG16’s prediction for the image of the tiger.</p>
<p>There are many different ways to compute adversarial images (Rauber, Brendel, and Bethge 2018). The range of values in the perturbations these <span epub:type="pagebreak" title="493" id="Page_493"/>methods create for a given image can vary considerably, so to find the smallest perturbation, it’s often worth trying a few different methods, also called <em>attacks</em>. We can compute adversaries to achieve different goals (Rauber and Brendel 2017b). For example, we can ask for a perturbation that simply causes the input to be misclassified. Another option asks for a perturbation that causes the input to be classified as a specific, desired class. To make <a href="#figure17-24">Figure 17-24</a>, we used an algorithm that is designed to make the classifier’s top seven predictions much more unlikely. That is, it takes in the starting image and the top seven predictions from the classifier and produces an adversary. When we add the adversary to the input and hand that to the classifier, none of its new top seven predictions contain any of the previous top seven predictions.  </p>
<p>We have to carefully construct adversarial perturbations, which suggests that they’re exploiting something subtle in our convnets.</p>
<p>We may find a way to build convnets that resist these attacks, but convolutional networks may be inherently vulnerable to these subtle image manipulations (Gilmer et al. 2018). The existence of adversaries suggests that convnets still hold surprises for us, and they shouldn’t be considered foolproof. There’s more to be learned about what’s going on inside of convolutional networks.</p>
<h2 id="h1-500723c17-0006">Summary</h2>
<p class="BodyFirst">In this chapter we looked at a couple of real convnets: a small one for classifying handwritten MNIST digits and the larger VGG16 network for classifying photos. Though our MNIST network was quite small, it was able to classify digits with about 99 percent accuracy.</p>
<p>We looked at the structure of VGG16, and then two different types of visualizations of its filters. We saw that the filters in this network start by looking for simple structures like edges and build up to complex and beautiful organic patterns. </p>
<p>Finally, we saw that convolutional networks used as image classifiers are susceptible to being fooled by adjusting the pixel values by tiny amounts that are imperceptible to a human observer.</p>
<p>In the next chapter we’ll look at how to build networks that figure out how to compress an input into a much smaller representation and then expand that again to produce something close to the original.</p>
</section>
</div></body></html>