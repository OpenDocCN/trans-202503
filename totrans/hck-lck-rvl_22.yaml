- en: <hgroup>
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">B</samp> <samp class="SANS_Dogma_OT_Bold_B_11">SCRAPING
    THE WEB</samp>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: </hgroup>
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, in order to research important data publicly available online, you’ll
    need to download a local copy. When websites don’t provide this data in structured
    downloadable formats like spreadsheets, JSON files, or databases, you can make
    your own copy using *web scraping* (or *screen scraping*): writing code that loads
    web pages for you and extracts their contents. These might include social media
    posts, court documents, or any other online data. You can use web scraping to
    download either full datasets or the same web page again and again on a regular
    basis to see if its content changes over time.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider the Parler dataset discussed in [Chapter 11](chapter11.xhtml).
    Before Parler was kicked offline by its hosting provider for refusing to moderate
    content that encourages and incites violence, the archivist @donk_enby wrote code
    to scrape all 32TB of videos—over a million of them—to distribute to researchers
    and journalists. This appendix teaches you how to do something similar, if the
    occasion arises.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll discuss legal considerations around web scraping and give a brief overview
    of HTTP, the protocol that web browsers use to communicate with websites. Finally,
    I describe three different techniques that allow you to scrape different types
    of websites. Complete [Chapters 7](chapter7.xhtml), [8](chapter8.xhtml), [9](chapter9.xhtml),
    and [11](chapter11.xhtml) before following along, since you’ll need the basic
    knowledge of Python programming, CSVs, HTML, and JSON covered there.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">Legal Considerations</samp>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Web scraping isn’t a crime, but its legality is still murky. In general, using
    computer systems with permission (like visiting a public website) is perfectly
    fine, but accessing them without authorization (like logging in to someone else’s
    account) is illegal hacking.
  prefs: []
  type: TYPE_NORMAL
- en: In the US, unauthorized access is a violation of the extremely outdated hacking
    law known as the Computer Fraud and Abuse Act of 1986, or CFAA. Web scraping shouldn’t
    fall under unauthorized access because it entails writing code simply to load
    public web pages that everyone can already access, rather than loading those pages
    the normal way (using a web browser). The problem is that scraping may violate
    a website’s terms of service, and there’s no legal consensus on whether this could
    constitute a violation of the CFAA—courts have ruled both ways.
  prefs: []
  type: TYPE_NORMAL
- en: Despite this, web scraping is an extremely common practice. Search engines like
    Google are essentially massive web scraping operations, as are archive sites like
    the Internet Archive’s Wayback Machine at [*https://<wbr>web<wbr>.archive<wbr>.org*](https://web.archive.org).
    Companies often use web scraping to keep track of airline ticket prices, job listings,
    and other public data. It’s also a critical tool for investigative reporting.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp>
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*The CFAA was originally passed, at least in part, in response to the 1983
    film* WarGames*. In the film, a teenage hacker, played by Matthew Broderick, breaks
    into a military supercomputer and almost starts World War III by mistake. At the
    time, there weren’t any laws against hacking computers. The wildly popular film
    scared Congress into passing such laws.*'
  prefs: []
  type: TYPE_NORMAL
- en: The Markup, a nonprofit newsroom that investigates the tech industry, summed
    up the case for web scraping in an article that includes several examples of investigative
    journalism that relied on it. For example, the newsroom Reveal scraped content
    from extremist groups on Facebook, as well as law enforcement groups, and found
    significant overlap in membership. Reuters also scraped social media and message
    boards and uncovered an underground market for adopted kids; that investigation
    led to a kidnapping conviction. You can read the full article at [*https://<wbr>themarkup<wbr>.org<wbr>/news<wbr>/2020<wbr>/12<wbr>/03<wbr>/why<wbr>-web<wbr>-scraping<wbr>-is<wbr>-vital<wbr>-to<wbr>-democracy*](https://themarkup.org/news/2020/12/03/why-web-scraping-is-vital-to-democracy).
  prefs: []
  type: TYPE_NORMAL
- en: Before you can start writing code to scrape the web yourself, you’ll need to
    understand what HTTP requests are.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">HTTP Requests</samp>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you load a web page, your web browser makes an *HTTP request*. You can
    think of this as the browser sending a message to the website’s server, saying,
    “I’d like to download the content for the page at this URL so I can look at it
    on my computer,” to which the server replies with an *HTTP response* that contains
    the content, typically HTML code. Your browser parses this HTML to figure out
    what else it needs to download to show you the full web page: images, fonts, Cascading
    Style Sheets (CSS) files that define how the web page looks, and JavaScript files
    that tell the website how to act. The browser makes another HTTP request for each
    of these resources, getting the content for them all. Websites also tend to make
    lots of HTTP requests while you’re using them, such as to check for updates and
    display them on the page in real time.'
  prefs: []
  type: TYPE_NORMAL
- en: HTTP requests and responses have *headers*, or metadata about the request or
    response. You might need to send specific headers for your scraping to work properly,
    depending on the website you’re trying to scrape. You might also need your code
    to keep track of *cookies*, which are required for any site with a login option.
    There are many types of requests you can incorporate into your web scraping code,
    such as *POST* requests, which are used to submit forms. However, the code in
    this appendix will make only *GET* requests, the simplest and most common type
    of request, which download the content from a URL.
  prefs: []
  type: TYPE_NORMAL
- en: Many sites don’t like web scrapers for a variety of reasons, including the fact
    that if a script is hammering a site with HTTP requests, this increases the site’s
    bandwidth costs and could even cause it to crash. Sometimes sites will add roadblocks,
    such as limiting the number of requests you can make in a short amount of time
    or requiring that the user (or bot) fill out a CAPTCHA, in an effort to hinder
    or prevent scraping.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp>
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Some time around 2002, when I was in high school, my friends and I decided
    to make a song lyrics website. Similar sites existed, but they were incomplete.
    I thought it would be simple to scrape the lyrics from those other sites and make
    a single site that had* all *of the lyrics. I wrote a script to scrape thousands
    of lyrics from one particular site, but my script crashed while it was running.
    I realized it was because the source website had gone down. A few days later,
    the site came back online with a message: the owner was overjoyed to learn how
    much traffic the site was getting, but to keep up with it, they had to raise money
    to keep the site online. I felt bad about it, and we never ended up launching
    that lyrics site.*'
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">Scraping Techniques</samp>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section describes three different techniques for web scraping, each introducing
    a different Python module. You’ll use a Python package called HTTPX to make HTTP
    requests, then use another called Beautiful Soup to help you select the data that
    you care about from a soup of messy HTML code. Finally, you’ll use a package called
    Selenium to write code that launches a web browser and controls what it does.
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping requires a lot of trial and error as well as a thorough understanding
    of the layout of the website that you’re scraping data from. This appendix gives
    you just a few examples, not a comprehensive overview, but they should give you
    a head start on writing your own web scraping scripts in the future.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Loading Pages with
    HTTPX</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: HTTPX is a third-party Python package that lets you make your own HTTP requests
    with Python. In this section, you’ll learn how to use it to scrape the most recent
    posts from any given user on the far-right social media site Gab, which you read
    about in [Chapters 1](chapter1.xhtml), [12](chapter12.xhtml), and [13](chapter13.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Install the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp> module
    with pip by running <samp class="SANS_TheSansMonoCd_W7Bold_B_11">python3 -m pip
    install</samp> <samp class="SANS_TheSansMonoCd_W7Bold_B_11">httpx</samp>. After
    importing <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp> into your
    code, you should be able to load a web page by running the <samp class="SANS_TheSansMonoCd_W7Bold_B_11">httpx.get()</samp>
    function and passing in a URL. This function returns a request object, and you
    can access the request’s content with <samp class="SANS_TheSansMonoCd_W5Regular_11">.content</samp>
    for binary data or <samp class="SANS_TheSansMonoCd_W5Regular_11">.text</samp>
    for text data. For example, Listing B-1 shows Python code to make an HTTP request
    to *https://<wbr>example<wbr>.com* and view its content.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing B-1: Scraping the HTML
    from</samp> <samp class="SANS_Futura_Std_Book_11">https://example.com</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: First, this code imports the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp>
    module. It then calls the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp>
    function, passing in a URL as an argument, and saves the response in the variable
    <samp class="SANS_TheSansMonoCd_W5Regular_11">r</samp>. Finally, it displays the
    <samp class="SANS_TheSansMonoCd_W5Regular_11">r.text</samp> variable, which is
    all of the HTML code that makes up *https://<wbr>example<wbr>.com*. (If you’re
    loading a binary file, like an image, then you can get the binary data in the
    <samp class="SANS_TheSansMonoCd_W5Regular_11">r.content</samp> variable.) This
    simple <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp> function
    is often all you need to scrape entire databases of information from the web.
    The script I’ll show you in this section that scrapes posts from Gab relies on
    this function.
  prefs: []
  type: TYPE_NORMAL
- en: Since web scraping means writing code that loads URLs, your first step should
    be to determine which URLs you need to load. The easiest way to do this is to
    use the built-in developer tools in your web browser. You can open them in most
    browsers by pressing the F12 key. In both Firefox and Chrome, you can see the
    HTTP requests your browser is making, and what the responses look like, in the
    Network tab of the developer tools. For example, if you open your browser’s developer
    tools and load the profile page of a Gab user, you can see what HTTP requests
    it makes to gather that user’s most recent posts. Once you have that information,
    you can write a script that makes the same HTTP requests for you.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp>
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp>*The developer tools built
    in to Firefox, Chrome, and other browsers are a great way to learn what data your
    web browser is sending back and forth on the websites you’re visiting, to see
    exactly how web pages are laid out, and more. For more about Firefox’s developer
    tools, see* [https://firefox-source-docs.mozilla.org/devtools-user/index.html](https://firefox-source-docs.mozilla.org/devtools-user/index.html)*;*
    *for Chrome, see* [https://developer.chrome.com/docs/devtools](https://developer.chrome.com/docs/devtools)*.*
  prefs: []
  type: TYPE_NORMAL
- en: For example, the Gab page for Marjorie Taylor Greene, the US congressperson
    who’s also a Christian nationalist and QAnon conspiracy theorist, is located at
    [*https://<wbr>gab<wbr>.com<wbr>/RealMarjorieGreene*](https://gab.com/RealMarjorieGreene).
    In a web browser, load that URL and then open the developer tools. Refresh the
    page to get all of the HTTP requests to show up in the Network tab.
  prefs: []
  type: TYPE_NORMAL
- en: In the Network tab, you should see several HTTP requests listed on the left
    half of the developer tools panel. When you click a request, the right half of
    the panel displays information about it. The right half has its own tabs that
    you can switch through to see details like the request’s headers, cookies, and
    the body of the request and its response.
  prefs: []
  type: TYPE_NORMAL
- en: 'When I loaded this page and looked through my browser’s HTTP requests and their
    responses, I decided I was most interested in the following URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[***https://<wbr>gab<wbr>.com<wbr>/api<wbr>/v1<wbr>/account<wbr>_by<wbr>_username<wbr>/RealMarjorieGreene***](https://gab.com/api/v1/account_by_username/RealMarjorieGreene) The
    response to this request includes a JSON object containing information about Greene’s
    Gab profile, including her Gab account ID, 3155503.'
  prefs: []
  type: TYPE_NORMAL
- en: '[***https://<wbr>gab<wbr>.com<wbr>/api<wbr>/v1<wbr>/accounts<wbr>/3155503<wbr>/statuses<wbr>?sort<wbr>_by<wbr>=newest***](https://gab.com/api/v1/accounts/3155503/statuses?sort_by=newest) The
    response to this request includes a JSON array of Greene’s most recent Gab posts.
    Her account ID is in the URL itself.'
  prefs: []
  type: TYPE_NORMAL
- en: The first URL let me look up the Gab ID of any account, and the second URL let
    me look up the recent posts from an account, based on its Gab ID. [Figure B-1](#figB-1)
    shows Firefox’s developer tools in action while loading this page.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of Firefox with Marjorie Taylor Greene’s Gab account loaded.
    The developer tools are open, the Network tab is selected, and a list of all of
    the HTTP requests from the browser is shown.](Images/FigureB-1.png)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure B-1: Viewing the JSON
    response to a specific request in the Firefox developer tools Network tab</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this response is in JSON format. I wanted to write a script
    that, given a Gab username, would download the latest posts from that user. In
    order to write it, I had to spend some time looking at the JSON in these responses
    to understand how it was structured and what information I was interested in.
    For example, since I wanted to start with a Gab username, my script would first
    need to load the URL *https://<wbr>gab<wbr>.com<wbr>/api<wbr>/v1<wbr>/account<wbr>_by<wbr>_username<wbr>/<username>*,
    replacing *<username>* with my target username. It would then need to parse the
    JSON it receives to extract this Gab user’s ID. Then, using that ID, it would
    need to load the URL *https://<wbr>gab<wbr>.com<wbr>/api<wbr>/v1<wbr>/accounts<wbr>/<id><wbr>/statuses<wbr>?sort<wbr>_by<wbr>=newest*,
    replacing *<id>* with the Gab ID of the target account. Finally, it would need
    to parse that JSON response to display the latest Gab posts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this research, I wrote the following script to scrape the latest posts
    from any target Gab account. Here’s the code for this web scraping script, *httpx<wbr>-example<wbr>.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This script first imports the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp>
    module, since it will need that module to make HTTP requests. Like many Python
    scripts throughout this book, it uses the <samp class="SANS_TheSansMonoCd_W5Regular_11">click</samp>
    module to accept CLI arguments. In this case, it accepts an argument called <samp
    class="SANS_TheSansMonoCd_W5Regular_11">gab_username</samp>, the username of the
    target Gab user ❶.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the <samp class="SANS_TheSansMonoCd_W5Regular_11">main()</samp> function
    runs, it downloads information about the target user by calling the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp>
    function and passing in the URL *https://<wbr>gab<wbr>.com<wbr>/api<wbr>/v1<wbr>/account<wbr>_by<wbr>_username<wbr>/<gab<wbr>_username>*,
    replacing *<gab_username>* with the value of the CLI argument and storing the
    result in the variable <samp class="SANS_TheSansMonoCd_W5Regular_11">r</samp>
    ❷. As my browser’s developer tools made clear, the response should be a JSON object,
    so the script next calls <samp class="SANS_TheSansMonoCd_W5Regular_11">r.json()</samp>
    on it to make HTTPX convert it into a dictionary called <samp class="SANS_TheSansMonoCd_W5Regular_11">user_info</samp>.
    It then checks to see if <samp class="SANS_TheSansMonoCd_W5Regular_11">user_info</samp>
    has an <samp class="SANS_TheSansMonoCd_W5Regular_11">error</samp> key; if so,
    it displays the error message and quits early. If you try loading that URL with
    an invalid username, you’ll see the error message in the <samp class="SANS_TheSansMonoCd_W5Regular_11">error</samp>
    key: the string <samp class="SANS_TheSansMonoCd_W5Regular_11">Record not found</samp>.'
  prefs: []
  type: TYPE_NORMAL
- en: Once the script has successfully retrieved information about a Gab user, it
    displays some of that information—the display name, number of followers, number
    of follows, and number of posts—in the terminal ❸. The script then uses HTTPX
    to make another HTTP request, this time to load the user’s posts. Note that this
    URL includes <samp class="SANS_TheSansMonoCd_W5Regular_11">user_info['id']</samp>,
    which is the ID of the user discovered from the previous HTTP request ❹. As before,
    it calls <samp class="SANS_TheSansMonoCd_W5Regular_11">r.json()</samp> to convert
    the JSON into a Python object, this time a list called <samp class="SANS_TheSansMonoCd_W5Regular_11">posts</samp>.
    In the following <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> loop,
    the script loops through the list of posts, displaying them one at a time.
  prefs: []
  type: TYPE_NORMAL
- en: You can find a complete copy of this code in the book’s GitHub repo at [*https://<wbr>github<wbr>.com<wbr>/micahflee<wbr>/hacks<wbr>-leaks<wbr>-and<wbr>-revelations<wbr>/blob<wbr>/main<wbr>/appendix<wbr>-b<wbr>/httpx<wbr>-example<wbr>.py*](https://github.com/micahflee/hacks-leaks-and-revelations/blob/main/appendix-b/httpx-example.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, I could use this script to download the recent posts
    of any Gab user by including their username as an argument. For example, here’s
    what it looked like when I ran this script on the account of Andrew Torba, Gab’s
    founder and owner and the author of the book *Christian Nationalism*, whose Gab
    username is <samp class="SANS_TheSansMonoCd_W5Regular_11">a</samp>:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The output shows Torba’s display name, statistics about his account, and several
    of his latest posts to Gab. As you can see, they’re on the fascist side. Torba
    has 3.8 million followers, because every Gab user automatically follows him when
    they create an account.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp>
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*While 3.8 million followers sounds like a lot, most of those accounts aren’t
    active. In 2021, I analyzed hacked Gab data and discovered that of the roughly
    4 million accounts, only 1.5 million of them had posted any content at all, only
    400,000 had posted more than 10 times, and only 100,000 of those had posted anything
    recently. You can read my analysis at* [https://theintercept.com/2021/03/15/gab-hack-donald-trump-parler-extremists/](https://theintercept.com/2021/03/15/gab-hack-donald-trump-parler-extremists/)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Try running *httpx<wbr>-example<wbr>.py* on any Gab account you’d like. Unless
    Gab’s website has changed, this should download the recent posts from that user.
    However, it’s possible that by the time you run this script, the site may have
    changed so that the script doesn’t work anymore. This is the unfortunate nature
    of web scraping. Every script you write that scrapes the web relies on websites
    acting one specific way; if they don’t, your script might break. It’s often a
    simple matter to update a script so it works again, though. To do so, you’d need
    to use your browser’s developer tools to figure out how the website changed, and
    then update your script to match its new URLs and behavior—basically, repeat what
    you just did. In the worst case, if the website has changed a lot, you may need
    to rewrite your scraping script from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python logic and HTTPX, you can also modify the script to get *all* of
    the posts for a given account, rather than just the recent ones. You could write
    a script that finds a target Gab user and downloads the list of accounts they
    follow. Or you can take a target Gab post and download a list of accounts that
    liked it. You’d just need to learn exactly which HTTP requests to make to get
    the information you’re interested in, and then have Python make those requests
    for you. Some of these tasks would be more complicated than others—for example,
    to get the data you’re looking for, you may need to create a Gab account and have
    your scraper make requests while you’re logged in. The more web scraping scripts
    like these you write, the better at it you’ll get.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about using the HTTPX package, check out its documentation at
    [*https://<wbr>www<wbr>.python<wbr>-httpx<wbr>.org*](https://www.python-httpx.org).
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Parsing HTML with
    Beautiful Soup</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scraping Gab was simple because the responses to the HTTP requests were in JSON
    format, but pulling specific information out of the HTML in a web page is more
    challenging. The easiest way to parse HTML in Python is to use a package aptly
    called Beautiful Soup (BS4 for short). Install the <samp class="SANS_TheSansMonoCd_W5Regular_11">bs4</samp>
    module by running <samp class="SANS_TheSansMonoCd_W7Bold_B_11">python3 -m pip
    install bs4</samp>.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here’s some code that uses the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp>
    module to download the HTML from *https://<wbr>example<wbr>.com*, like you did
    in the last section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This code imports the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp>
    module, then imports Beautiful Soup from the <samp class="SANS_TheSansMonoCd_W5Regular_11">bs4</samp>
    module. Next, it uses <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp>
    to make an HTTP request to *https://<wbr>example<wbr>.com* and stores the result
    in <samp class="SANS_TheSansMonoCd_W5Regular_11">r</samp>, allowing you to access
    the HTML string itself using the <samp class="SANS_TheSansMonoCd_W5Regular_11">r.text</samp>
    variable. As you saw in Listing B-1, this HTTP response is in HTML format and
    includes the page’s title inside the <samp class="SANS_TheSansMonoCd_W5Regular_11"><title></samp>
    tag, as well as two paragraphs of text within <samp class="SANS_TheSansMonoCd_W5Regular_11"><p</samp><samp
    class="SANS_TheSansMonoCd_W5Regular_11">></samp> tags inside the <samp class="SANS_TheSansMonoCd_W5Regular_11"><body></samp>
    tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using BS4, you can parse this HTML to select specific pieces of content—in
    this case, the page title and the content of the first paragraph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This code parses the HTML string (<samp class="SANS_TheSansMonoCd_W5Regular_11">r.text</samp>)
    using BS4, storing the resulting <samp class="SANS_TheSansMonoCd_W5Regular_11">BeautifulSoup</samp>
    object in the <samp class="SANS_TheSansMonoCd_W5Regular_11">soup</samp> variable
    defined in the first line of code. This allows you to use <samp class="SANS_TheSansMonoCd_W5Regular_11">soup</samp>
    to extract whatever information you’re interested in from the HTML. The code then
    displays the page title by printing the value of <samp class="SANS_TheSansMonoCd_W5Regular_11">soup.title.text</samp>.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the script searches for the first paragraph on the HTML page and displays
    its text by printing the value of <samp class="SANS_TheSansMonoCd_W5Regular_11">paragraph.text</samp>.
    Finally, it finds all of the links on the page (which are <samp class="SANS_TheSansMonoCd_W5Regular_11"><a></samp>
    tags), loops through them in a <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp>
    loop, and prints the URL for each link (the URL is defined in the <samp class="SANS_TheSansMonoCd_W5Regular_11">href</samp>
    attribute of <samp class="SANS_TheSansMonoCd_W5Regular_11"><a></samp> tags). The
    *https://<wbr>example<wbr>.com* web page has only one link, so the code displays
    just that.
  prefs: []
  type: TYPE_NORMAL
- en: For practice, next we’ll explore a script that scrapes content from Hacker News
    ([*https://<wbr>news<wbr>.ycombinator<wbr>.com*](https://news.ycombinator.com)),
    a news aggregator site about tech startups and computer science. Hacker News is
    similar to Reddit in that anyone can post links, and users then upvote and downvote
    those links, with the most popular ones rising to the top. Its web design is simple
    and has remained the same for many years, making it a good choice for web scraping
    practice.
  prefs: []
  type: TYPE_NORMAL
- en: Your practice script will download the title and URL from the first five pages
    of popular links. The front page of Hacker News displays the 30 most popular recent
    posts. If you scroll to the bottom and click More, you’ll see the second page
    of results, showing the next 30 most popular recent posts, at the [*https://news.ycombinator.com/?p*=*2*](https://news.ycombinator.com/?p=2)
    URL. Likewise, the third page of results has the URL [*https://news.ycombinator.com/?p*=*3*](https://news.ycombinator.com/?p=3),
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure B-2](#figB-2) shows a Firefox window with Hacker News loaded and the
    developer tools open. This time, I’ve switched to the Inspector tab, which allows
    you to inspect how the HTML of the page is laid out. The Inspector tab shows all
    of the HTML tags that make up the page, and when you mouse over an individual
    tag, your browser highlights the corresponding design element on the web page.
    In this example, I moused over an <samp class="SANS_TheSansMonoCd_W5Regular_11"><a></samp>
    tag, and the browser highlighted that element.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of Firefox, with Hacker News loaded and the developer tools
    open. The Inspector tab is selected in developer tools, showing the HTML that
    makes up the post.](Images/FigureB-2.png)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure B-2: Using Firefox’s
    developer tools to inspect the HTML that makes up a Hacker News post</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: 'The developer tools show that all posts in the Hacker News site are laid out
    in an HTML table. In HTML, tables are defined within <samp class="SANS_TheSansMonoCd_W5Regular_11"><table></samp>
    tags. Each row is a <samp class="SANS_TheSansMonoCd_W5Regular_11"><tr></samp>
    tag, and each cell within it has a <samp class="SANS_TheSansMonoCd_W5Regular_11"><td></samp>
    tag. Here’s the HTML code from a typical Hacker News post:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The rows with <samp class="SANS_TheSansMonoCd_W5Regular_11">class="athing"</samp>,
    or the value of the attribute <samp class="SANS_TheSansMonoCd_W5Regular_11">class</samp>
    set to <samp class="SANS_TheSansMonoCd_W5Regular_11">athing</samp>, contain links
    that users have posted. Inside each <samp class="SANS_TheSansMonoCd_W5Regular_11">athing</samp>
    row, there are three cells (that is, three <samp class="SANS_TheSansMonoCd_W5Regular_11"><</samp><samp
    class="SANS_TheSansMonoCd_W5Regular_11">td></samp> tags). The last of these cells
    contains the actual link, the <samp class="SANS_TheSansMonoCd_W5Regular_11"><a></samp>
    tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following script, *bs4-example.py*, scrapes the titles and URLs of the
    first five pages of the most popular posts recently posted on Hacker News, saving
    them in a CSV spreadsheet and also displaying them to the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: First, the script imports the <samp class="SANS_TheSansMonoCd_W5Regular_11">csv</samp>,
    <samp class="SANS_TheSansMonoCd_W5Regular_11">time</samp>, <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp>,
    and <samp class="SANS_TheSansMonoCd_W5Regular_11">bs4</samp> modules. In the <samp
    class="SANS_TheSansMonoCd_W5Regular_11">main()</samp> function, it opens a new
    CSV for writing called *output.csv*, creates a <samp class="SANS_TheSansMonoCd_W5Regular_11">csv.DictWriter()</samp>
    object, and uses that object to write the CSV headers (<samp class="SANS_TheSansMonoCd_W5Regular_11">Title</samp>
    and <samp class="SANS_TheSansMonoCd_W5Regular_11">URL</samp>, in this case), as
    you learned in [Chapter 9](chapter9.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: The following <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> loop
    loops through the results of <samp class="SANS_TheSansMonoCd_W5Regular_11">range(1,
    6)</samp>, saving each item as <samp class="SANS_TheSansMonoCd_W5Regular_11">page</samp>.
    The <samp class="SANS_TheSansMonoCd_W5Regular_11">range()</samp> function is useful
    for looping through a list of numbers; in this case, it starts with 1, then 2,
    and so on until it hits 6 and then stops, meaning it returns the numbers 1 through
    5\. The code displays the page number that it’s about to load, then makes the
    HTTP request to load that page using <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp>,
    creating a different URL for the current page on each loop. After making each
    HTTP request that gets a page of results, the code parses all of the HTML from
    that page using BS4, storing it as <samp class="SANS_TheSansMonoCd_W5Regular_11">soup</samp>.
  prefs: []
  type: TYPE_NORMAL
- en: Now things get slightly trickier. As noted earlier, all of the HTML table rows
    that have the class <samp class="SANS_TheSansMonoCd_W5Regular_11">athing</samp>
    contain links that users posted. The script gets a list of all of these rows by
    calling <samp class="SANS_TheSansMonoCd_W5Regular_11">soup.find_all("tr", class_="athing")</samp>.
    The <samp class="SANS_TheSansMonoCd_W5Regular_11">find_all()</samp> method searches
    the BS4 object <samp class="SANS_TheSansMonoCd_W5Regular_11">soup</samp> for all
    instances of the HTML tag <samp class="SANS_TheSansMonoCd_W5Regular_11"><tr></samp>
    and returns a list of matches. In this case, the code also includes <samp class="SANS_TheSansMonoCd_W5Regular_11">class_="athing"</samp>,
    which tells BS4 to include only tags that have the <samp class="SANS_TheSansMonoCd_W5Regular_11">class</samp>
    attribute set to <samp class="SANS_TheSansMonoCd_W5Regular_11">athing</samp>.
    The <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> loop loops through
    them, saving each item in the <samp class="SANS_TheSansMonoCd_W5Regular_11">table_row</samp>
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the code is looping through each table row that contains a link posted
    by a user, it goes on to find that link tag. There are several links in each table
    row, so it figures out which one is the link a user posted. First, it calls <samp
    class="SANS_TheSansMonoCd_W5Regular_11">table_row.find_all("td")</samp> to get
    a list of all of the table cells inside <samp class="SANS_TheSansMonoCd_W5Regular_11">table_row</samp>,
    storing that list in <samp class="SANS_TheSansMonoCd_W5Regular_11">table_cells</samp>.
    As noted earlier, the last cell contains the link that we care about. Therefore,
    the code pulls out just the last cell in this list, storing it in the variable
    <samp class="SANS_TheSansMonoCd_W5Regular_11">last_cell</samp> (the <samp class="SANS_TheSansMonoCd_W5Regular_11">−1</samp>
    index is the last item in a list). The code searches just <samp class="SANS_TheSansMonoCd_W5Regular_11">last_cell</samp>
    for the link it contains (the <samp class="SANS_TheSansMonoCd_W5Regular_11"><a></samp>
    tag), and uses <samp class="SANS_TheSansMonoCd_W5Regular_11">print()</samp> to
    display the link’s title and URL. Finally, it calls <samp class="SANS_TheSansMonoCd_W5Regular_11">writer.writerow()</samp>
    to also save this row into the CSV.
  prefs: []
  type: TYPE_NORMAL
- en: The code does this once for each of the page’s 30 rows. It then waits one second,
    using <samp class="SANS_TheSansMonoCd_W5Regular_11">time.sleep(1)</samp>, and
    moves on to the next page, until it has extracted all the links from the first
    five pages. When the script is finished running, it creates a file called *output.csv*
    that should contain the 150 most recent popular links posted to Hacker News. Most
    of the time when you’re scraping real data for an investigation, you’ll save it
    to a CSV spreadsheet, like this script did, or to a SQL database (as discussed
    in [Chapter 12](chapter12.xhtml)) so that you can work with it later.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp>
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*The <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">find_all()</samp>
    method in this code passes an argument called <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">class_</samp>
    instead of <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">class</samp>.
    This is because <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">class</samp>
    is a Python keyword and can’t be used as a variable name. If you want to use <samp
    class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">find_all()</samp> to select tags
    using any other attribute, then the argument name will be the same as the attribute
    name. For example, <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">soup.find
    _all("a", href="https://<wbr>example<wbr>.com")</samp>* *will find all link tags
    in <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">soup</samp> that have
    an href attribute set to* <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">https://<wbr>example<wbr>.com<wbr>.</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: You can also find a copy of this code in the book’s GitHub repo at [*https://<wbr>github<wbr>.com<wbr>/micahflee<wbr>/hacks<wbr>-leaks<wbr>-and<wbr>-revelations<wbr>/blob<wbr>/main<wbr>/appendix<wbr>-b<wbr>/bs4<wbr>-example<wbr>.py*](https://github.com/micahflee/hacks-leaks-and-revelations/blob/main/appendix-b/bs4-example.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what it looked like when I ran this script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Try running it yourself now. Assuming Hacker News hasn’t updated its web design,
    it should work fine; however, the URLs will differ because the most popular recent
    links on Hacker News are constantly changing.
  prefs: []
  type: TYPE_NORMAL
- en: This script scrapes only the first five pages of content on Hacker News. In
    theory, you could scrape *all* the content on the site since its founding in 2007\.
    To do so, you’d have to modify the script to stop not after page 5 but when it
    gets to the very last page, presumably one that doesn’t have any links on it.
    This assumes that the site will actually show you content that old and that you
    could make those millions of HTTP requests without it blocking your IP address.
    I don’t know if this is true or not with Hacker News—I haven’t attempted to scrape
    everything from this site myself.
  prefs: []
  type: TYPE_NORMAL
- en: I mentioned in the “HTTP Requests” section that some websites add roadblocks
    to make scraping more difficult, and this turned out to be true with Hacker News.
    When I first wrote this script, it didn’t include the <samp class="SANS_TheSansMonoCd_W5Regular_11">time
    .sleep(1)</samp> code, which waits one second between each HTTP request. I found
    that Hacker News limits how quickly you can make HTTP requests, and the fifth
    request in quick succession responded with an HTML page with the error message
    <samp class="SANS_TheSansMonoCd_W5Regular_11">Sorry, we're not able to serve your
    requests this quickly</samp>. I solved this problem by waiting one second between
    HTTP requests. It’s common to run into hurdles like this while you’re writing
    scrapers, but it’s also often a simple matter of modifying your script like this
    to get around these roadblocks.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about using the BS4 package, check out its documentation at [*https://<wbr>www<wbr>.crummy<wbr>.com<wbr>/software<wbr>/BeautifulSoup<wbr>/bs4<wbr>/doc<wbr>/*](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Automating Web Browsers
    with Selenium</samp>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes scraping websites is too challenging for Beautiful Soup alone. This
    is often the case with sites that are JavaScript-heavy, where viewing the HTML
    source doesn’t result in much information you’re interested in. This is true for
    sites like Facebook, YouTube, and Google Maps. It’s much simpler to get information
    from this sort of site by using a web browser than by untangling the complicated
    web of HTTP requests that you’d need to make to get the same information. Some
    websites also put up barriers to scraping. They might add JavaScript code that
    ensures visitors are using real web browsers before showing them content, preventing
    users from just making HTTP requests using cURL (discussed in [Chapter 4](chapter4.xhtml))
    or a Python package like HTTPX.
  prefs: []
  type: TYPE_NORMAL
- en: You can control a real web browser for scraping purposes by using software called
    Selenium. Scripts that just make HTTP requests are more efficient and run much
    quicker than using Selenium because they don’t require running a whole web browser
    and downloading all of the resources of the target website. When I’m writing a
    scraper, I generally start by attempting to scrape the site using HTTPX, but if
    this technique turns out to be too complicated, I switch to Selenium.
  prefs: []
  type: TYPE_NORMAL
- en: To use the Selenium Python package, you must also install a *web driver*, software
    that Selenium uses to control a web browser. Selenium supports Chrome, Firefox,
    Safari, and Edge. The example in this section uses the Firefox driver, which is
    called geckodriver.
  prefs: []
  type: TYPE_NORMAL
- en: To continue, follow the instructions for your operating system, then skip to
    the “Testing Selenium in the Python Interpreter” section.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">Installing Selenium and geckodriver
    on Windows</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For this task, Windows users should work with native Windows tools rather than
    WSL. Install the <samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp>
    Python module by opening PowerShell and running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Also make sure you have Firefox installed (see [*https://<wbr>www<wbr>.mozilla<wbr>.org<wbr>/en<wbr>-US<wbr>/firefox<wbr>/new<wbr>/*](https://www.mozilla.org/en-US/firefox/new/)).
  prefs: []
  type: TYPE_NORMAL
- en: To install geckodriver, go to *[https://<wbr>github<wbr>.com<wbr>/mozilla<wbr>/geckodriver<wbr>/releases<wbr>](https://github.com/mozilla/geckodriver/releases).*
    You’ll see several ZIP files for the latest version of geckodriver that you can
    download. Download the appropriate Windows version and unzip it. You should end
    up with a single file called *geckodriver.exe*. In File Explorer, copy this file
    and paste it into *C:\Windows\System32*. This will allow you to run geckodriver
    from PowerShell no matter what your working directory is.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">Installing Selenium and geckodriver
    on macOS</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you’re using macOS, open a terminal. Install the <samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp>
    Python module by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then install geckodriver by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This should give you everything you need to use Selenium in Python.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">Installing Selenium and geckodriver
    on Linux</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you’re using Linux, open a terminal. Install the <samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp>
    Python module by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Install geckodriver by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This should give you everything you need to use Selenium in Python.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">Testing Selenium in the Python
    Interpreter</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now that you have Selenium and geckodriver installed, test them out in the
    Python interpreter by loading this book’s git repo website on GitHub to get a
    feel for how Selenium allows you to control a web browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This code first imports <samp class="SANS_TheSansMonoCd_W5Regular_11">webdriver</samp>
    from the <samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp> module.
    It then creates a new Firefox driver by calling <samp class="SANS_TheSansMonoCd_W5Regular_11">webdriver.Firefox()</samp>
    and saves it in the variable <samp class="SANS_TheSansMonoCd_W5Regular_11">driver</samp>.
    When you create the Selenium driver, a new Firefox window should open on your
    computer, and a robot icon should appear in the address bar—this is how you know
    that this browser is being controlled by Selenium.
  prefs: []
  type: TYPE_NORMAL
- en: The code then instructs the browser to load the URL [*https://<wbr>github<wbr>.com<wbr>/micahflee<wbr>/hacks<wbr>-leaks<wbr>-and<wbr>-revelations*](https://github.com/micahflee/hacks-leaks-and-revelations).
    After running the command, you should see Firefox load that GitHub page. Once
    the page is loaded, including all of its JavaScript or other complicated components,
    you can write code to control it. In this case, the code just displays the title
    of the page in the terminal with <samp class="SANS_TheSansMonoCd_W5Regular_11">print(driver.title)</samp>.
    Finally, it quits Firefox.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">Automating Screenshots with
    Selenium</samp>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now let’s try something slightly more complicated. In this section, we’ll go
    over a script that will take two arguments: a location name and the filename of
    a screenshot to save. Using Selenium, the script will load Google Maps at [*https://<wbr>maps<wbr>.google<wbr>.com*](https://maps.google.com),
    search for the location, zoom in a little, turn on the satellite images layer,
    and take a screenshot of the satellite image of the location, saving it to disk.'
  prefs: []
  type: TYPE_NORMAL
- en: While I’m programming web scrapers, I find it helpful to have an interactive
    Python interpreter open in a terminal where I can test out Selenium or BS4 commands,
    allowing me to see if they work in real time without having to start my script
    over. When I’m writing a Selenium script, I open developer tools inside the browser
    I’m driving to inspect all of the HTML tags, which helps me figure out which commands
    to run. Once I get something working, I copy the working code into the script
    that I’m writing in my text editor.
  prefs: []
  type: TYPE_NORMAL
- en: For example, to search for the location in Google Maps, I needed to make the
    Selenium browser select the search box, type the location, and press ENTER. In
    HTML, tags often have <samp class="SANS_TheSansMonoCd_W5Regular_11">id</samp>
    attributes. By using the Firefox developer tools, I discovered that the search
    box in Google Maps, which is an <samp class="SANS_TheSansMonoCd_W5Regular_11"><input></samp>
    tag, includes the <samp class="SANS_TheSansMonoCd_W5Regular_11">id="searchboxinput"</samp>
    attribute, meaning the search box has an <samp class="SANS_TheSansMonoCd_W5Regular_11">id</samp>
    of <samp class="SANS_TheSansMonoCd_W5Regular_11">searchboxinput</samp>. That allowed
    me to enter code into the Python interpreter that would select the search box,
    type a search query into it, and press ENTER in the browser it was controlling.
    I didn’t always get it right on the first try, but after some trial and error,
    I wrote some working code. At this point, I added that code to my script.
  prefs: []
  type: TYPE_NORMAL
- en: I also used developer tools to figure out how to turn on the satellite image
    layer. In the bottom-left corner of Google Maps is an icon called the *minimap*
    that lets you toggle different layers on and off. The developer tools showed me
    that this icon had an <samp class="SANS_TheSansMonoCd_W5Regular_11">id</samp>
    of <samp class="SANS_TheSansMonoCd_W5Regular_11">minimap</samp> and that I could
    click one of the buttons in the minimap element to turn on the satellite layer;
    just like with the search box, I tested clicking this icon in the Python interpreter
    until I got it working.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following script, *selenium-example.py,* uses Selenium to take satellite
    image screenshots from Google Maps for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: First, the script imports the <samp class="SANS_TheSansMonoCd_W5Regular_11">click</samp>
    and <samp class="SANS_TheSansMonoCd_W5Regular_11">time</samp> modules and then
    several components from the <samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp>
    module. Specifically, it imports <samp class="SANS_TheSansMonoCd_W5Regular_11">webdriver</samp>,
    the component required to actually launch and control a web browser. It also imports
    <samp class="SANS_TheSansMonoCd_W5Regular_11">Keys</samp> and <samp class="SANS_TheSansMonoCd_W5Regular_11">By</samp>
    to automate pressing ENTER after searching and to search for HTML elements by
    their <samp class="SANS_TheSansMonoCd_W5Regular_11">id</samp> attribute.
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp>
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Exactly what you need to import from <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">selenium</samp>
    depends on what you’re trying to do. Consult the Selenium for Python documentation
    to learn exactly what you need and when—that’s how I figured it out.*'
  prefs: []
  type: TYPE_NORMAL
- en: The code includes Click decorators before the <samp class="SANS_TheSansMonoCd_W5Regular_11">main()</samp>
    function, making this a command line program that takes two arguments, <samp class="SANS_TheSansMonoCd_W5Regular_11">location</samp>
    and <samp class="SANS_TheSansMonoCd_W5Regular_11">screenshot_filename</samp>.
    The <samp class="SANS_TheSansMonoCd_W5Regular_11">location</samp> variable is
    a Google Maps search query, like *Manhattan, NY* or *The Great Pyramid of Giza*,
    and <samp class="SANS_TheSansMonoCd_W5Regular_11">screenshot_filename</samp> is
    the path to save the final screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: When the <samp class="SANS_TheSansMonoCd_W5Regular_11">main()</samp> function
    runs, the code starts by creating a Selenium web driver, which should open a Firefox
    window that the script will then control. The <samp class="SANS_TheSansMonoCd_W5Regular_11">driver.implicitly_wait(10)</samp>
    function tells Selenium to wait up to 10 seconds for page elements to load. The
    code loads [*https://<wbr>maps<wbr>.google<wbr>.com*](https://maps.google.com)
    in Firefox with the <samp class="SANS_TheSansMonoCd_W5Regular_11">driver.get()</samp>
    function, then finds the search box element on the page, storing it in the variable
    <samp class="SANS_TheSansMonoCd_W5Regular_11">search_box</samp>. It finds the
    search box by running <samp class="SANS_TheSansMonoCd_W5Regular_11">driver.find_element(By.ID,
    "searchboxinput")</samp>. Once the code has this search box object stored in <samp
    class="SANS_TheSansMonoCd_W5Regular_11">search_box</samp>, it clears any text
    in the text box by calling the <samp class="SANS_TheSansMonoCd_W5Regular_11">clear()</samp>
    method on it, and then it types the text in the <samp class="SANS_TheSansMonoCd_W5Regular_11">location</samp>
    string by calling <samp class="SANS_TheSansMonoCd_W5Regular_11">send_keys(location)</samp>.
    Finally, it presses ENTER to search for this location by calling <samp class="SANS_TheSansMonoCd_W5Regular_11">send_keys(Keys.RETURN)</samp>.
    At this point, Google Maps should search for the location.
  prefs: []
  type: TYPE_NORMAL
- en: The code then zooms in by selecting the <samp class="SANS_TheSansMonoCd_W5Regular_11"><body></samp>
    tag, the main HTML tag that contains all other tags, then telling Firefox to press
    the + key twice, which is the Google Maps keyboard shortcut to zoom in.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, Firefox has loaded Google Maps, searched for a location, and
    zoomed in on that location. The code then turns on the satellite image layer by
    locating the minimap in the corner of the screen. Once it finds this, it locates
    all of the <samp class="SANS_TheSansMonoCd_W5Regular_11"><button></samp> tags
    inside the minimap by calling the <samp class="SANS_TheSansMonoCd_W5Regular_11">find_elements(By.TAG_NAME,
    "button")</samp> method, and then it clicks the third button, calling the <samp
    class="SANS_TheSansMonoCd_W5Regular_11">click()</samp> method on the third element
    (which has an index of <samp class="SANS_TheSansMonoCd_W5Regular_11">2</samp>)
    on the list of buttons. This turns on the satellite images layer.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the script waits two seconds, just to make sure the satellite images
    have finished loading, and then saves a screenshot of the web page to <samp class="SANS_TheSansMonoCd_W5Regular_11">screenshot_filename</samp>.
    When it’s done, it quits Firefox.
  prefs: []
  type: TYPE_NORMAL
- en: You can find a complete copy of this code in the book’s GitHub repo at [*https://<wbr>github<wbr>.com<wbr>/micahflee<wbr>/hacks<wbr>-leaks<wbr>-and<wbr>-revelations<wbr>/blob<wbr>/main<wbr>/appendix<wbr>-b<wbr>/selenium<wbr>-example<wbr>.py*](https://github.com/micahflee/hacks-leaks-and-revelations/blob/main/appendix-b/selenium-example.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use *selenium-example.py* to generate Google Maps screenshots of any
    location you like. For example, I ran the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This opened a Firefox window that was controlled by Selenium. It loaded Google
    Maps, searched for *great pyramid of giza*, zoomed in, turned on the satellite
    images layer, and saved a screenshot of the window in the file *giza.png*. [Figure
    B-3](#figB-3) shows *giza.png*, scraped from Google Maps.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot that Selenium created showing a satellite image of the Great
    Pyramid of Giza. The Google Maps interface is displayed on the left, including
    a photo of the pyramid, information reviews, its address, and so on.](Images/FigureB-3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure B-3: A satellite image
    of the Great Pyramid of Giza from Selenium</samp>'
  prefs: []
  type: TYPE_NORMAL
- en: On your own, it might also be fun to try searching for *US Capitol*; *Washington,
    DC*; *Kremlin, Moscow*; or *Tokyo, Japan*.
  prefs: []
  type: TYPE_NORMAL
- en: This example script used Selenium to take screenshots. You could modify it so
    that Selenium automatically takes a screenshot each time a public figure posts
    to social media, so you’ll have a record of it in case they delete it. You’re
    not limited to cataloging information in this way, though; you can also use Selenium
    to extract information from web pages and store them in CSV spreadsheets or any
    other format you’d like, just like you can with BS4.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about Selenium for Python, check out its documentation at [*https://<wbr>selenium<wbr>-python<wbr>.readthedocs<wbr>.io*](https://selenium-python.readthedocs.io).
  prefs: []
  type: TYPE_NORMAL
- en: <samp class="SANS_Futura_Std_Bold_B_11">Next Steps</samp>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this appendix, I’ve gone over a few techniques for web scraping and provided
    some simple example scripts to show off the basics of how they work. However,
    in order to write code for your future web scraping projects, you’ll probably
    need to learn more about web development than is covered in this book, depending
    on what site you’re trying to scrape. For example, your HTTPX and BS4 scraper
    might need to first log in to a website and then make all of its future requests
    as that logged-in user in order to access the content you’re after. This would
    require making HTTP POST requests instead of just GET requests and keeping track
    of cookies, neither of which I’ve covered here.
  prefs: []
  type: TYPE_NORMAL
- en: As a next step, I recommend getting more comfortable with the developer tools
    built into browsers. This will help familiarize you with the HTTP requests your
    browser makes and what their responses include. Spend more time browsing the layout
    of HTML elements, as you did in this appendix. The more you learn about web development,
    including more complex topics like HTTP headers and cookies, the easier it will
    be for you to scrape the web. If you can access information in a web browser,
    you can write a script that automates accessing that information.
  prefs: []
  type: TYPE_NORMAL
