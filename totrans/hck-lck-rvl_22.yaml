- en: <hgroup>
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <hgroup>
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">B</samp> <samp class="SANS_Dogma_OT_Bold_B_11">SCRAPING
    THE WEB</samp>
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">B</samp> <samp class="SANS_Dogma_OT_Bold_B_11">抓取网络</samp>
- en: </hgroup>
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: </hgroup>
- en: 'Sometimes, in order to research important data publicly available online, you’ll
    need to download a local copy. When websites don’t provide this data in structured
    downloadable formats like spreadsheets, JSON files, or databases, you can make
    your own copy using *web scraping* (or *screen scraping*): writing code that loads
    web pages for you and extracts their contents. These might include social media
    posts, court documents, or any other online data. You can use web scraping to
    download either full datasets or the same web page again and again on a regular
    basis to see if its content changes over time.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，为了研究重要的在线公共数据，你需要下载一份本地副本。当网站没有提供结构化的可下载格式（如电子表格、JSON文件或数据库）时，你可以通过*网络抓取*（或*屏幕抓取*）来制作自己的副本：编写代码为你加载网页并提取其内容。这些内容可能包括社交媒体帖子、法院文件或任何其他在线数据。你可以使用网络抓取下载完整的数据集，或者定期下载同一网页，以查看其内容是否随着时间发生变化。
- en: For example, consider the Parler dataset discussed in [Chapter 11](chapter11.xhtml).
    Before Parler was kicked offline by its hosting provider for refusing to moderate
    content that encourages and incites violence, the archivist @donk_enby wrote code
    to scrape all 32TB of videos—over a million of them—to distribute to researchers
    and journalists. This appendix teaches you how to do something similar, if the
    occasion arises.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑[第11章](chapter11.xhtml)中讨论的Parler数据集。在Parler因拒绝审查鼓励和煽动暴力的内容而被其托管服务提供商下线之前，档案管理员@donk_enby编写了代码来抓取所有32TB的视频——超过一百万个——并分发给研究人员和记者。本附录将教你如何在类似情况下进行操作。
- en: I’ll discuss legal considerations around web scraping and give a brief overview
    of HTTP, the protocol that web browsers use to communicate with websites. Finally,
    I describe three different techniques that allow you to scrape different types
    of websites. Complete [Chapters 7](chapter7.xhtml), [8](chapter8.xhtml), [9](chapter9.xhtml),
    and [11](chapter11.xhtml) before following along, since you’ll need the basic
    knowledge of Python programming, CSVs, HTML, and JSON covered there.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我将讨论网络抓取的法律考虑，并简要概述HTTP，这是网页浏览器与网站通信时使用的协议。最后，我会介绍三种不同的技术，帮助你抓取不同类型的网站。在跟随本章之前，请完成[第7章](chapter7.xhtml)、[第8章](chapter8.xhtml)、[第9章](chapter9.xhtml)和[第11章](chapter11.xhtml)，因为你需要掌握其中讲解的Python编程、CSV、HTML和JSON的基础知识。
- en: <samp class="SANS_Futura_Std_Bold_B_11">Legal Considerations</samp>
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <samp class="SANS_Futura_Std_Bold_B_11">法律考虑</samp>
- en: Web scraping isn’t a crime, but its legality is still murky. In general, using
    computer systems with permission (like visiting a public website) is perfectly
    fine, but accessing them without authorization (like logging in to someone else’s
    account) is illegal hacking.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 网络抓取本身并不是犯罪，但其合法性仍然模糊不清。一般来说，使用已授权的计算机系统（如访问公共网站）是完全合法的，但未经授权访问（如登录他人账户）则属于非法黑客行为。
- en: In the US, unauthorized access is a violation of the extremely outdated hacking
    law known as the Computer Fraud and Abuse Act of 1986, or CFAA. Web scraping shouldn’t
    fall under unauthorized access because it entails writing code simply to load
    public web pages that everyone can already access, rather than loading those pages
    the normal way (using a web browser). The problem is that scraping may violate
    a website’s terms of service, and there’s no legal consensus on whether this could
    constitute a violation of the CFAA—courts have ruled both ways.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在美国，未经授权的访问违反了极其过时的黑客法——1986年的《计算机欺诈和滥用法案》（CFAA）。网络抓取不应被视为未经授权的访问，因为它仅涉及编写代码来加载所有人都能访问的公共网页，而不是像正常方式那样加载这些网页（使用网页浏览器）。问题在于，抓取可能会违反网站的服务条款，而且关于这是否构成违反CFAA的行为，法律界并无共识——法院对此有不同的裁定。
- en: Despite this, web scraping is an extremely common practice. Search engines like
    Google are essentially massive web scraping operations, as are archive sites like
    the Internet Archive’s Wayback Machine at [*https://<wbr>web<wbr>.archive<wbr>.org*](https://web.archive.org).
    Companies often use web scraping to keep track of airline ticket prices, job listings,
    and other public data. It’s also a critical tool for investigative reporting.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，网络抓取仍然是一个极为常见的做法。像Google这样的搜索引擎本质上是大规模的网络抓取操作，像互联网档案馆的Wayback Machine（[*https://<wbr>web<wbr>.archive<wbr>.org*](https://web.archive.org)）这样的归档网站也是如此。公司常常使用网络抓取来跟踪航空机票价格、招聘信息和其他公共数据。这也是调查报道的重要工具。
- en: <samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp>
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: <samp class="SANS_Dogma_OT_Bold_B_21">注意</samp>
- en: '*The CFAA was originally passed, at least in part, in response to the 1983
    film* WarGames*. In the film, a teenage hacker, played by Matthew Broderick, breaks
    into a military supercomputer and almost starts World War III by mistake. At the
    time, there weren’t any laws against hacking computers. The wildly popular film
    scared Congress into passing such laws.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*CFAA 最初的通过，至少部分是响应1983年电影《战争游戏》（WarGames）而通过的。在电影中，少年黑客（由马修·布罗德里克饰演）闯入一台军事超级计算机，并差点无意间引发了第三次世界大战。那时，并没有针对黑客入侵计算机的法律。这部广受欢迎的电影吓得国会通过了相关法律。*'
- en: The Markup, a nonprofit newsroom that investigates the tech industry, summed
    up the case for web scraping in an article that includes several examples of investigative
    journalism that relied on it. For example, the newsroom Reveal scraped content
    from extremist groups on Facebook, as well as law enforcement groups, and found
    significant overlap in membership. Reuters also scraped social media and message
    boards and uncovered an underground market for adopted kids; that investigation
    led to a kidnapping conviction. You can read the full article at [*https://<wbr>themarkup<wbr>.org<wbr>/news<wbr>/2020<wbr>/12<wbr>/03<wbr>/why<wbr>-web<wbr>-scraping<wbr>-is<wbr>-vital<wbr>-to<wbr>-democracy*](https://themarkup.org/news/2020/12/03/why-web-scraping-is-vital-to-democracy).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个调查科技行业的非营利新闻机构，《Markup》总结了网页抓取的案例，其中包括依赖网页抓取的几篇调查新闻。例如，Reveal 新闻室抓取了 Facebook
    上极端主义团体和执法团体的内容，发现这些团体的成员有很大重叠。路透社也抓取了社交媒体和留言板，揭露了一个地下市场，专门交易被领养的孩子；这项调查导致了一起绑架案件的定罪。你可以阅读完整的文章，链接地址是
    [*https://<wbr>themarkup<wbr>.org<wbr>/news<wbr>/2020<wbr>/12<wbr>/03<wbr>/why<wbr>-web<wbr>-scraping<wbr>-is<wbr>-vital<wbr>-to<wbr>-democracy*](https://themarkup.org/news/2020/12/03/why-web-scraping-is-vital-to-democracy)。
- en: Before you can start writing code to scrape the web yourself, you’ll need to
    understand what HTTP requests are.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始编写自己的网页抓取代码之前，你需要了解 HTTP 请求是什么。
- en: <samp class="SANS_Futura_Std_Bold_B_11">HTTP Requests</samp>
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <samp class="SANS_Futura_Std_Bold_B_11">HTTP 请求</samp>
- en: 'When you load a web page, your web browser makes an *HTTP request*. You can
    think of this as the browser sending a message to the website’s server, saying,
    “I’d like to download the content for the page at this URL so I can look at it
    on my computer,” to which the server replies with an *HTTP response* that contains
    the content, typically HTML code. Your browser parses this HTML to figure out
    what else it needs to download to show you the full web page: images, fonts, Cascading
    Style Sheets (CSS) files that define how the web page looks, and JavaScript files
    that tell the website how to act. The browser makes another HTTP request for each
    of these resources, getting the content for them all. Websites also tend to make
    lots of HTTP requests while you’re using them, such as to check for updates and
    display them on the page in real time.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当你加载网页时，你的网页浏览器会发出 *HTTP 请求*。你可以将其理解为浏览器向网站服务器发送一条消息，内容是：“我想下载这个 URL 页面上的内容，这样我就可以在我的电脑上查看它。”服务器会回复一个
    *HTTP 响应*，其中包含内容，通常是 HTML 代码。你的浏览器解析这段 HTML，以确定还需要下载哪些内容，以便完整显示网页：图像、字体、定义网页外观的层叠样式表（CSS）文件，以及告知网站如何运行的
    JavaScript 文件。浏览器会针对每个资源再发出一个 HTTP 请求，获取它们的内容。你在使用网站时，网站也通常会发出大量 HTTP 请求，例如检查更新并实时显示在页面上。
- en: HTTP requests and responses have *headers*, or metadata about the request or
    response. You might need to send specific headers for your scraping to work properly,
    depending on the website you’re trying to scrape. You might also need your code
    to keep track of *cookies*, which are required for any site with a login option.
    There are many types of requests you can incorporate into your web scraping code,
    such as *POST* requests, which are used to submit forms. However, the code in
    this appendix will make only *GET* requests, the simplest and most common type
    of request, which download the content from a URL.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP 请求和响应有 *头部*，即关于请求或响应的元数据。根据你要抓取的网站，你可能需要发送特定的头部，以确保抓取正常工作。你也可能需要让你的代码跟踪
    *cookies*，这些是任何具有登录选项的网站所必需的。你可以在你的网页抓取代码中使用许多不同类型的请求，例如 *POST* 请求，用于提交表单。然而，本附录中的代码将仅使用
    *GET* 请求，这是最简单、最常见的请求类型，用于从 URL 下载内容。
- en: Many sites don’t like web scrapers for a variety of reasons, including the fact
    that if a script is hammering a site with HTTP requests, this increases the site’s
    bandwidth costs and could even cause it to crash. Sometimes sites will add roadblocks,
    such as limiting the number of requests you can make in a short amount of time
    or requiring that the user (or bot) fill out a CAPTCHA, in an effort to hinder
    or prevent scraping.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 许多网站不喜欢网页抓取工具，原因有很多，包括如果一个脚本频繁向网站发送 HTTP 请求，这会增加网站的带宽成本，甚至可能导致网站崩溃。有时，网站会增加障碍，例如限制你在短时间内可以发送的请求数量，或者要求用户（或机器人）填写验证码，试图阻止或防止抓取行为。
- en: <samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp>
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: <samp class="SANS_Dogma_OT_Bold_B_21">注意</samp>
- en: '*Some time around 2002, when I was in high school, my friends and I decided
    to make a song lyrics website. Similar sites existed, but they were incomplete.
    I thought it would be simple to scrape the lyrics from those other sites and make
    a single site that had* all *of the lyrics. I wrote a script to scrape thousands
    of lyrics from one particular site, but my script crashed while it was running.
    I realized it was because the source website had gone down. A few days later,
    the site came back online with a message: the owner was overjoyed to learn how
    much traffic the site was getting, but to keep up with it, they had to raise money
    to keep the site online. I felt bad about it, and we never ended up launching
    that lyrics site.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*大约在 2002 年，当时我还在高中，我和我的朋友们决定创建一个歌词网站。类似的网站已经存在，但它们不完整。我认为从那些网站抓取歌词并创建一个包含*所有*歌词的网站应该很简单。我写了一个脚本，从一个特定的网站抓取了成千上万的歌词，但我的脚本在运行时崩溃了。我意识到是因为源网站已经宕机了。几天后，那个网站重新上线，并发布了一条信息：网站的所有者得知网站的流量有多大后非常高兴，但为了应对这些流量，他们需要筹集资金才能维持网站的运行。我感到很内疚，最终我们没有推出那个歌词网站。*'
- en: <samp class="SANS_Futura_Std_Bold_B_11">Scraping Techniques</samp>
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <samp class="SANS_Futura_Std_Bold_B_11">抓取技术</samp>
- en: This section describes three different techniques for web scraping, each introducing
    a different Python module. You’ll use a Python package called HTTPX to make HTTP
    requests, then use another called Beautiful Soup to help you select the data that
    you care about from a soup of messy HTML code. Finally, you’ll use a package called
    Selenium to write code that launches a web browser and controls what it does.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了三种不同的网页抓取技术，每种技术都引入了不同的 Python 模块。你将使用一个名为 HTTPX 的 Python 包来发起 HTTP 请求，然后使用另一个名为
    Beautiful Soup 的包来帮助你从混乱的 HTML 代码中选择你关心的数据。最后，你将使用一个名为 Selenium 的包编写代码，启动一个浏览器并控制其行为。
- en: Web scraping requires a lot of trial and error as well as a thorough understanding
    of the layout of the website that you’re scraping data from. This appendix gives
    you just a few examples, not a comprehensive overview, but they should give you
    a head start on writing your own web scraping scripts in the future.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 网页抓取需要大量的反复试验，并且需要彻底了解你要抓取数据的网站的布局。本附录提供了几个简单的示例，而不是一个全面的概述，但这些示例应该能为你未来编写自己的网页抓取脚本提供一个良好的开端。
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Loading Pages with
    HTTPX</samp>
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">使用 HTTPX 加载页面</samp>
- en: HTTPX is a third-party Python package that lets you make your own HTTP requests
    with Python. In this section, you’ll learn how to use it to scrape the most recent
    posts from any given user on the far-right social media site Gab, which you read
    about in [Chapters 1](chapter1.xhtml), [12](chapter12.xhtml), and [13](chapter13.xhtml).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: HTTPX 是一个第三方 Python 包，允许你用 Python 发起自己的 HTTP 请求。在本节中，你将学习如何使用它抓取极右翼社交媒体网站 Gab
    上任何给定用户的最新帖子，这个网站你在[第 1 章](chapter1.xhtml)、[第 12 章](chapter12.xhtml)和[第 13 章](chapter13.xhtml)中都有提到。
- en: Install the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp> module
    with pip by running <samp class="SANS_TheSansMonoCd_W7Bold_B_11">python3 -m pip
    install</samp> <samp class="SANS_TheSansMonoCd_W7Bold_B_11">httpx</samp>. After
    importing <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp> into your
    code, you should be able to load a web page by running the <samp class="SANS_TheSansMonoCd_W7Bold_B_11">httpx.get()</samp>
    function and passing in a URL. This function returns a request object, and you
    can access the request’s content with <samp class="SANS_TheSansMonoCd_W5Regular_11">.content</samp>
    for binary data or <samp class="SANS_TheSansMonoCd_W5Regular_11">.text</samp>
    for text data. For example, Listing B-1 shows Python code to make an HTTP request
    to *https://<wbr>example<wbr>.com* and view its content.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pip 安装 <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp> 模块，通过运行
    <samp class="SANS_TheSansMonoCd_W7Bold_B_11">python3 -m pip install</samp> <samp
    class="SANS_TheSansMonoCd_W7Bold_B_11">httpx</samp>。在将 <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp>
    导入到您的代码中后，您应该能够通过运行 <samp class="SANS_TheSansMonoCd_W7Bold_B_11">httpx.get()</samp>
    函数并传入一个 URL 来加载一个网页。此函数返回一个请求对象，您可以通过 <samp class="SANS_TheSansMonoCd_W5Regular_11">.content</samp>
    访问二进制数据，或者通过 <samp class="SANS_TheSansMonoCd_W5Regular_11">.text</samp> 访问文本数据。例如，清单
    B-1 显示了使用 Python 代码发起 HTTP 请求到 *https://<wbr>example<wbr>.com* 并查看其内容。
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Listing B-1: Scraping the HTML
    from</samp> <samp class="SANS_Futura_Std_Book_11">https://example.com</samp>'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">清单 B-1: 抓取来自</samp> <samp class="SANS_Futura_Std_Book_11">https://example.com</samp>
    的 HTML'
- en: First, this code imports the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp>
    module. It then calls the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp>
    function, passing in a URL as an argument, and saves the response in the variable
    <samp class="SANS_TheSansMonoCd_W5Regular_11">r</samp>. Finally, it displays the
    <samp class="SANS_TheSansMonoCd_W5Regular_11">r.text</samp> variable, which is
    all of the HTML code that makes up *https://<wbr>example<wbr>.com*. (If you’re
    loading a binary file, like an image, then you can get the binary data in the
    <samp class="SANS_TheSansMonoCd_W5Regular_11">r.content</samp> variable.) This
    simple <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp> function
    is often all you need to scrape entire databases of information from the web.
    The script I’ll show you in this section that scrapes posts from Gab relies on
    this function.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，这段代码导入了 <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp> 模块。接着调用了
    <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp> 函数，传入一个 URL 作为参数，并将响应保存在变量
    <samp class="SANS_TheSansMonoCd_W5Regular_11">r</samp> 中。最后，它显示了 <samp class="SANS_TheSansMonoCd_W5Regular_11">r.text</samp>
    变量，该变量包含了构成 *https://<wbr>example<wbr>.com* 的所有 HTML 代码。（如果您加载的是二进制文件，如图像，则可以在
    <samp class="SANS_TheSansMonoCd_W5Regular_11">r.content</samp> 变量中获取二进制数据。）这个简单的
    <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp> 函数通常就是您抓取整个网站数据库信息所需的全部功能。本节中我将展示的从
    Gab 抓取帖子的脚本就依赖于这个函数。
- en: Since web scraping means writing code that loads URLs, your first step should
    be to determine which URLs you need to load. The easiest way to do this is to
    use the built-in developer tools in your web browser. You can open them in most
    browsers by pressing the F12 key. In both Firefox and Chrome, you can see the
    HTTP requests your browser is making, and what the responses look like, in the
    Network tab of the developer tools. For example, if you open your browser’s developer
    tools and load the profile page of a Gab user, you can see what HTTP requests
    it makes to gather that user’s most recent posts. Once you have that information,
    you can write a script that makes the same HTTP requests for you.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于网页抓取意味着编写加载 URL 的代码，因此您的第一步应该是确定需要加载哪些 URL。最简单的方法是使用您网页浏览器内置的开发者工具。您可以通过按
    F12 键在大多数浏览器中打开它们。在 Firefox 和 Chrome 中，您可以在开发者工具的网络标签页中查看浏览器发出的 HTTP 请求以及响应的内容。例如，如果您打开浏览器的开发者工具并加载一个
    Gab 用户的个人资料页面，您可以看到它发出的 HTTP 请求，以及如何获取该用户的最新帖子。一旦获取到这些信息，您就可以编写一个脚本，自动为您发出相同的
    HTTP 请求。
- en: <samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp>
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: <samp class="SANS_Dogma_OT_Bold_B_21">注意</samp>
- en: <samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp>*The developer tools built
    in to Firefox, Chrome, and other browsers are a great way to learn what data your
    web browser is sending back and forth on the websites you’re visiting, to see
    exactly how web pages are laid out, and more. For more about Firefox’s developer
    tools, see* [https://firefox-source-docs.mozilla.org/devtools-user/index.html](https://firefox-source-docs.mozilla.org/devtools-user/index.html)*;*
    *for Chrome, see* [https://developer.chrome.com/docs/devtools](https://developer.chrome.com/docs/devtools)*.*
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <samp class="SANS_Dogma_OT_Bold_B_21">注意</samp>*Firefox、Chrome 和其他浏览器内置的开发者工具是了解你的网页浏览器在访问网站时来回传输数据的绝佳方式，能够查看网页的布局等更多信息。有关
    Firefox 开发者工具的更多内容，请参见* [https://firefox-source-docs.mozilla.org/devtools-user/index.html](https://firefox-source-docs.mozilla.org/devtools-user/index.html)*；*
    *Chrome 的相关信息请参见* [https://developer.chrome.com/docs/devtools](https://developer.chrome.com/docs/devtools)*。*
- en: For example, the Gab page for Marjorie Taylor Greene, the US congressperson
    who’s also a Christian nationalist and QAnon conspiracy theorist, is located at
    [*https://<wbr>gab<wbr>.com<wbr>/RealMarjorieGreene*](https://gab.com/RealMarjorieGreene).
    In a web browser, load that URL and then open the developer tools. Refresh the
    page to get all of the HTTP requests to show up in the Network tab.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Marjorie Taylor Greene 的 Gab 页面（这位美国国会议员也是一名基督教民族主义者和 QAnon 阴谋论者）位于 [*https://<wbr>gab<wbr>.com<wbr>/RealMarjorieGreene*](https://gab.com/RealMarjorieGreene)。在网页浏览器中加载该网址后，打开开发者工具。刷新页面以使所有
    HTTP 请求在“网络”标签中显示。
- en: In the Network tab, you should see several HTTP requests listed on the left
    half of the developer tools panel. When you click a request, the right half of
    the panel displays information about it. The right half has its own tabs that
    you can switch through to see details like the request’s headers, cookies, and
    the body of the request and its response.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在“网络”标签中，你应该能看到在开发者工具面板的左半部分列出了几个 HTTP 请求。当你点击某个请求时，面板的右半部分会显示该请求的详细信息。右半部分有自己的标签，你可以切换查看请求的头部信息、cookies
    以及请求和响应的正文内容。
- en: 'When I loaded this page and looked through my browser’s HTTP requests and their
    responses, I decided I was most interested in the following URLs:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我加载这个页面并查看浏览器的 HTTP 请求及其响应时，我决定我最感兴趣的是以下几个网址：
- en: '[***https://<wbr>gab<wbr>.com<wbr>/api<wbr>/v1<wbr>/account<wbr>_by<wbr>_username<wbr>/RealMarjorieGreene***](https://gab.com/api/v1/account_by_username/RealMarjorieGreene) The
    response to this request includes a JSON object containing information about Greene’s
    Gab profile, including her Gab account ID, 3155503.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[***https://<wbr>gab<wbr>.com<wbr>/api<wbr>/v1<wbr>/account<wbr>_by<wbr>_username<wbr>/RealMarjorieGreene***](https://gab.com/api/v1/account_by_username/RealMarjorieGreene) 该请求的响应包含一个
    JSON 对象，包含关于 Greene 的 Gab 个人资料信息，包括她的 Gab 账户 ID：3155503。'
- en: '[***https://<wbr>gab<wbr>.com<wbr>/api<wbr>/v1<wbr>/accounts<wbr>/3155503<wbr>/statuses<wbr>?sort<wbr>_by<wbr>=newest***](https://gab.com/api/v1/accounts/3155503/statuses?sort_by=newest) The
    response to this request includes a JSON array of Greene’s most recent Gab posts.
    Her account ID is in the URL itself.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[***https://<wbr>gab<wbr>.com<wbr>/api<wbr>/v1<wbr>/accounts<wbr>/3155503<wbr>/statuses<wbr>?sort<wbr>_by<wbr>=newest***](https://gab.com/api/v1/accounts/3155503/statuses?sort_by=newest) 该请求的响应包含一个
    JSON 数组，列出了 Greene 最近的 Gab 帖子。她的账户 ID 已包含在网址中。'
- en: The first URL let me look up the Gab ID of any account, and the second URL let
    me look up the recent posts from an account, based on its Gab ID. [Figure B-1](#figB-1)
    shows Firefox’s developer tools in action while loading this page.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个网址让我查找任何账户的 Gab ID，第二个网址让我根据其 Gab ID 查找某个账户的最近帖子。[图 B-1](#figB-1)展示了 Firefox
    开发者工具在加载此页面时的操作。
- en: '![A screenshot of Firefox with Marjorie Taylor Greene’s Gab account loaded.
    The developer tools are open, the Network tab is selected, and a list of all of
    the HTTP requests from the browser is shown.](Images/FigureB-1.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![Firefox 截图，展示 Marjorie Taylor Greene 的 Gab 账户。开发者工具已开启，选择了“网络”标签，并显示了浏览器的所有
    HTTP 请求列表。](Images/FigureB-1.png)'
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure B-1: Viewing the JSON
    response to a specific request in the Firefox developer tools Network tab</samp>'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <samp class="SANS_Futura_Std_Book_Oblique_I_11">图 B-1：在 Firefox 开发者工具的“网络”标签中查看特定请求的
    JSON 响应</samp>
- en: As you can see, this response is in JSON format. I wanted to write a script
    that, given a Gab username, would download the latest posts from that user. In
    order to write it, I had to spend some time looking at the JSON in these responses
    to understand how it was structured and what information I was interested in.
    For example, since I wanted to start with a Gab username, my script would first
    need to load the URL *https://<wbr>gab<wbr>.com<wbr>/api<wbr>/v1<wbr>/account<wbr>_by<wbr>_username<wbr>/<username>*,
    replacing *<username>* with my target username. It would then need to parse the
    JSON it receives to extract this Gab user’s ID. Then, using that ID, it would
    need to load the URL *https://<wbr>gab<wbr>.com<wbr>/api<wbr>/v1<wbr>/accounts<wbr>/<id><wbr>/statuses<wbr>?sort<wbr>_by<wbr>=newest*,
    replacing *<id>* with the Gab ID of the target account. Finally, it would need
    to parse that JSON response to display the latest Gab posts.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，响应是 JSON 格式的。我想写一个脚本，给定一个 Gab 用户名，能够下载该用户的最新帖子。为了编写这个脚本，我花了一些时间查看这些响应中的
    JSON 内容，理解它的结构以及我感兴趣的信息。例如，既然我想从一个 Gab 用户名开始，脚本首先需要加载网址 *https://<wbr>gab<wbr>.com<wbr>/api<wbr>/v1<wbr>/account<wbr>_by<wbr>_username<wbr>/<username>*，将
    *<username>* 替换为我的目标用户名。然后，它需要解析收到的 JSON，提取这个 Gab 用户的 ID。接着，使用该 ID，它需要加载网址 *https://<wbr>gab<wbr>.com<wbr>/api<wbr>/v1<wbr>/accounts<wbr>/<id><wbr>/statuses<wbr>?sort<wbr>_by<wbr>=newest*，将
    *<id>* 替换为目标账户的 Gab ID。最后，它需要解析该 JSON 响应并显示最新的 Gab 帖子。
- en: 'Based on this research, I wrote the following script to scrape the latest posts
    from any target Gab account. Here’s the code for this web scraping script, *httpx<wbr>-example<wbr>.py*:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这项研究，我编写了以下脚本来抓取任何目标Gab账户的最新帖子。以下是这个网页抓取脚本的代码，*httpx<wbr>-example<wbr>.py*：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This script first imports the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp>
    module, since it will need that module to make HTTP requests. Like many Python
    scripts throughout this book, it uses the <samp class="SANS_TheSansMonoCd_W5Regular_11">click</samp>
    module to accept CLI arguments. In this case, it accepts an argument called <samp
    class="SANS_TheSansMonoCd_W5Regular_11">gab_username</samp>, the username of the
    target Gab user ❶.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本首先导入 <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp> 模块，因为它需要这个模块来进行
    HTTP 请求。和本书中许多 Python 脚本一样，它使用 <samp class="SANS_TheSansMonoCd_W5Regular_11">click</samp>
    模块来接受命令行参数。在这种情况下，它接受一个名为 <samp class="SANS_TheSansMonoCd_W5Regular_11">gab_username</samp>
    的参数，这是目标 Gab 用户的用户名 ❶。
- en: 'When the <samp class="SANS_TheSansMonoCd_W5Regular_11">main()</samp> function
    runs, it downloads information about the target user by calling the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp>
    function and passing in the URL *https://<wbr>gab<wbr>.com<wbr>/api<wbr>/v1<wbr>/account<wbr>_by<wbr>_username<wbr>/<gab<wbr>_username>*,
    replacing *<gab_username>* with the value of the CLI argument and storing the
    result in the variable <samp class="SANS_TheSansMonoCd_W5Regular_11">r</samp>
    ❷. As my browser’s developer tools made clear, the response should be a JSON object,
    so the script next calls <samp class="SANS_TheSansMonoCd_W5Regular_11">r.json()</samp>
    on it to make HTTPX convert it into a dictionary called <samp class="SANS_TheSansMonoCd_W5Regular_11">user_info</samp>.
    It then checks to see if <samp class="SANS_TheSansMonoCd_W5Regular_11">user_info</samp>
    has an <samp class="SANS_TheSansMonoCd_W5Regular_11">error</samp> key; if so,
    it displays the error message and quits early. If you try loading that URL with
    an invalid username, you’ll see the error message in the <samp class="SANS_TheSansMonoCd_W5Regular_11">error</samp>
    key: the string <samp class="SANS_TheSansMonoCd_W5Regular_11">Record not found</samp>.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当 <samp class="SANS_TheSansMonoCd_W5Regular_11">main()</samp> 函数运行时，它通过调用 <samp
    class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp> 函数并传入网址 *https://<wbr>gab<wbr>.com<wbr>/api<wbr>/v1<wbr>/account<wbr>_by<wbr>_username<wbr>/<gab<wbr>_username>*，将
    <gab_username> 替换为命令行参数的值，并将结果存储在变量 <samp class="SANS_TheSansMonoCd_W5Regular_11">r</samp>
    中 ❷。正如我浏览器的开发者工具所示，响应应该是一个 JSON 对象，因此脚本接下来调用 <samp class="SANS_TheSansMonoCd_W5Regular_11">r.json()</samp>
    让 HTTPX 将其转换成一个名为 <samp class="SANS_TheSansMonoCd_W5Regular_11">user_info</samp>
    的字典。然后，它会检查 <samp class="SANS_TheSansMonoCd_W5Regular_11">user_info</samp> 是否有一个
    <samp class="SANS_TheSansMonoCd_W5Regular_11">error</samp> 键；如果有，它会显示错误信息并提前退出。如果你尝试用无效的用户名加载该网址，你将看到
    <samp class="SANS_TheSansMonoCd_W5Regular_11">error</samp> 键中的错误信息：字符串 <samp class="SANS_TheSansMonoCd_W5Regular_11">Record
    not found</samp>。
- en: Once the script has successfully retrieved information about a Gab user, it
    displays some of that information—the display name, number of followers, number
    of follows, and number of posts—in the terminal ❸. The script then uses HTTPX
    to make another HTTP request, this time to load the user’s posts. Note that this
    URL includes <samp class="SANS_TheSansMonoCd_W5Regular_11">user_info['id']</samp>,
    which is the ID of the user discovered from the previous HTTP request ❹. As before,
    it calls <samp class="SANS_TheSansMonoCd_W5Regular_11">r.json()</samp> to convert
    the JSON into a Python object, this time a list called <samp class="SANS_TheSansMonoCd_W5Regular_11">posts</samp>.
    In the following <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> loop,
    the script loops through the list of posts, displaying them one at a time.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦脚本成功获取了某个 Gab 用户的信息，它会在终端 ❸ 显示这些信息——显示名称、粉丝数、关注人数以及发布的帖子数。然后，脚本使用 HTTPX 发出另一个
    HTTP 请求，这次是加载用户的帖子。注意，这个 URL 包含了 <samp class="SANS_TheSansMonoCd_W5Regular_11">user_info['id']</samp>，这是通过之前的
    HTTP 请求 ❹ 获取到的用户 ID。如前所述，脚本调用 <samp class="SANS_TheSansMonoCd_W5Regular_11">r.json()</samp>
    来将 JSON 转换为 Python 对象，这次是一个名为 <samp class="SANS_TheSansMonoCd_W5Regular_11">posts</samp>
    的列表。在接下来的 <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> 循环中，脚本会遍历这个帖子列表，一次显示一个。
- en: You can find a complete copy of this code in the book’s GitHub repo at [*https://<wbr>github<wbr>.com<wbr>/micahflee<wbr>/hacks<wbr>-leaks<wbr>-and<wbr>-revelations<wbr>/blob<wbr>/main<wbr>/appendix<wbr>-b<wbr>/httpx<wbr>-example<wbr>.py*](https://github.com/micahflee/hacks-leaks-and-revelations/blob/main/appendix-b/httpx-example.py).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的 GitHub 仓库中找到这段代码的完整副本，链接为 [*https://<wbr>github<wbr>.com<wbr>/micahflee<wbr>/hacks<wbr>-leaks<wbr>-and<wbr>-revelations<wbr>/blob<wbr>/main<wbr>/appendix<wbr>-b<wbr>/httpx<wbr>-example<wbr>.py*](https://github.com/micahflee/hacks-leaks-and-revelations/blob/main/appendix-b/httpx-example.py)。
- en: 'At the time of writing, I could use this script to download the recent posts
    of any Gab user by including their username as an argument. For example, here’s
    what it looked like when I ran this script on the account of Andrew Torba, Gab’s
    founder and owner and the author of the book *Christian Nationalism*, whose Gab
    username is <samp class="SANS_TheSansMonoCd_W5Regular_11">a</samp>:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，我可以通过在脚本中加入用户的用户名作为参数，下载任何 Gab 用户的近期帖子。例如，当我运行此脚本时，以下是我获得的 Andrew Torba（Gab
    的创始人和所有者、书籍 *《基督教民族主义》* 的作者）的账户信息，他的 Gab 用户名是 <samp class="SANS_TheSansMonoCd_W5Regular_11">a</samp>：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output shows Torba’s display name, statistics about his account, and several
    of his latest posts to Gab. As you can see, they’re on the fascist side. Torba
    has 3.8 million followers, because every Gab user automatically follows him when
    they create an account.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了 Torba 的显示名称、账户统计数据以及他最近发布的几条 Gab 帖子。如你所见，这些帖子属于法西斯派别。Torba 拥有 380 万粉丝，因为每个
    Gab 用户在创建账户时都会自动关注他。
- en: <samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp>
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: <samp class="SANS_Dogma_OT_Bold_B_21">注意</samp>
- en: '*While 3.8 million followers sounds like a lot, most of those accounts aren’t
    active. In 2021, I analyzed hacked Gab data and discovered that of the roughly
    4 million accounts, only 1.5 million of them had posted any content at all, only
    400,000 had posted more than 10 times, and only 100,000 of those had posted anything
    recently. You can read my analysis at* [https://theintercept.com/2021/03/15/gab-hack-donald-trump-parler-extremists/](https://theintercept.com/2021/03/15/gab-hack-donald-trump-parler-extremists/)*.*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*尽管 380 万粉丝听起来很多，但这些账户中的大多数并不活跃。2021 年，我分析了被黑的 Gab 数据，发现约 400 万账户中，只有 150 万个账户发布过任何内容，只有
    40 万个账户发布过超过 10 次内容，而只有 10 万个账户发布过近期的内容。你可以阅读我的分析文章，链接为* [https://theintercept.com/2021/03/15/gab-hack-donald-trump-parler-extremists/](https://theintercept.com/2021/03/15/gab-hack-donald-trump-parler-extremists/)*。*'
- en: Try running *httpx<wbr>-example<wbr>.py* on any Gab account you’d like. Unless
    Gab’s website has changed, this should download the recent posts from that user.
    However, it’s possible that by the time you run this script, the site may have
    changed so that the script doesn’t work anymore. This is the unfortunate nature
    of web scraping. Every script you write that scrapes the web relies on websites
    acting one specific way; if they don’t, your script might break. It’s often a
    simple matter to update a script so it works again, though. To do so, you’d need
    to use your browser’s developer tools to figure out how the website changed, and
    then update your script to match its new URLs and behavior—basically, repeat what
    you just did. In the worst case, if the website has changed a lot, you may need
    to rewrite your scraping script from scratch.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在任何你喜欢的 Gab 账户上运行*httpx<wbr>-example<wbr>.py*。除非 Gab 的网站发生了变化，否则这应该会下载该用户的最近帖子。不过，也有可能在你运行这个脚本时，网站已经发生了变化，导致脚本无法再正常工作。这就是网页爬虫的遗憾之处。你写的每一个爬取网页的脚本都依赖于网站的行为是某种特定的方式；如果它们不再如此，脚本就可能会崩溃。不过，通常更新脚本使其重新运行是件简单的事。为此，你需要使用浏览器的开发者工具，找出网站的变化，并更新你的脚本，以匹配其新的
    URL 和行为——基本上就是重复你刚才做的事情。在最坏的情况下，如果网站发生了很大变化，你可能需要从头开始重写你的爬虫脚本。
- en: Using Python logic and HTTPX, you can also modify the script to get *all* of
    the posts for a given account, rather than just the recent ones. You could write
    a script that finds a target Gab user and downloads the list of accounts they
    follow. Or you can take a target Gab post and download a list of accounts that
    liked it. You’d just need to learn exactly which HTTP requests to make to get
    the information you’re interested in, and then have Python make those requests
    for you. Some of these tasks would be more complicated than others—for example,
    to get the data you’re looking for, you may need to create a Gab account and have
    your scraper make requests while you’re logged in. The more web scraping scripts
    like these you write, the better at it you’ll get.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 逻辑和 HTTPX，你还可以修改脚本，以获取某个账户的*所有*帖子，而不仅仅是最近的那些。你可以编写一个脚本，找到目标的 Gab 用户，并下载他们关注的账户列表。或者，你可以获取一个目标
    Gab 帖子，并下载喜欢该帖子的账户列表。你只需要学习准确的 HTTP 请求，以获取你感兴趣的信息，然后让 Python 为你发送这些请求。这些任务有些比其他任务更复杂——例如，要获取你想要的数据，你可能需要创建一个
    Gab 账户，并让你的爬虫在你登录时发送请求。你编写的每一个类似的网页爬虫脚本，都会让你在这方面变得越来越熟练。
- en: To learn more about using the HTTPX package, check out its documentation at
    [*https://<wbr>www<wbr>.python<wbr>-httpx<wbr>.org*](https://www.python-httpx.org).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于使用 HTTPX 包的信息，请查看其文档，链接为 [*https://<wbr>www<wbr>.python<wbr>-httpx<wbr>.org*](https://www.python-httpx.org)。
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Parsing HTML with
    Beautiful Soup</samp>
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">使用 Beautiful Soup
    解析 HTML</samp>
- en: Scraping Gab was simple because the responses to the HTTP requests were in JSON
    format, but pulling specific information out of the HTML in a web page is more
    challenging. The easiest way to parse HTML in Python is to use a package aptly
    called Beautiful Soup (BS4 for short). Install the <samp class="SANS_TheSansMonoCd_W5Regular_11">bs4</samp>
    module by running <samp class="SANS_TheSansMonoCd_W7Bold_B_11">python3 -m pip
    install bs4</samp>.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 爬取 Gab 很简单，因为 HTTP 请求的响应是 JSON 格式的，但从网页中的 HTML 中提取特定信息要更具挑战性。在 Python 中解析 HTML
    最简单的方法是使用一个叫做 Beautiful Soup（简称 BS4）的包。通过运行 <samp class="SANS_TheSansMonoCd_W7Bold_B_11">python3
    -m pip install bs4</samp> 安装 <samp class="SANS_TheSansMonoCd_W5Regular_11">bs4</samp>
    模块。
- en: 'For example, here’s some code that uses the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp>
    module to download the HTML from *https://<wbr>example<wbr>.com*, like you did
    in the last section:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下是一些代码，使用 <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp> 模块下载
    *https://<wbr>example<wbr>.com* 的 HTML，就像你在上一节做的那样：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This code imports the <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp>
    module, then imports Beautiful Soup from the <samp class="SANS_TheSansMonoCd_W5Regular_11">bs4</samp>
    module. Next, it uses <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp>
    to make an HTTP request to *https://<wbr>example<wbr>.com* and stores the result
    in <samp class="SANS_TheSansMonoCd_W5Regular_11">r</samp>, allowing you to access
    the HTML string itself using the <samp class="SANS_TheSansMonoCd_W5Regular_11">r.text</samp>
    variable. As you saw in Listing B-1, this HTTP response is in HTML format and
    includes the page’s title inside the <samp class="SANS_TheSansMonoCd_W5Regular_11"><title></samp>
    tag, as well as two paragraphs of text within <samp class="SANS_TheSansMonoCd_W5Regular_11"><p</samp><samp
    class="SANS_TheSansMonoCd_W5Regular_11">></samp> tags inside the <samp class="SANS_TheSansMonoCd_W5Regular_11"><body></samp>
    tag.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码导入了 `<samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp>` 模块，然后从
    `<samp class="SANS_TheSansMonoCd_W5Regular_11">bs4</samp>` 模块导入了 Beautiful Soup。接下来，使用
    `<samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp>` 发起一个 HTTP 请求到
    *https://<wbr>example<wbr>.com*，并将结果存储在 `<samp class="SANS_TheSansMonoCd_W5Regular_11">r</samp>`
    中，这样你就可以通过 `<samp class="SANS_TheSansMonoCd_W5Regular_11">r.text</samp>` 变量访问
    HTML 字符串。如你在清单 B-1 中看到的，这个 HTTP 响应是 HTML 格式的，包含了页面标题（在 `<samp class="SANS_TheSansMonoCd_W5Regular_11"><title></samp>`
    标签中）以及两个段落的文本（在 `<samp class="SANS_TheSansMonoCd_W5Regular_11"><p</samp><samp
    class="SANS_TheSansMonoCd_W5Regular_11">></samp>` 标签内，位于 `<samp class="SANS_TheSansMonoCd_W5Regular_11"><body></samp>`
    标签内）。
- en: 'Using BS4, you can parse this HTML to select specific pieces of content—in
    this case, the page title and the content of the first paragraph:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 BS4，你可以解析 HTML 来选择特定的内容——在这个例子中，是页面标题和第一段的内容：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code parses the HTML string (<samp class="SANS_TheSansMonoCd_W5Regular_11">r.text</samp>)
    using BS4, storing the resulting <samp class="SANS_TheSansMonoCd_W5Regular_11">BeautifulSoup</samp>
    object in the <samp class="SANS_TheSansMonoCd_W5Regular_11">soup</samp> variable
    defined in the first line of code. This allows you to use <samp class="SANS_TheSansMonoCd_W5Regular_11">soup</samp>
    to extract whatever information you’re interested in from the HTML. The code then
    displays the page title by printing the value of <samp class="SANS_TheSansMonoCd_W5Regular_11">soup.title.text</samp>.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用 BS4 解析 HTML 字符串（`<samp class="SANS_TheSansMonoCd_W5Regular_11">r.text</samp>`），并将解析结果的
    `<samp class="SANS_TheSansMonoCd_W5Regular_11">BeautifulSoup</samp>` 对象存储在第一行代码中定义的
    `<samp class="SANS_TheSansMonoCd_W5Regular_11">soup</samp>` 变量中。这使得你可以使用 `<samp
    class="SANS_TheSansMonoCd_W5Regular_11">soup</samp>` 从 HTML 中提取你感兴趣的信息。然后，代码通过打印
    `<samp class="SANS_TheSansMonoCd_W5Regular_11">soup.title.text</samp>` 的值来显示页面标题。
- en: Next, the script searches for the first paragraph on the HTML page and displays
    its text by printing the value of <samp class="SANS_TheSansMonoCd_W5Regular_11">paragraph.text</samp>.
    Finally, it finds all of the links on the page (which are <samp class="SANS_TheSansMonoCd_W5Regular_11"><a></samp>
    tags), loops through them in a <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp>
    loop, and prints the URL for each link (the URL is defined in the <samp class="SANS_TheSansMonoCd_W5Regular_11">href</samp>
    attribute of <samp class="SANS_TheSansMonoCd_W5Regular_11"><a></samp> tags). The
    *https://<wbr>example<wbr>.com* web page has only one link, so the code displays
    just that.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，脚本会在 HTML 页面中搜索第一段，并通过打印 `<samp class="SANS_TheSansMonoCd_W5Regular_11">paragraph.text</samp>`
    的值来显示其文本。最后，它会找到页面上的所有链接（这些是 `<samp class="SANS_TheSansMonoCd_W5Regular_11"><a></samp>`
    标签），通过一个 `<samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp>` 循环遍历这些链接，并打印每个链接的
    URL（URL 定义在 `<samp class="SANS_TheSansMonoCd_W5Regular_11">href</samp>` 属性中）。*https://<wbr>example<wbr>.com*
    网页只有一个链接，所以代码只会显示这个链接。
- en: For practice, next we’ll explore a script that scrapes content from Hacker News
    ([*https://<wbr>news<wbr>.ycombinator<wbr>.com*](https://news.ycombinator.com)),
    a news aggregator site about tech startups and computer science. Hacker News is
    similar to Reddit in that anyone can post links, and users then upvote and downvote
    those links, with the most popular ones rising to the top. Its web design is simple
    and has remained the same for many years, making it a good choice for web scraping
    practice.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，接下来我们将探索一个从 Hacker News ([*https://<wbr>news<wbr>.ycombinator<wbr>.com*](https://news.ycombinator.com))
    获取内容的脚本，Hacker News 是一个关于科技创业公司和计算机科学的新闻聚合网站。Hacker News 与 Reddit 类似，任何人都可以发布链接，用户可以对这些链接进行投票，最受欢迎的链接会排到最前面。它的网页设计简单，且多年未变，是进行网页抓取练习的一个好选择。
- en: Your practice script will download the title and URL from the first five pages
    of popular links. The front page of Hacker News displays the 30 most popular recent
    posts. If you scroll to the bottom and click More, you’ll see the second page
    of results, showing the next 30 most popular recent posts, at the [*https://news.ycombinator.com/?p*=*2*](https://news.ycombinator.com/?p=2)
    URL. Likewise, the third page of results has the URL [*https://news.ycombinator.com/?p*=*3*](https://news.ycombinator.com/?p=3),
    and so on.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你的练习脚本将下载前五页热门链接的标题和 URL。Hacker News 的首页显示最近最受欢迎的 30 条帖子。如果你滚动到页面底部并点击“更多”，你将看到第二页的结果，展示接下来
    30 条最受欢迎的帖子，URL 为 [*https://news.ycombinator.com/?p*=*2*](https://news.ycombinator.com/?p=2)。同样，第三页的结果有
    URL [*https://news.ycombinator.com/?p*=*3*](https://news.ycombinator.com/?p=3)，依此类推。
- en: '[Figure B-2](#figB-2) shows a Firefox window with Hacker News loaded and the
    developer tools open. This time, I’ve switched to the Inspector tab, which allows
    you to inspect how the HTML of the page is laid out. The Inspector tab shows all
    of the HTML tags that make up the page, and when you mouse over an individual
    tag, your browser highlights the corresponding design element on the web page.
    In this example, I moused over an <samp class="SANS_TheSansMonoCd_W5Regular_11"><a></samp>
    tag, and the browser highlighted that element.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 B-2](#figB-2) 显示了一个打开了 Hacker News 并启用了开发者工具的 Firefox 窗口。这次，我切换到了检查器（Inspector）标签，它允许你检查页面的
    HTML 布局。检查器标签显示了构成页面的所有 HTML 标签，当你将鼠标悬停在某个标签上时，浏览器会高亮显示网页上的相应设计元素。在这个例子中，我将鼠标悬停在一个
    <samp class="SANS_TheSansMonoCd_W5Regular_11"><a></samp> 标签上，浏览器高亮显示了该元素。'
- en: '![A screenshot of Firefox, with Hacker News loaded and the developer tools
    open. The Inspector tab is selected in developer tools, showing the HTML that
    makes up the post.](Images/FigureB-2.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![一个 Firefox 截图，显示了打开 Hacker News 并启用了开发者工具。开发者工具中的检查器标签被选中，显示了构成帖子内容的 HTML。](Images/FigureB-2.png)'
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure B-2: Using Firefox’s
    developer tools to inspect the HTML that makes up a Hacker News post</samp>'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <samp class="SANS_Futura_Std_Book_Oblique_I_11">图 B-2：使用 Firefox 开发者工具检查构成 Hacker
    News 帖子的 HTML</samp>
- en: 'The developer tools show that all posts in the Hacker News site are laid out
    in an HTML table. In HTML, tables are defined within <samp class="SANS_TheSansMonoCd_W5Regular_11"><table></samp>
    tags. Each row is a <samp class="SANS_TheSansMonoCd_W5Regular_11"><tr></samp>
    tag, and each cell within it has a <samp class="SANS_TheSansMonoCd_W5Regular_11"><td></samp>
    tag. Here’s the HTML code from a typical Hacker News post:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者工具显示，Hacker News 网站的所有帖子都以 HTML 表格的形式布局。在 HTML 中，表格通过 <samp class="SANS_TheSansMonoCd_W5Regular_11"><table></samp>
    标签定义。每一行是一个 <samp class="SANS_TheSansMonoCd_W5Regular_11"><tr></samp> 标签，其中的每一个单元格都有一个
    <samp class="SANS_TheSansMonoCd_W5Regular_11"><td></samp> 标签。以下是典型的 Hacker News
    帖子的 HTML 代码：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The rows with <samp class="SANS_TheSansMonoCd_W5Regular_11">class="athing"</samp>,
    or the value of the attribute <samp class="SANS_TheSansMonoCd_W5Regular_11">class</samp>
    set to <samp class="SANS_TheSansMonoCd_W5Regular_11">athing</samp>, contain links
    that users have posted. Inside each <samp class="SANS_TheSansMonoCd_W5Regular_11">athing</samp>
    row, there are three cells (that is, three <samp class="SANS_TheSansMonoCd_W5Regular_11"><</samp><samp
    class="SANS_TheSansMonoCd_W5Regular_11">td></samp> tags). The last of these cells
    contains the actual link, the <samp class="SANS_TheSansMonoCd_W5Regular_11"><a></samp>
    tag.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 带有 <samp class="SANS_TheSansMonoCd_W5Regular_11">class="athing"</samp> 的行，或者将属性
    <samp class="SANS_TheSansMonoCd_W5Regular_11">class</samp> 设置为 <samp class="SANS_TheSansMonoCd_W5Regular_11">athing</samp>
    的行，包含用户发布的链接。在每一行 <samp class="SANS_TheSansMonoCd_W5Regular_11">athing</samp>
    中，有三个单元格（即三个 <samp class="SANS_TheSansMonoCd_W5Regular_11"><</samp><samp class="SANS_TheSansMonoCd_W5Regular_11">td></samp>
    标签）。最后一个单元格包含实际的链接，即 <samp class="SANS_TheSansMonoCd_W5Regular_11"><a></samp>
    标签。
- en: 'The following script, *bs4-example.py*, scrapes the titles and URLs of the
    first five pages of the most popular posts recently posted on Hacker News, saving
    them in a CSV spreadsheet and also displaying them to the terminal:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本 *bs4-example.py* 会抓取 Hacker News 上最近发布的最受欢迎帖子前五页的标题和 URL，将它们保存到 CSV 表格中，并同时在终端显示：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: First, the script imports the <samp class="SANS_TheSansMonoCd_W5Regular_11">csv</samp>,
    <samp class="SANS_TheSansMonoCd_W5Regular_11">time</samp>, <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp>,
    and <samp class="SANS_TheSansMonoCd_W5Regular_11">bs4</samp> modules. In the <samp
    class="SANS_TheSansMonoCd_W5Regular_11">main()</samp> function, it opens a new
    CSV for writing called *output.csv*, creates a <samp class="SANS_TheSansMonoCd_W5Regular_11">csv.DictWriter()</samp>
    object, and uses that object to write the CSV headers (<samp class="SANS_TheSansMonoCd_W5Regular_11">Title</samp>
    and <samp class="SANS_TheSansMonoCd_W5Regular_11">URL</samp>, in this case), as
    you learned in [Chapter 9](chapter9.xhtml).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，脚本导入 <samp class="SANS_TheSansMonoCd_W5Regular_11">csv</samp>、<samp class="SANS_TheSansMonoCd_W5Regular_11">time</samp>、<samp
    class="SANS_TheSansMonoCd_W5Regular_11">httpx</samp> 和 <samp class="SANS_TheSansMonoCd_W5Regular_11">bs4</samp>
    模块。在 <samp class="SANS_TheSansMonoCd_W5Regular_11">main()</samp> 函数中，它打开一个新的 CSV
    文件进行写入，名为 *output.csv*，创建一个 <samp class="SANS_TheSansMonoCd_W5Regular_11">csv.DictWriter()</samp>
    对象，并使用该对象写入 CSV 头部（在此例中为 <samp class="SANS_TheSansMonoCd_W5Regular_11">Title</samp>
    和 <samp class="SANS_TheSansMonoCd_W5Regular_11">URL</samp>），正如你在 [第 9 章](chapter9.xhtml)
    中学到的。
- en: The following <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> loop
    loops through the results of <samp class="SANS_TheSansMonoCd_W5Regular_11">range(1,
    6)</samp>, saving each item as <samp class="SANS_TheSansMonoCd_W5Regular_11">page</samp>.
    The <samp class="SANS_TheSansMonoCd_W5Regular_11">range()</samp> function is useful
    for looping through a list of numbers; in this case, it starts with 1, then 2,
    and so on until it hits 6 and then stops, meaning it returns the numbers 1 through
    5\. The code displays the page number that it’s about to load, then makes the
    HTTP request to load that page using <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp>,
    creating a different URL for the current page on each loop. After making each
    HTTP request that gets a page of results, the code parses all of the HTML from
    that page using BS4, storing it as <samp class="SANS_TheSansMonoCd_W5Regular_11">soup</samp>.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> 循环遍历 <samp class="SANS_TheSansMonoCd_W5Regular_11">range(1,
    6)</samp> 的结果，将每个项目保存为 <samp class="SANS_TheSansMonoCd_W5Regular_11">page</samp>。<samp
    class="SANS_TheSansMonoCd_W5Regular_11">range()</samp> 函数用于遍历数字列表；在此案例中，它从 1 开始，然后是
    2，依此类推，直到到达 6 并停止，这意味着它返回 1 到 5 的数字。代码显示即将加载的页面号码，然后使用 <samp class="SANS_TheSansMonoCd_W5Regular_11">httpx.get()</samp>
    发出 HTTP 请求加载该页面，在每次循环中为当前页面创建不同的 URL。在每次发出 HTTP 请求获取页面结果后，代码使用 BS4 解析该页面的所有 HTML
    内容，并将其存储为 <samp class="SANS_TheSansMonoCd_W5Regular_11">soup</samp>。
- en: Now things get slightly trickier. As noted earlier, all of the HTML table rows
    that have the class <samp class="SANS_TheSansMonoCd_W5Regular_11">athing</samp>
    contain links that users posted. The script gets a list of all of these rows by
    calling <samp class="SANS_TheSansMonoCd_W5Regular_11">soup.find_all("tr", class_="athing")</samp>.
    The <samp class="SANS_TheSansMonoCd_W5Regular_11">find_all()</samp> method searches
    the BS4 object <samp class="SANS_TheSansMonoCd_W5Regular_11">soup</samp> for all
    instances of the HTML tag <samp class="SANS_TheSansMonoCd_W5Regular_11"><tr></samp>
    and returns a list of matches. In this case, the code also includes <samp class="SANS_TheSansMonoCd_W5Regular_11">class_="athing"</samp>,
    which tells BS4 to include only tags that have the <samp class="SANS_TheSansMonoCd_W5Regular_11">class</samp>
    attribute set to <samp class="SANS_TheSansMonoCd_W5Regular_11">athing</samp>.
    The <samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> loop loops through
    them, saving each item in the <samp class="SANS_TheSansMonoCd_W5Regular_11">table_row</samp>
    variable.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在情况稍微复杂一些。如前所述，所有具有类名 <samp class="SANS_TheSansMonoCd_W5Regular_11">athing</samp>
    的 HTML 表格行都包含用户发布的链接。脚本通过调用 <samp class="SANS_TheSansMonoCd_W5Regular_11">soup.find_all("tr",
    class_="athing")</samp> 获取所有这些行的列表。<samp class="SANS_TheSansMonoCd_W5Regular_11">find_all()</samp>
    方法会在 BS4 对象 <samp class="SANS_TheSansMonoCd_W5Regular_11">soup</samp> 中搜索所有的 HTML
    标签 <samp class="SANS_TheSansMonoCd_W5Regular_11"><tr></samp> 实例，并返回一个匹配项的列表。在这种情况下，代码还包含
    <samp class="SANS_TheSansMonoCd_W5Regular_11">class_="athing"</samp>，这告诉 BS4 只包括具有
    <samp class="SANS_TheSansMonoCd_W5Regular_11">class</samp> 属性并且设置为 <samp class="SANS_TheSansMonoCd_W5Regular_11">athing</samp>
    的标签。<samp class="SANS_TheSansMonoCd_W5Regular_11">for</samp> 循环遍历它们，将每个项目保存在 <samp
    class="SANS_TheSansMonoCd_W5Regular_11">table_row</samp> 变量中。
- en: Now that the code is looping through each table row that contains a link posted
    by a user, it goes on to find that link tag. There are several links in each table
    row, so it figures out which one is the link a user posted. First, it calls <samp
    class="SANS_TheSansMonoCd_W5Regular_11">table_row.find_all("td")</samp> to get
    a list of all of the table cells inside <samp class="SANS_TheSansMonoCd_W5Regular_11">table_row</samp>,
    storing that list in <samp class="SANS_TheSansMonoCd_W5Regular_11">table_cells</samp>.
    As noted earlier, the last cell contains the link that we care about. Therefore,
    the code pulls out just the last cell in this list, storing it in the variable
    <samp class="SANS_TheSansMonoCd_W5Regular_11">last_cell</samp> (the <samp class="SANS_TheSansMonoCd_W5Regular_11">−1</samp>
    index is the last item in a list). The code searches just <samp class="SANS_TheSansMonoCd_W5Regular_11">last_cell</samp>
    for the link it contains (the <samp class="SANS_TheSansMonoCd_W5Regular_11"><a></samp>
    tag), and uses <samp class="SANS_TheSansMonoCd_W5Regular_11">print()</samp> to
    display the link’s title and URL. Finally, it calls <samp class="SANS_TheSansMonoCd_W5Regular_11">writer.writerow()</samp>
    to also save this row into the CSV.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，代码正在遍历包含用户发布的链接的每个表格行，它会继续寻找该链接标签。每个表格行中有多个链接，因此它会找出哪个是用户发布的链接。首先，它调用<samp
    class="SANS_TheSansMonoCd_W5Regular_11">table_row.find_all("td")</samp>来获取<table_row>内所有表格单元格的列表，并将该列表存储在<samp
    class="SANS_TheSansMonoCd_W5Regular_11">table_cells</samp>中。如前所述，最后一个单元格包含我们关心的链接。因此，代码仅提取该列表中的最后一个单元格，并将其存储在变量<samp
    class="SANS_TheSansMonoCd_W5Regular_11">last_cell</samp>中（<samp class="SANS_TheSansMonoCd_W5Regular_11">−1</samp>索引是列表中的最后一项）。代码仅在<samp
    class="SANS_TheSansMonoCd_W5Regular_11">last_cell</samp>中搜索它包含的链接（<samp class="SANS_TheSansMonoCd_W5Regular_11"><a></samp>标签），并使用<samp
    class="SANS_TheSansMonoCd_W5Regular_11">print()</samp>显示该链接的标题和URL。最后，它调用<samp
    class="SANS_TheSansMonoCd_W5Regular_11">writer.writerow()</samp>，将该行也保存到CSV文件中。
- en: The code does this once for each of the page’s 30 rows. It then waits one second,
    using <samp class="SANS_TheSansMonoCd_W5Regular_11">time.sleep(1)</samp>, and
    moves on to the next page, until it has extracted all the links from the first
    five pages. When the script is finished running, it creates a file called *output.csv*
    that should contain the 150 most recent popular links posted to Hacker News. Most
    of the time when you’re scraping real data for an investigation, you’ll save it
    to a CSV spreadsheet, like this script did, or to a SQL database (as discussed
    in [Chapter 12](chapter12.xhtml)) so that you can work with it later.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码对页面的每一行30行执行一次。然后，它等待一秒钟，使用<samp class="SANS_TheSansMonoCd_W5Regular_11">time.sleep(1)</samp>，并继续处理下一页，直到从前五页中提取了所有链接。当脚本运行结束时，它会创建一个名为*output.csv*的文件，该文件应包含发布到Hacker
    News的150个最新流行链接。当你在进行调查时抓取真实数据时，通常会将数据保存到CSV电子表格中，就像这个脚本所做的那样，或者保存到SQL数据库中（如[第12章](chapter12.xhtml)所讨论的），以便以后进行处理。
- en: <samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp>
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: <samp class="SANS_Dogma_OT_Bold_B_21">注意</samp>
- en: '*The <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">find_all()</samp>
    method in this code passes an argument called <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">class_</samp>
    instead of <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">class</samp>.
    This is because <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">class</samp>
    is a Python keyword and can’t be used as a variable name. If you want to use <samp
    class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">find_all()</samp> to select tags
    using any other attribute, then the argument name will be the same as the attribute
    name. For example, <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">soup.find
    _all("a", href="https://<wbr>example<wbr>.com")</samp>* *will find all link tags
    in <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">soup</samp> that have
    an href attribute set to* <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">https://<wbr>example<wbr>.com<wbr>.</samp>'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*此代码中的<samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">find_all()</samp>方法传递了一个名为<samp
    class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">class_</samp>的参数，而不是<samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">class</samp>。这是因为<samp
    class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">class</samp>是Python的关键字，不能用作变量名。如果你希望使用<samp
    class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">find_all()</samp>来选择具有其他属性的标签，那么参数名称将与属性名称相同。例如，<samp
    class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">soup.find_all("a", href="https://<wbr>example<wbr>.com")</samp>*
    *将会找到<samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">soup</samp>中所有href属性设置为<samp
    class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">https://<wbr>example<wbr>.com<wbr>的链接标签。*'
- en: You can also find a copy of this code in the book’s GitHub repo at [*https://<wbr>github<wbr>.com<wbr>/micahflee<wbr>/hacks<wbr>-leaks<wbr>-and<wbr>-revelations<wbr>/blob<wbr>/main<wbr>/appendix<wbr>-b<wbr>/bs4<wbr>-example<wbr>.py*](https://github.com/micahflee/hacks-leaks-and-revelations/blob/main/appendix-b/bs4-example.py).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在本书的GitHub仓库中找到这段代码的副本，网址为[*https://<wbr>github<wbr>.com<wbr>/micahflee<wbr>/hacks<wbr>-leaks<wbr>-and<wbr>-revelations<wbr>/blob<wbr>/main<wbr>/appendix<wbr>-b<wbr>/bs4<wbr>-example<wbr>.py*](https://github.com/micahflee/hacks-leaks-and-revelations/blob/main/appendix-b/bs4-example.py)。
- en: 'Here’s what it looked like when I ran this script:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我运行这段脚本时的样子：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Try running it yourself now. Assuming Hacker News hasn’t updated its web design,
    it should work fine; however, the URLs will differ because the most popular recent
    links on Hacker News are constantly changing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以自己试试看。假设Hacker News没有更新其网页设计，应该能正常运行；不过，URL会有所不同，因为Hacker News上最热门的最近链接是不断变化的。
- en: This script scrapes only the first five pages of content on Hacker News. In
    theory, you could scrape *all* the content on the site since its founding in 2007\.
    To do so, you’d have to modify the script to stop not after page 5 but when it
    gets to the very last page, presumably one that doesn’t have any links on it.
    This assumes that the site will actually show you content that old and that you
    could make those millions of HTTP requests without it blocking your IP address.
    I don’t know if this is true or not with Hacker News—I haven’t attempted to scrape
    everything from this site myself.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本仅抓取Hacker News上的前五页内容。从理论上讲，你可以抓取自2007年该网站成立以来的*所有*内容。为了做到这一点，你需要修改脚本，使其在不停止于第5页，而是直到抓取到最后一页时才停止，假设那一页上没有任何链接。这假设该网站实际上会显示如此久远的内容，并且你可以在不被屏蔽IP地址的情况下发出数百万次HTTP请求。我不知道Hacker
    News是否真的是这样——我自己并没有尝试抓取这个网站上的所有内容。
- en: I mentioned in the “HTTP Requests” section that some websites add roadblocks
    to make scraping more difficult, and this turned out to be true with Hacker News.
    When I first wrote this script, it didn’t include the <samp class="SANS_TheSansMonoCd_W5Regular_11">time
    .sleep(1)</samp> code, which waits one second between each HTTP request. I found
    that Hacker News limits how quickly you can make HTTP requests, and the fifth
    request in quick succession responded with an HTML page with the error message
    <samp class="SANS_TheSansMonoCd_W5Regular_11">Sorry, we're not able to serve your
    requests this quickly</samp>. I solved this problem by waiting one second between
    HTTP requests. It’s common to run into hurdles like this while you’re writing
    scrapers, but it’s also often a simple matter of modifying your script like this
    to get around these roadblocks.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我在“HTTP请求”一节中提到过，某些网站会增加障碍来使抓取变得更加困难，事实证明，Hacker News也采取了这种做法。当我最初写这个脚本时，它没有包含
    <samp class="SANS_TheSansMonoCd_W5Regular_11">time .sleep(1)</samp> 代码，它会在每次HTTP请求之间等待一秒钟。我发现Hacker
    News限制了你发出HTTP请求的速度，快速连续的第五次请求返回了一个带有错误信息的HTML页面，错误信息是 <samp class="SANS_TheSansMonoCd_W5Regular_11">抱歉，我们无法这么快地处理您的请求</samp>。我通过在每次HTTP请求之间等待一秒来解决这个问题。遇到像这样的障碍是写抓取脚本时常见的情况，但通常只需要像这样修改脚本，就可以绕过这些障碍。
- en: To learn more about using the BS4 package, check out its documentation at [*https://<wbr>www<wbr>.crummy<wbr>.com<wbr>/software<wbr>/BeautifulSoup<wbr>/bs4<wbr>/doc<wbr>/*](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 想要了解更多关于使用BS4包的信息，可以查看它的文档，地址为[*https://<wbr>www<wbr>.crummy<wbr>.com<wbr>/software<wbr>/BeautifulSoup<wbr>/bs4<wbr>/doc<wbr>/*](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)。
- en: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">Automating Web Browsers
    with Selenium</samp>
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <samp class="SANS_Futura_Std_Bold_Condensed_Oblique_BI_11">用Selenium自动化浏览器</samp>
- en: Sometimes scraping websites is too challenging for Beautiful Soup alone. This
    is often the case with sites that are JavaScript-heavy, where viewing the HTML
    source doesn’t result in much information you’re interested in. This is true for
    sites like Facebook, YouTube, and Google Maps. It’s much simpler to get information
    from this sort of site by using a web browser than by untangling the complicated
    web of HTTP requests that you’d need to make to get the same information. Some
    websites also put up barriers to scraping. They might add JavaScript code that
    ensures visitors are using real web browsers before showing them content, preventing
    users from just making HTTP requests using cURL (discussed in [Chapter 4](chapter4.xhtml))
    or a Python package like HTTPX.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，仅靠Beautiful Soup进行网页抓取会显得过于挑战。尤其是在一些JavaScript较多的网站上，查看HTML源代码并不能提供你所关注的很多信息。这对于像Facebook、YouTube和Google
    Maps这样的网站尤其如此。通过Web浏览器从这些网站获取信息要比理清需要发送的复杂HTTP请求更为简单，这些请求本来可以获取相同的信息。一些网站也会设置抓取的障碍，它们可能会添加JavaScript代码，确保访客使用真实的Web浏览器才能显示内容，防止用户仅通过使用cURL（在[第4章](chapter4.xhtml)中讨论）或类似HTTPX的Python包发送HTTP请求。
- en: You can control a real web browser for scraping purposes by using software called
    Selenium. Scripts that just make HTTP requests are more efficient and run much
    quicker than using Selenium because they don’t require running a whole web browser
    and downloading all of the resources of the target website. When I’m writing a
    scraper, I generally start by attempting to scrape the site using HTTPX, but if
    this technique turns out to be too complicated, I switch to Selenium.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用名为Selenium的软件来控制真实的Web浏览器进行抓取。仅通过HTTP请求进行的脚本比使用Selenium更加高效，运行得更快，因为它们不需要启动完整的Web浏览器并下载目标网站的所有资源。当我写抓取程序时，我通常从尝试使用HTTPX进行抓取开始，但如果这种方法太复杂，我就会转而使用Selenium。
- en: To use the Selenium Python package, you must also install a *web driver*, software
    that Selenium uses to control a web browser. Selenium supports Chrome, Firefox,
    Safari, and Edge. The example in this section uses the Firefox driver, which is
    called geckodriver.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Selenium Python包，还必须安装一个*web driver*，这是Selenium用来控制网页浏览器的软件。Selenium支持Chrome、Firefox、Safari和Edge。本节中的示例使用的是Firefox驱动程序，名为geckodriver。
- en: To continue, follow the instructions for your operating system, then skip to
    the “Testing Selenium in the Python Interpreter” section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要继续，请按照操作系统的说明进行操作，然后跳至“在Python解释器中测试Selenium”部分。
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">Installing Selenium and geckodriver
    on Windows</samp>
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">在Windows上安装Selenium和geckodriver</samp>
- en: 'For this task, Windows users should work with native Windows tools rather than
    WSL. Install the <samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp>
    Python module by opening PowerShell and running the following command:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此任务，Windows用户应使用原生Windows工具，而不是WSL。通过打开PowerShell并运行以下命令来安装<samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp>
    Python模块：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Also make sure you have Firefox installed (see [*https://<wbr>www<wbr>.mozilla<wbr>.org<wbr>/en<wbr>-US<wbr>/firefox<wbr>/new<wbr>/*](https://www.mozilla.org/en-US/firefox/new/)).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 同时确保你已安装Firefox（请参见[*https://<wbr>www<wbr>.mozilla<wbr>.org<wbr>/en<wbr>-US<wbr>/firefox<wbr>/new<wbr>/*](https://www.mozilla.org/en-US/firefox/new/)）。
- en: To install geckodriver, go to *[https://<wbr>github<wbr>.com<wbr>/mozilla<wbr>/geckodriver<wbr>/releases<wbr>](https://github.com/mozilla/geckodriver/releases).*
    You’ll see several ZIP files for the latest version of geckodriver that you can
    download. Download the appropriate Windows version and unzip it. You should end
    up with a single file called *geckodriver.exe*. In File Explorer, copy this file
    and paste it into *C:\Windows\System32*. This will allow you to run geckodriver
    from PowerShell no matter what your working directory is.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装geckodriver，请访问 *[https://<wbr>github<wbr>.com<wbr>/mozilla<wbr>/geckodriver<wbr>/releases<wbr>](https://github.com/mozilla/geckodriver/releases)*。你将看到多个ZIP文件，包含最新版本的geckodriver，你可以下载适合的Windows版本并解压缩。解压后，你应该得到一个名为*geckodriver.exe*的文件。在文件资源管理器中，复制该文件并粘贴到*C:\Windows\System32*。这样，你就可以在任何工作目录下通过PowerShell运行geckodriver。
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">Installing Selenium and geckodriver
    on macOS</samp>
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">在macOS上安装Selenium和geckodriver</samp>
- en: 'If you’re using macOS, open a terminal. Install the <samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp>
    Python module by running the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用macOS，打开终端。通过运行以下命令安装<samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp>
    Python模块：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then install geckodriver by running the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行以下命令安装geckodriver：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This should give you everything you need to use Selenium in Python.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为你提供在Python中使用Selenium所需的一切。
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">Installing Selenium and geckodriver
    on Linux</samp>
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">在Linux上安装Selenium和geckodriver</samp>
- en: 'If you’re using Linux, open a terminal. Install the <samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp>
    Python module by running the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Linux，打开终端。通过运行以下命令安装<samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp>
    Python模块：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Install geckodriver by running the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下命令来安装geckodriver：
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This should give you everything you need to use Selenium in Python.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为你提供在Python中使用Selenium所需的一切。
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">Testing Selenium in the Python
    Interpreter</samp>
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">在Python解释器中测试Selenium</samp>
- en: 'Now that you have Selenium and geckodriver installed, test them out in the
    Python interpreter by loading this book’s git repo website on GitHub to get a
    feel for how Selenium allows you to control a web browser:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经安装了Selenium和geckodriver，可以在Python解释器中测试它们，加载本书的GitHub仓库网站，感受一下Selenium如何控制浏览器：
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This code first imports <samp class="SANS_TheSansMonoCd_W5Regular_11">webdriver</samp>
    from the <samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp> module.
    It then creates a new Firefox driver by calling <samp class="SANS_TheSansMonoCd_W5Regular_11">webdriver.Firefox()</samp>
    and saves it in the variable <samp class="SANS_TheSansMonoCd_W5Regular_11">driver</samp>.
    When you create the Selenium driver, a new Firefox window should open on your
    computer, and a robot icon should appear in the address bar—this is how you know
    that this browser is being controlled by Selenium.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码首先从<samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp>模块导入<samp
    class="SANS_TheSansMonoCd_W5Regular_11">webdriver</samp>。然后通过调用<samp class="SANS_TheSansMonoCd_W5Regular_11">webdriver.Firefox()</samp>来创建一个新的Firefox驱动，并将其保存在变量<samp
    class="SANS_TheSansMonoCd_W5Regular_11">driver</samp>中。当你创建Selenium驱动时，应该会在你的电脑上打开一个新的Firefox窗口，地址栏中会出现一个机器人图标——这就是你知道浏览器正在被Selenium控制的方式。
- en: The code then instructs the browser to load the URL [*https://<wbr>github<wbr>.com<wbr>/micahflee<wbr>/hacks<wbr>-leaks<wbr>-and<wbr>-revelations*](https://github.com/micahflee/hacks-leaks-and-revelations).
    After running the command, you should see Firefox load that GitHub page. Once
    the page is loaded, including all of its JavaScript or other complicated components,
    you can write code to control it. In this case, the code just displays the title
    of the page in the terminal with <samp class="SANS_TheSansMonoCd_W5Regular_11">print(driver.title)</samp>.
    Finally, it quits Firefox.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，代码指示浏览器加载网址[*https://<wbr>github<wbr>.com<wbr>/micahflee<wbr>/hacks<wbr>-leaks<wbr>-and<wbr>-revelations*](https://github.com/micahflee/hacks-leaks-and-revelations)。运行该命令后，你应该会看到Firefox加载该GitHub页面。一旦页面加载完成，包括所有的JavaScript或其他复杂组件，你就可以编写代码来控制它。在这个例子中，代码只是将页面的标题显示在终端上，使用<samp
    class="SANS_TheSansMonoCd_W5Regular_11">print(driver.title)</samp>。最后，它退出了Firefox。
- en: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">Automating Screenshots with
    Selenium</samp>
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: <samp class="SANS_Futura_Std_Bold_Condensed_B_11">使用Selenium自动化截图</samp>
- en: 'Now let’s try something slightly more complicated. In this section, we’ll go
    over a script that will take two arguments: a location name and the filename of
    a screenshot to save. Using Selenium, the script will load Google Maps at [*https://<wbr>maps<wbr>.google<wbr>.com*](https://maps.google.com),
    search for the location, zoom in a little, turn on the satellite images layer,
    and take a screenshot of the satellite image of the location, saving it to disk.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试做一些稍微复杂的事情。在这一节中，我们将讲解一个脚本，它将接受两个参数：一个位置名称和一个截图保存的文件名。使用Selenium，脚本将加载Google地图[*https://<wbr>maps<wbr>.google<wbr>.com*](https://maps.google.com)，搜索该位置，稍微放大地图，开启卫星图层，并截取该位置的卫星图像，保存到磁盘。
- en: While I’m programming web scrapers, I find it helpful to have an interactive
    Python interpreter open in a terminal where I can test out Selenium or BS4 commands,
    allowing me to see if they work in real time without having to start my script
    over. When I’m writing a Selenium script, I open developer tools inside the browser
    I’m driving to inspect all of the HTML tags, which helps me figure out which commands
    to run. Once I get something working, I copy the working code into the script
    that I’m writing in my text editor.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当我编写网页抓取程序时，我发现保持一个交互式Python解释器在终端中开启是很有帮助的，这样我可以实时测试Selenium或BS4命令，看它们是否能正常工作，而不需要重新启动脚本。当我写Selenium脚本时，我会在我控制的浏览器内打开开发者工具，检查所有的HTML标签，这有助于我决定运行哪些命令。一旦有了有效的代码，我就把它复制到我在文本编辑器中写的脚本里。
- en: For example, to search for the location in Google Maps, I needed to make the
    Selenium browser select the search box, type the location, and press ENTER. In
    HTML, tags often have <samp class="SANS_TheSansMonoCd_W5Regular_11">id</samp>
    attributes. By using the Firefox developer tools, I discovered that the search
    box in Google Maps, which is an <samp class="SANS_TheSansMonoCd_W5Regular_11"><input></samp>
    tag, includes the <samp class="SANS_TheSansMonoCd_W5Regular_11">id="searchboxinput"</samp>
    attribute, meaning the search box has an <samp class="SANS_TheSansMonoCd_W5Regular_11">id</samp>
    of <samp class="SANS_TheSansMonoCd_W5Regular_11">searchboxinput</samp>. That allowed
    me to enter code into the Python interpreter that would select the search box,
    type a search query into it, and press ENTER in the browser it was controlling.
    I didn’t always get it right on the first try, but after some trial and error,
    I wrote some working code. At this point, I added that code to my script.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了在 Google 地图中搜索位置，我需要让 Selenium 浏览器选择搜索框，输入位置并按下 ENTER。在 HTML 中，标签通常具有 <samp
    class="SANS_TheSansMonoCd_W5Regular_11">id</samp> 属性。通过使用 Firefox 开发者工具，我发现 Google
    地图中的搜索框是一个 <samp class="SANS_TheSansMonoCd_W5Regular_11"><input></samp> 标签，包含了
    <samp class="SANS_TheSansMonoCd_W5Regular_11">id="searchboxinput"</samp> 属性，这意味着搜索框的
    <samp class="SANS_TheSansMonoCd_W5Regular_11">id</samp> 为 <samp class="SANS_TheSansMonoCd_W5Regular_11">searchboxinput</samp>。这使我能够在
    Python 解释器中输入代码，选择搜索框，输入搜索查询并在它控制的浏览器中按下 ENTER。虽然我并不是每次第一次都能做对，但经过一些试验和错误后，我写出了可以工作的代码。到此为止，我将这段代码添加到了我的脚本中。
- en: I also used developer tools to figure out how to turn on the satellite image
    layer. In the bottom-left corner of Google Maps is an icon called the *minimap*
    that lets you toggle different layers on and off. The developer tools showed me
    that this icon had an <samp class="SANS_TheSansMonoCd_W5Regular_11">id</samp>
    of <samp class="SANS_TheSansMonoCd_W5Regular_11">minimap</samp> and that I could
    click one of the buttons in the minimap element to turn on the satellite layer;
    just like with the search box, I tested clicking this icon in the Python interpreter
    until I got it working.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我还使用开发者工具来弄清楚如何打开卫星图层。在 Google 地图的左下角有一个叫做 *minimap* 的图标，可以让你切换不同的图层。开发者工具显示，这个图标的
    <samp class="SANS_TheSansMonoCd_W5Regular_11">id</samp> 是 <samp class="SANS_TheSansMonoCd_W5Regular_11">minimap</samp>，我可以点击
    minimap 元素中的一个按钮来打开卫星图层；就像搜索框一样，我在 Python 解释器中测试点击这个图标，直到它成功为止。
- en: 'The following script, *selenium-example.py,* uses Selenium to take satellite
    image screenshots from Google Maps for you:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本 *selenium-example.py* 使用 Selenium 从 Google 地图获取卫星图像截图：
- en: '[PRE14]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: First, the script imports the <samp class="SANS_TheSansMonoCd_W5Regular_11">click</samp>
    and <samp class="SANS_TheSansMonoCd_W5Regular_11">time</samp> modules and then
    several components from the <samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp>
    module. Specifically, it imports <samp class="SANS_TheSansMonoCd_W5Regular_11">webdriver</samp>,
    the component required to actually launch and control a web browser. It also imports
    <samp class="SANS_TheSansMonoCd_W5Regular_11">Keys</samp> and <samp class="SANS_TheSansMonoCd_W5Regular_11">By</samp>
    to automate pressing ENTER after searching and to search for HTML elements by
    their <samp class="SANS_TheSansMonoCd_W5Regular_11">id</samp> attribute.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，脚本导入了 <samp class="SANS_TheSansMonoCd_W5Regular_11">click</samp> 和 <samp
    class="SANS_TheSansMonoCd_W5Regular_11">time</samp> 模块，然后从 <samp class="SANS_TheSansMonoCd_W5Regular_11">selenium</samp>
    模块导入了几个组件。具体来说，它导入了 <samp class="SANS_TheSansMonoCd_W5Regular_11">webdriver</samp>，这是实际启动和控制网页浏览器所需的组件。它还导入了
    <samp class="SANS_TheSansMonoCd_W5Regular_11">Keys</samp> 和 <samp class="SANS_TheSansMonoCd_W5Regular_11">By</samp>，用来在搜索后自动按下
    ENTER 并通过 <samp class="SANS_TheSansMonoCd_W5Regular_11">id</samp> 属性查找 HTML 元素。
- en: <samp class="SANS_Dogma_OT_Bold_B_21">NOTE</samp>
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: <samp class="SANS_Dogma_OT_Bold_B_21">注意</samp>
- en: '*Exactly what you need to import from <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">selenium</samp>
    depends on what you’re trying to do. Consult the Selenium for Python documentation
    to learn exactly what you need and when—that’s how I figured it out.*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*你需要从 <samp class="SANS_TheSansMonoCd_W5Regular_Italic_I_11">selenium</samp>
    导入的内容，取决于你想要做什么。查阅 Selenium for Python 文档，了解确切的需求和使用时机——我就是通过这个方式弄明白的。*'
- en: The code includes Click decorators before the <samp class="SANS_TheSansMonoCd_W5Regular_11">main()</samp>
    function, making this a command line program that takes two arguments, <samp class="SANS_TheSansMonoCd_W5Regular_11">location</samp>
    and <samp class="SANS_TheSansMonoCd_W5Regular_11">screenshot_filename</samp>.
    The <samp class="SANS_TheSansMonoCd_W5Regular_11">location</samp> variable is
    a Google Maps search query, like *Manhattan, NY* or *The Great Pyramid of Giza*,
    and <samp class="SANS_TheSansMonoCd_W5Regular_11">screenshot_filename</samp> is
    the path to save the final screenshot.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 代码在 <samp class="SANS_TheSansMonoCd_W5Regular_11">main()</samp> 函数之前包含了 Click
    装饰器，这使得它成为一个命令行程序，该程序接受两个参数，<samp class="SANS_TheSansMonoCd_W5Regular_11">location</samp>
    和 <samp class="SANS_TheSansMonoCd_W5Regular_11">screenshot_filename</samp>。<samp
    class="SANS_TheSansMonoCd_W5Regular_11">location</samp> 变量是一个 Google 地图搜索查询，如
    *曼哈顿，纽约* 或 *吉萨大金字塔*，而 <samp class="SANS_TheSansMonoCd_W5Regular_11">screenshot_filename</samp>
    是保存最终截图的路径。
- en: When the <samp class="SANS_TheSansMonoCd_W5Regular_11">main()</samp> function
    runs, the code starts by creating a Selenium web driver, which should open a Firefox
    window that the script will then control. The <samp class="SANS_TheSansMonoCd_W5Regular_11">driver.implicitly_wait(10)</samp>
    function tells Selenium to wait up to 10 seconds for page elements to load. The
    code loads [*https://<wbr>maps<wbr>.google<wbr>.com*](https://maps.google.com)
    in Firefox with the <samp class="SANS_TheSansMonoCd_W5Regular_11">driver.get()</samp>
    function, then finds the search box element on the page, storing it in the variable
    <samp class="SANS_TheSansMonoCd_W5Regular_11">search_box</samp>. It finds the
    search box by running <samp class="SANS_TheSansMonoCd_W5Regular_11">driver.find_element(By.ID,
    "searchboxinput")</samp>. Once the code has this search box object stored in <samp
    class="SANS_TheSansMonoCd_W5Regular_11">search_box</samp>, it clears any text
    in the text box by calling the <samp class="SANS_TheSansMonoCd_W5Regular_11">clear()</samp>
    method on it, and then it types the text in the <samp class="SANS_TheSansMonoCd_W5Regular_11">location</samp>
    string by calling <samp class="SANS_TheSansMonoCd_W5Regular_11">send_keys(location)</samp>.
    Finally, it presses ENTER to search for this location by calling <samp class="SANS_TheSansMonoCd_W5Regular_11">send_keys(Keys.RETURN)</samp>.
    At this point, Google Maps should search for the location.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当 <samp class="SANS_TheSansMonoCd_W5Regular_11">main()</samp> 函数运行时，代码首先创建一个
    Selenium 网页驱动，这应该会打开一个 Firefox 窗口，脚本将控制该窗口。<samp class="SANS_TheSansMonoCd_W5Regular_11">driver.implicitly_wait(10)</samp>
    函数告诉 Selenium 等待最多 10 秒钟，直到页面元素加载完成。代码通过 <samp class="SANS_TheSansMonoCd_W5Regular_11">driver.get()</samp>
    函数在 Firefox 中加载 [*https://<wbr>maps<wbr>.google<wbr>.com*](https://maps.google.com)，然后找到页面上的搜索框元素，并将其存储在变量
    <samp class="SANS_TheSansMonoCd_W5Regular_11">search_box</samp> 中。它通过运行 <samp
    class="SANS_TheSansMonoCd_W5Regular_11">driver.find_element(By.ID, "searchboxinput")</samp>
    来找到搜索框。一旦代码将该搜索框对象存储在 <samp class="SANS_TheSansMonoCd_W5Regular_11">search_box</samp>
    中，它通过调用 <samp class="SANS_TheSansMonoCd_W5Regular_11">clear()</samp> 方法清除文本框中的任何文本，然后通过调用
    <samp class="SANS_TheSansMonoCd_W5Regular_11">send_keys(location)</samp> 输入 <samp
    class="SANS_TheSansMonoCd_W5Regular_11">location</samp> 字符串中的文本。最后，它按下 ENTER 键以通过调用
    <samp class="SANS_TheSansMonoCd_W5Regular_11">send_keys(Keys.RETURN)</samp> 搜索该位置。此时，Google
    地图应该开始搜索该位置。
- en: The code then zooms in by selecting the <samp class="SANS_TheSansMonoCd_W5Regular_11"><body></samp>
    tag, the main HTML tag that contains all other tags, then telling Firefox to press
    the + key twice, which is the Google Maps keyboard shortcut to zoom in.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，代码通过选择 <samp class="SANS_TheSansMonoCd_W5Regular_11"><body></samp> 标签进行缩放，该标签是包含所有其他标签的主要
    HTML 标签，然后告诉 Firefox 按下 + 键两次，这是 Google 地图的缩放快捷键。
- en: At this point, Firefox has loaded Google Maps, searched for a location, and
    zoomed in on that location. The code then turns on the satellite image layer by
    locating the minimap in the corner of the screen. Once it finds this, it locates
    all of the <samp class="SANS_TheSansMonoCd_W5Regular_11"><button></samp> tags
    inside the minimap by calling the <samp class="SANS_TheSansMonoCd_W5Regular_11">find_elements(By.TAG_NAME,
    "button")</samp> method, and then it clicks the third button, calling the <samp
    class="SANS_TheSansMonoCd_W5Regular_11">click()</samp> method on the third element
    (which has an index of <samp class="SANS_TheSansMonoCd_W5Regular_11">2</samp>)
    on the list of buttons. This turns on the satellite images layer.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，Firefox已加载了Google地图，搜索了一个位置，并将视图放大至该位置。接着，代码通过定位屏幕角落的迷你地图，开启了卫星图像层。它找到后，通过调用<samp
    class="SANS_TheSansMonoCd_W5Regular_11">find_elements(By.TAG_NAME, "button")</samp>方法定位迷你地图中的所有<button>标签，然后点击列表中的第三个按钮，调用<samp
    class="SANS_TheSansMonoCd_W5Regular_11">click()</samp>方法（该按钮的索引为<samp class="SANS_TheSansMonoCd_W5Regular_11">2</samp>）。这将开启卫星图像层。
- en: Finally, the script waits two seconds, just to make sure the satellite images
    have finished loading, and then saves a screenshot of the web page to <samp class="SANS_TheSansMonoCd_W5Regular_11">screenshot_filename</samp>.
    When it’s done, it quits Firefox.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，脚本等待两秒钟，以确保卫星图像已加载完成，然后将网页截图保存为<samp class="SANS_TheSansMonoCd_W5Regular_11">screenshot_filename</samp>。完成后，它退出Firefox。
- en: You can find a complete copy of this code in the book’s GitHub repo at [*https://<wbr>github<wbr>.com<wbr>/micahflee<wbr>/hacks<wbr>-leaks<wbr>-and<wbr>-revelations<wbr>/blob<wbr>/main<wbr>/appendix<wbr>-b<wbr>/selenium<wbr>-example<wbr>.py*](https://github.com/micahflee/hacks-leaks-and-revelations/blob/main/appendix-b/selenium-example.py).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的GitHub仓库中找到该代码的完整副本，地址是[*https://<wbr>github<wbr>.com<wbr>/micahflee<wbr>/hacks<wbr>-leaks<wbr>-and<wbr>-revelations<wbr>/blob<wbr>/main<wbr>/appendix<wbr>-b<wbr>/selenium<wbr>-example<wbr>.py*](https://github.com/micahflee/hacks-leaks-and-revelations/blob/main/appendix-b/selenium-example.py)。
- en: 'You can use *selenium-example.py* to generate Google Maps screenshots of any
    location you like. For example, I ran the following command:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用*selenium-example.py*生成你喜欢的任何位置的Google地图截图。例如，我运行了以下命令：
- en: '[PRE15]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This opened a Firefox window that was controlled by Selenium. It loaded Google
    Maps, searched for *great pyramid of giza*, zoomed in, turned on the satellite
    images layer, and saved a screenshot of the window in the file *giza.png*. [Figure
    B-3](#figB-3) shows *giza.png*, scraped from Google Maps.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这打开了一个由Selenium控制的Firefox窗口。它加载了Google地图，搜索了*吉萨大金字塔*，缩小了视图，开启了卫星图像层，并将窗口截图保存为文件*giza.png*。[图B-3](#figB-3)显示了从Google地图抓取的*giza.png*。
- en: '![A screenshot that Selenium created showing a satellite image of the Great
    Pyramid of Giza. The Google Maps interface is displayed on the left, including
    a photo of the pyramid, information reviews, its address, and so on.](Images/FigureB-3.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![Selenium创建的截图，显示了吉萨大金字塔的卫星图像。左侧显示了Google地图界面，包括金字塔的照片、信息评论、地址等。](Images/FigureB-3.jpg)'
- en: '<samp class="SANS_Futura_Std_Book_Oblique_I_11">Figure B-3: A satellite image
    of the Great Pyramid of Giza from Selenium</samp>'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: <samp class="SANS_Futura_Std_Book_Oblique_I_11">图B-3：来自Selenium的吉萨大金字塔卫星图像</samp>
- en: On your own, it might also be fun to try searching for *US Capitol*; *Washington,
    DC*; *Kremlin, Moscow*; or *Tokyo, Japan*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以自己尝试搜索*美国国会大厦*；*华盛顿特区*；*克里姆林宫，莫斯科*；或者*东京，日本*，这可能会很有趣。
- en: This example script used Selenium to take screenshots. You could modify it so
    that Selenium automatically takes a screenshot each time a public figure posts
    to social media, so you’ll have a record of it in case they delete it. You’re
    not limited to cataloging information in this way, though; you can also use Selenium
    to extract information from web pages and store them in CSV spreadsheets or any
    other format you’d like, just like you can with BS4.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例脚本使用Selenium来截取截图。你可以修改它，使得每当公众人物在社交媒体上发布内容时，Selenium自动截图，这样你就可以保留记录，以防他们删除内容。不过，你不止可以以这种方式存档信息；你还可以使用Selenium从网页中提取信息，并将其存储到CSV电子表格或任何你想要的格式，就像使用BS4一样。
- en: To learn more about Selenium for Python, check out its documentation at [*https://<wbr>selenium<wbr>-python<wbr>.readthedocs<wbr>.io*](https://selenium-python.readthedocs.io).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于Python Selenium的信息，请查看其文档，访问[*https://<wbr>selenium<wbr>-python<wbr>.readthedocs<wbr>.io*](https://selenium-python.readthedocs.io)。
- en: <samp class="SANS_Futura_Std_Bold_B_11">Next Steps</samp>
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <samp class="SANS_Futura_Std_Bold_B_11">下一步</samp>
- en: In this appendix, I’ve gone over a few techniques for web scraping and provided
    some simple example scripts to show off the basics of how they work. However,
    in order to write code for your future web scraping projects, you’ll probably
    need to learn more about web development than is covered in this book, depending
    on what site you’re trying to scrape. For example, your HTTPX and BS4 scraper
    might need to first log in to a website and then make all of its future requests
    as that logged-in user in order to access the content you’re after. This would
    require making HTTP POST requests instead of just GET requests and keeping track
    of cookies, neither of which I’ve covered here.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录中，我介绍了一些网页抓取的技巧，并提供了一些简单的示例脚本，以展示它们的基本工作原理。然而，为了编写你未来网页抓取项目的代码，你可能需要学习更多的网页开发知识，而这些内容本书中并未涉及，这取决于你要抓取的网站。例如，你的HTTPX和BS4抓取器可能需要先登录到网站，然后以该登录用户身份发起后续的所有请求，才能访问你需要的内容。这就需要发送HTTP
    POST请求而不仅仅是GET请求，并且需要跟踪Cookies，这些在本书中并未涉及。
- en: As a next step, I recommend getting more comfortable with the developer tools
    built into browsers. This will help familiarize you with the HTTP requests your
    browser makes and what their responses include. Spend more time browsing the layout
    of HTML elements, as you did in this appendix. The more you learn about web development,
    including more complex topics like HTTP headers and cookies, the easier it will
    be for you to scrape the web. If you can access information in a web browser,
    you can write a script that automates accessing that information.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，我建议你更加熟悉浏览器内置的开发者工具。这将帮助你了解浏览器发出的HTTP请求以及它们的响应内容。花更多时间浏览HTML元素的布局，就像你在本附录中所做的那样。你了解的关于网页开发的内容越多，包括像HTTP头部和Cookies这样的复杂话题，网页抓取就会变得越容易。如果你能够在网页浏览器中访问某些信息，那么你就可以编写一个脚本来自动化访问这些信息。
