<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><h2 class="h1" id="ch09"><span epub:type="pagebreak" id="page_151" class="calibre2"/><strong class="calibre3"><span class="big">9</span><br class="calibre18"/>CUTTING THINGS DOWN TO SIZE: REGULARIZATION</strong></h2>
<div class="imagec"><img alt="Image" src="../images/common.jpg" class="calibre14"/></div>
<p class="noindent">A number of modern statistical methods “shrink” their classical counterparts. This is true for ML methods as well. In particular, the principle may be applied in:</p>
<ul class="calibre15">
<li class="noindent3">Boosting (covered in <a href="ch06.xhtml#ch06lev3sec8" class="calibre12">Section 6.3.8</a>)</li>
<li class="noindent3">Linear models</li>
<li class="noindent3">Support vector machines</li>
<li class="noindent3">Neural networks</li>
</ul>
<p class="indent">In this chapter, we’ll see why that may be advantageous and apply it to the linear model case. This will also lay the foundation for material in future chapters on support vector machines and neural networks.</p>
<h3 class="h2" id="ch09lev1">9.1 Motivation</h3>
<p class="noindent">Suppose we have sample data on human height, weight, and age. We denote the population means of these quantities by <em class="calibre13">μ</em><sub class="calibre27"><em class="calibre13">ht</em></sub>, <em class="calibre13">μ</em><sub class="calibre27"><em class="calibre13">wt</em></sub> and <em class="calibre13">μ</em><sub class="calibre27"><em class="calibre13">age</em></sub>. We estimate them from our sample data as the corresponding sample means, <img alt="Image" class="middle18" src="../images/unch09equ01.jpg"/> and <img alt="Image" class="middle19" src="../images/unch09equ02.jpg"/>.<span epub:type="pagebreak" id="page_152"/></p>
<p class="indent">We then add just a bit more notation, grouping these quantities into vectors</p>
<p class="center" id="ch09equ01"><img alt="Image" src="../images/ch09equ01.jpg" class="calibre57"/></p>
<p class="noindent">and</p>
<p class="center" id="ch09equ02"><img alt="Image" src="../images/ch09equ02.jpg" class="calibre58"/></p>
<p class="noindent">Amazingly, <em class="calibre13">James−Stein theory</em> says the best estimate of <em class="calibre13">μ</em> might NOT be <img alt="Image" class="middle20" src="../images/unch09equ03.jpg"/>. It might be a shrunken-down version of <img alt="Image" class="middle20" src="../images/unch09equ03.jpg"/>, say, 0.9<img alt="Image" src="../images/unch09equ03.jpg" class="calibre59"/>:</p>
<p class="center" id="ch09equ03"><img alt="Image" src="../images/ch09equ03.jpg" class="calibre60"/></p>
<p class="noindent">And the higher the dimension (3 here), the more shrinking needs to be done.</p>
<p class="indent">The intuition is this: for many samples, there are a few data points that are extreme on the fringes of the distribution. These points skew our estimators in the direction of being too large. So, it is optimal to shrink the estimators.</p>
<p class="indent">Note that, usually, different components of a vector will be shrunken by different amounts. Instead of <a href="ch09.xhtml#ch09equ03" class="calibre12">Equation 9.3</a>, the best estimator might be:</p>
<p class="center" id="ch09equ04"><img alt="Image" src="../images/ch09equ04.jpg" class="calibre61"/></p>
<p class="noindent">In this example, the second component actually expanded rather than shrank. Shrinking refers to the overall size of the vector (defined in the next section) and not the individual components.</p>
<p class="indent">How much shrinking should be done? In practice, this is typically decided by our usual approach of cross-validation.</p>
<p class="indent">Putting aside the mathematical theory—it’s quite deep—the implication for us in this book is that, for instance, the least squares estimator <img alt="Image" class="middle21" src="../images/betacap1.jpg"/> of the population coefficient vector <em class="calibre13">β</em> in the linear model is often too large and should be shrunken. Most interesting, <em class="calibre13">this turns out to be a possible remedy for overfitting</em>.</p>
<h3 class="h2" id="ch09lev2">9.2 Size of a Vector</h3>
<p class="noindent">Is the vector (15.2,3.0,−6.8) “large”? What do we mean by its size, anyway?</p>
<p class="indent">There are two main measures, called <em class="calibre13">ℓ</em><sub class="calibre27">1</sub> and <em class="calibre13">ℓ</em><sub class="calibre27">2</sub>, that are denoted by the “norm” notation, || || (two pairs of vertical bars). So the two norms are denoted || ||<sub class="calibre27">1</sub> and || ||<sub class="calibre27">2</sub>. For the example above, the <em class="calibre13">ℓ</em><sub class="calibre27">1</sub> norm is</p>
<p class="center" id="ch09equ05"><img alt="Image" src="../images/ch09equ05.jpg" class="calibre62"/></p>
<p class="noindent">that is, the sum of the absolute values of the vector elements. Here is the <em class="calibre13">ℓ</em><sub class="calibre27">2</sub> case:</p>
<p class="center" id="ch09equ06"><img alt="Image" src="../images/ch09equ06.jpg" class="calibre63"/></p>
<p class="noindent">This is the square root of the sums of squares of the vector elements. (Readers who remember their school geometry may notice that in 2 dimensions, this is simply the length of the diagonal of a right triangle—the famous Pythagorean theorem.)<span epub:type="pagebreak" id="page_153"/></p>
<h3 class="h2" id="ch09lev3">9.3 Ridge Regression and the LASSO</h3>
<p class="noindent">For years, James−Stein theory was mainly a mathematical curiosity suitable for theoretical research but not affecting mainstream data analysis. There was some usage of <em class="calibre13">ridge regression</em>, to be introduced below, but even that was limited. The big change came from the development of the <em class="calibre13">Least Absolute Shrinkage and Selection Operator (LASSO)</em> and its adoption by the ML community.</p>
<h4 class="h3" id="ch09lev3sec1"><em class="calibre22"><strong class="calibre3">9.3.1 How They Work</strong></em></h4>
<p class="noindent">Recall the basics of the least squares method for linear models, say, for the case of one feature: we choose <img alt="Image" class="middle22" src="../images/unch09equ07.jpg"/> to minimize the sum of squared prediction errors, as in <a href="ch08.xhtml#ch08equ10" class="calibre12">Equation 8.10</a>. For convenience, here is a copy of that expression:</p>
<p class="center" id="ch09equ07"><img alt="Image" src="../images/ch09equ07.jpg" class="calibre64"/></p>
<p class="noindent">The idea of ridge regression was to “put a damper” on that by adding vector size limitation. We now minimize <a href="ch09.xhtml#ch09equ07" class="calibre12">Equation 9.7</a>, <em class="calibre13">subject to the following constraint</em>:</p>
<p class="center" id="ch09equ08"><img alt="Image" src="../images/ch09equ08.jpg" class="calibre65"/></p>
<p class="noindent">Here <em class="calibre13">η</em> &gt; 0 is a hyperparameter set by the user, say, via cross-validation. The minimizing values of <img alt="Image" class="middle13" src="../images/unch08equ10.jpg"/> and <img alt="Image" class="middle12" src="../images/unch08equ09.jpg"/> are the ridge coefficients.</p>
<p class="indent">Here is the intuition behind such an approach. We are basically saying we wish to minimize the sum of squares as before <em class="calibre13">but</em> without allowing <img alt="Image" class="middle23" src="../images/unch09equ08.jpg"/> to get too large. It’s a compromise between, on the one hand, predicting the <em class="calibre13">Y</em><em class="calibre13"><sub class="calibre27">i</sub></em> well and, on the other, limiting the size of <img alt="Image" class="middle23" src="../images/unch09equ08.jpg"/>. (We hope that the shrinking will improve our prediction of future cases.) The hyperparameter <em class="calibre13">η</em> controls that trade-off.</p>
<p class="indent">It can be shown that this constrained minimization problem is equivalent to choosing <img alt="Image" class="middle13" src="../images/unch08equ10.jpg"/> and <img alt="Image" class="middle12" src="../images/unch08equ09.jpg"/> to minimize the quantity:</p>
<p class="center" id="ch09equ09"><img alt="Image" src="../images/ch09equ09.jpg" class="calibre66"/></p>
<p class="noindent">Here <em class="calibre13">λ</em> &gt; 0 is a hyperparameter that takes the place of <em class="calibre13">η</em>, which, again, is typically set via cross-validation.</p>
<p class="indent">This formulation (<a href="ch09.xhtml#ch09equ09" class="calibre12">Equation 9.9</a>) is actually the standard definition of ridge regression. The <em class="calibre13">η</em> version is easier to explain in terms of the James−Stein context, but this <em class="calibre13">λ</em> formulation should also make intuitive sense: that last term “penalizes” us in our minimizing the sum of squares. The larger we set <em class="calibre13">λ</em>, the greater the penalty, forcing us to limit the size of <img alt="Image" class="middle23" src="../images/unch09equ08.jpg"/>.</p>
<p class="indent">The LASSO version is almost the same as ridge but with an <em class="calibre13">ℓ</em><sub class="calibre27">1</sub> “damper” term rather than <em class="calibre13">ℓ</em><sub class="calibre27">2</sub>. It finds the values of <img alt="Image" class="middle13" src="../images/unch08equ10.jpg"/> and <img alt="Image" class="middle12" src="../images/unch08equ09.jpg"/> that minimize</p>
<p class="center" id="ch09equ10"><img alt="Image" src="../images/ch09equ10.jpg" class="calibre67"/><span epub:type="pagebreak" id="page_154"/></p>
<p class="noindent">In terms of <em class="calibre13">η</em>, for LASSO we minimize</p>
<p class="center" id="ch09equ11"><img alt="Image" src="../images/ch09equ11.jpg" class="calibre68"/></p>
<p class="noindent">subject to:</p>
<p class="center" id="ch09equ12"><img alt="Image" src="../images/ch09equ12.jpg" class="calibre65"/></p>
<h4 class="h3" id="ch09lev3sec2"><em class="calibre22"><strong class="calibre3">9.3.2 The Bias-Variance Trade-off, Avoiding Overfitting</strong></em></h4>
<p class="noindent">A major reason that the idea of shrinkage—often called <em class="calibre13">regularization</em>—has had such an impact on statistics and ML is that it is a tool to avoid overfitting. Here are the issues:</p>
<ul class="calibre15">
<li class="noindent3">On the one hand, we want to make the prediction sum of squares as small as possible, which can be shown to eliminate bias.</li>
<li class="noindent3">On the other hand, recall from <a href="ch08.xhtml#ch08lev8" class="calibre12">Section 8.8</a> that the sum of squares can be overly optimistic and thus smaller than we would get in predicting new cases in the future. A small value for that sum of squares may come with a large variance, due in part to the influence of extreme data points, as discussed earlier. Shrinkage reduces variance— a smaller quantity varies less than a larger one—thus partially neutralizing the pernicious effects of the extreme points.</li>
</ul>
<p class="noindent">So the hyperparameter <em class="calibre13">λ</em> is used to control where we want to be in that Bias-Variance Trade-off. Overfitting occurs when we are on the wrong side of that trade-off.</p>
<p class="indent"><em class="calibre13">The bottom line</em>: shrinkage reduces variance, and if this can be done without increasing bias much, it’s a win.</p>
<p class="indent">Again, regularization is used not only in the linear model, the case studied in this chapter, but also in support vector machines, neural nets, and so on. It can even be used in principal component analysis.</p>
<h4 class="h3" id="ch09lev3sec3"><em class="calibre22"><strong class="calibre3">9.3.3 Relation Between</strong></em> <em class="calibre22">λ<strong class="calibre3">, n, and p</strong></em></h4>
<p class="noindent">Again, the Bias-Variance Trade-off notion plays a central role here, with implications for dataset size. The larger <em class="calibre13">n</em> is (that is, the larger the sample size), the smaller the variance in <img alt="Image" class="middle21" src="../images/betacap1.jpg"/>, which means the lesser the need to shrink.</p>
<p class="indent">In other words, for large datasets, we may not need regularization. But recall from <a href="ch03.xhtml" class="calibre12">Chapter 3</a> that “large <em class="calibre13">n</em>” here is meant both in absolute terms and relative to <em class="calibre13">p</em>—for example, by the <img alt="Image" class="middle4" src="../images/unch08equ08.jpg"/> criterion following <a href="ch03.xhtml#ch03equ02" class="calibre12">Equation 3.2</a>. So, a very large dataset may still need regularization if there are numerous features.</p>
<p class="indent">In any event, the surest way to settle whether shrinkage is needed in a particular setting is to try it, once again, with cross-validation.</p>
<h4 class="h3" id="ch09lev3sec4"><em class="calibre22"><strong class="calibre3">9.3.4 Comparison, Ridge vs. LASSO</strong></em></h4>
<p class="noindent">The advantage of ridge regression is that its calculation is simple. There is an explicit, closed-form solution—that is, it is noniterative; the LASSO <span epub:type="pagebreak" id="page_155"/>requires iterative computation (though it does not have convergence problems).</p>
<p class="indent">But the success of the LASSO is due to its providing a <em class="calibre13">sparse</em> solution, meaning that often many of the elements of <img alt="Image" class="middle21" src="../images/betacap1.jpg"/> are 0s. The smaller we set <em class="calibre13">η</em>, the more 0s we have. We then discard the features having <img alt="Image" class="middle11" src="../images/unch08equ07.jpg"/> = 0, thereby achieving dimension reduction. Note that, of course, the resulting nonzero <img alt="Image" class="middle11" src="../images/unch08equ07.jpg"/> values are different from the corresponding OLS values.</p>
<h3 class="h2" id="ch09lev4">9.4 Software</h3>
<p class="noindent">Once again, we will use a <span class="literal">qe*</span>-series function, <span class="literal">qeLASSO()</span>, with the following call form:</p>
<pre class="calibre16">qeLASSO(data,yName,alpha = 1,
   holdout = floor(min(1000, 0.1 * nrow(data))))</pre>
<p class="noindent">The function wraps <span class="literal">cv.glmnet()</span> in the <span class="literal">glmnet</span> package. That package allows the user to specify either ridge regression or LASSO via the argument <span class="literal">alpha</span>, setting that value to 0 or 1, respectively; the default is LASSO. One can also set <span class="literal">alpha</span> to an intermediate value, combining the two approaches, something termed the <em class="calibre13">elastic net</em>.</p>
<p class="indent">The <span class="literal">cv.glmnet()</span> algorithm will start with a huge value of <em class="calibre13">λ</em> and then progressively reduce <em class="calibre13">λ</em>. This corresponds to starting with a very tiny value of <em class="calibre13">η</em> and progressively increasing it. Since a very tiny value of <em class="calibre13">η</em> means that no features are allowed, progressively increasing it means we start adding features. It is all arranged so that we add one feature at a time. The algorithm computes MSPE or OME at each step, using its own built-in cross-validation. The return value of the <span class="literal">qeLASSO()</span> wrapper is actually the object returned by <span class="literal">cv.glmnet()</span>, with a few additional components, such as <span class="literal">testAcc</span>.</p>
<p class="indent">That object will include one set of results for each value of <em class="calibre13">λ</em> run by the code. So, there will be one <img alt="Image" class="middle21" src="../images/betacap1.jpg"/> vector for each <em class="calibre13">λ</em>. However, when we do subsequent prediction, the code uses the specific value of <em class="calibre13">λ</em> that had the smallest mean cross-validated prediction error.</p>
<h3 class="h2" id="ch09lev5">9.5 Example: NYC Taxi Data</h3>
<p class="noindent">Let’s return to the New York City taxi data from <a href="ch05.xhtml#ch05lev3" class="calibre12">Section 5.3</a>.</p>
<pre class="calibre16">&gt; <span class="codestrong">yellout &lt;- qeLASSO(yell10k,'tripTime')</span>
&gt; <span class="codestrong">yellout$testAcc</span>
[1] 258.4983
&gt; <span class="codestrong">yellout$baseAcc</span>
[1] 442.4428</pre>
<p class="indent">We see that the features definitely are helpful in prediction, yielding a large reduction in MAPE relative to just using the overall mean for prediction.</p>
<p class="indent"><span epub:type="pagebreak" id="page_156"/>Recall that the LASSO typically yields a sparse <img alt="Image" class="middle21" src="../images/betacap1.jpg"/>, meaning that most of the coefficients are 0s. In this way, the LASSO can be used for dimension reduction, in addition to being used as a predictive model in its own right. Let’s explore this for the taxi data by inspecting the <span class="literal">coefs</span> component of the output.</p>
<p class="indent">Note first that, as usual, the features that are R factors are converted to dummy variables. How many are there?</p>
<pre class="calibre16">&gt; <span class="codestrong">length(yellout$coefs)</span>
[1] 475</pre>
<p class="indent">Considering that the original dataset had only 5 features, 475 is quite a lot! But remember, two of our features were the pickup and dropoff locations, of which there are hundreds, and thus hundreds of dummies.</p>
<p class="indent">Well, which coefficients are nonzero?</p>
<pre class="calibre16">&gt; <span class="codestrong">yellout$coefs</span>
475 x 1 sparse Matrix of class "dgCMatrix"
                           1
(Intercept)       401.500380
passenger_count     .
trip_distance     128.666529
PULocationID.1      .
PULocationID.3      .
PULocationID.4      .
PULocationID.7      .
PULocationID.8      .
...
PULocationID.130    .
PULocationID.131    .
PULocationID.132 -263.807074
PULocationID.133    .
...
PULocationID.263    .
PULocationID.264    .
DOLocationID.1     -4.005357
DOLocationID.3      .
...
DOLocationID.262    .
DOLocationID.263    .
DOLocationID.264    .
PUweekday           3.030196
&gt; <span class="codestrong">sum(yellout$coefs != 0)</span>
[1] 11</pre>
<p class="indent">Only 11 coefficients are nonzero, including pickup location 132 and dropoff location 1. That’s impressive dimension reduction.<span epub:type="pagebreak" id="page_157"/></p>
<h3 class="h2" id="ch09lev6">9.6 Example: Airbnb Data</h3>
<p class="noindent">Let’s revisit the Airbnb dataset analyzed in <a href="ch08.xhtml#ch08lev4sec3" class="calibre12">Section 8.4.3</a>, where we are predicting monthly rent.</p>
<pre class="calibre16">&gt; <span class="codestrong">Abb$square_feet &lt;- NULL</span>
&gt; <span class="codestrong">Abb$weekly_price &lt;- NULL</span>
&gt; <span class="codestrong">Abb &lt;- na.exclude(Abb)</span>
&gt; <span class="codestrong">z &lt;- qeLASSO(Abb,'monthly_price',holdout=NULL)</span></pre>
<p class="indent">The <span class="literal">qeLASSO()</span> function wraps <span class="literal">cv.glmnet()</span>. The latter has a generic <span class="literal">plot()</span> function, which we can access here:</p>
<pre class="calibre16">&gt; <span class="codestrong">plot(z)</span></pre>
<p class="indent">The plot, shown in <a href="ch09.xhtml#ch09fig01" class="calibre12">Figure 9-1</a>, displays the classic Bias-Variance Tradeoff, which is essentially U-shaped. As <em class="calibre13">λ</em> increases to the right (lower horizontal axis; the log is used), the number of nonzero coefficients (upper horizontal axis) decreases. At first, this produces reductions in MSPE. However, after we hit about 26 nonzero coefficients, this quantity rises. In bias-variance terms, increasing <em class="calibre13">λ</em> brought large reductions in variance with little increase in bias. But after hitting 26 features, the bias became the dominant factor.</p>
<p class="indent">At any rate, using 26 features, corresponding to <em class="calibre13">λ</em> ≈ <em class="calibre13">e</em><sup class="calibre11">4</sup> = 53.9, seems best, yielding a very substantial improvement in prediction accuracy. (Standard errors are also shown in the vertical bars extending above and below the curve.)</p>
<div class="image"><img alt="Image" id="ch09fig01" src="../images/ch09fig01.jpg" class="calibre69"/></div>
<p class="figcap"><em class="calibre13">Figure 9-1: MSPE, Airbnb data</em><span epub:type="pagebreak" id="page_158"/></p>
<p class="indent">Let’s try a prediction, say, taking row 18 from our data and changing the security deposit to $360 and the rating to 92. What would be our predicted value for the rent?</p>
<pre class="calibre16">&gt; <span class="codestrong">x18 &lt;- Abb[18,-4]</span>
&gt; <span class="codestrong">x18[4] &lt;- 360</span>
&gt; <span class="codestrong">x18[8] &lt;- 92</span>
&gt; <span class="codestrong">predict(z,x18)</span>
            1
[1,] 3750.618</pre>
<p class="indent">How much did our shrinkage approach change the coefficients compared to the OLS output in <a href="ch08.xhtml#ch08lev4sec4" class="calibre12">Section 8.4.4</a>? Well, for example, the estimated average premium for living in ZIP code 94123 was $1,639.61 with the OLS model. What is it now, using LASSO?</p>
<pre class="calibre16">&gt; <span class="codestrong">z$coefs</span>
...
zipcode.94118           .
zipcode.94121           .
zipcode.94122           .
zipcode.94123         698.6044574
zipcode.94124           .
zipcode.94127           .
...</pre>
<p class="indent">Ah, so it did shrink. On the other hand, LASSO shrinks the vector, not necessarily individual elements, which could even grow a bit. Of course, many elements were indeed shrunken all the way to 0.</p>
<p class="indent">Recall how the process works: it begins with no features in the model at all, which corresponds to a huge value of <em class="calibre13">λ</em>. At each step, <em class="calibre13">λ</em> is reduced, possibly resulting in our acquiring a new feature. We can also view the order in which the features were brought into the model:</p>
<pre class="calibre16">&gt; <span class="codestrong">z$whenEntered</span>
            bedrooms      guests_included     security_deposit
                   2                   10                   13
       zipcode.94112 review_scores_rating        zipcode.94123
                  15                   16                   17
       zipcode.94133        zipcode.94105        zipcode.94117
                  22                   24                   24
       zipcode.94124        zipcode.94127        zipcode.94114
                  24                   25                   26
       zipcode.94111        zipcode.94131            bathrooms
                  27                   27                   27
       zipcode.94110        zipcode.94116        zipcode.94102
                  28                   29                   30
       zipcode.94108        zipcode.94122        zipcode.94118
                  30                   30                   31
       zipcode.94132        zipcode.94107        zipcode.94121
                  31                   33                   34
       zipcode.94158        zipcode.94115        zipcode.94134
                  35                   37                   37
       zipcode.94104        zipcode.94109       minimum_nights
                  44                   56                   57
      maximum_nights
                  57</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_159"/>Not too surprisingly, the first feature chosen by the process was the number of bedrooms. But perhaps less intuitively, the process’s second choice was a dummy variable regarding guests. Our example above, a dummy for ZIP code 94123, came in at the 17th step. One might view this ordering as a report on the importance of each selected feature.</p>
<div class="note">
<p class="notet"><strong class="calibre3"><span class="notes">NOTE</span></strong></p>
<p class="notep"><em class="calibre13">Since our emphasis in this book is on prediction from data rather than description of data, we have not discussed the issue of feature importance before now. We only present it here as an aid to understanding how LASSO works. However, it is available in some of the packages used in this book. For instance, see the</em> <span class="codeitalic1">importance()</span> <em class="calibre13">function in the</em> <span class="codeitalic1">randomForests</span> <em class="calibre13">package.</em></p>
</div>
<h3 class="h2" id="ch09lev7">9.7 Example: African Soil Data</h3>
<p class="noindent">As noted in <a href="ch06.xhtml#ch06lev2sec4" class="calibre12">Section 6.2.4</a>, the importance of the African soil dataset is that it has <em class="calibre13">p</em> &gt; <em class="calibre13">n</em>, with the number of features being almost triple the number of data points. This is considered a very difficult situation.</p>
<p class="indent">Remember, to many analysts, the very essence of LASSO is dimension reduction, so it will be very interesting to see how LASSO does on this data.</p>
<h4 class="h3" id="ch09lev7sec1"><em class="calibre22"><strong class="calibre3">9.7.1 LASSO Analysis</strong></em></h4>
<p class="noindent">Again, we will predict soil acidity, pH:</p>
<pre class="calibre16">&gt; <span class="codestrong">afrsoil1 &lt;- afrsoil[,c(1:3578,3597)]</span>
&gt; <span class="codestrong">z &lt;- qeLASSO(afrsoil1,'pH',holdout=NULL)</span></pre>
<p class="indent">The <span class="literal">nzero</span> component of the output tells us how many features the process has chosen at each step:</p>
<pre class="calibre16">&gt; <span class="codestrong">z$nzero</span>
 s0  s1  s2  s3  s4  s5  s6  s7  s8  s9 s10 s11 s12 s13 s14 s15 s16 s17 s18 s19
  0   2   2   2   4   4   5   5   6   6   6   7   8   8  10  12  17  16  14  13
s20 s21 s22 s23 s24 s25 s26 s27 s28 s29 s30 s31 s32 s33 s34 s35 s36 s37 s38 s39
 13  13  12  15  15  15  15  12  13  13  13  13  13  14  16  15  16  16  17  18
s40 s41 s42 s43 s44 s45 s46 s47 s48 s49 s50 s51 s52 s53 s54 s55 s56 s57 s58 s59
 21  21  22  22  24  24  24  24  40  25  26  28  35  44  35  41  40  42  44  42
s60 s61 s62 s63 s64 s65 s66 s67 s68 s69 s70 s71 s72 s73 s74 s75 s76 s77 s78 s79
 43  56  50  70  61  66  64  62  57  58  64  73  79  84  85  85  97  97 102  82
s80 s81 s82 s83 s84 s85 s86 s87 s88 s89 s90 s91 s92 s93 s94 s95 s96 s97 s98 s99
 85  80  81  70  58  77  83  80  77  80  82  93  99  86 141 131 177 140 142 156</pre>
<p class="indent"><span epub:type="pagebreak" id="page_160"/>And the <span class="literal">lambda</span> component gives the corresponding <em class="calibre13">λ</em> values:</p>
<pre class="calibre16">&gt; <span class="codestrong">z$lambda</span>
  [1] 0.342642919 0.327069269 0.312203466 0.298013337 0.284468171 0.271538653
  [7] 0.259196803 0.247415908 0.236170473 0.225436161 0.215189739 0.205409033
 [13] 0.196072876 0.187161061 0.178654302 0.170534188 0.162783146 0.155384401
 [19] 0.148321940 0.141580479 0.135145428 0.129002860 0.123139480 0.117542601
 [25] 0.112200108 0.107100440 0.102232560 0.097585932 0.093150501 0.088916667
 [31] 0.084875267 0.081017555 0.077335183 0.073820179 0.070464938 0.067262198
 [37] 0.064205027 0.061286810 0.058501230 0.055842258 0.053304142 0.050881386
 [43] 0.048568749 0.046361224 0.044254035 0.042242621 0.040322628 0.038489903
 [49] 0.036740477 0.035070566 0.033476554 0.031954993 0.030502590 0.029116200
 [55] 0.027792824 0.026529597 0.025323786 0.024172781 0.023074090 0.022025337
 [61] 0.021024252 0.020068667 0.019156515 0.018285822 0.017454703 0.016661360
 [67] 0.015904076 0.015181211 0.014491201 0.013832554 0.013203843 0.012603708
 [73] 0.012030850 0.011484029 0.010962062 0.010463820 0.009988223 0.009534243
 [79] 0.009100897 0.008687247 0.008292398 0.007915496 0.007555724 0.007212305
 [85] 0.006884495 0.006571584 0.006272895 0.005987782 0.005715628 0.005455844
 [91] 0.005207868 0.004971162 0.004745215 0.004529538 0.004323663 0.004127146
 [97] 0.003939561 0.003760502 0.003589581 0.003426429</pre>
<p class="indent">The corresponding graph is shown in <a href="ch09.xhtml#ch09fig02" class="calibre12">Figure 9-2</a>.</p>
<div class="image"><img alt="Image" id="ch09fig02" src="../images/ch09fig02.jpg" class="calibre70"/></div>
<p class="figcap"><em class="calibre13">Figure 9-2: African soil data</em></p>
<p class="indent">Here we have a rather incomplete result. The smallest MSPE came from the smallest <em class="calibre13">λ</em> value tried by the software (0.003426429), but the curve seems <span epub:type="pagebreak" id="page_161"/>to suggest that even smaller values would do better. So, we might rerun with a custom set of <em class="calibre13">λ</em> values, rather than using the default value sequence.</p>
<p class="indent">Nevertheless, even if we choose to settle for <em class="calibre13">λ</em> = 0.003426429, that value would be pretty good. LASSO retained 156 features out of the original 3,578. That’s quite a dimension reduction.</p>
<h3 class="h2" id="ch09lev8">9.8 Optional Section: The Famous LASSO Picture</h3>
<p class="noindent">This section has a bit more mathematical content, and it can be safely skipped, as it is not used in the sequel. However, readers who are curious as to why the LASSO retains some of the original features but excludes others may find this section helpful.</p>
<p class="indent">As mentioned, a key property of the LASSO is that it usually provides a <em class="calibre13">sparse</em> solution for <img alt="Image" class="middle21" src="../images/betacap1.jpg"/>, meaning that many of the <img alt="Image" class="middle11" src="../images/unch08equ07.jpg"/> values are 0. In other words, many features are discarded, thus providing a means of dimension reduction. <a href="ch09.xhtml#ch09fig03" class="calibre12">Figure 9-3</a> shows why. Here is how it works.</p>
<div class="image"><img alt="Image" id="ch09fig03" src="../images/ch09fig03.jpg" class="calibre71"/></div>
<p class="figcap"><em class="calibre13">Figure 9-3: Feature subsetting nature of the LASSO</em></p>
<p class="indent"><a href="ch09.xhtml#ch09fig03" class="calibre12">Figure 9-3</a> is for the case of <em class="calibre13">p</em> = 2 predictors, whose coefficients are <em class="calibre13">b</em><sub class="calibre27">1</sub> and <em class="calibre13">b</em><sub class="calibre27">2</sub>. (For simplicity, we assume there is no constant term <em class="calibre13">b</em><sub class="calibre27">0</sub>.) Let <em class="calibre13">U</em> and <em class="calibre13">V</em> denote the corresponding features. Write <em class="calibre13">b</em> = (<em class="calibre13">b</em><sub class="calibre27">1</sub>, <em class="calibre13">b</em><sub class="calibre27">2</sub>) for the vector of the <em class="calibre13">b</em><em class="calibre13"><sub class="calibre27">i</sub></em>.</p>
<p class="indent">Without shrinkage, we would choose <em class="calibre13">b</em> to minimize the sum of squared errors:</p>
<p class="center" id="ch09equ13"><img alt="Image" src="../images/ch09equ13.jpg" class="calibre72"/></p>
<p class="indent">The horizontal and vertical axes are for <em class="calibre13">b</em><sub class="calibre27">1</sub> and <em class="calibre13">b</em><sub class="calibre27">2</sub>, as shown. The key point is that for any value that we set in <a href="ch09.xhtml#ch09equ13" class="calibre12">Equation 9.13</a> for SSE, the points (<em class="calibre13">b</em><sub class="calibre27">1</sub>, <em class="calibre13">b</em><sub class="calibre27">2</sub>) that solve the resulting equation form an ellipse. The value of (<em class="calibre13">b</em><sub class="calibre27">1</sub>, <em class="calibre13">b</em><sub class="calibre27">2</sub>) computed by the LASSO is just one point in the given ellipse; lots of other (<em class="calibre13">b</em><sub class="calibre27">1</sub>, <em class="calibre13">b</em><sub class="calibre27">2</sub>) values yield the same SSE.<span epub:type="pagebreak" id="page_162"/></p>
<p class="indent">As we vary the SSE value, we get various concentric ellipses, two of which are shown in <a href="ch09.xhtml#ch09fig03" class="calibre12">Figure 9-3</a>. Larger values of SSE correspond to larger ellipses.</p>
<p class="indent">Now, what happens when we give the LASSO algorithm a value of <em class="calibre13">λ</em> or <em class="calibre13">η</em>? As noted earlier, either quantity can be used, but it will be easier to assume the latter. So, what will the LASSO algorithm do when we give it a value of <em class="calibre13">η</em>?</p>
<ul class="calibre15">
<li class="noindent3">The algorithm will minimize SSE, subject to the constraint:
<p class="center" id="ch09equ14"><img alt="Image" src="../images/ch09equ14.jpg" class="calibre73"/></p>
<p class="noindent">Let’s denote that minimum value of SSE by SSE<em class="calibre13"><sub class="calibre27">alg</sub></em>, and denote the corresponding (<em class="calibre13">b</em><sub class="calibre27">1</sub>, <em class="calibre13">b</em><sub class="calibre27">2</sub>) value by (<em class="calibre13">b</em><sub class="calibre27">1</sub>, <em class="calibre13">b</em><sub class="calibre27">2</sub>)<em class="calibre13"><sub class="calibre27">alg</sub></em>.</p></li>
<li class="noindent3">On the one hand, the point (<em class="calibre13">b</em><sub class="calibre27">1</sub>, <em class="calibre13">b</em><sub class="calibre27">2</sub>)<em class="calibre13"><sub class="calibre27">alg</sub></em> will be on the ellipse associated with SSE<em class="calibre13"><sub class="calibre27">alg</sub></em>.</li>
<li class="noindent3">On the other hand, <a href="ch09.xhtml#ch09equ14" class="calibre12">Equation 9.14</a> says that (<em class="calibre13">b</em><sub class="calibre27">1</sub>, <em class="calibre13">b</em><sub class="calibre27">2</sub>)<em class="calibre13"><sub class="calibre27">alg</sub></em> must be somewhere in the diamond in the picture, whose corners are at (<em class="calibre13">η</em>, 0), (0, <em class="calibre13">η</em>), and so on.</li>
<li class="noindent3">So, (<em class="calibre13">b</em><sub class="calibre27">1</sub>, <em class="calibre13">b</em><sub class="calibre27">2</sub>)<em class="calibre13"><sub class="calibre27">alg</sub></em> must lie on an ellipse that intersects with the diamond.</li>
<li class="noindent3">But remember, we want SSE to be as small as possible, subject to <a href="ch09.xhtml#ch09equ14" class="calibre12">Equation 9.14</a>. Recall, too, that smaller SSE values correspond to smaller ellipses. So the ellipse for SSE<em class="calibre13"><sub class="calibre27">alg</sub></em> must <em class="calibre13">just barely touch the diamond</em>, as seen in the outer ellipse in <a href="ch09.xhtml#ch09fig03" class="calibre12">Figure 9-3</a>.</li>
<li class="noindent3">In the figure, the “just barely touch” point is at one of the corners of the diamond. And each of the corners has either <em class="calibre13">b</em><sub class="calibre27">1</sub> or <em class="calibre13">b</em><sub class="calibre27">2</sub> equal to 0—sparsity!</li>
<li class="noindent3">Is that sparsity some kind of coincidence? No! Here’s why: depending on the relative values of our input data (<em class="calibre13">U</em><em class="calibre13"><sub class="calibre27">i</sub></em>, <em class="calibre13">V</em><em class="calibre13"><sub class="calibre27">i</sub></em>), the ellipses in the picture will have different orientations. The ones in the picture are pointing approximately “northwest and southeast.” But it is clear from inspection that most orientations will result in the touch point being at one of the corners and hence a sparse solution.</li>
</ul>
<p class="indent">Thus the LASSO will usually be sparse, which is the major reason for its popularity. And what about ridge regression? In that case, the diamond becomes a circle, so there is no sparseness property.</p>
<h3 class="h2" id="ch09lev9">9.9 Coming Up</h3>
<p class="noindent">Next, we take an entirely different approach. With k-NN and decision trees, no linearity was used, and then this property was explicitly assumed. In <a href="part4.xhtml" class="calibre12">Part IV</a>, we cover methods in which linearity is used, but only indirectly.</p>
</div></body></html>