- en: '**22'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SPEEDING UP INFERENCE**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: What are techniques to speed up model inference through optimization without
    changing the model architecture or sacrificing accuracy?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning and AI, *model inference* refers to making predictions or
    generating outputs using a trained model. The main general techniques for improving
    model performance during inference include parallelization, vectorization, loop
    tiling, operator fusion, and quantization, which are discussed in detail in the
    following sections.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallelization**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One common way to achieve better parallelization during inference is to run
    the model on a batch of samples rather than on a single sample at a time. This
    is sometimes also referred to as *batched inference* and assumes that we are receiving
    multiple input samples or user inputs simultaneously or within a short time window,
    as illustrated in [Figure 22-1](ch22.xhtml#ch22fig1).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/22fig01.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-1: Sequential inference and batched inference*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 22-1](ch22.xhtml#ch22fig1) shows sequential inference processing one
    item at a time, which creates a bottleneck if there are several samples waiting
    to be classified. In batched inference, the model processes all four samples at
    the same time.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '**Vectorization**'
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Vectorization* refers to performing operations on entire data structures,
    such as arrays (tensors) or matrices, in a single step rather than using iterative
    constructs like `for` loops. Using vectorization, multiple operations from the
    loop are performed simultaneously using single instruction, multiple data (SIMD)
    processing, which is available on most modern CPUs.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: This approach takes advantage of the low-level optimizations in many computing
    systems and often results in significant speedups. For example, it might rely
    on BLAS.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '*BLAS* (which is short for *Basic Linear Algebra Subprograms*) is a specification
    that prescribes a set of low-level routines for performing common linear algebra
    operations such as vector addition, scalar multiplication, dot products, matrix
    multiplication, and others. Many array and deep learning libraries like NumPy
    and PyTorch use BLAS under the hood.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate vectorization with an example, suppose we wanted to compute the
    dot product between two vectors. The non-vectorized way of doing this would be
    to use a `for` loop, iterating over each element of the array one by one. However,
    this can be quite slow, especially for large arrays. With vectorization, you can
    perform the dot product operation on the entire array at once, as shown in [Figure
    22-2](ch22.xhtml#ch22fig2).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/22fig02.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: '*Figure 22-2: A classic* for *loop versus a vectorized dot product computation
    in Python*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: In the context of linear algebra or deep learning frameworks like TensorFlow
    and PyTorch, vectorization is typically done automatically. This is because these
    frameworks are designed to work with multidimensional arrays (also known as *tensors*),
    and their operations are inherently vectorized. This means that when you perform
    functions using these frameworks, you automatically leverage the power of vectorization,
    resulting in faster and more efficient computations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性代数或深度学习框架（如 TensorFlow 和 PyTorch）中，矢量化通常是自动进行的。这是因为这些框架设计用于处理多维数组（也称为*张量*），其操作本质上是矢量化的。这意味着，当你使用这些框架执行函数时，自动利用了矢量化的优势，从而实现更快、更高效的计算。
- en: '**Loop Tiling**'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**循环分块**'
- en: '*Loop tiling* (also often referred to as *loop nest optimization*) is an advanced
    optimization technique to enhance data locality by breaking down a loop’s iteration
    space into smaller chunks or “tiles.” This ensures that once data is loaded into
    cache, all possible computations are performed on it before the cache is cleared.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*循环分块*（也常被称为*循环嵌套优化*）是一种先进的优化技术，通过将循环的迭代空间拆分成更小的块或“瓦片”来增强数据局部性。这确保了数据一旦加载到缓存中，在缓存清空之前，所有可能的计算都会在其上执行。'
- en: '[Figure 22-3](ch22.xhtml#ch22fig3) illustrates the concept of loop tiling for
    accessing elements in a two-dimensional array. In a regular `for` loop, we iterate
    over columns and rows one element at a time, whereas in loop tiling, we subdivide
    the array into smaller tiles.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 22-3](ch22.xhtml#ch22fig3) 说明了二维数组中访问元素的循环分块概念。在常规的`for`循环中，我们一次迭代处理一个元素，遍历列和行，而在循环分块中，我们将数组细分为更小的块。'
- en: '![Image](../images/22fig03.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/22fig03.jpg)'
- en: '*Figure 22-3: Loop tiling in a two-dimensional array*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 22-3：二维数组中的循环分块*'
- en: Note that in languages such as Python, we don’t usually perform loop tiling,
    because Python and many other high-level languages do not allow control over cache
    memory like lower-level languages such as C and C++ do. These kinds of optimizations
    are often handled by underlying libraries like NumPy and PyTorch when performing
    operations on large arrays.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在像 Python 这样的语言中，我们通常不会进行循环分块，因为 Python 和许多其他高级语言不像 C 和 C++ 这样的低级语言那样可以控制缓存内存。这类优化通常由底层库（如
    NumPy 和 PyTorch）在对大数组进行操作时处理。
- en: '**Operator Fusion**'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**操作符融合**'
- en: '*Operator fusion*, sometimes called *loop fusion*, is an optimization technique
    that combines multiple loops into a single loop. This is illustrated in [Figure
    22-4](ch22.xhtml#ch22fig4), where two separate loops to calculate the sum and
    the product of an array of numbers are fused into a single loop.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*操作符融合*，有时也叫做*循环融合*，是一种优化技术，它将多个循环合并为一个循环。这在[图 22-4](ch22.xhtml#ch22fig4)中得到了说明，其中两个独立的循环分别计算一个数字数组的和与积，被融合成了一个循环。'
- en: '![Image](../images/22fig04.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/22fig04.jpg)'
- en: '*Figure 22-4: Fusing two* for *loops (left) into one (right)*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 22-4：将两个* for *循环（左）融合为一个（右）*'
- en: Operator fusion can improve the performance of a model by reducing the overhead
    of loop control, decreasing memory access times by improving cache performance,
    and possibly enabling further optimizations through vectorization.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 操作符融合可以通过减少循环控制的开销、通过提高缓存性能减少内存访问时间，并可能通过矢量化启用进一步的优化，从而提高模型的性能。
- en: You might think this behavior of vectorization would be incompatible with loop
    tiling, in which we break a `for` loop into multiple loops. However, these techniques
    are actually complementary, used for different optimizations, and applicable in
    different situations. Operator fusion is about reducing the total number of loop
    iterations and improving data locality when the entire data fits into cache. Loop
    tiling is about improving cache utilization when dealing with larger multidimensional
    arrays that do not fit into cache.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为，矢量化行为与循环分块不兼容，因为在循环分块中，我们将一个`for`循环拆分为多个循环。然而，这些技术实际上是互补的，用于不同的优化，并适用于不同的场景。操作符融合旨在减少循环迭代的总次数，并在整个数据适合缓存时提高数据局部性。而循环分块则是在处理无法完全装入缓存的更大多维数组时，改进缓存利用率。
- en: Related to operator fusion is the concept of *reparameterization*, which can
    often also be used to simplify multiple operations into one. Popular examples
    include training a network with multibranch architectures that are reparameterized
    into single-stream architectures during inference. This reparameterization approach
    differs from traditional operator fusion in that it does not merge multiple operations
    into a single operation. Instead, it rearranges the operations in the network
    to create a more efficient architecture for inference. In the so-called RepVGG
    architecture, for example, each branch during training consists of a series of
    convolutions. Once training is complete, the model is reparameterized into a single
    sequence of convolutions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantization**'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Quantization* reduces the computational and storage requirements of machine
    learning models, particularly deep neural networks. This technique involves converting
    the floating-point numbers (technically discrete but representing continuous values
    within a specific range) for implementing weights and biases in a trained neural
    network to more discrete, lower-precision representations such as integers. Using
    less precision reduces the model size and makes it quicker to execute, which can
    lead to significant improvements in speed and hardware efficiency during inference.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of deep learning, it has become increasingly common to quantize
    trained models down to 8-bit and 4-bit integers. These techniques are especially
    prevalent in the deployment of large language models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: There are two main categories of quantization. In *post-training quantization*,
    the model is first trained normally with full-precision weights, which are then
    quantized after training. *Quantization-aware training*, on the other hand, introduces
    the quantization step during the training process. This allows the model to learn
    to compensate for the effects of quantization, which can help maintain the model’s
    accuracy.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s important to note that quantization can occasionally lead to a
    reduction in model accuracy. Since this chapter focuses on techniques to speed
    up model inference *without* sacrificing accuracy, quantization is not as good
    a fit for this chapter as the previous categories.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '*Other techniques to improve inference speeds include knowledge distillation
    and pruning, discussed in [Chapter 6](ch06.xhtml). However, these techniques affect
    the model architecture, resulting in smaller models, so they are out of scope
    for this chapter’s question.*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**22-1.** [Chapter 7](ch07.xhtml) covered several multi-GPU training paradigms
    to speed up model training. Using multiple GPUs can, in theory, also speed up
    model inference. However, in reality, this approach is often not the most efficient
    or most practical option. Why is that?'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '**22-2.** Vectorization and loop tiling are two strategies for optimizing operations
    that involve accessing array elements. What would be the ideal situation in which
    to use each?'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The official BLAS website: *[https://www.netlib.org/blas/](https://www.netlib.org/blas/)*.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper that proposed loop tiling: Michael Wolfe, “More Iteration Space Tiling”
    (1989), *[https://dl.acm.org/doi/abs/10.1145/76263.76337](https://dl.acm.org/doi/abs/10.1145/76263.76337)*.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RepVGG CNN architecture merging operations in inference mode: Xiaohan Ding
    et al., “RepVGG: Making VGG-style ConvNets Great Again” (2021), *[https://arxiv.org/abs/2101.03697](https://arxiv.org/abs/2101.03697)*.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A new method for quantizing the weights in large language models down to 8-bit
    integer representations: Tim Dettmers et al., “LLM.int8(): 8-bit Matrix Multiplication
    for Transformers at Scale” (2022), *[https://arxiv.org/abs/2208.07339](https://arxiv.org/abs/2208.07339)*.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A new method for quantizing the weights in LLMs farther down to 4-bit integers:
    Elias Frantar et al., “GPTQ: Accurate Post-Training Quantization for Generative
    Pre-trained Transformers” (2022), *[https://arxiv.org/abs/2210.17323](https://arxiv.org/abs/2210.17323)*.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
