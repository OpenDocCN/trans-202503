<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><h2 class="h1" id="ch12"><span epub:type="pagebreak" id="page_199" class="calibre2"/><strong class="calibre3"><span class="big">12</span><br class="calibre18"/>IMAGE CLASSIFICATION</strong></h2>
<div class="imagec"><img alt="Image" src="../images/common.jpg" class="calibre14"/></div>
<p class="noindent">Probably the most widely celebrated application of NNs is image classification. Indeed, the popularity of NNs today is largely due to some spectacular successes of NNs in image classification contests in the early 2000s. Earlier, the NN field had been treated largely as a curiosity and not a mainstream tool.</p>
<p class="indent">And the surge in popularity of NNs for image classification then had a feedback effect: the more NNs did well with images, the more image classification researchers used NNs as their tool, thus the more they refined use of NNs for imaging, which in turn led to more NN success in the contests.</p>
<p class="indent">In principle, <em class="calibre13">any</em> of the methods in this book could be used on images. The features are the pixel intensities, and the outcome is the class of image. Consider, for instance, the famous MNIST data. Here we have 70,000 images of handwritten digits, with each image having 28 rows and 28 columns of pixels. Each pixel has an intensity (brightness) number between 0 and 255, and we have 28<sup class="calibre11">2</sup> = 784 pixels, so we have 784 features and 10 classes.<span epub:type="pagebreak" id="page_200"/></p>
<p class="indent">The “secret sauce” for NNs in the image field has been <em class="calibre13">convolutional</em> operations, leading to the term <em class="calibre13">convolutional neural networks (CNNs)</em>. Actually, those operations were not really new; they borrow from classical image processing techniques. And most important, convolutional operations are not inherent to NNs. They could be used with other ML methods, and in fact, some researchers have developed <em class="calibre13">convolutional SVMs</em>. But again, the momentum in the image field is solidly with CNNs.</p>
<p class="indent">Thus the focus of this chapter on images will be on NNs. We’ll start with a non-NN example to make the point that any ML method might be used and then get into CNNs.</p>
<h3 class="h2" id="ch12lev1">12.1 Example: The Fashion MNIST Data</h3>
<p class="noindent">It’s standard to use the MNIST data as one’s introductory example, but let’s be a little different here. The Fashion MNIST data is of the same size as MNIST (28 × 28 pixel structure, 10 classes, 70,000 images) but consists of pictures of clothing (10 types) rather than digits. (The dataset is available at <a href="https://github.com/zalandoresearch/fashion-mnist" class="calibre12"><em class="calibre13">https://github.com/zalandoresearch/fashion-mnist</em></a>.)</p>
<p class="indent">One important difference is that while MNIST can be considered basically black and white, Fashion MNIST truly has “shades of gray.” An example is shown in <a href="ch12.xhtml#ch12fig01" class="calibre12">Figure 12-1</a>. The blurriness is due to the low 28 × 28 resolution of the image set.</p>
<div class="image"><img alt="Image" id="ch12fig01" src="../images/ch12fig01.jpg" class="calibre99"/></div>
<p class="figcap"><em class="calibre13">Figure 12-1: A Fashion MNIST image</em></p>
<p class="indent">This makes the dataset more challenging, and accuracy rates are generally somewhat lower for Fashion MNIST than for MNIST.</p>
<h4 class="h3" id="ch12lev1sec1"><em class="calibre22"><strong class="calibre3">12.1.1 A First Try Using a Logit Model</strong></em></h4>
<p class="noindent">The dataset actually comes already partitioned into training and test sets (60,000 and 10,000 rows, respectively), but for convenience, let’s just stick to the training set, which I have named <code>ftrn</code>, with columns V1, V2, . . . , V785. That last column is the clothing type, with values 0 through 9.</p>
<p class="indent"><span epub:type="pagebreak" id="page_201"/>Let’s try a logistic model on this data (which, by the way, took about 2 hours to run):</p>
<pre class="calibre16">&gt; <span class="codestrong">z &lt;- qeLogit(ftrn,'V785')</span>
&gt; <span class="codestrong">z$testAcc</span>
[1] 0.205
&gt; <span class="codestrong">z$baseAcc</span>
[1] 0.8998305</pre>
<p class="indent">So, we were attaining about 80 percent accuracy. The base was only about 10 percent accuracy, which makes sense: there are roughly equal numbers of the 10 clothing types, so random guessing would give us about 10 percent. Thus 80 percent is not bad. But since the world’s record best accuracy on this dataset is in the high 90s, we would like to do better.</p>
<h4 class="h3" id="ch12lev1sec2"><em class="calibre22"><strong class="calibre3">12.1.2 Refinement via PCA</strong></em></h4>
<p class="noindent">We might surmise, as we did above, that <em class="calibre13">p</em> = 784 is too large and is in need of dimension reduction. One possible remedy would be to use PCA:</p>
<pre class="calibre16">&gt; <span class="codestrong">z &lt;- qePCA(ftrn,'V785','qeLogit',pcaProp=0.8)</span>
&gt; <span class="codestrong">z$testAcc</span>
[1] 0.172</pre>
<p class="noindent">Ah, now we are up to about 83 percent accuracy. (And it took only about a minute to run.)</p>
<p class="indent">We could try different values of the number of principal components, but a better approach would likely be to take advantage of what we know about images, as we will now see.</p>
<h3 class="h2" id="ch12lev2">12.2 Convolutional Models</h3>
<p class="noindent">Though the CNN structure may seem complex at first, it actually is based on simple ideas. Let’s get started.</p>
<h4 class="h3" id="ch12lev2sec1"><em class="calibre22"><strong class="calibre3">12.2.1 Need for Recognition of Locality</strong></em></h4>
<p class="noindent">It’s no coincidence that the picture in <a href="ch12.xhtml#ch12fig01" class="calibre12">Figure 12-1</a> looks blurry. Remember, these are very low-resolution images (28 × 28 pixels). Yet even though this is tiny as images go, it gives us 784 features. With <em class="calibre13">n</em> = 70, 000, our “<img alt="Image" src="../images/prootn.jpg" class="calibre26"/>” rule of thumb (<a href="ch03.xhtml#ch03equ02" class="calibre12">Equation 3.2</a>) would suggest a maximum of about 260 features, which is well short of 784. And while that rule is conservative—CNNs do well in spite of ending up with <em class="calibre13">p</em> much greater than <em class="calibre13">n</em>—it’s clear that using the 784 pixels as if they were unrelated features is going to impede our ability to predict new cases well.</p>
<p class="indent">We need to exploit the <em class="calibre13">locality</em> of our images. An image pixel tends to be correlated with its neighboring pixels, and the nature of this relationship should help us classify the image. Is that pixel part of a short straight line, say, or maybe a small circle? The <em class="calibre13">convolutional model</em> is designed with this in <span epub:type="pagebreak" id="page_202"/>mind, consisting of various operations that are applied to patches within an image. These patches are often called <em class="calibre13">tiles</em>, which we will work with here.</p>
<h4 class="h3" id="ch12lev2sec2"><em class="calibre22"><strong class="calibre3">12.2.2 Overview of Convolutional Methods</strong></em></h4>
<p class="noindent">Let’s first sneak a look at the code and run it on the Fashion MNIST data. After that, we’ll explain the operations in the code. We’ll use code adapted from an RStudio example:</p>
<pre class="calibre16"># set up 5 image op layers
&gt; <span class="codestrong">conv1 &lt;- list(type='conv2d',filters=32,kern=3)</span>
&gt; <span class="codestrong">conv2 &lt;- list(type='pool',kern=2)</span>
&gt; <span class="codestrong">conv3 &lt;- list(type='conv2d',filters=64,kern=3)</span>
&gt; <span class="codestrong">conv4 &lt;- list(type='pool',kern=2)</span>
&gt; <span class="codestrong">conv5 &lt;- list(type='drop',drop=0.5)</span>
<br class="calibre1"/>
# note that qeNeural() by default sets up two hidden layers; these will
# come after the convolutional ones
&gt; <span class="codestrong">z &lt;- qeNeural(ftrn,'V785',</span>
   <span class="codestrong">conv=list(conv1,conv2,conv3,conv4,conv5),xShape=c(28,28))</span>
&gt; <span class="codestrong">z$testAcc</span>
[1] 0.075</pre>
<p class="noindent">Ah, now we are up to more than 92 percent correct classification. And we almost certainly could do even better by tuning the hyperparameters (including changing the number and structure of the image operation layers).</p>
<p class="indent">What do we see here?</p>
<ul class="calibre15">
<li class="noindent3">Using <code>qeNeural()</code>’s <code>conv</code> argument, we have set up five image-operation layers.</li>
<li class="noindent3">The image-operation layers are followed by “ordinary” layers (not specified here), thus taking <code>qeNeural()</code>’s default value of two layers of 100 neurons each. The “ordinary” layers are termed <em class="calibre13">dense</em> layers or <em class="calibre13">fully connected</em> layers.</li>
<li class="noindent3">The first image-operation layer performs a convolutional operation on the input image, which involves extracting tiles and forming linear combinations of the image intensities within each tile. These linear combinations become outputs of the layer.</li>
<li class="noindent3">Recall from earlier chapters that in the linear combination, say, 3<em class="calibre13">a</em> − 1.5<em class="calibre13">b</em> + 16.2<em class="calibre13">c</em>, the numbers 3, −1.5, and 16.2 are called <em class="calibre13">coefficients</em>. In the CNN context, they are called <em class="calibre13">weights</em>.</li>
<li class="noindent3">Usually we will use many different sets of weights. The <code>conv2d</code> parameter <code>filters</code> specifies the number of sets of weights that we want for a layer. It acts analogously with the number of neurons in a dense layer.</li>
<li class="noindent3">The <code>conv2d</code> parameter <code>kern</code> value specifies the tile size, with the value 3 in the first layer meaning 3 × 3 tiles.</li>
<li class="noindent3"><span epub:type="pagebreak" id="page_203"/>Another <code>conv2d</code> parameter <code>stride</code> controls the number of tiles in an image by specifying the amount of overlap a tile has with its neighbors, which will be explained below.</li>
<li class="noindent3">The argument <code>xShape</code> specifies the size of an image, such as 28 × 28 in the current example.</li>
<li class="noindent3">For color data of that size, we would denote the situation as 28 × 28 × 3, with the 3 referring to the number of primary colors, red, yellow, and blue. We would have a 28 × 28 array of red intensities, then another for yellow, and finally one for blue. Then we would set <em class="calibre13">xShape</em> to (28,28,3).</li>
<li class="noindent3">That third coordinate, 3, is called a <em class="calibre13">channel</em>. We don’t call it “color,” because it may not be a color, as will be seen below.</li>
</ul>
<p class="indent">The output of one layer is input to the next, and its dimensions may be, say, 13 × 13 × 64. That would be treated as a 13 × 13 “image” with 64 “primary colors,” both of which are artificial. The point is that, mathematically, any three-dimensional array can be treated this way, and it makes the code simpler to do so. (An array of three or more dimensions is called a <em class="calibre13">tensor</em>, hence the name <code>tensorflow</code> for the Python package underlying what we do here.)</p>
<h4 class="h3" id="ch12lev2sec3"><em class="calibre22"><strong class="calibre3">12.2.3 Image Tiling</strong></em></h4>
<p class="noindent">The first and third layers in the above code example perform a <em class="calibre13">convolution</em>  operation. (Readers with a background in probability theory, Fourier analysis, and so on will find that the term is used somewhat differently in ML.) To explain, we first need to discuss breaking an image into tiles.</p>
<p class="indent">Consider this toy example of a 6 × 6 grayscale image:</p>
<p class="center" id="ch12equ01"><img alt="Image" src="../images/ch12equ01.jpg" class="calibre100"/></p>
<p class="noindent">For example, the intensity of the pixel in row 2, column 4 of the image is 11. In R, we would store this in a matrix of 6 rows and 6 columns. (An R matrix is like a data frame in which the elements are all numeric or all character strings and so on.)</p>
<p class="indent">We could break this into non-overlapping tiles of size 3 × 3:<span epub:type="pagebreak" id="page_204"/></p>
<p class="center" id="ch12equ02"><img alt="Image" src="../images/ch12equ02.jpg" class="calibre101"/></p>
<p class="noindent">So, our original matrix has been partitioned into four submatrices or tiles.</p>
<p class="indent">We can also have overlapping tiles, using a number called the <em class="calibre13">stride</em>. In the above example, the stride is 3: The first column of the upper-right tile</p>
<p class="center" id="ch12equ03"><img alt="Image" src="../images/ch12equ03.jpg" class="calibre102"/></p>
<p class="noindent">is 3 columns to the right of the first column of the upper-left tile located in <a href="ch12.xhtml#ch12equ02" class="calibre12">Equation 12.2</a></p>
<p class="center" id="ch12equ04"><img alt="Image" src="../images/ch12equ04.jpg" class="calibre103"/></p>
<p class="noindent">and so on. Similar statements hold for rows. For example, the first row of the lower-right tile is 3 rows below the first row of the upper-right tile.</p>
<p class="indent">With a stride of 1, say, our first 3 × 3 tile would still be</p>
<p class="center" id="ch12equ05"><img alt="Image" src="../images/ch12equ05.jpg" class="calibre104"/></p>
<p class="noindent">But the second would be just 1 column to the right of the first tile</p>
<p class="center" id="ch12equ06"><img alt="Image" src="../images/ch12equ06.jpg" class="calibre104"/></p>
<p class="noindent">and so on.</p>
<p class="indent">The default value of <code>stride</code> in <code>'conv2d'</code> operations is 1.</p>
<h4 class="h3" id="ch12lev2sec4"><em class="calibre22"><strong class="calibre3">12.2.4 The Convolution Operation</strong></em></h4>
<p class="noindent">Say we are using 3 × 3 tiles. It is convenient to express the coefficients of a linear combination in matrix form too, say, in the weights matrix:<span epub:type="pagebreak" id="page_205"/></p>
<p class="center" id="ch12equ07"><img alt="Image" src="../images/ch12equ07.jpg" class="calibre105"/></p>
<p class="noindent">For a given tile:</p>
<ul class="calibre15">
<li class="noindent3">The tile’s element in row 1, column 1 will be multiplied by <em class="calibre13">w</em><sub class="calibre27">11</sub>.</li>
<li class="noindent3">The tile’s element in row 1, column 2 will be multiplied by <em class="calibre13">w</em><sub class="calibre27">12</sub>.</li>
<li class="noindent3">. . .</li>
<li class="noindent3">The tile’s element in row 3, column 3 will be multiplied by <em class="calibre13">w</em><sub class="calibre27">33</sub>.</li>
</ul>
<p class="noindent">All of those products will be summed to produce a single number.<span epub:type="pagebreak" id="page_206"/></p>
<p class="indent">This set of weights is then applied to each tile. Applying to the upperleft tile in <a href="ch12.xhtml#ch12equ02" class="calibre12">Equation 12.2</a>, we have the single number:</p>
<p class="center" id="ch12equ08"><img alt="Image" src="../images/ch12equ08.jpg" class="calibre106"/></p>
<p class="noindent">We apply the same set of weights to each tile. For instance, applying the weights to the upper-right tile, we have:</p>
<p class="center" id="ch12equ09"><img alt="Image" src="../images/ch12equ09.jpg" class="calibre107"/></p>
<p class="noindent">We do the same for the lower-left and lower-right tiles, yielding four numbers altogether, which we arrange in a 2 × 2 matrix. That is output to the next layer.</p>
<p class="indent">Now suppose we have, say, 12 filters. That means 12 different sets of weights—that is, 12 different versions of the matrix in <a href="ch12.xhtml#ch12equ07" class="calibre12">Equation 12.7</a>. That means 12 different 2 × 2 matrices coming out of this layer. Thus the output of this layer is described as 2 × 2 × 12. To be sure, yes, the total output of this layer will be 48 numbers, but we think of them as consisting of 12 sets of 2 × 2 matrices, hence the 2 × 2 × 12 notation.</p>
<p class="indent">Note we are not choosing these weights ourselves. They are chosen by the NN algorithm, which will minimize the overall prediction sum of squares. We choose the <em class="calibre13">number</em> of sets, 12 here, but not the sets themselves. The algorithm will try many different collections of 12 sets of weights, in hope of finding a collection that minimizes the prediction sum of squares.</p>
<p class="indent">So . . . there is really nothing new. We are taking linear combinations of the inputs and feeding them to the next layer, just as in the last chapter. In the end, the algorithm minimizes the sum of squared prediction errors, just as before.</p>
<p class="indent">The difference, though, is the structuring of the data into tiles, exploiting locality. The role of the weights is to determine the relative importance of various pixels, especially in how they work together.</p>
<p class="indent">It will be helpful to visualize this as a “building” with 12 “floors.” Each “floor” consists of four “rooms,” arranged in two rows of two rooms each. We will take this approach below.</p>
<p class="indent">Note that we still have the usual Bias-Variance Trade-off as with dense layers: the more filters, the more opportunities to reduce bias, but the more variance we incur in the weights.</p>
<h4 class="h3" id="ch12lev2sec5"><em class="calibre22"><strong class="calibre3">12.2.5 The Pooling Operation</strong></em></h4>
<p class="noindent">Recall the second layer in the example in <a href="ch12.xhtml#ch12lev2" class="calibre12">Section 12.2</a>:</p>
<pre class="calibre16">conv2 &lt;- list(type='pool',kern=2)</pre>
<p class="noindent">This is not a convolutional layer; it’s a <em class="calibre13">pooling</em> layer.</p>
<p class="indent">Pooling involves replacing the elements in a tile by some representative value, say, the mean or the median, or even the maximum value in the tile. The latter is quite common, in fact, and is the one used in the <code>regtools</code> and <code>qeML</code> packages.<span epub:type="pagebreak" id="page_207"/></p>
<p class="indent">The reader may wonder, “Isn’t pooling a special case of convolutional operations? For example, isn’t taking the mean in a 2 × 2 tile the same as a convolutional operation with all the weights being 0.25?” The answer is yes, but with one big difference: here the weights are fixed at 0.25; they are not chosen by the algorithm.</p>
<p class="indent">Unlike the <code>conv2d</code> operation, where the default stride is 1, for pooling, the default stride is the tile size, specified above as 2.</p>
<h4 class="h3" id="ch12lev2sec6"><em class="calibre22"><strong class="calibre3">12.2.6 Shape Evolution Across Layers</strong></em></h4>
<p class="noindent">Now, what will be the structure of the output from this second layer? Let’s reason this out. Here again are the specifications of the first two layers:</p>
<pre class="calibre16">conv1 &lt;- list(type='conv2d',filters=32,kern=3)
conv2 &lt;- list(type='pool',kern=2)</pre>
<p class="noindent">The input to the first layer was 28 × 28, or 28 × 28 × 1. The first breaks things into 3 × 3 tiles with a stride of 1. Just as there is a 2 × 2 array of tiles in <a href="ch12.xhtml#ch12equ02" class="calibre12">Equation 12.2</a>, here we will have a 26 × 26 array of tiles, again taking into account that the stride is 1.</p>
<p class="indent">So in that first layer, each filter will output a total of 26<sup class="calibre11">2</sup> numbers, in 26 × 26 form. With 32 filters, the total output of that first layer will be 26 × 26 × 32. In the “building” metaphor, this means 32 floors, with each floor having 26 rows of rooms with 26 rooms per row. Note again here that each “room” holds one number.</p>
<p class="indent">Now, what then will happen at the second layer? It will receive 32 tiles sized 26 × 26. What will it do with them?</p>
<p class="indent">The tile size used by this layer, as discussed above, is 2 × 2, with a stride of 2. Applying this to an inputted 26 × 26 tile, 13 rows of 13 2 × 2 tiles in each row will be formed. In each 2 × 2 tile, the maximum value among the 4 numbers will be extracted.</p>
<p class="indent">Again using the “building” metaphor, each “floor” will produce 13<sup class="calibre11">2</sup> = 169 numbers, arranged in 13 × 13 form. Since we have 32 “floors,” the total output of this layer will be in the form 13 × 13 × 32. (The <code>regtools</code> and <code>qeML</code> packages use the two-dimensional form of the pooling operation, so the pooling is done within floors and not across floors.)</p>
<h4 class="h3" id="ch12lev2sec7"><em class="calibre22"><strong class="calibre3">12.2.7 Dropout</strong></em></h4>
<p class="noindent">As with the dense layers, the danger of overfitting—too many neurons per convolutional layer or too many convolutional layers—is high. The antidote is dropout, for example:</p>
<pre class="calibre16">&gt; <span class="codestrong">conv5 &lt;- list(type='drop',drop=0.5)</span></pre>
<p class="noindent">This specifies randomly deleting 50 percent of the nodes in this layer.<span epub:type="pagebreak" id="page_208"/></p>
<h4 class="h3" id="ch12lev2sec8"><em class="calibre22"><strong class="calibre3">12.2.8 Summary of Shape Evolution</strong></em></h4>
<p class="noindent">The <code>keras</code> package gives us a summary of our CNN on request:</p>
<pre class="calibre16">&gt; <span class="codestrong">z$model</span>
Model
Model: "sequential"
__
Layer (type)                        Output Shape                    Param #
===============================================================================
conv2d (Conv2D)                     (None, 26, 26, 32)              320
__
max_pooling2d (MaxPooling2D)        (None, 13, 13, 32)              0
__
conv2d_1 (Conv2D)                   (None, 11, 11, 64)              18496
__
max_pooling2d_1 (MaxPooling2D)      (None, 5, 5, 64)                0
__
dropout (Dropout)                   (None, 5, 5, 64)                0
__
flatten (Flatten)                   (None, 1600)                    0
__
dense (Dense)                       (None, 100)                     160100
__
dense_1 (Dense)                     (None, 10)                      1010
===============================================================================
Total params: 179,926
Trainable params: 179,926
Non-trainable params: 0
_</pre>
<p class="noindent">Recall that <code>qeNeural()</code> calls <code>regtools::krsFit()</code>, which in turn makes calls to the R <code>keras</code> package, so this output actually comes from the latter.</p>
<p class="indent">That last column shows the number of weights at each layer. For instance, here is where that 320 figure came from: each filter—that is, each set of numbers <em class="calibre13">w</em><em class="calibre13"><sub class="calibre27">ij</sub></em>—is a 3 × 3 matrix, thus consisting of 9 numbers. There is also an intercept term <em class="calibre13">w</em><sub class="calibre27">0</sub> (like <em class="calibre13">β</em><sub class="calibre27">0</sub> in a linear regression model), for a total of 10 weights. Since there were 32 filters, we have 320 weights, as shown in the output table above.</p>
<p class="indent">The <code>flatten</code> layer merely converts from our <em class="calibre13">a</em> × <em class="calibre13">c</em> form to ordinary data. The output of our second pooling layer had form 5 × 5 × 64, which amounts to 1,600 numbers. In order to be used by a dense layer, the data is converted to a single vector of length 1,600.</p>
<p class="indent">Altogether, we have <em class="calibre13">p</em> = 179926 but only <em class="calibre13">n</em> = 65000. So we are definitely overfitting. The fact that many such models have been found to work well is quite a controversy in the ML community!<span epub:type="pagebreak" id="page_209"/></p>
<h4 class="h3" id="ch12lev2sec9"><em class="calibre22"><strong class="calibre3">12.2.9 Translation Invariance</strong></em></h4>
<p class="noindent">The weight structure lends <em class="calibre13">translation invariance</em>—a fancy term that actually has a simple meaning—to our analysis. Say we are using 3 × 3 for our tile size. That’s 9 pixels. For any tile, consider the pixel in the upper-left corner of the tile. Then we have the same weight <em class="calibre13">w</em><sub class="calibre27">11</sub> for that pixel, regardless of whether the tile is near the top of the picture, say, or the bottom.</p>
<p class="indent">For facial recognition, for instance, this means that, to a large extent, we don’t have to worry whether the face is near the top of the picture, near the bottom, or near the middle. (Problems do occur near the edges of the picture, so the property holds only approximately.) The same statement would hold for left-right positioning.</p>
<h3 class="h2" id="ch12lev3">12.3 Tricks of the Trade</h3>
<p class="noindent">Well, then, how in the world is one supposed to come up with the models? How many layers? What kinds of layers? What parameter values?</p>
<p class="indent">One might set some of the model on a hunch informed by the nature of the dataset, such as the size of various parts of the image, the image texture, and so on. But at the end of the day, the answer tends to be rather prosaic: after years of experimenting with various architectures (configurations), this one seems to work with certain kinds of images. Some architectures have been successful in wide-enough application that they have acquired names and become standards, such as AlexNet.</p>
<h4 class="h3" id="ch12lev3sec1"><em class="calibre22"><strong class="calibre3">12.3.1 Data Augmentation</strong></em></h4>
<p class="noindent">One approach to dealing with smaller image sets is <em class="calibre13">data augmentation</em>. The idea here is simple: form new images from existing ones. One might shift a given image horizontally or vertically, shrink or enlarge the image, flip it horizontally or vertically, and so on. The motivation for this is that, later, we might be asked to classify a new image that is very similar to one in our training set but is, say, much higher or lower within the image frame. We want our algorithm to recognize the new image as being similar to the one in the training set.</p>
<p class="indent">This is especially important for medical tissue images, say, from a biopsy, as there is no sense of orientation—no top or bottom, left or right, or back or front. This is in contrast to MNIST, for instance, where a ‘6’ is an upsidedown ‘9’ and the two are quite different.</p>
<p class="indent">We can perform data augmentation using the <code>OpenImageR</code> package, with its <code>Augmentation()</code> function. In the latter, for instance, we can do a vertical flip operation:</p>
<pre class="calibre16">&gt; <span class="codestrong">h18f &lt;- Augmentation(matrix(h18,nrow=28),flip_mode='vertical')</span>
&gt; <span class="codestrong">imageShow(matrix(h18f,nrow=28))</span></pre>
<p class="indent">The <code>keras</code> package also offers data augmentation services, including a <em class="calibre13">shear</em> (twist) operation.<span epub:type="pagebreak" id="page_210"/></p>
<h4 class="h3" id="ch12lev3sec2"><em class="calibre22"><strong class="calibre3">12.3.2 Pretrained Networks</strong></em></h4>
<p class="noindent">A big issue in the image classification community is <em class="calibre13">transfer learning</em>. Here the issue is that, instead of starting from scratch in designing a neural network—dense layers, convolutional layers, and details of each—one builds on some network that others have found useful. One then either uses that network as is or takes it as a starting point and does some tweaking.</p>
<h3 class="h2" id="ch12lev4">12.4 So, What About the Overfitting Issue?</h3>
<p class="noindent">As noted in <a href="ch12.xhtml#ch12lev2sec8" class="calibre12">Section 12.2.8</a>, the success of heavily overparameterized networks in image classification seems to contradict the conventional wisdom regarding overfitting. This has been the subject of much speculation in the ML community.</p>
<p class="indent">A key point may be that misclassification rates in image contexts tend to be very low, near 0 for highly tuned networks. In that sense, we are essentially in the settings that were termed <em class="calibre13">separable</em> in <a href="ch10.xhtml" class="calibre12">Chapter 10</a>. Some insight into this issue may then be gained by revisiting <a href="ch10.xhtml#ch10fig04" class="calibre12">Figure 10-4</a> in that chapter.</p>
<p class="indent">As was pointed out, there are many lines, infinitely many, in fact, that could be used to distinguish the two classes, and thus be used to predict a new case. SVM chooses a particular line for this—the one halfway between the two closest points in the two classes—but again, one might use many other lines instead.</p>
<p class="indent">Indeed, the separator need not be a straight line. It could be a “curvy” line, say, one obtained by using a polynomial kernel with SVM. Because of the clean separation of the two classes, there is plenty of wiggle room in which we could fit a very wiggly curve, say, a polynomial, of very high degree. And the higher the degree, the more coefficients in the equation of the curve—that is, the larger the value of <em class="calibre13">p</em>.</p>
<p class="indent">The result: we could fit a curve that has a value of <em class="calibre13">p</em> much greater than <em class="calibre13">n</em> yet still get perfect prediction accuracy. Noting the connection of NNs to polynomial regression (see <a href="ch11.xhtml#ch11lev9" class="calibre12">Section 11.9</a>), we have a plausible explanation for the success of overparameterization in image classification.</p>
<h3 class="h2" id="ch12lev5">12.5 Conclusions</h3>
<p class="noindent">In spite of this book’s aim to avoid writing many equations, the topic here is easily the most mathematical of all the chapters. Viewed from a high level, CNNs work from a very simple idea: break an image into tiles and then apply an NN to the tiled data. But the old saying “The devil is in the details” is quite apt here. It can be challenging, for instance, to keep clear in one’s mind the dimensionality of chunks of data as we move from layer to layer. Readers who wish to pursue further study beyond the introduction here will find a background in linear algebra and calculus to be quite useful.</p>
</div></body></html>