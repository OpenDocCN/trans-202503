<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" epub:prefix="index: http://www.index.com/" lang="en" xml:lang="en">
<head>
<title>Chapter 6: Newer Applications of Geometry in Machine Learning</title>
<link href="NSTemplate_v1.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:ea5eeba6-9dea-4463-bfe4-b91d6c6b5861" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter chapter">
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_131" title="131"/><a class="XrefDestination" id="6"/><span class="XrefDestination" id="xref-503083c06-001"/>6</span><br/>
<span class="ChapterTitle"><a class="XrefDestination" id="OtherApplicationsofGeometryinMachineLearning"/><span class="XrefDestination" id="xref-503083c06-002"/>Newer Applications of Geometry in Machine Learning</span></h1>
</header>
<figure class="opener">
<img alt="" src="image_fi/book_art/chapterart.png"/>
</figure>
<p class="ChapterIntro">In <span class="xref" itemid="xref_target_Chapter 5"><a href="c05.xhtml">Chapter 5</a></span>, we explored the contributions of metric geometry to machine learning and its myriad uses in model measurements and input. However, geometry has provided many other contributions to machine learning; in this chapter, we’ll explore tangent-space-based approaches to model estimation, exterior calculus, tools related to the intersection of curves (which can be used to replace linear algebra in algorithms), and rank-based models that involve vector fields acting on datasets’ tangent spaces. We’ll see how these tools can help in supervised learning on small datasets, help communities plan for disasters, and discern choice preferences of customers.</p>
<h2 id="h1-503083c06-0001"><span epub:type="pagebreak" id="Page_132" title="132"/><a class="XrefDestination" id="UsingdgLARSforClassificationandRegression"/><span class="XrefDestination" id="xref-503083c06-003"/>Working with Nonlinear Spaces</h2>
<p class="BodyFirst">Our first tool helps mathematicians and machine learning engineers work with nonlinear spaces such as manifolds; it’s the definition of a point’s <em>tangent space</em>. Thinking back to calculus classes, we recall the <em>tangent lines</em> of a function are lines that touch a point on a curve without crossing the curve—where the slope of the curve equals the slope of the tangent line (giving the first derivative of the curve). Consider the sine wave example and a point on that sine wave, along with its tangent line, shown in <a href="#figure6-1" id="figureanchor6-1">Figure 6-1</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06001.png"/>
<figcaption><p><a id="figure6-1">Figure 6-1</a>: A sine wave example with tangent line drawn at one of the local maxima</p></figcaption>
</figure>
<p>This kind of tangent line works well in two dimensions. However, trying to define tangent lines to a point on a surface gets trickier, as many (infinitely many) lines can be tangent to a given point (see <a href="#figure6-2" id="figureanchor6-2">Figure 6-2</a>, point A).</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06002r.png"/>
<figcaption><p><a id="figure6-2">Figure 6-2</a>: An ellipse with multiple possible tangent lines through point A</p></figcaption>
</figure>
<p>In fact, the lines in <a href="#figure6-2">Figure 6-2</a> form a two-dimensional plane tangent to point A, akin to a sheet of paper that touches the ellipse at point A. What one could do is glue this <em>tangent plane</em> to point A, as in <a href="#figure6-3" id="figureanchor6-3">Figure 6-3</a>.</p>
<span epub:type="pagebreak" id="Page_133" title="133"/><figure>
<img alt="" class="" src="image_fi/503083c06/f06003r.png"/>
<figcaption><p><a id="figure6-3">Figure 6-3</a>: An ellipse with point A and the tangent plane associated with point A that extends tangent lines to tangent planes and spaces</p></figcaption>
</figure>
<p>In the case of higher-dimensional objects, the tangent spaces can grow to involve more dimensions (it can be 3-dimensional, 100-dimensional, or even an infinite-dimensional box). These tangent spaces have a nice connection to linear algebra. Remember that a vector space can be defined by a set of independent vectors collected into a matrix, called the <em>basis</em> of the space, technically known as <em>Hamel basis</em>. The basis for the tangent space of an object at a point, in fact, is the set of a point’s partial derivatives. As mentioned earlier, in a one-dimensional space, this is exactly the slope of the tangent line. This gives a nice Euclidean space associated with each point on the manifold, which can be used to derive unit distances between points, provide mappings to a Euclidean space from a manifold, and understand multicollinearity. Multicollinearity occurs when variables are strongly correlated, which results in matrix columns or rows that are identical or nearly identical (causing singular matrices). Multicollinearity is a problem for regression-based algorithms, as it leads to redundant predictors and singular matrices. Variables with perfect overlap of variance (highly collinear predictors) will have the same tangent space or at least share some overlapping tangent space.</p>
<h3 id="h2-503083c06-0001"><a class="XrefDestination" id="IntroducingdgLARS"/><span class="XrefDestination" id="xref-503083c06-004"/>Introducing dgLARS</h3>
<p class="BodyFirst">One useful machine learning algorithm based on tangent spaces is the <em>dgLARS</em> algorithm (dgLARS stands for “differential geometry least angle regression”). dgLARS extends traditional least angle regression (LARS) to an algorithm that fits to a given model’s error tangent space. The LARS algorithm traditionally starts with each coefficient in the regression model set to 0, with predictors added progressively according to which predictor is most correlated with the outcome; coefficients are adjusted through least squares computation until a higher correlation enters the model. When multiple predictors have entered the model, coefficients are increased in joint least squares directions.</p>
<p><span epub:type="pagebreak" id="Page_134" title="134"/>dgLARS considers the model’s tangent space, scaling the score function used to optimize the coefficients. Each update to the model is done using the square root of a tool called the <em>conditional Fisher information</em>. The conditional Fisher information roughly measures the amount of information a given variable contains relative to a target (such as an outcome variable). For more technical-minded readers, the Fisher information of a parameter is the variance of the parameter’s score, which is the partial derivative of that parameter with respect to the natural logarithm of the likelihood function.</p>
<p>Let’s make this concrete with an example. Say we are creating a model to understand factors that impact adolescent risk-taking behaviors, such as drug use or petty crime. We may have many known factors measured already (such as family socioeconomic status, secondary school grades, and prior school or legal incidents). However, we’d like to compare an index from a survey we’ve designed to measure risk propensity in adolescents (index 1) to a known index that measures risk-taking in adults (index 2). Both surveys likely capture different types of information and different levels of relevant information with respect to our risk outcomes (<a href="#figure6-4" id="figureanchor6-4">Figure 6-4</a>).</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06004.png"/>
<figcaption><p><a id="figure6-4">Figure 6-4</a>: Comparison of number of questions loading onto a risk index</p></figcaption>
</figure>
<p>In the <a href="#figure6-5" id="figureanchor6-5">Figure 6-5</a> example, survey 1’s index contains a greater volume of information. However, there may be some overlap between the variables or irrelevant information in survey 2’s index, and it would be nice to have a measurement that can capture such information. Perhaps there is some overlap of information between the questions asked and some irrelevant information in each survey as well. Let’s consider unique and relevant information contained in each risk-propensity survey (<a href="#figure6-5">Figure 6-5</a>).</p>
<span epub:type="pagebreak" id="Page_135" title="135"/><figure>
<img alt="" class="" src="image_fi/503083c06/f06005.png"/>
<figcaption><p><a id="figure6-5">Figure 6-5</a>: Two adjusted measures of relevant information captured in index questions</p></figcaption>
</figure>
<p>From <a href="#figure6-5">Figure 6-5</a>, we see that index 1 and index 2 contain some irrelevant information and some overlapping information. Index 1 does seem to contain a bit more information than index 2, and if we had to choose which survey to administer to a larger population of at-risk adolescents, we’d be better off starting with index 1.</p>
<p>This is a bit how Fisher information and variable selection in dgLARS happen. Technically, a <em>score</em> is calculated through partial derivatives of the model’s log likelihood function, and the variance of this score is the Fisher information, which can be entered into a matrix to capture information across partial derivatives of the model. Interestingly, this matrix can also be derived as the Hessian of the relative entropy (Kullback–Leiber divergence), and the Fisher information gives the curvature of relative entropy in this case. At a less technical level, the Fisher information used to select variables has to do with both a statistical measure (the log likelihood function) and the information geometry of the independent variables being considered.</p>
<p>In the case of generalized linear models, the Fisher information matrix can be used to derive a score called the <em>conditional Rao score</em>, which can test whether the coefficient for a given variable is statistically different from 0. If the score is not statistically different from 0, the variable is dropped from consideration in the model. In the dgLARS algorithm, these calculations are done by searching coefficient vectors in the model error’s tangent space, starting with the null model. This space’s geometry is then iteratively partitioned into three sets: <em>selected predictors</em>, which have good fit scores in the error tangent space; <em>redundant predictors</em>, which share an error tangent space with selected predictors; and <em>nonselected predictors</em>, which have poor fit scores in the error tangent space. Thus, dgLARS leverages information about the model’s geometry to find a best-fitting model.</p>
<p><span epub:type="pagebreak" id="Page_136" title="136"/>dgLARS has had success on problems involving more predictors than observations in datasets, and many of the publications on this algorithm focus on genomics applications, where the number of patients might be 300 and the number of genes sequences might be in the 1,000,000 range. R provides a package, dglars, that implements this algorithm for generalized linear models, including link functions for logistic regression, Poisson regression, linear regression, and gamma regression. For those unfamiliar with generalized linear regression, a <em>link function</em> essentially is a special type of transformation of a dependent variable that allows regression to work mathematically for count variables, binary variables, and other types of not normally distributed dependent variables (with some restrictions on the geometry of the distribution).</p>
<p>Now that we’ve covered some of the theory, let’s put it into practice.</p>
<h3 id="h2-503083c06-0002"><a class="XrefDestination" id="UsingdgLARStoPredictDepression"/><span class="XrefDestination" id="xref-503083c06-005"/>Predicting Depression with dgLARS</h3>
<p class="BodyFirst">We’d like to predict self-reported depression based on school issues and IQ. The data we’ll use is self-reported school issues in a self-selected sample of profoundly gifted Quora users (with IQs above 155), including seven main school issues (bullying, teacher hostility, boredom, depression, lack of motivation, outside learning, put in remediation courses). The data was reported across 22 individuals who provided scores in the profoundly gifted range and discussed at least one of the issues of interest in a school system, with a bias toward users in the United States. This dataset was collected from the platform and processed manually to obtain a final dataset with categories of school issues for posters who met the IQ criterion. You can find the dataset in the book’s files. Let’s load the data first with the code in <a href="#listing6-1" id="listinganchor6-1">Listing 6-1</a>.</p>
<pre><code>#load data
mydata&lt;-read.csv("QuoraSample.csv")
set.seed(1)</code></pre>
<p class="CodeListingCaption"><a id="listing6-1">Listing 6-1</a>: A script that loads the Quora dataset</p>
<p>The code in <a href="#listing6-1">Listing 6-1</a> loads a dataset examining educational interventions and a psychological metric of self-esteem. Now let’s modify the script to run the dgLARS algorithm on the dataset. The R package gives us the option of doing cross-validation or running the algorithm without cross-validation. Let’s modify <a href="#listing6-1">Listing 6-1</a> to run both model options:</p>
<pre><code>#run analysis
library(dglars)
dg&lt;-dglars(factor(Depression)~.,data=mydata,family="binomial")
dg1&lt;-cvdglars(factor(Depression)~.,data=mydata,
family="binomial",control=list(nfold=2))</code></pre>
<p>This runs the cross-validated and non-cross-validated dgLARS algorithms on the full set of student data. The cross-validated version does not work well on small datasets with sparse predictors, so if you run into an error, keep trying to run the cross-validated version, as some partitions may produce an error related to splitting and fitting models to the splits. The <span epub:type="pagebreak" id="Page_137" title="137"/>output from the two dgLARS algorithms should agree on many predictors, though the cross-validated split version of the algorithm may vary a bit, as the data is randomly partitioned. Let’s add our script to look at the summary of the first model:</p>
<pre><code>#examine results of non-cross-validated model
<b>&gt;summary(dg)</b>
Summary of the Selected Model

    Formula: factor(Depression) ~ 1
     Family: 'binomial'
       Link: 'logit'

Coefficients:
     Estimate
Int.  -1.5041
---

                 g: 1.265
     Null deviance: 20.86
 Residual deviance: 20.86
               AIC: 22.86

Algorithm 'pc' (method='dgLASSO')</code></pre>
<p>No factors are selected as important predictors in this model. There are a few main reasons why no factors might be selected by the model, including the existence of subpopulations with opposite effects that “average out” the angles of the subpopulations, the existence of outliers that warp the geometry, or a true null effect for predictors. Model fit is reasonable, with an Akaike information criterion (AIC) of 20.86 and a residual deviance much smaller than the null deviance with no terms added to the model. The dispersion parameter tells us that the data fits reasonably well to the binomial distribution (see how <code>g</code> is close to 1) without problems in the distribution that sometimes occur in real-world data.</p>
<p>Now let’s add to our script to look at the results from the cross-validated trials:</p>
<pre><code>#examine cross-validated model
<b>&gt;summary(dg1)</b>
Call:  cvdglars(formula=factor(Depression) ~ ., family="binomial",
    data=mydata, control=list(nfold=2))

Coefficients:
                       Estimate
Int.                    -1.6768
Bullying                 0.3066
Put.in.Remedial.Course   0.2292

dispersion parameter for binomial family taken to be 1

Details:
   number of non-zero estimates: 3
<span epub:type="pagebreak" id="Page_138" title="138"/>      cross-validation deviance: 12.33
                              g: 0.9838
                        n. fold: 2

Algorithm 'pc' (method='dgLASSO')</code></pre>
<p>The cross-validated model should give a bit different result than the non-cross-validated model, suggesting the models are finding some consistency but not completely overlapping across folds. In the cross-validated model, profoundly gifted children put in remedial courses tend to have higher rates of depression, and children like the ones in this sample who are put in remedial courses and start showing signs of depression may benefit from this intervention. As you can see, bullying is also a potential issue leading to depression, suggesting something we likely already know: that bullying in general shouldn’t be tolerated in a school for optimal mental health outcomes.</p>
<p>Given the small sample size, it’s likely that a generalized linear regression model would struggle to estimate the coefficients. The necessary sample size for topology- and geometry-based linear models seems to be smaller than linear regression, and the consistent results on this problem suggest these models can work on very, very small data. However, there is still probably a minimum sample size needed for cross-validation, so it’s best to avoid doing even these analyses if the sample size is less than 10.</p>
<h3 id="h2-503083c06-0003"><a class="XrefDestination" id="UsingdgLARStoPredictCreditDefaulting"/><span class="XrefDestination" id="xref-503083c06-006"/>Predicting Credit Default with dgLARS</h3>
<p class="BodyFirst">To understand how dgLARS works on a larger dataset with a binary outcome (logistic regression) and more observations and variables, let’s consider another dataset. The UCI credit default dataset includes 30,000 credit cases in Taiwan (late 2005) and 23 predictors of defaulting, including demographics (age, marriage status, education status, and gender), credit limit, and prior usage and payment information. The goals of our analysis are to figure out what predictors are related to whether an account ends up defaulting and to assess the model fit of our dgLARS model.</p>
<p>Let’s get started on this with the code in <a href="#listing6-2" id="listinganchor6-2">Listing 6-2</a>.</p>
<pre><code>#load data
mydata&lt;-read.csv("UCIDefaultData.csv")

#load library
library(dglars)

#run analysis, scaling the predictors such that big numbers don't
#result in large differences in coefficient values
#scale the predictor data
mydata1&lt;-scale(mydata[,-c(1,25)])
mydata1&lt;-cbind(mydata1,mydata$default.payment.next.month)
colnames(mydata1)[24]&lt;-"default.payment.next.month"

#run the dglars function with and without cross-validation
dg&lt;-dglars(factor(default.payment.next.month)~.,
<span epub:type="pagebreak" id="Page_139" title="139"/>data=as.data.frame(mydata1),family="binomial")
dg1&lt;-cvdglars(factor(default.payment.next.month)~.,
data=as.data.frame(mydata1),family="binomial")
summary(dg)
summary(dg1)</code></pre>
<p class="CodeListingCaption"><a id="listing6-2">Listing 6-2</a>: A script that loads, processes, and analyzes the UCI credit default dataset with the dgLARS and cross-validated dgLARS algorithms</p>
<p>This should yield two models with coefficients for most predictors. The first model is the non-cross-validated model version (DG), and the second model is the cross-validated version (DG1). <a href="#table6-1" id="tableanchor6-1">Table 6-1</a> shows results from our run of the code.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table6-1">Table 6-1</a>: Coefficients of Terms in the UCI Credit Default dgLARS Model</p></figcaption>
<table border="1" id="table-503083c06-0001">
<thead>
<tr>
<td><b>Column1</b></td>
<td><b>DG estimate</b></td>
<td><b>DG1 estimate</b></td>
</tr>
</thead>
<tbody>
<tr>
<td>Int.</td>
<td>–1.47</td>
<td>–1.45</td>
</tr>
<tr>
<td>LIMIT_BAL</td>
<td>–0.10</td>
<td>–0.09</td>
</tr>
<tr>
<td>SEX</td>
<td>–0.05</td>
<td>–0.04</td>
</tr>
<tr>
<td>EDUCATION</td>
<td>–0.08</td>
<td>–0.07</td>
</tr>
<tr>
<td>MARRIAGE</td>
<td>–0.08</td>
<td>–0.07</td>
</tr>
<tr>
<td>AGE</td>
<td>0.07</td>
<td>0.06</td>
</tr>
<tr>
<td>PAY_0</td>
<td>0.65</td>
<td>0.65</td>
</tr>
<tr>
<td>PAY_2</td>
<td>0.10</td>
<td>0.09</td>
</tr>
<tr>
<td>PAY_3</td>
<td>0.09</td>
<td>0.09</td>
</tr>
<tr>
<td>PAY_4</td>
<td>0.03</td>
<td>0.03</td>
</tr>
<tr>
<td>PAY_5</td>
<td>0.04</td>
<td>0.04</td>
</tr>
<tr>
<td>PAY_6</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td>BILL_AMT1</td>
<td>–0.39</td>
<td>–0.15</td>
</tr>
<tr>
<td>BILL_AMT2</td>
<td>0.16</td>
<td>0.02</td>
</tr>
<tr>
<td>BILL_AMT3</td>
<td>0.09</td>
<td>0.02</td>
</tr>
<tr>
<td>BILL_AMT5</td>
<td>0.03</td>
<td>0.00</td>
</tr>
<tr>
<td>BILL_AMT6</td>
<td>0.02</td>
<td>0.00</td>
</tr>
<tr>
<td>PAY_AMT1</td>
<td>–0.22</td>
<td>–0.17</td>
</tr>
<tr>
<td>PAY_AMT2</td>
<td>–0.22</td>
<td>–0.19</td>
</tr>
<tr>
<td>PAY_AMT3</td>
<td>–0.05</td>
<td>–0.05</td>
</tr>
<tr>
<td>PAY_AMT4</td>
<td>–0.06</td>
<td>–0.06</td>
</tr>
<tr>
<td>PAY_AMT5</td>
<td>–0.05</td>
<td>–0.05</td>
</tr>
<tr>
<td>PAY_AMT6</td>
<td>–0.04</td>
<td>–0.03</td>
</tr>
</tbody>
</table>
</figure>
<p>Some of the biggest predictors of default include the prior month’s billing and payment history. Those with lower usage (<code>BILL_AMT1</code>), lower <span epub:type="pagebreak" id="Page_140" title="140"/>payments (<code>PAY_AMT1</code>), and on-time payments (<code>PAY_0</code>) are less likely to default on payment in the following month. This makes a lot of sense, given that most lending metrics prioritize lending at the best rates to those with low loads of debt and a track record of on-time payment.</p>
<p>The cross-validated dgLARS model penalizes prior month usage and payment total less than the non-cross-validated model, suggesting that prior month on-time payment is more important than specific numbers. The AIC score on the first model is fairly large (27,924), but it is quite a bit smaller than the null model’s AIC score (31,705), indicating a better fit than the null model even with several predictors included.</p>
<p>Now let’s compare this model with logistic regression and compare the AIC fit statistics, adding the following to our script:</p>
<pre><code>#run logistic regression
gl&lt;-glm(factor(default.payment.next.month)~.,
data=as.data.frame(mydata1),family="binomial"(link="logit"))

#calculate AIC of the model
AIC(gl)</code></pre>
<p>This snippet of code runs the logistic regression on the dataset and calculates the model’s AIC. In this example, the AIC should come out to around 27,925, almost exactly that of the dgLARS models. This suggests a convergence of logistic regression and the dgLARS algorithm; at this large a sample size, this result is expected. Logistic regression is the typical tool for large sample sizes, and it doesn’t seem that we get a gain from using dgLARS in this case. However, given the convergence on large sample sizes, it’s likely that dgLARS gives quality results at the smaller sample sizes that won’t work with logistic regression.</p>
<h2 id="h1-503083c06-0002"><a class="XrefDestination" id="UnderstandingDiscreteExteriorDerivativesandTheirApplications"/><span class="XrefDestination" id="xref-503083c06-007"/>Applying Discrete Exterior Derivatives</h2>
<p class="BodyFirst">Another useful tool that has come out of differential geometry is <em>discrete exterior derivatives</em>. Discrete exterior derivatives involve building up discrete shapes from lower-dimensional discrete shapes. In prior chapters, we examined the concept of homology, which counts the holes in a given object; technically, this is done by finding an object’s boundaries at a specific dimension. For instance, consider the boundaries of a triangle (<a href="#figure6-6" id="figureanchor6-6">Figure 6-6</a>).</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06006.png"/>
<figcaption><p><a id="figure6-6">Figure 6-6</a>: The boundaries of a triangle</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_141" title="141"/>We can take this a step further and break down lines into each point connected by the line, as in <a href="#figure6-7" id="figureanchor6-7">Figure 6-7</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06007r.png"/>
<figcaption><p><a id="figure6-7">Figure 6-7</a>: The boundaries of a line</p></figcaption>
</figure>
<p>Just as we can take apart shapes by identifying and separating out boundaries, we can also build shapes up from lower-dimensional boundaries by combining those boundaries. Technically, this is called <em>cohomology</em>, which is the realm of discrete exterior calculus. We might start with two points that are related in some way (perhaps within a certain distance of each other or sharing a characteristic) and connect them with a line (<a href="#figure6-8" id="figureanchor6-8">Figure 6-8</a>).</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06008r.png"/>
<figcaption><p><a id="figure6-8">Figure 6-8</a>: Two points built into a line</p></figcaption>
</figure>
<p>For more technically minded readers, we’re looking at the discrete version of differential forms, which are cochains on simplicial complexes. These differential forms have vector fields associated with them. We can then define operators that change those fields or objects, combine them, or count what exists within a field or cochain. This allows us to wrangle certain types of data to understand problems like resource capacity in electrical grids or burden of disease within social networks (or even rendering graphics across groups of pixels within a computer screen).</p>
<p>This can continue up to arbitrarily many dimensions, with lines building up triangles and triangles building up tetrahedra and so on. We can also jump levels with discrete exterior derivatives, going from points to triangles or tetrahedra rather than lines. Thus, discrete data, such as rendering pixel data or engineering data, can be grouped and connected for further study.</p>
<p>One of the newer applications of discrete exterior derivatives (and homology) is within social network analysis. As we mentioned in prior chapters, graphs are discrete objects of zero and one dimension (points and lines); however, connections between individuals can be extended from two-way mutual interactions (lines connecting points) to cliques of <span epub:type="pagebreak" id="Page_142" title="142"/>3-way (triangle) or 4-way (tetrahedron) or 100-way mutual interactions (a very large dimensional sort of object), as demonstrated by three colleagues mutually connected in <a href="#figure6-9" id="figureanchor6-9">Figure 6-9</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06009.png"/>
<figcaption><p><a id="figure6-9">Figure 6-9</a>: A graph of three colleagues working on projects together represented through a three-way interaction</p></figcaption>
</figure>
<p>On the left of <a href="#figure6-9">Figure 6-9</a>, we see three colleagues (Colleen, Jodelle, and Yaé) who collaborate in pairs but have not worked on a project involving all three of them. On the right of <a href="#figure6-9">Figure 6-9</a>, we see a representation of a project that involves all three colleagues working together. If they collaborate on many papers, we can sum up their two-way collaborations and their three-way collaborations to get a summary total for each <em>n</em>-way collaboration. This might be useful for understanding the strength of this collaborative network’s parts.</p>
<p>Let’s explore how these concepts can help in disaster logistics planning. Suppose there are four towns in a region with all towns connected to at least one other town by a road. Suppose also that each town has its own stock of supplies (perhaps liters of water for each resident) in case a cyclone hits the region and limits transportation for days or weeks. We can model this by creating a graph in R using the code in <a href="#listing6-3" id="listinganchor6-3">Listing 6-3</a>.</p>
<pre><code>#create matrix of town connections and miles between each town
towns&lt;-matrix(c(0,0,0,4,0,0,12,2,0,12,0,6,4,2,6,0),nrow=4)

#create a graph from the matrix with connections between modes
#going in both directions (undirected graph) with weights
library(igraph)
g1&lt;-graph_from_adjacency_matrix(towns,mode="undirected",weighted=T)

#plot town graph with edges labeled by weights
plot(g1,edge.label=E(g1)$weight,main="Plot of Connected Towns by Road Miles")

#add resource (perhaps liters of water per resident)
V(g1)$number&lt;-c(10,500,80,200)
plot(g1,edge.label=E(g1)$weight,vertex.label=V(g1)$number,vertex.size=40,
main="Situation 1")

#find maximal cliques, representing connected resources
mc1&lt;-max_cliques(g1)
mc1

#add resources that are mutually connected between towns
c1&lt;-V(g1)$number[mc1[[1]][1]]+V(g1)$number[mc1[[1]][2]]
<span epub:type="pagebreak" id="Page_143" title="143"/>c2&lt;-V(g1)$number[mc1[[2]][1]]+V(g1)$number[mc1[[2]][2]]+
V(g1)$number[mc1[[2]][3]]

#examine time needed to transport using shortest paths
dis1&lt;-distances(g1,v=V(g1),mode=c("all"),weights=E(g1)$weight,
algorithm="dijkstra")</code></pre>
<p class="CodeListingCaption"><a id="listing6-3">Listing 6-3</a>: A script that generates the example graph of connected towns, plots the graph, adds resources to each town, visualizes these resources, and analyzes mutually connected town resources</p>
<p><a href="#listing6-3">Listing 6-3</a> creates a matrix of towns connected by roads and then converts this into a weighted graph. Once it is in graph form, we can add in information about resources available in each town and plot a picture with this information included, along with the distances between towns connected by a road. We can then calculate mutual resources between towns and minimum travel distances from a given town to another. This will help us assess resources available in a disaster and the best routes down which to send supplies.</p>
<p><a href="#listing6-3">Listing 6-3</a>’s first plot should output a diagram showing which towns are connected and the number of miles between towns, as shown in <a href="#figure6-10" id="figureanchor6-10">Figure 6-10</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06010.png"/>
<figcaption><p><a id="figure6-10">Figure 6-10</a>: A plot of town connectivity and miles of road between connected towns</p></figcaption>
</figure>
<p><a href="#figure6-10">Figure 6-10</a> shows that towns 2, 3, and 4 are connected by multiple roads, such that if one road is blocked, the town can still be reached by looping around through another town. However, town 1 is relatively isolated despite being located only 4 miles from the nearest town. The road connecting towns 2 and 3 is quite long (perhaps this is a back road that meanders through a densely wooded area or around several canals).</p>
<p>The second <code>plot</code> in <a href="#listing6-3">Listing 6-3</a> should output a graph that adds total resources for each town, as shown in <a href="#figure6-11" id="figureanchor6-11">Figure 6-11</a>.</p>
<span epub:type="pagebreak" id="Page_144" title="144"/><figure>
<img alt="" class="" src="image_fi/503083c06/F06011r.png"/>
<figcaption><p><a id="figure6-11">Figure 6-11</a>: A plot of town connectivity, miles of road between connected towns, and resources within each town</p></figcaption>
</figure>
<p><a href="#figure6-11">Figure 6-11</a> gives us a richer understanding of connectivity and potentially shared resources among the four towns. Notice that town 1 has a relatively small water supply stocked for the disaster (10 liters per resident). However, if the road between towns 1 and 4 holds up during the disaster, it is easy to move some of the water from town 4 (with 200 liters per resident) to town 1, such that town 1 has sufficient water. It’s also easy to move water between towns 4 and 2 (which has 500 liters per resident stocked up) provided the road directly connecting these towns holds up in the disaster.</p>
<p>The maximal clique calculation yields mutually connected towns (towns with mutual <em>n</em>-way connections). This gives a connection between towns 1 and 4 (with the single road) and towns 2, 3, and 4, which are mutually connected. From this information, we can calculate resources at each town’s disposal should the roads hold between towns. Towns 1 and 4 mutually contain 210 resident-liters; the three-way clique (towns 2, 3, and 4) contains 780 resident-liters.</p>
<p>Using a shortest path algorithm, we can calculate shortest routes between any two towns to understand how quickly supplies might be routed between towns should supplies run low in a given town and roads are not damaged by the disaster. <a href="#table6-2" id="tableanchor6-2">Table 6-2</a> gives the miles that would need to be driven between towns in this scenario.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table6-2">Table 6-2</a>: Shortest Distances Between Pairs of Towns</p></figcaption>
<table border="1" id="table-503083c06-0002">
<thead>
<tr>
<td/>
<td><b>Town 1</b></td>
<td><b>Town 2</b></td>
<td><b>Town 3</b></td>
<td><b>Town 4</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><b>Town 1</b></td>
<td>0</td>
<td>6</td>
<td>10</td>
<td>4</td>
</tr>
<tr>
<td><b>Town 2</b></td>
<td>6</td>
<td>0</td>
<td>8</td>
<td>2</td>
</tr>
<tr>
<td><b>Town 3</b></td>
<td>10</td>
<td>8</td>
<td>0</td>
<td>6</td>
</tr>
<tr>
<td><b>Town 4</b></td>
<td>4</td>
<td>2</td>
<td>6</td>
<td>0</td>
</tr>
</tbody>
</table>
</figure>
<p><span epub:type="pagebreak" id="Page_145" title="145"/>The longest route in this scenario (10 miles) is trying to get supplies from town 3 to town 1 when town 1 runs low on water. Notice that it’s shorter to bypass the longest road to route supplies between towns 2 and 3 (8 miles versus 12 miles).</p>
<p>We can examine a scenario where one or more roads is damaged in the disaster by adding to <a href="#listing6-3">Listing 6-3</a> to remove roads connecting towns and re-examine mutual supplies and shortest paths:</p>
<pre><code>#remove the link between vertices 2 and 4, providing an efficient supply
#route
g2&lt;-delete_edges(g1,edges="2|4")
V(g2)$number&lt;-c(10,500,80,200)
plot(g2,edge.label=E(g2)$weight,vertex.label=V(g2)$number,vertex.size=40,
main="Situation 2")

#find maximal cliques
mc2&lt;-max_cliques(g2)
mc2

#find resources between sites
c1&lt;-V(g2)$number[mc2[[1]][1]]+V(g2)$number[mc2[[1]][2]]
c2&lt;-V(g2)$number[mc2[[2]][1]]+V(g2)$number[mc2[[2]][2]]
c3&lt;-V(g2)$number[mc2[[3]][1]]+V(g2)$number[mc2[[3]][2]]

#examine time needed to transport using shortest paths
dis2&lt;-distances(g2,v=V(g2),mode=c("all"),weights=E(g2)$weight,
algorithm="dijkstra")</code></pre>
<p>This script removes a link between towns 2 and 4, recalculating the metrics to help us assess how the situation has changed with the blockage of the road between towns 2 and 4. These modifications should yield a plot of situation 2 similar to <a href="#figure6-12" id="figureanchor6-12">Figure 6-12</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/F06012r.png"/>
<figcaption><p><a id="figure6-12">Figure 6-12</a>: A modified plot of situation 1 with one road destroyed in the disaster</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_146" title="146"/>As <a href="#figure6-12">Figure 6-12</a> shows, this scenario breaks the triangle present in situation 1, isolating town 2. If town 1’s water supply runs low, the water must be shared between towns 1 and 2 with the expectation that it may be difficult to move water from town 2 to replenish the supplies.</p>
<p>If we look at the cliques, we can see two-way connections among towns, and mutual supplies between towns are more spread out than in the three-way connection present in situation 1. Towns 2 and 3 have a decent mutual supply, but the other towns may have delays in routing needed water. In addition, the miles needed to travel increases, as we can see in the shortest path table (<a href="#table6-3" id="tableanchor6-3">Table 6-3</a>).</p>
<figure>
<figcaption class="TableTitle"><p><a id="table6-3">Table 6-3</a>: Shortest Distances Among Pairs of Towns</p></figcaption>
<table border="1" id="table-503083c06-0003">
<thead>
<tr>
<td/>
<td><b>Town 1</b></td>
<td><b>Town 2</b></td>
<td><b>Town 3</b></td>
<td><b>Town 4</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><b>Town 1</b></td>
<td>0</td>
<td>22</td>
<td>10</td>
<td>4</td>
</tr>
<tr>
<td><b>Town 2</b></td>
<td>22</td>
<td>0</td>
<td>12</td>
<td>18</td>
</tr>
<tr>
<td><b>Town 3</b></td>
<td>10</td>
<td>12</td>
<td>0</td>
<td>6</td>
</tr>
<tr>
<td><b>Town 4</b></td>
<td>4</td>
<td>18</td>
<td>6</td>
<td>0</td>
</tr>
</tbody>
</table>
</figure>
<p>Routing water from the town with the largest supply (town 2) has become a lot more difficult, with greatly increased expected travel times. Should scenario 2 appear likely, it’s probably best to redistribute the water prior to the disaster to avoid delays in routing.</p>
<p>Discrete exterior derivatives have other applications. Graphics rendering and engineering problems have been particular areas of interest within discrete exterior derivatives (and discrete exterior calculus in general). A few of the more common applications in engineering are flux and flow calculations on discrete objects or computer modeling of processes. Within graphics rendering, graphs are typically replaced with more general meshes.</p>
<p>In some instances, it is easier to use cohomology (and its tool, discrete exterior derivatives) to study an object or point cloud than to use persistent homology, as the end result will find the same boundaries and objects. However, little has been done to make explicit R packages for applying discrete exterior derivatives to data, and code must be parsed together as in <a href="#listing6-3">Listing 6-3</a>. Automation and object manipulation code would help facilitate the adoption of discrete exterior derivatives within data science and other fields.</p>
<h2 id="h1-503083c06-0003"><a class="XrefDestination" id="ReplacingLinearAlgebrawithNonlinearAlgebrainMachineLearningAlgorithms"/><span class="XrefDestination" id="xref-503083c06-008"/>Nonlinear Algebra in Machine Learning Algorithms</h2>
<p class="BodyFirst">Another intriguing and recent development with respect to geometry in machine learning is the notion of nonlinear algebra in machine learning algorithms. Many machine learning algorithms rely heavily on linear <span epub:type="pagebreak" id="Page_147" title="147"/>algebra to compute things such as gradients, least squares estimators, and so on. However, in relationships and spaces that aren’t flat or involving straight lines, the linear algebra provides only an approximation of quantities calculated. Take a look at <a href="#figure6-13" id="figureanchor6-13">Figure 6-13</a>, which shows a straight line (assumed by linear algebra tools, such as those used in regression) and a curved line (which may cause an estimation problem for linear algebra tools).</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06013r.png"/>
<figcaption><p><a id="figure6-13">Figure 6-13</a>: A straight line and curved line</p></figcaption>
</figure>
<p>Nonlinearity introduces error into the calculations and final result of an algorithm when curves and nonlinear spaces are estimated using linear tools. Imagine using a ruler to measure the lower line in <a href="#figure6-13">Figure 6-13</a>. It would be hard to get an exact length of the line relative to the straight line above. If the length of the line were a quantity that needed to be minimized or maximized by an algorithm, such a measurement could potentially find a nonglobal solution or distort the quantity enough to cause problems in predictive accuracy or model fit statistics like sum of square error or Bayesian information criterion.</p>
<p>One proposed alternative to linear algebraic calculations within machine algorithms is to use <em>numerical algebraic geometry</em>, a branch of nonlinear algebra that deals with the intersections of curves. For instance, consider the two-dimensional ellipse intersected by a one-dimensional curve, shown in <a href="#figure6-14" id="figureanchor6-14">Figure 6-14</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06014r.png"/>
<figcaption><p><a id="figure6-14">Figure 6-14</a>: A plot of a curve intersecting an ellipse</p></figcaption>
</figure>
<p>Different types of matrices and matrix operations can be used to solve nonlinear systems analogously to how linear algebra solves linear systems; numbers populating a matrix are simply replaced with polynomial equations. Some of these intersecting curve problems are nonconvex problems, which often pose issues to machine learning algorithms and the linear algebra powering them.</p>
<p><span epub:type="pagebreak" id="Page_148" title="148"/><em>Convex optimization problems </em>are those in which the optimization function creates a region in which a line passing through the region is in the region continuously (rather than passing multiple regions and nonregions within the object), as shown in <a href="#figure6-15" id="figureanchor6-15">Figure 6-15</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06015r.png"/>
<figcaption><p><a id="figure6-15">Figure 6-15</a>: A convex object</p></figcaption>
</figure>
<p>If, however, the region contains holes or indents, this is no longer the case, and the region is designated as <em>nonconvex</em>, such as in <a href="#figure6-16" id="figureanchor6-16">Figure 6-16</a>, where the hole inside the region splits the interior set into the set within the region and the set within the hole.</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06016r.png"/>
<figcaption><p><a id="figure6-16">Figure 6-16</a>: A nonconvex object</p></figcaption>
</figure>
<p>Optimization algorithms often struggle with nonconvex functions and regions within the optimization function, as the linear programming commonly used to solve these problems isn’t as amenable to nonconvex optimization problems and as stepwise methods can get stuck in the local optima around the hole or divots in the region. Unfortunately, many real-world datasets and the optimization functions used on them involve nonconvex regions. Numerical algebraic geometry offers an accurate solution for nonconvex optimization problems, which come up often in real-world data situations. Once the system of polynomials is set up in matrix form, many software platforms and programs exist to do the computations, including the Bertini and Macauley software, which can connect to both R and Python. This allows for solvers that work well in nonconvex optimization problems.</p>
<p>Several recent publications and workshops in numerical algebraic geometry suggest that nonlinear algebra is a viable alternative to the <span epub:type="pagebreak" id="Page_149" title="149"/>current machinery in algorithms for other types of problems that may be convex, loosening mathematical assumptions and providing improvements in accuracy. One recent paper (Evans, 2018) found that the local geometry of many possible solutions overlaps for many types of statistical models (including Lasso, ARIMA models for time-series data, and Bayesian networks). This means that algorithms can’t distinguish well between potential predictor sets or parameter values in the model space, particularly for the sample sizes commonly used and suggested for these problems. One can solve this problem by either fitting the model in tangent space, as we saw in the previous section, or using numerical algebraic geometry instead of linear algebra for optimization. This suggests some immediate applications of numerical algebraic geometry and other nonlinear approaches to machine learning algorithms for improved algorithm performance and model fits.</p>
<p>Unfortunately, packages to implement these algorithms do not exist in R at this time, so we won’t explore this further in an example.</p>
<h2 id="h1-503083c06-0004"><a class="XrefDestination" id="UsingHodgeRank"/><span class="XrefDestination" id="xref-503083c06-009"/>Comparing Choice Rankings with HodgeRank</h2>
<p class="BodyFirst">Discerning choice preference across a population of customers is a common machine learning task. For instance, a company might want customers to compare items on a list of potential new features in software to prioritize the engineering team’s time in the coming year. Comparing choice rankings also helps companies market new products and services to existing users and derive new campaigns for items that are likely to sell well with existing customers.</p>
<p>Let’s look at a simple example of ranking activity preference for a day at the beach. Perhaps we’re looking at data that a hotel collected from recent guests on which activities they preferred during their stay; this would allow them to prioritize beach usage and staff hiring to meet future demand for the main activities their guests prefer. Here, we have three choices of activity (lying on the beach, swimming, or surfing), with one preference as a clear favorite (surfing). <a href="#figure6-17" id="figureanchor6-17">Figure 6-17</a> summarizes this simple situation.</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06017.png"/>
<figcaption><p><a id="figure6-17">Figure 6-17</a>: A diagram ranking three choices relative to the other choices</p></figcaption>
</figure>
<p>We can complicate this problem by adding a potentially new fourth activity that is preferred to the other three: kiteboarding. The ranking is still relatively easy to compute, as all activities are preferred to lying on the <span epub:type="pagebreak" id="Page_150" title="150"/>beach, one is preferable only to lying on the beach (swimming), one is preferred to every activity but kiteboarding (surfing), and one is preferred to the other three (kiteboarding), as shown in <a href="#figure6-18" id="figureanchor6-18">Figure 6-18</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06018.png"/>
<figcaption><p><a id="figure6-18">Figure 6-18</a>: A diagram with another activity added to the preference data</p></figcaption>
</figure>
<p>All of the information is given in the diagram, with this particular person filling out all choices relative to each other. This is rarely the case with real-world data, as shown in <a href="#figure6-19" id="figureanchor6-19">Figure 6-19</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06019.png"/>
<figcaption><p><a id="figure6-19">Figure 6-19</a>: An incomplete preference diagram of the four activities</p></figcaption>
</figure>
<p>However, this information still shows a strong preference toward kiteboarding, with all pair-ranks existing for kiteboarding pairs and all pair-ranks pointing toward kiteboarding as the most preferred activity. In real-world data, it’s common not only to have missing information but to have preference loops in the data, such as the one in <a href="#figure6-20" id="figureanchor6-20">Figure 6-20</a>, where surfing is preferred to swimming, and kiteboarding is preferred to surfing but not to swimming.</p>
<span epub:type="pagebreak" id="Page_151" title="151"/><figure>
<img alt="" class="" src="image_fi/503083c06/f06020.png"/>
<figcaption><p><a id="figure6-20">Figure 6-20</a>: An example of incomplete and inconsistent ranking preference data</p></figcaption>
</figure>
<p>This gives a local inconsistency of where kiteboarding and swimming might rank relative to each other when other options are available. However, surfing and kiteboarding are preferred to two other activities, suggesting they rank toward the top of possible options, and kiteboarding is preferred to surfing (giving a tiebreaker of sorts).</p>
<p>The situation becomes a bit more nebulous when no consistent preferences exist globally, with each activity preferred to another activity, as shown in <a href="#figure6-21" id="figureanchor6-21">Figure 6-21</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06021.png"/>
<figcaption><p><a id="figure6-21">Figure 6-21</a>: A diagram of incomplete preference data with no consistent preferences</p></figcaption>
</figure>
<p>In the case of <a href="#figure6-21">Figure 6-21</a>, nothing can be said about which activity would be preferred to other activities, and results of the analyses would be inconclusive. This is common when customers are asked to rank features in financial apps or guests are asked to rank preferred activities. Most customers will exist in subgroups with their own unique needs, which might be the opposite needs of another important customer subgroup. It’s a challenge to prioritize features for development or choices to give guests to please the largest number of customers.</p>
<p><span epub:type="pagebreak" id="Page_152" title="152"/>Many algorithms exist to do pairwise-rank comparisons to get a ranked list of preferences relative to each other (SVMRank, PageRank, and more); however, in general, they do not provide information about local and global inconsistencies in rank that might influence where an item or choice is placed relative to others. Algebraic geometry recently added a tool to the collection of pairwise-rank algorithms that can decompose the ranking results to include information about local and global inconsistencies of items; that would be <em>HodgeRank</em>, which can derive this information by leveraging an algebraic-geometry-based theorem common in engineering problems: the Hodge–Helmholtz decomposition.</p>
<p><em>The Hodge</em>–<em>Helmholtz decomposition</em> partitions a vector flow (or flow on a graph) into three distinct components, shown in <a href="#figure6-22" id="figureanchor6-22">Figure 6-22</a>: the <em>gradient flow</em>, which is locally and globally consistent; the <em>curl flow</em>, which is locally consistent but globally inconsistent; and the <em>harmonic flow</em>, which is locally inconsistent but globally consistent.</p>
<figure>
<img alt="" class="" src="image_fi/503083c06/f06022.png"/>
<figcaption><p><a id="figure6-22">Figure 6-22</a>: A diagram showing the flows broken down by the Hodge–Helmholtz decomposition</p></figcaption>
</figure>
<p>In the beach examples, <a href="#figure6-20">Figure 6-20</a> has a curl flow involving swimming, surfing, and kiteboarding (also harmonic if another activity is not ranked there). <a href="#figure6-21">Figure 6-21</a> is an example of global harmonic flow (as well as local curl flow).</p>
<p>The HodgeRank algorithm essentially extends PageRank for pairwise-ranking problems; the math boils down to a least squares problem on the graph data, allowing for assessments of global ranking and local ranking consistency. Thus, suspicious rankings can be flagged for a human analyst’s review. The algorithm also allows for a lot of missing data in the original pairwise ranking sets, making it widely applicable to the often-incomplete data on ranking problems in the real world (where users won’t click through three million video options to rank each relative to all of the others!). While a package does not exist in R and thus we will not walk through an example, implementations do exist in Matlab, and readers familiar with Matlab who are interested in this algorithm are encouraged to use the resources listed for HodgeRank.</p>
<h2 id="h1-503083c06-0005"><span epub:type="pagebreak" id="Page_153" title="153"/><a class="XrefDestination" id="Summary"/><span class="XrefDestination" id="xref-503083c06-010"/>Summary</h2>
<p class="BodyFirst">In this chapter, we learned about newer algorithms derived from differential and algebraic geometry and explored the use of both dgLARS and discrete exterior calculus on data analysis problems, including the Quora gifted sample, a credit-lending sample, and a disaster-planning scenario. Many more algorithms are being developed, and we’ve given an overview of how nonlinear algebra and Hodge theory have contributed to machine learning in recent years, impacting important types of industry problems (such as preference ranking and parametric model estimation).</p>
<p>In the next chapter, we’ll return to persistent homology and examine another tool of algebraic topology called the Mapper algorithm. Both of these will be used on our student sample introduced in this chapter.</p>
</section>
</body>
</html>