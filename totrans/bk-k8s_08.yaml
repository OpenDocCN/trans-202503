- en: '6'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: WHY KUBERNETES MATTERS
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Containers enable us to transform the way we package and deploy application
    components, but orchestration of containers in a cluster enables the real advantage
    of a containerized microservice architecture. As described in [Chapter 1](ch01.xhtml#ch01),
    the main benefits of modern application architecture are scalability, reliability,
    and resiliency, and all three of those benefits require a container orchestration
    environment like Kubernetes in order to run many instances of containerized application
    components across many different servers and networks.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll begin by looking at some cross-cutting concerns that
    exist when running containers across multiple servers in a cluster. We’ll then
    describe the core Kubernetes concepts designed to address those concerns. With
    that introduction complete, we’ll spend the bulk of the chapter actually installing
    a Kubernetes cluster, including important add-on components like networking and
    storage.
  prefs: []
  type: TYPE_NORMAL
- en: Running Containers in a Cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The need to distribute our application components across multiple servers is
    not new to modern application architecture. To build a scalable and reliable application,
    we have always needed to take advantage of multiple servers to handle the application’s
    load and preclude a single point of failure. The fact that we are now running
    these components in containers does not change the need for multiple servers;
    we are still ultimately using CPUs and we are still ultimately dependent on hardware.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, a container orchestration environment brings challenges that
    may not have existed with other kinds of application infrastructure. When the
    container is the smallest individual module around which we build our system,
    we end up with application components that are much more self-contained and “opaque”
    from the perspective of our infrastructure. This means that instead of having
    a static application architecture through which we choose in advance what application
    components are assigned to specific servers, with Kubernetes, we try to make it
    possible for any container to run anywhere.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Cutting Concerns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The ability to run any container anywhere maximizes our flexibility, but it
    adds complexity to Kubernetes itself. Kubernetes does not know in advance what
    containers it will be asked to run, and the container workload is continuously
    changing as new applications are deployed or applications experience changes in
    load. To rise to this challenge, Kubernetes needs to account for the following
    design parameters that apply to all container orchestration software, no matter
    what containers are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dynamic scheduling** New containers must be allocated to a server, and allocations
    can change due to configuration changes or failures.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed state** The entire cluster must keep information about what containers
    are running and where, even during hardware or network failures.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multitenancy** It should be possible to run multiple applications in a single
    cluster, with isolation for security and reliability.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware isolation** Clusters must run in cloud environments and on regular
    servers of various types, isolating containers from the differences in these environments.'
  prefs: []
  type: TYPE_NORMAL
- en: The best term to use to refer to these design parameters is *cross-cutting concern*,
    because they apply to any kind of containerized software that we might need to
    deploy, and even to the Kubernetes infrastructure itself. These parameters work
    together with the container orchestration requirements we saw in [Chapter 1](ch01.xhtml#ch01)
    and ultimately drive the Kubernetes architecture and key design decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Concepts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To address these cross-cutting concerns, the Kubernetes architecture allows
    anything to come and go at any time. This includes not only the containerized
    applications deployed to Kubernetes, but also the fundamental software components
    of Kubernetes itself, and even the underlying hardware such as servers, network
    connections, and storage.
  prefs: []
  type: TYPE_NORMAL
- en: Separate Control Plane
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Obviously, for Kubernetes to be a container orchestration environment, it requires
    the ability to run containers. This ability is provided by a set of worker machines
    called *nodes*. Each node runs a *kubelet* service that interfaces with the underlying
    container runtime to start and monitor containers.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes also has a set of core software components that manage the worker
    nodes and their containers, but these software components are deployed separately
    from the worker nodes. These core Kubernetes software components are together
    referred to as the *control plane*. Because the control plane is separate from
    the worker nodes, the worker nodes can run the control plane, gaining the benefits
    of containerization for the Kubernetes core software components. A separate control
    plane also means that Kubernetes itself has a microservice architecture, which
    allows customization of each Kubernetes cluster. For example, one control plane
    component, the *cloud controller manager*, is used only when deploying Kubernetes
    to a cloud provider, and it’s customized based on the cloud provider used. This
    design provides hardware isolation for application containers and the rest of
    the Kubernetes control plane, while still allowing us to take advantage of the
    specific features of each cloud provider.
  prefs: []
  type: TYPE_NORMAL
- en: Declarative API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: One critical component of the Kubernetes control plane is the *API server*.
    The API server provides an interface for cluster control and monitoring that other
    cluster users and control plane components use. In defining the API, Kubernetes
    could have chosen an *imperative* style, in which each API endpoint is a command
    such as “run a container” or “allocate storage.” Instead, the API is *declarative*,
    providing endpoints such as *create*, *patch*, *get*, and *delete*. The effect
    of these commands is to create, read, update, and delete *resources* from the
    cluster configuration—the specific configuration of each resource tells Kubernetes
    what we want the cluster to do.
  prefs: []
  type: TYPE_NORMAL
- en: This declarative API is essential to meet the cross-cutting concerns of dynamic
    scheduling and distributed state. Because a declarative API simply reports or
    updates cluster configuration, reacting to server or network failures that might
    cause a command to be missed is very easy. Consider an example in which the API
    server connection is lost just after an `apply` command is issued to change the
    cluster configuration. When the connection is restored, the client can simply
    query the cluster configuration and determine whether the command was received
    successfully. Or, even easier, the client can just issue the same `apply` command
    again, knowing that as long as the cluster configuration ends up as desired, Kubernetes
    will be trying to do the “right thing” to the actual cluster. This core principle
    is known as *idempotence*, meaning it is safe to issue the same command multiple
    times because it will be applied at most once.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Healing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Building on the declarative API, Kubernetes is designed to be *self-healing*.
    This means that the control plane components continually monitor both the cluster
    configuration and the actual cluster state and try to bring them into alignment.
    Every resource in the cluster configuration has an associated status and event
    log reflecting how the configuration has actually caused a change in the cluster
    state.
  prefs: []
  type: TYPE_NORMAL
- en: The separation of configuration and state makes Kubernetes very resilient. For
    example, a resource representing containers may be in a `Running` state if the
    containers have been scheduled and are actually running. If the Kubernetes control
    plane loses connection to the server on which the containers are running, it can
    immediately set the status to `Unknown` and then work to either reestablish connection
    or treat the node as failed and reschedule the containers.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, using a declarative API and self-healing approach has important
    implications. Because the Kubernetes API is declarative, a “success” response
    to a command means only that the cluster configuration was updated. It does not
    mean that the actual state of the cluster was updated, as it might take time to
    achieve the requested state, or there might be issues that prevent the cluster
    from achieving that state. As a result, we cannot assume that just because we
    created the appropriate resources, the cluster is running the containers we expect.
    Instead, we must watch the status of the resources and explore the event log to
    diagnose any issues that the Kubernetes control plane had in making the actual
    cluster state match the configuration we specified.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With some core Kubernetes concepts under our belts, we’ll use the `kubeadm`
    Kubernetes administration tool to deploy a highly available Kubernetes cluster
    across multiple virtual machines.
  prefs: []
  type: TYPE_NORMAL
- en: '**CHOOSING A KUBERNETES DISTRIBUTION**'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than using a particular Kubernetes distribution as we did in [Chapter
    1](ch01.xhtml#ch01), we’ll deploy a “vanilla” Kubernetes cluster using the generic
    upstream repository. This approach gives us the best opportunity to follow along
    with the cluster deployment and will make it easier to explore the cluster in-depth
    in the next several chapters. However, when you’re ready to deploy a Kubernetes
    cluster of your own, especially for production work, consider using a prebuilt
    Kubernetes distribution for ease of management and built-in security. The Cloud
    Native Computing Foundation (CNCF) publishes a set of conformance tests that you
    can use to ensure that the Kubernetes distribution you choose is conformant to
    the Kubernetes specification.
  prefs: []
  type: TYPE_NORMAL
- en: Our Kubernetes cluster will be split across four virtual machines, labeled `host01`
    through `host04`. Three of these, `host01` through `host03`, will run control
    plane components, whereas the fourth will act solely as a worker node. We’ll have
    three control plane nodes because that is the smallest number required to run
    a highly available cluster. Kubernetes uses a voting scheme to provide failover,
    and at least three control plane nodes are required; this allows the cluster to
    detect which side should keep running in the event of a network failure. Also,
    to keep the cluster as small as possible for our examples, we’ll configure Kubernetes
    to run regular containers on the control plane nodes even though we would avoid
    doing that for a production cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  prefs: []
  type: TYPE_NORMAL
- en: Start by following the instructions for this chapter to get all four virtual
    machines up and running, either in Vagrant or AWS. The automated provisioning
    will set up all four machines with `containerd` and `crictl`, so we don’t need
    to do it manually. The automated provisioning script will also set up either `kube-vip`
    or an AWS network load balancer to provide required high-availability functionality,
    as discussed below.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*You can install Kubernetes automatically using the* extra *provisioning script
    provided with this chapter’s examples. See the README file for this chapter for
    instructions.*'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need to run commands on each of the four virtual machines, so you might
    want to open terminal tabs for each one. However, the first series of commands
    needs to be run on all of the hosts, so the automation sets up a command called
    `k8s-all` to do that from `host01`. You can explore the content of this script
    in */usr/local/bin/k8s-all* or by looking at the *k8s* Ansible role in the *setup*
    directory of the examples.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisite Packages
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The first step is to make sure the `br_netfilter` kernel module is enabled
    and set to load on boot. Kubernetes uses advanced features of the Linux firewall
    to handle networking across the cluster, so we need this module. Run these two
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The first command ensures that the module is installed for the currently running
    kernel, and the second command adds it to the list of modules to run on boot.
    The slightly odd quoting in the second command ensures that the shell redirection
    happens on the remote hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, in [Listing 6-1](ch06.xhtml#ch06list1), we’ll set some Linux kernel parameters
    to enable advanced network features that are also needed for networking across
    the cluster by using the `sysctl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-1: Kernel settings*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This command enables the following Linux kernel network features:'
  prefs: []
  type: TYPE_NORMAL
- en: net.ipv4.ip_forward Transfer packets from one network interface to another (for
    example, from an interface inside a container’s network namespace to a host network).
  prefs: []
  type: TYPE_NORMAL
- en: net.bridge.bridge-nf-call-ip6tables Run IPv6 bridge traffic through the `iptables`
    firewall.
  prefs: []
  type: TYPE_NORMAL
- en: net.bridge.bridge-nf-call-iptables Run IPv4 bridge traffic through the `iptables`
    firewall.
  prefs: []
  type: TYPE_NORMAL
- en: The need for the last two items will become clear in [Chapter 9](ch09.xhtml#ch09)
    when we discuss how Kubernetes provides networking for Services.
  prefs: []
  type: TYPE_NORMAL
- en: These `sysctl` changes in [Listing 6-1](ch06.xhtml#ch06list1) do not persist
    after a reboot. The automated scripts do handle making the changes persistent,
    so if you reboot your virtual machines, either run the `extra` provisioning script,
    or run these commands again.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve now finished configuring the Linux kernel to support our Kubernetes deployment
    and are almost ready for the actual install. First we need to install some prerequisite
    packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `apt-transport-https` package ensures that `apt` can support connecting
    to repositories via secure HTTP. The other two packages are needed for one of
    the cluster add-ons that we’ll install after our cluster is up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Packages
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can now add the Kubernetes repository to install the `kubeadm` tool that
    will set up our cluster. First, add the GPG key used to check the package signatures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This command uses `curl` to download the GPG key. It then uses `gpg` to reformat
    it, and then it writes the result to */usr/share/keyrings*. The command line flags
    `fsSL` put `curl` in a mode that behaves better for chained commands, including
    avoiding unnecessary output, following server redirects, and terminating with
    an error if there is a problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we add the repository configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As before, the quoting is essential to ensure that the command is passed correctly
    via SSH to all the other hosts in the cluster. The command configures `kubernetes-xenial`
    as the distribution; this distribution is used for any version of Ubuntu, starting
    with the older Ubuntu Xenial.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have created this new repository, we then need to run `apt update`
    on all hosts to download the list of packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can install the packages we need using `apt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `source` command loads a file with a variable to install a specific Kubernetes
    version. This file is created by the automated scripts and ensures that we use
    a consistent Kubernetes version for all chapters. You can update the automated
    scripts to choose which Kubernetes version to install.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `apt` command installs the following three packages along with some dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: kubelet Service for all worker nodes that interfaces with the container engine
    to run containers as scheduled by the control plane
  prefs: []
  type: TYPE_NORMAL
- en: kubeadm Administration tool that we’ll use to install Kubernetes and maintain
    our cluster
  prefs: []
  type: TYPE_NORMAL
- en: kubectl Command line client that we’ll use to inspect our Kubernetes cluster
    and to create and delete resources
  prefs: []
  type: TYPE_NORMAL
- en: 'The `kubelet` package starts its service immediately, but because we haven’t
    installed the control plane yet, the service will be in a failed state at first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to control the version of the packages we just installed because we
    want to upgrade all of the components of our cluster together. To protect ourselves
    from accidentally updating these packages, we’ll hold them at their current version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This command prevents the standard `apt full-upgrade` command from updating
    these packages. Instead, if we upgrade our cluster, we’ll need to specify the
    exact version that we want by using `apt install`.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Initialization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The next command, `kubeadm init`, initializes the control plane and provides
    the `kubelet` worker node service configuration for all the nodes. We’ll run `kubeadm
    init` on one node in our cluster and then use `kubeadm join` on each of the other
    nodes so that they join the existing cluster.
  prefs: []
  type: TYPE_NORMAL
- en: To run `kubeadm init`, we first create a YAML configuration file. This approach
    has a few advantages. It greatly shortens the number of command line flags that
    we need to remember, and it lets us keep the cluster configuration in a repository,
    giving us configuration control over the cluster. We then can update the YAML
    file and rerun `kubeadm` to make cluster configuration changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The automation scripts for this chapter have populated a YAML configuration
    file in */etc/kubernetes*, so it’s ready to use. The following shows the contents
    of that file:'
  prefs: []
  type: TYPE_NORMAL
- en: '*kubeadm-init.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This YAML file has three documents, separated by dashes (`---`). The first
    document is specific to initializing the cluster, the second has more generic
    configuration, and the third is used to provide settings for `kubelet` across
    all the nodes. Let’s look at the purpose of each of these configuration items:'
  prefs: []
  type: TYPE_NORMAL
- en: apiVersion / kind Tells Kubernetes about the purpose of each YAML document,
    so it can validate the contents.
  prefs: []
  type: TYPE_NORMAL
- en: bootstrapTokens Configures a secret that other nodes can use to join the cluster.
    The `token` should be kept secret in a production cluster. It is set to expire
    automatically after two hours, so if we want to join more nodes later, we’ll need
    to make another one.
  prefs: []
  type: TYPE_NORMAL
- en: nodeRegistration Configuration to pass to the `kubelet` service running on `host01`.
    The `node-ip` field ensures that `kubelet` registers the correct IP address with
    the API server so that the API server can communicate with it. The `taints` field
    ensures that regular containers can be scheduled onto control plane nodes.
  prefs: []
  type: TYPE_NORMAL
- en: localAPIEndpoint The local IP address that the API server should use. Our virtual
    machine has multiple IP addresses, and we want the API server listening on the
    correct network.
  prefs: []
  type: TYPE_NORMAL
- en: certificateKey Configures a secret that other nodes will use to gain access
    to the certificates for the API server. It’s needed so that all of the API server
    instances in our highly available cluster can use the same certificate. Keep it
    secret in a production cluster.
  prefs: []
  type: TYPE_NORMAL
- en: networking All containers in the cluster will get an IP address from the `podSubnet`,
    no matter what host they run on. Later, we’ll install a network driver that will
    ensure that every container on all hosts in the cluster can communicate.
  prefs: []
  type: TYPE_NORMAL
- en: controlPlaneEndpoint The API server’s external address. For a highly available
    cluster, this IP address needs to reach any API server instance, not just the
    first one.
  prefs: []
  type: TYPE_NORMAL
- en: serverTLSBootstrap Instructs `kubelet` to use the controller manager’s certificate
    authority to request server certificates.
  prefs: []
  type: TYPE_NORMAL
- en: The `apiVersion` and `kind` fields will appear in every Kubernetes YAML file.
    The `apiVersion` field defines a group of related Kubernetes resources, including
    a version number. The `kind` field then selects the specific resource type within
    that group. This not only allows the Kubernetes project and other vendors to add
    new groups of resources over time, but it also allows updates to existing resource
    specifications while maintaining backward compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: '**HIGHLY AVAILABLE CLUSTERS**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `controlPlaneEndpoint` field is used to configure the most important requirement
    for a highly available cluster: an IP address that reaches all of the API servers.
    We need to establish this IP address immediately when we initialize the cluster
    because it is used to generate certificates with which clients will verify the
    API server’s identity. The best way to provide a cluster-wide IP address depends
    on where the cluster is running; for example, in a cloud environment, using the
    provider’s built-in capability, such as an Elastic Load Balancer (ELB) in Amazon
    Web Services or an Azure Load Balancer, is best.'
  prefs: []
  type: TYPE_NORMAL
- en: Because of the nature of the two different environments, the examples for this
    book use `kube-vip` when running with Vagrant, and ELB when running in Amazon
    Web Services. The top-level *README.md* file in the example documentation has
    more details. The installation and configuration is done automatically so there’s
    nothing more to configure. We can just use `192.168.61.10:6443` and expect traffic
    to get to any of the API server instances running on `host01` through `host03`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we have the cluster configuration ready to go in a YAML file, the `kubeadm
    init` command to initialize the cluster is simple. We run this command solely
    on `host01`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `--config` option points to the YAML configuration file (*kubeadm-init.yaml*)
    that we looked at earlier, and the `--upload-certs` option tells `kubeadm` that
    it should upload the API server’s certificates to the cluster’s distributed storage.
    The other control plane nodes then can download those certificates when they join
    the cluster, allowing all API server instances to use the same certificates so
    that clients will trust them. The certificates are encrypted using the `certificateKey`
    we provided, which means that the other nodes will need this key to decrypt them.
  prefs: []
  type: TYPE_NORMAL
- en: The `kubeadm init` command initializes the control plane’s components on `host01`.
    These components are run in containers and managed by the `kubelet` service, which
    makes them easy to upgrade. Several container images will be downloaded, so the
    command might take a while, depending on the speed of your virtual machines and
    your internet connection.
  prefs: []
  type: TYPE_NORMAL
- en: Joining Nodes to the Cluster
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `kubeadm init` command prints out a `kubeadm join` command that we can use
    to join other nodes to the cluster. However, the automation scripts have already
    prestaged a configuration file to each of the other nodes to ensure that they
    join as the correct type of node. The servers `host02` and `host03` will join
    as additional control plane nodes, whereas `host04` will join solely as a worker
    node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the YAML configuration file for `host02` with its specific settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '*kubeadm-join.yaml (host02)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This resource has a type of `JoinConfiguration`, but most of the fields are
    the same as the `InitConfiguration` in the *kubeadm-init.yaml* file. Most important,
    the `token` and `certificateKey` match the secret we set up earlier, so this node
    will be able to validate itself with the cluster and decrypt the API server certificates.
  prefs: []
  type: TYPE_NORMAL
- en: One difference is the addition of `ignorePreflightErrors`. This section appears
    only when we are installing `kube-vip`, as in that case we need to prestage the
    configuration file for `kube-vip` to the */etc/kubernetes/manifests* directory,
    and we need to tell `kubeadm` that it is okay for that directory to already exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we have this YAML configuration file, the `kubeadm join` command is
    simple. Run it on `host02`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As before, this command runs the control plane components as containers using
    the `kubelet` service on this node, so it will take some time to download the
    container images and start the containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it finishes, run the exact same command on `host03`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The automation script set up the YAML file with the correct IP address for each
    host, so the differences in configuration between each of the hosts is already
    accounted for.
  prefs: []
  type: TYPE_NORMAL
- en: When this command completes, we’ll have created a highly available Kubernetes
    cluster, with the control plane components running on three separate hosts. However,
    we do not yet have any regular worker nodes. Let’s fix that issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll begin by joining `host04` as a regular worker node and running exactly
    the same `kubeadm join` command on `host04`, but the YAML configuration file will
    be a little different. Here’s that file:'
  prefs: []
  type: TYPE_NORMAL
- en: '*kubeadm-join.yaml (host04)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This YAML file is missing the `controlPlane` field, so `kubeadm` configures
    it as a regular worker node rather than a control plane node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s join `host04` to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This command completes a little faster because it doesn’t need to download
    the control plane container images and run them. We now have four nodes in the
    cluster, which we can verify by running `kubectl` back on `host01`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The first command sets an environment variable to tell `kubectl` what configuration
    file to use. The */etc/kubernetes/admin.conf* file was created automatically by
    `kubeadm` when it initialized `host01` as a control plane node. That file tells
    `kubectl` what address to use for the API server, what certificate to use to verify
    the secure connection, and how to authenticate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The four nodes currently should be reporting a status of `NotReady`. Let’s
    run the `kubectl describe` command to get the node details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We haven’t yet installed a network driver for our Kubernetes cluster, and as
    a result, all of the nodes are reporting a status of `NotReady`, which means that
    they won’t accept regular application workloads. Kubernetes communicates this
    by placing a *taint* in the node’s configuration. A taint restricts what can be
    scheduled on a node. We can list the taints on the nodes using `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We select an output format of `json` so that we can use `jq` to print just the
    information we need. Because all the nodes have a status of `NotReady`, they have
    a `not-ready` taint set to `NoSchedule`, which prevents the Kubernetes scheduler
    from scheduling containers onto them.
  prefs: []
  type: TYPE_NORMAL
- en: By specifying `taints` as an empty array in the `kubeadm` configuration, we
    prevented the three control plane nodes from having an additional control plane
    taint. In a production cluster, this taint keeps application containers separate
    from the control plane containers for security reasons, so we would leave it in
    place. For our example cluster, though, it would mean that we need multiple extra
    virtual machines to act as worker nodes, which we don’t want.
  prefs: []
  type: TYPE_NORMAL
- en: The command `kubectl taint` would allow us to remove the `not-ready` taint manually,
    but the correct approach is to install a network driver as a cluster add-on so
    that the nodes will properly report `Ready`, enabling us to run containers on
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Cluster Add-ons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve installed `kubelet` on four separate nodes and installed the control plane
    on three of those nodes and joined them to our cluster. For the rest, we’ll use
    the control plane to install cluster add-ons. These add-ons are similar to regular
    applications that we would deploy. They consist of Kubernetes resources and run
    in containers, but they provide essential services to the cluster that our applications
    will use.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a basic cluster up and running, we need to install three types of add-ons:
    a *network driver*, a *storage driver*, and an *ingress controller*. We will also
    install a fourth optional add-on, a *metrics server*.'
  prefs: []
  type: TYPE_NORMAL
- en: Network Driver
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Kubernetes networking is based on the Container Network Interface (CNI) standard.
    Anyone can build a new network driver for Kubernetes by implementing this standard,
    and as a result, several choices are available for Kubernetes network drivers.
    We’ll demonstrate different network plug-ins in [Chapter 8](ch08.xhtml#ch08),
    but most of the clusters in this book use the Calico network driver because it
    is the default choice for many Kubernetes platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, download the primary YAML configuration file for Calico:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `-L` option tells `curl` to follow any HTTP redirects, whereas the `-O`
    option tells `curl` to save the content in a file using the same filename as in
    the URL. The value of the `calico_url` environment variable is set in the `k8s-ver`
    script that also specified the Kubernetes version. This is essential, as Calico
    is sensitive to the specific version of Kubernetes we’re running, so it’s important
    to choose values that are compatible.
  prefs: []
  type: TYPE_NORMAL
- en: The primary YAML configuration is written to the local file *tigera-operator.yaml*.
    This refers to the fact that the initial installation is a Kubernetes Operator,
    which then creates all of the other cluster resources to install Calico. We’ll
    explore operators in [Chapter 17](ch17.xhtml#ch17).
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to this primary YAML configuration, the automated scripts for this
    chapter have added a file called *custom-resources.yaml* that provides necessary
    configuration for our example cluster. We now can tell the Kubernetes API server
    to apply all the resources in these files to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Kubernetes takes a few minutes to download container images and start containers,
    and then Calico will be running in our cluster and our nodes should report a status
    of `Ready`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Calico works by installing a *DaemonSet*, a Kubernetes resource that tells the
    cluster to run a specific container or set of containers on every node. The Calico
    containers then provide network services for any containers running on that node.
    However, that raises an important question. When we installed Calico in our cluster,
    all of our nodes had a taint that told Kubernetes not to schedule containers on
    them. How was Calico able to run its containers on all the nodes? The answer is
    *tolerations*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A toleration is a configuration setting applied to a resource that instructs
    Kubernetes it can be scheduled on a node despite a taint possibly being present.
    Calico specifies a toleration when it adds its DaemonSet to the cluster, as we
    can see with `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `-n` option selects the `calico-system` *Namespace*. Namespaces are a way
    to keep Kubernetes resources separate from one another on a cluster, for security
    reasons as well as to avoid naming collisions. Also, as before, we request JSON
    output and use `jq` to select only the field we’re interested in. If you want
    to see the entire configuration for the resource, use `-o=json` without `jq` or
    use `-o=yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: This DaemonSet has three tolerations, and the second one provides the behavior
    we need. It tells the Kubernetes scheduler to go ahead and schedule it even on
    nodes that have a `NoSchedule` taint. Calico then can get itself started before
    the node is ready, and once it’s running, the node changes its status to `Ready`
    so that normal application containers can be scheduled. The control plane components
    needed a similar toleration in order to run on nodes before they show `Ready`.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Storage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The cluster nodes are ready, so if we deployed a regular application, its containers
    would run. However, applications that require persistent storage would fail to
    start because the cluster doesn’t yet have a storage driver. Like network drivers,
    several storage drivers are available for Kubernetes. The Container Storage Interface
    (CSI) provides the standard that storage drivers need to meet to work with Kubernetes.
    We’ll use Longhorn, a storage driver from Rancher; it’s easy to install and doesn’t
    require any underlying hardware like extra block devices or access to cloud-based
    storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Longhorn makes use of the iSCSI and NFS software we installed earlier. It expects
    all of our nodes to have the `iscsid` service enabled and running, so let’s make
    sure that’s true on all our nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We now can install Longhorn on the cluster. The process for installing Longhorn
    looks a lot like Calico. Start by downloading the Longhorn YAML configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `longhorn_url` environment variable is also set by the `k8s-ver` script,
    which allows us to ensure compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Longhorn using `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As before, `kubectl apply` ensures that the resources in the YAML file are applied
    to the cluster, creating or updating them as necessary. The `kubectl apply` command
    supports URLs as the source of the resource it applies to the cluster, but for
    these three installs, we run a separate `curl` command because it’s convenient
    to have a local copy of what was applied to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Longhorn is now installed on the cluster, which we’ll verify as we explore the
    cluster in the rest of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress Controller
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We now have networking and storage, but the networking allows access to containers
    only from within our cluster. We need another service that exposes our containerized
    applications outside the cluster. The easiest way to do that is to use an ingress
    controller. As we’ll describe in [Chapter 9](ch09.xhtml#ch09), an ingress controller
    watches the Kubernetes cluster for *Ingress* resources and routes network traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by downloading the ingress controller YAML configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As in our earlier example, the `ingress_url` environment variable is set by
    the `k8s-ver` script so that we can ensure compatibility. In this case, the URL
    ends in the generic path of *deploy.yaml*, so we use `-o` to provide a filename
    to `curl` to make clear the purpose of the downloaded YAML file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the ingress controller using `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates a lot of resources, but there are two main parts: an NGINX web
    server that actually performs routing of HTTP traffic, and a component that watches
    for changes in Ingress resources in the cluster and configures NGINX accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s one more step we need. As installed, the ingress controller tries to
    request an external IP address to allow traffic to reach it from outside the cluster.
    Because we’re running a sample cluster with no access to external IP addresses,
    this won’t work. Instead, we’ll be accessing our ingress controller using port
    forwarding from our cluster hosts. At the moment, our ingress controller is set
    up for this port forwarding, but it’s using a random port. We would like to select
    the port to be sure that we know where to find the ingress controller. At the
    same time, we’ll also add an annotation so that this ingress controller will be
    the default for this cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply the port changes, we’re going to provide our Kubernetes cluster an
    with extra YAML configuration with just the changes we need. Here’s that YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ingress-patch.yaml*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This file specifies the name and Namespace of the Service to ensure that Kubernetes
    knows where to apply these changes. It also specifies the `port` configuration
    we’re updating, along with the `nodePort`, which is the port on our cluster nodes
    that will be used for port forwarding. We’ll look at NodePort service types and
    port forwarding in more detail in [Chapter 9](ch09.xhtml#ch09).
  prefs: []
  type: TYPE_NORMAL
- en: 'To patch the service, we use the `kubectl patch` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To apply the annotation, use the `kubectl annotate` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Kubernetes reports the change to each resource as we make it, so we know that
    our changes have been applied.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics Server
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our final add-on is a *metrics server* that collects utilization metrics from
    our nodes, enabling the use of autoscaling. To do this, it needs to connect to
    the `kubelet` instances in our cluster. For security, it needs to verify the HTTP/S
    certificate when it connects to a `kubelet`. This is why we configured `kubelet`
    to request a certificate signed by the controller manager rather than allowing
    the `kubelet` to generate self-signed certificates.
  prefs: []
  type: TYPE_NORMAL
- en: 'During setup, `kubelet` created a certificate request on each node, but the
    requests were not automatically approved. Let’s find these requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Each `kubelet` has a client certificate that it uses to authenticate to the
    API server; these were automatically approved during bootstrap. The requests we
    need to approve are for `kubelet-serving` certificates, which are used when clients
    such as our metrics server connect to `kubelet`. As soon as the request is approved,
    the controller manager signs the certificate. The `kubelet` then collects the
    certificate and starts using it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can approve all of these requests at once by querying for the name of all
    of the `kubelet-serving` requests and then passing those names to `kubectl certificate
    approve`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We now can install our metrics server by downloading and applying its YAML
    configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This component is the last one we need to install, so we can leave this directory.
    With these cluster add-ons, we now have a complete, highly available Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring a Cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before deploying our first application onto this brand-new Kubernetes cluster,
    let’s explore what’s running on it. The commands we use here will come in handy
    later as we debug our own applications and a cluster that isn’t working correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use `crictl`, the same command we used to explore running containers
    in [Part I](part01.xhtml#part01), to see what containers are running on `host01`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The control plane node is very busy, as this list includes Kubernetes control
    plane components, Calico components, and Longhorn components. Running this command
    on all the nodes and sorting out what containers are running where and for what
    purpose would be confusing. Fortunately, `kubectl` provides a clearer picture,
    although knowing that we can get down to these lower-level details and see exactly
    what containers are running on a given node is nice.
  prefs: []
  type: TYPE_NORMAL
- en: To explore the cluster with `kubectl`, we need to know how the cluster resources
    are organized into Namespaces. As mentioned previously, Kubernetes Namespaces
    provide security and avoid name collisions. To ensure idempotence, Kubernetes
    needs each resource to have a unique name. By dividing resources into Namespaces,
    we allow multiple resources to have the same name while still enabling the API
    server to know exactly which resource we mean, which also supports multitenancy,
    one of our cross-cutting concerns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though we just set up the cluster, it’s already populated with several
    Namespaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As we run `kubectl` commands, they will apply to the `default` Namespace unless
    we use the `-n` option to specify a different Namespace.
  prefs: []
  type: TYPE_NORMAL
- en: To see what containers are running, we ask `kubectl` to get the list of Pods.
    We look at Kubernetes Pods in much more detail in [Chapter 7](ch07.xhtml#ch07).
    For now, just know that a Pod is a group of one or more containers, much like
    the Pods that we created with `crictl` in [Part I](part01.xhtml#part01).
  prefs: []
  type: TYPE_NORMAL
- en: 'If we try to list Pods in the `default` Namespace, we can see that there aren’t
    any yet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, as we installed cluster infrastructure components, they’ve been created
    in other Namespaces. That way, when we configure normal user accounts, we can
    prevent those users from viewing or editing the cluster infrastructure. The Kubernetes
    infrastructure components were all installed into the `kube-system` Namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We cover the control plane components in [Chapter 11](ch11.xhtml#ch11). For
    now, let’s explore just one of the control plane Pods, the API server running
    on `host01`. We can get all of the details for this Pod using `kubectl describe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The Namespace and name together uniquely identify this Pod. We also see the
    node on which the Pod is scheduled, its status, and details about the actual containers,
    including a container ID that we can use with `crictl` to find the container in
    the underlying `containerd` runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also verify that Calico deployed into our cluster as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Earlier we saw that Calico installed a DaemonSet resource. Kubernetes has used
    the configuration in this DaemonSet to automatically create a `calico-node` Pod
    for each node. Like Kubernetes itself, Calico also uses a separate control plane
    to handle overall configuration of the network, and the other Pods provide that
    control plane.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we’ll see the containers that are running for Longhorn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Like Calico, Longhorn uses DaemonSets so that it can run containers on every
    node. These containers provide storage services to the other containers on the
    node. Longhorn also includes a number of other containers that serve as a control
    plane, including providing the CSI implementation that Kubernetes uses to tell
    Longhorn to create storage when needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We put a lot of effort into setting up this cluster, so it would be a shame
    to end this chapter without running at least one application on it. In the next
    chapter, we will look at many different ways to run containers, but let’s quickly
    run a simple NGINX web server in our Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'That may look like an imperative command, but under the hood, `kubectl` is
    creating a Pod resource using the name and container image we specified, and then
    it’s applying that resource on the cluster. Let’s inspect the default Namespace
    again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: We used `-o wide` to see extra information about the Pod, including its IP address
    and where it was scheduled, which can be different each time the Pod is created.
    In this case, the Pod was scheduled to `host02`, showing that we were successful
    in allowing regular application containers to be deployed to our control plane
    nodes. The IP address comes from the Pod CIDR we configured, and Calico automatically
    assigns it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calico also handles routing traffic so that we can reach the Pod from any container
    in the cluster as well as from the host network. Let’s verify that, starting with
    a regular `ping`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Use your Pod’s IP address in the place of the one shown here.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use `curl` to verify that the NGINX web server is working:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The Kubernetes cluster is working and ready for us to deploy applications. Kubernetes
    will take advantage of all of the nodes in the cluster to load balance our applications
    and provide resiliency in the event of any failures.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we’ve explored how Kubernetes is architected with the flexibility
    to allow cluster components to come and go at any time. This applies not only
    to containerized applications but also to the cluster components, including control
    plane microservices and the underlying servers and networks the cluster uses.
    We were able to bootstrap a cluster and then dynamically add nodes to it, configure
    those nodes to accept certain types of containers, and then dynamically add networking
    and storage drivers using the Kubernetes cluster itself to run and monitor them.
    Finally, we deployed our first container to a Kubernetes cluster, allowing it
    to automatically schedule the container onto an available node, using our network
    driver to access the container from the host network.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a highly available cluster, we can look at how to deploy an
    application to Kubernetes. We’ll explore some key Kubernetes resources that we
    need to create a scalable, reliable application. This process will provide a foundation
    for exploring Kubernetes in detail, including understanding what happens when
    our applications don’t run as expected and how to debug issues with our application
    or the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
