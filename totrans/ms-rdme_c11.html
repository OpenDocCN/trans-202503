<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" epub:prefix="index: http://www.index.com/" lang="en" xml:lang="en">
	<head>
		<title>Chapter 11: Creating Evolvable Architectures</title>
		<link href="NSTemplate_v1.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:7db14923-61d0-434f-baa0-3e20bf74259e" name="Adept.expected.resource"/>
	</head>
	<body epub:type="bodymatter chapter">
		<section>
			<header>
				<h1 class="chapter"><span class="ChapterNumber"><span epub:type="pagebreak" id="Page_193" title="193"/>11</span><br/><span class="ChapterTitle">Creating Evolvable Architectures</span></h1>
			</header>
			<p class="BodyFirst"><span class="DropCap">R</span>equirements volatility—changing customer demands—is an unavoidable challenge for software projects. Product requirements and context will change over time; your application must change as well. But changing requirements can cause instability and derail development.</p>
			<p>
				Managers try to deal with requirements volatility using iterative development processes like Agile development (discussed in the next chapter). You can do your part to accommodate changing requirements by building <em>evolvable architectures</em>. Evolvable architectures eschew complexity, the enemy of evolvability.</p>
			<p>This chapter will teach you techniques that can make your software simpler and thus easier to evolve. Paradoxically, achieving simplicity in software can be difficult; without conscious effort, code will grow tangled and complex. We’ll begin by describing complexity and how it leads to rigid and confusing codebases. We’ll then show you design principles that reduce complexity. Finally, we’ll translate these design principles into concrete API and data layer best practices.</p>
			<h2 id="h1-501836c11-0001"><span epub:type="pagebreak" id="Page_194" title="194"/>Understanding Complexity</h2>
			<p class="BodyFirst">In <em>A Philosophy of Software Design</em> (Yaknyam Press, 2018), Stanford computer science professor John Ousterhout writes, “Complexity is anything related to the structure of a system that makes it hard to understand and modify the system.” Per Ousterhout, complex systems have two characteristics: high <em>dependency</em> and high <em>obscurity</em>. We add a third: high <em>inertia</em>.</p>
			<p>
				High <em>dependency</em> leads software to rely on other code’s API or behavior. Dependency is obviously unavoidable and even desirable, but a balance must be struck. Every new connection and assumption makes code harder to change. High-dependency systems are hard to modify because they have <em>tight coupling</em> and high <em>change amplification</em>. Tight coupling describes modules that depend heavily on one another. It leads to high change amplification, where a single change requires modifications in dependencies as well. Thoughtful API design and a restrained use of abstraction will minimize tight coupling and change amplification.</p>
			<p>
				High <em>obscurity</em> makes it difficult for programmers to predict a change’s side effects, how code behaves, and where changes need to be made. Obscure code takes longer to learn, and developers are more likely to inadvertently break things. <em>God objects</em><em> </em>that “know” too much, global state that encourages side effects, excessive indirection that obscures code, and <em>action at distance</em> that affects behavior in distant parts of the program are all symptoms of high obscurity. APIs with clear contracts and standard patterns reduce obscurity.</p>
			<p><em>Inertia</em>, the characteristic that we’ve added to Ousterhout’s list, is software’s tendency to stay in use. Easily discarded code used for a quick experiment has low inertia. A service that powers a dozen business-critical applications has high inertia. Complexity’s cost accrues over time, so high-inertia, high-change systems should be simplified, while low-inertia or low-change systems can be left complex (as long as you discard them or continue to leave them alone).</p>
			<p><span epub:type="pagebreak" id="Page_195" title="195"/>Complexity cannot always be eliminated, but you can choose where to put it. Backward-compatible changes (discussed later) might make code simpler to use but more complicated to implement. Layers of indirection to decouple subsystems reduce dependency but increase obscurity. Be thoughtful about when, where, and how to manage complexity.</p>
			<h2 id="h1-501836c11-0002">Design for Evolvability</h2>
			<p class="BodyFirst">Faced with unknown future requirements, engineers usually choose one of two tactics: they try to guess what they’ll need in the future, or they build abstractions as an escape hatch to make subsequent code changes easier. Don’t play this game; both approaches lead to complexity. Keep things simple (known as KISS—<em>keep it simple, stupid</em>—thanks to the US Navy’s penchant for acronyms and tough love). Use the KISS mnemonic to remember to build with simplicity in mind. Simple code lets you add complexity later, when the need becomes clear and the change becomes unavoidable.</p>
			<p>
				The easiest way to keep code simple is to avoid writing it altogether. Tell yourself that <em>you ain’t gonna need it</em><em> (YAGNI</em><em>)</em>. When you do write code, use the principle of least astonishment and encapsulation. These design principles will keep your code easy to evolve.</p>
			<h3 id="h2-501836c11-0001">You Ain’t Gonna Need It</h3>
			<p class="BodyFirst">YAGNI is a deceptively simple design practice: don’t build things you don’t need. YAGNI violations happen when developers get excited, fixated, or worried about some aspect of their code. It’s difficult to predict what you’ll need and what you won’t. Every wrong guess is wasted effort. After the initial waste, the code continues to bog things down, it needs to be maintained, developers need to understand it, and it must be built and tested.</p>
			<p>
				Luckily, there are a few habits you can build to avoid unnecessary development. Avoid premature optimization, unnecessarily flexible abstractions, and product features that aren’t needed for a <em>minimum <span epub:type="pagebreak" id="Page_196" title="196"/>viable product</em><em> (MVP</em><em>)</em>—the bare-minimum feature set that you need to get user feedback.</p>
			<p><em>Premature optimization</em> occurs when a developer adds a performance optimization to code before it’s proven to be needed. In the classic scenario, a developer sees an area of code that could be made faster or more scalable by adding complex logic and architectural layers such as caches, sharded databases, or queues. The developer optimizes the code before it’s been shipped, before anyone has used it. After shipping the code, the developer discovers that the optimization was not needed. Removing optimization never happens, and complexity accrues. Most performance and scalability improvements come with a high complexity cost. For example, a cache is fast, but it can also become inconsistent with underlying data.</p>
			<p>Flexible abstractions—plugin architectures, wrapper interfaces, and generic data structures like key-value pairs—are another temptation. Developers think they can easily adjust if some new requirement pops up. But abstraction comes with a cost; it boxes implementations into rigid boundaries that the developer ends up fighting later. Flexibility also makes code harder to read and understand. Take this distributed queuing interface:</p>
			<pre><code>interface IDistributedQueue { void send<em>(String queue, Object message)</em>; Object receive<em>(String queue)</em>;
}</code></pre>
			<p><code>IDistributedQueue</code> looks simple enough: you send and receive messages. But what if the underlying queue supports both keys and values for a message (as Apache Kafka does) or message ACKing (as Amazon’s Simple Queue Service does)? Developers are faced with a choice: Should the interface be the union of all features from all message queues or the intersection of all features? The union of all features produces an interface where no single implementation works for all methods. The <span epub:type="pagebreak" id="Page_197" title="197"/>intersection of all features leads to a limited interface that doesn’t have enough features to be useful. You’re better off directly using an implementation. You can refactor later if you decide you need to support another implementation.</p>
			<p>
				The best way to keep your code flexible is to simply have less of it. For everything you build, ask yourself what is absolutely necessary, and throw away the rest. This technique—called <em>Muntzing</em>—will keep your software trim and adaptable.</p>
			<p>Adding cool new product features is also tempting. Developers talk themselves into the cool-feature pitfall for a variety of reasons: they mistake their usage for what most users want, they think it’ll be easy to add, or they think it’ll be neat! Each new feature takes time to build and maintain, and you don’t know if the feature will actually be useful.</p>
			<p>Building an MVP will keep you honest about what you really need. MVPs allow you to test an idea without investing in a full-fledged implementation.</p>
			<p>There are, of course, caveats to YAGNI. As your experience grows, you’ll get better at predicting when flexibility and optimization are necessary. In the meantime, place interface shims where you suspect optimizations can be inserted, but don’t actually implement them. For example, if you are creating a new file format and suspect you’ll need compression or encryption later, provide a header that specifies the encoding, but only implement the uncompressed encoding. You can add compression in the future, and the header will make it easy for new code to read older files.</p>
			<h3 id="h2-501836c11-0002">Principle of Least Astonishment</h3>
			<p class="BodyFirst">The <em>principle of least astonishment</em> is pretty clear: don’t surprise users. Build features that behave as users first expect. Features with a high learning curve or strange behavior frustrate users. Similarly, don’t surprise developers. Surprising code is obscure, which causes complexity. You can eliminate surprises by keeping code specific, avoiding implicit knowledge, and using standard libraries and patterns.</p>
			<p><span epub:type="pagebreak" id="Page_198" title="198"/>Anything nonobvious that a developer needs to know to use an API and is not part of the API itself is considered <em>implicit knowledge</em>. APIs that require implicit knowledge will surprise developers, causing bugs and a high learning curve. Two common implicit knowledge violations are hidden ordering requirements and hidden argument requirements.</p>
			<p><em>Ordering requirements</em> dictate that actions take place in a specific sequence. Method ordering is a frequent violation: method A must be called before method B, but the API allows method B to be called first, surprising the developer with a runtime error. Documenting an ordering requirement is good, but it’s better not to have one in the first place. Avoid method ordering by having methods invoke submethods:</p>
			<pre><code>pontoonWorples() { if(!flubberized) { flubberize() } // ...
}</code></pre>
			<p>
				There are other approaches to avoiding ordering: combining the methods into one, using the <em>builder pattern</em>, using the type system and having <code>pontoonWorples</code> work only on <code>FlubberizedWorples</code> rather than all <code>Worples</code>, and so on. All of these are better than requiring your user to know about hidden requirements. If nothing else, you can at least make the method name give developers a heads-up by calling it <code>pontoonFlubberizedWorples()</code>. Counterintuitively, short method and variable names actually increase cognitive load. Specific, longer method names are more descriptive and easier to understand.</p>
			<p><em>Hidden argument requirements</em> occur when a method signature implies a wider range of valid inputs than the method actually accepts. For example, accepting an <code>int</code> while only allowing numbers 1 to 10 is a hidden constraint. Requiring that a certain value field is set in a plain JSON object is also requiring implicit knowledge on the part of the user. Make argument <span epub:type="pagebreak" id="Page_199" title="199"/>requirements specific and visible. Use specific types that accurately capture your constraints; when using flexible types like JSON, consider using JSON Schema to describe the expected object. At the least, advertise argument requirements in documentation when they can’t be made programmatically visible.</p>
			<p>
				Finally, use standard libraries and development patterns. Implementing your own square root method is surprising; using a language’s built-in <code>sqrt()</code> method is not. The same rule applies for development patterns: use idiomatic code style and development patterns.</p>
			<h3 id="h2-501836c11-0003">Encapsulate Domain Knowledge</h3>
			<p class="BodyFirst">Software changes as business requirements change. Encapsulate domain knowledge by grouping software based on business domain—accounting, billing, shipping, and so on. Mapping software components to business domains will keep code changes focused and clean.</p>
			<p>
				Encapsulated domains naturally gravitate toward <em>high cohesion</em> and <em>low coupling</em>—desirable traits. Highly cohesive software with low coupling is more evolvable because changes tend to have a smaller “blast radius.” Code is highly <em>cohesive</em> when methods, variables, and classes that relate to one another are near each other in modules or packages. <em>Decoupled</em> code is self-contained; a change to its logic does not require changes to other software components.</p>
			<p>Developers often think about software in terms of layers: frontend, middle tier, and backend. Layered code is grouped according to technical domain, with all the UI code in one place and all object persistence in another. Grouping code by technical domain works great within a single business domain but grows messy as businesses grow. Separate teams form around each tier, increasing coordination cost since every business logic change slices through all tiers. And shared horizontal layers make it too easy for developers to mix business logic between domains, leading to complex code.</p>
			<p><span epub:type="pagebreak" id="Page_200" title="200"/>Identifying domain boundaries and encapsulating domain knowledge is as much art as science. There is an entire architectural approach called <em>Domain-Driven Design</em><em> (DDD</em><em>)</em>, which defines an extensive set of concepts and practices to map business concepts to software. Full-blown DDD is necessary only for the most complex situations. Still, familiarizing yourself with DDD will help you make better design decisions.</p>
			<h2 id="h1-501836c11-0003">Evolvable APIs</h2>
			<p class="BodyFirst">As requirements change, you’ll need to change your APIs, the shared interfaces between code. Changing an API is easy to do, but it’s hard to do right. Many small, rational changes can lead to a sprawling mess. Worse, a minor API change can completely break compatibility. If an API changes in an incompatible way, clients will break; these breakages may not be immediately obvious, especially changes that break at runtime, not during compilation. APIs that are small, clearly defined, compatible, and versioned will be easier to use and evolve.</p>
			<h3 id="h2-501836c11-0004">Keep APIs Small</h3>
			<p class="BodyFirst">Small APIs are easier to understand and evolve. Larger APIs put more cognitive load on developers, and you’ll have more code to document, support, debug, and maintain. Every new method or field grows the API and locks you further into a specific usage pattern.</p>
			<p>Apply the YAGNI philosophy: only add API methods or fields that are immediately needed. When creating an API data model, only add methods you need at the time. When bootstrapping your API using a framework or generator tool, eliminate fields or methods you’re not using.</p>
			<p>API methods with a lot of fields should have sensible defaults. Developers can focus on relevant fields knowing they’ll inherit defaults for the others. Defaults make large APIs feel small.</p>
			<h3 id="h2-501836c11-0005"><span epub:type="pagebreak" id="Page_201" title="201"/>Expose Well-Defined Service APIs</h3>
			<p class="BodyFirst">Evolvable systems have clearly defined request and response schemas that are versioned and have clear compatibility contracts. Schema definitions should be published so they can be used to automatically test both client and server code (see “Package Different Resources Separately” in Chapter 8 for more).</p>
			<p>
				Use standard tools to define service APIs. A well-defined service will declare its schemas, request and response methods, and exceptions. OpenAPI is commonly used for RESTful services, while non-REST services use Protocol Buffers, Thrift, or a similar <em>interface definition language</em><em> (IDL</em><em>)</em>. Well-defined service APIs make compile-time validation easier and keep clients, servers, and documentation in sync.</p>
			<p>Interface definition tools come with code generators that convert service definitions to client and server code. Documentation can also be generated, and test tools can use IDLs to generate stubs and mock data. Some tools even have discoverability features to find services, learn who maintains them, and show how the service is used.</p>
			<p>Use your company’s API definition framework if they’ve already chosen one; choosing a “better” one will require too much interoperability work. If your company still hand-rolls REST APIs and the JSON interfaces are evolved in code without a formal spec, your best bet is OpenAPI, as it can be retrofitted on preexisting REST services and does not require major migrations to adopt.</p>
			<h3 id="h2-501836c11-0006">Keep API Changes Compatible</h3>
			<p class="BodyFirst">Keeping API changes compatible lets client and server versions evolve independently. There are two forms of compatibility to consider: forward and backward.</p>
			<p><em>Forward-compatible</em> changes allow clients to use a new version of an API when invoking an older service version. A web service that’s running version 1.0 of an API but can receive calls from a client using version 1.1 of the API is forward compatible.</p>
			<p><span epub:type="pagebreak" id="Page_202" title="202"/><em>Backward-compatible</em> changes are the opposite: new versions of the library or service do not require changes in older client code. A change is backward compatible if code developed against version 1.0 of an API continues to compile and run when used with version 1.1.</p>
			<p>Let’s take a look at a simple gRPC Hello World service API defined with Protocol Buffers:</p>
			<pre><code>service Greeter { rpc SayHello (HelloRequest) returns (HelloReply) {}
}
message HelloRequest { string name = 1; int32 favorite_number = 2;
}
message HelloReply { string message = 1;
}</code></pre>
			<p>
				The <code>Greeter</code> service has one method, called <code>SayHello</code>, which receives a <code>HelloRequest</code> and returns a <code>HelloReply</code> with a fun message about <code>favorite_number</code>. The numbers next to each field are <em>field ordinals</em>; Protocol Buffers internally refer to fields using numbers rather than strings.</p>
			<p>Suppose we decide to send our greeting over email. We’d need to add an email field:</p>
			<pre><code>message HelloRequest { string name = 1; int32 favorite_number = 2; required string email = 3;
}</code></pre>
			<p>
				This is a <em>backward-incompatible</em> change because old clients don’t supply an email. When a client invokes the new <code>SayHello</code> with the old <code>HelloRequest</code>, the email will be missing, so the service can’t parse the <span epub:type="pagebreak" id="Page_203" title="203"/>request. Removing the <code>required</code> keyword and skipping emails when no address is supplied will maintain backward compatibility.</p>
			<p>
				Required fields are such an evolvability problem that they were removed from Protocol Buffers v3. Kenton Varda, the primary author of Protocol Buffers v2, said, “The ‘required’ keyword in Protocol Buffers turned out to be a horrible mistake” (<a class="LinkURL" href="https://capnproto.org/faq.html#how-do-i-make-a-field-required-like-in-protocol-buffers">https://capnproto.org/faq.html#how-do-i-make-a-field-required-like-in-protocol-buffers</a>). Many other systems have required fields, so be careful, and remember, “Required is forever.”</p>
			<p>
				We can create a <em>forward-incompatible</em> change by tweaking <code>HelloRequest</code>. Perhaps we want to accommodate negative favorite numbers. The Protocol Buffer documentation reads, “If your field is likely to have negative values, use <code>sint32</code> instead,” so we change the type of <code>favorite_number</code> accordingly:</p>
			<pre><code>message HelloRequest { string name = 1; sint32 favorite_number = 2;
}</code></pre>
			<p>
				Changing <code>int32</code> to <code>sint32</code> is both backward and forward incompatible. A client with the new <code>HelloRequest</code> will encode <code>favorite_number</code> using a different serialization scheme than an old version of <code>Greeter</code>, so the service will fail to parse it; and the new version of <code>Greeter</code> will fail to parse messages from old clients!</p>
			<p>
				The <code>sint32</code> change can be made forward compatible by adding a new field. Protocol Buffers let us rename fields as long as the field number remains the same.</p>
			<pre><code>message HelloRequest { string name = 1; int32 _deprecated_favorite_number = 2; sint32 favorite_number = 3;
}</code></pre>
			<p><span epub:type="pagebreak" id="Page_204" title="204"/>The server code needs to handle both fields for as long as the old clients are supported. Once rolled out, we can monitor how often clients use the deprecated field and clean up once clients upgrade or a deprecation schedule expires.</p>
			<p>
				Our example uses Protocol Buffers because they have a strongly typed system, and type compatibility is more straightforward to demonstrate and reason about. The same problems occur in other contexts, including “simple” REST services. When client and server content expectations diverge, errors crop up no matter what format you’re using. Moreover, it’s not just the message fields you need to worry about: a change in <em>semantics</em> of the message, or the logic of what happens when certain events transpire, can also be backward or forward incompatible.</p>
			<h3 id="h2-501836c11-0007">Version APIs</h3>
			<p class="BodyFirst">As APIs evolve over time, you will need to decide how to handle compatibility across multiple versions. Fully backward- and forward-compatible changes interoperate with all previous and future versions of an API; this can be hard to maintain, creating <em>cruft</em> like the logic for dealing with deprecated fields. Less stringent compatibility guarantees allow for more radical changes.</p>
			<p>Eventually, you’ll want to change your API in a way that isn’t compatible with old clients—requiring a new field, for example. Versioning your APIs means you introduce a new version when changes are made. Old clients can continue using the old API version. Tracking versions also helps you communicate with your customers—they can tell you what version they’re using, and you can market new features with a new version.</p>
			<p>API versions are usually managed with an API gateway or a service mesh. Versioned requests are routed to the appropriate service: a v2 request will be routed to a v2.X.X service instance, while a v3 request will be routed to a v3.X.X service instance. Absent a gateway, clients invoke RPC calls directly to version-specific service hosts, or a single service instance runs multiple versions internally.</p>
			<p><span epub:type="pagebreak" id="Page_205" title="205"/>API versioning comes with a cost. Older major versions of the service need to be maintained, and bug fixes need to be backported to prior versions. Developers need to keep track of which versions support which features. Lack of version management tooling can push version management on to engineers.</p>
			<p>Be pragmatic about versioning methodologies. Semantic versioning, discussed in Chapter 5, is a common API versioning scheme, but many companies version APIs using dates or other numeric schemes. Version numbers can be specified in URI paths, query parameters, or HTTP Accept headers, or using a myriad of other techniques. There are trade-offs between all of these approaches, and a lot of strong opinions. Defer to whatever the standard is at your company; if there isn’t one, ask your manager and tech leads for their thoughts on what’s best.</p>
			<p>Keep documentation versioned along with your APIs. Developers dealing with older versions of your code need accurate documentation. It’s really confusing for users to reference a different version of the documentation and discover that the API they’re using doesn’t match up. Committing API documentation in the main code repository helps to keep documentation and code in sync. Pull requests for code can update documentation as they change APIs and behavior.</p>
			<p>API versioning is most valuable when client code is hard to change. You’ll usually have the least control over external (customer) clients, so customer-facing APIs are the most important to version. If your team controls both the service and client, you might be able to get away without internal API versioning.</p>
			<h2 id="h1-501836c11-0004">Evolvable Data</h2>
			<p class="BodyFirst">APIs are more ephemeral than persisted data; once the client and server APIs are upgraded, the work is done. Data must be evolved as applications change. Data evolution runs the gamut from simple schema changes such as adding or removing a column to rewriting records with <span epub:type="pagebreak" id="Page_206" title="206"/>new schemas, fixing corruption, rewriting to match new business logic, and massive migrations from one database to another.</p>
			<p>Isolating databases and using explicit schemas will make data evolution more manageable. With an isolated database, you need only worry about the impact of a change on your own application. Schemas protect you from reading or writing malformed data, and automated schema migrations make schema changes predictable.</p>
			<h3 id="h2-501836c11-0008">Isolate Databases</h3>
			<p class="BodyFirst">Shared databases are difficult to evolve and will result in a loss of <em>autonomy</em>—a developer’s or team’s ability to make independent changes to the system. You will not be able to safely modify schemas, or even read and write, without worrying about how everyone is using your database. The architecture grows brittle as schemas become an unofficial, deeply entrenched API. Separate application databases make changes easier.</p>
			<p>
				Isolated databases are accessed by only a single application, while shared databases are accessed by more than one (see <a href="#figure11-1" id="figureanchor11-1">Figure 11-1</a>).</p>
			<figure>
				<img alt="f11001" src="image_fi/501836c11/f11001.png"/>
				<figcaption>
					<p><a id="figure11-1">Figure 11-1</a>: Shared databases</p>
				</figcaption>
			</figure>
			<p>
				Shared databases present several problems. Applications with shared databases can grow to depend directly on each other’s data. Applications act as a control point for the underlying data they serve. You can’t apply business logic on your raw data before serving it, and you can’t easily redirect queries to a new data store during a migration <span epub:type="pagebreak" id="Page_207" title="207"/>if queries bypass your application. If there are multiple applications writing, the meaning (<em>semantics</em>) of the data might diverge, making it harder for readers to reason about. Application data is not protected, so other applications might mutate it in unexpected ways. Schemas aren’t isolated; a change in one application’s schema can impact others. Nor is performance isolated, so if an application overwhelms the database, all other applications will be affected. In some cases, security boundaries might be violated.</p>
			<p>
				By contrast, isolated databases have just a single reader and writer (see <a href="#figure11-2" id="figureanchor11-2">Figure 11-2</a>). All other traffic goes through remote procedural calls.</p>
			<figure>
				<img alt="f11002" src="image_fi/501836c11/f11002.png"/>
				<figcaption>
					<p><a id="figure11-2">Figure 11-2</a>: Isolated databases</p>
				</figcaption>
			</figure>
			<p>Isolated databases afford you all of the flexibility and isolation that shared databases do not. You need only worry about your own application when making database schema changes, and database performance is governed by your usage.</p>
			<p>There are occasional cases where a shared database is valuable. When breaking a monolith up, sharing a database serves as a useful intermediate step before data has been migrated to a new isolated database. Managing many databases comes at a high operational cost. Early on, it might make sense to co-locate many databases on the same machines. But make sure any shared databases eventually get isolated and split up or replaced.</p>
			<h3 id="h2-501836c11-0009"><span epub:type="pagebreak" id="Page_208" title="208"/>Use Schemas</h3>
			<p class="BodyFirst">Rigid predefined columns and types, and the heavyweight processes for evolving them, have led to the emergence of popular <em>schemaless</em> data management. Most modern datastores support storing JSON or JSON-like objects without predeclaring its structure. Schemaless doesn’t literally mean “no schema” (data would be unusable); rather, schemaless data has an implicit schema that is supplied or inferred at read time.</p>
			<p>In practice, we’ve found that a schemaless approach has significant data integrity and complexity problems. A strongly typed schema-forward approach decreases the obscurity, and therefore complexity, of your application. The short-term simplicity is not usually worth the obscurity trade-off. Like code itself, data is sometimes described as “write once, read many”; use schemas to make reads easier.</p>
			<p>You’d think not having a schema would make a change easier: you simply start or stop writing fields as you need to evolve data. Schemaless data actually makes changes harder because you don’t know what you’re breaking as you evolve your data. Data quickly becomes an unintelligible hodgepodge of different record types. Developers, business analysts, and data scientists struggle to keep up. It’s going to be a tough day for the data scientist that wants to parse this JSON data:</p>
			<pre><code>{"name": "Fred", "location": [37.33, -122.03], "enabled": true}
{"name": "Li", "enabled": "false"}
{"name": "Willa", "location": "Boston, MA", "enabled": 0}</code></pre>
			<p>Defining explicit schemas for your data will keep your application stable and make your data usable. Explicit schemas let you sanity-check data as it is written. Parsing data using explicit schemas is usually faster, too. Schemas also help you detect when forward- and backward-incompatible changes are made. Data scientists know what to expect with data in the following table.</p>
			<pre><code><span epub:type="pagebreak" id="Page_209" title="209"/>CREATE TABLE users ( id BIGINT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(100) NOT NULL, latitude DECIMAL, longitude DECIMAL, enabled BOOLEAN NOT NULL
);</code></pre>
			<p>The rigidity of explicit schemas also carries a cost: they can be difficult to change. This is by design. Schemas force you to slow down and think through how existing data is going to be migrated and how downstream users will be affected.</p>
			<p>Don’t hide schemaless data inside schematized data. It’s tempting to be lazy and stuff a JSON string into a field called “data” or define a map of strings to contain arbitrary key-value pairs. Hiding schemaless data is self-defeating; you get all of the pain of explicit schemas but none of the gain.</p>
			<p>There are some cases where a schemaless approach is warranted. If your primary goal is to move fast—perhaps before you know what you need, when you are iterating rapidly, or when old data has little to no value—a schemaless approach lets you cut corners. Some data is legitimately nonuniform; some records have certain fields that others don’t. Flipping data from explicit to implicit schema is also a helpful trick when migrating data; you might temporarily make data schemaless to ease the transition to a new explicit schema.</p>
			<h3 id="h2-501836c11-0010">Automate Schema Migrations</h3>
			<p class="BodyFirst">Changing a database’s schema is dangerous. A minor tweak—adding an index or dropping a column—can cause the entire database or application to grind to a halt. Managing database changes by manually executing <em>database description language</em><em> (DDL</em><em>)</em> commands directly on the database is error prone. Database schemas in different environments <span epub:type="pagebreak" id="Page_210" title="210"/>diverge, the state of the database is uncertain, no one knows who changed what when, and performance impacts are unclear. The mix of error-prone changes and the potential for major downtime is an explosive combination.</p>
			<p>Database schema management tools make database changes less error prone. Automated tooling does two things for you: it forces you to track the entire history of a schema, and it gives you tools to migrate a schema from one version to another. Track schema changes, use automated database tools, and work with your database team to manage schema evolution.</p>
			<p>The entire history of a schema is usually kept in a series of files defining every change from the initial creation of a schema all the way to its current form. Tracking DDL changes in files helps developers see how the schema has changed over time. Files tracked in a version control system will show who made which changes, when, and why. Pull requests will afford the opportunity for schema reviews and linting.</p>
			<p>We can take our users table from the “Use Schemas” section and put it in a versioned file for a schema migration tool like Liquibase:</p>
			<pre><code>--liquibase formatted sql
--changeset criccomini:create-users-table
CREATE TABLE users ( id BIGINT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(100) NOT NULL, latitude DECIMAL, longitude DECIMAL, enabled BOOLEAN NOT NULL
);
--rollback DROP TABLE users</code></pre>
			<p>
				We can then define an <code>ALTER</code> in a separate block:</p>
			<pre><code>--changeset dryaboy:add-email
ALTER TABLE users ADD email VARCHAR(255);
--rollback DROP COLUMN email</code></pre>
			<p><span epub:type="pagebreak" id="Page_211" title="211"/>Liquibase can use these files to upgrade or downgrade schemas through the CLI:</p>
			<pre><code>$ liquibase update</code></pre>
			<p>
				If Liquibase is pointed at an empty database, it will run both the <code>CREATE</code> and <code>ALTER</code> commands. If it’s pointed at a database where the <code>CREATE</code> has already been executed, it will run only the <code>ALTER</code>. Tools like Liquibase often track the current version of a database schema in special metadata tables in the database itself, so don’t be surprised if you find tables with names like <code>DATABASECHANGELOG</code> or <code>DATABASECHANGELOGLOCK</code>.</p>
			<p>
				In the previous example, the Liquibase command is run from the command line, usually by a <em>database administrator</em><em> (DBA</em><em>)</em>. Some teams will automate the execution itself through a commit hook or a web UI.</p>
			<p>Don’t couple database and application lifecycles. Tying schema migrations to application deployment is dangerous. Schema changes are delicate and can have serious performance implications. Separating database migrations from application deployment lets you control when schema changes go out.</p>
			<p>
				Liquibase is just one tool that can manage database migrations; there are others like Flyway and Alembic. Many <em>object-resource mappers</em><em> (ORMs</em><em>)</em> come with schema migration managers as well. If your company already has one in place, use it; if not, work with the team to figure out what to use. Once selected, use the database migration system for all the changes; circumventing it will negate the tool’s benefit since reality has diverged from what’s tracked.</p>
			<p>
				More sophisticated database operations tools also exist. Tools like GitHub’s gh-ost and Percona’s pt-online-schema-change help database administrators run large schema changes without impacting performance. Other tools like Skeema and Square’s Shift provide more sophisticated versioning that lets you “diff” database schemas and <span epub:type="pagebreak" id="Page_212" title="212"/>automatically derive changes. All of these tools help make database evolution safer.</p>
			<p>Most migration tools support rollbacks, which undo a migration’s changes. Rollbacks can only do so much, so be careful. For example, rolling back a column deletion will recreate a column, but it will not recreate the data that used to be stored in that column! Backing up a table prior to deletion is prudent.</p>
			<p>Because of the permanent and large-scale nature of these types of changes, organizations will often have specific subteams responsible for ensuring the changes are done correctly. These might be DBAs, operations or SREs, or a set of senior engineers familiar with the tools, performance implications, and application-specific concerns. These teams are a great resource for understanding the nuances of data storage systems; learn from them.</p>
			<h3 id="h2-501836c11-0011">Maintain Schema Compatibility</h3>
			<p class="BodyFirst">Data written to disk has the same compatibility problems that APIs have. Like APIs, the reader and writer of the data can change independently; they might not be the same software and might not be on the same machine. And like APIs, data has a schema with field names and types. Changing schemas in a forward- or backward-incompatible way can break applications. Use schema compatibility checks to detect incompatible changes, and use data products to decouple internal and external schemas.</p>
			<p>Developers think databases are an implementation detail that’s hidden from other systems. Fully encapsulated databases are ideal but not often realized in practice. Even if a production database is hidden behind an application, the data is often exported to data warehouses.</p>
			<p>
				Data warehouses are databases used for analytic and reporting purposes. Organizations set up an <em>extract, transform, load</em><em> (ETL</em><em>)</em> data pipeline that extracts data from production databases and transforms and loads it into a data warehouse (see <a href="#figure11-3" id="figureanchor11-3">Figure 11-3</a>).</p>
				<span epub:type="pagebreak" id="Page_213" title="213"/>
				<figure>
				<img alt="f11003" src="image_fi/501836c11/f11003.png"/>
				<figcaption>
					<p><a id="figure11-3">Figure 11-3</a>: ETL data pipeline</p>
				</figcaption>
			</figure>
			<p>ETL pipelines depend heavily on database schemas. Simply dropping the column in a production database could cause the entire data pipeline to grind to a halt. Even if dropping a column doesn’t break the data pipeline, downstream users might be using the field for reporting, machine learning models, or ad hoc queries.</p>
			<p>
				Other systems might also depend on your database schemas. <em>Change data capture</em><em> (CDC</em><em>)</em> is an event-based architecture that converts insert, update, and delete operations into messages for downstream consumers. An insert into a “members” table might trigger a message that an email service uses to send an email to the new user. Such messages are an implicit API, and making backward-incompatible schema changes can break other services.</p>
			<p>Data warehouse pipelines and downstream users must be protected from breaking schema changes. Validate that your schema changes are safe before executing them in production. Compatibility checks should be done as early as possible, ideally at code commit time by inspecting database DDL statements. Executing DDL statements in a preproduction integration testing environment, if one exists, can also protect changes. Run your DDL statements and integration tests to verify that downstream systems don’t break.</p>
			<p>
				You can also protect internal schemas by exporting a <em>data product</em> that explicitly decouples internal schemas from downstream users. Data products map internal schemas to separate user-facing schemas; the development team owns both the production database and the <span epub:type="pagebreak" id="Page_214" title="214"/>published data products. Separate data products, which might simply be database views, allow teams to maintain compatibility with data consumers without having to freeze their internal database schemas.</p>
			<h2 id="h1-501836c11-0005">Do’s and Don’ts</h2>
			<table border="1" class="trade" id="tabular-501836c11-0001">
				<thead>
					<tr>
						<td><b>Do’s</b></td>
						<td><b>Don’ts</b></td>
					</tr>
				</thead>
				<tbody>
					<tr>
						<td><b>DO</b> remember YAGNI: “You Ain’t Gonna Need It.” 
						</td>
						<td><b>DON’T</b> build too many abstractions without purpose.
						</td>
					</tr>
					<tr>
						<td><b>DO</b> use standard libraries and development patterns.
						</td>
						<td><b>DON’T</b> write methods with hidden ordering or argument requirements.
						</td>
					</tr>
					<tr>
						<td><b>DO</b> use an IDL to define your APIs.
						</td>
						<td><b>DON’T</b> surprise other developers with exotic code.
						</td>
					</tr>
					<tr>
						<td><b>DO</b> version external APIs and documentation.
						</td>
						<td><b>DON’T</b> make incompatible API changes.
						</td>
					</tr>
					<tr>
						<td><b>DO</b> isolate application databases from each other.
						</td>
						<td><b>DON’T</b> be dogmatic about internal API versioning.
						</td>
					</tr>
					<tr>
						<td><b>DO</b> define explicit schemas for all your data.
						</td>
						<td><b>DON’T</b> embed schemaless data in string or byte fields.
						</td>
					</tr>
					<tr>
						<td><b>DO</b> use migration tools to automate database schema management.
						</td>
						<td/>
					</tr>
					<tr>
						<td><b>DO</b> maintain schema compatibility if downstream consumers use your data.
						</td>
						<td/>
					</tr>
				</tbody>
			</table>
			<h2 id="h1-501836c11-0006">Level Up</h2>
			<p class="BodyFirst">A book-length treatment on evolvable architectures can be found in <em>Building Evolutionary Architectures</em>, written by Neal Ford, Rebecca Parsons, and Patrick Kua (O’Reilly Media, 2017). For more depth on evolvable APIs and data, see their work. They discuss DDD briefly; for the full experience, refer to <em>Implementing Domain Driven Design</em> by Vaughn Vernon (Addison-Wesley Professional, 2013).</p>
			<p><span epub:type="pagebreak" id="Page_215" title="215"/>We cite John Ousterhout’s work on complexity in the beginning of the chapter. See his excellent (and short) book, <em>A Philosophy of Software Design</em> (Yaknyam Press, 2018), to learn more about complexity and how to manage it.</p>
			<p>
				Zach Tellman’s <em>Elements of Clojure</em> (<a class="LinkURL" href="http://lulu.com">lulu.com</a>, 2019) is a wonderful book that has only four chapters: “Names,” “Idioms,” “Indirection,” and “Composition.” It is a lucid, concise discussion of these four topics, which will help you build evolvable architectures (even if you never touch a line of Clojure).</p>
			<p>
				Richard Hickey has a beautiful talk called <em>Simple Made Easy</em> (<a class="LinkURL" href="https://www.youtube.com/watch?v=oytL881p-nQ">https://www.youtube.com/watch?v=oytL881p-nQ</a>). Hickey’s talk discusses simplicity, complexity, “easiness,” and how to build good software. The talk is a must-watch.</p>
			<p><em>Data Mesh: Building a Next Generation Data Architecture</em> by Zhamak Dehghani (coming out end of 2021) contains a deeper discussion of data products.</p>
			<p><em>Designing Data-Intensive Applications</em> by Martin Kleppman (O’Reilly Media, 2017) is an excellent book that covers, among other things, the subjects of data evolution, data schemas, IDLs, and change data capture. This book is an instant classic, and we highly recommend it.</p>
			<aside class="endnote" epub:type="rearnote">
</aside>
		</section>
	</body>
</html>