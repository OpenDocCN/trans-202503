<html><head></head><body>
<h2 class="h2" id="ch19"><span epub:type="pagebreak" id="page_309"/><span class="big">19</span><br/>TUNING QUALITY OF SERVICE</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">Ideally, our applications would use minimal or highly predictable processing, memory, storage, and network resources. In the real world, though, applications are “bursty,” with changes in load driven by user demand, large amounts of data, or complex processing. In a Kubernetes cluster, where application components are deployed dynamically to various nodes in the cluster, uneven distribution of load across those nodes can cause performance bottlenecks.</p>&#13;
<p class="indent">From an application architecture standpoint, the more we can make the application components small and scalable, the more we can evenly distribute load across the cluster. Unfortunately, it’s not always possible to solve performance issues with horizontal scaling. In this chapter, we’ll look at how we can use resource specifications to provide hints to the cluster about how to schedule our Pods, with the goal of making application performance more predictable.</p>&#13;
<h3 class="h3" id="ch00lev1sec77"><span epub:type="pagebreak" id="page_310"/>Achieving Predictability</h3>&#13;
<p class="noindent">In normal, everyday language, the term <em>real time</em> has the sense of something that happens quickly and continuously. But in computer science, we make a distinction between <em>real time</em> and <em>real fast</em> to such a degree that they are thought of as opposites. This is due to the importance of predictability.</p>&#13;
<p class="indent">Real-time processing is simply processing that needs to keep up with some activity that is happening in the real world. It could be anything from airplane cockpit software that needs to keep up with sensor data input and maintain up-to-date electronic flight displays, to a video streaming application that needs to receive and decode each frame of video in time to display it. In real-time systems, it is critical that we can guarantee that processing will be “fast enough” to keep up with the real-world requirement.</p>&#13;
<p class="indent">Fast enough is all we need. It’s not necessary for the processing to go any faster than the real world, as there isn’t anything else for the application to do. But even a single time interval when the processing is slower than the real world means we fall behind our inputs or outputs, leading to annoyed movie watchers—or even to crashed airplanes.</p>&#13;
<p class="indent">For this reason, the main goal in real-time systems is predictability. Resources are allocated based on the worst-case scenario the system will encounter, and we’re willing to provide significantly more processing than necessary to have plenty of margin on that worst case. Indeed, it’s common to require these types of systems to stay under 50 percent utilization of the available processing and memory, even at maximum expected load.</p>&#13;
<p class="indent">But whereas responsiveness is always important, most applications don’t operate in a real-time environment, and this additional resource margin is expensive. For that reason, most systems try to find a balance between predictability and efficiency, which means that we are often willing to tolerate a bit of slower performance from our application components as long as it is temporary.</p>&#13;
<h3 class="h3" id="ch00lev1sec78">Quality of Service Classes</h3>&#13;
<p class="noindent">To help us balance predictability and efficiency for the containers in a cluster, Kubernetes allocates Pods to one of three different Quality of Service classes: <span class="literal">BestEffort</span>, <span class="literal">Burstable</span>, and <span class="literal">Guaranteed</span>. In a way, we can think of these as descriptive. <span class="literal">BestEffort</span> is used when we don’t provide Kubernetes with any resource requirements, and it can only do its best to provide enough resources for the Pod. <span class="literal">Burstable</span> is used when a Pod might exceed its resource request. <span class="literal">Guaranteed</span> is used when we provide consistent resource requirements and our Pod is expected to stay within them. Because these classes are descriptive and are based solely on how the containers in the Pod specify their resource requirements, there is no way to specify the QoS for a Pod manually.</p>&#13;
<p class="indent">The QoS class is used in two ways. First, Pods in a QoS class are grouped together for Linux control groups (cgroups) configuration. As we saw in <a href="ch03.xhtml#ch03">Chapter 3</a>, cgroups are used to control resource utilization, especially processing and memory, for a group of processes, so a Pod’s cgroup affects its <span epub:type="pagebreak" id="page_311"/>priority in use of processing time when the system load is high. Second, if the node needs to start evicting Pods due to lack of memory resources, the QoS class affects which Pods are evicted first.</p>&#13;
<h4 class="h4" id="ch00lev2sec107">BestEffort</h4>&#13;
<p class="noindent">The simplest case is one in which we declare a Pod with no <span class="literal">limits</span>. In that case, the Pod is assigned to the <span class="literal">BestEffort</span> class. Let’s create an example Pod to explore what that means.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on getting set up.</em></p>&#13;
</div>&#13;
<p class="indent">Here’s the Pod definition:</p>&#13;
<p class="noindent6"><em>best-effort.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: best-effort&#13;
spec:&#13;
  containers:&#13;
  - name: best-effort&#13;
    image: busybox&#13;
    command: ["/bin/sleep", "infinity"]&#13;
  nodeName: host01</pre>&#13;
<p class="indent">This definition includes no <span class="literal">resources</span> field at all, but the QoS class would be the same if we included a <span class="literal">resources</span> field with <span class="literal">requests</span> but no <span class="literal">limits</span>.</p>&#13;
<p class="indent">We use <span class="literal">nodeName</span> to force this Pod onto <span class="literal">host01</span> so that we can observe how its resource use is configured. Let’s apply it to to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/best-effort.yaml</span> &#13;
pod/best-effort created</pre>&#13;
<p class="indent">After the Pod is running, we can look at its details to see that it has been allocated to the <span class="literal">BestEffort</span> QoS class:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get po best-effort -o json | jq .status.qosClass</span>&#13;
"BestEffort"</pre>&#13;
<p class="indent">We can use the <span class="literal">cgroup-info</span> script we saw in <a href="ch14.xhtml#ch14">Chapter 14</a> to see how the QoS class affects the cgroup configuration for containers in the Pod:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">/opt/cgroup-info best-effort</span>&#13;
&#13;
Container Runtime&#13;
-----------------&#13;
Pod ID: 205...&#13;
&#13;
<span epub:type="pagebreak" id="page_312"/>Cgroup path: /kubepods.slice/kubepods-besteffort.slice/kubepods-...&#13;
&#13;
CPU Settings&#13;
------------&#13;
CPU Shares: 2&#13;
CPU Quota (us): -1 per 100000&#13;
&#13;
Memory Settings&#13;
---------------&#13;
Limit (bytes): 9223372036854771712</pre>&#13;
<p class="indent">The Pod is effectively unlimited in CPU and memory usage. However, the Pod’s cgroup is under the <em>kubepods-besteffort.slice</em> path, reflecting its allocation to the <span class="literal">BestEffort</span> QoS class. This allocation has an immediate effect on its CPU priority, as we can see when we compare the <span class="literal">cpu.shares</span> allocated to the <span class="literal">BestEffort</span> class compared to the <span class="literal">Burstable</span> class:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-besteffort.slice/cpu.shares</span> &#13;
2&#13;
root@host01:~# <span class="codestrong1">cat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-burstable.slice/cpu.shares</span> &#13;
1157</pre>&#13;
<p class="indent">As we saw in <a href="ch14.xhtml#ch14">Chapter 14</a>, these values are relative, so this configuration means that when our system’s processing load is high, containers in <span class="literal">Burstable</span> Pods are going to be allocated more than 500 times the processor share that containers in <span class="literal">BestEffort</span> Pods receive. This value is based on the number of Pods that are already in the <span class="literal">BestEffort</span> and <span class="literal">Burstable</span> QoS classes, including the various cluster infrastructure components already running on <em>host01</em>, thus you might see a slightly different value.</p>&#13;
<p class="indent">The <em>kubepods.slice</em> cgroup sits at the same level as cgroups for user and system processes, so when the system is loaded it gets an approximately equal share of processing time as those other cgroups. Based on the <em>cpu.shares</em> identified within the <em>kubepods.slice</em> cgroup, <span class="literal">BestEffort</span> Pods are receiving less than 1 percent of the total share of processing compared to <span class="literal">Burstable</span> Pods, even without considering any processor time allocated to <span class="literal">Guaranteed</span> Pods. This means that <span class="literal">BestEffort</span> Pods receive almost no processor time when the system is loaded, so they should be used only for background processing that can run when the cluster is idle. In addition, because Pods are placed in the <span class="literal">BestEffort</span> class only if they have no <span class="literal">limits</span> specified, they cannot be created in a Namespace with limit quotas. So most of our application Pods will be in one of the other two QoS classes.</p>&#13;
<h4 class="h4" id="ch00lev2sec108">Burstable</h4>&#13;
<p class="noindent">Pods are placed in the <span class="literal">Burstable</span> class if they specify both <span class="literal">requests</span> and <span class="literal">limits</span> and if those two specifications are different. As we saw in <a href="ch14.xhtml#ch14">Chapter 14</a>, the <span class="literal">requests</span> specification is used for scheduling purposes, whereas the <span class="literal">limits</span> specification is used for runtime enforcement. In other words, Pods in this <span epub:type="pagebreak" id="page_313"/>situation can have “bursts” of resource utilization above their <span class="literal">requests</span> level, but they cannot exceed their <span class="literal">limits</span>.</p>&#13;
<p class="indent">Let’s look at an example:</p>&#13;
<p class="noindent6"><em>burstable.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: burstable&#13;
spec:&#13;
  containers:&#13;
  - name: burstable&#13;
    image: busybox&#13;
    command: ["/bin/sleep", "infinity"]&#13;
    resources:&#13;
      requests:&#13;
        memory: "64Mi"&#13;
        cpu: "50m"&#13;
      limits:&#13;
        memory: "128Mi"&#13;
        cpu: "100m"&#13;
  nodeName: host01</pre>&#13;
<p class="indent">This Pod definition supplies both <span class="literal">requests</span> and <span class="literal">limits</span> resource requirements, and they are different, so we should expect this Pod to be placed in the <span class="literal">Burstable</span> class.</p>&#13;
<p class="indent">Let’s apply this Pod to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/burstable.yaml</span> &#13;
pod/burstable created</pre>&#13;
<p class="indent">Next, let’s verify that it was assigned to the <span class="literal">Burstable</span> QoS class:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get po burstable -o json | jq .status.qosClass</span>&#13;
"Burstable"</pre>&#13;
<p class="indent">Indeed, the cgroup configuration follows the QoS class and the <span class="literal">limits</span> we specified:</p>&#13;
<pre>root@host01:~# /opt/cgroup-info burstable&#13;
&#13;
Container Runtime&#13;
-----------------&#13;
Pod ID: 8d0...&#13;
Cgroup path: /kubepods.slice/kubepods-burstable.slice/kubepods-...&#13;
&#13;
CPU Settings&#13;
------------&#13;
CPU Shares: 51&#13;
CPU Quota (us): 10000 per 100000&#13;
&#13;
<span epub:type="pagebreak" id="page_314"/>Memory Settings&#13;
---------------&#13;
Limit (bytes): 134217728</pre>&#13;
<p class="indent">The <span class="literal">limits</span> specified for this Pod were used to set both a CPU limit and a memory limit. Also, as we expect, this Pod’s cgroup is placed within <em>kubepods-burstable.slice</em>.</p>&#13;
<p class="indent">Adding another Pod to the <span class="literal">Burstable</span> QoS class has caused Kubernetes to rebalance the allocation of processor time:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-besteffort.slice/cpu.shares</span> &#13;
2&#13;
root@host01:~# <span class="codestrong1">cat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-burstable.slice/cpu.shares</span> &#13;
1413</pre>&#13;
<p class="indent">The result is that Pods in the <span class="literal">Burstable</span> QoS class now show a value of 1413 for <em>cpu.shares</em>, whereas Pods in the <span class="literal">BestEffort</span> class still show 2. This means that the relative processor share under load is now 700 to 1 in favor of Pods in the <span class="literal">Burstable</span> class. Again, you may see slightly different values based on how many infrastructure Pods Kubernetes has allocated to <span class="literal">host01</span>.</p>&#13;
<p class="indent">Because <span class="literal">Burstable</span> Pods are scheduled based on <span class="literal">requests</span> but cgroup runtime enforcement is based on <span class="literal">limits</span>, a node’s processor and memory resources can be overcommitted. It works fine as long as the Pods on a node balance out one another so that the average utilization matches the <span class="literal">requests</span>. It becomes a problem if the average utilization exceeds the <span class="literal">requests</span>. In that case, Pods will see their CPU throttled and may even be evicted if memory becomes scarce, as we saw in <a href="ch10.xhtml#ch10">Chapter 10</a>.</p>&#13;
<h4 class="h4" id="ch00lev2sec109">Guaranteed</h4>&#13;
<p class="noindent">If we want to increase predictability for the processing and memory available to a Pod, we can place it in the <span class="literal">Guaranteed</span> QoS class by giving the <span class="literal">requests</span> and <span class="literal">limits</span> equal settings. Here’s an example:</p>&#13;
<p class="noindent6"><em>guaranteed.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: guaranteed&#13;
spec:&#13;
  containers:&#13;
  - name: guaranteed&#13;
    image: busybox&#13;
    command: ["/bin/sleep", "infinity"]&#13;
    resources:&#13;
      limits:&#13;
        memory: "64Mi"&#13;
<span epub:type="pagebreak" id="page_315"/>        cpu: "50m"&#13;
  nodeName: host01</pre>&#13;
<p class="indent">In this example, only <span class="literal">limits</span> is specified given that Kubernetes automatically sets the <span class="literal">requests</span> to match the <span class="literal">limits</span> if <span class="literal">requests</span> is missing.</p>&#13;
<p class="indent">Let’s apply this to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/guaranteed.yaml</span> &#13;
pod/guaranteed created</pre>&#13;
<p class="indent">After the Pod is running, verify the QoS class:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get po guaranteed -o json | jq .status.qosClass</span>&#13;
"Guaranteed"</pre>&#13;
<p class="indent">The cgroups configuration looks a little different:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">/opt/cgroup-info guaranteed</span>&#13;
&#13;
Container Runtime&#13;
-----------------&#13;
Pod ID: 146...&#13;
Cgroup path: /kubepods.slice/kubepods-...&#13;
&#13;
CPU Settings&#13;
------------&#13;
CPU Shares: 51&#13;
CPU Quota (us): 5000 per 100000&#13;
&#13;
Memory Settings&#13;
---------------&#13;
Limit (bytes): 67108864</pre>&#13;
<p class="indent">Rather than place these containers into a separate directory, containers in the <span class="literal">Guaranteed</span> QoS class are placed directly in <em>kubepods.slice</em>. Putting them in this location has the effect of privileging containers in <span class="literal">Guaranteed</span> Pods when the system is loaded because those containers receive their CPU shares individually rather than as a class.</p>&#13;
<h4 class="h4" id="ch00lev2sec110">QoS Class Eviction</h4>&#13;
<p class="noindent">The privileged treatment of Pods in the <span class="literal">Guaranteed</span> QoS class extends to Pod eviction as well. As described in <a href="ch03.xhtml#ch03">Chapter 3</a>, cgroup enforcement of memory limits is handled by the OOM killer. The OOM killer also runs when a node is completely out of memory. To help the OOM killer choose which containers to terminate, Kubernetes sets the <span class="literal">oom_score_adj</span> parameter based on the QoS class of the Pod. This parameter can have a value from –1000 to 1000. The higher the number, the more likely the OOM killer will choose a process to be killed.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_316"/>The <span class="literal">oom_score_adj</span> value is recorded in <em>/proc</em> for each process. The automation has added a script called <em>oom-info</em> to retrieve it for a given Pod. Let’s check the values for the Pods in each QoS class:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">/opt/oom-info best-effort</span>&#13;
OOM Score Adjustment: 1000&#13;
root@host01:~# <span class="codestrong1">/opt/oom-info burstable</span>&#13;
OOM Score Adjustment: 968&#13;
root@host01:~# <span class="codestrong1">/opt/oom-info guaranteed</span>&#13;
OOM Score Adjustment: -997</pre>&#13;
<p class="indent">Pods in the <span class="literal">BestEffort</span> QoS class have the maximum adjustment of 1000, so they would be targeted first by the OOM killer. Pods in the <span class="literal">Burstable</span> QoS class have a score calculated based on the amount of memory specified in the <span class="literal">requests</span> field, as a percentage of the node’s total memory capacity. This value will therefore be different for every Pod but will always be between 2 and 999. Thus, Pods in the <span class="literal">Burstable</span> QoS class will always be second in priority for the OOM killer. Meanwhile, Pods in the <span class="literal">Guaranteed</span> QoS class are set close to the minimum value, in this case –997, so they are protected from the OOM killer as much as possible.</p>&#13;
<p class="indent">Of course, as mentioned in <a href="ch03.xhtml#ch03">Chapter 3</a>, the OOM killer terminates a process immediately, so it is an extreme measure. When memory on a node is low but not yet exhausted, Kubernetes attempts to evict Pods to reclaim memory. This eviction is also prioritized based on the QoS class. Pods in the <span class="literal">BestEffort</span> class and Pods in the <span class="literal">Burstable</span> class that are using more than their <span class="literal">requests</span> value (high-use <span class="literal">Burstable</span>) are the first to be evicted, followed by Pods in the <span class="literal">Burstable</span> class that are using less than their <span class="literal">requests</span> value (low-use <span class="literal">Burstable</span>) and Pods in the <span class="literal">Guaranteed</span> class.</p>&#13;
<p class="indent">Before moving on, let’s do some cleanup:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete po/best-effort po/burstable po/guaranteed</span>&#13;
pod "best-effort" deleted&#13;
pod "burstable" deleted&#13;
pod "guaranteed" deleted</pre>&#13;
<p class="indent">Now we can have a fresh start when we look at Pod priorities later in this chapter.</p>&#13;
<h4 class="h4" id="ch00lev2sec111">Choosing a QoS Class</h4>&#13;
<p class="noindent">Given this prioritization in processing time and eviction priority, it might be tempting to place all Pods in the <span class="literal">Guaranteed</span> QoS class. And there are application components for which this is a viable strategy. As described in <a href="ch07.xhtml#ch07">Chapter 7</a>, we can configure a HorizontalPodAutoscaler to make new Pod instances automatically if the existing instances are consuming a significant percentage of their allocated resources. This means that we can request a reasonable <span class="literal">limits</span> value for Pods in a Deployment and allow the cluster to automatically scale the Deployment if we’re getting too close to the limit across those Pods. If the cluster is running in a cloud environment, we can even <span epub:type="pagebreak" id="page_317"/>extend autoscaling to the node level, dynamically creating new cluster nodes when load is high and reducing the number of nodes when the cluster is idle.</p>&#13;
<p class="indent">Using only <span class="literal">Guaranteed</span> Pods together with autoscaling sounds great, but it assumes that our application components are easily scalable. It also only works well when our application load consists of many small requests, so that an increase in load primarily means we are handing similar-sized requests from more users. If we have application components that periodically handle large or complex requests, we must set the <span class="literal">limits</span> for those components to accommodate the worst-case scenario. Given that Pods in the <span class="literal">Guaranteed</span> QoS class have <span class="literal">requests</span> equal to <span class="literal">limits</span>, our cluster will need enough resources to handle this worst-case scenario, or we won’t even be able to schedule our Pods. This results in a cluster that is largely idle unless the system is under its maximum load. Similarly, if we have scalability limitations such as dependency on specialized hardware, we might have a natural limit on the number of Pods we can create for a component, forcing each Pod to have more resources to handle its share of the overall load.</p>&#13;
<p class="indent">For this reason, it makes sense to balance the use of the <span class="literal">Guaranteed</span> and <span class="literal">Burstable</span> QoS classes for our Pods. Any Pods that have consistent load, or that can feasibly be scaled horizontally to meet additional demand, should be in the <span class="literal">Guaranteed</span> class. Pods that are harder to scale, or need to handle a mix of large and small workloads, should be in the <span class="literal">Burstable</span> class. These Pods should specify their <span class="literal">requests</span> based on their average utilization, and specify <span class="literal">limits</span> based on their worst-case scenario. Specifying resource requirements in this way will ensure that the cluster’s expected performance margin can be monitored by simply comparing the allocated resources to the cluster capacity. Finally, if a large request causes multiple application components to run at their worst-case utilization simultaneously, it may be worth running performance tests and exploring anti-affinity, as described in <a href="ch18.xhtml#ch18">Chapter 18</a>, to avoid overloading a single node.</p>&#13;
<h3 class="h3" id="ch00lev1sec79">Pod Priority</h3>&#13;
<p class="noindent">In addition to using hints to help the Kubernetes cluster understand how to manage Pods when the system is highly loaded, it is possible to tell the cluster directly to give some Pods a higher priority than others. This higher priority applies during Pod eviction, as Pods will be evicted in priority order within their QoS class. It also applies during scheduling because the Kubernetes scheduler will evict Pods if necessary to be able to schedule a higher-priority Pod.</p>&#13;
<p class="indent">Pod priority is a simple numeric field; higher numbers are higher priority. Numbers greater than one billion are reserved for critical system Pods. To assign a priority to a Pod, we must create a <em>PriorityClass</em> resource first. Here’s an example:</p>&#13;
<p class="noindent6"><em>essential.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: scheduling.k8s.io/v1&#13;
kind: PriorityClass&#13;
<span epub:type="pagebreak" id="page_318"/>metadata:&#13;
  name: essential&#13;
value: 999999</pre>&#13;
<p class="indent">Let’s apply this to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/essential.yaml</span> &#13;
priorityclass.scheduling.k8s.io/essential created</pre>&#13;
<p class="indent">Now that this PriorityClass has been defined, we can apply it to Pods. However, let’s first create a large number of low-priority Pods through which we can see Pods being preempted. We’ll use this Deployment:</p>&#13;
<pre>---&#13;
kind: Deployment&#13;
apiVersion: apps/v1&#13;
metadata:&#13;
  name: lots&#13;
spec:&#13;
  replicas: 1000&#13;
  selector:&#13;
    matchLabels:&#13;
      app: lots&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: lots&#13;
    spec:&#13;
      containers:&#13;
      - name: sleep&#13;
        image: busybox&#13;
        command: ["/bin/sleep", "infinity"]&#13;
        resources:&#13;
          limits:&#13;
            memory: "64Mi"&#13;
            cpu: "250m"</pre>&#13;
<p class="indent">This is a basic Deployment that runs <span class="literal">sleep</span> and doesn’t request very much memory or CPU, but it does set <span class="literal">replicas</span> to <span class="literal">1000</span>, so we’re asking our Kubernetes cluster to create 1,000 Pods. The example cluster isn’t large enough to deploy 1,000 Pods, both because we don’t have sufficient resources to meet the specification and because a node is limited to 110 Pods by default. Still, let’s apply it to the cluster, as shown in <a href="ch19.xhtml#ch19list1">Listing 19-1</a>, and the scheduler will create as many Pods as it can:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/lots.yaml</span> &#13;
deployment.apps/lots created</pre>&#13;
<p class="caption" id="ch19list1"><em>Listing 19-1: Deploy lots of Pods</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_319"/>Let’s describe the Deployment to see how things are going:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe deploy lots</span>&#13;
Name:                   lots&#13;
Namespace:              default&#13;
...&#13;
Replicas:               1000 desired ... | 7 available | 993 unavailable&#13;
...</pre>&#13;
<p class="indent">We managed to get only seven Pods in our example cluster, given the number of Pods already running for cluster infrastructure components. Unfortunately, that’s all the Pods we’ll get:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe node host01</span>&#13;
Name:               host01&#13;
  (Total limits may be over 100 percent, i.e., overcommitted.)&#13;
Allocated resources:&#13;
...&#13;
  Resource           Requests     Limits&#13;
  --------           --------     ------&#13;
  cpu             <span class="ent">➊</span> 1898m (94%)  768m (38%)&#13;
  memory             292Mi (15%)  192Mi (10%)&#13;
  ephemeral-storage  0 (0%)       0 (0%)&#13;
  hugepages-2Mi      0 (0%)       0 (0%)&#13;
...</pre>&#13;
<p class="indent">The data for <span class="literal">host01</span> shows that we’ve allocated 94 percent of the available CPU <span class="ent">➊</span>. But each of our Pods is requesting 250 millicores, so there isn’t enough capacity remaining to schedule another Pod on this node. The other two nodes are in a similar situation, with insufficient CPU room to schedule any more Pods. Still, the cluster is performing just fine. We’ve theoretically allocated all of the processing power, but those containers are just running <span class="literal">sleep</span>, and as such, they aren’t actually using much CPU.</p>&#13;
<p class="indent">Also, it’s important to remember that the <span class="literal">requests</span> field is used for scheduling, so even though we have a number of infrastructure <span class="literal">BestEffort</span> Pods that specify <span class="literal">requests</span> but no <span class="literal">limits</span> and we have plenty of <span class="literal">Limits</span> capacity on this node, we still don’t have any room for scheduling new Pods. Only <span class="literal">Limits</span> can be overcommitted, not <span class="literal">Requests</span>.</p>&#13;
<p class="indent">Because we have no more CPU to allocate to Pods, the rest of the Pods in our Deployment are stuck in a Pending state:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get po | grep -c Pending</span>&#13;
993</pre>&#13;
<p class="indent">All 993 of these Pods have the default pod priority of 0. As a result, when we create a new Pod using the <span class="literal">essential</span> PriorityClass, it will jump to the front of the scheduling queue. Not only that, but the cluster will evict Pods as necessary to enable it to be scheduled.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_320"/>Here’s the Pod definition:</p>&#13;
<p class="noindent6"><em>needed.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: needed&#13;
spec:&#13;
  containers:&#13;
  - name: needed&#13;
    image: busybox&#13;
    command: ["/bin/sleep", "infinity"]&#13;
    resources:&#13;
      limits:&#13;
        memory: "64Mi"&#13;
        cpu: "250m"&#13;
  priorityClassName: essential</pre>&#13;
<p class="indent">The key difference here is the specification of the <span class="literal">priorityClassName</span>, matching the PriorityClass we created. Let’s apply this to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/needed.yaml</span> &#13;
pod/needed created</pre>&#13;
<p class="indent">It will take the cluster a little time to evict another Pod so that this one can be scheduled, but after a minute or so it will start running:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get po needed</span>&#13;
NAME     READY   STATUS    RESTARTS   AGE&#13;
needed   1/1     Running   0          36s</pre>&#13;
<p class="indent">To allow this to happen, one of the Pods from the <span class="literal">lots</span> Deployment we created in <a href="ch19.xhtml#ch19list1">Listing 19-1</a> had to be evicted:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe deploy lots</span>&#13;
Name:                   lots&#13;
Namespace:              default&#13;
CreationTimestamp:      Fri, 01 Apr 2022 19:20:52 +0000&#13;
Labels:                 &lt;none&gt;&#13;
Annotations:            deployment.kubernetes.io/revision: 1&#13;
Selector:               app=lots&#13;
Replicas:               1000 desired ... | <span class="ent">➊</span> 6 available | 994 unavailable</pre>&#13;
<p class="indent">We’re now down to only six Pods available in the Deployment <span class="ent">➊</span>, as one Pod was evicted. It’s worth noting that being in the <span class="literal">Guaranteed</span> QoS class did not prevent this Pod from being evicted. The <span class="literal">Guaranteed</span> QoS class gets priority for evictions caused by node resource usage, but not for eviction caused by the scheduler finding room for a higher-priority Pod.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_321"/>Of course, the ability to specify a higher priority for a Pod, resulting in the eviction of other Pods, is powerful and should be used sparingly. Normal users do not have the ability to create a new PriorityClass, and administrators can apply a quota to limit the use of a PriorityClass in a given Namespace, effectively limiting normal users from creating high-priority Pods.</p>&#13;
<h3 class="h3" id="ch00lev1sec80">Final Thoughts</h3>&#13;
<p class="noindent">Deploying an application to Kubernetes so that it is performant and reliable requires an understanding of the application architecture and of the normal and worst-case load for each component. Kubernetes QoS classes allow us to shape the way that Pods are deployed to nodes to achieve a balance of predictability and efficiency in the use of resources. Additionally, both QoS classes and Pod priorities allow us to provide hints to the Kubernetes cluster so the deployed applications degrade gracefully as the load on the cluster becomes too high.</p>&#13;
<p class="indent">In the next chapter, we’ll bring together the ideas we’ve seen on how to best use the features of a Kubernetes cluster to deploy performant, resilient applications. We’ll also explore how we can monitor those applications and respond automatically to changes in behavior.<span epub:type="pagebreak" id="page_322"/></p>&#13;
</body></html>