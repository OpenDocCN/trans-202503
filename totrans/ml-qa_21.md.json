["```\nTranslate the following German sentences into English:\n\nExample 1:\nGerman: \"Ich liebe Pfannkuchen.\"\nEnglish: \"I love pancakes.\"\n\nExample 2:\nGerman: \"Das Wetter ist heute schoen.\"\nEnglish: \"The weather is nice today.\"\n\nTranslate this sentence:\nGerman: \"Wo ist die naechste Bushaltestelle?\"\n```", "```\nx = EmbeddingLayer(input_ids)\nx = concatenate([soft_prompt_tensor, x],\n                 dim=seq_len)\noutput = model(x)\n```", "```\ndef transformer_block_with_prefix(x):\n ➊ soft_prompt = FullyConnectedLayers(# Prefix\n      soft_prompt)                     # Prefix\n ➋ x = concatenate([soft_prompt, x],  # Prefix\n                     dim=seq_len)      # Prefix\n ➌ residual = x\n    x = SelfAttention(x)\n    x = LayerNorm(x + residual)\n    residual = x\n    x = FullyConnectedLayers(x)\n    x = LayerNorm(x + residual)\n    return x\n```", "```\ndef transformer_block_with_adapter(x):\n    residual = x\n    x = SelfAttention(x)\n    x = FullyConnectedLayers(x)  # Adapter\n    x = LayerNorm(x + residual)\n    residual = x\n    x = FullyConnectedLayers(x)\n    x = FullyConnectedLayers(x)  # Adapter\n    x = LayerNorm(x + residual)\n    return x\n```", "```\ndef lora_forward_matmul(x):\n    h = x . W  # Regular matrix multiplication\n    h += x . (W_A . W_B) * scalar\n    return h\n```"]