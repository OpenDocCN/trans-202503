- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Geometric Structure of Data
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: You might wonder why you need data science methods rooted in topology and geometry
    when traditional data science methods are already so popular and powerful. The
    answer to this has two parts. First, data today comes in a variety of formats
    far more exotic than the usual spreadsheet, such as a social network or a text
    document. While such exotic data used to be referred to as *unstructured*, we
    now recognize that it often is structured, but with a more sophisticated geometric
    structure than a series of spreadsheets in a relational database. Topological
    and geometric data science allow us to work directly in these exotic realms and
    translate them into the more familiar realm of spreadsheets. Second, a relatively
    recent discovery suggests that geometry even lurks behind spreadsheet-structured
    data. With topological and geometric data science, we can harness the power of
    this hidden geometry.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start this chapter with a quick refresher of the main concepts in traditional
    machine learning, discussing what it means for data to be structured and how this
    is typically used by machine learning algorithms. We’ll then review supervised
    learning, overfitting, and the curse of dimensionality from a geometric perspective.
    Next, we’ll preview a few other common types of data—network data, image data,
    and text data—and hint at how we can use their geometry in machine learning. If
    you’re already familiar with traditional machine learning and the challenges of
    applying it to modern forms of data, you’re welcome to skip to [Chapter 2](c02.xhtml),
    where the technical content officially begins, although you may find the geometric
    perspectives of traditional machine learning topics offered in this chapter interesting
    regardless.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning Categories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many types of machine learning algorithms exist, and more are invented every
    day. It can be hard to keep track of all the latest developments, but it helps
    to think of machine learning algorithms as falling into a few basic categories.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Supervised learning* algorithms generally aim to predict something, perhaps
    a treatment outcome under a new hospital protocol or the probability of a client
    leaving in the next six months. The variable we predict is called the *dependent
    variable* or *target*, and the variables used to predict it are called *independent
    variables* or *predictors*. When we’re predicting a numerical variable (such as
    a symptom severity scale), this is called *regression*; when we’re predicting
    a categorical variable (such as survived or died classes), this is called *classification*.'
  prefs: []
  type: TYPE_NORMAL
- en: Some of the most popular supervised learning algorithms are *k*-nearest neighbors
    (*k*-NN), naive Bayes classifiers, support vector machines, random forests, gradient
    boosting, and neural networks. You don’t need to know any of these topics to read
    this book, but it will help to be familiar with at least one regression method
    and one classification method, such as linear regression and logistic regression.
    That said, don’t fret if you’re not sure about them—this chapter will cover the
    concepts you need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each supervised learning algorithm is a specific type of function that has
    as many input variables as there are independent variables in the data. We think
    of this function as predicting the value of the dependent variable for any choice
    of values of the independent variables (see [Figure 1-1](#figure1-1)). For linear
    regression with independent variables *x*[1], *x*[2], . . . , *x*[*n*], this is
    a linear function: *f*(*x*[1], *x*[2], . . . , *x*[*n*]) = *a*[1]*x*[1] + *a*[2]
    *x*[2] + . . . + **. For other methods, it is a complicated nonlinear function.
    For many methods, this function is nonparametric, meaning we can compute it algorithmically
    but can’t write it down with a formula.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](image_fi/503083c01/f01001.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1-1: The flow of independent variables, supervised machine learning
    algorithm (viewed as a function), and prediction of a dependent variable'
  prefs: []
  type: TYPE_NORMAL
- en: Using a supervised learning algorithm usually involves splitting your data into
    two sets. There’s *training* data, where the algorithm tries to adjust the parameters
    in the function so that the predicted values are as close as possible to the actual
    values. In the previous linear regression example, the parameters are the coefficients
    *a*[*i*]. There’s also *testing* data, where we measure how good a job the algorithm
    is doing. A *hyperparameter* is any parameter that the user must specify (as opposed
    to the parameters that are learned directly from the data during the training
    process). The *k* specifying number of nearest neighbors in *k*-NN is an example
    of a hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: After a supervised learning algorithm has been trained, we can use it to make
    new predictions and to estimate the impact of each independent variable on the
    dependent variable (called *feature importance*). Feature importance is helpful
    for making intervention decisions. For instance, knowing which factors most influence
    patient death from an infectious disease can inform vaccination strategies when
    vaccine supplies are limited.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Unsupervised learning* algorithms tend to focus on data exploration—for example,
    reducing the dimensionality of a dataset to better visualize it, finding how the
    data points are related to each other, or detecting anomalous data points. In
    unsupervised learning, there is no dependent variable—just independent variables.
    Accordingly, we don’t split the data into training and testing sets; we just apply
    unsupervised methods to all the data. Applications of unsupervised learning include
    market segmentation and ancestry group visualization based on millions of genetic
    markers. Examples of unsupervised learning algorithms include k-means clustering,
    hierarchical clustering, and principal component analysis (PCA)—but again, you
    don’t need to know any of these topics to read this book.'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised and supervised learning can be productively combined. For example,
    you might use unsupervised learning to find new questions about the data and use
    supervised learning to answer them. You might use unsupervised learning for dimension
    reduction as a preprocessing step to improve the performance of a supervised learning
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Matching Algorithms and Other Machine Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another common application of machine learning involves *matching algorithms*,
    which compute the distance between points to find similar individuals within a
    dataset. These algorithms are commonly used to recommend a product to a user;
    they’re also used in data integrity checks to make sure that sufficient data exists
    on certain populations before allowing a machine learning algorithm to create
    a model.
  prefs: []
  type: TYPE_NORMAL
- en: Data integrity is increasingly important with the rise of machine learning and
    artificial intelligence. If important subgroups within a population aren’t captured
    well in the data, the algorithm will bias itself toward the majority groups. For
    example, if language recognition systems don’t have sufficient data from African
    or Asian language groups, it’s difficult for the system to learn human speech
    sounds unique to those language groups, such as Khoisan clicks or Mandarin tones.
    Matching algorithms are also used to try to tease out cause and effect from empirical
    evidence when a randomized controlled trial is not possible because they allow
    us to pair up similar participants as though they had been assigned to a treatment
    group and a placebo group.
  prefs: []
  type: TYPE_NORMAL
- en: We could mention many other types of algorithms, and, in practice, there tends
    to be overlap between algorithm categories. For instance, YouTube’s recommendation
    algorithm uses *deep learning* (which involves machine learning based on neural
    networks with multiple “hidden layers”) in both a supervised and an unsupervised
    way, as well as matching algorithms and another pillar of machine learning called
    *reinforcement learning* (where algorithms develop strategies on their own by
    exploring a real or simulated environment—beyond the reach of this book). However,
    the basic road map to machine learning provided earlier will guide us throughout
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s take a closer look at the format of the data these algorithms are
    expecting.
  prefs: []
  type: TYPE_NORMAL
- en: Structured Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning algorithms, and data science and statistical methods more generally,
    typically operate on *structured data* (also called *tabular data*), which means
    a spreadsheet-type object (a data frame or matrix) in which the columns are the
    variables and the rows are the *data points* (also called *instances* or *observations*).
    These are usually stored in a relational database along with other structured
    data. The tabular structure is what allows us to talk about independent variables,
    dependent variables, and data points. A big focus of this book is how to deal
    with data that doesn’t come presented in this nice format. But even with tabular
    data, a geometric perspective can be quite useful.
  prefs: []
  type: TYPE_NORMAL
- en: To start, let’s dive into an example that will show how geometry can help us
    better understand and work with categorical variables in structured data.
  prefs: []
  type: TYPE_NORMAL
- en: The Geometry of Dummy Variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Figure 1-2](#figure1-2) shows part of a spreadsheet stored as a Microsoft
    Excel workbook. The last column here is Outcome, so we view that as the dependent
    variable; the other three columns are the independent variables. If we used this
    data for a supervised learning algorithm, it would be a regression task (since
    the dependent variable is numerical). The first independent variable is binary
    numerical (taking values 0 and 1), the second independent variable is discrete
    numerical (taking whole-number values), and the third independent variable is
    categorical (with three categories of gender). Some algorithms accept categorical
    variables, whereas others require all variables to be numerical. Unless the categories
    are ordered (such as survey data with values such as “very dissatisfied,” “dissatisfied,”
    “satisfied,” and “very satisfied”), the way to convert a categorical variable
    to numerical is to replace it with a collection of binary *dummy variables*, which
    encode each category in a yes/no format represented by the values 1 and 0\. In
    [Figure 1-3](#figure1-3), we’ve replaced the gender variable column with two dummy
    variable columns.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c01/f01002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-2: An example of structured data in a Microsoft Excel workbook'
  prefs: []
  type: TYPE_NORMAL
- en: Even for such a common and simple process as this, geometric considerations
    help illuminate what is going on. If the values of a categorical variable are
    ordered, then we can convert them to a single numerical variable by placing the
    values along a one-dimensional axis in a way that reflects the order of these
    values. For example, the survey values “satisfied,” “very satisfied,” and “extremely
    satisfied” could be coded as 1, 2, and 3, or if you wanted the difference between
    “satisfied” and “very satisfied” to be smaller than the difference between “very
    satisfied” and “extremely satisfied,” then you could code these as, say, 1, 2,
    and 4.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c01/f01003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-3: A transformed, structured dataset in which the categorical variable
    has been replaced with two binary dummy variables'
  prefs: []
  type: TYPE_NORMAL
- en: If the categories are not ordered—such as Male, Female, and Non-Binary—we wouldn’t
    want to force them all into one dimension because that would artificially impose
    an order on them and would make some of them closer than others (see [Figure 1-4](#figure1-4)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c01/f01004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-4: Placing the values of a categorical variable in a single dimension
    makes some of them closer than others.'
  prefs: []
  type: TYPE_NORMAL
- en: Geometrically, we are creating new axes for the different categories when we
    create dummy variables. There are two ways of doing this. Sometimes, you’ll see
    people use one dummy variable for each value of the categorical variable, whereas
    at other times you’ll see people use dummy variables for all but one of the values
    (as we did in [Figure 1-3](#figure1-3)). To understand the difference, let’s take
    a look at our three-category gender variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using three dummy variables places the categories as the vertices of an equilateral
    triangle: Male has coordinates (1,0,0), Female has coordinates (0,1,0), and Non-Binary
    has coordinates (0,0,1). This ensures the categories are all at the same distance
    from each other. Using only two dummy variables means Male has coordinates (1,0),
    Female has coordinates (0,1), and Non-Binary has coordinates (0,0). This projects
    our equilateral triangle in three-dimensional space down to a right triangle in
    the two-dimensional plane, and in doing so it distorts the distances. Male and
    Female are now closer to Non-Binary than they are to each other, because they
    are separated by the length √2 ≈ 1.4 hypotenuse in this isosceles right triangle
    with side lengths 1 (see [Figure 1-5](#figure1-5)). Consequently, some machine
    learning algorithms would mistakenly believe the categories Male and Female are
    more similar to the category Non-Binary than they are to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c01/f01005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-5: Two approaches to creating gender dummy variables. On the left,
    we have one axis for each category, which ensures the categories are equidistant.
    On the right, we use only two axes, which causes some categories to be closer
    than others.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So why are both dummy variable methods used? Using *n* dummy variables for
    an *n*-value categorical variable rather than *n* – 1 leads to *multicollinearity*,
    which in statistical language is a correlation among the independent variables.
    The correlation here is that each of the dummy variables is completely and linearly
    determined by the others. Algebraically, this is a *linear dependence*, which
    means one column is a linear combination of the other columns. This linear dependence
    can be seen geometrically: when placing the *n* categories in *n*-dimensional
    space, they span an (*n* – 1)-dimensional plane only. In [Figure 1-5](#figure1-5),
    linear combinations of the three vectors on the left span only the plane containing
    the triangle, whereas on the right the linear combinations span the full two-dimensional
    coordinate system.'
  prefs: []
  type: TYPE_NORMAL
- en: Multicollinearity causes computational problems for linear and logistic regression,
    so for those algorithms, we should use *n* – 1 dummy variables rather than all
    *n*. Even for methods that don’t run into this specific computational issue, using
    fewer independent variables when possible is generally better because this helps
    reduce the curse of dimensionality—a fundamental topic in data science that we’ll
    visit from a geometric perspective shortly.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, for algorithms like *k*-NN, where distances between data
    points are crucial, we don’t want to drop one of the dummy variables, as that
    would skew the distances (as we saw in [Figure 1-5](#figure1-5)) and lead to subpar
    performance. There is a time and a place for both dummy variable methods, and
    thinking geometrically can help us decide which to use when.
  prefs: []
  type: TYPE_NORMAL
- en: After using dummy variables to convert all categorical variables to numerical
    variables, we are ready to consider the geometry of the spreadsheet.
  prefs: []
  type: TYPE_NORMAL
- en: The Geometry of Numerical Spreadsheets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can think of a numerical spreadsheet as describing a collection of points
    (one for each row) in a *Euclidean vector space*, **R**^(*d*), which is a geometric
    space that looks like a flat piece of paper in two dimensions and a solid brick
    in three dimensions but extends infinitely in all directions and can be any dimension.
    Here, *d* is the number of columns, which is also the dimension of the space.
    Each column in the numerical dataset represents an axis in this space. Concretely,
    the *d*-dimensional coordinates of each data point are simply the values in that
    row.
  prefs: []
  type: TYPE_NORMAL
- en: When *d* = 1, this Euclidean vector space **R**^(*d*) is a line. When *d* =
    2, it is a plane. When *d* = 3, it is the usual three-dimensional space we are
    used to thinking in. While humans can’t really visualize more than three perpendicular
    axes, higher-dimensional geometry can be analyzed with mathematical and computational
    tools regardless. It is important to recognize here that just as there are many
    two-dimensional shapes beyond a flat plane (for instance, the surface of a sphere
    or of a donut, or even stranger ones like a Möbius strip), there are many three-dimensional
    geometric spaces beyond the familiar Euclidean space (such as the inside of a
    sphere in three dimensions or the surface of a sphere in four dimensions). This
    holds for higher-dimensional spaces as well. Working with structured data has
    traditionally meant viewing the data from the perspective of **R**^(*d*) rather
    than any of these other kinds of geometric spaces.
  prefs: []
  type: TYPE_NORMAL
- en: The Euclidean vector space structure of **R**^(*d*) is powerful; it allows us
    to compute all kinds of useful things. We can compute the distance between any
    pair of data points, which is necessary for a wide range of machine learning algorithms.
    We can compute the line segment connecting any two points, which is used by the
    Synthetic Minority Oversampling Technique (SMOTE) to adjust training samples with
    imbalanced classes. We can compute the mean of each coordinate in the data, which
    is helpful for *imputing* missing values (that is, filling in missing values with
    best guesses for the true value based on known data).
  prefs: []
  type: TYPE_NORMAL
- en: However, this nice Euclidean vector space structure is also specific and rigid.
    Thankfully, we can compute distances between points, shortest paths connecting
    points, and various forms of interpolation in much more general settings where
    we don’t have global Euclidean coordinates, including *manifolds* (geometric objects
    like spheres that when zoomed in look like the usual Euclidean space but globally
    can have much more interesting shape and structure—to come in [Chapter 5](c05.xhtml))
    and *networks* (relational structures formally introduced in [Chapter 2](c02.xhtml)).
  prefs: []
  type: TYPE_NORMAL
- en: As a concrete example, suppose you are working with large-scale geospatial data,
    such as ZIP code–based crime statistics. How do you cluster data points or make
    any kind of predictive model? The most straightforward approach is to use latitude
    and longitude as variables to convey the geospatial aspect of the data. But problems
    quickly arise because this approach projects the round Earth down to a flat plane
    in a way that distorts the distances quite significantly. For instance, longitude
    ranges from –180° to +180°, so two points on opposite sides of the prime meridian
    could be very close to each other in terms of miles but extremely far from each
    other in terms of longitudes (see [Figure 1-6](#figure1-6)). It’s helpful to have
    machine learning algorithms that can work on spherical data without the need to
    map data onto a flat surface.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c01/f01006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-6: Using latitude and longitude (left) as variables for geospatial
    data distorts distances between data points. Shown here, very near points on opposite
    sides of the prime meridian are represented by very far points in the longitude-latitude
    plane (right).'
  prefs: []
  type: TYPE_NORMAL
- en: Even when you are working with data that is already structured as a tabular
    spreadsheet, there might be hidden geometry that is relevant. For example, imagine
    you have three numerical variables (so that your data lives in **R**³) but all
    your data points live on or near the surface of a sphere in this three-dimensional
    space. Would you want to consider distances between points as the path lengths
    along the sphere’s surface (which is what is done in spherical geometry) or straight-line
    distances that cut through the sphere (which is what is done with traditional
    Euclidean machine learning)? The answer depends on the context and is something
    that data scientists must decide based on domain knowledge—it is not typically
    something that an algorithm should decide on its own. For example, if your data
    points represent locations in a room that an aerial drone could visit, then Euclidean
    distance is better; if your data points represent airports across the globe that
    an international airline services, then spherical geometry is better.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the main tasks of topological and geometric data science is discovering
    the geometric objects on which your data points naturally live (like the sphere
    in the airport example, but perhaps very complex shapes in high dimensions). The
    other main task is exploiting these geometric objects, which usually involves
    one or more of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying versions of the usual machine learning algorithms that have been adapted
    to more general geometric settings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying new geometrically powered algorithms that are based on the shape of
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing meaningful global coordinates to transform your data into a structured
    spreadsheet in a way that traditional statistical and machine learning tools can
    be successfully applied
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main goal of this book is to carefully go through all these ideas and show
    you how to implement them easily and effectively. But first, in the remainder
    of this introductory chapter, we’ll explain some of the geometry involved in a
    few traditional data science topics (like we did earlier with dummy variables).
    We’ll also hint at the geometry involved in a few different types of “unstructured”
    data, as a preview of what’s to come later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: The Geometry of Supervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we’ll provide a geometric view of a few standard machine learning
    topics: classification, regression, overfitting, and the curse of dimensionality.'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once a dataset has been converted to a numerical spreadsheet (with *d* columns,
    let’s say), the job of a supervised classifier is to label the predicted class
    of each new input data point. This can be viewed in terms of *decision boundaries*,
    which means we carve up the space **R**^(*d*) into nonoverlapping regions and
    assign a class to each region, indicating the label that the classifier will predict
    for all points in the region. (Note that the same class can be assigned to multiple
    regions.) The type of geometric shapes that are allowed for the regions is determined
    by the choice of supervised classifier method, while the particular details of
    these shapes are learned from the data in the training process. This provides
    an illuminating geometric window into the classification process.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 1-7](#figure1-7), we see decision boundaries for a few standard classification
    algorithms in a simple binary classification example when *d* = 2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c01/f01007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-7: The decision boundaries in two dimensions for a few classification
    algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression produces linear decision boundaries. (Though, by adding
    higher-order terms to the model, you can achieve nonlinear decision boundaries.)
    Decision trees are built by splitting the independent variables with individual
    inequalities, which results in decision boundaries made up of horizontal and vertical
    line segments. In higher dimensions, instead of horizontal and vertical lines,
    we have planes that are aligned with the coordinate axes. Random forests, which
    are ensembles of decision trees, still produce decision boundaries of this form,
    but they tend to involve many more pieces, producing curved-looking shapes that
    are really made out of lots of small horizontal and vertical segments. The *k*-NN
    classifiers produce polygonal decision boundaries since they carve up the space
    based on which of the finitely many training data points are closest. Neural networks
    can produce complex, curving decision boundaries; this high level of flexibility
    is both a blessing and a curse because it can lead to overfitting if you’re not
    careful (we’ll discuss overfitting shortly).
  prefs: []
  type: TYPE_NORMAL
- en: Studying the decision boundaries produced by different classifier algorithms
    can help you better understand how each algorithm works; it can also help you
    choose which algorithm to use based on how your data looks (for higher-dimensional
    data, where *d* > 2, you can get two-dimensional snapshots of the data by plotting
    different pairs of variables). Just remember that there are many choices involved—which
    variables to use, whether to include higher-order terms, what values to set for
    the hyperparameters, and so on—and all of these choices influence the types of
    decision boundaries that are possible. Whenever you encounter a classification
    algorithm that you aren’t familiar with yet, one of the best ways to develop an
    intuition for it is to plot the decision boundaries it produces and see how they
    vary as you adjust the hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Supervised regression can also be viewed geometrically, though it is a little
    harder to visualize. Rather than carving up space into finitely many regions based
    on the class predictions, regression algorithms assign a numerical value to each
    point in the space; when *d* = 2, this can be plotted as a heatmap or a three-dimensional
    surface. [Figure 1-8](#figure1-8) shows an example of this, where we first create
    10 random points with random dependent variable values (shown in the top plot
    with the circle size indicating the value), then we 3D scatterplot the predicted
    values for a dense grid of points, and finally we shade according to height when
    using *k*-NN with *k* = 3 (bottom left) and a random forest (bottom right).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c01/f01008_m.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-8: The training data with dependent variable values indicated by circle
    size (top plot), and the three-dimensional prediction surfaces for two nonlinear
    regression algorithms: 3-NN (bottom left) and random forest (bottom right)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prediction surface for linear regression (not shown here) is one large,
    slanted plane, whereas for the two methods illustrated here the surface is a collection
    of finitely many flat regions for which the prediction value remains constant.
    Notice that these regions are polygons for *k*-NN and rectangles for the random
    forest; this will always be the case. Also, for the particular choice of hyperparameters
    used in this example, the regions here are smaller for the random forest than
    for *k*-NN. Put another way, the random forest here slices up the data space with
    surgical precision compared to the *k*-NN algorithm; the latter is more like carving
    a pumpkin with a butcher’s knife. But this will not always be the case—this comparison
    of coarseness depends on the number of trees used in the random forest and the
    number of neighbors used in *k*-NN. Importantly, a finer partition for regression
    is like a more flexible decision boundary for classification: it often looks good
    on the training data but then generalizes poorly to new data. This brings us to
    our next topic.'
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s return to the decision boundary plots in [Figure 1-7](#figure1-7). At
    first glance, it would seem that the more flexible the boundaries are, the better
    the algorithm performs. This is true when considering the training data, but what
    really matters is how well algorithms perform on test data. A well-known issue
    in predictive analytics is *overfitting*, which is when a predictive algorithm
    is so flexible that it learns the particular details of the training data and,
    in doing so, will actually end up performing worse on new unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 1-7](#figure1-7), the logistic regression algorithm misclassifies
    the leftmost circle, whereas the decision tree creates a long sliver-shaped region
    to correctly classify that single point. If circles tend to fall on the left and
    pluses on the right, then it’s quite likely that creating this sliver region will
    hurt the classification performance overall when it is used on new data—and if
    so, this is an example of the decision tree overfitting the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, as a predictive algorithm’s flexibility increases, the training
    error tends to keep decreasing until it eventually stabilizes, whereas the test
    error tends to decrease at first and then reaches a minimum and then increases
    (see [Figure 1-9](#figure1-9)). We want the bottom of the test error curve: that’s
    the best predictive performance we’re able to achieve. It occurs when the algorithm
    is flexible enough to fit the true shape of the data distribution but rigid enough
    that it doesn’t learn spurious details specific to the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c01/f01009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-9: A plot of training error versus test error as a function of a predictive
    algorithm’s flexibility, illustrating the concept of overfitting'
  prefs: []
  type: TYPE_NORMAL
- en: 'The general behavior illustrated in [Figure 1-9](#figure1-9) often occurs when
    varying hyperparameters: the classifier decision boundaries become more flexible
    as the number of neurons in a neural network increases, as the number of neighbors
    *k* in *k*-NN decreases, as the number of branches in a decision tree increases,
    and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: The Curse of Dimensionality
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes, it helps to think of the x-axis in [Figure 1-9](#figure1-9) as indicating
    complexity rather than flexibility. More flexible algorithms tend to be more complex,
    and vice versa. One of the simplest yet most important measures of the complexity
    of an algorithm is the number of independent variables it uses. This is also called
    the *dimensionality* of the data. If the number of independent variables is *d*,
    then we can think of the algorithm as inputting points in a *d*-dimensional Euclidean
    vector space **R**^(*d*). For a fixed number of data points, using too few independent
    variables tends to cause underfitting, whereas using too many tends to cause overfitting.
    Thus, [Figure 1-9](#figure1-9) can also be interpreted as showing what happens
    to a predictive algorithm’s error scores as the dimensionality of the data increases
    without increasing the size of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This eventual increase in test error as dimensionality increases is an instance
    of a general phenomenon known as the *curse of dimensionality*. When dealing with
    structured data where the number of columns is large relative to the number of
    rows (a common situation in genomics, among other areas), overfitting is likely
    for predictive algorithms, and the numerical linear algebra driving many machine
    learning methods breaks down. This is an enormous problem, and many techniques
    have been developed to help counteract it—some of which will come up later in
    this book. For now, let’s see how geometry can shed some light on the curse of
    dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: One way to understand the curse of dimensionality is to think of Euclidean distances,
    meaning straight-line distance as the bird flies in however many dimensions exist.
    Imagine two pairs of points drawn on a square sheet of paper, where the points
    in one pair are near each other and the points in the other pair are far from
    each other, as in [Figure 1-10](#figure1-10).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c01/f01010r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-10: A plot of two pairs of points in a two-dimensional space'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s perturb these points a bit by adding some Gaussian noise; that is, we’ll
    draw four vectors from a bivariate normal distribution and add these to the coordinates
    of the four points. Doing this moves the points slightly in random directions.
    Let’s do this many times, and each time we’ll record the Euclidean distance between
    the left pair after perturbation and also between the right pair. If the perturbation
    is large enough, we might occasionally end up with the points on the left farther
    from each other than the points on the right, but, overall, the Euclidean distances
    for the left perturbations will be smaller than those for the right perturbations,
    as we see in the histogram in [Figure 1-11](#figure1-11).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c01/f01011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-11: A histogram of the Euclidean distances after random small perturbations
    for the nearby points on the left side of [Figure 1-10](#figure1-10) (shown in
    light gray) and the faraway points on the right side of that figure (shown in
    dark gray)'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s embed our square sheet of paper as a two-dimensional plane inside
    a higher-dimensional Euclidean space **R**^(*d*) and then repeat this experiment
    of perturbing the points and computing Euclidean distances. In higher dimensions,
    these perturbations take place in more directions. Concretely, you can think of
    this as padding the *x* and *y* coordinates for our points with *d*-2 zeros and
    then adding a small amount of noise to each of the *d* coordinates. [Figure 1-12](#figure1-12)
    shows the resulting histograms of Euclidean distances when doing this process
    for *d* = 10 and *d* = 100.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c01/f01012_m.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-12: Histograms of Euclidean distances as in [Figure 1-11](#figure1-11),
    except after embedding in *d* = 10 dimensions (left) and *d* = 100 dimensions
    (right)'
  prefs: []
  type: TYPE_NORMAL
- en: We see that as the dimension *d* increases, the two distributions come together
    and overlap more and more. Consequently, when there is noise involved (as there
    always is in the real world), adding extra dimensions destroys our ability to
    discern between the close pair of points and the far pair. Put another way, the
    signal in your data will become increasingly lost to the noise as the dimensionality
    of your data increases, unless you are certain that these additional dimensions
    contain additional signal. This is a pretty remarkable insight that we see revealed
    here through relatively simple Euclidean geometry!
  prefs: []
  type: TYPE_NORMAL
- en: We can also use perturbations to see why large dimensionality can lead to overfitting.
    In [Figure 1-13](#figure1-13), on the left, we see four points in the plane **R**²
    labeled by two classes in a configuration that is not linearly separable (meaning
    a logistic regression classifier without higher-order terms won’t be able to correctly
    class all these points).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c01/f01013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-13: Two classes of data points that are not linearly separable in
    two dimensions (left) but are linearly separable after placing them in three dimensions
    (right) and perturbing them there'
  prefs: []
  type: TYPE_NORMAL
- en: Even if we perturb the points with a small amount of noise, there will be no
    line separating the two classes. On the right side of this figure, we have embedded
    these points in **R**³ simply by adding a third coordinate to each point that
    is equal to a constant. (Geometrically, this means we lay the original **R**²
    down flat above the *xy*-plane in this three-dimensional space.) We then perturb
    the points a small amount. After this particular perturbation in three dimensions,
    we see that the two classes do become linearly separable, meaning a logistic regression
    classifier will be able to achieve 100 percent accuracy here. (In the figure,
    we have sketched a slanted plane that separates the two classes.)
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, this additional flexibility seems like a good thing (and sometimes
    it can be!) since it allowed us to increase our training accuracy. But notice
    that our classifier in three dimensions didn’t learn a meaningful way to separate
    the classes in a way likely to generalize on new, unseen data. It really just
    learned the vertical noise from one particular perturbation. In other words, increasing
    the dimensionality of data tends to increase the likelihood that classifiers will
    fit noise in the training data, and this is a recipe for overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a geometric perspective of the computational challenges caused
    by the curse of dimensionality. Imagine a square with a length of 10 units on
    both sides, giving an area of 100 units. If we add another axis, we’ll get a volume
    of 1,000 units. Add another, and we’ll have a four-dimensional cube with a four-dimensional
    volume of 10,000 units. This means data becomes more spread out—sparser—as the
    dimensionality increases. If we take a relatively dense dataset with 100 points
    in a low-dimensional space and place it in a space with 1,000 dimensions, then
    there will be a lot of the space that isn’t near any of those 100 points. Someone
    wandering about in that space looking for points may not find one without a lot
    of effort. If there’s a finite time frame for the person to look, they might not
    find a point within that time frame. Simply put, computations are harder in higher
    dimensions because there are more coordinates to keep track of and data points
    are harder to find.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapters, we’ll look at a few different ways to wrangle high-dimensional
    data through geometry, including ways to reduce the data’s dimensionality, create
    algorithms that model the data geometry explicitly to fit models, and calculate
    distances in ways that work better than Euclidean distance in high-dimensional
    spaces. The branch of machine learning algorithms designed to handle high-dimensional
    data is still growing thanks to subjects like genomics and proteomics, where datasets
    typically have millions or billions of independent variables. It is said that
    necessity is the mother of invention, and indeed many machine learning methods
    have been invented out of the necessity of dealing with high-dimensional real-world
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most of the data that exists today does not naturally live in a spreadsheet
    format. Examples include text data, network data, image data, and even video or
    sound clip data. Each of these formats comes with its own geometry and analytic
    challenges. Let’s start exploring some of these types of unstructured data and
    see how geometry can help us understand and model the data.
  prefs: []
  type: TYPE_NORMAL
- en: Network Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the next chapter, you’ll get the official definitions related to networks,
    but you might already have a sense of networks from dealing with social media.
    Facebook friendships form an undirected network (nodes as Facebook users and edges
    as friendships among them), and Twitter accounts form a directed network (directional
    edges because you have both followers and accounts you follow). There is nothing
    inherently Euclidean or spreadsheet structured about network data. In recent years,
    deep learning has been extended from the usual Euclidean spreadsheet setting to
    something much more general called *Riemannian manifolds* (which we’ll get to
    in [Chapter 5](c05.xhtml)); the main application of this generalization (called
    *geometric deep learning*) has been network data, especially for social media
    analytics.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, Facebook uses geometric deep learning algorithms to automatically
    detect fake “bot” accounts. In addition to looking at traditional structured data
    associated with each account such as demographics and number of friends, these
    detection algorithms use the rich non-Euclidean geometry of each account’s network
    of friends. Intuitively speaking, it’s easy to create fake accounts that have
    realistic-looking interests and numbers of friends, but it’s hard to do this in
    a way such that these accounts’ friendship networks are structured similarly to
    the organic friendship networks formed by real people. Network geometry provides
    ways of measuring this notion of “similarity.”
  prefs: []
  type: TYPE_NORMAL
- en: Geometric deep learning has also been used to detect fake news on Twitter by
    transforming the detailed propagation patterns of stories through the network
    into independent variables for a supervised learning algorithm. We won’t get to
    geometric deep learning in this book, but there is still plenty to do and say
    when it comes to working with network data. For example, we can use geometric
    properties of a network to extract numerical variables that bring network data
    back into the familiar territory of structured data.
  prefs: []
  type: TYPE_NORMAL
- en: Image Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another form of “unstructured” data that actually has a rich geometric structure
    is image data. You can think of each pixel in an image as a numerical variable
    if the image is grayscale or as three variables if it is color (red, green, and
    blue values). We can then try to use these variables to cluster images with an
    unsupervised algorithm or classify them with a supervised algorithm. But the problem
    when doing this is that there is no spatial awareness. A pair of adjacent pixels
    is treated the same as a pair of pixels on opposite sides of the image. A large
    branch of deep learning, called *convolutional neural networks* (CNNs), has been
    developed to bring spatial awareness into the picture. CNNs create new variables
    from the pixel values by sliding small windows around the image. Success in this
    realm is largely what brought widespread public acclaim to deep learning, as CNNs
    smashed all the previous records in image recognition and classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a simple case of two images that could be included in a larger
    animal classification dataset used in conservation efforts (see [Figure 1-14](#figure1-14)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c01/f01014_m.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-14: An elephant (left) and a lioness (right) at Kruger National Park'
  prefs: []
  type: TYPE_NORMAL
- en: The animals are shown in natural environments where leaves, branches, and lighting
    vary. They have different resolutions. The colors of each animal vary. The extant
    shapes related to both the animals and the other stuff near the animals differ.
    Manually deriving meaningful independent variables to classify these images would
    be difficult. Thankfully, CNNs are built to handle such image data and to automatically
    create useful independent variables.
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea is to consider each image as a mathematical surface (see [Figure
    1-15](#figure1-15)) and then walk across this surface creating a map of its salient
    features—peaks, valleys, and other relevant geometric occurrences. The next layer
    in the CNN walks across this map and creates a map of its salient features, which
    is then fed to the next layer, and so on. In the end, the CNN converts each image
    to a sequence of maps that hierarchically encode the image’s content, with the
    final layer being the map that is actually used for classification. For these
    animal images, the first map might identify high-contrast regions in the image.
    The next map might assemble these regions into outlines of shapes. The following
    map might indicate which of these shapes are animals. Another layer might locate
    specific anatomical features within the animals—and these anatomical features
    could then form the basis for the final species classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The precise way the CNN builds these maps is learned internally through the
    supervised training process: as the algorithm is fed labeled data, connections
    between neurons in each layer forge, break, and forge again until the final layer
    is as helpful as possible for the classification task. We’ll further explore CNNs
    and their quantum versions in [Chapter 10](c10.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c01/f01015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-15: The head of the lioness in [Figure 1-14](#figure1-14), viewed
    geometrically as a 3D mathematical surface'
  prefs: []
  type: TYPE_NORMAL
- en: Using methods from computational geometry to quantify peaks and valleys has
    applications beyond image recognition and classification. A scientist might want
    to understand the dynamic process or structure of a scientific phenomenon, such
    as the flow of water or light on an object. The peaks, valleys, and contours of
    the object impact how light will scatter when it hits the object, and they’ll
    also determine how liquids would flow down the object. We’ll cover how to mine
    data for relevant peaks, valleys, and contours later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Text Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another form of “unstructured” data that has risen to prominence in recent years
    is text data. Here, the structure that comes with the data is not spatial like
    it is for images; it’s linguistic. State-of-the-art text processing (for instance,
    used by Google to process search phrases or by Facebook and Twitter to detect
    posts that violate platform policies) harnesses deep learning to create something
    called *vector embeddings*, which translate text into **Rd, where each word or
    sentence is represented as a point in a Euclidean vector space. The coordinates
    of each word or sentence are learned from data by reading vast amounts of text,
    and the deep learning algorithm chooses them in a way that in essence translates
    linguistic meaning into geometric meaning. We’ll explore deep learning text embeddings
    in [Chapter 9](c09.xhtml).**
  prefs: []
  type: TYPE_NORMAL
- en: '**For example, we might want to visualize different sets of variables concerning
    text documents. Because the variables form a high-dimensional space, we can’t
    plot them in a way that humans can visualize. In later chapters, we’ll learn about
    geometric ways to map high-dimensional data into lower-dimensional spaces such
    that the data can be visualized easily in a plot. We can decorate these plots
    with colors or different shapes based on the document type or other relevant document
    properties. If similar documents cluster together in these plots, it’s likely
    that some of the variables involved will help us distinguish between documents.
    New documents with unknown properties but measured values for these variables
    can then be grouped by a classification algorithm. We’ll explore this further
    in [Chapter 9](c09.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter provided a brief review of the main concepts of traditional machine
    learning, but it put a geometric spin on these concepts that will likely be new
    for most readers. Woven into this review was a discussion of what it means for
    data to be structured. The main takeaway is that essentially all data has meaningful
    structure, but this structure is often of a geometric nature, and geometric tools
    are needed to put the data into a more traditional spreadsheet format. This is
    a theme we’ll develop in much more depth throughout the book. This chapter also
    hinted at some of the important geometry hidden in spreadsheet data. One of the
    main goals of the book is to show how to use this hidden geometry to improve the
    performance of machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start in Chapters [2](c02.xhtml) and [3](c03.xhtml) by diving into algorithms
    designed for analyzing network data, including social and geographic networks.
    This includes local and global metrics to understand network structure and the
    role of individuals in the network, clustering methods developed for use on network
    data, link prediction algorithms to suggest new edges in a network, and tools
    for understanding how processes or epidemics spread through networks.****
  prefs: []
  type: TYPE_NORMAL
