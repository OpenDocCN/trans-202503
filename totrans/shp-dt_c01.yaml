- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Geometric Structure of Data
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: You might wonder why you need data science methods rooted in topology and geometry
    when traditional data science methods are already so popular and powerful. The
    answer to this has two parts. First, data today comes in a variety of formats
    far more exotic than the usual spreadsheet, such as a social network or a text
    document. While such exotic data used to be referred to as *unstructured*, we
    now recognize that it often is structured, but with a more sophisticated geometric
    structure than a series of spreadsheets in a relational database. Topological
    and geometric data science allow us to work directly in these exotic realms and
    translate them into the more familiar realm of spreadsheets. Second, a relatively
    recent discovery suggests that geometry even lurks behind spreadsheet-structured
    data. With topological and geometric data science, we can harness the power of
    this hidden geometry.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start this chapter with a quick refresher of the main concepts in traditional
    machine learning, discussing what it means for data to be structured and how this
    is typically used by machine learning algorithms. We’ll then review supervised
    learning, overfitting, and the curse of dimensionality from a geometric perspective.
    Next, we’ll preview a few other common types of data—network data, image data,
    and text data—and hint at how we can use their geometry in machine learning. If
    you’re already familiar with traditional machine learning and the challenges of
    applying it to modern forms of data, you’re welcome to skip to [Chapter 2](c02.xhtml),
    where the technical content officially begins, although you may find the geometric
    perspectives of traditional machine learning topics offered in this chapter interesting
    regardless.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning Categories
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many types of machine learning algorithms exist, and more are invented every
    day. It can be hard to keep track of all the latest developments, but it helps
    to think of machine learning algorithms as falling into a few basic categories.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Learning
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Supervised learning* algorithms generally aim to predict something, perhaps
    a treatment outcome under a new hospital protocol or the probability of a client
    leaving in the next six months. The variable we predict is called the *dependent
    variable* or *target*, and the variables used to predict it are called *independent
    variables* or *predictors*. When we’re predicting a numerical variable (such as
    a symptom severity scale), this is called *regression*; when we’re predicting
    a categorical variable (such as survived or died classes), this is called *classification*.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Some of the most popular supervised learning algorithms are *k*-nearest neighbors
    (*k*-NN), naive Bayes classifiers, support vector machines, random forests, gradient
    boosting, and neural networks. You don’t need to know any of these topics to read
    this book, but it will help to be familiar with at least one regression method
    and one classification method, such as linear regression and logistic regression.
    That said, don’t fret if you’re not sure about them—this chapter will cover the
    concepts you need.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最受欢迎的监督学习算法包括*k*最近邻（*k*-NN）、朴素贝叶斯分类器、支持向量机、随机森林、梯度提升和神经网络。阅读本书不需要掌握这些话题，但如果对至少一种回归方法和一种分类方法有所了解（如线性回归和逻辑回归），会有所帮助。话虽如此，如果你不确定这些内容，也不用担心——这一章会涵盖你所需的概念。
- en: 'Each supervised learning algorithm is a specific type of function that has
    as many input variables as there are independent variables in the data. We think
    of this function as predicting the value of the dependent variable for any choice
    of values of the independent variables (see [Figure 1-1](#figure1-1)). For linear
    regression with independent variables *x*[1], *x*[2], . . . , *x*[*n*], this is
    a linear function: *f*(*x*[1], *x*[2], . . . , *x*[*n*]) = *a*[1]*x*[1] + *a*[2]
    *x*[2] + . . . + **. For other methods, it is a complicated nonlinear function.
    For many methods, this function is nonparametric, meaning we can compute it algorithmically
    but can’t write it down with a formula.**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 每种监督学习算法都是一种特定类型的函数，具有与数据中的独立变量数量相同的输入变量。我们将这个函数视为根据独立变量的任何值预测因变量的值（参见[图 1-1](#figure1-1)）。对于具有独立变量*x*[1]、*x*[2]、...、*x*[*n*]的线性回归，这是一个线性函数：*f*(*x*[1]、*x*[2]、...、*x*[*n*])
    = *a*[1]*x*[1] + *a*[2]*x*[2] + ...。对于其他方法，它是一个复杂的非线性函数。对于许多方法，这个函数是非参数化的，这意味着我们可以通过算法计算它，但无法用公式表达它。**
- en: '**![](image_fi/503083c01/f01001.png)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](image_fi/503083c01/f01001.png)**'
- en: 'Figure 1-1: The flow of independent variables, supervised machine learning
    algorithm (viewed as a function), and prediction of a dependent variable'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1-1：独立变量流、监督机器学习算法（视为函数）和因变量预测
- en: Using a supervised learning algorithm usually involves splitting your data into
    two sets. There’s *training* data, where the algorithm tries to adjust the parameters
    in the function so that the predicted values are as close as possible to the actual
    values. In the previous linear regression example, the parameters are the coefficients
    *a*[*i*]. There’s also *testing* data, where we measure how good a job the algorithm
    is doing. A *hyperparameter* is any parameter that the user must specify (as opposed
    to the parameters that are learned directly from the data during the training
    process). The *k* specifying number of nearest neighbors in *k*-NN is an example
    of a hyperparameter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用监督学习算法通常涉及将数据划分为两个集合。一个是*训练*数据，其中算法试图调整函数中的参数，使得预测值尽可能接近实际值。在前面的线性回归示例中，参数是系数*a*[*i*]。另一个是*测试*数据，我们在此测量算法执行的效果。*超参数*是用户必须指定的任何参数（与通过训练过程直接从数据中学习到的参数不同）。*k*表示最近邻数目的超参数在*k*-NN中就是一个例子。
- en: After a supervised learning algorithm has been trained, we can use it to make
    new predictions and to estimate the impact of each independent variable on the
    dependent variable (called *feature importance*). Feature importance is helpful
    for making intervention decisions. For instance, knowing which factors most influence
    patient death from an infectious disease can inform vaccination strategies when
    vaccine supplies are limited.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习算法训练完成后，我们可以使用它来做出新的预测，并估计每个独立变量对因变量的影响（称为*特征重要性*）。特征重要性有助于做出干预决策。例如，了解哪些因素最能影响患者因传染病死亡，可以在疫苗供应有限时为疫苗接种策略提供参考。
- en: Unsupervised Learning
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督学习
- en: '*Unsupervised learning* algorithms tend to focus on data exploration—for example,
    reducing the dimensionality of a dataset to better visualize it, finding how the
    data points are related to each other, or detecting anomalous data points. In
    unsupervised learning, there is no dependent variable—just independent variables.
    Accordingly, we don’t split the data into training and testing sets; we just apply
    unsupervised methods to all the data. Applications of unsupervised learning include
    market segmentation and ancestry group visualization based on millions of genetic
    markers. Examples of unsupervised learning algorithms include k-means clustering,
    hierarchical clustering, and principal component analysis (PCA)—but again, you
    don’t need to know any of these topics to read this book.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*无监督学习*算法通常专注于数据探索——例如，通过减少数据集的维度来更好地可视化数据，查找数据点之间的关系，或者检测异常数据点。在无监督学习中，没有因变量——只有自变量。因此，我们不会将数据拆分为训练集和测试集；我们只是将无监督方法应用于所有数据。无监督学习的应用包括基于数百万个遗传标记的市场细分和祖先群体可视化。无监督学习算法的例子包括k-means聚类、层次聚类和主成分分析（PCA）——但同样，你不需要了解这些主题就可以阅读本书。'
- en: Unsupervised and supervised learning can be productively combined. For example,
    you might use unsupervised learning to find new questions about the data and use
    supervised learning to answer them. You might use unsupervised learning for dimension
    reduction as a preprocessing step to improve the performance of a supervised learning
    algorithm.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习和监督学习可以有效地结合。例如，你可以使用无监督学习来发现数据中的新问题，并使用监督学习来回答这些问题。你还可以使用无监督学习进行降维，作为预处理步骤，以提高监督学习算法的性能。
- en: Matching Algorithms and Other Machine Learning
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 匹配算法与其他机器学习
- en: Another common application of machine learning involves *matching algorithms*,
    which compute the distance between points to find similar individuals within a
    dataset. These algorithms are commonly used to recommend a product to a user;
    they’re also used in data integrity checks to make sure that sufficient data exists
    on certain populations before allowing a machine learning algorithm to create
    a model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的另一个常见应用是*匹配算法*，它通过计算点之间的距离来查找数据集中相似的个体。这些算法通常用于向用户推荐产品；它们也用于数据完整性检查，以确保在允许机器学习算法创建模型之前，某些人群的数据是充足的。
- en: Data integrity is increasingly important with the rise of machine learning and
    artificial intelligence. If important subgroups within a population aren’t captured
    well in the data, the algorithm will bias itself toward the majority groups. For
    example, if language recognition systems don’t have sufficient data from African
    or Asian language groups, it’s difficult for the system to learn human speech
    sounds unique to those language groups, such as Khoisan clicks or Mandarin tones.
    Matching algorithms are also used to try to tease out cause and effect from empirical
    evidence when a randomized controlled trial is not possible because they allow
    us to pair up similar participants as though they had been assigned to a treatment
    group and a placebo group.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习和人工智能的兴起，数据完整性变得越来越重要。如果数据中未能很好地捕捉到某些重要的子群体，算法就会倾向于偏向多数群体。例如，如果语言识别系统没有来自非洲或亚洲语言群体的足够数据，系统就很难学习到这些语言群体特有的人类语音声音，例如科伊桑语的咔嗒音或普通话的声调。匹配算法也被用来尝试从经验数据中提取因果关系，当随机对照试验不可行时，它们能够将相似的参与者配对，就像他们被分配到治疗组和安慰剂组一样。
- en: We could mention many other types of algorithms, and, in practice, there tends
    to be overlap between algorithm categories. For instance, YouTube’s recommendation
    algorithm uses *deep learning* (which involves machine learning based on neural
    networks with multiple “hidden layers”) in both a supervised and an unsupervised
    way, as well as matching algorithms and another pillar of machine learning called
    *reinforcement learning* (where algorithms develop strategies on their own by
    exploring a real or simulated environment—beyond the reach of this book). However,
    the basic road map to machine learning provided earlier will guide us throughout
    this book.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提及许多其他类型的算法，在实践中，算法类别之间往往存在重叠。例如，YouTube的推荐算法在监督和无监督方式中都使用*深度学习*（这涉及基于具有多个“隐藏层”的神经网络的机器学习），以及匹配算法和机器学习的另一个支柱*强化学习*（其中算法通过探索真实或模拟环境来自行开发策略——超出本书的范围）。然而，前面提供的机器学习基本路线图将指导我们贯穿本书。
- en: Next, let’s take a closer look at the format of the data these algorithms are
    expecting.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们更仔细地看一下这些算法期望的数据格式。
- en: Structured Data
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化数据
- en: Machine learning algorithms, and data science and statistical methods more generally,
    typically operate on *structured data* (also called *tabular data*), which means
    a spreadsheet-type object (a data frame or matrix) in which the columns are the
    variables and the rows are the *data points* (also called *instances* or *observations*).
    These are usually stored in a relational database along with other structured
    data. The tabular structure is what allows us to talk about independent variables,
    dependent variables, and data points. A big focus of this book is how to deal
    with data that doesn’t come presented in this nice format. But even with tabular
    data, a geometric perspective can be quite useful.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法，以及数据科学和统计方法通常在*结构化数据*（也称为*表格数据*）上运行，这意味着一种类似电子表格的对象（数据框或矩阵），其中列是变量，行是*数据点*（也称为*实例*或*观测*）。这些通常存储在关系数据库中，与其他结构化数据一起。表格结构使我们能够讨论自变量、因变量和数据点。本书的重点之一是如何处理不以这种良好格式呈现的数据。但即使是表格数据，几何视角也可能非常有用。
- en: To start, let’s dive into an example that will show how geometry can help us
    better understand and work with categorical variables in structured data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们深入研究一个示例，展示几何如何帮助我们更好地理解和处理结构化数据中的分类变量。
- en: The Geometry of Dummy Variables
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟变量的几何形式
- en: '[Figure 1-2](#figure1-2) shows part of a spreadsheet stored as a Microsoft
    Excel workbook. The last column here is Outcome, so we view that as the dependent
    variable; the other three columns are the independent variables. If we used this
    data for a supervised learning algorithm, it would be a regression task (since
    the dependent variable is numerical). The first independent variable is binary
    numerical (taking values 0 and 1), the second independent variable is discrete
    numerical (taking whole-number values), and the third independent variable is
    categorical (with three categories of gender). Some algorithms accept categorical
    variables, whereas others require all variables to be numerical. Unless the categories
    are ordered (such as survey data with values such as “very dissatisfied,” “dissatisfied,”
    “satisfied,” and “very satisfied”), the way to convert a categorical variable
    to numerical is to replace it with a collection of binary *dummy variables*, which
    encode each category in a yes/no format represented by the values 1 and 0\. In
    [Figure 1-3](#figure1-3), we’ve replaced the gender variable column with two dummy
    variable columns.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-2](#figure1-2)显示了存储为Microsoft Excel工作簿的电子表格的一部分。这里的最后一列是Outcome，因此我们将其视为因变量；其他三列是自变量。如果我们将这些数据用于监督学习算法，那将是一个回归任务（因为因变量是数值）。第一个自变量是二进制数值（取值为0和1），第二个自变量是离散数值（取整数值），第三个自变量是分类的（有三个性别类别）。一些算法接受分类变量，而另一些要求所有变量都是数值。除非类别是有序的（例如调查数据，其值为“非常不满意”、“不满意”、“满意”和“非常满意”），将分类变量转换为数值的方法是用一组二进制*虚拟变量*替换它，这些变量用值1和0表示每个类别的是/否格式。在[图1-3](#figure1-3)中，我们用两个虚拟变量列替换了性别变量列。'
- en: '![](image_fi/503083c01/f01002.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c01/f01002.png)'
- en: 'Figure 1-2: An example of structured data in a Microsoft Excel workbook'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图1-2：Microsoft Excel工作簿中结构化数据的示例
- en: Even for such a common and simple process as this, geometric considerations
    help illuminate what is going on. If the values of a categorical variable are
    ordered, then we can convert them to a single numerical variable by placing the
    values along a one-dimensional axis in a way that reflects the order of these
    values. For example, the survey values “satisfied,” “very satisfied,” and “extremely
    satisfied” could be coded as 1, 2, and 3, or if you wanted the difference between
    “satisfied” and “very satisfied” to be smaller than the difference between “very
    satisfied” and “extremely satisfied,” then you could code these as, say, 1, 2,
    and 4.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对于这样一个常见且简单的过程，几何考虑也有助于阐明正在发生的事情。如果分类变量的值是有序的，那么我们可以通过将这些值沿着一个一维轴放置，以反映这些值的顺序，将它们转换为一个单一的数值变量。例如，调查中的“满意”，“非常满意”和“极度满意”可以编码为1、2和3，或者如果你希望“满意”和“非常满意”之间的差异小于“非常满意”和“极度满意”之间的差异，那么你可以将它们编码为1、2和4。
- en: '![](image_fi/503083c01/f01003.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c01/f01003.png)'
- en: 'Figure 1-3: A transformed, structured dataset in which the categorical variable
    has been replaced with two binary dummy variables'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图1-3：一个转换后的结构化数据集，其中分类变量已被替换为两个二进制虚拟变量
- en: If the categories are not ordered—such as Male, Female, and Non-Binary—we wouldn’t
    want to force them all into one dimension because that would artificially impose
    an order on them and would make some of them closer than others (see [Figure 1-4](#figure1-4)).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果类别没有顺序——比如男性、女性和非二元——我们不希望强制将它们全部放入一个维度中，因为这样会人为地对它们进行排序，并使其中一些类别比其他类别更接近（见[图1-4](#figure1-4)）。
- en: '![](image_fi/503083c01/f01004.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c01/f01004.png)'
- en: 'Figure 1-4: Placing the values of a categorical variable in a single dimension
    makes some of them closer than others.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图1-4：将分类变量的值放在一个维度中会使其中一些类别比其他类别更接近。
- en: Geometrically, we are creating new axes for the different categories when we
    create dummy variables. There are two ways of doing this. Sometimes, you’ll see
    people use one dummy variable for each value of the categorical variable, whereas
    at other times you’ll see people use dummy variables for all but one of the values
    (as we did in [Figure 1-3](#figure1-3)). To understand the difference, let’s take
    a look at our three-category gender variable.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 几何上，当我们创建虚拟变量时，我们为不同的类别创建了新的轴。有两种方法可以做到这一点。有时，你会看到人们为每个分类变量的值使用一个虚拟变量，而在其他时候，你会看到人们为除一个值之外的所有值使用虚拟变量（就像我们在[图1-3](#figure1-3)中所做的那样）。为了理解区别，让我们看看我们的三类别性别变量。
- en: 'Using three dummy variables places the categories as the vertices of an equilateral
    triangle: Male has coordinates (1,0,0), Female has coordinates (0,1,0), and Non-Binary
    has coordinates (0,0,1). This ensures the categories are all at the same distance
    from each other. Using only two dummy variables means Male has coordinates (1,0),
    Female has coordinates (0,1), and Non-Binary has coordinates (0,0). This projects
    our equilateral triangle in three-dimensional space down to a right triangle in
    the two-dimensional plane, and in doing so it distorts the distances. Male and
    Female are now closer to Non-Binary than they are to each other, because they
    are separated by the length √2 ≈ 1.4 hypotenuse in this isosceles right triangle
    with side lengths 1 (see [Figure 1-5](#figure1-5)). Consequently, some machine
    learning algorithms would mistakenly believe the categories Male and Female are
    more similar to the category Non-Binary than they are to each other.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用三个虚拟变量将类别放置在一个等边三角形的顶点上：男性的坐标为（1,0,0），女性的坐标为（0,1,0），非二元的坐标为（0,0,1）。这确保了各个类别之间的距离相等。只使用两个虚拟变量意味着男性的坐标为（1,0），女性的坐标为（0,1），非二元的坐标为（0,0）。这将我们的等边三角形在三维空间中投影到二维平面上的直角三角形，这样做会扭曲距离。男性和女性现在比彼此更接近非二元，因为它们在这个等腰直角三角形中被长度√2
    ≈ 1.4的斜边分开（见[图1-5](#figure1-5)）。因此，一些机器学习算法会错误地认为类别男性和女性与类别非二元更相似，而不是彼此。
- en: '![](image_fi/503083c01/f01005.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c01/f01005.png)'
- en: 'Figure 1-5: Two approaches to creating gender dummy variables. On the left,
    we have one axis for each category, which ensures the categories are equidistant.
    On the right, we use only two axes, which causes some categories to be closer
    than others.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图1-5：创建性别虚拟变量的两种方法。左边，我们为每个类别使用一个轴，这确保了各个类别之间的距离相等。右边，我们只使用两个轴，这导致一些类别比其他类别更接近。
- en: 'So why are both dummy variable methods used? Using *n* dummy variables for
    an *n*-value categorical variable rather than *n* – 1 leads to *multicollinearity*,
    which in statistical language is a correlation among the independent variables.
    The correlation here is that each of the dummy variables is completely and linearly
    determined by the others. Algebraically, this is a *linear dependence*, which
    means one column is a linear combination of the other columns. This linear dependence
    can be seen geometrically: when placing the *n* categories in *n*-dimensional
    space, they span an (*n* – 1)-dimensional plane only. In [Figure 1-5](#figure1-5),
    linear combinations of the three vectors on the left span only the plane containing
    the triangle, whereas on the right the linear combinations span the full two-dimensional
    coordinate system.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么要同时使用两种虚拟变量方法呢？对于*n*值分类变量，使用*n*个虚拟变量而不是*n* – 1会导致*多重共线性*，在统计语言中，这是独立变量之间的相关性。这里的相关性是每个虚拟变量完全且线性地由其他变量决定。代数上，这是*线性相关性*，意味着一列是其他列的线性组合。这种线性相关性在几何上可以看到：将*n*个类别放置在*n*维空间中时，它们仅覆盖一个(*n*
    – 1)维平面。在[图1-5](#figure1-5)中，左侧三个向量的线性组合仅覆盖包含三角形的平面，而右侧的线性组合则覆盖整个二维坐标系。
- en: Multicollinearity causes computational problems for linear and logistic regression,
    so for those algorithms, we should use *n* – 1 dummy variables rather than all
    *n*. Even for methods that don’t run into this specific computational issue, using
    fewer independent variables when possible is generally better because this helps
    reduce the curse of dimensionality—a fundamental topic in data science that we’ll
    visit from a geometric perspective shortly.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 多重共线性会导致线性回归和逻辑回归的计算问题，因此对于这些算法，我们应该使用*n* – 1个虚拟变量而不是全部*n*个。即使对于不会遇到这个特定计算问题的方法，尽可能使用较少的独立变量通常更好，因为这有助于减少维度诅咒——这是数据科学中的一个基本主题，我们将很快从几何角度讨论。
- en: On the other hand, for algorithms like *k*-NN, where distances between data
    points are crucial, we don’t want to drop one of the dummy variables, as that
    would skew the distances (as we saw in [Figure 1-5](#figure1-5)) and lead to subpar
    performance. There is a time and a place for both dummy variable methods, and
    thinking geometrically can help us decide which to use when.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，对于像*k*-NN这样的算法，数据点之间的距离至关重要，我们不希望丢弃虚拟变量之一，因为那会扭曲距离（正如我们在[图1-5](#figure1-5)中看到的那样），并导致性能不佳。在使用虚拟变量方法时，有时机会和地点，思考几何学可以帮助我们决定何时使用哪种方法。
- en: After using dummy variables to convert all categorical variables to numerical
    variables, we are ready to consider the geometry of the spreadsheet.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用虚拟变量将所有分类变量转换为数值变量后，我们准备考虑电子表格的几何形态。
- en: The Geometry of Numerical Spreadsheets
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数字电子表格的几何形态
- en: We can think of a numerical spreadsheet as describing a collection of points
    (one for each row) in a *Euclidean vector space*, **R**^(*d*), which is a geometric
    space that looks like a flat piece of paper in two dimensions and a solid brick
    in three dimensions but extends infinitely in all directions and can be any dimension.
    Here, *d* is the number of columns, which is also the dimension of the space.
    Each column in the numerical dataset represents an axis in this space. Concretely,
    the *d*-dimensional coordinates of each data point are simply the values in that
    row.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将数字电子表格看作描述*欧几里得向量空间* **R**^(*d*) 中的一组点（每行一个点），这是一个几何空间，在二维中看起来像一张平面纸，在三维中看起来像一个实心砖块，但在所有方向上无限延伸，可以是任意维度。这里，*d*
    是列数，也是空间的维度。数字数据集中的每一列代表这个空间中的一个轴。具体来说，每个数据点的*d*维坐标只是该行中的值。
- en: When *d* = 1, this Euclidean vector space **R**^(*d*) is a line. When *d* =
    2, it is a plane. When *d* = 3, it is the usual three-dimensional space we are
    used to thinking in. While humans can’t really visualize more than three perpendicular
    axes, higher-dimensional geometry can be analyzed with mathematical and computational
    tools regardless. It is important to recognize here that just as there are many
    two-dimensional shapes beyond a flat plane (for instance, the surface of a sphere
    or of a donut, or even stranger ones like a Möbius strip), there are many three-dimensional
    geometric spaces beyond the familiar Euclidean space (such as the inside of a
    sphere in three dimensions or the surface of a sphere in four dimensions). This
    holds for higher-dimensional spaces as well. Working with structured data has
    traditionally meant viewing the data from the perspective of **R**^(*d*) rather
    than any of these other kinds of geometric spaces.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当*d* = 1时，这个欧几里得向量空间**R**^(*d*)是一个直线。当*d* = 2时，它是一个平面。当*d* = 3时，它就是我们习惯思考的三维空间。虽然人类实际上无法直观地理解超过三个垂直坐标轴的空间，但即便如此，我们仍然可以通过数学和计算工具分析高维几何。这里需要注意的是，正如平面之外存在许多二维形状（例如球面或圆环面，甚至更奇特的莫比乌斯带），三维空间之外也存在许多三维几何空间（如三维球体内部或四维球体表面）。这同样适用于更高维的空间。传统上，处理结构化数据意味着从**R**^(*d*)的角度来看数据，而非从这些其他几何空间的角度来看。
- en: The Euclidean vector space structure of **R**^(*d*) is powerful; it allows us
    to compute all kinds of useful things. We can compute the distance between any
    pair of data points, which is necessary for a wide range of machine learning algorithms.
    We can compute the line segment connecting any two points, which is used by the
    Synthetic Minority Oversampling Technique (SMOTE) to adjust training samples with
    imbalanced classes. We can compute the mean of each coordinate in the data, which
    is helpful for *imputing* missing values (that is, filling in missing values with
    best guesses for the true value based on known data).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**R**^(*d*)的欧几里得向量空间结构非常强大；它允许我们计算各种有用的内容。我们可以计算任何一对数据点之间的距离，这对于广泛的机器学习算法来说是必要的。我们可以计算连接任意两点的线段，这被合成少数类过采样技术（SMOTE）用于调整训练样本中的不平衡类别。我们可以计算数据中每个坐标的均值，这对于*填补*缺失值很有帮助（即，通过已知数据对缺失值进行最佳猜测，从而填补缺失值）。'
- en: However, this nice Euclidean vector space structure is also specific and rigid.
    Thankfully, we can compute distances between points, shortest paths connecting
    points, and various forms of interpolation in much more general settings where
    we don’t have global Euclidean coordinates, including *manifolds* (geometric objects
    like spheres that when zoomed in look like the usual Euclidean space but globally
    can have much more interesting shape and structure—to come in [Chapter 5](c05.xhtml))
    and *networks* (relational structures formally introduced in [Chapter 2](c02.xhtml)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种漂亮的欧几里得向量空间结构也具有特定和僵化的性质。幸运的是，我们可以在没有全局欧几里得坐标的更一般的环境中计算点之间的距离、连接点的最短路径以及各种形式的插值，包括*流形*（像球面这样的几何对象，放大看时与通常的欧几里得空间相似，但在全局上可以具有更有趣的形状和结构——将在[第5章](c05.xhtml)中讲解）和*网络*（在[第2章](c02.xhtml)中正式引入的关系结构）。
- en: As a concrete example, suppose you are working with large-scale geospatial data,
    such as ZIP code–based crime statistics. How do you cluster data points or make
    any kind of predictive model? The most straightforward approach is to use latitude
    and longitude as variables to convey the geospatial aspect of the data. But problems
    quickly arise because this approach projects the round Earth down to a flat plane
    in a way that distorts the distances quite significantly. For instance, longitude
    ranges from –180° to +180°, so two points on opposite sides of the prime meridian
    could be very close to each other in terms of miles but extremely far from each
    other in terms of longitudes (see [Figure 1-6](#figure1-6)). It’s helpful to have
    machine learning algorithms that can work on spherical data without the need to
    map data onto a flat surface.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 举一个具体的例子，假设你正在处理大规模的地理空间数据，比如基于邮政编码的犯罪统计数据。你如何对数据点进行聚类或构建任何预测模型？最直接的方法是使用纬度和经度作为变量来传达数据的地理空间特征。但是，这种方法很快就会遇到问题，因为它将圆形的地球投影到平面上，这种方式会显著扭曲距离。例如，经度的范围是从-180°到+180°，所以位于本初子午线两侧的两个点，可能在英里数上非常接近，但在经度上却可能相距非常远（见[图1-6](#figure1-6)）。因此，能够在球面数据上工作的机器学习算法非常有用，避免了将数据映射到平面表面上的需要。
- en: '![](image_fi/503083c01/f01006.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c01/f01006.png)'
- en: 'Figure 1-6: Using latitude and longitude (left) as variables for geospatial
    data distorts distances between data points. Shown here, very near points on opposite
    sides of the prime meridian are represented by very far points in the longitude-latitude
    plane (right).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1-6：将纬度和经度（左图）作为地理空间数据的变量会扭曲数据点之间的距离。如图所示，位于零度经线两侧非常接近的点，在经纬度平面（右图）上表示为非常远的点。
- en: Even when you are working with data that is already structured as a tabular
    spreadsheet, there might be hidden geometry that is relevant. For example, imagine
    you have three numerical variables (so that your data lives in **R**³) but all
    your data points live on or near the surface of a sphere in this three-dimensional
    space. Would you want to consider distances between points as the path lengths
    along the sphere’s surface (which is what is done in spherical geometry) or straight-line
    distances that cut through the sphere (which is what is done with traditional
    Euclidean machine learning)? The answer depends on the context and is something
    that data scientists must decide based on domain knowledge—it is not typically
    something that an algorithm should decide on its own. For example, if your data
    points represent locations in a room that an aerial drone could visit, then Euclidean
    distance is better; if your data points represent airports across the globe that
    an international airline services, then spherical geometry is better.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你在处理已经结构化为表格的电子表格数据时，可能仍然存在隐藏的几何结构是相关的。例如，假设你有三个数值变量（因此你的数据位于**R**³空间中），但所有数据点都位于或接近三维空间中的一个球面上。你是否希望考虑点与点之间的距离是沿球面表面的路径长度（这就是球面几何中所做的）还是穿过球体的直线距离（这是传统欧几里得机器学习中所做的）？答案取决于上下文，这是数据科学家需要根据领域知识来决定的——这通常不是算法应该自行决定的内容。例如，如果你的数据点代表一个空中无人机可以访问的房间中的位置，那么欧几里得距离更好；如果你的数据点代表全球范围内的机场，并且有国际航空公司提供服务，那么球面几何更好。
- en: 'One of the main tasks of topological and geometric data science is discovering
    the geometric objects on which your data points naturally live (like the sphere
    in the airport example, but perhaps very complex shapes in high dimensions). The
    other main task is exploiting these geometric objects, which usually involves
    one or more of the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑学和几何数据科学的主要任务之一是发现数据点自然存在的几何对象（就像机场示例中的球面，但在高维空间中可能是非常复杂的形状）。另一个主要任务是利用这些几何对象，这通常涉及以下一个或多个方面：
- en: Applying versions of the usual machine learning algorithms that have been adapted
    to more general geometric settings
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用已适应更一般几何环境的常规机器学习算法版本
- en: Applying new geometrically powered algorithms that are based on the shape of
    data
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用基于数据形状的全新几何算法
- en: Providing meaningful global coordinates to transform your data into a structured
    spreadsheet in a way that traditional statistical and machine learning tools can
    be successfully applied
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供有意义的全球坐标，将数据转换为结构化的电子表格，以便传统的统计和机器学习工具能够成功应用
- en: The main goal of this book is to carefully go through all these ideas and show
    you how to implement them easily and effectively. But first, in the remainder
    of this introductory chapter, we’ll explain some of the geometry involved in a
    few traditional data science topics (like we did earlier with dummy variables).
    We’ll also hint at the geometry involved in a few different types of “unstructured”
    data, as a preview of what’s to come later in the book.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的主要目标是详细讲解所有这些概念，并展示如何轻松有效地实现它们。但首先，在本介绍性章节的剩余部分，我们将解释一些传统数据科学主题中涉及的几何内容（就像我们之前讲解虚拟变量时所做的）。我们还将暗示一些不同类型的“非结构化”数据所涉及的几何结构，作为后续章节的预览。
- en: The Geometry of Supervised Learning
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督学习的几何学
- en: 'In this section, we’ll provide a geometric view of a few standard machine learning
    topics: classification, regression, overfitting, and the curse of dimensionality.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将提供一些标准机器学习主题的几何视角：分类、回归、过拟合以及维度灾难。
- en: Classification
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分类
- en: Once a dataset has been converted to a numerical spreadsheet (with *d* columns,
    let’s say), the job of a supervised classifier is to label the predicted class
    of each new input data point. This can be viewed in terms of *decision boundaries*,
    which means we carve up the space **R**^(*d*) into nonoverlapping regions and
    assign a class to each region, indicating the label that the classifier will predict
    for all points in the region. (Note that the same class can be assigned to multiple
    regions.) The type of geometric shapes that are allowed for the regions is determined
    by the choice of supervised classifier method, while the particular details of
    these shapes are learned from the data in the training process. This provides
    an illuminating geometric window into the classification process.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据集被转换为一个数值化的电子表格（假设有 *d* 列），监督分类器的任务是标记每个新输入数据点的预测类别。这可以通过 *决策边界* 来理解，即我们将
    **R**^(*d*) 空间划分为不重叠的区域，并为每个区域分配一个类别，表示分类器将为该区域中的所有点预测的标签。（注意，相同的类别可以分配给多个区域。）这些区域允许的几何形状类型由所选的监督分类器方法决定，而这些形状的具体细节则通过训练过程从数据中学习。这为分类过程提供了一个启发性的几何视角。
- en: In [Figure 1-7](#figure1-7), we see decision boundaries for a few standard classification
    algorithms in a simple binary classification example when *d* = 2.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 1-7](#figure1-7)中，我们看到在 *d* = 2 的简单二分类示例中，一些标准分类算法的决策边界。
- en: '![](image_fi/503083c01/f01007.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c01/f01007.png)'
- en: 'Figure 1-7: The decision boundaries in two dimensions for a few classification
    algorithms'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1-7：几种分类算法在二维空间中的决策边界
- en: Logistic regression produces linear decision boundaries. (Though, by adding
    higher-order terms to the model, you can achieve nonlinear decision boundaries.)
    Decision trees are built by splitting the independent variables with individual
    inequalities, which results in decision boundaries made up of horizontal and vertical
    line segments. In higher dimensions, instead of horizontal and vertical lines,
    we have planes that are aligned with the coordinate axes. Random forests, which
    are ensembles of decision trees, still produce decision boundaries of this form,
    but they tend to involve many more pieces, producing curved-looking shapes that
    are really made out of lots of small horizontal and vertical segments. The *k*-NN
    classifiers produce polygonal decision boundaries since they carve up the space
    based on which of the finitely many training data points are closest. Neural networks
    can produce complex, curving decision boundaries; this high level of flexibility
    is both a blessing and a curse because it can lead to overfitting if you’re not
    careful (we’ll discuss overfitting shortly).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归产生线性决策边界。（不过，通过在模型中加入高阶项，您可以实现非线性决策边界。）决策树通过用各自的不等式划分自变量来构建，这导致决策边界由水平和垂直线段组成。在更高的维度中，我们不再是水平和垂直线，而是与坐标轴对齐的平面。随机森林是决策树的集成，仍然会产生这种形式的决策边界，但它们通常包含更多的部分，形成看起来弯曲的形状，实际上是由许多小的水平和垂直线段组成的。*k*-NN
    分类器产生多边形决策边界，因为它们根据哪些有限的训练数据点最接近来划分空间。神经网络可以产生复杂的、弯曲的决策边界；这种高度的灵活性既是优点也是缺点，因为如果不小心，它可能会导致过拟合（我们稍后会讨论过拟合问题）。
- en: Studying the decision boundaries produced by different classifier algorithms
    can help you better understand how each algorithm works; it can also help you
    choose which algorithm to use based on how your data looks (for higher-dimensional
    data, where *d* > 2, you can get two-dimensional snapshots of the data by plotting
    different pairs of variables). Just remember that there are many choices involved—which
    variables to use, whether to include higher-order terms, what values to set for
    the hyperparameters, and so on—and all of these choices influence the types of
    decision boundaries that are possible. Whenever you encounter a classification
    algorithm that you aren’t familiar with yet, one of the best ways to develop an
    intuition for it is to plot the decision boundaries it produces and see how they
    vary as you adjust the hyperparameters.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 研究不同分类器算法产生的决策边界可以帮助您更好地理解每个算法的工作原理；它还可以帮助您根据数据的外观选择要使用的算法（对于维度更高的数据，其中*d* >
    2，您可以通过绘制不同变量对的二维快照来获取数据）。只需记住，涉及许多选择——要使用哪些变量，是否包括高阶项，要为超参数设置什么值等等——所有这些选择都会影响可能的决策边界类型。每当遇到一个您尚不熟悉的分类算法时，开发对其的直觉的最佳方法之一是绘制它产生的决策边界，并查看随着您调整超参数而变化的方式。
- en: Regression
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 回归
- en: Supervised regression can also be viewed geometrically, though it is a little
    harder to visualize. Rather than carving up space into finitely many regions based
    on the class predictions, regression algorithms assign a numerical value to each
    point in the space; when *d* = 2, this can be plotted as a heatmap or a three-dimensional
    surface. [Figure 1-8](#figure1-8) shows an example of this, where we first create
    10 random points with random dependent variable values (shown in the top plot
    with the circle size indicating the value), then we 3D scatterplot the predicted
    values for a dense grid of points, and finally we shade according to height when
    using *k*-NN with *k* = 3 (bottom left) and a random forest (bottom right).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 监督回归也可以从几何角度来看，尽管有点难以可视化。回归算法不是基于类别预测将空间划分为有限数量的区域，而是为空间中的每个点分配一个数值；当*d* = 2时，这可以绘制为热图或三维曲面。[图1-8](#figure1-8)展示了一个例子，在这个例子中，我们首先创建了10个具有随机依赖变量值的随机点（在顶部图中显示，圆圈大小表示值），然后我们为一组密集点的预测值绘制了3D散点图，最后在使用*k*=3的*k*-NN（左下）和随机森林（右下）时根据高度进行着色。
- en: '![](image_fi/503083c01/f01008_m.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c01/f01008_m.png)'
- en: 'Figure 1-8: The training data with dependent variable values indicated by circle
    size (top plot), and the three-dimensional prediction surfaces for two nonlinear
    regression algorithms: 3-NN (bottom left) and random forest (bottom right)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图1-8：训练数据，依赖变量数值由圆圈大小表示（顶部图），以及两种非线性回归算法的三维预测曲面：3-NN（左下）和随机森林（右下）
- en: 'The prediction surface for linear regression (not shown here) is one large,
    slanted plane, whereas for the two methods illustrated here the surface is a collection
    of finitely many flat regions for which the prediction value remains constant.
    Notice that these regions are polygons for *k*-NN and rectangles for the random
    forest; this will always be the case. Also, for the particular choice of hyperparameters
    used in this example, the regions here are smaller for the random forest than
    for *k*-NN. Put another way, the random forest here slices up the data space with
    surgical precision compared to the *k*-NN algorithm; the latter is more like carving
    a pumpkin with a butcher’s knife. But this will not always be the case—this comparison
    of coarseness depends on the number of trees used in the random forest and the
    number of neighbors used in *k*-NN. Importantly, a finer partition for regression
    is like a more flexible decision boundary for classification: it often looks good
    on the training data but then generalizes poorly to new data. This brings us to
    our next topic.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的预测曲面（此处未显示）是一个大的倾斜平面，而这里展示的两种方法的曲面是一组有限数量的平坦区域，其中预测值保持恒定。请注意，这些区域对于*k*-NN是多边形，对于随机森林是矩形；这种情况总是如此。此外，在本例中使用的超参数选择，随机森林的区域比*k*-NN的区域要小。换句话说，与*k*-NN算法相比，随机森林在数据空间中以手术精度切割数据；后者更像是用屠夫刀切南瓜。但这并不总是如此——这种粗糙度的比较取决于随机森林中使用的树的数量和*k*-NN中使用的邻居的数量。重要的是，对于回归的更细分区就像对于分类的更灵活的决策边界：它通常在训练数据上看起来不错，但在新数据上泛化效果不佳。这将引出我们的下一个主题。
- en: Overfitting
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 过拟合
- en: Let’s return to the decision boundary plots in [Figure 1-7](#figure1-7). At
    first glance, it would seem that the more flexible the boundaries are, the better
    the algorithm performs. This is true when considering the training data, but what
    really matters is how well algorithms perform on test data. A well-known issue
    in predictive analytics is *overfitting*, which is when a predictive algorithm
    is so flexible that it learns the particular details of the training data and,
    in doing so, will actually end up performing worse on new unseen data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到[图1-7](#figure1-7)中的决策边界图。乍一看，似乎边界越灵活，算法的表现越好。这在考虑训练数据时是正确的，但真正重要的是算法在测试数据上的表现。预测分析中的一个著名问题是*过拟合*，即当一个预测算法过于灵活，以至于学习到了训练数据的特定细节，从而在新的未见数据上表现得更差。
- en: In [Figure 1-7](#figure1-7), the logistic regression algorithm misclassifies
    the leftmost circle, whereas the decision tree creates a long sliver-shaped region
    to correctly classify that single point. If circles tend to fall on the left and
    pluses on the right, then it’s quite likely that creating this sliver region will
    hurt the classification performance overall when it is used on new data—and if
    so, this is an example of the decision tree overfitting the training data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图1-7](#figure1-7)中，逻辑回归算法错误地分类了最左边的圆，而决策树则创建了一个长条形区域，正确地分类了这个点。如果圆形倾向于位于左边，十字形则倾向于位于右边，那么创建这个长条形区域可能会在应用到新数据时损害分类性能——如果是这样的话，这就是决策树过拟合训练数据的一个例子。
- en: 'In general, as a predictive algorithm’s flexibility increases, the training
    error tends to keep decreasing until it eventually stabilizes, whereas the test
    error tends to decrease at first and then reaches a minimum and then increases
    (see [Figure 1-9](#figure1-9)). We want the bottom of the test error curve: that’s
    the best predictive performance we’re able to achieve. It occurs when the algorithm
    is flexible enough to fit the true shape of the data distribution but rigid enough
    that it doesn’t learn spurious details specific to the training data.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，随着预测算法灵活性的增加，训练误差倾向于不断下降，直到最终稳定，而测试误差则先下降，然后达到最小值，再上升（见[图1-9](#figure1-9)）。我们希望测试误差曲线的最低点：那是我们能够实现的最佳预测性能。它发生在算法足够灵活，能够拟合数据分布的真实形状，但又足够刚性，不会学习到仅适用于训练数据的虚假细节时。
- en: '![](image_fi/503083c01/f01009.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c01/f01009.png)'
- en: 'Figure 1-9: A plot of training error versus test error as a function of a predictive
    algorithm’s flexibility, illustrating the concept of overfitting'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图1-9：训练误差与测试误差作为预测算法灵活性的函数的图示，展示了过拟合的概念
- en: 'The general behavior illustrated in [Figure 1-9](#figure1-9) often occurs when
    varying hyperparameters: the classifier decision boundaries become more flexible
    as the number of neurons in a neural network increases, as the number of neighbors
    *k* in *k*-NN decreases, as the number of branches in a decision tree increases,
    and so on.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图1-9](#figure1-9)中所示的普遍行为通常发生在调整超参数时：随着神经网络中神经元数量的增加、*k*-NN中邻居数*k*的减少、决策树中分支数量的增加等，分类器的决策边界变得更加灵活。
- en: The Curse of Dimensionality
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高维灾难
- en: Sometimes, it helps to think of the x-axis in [Figure 1-9](#figure1-9) as indicating
    complexity rather than flexibility. More flexible algorithms tend to be more complex,
    and vice versa. One of the simplest yet most important measures of the complexity
    of an algorithm is the number of independent variables it uses. This is also called
    the *dimensionality* of the data. If the number of independent variables is *d*,
    then we can think of the algorithm as inputting points in a *d*-dimensional Euclidean
    vector space **R**^(*d*). For a fixed number of data points, using too few independent
    variables tends to cause underfitting, whereas using too many tends to cause overfitting.
    Thus, [Figure 1-9](#figure1-9) can also be interpreted as showing what happens
    to a predictive algorithm’s error scores as the dimensionality of the data increases
    without increasing the size of the dataset.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，考虑[图1-9](#figure1-9)中的x轴表示复杂性而非灵活性会有所帮助。更灵活的算法往往更复杂，反之亦然。衡量算法复杂性的一个最简单但最重要的标准是它使用的独立变量的数量。这也被称为数据的*维度*。如果独立变量的数量是*d*，那么我们可以认为该算法是在一个*d*维欧几里得向量空间**R**^(*d*)中输入点。对于固定数量的数据点，使用过少的独立变量往往会导致欠拟合，而使用过多的独立变量则容易导致过拟合。因此，[图1-9](#figure1-9)也可以解释为展示随着数据维度增加而不增加数据集大小时，预测算法的错误分数发生了什么变化。
- en: This eventual increase in test error as dimensionality increases is an instance
    of a general phenomenon known as the *curse of dimensionality*. When dealing with
    structured data where the number of columns is large relative to the number of
    rows (a common situation in genomics, among other areas), overfitting is likely
    for predictive algorithms, and the numerical linear algebra driving many machine
    learning methods breaks down. This is an enormous problem, and many techniques
    have been developed to help counteract it—some of which will come up later in
    this book. For now, let’s see how geometry can shed some light on the curse of
    dimensionality.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 随着维度的增加而最终导致测试误差增大的现象是一个普遍现象，称为*维度灾难*。在处理结构化数据时，列的数量相对于行的数量较大（如基因组学等领域的常见情况），预测算法容易出现过拟合，而且许多机器学习方法中驱动的数值线性代数也会崩溃。这是一个巨大的问题，许多技术已经被开发出来以帮助应对这一问题——其中一些将在本书后续部分讨论。现在，让我们看看几何学如何为维度灾难提供一些启示。
- en: One way to understand the curse of dimensionality is to think of Euclidean distances,
    meaning straight-line distance as the bird flies in however many dimensions exist.
    Imagine two pairs of points drawn on a square sheet of paper, where the points
    in one pair are near each other and the points in the other pair are far from
    each other, as in [Figure 1-10](#figure1-10).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 理解维度灾难的一种方法是考虑欧几里得距离，即在任意多个维度上，像鸟飞行一样的直线距离。想象在一张方形纸上画出两对点，其中一对点彼此靠近，而另一对点彼此远离，如[图1-10](#figure1-10)所示。
- en: '![](image_fi/503083c01/f01010r.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c01/f01010r.png)'
- en: 'Figure 1-10: A plot of two pairs of points in a two-dimensional space'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图1-10：二维空间中两对点的绘图
- en: Let’s perturb these points a bit by adding some Gaussian noise; that is, we’ll
    draw four vectors from a bivariate normal distribution and add these to the coordinates
    of the four points. Doing this moves the points slightly in random directions.
    Let’s do this many times, and each time we’ll record the Euclidean distance between
    the left pair after perturbation and also between the right pair. If the perturbation
    is large enough, we might occasionally end up with the points on the left farther
    from each other than the points on the right, but, overall, the Euclidean distances
    for the left perturbations will be smaller than those for the right perturbations,
    as we see in the histogram in [Figure 1-11](#figure1-11).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过加入一些高斯噪声来扰动这些点；也就是说，我们将从双变量正态分布中绘制四个向量，并将这些向量加到四个点的坐标上。这样做会使点在随机方向上稍微移动。我们将这样做多次，每次记录扰动后左边一对点之间的欧几里得距离，以及右边一对点之间的距离。如果扰动足够大，我们偶尔会发现左边的点距离彼此比右边的点还远，但总体而言，左边扰动的欧几里得距离会小于右边扰动的欧几里得距离，正如[图1-11](#figure1-11)中的直方图所示。
- en: '![](image_fi/503083c01/f01011.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c01/f01011.png)'
- en: 'Figure 1-11: A histogram of the Euclidean distances after random small perturbations
    for the nearby points on the left side of [Figure 1-10](#figure1-10) (shown in
    light gray) and the faraway points on the right side of that figure (shown in
    dark gray)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1-11：在[图 1-10](#figure1-10)的左侧，经过随机小幅扰动后的欧几里得距离的直方图（附近点显示为浅灰色），而右侧则是远离点的直方图（显示为深灰色）。
- en: Next, let’s embed our square sheet of paper as a two-dimensional plane inside
    a higher-dimensional Euclidean space **R**^(*d*) and then repeat this experiment
    of perturbing the points and computing Euclidean distances. In higher dimensions,
    these perturbations take place in more directions. Concretely, you can think of
    this as padding the *x* and *y* coordinates for our points with *d*-2 zeros and
    then adding a small amount of noise to each of the *d* coordinates. [Figure 1-12](#figure1-12)
    shows the resulting histograms of Euclidean distances when doing this process
    for *d* = 10 and *d* = 100.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将我们的方形纸片嵌入为一个二维平面，并将其放置在一个更高维度的欧几里得空间**R**^(*d*)中，然后重复扰动这些点并计算欧几里得距离的实验。在更高维度中，这些扰动会在更多的方向上发生。具体来说，你可以把每个点的*x*和*y*坐标用*d*-2个零填充，然后对每个*d*个坐标添加少量噪声。[图
    1-12](#figure1-12)显示了在*d* = 10和*d* = 100维度下进行此过程时欧几里得距离的结果直方图。
- en: '![](image_fi/503083c01/f01012_m.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c01/f01012_m.png)'
- en: 'Figure 1-12: Histograms of Euclidean distances as in [Figure 1-11](#figure1-11),
    except after embedding in *d* = 10 dimensions (left) and *d* = 100 dimensions
    (right)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1-12：如[图 1-11](#figure1-11)所示的欧几里得距离直方图，不同之处在于这里的数据是在*d* = 10维（左）和*d* = 100维（右）下嵌入后的结果。
- en: We see that as the dimension *d* increases, the two distributions come together
    and overlap more and more. Consequently, when there is noise involved (as there
    always is in the real world), adding extra dimensions destroys our ability to
    discern between the close pair of points and the far pair. Put another way, the
    signal in your data will become increasingly lost to the noise as the dimensionality
    of your data increases, unless you are certain that these additional dimensions
    contain additional signal. This is a pretty remarkable insight that we see revealed
    here through relatively simple Euclidean geometry!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，随着维度*d*的增加，两个分布逐渐重叠并且越来越接近。因此，当涉及到噪声时（如现实世界中常有的情况），增加额外的维度会破坏我们区分近点对和远点对的能力。换句话说，随着数据维度的增加，数据中的信号会越来越容易被噪声掩盖，除非你确信这些额外的维度包含了额外的信号。这是一个相当深刻的洞察，通过相对简单的欧几里得几何学我们可以看到这一点！
- en: We can also use perturbations to see why large dimensionality can lead to overfitting.
    In [Figure 1-13](#figure1-13), on the left, we see four points in the plane **R**²
    labeled by two classes in a configuration that is not linearly separable (meaning
    a logistic regression classifier without higher-order terms won’t be able to correctly
    class all these points).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用扰动来解释为什么高维度会导致过拟合。在[图 1-13](#figure1-13)中，左侧显示了四个点在平面**R**²中，它们被两个类别标记，且配置不可线性分割（这意味着没有高阶项的逻辑回归分类器将无法正确分类这些点）。
- en: '![](image_fi/503083c01/f01013.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c01/f01013.png)'
- en: 'Figure 1-13: Two classes of data points that are not linearly separable in
    two dimensions (left) but are linearly separable after placing them in three dimensions
    (right) and perturbing them there'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1-13：在二维空间中不可线性分割的两类数据点（左）通过将其嵌入三维空间后（右）并进行扰动后可以线性分割。
- en: Even if we perturb the points with a small amount of noise, there will be no
    line separating the two classes. On the right side of this figure, we have embedded
    these points in **R**³ simply by adding a third coordinate to each point that
    is equal to a constant. (Geometrically, this means we lay the original **R**²
    down flat above the *xy*-plane in this three-dimensional space.) We then perturb
    the points a small amount. After this particular perturbation in three dimensions,
    we see that the two classes do become linearly separable, meaning a logistic regression
    classifier will be able to achieve 100 percent accuracy here. (In the figure,
    we have sketched a slanted plane that separates the two classes.)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们对这些点进行少量噪声扰动，也无法找到一条线来分隔这两类数据。在图的右侧，我们通过为每个点添加一个常数作为第三坐标，将这些点嵌入了**R**³。（几何上，这意味着我们把原本的**R**²平面放置在这个三维空间的*xy*平面之上。）然后我们对这些点进行小幅扰动。经过这次三维扰动后，我们可以看到这两类数据变得可线性分割，这意味着逻辑回归分类器在这里将能够达到100%的准确率。（在图中，我们勾画了一条倾斜的平面来分割这两类数据。）
- en: At first glance, this additional flexibility seems like a good thing (and sometimes
    it can be!) since it allowed us to increase our training accuracy. But notice
    that our classifier in three dimensions didn’t learn a meaningful way to separate
    the classes in a way likely to generalize on new, unseen data. It really just
    learned the vertical noise from one particular perturbation. In other words, increasing
    the dimensionality of data tends to increase the likelihood that classifiers will
    fit noise in the training data, and this is a recipe for overfitting.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 刚开始，这种额外的灵活性看起来像是一件好事（有时候确实如此！），因为它让我们能够提高训练准确率。但请注意，我们在三维空间中的分类器并没有学会以一种有意义的方式分开各个类别，以便能在新的、未见过的数据上进行泛化。它实际上只是学会了从某个特定扰动中提取垂直噪声。换句话说，增加数据的维度往往会增加分类器拟合训练数据中噪声的可能性，而这恰恰是导致过拟合的原因。
- en: There is also a geometric perspective of the computational challenges caused
    by the curse of dimensionality. Imagine a square with a length of 10 units on
    both sides, giving an area of 100 units. If we add another axis, we’ll get a volume
    of 1,000 units. Add another, and we’ll have a four-dimensional cube with a four-dimensional
    volume of 10,000 units. This means data becomes more spread out—sparser—as the
    dimensionality increases. If we take a relatively dense dataset with 100 points
    in a low-dimensional space and place it in a space with 1,000 dimensions, then
    there will be a lot of the space that isn’t near any of those 100 points. Someone
    wandering about in that space looking for points may not find one without a lot
    of effort. If there’s a finite time frame for the person to look, they might not
    find a point within that time frame. Simply put, computations are harder in higher
    dimensions because there are more coordinates to keep track of and data points
    are harder to find.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一个几何角度来解释由维度诅咒所引起的计算挑战。想象一个边长为10单位的正方形，面积为100单位。如果我们再添加一个轴，那么我们就得到了一个体积为1,000单位的立方体。如果再加一个，我们就得到了一个四维立方体，体积为10,000单位。这意味着数据会变得更加分散——随着维度的增加，数据变得更加稀疏。如果我们将一个低维空间中100个点的相对密集数据集放入一个1,000维的空间中，那么在这个空间中就会有很多地方距离这100个点都很远。一个人在这个空间中四处寻找点，可能需要付出大量努力才能找到一个。如果他有限的时间框架内进行寻找，他可能根本找不到任何点。简单来说，高维度下的计算更困难，因为需要跟踪更多的坐标，数据点也更难找到。
- en: In the following chapters, we’ll look at a few different ways to wrangle high-dimensional
    data through geometry, including ways to reduce the data’s dimensionality, create
    algorithms that model the data geometry explicitly to fit models, and calculate
    distances in ways that work better than Euclidean distance in high-dimensional
    spaces. The branch of machine learning algorithms designed to handle high-dimensional
    data is still growing thanks to subjects like genomics and proteomics, where datasets
    typically have millions or billions of independent variables. It is said that
    necessity is the mother of invention, and indeed many machine learning methods
    have been invented out of the necessity of dealing with high-dimensional real-world
    datasets.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将通过几何学探讨几种处理高维数据的方法，包括减少数据维度、创建能够显式建模数据几何以拟合模型的算法，以及计算在高维空间中比欧几里得距离更有效的距离。设计用于处理高维数据的机器学习算法领域仍在不断发展，这要归功于基因组学和蛋白质组学等学科，在这些领域中，数据集通常包含数百万或数十亿个独立变量。人们常说需求是发明之母，的确，许多机器学习方法正是在应对高维现实世界数据集的需求下发明出来的。
- en: Unstructured Data
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非结构化数据
- en: Most of the data that exists today does not naturally live in a spreadsheet
    format. Examples include text data, network data, image data, and even video or
    sound clip data. Each of these formats comes with its own geometry and analytic
    challenges. Let’s start exploring some of these types of unstructured data and
    see how geometry can help us understand and model the data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 今天大多数数据并不是自然存在于电子表格格式中的。比如文本数据、网络数据、图像数据，甚至是视频或声音剪辑数据。每种格式都有其自身的几何特征和分析挑战。让我们开始探索一些这些类型的非结构化数据，看看几何学如何帮助我们理解和建模这些数据。
- en: Network Data
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络数据
- en: In the next chapter, you’ll get the official definitions related to networks,
    but you might already have a sense of networks from dealing with social media.
    Facebook friendships form an undirected network (nodes as Facebook users and edges
    as friendships among them), and Twitter accounts form a directed network (directional
    edges because you have both followers and accounts you follow). There is nothing
    inherently Euclidean or spreadsheet structured about network data. In recent years,
    deep learning has been extended from the usual Euclidean spreadsheet setting to
    something much more general called *Riemannian manifolds* (which we’ll get to
    in [Chapter 5](c05.xhtml)); the main application of this generalization (called
    *geometric deep learning*) has been network data, especially for social media
    analytics.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，你将获得与网络相关的官方定义，但通过处理社交媒体，你或许已经对网络有了一定的了解。Facebook 的好友关系构成了一个无向网络（节点是 Facebook
    用户，边是用户之间的好友关系），而 Twitter 账户则构成了一个有向网络（有向边，因为你既有关注者，也有你关注的账户）。网络数据本身没有什么欧几里得空间或电子表格结构的特点。近年来，深度学习已经从传统的欧几里得电子表格设置扩展到一种更为一般的结构——*黎曼流形*（我们将在[第5章](c05.xhtml)讨论）；这一概念的主要应用（称为*几何深度学习*）就是网络数据，尤其是在社交媒体分析中的应用。
- en: For instance, Facebook uses geometric deep learning algorithms to automatically
    detect fake “bot” accounts. In addition to looking at traditional structured data
    associated with each account such as demographics and number of friends, these
    detection algorithms use the rich non-Euclidean geometry of each account’s network
    of friends. Intuitively speaking, it’s easy to create fake accounts that have
    realistic-looking interests and numbers of friends, but it’s hard to do this in
    a way such that these accounts’ friendship networks are structured similarly to
    the organic friendship networks formed by real people. Network geometry provides
    ways of measuring this notion of “similarity.”
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Facebook 使用几何深度学习算法自动检测虚假“机器人”账户。除了查看与每个账户相关的传统结构化数据，如人口统计信息和好友数量，这些检测算法还利用了每个账户好友网络的丰富非欧几里得几何特征。从直观上来说，很容易创建看起来有着真实兴趣和好友数量的虚假账户，但要做到使这些账户的好友网络结构类似于真实人群形成的自然好友网络却不容易。网络几何提供了衡量这种“相似性”的方法。
- en: Geometric deep learning has also been used to detect fake news on Twitter by
    transforming the detailed propagation patterns of stories through the network
    into independent variables for a supervised learning algorithm. We won’t get to
    geometric deep learning in this book, but there is still plenty to do and say
    when it comes to working with network data. For example, we can use geometric
    properties of a network to extract numerical variables that bring network data
    back into the familiar territory of structured data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 几何深度学习还被用于通过将故事在网络中的传播模式转化为监督学习算法的独立变量，来检测 Twitter 上的假新闻。尽管本书不会涉及几何深度学习，但在处理网络数据时仍有很多事情可以做和需要讨论。例如，我们可以利用网络的几何属性来提取数值变量，从而将网络数据带回到熟悉的结构化数据领域。
- en: Image Data
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像数据
- en: Another form of “unstructured” data that actually has a rich geometric structure
    is image data. You can think of each pixel in an image as a numerical variable
    if the image is grayscale or as three variables if it is color (red, green, and
    blue values). We can then try to use these variables to cluster images with an
    unsupervised algorithm or classify them with a supervised algorithm. But the problem
    when doing this is that there is no spatial awareness. A pair of adjacent pixels
    is treated the same as a pair of pixels on opposite sides of the image. A large
    branch of deep learning, called *convolutional neural networks* (CNNs), has been
    developed to bring spatial awareness into the picture. CNNs create new variables
    from the pixel values by sliding small windows around the image. Success in this
    realm is largely what brought widespread public acclaim to deep learning, as CNNs
    smashed all the previous records in image recognition and classification tasks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种“非结构化”数据实际上具有丰富的几何结构，那就是图像数据。如果图像是灰度的，可以将每个像素视为一个数值变量；如果是彩色图像（红色、绿色和蓝色值），则每个像素可以视为三个变量。然后，我们可以尝试使用这些变量通过无监督算法对图像进行聚类，或者使用有监督算法进行分类。但是，进行这一操作时的问题在于缺乏空间意识。相邻的像素对与图像两端的像素对被视为相同。深度学习的一个重要分支——*卷积神经网络*（CNN）被开发出来，目的是将空间意识引入图像。CNN通过在图像上滑动小窗口，从像素值中创建新的变量。这一领域的成功在很大程度上使得深度学习广泛获得公众的赞誉，因为CNN打破了所有图像识别和分类任务的记录。
- en: Let’s consider a simple case of two images that could be included in a larger
    animal classification dataset used in conservation efforts (see [Figure 1-14](#figure1-14)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的案例，假设有两张图像可能被纳入用于保护工作的大型动物分类数据集中（见[图1-14](#figure1-14)）。
- en: '![](image_fi/503083c01/f01014_m.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c01/f01014_m.png)'
- en: 'Figure 1-14: An elephant (left) and a lioness (right) at Kruger National Park'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图1-14：克鲁格国家公园的象（左）和母狮（右）
- en: The animals are shown in natural environments where leaves, branches, and lighting
    vary. They have different resolutions. The colors of each animal vary. The extant
    shapes related to both the animals and the other stuff near the animals differ.
    Manually deriving meaningful independent variables to classify these images would
    be difficult. Thankfully, CNNs are built to handle such image data and to automatically
    create useful independent variables.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 动物们展示在自然环境中，那里树叶、树枝和光照各不相同。它们有不同的分辨率。每个动物的颜色也不同。与动物及其周围物体相关的形状存在差异。手动推导出有意义的独立变量以对这些图像进行分类是困难的。幸运的是，卷积神经网络（CNN）被设计来处理这种图像数据，并自动创建有用的独立变量。
- en: The basic idea is to consider each image as a mathematical surface (see [Figure
    1-15](#figure1-15)) and then walk across this surface creating a map of its salient
    features—peaks, valleys, and other relevant geometric occurrences. The next layer
    in the CNN walks across this map and creates a map of its salient features, which
    is then fed to the next layer, and so on. In the end, the CNN converts each image
    to a sequence of maps that hierarchically encode the image’s content, with the
    final layer being the map that is actually used for classification. For these
    animal images, the first map might identify high-contrast regions in the image.
    The next map might assemble these regions into outlines of shapes. The following
    map might indicate which of these shapes are animals. Another layer might locate
    specific anatomical features within the animals—and these anatomical features
    could then form the basis for the final species classification.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是将每张图像视为一个数学表面（见[图1-15](#figure1-15)），然后在这个表面上行走，创建出其显著特征的地图——峰值、谷底和其他相关的几何事件。CNN的下一层在这张地图上继续行走，并创建出其显著特征的地图，随后传递到下一层，以此类推。最终，CNN将每张图像转换为一系列地图，这些地图按层次结构编码图像的内容，最终层是实际用于分类的地图。对于这些动物图像，第一张地图可能会识别图像中的高对比度区域。接下来的地图可能会将这些区域组合成形状的轮廓。再接下来的地图可能会标明哪些形状是动物。另一个层次可能会定位动物体内的特定解剖特征——这些解剖特征随后可以作为最终物种分类的基础。
- en: 'The precise way the CNN builds these maps is learned internally through the
    supervised training process: as the algorithm is fed labeled data, connections
    between neurons in each layer forge, break, and forge again until the final layer
    is as helpful as possible for the classification task. We’ll further explore CNNs
    and their quantum versions in [Chapter 10](c10.xhtml).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: CNN构建这些地图的精确方式是通过监督训练过程内部学习的：随着算法被提供带标签的数据，每一层神经元之间的连接会形成、断裂，然后再次形成，直到最终层对分类任务尽可能有帮助。我们将在[第10章](c10.xhtml)进一步探讨CNN及其量子版本。
- en: '![](image_fi/503083c01/f01015.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c01/f01015.png)'
- en: 'Figure 1-15: The head of the lioness in [Figure 1-14](#figure1-14), viewed
    geometrically as a 3D mathematical surface'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图1-15：在几何上视为3D数学表面的[图1-14](#figure1-14)中的母狮头部。
- en: Using methods from computational geometry to quantify peaks and valleys has
    applications beyond image recognition and classification. A scientist might want
    to understand the dynamic process or structure of a scientific phenomenon, such
    as the flow of water or light on an object. The peaks, valleys, and contours of
    the object impact how light will scatter when it hits the object, and they’ll
    also determine how liquids would flow down the object. We’ll cover how to mine
    data for relevant peaks, valleys, and contours later in this book.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用计算几何方法来量化峰值和谷值在图像识别和分类之外还有应用。科学家可能想要了解科学现象的动态过程或结构，比如水流或光线在物体上的流动。物体的峰值、谷值和轮廓会影响光线撞击物体时的散射方式，也会决定液体在物体上流动的方式。我们将在本书的后面部分讨论如何挖掘相关峰值、谷值和轮廓的数据。
- en: Text Data
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本数据
- en: Another form of “unstructured” data that has risen to prominence in recent years
    is text data. Here, the structure that comes with the data is not spatial like
    it is for images; it’s linguistic. State-of-the-art text processing (for instance,
    used by Google to process search phrases or by Facebook and Twitter to detect
    posts that violate platform policies) harnesses deep learning to create something
    called *vector embeddings*, which translate text into **Rd, where each word or
    sentence is represented as a point in a Euclidean vector space. The coordinates
    of each word or sentence are learned from data by reading vast amounts of text,
    and the deep learning algorithm chooses them in a way that in essence translates
    linguistic meaning into geometric meaning. We’ll explore deep learning text embeddings
    in [Chapter 9](c09.xhtml).**
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来崭露头角的另一种“非结构化”数据形式是文本数据。这里，数据带有的结构不像图像那样是空间性的；而是语言性的。最先进的文本处理（例如，谷歌用于处理搜索短语或Facebook和Twitter用于检测违反平台政策的帖子）利用深度学习来创建一种称为*向量嵌入*的东西，将文本转换为**Rd，其中每个单词或句子都表示为欧几里得向量空间中的一个点。每个单词或句子的坐标是通过阅读大量文本数据学习到的，并且深度学习算法以一种方式选择它们，从本质上将语言意义转化为几何意义。我们将在[第9章](c09.xhtml)中探讨深度学习文本嵌入。
- en: '**For example, we might want to visualize different sets of variables concerning
    text documents. Because the variables form a high-dimensional space, we can’t
    plot them in a way that humans can visualize. In later chapters, we’ll learn about
    geometric ways to map high-dimensional data into lower-dimensional spaces such
    that the data can be visualized easily in a plot. We can decorate these plots
    with colors or different shapes based on the document type or other relevant document
    properties. If similar documents cluster together in these plots, it’s likely
    that some of the variables involved will help us distinguish between documents.
    New documents with unknown properties but measured values for these variables
    can then be grouped by a classification algorithm. We’ll explore this further
    in [Chapter 9](c09.xhtml).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**例如，我们可能想要可视化关于文档的不同变量集。由于变量形成了一个高维空间，我们无法以人类可视化的方式绘制它们。在后面的章节中，我们将学习将高维数据映射到低维空间的几何方式，以便数据可以在图中轻松可视化。我们可以根据文档类型或其他相关文档属性在这些图中使用颜色或不同形状进行装饰。如果类似的文档在这些图中聚集在一起，那么涉及的一些变量很可能会帮助我们区分文档。具有未知属性但对这些变量进行了测量的新文档可以通过分类算法进行分组。我们将在[第9章](c09.xhtml)进一步探讨这一点。'
- en: Summary
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter provided a brief review of the main concepts of traditional machine
    learning, but it put a geometric spin on these concepts that will likely be new
    for most readers. Woven into this review was a discussion of what it means for
    data to be structured. The main takeaway is that essentially all data has meaningful
    structure, but this structure is often of a geometric nature, and geometric tools
    are needed to put the data into a more traditional spreadsheet format. This is
    a theme we’ll develop in much more depth throughout the book. This chapter also
    hinted at some of the important geometry hidden in spreadsheet data. One of the
    main goals of the book is to show how to use this hidden geometry to improve the
    performance of machine learning algorithms.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本章简要回顾了传统机器学习的主要概念，但是在这些概念上加入了几何视角，这对大多数读者可能是新鲜的。在这次回顾中，还讨论了数据结构的含义。主要的观点是，基本上所有的数据都有意义的结构，但这种结构通常是几何性质的，需要几何工具将数据整理成更传统的电子表格格式。这是本书将在全书中更深入探讨的主题。本章还暗示了电子表格数据中一些重要的几何隐藏内容。本书的主要目标之一是展示如何利用这些隐藏的几何特性来提升机器学习算法的性能。
- en: We’ll start in Chapters [2](c02.xhtml) and [3](c03.xhtml) by diving into algorithms
    designed for analyzing network data, including social and geographic networks.
    This includes local and global metrics to understand network structure and the
    role of individuals in the network, clustering methods developed for use on network
    data, link prediction algorithms to suggest new edges in a network, and tools
    for understanding how processes or epidemics spread through networks.****
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [2](c02.xhtml) 章和第 [3](c03.xhtml) 章，我们将深入探讨用于分析网络数据的算法，包括社交网络和地理网络。这些算法涵盖了用于理解网络结构及个体在网络中角色的本地和全局度量标准，为网络数据开发的聚类方法，用于预测网络中新边的链接预测算法，以及用于理解网络中过程或传染病传播的工具。****
