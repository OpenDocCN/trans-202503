- en: '**7**'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**VARIABLES IN A HIGH-LEVEL LANGUAGE**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: 'This chapter explores the low-level implementation of variables found in high-level
    languages. Although assembly language programmers usually have a good feel for
    the connection between variables and memory locations, HLLs add sufficient abstraction
    to obscure this relationship. We’ll cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: The runtime memory organization typical for most compilers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the compiler breaks up memory into different sections and places variables
    into each
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The attributes that differentiate variables from other objects
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between static, automatic, and dynamic variables
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How compilers organize automatic variables in a stack frame
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The primitive data types that hardware provides for variables
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How machine instructions encode the address of a variable
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you finish reading this chapter, you should have a good understanding of
    how to declare variables in your program to use the least amount of memory and
    produce fast-running code.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '**7.1 Runtime Memory Organization**'
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As [Chapter 4](ch04.xhtml#ch04) discussed, operating systems (like macOS, Linux,
    or Windows) put different types of data into different sections (or *segments*)
    of main memory. Although it’s possible to control the memory organization by running
    a linker and specifying various command-line parameters, by default Windows loads
    a typical program into memory using an organization like the one shown in [Figure
    7-1](ch07.xhtml#ch7fig1) (macOS and Linux are similar, although they rearrange
    some of the sections).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/07fig01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-1: Typical runtime memory organization for Windows*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The operating system reserves the lowest memory addresses. Generally, your application
    cannot access data (or execute instructions) at the lowest addresses in memory.
    One reason the OS reserves this space is to help detect `NULL` pointer references.
    Programmers often initialize pointers with `NULL` (`0`) to indicate that the pointer
    is not valid. Should you attempt to access memory location `0` under such an OS,
    the OS will generate a *general protection fault* to indicate that it’s an invalid
    memory location.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'The remaining seven sections of memory hold different types of data associated
    with your program: the stack, the heap, the code, constants, read-only data, static
    (initialized) variables, and storage (uninitialized) variables.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, a given application can live with the default layouts chosen
    for these sections by the compiler and linker/loader. In some cases, however,
    knowing the memory layout can help you develop shorter programs. For example,
    because the code section is usually read-only, you might be able to combine the
    code, constant, and read-only data sections into a single section, thereby saving
    any padding space that the compiler/linker may place between these sections. Although
    for large applications this is probably insignificant, for small programs it can
    have a big impact on the size of the executable.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Next we’ll discuss each of these sections in detail.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '**7.1.1 The Code, Constant, and Read-Only Sections**'
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The code (or *text*) section in memory contains the machine instructions for
    a program. Your compiler translates each statement you write into a sequence of
    one or more byte values (machine instruction opcodes). The CPU interprets these
    opcode values during program execution.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Most compilers also attach a program’s read-only data and constant pool (constant
    table) sections to the code section because, like the code instructions, the read-only
    data is already write-protected. However, it is perfectly possible under Windows,
    macOS, Linux, and many other operating systems to create a separate section in
    the executable file and mark it as read-only. As a result, some compilers do support
    a separate read-only data section, and some compilers even create a different
    section (the constant pool) for the constants that the compiler emits. These sections
    contain initialized data, tables, and other objects that the program should not
    change during program execution.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Many compilers generate multiple code sections and leave it up to the linker
    to combine them into a single code segment prior to execution. To understand why,
    consider the following short Pascal code fragment:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Without worrying about how it does so, assume that the compiler can figure
    out that the `then` section of this `if` statement executes far more often than
    the `else` section. An assembly programmer, wanting to write the fastest possible
    code, might encode this sequence as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This assembly code might seem a bit convoluted, but keep in mind that any control
    transfer instruction is probably going to consume a lot of time because of pipelined
    operation on modern CPUs (see [Chapter 9](ch09.xhtml#ch09) of *WGC1* for the details).
    Code that executes without branching (or that falls straight through) executes
    the fastest. In the previous example, the common case falls straight through 99.9
    percent of the time. The rare case winds up executing two branches (one to transfer
    to the `else` section and one to return to the normal control flow). But because
    this code rarely executes, it can afford to take longer to do so.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'Many compilers use a little trick to move sections of code around like this
    in the machine code they generate—they emit the code sequentially, but place the
    `else` code in a separate section. The following MASM code demonstrates this technique:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 许多编译器使用一个小技巧，在它们生成的机器代码中像这样移动代码段——它们顺序地发出代码，但将`else`代码放置在一个独立的段中。以下 MASM 代码演示了这种技术：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Even though the `else` section code appears to immediately follow the `then`
    section’s code, placing it in a different segment tells the assembler/linker to
    move this code and combine it with other code in the `alternateCode` segment.
    This little trick, because it relies upon the assembler or linker to move the
    code, can simplify HLL compilers. (GCC, for example, uses this approach to move
    code around in the assembly language file it emits.) As a result, you will see
    this trick being used on occasion and can expect some compilers to produce multiple
    code segments.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`else`部分的代码似乎紧接着`then`部分的代码，但将其放置在不同的段落中会告诉汇编器/链接器将这段代码移动并与其他代码合并到`alternateCode`段中。这个小技巧依赖于汇编器或链接器来移动代码，因此可以简化高级语言编译器（例如，GCC
    就采用这种方法来移动它发出的汇编语言文件中的代码）。因此，你会偶尔看到这个技巧被使用，并且可以预期某些编译器会生成多个代码段。
- en: '**7.1.2 The Static Variables Section**'
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**7.1.2 静态变量部分**'
- en: 'Many languages provide the ability to initialize a global variable during the
    compilation phase. For example, in C/C++ you could use statements like the following
    to provide initial values for these static objects:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 许多语言在编译阶段提供初始化全局变量的能力。例如，在 C/C++ 中，你可以使用如下语句为这些静态对象提供初始值：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In C/C++ and other languages, the compiler places these initial values in the
    executable file. When you execute the application, the OS loads the portion of
    the executable file that contains these static variables into memory so that the
    values appear at the addresses associated with those variables. Therefore, when
    the program in this example first begins execution, `i` and `ch` will have these
    values bound to them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在 C/C++ 和其他语言中，编译器将这些初始值放置在可执行文件中。当你执行应用程序时，操作系统会将包含这些静态变量的可执行文件部分加载到内存中，以便这些值出现在与这些变量相关联的地址上。因此，当本示例中的程序第一次开始执行时，`i`和`ch`将会绑定这些初始值。
- en: 'The static section is often called the `DATA` or `_DATA` segment in the assembly
    listings that most compilers produce. As an example, consider the following C
    code fragment:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 静态部分通常在大多数编译器生成的汇编清单中被称为`DATA`或`_DATA`段。以下是一个 C 代码片段的示例：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here’s the MASM assembly code that the Visual C++ compiler emits for those
    declarations:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Visual C++ 编译器为这些声明生成的 MASM 汇编代码：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, the Visual C++ compiler places these variables in the `_DATA`
    segment.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Visual C++ 编译器将这些变量放置在`_DATA`段中。
- en: '**7.1.3 The Storage Variables Section**'
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**7.1.3 存储变量部分**'
- en: Most operating systems zero out memory prior to program execution. Therefore,
    if an initial value of `0` is suitable, you don’t need to waste any disk space
    with the static object’s initial value. Generally, however, compilers treat uninitialized
    variables in a static section as though you’ve initialized them with `0`, which
    consumes disk space. Some operating systems provide another section type, the
    storage variables section (also known as the *BSS section*), to avoid this wasted
    disk space.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数操作系统会在程序执行前将内存清零。因此，如果初始值为`0`是合适的，你就不需要浪费磁盘空间存储静态对象的初始值。然而，通常情况下，编译器会将静态部分中的未初始化变量当作已初始化为`0`来处理，这会占用磁盘空间。一些操作系统提供了另一种段类型，存储变量部分（也称为*BSS段*），以避免浪费磁盘空间。
- en: 'This section is where compilers typically store static objects that don’t have
    an explicit initial value. BSS, as noted in [Chapter 4](ch04.xhtml#ch04), stands
    for “block started by a symbol,” which is an old assembly language term describing
    a pseudo-opcode you would use to allocate storage for an uninitialized static
    array. In modern operating systems like Windows and Linux, the compiler/linker
    puts all uninitialized variables into a BSS section that simply tells the OS how
    many bytes to set aside for that section. When the OS loads the program into memory,
    it reserves sufficient memory for all the objects in the BSS section and fills
    this range of memory with zeros. Note that the BSS section in the executable file
    doesn’t contain any actual data, so programs that declare large uninitialized
    static arrays in a BSS section will consume less disk space. The following is
    the C/C++ example from the previous section, modified to remove the initializers
    so that the compiler will place the variables in the BSS section:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here is the Visual C++ output:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Not all compilers use a BSS section. Many Microsoft languages and linkers, for
    example, simply combine the uninitialized objects with the static/read-only data
    section and explicitly give them an initial value of `0`. Although Microsoft claims
    that this scheme is faster, it certainly makes executable files larger if your
    code has large, uninitialized arrays (because each byte of the array winds up
    in the executable file—something that would not happen if the compiler placed
    the array in a BSS section). Note, however, that this is a default condition and
    you can change it by setting the appropriate linker flags.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '**7.1.4 The Stack Section**'
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The stack is a data structure that expands and contracts in response to procedure
    invocations and returns, among other things. At runtime, the system places all
    automatic variables (nonstatic local variables), subroutine parameters, temporary
    values, and other objects in the stack section of memory in a special data structure
    called the *activation record* (which is aptly named, as the system creates it
    when a subroutine first begins execution and deallocates it when the subroutine
    returns to its caller). Therefore, the stack section in memory is very busy.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Many CPUs implement the stack using a special-purpose register called the *stack
    pointer*. Other CPUs (particularly some RISC CPUs) don’t provide an explicit stack
    pointer, instead using a general-purpose register for stack implementation. If
    a CPU provides a stack pointer, we say that the CPU supports a *hardware stack*;
    if it uses a general-purpose register, then we say that it uses a *software-implemented
    stack*. The 80x86 is a good example of a CPU that provides a hardware stack, and
    the PowerPC family is a good example of a CPU family with a software-implemented
    stack (most PowerPC programs use R1 as the stack pointer register). The ARM CPU
    supports a pseudo–hardware stack; it assigns one of the general-purpose registers
    as the hardware stack pointer but still requires an application to explicitly
    maintain the stack. Systems that provide hardware stacks can generally manipulate
    data on the stack using fewer instructions than systems with a software-implemented
    stack. On the other hand, RISC CPU designers who’ve chosen to use a software stack
    implementation feel that the presence of a hardware stack actually slows down
    all instructions the CPU executes. In theory, you could argue that the RISC designers
    are right; in practice, the 80x86 family includes some of the fastest CPUs around,
    providing ample proof that having a hardware stack doesn’t necessarily mean you’ll
    wind up with a slow CPU.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '**7.1.5 The Heap Section and Dynamic Memory Allocation**'
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Although simple programs may need only static and automatic variables, sophisticated
    programs need to be able to allocate and deallocate storage dynamically under
    program control. In the C and HLA languages, you would use the `malloc()` and
    `free()` functions for this purpose. C++ provides the `new` and `delete` (and
    `std::unique_ptr`) operators. Pascal uses `new` and `dispose`. Java and Swift
    use `new` (deallocation is automatic in these languages). Other languages provide
    comparable routines. These memory allocation routines have a few things in common:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: They let the programmer request how many bytes of storage to allocate (either
    by explicitly specifying the number of bytes to allocate or by specifying some
    data type whose size is known).
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They return a *pointer* to the newly allocated storage (that is, the address
    of that storage).
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They provide a facility for returning the storage space to the system once it
    is no longer needed so the system can reuse it in a future allocation call.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic memory allocation takes place in a section of memory known as the *heap*.
    Generally, an application refers to data on the heap using pointer variables,
    either implicitly or explicitly; some languages, like Java and Swift, implicitly
    use pointers behind the scenes. Thus, these objects in heap memory are usually
    referred to as *anonymous variables* because they are referred to by their memory
    address (via pointers) rather than by a name.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: The OS and application create the heap section in memory after the program begins
    execution; the heap is never a part of the executable file. Generally, the OS
    and language runtime libraries maintain the heap for an application. Despite the
    variations in memory management implementations, it’s a good idea for you to have
    a basic idea of how heap allocation and deallocation operate, because using them
    inappropriately will have a very negative impact on your application performance.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '**7.2 What Is a Variable?**'
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you consider the word *variable*, it’s obvious that it describes something
    that *varies*. But exactly what is it that varies? Most programmers would say
    that it’s the value that can vary during program execution. In fact, though, there
    are several things that can vary, so before defining a variable explicitly, we’ll
    discuss some characteristics that variables (and other objects) may possess.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '**7.2.1 Attributes**'
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An *attribute* is some feature that is associated with an object. For example,
    common attributes of a variable include its name, its memory address, its size
    (in bytes), its runtime value, and a data type associated with that value. Different
    objects may have different sets of attributes. For example, a data type is an
    object that has attributes such as a name and size, but it won’t usually have
    a value or memory location associated with it. A constant can have attributes
    such as a value and a data type, but it doesn’t have a memory location and it
    might not have a name (for example, if it’s a literal constant). A variable may
    possess all of these attributes. Indeed, the attribute list usually determines
    whether an object is a constant, data type, variable, or something else.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '**7.2.2 Binding**'
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Binding*, introduced in [Chapter 6](ch06.xhtml#ch06), is the process of associating
    an attribute with an object. For example, when a value is assigned to a variable,
    the value is bound to that variable at the point of the assignment. This bond
    remains until some other value is bound to the variable (via another assignment
    operation). Likewise, if you allocate memory for a variable while the program
    is running, the variable is bound to the memory address at that point. The variable
    and address are bound until you associate a different address with the variable.
    Binding needn’t occur at runtime. For example, values are bound to constant objects
    during compilation, and these bonds cannot change while the program is running.
    Similarly, addresses are bound to some variables at compile time, and those memory
    addresses cannot change during program execution (see “Binding Times” on [page
    150](ch06.xhtml#page_150) for more details).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '**7.2.3 Static Objects**'
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Static* objects have an attribute bound to them prior to the application’s
    execution. Constants are good examples of static objects; they have the same value
    bound to them throughout program execution.^([1](footnotes.xhtml#ch7fn1)) Global
    (program-level) variables in programming languages like Pascal, C/C++, and Ada
    are also examples of static objects because they have the same memory address
    bound to them throughout the program’s lifetime. The system binds attributes to
    a static object before the program begins execution (usually during compilation,
    linking, or even loading, though it is possible to bind values even earlier).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '**7.2.4 Dynamic Objects**'
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Dynamic* objects have some attribute bound to them during program execution.
    While it is running, the program may choose to change that attribute (*dynamically*).
    Dynamic attributes usually cannot be determined at compile time. Examples of dynamic
    attributes include values bound to variables at runtime and memory addresses bound
    to certain variables at runtime (for example, via a `malloc()` or other memory
    allocation function call).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '**7.2.5 Scope**'
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *scope* of an identifier is the section of the program where the identifier’s
    name is bound to the object. Because names in most compiled languages exist only
    during compilation, scope is usually a static attribute (although in some languages
    it can be dynamic, as I’ll explain shortly). By controlling where a name is bound
    to an object, you can reuse that name elsewhere in the program.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Most modern programming languages (such as C/C++/C#, Java, Pascal, Swift, and
    Ada) support the concept of *local* and *global* variables. A local variable’s
    name is bound to a particular object only within a given section of a program
    (for example, within a particular function). Outside the scope of that object,
    the name can be bound to a different object. This allows a global and a local
    object to share the same name without any ambiguity. This may seem potentially
    confusing, but being able to reuse variable names like `i` or `j` throughout a
    project can spare you from having to dream up equally meaningless unique variable
    names for loop indexes and other uses in the program. The scope of the object’s
    declaration determines where the name applies to a given object.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: In interpretive languages, where the interpreter maintains the identifier names
    during program execution, scope can be a dynamic attribute. For example, in various
    versions of the BASIC programming language, `dim` is an executable statement.
    Before you execute `dim`, the name you define might have a completely different
    meaning than it does after you execute `dim`. SNOBOL4 is another language that
    supports dynamic scope. Still, most programming languages avoid dynamic scope
    because using it can result in difficult-to-understand programs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Technically, scope can apply to any attribute, not just names, but this book
    will use the term only in contexts where a name is bound to a given variable.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '**7.2.6 Lifetime**'
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *lifetime* of an attribute extends from the point when you first bind an
    attribute to an object to the point you break that bond, perhaps by binding a
    different attribute to the object. If the program associates some attribute with
    an object and never breaks that bond, the lifetime of the attribute is from the
    point of association to the point the program terminates. For example, the lifetime
    of a variable is from the time you first allocate memory for the variable to the
    moment you deallocate that variable’s storage. Because a program binds static
    objects prior to execution (and static attributes do not change during program
    execution), the lifetime of a static object extends from when the program begins
    execution to when it terminates.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '**7.2.7 Variable Definition**'
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To return to the question that started this section, we can now define *variable*
    as an object that can have a value bound to it dynamically. That is, the program
    can change the variable’s value attribute at runtime. Note the operative word
    *can*. It is necessary only for the program *to be able* to change a variable’s
    value at runtime; it doesn’t *have* to do so for the object to be considered a
    variable.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: While dynamic binding of a value to an object is the defining attribute of a
    variable, other attributes may be dynamic or static. For example, the memory address
    of a variable can be statically bound to the variable at compile time or dynamically
    bound at runtime. Likewise, variables in some languages have dynamic types that
    change during program execution, while other variables have static types that
    remain fixed over an application’s execution. Only the binding of the value determines
    whether the object is a variable or something else (such as a constant).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '**7.3 Variable Storage**'
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Values must be stored in and retrieved from memory.^([2](footnotes.xhtml#ch7fn2))
    To do this, a compiler must bind a variable to one or more memory locations. The
    variable’s type determines the amount of storage it requires. Character variables
    may require as little as a single byte of storage, while large arrays or records
    can require thousands, millions, or more. To associate a variable with some memory,
    a compiler (or runtime system) binds the address of that memory location to that
    variable. When a variable requires two or more memory locations, the system usually
    binds the address of the first memory location to the variable and assumes that
    the contiguous locations following that address are also bound to the variable
    at runtime.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'Three types of bindings are possible between variables and memory locations:
    static binding, pseudo-static (automatic) binding, and dynamic binding. Variables
    are generally classified as static, automatic, or dynamic based upon how they
    are bound to their memory locations.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '**7.3.1 Static Binding and Static Variables**'
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Static binding occurs prior to runtime, at one of four possible times: at language
    design time, at compile time, at link time, or when the system loads the application
    into memory (but prior to execution). Binding at language design time is not all
    that common, but it does occur in some languages (especially assembly languages).
    Binding at compile time is common in assemblers and compilers that directly produce
    executable code. Binding at link time is fairly common (for example, some Windows
    compilers do this). Binding at load time, when the OS copies the executable into
    memory, is probably the most common for static variables. We’ll look at each possibility
    in turn.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '**7.3.1.1 Binding at Language Design Time**'
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An address can be assigned at language design time when a language designer
    associates a language-defined variable with a specific hardware address (for example,
    an I/O device or a special kind of memory), and that address never changes in
    any program. Such objects are common in embedded systems and rarely found in applications
    on general-purpose computer systems. For example, on an 8051 microcontroller,
    many C compilers and assemblers automatically associate certain names with fixed
    locations in the 128 bytes of data space found on the CPU. CPU register references
    in assembly language are good examples of variables bound to some location at
    language design time.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '**7.3.1.2 Binding at Compile Time**'
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An address can be assigned at compile time when the compiler knows the memory
    region where it can place static variables at runtime. Generally, such compilers
    generate absolute machine code that must be loaded at a specific address in memory
    prior to execution. Most modern compilers generate relocatable code and, therefore,
    don’t fall into this category. Nevertheless, lower-end compilers, high-speed student
    compilers, and compilers for embedded systems often use this binding technique.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '**7.3.1.3 Binding at Link Time**'
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Certain linkers and related tools can link together various relocatable object
    modules of an application and create an absolute load module. So, while the compiler
    produces relocatable code, the linker binds memory addresses to the variables
    (and machine instructions). Usually, the programmer specifies (via command-line
    parameters or a linker script file) the base address of all the static variables
    in the program; the linker will bind the static variables to consecutive addresses
    starting at the base address. Programmers who are placing their applications in
    read-only memory (ROM), such as a BIOS (Basic Input/Output System) ROM for a PC,
    often employ this scheme.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '**7.3.1.4 Binding at Load Time**'
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The most common form of static binding occurs at load time. Executable formats
    such as Microsoft’s PE/COFF and Linux’s ELF usually embed relocation information
    in the executable file. The OS, when it loads the application into memory, decides
    where to place the block of static variable objects and then patches all the addresses
    within instructions that reference those static objects. This allows the loader
    (for example, the OS) to assign a different address to a static object each time
    it loads it into memory.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '**7.3.1.5 Static Variable Binding**'
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A static variable has a memory address bound to it prior to program execution,
    and enjoys a couple of advantages over other variable types. Because the compiler
    knows a static variable’s address prior to runtime, it can often use an *absolute
    addressing mode* or some other simple addressing mode to access that variable.
    Static variable access is often more efficient than other variable accesses because
    it doesn’t require any additional setup.^([3](footnotes.xhtml#ch7fn3))
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of static variables is that they retain any value bound to them
    until you explicitly bind another value or until the program terminates. This
    means that static variables retain values while other events (such as procedure
    activation and deactivation) occur. Different threads in a multithreaded application
    can also share data using static variables.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Static variables also have a few disadvantages worth mentioning. First of all,
    because the lifetime of a static variable matches that of the program, it consumes
    memory the entire time the program is running. This is true even if the program
    no longer requires the value held by the static object.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Another disadvantage to static variables (particularly when using the absolute
    addressing mode) is that the entire absolute address must usually be encoded as
    part of the instruction, which makes the instruction much larger. Indeed, on most
    RISC processors an absolute addressing mode isn’t even available because you cannot
    encode an absolute address in a single instruction.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Finally, code that uses static objects is not *reentrant* (meaning two threads
    or processes can concurrently execute the same code sequence); this means more
    effort is required to use that code in a multithreaded environment (where two
    copies of a section of code could be executing simultaneously, both accessing
    the same static object). However, multithreaded operation introduces a lot of
    complexity that is beyond the scope of this chapter, so we’ll ignore this issue
    for now.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '*See any good textbook on operating system design or concurrent programming
    for more details concerning the use of static objects.* Foundations of Multithreaded,
    Parallel, and Distributed Programming *by Gregory R. Andrews (Addison-Wesley,
    1999) is a good place to start.*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates the use of static variables in a C program
    and shows the 80x86 code that the Visual C++ compiler generates to access them:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As the comments point out, the assembly language code the compiler emits uses
    the displacement-only addressing mode to access all the static variables.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '**7.3.2 Pseudo-Static Binding and Automatic Variables**'
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Automatic variables have an address bound to them when a procedure or other
    block of code begins execution. The program releases that storage when the block
    or procedure completes execution. We call these objects *automatic* variables
    because the runtime code automatically allocates and deallocates storage for them,
    as needed.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: In most programming languages, automatic variables use a combination of static
    and dynamic binding known as *pseudo-static binding*. The compiler assigns an
    offset from a base address to a variable name during compilation. At runtime the
    offset always remains fixed, but the base address can vary. For example, a procedure
    or function allocates storage for a block of local variables (the activation record,
    introduced earlier in the chapter) and then accesses the local variables at fixed
    offsets from the start of that block of storage. Although the program cannot determine
    the final memory address of the variable until runtime, the compiler can select
    an offset that never changes during program execution, hence the name *pseudo-static*.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Some programming languages use the term *local variables* in place of automatic
    variables. A local variable’s name is statically bound to a given procedure or
    block (that is, the scope of the name is limited to that procedure or block of
    code). Therefore, *local* is a static attribute in this context. It’s easy to
    see why the terms *local variable* and *automatic variable* are often confused.
    In some programming languages, such as Pascal, local variables are always automatic
    variables and vice versa. Nonetheless, always keep in mind that *local* is a static
    attribute and *automatic* is a dynamic one.^([4](footnotes.xhtml#ch7fn4))
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Automatic variables have a couple of important advantages. First, they consume
    storage only while the procedure or block containing them is executing. This allows
    multiple blocks and procedures to share the same pool of memory for their automatic
    variable needs. Although some extra code must execute in order to manage automatic
    variables (in the activation record), this requires only a few machine instructions
    on most CPUs and has to be done only once for each procedure/block entry and exit.
    While in certain circumstances, the cost can be significant, the extra time and
    space needed to set up and tear down the activation record is usually inconsequential.
    Another advantage of automatic variables is that they often use a *base-plus-offset*
    addressing mode, where the base of the activation record is kept in a register
    and the offsets into the activation record are small—often 256 bytes or fewer.
    Therefore, CPUs don’t have to encode a full 32-bit (for example) address as part
    of the machine instruction—just an 8-bit (or other small) displacement, yielding
    shorter instructions. It’s also worth noting that automatic variables are “thread-safe”
    and code that uses automatic variables can be reentrant. This is because each
    thread maintains its own stack space (or similar data structure) where compilers
    maintain automatic variables; therefore, each thread will have its own copy of
    any automatic variables the program uses.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Automatic variables do have some disadvantages, though. If you want to initialize
    an automatic variable, you have to use machine instructions to do so. You can’t
    initialize an automatic variable, as you can static variables, when the program
    loads into memory. Also, any values maintained in automatic variables are lost
    whenever you exit the block or procedure containing them. As noted, automatic
    variables require a small amount of overhead; some machine instructions must execute
    in order to build and destroy the activation record containing those variables.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a short C example that uses automatic variables and the 80x86 assembly
    code that the Microsoft Visual C++ compiler produces for it:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that when accessing automatic variables, the assembly code uses a *base-plus-displacement*
    addressing mode (for example, `j$[rsp]`). This addressing mode is often shorter
    than the displacement-only or RIP-relative addressing mode that static variables
    use (assuming, of course, that the offset to the automatic object is within 127
    bytes of the base address held in RSP).^([5](footnotes.xhtml#ch7fn5))
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '**7.3.3 Dynamic Binding and Dynamic Variables**'
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A dynamic variable has storage bound to it at runtime. In some languages, the
    application programmer is completely responsible for binding addresses to dynamic
    objects; in other languages, the runtime system automatically allocates and deallocates
    storage for a dynamic variable.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic variables are generally allocated on the heap via a memory allocation
    function such as `malloc()` or `new()` (or `std::unique_ptr`). The compiler has
    no way of determining the runtime address of a dynamic object, so the program
    must always refer to a dynamic object indirectly—that is, by using a pointer.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The big advantage to dynamic variables is that the application controls their
    lifetimes. Dynamic variables consume storage only as long as necessary, and the
    runtime system can reclaim that storage when the variable no longer requires it.
    Unlike automatic variables, the lifetime of a dynamic variable is not tied to
    the lifetime of some other object, such as a procedure or code block entry and
    exit. Memory is bound to a dynamic variable at the point the variable first needs
    it, and can be released when the variable no longer needs it. For variables that
    require considerable storage, then, dynamic allocation can make efficient use
    of memory.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage to dynamic variables is that most code references dynamic
    objects using a pointer. If that pointer value is already sitting in a CPU register,
    the program can usually reference that data using a short machine instruction,
    requiring no extra bits to encode an offset or address.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic variables have several disadvantages as well. First, some storage overhead
    is often necessary to maintain them. Static and automatic objects usually don’t
    require extra storage; the runtime system, on the other hand, often requires some
    number of bytes to keep track of each dynamic variable in the system. This overhead
    ranges anywhere from 4 or 8 bytes to many dozens of bytes (in an extreme case)
    and keeps track of things like the current memory address of the object, the size
    of the object, and its type. If you’re allocating small objects, like integers
    or characters, the amount of storage required for bookkeeping purposes could exceed
    the storage required for the actual data. Also, since most languages reference
    dynamic objects using pointer variables, those pointers require some additional
    storage above and beyond the actual storage for the dynamic data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Another problem with dynamic variables is performance. Because dynamic data
    is usually found in memory, the CPU has to access memory (which is slower than
    cached memory) on nearly every dynamic variable access. Even worse, accessing
    dynamic data often requires two memory accesses—one to fetch the pointer’s value
    and one to fetch the dynamic data, indirectly through the pointer. Managing the
    heap, where the runtime system keeps the dynamic data, can also impact performance.
    Whenever an application requests storage for a dynamic object, the runtime system
    has to search for a contiguous block of free memory large enough to satisfy the
    request. This search operation can be computationally expensive, depending on
    the heap’s organization (which affects the amount of overhead storage associated
    with each dynamic variable). Furthermore, when releasing a dynamic object, the
    runtime system may need to execute some code in order to free up that storage
    for use by other dynamic objects. These runtime heap allocation and deallocation
    operations are usually far more expensive than allocating and deallocating a block
    of automatic variables during procedure entry/exit.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Another consideration with dynamic variables is that some languages (such as
    Pascal and C/C++^([6](footnotes.xhtml#ch7fn6))) require the application programmer
    to explicitly allocate and deallocate storage for dynamic variables. Without automatic
    allocation and deallocation, defects due to human error can creep into the code.
    This is why languages such as C#, Java, and Swift attempt to handle dynamic allocation
    automatically, even though this process can be slower.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a short example in C that demonstrates the kind of code that the Microsoft
    Visual C++ compiler generates in order to access dynamic objects allocated with
    `malloc()`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here’s the machine code the compiler generates, including (manually inserted)
    comments that describe the extra work needed to access dynamically allocated objects:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you can see, accessing dynamically allocated variables via a pointer requires
    a lot of extra work.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '**7.4 Common Primitive Data Types**'
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Computer data always has a data type attribute that describes how the program
    interprets that data. The data type also determines the size (in bytes) of the
    data in memory. Data types can be divided into two categories: *primitive data
    types*, which the CPU can hold in a CPU register and operate upon directly, and
    *composite data types*, which are composed of smaller primitive data types. In
    the following sections we’ll review (from *WGC1*) the primitive data types found
    on most modern CPUs, and in the next chapter I’ll begin discussing composite data
    types.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '**7.4.1 Integer Variables**'
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most programming languages provide some mechanism for storing integer values
    in memory variables. In general, a programming language uses either unsigned binary
    representation, two’s-complement representation, or binary-coded decimal representation
    (or a combination of these) to represent integer values.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the most fundamental property of an integer variable in a programming
    language is the number of bits allocated to represent that integer value. In most
    modern programming languages, the number of bits used to represent an integer
    value is usually 8, 16, 32, 64, or some other power of two. Many languages provide
    only a single size for representing integers, but some languages let you select
    from several different sizes. You choose the size based on the range of values
    you want to represent, the amount of memory you want the variable to consume,
    and the performance of arithmetic operations involving that value. [Table 7-1](ch07.xhtml#ch7tab1)
    lists some common sizes and ranges for various signed, unsigned, and decimal integer
    variables.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Not all languages support all of these different sizes (indeed, to support all
    of them in the same program, you’d probably have to use assembly language). As
    noted earlier, some languages provide only a single size, which is usually the
    processor’s native integer size (that is, the size of a CPU general-purpose integer
    register).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'Languages that do provide multiple integer sizes often don’t give you an explicit
    selection of sizes from which to choose. For example, the C programming language
    provides up to five different integer sizes: `char` (which is always 1 byte),
    `short`, `int`, `long`, and `long long`. With the exception of the `char` type,
    C does not specify the sizes of these integer types other than to state that `short`
    integers are less than or equal to `int` objects in size, `int` objects are less
    than or equal to `long` integers in size, and `long` integers are less than or
    equal to `long long` integers in size. (In fact, all four could be the same size.)
    C programs that depend on integers being a certain size may fail when compiled
    with different compilers that don’t use the same sizes as the original compiler.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '*C99 and C++11 include types of exact sizes: int8_t, int16_t, int32_t, int64_t,
    and so on.*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-1:** Common Integer Sizes and Their Ranges'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '| **Size, in bits** | **Representation** | **Unsigned range** |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| 8 | Unsigned | `0..255` |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: '|  | Signed | `-128..+127` |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '|  | Decimal | `0..99` |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| 16 | Unsigned | `0..65,536` |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '|  | Signed | `-32768..+32,767` |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '|  | Decimal | `0..9999` |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| 32 | Unsigned | `0..4,294,967,295` |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '|  | Signed | `-2,147,483,648..+2,147,483,647` |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '|  | Decimal | `0..99999999` |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| 64 | Unsigned | `0..18,466,744,073,709,551,615` |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '|  | Signed | `-9,223,372,036,854,775,808.. +9,223,372,036,854,775,807` |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '|  | Decimal | `0..9999999999999999` |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| 128 | Unsigned | `0..340,282,366,920,938,463,563,374,607,431,768,211,455`
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '|  | Signed | `-170,141,183,460,469,231,731,687,303,715,884,105,728.. +170,141,183,460,469,231,731,687,303,715,884,105,727`
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '|  | Decimal | `0..99999999999999999999999999999999` |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: While it may seem inconvenient that various programming languages avoid specifying
    an exact size for an integer variable, keep in mind that this ambiguity is intentional.
    When you declare an “integer” variable in a given programming language, the language
    leaves it up to the compiler’s implementer to choose the *best* size for that
    integer, based on performance and other considerations. The definition of “best”
    may change based on the CPU for which the compiler generates code. For example,
    a compiler for a 16-bit processor may choose to implement 16-bit integers because
    the CPU processes them most efficiently. A compiler for a 32-bit processor, however,
    may choose to implement 32-bit integers (for the same reason). Languages that
    specify the exact size of various integer formats (such as Java) can suffer as
    processor technology evolves and it becomes more efficient to process larger data
    objects. For example, when the world switched from 16-bit processors to 32-bit
    processors in general-purpose computer systems, it was actually faster to do 32-bit
    arithmetic on most of the newer processors. Therefore, compiler writers redefined
    *integer* to mean “32-bit integer” in order to maximize the performance of programs
    employing integer arithmetic.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Some programming languages provide support for unsigned integer variables as
    well as signed integers. At first glance, it might seem that the whole purpose
    behind supporting unsigned integers is to provide twice the number of positive
    values when negative values aren’t required. In fact, there are many other reasons
    great programmers might choose unsigned over signed integers when writing efficient
    code.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: The Swift programming language gives you explicit control over the size of integers.
    Swift provides 8-bit (signed) integers (`Int8`), 16-bit integers (`Int16`), 32-bit
    integers (`Int32`), and 64-bit integers (`Int64`). Swift also provides an `Int`
    type that’s either 32 bits or 64 bits depending on the native (most efficient)
    integer format for the underlying CPU. Swift further provides 8-bit unsigned integers
    (`UInt8`), 16-bit unsigned integers (`UInt16`), 32-bit unsigned integers (`UInt32`),
    64-bit unsigned integers (`UInt64`), and a generic `UInt` type whose size is determined
    by the native CPU size.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: On some CPUs, unsigned integer multiplication and division are faster than their
    signed counterparts. You can compare values within the range `0..n` more efficiently
    using unsigned integers rather than signed integers (the unsigned case requires
    only a single comparison against n); this is especially important when checking
    bounds of array indices where the array’s element index begins at `0`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'Many programming languages allow you to include variables of different sizes
    within the same arithmetic expression. The compiler automatically sign-extends
    or zero-extends operands to the larger size within an expression as needed to
    compute the final result. The problem with this automatic conversion is that it
    hides the fact that extra work is required to process the expression, and the
    expressions themselves don’t explicitly show this. An assignment statement such
    as:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'could be a short sequence of machine instructions if the operands are all the
    same size, or it could require some additional instructions if the operands have
    different sizes. For example, consider the following C code:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Compiling it with the Visual C++ compiler gives the following two assembly
    language sequences for the two assignment statements:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see, the statement that operates on variables whose sizes are all
    the same uses fewer instructions than the one that mixes operand sizes in the
    expression.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: When using different-sized integers in an expression, it’s also important to
    note that not all CPUs support all operand sizes equally efficiently. While it
    makes sense that using an integer size larger than the CPU’s general-purpose integer
    registers will produce inefficient code, it might not be quite as obvious that
    using *smaller* integer values can be inefficient as well. Many RISC CPUs work
    only on operands that are exactly the same size as the general-purpose registers.
    Smaller operands must first be zero-extended or sign-extended to the size of a
    general-purpose register prior to any calculations involving those values. Even
    on CISC processors, such as the 80x86, that have hardware support for different
    sizes of integers, using certain sizes can be more expensive. For example, under
    32-bit operating systems, instructions that manipulate 16-bit operands require
    an extra *opcode prefix byte* and are therefore larger than instructions that
    operate on 8-bit or 32-bit operands.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '**7.4.2 Floating-Point/Real Variables**'
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Like integers, many HLLs provide multiple floating-point variable sizes. Most
    languages provide at least two different sizes: a 32-bit single-precision floating-point
    format and a 64-bit double-precision floating-point format, based on the IEEE
    754 floating-point standard. A few languages provide 80-bit floating-point variables
    (Swift is a good example), based on Intel’s 80-bit extended-precision floating-point
    format, but that usage is increasingly rare. The later ARM processors support
    quad-precision floating-point arithmetic (128-bit); some variants of GCC support
    a `_float128` type that uses quad-precision arithmetic.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Different floating-point formats trade off space and performance for precision.
    Calculations involving smaller floating-point formats are usually quicker than
    calculations involving the larger formats. However, you give up precision to achieve
    improved performance and size savings (see [Chapter 4](ch04.xhtml#ch04) of *WGC1*
    for the details).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: As with expressions involving integer arithmetic, you should avoid mixing different-sized
    floating-point operands in an expression. The CPU (or FPU) must convert all floating-point
    values to the same format before using them. This can involve additional instructions
    (consuming more memory) and additional time. Therefore, you should try to use
    the same floating-point types throughout an expression wherever possible.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Conversion between integer and floating-point formats is another expensive operation
    you should avoid. Modern HLLs attempt to keep variables’ values in registers as
    much as possible. Unfortunately, on some modern CPUs it’s impossible to move data
    between the integer and floating-point registers without first copying that data
    to memory (which is expensive, because memory is slow). Furthermore, conversion
    between integer and floating-point numbers often involves several specialized
    instructions, all of which consume time and memory. Whenever possible, avoid these
    conversions.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '**7.4.3 Character Variables**'
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Standard character data in most modern HLLs consumes 1 byte per character. On
    CPUs that support byte addressing, such as the Intel 80x86 processor, a compiler
    can reserve a single byte of storage for each character variable and efficiently
    access that character variable in memory. Some RISC CPUs, however, cannot access
    data in memory except in 32-bit chunks (or another size other than 8 bits).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: For CPUs that cannot address individual bytes in memory, HLL compilers usually
    reserve 32 bits for a character variable and use only the LO byte of that double-word
    variable for the character data. Because few programs have a large number of scalar
    character variables,^([7](footnotes.xhtml#ch7fn7)) the amount of space wasted
    is hardly an issue in most systems. However, if you have an unpacked array of
    characters, then the wasted space can become significant. We’ll return to this
    issue in [Chapter 8](ch08.xhtml#ch08).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Modern programming languages support the Unicode character set. Unicode characters
    can require between 1 and 4 bytes of memory to hold the character’s data value
    (depending on the underlying encoding, such as UTF-8, UTF-16, or UTF-32). As time
    passes, Unicode will likely replace the ASCII character set for most character-
    and string-oriented operations except in those programs that require high-performance
    random access to characters within strings (where Unicode performance suffers).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '**7.4.4 Boolean Variables**'
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A Boolean variable requires only a single bit to represent the two values `true`
    or `false`. HLLs usually reserve the smallest amount of memory possible for these
    variables (a byte on machines that support byte addressing, and a larger amount
    of memory on those CPUs that can address only 16-bit, 32-bit, or 64-bit memory
    values). However, this isn’t always the case. Some languages (like FORTRAN) allow
    you to create multibyte Boolean variables (for example, the FORTRAN `LOGICAL*4`
    data type).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Some languages (early versions of C/C++, for example) don’t support an explicit
    Boolean data type. Instead, they use an integer data type to represent Boolean
    values. Those C/C++ implementations use `0` and nonzero to represent `false` and
    `true`, respectively. In such languages, you get to choose the size of your Boolean
    variables by choosing the size of the integer you use to hold them. For example,
    in a typical older 32-bit implementation of the C/C++ languages, you can define
    1-byte, 2-byte, or 4-byte Boolean values as shown in [Table 7-2](ch07.xhtml#ch7tab2).^([8](footnotes.xhtml#ch7fn8))
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-2:** Defining Boolean Value Sizes'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '| **C integer data type** | **Size of Boolean object** |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| `char` | 1 byte |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| `short int` | 2 bytes |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| `long int` | 4 bytes |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: Some languages, under certain circumstances, use only a single bit of storage
    for a Boolean variable when that variable is a field of a record or an element
    of an array. We’ll return to this discussion in [Chapters 8](ch08.xhtml#ch08)–[11](ch11.xhtml#ch11)
    when considering composite data structures.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '**7.5 Variable Addresses and High-Level Languages**'
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The organization, class, and type of variables in your programs can affect the
    efficiency of the code that a compiler produces. Additionally, issues like the
    order of declaration, the size of the object, and the placement of the object
    in memory can have a big impact on the running time of your programs. This section
    describes how you can organize your variable declarations to produce efficient
    code.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: As for immediate constants encoded in machine instructions, many CPUs provide
    specialized addressing modes that access memory more efficiently than other, more
    general, addressing modes. Just as you can reduce the size and improve the speed
    of your programs by carefully selecting the constants you use, you can make your
    programs more efficient by carefully choosing how you declare variables. Whereas
    with constants you’re primarily concerned with their values, with variables you
    must consider the address in memory where the compiler places them.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'The 80x86 is a typical example of a CISC processor that provides multiple address
    sizes. When running on a modern 32- or 64-bit operating system like macOS, Linux,
    or Windows, the 80x86 CPU supports three address sizes: 0 bit, 8 bit, and 32 bit.
    The 80x86 uses 0-bit displacements for register-indirect addressing modes. We’ll
    ignore the 0-bit displacement addressing mode for now because 80x86 compilers
    generally don’t use it to access variables you explicitly declare in your code.
    The 8-bit and 32-bit displacement addressing modes are the more interesting ones
    for the current discussion.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '**7.5.1 Allocating Storage for Global and Static Variables**'
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The 32-bit displacement is, perhaps, the easiest to understand. Variables you
    declare in your program, which the compiler allocates in memory rather than in
    a register, have to appear somewhere in memory. On most 32-bit processors, the
    address bus is 32 bits wide, so it takes a 32-bit address to access a variable
    at an arbitrary location in memory. An instruction that encodes this 32-bit address
    can access any memory variable. The 80x86 provides the *displacement-only* addressing
    mode, whose effective address is exactly the 32-bit constant embedded in the instruction.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: A problem with 32-bit addresses (one that gets even worse as we move to 64-bit
    processors with a 64-bit address) is that the address winds up consuming the largest
    portion of the instruction’s encoding. Certain forms of the displacement-only
    addressing mode on the 80x86, for example, have a 1-byte opcode and a 4-byte address.
    Therefore, 80 percent of the instruction’s size is consumed by the address. Were
    the 64-bit variants of the 80x86 (x86-64) to actually encode a 64-bit absolute
    address as part of the instruction, the instruction would be 9 bytes long and
    consume nearly 90 percent of the instruction’s bytes. To avoid this, the x86-64
    modified the displacement-only addressing mode. It no longer encodes the absolute
    address in memory as part of the instruction; instead, it encodes a signed 32-bit
    offset (±2 billion bytes) into the instruction.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: On typical RISC processors, the situation is even worse. Because the instructions
    are uniformly 32 bits long on typical RISC CPUs, you cannot encode a 32-bit address
    as part of the instruction. In order to access a variable at an arbitrary 32-
    or 64-bit address in memory, you need to load the 32- or 64-bit address of that
    variable into a register and then use the register-indirect addressing mode to
    access it. For a 32-bit address, this could require three 32-bit instructions,
    as [Figure 7-2](ch07.xhtml#ch7fig2) demonstrates; that’s expensive in terms of
    both speed and space. It gets even more expensive with 64-bit addresses.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Because RISC CPUs don’t run horribly slower than CISC processors, compilers
    rarely generate code this bad. In reality, programs running on RISC CPUs often
    keep base addresses to blocks of objects in registers, so they can efficiently
    access variables in those blocks using short offsets from the base register. But
    how do compilers deal with arbitrary addresses in memory?
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/07fig02.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-2: RISC CPU access of an absolute address*'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '**7.5.2 Using Automatic Variables to Reduce Offset Sizes**'
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One way to avoid large instruction sizes with large displacements is to use
    an addressing mode with a smaller displacement. The 80x86 (and x86-64), for example,
    provide an 8-bit displacement form for the base-plus-indexed addressing mode.
    This form allows you to access data at an offset of –128 through +127 bytes around
    a base address contained in a register. RISC processors have similar features,
    although the number of displacement bits is usually larger, allowing a greater
    range of addresses.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: By pointing a register at some base address in memory and placing your variables
    near that base address, you can use the shorter forms of these instructions so
    your program will be smaller and run faster. This isn’t too difficult if you’re
    working in assembly language and you have direct access to the CPU’s registers.
    However, if you’re working in an HLL you may not have direct access to the CPU’s
    registers, and even if you did, you probably couldn’t convince the compiler to
    allocate your variables at convenient addresses. How do you take advantage of
    this small-displacement addressing mode in your HLL programs? The answer is that
    you don’t explicitly specify the use of this addressing mode; the compiler does
    it for you automatically.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following trivial function in Pascal:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Upon entry into this function, the compiled code constructs an activation record
    (sometimes called a *stack frame*). An activation record, as you saw earlier in
    the chapter, is a data structure in memory where the system keeps the local data
    associated with a function or procedure. The activation record includes parameter
    data, automatic variables, the return address, temporary variables that the compiler
    allocates, and machine state information (for example, saved register values).
    The runtime system allocates storage for an activation record on the fly and,
    in fact, two different calls to the procedure or function may place the activation
    record at different addresses in memory. In order to access the data in an activation
    record, most HLLs point a register (usually called the *frame pointer*) at the
    activation record, and then the procedure or function references automatic variables
    and parameters at some offset from this frame pointer. Unless you have many automatic
    variables and parameters, or your automatic variables and parameters are quite
    large, these variables generally appear in memory at an offset near the base address.
    This means that the CPU can use a small offset when referencing variables near
    the base address held in the frame pointer. In the Pascal example given earlier,
    parameters `i` and `j` and the local variable `k` would most likely be within
    a few bytes of the frame pointer’s address, so the compiler can encode these instructions
    using a small displacement rather than a large displacement. If your compiler
    allocates local variables and parameters in an activation record, all you have
    to do is arrange your variables in the activation record so that they appear near
    its base address. But how do you do that?
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Construction of an activation record begins in the code that calls a procedure.
    The caller places the parameter data (if any) in the activation record. Then the
    execution of an assembly language `call` (or equivalent) instruction adds the
    return address to the activation record. At this point, construction of the activation
    record continues within the procedure itself. The procedure copies the register
    values and other important state information and then makes room in the activation
    record for local variables. The procedure must also update the frame-pointer register
    (such as EBP on the 80x86, or RBP on the x86-64) so that it points at the base
    address of the activation record.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'To see what a typical activation record looks like, consider the following
    HLA procedure declaration:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Whenever an HLA program calls this `ARDemo` procedure, it builds the activation
    record by pushing the data for the parameters onto the stack in the order they
    appear in the parameter list, from left to right. Therefore, the calling code
    first pushes the value for the `i` parameter, then for the `j` parameter, and
    finally for the `k` parameter. After pushing the parameters, the program calls
    the `ARDemo` procedure. Immediately upon entry into the procedure, the stack contains
    these four items, arranged as shown in [Figure 7-3](ch07.xhtml#ch7fig3), assuming
    the stack grows from high-memory addresses to low-memory addresses (as it does
    on most processors).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/07fig03.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-3: Stack organization immediately upon entry into ARDemo*'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: The first few instructions in `ARDemo` push the current value of the frame-pointer
    register (such as EBP on the 32-bit 80x86, or RBP on the x86-64) onto the stack
    and then copy the value of the stack pointer (ESP/RSP on the 80x86/x86-64) into
    the frame pointer. Next, the code drops the stack pointer down in memory to make
    room for the local variables. This produces the stack organization shown in [Figure
    7-4](ch07.xhtml#ch7fig4) on the 80x86 CPU.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: To access objects in the activation record, you must use offsets from the frame-pointer
    register (EBP in [Figure 7-4](ch07.xhtml#ch7fig4)) to the desired object.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/07fig04.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-4: Activation record for ARDemo (32-bit 80x86)*'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The two items of immediate interest are the parameters and the local variables.
    As [Figure 7-5](ch07.xhtml#ch7fig5) shows, you can access the parameters at positive
    offsets from the frame-pointer register, and the local variables at negative offsets
    from the frame-pointer register.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/07fig05.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-5: Offsets of objects in the ARDemo activation record on the 32-bit
    80x86*'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Intel specifically reserves the EBP/RBP (base-pointer register) to point at
    the base of the activation record. Therefore, compilers typically use this register
    as the frame-pointer register when allocating activation records on the stack.
    Some compilers instead attempt to use the 80x86 ESP/RSP (stack pointer) register
    to point to the activation record because this reduces the number of instructions
    in the program. Whether the compiler uses EBP/RBP, ESP/RSP, or some other register
    as the frame pointer, the bottom line is that the compiler typically points some
    register at the activation record, and most of the local variables and parameters
    are near the activation record’s base address.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in [Figure 7-5](ch07.xhtml#ch7fig5), all the local variables
    and parameters in the `ARDemo` procedure are within 127 bytes of the frame-pointer
    register (EBP). This means that on the 80x86 CPU, an instruction that references
    one of these variables or parameters will be able to encode the offset from EBP
    using a single byte. As mentioned earlier, because of the way the program builds
    the activation record, parameters appear at positive offsets from the frame-pointer
    register, and local variables appear at negative offsets from the frame-pointer
    register.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'For procedures that have only a few parameters and local variables, the CPU
    will be able to access all parameters and local variables using a small offset
    (that is, 8 bits on the 80x86, some possibly larger value on various RISC processors).
    Consider, however, the following C/C++ function:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The activation record for this function on the 32-bit 80x86 appears in [Figure
    7-6](ch07.xhtml#ch7fig6).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/07fig06.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-6: Activation record for BigLocals() function*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '*One difference between this activation record and the ones for the Pascal
    and HLA functions is that C pushes its parameters on the stack in the reverse
    order (that is, it pushes the last parameter first and the first parameter last).
    This difference, however, does not impact our discussion at all.*'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: The important thing to note in [Figure 7-6](ch07.xhtml#ch7fig6) is that the
    local variables `array` and `k` have large negative offsets. With offsets of –1,024
    and –1,028, the displacements from EBP to `array` and `k` are well outside the
    range that the compiler can encode into a single byte on the 80x86\. Therefore,
    the compiler has no choice but to encode these displacements using a 32-bit value.
    Of course, this makes accessing these local variables in the function quite a
    bit more expensive.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Nothing can be done about the array variable in this example (no matter where
    you put it, the offset to the base address of the array will be at least 1,024
    bytes from the activation record’s base address). However, consider the activation
    record in [Figure 7-7](ch07.xhtml#ch7fig7).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/07fig07.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-7: Another possible activation record layout for the BigLocals()
    function*'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'The compiler has rearranged the local variables in this activation record.
    Although it still takes a 32-bit displacement to access the `array` variable,
    accessing `k` now uses an 8-bit displacement (on the 32-bit 80x86) because `k`’s
    offset is –4\. You can produce these offsets with the following code:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In theory, rearranging the order of the variables in the activation record isn’t
    terribly difficult for a compiler to do, so you’d expect the compiler to make
    this modification so that it can access as many local variables as possible using
    small displacements. In practice, not all compilers actually do this optimization,
    for various technical and practical reasons (specifically, it can break some poorly
    written code that makes assumptions about the placement of variables in the activation
    record).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to ensure that the maximum number of local variables in your procedure
    have the smallest possible displacements, the solution is trivial: declare all
    your 1-byte variables first, your 2-byte variables second, your 4-byte variables
    next, and so on, up to the largest local variable in your function. Generally,
    though, you’re probably more interested in reducing the size of the maximum number
    of instructions in your function rather than reducing the size of the offsets
    required by the maximum number of variables in your function. For example, if
    you have 128 1-byte variables and you declare these variables first, you’ll need
    only a 1-byte displacement if you access them. However, if you never access these
    variables, the fact that they have a 1-byte displacement rather than a 4-byte
    displacement saves you nothing. The only time you save any space is when you actually
    access that variable’s value in memory via some machine instruction that uses
    a 1-byte rather than a 4-byte displacement. Therefore, to reduce your function’s
    object code size, you want to maximize the number of instructions that use a small
    displacement. If you refer to a 100-byte array far more often than any other variable
    in your function, you’re probably better off declaring that array first, even
    if it leaves only 28 bytes of storage (on the 80x86) for other variables that
    will use the shorter displacement.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: RISC processors typically use a signed 12-bit or 16-bit offset to access fields
    of the activation record. Thus, you have more latitude with your declarations
    when using a RISC chip (which is good, because when you do exceed the 12-bit or
    16-bit limitation, accessing a local variable gets really expensive). Unless you’re
    declaring one or more arrays that consume more than 2,048 (12 bits) or 32,768
    bytes (combined), the typical compiler for a RISC chip will generate decent code.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: This same argument applies to parameters as well as local variables. However,
    it’s rare to find code passing a large data structure (by value) to a function
    because of the expense involved.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '**7.5.3 Allocating Storage for Intermediate Variables**'
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Intermediate variables are local to one procedure/function but global to another.
    You’ll find them in block-structured languages—like Free Pascal, Delphi, Ada,
    Modula-2, Swift, and HLA—that support nested procedures. Consider the following
    example program in Swift:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that nested procedures can access variables found in the main program (that
    is, global variables) as well as variables found in procedures containing the
    nested procedure (that is, the intermediate variables). As you’ve seen, local
    variable access is inexpensive compared to global variable access (because you
    always have to use a larger offset to access global objects within a procedure).
    Intermediate variable access, as is done in the `procTwo` procedure, is expensive.
    The difference between local and global variable accesses is the size of the offset/displacement
    coded into the instruction, with local variables typically using a shorter offset
    than is possible for global objects. Intermediate accesses, on the other hand,
    typically require several machine instructions. This makes the instruction sequence
    that accesses an intermediate variable several times slower and several times
    larger than accessing a local (or even global) variable.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: The problem with using intermediate variables is that the compiler must maintain
    either a linked list of activation records or a table of pointers to the activation
    records (a *display*) in order to reference intermediate objects. To access an
    intermediate variable, the `procTwo` procedure must either follow a chain of links
    (there would be only one link in this example) or do a table lookup in order to
    get a pointer to `procOne`’s activation record. Worse still, maintaining the display
    of this linked list of pointers isn’t exactly cheap. The work needed to maintain
    these objects has to be done on every procedure/function entry and exit, even
    when the procedure or function doesn’t access any intermediate variables on a
    particular call. Although there are, arguably, some software engineering benefits
    to using intermediate variables (having to do with information hiding) versus
    a global variable, keep in mind that accessing intermediate objects is expensive.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '**7.5.4 Allocating Storage for Dynamic Variables and Pointers**'
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pointer access in an HLL provides another opportunity for optimization in your
    code. Pointers can be expensive to use but, under certain circumstances, they
    can actually make your programs more efficient by reducing displacement sizes.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'A pointer is simply a memory variable whose value is the address of some other
    memory object (therefore, pointers are the same size as an address on the machine).
    Because most modern CPUs support indirection only via a machine register, indirectly
    accessing an object is typically a two-step process: first the code has to load
    the value of the pointer variable into a register, and then it has to refer (indirectly)
    to the object through that register.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following C/C++ code fragment:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here is the corresponding 80x86/HLA assembly code:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Had `pi` been a regular variable rather than pointer object, this code could
    have dispensed with the `mov([ebx], eax);` instruction and simply moved `pi` directly
    into `eax`. Therefore, the use of this pointer variable has both increased the
    program’s size and reduced the execution speed by inserting an extra instruction
    into the code sequence that the compiler generates.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if you indirectly refer to an object several times in close succession,
    the compiler may be able to reuse the pointer value it has loaded into the register,
    amortizing the cost of the extra instruction across several different instructions.
    Consider the following C/C++ code sequence:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here’s the corresponding 80x86/HLA code:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This code loads the actual pointer value into EBX only once. From that point
    forward, the code will simply use the pointer value contained in EBX to reference
    the object at which `pi` is pointing. Of course, any compiler that can do this
    optimization can probably eliminate five redundant memory loads and stores from
    this assembly language sequence, but let’s assume they’re not redundant for the
    time being. Because the code didn’t have to reload EBX with the value of `pi`
    every time it wanted to access the object at which `pi` points, there’s only one
    instruction of overhead (`mov(pi, ebx);`) amortized across six instructions. That’s
    not too bad at all.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, you could make a good argument that this code is more optimal than accessing
    a local or global variable directly. An instruction of the form
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: encodes a 0-bit displacement. Therefore, this move instruction is only 2 bytes
    long rather than 3, 5, or even 6 bytes long. If `pi` is a local variable, it’s
    quite possible that the original instruction that copies `pi` into EBX is only
    3 bytes long (a 2-byte opcode and a 1-byte displacement). Because instructions
    of the form `mov([ebx],eax);` are only 2 bytes long, it only takes three instructions
    to “break even” on the byte count using indirection rather than an 8-bit displacement.
    After the third instruction that references whatever `pi` points at, the code
    involving the pointer is actually shorter.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: You can even use indirection to provide efficient access to a block of global
    variables. As noted earlier, the compiler generally cannot determine the address
    of a global object while it’s compiling your program. Therefore, it has to assume
    the worst case and allow for the largest possible displacement/offset when generating
    machine code to access a global variable. Of course, you’ve just seen that you
    can reduce the size of the displacement value from 32 bits down to 0 bits by using
    a pointer to the object rather than accessing the object directly. Therefore,
    you could take the address of the global object (with the C/C++ `&` operator,
    for example) and then use indirection to access the variable. The problem with
    this approach is that it requires a register (a precious commodity on any processor,
    but especially on the 32-bit 80x86, which has only six general-purpose registers
    to utilize). If you access the same variable many times in rapid succession, this
    0-bit displacement trick can make your code more efficient. However, it’s somewhat
    rare to access the same variable repeatedly in a short sequence of code without
    also needing to access several other variables. This means the compiler may have
    to flush the pointer from the register and reload the pointer value later, reducing
    the efficiency of this approach. If you’re working on a RISC chip or x86-64 with
    many registers, you can probably employ this trick to your advantage. On a processor
    with a limited number of registers, though, you won’t be able to employ it as
    often.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '**7.5.5 Using Records/Structures to Reduce Instruction Offset Sizes**'
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There’s also a trick you can use to access several variables with a single
    pointer: put all those variables into a structure and then use the structure’s
    address. By accessing the fields of the structure via the pointer, you can get
    away with using smaller instructions to access the objects. This works almost
    exactly as you’ve seen for activation records (indeed, activation records are,
    literally, records that the program references indirectly via the frame-pointer
    register). About the only difference between accessing objects indirectly in a
    user-defined record/structure and accessing objects in the activation record is
    that most compilers won’t let you refer to fields in a user structure/record using
    negative offsets. Therefore, you’re limited to about half the number of bytes
    that are normally accessible in an activation record. For example, on the 80x86
    you can access the object at offset 0 from a pointer using a 0-bit displacement
    and objects at offsets 1 through +127 using a single-byte displacement. Consider
    the following C/C++ example that uses this trick:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: A well-designed compiler will load the value of `pv` into a register exactly
    once for this code fragment. Because all the fields of the `vars` structure are
    within 127 bytes of the base address of the structure in memory, an 80x86 compiler
    can emit a sequence of instructions that require only 1-byte offsets, even though
    the `v` variable itself is a static/global object. Note, by the way, that the
    first field in the `vars` structure is special. Because this is at offset 0 in
    the structure, you can use a 0-bit displacement when accessing this field. Therefore,
    it’s a good idea to put your most-often-referenced field first in a structure
    if you’re going to refer to that structure indirectly.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Using indirection in your code does come at a cost. On a limited-register CPU
    such as the 32-bit 80x86, using this approach will tie up a register for a while,
    and that may effectively cause the compiler to generate worse code. If the compiler
    must constantly reload the register with the address of the structure in memory,
    the savings from this technique evaporate rather quickly. Tricks such as this
    one vary in effectiveness across different processors (and different compilers
    for the same processor), so be sure to look at the code your compiler generates
    to verify that a trick is actually saving rather than costing you something.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '**7.5.6 Storing Variables in Machine Registers**'
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While we’re on the subject of registers, it’s worthwhile to point out one other
    0-bit displacement way to access variables in your programs: by keeping them in
    machine registers. Machine registers are always the most efficient place to store
    variables and parameters. Unfortunately, only in assembly language and, to a limited
    extent, C/C++, do you have any control over whether the compiler should keep a
    variable or parameter in a register. In some respects, this is not bad. Good compilers
    do a much better job of register allocation than the casual programmer does. However,
    an expert programmer can do a better job of register allocation than a compiler,
    because the expert programmer understands the data the program will be processing
    and the frequency of access to a particular memory location. (And of course, the
    expert programmer can first look at what the compiler is doing, whereas the compiler
    doesn’t have the benefit of seeing what the programmer has done.)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Some languages, such as Delphi, provide limited support for programmer-directed
    register allocation. In particular, the Delphi compiler allows you to tell it
    to pass the first three (ordinal) parameters for a function or procedure in the
    EAX, EDX, and ECX registers. This option is known as the *fastcall calling convention*,
    and several C/C++ compilers support it as well.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: In Delphi and certain other languages, opting for the fastcall parameter passing
    convention is the only control you get. The C/C++ language, however, provides
    the `register` keyword, a storage specifier (much like the `const`, `static`,
    and `auto` keywords) that tells the compiler that the programmer expects to use
    the variable frequently so the compiler should attempt to keep it in a register.
    Note that the compiler can also choose to ignore the `register` keyword (in which
    case it reserves variable storage using automatic allocation). Many compilers
    ignore the `register` keyword altogether because the compiler’s authors assume,
    somewhat arrogantly, that they can do a better job of register allocation than
    any programmer. Of course, on some register-starved machines such as the 32-bit
    80x86, there are so few registers to work with that it might not even be possible
    to allocate a variable to a register throughout the execution of some function.
    Nevertheless, some compilers do respect the programmer’s wishes and *will* allocate
    a few variables in registers if you request that they do so.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Most RISC compilers reserve several registers for passing parameters and several
    registers for local variables. Therefore, it’s a good idea (if possible) to place
    the parameters you access most frequently first in the parameter declaration because
    they’re probably the ones the compiler would allocate in a register.^([9](footnotes.xhtml#ch7fn9))
    The same is true for local variable declarations. Always declare frequently used
    local variables first, because many compilers may allocate those (ordinal) variables
    in registers.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: One problem with compiler register allocation is that it is static. That is,
    the compiler determines which variables to place in registers based on an analysis
    of your source code during compilation, not during runtime. Compilers often make
    assumptions (which are usually correct) like “this function references variable
    `xyz` far more often than any other variable, so it’s a good candidate for a register
    variable.” Indeed, by placing the variable in a register, the compiler will certainly
    reduce the size of the program. However, it could also be the case that all those
    references to `xyz` sit in code that rarely, if ever, executes. Although the compiler
    might save some space (by emitting smaller instructions to access registers rather
    than memory), the code won’t run appreciably faster. After all, if the code rarely
    or never executes, then making that code run faster does not contribute much to
    the program’s execution time. On the other hand, it’s also quite possible to bury
    a single reference to some variable in a deeply nested loop that executes many
    times. With only one reference in the entire function, the compiler’s optimizer
    may overlook the fact that the executing program references the variable frequently.
    Although compilers have gotten smarter about handling variables inside loops,
    the fact is, no compiler can predict how many times an arbitrary loop will execute
    at runtime. Human beings are much better at predicting this sort of behavior (or,
    at least, measuring it with a profiler) and thus are best positioned to make good
    decisions about variable allocation in registers.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '**7.6 Variable Alignment in Memory**'
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On many processors (particularly RISC), there is another efficiency concern
    you must take into consideration. Many modern processors will not let you access
    data at an arbitrary address in memory. Instead, all accesses must take place
    on some native boundary (usually 4 bytes) that the CPU supports.^([10](footnotes.xhtml#ch7fn10))
    Even when a CISC processor allows memory accesses at arbitrary byte boundaries,
    it’s often more efficient to access primitive objects (bytes, words, and double
    words) on a boundary that is a multiple of the object’s size (see [Figure 7-8](ch07.xhtml#ch7fig8)).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/07fig08.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-8: Variable alignment in memory*'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: If the CPU supports unaligned accesses—that is, if the CPU allows you to access
    a memory object on a boundary that is not a multiple of the object’s primitive
    size—then you should be able to pack the variables into the activation record.
    This way, you would obtain the maximum number of variables having a short offset.
    However, because unaligned accesses are sometimes slower than aligned accesses,
    many optimizing compilers insert *padding bytes* into the activation record in
    order to ensure that all variables are aligned on a reasonable boundary for their
    native size (see [Figure 7-9](ch07.xhtml#ch7fig9)). This trades off slightly better
    performance for a slightly larger program.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/07fig09.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-9: Padding bytes in an activation record*'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if you put all your double-word declarations first, your word declarations
    second, your byte declarations third, and your array/structure declarations last,
    you can improve both the speed and size of your code. The compiler usually ensures
    that the first local variable you declare appears at a reasonable boundary (typically
    a double-word boundary). By declaring all your double-word variables first, you
    ensure that they all appear at an address that is a multiple of 4 (because compilers
    usually allocate adjacent variables in your declarations in adjacent locations
    in memory). The first word-sized object you declare will also appear at an address
    that is a multiple of 4—and that means its address is also a multiple of 2 (which
    is best for word accesses). By declaring all your word variables together, you
    ensure that each one appears at an address that is a multiple of 2\. On processors
    that allow byte access to memory, the placement of the byte variables (with respect
    to efficiently accessing the byte data) is irrelevant. By declaring all your local
    byte variables last in a procedure or function, you generally ensure that such
    declarations do not impact the performance of the double-word and word variables
    you also use in the function. [Figure 7-10](ch07.xhtml#ch7fig10) shows what a
    typical activation record will look like if you declare your variables as in the
    following function:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![Image](../images/07fig10.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-10: Aligned variables in an activation record (32-bit 80x86)*'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Note how all the double-word variables (`d1`, `d2`, and `d3`) begin at addresses
    that are multiples of 4 (–4, –8, and –12). Also, notice how all the word-sized
    variables (`w1` and `w2`) begin at addresses that are multiples of 2 (–14 and
    –16). The byte variables (`b1`, `b2`, and `b3`) begin at arbitrary addresses in
    memory (both even and odd addresses).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'Now consider the following function, which has arbitrary (unordered) variable
    declarations, and the corresponding activation record shown in [Figure 7-11](ch07.xhtml#ch7fig11):'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![Image](../images/07fig11.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-11: Unaligned variables in an activation record (32-bit 80x86)*'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, every variable except the byte variables appears at an address
    that is inappropriate for the object. On processors that allow memory accesses
    at arbitrary addresses, it may take more time to access a variable that is not
    aligned on an appropriate address boundary.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Some processors don’t allow a program to access an object at an unaligned address.
    Most RISC processors, for example, can’t access memory except at 32-bit address
    boundaries. To access a short or byte value, some RISC processors require the
    software to read a 32-bit value and extract the 16-bit or 8-bit value (that is,
    the CPU forces the software to treat bytes and words as packed data). The extra
    instructions and memory accesses needed to pack and unpack this data reduce the
    speed of memory access by a considerable amount (two or more instructions—usually
    more—may be needed to fetch a byte or word from memory). Writing data to memory
    is even worse because the CPU must first fetch the data from memory, merge the
    new data with the old data, and then write the result back to memory. Therefore,
    most RISC compilers won’t create an activation record similar to the one in [Figure
    7-11](ch07.xhtml#ch7fig11). Instead, they’ll add padding bytes so that every memory
    object begins at an address boundary that is a multiple of 4 bytes (see [Figure
    7-12](ch07.xhtml#ch7fig12)).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/07fig12.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-12: RISC compilers force aligned access by adding padding bytes.*'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Notice in [Figure 7-12](ch07.xhtml#ch7fig12) that all of the variables are at
    addresses that are multiples of 32 bits. Therefore, a RISC processor has no problems
    accessing any of these variables. The cost, of course, is that the activation
    record is quite a bit larger (the local variables consume 32 bytes rather than
    19 bytes).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Although the example in [Figure 7-12](ch07.xhtml#ch7fig12) is typical for 32-bit
    RISC-based compilers, that’s not to suggest that compilers for CISC CPUs don’t
    do this as well. Many compilers for the 80x86, for example, also build this activation
    record in order to improve the performance of the code the compiler generates.
    Although declaring your variables in a misaligned fashion may not slow down your
    code on a CISC CPU, it may use additional memory.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if you work in assembly language, it’s generally up to you to declare
    your variables in a manner that is appropriate or efficient for your particular
    processor. In HLA (on the 80x86), for example, the following two procedure declarations
    result in the activation records shown in [Figures 7-10](ch07.xhtml#ch7fig10),
    [7-11](ch07.xhtml#ch7fig11), and [7-12](ch07.xhtml#ch7fig12).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: HLA procedures someFunction and someFunction3 will produce the fastest-running
    code on any 80x86 processor because all variables are aligned on an appropriate
    boundary; HLA procedures someFunction and someFunction2 will produce the most
    compact activation records on an 80x86 CPU, because there is no padding between
    variables in the activation record. If you’re working in assembly language on
    a RISC CPU, you’ll probably want to choose the equivalent of someFunction or someFunction3
    to make it easier to access the variables in memory.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '**7.6.1 Records and Alignment**'
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Records/structures in HLLs also have alignment issues that should concern you.
    Recently, CPU manufacturers have been promoting *application binary interface
    (ABI)* standards to promote interoperability between different programming languages
    and their implementations. Although not all languages and compilers adhere to
    these suggestions, many of the newer compilers do. Among other things, these ABI
    specifications describe how the compilers should organize fields within a record
    or structure object in memory. Although the rules vary by CPU, one that applies
    to most ABIs is that a compiler should align a record/structure field at an offset
    that is a multiple of the object’s size. If two adjacent fields in the record
    or structure have different sizes, and the placement of the first field in the
    structure would cause the second field to appear at an offset that is not a multiple
    of that second field’s native size, then the compiler will insert some padding
    bytes to push the second field to a higher offset that is appropriate for that
    second object’s size.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: In actual practice, ABIs for different CPUs and OSes have minor differences
    based on the CPUs’ ability to access objects at different addresses in memory.
    Intel, for example, suggests that compiler writers align bytes at any offset,
    words at even offsets, and everything else at offsets that are a multiple of 4\.
    Some ABIs recommend placing 64-bit objects at 8-byte boundaries within a record.
    The x86-64 SSE and AVX instructions require 16- and 32-byte alignment for 128-bit
    and 256-bit data values. Some CPUs, which have a difficult time accessing objects
    smaller than 32 bits, may suggest a minimum alignment of 32 bits for all objects
    in a record/structure. The rules vary depending on the CPU and whether the manufacturer
    wants to promote faster-executing code (the usual case) or smaller data structures.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: If you are writing code for a single CPU (such as an Intel-based PC) with a
    single compiler, learn that compiler’s rules for padding fields and adjust your
    declarations for maximum performance and minimal waste. However, if you ever need
    to compile your code using several different compilers, particularly compilers
    for several different CPUs, following one set of rules will work fine on one machine
    and produce less efficient code on several others. Fortunately, there are some
    rules that can help reduce the inefficiencies created by recompiling for a different
    ABI.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'From a performance/memory usage standpoint, the best solution is the same rule
    we saw earlier for activation records: when declaring fields in a record, group
    all like-sized objects together and put all the larger (scalar) objects first
    and the smaller objects last in the record/structure. This scheme produces the
    least amount of waste (padding bytes) and provides the highest performance across
    most of the existing ABIs. The only drawback to this approach is that you have
    to organize the fields by their native size rather than by their logical relationship
    to one another. However, because all fields of a record/structure are logically
    related insofar as they are all members of that same record/structure, this problem
    isn’t as bad as employing this organization for all of a particular function’s
    local variables.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Many programmers try to add padding fields themselves to a structure. For example,
    the following type of code is common in the Linux kernel and other bits and pieces
    of overly hacked software:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `padding0` and `padding1` fields in this structure were added to manually
    align the `dwordValue` and `dwordValue2` fields at offsets that are even multiples
    of 4.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: While this padding is not unreasonable, if you’re using a compiler that doesn’t
    automatically align the fields, remember that an attempt to compile this code
    on a different machine can produce unexpected results. For example, if a compiler
    aligns all fields on a 32-bit boundary, regardless of size, then this structure
    declaration will consume two extra double words to hold the two `paddingX` arrays.
    This winds up wasting space for no good reason. Keep this fact in mind if you
    decide to manually add the padding fields.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Many compilers that automatically align fields in a structure provide an option
    to turn off this feature. This is particularly true for compilers generating code
    for CPUs where the alignment is optional and the compiler does it only to achieve
    a slight performance boost. If you’re going to manually add padding fields to
    your record/structure, you need to specify this option so that the compiler doesn’t
    realign the fields after you’ve manually aligned them.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: In theory, a compiler is free to rearrange the offsets of local variables within
    an activation record. However, it would be extremely rare for a compiler to rearrange
    the fields of a user-defined record or structure. Too many external programs and
    data structures depend on the fields of a record appearing in the same order as
    they are declared. This is particularly true when passing record/structure data
    between code written in two separate languages (for example, when calling a function
    written in assembly language) or when dumping record data directly to a disk file.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'In assembly language, the amount of effort needed to align fields varies from
    pure manual labor to a rich set of features capable of automatically handling
    almost any ABI. Some (low-end) assemblers don’t even provide record or structure
    data types. In such systems, the assembly programmer has to manually specify the
    offsets into a record structure (typically by declaring, as constants, the numeric
    offsets into the structure). Other assemblers (for example, NASM) provide macros
    that automatically generate the equates for you. In these systems, the programmer
    has to manually provide padding fields to align certain fields on a given boundary.
    Some assemblers, such as MASM, provide simple alignment facilities. You may specify
    the value `1`, `2`, or `4` when declaring a `struct` in MASM and the assembler
    will align all fields on either the alignment value you specify or at an offset
    that is a multiple of the object’s size, whichever is smaller, by automatically
    adding padding bytes to the structure. Also, note that MASM adds a sufficient
    number of padding bytes to the end of the structure so that the whole structure’s
    length is a multiple of the alignment size. Consider the following `struct` declaration
    in MASM:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In this example, MASM will add an extra byte of padding to the end of the structure
    so that its length is a multiple of 2 bytes.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'MASM also lets you control the alignment of individual fields within a structure
    by using the `align` directive. The following structure declaration is equivalent
    to the current example (note the absence of the alignment value operand in the
    `struct` operand field):'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The default field alignment for MASM structures is unaligned. That is, a field
    begins at the next available offset within the structure, regardless of the field’s
    (and the previous field’s) size.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'The High-Level Assembly (HLA) language probably provides the greatest control
    (both automatic and manual) over record field alignment. As with MASM, the default
    record alignment is unaligned. Also as with MASM, you can use HLA’s `align` directive
    to manually align fields in an HLA record. The following is the HLA version of
    the previous MASM example:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'HLA also lets you specify an automatic alignment for all fields in a record.
    For example:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: There is a subtle difference between this HLA record and the earlier MASM structure
    (with automatic alignment). Remember, when you specify a directive of the form
    `Student struct 2`, MASM aligns all fields on a boundary that is a multiple of
    2 or a multiple of the object’s size, *whichever is smaller*. HLA, on the other
    hand, will always align all fields on a 2-byte boundary using this declaration,
    even if the field is a byte.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'The ability to force field alignment to a minimum size is a nice feature if
    you’re working with data structures generated on a different machine (or compiler)
    that forces this kind of alignment. However, this type of alignment can unnecessarily
    waste space in a record for certain declarations if you only want the fields to
    be aligned on their natural boundaries (which is what MASM does). Fortunately,
    HLA provides another syntax for record declarations that lets you specify both
    the maximum and minimum alignment that HLA will apply to a field:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The `maxAlign` item specifies the largest alignment that HLA will use within
    the record. HLA will align any object whose native size is larger than `maxAlign`
    on a boundary of `maxAlign` bytes. Similarly, HLA will align any object whose
    size is smaller than `minAlign` on a boundary of at least `minAlign` bytes. HLA
    will align objects whose native size is between `minAlign` and `maxAlign` on a
    boundary that is a multiple of that object’s size. The following HLA and MASM
    record/structure declarations are equivalent:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the MASM code:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here’s the HLA code:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Although few HLLs provide facilities within the language’s design to control
    the alignment of fields within records (or other data structures), many compilers
    provide extensions to those languages, in the form of compiler pragmas, that let
    programmers specify default variable and field alignment. Because few languages
    have standards for this, you’ll have to check your particular compiler’s reference
    manual (note that C++11 is one of the few languages that provides alignment support).
    Although such extensions are nonstandard, they are often quite useful, especially
    when you’re linking code compiled by different languages or trying to squeeze
    the last bit of performance out of a system.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '**7.7 For More Information**'
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Aho, Alfred V., Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman. *Compilers:
    Principles, Techniques, and Tools*. 2nd ed. Essex, UK: Pearson Education Limited,
    1986.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'Barrett, William, and John Couch. *Compiler Construction: Theory and Practice*.
    Chicago: SRA, 1986.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Dershem, Herbert, and Michael Jipping. *Programming Languages, Structures and
    Models*. Belmont, CA: Wadsworth, 1990.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'Duntemann, Jeff. *Assembly Language Step-by-Step*. 3rd ed. Indianapolis: Wiley,
    2009.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'Fraser, Christopher, and David Hansen. *A Retargetable C Compiler: Design and
    Implementation*. Boston: Addison-Wesley Professional, 1995.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'Ghezzi, Carlo, and Jehdi Jazayeri. *Programming Language Concepts*. 3rd ed.
    New York: Wiley, 2008.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'Hoxey, Steve, Faraydon Karim, Bill Hay, and Hank Warren, eds. *The PowerPC
    Compiler Writer’s Guide*. Palo Alto, CA: Warthman Associates for IBM, 1996.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyde, Randall. *The Art of Assembly Language*. 2nd ed. San Francisco: No Starch
    Press, 2010.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Intel. “Intel 64 and IA-32 Architectures Software Developer Manuals.” Updated
    November 11, 2019\. *[https://software.intel.com/en-us/articles/intel-sdm](https://software.intel.com/en-us/articles/intel-sdm)*.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'Ledgard, Henry, and Michael Marcotty. *The Programming Language Landscape*.
    Chicago: SRA, 1986.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'Louden, Kenneth C. *Compiler Construction: Principles and Practice*. Boston:
    Cengage, 1997.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'Louden, Kenneth C., and Kenneth A. Lambert. *Programming Languages: Principles
    and Practice*. 3rd ed. Boston: Course Technology, 2012.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'Parsons, Thomas W. *Introduction to Compiler Construction*. New York: W. H.
    Freeman, 1992.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'Pratt, Terrence W., and Marvin V. Zelkowitz. *Programming Languages, Design
    and Implementation*. 4th ed. Upper Saddle River, NJ: Prentice Hall, 2001.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'Sebesta, Robert. *Concepts of Programming Languages*. 11th ed. Boston: Pearson,
    2016.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
