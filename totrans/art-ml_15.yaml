- en: '**11'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LINEAR MODELS ON STEROIDS: NEURAL NETWORKS**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The method of neural networks (NNs) is probably the best-known ML technology
    among the general public. The science fiction−sounding name is catchy—even more
    so with the advent of the term *deep learning*—and NNs have become the favorite
    approach to image classification in applications that also intrigue the general
    public, such as facial recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet NNs are probably the most challenging ML technology to use well, with problems
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: “Black box” operation, where it’s not clear what’s going on inside
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numerous hyperparameters to tune
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tendency toward overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possibly lengthy computation time, with some large-data cases running for hours
    or even days when large amounts of RAM may be needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convergence issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see what all the fuss is about.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The term *neural network* alludes to an ML method that is inspired by the biology
    of human thought. In a two-class classification problem, for instance, the predictor
    variables serve as inputs to a *neuron*, outputting 1 or 0, with 1 meaning that
    the neuron *fires*—and we decide class 1\. NNs consist of several *hidden layers*
    in which the outputs of one layer of neurons are fed into the next layer and so
    on, until the process reaches the final output layer. This, too, has been given
    biological interpretation. The terms *node* and *units* are synonymous with *neurons*.
  prefs: []
  type: TYPE_NORMAL
- en: The method was later generalized, using *activation functions* with outputs
    other than just 1 and 0 and allowing backward feedback from later layers to earlier
    ones. This led development of the field somewhat away from the biological motivation,
    and some questioned the biological interpretation anyway, but NNs have a strong
    appeal for many in the machine learning community. Indeed, well-publicized large
    projects using *deep learning* have revitalized interest in NNs.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-1](ch11.xhtml#ch11fig01), generated by the `neuralnet` package on
    our vertebrae data, illustrates how the method works. (We will not be using that
    package, but it does produce nice displays.)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch11fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-1: Vertebrae NN with one hidden layer*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the overview:'
  prefs: []
  type: TYPE_NORMAL
- en: A neural network consists of a number of *layers* (three in this case).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In pictures describing a particular network, there is an input layer on the
    far left (the vertebrae measurements here) and an output layer on the far right,
    which, in this case, is giving the class predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are one or more *hidden* layers in between, with one in this case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The outputs of one layer are fed as inputs into the next layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output is typically a single number, for regression problems, and *c* numbers,
    for *c*-class classification problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inputs into a layer are fed through what amounts to a linear model. The
    outputs of a layer are fed through an *activation function*, which is analogous
    to kernel functions in SVM, to accommodate nonlinear relations. In [Figure 11-1](ch11.xhtml#ch11fig01),
    the activation function used was our old friend the logit, *a*(*t*) = 1/[1 + exp
    (−*t*)] (though in an entirely different context; we are *not* performing logistic
    regression).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does all this play out in [Figure 11-1](ch11.xhtml#ch11fig01)? Let’s look
    at some of the numbers in the diagram. For example, the input to the first circle,
    center column is 1.0841 · 1 + 0.84311 V1 + 0.49439 V2 + . . . , which is a linear
    combination of the features. A different linear combination is fed into the second
    circle. The output of each circle is fed into the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: How are the coefficients (*weights*) in these linear combinations computed?
    We forgo a detailed mathematical answer here, but in essence, we minimize the
    sum of squared prediction errors in the regression case. In the classification
    case, we choose the weights to minimize the overall misclassification rate or
    a variant thereof.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 Working on Top of a Complex Infrastructure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we begin, a few words are in order regarding `qeNeural()`, our `qe*`-series
    function for building neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve noted throughout the book, the `qe*`-series functions are mainly wrappers—that
    is, convenient wrappers to other functions. This is done so that the series can
    provide a uniform, quick-and-easy user interface to a variety of ML algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our function `qeSVM()`, for instance, wraps the `svm()` function in the package
    `e1071`. What about `qeNeural()`? There is quite a tale here! What happens (approximately)
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: The function `qeNeural()` wraps the `regtools()` function `krsFit()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function `krsFit()` wraps a number of functions in the R `keras` package
    for NNs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The R `keras` package wraps the R `tensorflow` package.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The R `tensorflow` package wraps the Python package of the same name.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And much of `tensorflow` is actually written in the C language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And much of this, in turn, depends on the function `reticulate()` from the package
    of the same name. Its role is to translate between R and Python.
  prefs: []
  type: TYPE_NORMAL
- en: Setting this up, then, can be a bit delicate. See the RStudio site for help
    with your particular platform (for instance, [*https://tensorflow.rstudio.com/tutorials/quickstart/beginner.xhtml*](https://tensorflow.rstudio.com/tutorials/quickstart/beginner.xhtml)).
    The R interfaces in the above list, as well as `reticulate`, were developed by
    RStudio.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to keep these points in mind about the “bilingual” nature of
    the software. For example, one implication is that even if you call `set.seed()`
    before your NNs run, you still will notice some variation from one run to the
    next. This will mystify you if you don’t know that Python has its own random number
    generator!
  prefs: []
  type: TYPE_NORMAL
- en: '11.3 Example: Vertebrae Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Say we wish to fit a model and then do prediction. As before, we’ll specify
    no holdout set so that as much data as possible is used in the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The fitting process is iterative, and a report is given on each iteration or
    *epoch*. The number of epochs is a hyperparameter. This and the other hyperparameters
    will be discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: As an example of prediction, consider a patient similar to the first one in
    our data, but with V2 being 18 rather than 22.55\. What would be our predicted
    class?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We predict class DH.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4 Neural Network Hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NN libraries are notorious for having tons of hyperparameters. Our `qeNeural()`
    function has been designed to avoid this, having only a few hyperparameters. The
    call form is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is what the NN-specific arguments represent:'
  prefs: []
  type: TYPE_NORMAL
- en: hidden   Specifies the number of hidden layers and number of units per layer
    (this need not be constant across layers). The default means two hidden layers
    with 100 units each. If a number in this vector is fractional, it indicates *dropout*,
    which will be discussed below.
  prefs: []
  type: TYPE_NORMAL
- en: nEpoch   Specifies the number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: acts   Specifies the activation functions, with one for each hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: learnRate   Very similar to what we saw in gradient boosting (see [Section 6.3.8](ch06.xhtml#ch06lev3sec8)).
  prefs: []
  type: TYPE_NORMAL
- en: conv, xShape   Arguments used in image classification settings, which will be
    discussed in [Chapter 12](ch12.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: 'The analyst can use the `keras` package directly for more detailed control.
    These arguments address one or both of these aims:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Controlling the Bias-Variance Trade-off: `hidden`, `nEpoch`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dealing with convergence issues: `nEpoch`, `acts`, `learnRate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reader may be surprised to see the number of epochs—that is, the number
    of iterations—in the Bias-Variance Trade-off list above. In most iterative algorithms,
    the more iterations the better. But empirically, analysts have found that having
    too many iterations in NNs may result in overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5 Activation Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we were to simply input and output linear functions at each layer, we would
    have linear functions of linear functions of linear functions . . . , which would
    still be a linear function after all that combining. To be able to model nonlinear
    relations, we instead place *activation functions*, *a*(*t*), at the output of
    each layer.
  prefs: []
  type: TYPE_NORMAL
- en: Over the years, there has been some debate as to good choices for the activation
    function. In principle, any nonlinear function should work, but issues do arise,
    especially concerning the all-important convergence problem.
  prefs: []
  type: TYPE_NORMAL
- en: Consider once again [Figure 6-2](ch06.xhtml#ch06fig02). The minimum around 2.2
    comes at a rather sharp dip (in calculus terms, a large second derivative). But
    what if the curve were to look like that in [Figure 11-2](ch11.xhtml#ch11fig02)?
    There is a rather shallow trough near the minimum, extending, say, between −4
    and 4\. Here even a larger learning rate might have us spending many iterations
    with almost no progress. This is the *vanishing gradient problem*. And if the
    curve is very sharp near the minimum, we may have an *exploding gradient problem*,
    which can wreak havoc with even very small learning rates.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch11fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-2: Shallow minimum region*'
  prefs: []
  type: TYPE_NORMAL
- en: The choice of activation function plays a big role in these things. There is
    a multiplicative effect across layers. (Again, for those who know calculus, this
    is the Chain Rule in action.) And quantities in the interval (−1,1) become smaller
    and smaller when multiplied together, so that multiplicative effect results in
    smaller and smaller numbers, hence a vanishing gradient. If the gradient is large
    at each layer, we may develop an exploding gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'After years of trial and error, the popular choice among NN users today is
    the *Rectified Linear Unit (ReLU)*: *f*(*x*) is 0 for *x* < 0 but is equal to
    *x* for *x* ≥ 0.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.6 Regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As noted, NNs have a tendency to overfit, with many having thousands of weights
    and some even millions. Remember, the weights are essentially linear regression
    coefficients, so the total number of weights is effectively the new value of *p*
    (that is, our number of features). We must find some way to reduce that value.
  prefs: []
  type: TYPE_NORMAL
- en: '***11.6.1 L1 and L2 Regularization***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since NNs (typically) minimize a sum of squares, we can apply a penalty term
    to reduce the size of the solution, just as in the cases of ridge regression and
    the LASSO. Recall also that in the LASSO, with the *ℓ*[1] penalty, this tends
    to produce a sparse solution, with most coefficients being 0s.
  prefs: []
  type: TYPE_NORMAL
- en: Well, that is exactly what we want here. We fear we have too many weights, and
    we hope that applying an *ℓ*[1] penalty will render most of them nil.
  prefs: []
  type: TYPE_NORMAL
- en: However, that may not happen with NNs due to the use of nonlinear activation
    functions. The problem is that the contours in [Figure 9-3](ch09.xhtml#ch09fig03)
    are no longer ellipses, and thus the “first contact point” will not likely be
    at a corner of the diamond.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, *ℓ*[1] will still shrink the weights, as will *ℓ*[2], so we should
    achieve dimension reduction in some sense.
  prefs: []
  type: TYPE_NORMAL
- en: '***11.6.2 Regularization by Dropout***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If a weight is 0, then in a picture of the network, such as [Figure 11-1](ch11.xhtml#ch11fig01),
    the corresponding link is removed. So, if the goal is to remove some links, why
    not simply remove some links directly? Or better yet, remove entire nodes. That
    is exactly what *dropout* does.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if our dropout rate is 0.2, we randomly (and temporarily) choose
    20 percent of the links from the given layer and remove them. There are further
    details that we will not list here, but this is the essence of the method.
  prefs: []
  type: TYPE_NORMAL
- en: '11.7 Example: Fall Detection Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s revisit the dataset analyzed in [Section 8.9.4](ch08.xhtml#ch08lev9sec4).
    We’ll do a grid search for a good hyperparameter combination.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the `qeFT()` argument `pars` defines the grid in that it specifies
    the range of values we wish to explore.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: So, we are varying the number of neurons per layer (5, 100, 250) and the dropout
    rate (none, 0.2, 0.5). We could also have varied `nEpoch` and even the activation
    functions. Note, too, that we could have tried having different numbers of neurons
    in different layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing to notice is how much smaller the smallest value is than the
    largest. In fact, the latter is actually about the same as the base accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Without the features, we would have an error rate of 72 percent.
  prefs: []
  type: TYPE_NORMAL
- en: So, exploring the use of different values of the hyperparameter really paid
    off here.
  prefs: []
  type: TYPE_NORMAL
- en: But still, interesting patterns emerge here, notably the effect of the learning
    rate. The smaller values tended to do poorly. Remember, if our learning rate is
    too small, not only might it slow down convergence, but it also may leave us stuck
    at a local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, note that in this case, a smaller value for the dropout rate seemed
    to produce better results.
  prefs: []
  type: TYPE_NORMAL
- en: '11.8 Pitfall: Convergence Problems'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As noted, it is often a challenge to configure NN analysis so that proper convergence
    to a good solution is attained. In some cases, one might even encounter the *broken
    clock problem*—that is, the network predicts the same value no matter what the
    inputs are.
  prefs: []
  type: TYPE_NORMAL
- en: 'Or, one might encounter output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here `nan` stands for “not a number.” That ominous-sounding message may mean
    the code attempted to divide by 0, which may be due to the vanishing gradient
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: The following describes a few tricks we can try, typically specified via one
    or more hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, convergence problems may be solved by scaling the data, either
    using the R `scale()` function or by mapping to [0,1]. It is recommended that
    one routinely scale one’s data; in `qeNeural()`, scaling is actually hardwired
    into the software.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some values to tweak:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning rate**   Discussed in [Section 6.3.8](ch06.xhtml#ch06lev3sec8).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Activation function**   Try changing to one with a steeper/shallower slope.
    For example, the function *a*(*t*) = 1/(1 + exp (−2*t*)) is steeper around *t*
    = 0 than the ordinary logistic function.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Early stopping**   In most algorithms, the more iterations the better, but
    in NNs, many issues depart from conventional wisdom. Running the algorithm for
    too long may result in convergence to a poor solution. This leads to the notion
    of *early stopping*, of which there are many variants.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Momentum**   The rough idea here is that “We’re on a roll,” with the last
    few epochs producing winning moves in the right direction, reducing validation
    error each time. So, instead of calculating the next step size individually, why
    not combine the last few step sizes? The next step size will be set to a weighted
    average of the last few, with heavier weight on the more recent ones. (This hyperparameter
    is not available in `qeNeural()` but may be accessed through the `keras` package
    directly.)'
  prefs: []
  type: TYPE_NORMAL
- en: Note that regression applications, as opposed to classification, may be especially
    prone to convergence problems, since *Y* is unbounded.
  prefs: []
  type: TYPE_NORMAL
- en: 11.9 Close Relation to Polynomial Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [Section 8.11](ch08.xhtml#ch08lev11), we introduced polynomial regression,
    a linear model in which the features are in polynomial form. So, for instance,
    instead of just having people’s heights and ages as features, in a quadratic model
    we now would also have the squares of heights and ages, as well as a cross-product
    term, height × age.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomials popped up again in SVM, with polynomial kernels. We might have,
    for instance, not just height and age but also the squares of heights and ages,
    as well the height × age term. And we noted that even the use of the radial basis
    function, a nonpolynomial kernel, is approximately a polynomial due to Taylor
    series expansion.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that NNs essentially do polynomial regression as well. To see this,
    let’s look again at [Figure 11-1](ch11.xhtml#ch11fig01). Suppose we take as our
    activation function the squaring function *t*² . That is not a common choice at
    all, but we’ll start with that and then extend the argument.
  prefs: []
  type: TYPE_NORMAL
- en: So, in the hidden layer in [Figure 11-1](ch11.xhtml#ch11fig01), a circle forms
    a linear combination of the inputs and then outputs the square of the linear combination.
    That means the outputs of the hidden layer are second-degree polynomials in the
    inputs. If we were to have a second hidden layer, its outputs would be fourth-degree
    polynomials.
  prefs: []
  type: TYPE_NORMAL
- en: What if our activation function itself were to be a polynomial? Then again,
    each successive layer would give us higher and higher degree polynomials in the
    inputs. Since NNs minimize the sum of squared prediction errors, just as in the
    linear model, you can see that the minimizing solution will be that of polynomial
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: And what about the popular activation functions? One is the *hyperbolic tangent*,
    *tanh*(*t*), whose graph looks similar to the logistic function. But it too has
    a Taylor series expansion, so what we are doing is approximately polynomial regression.
  prefs: []
  type: TYPE_NORMAL
- en: ReLU does not have a Taylor series expansion, but we can form a polynomial approximation
    there too.
  prefs: []
  type: TYPE_NORMAL
- en: In that case, why not just use polynomial regression in the first place? Why
    NNs? One answer is that it just would be computationally infeasible for large-
    *p* data, where we could have a very large number of polynomial terms in calling
    `lm()` or `glm()`. This would cause memory issues. (It’s less of a problem for
    NNs because they find the least squares solutions iteratively. This may cause
    convergence problems but at least uses less memory.) The kernel trick is very
    helpful here, and there is even the *kernel ridge regression* method that applies
    this to linear ridge models, but it turns out that this too is infeasible for
    large-*n* cases.
  prefs: []
  type: TYPE_NORMAL
- en: NNs have their own computational issues, as noted, but through trying many combinations
    of hyperparameters, we may still have a good outcome. Also, if we manage to find
    a good NN fit on some classes of problems, sometimes we can tweak it to find a
    good NN fit on some related class (*transfer learning*).
  prefs: []
  type: TYPE_NORMAL
- en: 11.10 Bias vs. Variance in Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We often refer to the number of hidden layers as the *depth* of a network and
    the number of units per layer as the *width*. The larger the product of these
    two (actually depth times the square of the width), the more weights or parameters
    the network has. As discussed in [Section 8.10.1](ch08.xhtml#ch08lev10sec1), the
    more parameters a model has, the more variance increases, even though bias is
    reduced.
  prefs: []
  type: TYPE_NORMAL
- en: This can be seen as well in light of the polynomial regression connection to
    NNs described in the previous section. Roughly speaking, the larger the number
    of hidden layers in an NN, the higher the degree of a polynomial regression approximation.
    And the higher the degree of a polynomial regression model, the smaller the bias
    but the larger the variance.
  prefs: []
  type: TYPE_NORMAL
- en: So, NNs are not immune to the Bias-Variance Trade-off. This must be kept in
    mind when designing an NN’s architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 11.11 Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NNs have played a major role in the “ML revolution” of recent years, with notable
    success in certain types of applications. But they can incur huge computational
    costs, in some cases having run times measured in hours or even days, and can
    have vexing convergence problems.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, folklore in the ML community suggests that NNs are not especially
    effective with *tabular data*, meaning the type stored in data frames—that is,
    every dataset we have seen so far in this book. The reader may wish to reserve
    NNs for usage in applications such as image recognition and natural language processing,
    which will be covered in the next two chapters.
  prefs: []
  type: TYPE_NORMAL
