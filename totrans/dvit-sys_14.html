<html><head></head><body>
<h2 class="h2" id="ch14"><span epub:type="pagebreak" id="page_669"/><span class="big">14</span><br/>LEVERAGING SHARED MEMORY IN THE MULTICORE ERA</h2>&#13;
<p class="aff"><em>The world is changed.</em></p>&#13;
<p class="aff"><em>I feel it in the silica.</em></p>&#13;
<p class="aff"><em>I feel it in the transistor.</em></p>&#13;
<p class="aff"><em>I see it in the core.</em></p>&#13;
<p class="aff"><em>–With apologies to Galadriel</em></p>&#13;
<p class="aff">Lord of the Rings: Fellowship of the Ring</p>&#13;
<div class="imagec"><img alt="image" src="../images/common.jpg"/></div>&#13;
<p class="noindents">Until now, our discussion of architecture has focused on a purely single-CPU world. But the world has changed. Today’s CPUs have multiple <em>cores</em>, or compute units. In this chapter, we discuss multicore architectures, and how to leverage them to speed up the execution of programs.</p>&#13;
<p class="note"><strong><span class="black">Note</span> CPUS, PROCESSORS, AND CORES</strong></p>&#13;
<p class="note1"><span epub:type="pagebreak" id="page_670"/>In many instances in this chapter, the terms <em>processor</em> and <em>CPU</em> are used interchangeably. At a fundamental level, a <em>processor</em> is any circuit that performs some computation on external data. Based on this definition, the <em>central processing unit</em> (CPU) is an example of a processor. A processor or a CPU with multiple compute cores is referred to as a <em>multicore processor</em> or a <em>multicore CPU</em>. A <em>core</em> is a compute unit that contains many of the components that make up the classical CPU: an ALU, registers, and a bit of cache. Although a <em>core</em> is different from a processor, it is not unusual to see these terms used interchangeably in the literature (especially if the literature originated at a time when multicore processors were still considered novel).</p>&#13;
<p class="indent">In 1965, the founder of Intel, Gordon Moore, estimated that the number of transistors in an integrated circuit would double every year. His prediction, now known as <em>Moore’s Law</em>, was later revised to transistor counts doubling every <em>two</em> years. Despite the evolution of electronic switches from Bardeen’s transistor to the tiny chip transistors that are currently used in modern computers, Moore’s Law has held true for the past 50 years. However, the turn of the millennium saw processor design hit several critical performance walls:</p>&#13;
<div class="ul-none">&#13;
<p class="ul-indent">The <em>memory wall</em>: Improvements in memory technology did not keep pace with improvements in clock speed, resulting in memory becoming a bottleneck to performance. As a result, continuously speeding up the execution of a CPU no longer improves its overall system performance.</p>&#13;
<p class="ul-indent">The <em>power wall</em>: Increasing the number of transistors on a processor necessarily increases that processor’s temperature and power consumption, which in turn increases the required cost to power and cool the system. With the proliferation of multicore systems, power is now the dominant concern in computer system design.</p>&#13;
</div>&#13;
<p class="indent">The power and memory walls caused computer architects to change the way they designed processors. Instead of adding more transistors to increase the speed at which a CPU executes a single stream of instructions, architects began adding multiple <em>compute cores</em> to a CPU. Compute cores are simplified processing units that contain fewer transistors than traditional CPUs and are generally easier to create. Combining multiple cores on one CPU allows the CPU to execute <em>multiple</em> independent streams of instructions at once.</p>&#13;
<p class="note"><strong><span class="black">Warning</span> MORE CORES != BETTER</strong></p>&#13;
<p class="note-w"><span epub:type="pagebreak" id="page_671"/>It may be tempting to assume that all cores are equal and that the more cores a computer has, the better it is. This is not necessarily the case! For example, <em>graphics processing unit</em> (GPU) cores have even fewer transistors than CPU cores, and are specialized for particular tasks involving vectors. A typical GPU can have 5,000 or more GPU cores. However, GPU cores are limited in the types of operations that they can perform and are not always suitable for general-purpose computing like the CPU core. Computing with GPUs is known as <em>manycore</em> computing. In this chapter, we concentrate on <em>multicore</em> computing. See <a href="ch15.xhtml#ch15">Chapter 15</a> for a discussion of manycore computing.</p>&#13;
<h5 class="h5" id="lev3_112">Taking a Closer Look: How Many Cores?</h5>&#13;
<p class="noindent">Almost all modern computer systems have multiple cores, including small devices like the Raspberry Pi.<sup><a href="ch14.xhtml#fn14_1" id="rfn14_1">1</a></sup> Identifying the number of cores on a system is critical for accurately measuring the performance of multicore programs. On Linux and macOS computers, the <code>lscpu</code> command provides a summary of a system’s architecture. In the following example, we show the output of the <code>lscpu</code> command when run on a sample machine (some output is omitted to emphasize the key features):</p>&#13;
<pre>&#13;
$ <span class="codestrong1">lscpu</span><br/>&#13;
<br/>&#13;
Architecture:          x86_64<br/>&#13;
CPU op-mode(s):        32-bit, 64-bit<br/>&#13;
Byte Order:            Little Endian<br/>&#13;
CPU(s):                8<br/>&#13;
On-line CPU(s) list:   0-7<br/>&#13;
Thread(s) per core:    2<br/>&#13;
Core(s) per socket:    4<br/>&#13;
Socket(s):             1<br/>&#13;
Model name:            Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz<br/>&#13;
CPU MHz:               1607.562<br/>&#13;
CPU max MHz:           3900.0000<br/>&#13;
CPU min MHz:           1600.0000<br/>&#13;
L1d cache:             32K<br/>&#13;
L1i cache:             32K<br/>&#13;
L2 cache:              256K<br/>&#13;
L3 cache:              8192K<br/>&#13;
...</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_672"/>The <code>lscpu</code> command gives a lot of useful information, including the type of processors, the core speed, and the number of cores. To calculate the number of <em>physical</em> (or actual) cores on a system, multiply the number of sockets by the number of cores per socket. The sample <code>lscpu</code> output shown in the preceding example reveals that the system has one socket with four cores per socket, or four physical cores in total.</p>&#13;
<div class="g-box">&#13;
<p class="box-title">HYPERTHREADING</p>&#13;
<p class="noindentt">At first glance, it may appear that the system in the previous example has eight cores in total. After all, this is what the “CPU(s)” field seems to imply. However, that field actually indicates the number of <em>hyperthreaded</em> (logical) cores, not the number of physical cores. Hyperthreading, or simultaneous multithreading (SMT), enables the efficient processing of multiple threads on a single core. Although hyperthreading can decrease the overall runtime of a program, performance on hyperthreaded cores does not scale at the same rate as on physical cores. However, if one task idles (e.g., due to a control hazard, see “Pipelining Hazards: Control Hazards” on <a href="ch05.xhtml#lev2_106">page 279</a>), another task can still utilize the core. In short, hyperthreading was introduced to improve <em>process throughput</em> (which measures the number of processes that complete in a given unit of time) rather than <em>process speedup</em> (which measures the amount of runtime improvement of an individual process). Much of our discussion of performance in the coming chapter will focus on speedup.</p>&#13;
</div>&#13;
<h3 class="h3" id="lev1_105">14.1 Programming Multicore Systems</h3>&#13;
<p class="noindent">Most of the common languages that programmers know today were created prior to the multicore age. As a result, many languages cannot <em>implicitly</em> (or automatically) employ multicore processors to speed up the execution of a program. Instead, programmers must specifically write software to leverage the multiple cores on a system.</p>&#13;
<h4 class="h4" id="lev2_234">14.1.1 The Impact of Multicore Systems on Process Execution</h4>&#13;
<p class="noindent">Recall that a process can be thought of as an abstraction of a running program (see “Processes” on <a href="ch13.xhtml#lev1_100">page 624</a>). Each process executes in its own virtual address space. The operating system (OS) schedules processes for execution on the CPU; a <em>context switch</em> occurs when the CPU changes which process it currently executes.</p>&#13;
<p class="indent"><a href="ch14.xhtml#ch14fig1">Figure 14-1</a> illustrates how five example processes may execute on a single-core CPU.</p>&#13;
<div class="imagec" id="ch14fig1"><span epub:type="pagebreak" id="page_673"/><img alt="image" src="../images/14fig01.jpg"/></div>&#13;
<p class="figcap"><em>Figure 14-1: An execution time sequence for five processes as they share a single CPU core</em></p>&#13;
<p class="indent">The horizontal axis is time, with each time slice taking one unit of time. A box indicates when a process is using the single-core CPU. Assume that each process executes for one full time slice before a context switch occurs. So, Process 1 uses the CPU during time steps T1 and T3.</p>&#13;
<p class="indent">In this example, the order of process execution is P1, P2, P1, P2, P4, P2, P3, P4, P5, P3, P5. We take a moment here to distinguish between two measures of time. The <em>CPU time</em> measures the amount of time a process takes to execute on a CPU. In contrast, the <em>wall-clock time</em> measures the amount of time a human perceives a process takes to complete. The wall-clock time is often significantly longer than the CPU time, due to context switches. For example, Process 1’s CPU time requires two time units, whereas its wall-clock time is three time units.</p>&#13;
<p class="indent">When the total execution time of one process overlaps with another, the processes are running <em>concurrently</em> with each other. Operating systems employed concurrency in the single-core era to give the illusion that a computer can execute many things at once (e.g., you can have a calculator program, a web browser, and a word processing document all open at the same time). In truth, each process executes serially and the operating system determines the order in which processes execute and complete (which often differs in subsequent runs); see “Multiprogramming and Context Switching” on <a href="ch13.xhtml#lev2_221">page 625</a>.</p>&#13;
<p class="indent">Returning to the example, observe that Process 1 and Process 2 run concurrently with each other, since their executions overlap at time points T2–T4. Likewise, Process 2 runs concurrently with Process 4, because their executions overlap at time points T4–T6. In contrast, Process 2 does <em>not</em> run concurrently with Process 3, because they share no overlap in their execution; Process 3 only starts running at time T7, whereas Process 2 completes at time T6.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_674"/>A multicore CPU enables the OS to schedule a different process to each available core, allowing processes to execute <em>simultaneously</em>. The simultaneous execution of instructions from processes running on multiple cores is referred to as <em>parallel execution</em>. <a href="ch14.xhtml#ch14fig2">Figure 14-2</a> shows how our example processes might execute on a dual-core system.</p>&#13;
<div class="imagec" id="ch14fig2"><img alt="image" src="../images/14fig02.jpg"/></div>&#13;
<p class="figcap"><em>Figure 14-2: An execution time sequence for five processes, extended to include two CPU cores (one in dark gray, the other in light gray)</em></p>&#13;
<p class="indent">In this example, the two CPU cores are colored differently. Suppose that the process execution order is again P1, P2, P1, P2, P4, P2, P3, P4, P5, P3, P5. The presence of multiple cores enables certain processes to execute <em>sooner</em>. For example, during time unit T1, the first core executes Process 1 while the second core executes Process 2. At time T2, the first core executes Process 2 while the second executes Process 1. Thus, Process 1 finishes executing after time T2, whereas Process 2 finishes executing at time T3.</p>&#13;
<p class="indent">Note that the parallel execution of multiple processes increases just the number of processes that execute at any one time. In <a href="ch14.xhtml#ch14fig2">Figure 14-2</a>, all the processes complete execution by time unit T7. However, each individual process still requires the same amount of CPU time to complete as shown in <a href="ch14.xhtml#ch14fig1">Figure 14-1</a>. For example, Process 2 requires three time units regardless of execution on a single or multicore system (i.e., its <em>CPU time</em> remains the same). A multicore processor increases the <em>throughput</em> of process execution, or the number of processes that can complete in a given period of time. Thus, even though the CPU time of an individual process remains unchanged, its wall-clock time may decrease.</p>&#13;
<h4 class="h4" id="lev2_235">14.1.2 Expediting Process Execution with Threads</h4>&#13;
<p class="noindent">One way to speed up the execution of a single process is to decompose it into lightweight, independent execution flows called <em>threads</em>. <a href="ch14.xhtml#ch14fig3">Figure 14-3</a> shows how a process’s virtual address space changes when it is multithreaded with two threads. While each thread has its own private allocation of call stack memory, all threads <em>share</em> the program data, instructions, and the heap allocated to the multithreaded process.</p>&#13;
<div class="imagec" id="ch14fig3"><span epub:type="pagebreak" id="page_675"/><img alt="image" src="../images/14fig03.jpg"/></div>&#13;
<p class="figcap"><em>Figure 14-3: Comparing the virtual address space of a single-threaded and a multithreaded process with two threads</em></p>&#13;
<p class="indent">The OS schedules threads in the same manner as it schedules processes. On a multicore processor, the OS can speed up the execution of a multithreaded program by scheduling the different threads to run on separate cores. The maximum number of threads that can execute in parallel is equal to the number of physical cores on the system. If the number of threads exceeds the number of physical cores, the remaining threads must wait their turn to execute (similar to how processes execute on a single core).</p>&#13;
<h5 class="h5" id="lev3_113">An Example: Scalar Multiplication</h5>&#13;
<p class="noindent">As an initial example of how to use multithreading to speed up an application, consider the problem of performing scalar multiplication of an array <code>array</code> and some integer <code>s</code>. In scalar multiplication, each element in the array is scaled by multiplying the element with <code>s</code>.</p>&#13;
<p class="indent">A serial implementation of a scalar multiplication function follows:</p>&#13;
<pre>&#13;
void scalar_multiply(int * array, long length, int s) {<br/>&#13;
    for (i = 0; i &lt; length; i++) {<br/>&#13;
      array[i] = array[i] * s;<br/>&#13;
    }<br/>&#13;
}</pre>&#13;
<p class="indent">Suppose that <code>array</code> has <em>N</em> total elements. To create a multithreaded version of this application with <em>t</em> threads, it is necessary to:</p>&#13;
<div class="number">&#13;
<p class="number">1. Create <em>t</em> threads.</p>&#13;
<p class="number">2. Assign each thread a subset of the input array (i.e., <em>N</em>/<em>t</em> elements).</p>&#13;
<p class="number">3. Instruct each thread to multiply the elements in its array subset by <code>s</code>.</p>&#13;
</div>&#13;
<p class="indent">Suppose that the serial implementation of <code>scalar_multiply</code> spends 60 seconds multiplying an input array of 100 million elements. To build a version that executes with <em>t</em> = 4 threads, we assign each thread one fourth of the total input array (25 million elements).</p>&#13;
<p class="indent"><a href="ch14.xhtml#ch14fig4">Figure 14-4</a> shows what happens when we run four threads on a single core. As before, the execution order is left to the operating system. In this scenario, assume that the thread execution order is Thread 1, Thread 3, Thread 2, Thread 4. On a single-core processor (represented by the squares), each thread executes sequentially. Thus, the multithreaded process running on one core will still take 60 seconds to run (perhaps a little longer, given the overhead of creating threads).</p>&#13;
<div class="imagec" id="ch14fig4"><span epub:type="pagebreak" id="page_676"/><img alt="image" src="../images/14fig04.jpg"/></div>&#13;
<p class="figcap"><em>Figure 14-4: Running four threads on a single-core CPU</em></p>&#13;
<p class="indent">Now suppose that we run our multithreaded process on a dual-core system. <a href="ch14.xhtml#ch14fig5">Figure 14-5</a> shows the result. Again, assume <em>t</em> = 4 threads, and that the thread execution order is Thread 1, Thread 3, Thread 2, Thread 4. Our two cores are represented by shaded squares. Since the system is dual-core, Thread 1 and Thread 3 execute in parallel during time step T1. Threads 2 and 4 then execute in parallel during time step T2. Thus, the multithreaded process that originally took 60 seconds to run now runs in 30 seconds.</p>&#13;
<div class="imagec" id="ch14fig5"><img alt="image" src="../images/14fig05.jpg"/></div>&#13;
<p class="figcap"><em>Figure 14-5: Running four threads on a dual-core CPU</em></p>&#13;
<p class="indent">Finally, suppose that the multithreaded process (<em>t</em> = 4) is run on a quad-core CPU. <a href="ch14.xhtml#ch14fig6">Figure 14-6</a> shows one such execution sequence. Each of the four cores in <a href="ch14.xhtml#ch14fig6">Figure 14-6</a> is shaded differently. On the quad-core system, each thread executes in parallel during time slice T1. Thus, on a quad-core CPU, the multithreaded process that originally took 60 seconds now runs in 15 seconds.</p>&#13;
<div class="imagec" id="ch14fig6"><span epub:type="pagebreak" id="page_677"/><img alt="image" src="../images/14fig06.jpg"/></div>&#13;
<p class="figcap"><em>Figure 14-6: Running four threads on a quad-core CPU</em></p>&#13;
<p class="indent">In general, if the number of threads matches the number of cores (<em>c</em>) and the operating system schedules each thread to run on a separate core in parallel, then the multithreaded process should run in approximately 1/<em>c</em> of the time. Such linear speedup is ideal, but not frequently observed in practice. For example, if there are many other processes (or multithreaded processes) waiting to use the CPU, they will all compete for the limited number of cores, resulting in <em>resource contention</em> among the processes. If the number of specified threads exceeds the number of CPU cores, each thread must wait its turn to run. We explore other factors that often prevent linear speedup in “Measuring the Performance of Parallel Programs” on <a href="ch14.xhtml#lev1_108">page 709</a>.</p>&#13;
<h3 class="h3" id="lev1_106">14.2 Hello Threading! Writing Your First Multithreaded Program</h3>&#13;
<p class="noindent">In this section, we examine the ubiquitous POSIX thread library <em>Pthreads</em>. POSIX is an acronym for Portable Operating System Interface. It is an IEEE standard that specifies how UNIX systems look, act, and feel. The POSIX threads API is available on almost all UNIX-like operating systems, each of which meets the standard in its entirety or to some great degree. So, if you write parallel code using POSIX threads on a Linux machine, it will certainly work on other Linux machines, and it will likely work on machines running macOS or other UNIX variants.</p>&#13;
<p class="indent">Let’s begin by analyzing an example “Hello World” Pthreads program.<sup><a href="ch14.xhtml#fn14_2" id="rfn14_2">2</a></sup> For brevity, we have excluded error handling in the listing, though the downloadable version contains sample error handling.</p>&#13;
<pre>&#13;
#include &lt;stdio.h&gt;<br/>&#13;
#include &lt;stdlib.h&gt;<br/>&#13;
#include &lt;pthread.h&gt;<br/>&#13;
<br/>&#13;
/* The "thread function" passed to pthread_create.  Each thread executes this<br/>&#13;
 * function and terminates when it returns from this function. */<br/>&#13;
void *HelloWorld(void *id) {<br/>&#13;
<span epub:type="pagebreak" id="page_678"/>    /* We know the argument is a pointer to a long, so we cast it from a<br/>&#13;
     * generic (void *) to a (long *). */<br/>&#13;
    long *myid = (long *) id;<br/>&#13;
<br/>&#13;
    printf("Hello world! I am thread %ld\n", *myid);<br/>&#13;
<br/>&#13;
    return NULL; // We don't need our threads to return anything.<br/>&#13;
}<br/>&#13;
<br/>&#13;
int main(int argc, char **argv) {<br/>&#13;
    int i;<br/>&#13;
    int nthreads; //number of threads<br/>&#13;
    pthread_t *thread_array; //pointer to future thread array<br/>&#13;
    long *thread_ids;<br/>&#13;
<br/>&#13;
    // Read the number of threads to create from the command line.<br/>&#13;
    if (argc !=2) {<br/>&#13;
        fprintf(stderr, "usage: %s &lt;n&gt;\n", argv[0]);<br/>&#13;
        fprintf(stderr, "where &lt;n&gt; is the number of threads\n");<br/>&#13;
        return 1;<br/>&#13;
    }<br/>&#13;
    nthreads = strtol(argv[1], NULL, 10);<br/>&#13;
<br/>&#13;
    // Allocate space for thread structs and identifiers.<br/>&#13;
    thread_array = malloc(nthreads * sizeof(pthread_t));<br/>&#13;
    thread_ids = malloc(nthreads * sizeof(long));<br/>&#13;
<br/>&#13;
    // Assign each thread an ID and create all the threads.<br/>&#13;
    for (i = 0; i &lt; nthreads; i++) {<br/>&#13;
        thread_ids[i] = i;<br/>&#13;
        pthread_create(&amp;thread_array[i], NULL, HelloWorld, &amp;thread_ids[i]);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    /* Join all the threads. Main will pause in this loop until all threads<br/>&#13;
     * have returned from the thread function. */<br/>&#13;
    for (i = 0; i &lt; nthreads; i++) {<br/>&#13;
        pthread_join(thread_array[i], NULL);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    free(thread_array);<br/>&#13;
    free(thread_ids);<br/>&#13;
<br/>&#13;
    return 0;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">Let’s examine this program in smaller components. Notice the inclusion of the <code>pthread.h</code> header file, which declares <code>pthread</code> types and functions. Next, the <code>HelloWorld</code> function defines the <em>thread function</em> that we later pass to <span epub:type="pagebreak" id="page_679"/><code>pthread_create</code>. A thread function is analogous to a <code>main</code> function for a worker (created) thread—a thread begins execution at the start of its thread function and terminates when it reaches the end. Each thread executes the thread function using its private execution state (i.e., its own stack memory and register values). Note also that the thread function is of type <code>void *</code>. Specifying an <em>anonymous pointer</em> in this context allows programmers to write thread functions that deal with arguments and return values of different types (see “The void * Type and Type Recasting” on <a href="ch02.xhtml#lev2_38">page 222</a>). Lastly, in the <code>main</code> function, the main thread initializes the program state before creating and joining the worker threads.</p>&#13;
<h4 class="h4" id="lev2_236">14.2.1 Creating and Joining Threads</h4>&#13;
<p class="noindent">The program first starts as a single-threaded process. As it executes the <code>main</code> function, it reads the number of threads to create, and it allocates memory for two arrays: <code>thread_array</code> and <code>thread_ids</code>. The <code>thread_array</code> array contains the set of addresses for each thread created. The <code>thread_ids</code> array stores the set of arguments that each thread is passed. In this example, each thread is passed the address of its rank (or ID, represented by <code>thread_ids[i]</code>).</p>&#13;
<p class="indent">After all the preliminary variables are allocated and initialized, the <code>main</code> thread executes the two major steps of multithreading:</p>&#13;
<ul>&#13;
<li class="noindent">The <em>creation</em> step, in which the main thread spawns one or more worker threads. After being spawned, each worker thread runs within its own execution context concurrently with the other threads and processes on the system.</li>&#13;
<li class="noindent">The <em>join</em> step, in which the main thread waits for all the workers to complete before proceeding as a single-thread process. Joining a thread that has terminated frees the thread’s execution context and resources. Attempting to join a thread that <em>hasn’t</em> terminated blocks the caller until the thread terminates, similar to the semantics of the <code>wait</code> function for processes (see “exit and wait” on <a href="ch13.xhtml#lev2_226">page 635</a>).</li>&#13;
</ul>&#13;
<p class="indent">The Pthreads library offers a <code>pthread_create</code> function for creating threads and a <code>pthread_join</code> function for joining them. The <code>pthread_create</code> function has the following signature:</p>&#13;
<pre>&#13;
pthread_create(pthread_t *thread, const pthread_attr_t *attr,<br/>&#13;
               void *thread_function, void *thread_args)<br/>&#13;
</pre>&#13;
<p class="indent">The function takes a pointer to a thread struct (of type <code>pthread_t</code>), a pointer to an attribute struct (normally set to <code>NULL</code>), the name of the function the thread should execute, and the array of arguments to pass to the thread function when it starts.</p>&#13;
<p class="indent">The Hello World program calls <code>pthread_create</code> in the <code>main</code> function using:</p>&#13;
<pre>pthread_create(&amp;thread_array[i], NULL, HelloWorld, &amp;thread_ids[i]);</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_680"/>Here:</p>&#13;
<ul>&#13;
<li class="noindent"><code>&amp;thread_array[i]</code> contains the address of thread <em>i</em>. The <code>pthread_create</code> function allocates a <code>pthread_t</code> thread object and stores its address at this location, enabling the programmer to reference the thread later (e.g., when joining it).</li>&#13;
<li class="noindent"><code>NULL</code> specifies that the thread should be created with default attributes. In most programs, it is safe to leave this second parameter as <code>NULL</code>.</li>&#13;
<li class="noindent"><code>HelloWorld</code> names the thread function that the created thread should execute. This function behaves like the “main” function for the thread. For an arbitrary thread function (e.g., <code>function</code>), its prototype must match the form <code>void * function(void *)</code>.</li>&#13;
<li class="noindent"><code>&amp;thread_ids[i]</code> specifies the address of the arguments to be passed to thread <em>i</em>. In this case, <code>thread_ids[i]</code> contains a single <code>long</code> representing the thread’s ID. Since the last argument to <code>pthread_create</code> must be a pointer, we pass the <em>address</em> of the thread’s ID.</li>&#13;
</ul>&#13;
<p class="indent">To generate several threads that execute the <code>HelloWorld</code> thread function, the program assigns each thread a unique ID and creates each thread within a <code>for</code> loop:</p>&#13;
<pre>&#13;
for (i = 0; i &lt; nthreads; i++) {<br/>&#13;
    thread_ids[i] = i;<br/>&#13;
    pthread_create(&amp;thread_array[i], NULL, HelloWorld, &amp;thread_ids[i]);<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">The OS schedules the execution of each created thread; the user cannot make any assumption on the order in which the threads will execute.</p>&#13;
<p class="indent">The <code>pthread_join</code> function suspends the execution of its caller until the thread it references terminates. Its signature is:</p>&#13;
<pre>pthread_join(pthread_t thread, void **return_val)</pre>&#13;
<p class="indent">The <code>pthread_join</code> takes as input a <code>pthread_t</code> struct, indicating which thread to wait on, and an optional pointer argument that specifies where the thread’s return value should be stored.</p>&#13;
<p class="indent">The Hello World program calls <code>pthread_join</code> in <code>main</code> using:</p>&#13;
<pre>pthread_join(thread_array[t], NULL);</pre>&#13;
<p class="indent">This line indicates that the main thread must wait on the termination of thread <code>t</code>. Passing <code>NULL</code> as the second argument indicates that the program does not use the thread’s return value.</p>&#13;
<p class="indent">In the previous program, <code>main</code> calls <code>pthread_join</code> in a loop because <em>all</em> of the worker threads need to terminate before the <code>main</code> function proceeds to clean up memory and terminate the process:</p>&#13;
<pre><span epub:type="pagebreak" id="page_681"/>&#13;
for (i = 0; i &lt; nthreads; i++) {<br/>&#13;
    pthread_join(thread_array[i], NULL);<br/>&#13;
}<br/>&#13;
</pre>&#13;
<h4 class="h4" id="lev2_237">14.2.2 The Thread Function</h4>&#13;
<p class="noindent">In the previous program, each spawned thread prints out <code>Hello world! I am thread n</code>, where <code>n</code> is the thread’s unique ID. After the thread prints out its message, it terminates. Let’s take a closer look at the <code>HelloWorld</code> function:</p>&#13;
<pre>&#13;
void *HelloWorld(void *id) {<br/>&#13;
    long *myid = (long*)id;<br/>&#13;
<br/>&#13;
    printf("Hello world! I am thread %ld\n", *myid);<br/>&#13;
<br/>&#13;
    return NULL;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">Recall that <code>pthread_create</code> passes the arguments to the thread function using the <code>thread_args</code> parameter. In the <code>pthread_create</code> function in <code>main</code>, the Hello World program specified that this parameter is in fact the thread’s ID. Note that the parameter to <code>HelloWorld</code> must be declared as a generic or anonymous pointer (<code>void *</code>) (see “The void * Type and Type Recasting” on <a href="ch02.xhtml#lev2_38">page 126</a>). The Pthreads library uses <code>void *</code> to make <code>pthread_create</code> more general purpose by not prescribing a parameter type. As a programmer, the <code>void *</code> is mildly inconvenient given that it must be recast before use. Here, we <em>know</em> the parameter is of type <code>long *</code> because that’s what we passed to <code>pthread_create</code> in <code>main</code>. Thus, we can safely cast the value as a <code>long *</code> and dereference the pointer to access the <code>long</code> value. Many parallel programs follow this structure.</p>&#13;
<p class="indent">Similar to the thread function’s parameter, the Pthreads library avoids prescribing the thread function’s return type by specifying another <code>void *</code>: the programmer is free to return any pointer from the thread function. If the program needs to access the thread’s return value, it can retrieve it via the second argument to <code>pthread_join</code>. In our example, the thread has no need to return a value, so it simply returns a <code>NULL</code> pointer.</p>&#13;
<h4 class="h4" id="lev2_238">14.2.3 Running the Code</h4>&#13;
<p class="noindent">The command that follows shows how to use GCC to compile the program. Building a Pthreads application requires that the <code>-lpthread</code> linker flag be passed to GCC to ensure that the Pthreads functions and types are accessible:</p>&#13;
<pre>$ <span class="codestrong1">gcc -o hellothreads hellothreads.c -lpthread</span></pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_682"/>Running the program without a command line argument results in a usage message:</p>&#13;
<pre>$ <span class="codestrong1">./hellothreads</span><br/>&#13;
usage: ./hellothreads &lt;n&gt;<br/>&#13;
where &lt;n&gt; is the number of threads</pre>&#13;
<p class="indent">Running the program with four threads yields the following output:</p>&#13;
<pre>$ <span class="codestrong1">./hellothreads 4</span><br/>&#13;
Hello world! I am thread 1<br/>&#13;
Hello world! I am thread 2<br/>&#13;
Hello world! I am thread 3<br/>&#13;
Hello world! I am thread 0</pre>&#13;
<p class="indent">Notice that each thread prints its unique ID number. In this run, thread 1’s output displays first, followed by threads 2, 3, and 0. If we run the program again, we may see the output displayed in a different order:</p>&#13;
<pre>$ <span class="codestrong1">./hellothreads 4</span><br/>&#13;
Hello world! I am thread 0<br/>&#13;
Hello world! I am thread 1<br/>&#13;
Hello world! I am thread 2<br/>&#13;
Hello world! I am thread 3</pre>&#13;
<p class="indent">Recall that the operating system’s scheduler determines the thread execution order. From a user’s perspective, the order is <em>effectively random</em> due to being influenced by many factors that vary outside the user’s control (e.g., available system resources, the system receiving input, or OS scheduling). Since all threads are running concurrently with one another and each thread executes a call to <code>printf</code> (which prints to <code>stdout</code>), the first thread that prints to <code>stdout</code> will have its output show up first. Subsequent executions may (or may not) result in different output.</p>&#13;
<p class="note"><strong><span class="black">Warning</span> THREAD EXECUTION ORDER</strong></p>&#13;
<p class="note-w">You should <em>never</em> make any assumptions about the order in which threads will execute. If the correctness of your program requires that threads run in a particular order, you must add synchronization (see “Synchronizing Threads” on <a href="ch14.xhtml#lev1_107">page 686</a>) to your program to prevent threads from running when they shouldn’t.</p>&#13;
<h4 class="h4" id="lev2_239">14.2.4 Revisiting Scalar Multiplication</h4>&#13;
<p class="noindent">Let’s explore how to create a multithreaded implementation of the scalar multiplication program from “An Example: Scalar Multiplication” on <a href="ch14.xhtml#lev3_113">page 675</a>. Recall that our general strategy for parallelizing <code>scalar_multiply</code> is to create multiple threads, assign each thread a subset of the input array, and instruct each thread to multiply the elements in its array subset by <code>s</code>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_683"/>The following is a thread function that accomplishes this task. Notice that we have moved <code>array</code>, <code>length</code>, and <code>s</code> to the global scope of the program.</p>&#13;
<pre>&#13;
long *array; //allocated in main<br/>&#13;
long length; //set in main (1 billion)<br/>&#13;
long nthreads; //number of threads<br/>&#13;
long s; //scalar<br/>&#13;
<br/>&#13;
void *scalar_multiply(void *id) {<br/>&#13;
    long *myid = (long *) id;<br/>&#13;
    int i;<br/>&#13;
<br/>&#13;
    //assign each thread its own chunk of elements to process<br/>&#13;
    long chunk = length / nthreads;<br/>&#13;
    long start = *myid * chunk;<br/>&#13;
    long end  = start + chunk;<br/>&#13;
    if (*myid == nthreads - 1) {<br/>&#13;
        end = length;<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    //perform scalar multiplication on assigned chunk<br/>&#13;
    for (i = start; i &lt; end; i++) {<br/>&#13;
        array[i] *= s;<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    return NULL;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">Let’s break this down into parts. Recall that the first step is to assign each thread a component of the array. The following lines accomplish this task:</p>&#13;
<pre>long chunk = length / nthreads;<br/>&#13;
long start = *myid * chunk;<br/>&#13;
long end  = start + chunk;</pre>&#13;
<p class="indent">The variable <code>chunk</code> stores the number of elements that each thread is assigned. To ensure that each thread gets roughly the same amount of work, we first set the chunk size to the number of elements divided by the number of threads, or <code>length / nthreads</code>.</p>&#13;
<p class="indent">Next, we assign each thread a distinct range of elements to process. Each thread computes its range’s <code>start</code> and <code>end</code> index using the <code>chunk</code> size and its unique thread ID.</p>&#13;
<p class="indent">For example, with four threads (with IDs 0–3) operating over an array with 100 million elements, each thread is responsible for processing a 25 million element <code>chunk</code>. Incorporating the thread ID assigns each thread a unique subset of the input.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_684"/>The next two lines account for the case in which <code>length</code> is not evenly divisible by the number of threads:</p>&#13;
<pre>&#13;
if (*myid == nthreads - 1) {<br/>&#13;
    end = length;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">Suppose that we specified three rather than four threads. The nominal chunk size would be 33,333,333 elements, leaving one element unaccounted for. The code in the previous example would assign the remaining element to the last thread.</p>&#13;
<p class="note"><strong><span class="black">Note</span> CREATING BALANCED INPUT</strong></p>&#13;
<p class="note1">The chunking code just shown is imperfect. In the case where the number of threads does not evenly divide the input, the remainder is assigned to the last thread. Consider a sample run in which the array has 100 elements, and 12 threads are specified. The nominal chunk size would be 8, and the remainder would be 4. With the example code, the first 11 threads will each have 8 assigned elements, whereas the last thread will be assigned 12 elements. Consequently, the last thread performs 50% more work than the other threads. A potentially better way to chunk this example is to have the first 4 threads process 9 elements each, whereas the last 8 threads process 8 elements each. This will result in better <em>load balancing</em> of the input across the threads.</p>&#13;
<p class="indent">With an appropriate local <code>start</code> and <code>end</code> index computed, each thread is now ready to perform scalar multiplication on its component of the array. The last portion of the <code>scalar_multiply</code> function accomplishes this:</p>&#13;
<pre>&#13;
for (i = start; i &lt; end; i++) {<br/>&#13;
    array[i] *= s;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<h4 class="h4" id="lev2_240">14.2.5 Improving Scalar Multiplication: Multiple Arguments</h4>&#13;
<p class="noindent">A key weakness of the previous implementation is the wide use of global variables. Our original discussion in “Parts of Program Memory and Scope” on <a href="ch02.xhtml#lev1_9">page 64</a> showed that, although useful, global variables should generally be avoided in C. To reduce the number of global variables in the program, one solution is to declare a <code>t_arg</code> struct as follows in the global scope:</p>&#13;
<pre>&#13;
struct t_arg {<br/>&#13;
    int *array; // pointer to shared array<br/>&#13;
    long length; // num elements in array<br/>&#13;
    long s; //scaling factor<br/>&#13;
    long numthreads; // total number of threads<br/>&#13;
    long id; //  logical thread id<br/>&#13;
};<br/>&#13;
</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_685"/>Our main function would, in addition to allocating <code>array</code> and setting local variables <code>length</code>, <code>nthreads</code>, and <code>s</code> (our scaling factor), allocate an array of <code>t_arg</code> records:</p>&#13;
<pre>&#13;
long nthreads = strtol(argv[1], NULL, 10); //get number of threads<br/>&#13;
long length = strtol(argv[2], NULL, 10); //get length of array<br/>&#13;
long s = strtol( argv[3], NULL, 10 ); //get scaling factor<br/>&#13;
<br/>&#13;
int *array = malloc(length*sizeof(int));<br/>&#13;
<br/>&#13;
//allocate space for thread structs and identifiers<br/>&#13;
pthread_t *thread_array = malloc(nthreads * sizeof(pthread_t));<br/>&#13;
struct t_arg *thread_args = malloc(nthreads * sizeof(struct t_arg));<br/>&#13;
<br/>&#13;
//Populate thread arguments for all the threads<br/>&#13;
for (i = 0; i &lt; nthreads; i++){<br/>&#13;
    thread_args[i].array = array;<br/>&#13;
    thread_args[i].length = length;<br/>&#13;
    thread_args[i].s = s;<br/>&#13;
    thread_args[i].numthreads = nthreads;<br/>&#13;
    thread_args[i].id = i;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">Later in <code>main</code>, when <code>pthread_create</code> is called, the thread’s associated <code>t_args</code> struct is passed as an argument:</p>&#13;
<pre>&#13;
for (i = 0; i &lt; nthreads; i++){<br/>&#13;
    pthread_create(&amp;thread_array[i], NULL, scalar_multiply, &amp;thread_args[i]);<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">Lastly, our <code>scalar_multiply</code> function would look like the following:</p>&#13;
<pre>&#13;
void * scalar_multiply(void* args) {<br/>&#13;
    //cast to a struct t_arg from void*<br/>&#13;
    struct t_arg * myargs = (struct t_arg *) args;<br/>&#13;
<br/>&#13;
    //extract all variables from struct<br/>&#13;
    long myid =  myargs-&gt;id;<br/>&#13;
    long length = myargs-&gt;length;<br/>&#13;
    long s = myargs-&gt;s;<br/>&#13;
    long nthreads = myargs-&gt;numthreads;<br/>&#13;
    int * ap = myargs-&gt;array; //pointer to array in main<br/>&#13;
<br/>&#13;
    //code as before<br/>&#13;
    long chunk = length/nthreads;<br/>&#13;
    long start = myid * chunk;<br/>&#13;
    long end  = start + chunk;<br/>&#13;
    if (myid == nthreads-1) {<br/>&#13;
        end = length;<br/>&#13;
    <span epub:type="pagebreak" id="page_686"/>}<br/>&#13;
<br/>&#13;
    int i;<br/>&#13;
    for (i = start; i &lt; end; i++) {<br/>&#13;
        ap[i] *= s;<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    return NULL;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">Implementing this program fully is an exercise we leave to the reader. Please note that error handling has been omitted for the sake of brevity.</p>&#13;
<h3 class="h3" id="lev1_107">14.3 Synchronizing Threads</h3>&#13;
<p class="noindent">In the examples we’ve looked at thus far, each thread executes without sharing data with any other threads. In the scalar multiplication program, for instance, each element of the array is entirely independent of all the others, making it unnecessary for the threads to share data.</p>&#13;
<p class="indent">However, a thread’s ability to easily share data with other threads is one of its main features. Recall that all the threads of a multithreaded process share the heap common to the process. In this section, we study the data sharing and protection mechanisms available to threads in detail.</p>&#13;
<p class="indent"><em>Thread synchronization</em> refers to forcing threads to execute in a particular order. Even though synchronizing threads can add to the runtime of a program, it is often necessary to ensure program correctness. In this section, we primarily discuss how one synchronization construct (a <em>mutex</em>) helps ensure the correctness of a threaded program. We conclude the section with a discussion of some other common synchronization constructs: <em>semaphores</em>, <em>barriers</em>, and <em>condition variables</em>.</p>&#13;
<h5 class="h5" id="lev3_114">CountSort</h5>&#13;
<p class="noindent">Let’s study a slightly more complicated example called CountSort. The CountSort algorithm is a simple linear (O(<em>N</em>)) sorting algorithm for sorting a known small range of <em>R</em> values, where <em>R</em> is much smaller than <em>N</em>. To illustrate how CountSort works, consider an array <code>A</code> of 15 elements, all of which contain random values between 0 and 9 (10 possible values):</p>&#13;
<pre>A = [9, 0, 2, 7, 9, 0, 1, 4, 2, 2, 4, 5, 0, 9, 1]</pre>&#13;
<p class="indent">For a particular array, CountSort works as follows:</p>&#13;
<div class="number">&#13;
<p class="number">1. It counts the frequency of each value in the array.</p>&#13;
<p class="number">2. It overwrites the original array by enumerating each value by its frequency.</p>&#13;
</div>&#13;
<p class="indent">After step 1, the frequency of each value is placed in a <code>counts</code> array of length 10, where the value of <code>counts[i]</code> is the frequency of the value <em>i</em> in <span epub:type="pagebreak" id="page_687"/>array <code>A</code>. For example, since there are three elements with value 2 in array <code>A</code>, <code>counts[2]</code> is 3.</p>&#13;
<p class="indent">The corresponding <code>counts</code> array for the previous example looks like the following:</p>&#13;
<pre>counts = [3, 2, 3, 0, 2, 1, 0, 1, 0, 3]</pre>&#13;
<p class="indent">Note that the sum of all the elements in the <code>counts</code> array is equal to the length of <code>A</code>, or 15.</p>&#13;
<p class="indent">Step 2 uses the <code>counts</code> array to overwrite <code>A</code>, using the frequency counts to determine the set of indices in <code>A</code> that store each consecutive value in sorted order. So, since the <code>counts</code> array indicates that there are three elements with value 0 and two elements with value 1 in array <code>A</code>, the first three elements of the final array will be 0, and the next two will be 1.</p>&#13;
<p class="indent">After running step 2, the final array looks like the following:</p>&#13;
<pre>A = [0, 0, 0, 1, 1, 2, 2, 2, 4, 4, 5, 7, 9, 9, 9]</pre>&#13;
<p class="indent">Following is a serial implementation of the CountSort algorithm, with the <code>count</code> (step 1) and <code>overwrite</code> (step 2) functions clearly delineated. For brevity, we do not reproduce the whole program here, though you can download the source.<sup><a href="ch14.xhtml#fn14_3" id="rfn14_3">3</a></sup></p>&#13;
<pre>&#13;
#define MAX 10 //the maximum value of an element. (10 means 0-9)<br/>&#13;
<br/>&#13;
/*step 1:<br/>&#13;
 * compute the frequency of all the elements in the input array and store<br/>&#13;
 * the associated counts of each element in array counts. The elements in the<br/>&#13;
 * counts array are initialized to zero prior to the call to this function.<br/>&#13;
*/<br/>&#13;
void countElems(int *counts, int *array_A, long length) {<br/>&#13;
    int val, i;<br/>&#13;
    for (i = 0; i &lt; length; i++) {<br/>&#13;
      val = array_A[i]; //read the value at index i<br/>&#13;
      counts[val] = counts[val] + 1; //update corresponding location in counts<br/>&#13;
    }<br/>&#13;
}<br/>&#13;
<br/>&#13;
/* step 2:<br/>&#13;
 * overwrite the input array (array_A) using the frequencies stored in the<br/>&#13;
 *  array counts<br/>&#13;
*/<br/>&#13;
void writeArray(int *counts, int *array_A) {<br/>&#13;
    int i, j = 0, amt;<br/>&#13;
<br/>&#13;
    for (i = 0; i &lt; MAX; i++) { //iterate over the counts array<br/>&#13;
        amt = counts[i]; //capture frequency of element i<br/>&#13;
        while (amt &gt; 0) { //while all values aren't written<br/>&#13;
            array_A[j] = i; //replace value at index j of array_A with i<br/>&#13;
<span epub:type="pagebreak" id="page_688"/>            j++; //go to next position in array_A<br/>&#13;
            amt--; //decrease the amount written by 1<br/>&#13;
        }<br/>&#13;
    }<br/>&#13;
}<br/>&#13;
<br/>&#13;
/* main function:<br/>&#13;
 * gets array length from command line args, allocates a random array of that<br/>&#13;
 * size, allocates the counts array, the executes step 1 of the CountSort<br/>&#13;
 * algorithm (countsElem) followed by step 2 (writeArray).<br/>&#13;
*/<br/>&#13;
int main( int argc, char **argv ) {<br/>&#13;
    //code ommitted for brevity -- download source to view full file<br/>&#13;
<br/>&#13;
    srand(10); //use of static seed ensures the output is the same every run<br/>&#13;
<br/>&#13;
    long length = strtol( argv[1], NULL, 10 );<br/>&#13;
    int verbose = atoi(argv[2]);<br/>&#13;
<br/>&#13;
    //generate random array of elements of specified length<br/>&#13;
    int *array = malloc(length * sizeof(int));<br/>&#13;
    genRandomArray(array, length);<br/>&#13;
<br/>&#13;
    //print unsorted array (commented out)<br/>&#13;
    //printArray(array, length);<br/>&#13;
<br/>&#13;
    //allocate counts array and initializes all elements to zero.<br/>&#13;
    int counts[MAX] = {0};<br/>&#13;
<br/>&#13;
    countElems(counts, array, length); //calls step 1<br/>&#13;
    writeArray(counts, array); //calls step2<br/>&#13;
<br/>&#13;
    //print sorted array (commented out)<br/>&#13;
    //printArray(array, length);<br/>&#13;
<br/>&#13;
    free(array); //free memory<br/>&#13;
<br/>&#13;
    return 0;<br/>&#13;
}</pre>&#13;
<p class="indent">Running this program on an array of size 15 yields the following output:</p>&#13;
<pre>&#13;
$ <span class="codestrong1">./countSort 15 1</span><br/>&#13;
array before sort:<br/>&#13;
5 8 8 5 8 7 5 1 7 7 3 3 8 3 4<br/>&#13;
result after sort:<br/>&#13;
1 3 3 3 4 5 5 5 7 7 7 8 8 8 8<br/>&#13;
</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_689"/>The second parameter to this program is a <em>verbose</em> flag, which indicates whether the program prints output. This is a useful option for larger arrays for which we may want to run the program but not necessarily print out the output.</p>&#13;
<h5 class="h5" id="lev3_115">Parallelizing countElems: An Initial Attempt</h5>&#13;
<p class="noindent">CountSort consists of two primary steps, each of which benefits from being parallelized. In the remainder of the chapter, we primarily concentrate on the parallelization of step 1, or the <code>countElems</code> function. Parallelizing the <code>writeArray</code> function is left as an exercise for the reader.</p>&#13;
<p class="indent">The code block that follows depicts a first attempt at creating a threaded <code>countElems</code> function. Parts of the code (argument parsing, error handling) are omitted in this example for the sake of brevity, but the full source can be downloaded.<sup><a href="ch14.xhtml#fn14_4" id="rfn14_4">4</a></sup> In the code that follows, each thread attempts to count the frequency of the array elements in its assigned component of the global array and updates a global count array with the discovered counts:</p>&#13;
<pre>&#13;
/*parallel version of step 1 (first cut) of CountSort algorithm:<br/>&#13;
 * extracts arguments from args value<br/>&#13;
 * calculates the portion of the array that thread is responsible for counting<br/>&#13;
 * computes the frequency of all the elements in assigned component and stores<br/>&#13;
 * the associated counts of each element in counts array<br/>&#13;
*/<br/>&#13;
void *countElems( void *args ) {<br/>&#13;
    struct t_arg * myargs = (struct t_arg *)args;<br/>&#13;
    //extract arguments (omitted for brevity)<br/>&#13;
    int *array = myargs-&gt;ap;<br/>&#13;
    long *counts = myargs-&gt;countp;<br/>&#13;
    //... (get nthreads, length, myid)<br/>&#13;
<br/>&#13;
    //assign work to the thread<br/>&#13;
    long chunk = length / nthreads; //nominal chunk size<br/>&#13;
    long start = myid * chunk;<br/>&#13;
    long end = (myid + 1) * chunk;<br/>&#13;
    long val;<br/>&#13;
    if (myid == nthreads-1) {<br/>&#13;
        end = length;<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    long i;<br/>&#13;
    //heart of the program<br/>&#13;
    for (i = start; i &lt; end; i++) {<br/>&#13;
        val = array[i];<br/>&#13;
        counts[val] = counts[val] + 1;<br/>&#13;
    }<br/>&#13;
    return NULL;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_690"/>The <code>main</code> function looks nearly identical to our earlier sample programs:</p>&#13;
<pre>&#13;
int main(int argc, char **argv) {<br/>&#13;
<br/>&#13;
    if (argc != 4) {<br/>&#13;
        //print out usage info (ommitted for brevity)<br/>&#13;
        return 1;<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    srand(10); //static seed to assist in correctness check<br/>&#13;
<br/>&#13;
    //parse command line arguments<br/>&#13;
    long t;<br/>&#13;
    long length = strtol(argv[1], NULL, 10);<br/>&#13;
    int verbose = atoi(argv[2]);<br/>&#13;
    long nthreads = strtol(argv[3], NULL, 10);<br/>&#13;
<br/>&#13;
    //generate random array of elements of specified length<br/>&#13;
    int *array = malloc(length * sizeof(int));<br/>&#13;
    genRandomArray(array, length);<br/>&#13;
<br/>&#13;
    //specify counts array and initialize all elements to zero<br/>&#13;
    long counts[MAX] = {0};<br/>&#13;
<br/>&#13;
    //allocate threads and args array<br/>&#13;
    pthread_t *thread_array; //pointer to future thread array<br/>&#13;
    thread_array = malloc(nthreads * sizeof(pthread_t)); //allocate the array<br/>&#13;
    struct t_arg *thread_args = malloc( nthreads * sizeof(struct t_arg) );<br/>&#13;
<br/>&#13;
    //fill thread array with parameters<br/>&#13;
    for (t = 0; t &lt; nthreads; t++) {<br/>&#13;
        //ommitted for brevity...<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    for (t = 0; t &lt; nthreads; t++) {<br/>&#13;
        pthread_create(&amp;thread_array[t], NULL, countElems, &amp;thread_args[t]);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    for (t = 0; t &lt; nthreads; t++) {<br/>&#13;
        pthread_join(thread_array[t], NULL);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    free(thread_array);<br/>&#13;
    free(array);<br/>&#13;
<span epub:type="pagebreak" id="page_691"/>    if (verbose) {<br/>&#13;
        printf("Counts array:\n");<br/>&#13;
        printCounts(counts);<br/>&#13;
    }<br/>&#13;
    return 0;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">For reproducibility purposes, the random number generator is seeded with a static value (10) to ensure that <code>array</code> (and therefore <code>counts</code>) always contains the same set of numbers. An additional function (<code>printCounts</code>) prints out the contents of the global <code>counts</code> array. The expectation is that, regardless of the number of threads used, the contents of the <code>counts</code> array should always be the same. For brevity, error handling has been removed from the listing.</p>&#13;
<p class="indent">Compiling the program and running it with one, two, and four threads over 10 million elements produces the following:</p>&#13;
<pre>&#13;
$ <span class="codestrong1">gcc -o countElems_p countElems_p.c -lpthread</span><br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p 10000000 1 1</span><br/>&#13;
Counts array:<br/>&#13;
999170 1001044 999908 1000431 999998 1001479 999709 997250 1000804 1000207<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p 10000000 1 2</span><br/>&#13;
Counts array:<br/>&#13;
661756 661977 657828 658479 657913 659308 658561 656879 658070 657276<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p 10000000 1 4</span><br/>&#13;
Counts array:<br/>&#13;
579846 580814 580122 579772 582509 582713 582518 580917 581963 581094<br/>&#13;
</pre>&#13;
<p class="indent">Note that the printed results change significantly on each run. In particular, they seem to change as we vary the number of threads! This should not happen, since our use of the static seed guarantees the same set of numbers every run. These results contradict one of the cardinal rules for threaded programs: the output of a program should be correct and consistent <em>regardless</em> of the number of threads used.</p>&#13;
<p class="indent">Since our first attempt at parallelizing <code>countElems</code> doesn’t seem to be working, let’s delve deeper into what this program is doing and examine how we might fix it.</p>&#13;
<h5 class="h5" id="lev3_116">Data Races</h5>&#13;
<p class="noindent">To understand what’s going on, let’s consider an example run with two threads on two separate cores of a multicore system. Recall that the execution of any thread can be preempted at any time by the OS, which means that each thread could be running different instructions of a particular function at any given time (or possibly the same instruction). <a href="ch14.xhtml#ch14tab1">Table 14-1</a> shows one possible path of execution through the <code>countElems</code> function. To better <span epub:type="pagebreak" id="page_692"/>illustrate what is going on, we translated the line <code>counts[val] = counts[val] + 1</code> into the following sequence of equivalent instructions:</p>&#13;
<div class="number">&#13;
<p class="number">1. <em>Read</em> <code>counts[val]</code> and place into a register.</p>&#13;
<p class="number">2. <em>Modify</em> the register by incrementing it by one.</p>&#13;
<p class="number">3. <em>Write</em> the contents of the register to <code>counts[val]</code>.</p>&#13;
</div>&#13;
<p class="indent">This is known as the <em>read–modify–write</em> pattern. In the example shown in <a href="ch14.xhtml#ch14tab1">Table 14-1</a>, each thread executes on a separate core (Thread 0 on Core 0, Thread 1 on Core 1). We start inspecting the execution of the process at time step <em>i</em>, where both threads have a <code>val</code> of 1.</p>&#13;
<p class="tabcap" id="ch14tab1"><strong>Table 14-1:</strong> A Possible Execution Sequence of Two Threads Running <code>countElems</code></p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:40%"/>&#13;
<col style="width:40%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab"><strong>Time</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Thread 0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Thread 1</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>i</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Read <code>counts[1]</code> and place into Core 0’s register</p></td>&#13;
<td style="vertical-align: top"><p class="tab">…</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>i</em> + 1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Increment register by 1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Read <code>counts[1]</code> and place into Core 1’s register</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>i</em> + 2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Overwrite <code>counts[1]</code> with contents of register</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Increment register by 1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>i</em> + 3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">…</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Overwrite <code>counts[1]</code> with contents of register</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">Suppose that, prior to the execution sequence in <a href="ch14.xhtml#ch14tab1">Table 14-1</a>, <code>counts[1]</code> contains the value 60. In time step <em>i</em>, Thread 0 reads <code>counts[1]</code> and places the value 60 in Core 0’s register. In time step <em>i</em> + 1, while Thread 0 increments Core 0’s register by one, the <em>current</em> value in <code>counts[1]</code> (60) is read into Core 1’s register by Thread 1. In time step <em>i</em> + 2, Thread 0 updates <code>counts[1]</code> with the value 61 while Thread 1 increments the value stored in its local register (60) by one. The end result is that during time step <em>i</em> + 3, the value <code>counts[1]</code> is overwritten by Thread 1 with the value 61, not 62 as we would expect! This causes <code>counts[1]</code> to essentially “lose” an increment!</p>&#13;
<p class="indent">We refer to the scenario in which two threads attempt to write to the same location in memory as a <em>data race</em> condition. More generally, a <em>race condition</em> refers to any scenario in which the simultaneous execution of two operations gives an incorrect result. Note that a simultaneous read of the <code>counts[1]</code> location would <em>not</em> in and of itself constitute a race condition, because values can generally read alone from memory without issue. It was the combination of this step with the writes to <code>counts[1]</code> that caused the incorrect result. This read–modify–write pattern is a common source of a particular type of race condition, called a <em>data race</em>, in most threaded programs. In our discussion of race conditions and how to fix them, we focus on data races.</p>&#13;
<p class="note"><span epub:type="pagebreak" id="page_693"/><strong><span class="black">Note</span> ATOMIC OPERATIONS</strong></p>&#13;
<p class="note1">An operation is defined as being <em>atomic</em> if a thread perceives it as executing without interruption (in other words, as an “all or nothing” action). In some libraries, a keyword or type is used to specify that a block of computation should be treated as being atomic. In the previous example, the line <code>counts[val] = counts[val] + 1</code> (even if written as <code>counts[val]++</code>) is <em>not</em> atomic, because this line actually corresponds to several instructions at the machine level. A synchronization construct like mutual exclusion is needed to ensure that there are no data races. In general, all operations should be assumed to be nonatomic unless mutual exclusion is explicitly enforced.</p>&#13;
<p class="indent">Keep in mind that not all execution sequences of the two threads cause a race condition. Consider the sample execution sequence of Threads 0 and 1 in <a href="ch14.xhtml#ch14tab2">Table 14-2</a>.</p>&#13;
<p class="tabcap" id="ch14tab2"><strong>Table 14-2:</strong> Another Possible Execution Sequence of Two Threads Running <code>countElems</code></p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:10%"/>&#13;
<col style="width:45%"/>&#13;
<col style="width:45%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab"><strong>Time</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Thread 0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Thread 1</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>i</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Read <code>counts[1]</code> and place into Core 0’s register</p></td>&#13;
<td style="vertical-align: top"><p class="tab">…</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>i</em> + 1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Increment register by 1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">…</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>i</em> + 2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Overwrite <code>counts[1]</code> with contents of register</p></td>&#13;
<td style="vertical-align: top"><p class="tab">…</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>i</em> + 3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">…</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Read <code>counts[1]</code> and place into Core 1’s register</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>i</em> + 4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">…</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Increment register by 1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>i</em> + 5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">…</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Overwrite <code>counts[1]</code> with contents of register</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">In this execution sequence, Thread 1 does not read from <code>counts[1]</code> until after Thread 0 updates it with its new value (61). The end result is that Thread 1 reads the value 61 from <code>counts[1]</code> and places it into Core 1’s register during time step <em>i</em> + 3, and writes the value 62 to <code>counts[1]</code> in time step <em>i</em> + 5.</p>&#13;
<p class="indent">To fix a data race, we must first isolate the <em>critical section</em>, or the subset of code that must execute <em>atomically</em> (in isolation) to ensure correct behavior. In threaded programs, blocks of code that update a shared resource are typically identified to be critical sections.</p>&#13;
<p class="indent">In the <code>countElems</code> function, updates to the <code>counts</code> array should be put in a critical section to ensure that values are not lost due to multiple threads updating the same location in memory:</p>&#13;
<pre>&#13;
long i;<br/>&#13;
for (i = start; i &lt; end; i++) {<br/>&#13;
    val = array[i];<br/>&#13;
    counts[val] = counts[val] + 1; //this line needs to be protected<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">Since the fundamental problem in <code>countElems</code> is the simultaneous access of <code>counts</code> by multiple threads, a mechanism is needed to ensure that only one thread executes within the critical section at a time. Using a synchronization <span epub:type="pagebreak" id="page_694"/>construct (like a mutex, which is covered in the next section) will force the threads to enter the critical section sequentially.</p>&#13;
<h4 class="h4" id="lev2_241">14.3.1 Mutual Exclusion</h4>&#13;
<p class="right"><em>What is the mutex? The answer is out there, and it’s looking for you,</em><br/> <em>and it will find you if you want it to.</em></p>&#13;
<p class="right">—Trinity, explaining mutexes to Neo (with apologies to <em>The Matrix</em>)</p>&#13;
<p class="indenta">To fix the data race, let’s use a synchronization construct known as a mutual exclusion lock, or <em>mutex</em>. Mutual exclusion locks are a type of synchronization primitive that ensures that only one thread enters and executes the code inside the critical section at any given time.</p>&#13;
<p class="indent">Before using a mutex, a program must first declare the mutex in memory that’s shared by threads (often as a global variable), and then initialize the mutex before the threads need to use it (typically in the <code>main</code> function).</p>&#13;
<p class="indent">The Pthreads library defines a <code>pthread_mutex_t</code> type for mutexes. To declare a mutex variable, add this line:</p>&#13;
<pre>pthread_mutex_t mutex;</pre>&#13;
<p class="indent">To initialize the mutex use the <code>pthread_mutex_init</code> function, which takes the address of a mutex and an attribute structure, typically set to <code>NULL</code>:</p>&#13;
<pre>pthread_mutex_init(&amp;mutex, NULL);</pre>&#13;
<p class="indent">When the mutex is no longer needed (typically at the end of the <code>main</code> function, after <code>pthread_join</code>), a program should release the mutex structure by invoking the <code>pthread_mutex_destroy</code> function:</p>&#13;
<pre>pthread_mutex_destroy(&amp;mutex);</pre>&#13;
<h5 class="h5" id="lev3_117">The Mutex: Locked and Loaded</h5>&#13;
<p class="noindent">The initial state of a mutex is unlocked, meaning it’s immediately usable by any thread. To enter a critical section, a thread must first acquire a lock. This is accomplished with a call to the <code>pthread_mutex_lock</code> function. After a thread has the lock, no other thread can enter the critical section until the thread with the lock releases it. If another thread calls <code>pthread_mutex_lock</code> and the mutex is already locked, the thread will <em>block</em> (or wait) until the mutex becomes available. Recall that blocking implies that the thread will not be scheduled to use the CPU until the condition it’s waiting for (the mutex being available) becomes true (see “Process State” on <a href="ch13.xhtml#lev2_222">page 627</a>).</p>&#13;
<p class="indent">When a thread exits the critical section it must call the <code>pthread_mutex_unlock</code> function to release the mutex, making it available for another thread. Thus, at most one thread may acquire the lock and enter the critical section at a time, which prevents multiple threads from <em>racing</em> to read and update shared variables.</p>&#13;
<p class="indent">Having declared and initialized a mutex, the next question is where the lock and unlock functions should be placed to best enforce the critical <span epub:type="pagebreak" id="page_695"/>section. Here is an initial attempt at augmenting the <code>countElems</code> function with a mutex:<sup><a href="ch14.xhtml#fn14_5" id="rfn14_5">5</a></sup></p>&#13;
<pre>&#13;
pthread_mutex_t mutex; //global declaration of mutex, initialized in main()<br/>&#13;
<br/>&#13;
/*parallel version of step 1 of CountSort algorithm (attempt 1 with mutexes):<br/>&#13;
 * extracts arguments from args value<br/>&#13;
 * calculates component of the array that thread is responsible for counting<br/>&#13;
 * computes the frequency of all the elements in assigned component and stores<br/>&#13;
 * the associated counts of each element in counts array<br/>&#13;
*/<br/>&#13;
void *countElems( void *args ) {<br/>&#13;
    //extract arguments<br/>&#13;
    //ommitted for brevity<br/>&#13;
    int *array = myargs-&gt;ap;<br/>&#13;
    long *counts = myargs-&gt;countp;<br/>&#13;
<br/>&#13;
    //assign work to the thread<br/>&#13;
    long chunk = length / nthreads; //nominal chunk size<br/>&#13;
    long start = myid * chunk;<br/>&#13;
    long end = (myid + 1) * chunk;<br/>&#13;
    long val;<br/>&#13;
    if (myid == nthreads - 1) {<br/>&#13;
        end = length;<br/>&#13;
    }<br/>&#13;
    long i;<br/>&#13;
<br/>&#13;
    //heart of the program<br/>&#13;
    pthread_mutex_lock(&amp;mutex); //acquire the mutex lock<br/>&#13;
    for (i = start; i &lt; end; i++) {<br/>&#13;
        val = array[i];<br/>&#13;
        counts[val] = counts[val] + 1;<br/>&#13;
    }<br/>&#13;
    pthread_mutex_unlock(&amp;mutex); //release the mutex lock<br/>&#13;
<br/>&#13;
    return NULL;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">The mutex initialize and destroy functions are placed in <code>main</code> around the thread creation and join functions:</p>&#13;
<pre>&#13;
//code snippet from main():<br/>&#13;
<br/>&#13;
pthread_mutex_init(&amp;mutex, NULL); //initialize the mutex<br/>&#13;
<br/>&#13;
for (t = 0; t &lt; nthreads; t++) {<br/>&#13;
    pthread_create( &amp;thread_array[t], NULL, countElems, &amp;thread_args[t] );<br/>&#13;
}<br/><br/>&#13;
<span epub:type="pagebreak" id="page_696"/>for (t = 0; t &lt; nthreads; t++) {<br/>&#13;
    pthread_join(thread_array[t], NULL);<br/>&#13;
}<br/>&#13;
pthread_mutex_destroy(&amp;mutex); //destroy (free) the mutex<br/>&#13;
</pre>&#13;
<p class="indent">Let’s recompile and run this new program while varying the number of threads:</p>&#13;
<pre>&#13;
$ <span class="codestrong1">./countElems_p_v2 10000000 1 1</span><br/>&#13;
Counts array:<br/>&#13;
999170 1001044 999908 1000431 999998 1001479 999709 997250 1000804 1000207<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p_v2 10000000 1 2</span><br/>&#13;
Counts array:<br/>&#13;
999170 1001044 999908 1000431 999998 1001479 999709 997250 1000804 1000207<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p_v2 10000000 1 4</span><br/>&#13;
Counts array:<br/>&#13;
999170 1001044 999908 1000431 999998 1001479 999709 997250 1000804 1000207<br/>&#13;
</pre>&#13;
<p class="indent">Excellent, the output is <em>finally</em> consistent regardless of the number of threads used!</p>&#13;
<p class="indent">Recall that another primary goal of threading is to reduce the runtime of a program as the number of threads increases (i.e., to <em>speed up</em> program execution). Let’s benchmark the performance of the <code>countElems</code> function. Although it may be tempting to use a command line utility like <code>time -p</code>, recall that invoking <code>time -p</code> measures the wall-clock time of the <em>entire</em> program (including the generation of random elements) and <em>not</em> just the running of the <code>countElems</code> function. In this case, it is better to use a system call like <code>gettimeofday</code>, which allows a user to accurately measure the wall-clock time of a particular section of code. Benchmarking <code>countElems</code> on 100 million elements yields the following run times:</p>&#13;
<pre>&#13;
$ <span class="codestrong1">./countElems_p_v2 100000000 0 1</span><br/>&#13;
Time for Step 1 is 0.368126 s<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p_v2 100000000 0 2</span><br/>&#13;
Time for Step 1 is 0.438357 s<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p_v2 100000000 0 4</span><br/>&#13;
Time for Step 1 is 0.519913 s</pre>&#13;
<p class="indent">Adding more threads causes the program to get <em>slower</em>! This goes against the goal of making programs <em>faster</em> with threads.</p>&#13;
<p class="indent">To understand what is going on, consider where the locks are placed in the <code>countsElems</code> function:</p>&#13;
<pre><span epub:type="pagebreak" id="page_697"/>&#13;
//code snippet from the countElems function from earlier<br/>&#13;
//the heart of the program<br/>&#13;
pthread_mutex_lock(&amp;mutex); //acquire the mutex lock<br/>&#13;
for (i = start; i &lt; end; i++){<br/>&#13;
    val = array[i];<br/>&#13;
    counts[val] = counts[val] + 1;<br/>&#13;
}<br/>&#13;
pthread_mutex_unlock(&amp;mutex); //release the mutex lock<br/>&#13;
</pre>&#13;
<p class="indent">In this example, we placed the lock around the <em>entirety</em> of the <code>for</code> loop. Even though this placement solves the correctness problems, it’s an extremely poor decision from a performance perspective—the critical section now encompasses the entire loop body. Placing locks in this manner guarantees that only one thread can execute the loop at a time, effectively serializing the program!</p>&#13;
<h5 class="h5" id="lev3_118">The Mutex: Reloaded</h5>&#13;
<p class="noindent">Let’s try another approach and place the mutex locking and unlocking functions within every iteration of the loop:</p>&#13;
<pre>&#13;
/*modified code snippet of countElems function:<br/>&#13;
 *locks are now placed INSIDE the for loop!<br/>&#13;
*/<br/>&#13;
//the heart of the program<br/>&#13;
for (i = start; i &lt; end; i++) {<br/>&#13;
    val = array[i];<br/>&#13;
    pthread_mutex_lock(&amp;m); //acquire the mutex lock<br/>&#13;
    counts[val] = counts[val] + 1;<br/>&#13;
    pthread_mutex_unlock(&amp;m); //release the mutex lock<br/>&#13;
}&#13;
</pre>&#13;
<p class="indent">This may initially look like a better solution because each thread can enter the loop in parallel, serializing only when reaching the lock. The critical section is very small, encompassing only the line <code>counts[val] = counts[val] + 1</code>.</p>&#13;
<p class="indent">Let’s first perform a correctness check on this version of the program:</p>&#13;
<pre>&#13;
$ <span class="codestrong1">./countElems_p_v3 10000000 1 1</span><br/>&#13;
Counts array:<br/>&#13;
999170 1001044 999908 1000431 999998 1001479 999709 997250 1000804 1000207<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p_v3 10000000 1 2</span><br/>&#13;
Counts array:<br/>&#13;
999170 1001044 999908 1000431 999998 1001479 999709 997250 1000804 1000207<br/>&#13;
<br/>&#13;
<span epub:type="pagebreak" id="page_698"/>$ <span class="codestrong1">./countElems_p_v3 10000000 1 4</span><br/>&#13;
Counts array:<br/>&#13;
999170 1001044 999908 1000431 999998 1001479 999709 997250 1000804 1000207<br/>&#13;
</pre>&#13;
<p class="indent">So far so good. This version of the program also produces consistent output regardless of the number of threads employed.</p>&#13;
<p class="indent">Now, let’s look at performance:</p>&#13;
<pre>&#13;
$ <span class="codestrong1">./countElems_p_v3 100000000 0 1</span><br/>&#13;
Time for Step 1 is 1.92225 s<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p_v3 100000000 0 2</span><br/>&#13;
Time for Step 1 is 10.9704 s<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p_v3 100000000 0 4</span><br/>&#13;
Time for Step 1 is 9.13662 s<br/>&#13;
</pre>&#13;
<p class="indent">Running this version of the code yields (amazingly enough) a <em>significantly slower</em> runtime!</p>&#13;
<p class="indent">As it turns out, locking and unlocking a mutex are expensive operations. Recall what was covered in the discussion on function call optimizations (see “Function Inlining” on <a href="ch12.xhtml#lev1_95">page 604</a>): calling a function repeatedly (and needlessly) in a loop can be a major cause of slowdown in a program. In our prior use of mutexes, each thread locks and unlocks the mutex exactly once. In the current solution, each thread locks and unlocks the mutex <em>n</em>/<em>t</em> times, where <em>n</em> is the size of the array, <em>t</em> is the number of threads, and <em>n</em>/<em>t</em> is the size of the array component assigned to each particular thread. As a result, the cost of the additional mutex operations slows down the loop’s execution considerably.</p>&#13;
<h5 class="h5" id="lev3_119">The Mutex: Revisited</h5>&#13;
<p class="noindent">In addition to protecting the critical section to achieve correct behavior, an ideal solution would use the lock and unlock functions as little as possible, and reduce the critical section to the smallest possible size.</p>&#13;
<p class="indent">The original implementation satisfies the first requirement, whereas the second implementation tries to accomplish the second. At first glance, it appears that the two requirements are incompatible with each other. Is there a way to actually accomplish both (and while we are at it, speed up the execution of our program)?</p>&#13;
<p class="indent">For the next attempt, each thread maintains a private, <em>local</em> array of counts on its stack. Because the array is local to each thread, a thread can access it without locking—there’s no risk of a race condition on data that isn’t shared between threads. Each thread processes its assigned subset of the shared array and populates its local counts array. After counting up all the values within its subset, each thread:</p>&#13;
<div class="number">&#13;
<p class="number">1. Locks the shared mutex (entering a critical section).</p>&#13;
<p class="number"><span epub:type="pagebreak" id="page_699"/>2. Adds the values from its local counts array to the shared counts array.</p>&#13;
<p class="number">3. Unlocks the shared mutex (exiting the critical section).</p>&#13;
</div>&#13;
<p class="indent">Restricting each thread to update the shared counts array only once significantly reduces the contention for shared variables and minimizes expensive mutex operations.</p>&#13;
<p class="indent">The following is our revised <code>countElems</code> function:<sup><a href="ch14.xhtml#fn14_6" id="rfn14_6">6</a></sup></p>&#13;
<pre>&#13;
/*parallel version of step 1 of CountSort algorithm (final attempt w/mutexes):<br/>&#13;
 * extracts arguments from args value<br/>&#13;
 * calculates component of the array that thread is responsible for counting<br/>&#13;
 * computes the frequency of all the elements in assigned component and stores<br/>&#13;
 * the associated counts of each element in counts array<br/>&#13;
*/<br/>&#13;
void *countElems( void *args ) {<br/>&#13;
    //extract arguments<br/>&#13;
    //ommitted for brevity<br/>&#13;
    int *array = myargs-&gt;ap;<br/>&#13;
    long *counts = myargs-&gt;countp;<br/>&#13;
<br/>&#13;
    //local declaration of counts array, initializes every element to zero.<br/>&#13;
    long local_counts[MAX] = {0};<br/>&#13;
<br/>&#13;
    //assign work to the thread<br/>&#13;
    long chunk = length / nthreads; //nominal chunk size<br/>&#13;
    long start = myid * chunk;<br/>&#13;
    long end = (myid + 1) * chunk;<br/>&#13;
    long val;<br/>&#13;
    if (myid == nthreads-1)<br/>&#13;
        end = length;<br/>&#13;
<br/>&#13;
    long i;<br/>&#13;
<br/>&#13;
    //heart of the program<br/>&#13;
    for (i = start; i &lt; end; i++) {<br/>&#13;
        val = array[i];<br/>&#13;
<br/>&#13;
        //updates local counts array<br/>&#13;
        local_counts[val] = local_counts[val] + 1;<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    //update to global counts array<br/>&#13;
    pthread_mutex_lock(&amp;mutex); //acquire the mutex lock<br/>&#13;
    for (i = 0; i &lt; MAX; i++) {<br/>&#13;
        counts[i] += local_counts[i];<br/>&#13;
    }<br/>&#13;
    pthread_mutex_unlock(&amp;mutex); //release the mutex lock<br/>&#13;
<span epub:type="pagebreak" id="page_700"/>    return NULL;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">This version has a few additional features:</p>&#13;
<ul>&#13;
<li class="noindent">The presence of <code>local_counts</code>, an array that is private to the scope of each thread (i.e., allocated in the thread’s stack). Like <code>counts</code>, <code>local_counts</code> contains <code>MAX</code> elements, given that <code>MAX</code> is the maximum value any element can hold in our input array.</li>&#13;
<li class="noindent">Each thread makes updates to <code>local_counts</code> at its own pace, without any contention for shared variables.</li>&#13;
<li class="noindent">A single call to <code>pthread_mutex_lock</code> protects each thread’s update to the global <code>counts</code> array, which happens only once at the end of each thread’s execution.</li>&#13;
</ul>&#13;
<p class="indent">In this manner, we reduce the time each thread spends in a critical section to just updating the shared counts array. Even though only one thread can enter the critical section at a time, the time each thread spends there is proportional to <code>MAX</code>, not <em>n</em>, the length of the global array. Since <code>MAX</code> is much less than <em>n</em>, we should see an improvement in performance.</p>&#13;
<p class="indent">Let’s now benchmark this version of our code:</p>&#13;
<pre>&#13;
$ <span class="codestrong1">./countElems_p_v3 100000000 0 1</span><br/>&#13;
Time for Step 1 is 0.334574 s<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p_v3 100000000 0 2</span><br/>&#13;
Time for Step 1 is 0.209347 s<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p_v3 100000000 0 4</span><br/>&#13;
Time for Step 1 is 0.130745 s<br/>&#13;
</pre>&#13;
<p class="indent">Wow, what a difference! Our program not only computes the correct answers, but also executes faster as we increase the number of threads.</p>&#13;
<p class="indent">The lesson to take away here is this: to efficiently minimize a critical section, use local variables to collect intermediate values. After the hard work requiring parallelization is over, use a mutex to safely update any shared variable(s).</p>&#13;
<h5 class="h5" id="lev3_120">Deadlock</h5>&#13;
<p class="noindent">In some programs, waiting threads have dependencies on one another. A situation called <em>deadlock</em> can arise when multiple synchronization constructs like mutexes are incorrectly applied. A deadlocked thread is blocked from execution by another thread, which <em>itself</em> is blocked on a blocked thread. Gridlock (in which cars in all directions cannot move forward due to being blocked by other cars) is a common real-world example of deadlock that occurs at busy city intersections.</p>&#13;
<p class="indent">To illustrate a deadlock scenario in code, let’s consider an example where multithreading is used to implement a banking application. Each user’s <span epub:type="pagebreak" id="page_701"/>account is defined by a balance and its own mutex (ensuring that no race conditions can occur when updating the balance):</p>&#13;
<pre>&#13;
struct account {<br/>&#13;
    pthread_mutex_t lock;<br/>&#13;
    int balance;<br/>&#13;
};<br/>&#13;
</pre>&#13;
<p class="indent">Consider the following naive implementation of a <code>Transfer</code> function that moves money from one bank account to another:</p>&#13;
<pre>&#13;
void *Transfer(void *args){<br/>&#13;
    //argument passing removed to increase readability<br/>&#13;
    //...<br/>&#13;
<br/>&#13;
    pthread_mutex_lock(&amp;fromAcct-&gt;lock);<br/>&#13;
    pthread_mutex_lock(&amp;toAcct-&gt;lock);<br/>&#13;
<br/>&#13;
    fromAcct-&gt;balance -= amt;<br/>&#13;
    toAcct-&gt;balance += amt;<br/>&#13;
<br/>&#13;
    pthread_mutex_unlock(&amp;fromAcct-&gt;lock);<br/>&#13;
    pthread_mutex_unlock(&amp;toAcct-&gt;lock);<br/>&#13;
<br/>&#13;
    return NULL;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">Suppose that Threads 0 and 1 are executing concurrently and represent users A and B, respectively. Now consider the situation in which A and B want to transfer money to each other: A wants to transfer 20 dollars to B, while B wants to transfer 40 dollars to A.</p>&#13;
<p class="indent">In the path of execution highlighted by <a href="ch14.xhtml#ch14fig7">Figure 14-7</a>, both threads concurrently execute the <code>Transfer</code> function. Thread 0 acquires the lock of <code>acctA</code> while Thread 1 acquires the lock of <code>acctB</code>. Now consider what happens. To continue executing, Thread 0 needs to acquire the lock on <code>acctB</code>, which Thread 1 holds. Likewise, Thread 1 needs to acquire the lock on <code>acctA</code> to continue executing, which Thread 0 holds. Since both threads are blocked on each other, they are in deadlock.</p>&#13;
<div class="imagec" id="ch14fig7"><img alt="image" src="../images/14fig07.jpg"/></div>&#13;
<p class="figcap"><em>Figure 14-7: An example of deadlock</em></p>&#13;
<p class="indent">Although the OS provides some protection against deadlock, programmers should be mindful about writing code that increases the likelihood of <span epub:type="pagebreak" id="page_702"/>deadlock. For example, the preceding scenario could have been avoided by rearranging the locks so that each lock/unlock pair surrounds only the balance update statement associated with it:</p>&#13;
<pre>&#13;
void *Transfer(void *args){<br/>&#13;
    //argument passing removed to increase readability<br/>&#13;
    //...<br/>&#13;
<br/>&#13;
    pthread_mutex_lock(&amp;fromAcct-&gt;lock);<br/>&#13;
    fromAcct-&gt;balance -= amt;<br/>&#13;
    pthread_mutex_unlock(&amp;fromAcct-&gt;lock);<br/>&#13;
<br/>&#13;
    pthread_mutex_lock(&amp;toAcct-&gt;lock);<br/>&#13;
    toAcct-&gt;balance += amt;<br/>&#13;
    pthread_mutex_unlock(&amp;toAcct-&gt;lock);<br/>&#13;
<br/>&#13;
    return NULL;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">Deadlock is not a situation that is unique to threads. Processes (especially those that are communicating with one another) can deadlock with one another. Programmers should be mindful of the synchronization primitives they use and the consequences of using them incorrectly.</p>&#13;
<h4 class="h4" id="lev2_242">14.3.2 Semaphores</h4>&#13;
<p class="noindent">Semaphores are commonly used in operating systems and concurrent programs where the goal is to manage concurrent access to a pool of resources. When using a semaphore, the goal isn’t <em>who</em> owns what, but <em>how many</em> resources are still available. Semaphores are different from mutexes in several ways:</p>&#13;
<ul>&#13;
<li class="noindent">Semaphores need not be in a binary (locked or unlocked) state. A special type of semaphore called a <em>counting semaphore</em> can range in value from 0 to some <em>r</em>, where <em>r</em> is the number of possible resources. Any time a resource is produced, the semaphore is incremented. Any time a resource is being used, the semaphore is decremented. When a counting semaphore has a value of 0, it means that no resources are available, and any other threads that attempt to acquire a resource must wait (e.g., block).</li>&#13;
<li class="noindent">Semaphores can be locked by default.</li>&#13;
</ul>&#13;
<p class="indent">While a mutex and condition variables can simulate the functionality of a semaphore, using a semaphore may be simpler and more efficient in some cases. Semaphores also have the advantage that <em>any</em> thread can unlock the semaphore (in contrast to a mutex, where the calling thread must unlock it).</p>&#13;
<p class="indent">Semaphores are not part of the Pthreads library, but that does not mean that you cannot use them. On Linux and macOS systems, semaphore primitives can be accessed from <code>semaphore.h</code>, typically located in <code>/usr/include</code>. Since <span epub:type="pagebreak" id="page_703"/>there is no standard, the function calls may differ on different systems. That said, the semaphore library has similar declarations to those for mutexes:</p>&#13;
<ul>&#13;
<li class="noindent">Declare a semaphore (type <code>sem_t</code>, e.g., <code>sem_t semaphore</code>).</li>&#13;
<li class="noindent">Initialize a semaphore using <code>sem_init</code> (usually in <code>main</code>). The <code>sem_init</code> function has three parameters: the first is the address of a semaphore, the second is its initial state (locked or unlocked), and the third parameter indicates whether the semaphore should be shared with the threads of a process (e.g., with value 0) or between processes (e.g., with value 1). This is useful because semaphores are commonly used for process synchronization. For example, initializing a semaphore with the call <code>sem_init(&amp;semaphore, 1, 0)</code> indicates that our semaphore is initially locked (the second parameter is 1), and is to be shared among the threads of a common process (the third parameter is 0). In contrast, mutexes always start out unlocked. It is important to note that in macOS, the equivalent function is <code>sem_open</code>.</li>&#13;
<li class="noindent">Destroy a semaphore using <code>sem_destroy</code> (usually in <code>main</code>). This function only takes a pointer to the semaphore (<code>sem_destroy(&amp;semaphore)</code>). Note that in macOS, the equivalent function may be <code>sem_unlink</code> or <code>sem_close</code>.</li>&#13;
<li class="noindent">The <code>sem_wait</code> function indicates that a resource is being used, and decrements the semaphore. If the semaphore’s value is greater than 0 (indicating there are still resources available), the function will immediately return, and the thread is allowed to proceed. If the semaphore’s value is already 0, the thread will block until a resource becomes available (i.e., the semaphore has a positive value). A call to <code>sem_wait</code> typically looks like <code>sem_wait(&amp;semaphore)</code>.</li>&#13;
<li class="noindent">The <code>sem_post</code> function indicates that a resource is being freed, and increments the semaphore. This function returns immediately. If there is a thread waiting on the semaphore (i.e., the semaphore’s value was previously 0), then the other thread will take ownership of the freed resource. A call to <code>sem_post</code> looks like <code>sem_post(&amp;semaphore)</code>.</li>&#13;
</ul>&#13;
<h4 class="h4" id="lev2_243">14.3.3 Other Synchronization Constructs</h4>&#13;
<p class="noindent">Mutexes and semaphores are not the only example of synchronization constructs that can be used in the context of multithreaded programs. In this subsection we will briefly discuss the barrier and condition variable synchronization constructs, which are both part of the Pthreads library.</p>&#13;
<h5 class="h5" id="lev3_121">Barriers</h5>&#13;
<p class="noindent">A <em>barrier</em> is a type of synchronization construct that forces <em>all</em> threads to reach a common point in execution before releasing the threads to continue executing concurrently. Pthreads offers a barrier synchronization primitive. To use Pthreads barriers, it is necessary to do the following:</p>&#13;
<ul>&#13;
<li class="noindent">Declare a barrier global variable (e.g., <code>pthread_barrier_t barrier</code>)</li>&#13;
<li class="noindent"><span epub:type="pagebreak" id="page_704"/>Initialize the barrier in <code>main</code> (<code>pthread_barrier_init(&amp;barrier)</code>)</li>&#13;
<li class="noindent">Destroy the barrier in <code>main</code> after use (<code>pthread_barrier_destroy(&amp;barrier)</code>)</li>&#13;
<li class="noindent">Use the <code>pthread_barrier_wait</code> function to create a synchronization point.</li>&#13;
</ul>&#13;
<p class="indent">The following program shows the use of a barrier in a function called <code>threadEx</code>:</p>&#13;
<pre>&#13;
void *threadEx(void *args){<br/>&#13;
    //parse args<br/>&#13;
    //...<br/>&#13;
    long myid = myargs-&gt;id;<br/>&#13;
    int nthreads = myargs-&gt;numthreads;<br/>&#13;
    int *array = myargs-&gt;array<br/>&#13;
<br/>&#13;
    printf("Thread %ld starting thread work!\n", myid);<br/>&#13;
    pthread_barrier_wait(&amp;barrier); //forced synchronization point<br/>&#13;
    printf("All threads have reached the barrier!\n");<br/>&#13;
    for (i = start; i &lt; end; i++) {<br/>&#13;
        array[i] = array[i] * 2;<br/>&#13;
    }<br/>&#13;
    printf("Thread %ld done with work!\n", myid);<br/>&#13;
<br/>&#13;
    return NULL;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">In this example, no thread can start processing its assigned portion of the array until <em>every</em> thread has printed out the message that they are starting work. Without the barrier, it is possible for one thread to have finished work before the other threads have printed their starting work message! Notice that it is <em>still</em> possible for one thread to print that it is done doing work before another thread finishes.</p>&#13;
<h5 class="h5" id="lev3_122">Condition Variables</h5>&#13;
<p class="noindent">Condition variables force a thread to block until a particular condition is reached. This construct is useful for scenarios in which a condition must be met before the thread does some work. In the absence of condition variables, a thread would have to repeatedly check to see whether the condition is met, continuously utilizing the CPU. Condition variables are always used in conjunction with a mutex. In this type of synchronization construct, the mutex enforces mutual exclusion, whereas the condition variable ensures that particular conditions are met before a thread acquires the mutex.</p>&#13;
<p class="indent">POSIX condition variables have the type <code>pthread_cond_t</code>. Like the mutex and barrier constructs, condition variables must be initialized prior to use and destroyed after use.</p>&#13;
<p class="indent">To initialize a condition variable, use the <code>pthread_cond_init</code> function. To destroy a condition variable, use the <code>pthread_cond_destroy</code> function.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_705"/>The two functions commonly invoked when using condition variables are <code>pthread_cond_wait</code> and <code>pthread_cond_signal</code>. Both functions require the address of a mutex in addition to the address of the condition variable.</p>&#13;
<ul>&#13;
<li class="noindent">The <code>pthread_cond_wait(&amp;cond, &amp;mutex)</code> function takes the addresses of a condition variable <code>cond</code> and a mutex <code>mutex</code> as its arguments. It causes the calling thread to block on the condition variable <code>cond</code> until another thread signals it (or “wakes” it up).</li>&#13;
<li class="noindent">The <code>pthread_cond_signal(&amp;cond)</code> function causes the calling thread to unblock (or signal) another thread that is waiting on the condition variable <code>cond</code> (based on scheduling priority). If no threads are currently blocked on the condition, then the function has no effect. Unlike <code>pthread_cond_wait</code>, the <code>pthread_cond_signal</code> function can be called by a thread regardless of whether it owns the mutex in which <code>pthread_cond_wait</code> is called.</li>&#13;
</ul>&#13;
<h5 class="h5" id="lev3_123">Condition Variable Example</h5>&#13;
<p class="noindent">Traditionally, condition variables are most useful when a subset of threads are waiting on another set to complete some action. In the following example, we use multiple threads to simulate a set of farmers collecting eggs from a set of chickens. “Chicken” and “Farmer” represent two separate classes of threads. The full source of this program can be downloaded;<sup><a href="ch14.xhtml#fn14_7" id="rfn14_7">7</a></sup> note that this listing excludes many comments/error handling for brevity.</p>&#13;
<p class="indent">The <code>main</code> function creates a shared variable <code>num_eggs</code> (which indicates the total number of eggs available at any given time), a shared <code>mutex</code> (which is used whenever a thread accesses <code>num_eggs</code>), and a shared condition variable <code>eggs</code>. It then creates two Chicken and two Farmer threads:</p>&#13;
<pre>&#13;
int main(int argc, char **argv){<br/>&#13;
    //... declarations omitted for brevity<br/>&#13;
<br/>&#13;
    // these will be shared by all threads via pointer fields in t_args<br/>&#13;
    int num_eggs;           // number of eggs ready to collect<br/>&#13;
    pthread_mutex_t mutex;  // mutex associated with cond variable<br/>&#13;
    pthread_cond_t  eggs;   // used to block/wake-up farmer waiting for eggs<br/>&#13;
<br/>&#13;
    //... args parsing removed for brevity<br/>&#13;
<br/>&#13;
    num_eggs = 0; // number of eggs ready to collect<br/>&#13;
    ret = pthread_mutex_init(&amp;mutex, NULL); //initialize the mutex<br/>&#13;
    pthread_cond_init(&amp;eggs, NULL); //initialize the condition variable<br/>&#13;
<br/>&#13;
    //... thread_array and thread_args creation/filling omitted for brevity<br/>&#13;
<br/>&#13;
    // create some chicken and farmer threads<br/>&#13;
    for (i = 0; i &lt; (2 * nthreads); i++) {<br/>&#13;
        if ( (i % 2) == 0 ) {<br/>&#13;
            ret = pthread_create(&amp;thread_array[i], NULL,<br/>&#13;
<span epub:type="pagebreak" id="page_706"/>                                 chicken, &amp;thread_args[i]);<br/>&#13;
        }<br/>&#13;
        else {<br/>&#13;
            ret = pthread_create(&amp;thread_array[i], NULL,<br/>&#13;
                                 farmer, &amp;thread_args[i] );<br/>&#13;
        }<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    // wait for chicken and farmer threads to exit<br/>&#13;
    for (i = 0; i &lt; (2 * nthreads); i++)  {<br/>&#13;
        ret = pthread_join(thread_array[i], NULL);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    // clean-up program state<br/>&#13;
    pthread_mutex_destroy(&amp;mutex); //destroy the mutex<br/>&#13;
    pthread_cond_destroy(&amp;eggs);   //destroy the cond var<br/>&#13;
<br/>&#13;
    return 0;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">Each Chicken thread is responsible for laying a certain number of eggs:</p>&#13;
<pre>&#13;
void *chicken(void *args ) {<br/>&#13;
    struct t_arg *myargs = (struct t_arg *)args;<br/>&#13;
    int *num_eggs, i, num;<br/>&#13;
<br/>&#13;
    num_eggs = myargs-&gt;num_eggs;<br/>&#13;
    i = 0;<br/>&#13;
<br/>&#13;
    // lay some eggs<br/>&#13;
    for (i = 0; i &lt; myargs-&gt;total_eggs; i++) {<br/>&#13;
        usleep(EGGTIME); //chicken sleeps<br/>&#13;
<br/>&#13;
        pthread_mutex_lock(myargs-&gt;mutex);<br/>&#13;
        *num_eggs = *num_eggs + 1;  // update number of eggs<br/>&#13;
        num = *num_eggs;<br/>&#13;
        pthread_cond_signal(myargs-&gt;eggs); // wake a sleeping farmer (squawk)<br/>&#13;
        pthread_mutex_unlock(myargs-&gt;mutex);<br/>&#13;
<br/>&#13;
        printf("chicken %d created egg %d available %d\n",myargs-&gt;id,i,num);<br/>&#13;
    }<br/>&#13;
    return NULL;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">To lay an egg, a Chicken thread sleeps for a while, acquires the mutex and updates the total number of available eggs by one. Prior to releasing the mutex, the Chicken thread “wakes up” a sleeping Farmer (presumably <span epub:type="pagebreak" id="page_707"/>by squawking). The Chicken thread repeats the cycle until it has laid all the eggs it intends to (<code>total_eggs</code>).</p>&#13;
<p class="indent">Each Farmer thread is responsible for collecting <code>total_eggs</code> eggs from the set of chickens (presumably for their breakfast):</p>&#13;
<pre>&#13;
void *farmer(void *args ) {<br/>&#13;
    struct t_arg * myargs = (struct t_arg *)args;<br/>&#13;
    int *num_eggs, i, num;<br/>&#13;
<br/>&#13;
    num_eggs = myargs-&gt;num_eggs;<br/>&#13;
<br/>&#13;
    i = 0;<br/>&#13;
<br/>&#13;
    for (i = 0; i &lt; myargs-&gt;total_eggs; i++) {<br/>&#13;
        pthread_mutex_lock(myargs-&gt;mutex);<br/>&#13;
        while (*num_eggs == 0 ) { // no eggs to collect<br/>&#13;
            // wait for a chicken to lay an egg<br/>&#13;
            pthread_cond_wait(myargs-&gt;eggs, myargs-&gt;mutex);<br/>&#13;
        }<br/>&#13;
<br/>&#13;
        // we hold mutex lock here and num_eggs &gt; 0<br/>&#13;
        num = *num_eggs;<br/>&#13;
        *num_eggs = *num_eggs - 1;<br/>&#13;
        pthread_mutex_unlock(myargs-&gt;mutex);<br/>&#13;
<br/>&#13;
        printf("farmer %d gathered egg %d available %d\n",myargs-&gt;id,i,num);<br/>&#13;
    }<br/>&#13;
    return NULL;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">Each Farmer thread acquires the mutex prior to checking the shared <code>num_eggs</code> variable to see whether any eggs are available (<code>*num_eggs == 0</code>). While there aren’t any eggs available, the Farmer thread blocks (i.e., takes a nap).</p>&#13;
<p class="indent">After the Farmer thread “wakes up” due to a signal from a Chicken thread, it checks to see that an egg is still available (another Farmer could have grabbed it first) and if so, the Farmer “collects” an egg (decrementing <code>num_eggs</code> by one) and releases the mutex.</p>&#13;
<p class="indent">In this manner, the Chicken and Farmer work in concert to lay/collect eggs. Condition variables ensure that no Farmer thread collects an egg until it is laid by a Chicken thread.</p>&#13;
<h5 class="h5" id="lev3_124">Broadcasting</h5>&#13;
<p class="noindent">Another function used with condition variables is <code>pthread_cond_broadcast</code>, which is useful when multiple threads are blocked on a particular condition. Calling <code>pthread_cond_broadcast(&amp;cond)</code> wakes up <em>all</em> threads that are blocked on condition <code>cond</code>. In this next example, we show how condition variables can implement the barrier construct discussed previously:</p>&#13;
<pre><span epub:type="pagebreak" id="page_708"/>&#13;
// mutex (initialized in main)<br/>&#13;
pthread_mutex_t mutex;<br/>&#13;
<br/>&#13;
// condition variable signifying the barrier (initialized in main)<br/>&#13;
pthread_cond_t barrier;<br/>&#13;
<br/>&#13;
void *threadEx_v2(void *args){<br/>&#13;
    // parse args<br/>&#13;
    // ...<br/>&#13;
<br/>&#13;
    long myid = myargs-&gt;id;<br/>&#13;
    int nthreads = myargs-&gt;numthreads;<br/>&#13;
    int *array = myargs-&gt;array<br/>&#13;
<br/>&#13;
    // counter denoting the number of threads that reached the barrier<br/>&#13;
    int *n_reached = myargs-&gt;n_reached;<br/>&#13;
<br/>&#13;
    // start barrier code<br/>&#13;
    pthread_mutex_lock(&amp;mutex);<br/>&#13;
    *n_reached++;<br/>&#13;
<br/>&#13;
    printf("Thread %ld starting work!\n", myid)<br/>&#13;
<br/>&#13;
    // if some threads have not reached the barrier<br/>&#13;
    while (*n_reached &lt; nthreads) {<br/>&#13;
        pthread_cond_wait(&amp;barrier, &amp;mutex);<br/>&#13;
    }<br/>&#13;
    // all threads have reached the barrier<br/>&#13;
    printf("all threads have reached the barrier!\n");<br/>&#13;
    pthread_cond_broadcast(&amp;barrier);<br/>&#13;
<br/>&#13;
    pthread_mutex_unlock(&amp;mutex);<br/>&#13;
    // end barrier code<br/>&#13;
<br/>&#13;
    // normal thread work<br/>&#13;
    for (i = start; i &lt; end; i++) {<br/>&#13;
        array[i] = array[i] * 2;<br/>&#13;
    }<br/>&#13;
    printf("Thread %ld done with work!\n", myid);<br/>&#13;
<br/>&#13;
    return NULL;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">The function <code>threadEx_v2</code> has identical functionality to <code>threadEx</code>. In this example, the condition variable is named <code>barrier</code>. As each thread acquires the lock, it increments <code>n_reached</code>, the number of threads that have reached that point. While the number of threads that have reached the barrier is less <span epub:type="pagebreak" id="page_709"/>than the total number of threads, the thread waits on the condition variable <code>barrier</code> and mutex <code>mutex</code>.</p>&#13;
<p class="indent">However, when the last thread reaches the barrier, it calls <code>pthread_cond _broadcast(&amp;barrier)</code>, which releases <em>all</em> the other threads that are waiting on the condition variable <code>barrier</code>, enabling them to continue execution.</p>&#13;
<p class="indent">This example is useful for illustrating the <code>pthread_cond_broadcast</code> function; however, it is best to use the Pthreads barrier primitive whenever barriers are necessary in a program.</p>&#13;
<p class="indent">One question that students tend to ask is if the <code>while</code> loop around the call to <code>pthread_cond_wait</code> in the <code>farmer</code> and <code>threadEx_v2</code> code can be replaced with an <code>if</code> statement. This <code>while</code> loop is in fact absolutely necessary for two main reasons. First, the condition may change prior to the woken thread arriving to continue execution. The <code>while</code> loop enforces that the condition be retested one last time. Second, the <code>pthread_cond_wait</code> function is vulnerable to <em>spurious wakeups</em>, in which a thread is erroneously woken up even though the condition may not be met. The <code>while</code> loop is in fact an example of a <em>predicate loop</em>, which forces a final check of the condition variable before releasing the mutex. The use of predicate loops is therefore correct practice when using condition variables.</p>&#13;
<h3 class="h3" id="lev1_108">14.4 Measuring the Performance of Parallel Programs</h3>&#13;
<p class="noindent">So far, we have used the <code>gettimeofday</code> function to measure the amount of time it takes for programs to execute. In this section, we discuss how to measure how well a parallel program performs in comparison to a serial program as well as other topics related to measuring the performance of parallel programs.</p>&#13;
<h4 class="h4" id="lev2_244">14.4.1 Parallel Performance Basics</h4>&#13;
<h5 class="h5" id="lev3_125">Speedup</h5>&#13;
<p class="noindent">Suppose that a program takes <em>T</em><sub>c</sub> time to execute on <em>c</em> cores. Thus, the serial version of the program would take <em>T</em><sub>1</sub> time. The speedup of the program on <em>c</em> cores is then expressed by this equation:</p>&#13;
<div class="image1"><img alt="image" src="../images/equ0709-01.jpg"/></div>&#13;
<p class="indent">If a serial program takes 60 seconds to execute, while its parallel version takes 30 seconds on 2 cores, the corresponding speedup is 2. Likewise if that program takes 15 seconds on 4 cores, the speedup is 4. In an ideal scenario, a program running on <em>n</em> cores with <em>n</em> total threads has a speedup of <em>n</em>.</p>&#13;
<p class="indent">If the speedup of a program is greater than 1, it indicates that the parallelization yielded some improvement. If the speedup is less than 1, then the parallel solution is in fact slower than the serial solution. It is possible for a program to have a speedup greater than <em>n</em> (for example, as a side effect of additional caches reducing accesses to memory). Such cases are referred to as <em>superlinear speedup</em>.</p>&#13;
<h5 class="h5" id="lev3_126"><span epub:type="pagebreak" id="page_710"/>Efficiency</h5>&#13;
<p class="noindent">Speedup doesn’t factor in the number of cores—it is simply the ratio of the serial time to the parallel time. For example, if a serial program takes 60 seconds, but a parallel program takes 30 seconds on four cores, it still gets a speedup of 2. However, that metric doesn’t capture the fact that it ran on four cores.</p>&#13;
<p class="indent">To measure the speedup per core, use efficiency:</p>&#13;
<div class="image1"><img alt="image" src="../images/equ0710-01.jpg"/></div>&#13;
<p class="indent">Efficiency typically varies from 0 to 1. An efficiency of 1 indicates that the cores are being used perfectly. If efficiency is close to 0, then there is little to no benefit to parallelism, as the additional cores do not improve performance. If efficiency is greater than 1, it indicates superlinear speedup.</p>&#13;
<p class="indent">Let’s revisit the previous example in which a serial program takes 60 seconds. If the parallel version takes 30 seconds on two cores, then its efficiency is 1 (or 100%). If instead the program takes 30 seconds on four cores, then the efficiency drops to 0.5 (or 50%).</p>&#13;
<h5 class="h5" id="lev3_127">Parallel Performance in the Real World</h5>&#13;
<p class="noindent">In an ideal world, speedup is linear. For each additional compute unit, a parallel program should achieve a commensurate amount of speedup. However, this scenario rarely occurs in the real world. Most programs contain a necessarily serial component that exists due to inherent dependencies in the code. The longest set of dependencies in a program is referred to as its <em>critical path</em>. Reducing the length of a program’s critical path is an important first step in its parallelization. Thread synchronization points and (for programs running on multiple compute nodes) communication overhead between processes are other components in the code that can limit a program’s parallel performance.</p>&#13;
<p class="note"><strong><span class="black">Warning</span> NOT ALL PROGRAMS ARE GOOD CANDIDATES FOR PARALLELISM!</strong></p>&#13;
<p class="note-w">The length of the critical path can make some programs downright <em>hard</em> to parallelize. As an example, consider the problem of generating the <em>n</em>th Fibonacci number. Since every Fibonacci number is dependent on the two before it, parallelizing this program efficiently is very difficult!</p>&#13;
<p class="indent">Consider the parallelization of the <code>countElems</code> function of the CountSort algorithm from earlier in this chapter. In an ideal world, we would expect the speedup of the program to be linear with respect to the number of cores. However, let’s measure its runtime (in this case, running on a quad-core system with eight logical threads):</p>&#13;
<pre><span epub:type="pagebreak" id="page_711"/>&#13;
$ <span class="codestrong1">./countElems_p_v3 100000000 0 1</span><br/>&#13;
Time for Step 1 is 0.331831 s<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p_v3 100000000 0 2</span><br/>&#13;
Time for Step 1 is 0.197245 s<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p_v3 100000000 0 4</span><br/>&#13;
Time for Step 1 is 0.140642 s<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p_v3 100000000 0 8</span><br/>&#13;
Time for Step 1 is 0.107649 s<br/>&#13;
</pre>&#13;
<p class="indent"><a href="ch14.xhtml#ch14tab3">Table 14-3</a> shows the speedup and efficiency for these multithreaded executions.</p>&#13;
<p class="tabcap" id="ch14tab3"><strong>Table 14-3:</strong> Performance Benchmarks</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:40%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
</colgroup>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Number of threads</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">4</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">8</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Speedup</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1.68</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">2.36</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">3.08</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Efficiency</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.84</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.59</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.39</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">We have 84% efficiency with two cores, but the core efficiency falls to 39% with eight cores. Notice that the ideal speedup of eight was not met. One reason for this is that the overhead of assigning work to threads and the serial update to the <code>counts</code> array starts dominating performance at higher numbers of threads. Second, resource contention by the eight threads (remember this is a quad-core processor) reduces core efficiency.</p>&#13;
<h5 class="h5" id="lev3_128">Amdahl’s Law</h5>&#13;
<p class="noindent">In 1967, Gene Amdahl, a leading computer architect at IBM, predicted that the maximum speedup that a computer program can achieve is limited by the size of its necessarily serial component (now referred to as Amdahl’s Law). More generally, Amdahl’s Law states that for every program, there is a component that can be sped up (i.e., the fraction of a program that can be optimized or parallelized, <em>P</em>), and a component that <em>cannot</em> be sped up (i.e., the fraction of a program that is inherently serial, or <em>S</em>). Even if the time needed to execute the optimizable or parallelizable component <em>P</em> is reduced to zero, the serial component <em>S</em> will exist, and will come to eventually dominate performance. Since <em>S</em> and <em>P</em> are fractions, note that <em>S</em> + <em>P</em> = 1.</p>&#13;
<p class="indent">Consider a program that executes on one core in time <em>T</em><sub>1</sub>. Then, the fraction of the program execution that is necessarily serial takes <em>S</em> × <em>T</em><sub>1</sub> time to run, and the parallelizable fraction of program execution (<em>P</em> = 1 <em>– S</em>) takes <em>P</em> × <em>T</em><sub>1</sub> to run.</p>&#13;
<p class="indent">When the program executes on <em>c</em> cores, the serial fraction of the code still takes <em>S</em> × <em>T</em><sub>1</sub> time to run (all other conditions being equal), but the parallelizable fraction can be divided into <em>c</em> cores. Thus, the maximum improvement for the parallel processor with <em>c</em> cores to run the same job is:</p>&#13;
<div class="image1"><img alt="image" src="../images/equ0711-01.jpg"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_712"/>As <em>c</em> increases, the execution time on the parallel processor becomes dominated by the serial fraction of the program.</p>&#13;
<p class="indent">To understand the impact of Amdahl’s law, consider a program that is 90% parallelizable and executes in 10 seconds on 1 core. In our equation, the parallelizable component (<em>P</em>) is 0.9, while the serial component (<em>S</em>) is 0.1. <a href="ch14.xhtml#ch14tab4">Table 14-4</a> depicts the corresponding total time on <em>c</em> cores (<em>T</em><sub><em>c</em></sub>) according to Amdahl’s Law, and the associated speedup.</p>&#13;
<p class="tabcap" id="ch14tab4"><strong>Table 14-4:</strong> The Effect of Amdahl’s Law on a 10-Second Program that is 90% Parallelizable</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab"><strong>Number of cores</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab-c"><strong>Serial time (s)</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Parallel time (s)</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Total time (</strong><em>T</em><sub><em>c</em></sub> <strong>s)</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Speedup (over one core)</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">  1.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.26</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">100</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.09</p></td>&#13;
<td style="vertical-align: top"><p class="tab">  1.09</p></td>&#13;
<td style="vertical-align: top"><p class="tab">9.17</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1000</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.009</p></td>&#13;
<td style="vertical-align: top"><p class="tab">  1.009</p></td>&#13;
<td style="vertical-align: top"><p class="tab">9.91</p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">Observe that, over time, the serial component of the program begins to dominate, and the effect of adding more and more cores seems to have little to no effect.</p>&#13;
<p class="indent">A more formal way to look at this requires incorporating Amdahl’s calculation for <em>T</em><sub>c</sub> into the equation for speedup:</p>&#13;
<div class="image1"><img alt="image" src="../images/equ0712-01.jpg"/></div>&#13;
<p class="indent">Taking the limit of this equation shows that as the number of cores (<em>c</em>) approaches infinity, speedup approaches 1/<em>S</em>. In the example shown in <a href="ch14.xhtml#ch14tab4">Table 14-4</a>, speedup approaches 1/0.1, or 10.</p>&#13;
<p class="indent">As another example, consider a program where <em>P</em> = 0.99. In other words, 99% of the program is parallelizable. As <em>c</em> approaches infinity, the serial time starts to dominate the performance (in this example, <em>S</em> = 0.01). Thus, speedup approaches 1/0.01 or 100. In other words, even with a million cores, the maximum speedup achievable by this program is only 100.</p>&#13;
<div class="g-box">&#13;
<p class="box-title">ALL IS NOT LOST: THE LIMITS OF AMDAHL’S LAW</p>&#13;
<p class="noindentt">When learning about Amdahl’s Law, it’s important to consider the <em>intentions</em> of its originator, Gene Amdahl. In his own words, the law was proposed to demonstrate “the continued validity of the single processor approach, and the weakness of the multiple processor approach in terms of application to real problems and their attendant irregularities.”<sup><a href="ch14.xhtml#fn14_8" id="rfn14_8">8</a></sup> In his 1967 paper Amdahl expanded on this concept, writing: “For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits, <span epub:type="pagebreak" id="page_713"/>and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution.” Subsequent work challenged some of the key assumptions made by Amdahl. Read about the Gustafson–Barsis Law in the next subsection for a discussion on the limits of Amdahl’s Law and a different argument on how to think about the benefits of parallelism.</p>&#13;
</div>&#13;
<h4 class="h4" id="lev2_245">14.4.2 Advanced Topics</h4>&#13;
<h5 class="h5" id="lev3_129">Gustafson–Barsis Law</h5>&#13;
<p class="noindent">In 1988, John L. Gustafson, a computer scientist and researcher at Sandia National Labs, wrote a paper called “Reevaluating Amdahl’s Law.”<sup><a href="ch14.xhtml#fn14_9" id="rfn14_9">9</a></sup> In this paper, Gustafson calls to light a critical assumption that was made about the execution of a parallel program that is not always true.</p>&#13;
<p class="indent">Specifically, Amdahl’s law implies that the number of compute cores <em>c</em> and the fraction of a program that is parallelizable <em>P</em> are independent of each other. Gustafson notes that this “is virtually never the case.” While benchmarking a program’s performance by varying the number of cores on a fixed set of data is a useful academic exercise, in the real world, more cores (or processors, as examined in our discussion of distributed memory) are added as the problem grows large. “It may be most realistic,” Gustafson writes, “to assume run time, not problem size, is constant.”</p>&#13;
<p class="indent">Therefore, according to Gustafson, it is most accurate to say that “The amount of work that can be done in parallel varies linearly with the number of processors.”</p>&#13;
<p class="indent">Consider a <em>parallel</em> program that takes time <em>T</em><sub>c</sub> to run on a system with <em>c</em> cores. Let <em>S</em> represent the fraction of the program execution that is necessarily serial and takes <em>S</em> × <em>T</em><sub>c</sub> time to run. Thus, the parallelizable fraction of the program execution, <em>P</em> = 1 <em>– S</em>, takes <em>P</em> × <em>T</em><sub>c</sub> time to run on <em>c</em> cores.</p>&#13;
<p class="indent">When the same program is run on just one core, the serial fraction of the code still takes <em>S</em> × <em>T</em><sub>c</sub> (assuming all other conditions are equal). However, the parallelizable fraction (which was divided between <em>c</em> cores) now has to be executed by just one core to run serially and takes <em>P</em> × <em>T</em><sub>c</sub> × <em>c</em> time. In other words, the parallel component will take <em>c</em> times as long on a single-core system. It follows that the scaled speedup would be:</p>&#13;
<div class="image1"><img alt="image" src="../images/equ0713-01.jpg"/></div>&#13;
<p class="indent">This shows that the scaled speedup increases linearly with the number of compute units.</p>&#13;
<p class="indent">Consider our prior example in which 99% of a program is parallelizable (i.e., <em>P</em> = 0.99). Applying the scaled speedup equation, the theoretical speedup on 100 processors would be 99.01. On 1,000 processors, it would be 990.01. Notice that the efficiency stays constant at <em>P</em>.</p>&#13;
<p class="indent">As Gustafson concludes, “speedup should be measured by scaling the problem to the number of processors, not by fixing a problem size.” <span epub:type="pagebreak" id="page_714"/>Gustafson’s result is notable because it shows that it is possible to get increasing speedup by updating the number of processors. As a researcher working in a national supercomputing facility, Gustafson was more interested in doing <em>more work</em> in a constant amount of time. In several scientific fields, the ability to analyze more data usually leads to higher accuracy or fidelity of results. Gustafson’s work showed that it was possible to get large speedups on large numbers of processors, and revived interest in parallel processing.<sup><a href="ch14.xhtml#fn14_10" id="rfn14_10">10</a></sup></p>&#13;
<h5 class="h5" id="lev3_130">Scalability</h5>&#13;
<p class="noindent">We describe a program as <em>scalable</em> if we see improving (or constant) performance as we increase the number of resources (cores, processors) or the problem size. Two related concepts are <em>strong scaling</em> and <em>weak scaling</em>. It is important to note that “weak” and “strong” in this context do not indicate the <em>quality</em> of a program’s scalability, but are simply different ways to measure scalability.</p>&#13;
<p class="indent">We say that a program is <em>strongly scalable</em> if increasing the number of cores/processing units on a <em>fixed</em> problem size yields an improvement in performance. A program displays strong linear scalability if, when run on <em>n</em> cores, the speedup is also <em>n</em>. Of course, Amdahl’s Law guarantees that after some point, adding additional cores makes little sense.</p>&#13;
<p class="indent">We say that a program is <em>weakly scalable</em> if increasing the size of the data at the same rate as the number of cores (i.e., if there is a fixed data size per core/processor) results in constant or an improvement in performance. We say a program displays weak linear scalability if we see an improvement of <em>n</em> if the work per core is scaled up by a factor of <em>n</em>.</p>&#13;
<h5 class="h5" id="lev3_131">General Advice Regarding Measuring Performance</h5>&#13;
<p class="noindent">We conclude our discussion on performance with some notes about benchmarking and performance on hyperthreaded cores.</p>&#13;
<p class="noindenta"><strong>Run a program multiple times when benchmarking.</strong>   In many of the examples shown thus far in this book, we run a program only once to get a sense of its runtime. However, this is not sufficient for formal benchmarks. Running a program once is <em>never</em> an accurate measure of a program’s true runtime! Context switches and other running processes can temporarily cause the runtime to radically fluctuate. Therefore, it is always best to run a program several times and report an average runtime together with as many details as feasible, including number of runs, observed variability of the measurements (e.g., error bars, minimum, maximum, median, standard deviation) and conditions under which the measurements were taken.</p>&#13;
<p class="noindenta"><strong>Be careful where you measure timing.</strong>   The <code>gettimeofday</code> function is useful in helping to accurately measure the time a program takes to run. However, it can also be abused. Even though it may be tempting to place the <code>gettimeofday</code> call around only the thread creation and joining component in <code>main</code>, it is important to consider what exactly you are trying to time. For <span epub:type="pagebreak" id="page_715"/>example, if a program reads in an external data file as a necessary part of its execution, the time for file reading should likely be included in the program’s timing.</p>&#13;
<p class="noindenta"><strong>Be aware of the impact of hyperthreaded cores.</strong>   As discussed in “Taking a Closer Look: How Many Cores?” on <a href="ch14.xhtml#lev3_112">page 671</a> and “Multicore and Hardware Multithreading” on <a href="ch05.xhtml#lev2_108">page 283</a>, hyperthreaded (logical) cores are capable of executing multiple threads on a single core. In a quad-core system with two logical threads per core, we say there are eight hyperthreaded cores on the system. Running a program in parallel on eight logical cores in many cases yields better wall time than running a program on four cores. However, due to the resource contention that usually occurs with hyperthreaded cores, you may see a dip in core efficiency and nonlinear speedup.</p>&#13;
<p class="noindenta"><strong>Beware of resource contention.</strong>   When benchmarking, it’s always important to consider what <em>other</em> processes and threaded applications are running on the system. If your performance results ever look a bit strange, it is worth quickly running <code>top</code> to see whether there are any other users also running resource-intensive tasks on the same system. If so, try using a different system to benchmark (or wait until the system is not so heavily used).</p>&#13;
<h3 class="h3" id="lev1_109">14.5 Cache Coherence and False Sharing</h3>&#13;
<p class="noindent">Multicore caches can have profound implications on a multithreaded program’s performance. First, however, let’s quickly review some of the basic concepts related to cache design (see “CPU Caches” on page 1299 for more details):</p>&#13;
<ul>&#13;
<li class="noindent">Data/instructions are not transported <em>individually</em> to the cache. Instead, data is transferred in <em>blocks</em>, and block sizes tend to get larger at lower levels of the memory hierarchy.</li>&#13;
<li class="noindent">Each cache is organized into a series of sets, with each set having a number of lines. Each line holds a single block of data.</li>&#13;
<li class="noindent">The individual bits of a memory address are used to determine the set, tag, and block offset of the cache to which to write a block of data.</li>&#13;
<li class="noindent">A <em>cache hit</em> occurs when the desired data block exists in the cache. Otherwise, a <em>cache miss</em> occurs, and a lookup is performed on the next lower level of the memory hierarchy (which can be cache or main memory).</li>&#13;
<li class="noindent">The <em>valid bit</em> indicates if a block at a particular line in the cache is safe to use. If the valid bit is set to 0, the data block at that line cannot be used (e.g., the block could contain data from an exited process).</li>&#13;
<li class="noindent">Information is written to cache/memory based on two main strategies. In the <em>write-through</em> strategy, the data is written to cache and <span epub:type="pagebreak" id="page_716"/>main memory simultaneously. In the <em>write-back</em> strategy, data is written only to cache and gets written to lower levels in the hierarchy after the block is evicted from the cache.</li>&#13;
</ul>&#13;
<h4 class="h4" id="lev2_246">14.5.1 Caches on Multicore Systems</h4>&#13;
<p class="noindent">Recall that in shared memory architectures each core can have its own cache (see “Looking Ahead: Caching on Multicore Processors” on <a href="ch11.xhtml#lev1_91">page 581</a>) and that multiple cores can share a common cache. <a href="ch14.xhtml#ch14fig8">Figure 14-8</a> depicts an example dual-core CPU. Even though each core has its own local L1 cache, the cores share a common L2 cache.</p>&#13;
<div class="imagec" id="ch14fig8"><img alt="image" src="../images/14fig08.jpg"/></div>&#13;
<p class="figcap"><em>Figure 14-8: An example dual-core CPU with separate L1 caches and a shared L2 cache</em></p>&#13;
<p class="indent">Multiple threads in a single executable may execute separate functions. Without a <em>cache coherency</em> strategy (see “Cache Coherency” on <a href="ch11.xhtml#lev2_198">page 583</a>) to ensure that each cache maintains a consistent view of shared memory, it is possible for shared variables to be updated inconsistently. As an example, consider the dual-core processor in <a href="ch14.xhtml#ch14fig8">Figure 14-8</a>, where each core is busy executing separate threads concurrently. The thread assigned to Core 0 has a local variable <code>x</code>, whereas the thread executing on Core 1 has a local variable <code>y</code>, and both threads have shared access to a global variable <code>g</code>. <a href="ch14.xhtml#ch14tab5">Table 14-5</a> shows one possible path of execution.</p>&#13;
<p class="tabcap" id="ch14tab5"><strong>Table 14-5:</strong> Problematic Data Sharing Due to Caching</p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:30%"/>&#13;
<col style="width:35%"/>&#13;
<col style="width:35%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab"><strong>Time</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Core 0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Core 1</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><code>g = 5</code></p></td>&#13;
<td style="vertical-align: top"><p class="tab">(other work)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">(other work)</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><code>y = g*4</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><code>x += g</code></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><code>y += g*2</code></p></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent">Suppose that the initial value of <code>g</code> is 10, and the initial values of <code>x</code> and <code>y</code> are both 0. What is the final value of <code>y</code> at the end of this sequence of operations? Without cache coherence, this is a very difficult question to answer <span epub:type="pagebreak" id="page_717"/>given that there are at least three stored values of <code>g</code>: one in Core 0’s L1 cache, one in Core 1’s L1 cache, and a separate copy of <code>g</code> stored in the shared L2 cache.</p>&#13;
<p class="indent"><a href="ch14.xhtml#ch14fig9">Figure 14-9</a> shows one possible erroneous result after the sequence of operations in <a href="ch14.xhtml#ch14tab5">Table 14-5</a> completes. Suppose that the L1 caches implement a write-back policy. When the thread executing on Core 0 writes the value 5 to <code>g</code>, it updates only the value of <code>g</code> in Core 0’s L1 cache. The value of <code>g</code> in Core 1’s L1 cache still remains 10, as does the copy in the shared L2 cache. Even if a write-through policy is implemented, there is no guarantee that the copy of <code>g</code> stored in Core 1’s L1 cache gets updated! In this case, <code>y</code> will have the final value of <code>60</code>.</p>&#13;
<div class="imagec" id="ch14fig9"><img alt="image" src="../images/14fig09.jpg"/></div>&#13;
<p class="figcap"><em>Figure 14-9: A problematic update to caches that do not employ cache coherency</em></p>&#13;
<p class="indent">A cache coherence strategy invalidates or updates cached copies of shared values in other caches when a write to the shared data value is made in one cache. The <em>modified shared invalid</em> (MSI) protocol (discussed in detail in “The MSI Protocol” on <a href="ch11.xhtml#lev2_199">page 584</a>) is one example of an invalidating cache coherency protocol.</p>&#13;
<p class="indent">A common technnique for implementing MSI is snooping. Such a <em>snoopy cache</em> “snoops” on the memory bus for possible write signals. If the snoopy cache detects a write to a shared cache block, it invalidates its line containing that cache block. The end result is that the only valid version of the block is in the cache that is written to, whereas <em>all other</em> copies of the block in other caches are marked as invalid.</p>&#13;
<p class="indent">Employing the MSI protocol with snoooping would yield the correct final assignment of <code>30</code> to variable <code>y</code> in the previous example.</p>&#13;
<h4 class="h4" id="lev2_247">14.5.2 False Sharing</h4>&#13;
<p class="noindent">Cache coherence guarantees correctness, but it can potentially harm performance. Recall that when the thread updates <code>g</code> on Core 0, the snoopy cache invalidates not only <code>g</code>, but the <em>entire cache line</em> that <code>g</code> is a part of.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_718"/>Consider our initial attempt at parallelizing the <code>countElems</code> function of the CountSort algorithm.<sup>4</sup> For convenience, the function is reproduced here:</p>&#13;
<pre>&#13;
/*parallel version of step 1 (first cut) of CountSort algorithm:<br/>&#13;
 * extracts arguments from args value<br/>&#13;
 * calculates portion of the array this thread is responsible for counting<br/>&#13;
 * computes the frequency of all the elements in assigned component and stores<br/>&#13;
 * the associated counts of each element in counts array<br/>&#13;
*/<br/>&#13;
void *countElems(void *args){<br/>&#13;
    //extract arguments<br/>&#13;
    //ommitted for brevity<br/>&#13;
    int *array = myargs-&gt;ap;<br/>&#13;
    long *counts = myargs-&gt;countp;<br/>&#13;
<br/>&#13;
    //assign work to the thread<br/>&#13;
    //compute chunk, start, and end<br/>&#13;
    //ommited for brevity<br/>&#13;
<br/>&#13;
    long i;<br/>&#13;
    //heart of the program<br/>&#13;
    for (i = start; i &lt; end; i++){<br/>&#13;
        val = array[i];<br/>&#13;
        counts[val] = counts[val] + 1;<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    return NULL;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">In our previous discussion of this function (see “Data Races” on <a href="ch14.xhtml#lev3_116">page 691</a>), we pointed out how data races can cause the <code>counts</code> array to not populate with the correct set of counts. Let’s see what happens if we attempt to <em>time</em> this function. We add timing code to <code>main</code> using <code>getimeofday</code> as before.<sup>6</sup> Benchmarking the initial version of <code>countElems</code> as just shown on 100 million elements yields the following times:</p>&#13;
<pre>&#13;
$ <span class="codestrong1">./countElems_p 100000000 0 1</span><br/>&#13;
Time for Step 1 is 0.336239 s<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p 100000000 0 2</span><br/>&#13;
Time for Step 1 is 0.799464 s<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_p 100000000 0 4</span><br/>&#13;
Time for Step 1 is 0.767003 s<br/>&#13;
</pre>&#13;
<p class="indent">Even without any synchronization constructs, this version of the program <em>still gets slower</em> as the number of threads increases!</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_719"/>To understand what is going on, let’s revisit the <code>counts</code> array. This holds the frequency of occurrence of each number in our input array. The maximum value is determined by the variable <code>MAX</code>. In our example program, <code>MAX</code> is set to 10. In other words, the <code>counts</code> array takes up 40 bytes of space.</p>&#13;
<p class="indent">Recall that the cache details on a Linux system (see “Looking Ahead: Caching on Multicore Processors” on <a href="ch11.xhtml#lev1_91">page 581</a>) are located in the <code>/sys/devices/system/cpu/</code> directory. Each logical core has its own <code>cpu</code> subdirectory called <code>cpuk</code>, where <code>k</code> indicates the <em>k</em>th logical core. Each <code>cpu</code> subdirectory in turn has separate <code>index</code> directories that indicate the caches available to that core.</p>&#13;
<p class="indent">The <code>index</code> directories contain files with numerous details about each logical core’s caches. The contents of a sample <code>index0</code> directory are shown here (<code>index0</code> typically corresponds to a Linux system’s L1 cache):</p>&#13;
<pre>&#13;
$ <span class="codestrong1">ls /sys/devices/system/cpu/cpu0/cache/index0</span><br/>&#13;
coherency_line_size      power            type<br/>&#13;
level                    shared_cpu_list  uevent<br/>&#13;
number_of_sets           shared_cpu_map   ways_of_associativity<br/>&#13;
physical_line_partition  size<br/>&#13;
</pre>&#13;
<p class="indent">To discover the cache line size of the L1 cache, use this command:</p>&#13;
<pre>$ <span class="codestrong1">cat /sys/devices/system/cpu/cpu0/cache/coherency_line_size</span><br/>&#13;
64</pre>&#13;
<p class="indent">The output reveals that the L1 cache line size for the machine is 64 bytes. In other words, the 40-byte <code>counts</code> array fits <em>within one cache line</em>.</p>&#13;
<p class="indent">Recall that with invalidating cache coherence protocols like MSI, every time a program updates a shared variable, the <em>entire cache line in other caches storing the variable is invalidated</em>. Let’s consider what happens when two threads execute the preceding function. One possible path of execution is shown in <a href="ch14.xhtml#ch14tab6">Table 14-6</a> (assuming that each thread is assigned to a separate core, and the variable <code>x</code> is local to each thread).</p>&#13;
<p class="tabcap" id="ch14tab6"><strong>Table 14-6:</strong> A Possible Execution Sequence of Two Threads Running <code>countElems</code></p>&#13;
<table class="line">&#13;
<colgroup>&#13;
<col style="width:10%"/>&#13;
<col style="width:45%"/>&#13;
<col style="width:45%"/>&#13;
</colgroup>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab"><strong>Time</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Thread 0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Thread 1</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>i</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Reads <code>array[x]</code></p></td>&#13;
<td style="vertical-align: top"><p class="tab">…</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"/></td>&#13;
<td style="vertical-align: top"><p class="tab">(1)</p></td>&#13;
<td style="vertical-align: top"><p class="tab"/></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>i</em> + 1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Increments <code>counts[1]</code> (<strong>invalidates</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Reads <code>array[x]</code> (4)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"/></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>cache line</strong>)</p></td>&#13;
<td style="vertical-align: top"><p class="tab"/></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>i</em> + 2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Reads <code>array[x]</code> (6)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Increments <code>counts[4]</code> (<strong>invalidates</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"/></td>&#13;
<td style="vertical-align: top"><p class="tab"/></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>cache line</strong>)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>i</em> + 3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Increments <code>counts[6]</code> (<strong>invalidates</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Reads <code>array[x]</code> (2)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"/></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>cache line</strong>)</p></td>&#13;
<td style="vertical-align: top"><p class="tab"/></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>i</em> + 4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Reads <code>array[x]</code> (3)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Increments <code>counts[2]</code> (<strong>invalidates</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"/></td>&#13;
<td style="vertical-align: top"><p class="tab"/></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>cache line</strong>)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>i</em> + 5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Increments <code>counts[3]</code> (<strong>invalidates</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">…</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"/></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>cache line</strong>)</p></td>&#13;
<td style="vertical-align: top"><p class="tab"/></td>&#13;
</tr>&#13;
</table>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_720"/>During time step <em>i</em>, Thread 0 reads the value at <code>array[x]</code> in its part of the array, which is a 1 in this example. During time steps <em>i</em> + 1 to <em>i</em> + 5, each thread reads a value from <code>array[x]</code>. Note that each thread is looking at different components of the array. Not only that, each read of <code>array</code> in our sample execution yields unique values (so no race conditions in this sample execution sequence!). After reading the value from <code>array[x]</code>, each thread increments the associated value in <code>counts</code>.</p>&#13;
<p class="indent">Recall that the <code>counts</code> array <em>fits on a single cache line</em> in our L1 cache. As a result, every write to <code>counts</code> invalidates the <em>entire line</em> in <em>every other L1 cache</em>. The end result is that, despite updating <em>different</em> memory locations in <code>counts</code>, any cache line containing <code>counts</code> is <em>invalidated</em> with <em>every update</em> to <code>counts</code>!</p>&#13;
<p class="indent">The invalidation forces all L1 caches to update the line with a “valid” version from L2. The repeated invalidation and overwriting of lines from the L1 cache is an example of <em>thrashing</em>, where repeated conflicts in the cache cause a series of misses.</p>&#13;
<p class="indent">The addition of more cores makes the problem worse, given that now more L1 caches are invalidating the line. As a result, adding additional threads slows down the runtime, despite the fact that each thread is accessing different elements of the <code>counts</code> array! This is an example of <em>false sharing</em>, or the illusion that individual elements are being shared by multiple cores. In the previous example, it appears that all the cores are accessing the same elements of <code>counts</code>, even though this is not the case.</p>&#13;
<h4 class="h4" id="lev2_248">14.5.3 Fixing False Sharing</h4>&#13;
<p class="noindent">One way to fix an instance of false sharing is to pad the array (in our case <code>counts</code>) with additional elements so that it doesn’t fit in a single cache line. However, padding can waste memory, and may not eliminate the problem from all architectures (consider the scenario in which two different machines have different L1 cache sizes). In most cases, writing code to support different cache sizes is generally not worth the gain in performance.</p>&#13;
<p class="indent">A better solution is to have threads write to <em>local storage</em> whenever possible. Local storage in this context refers to memory that is <em>local</em> to a thread. The following solution reduces false sharing by choosing to perform updates to a locally declared version of <code>counts</code> called <code>local_counts</code>.</p>&#13;
<p class="indent">Let’s revisit the final version of our <code>countElems</code> function:<sup>6</sup></p>&#13;
<pre>&#13;
/*parallel version of CountSort algorithm step 1 (final attempt with mutexes):<br/>&#13;
 * extracts arguments from args value<br/>&#13;
 * calculates the portion of the array this thread is responsible for counting<br/>&#13;
 * computes the frequency of all the elements in assigned component and stores<br/>&#13;
 * the associated counts of each element in counts array<br/>&#13;
*/<br/>&#13;
void *countElems( void *args ){<br/>&#13;
    //extract arguments<br/>&#13;
    //omitted for brevity<br/>&#13;
    int *array = myargs-&gt;ap;<br/>&#13;
    long *counts = myargs-&gt;countp;<br/>&#13;
<br/>&#13;
    <span epub:type="pagebreak" id="page_721"/>long local_counts[MAX] = {0}; //local declaration of counts array<br/>&#13;
<br/>&#13;
    //assign work to the thread<br/>&#13;
    //compute chunk, start, and end values (omitted for brevity)<br/>&#13;
<br/>&#13;
    long i;<br/>&#13;
<br/>&#13;
    //heart of the program<br/>&#13;
    for (i = start; i &lt; end; i++){<br/>&#13;
        val = array[i];<br/>&#13;
        local_counts[val] = local_counts[val] + 1; //update local counts array<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    //update to global counts array<br/>&#13;
    pthread_mutex_lock(&amp;mutex); //acquire the mutex lock<br/>&#13;
    for (i = 0; i &lt; MAX; i++){<br/>&#13;
        counts[i] += local_counts[i];<br/>&#13;
    }<br/>&#13;
    pthread_mutex_unlock(&amp;mutex); //release the mutex lock<br/>&#13;
<br/>&#13;
    return NULL;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">The use of <code>local_counts</code> to accumulate frequencies in lieu of <code>counts</code> is the major source of reduction of false sharing in this example:</p>&#13;
<pre>&#13;
for (i = start; i &lt; end; i++){<br/>&#13;
    val = array[i];<br/>&#13;
    local_counts[val] = local_counts[val] + 1; //updates local counts array<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">Since cache coherence is meant to maintain a consistent view of shared memory, the invalidations trigger only on <em>writes</em> to <em>shared values</em> in memory. Since <code>local_counts</code> is not shared among the different threads, a write to it will not invalidate its associated cache line.</p>&#13;
<p class="indent">In the last component of the code, the mutex enforces correctness by ensuring that only one thread updates the shared <code>counts</code> array at a time:</p>&#13;
<pre>&#13;
//update to global counts array<br/>&#13;
pthread_mutex_lock(&amp;mutex); //acquire the mutex lock<br/>&#13;
for (i = 0; i &lt; MAX; i++){<br/>&#13;
    counts[i] += local_counts[i];<br/>&#13;
}<br/>&#13;
pthread_mutex_unlock(&amp;mutex); //release the mutex lock<br/>&#13;
</pre>&#13;
<p class="indent">Since <code>counts</code> is located on a single cache line, it will still get invalidated with every write. The difference is that the penalty here is at most <code>MAX</code> × <em>t</em> <span epub:type="pagebreak" id="page_722"/>writes vs. <em>n</em> writes, where <em>n</em> is the length of our input array, and <em>t</em> is the number of threads employed.</p>&#13;
<h3 class="h3" id="lev1_110">14.6 Thread Safety</h3>&#13;
<p class="noindent">So far, we have covered synchronization constructs that programmers can use to ensure that their multithreaded programs are consistent and correct regardless of the number of threads employed. However, it is not always safe to make the assumption that standard C library functions can be used “as is” in the context of any multithreaded application. Not all functions in the C library are <em>thread safe</em>, or capable of being run by multiple threads while guaranteeing a correct result without unintended side effects. To ensure that the programs <em>we</em> write are thread safe, it is important to use synchronization primitives like mutexes and barriers to enforce that multithreaded programs are consistent and correct regardless of how the number of threads varies.</p>&#13;
<p class="indent">Another closely related concept related to thread safety is re-entrancy. All thread safe code is re-entrant; however, not all re-entrant code is thread safe. A function is <em>re-entrant</em> if it can be re-executed/partially executed by a function without causing issue. By definition, re-entrant code ensures that accesses to the global state of a program always result in that global state remaining consistent. While re-entrancy is often (incorrectly) used as a synonym for thread safety, there are special cases for which re-entrant code is not thread safe.</p>&#13;
<p class="indent">When writing multithreaded code, verify that the C library functions used are indeed thread safe. Fortunately, the list of thread-unsafe C library functions is fairly small. The Open Group kindly maintains a list of thread unsafe functions.<sup><a href="ch14.xhtml#fn14_11" id="rfn14_11">11</a></sup></p>&#13;
<h4 class="h4" id="lev2_249">14.6.1 Fixing Issues of Thread Safety</h4>&#13;
<p class="noindent">Synchronization primitives are the most common way to fix issues related to thread safety. However, unknowingly using thread-unsafe C library functions can cause subtle issues. Let’s look at a slightly modified version of our <code>countsElem</code> function called <code>countElemsStr</code>, which attempts to count the frequency of digits in a given string, where each digit is separated by spaces. The following program has been edited for brevity; the full source of this program is available online.<sup><a href="ch14.xhtml#fn14_12" id="rfn14_12">12</a></sup></p>&#13;
<pre>&#13;
/* computes the frequency of all the elements in the input string and stores<br/>&#13;
 * the associated counts of each element in the array called counts. */<br/>&#13;
void countElemsStr(int *counts, char *input_str) {<br/>&#13;
    int val, i;<br/>&#13;
    char *token;<br/>&#13;
    token = strtok(input_str, " ");<br/>&#13;
    while (token != NULL) {<br/>&#13;
        val = atoi(token);<br/>&#13;
        counts[val] = counts[val] + 1;<br/>&#13;
        token = strtok(NULL, " ");<br/>&#13;
<span epub:type="pagebreak" id="page_723"/>    }<br/>&#13;
}<br/>&#13;
<br/>/* main function:<br/>&#13;
 * calls countElemsStr on a static string and counts up all the digits in<br/>&#13;
 * that string. */<br/>&#13;
int main( int argc, char **argv ) {<br/>&#13;
    //lines omitted for brevity, but gets user defined length of string<br/>&#13;
<br/>&#13;
    //fill string with n digits<br/>&#13;
    char *inputString = calloc(length * 2, sizeof(char));<br/>&#13;
    fillString(inputString, length * 2);<br/>&#13;
<br/>&#13;
    countElemsStr(counts, inputString);<br/>&#13;
<br/>&#13;
    return 0;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">The <code>countElemsStr</code> function uses the <code>strtok</code> function (as examined in our discussion in “strtok, strtok_r” on <a href="ch02.xhtml#lev3_22">page 100</a>) to parse each digit (stored in <code>token</code>) in the string, before converting it to an integer and making the associated updates in the <code>counts</code> array.</p>&#13;
<p class="indent">Compiling and running this program on 100,000 elements yields the following output:</p>&#13;
<pre>&#13;
$ <span class="codestrong1">gcc -o countElemsStr countElemsStr.c</span><br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElemsStr 100000 1</span><br/>&#13;
contents of counts array:<br/>&#13;
9963 9975 9953 10121 10058 10017 10053 9905 9915 10040<br/>&#13;
</pre>&#13;
<p class="indent">Now, let’s take a look at a multithreaded version of <code>countElemsStr</code>:<sup><a href="ch14.xhtml#fn14_13" id="rfn14_13">13</a></sup></p>&#13;
<pre>&#13;
/* parallel version of countElemsStr (First cut):<br/>&#13;
 * computes the frequency of all the elements in the input string and stores<br/>&#13;
 * the associated counts of each element in the array called counts<br/>&#13;
*/<br/>&#13;
void *countElemsStr(void *args) {<br/>&#13;
    //parse args<br/>&#13;
    struct t_arg *myargs = (struct t_arg *)args;<br/>&#13;
    //omitted for brevity<br/>&#13;
<br/>&#13;
    //local variables<br/>&#13;
    int val, i;<br/>&#13;
    char *token;<br/>&#13;
    int local_counts[MAX] = {0};<br/>&#13;
<br/>&#13;
    //compute local start and end values and chunk size:<br/>&#13;
    //omitted for brevity<br/>&#13;
<br/>&#13;
<span epub:type="pagebreak" id="page_724"/>    //tokenize values<br/>&#13;
    token = strtok(input_str + start, " ");<br/>&#13;
    while (token != NULL) {<br/>&#13;
        val = atoi(token); //convert to an int<br/>&#13;
        local_counts[val] = local_counts[val] + 1; //update associated counts<br/>&#13;
        token = strtok(NULL, " ");<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    pthread_mutex_lock(&amp;mutex);<br/>&#13;
    for (i = 0; i &lt; MAX; i++) {<br/>&#13;
        counts[i] += local_counts[i];<br/>&#13;
    }<br/>&#13;
    pthread_mutex_unlock(&amp;mutex);<br/>&#13;
<br/>&#13;
    return NULL;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">In this version of the program, each thread processes a separate section of the string referenced by <code>input_str</code>. The <code>local_counts</code> array ensures that the bulk of the write operations occur to local storage. A mutex is employed to ensure that no two threads write to the shared variable <code>counts</code>.</p>&#13;
<p class="indent">However, compiling and running this program yields the following results:</p>&#13;
<pre>&#13;
$ <span class="codestrong1">gcc -o countElemsStr_p countElemsStr_p.c -lpthread</span><br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElemsStr_p 100000 1 1</span><br/>&#13;
contents of counts array:<br/>&#13;
9963 9975 9953 10121 10058 10017 10053 9905 9915 10040<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElemsStr_p 100000 1 2</span><br/>&#13;
contents of counts array:<br/>&#13;
498 459 456 450 456 471 446 462 450 463<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElemsStr_p 100000 1 4</span><br/>&#13;
contents of counts array:<br/>&#13;
5038 4988 4985 5042 5056 5013 5025 5035 4968 5065<br/>&#13;
</pre>&#13;
<p class="indent">Even though mutex locks are used around accesses to the <code>counts</code> array, the results from separate runs are radically different. This issue arises because the <code>countsElemsStr</code> function is not thread safe, because the string library function <code>strtok</code> is <em>not thread safe</em>! Visiting the OpenGroup website<sup>11</sup> confirms that <code>strtok</code> is on the list of thread-unsafe functions.</p>&#13;
<p class="indent">To fix this issue, it suffices to replace <code>strtok</code> with its thread-safe alternative, <code>strtok_r</code>. In the latter function, a pointer is used as the last parameter to help the thread keep track of where in the string it is parsing. Here is the fixed function with <code>strtok_r</code>:<sup><a href="ch14.xhtml#fn14_14" id="rfn14_14">14</a></sup></p>&#13;
<pre><span epub:type="pagebreak" id="page_725"/>&#13;
/* parallel version of countElemsStr (First cut):<br/>&#13;
 * computes the frequency of all the elements in the input string and stores<br/>&#13;
 * the associated counts of each element in the array called counts */<br/>&#13;
void* countElemsStr(void* args) {<br/>&#13;
    //parse arguments<br/>&#13;
    //omitted for brevity<br/>&#13;
<br/>&#13;
    //local variables<br/>&#13;
    int val, i;<br/>&#13;
    char * token;<br/>&#13;
    int local_counts[MAX] = {0};<br/>&#13;
    char * saveptr; //for saving state of strtok_r<br/>&#13;
<br/>&#13;
    //compute local start and end values and chunk size:<br/>&#13;
    //omitted for brevity<br/>&#13;
<br/>&#13;
    //tokenize values<br/>&#13;
    token = strtok_r(input_str+start, " ", &amp;saveptr);<br/>&#13;
    while (token != NULL) {<br/>&#13;
        val = atoi(token); //convert to an int<br/>&#13;
        local_counts[val] = local_counts[val]+1; //update associated counts<br/>&#13;
        token = strtok_r(NULL, " ", &amp;saveptr);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    pthread_mutex_lock(&amp;mutex);<br/>&#13;
    for (i = 0; i &lt; MAX; i++) {<br/>&#13;
        counts[i]+=local_counts[i];<br/>&#13;
    }<br/>&#13;
    pthread_mutex_unlock(&amp;mutex);<br/>&#13;
<br/>&#13;
    return NULL;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">The only change in this version of the code is the declaration of the character pointer <code>saveptr</code> and replacing all instances of <code>strtok</code> with <code>strtok_r</code>. Rerunning the code with these changes yields the following output:</p>&#13;
<pre>&#13;
$ <span class="codestrong1">gcc -o countElemsStr_p_v2 countElemsStr_p_v2.c -lpthread</span><br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElemsStr_p_v2 100000 1 1</span><br/>&#13;
contents of counts array:<br/>&#13;
9963 9975 9953 10121 10058 10017 10053 9905 9915 10040<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElemsStr_p_v2 100000 1 2</span><br/>&#13;
contents of counts array:<br/>&#13;
9963 9975 9953 10121 10058 10017 10053 9905 9915 10040<br/>&#13;
<br/>&#13;
<span epub:type="pagebreak" id="page_726"/>$ <span class="codestrong1">./countElemsStr_p_v2 100000 1 4</span><br/>&#13;
contents of counts array:<br/>&#13;
9963 9975 9953 10121 10058 10017 10053 9905 9915 10040<br/>&#13;
</pre>&#13;
<p class="indent">Now the program produces the same result for every run. The use of <code>saveptr</code> in conjunction with <code>strtok_r</code> ensures that each thread can independently track their location when parsing the string.</p>&#13;
<p class="indent">The takeaway from this section is that one should always check the list of thread-unsafe functions in C<sup>11</sup> when writing multithreaded applications. Doing so can save the programmer a lot of heartache and frustration when writing and debugging threaded applications.</p>&#13;
<h3 class="h3" id="lev1_111">14.7 Implicit Threading with OpenMP</h3>&#13;
<p class="noindent">Thus far, we have presented shared memory programming using POSIX threads. Although Pthreads are great for simple applications, they become increasingly difficult to use as programs themselves become more complex. POSIX threads are an example of <em>explicit parallel programming</em> of threads, requiring a programmer to specify exactly what each thread is required to do and when each thread should start and stop.</p>&#13;
<p class="indent">With Pthreads, it can also be challenging to <em>incrementally</em> add parallelism to an existing sequential program. That is, one must often rewrite the program entirely to use threads, which is often not desirable when attempting to parallelize a large, existing codebase.</p>&#13;
<p class="indent">The Open Multiprocessing (OpenMP) library implements an <em>implicit</em> alternative to Pthreads. OpenMP is built in to GCC and other popular compilers such as LLVM and Clang, and can be used with the C, C++, and Fortran programming languages. A key advantage of OpenMP is that it enables programmers to parallelize components of existing, sequential C code by adding <em>pragmas</em> (special compiler directives) to parts of the code. Pragmas specific to OpenMP begin with <code>#pragma omp</code>.</p>&#13;
<p class="indent">Detailed coverage of OpenMP is outside the scope of this book, but we do cover some common pragmas and show how several can be used in the context of some sample applications.</p>&#13;
<h4 class="h4" id="lev2_250">14.7.1 Common Pragmas</h4>&#13;
<p class="noindent">Here are some of the most commonly used pragmas in OpenMP programs:</p>&#13;
<div class="ul-none">&#13;
<p class="ul-noindent-left"><span class="codestrong">#pragma omp parallel</span>  This pragma creates a team of threads and has each thread run the code in its scope (usually a function call) on each thread. An invocation of this pragma is usually equivalent to an invocation of the <code>pthread_create</code> and <code>pthread_join</code> function pairing discussed in “Creating and Joining Threads” on <a href="ch14.xhtml#lev2_236">page 679</a>. The pragma may have a number of clauses, including the following:</p>&#13;
<p class="ul-noindent-left1"><span class="codestrong">num_threads</span>  Specifies the number of threads to create.</p>&#13;
<p class="ul-noindent-left1"><span class="codestrong">private</span>  A list of variables that should be private (or local) to each thread. Variables that should be private to a thread can also be declared <span epub:type="pagebreak" id="page_727"/>within the scope of the pragma (see below for an example). Each thread gets its own copy of each variable.</p>&#13;
<p class="ul-noindent-left1"><span class="codestrong">shared</span>  A listing of variables that should be shared among the threads. There is one copy of the variable that is shared among all threads.</p>&#13;
<p class="ul-noindent-left1"><span class="codestrong">default</span>  Indicates whether the determination of which variables should be shared is left up to the compiler. In most cases, we want to use <code>default(none)</code> and specify explicitly which variables should be shared and which should be private.</p>&#13;
<p class="ul-noindent-left"><span class="codestrong">#pragma omp for</span>  Specifies that each thread execute a subset of iterations of a <code>for</code> loop. Although the scheduling of the loops is up to the system, the default is usually the “chunking” method first discussed in “Revisiting Scalar Multiplication” on <a href="ch14.xhtml#lev2_239">page 682</a>. This is a <em>static</em> form of scheduling: each thread gets an assigned chunk, and then processes the iterations in its chunk. However, OpenMP also makes <em>dynamic</em> scheduling easy. In dynamic scheduling, each thread gets a number of iterations, and requests a new set upon completing processing their iteration. The scheduling policy can be set using the following clause:</p>&#13;
<p class="ul-noindent-left1"><span class="codestrong">schedule(dynamic)</span>  Specifies that a <em>dynamic</em> form of scheduling should be used. While this is advantageous in some cases, the static (default) form of scheduling is usually faster.</p>&#13;
<p class="ul-noindent-left"><span class="codestrong">#pragma omp parallel for</span>  This pragma is a combination of the <code>omp parallel</code> and the <code>omp for</code> pragmas. Unlike the <code>omp for</code> pragma, the <code>omp parallel for</code> pragma also generates a team of threads before assigning each thread a set of iterations of the loop.</p>&#13;
<p class="ul-noindent-left"><span class="codestrong">#pragma omp critical</span>  This pragma is used to specify that the code under its scope should be treated as a <em>critical section</em>—that is, only one thread should execute the section of code at a time to ensure correct behavior.</p>&#13;
</div>&#13;
<p class="noindent">There are also several <em>functions</em> that a thread can access that are often useful for execution. For example:</p>&#13;
<div class="ul-none">&#13;
<p class="ul-noindent-left"><span class="codestrong">omp_get_num_threads</span>  Returns the number of threads in the current team that is being executed.</p>&#13;
<p class="ul-noindent-left"><span class="codestrong">omp_set_num_threads</span>  Sets the number of threads that a team should have.</p>&#13;
<p class="ul-noindent-left"><span class="codestrong">omp_get_thread_num</span>  Returns the identifier of the calling thread.</p>&#13;
</div>&#13;
<p class="note"><strong><span class="black">Warning</span> THE OMP PARALLEL FOR DIRECTIVE WORKS ONLY WITH FOR LOOPS!</strong></p>&#13;
<p class="note-w">Keep in mind that the <code>omp parallel for</code> pragma works <em>only</em> with <code>for</code> loops. Other types of loops, such as <code>while</code> loops and <code>do</code>–<code>while</code> loops, are not supported.</p>&#13;
<h4 class="h4" id="lev2_251"><span epub:type="pagebreak" id="page_728"/>14.7.2 Hello Threading: OpenMP Flavored</h4>&#13;
<p class="noindent">Let’s revisit our “Hello World” program,<sup>2</sup> now using OpenMP instead of Pthreads:</p>&#13;
<pre>&#13;
#include &lt;stdio.h&gt;<br/>&#13;
#include &lt;stdlib.h&gt;<br/>&#13;
#include &lt;omp.h&gt;<br/>&#13;
<br/>&#13;
void HelloWorld( void ) {<br/>&#13;
    long myid = omp_get_thread_num();<br/>&#13;
    printf( "Hello world! I am thread %ld\n", myid );<br/>&#13;
}<br/>&#13;
<br/>&#13;
int main( int argc, char** argv ) {<br/>&#13;
    long nthreads;<br/>&#13;
<br/>&#13;
    if (argc !=2) {<br/>&#13;
        fprintf(stderr, "usage: %s &lt;n&gt;\n", argv[0]);<br/>&#13;
        fprintf(stderr, "where &lt;n&gt; is the number of threads\n");<br/>&#13;
        return 1;<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    nthreads = strtol( argv[1], NULL, 10 );<br/>&#13;
<br/>&#13;
    #pragma omp parallel num_threads(nthreads)<br/>&#13;
        HelloWorld();<br/>&#13;
<br/>&#13;
    return 0;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">Note that the OpenMP program is <em>much</em> shorter than the Pthreads version. To access the OpenMP library functions, we include the header file <code>omp.h</code>. The <code>omp parallel num_threads(nthreads)</code> pragma in <code>main</code> creates a set of threads, where each thread calls the <code>HelloWorld</code> function. The clause <code>num _threads(nthreads)</code> specifies that a total of <code>nthreads</code> should be generated. The pragma also joins each created thread back to a single-threaded process. In other words, all the low-level work of creating and joining threads is <em>abstracted</em> away from the programmer and is accomplished with the inclusion of just one pragma. For this reason, OpenMP is considered an <em>implicit threading</em> library.</p>&#13;
<p class="indent">OpenMP also abstracts away the need to explicitly manage thread IDs. In the context of <code>HelloWorld</code>, the <code>omp_get_thread_num</code> function extracts the unique ID associated with the thread that is running it.</p>&#13;
<h5 class="h5" id="lev3_132">Compiling the code</h5>&#13;
<p class="noindent">Let’s compile and run this program by passing the <code>-fopenmp</code> flag to the compiler, which signals that we’re compiling with OpenMP:</p>&#13;
<pre><span epub:type="pagebreak" id="page_729"/>&#13;
$ <span class="codestrong1">gcc -o hello_mp hello_mp.c -fopenmp</span><br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./hello_mp 4</span><br/>&#13;
Hello world! I am thread 2<br/>&#13;
Hello world! I am thread 3<br/>&#13;
Hello world! I am thread 0<br/>&#13;
Hello world! I am thread 1<br/>&#13;
</pre>&#13;
<p class="indent">Since the execution of threads can change with subsequent runs, rerunning this program results in a different sequence of messages:</p>&#13;
<pre>&#13;
$ <span class="codestrong1">./hello_mp 4</span><br/>&#13;
Hello world! I am thread 3<br/>&#13;
Hello world! I am thread 2<br/>&#13;
Hello world! I am thread 1<br/>&#13;
Hello world! I am thread 0<br/>&#13;
</pre>&#13;
<p class="indent">This behavior is consistent with our example with Pthreads (see “Hello Threading! Writing Your First Multithreaded Program” on <a href="ch14.xhtml#lev1_106">page 677</a>).</p>&#13;
<h4 class="h4" id="lev2_252">14.7.3 A More Complex Example: CountSort in OpenMP</h4>&#13;
<p class="noindent">A powerful advantage of OpenMP is that it enables programmers to incrementally parallelize their code. To see this in action, let’s parallelize the more complex CountSort algorithm discussed earlier in this chapter. Recall that this algorithm sorts arrays containing a small range of values. The main function of the serial program<sup>3</sup> looks like the following:</p>&#13;
<pre>&#13;
int main( int argc, char **argv ) {<br/>&#13;
    //parse args (omitted for brevity)<br/>&#13;
<br/>&#13;
    srand(10); //use of static seed ensures the output is the same every run<br/>&#13;
<br/>&#13;
    //generate random array of elements of specified length<br/>&#13;
    //(omitted for brevity)<br/>&#13;
<br/>&#13;
    //allocate counts array and initializes all elements to zero.<br/>&#13;
    int counts[MAX] = {0};<br/>&#13;
<br/>&#13;
    countElems(counts, array, length); //calls step 1<br/>&#13;
    writeArray(counts, array); //calls step2<br/>&#13;
<br/>&#13;
    free(array); //free memory<br/>&#13;
<br/>&#13;
    return 0;<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_730"/>The <code>main</code> function, after doing some command line parsing and generating a random array, calls the <code>countsElems</code> function followed by the <code>writeArray</code> function.</p>&#13;
<h5 class="h5" id="lev3_133">Parallelizing CountElems Using OpenMP</h5>&#13;
<p class="noindent">There are several ways to parallelize the preceding program. One way (shown in the example that follows) uses the <code>omp parallel</code> pragma in the context of the <code>countElems</code> and <code>writeArray</code> functions. As a result, no changes need to be made to the <code>main</code> function.<sup><a href="ch14.xhtml#fn14_15" id="rfn14_15">15</a></sup></p>&#13;
<p class="indent">First, let’s examine how to parallelize the <code>countElems</code> function using OpenMP:</p>&#13;
<pre>&#13;
void countElems(int *counts, int *array, long length) {<br/>&#13;
<br/>&#13;
    #pragma omp parallel default(none) shared(counts, array, length)<br/>&#13;
    {<br/>&#13;
        int val, i, local[MAX] = {0};<br/>&#13;
        #pragma omp for<br/>&#13;
        for (i = 0; i &lt; length; i++) {<br/>&#13;
            val = array[i];<br/>&#13;
            local[val]++;<br/>&#13;
        }<br/>&#13;
<br/>&#13;
       #pragma omp critical<br/>&#13;
       {<br/>&#13;
           for (i = 0; i &lt; MAX; i++) {<br/>&#13;
               counts[i] += local[i];<br/>&#13;
           }<br/>&#13;
       }<br/>&#13;
   }<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">In this version of the code, three pragmas are employed. The <code>#pragma omp parallel</code> pragma indicates that a team of threads should be created. The <code>omp_set_num_threads(nthreads)</code> line in <code>main</code> sets the default size of the thread team to be <code>nthreads</code>. If the <code>omp_set_num_threads</code> function is not used, then the number of threads assigned will equal the number of cores in the system. As a reminder, the <code>omp parallel</code> pragma implicitly creates threads at the beginning of the block and joins them at the end of the block. Braces (<code>{}</code>) are used to specify scope. The <code>shared</code> clause declares that the variables <code>counts</code>, <code>array</code>, and <code>length</code> are shared (global) among all the threads. Thus, the variables <code>val</code>, <code>i</code>, and <code>local[MAX]</code> are declared <em>locally</em> in each thread.</p>&#13;
<p class="indent">The next pragma is <code>#pragma omp for</code>, which parallelizes the <code>for</code> loop, splitting the number of iterations among the number of threads. OpenMP calculates how best to split up the iterations of the loop. As previously mentioned, the default strategy is usually a chunking method, wherein each thread gets roughly the same number of iterations to compute. Thus, each <span epub:type="pagebreak" id="page_731"/>thread reads a component of the shared array <code>array</code>, and accumulates its counts in its local array <code>local</code>.</p>&#13;
<p class="indent">The <code>#pragma omp critical</code> pragma indicates that the code in the scope of the critical section should be executed by exactly one thread at a time. This is equivalent to the mutex that was employed in the Pthreads version of this program. Here, each thread increments the shared <code>counts</code> array one at a time.</p>&#13;
<p class="indent">Let’s get a sense of the performance of this function by running it with 100 million elements:</p>&#13;
<pre>&#13;
$ <span class="codestrong1">./countElems_mp 100000000 1</span><br/>&#13;
Run Time for Phase 1 is 0.249893<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_mp 100000000 2</span><br/>&#13;
Run Time for Phase 1 is 0.124462<br/>&#13;
<br/>&#13;
$ <span class="codestrong1">./countElems_mp 100000000 4</span><br/>&#13;
Run Time for Phase 1 is 0.068749<br/>&#13;
</pre>&#13;
<p class="indent">This is excellent performance, with our function getting a speedup of 2 on two threads, and a speedup of 3.63 on four threads. We get even better performance than the Pthreads implementation!</p>&#13;
<h5 class="h5" id="lev3_134">The writeArray Function in OpenMP</h5>&#13;
<p class="noindent">Parallelizing the <code>writeArray</code> function is <em>much</em> harder. The following code shows one possible solution:</p>&#13;
<pre>&#13;
void writeArray(int *counts, int *array) {<br/>&#13;
    int i;<br/>&#13;
<br/>&#13;
    //assumed the number of threads is no more than MAX<br/>&#13;
    #pragma omp parallel for schedule(dynamic)<br/>&#13;
    for (i = 0; i &lt; MAX; i++) {<br/>&#13;
        int j = 0, amt, start = 0;<br/>&#13;
        for (j = 0; j &lt; i; j++) {  //calculate the "true" start position<br/>&#13;
            start += counts[j];<br/>&#13;
        }<br/>&#13;
<br/>&#13;
        amt = counts[i]; //the number of array positions to fill<br/>&#13;
<br/>&#13;
        //overwrite amt elements with value i, starting at position start<br/>&#13;
        for (j = start; j &lt; start + amt; j++) {<br/>&#13;
            array[j] = i;<br/>&#13;
        }<br/>&#13;
    }<br/>&#13;
}<br/>&#13;
</pre>&#13;
<p class="indent">Prior to parallelizing, we made a change to this function because the old version of <code>writeArray</code> caused <code>j</code> to have a dependency on the previous <span epub:type="pagebreak" id="page_732"/>iterations of the loop. In this version, each thread calculates its unique <code>start</code> value based on the sum of all the previous elements in <code>counts</code>.</p>&#13;
<p class="indent">When this dependency is removed, the parallelization is pretty straightforward. The <code>#pragma omp parallel for</code> pragma generates a team of threads and parallelizes the <code>for</code> loop by assigning each thread a subset of the iterations of the loop. As a reminder, this pragma is a combination of the <code>omp parallel</code> and the <code>omp for</code> pragmas (which were used in the parallelization of <code>countElems</code>).</p>&#13;
<p class="indent">A chunking approach to scheduling threads (as shown in the earlier <code>countElems</code> function) is not appropriate here, because it is possible that each element in <code>counts</code> has a radically different frequency. Therefore, the threads will not have equal work, resulting in some threads being assigned more work than others. Therefore, the <code>schedule(dynamic)</code> clause is employed, so that each thread completes the iteration it is assigned before requesting a new iteration from the thread manager.</p>&#13;
<p class="indent">Since each thread is writing to distinct array locations, mutual exclusion is not needed for this function.</p>&#13;
<p class="indent">Notice how much cleaner the OpenMP code is than the POSIX thread implementation. The code is very readable and required very little modification. This is one of the powers of <em>abstraction</em>, in which the implementation details are hidden from the programmer.</p>&#13;
<p class="indent">However, a necessary trade-off for abstraction is control. The programmer assumes that the compiler is “smart” enough to take care of the particulars of parallelization and thus has an easier time parallelizing their application. However, the programmer no longer makes detailed decisions about the particulars of that parallelization. Without a clear idea of how OpenMP pragmas execute under the hood, it can be difficult to debug an OpenMP application or know which pragma is the most appropriate to use at a given time.</p>&#13;
<h4 class="h4" id="lev2_253">14.7.4 Learning More About OpenMP</h4>&#13;
<p class="noindent">A deeper discussion of OpenMP is beyond the scope of this book, but there are useful free resources for learning<sup><a href="ch14.xhtml#fn14_16" id="rfn14_16">16</a></sup> and using<sup><a href="ch14.xhtml#fn14_17" id="rfn14_17">17</a></sup> OpenMP.</p>&#13;
<h3 class="h3" id="lev1_112">14.8 Summary</h3>&#13;
<p class="noindent">This chapter provided an overview of multicore processors and how to program them. Specifically, we cover the POSIX threads (or Pthreads) library and how to use it to create correct multithreaded programs that speed up a single-threaded program’s performance. Libraries like POSIX and OpenMP utilize the <em>shared memory</em> model of communication, as threads share data in a common memory space.</p>&#13;
<h4 class="h4" id="lev2_254"><span epub:type="pagebreak" id="page_733"/>Key Takeaways</h4>&#13;
<p class="noindent"><strong>Threads are the fundamental unit of concurrent programs.</strong>   To parallelize a serial program, programmers utilize lightweight constructs known as <em>threads</em>. For a particular multithreaded process, each thread has its own allocation of stack memory, but shares the program data, heap and instructions of the process. Like processes, threads run <em>nondeterministically</em> on the CPU (i.e., the order of execution changes between runs, and which thread is assigned to which core is left up to the operating system).</p>&#13;
<p class="noindenta"><strong>Synchronization constructs ensure that programs work correctly.</strong>   A consequence of shared memory is that threads can accidentally overwrite data residing in shared memory. A <em>race condition</em> can occur whenever two operations incorrectly update a shared value. When that shared value is data, a special type of race condition called a <em>data race</em> can arise. Synchronization constructs (mutexes, semaphores, etc.) help to guarantee program correctness by ensuring that threads execute one at a time when updating shared variables.</p>&#13;
<p class="noindenta"><strong>Be mindful when using synchronization constructs.</strong>   Synchronization inherently introduces points of serial computation in an otherwise parallel program. It is therefore important to be aware of <em>how</em> one uses synchronization concepts. The set of operations that must run atomically is referred to as a <em>critical section</em>. If a critical section is too big, the threads will execute serially, yielding no improvement in runtime. Use synchronization constructs sloppily, and situations like <em>deadlock</em> may inadvertently arise. A good strategy is to have threads employ local variables as much as possible and update shared variables only when necessary.</p>&#13;
<p class="noindenta"><strong>Not all components of a program are parallelizable.</strong>   Some programs necessarily have large serial components that can hinder a multithreaded program’s performance on multiple cores (e.g., <em>Amdahl’s Law</em>). Even when a high percentage of a program is parallelizable, speedup is rarely linear. Readers are also encouraged to look at other metrics such as efficiency and scalability when ascertaining the performance of their programs.</p>&#13;
<h4 class="h4" id="lev2_255">Further Reading</h4>&#13;
<p class="noindent">This chapter is meant to give a taste of concurrency topics with threads; it is by no means exhaustive. To learn more about programming with POSIX threads and OpenMP, check out the excellent tutorials on Pthreads<sup><a href="ch14.xhtml#fn14_18" id="rfn14_18">18</a></sup> and OpenMP<sup><a href="ch14.xhtml#fn14_19" id="rfn14_19">19</a></sup> by Blaise Barney from Lawrence Livermore National Labs. For automated tools for debugging parallel programs, readers are encouraged to check out the Helgrind<sup><a href="ch14.xhtml#fn14_20" id="rfn14_20">20</a></sup> and DRD<sup><a href="ch14.xhtml#fn14_21" id="rfn14_21">21</a></sup> Valgrind tools.</p>&#13;
<p class="indent">In the final chapter of the book, we give a high-level overview of other common parallel architectures and how to program them.</p>&#13;
<h3 class="h3" id="lev1_113"><span epub:type="pagebreak" id="page_734"/>Notes</h3>&#13;
<p class="fnote"><a href="ch14.xhtml#rfn14_1" id="fn14_1">1.</a> <em><a href="https://www.raspberrypi.org/">https://www.raspberrypi.org/</a></em></p>&#13;
<p class="fnote"><a href="ch14.xhtml#rfn14_2" id="fn14_2">2.</a> Available at <em><a href="https://diveintosystems.org/book/C14-SharedMemory/_attachments/hellothreads.c">https://diveintosystems.org/book/C14-SharedMemory/_attachments/hellothreads.c</a></em>.</p>&#13;
<p class="fnote"><a href="ch14.xhtml#rfn14_3" id="fn14_3">3.</a> Available at <em><a href="https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort.c">https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort.c</a></em>.</p>&#13;
<p class="fnote"><a href="ch14.xhtml#rfn14_4" id="fn14_4">4.</a> Available at <em><a href="https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p.c">https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p.c</a></em>.</p>&#13;
<p class="fnote"><a href="ch14.xhtml#rfn14_5" id="fn14_5">5.</a> The full source can be downloaded from <em><a href="https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v2.c">https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v2.c</a></em>.</p>&#13;
<p class="fnote"><a href="ch14.xhtml#rfn14_6" id="fn14_6">6.</a> The full source code for this final program can be accessed at <em><a href="https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v3.c">https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElems_p_v3.c</a></em>.</p>&#13;
<p class="fnote"><a href="ch14.xhtml#rfn14_7" id="fn14_7">7.</a> Available at <em><a href="https://diveintosystems.org/book/C14-SharedMemory/_attachments/layeggs.c">https://diveintosystems.org/book/C14-SharedMemory/_attachments/layeggs.c</a></em>.</p>&#13;
<p class="fnote"><a href="ch14.xhtml#rfn14_8" id="fn14_8">8.</a> Gene Amdahl. “Validity of the single processor approach to achieving large scale computing capabilities,” <em>Proceedings of the April 18-20, 1967, Spring Joint Computer Conference</em>, pp. 483–485, ACM, 1967.</p>&#13;
<p class="fnote"><a href="ch14.xhtml#rfn14_9" id="fn14_9">9.</a> John Gustafson, “Reevaluating Amdahl’s law,” <em>Communications of the ACM</em> 31(5), pp. 532–533, 1988.</p>&#13;
<p class="fnote1"><a href="ch14.xhtml#rfn14_10" id="fn14_10">10.</a> Caroline Connor, “Movers and Shakers in HPC: John Gustafson,” <em>HPC Wire</em>, <em><a href="http://www.hpcwire.com/hpcwire/2010-10-20/movers_and_shakers_in_hpc_john_gustafson.html">http://www.hpcwire.com/hpcwire/2010-10-20/movers_and_shakers_in_hpc_john_gustafson.html</a></em>.</p>&#13;
<p class="fnote1"><a href="ch14.xhtml#rfn14_11" id="fn14_11">11.</a> <em><a href="http://pubs.opengroup.org/onlinepubs/009695399/functions/xsh_chap02_09.html">http://pubs.opengroup.org/onlinepubs/009695399/functions/xsh_chap02_09.html</a></em></p>&#13;
<p class="fnote1"><a href="ch14.xhtml#rfn14_12" id="fn14_12">12.</a> <a href="https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr.c">https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr.c</a></p>&#13;
<p class="fnote1"><a href="ch14.xhtml#rfn14_13" id="fn14_13">13.</a> Available at <em><a href="https://diveintosystems.org/book/C14-SharedMemory/_attachmentscountElemsStr_p.c">https://diveintosystems.org/book/C14-SharedMemory/_attachmentscountElemsStr_p.c</a></em>.</p>&#13;
<p class="fnote1"><a href="ch14.xhtml#rfn14_14" id="fn14_14">14.</a> Full source code available at <a href="https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr_p_v2.c">https://diveintosystems.org/book/C14-SharedMemory/_attachments/countElemsStr_p_v2.c</a>.</p>&#13;
<p class="fnote1"><a href="ch14.xhtml#rfn14_15" id="fn14_15">15.</a> A full version of the program is available at <em><a href="https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort_mp.c">https://diveintosystems.org/book/C14-SharedMemory/_attachments/countSort_mp.c</a></em>.</p>&#13;
<p class="fnote1"><a href="ch14.xhtml#rfn14_16" id="fn14_16">16.</a> Blaise Barney, “OpenMP,” <a href="https://hpc.llnl.gov/tuts/openMP/">https://hpc.llnl.gov/tuts/openMP/</a></p>&#13;
<p class="fnote1"><a href="ch14.xhtml#rfn14_17" id="fn14_17">17.</a> Richard Brown and Libby Shoop, “Multicore Programming with OpenMP,” <em>CSinParallel: Parallel Computing in the Computer Science Curriculum</em>, <a href="http://selkie.macalester.edu/csinparallel/modules/MulticoreProgramming/build/html/index.html">http://selkie.macalester.edu/csinparallel/modules/MulticoreProgramming/build/html/index.html</a></p>&#13;
<p class="fnote1"><a href="ch14.xhtml#rfn14_18" id="fn14_18">18.</a> <em><a href="https://hpc-tutorials.llnl.gov/posix/">https://hpc-tutorials.llnl.gov/posix/</a></em></p>&#13;
<p class="fnote1"><a href="ch14.xhtml#rfn14_19" id="fn14_19">19.</a> <em><a href="https://hpc.llnl.gov/tuts/openMP/">https://hpc.llnl.gov/tuts/openMP/</a></em></p>&#13;
<p class="fnote1"><a href="ch14.xhtml#rfn14_20" id="fn14_20">20.</a> <em><a href="https://valgrind.org/docs/manual/hg-manual.html">https://valgrind.org/docs/manual/hg-manual.html</a></em></p>&#13;
<p class="fnote1"><a href="ch14.xhtml#rfn14_21" id="fn14_21">21.</a> <em><a href="https://valgrind.org/docs/manual/drd-manual.html">https://valgrind.org/docs/manual/drd-manual.html</a></em></p>&#13;
</body></html>