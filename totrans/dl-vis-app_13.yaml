- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classifiers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we introduce four important classification algorithms, building
    on the basics of classification that we covered in Chapter 7\. We often use these
    algorithms to help us study and understand our data. In some cases, we can even
    build a final classification system from them. In other cases, we can use the
    understanding we gain from these methods to design a deep learning classifier,
    as discussed in later chapters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: We will usually illustrate our classifiers using 2D data and usually only two
    classes because that’s easy to draw and understand, but modern classifiers are
    capable of handling data with any number of dimensions (or features) and huge
    numbers of classes. Modern libraries let us apply most of these algorithms to
    our own data with just a handful of lines of code.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Types of Classifiers
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we get into specific algorithms, let’s break down the world of classifiers
    into two main approaches: *parametric* and *nonparametric*.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: In the parametric approach, we usually think of the algorithm as starting with
    a preconceived description of the data it’s working with, and it then searches
    for the best parameters of that description to make it fit. For instance, if we
    think that our data follows a normal distribution, we can look for the mean and
    standard deviation that best fit it.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: In the nonparametric approach, we let the data lead the way, and we try to come
    up with some way to represent it only after we’ve analyzed it. For example, we
    may look at all of the data and attempt to find a boundary that splits it into
    two or more classes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: In reality, these two approaches are more conceptual than strict. For example,
    we can argue that simply choosing a particular kind of learning algorithm means
    that we’re making assumptions about our data. And we can argue that we’re always
    learning about the data itself by processing it. But these are useful generalizations,
    and we’ll use them to organize our discussion.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by looking at two nonparametric classifiers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: k-Nearest Neighbors
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We begin with a nonparametric algorithm called *k-nearest neighbors*, or *kNN*.
    As usual, the letter *k* at the start refers not to a word but to a number. We
    can pick any integer that’s 1 or larger. Because we set this value before the
    algorithm runs, it’s a hyperparameter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 7, we saw an algorithm called *k-means clustering*. Despite the similarity
    in names, that algorithm and *k*-nearest neighbor are different techniques. One
    key difference is that *k*-means clustering learns from unlabeled data, whereas
    kNN works with labeled data. In other words, *k*-means clustering and kNN fall
    into the classes of unsupervised and supervised learning, respectively.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: kNN is fast to train, because all it does is save a copy of every incoming sample
    into a database. The interesting part comes when training is complete, and a new
    sample arrives to be classified. The central idea of how kNN classifies a new
    sample is appealingly geometric, as shown in [Figure 11-1](#figure11-1).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![F11001](Images/F11001.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-1: To find the class for a new sample, shown as a star, we find the
    most popular of its *k* neighbors.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 11-1](#figure11-1)(a) we have a sample point (a star) amid a bunch
    of other samples that represent three classes (circle, square, and triangle).
    To determine a class for our new sample, we look at the *k* nearest samples (or
    *neighbors*), and we count up their classes. Whichever class is most populous
    becomes the new sample’s class. We show which samples are considered for different
    values of *k* by a line to each of the *k* nearest samples. In [Figure 11-1](#figure11-1)(b)
    we’ve set *k* to 1, meaning we want to use the class of the nearest sample. In
    this case it’s a red circle, so this new sample is classified as a circle. In
    [Figure 11-1](#figure11-1)(c) we’ve set *k* to 9, so we look at the nine nearest
    points. Here we find 3 circles, 4 squares, and 2 triangles. Because there are
    more squares than any other class, the star is classified as a square. In [Figure
    11-1](#figure11-1)(d) we’ve set *k* to 25\. Now we have 6 circles, 13 squares,
    and 6 triangles, so again the star is classified as a square.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: To sum this up, kNN accepts a new sample to evaluate, along with a value of
    *k*. It then finds the closest *k* samples to the new sample, and we assign the
    new sample to the class with the largest number of representatives among the *k*
    samples we found. There are various ways to break ties and handle exceptional
    cases, but that’s the basic idea.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Note that kNN does not create explicit boundaries between groups of points.
    There’s no notion here of regions or areas that samples belong to. We say that
    kNN is an *on-demand*, or *lazy*, algorithm because it does no processing of the
    samples during the learning stage. When learning, kNN just stashes the samples
    in its internal memory and it’s done.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: kNN is attractive because it’s simple, and training it is usually exceptionally
    fast. On the other hand, kNN can require a lot of memory, because (in its basic
    form) it’s saving all the input samples. Using large amounts of memory can slow
    down the algorithm. Another problem is that the classification of new points is
    often slow (compared to other algorithms we’ll see) because of the cost of searching
    for neighbors. Every time we want to classify a new piece of data, we have to
    find its *k* nearest neighbors, which requires work. Of course, there are many
    ways to enhance the algorithm to speed this up, but it still remains a relatively
    slow way to classify. For applications where classification speed is important,
    like real-time systems and websites, the time required by kNN to produce each
    answer can take it out of the running.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: kNN之所以具有吸引力，是因为它简单，并且训练速度通常非常快。另一方面，kNN可能需要大量内存，因为（在其基本形式下）它保存所有输入样本。使用大量内存可能会导致算法变慢。另一个问题是，新的数据点的分类通常很慢（与我们接下来看到的其他算法相比），这是由于寻找邻居的成本。每次我们想要对一个新数据进行分类时，都必须找到它的*k*个最近邻，这需要一定的计算。当然，有许多方法可以增强算法以加快这一过程，但它仍然是一个相对较慢的分类方法。对于分类速度至关重要的应用，例如实时系统和网站，kNN每次产生答案所需的时间可能会使其无法使用。
- en: Another problem with this technique is that it depends on having lots of neighbors
    nearby (after all, if the nearest neighbors are all very far away, then they don’t
    offer a good proxy for other examples that are like the one we’re trying to classify).
    This means we need a lot of training data. If we have lots of features (that is,
    our data has many dimensions) then kNN quickly succumbs to the curse of dimensionality,
    which we discussed in Chapter 7\. As the dimensionality of the space goes up,
    if we don’t also significantly increase the number of training samples, then the
    number of samples in any local neighborhood drops, making it harder for kNN to
    get a good collection of nearby points.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术的另一个问题是，它依赖于附近有大量邻居（毕竟，如果最近的邻居都非常远，那么它们就无法为我们尝试分类的其他相似样本提供很好的代理）。这意味着我们需要大量的训练数据。如果我们有很多特征（也就是我们的数据有很多维度），那么kNN很快就会遭遇维度灾难，这在第7章中我们已经讨论过。随着空间的维度增加，如果我们没有显著增加训练样本的数量，那么任何局部邻域中的样本数都会减少，这使得kNN很难得到一个好的邻近点集合。
- en: Let’s put kNN to the test. In [Figure 11-2](#figure11-2) we show a “smile” dataset
    of 2D data that falls into two classes. In this figure, and the similar ones that
    follow, the data is made of points. Since points are hard to see, we’re drawing
    a filled circle around each point as a visual aid.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来测试一下kNN。在[图11-2](#figure11-2)中，我们展示了一个“微笑”数据集，该数据集属于两类。在此图以及接下来类似的图中，数据由点组成。由于点难以观察，我们在每个点周围绘制一个实心圆圈作为视觉辅助。
- en: '![F11002](Images/F11002.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![F11002](Images/F11002.png)'
- en: 'Figure 11-2: A smile dataset of 2D points. There are two classes, blue and
    orange.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图11-2：一个二维点的微笑数据集。共有两类，蓝色和橙色。
- en: Using kNN with different values of *k* gives us the results in [Figure 11-3](#figure11-3).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同*k*值的kNN给出了[图11-3](#figure11-3)中的结果。
- en: '![F11003](Images/F11003.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![F11003](Images/F11003.png)'
- en: 'Figure 11-3: Classifying with the points in [Figure 11-2](#figure11-2) using
    kNN for different values of *k*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图11-3：使用不同*k*值的kNN对[图11-2](#figure11-2)中的点进行分类
- en: Although kNN doesn’t produce explicit classification boundaries, we can see
    that when *k* is small and we compare our input point with only a few neighbors,
    the space is broken up into sections that share a rather rough border. As *k*
    gets larger and we use more neighbors, that border smooths out because we’re getting
    a better overall picture of the environment around the new sample.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管kNN不会生成明确的分类边界，但我们可以看到，当*k*较小时，我们只与少数邻居进行比较时，空间被分割成一些具有相当粗糙边界的区域。随着*k*增大，我们使用更多的邻居时，那些边界会变得平滑，因为我们对新样本周围的环境有了更全面的了解。
- en: To make things more interesting, let’s add some noise to our data so that the
    edges aren’t so easy to find. [Figure 11-4](#figure11-4) shows a noisy version
    of [Figure 11-2](#figure11-2).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让事情变得更有趣，我们向数据中添加一些噪声，这样边界就不容易找到。[图11-4](#figure11-4)展示了[图11-2](#figure11-2)的一个噪声版本。
- en: '![F11004](Images/F11004.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![F11004](Images/F11004.png)'
- en: 'Figure 11-4: A noisy version of the smile dataset from [Figure 11-2](#figure11-2)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图11-4：来自[图11-2](#figure11-2)的噪声版本的微笑数据集
- en: The results for different values of *k* are shown in [Figure 11-5](#figure11-5).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 不同*k*值的结果显示在[图11-5](#figure11-5)中。
- en: '![F11005](Images/F11005.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![F11005](Images/F11005.png)'
- en: 'Figure 11-5: Using kNN and [Figure 11-4](#figure11-4) to assign a class to
    points in the plane'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-5：使用 kNN 和 [图 11-4](#figure11-4) 将类别分配给平面中的点
- en: We can see that in the presence of noise, small values of *k* lead to ragged
    borders. In this example, we have to get *k* up to 50 before we see fairly smooth
    boundaries. Also notice that as *k* increases, the smile shape contracts, since
    the perimeter gets eroded by the larger number of background points.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在存在噪声的情况下，较小的 *k* 值会导致边界不规则。在这个例子中，我们需要将 *k* 增加到 50 才能看到相对平滑的边界。还需要注意的是，随着
    *k* 增加，微笑形状会收缩，因为较多的背景点侵蚀了边缘。
- en: Because kNN doesn’t explicitly represent the boundaries between classes, it
    can handle any kind of boundaries, or any distribution of classes. To see this,
    let’s add some eyes to our smile, creating three disconnected sets of the same
    class. The resulting noisy data is in [Figure 11-6](#figure11-6).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 kNN 并没有明确表示类别之间的边界，它可以处理任何类型的边界或任何类别的分布。为了说明这一点，我们可以在微笑上添加一些眼睛，创建三个不相连的相同类别的数据集。生成的带噪声数据见
    [图 11-6](#figure11-6)。
- en: '![F11006](Images/F11006.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![F11006](Images/F11006.png)'
- en: 'Figure 11-6: A noisy dataset of a smile and two eyes'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-6：带噪声的数据集，包含一个微笑和两只眼睛
- en: The resulting classifications for different values of *k* are shown in [Figure
    11-7](#figure11-7).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 不同 *k* 值的结果分类见 [图 11-7](#figure11-7)。
- en: In this example, a value of about 20 for *k* looks best. Too small a value of
    *k* can give us ragged edges and noisy results, but too large a value of *k* can
    start to erode the features. As is so often the case, finding the best hyperparameter
    for this algorithm for any given dataset is a matter of repeated experimentation.
    We can use cross-validation to automatically score the quality of each result,
    which is particularly useful when there are many dimensions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，约 20 的 *k* 值看起来最好。过小的 *k* 值可能导致边缘不规则和结果噪声，但过大的 *k* 值可能会开始侵蚀特征。像往常一样，找到最佳的超参数对于任何给定数据集而言，都是通过反复实验来完成的。我们可以使用交叉验证来自动评分每个结果的质量，这在维度很多的情况下特别有用。
- en: 'kNN is a great nonparametric algorithm: it’s easy to understand and program,
    and when the dataset isn’t too big, training is extremely fast and classification
    of new data isn’t too slow. But when the dataset gets large, kNN becomes less
    appealing: memory requirements go up because every sample is kept, and classification
    gets slower because the search gets slower. These problems are shared by most
    nonparametric algorithms.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: kNN 是一个很棒的非参数算法：它容易理解和编程，而且当数据集不太大时，训练非常快速，分类新数据也不会太慢。但当数据集变大时，kNN 就不那么吸引人了：内存需求增加，因为每个样本都会被保留，分类变得更慢，因为搜索变得更慢。这些问题是大多数非参数算法共有的。
- en: '![F11007](Images/F11007.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![F11007](Images/F11007.png)'
- en: 'Figure 11-7: kNN doesn’t create boundaries between clusters of samples, so
    it works even when a class is broken up into pieces.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-7：kNN 不会在样本簇之间创建边界，因此即使一个类别被拆分成几个部分，它也能正常工作。
- en: Decision Trees
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树
- en: Let’s consider another nonparametric classification method, called *decision
    trees*. This algorithm builds a data structure from the points in the sample set,
    which is then used to classify new points. Let’s begin by taking a look at this
    structure, and then we’ll see how to build it.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑另一种非参数分类方法，称为 *决策树*。该算法从样本集中的点构建一个数据结构，然后用这个结构来分类新点。让我们首先看看这个结构，然后再看看如何构建它。
- en: Introduction to Trees
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 树的介绍
- en: We can illustrate the basic idea behind decision trees with the familiar parlor
    game called 20 Questions. In this game, one player (the chooser) thinks of a specific
    target object, which is often a person, place, or thing. The other player (the
    guesser) then asks a series of yes/no questions. If the guesser can correctly
    identify the target in 20 or fewer questions, they win. One reason the game endures
    is that it’s fun to narrow down the enormous number of possible people, places,
    and things to one specific instance with such a small number of simple questions
    (perhaps surprisingly, with 20 yes/no questions we can only distinguish just over
    a million distinct targets).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个熟悉的娱乐游戏“20个问题”来说明决策树的基本思想。在这个游戏中，一名玩家（选择者）想一个特定的目标物体，通常是一个人、地方或事物。另一名玩家（猜测者）接着问一系列是/否问题。如果猜测者能在20个问题内正确识别目标物体，他们就获胜。这个游戏经久不衰的原因之一是，它很有趣，通过如此少量的简单问题就能把庞大的可能对象范围缩小到一个特定实例（或许令人惊讶的是，20个是/否问题我们只能区分出超过一百万个不同的目标）。
- en: We can draw a typical game of 20 questions in graphical form, as in [Figure
    11-8](#figure11-8).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![F11008](Images/F11008.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-8: A tree for playing 20 questions. Note that after each decision
    there are exactly two choices, one each for “yes” and “no.”'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: We call a structure like [Figure 11-8](#figure11-8) a *tree* because it looks
    something like an upside-down tree. Such trees have a bunch of associated terms
    that are worth knowing. We say that each splitting point in the tree is a *node*,
    and each line connecting nodes is a *link*, *edge*, or *branch*. Following the
    tree analogy, the node at the top is the *root*, and the nodes at the bottom are
    *leaves*, or *terminal nodes*. Nodes between the root and the leaves are called
    *internal nodes* or *decision nodes*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: If a tree has a perfectly symmetrical shape, we say the tree is *balanced*,
    otherwise it’s *unbalanced*. In practice, almost all trees are unbalanced when
    they’re made, but we can run algorithms to make them closer to being balanced
    if a particular application prefers that. We also say that every node has a *depth*,
    which is a number that gives the smallest number of nodes we must go through to
    reach the root. The root has a depth of 0, the nodes immediately below it have
    a depth of 1, and so on.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-9](#figure11-9) shows a tree with these labels.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![F11009](Images/F11009.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-9: Some terminology for a tree'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: It’s also common to use the terms associated with family trees, though these
    abstract trees don’t require the union of two nodes to produce children. Every
    node (except the root) has a node above it. We call this the *parent* of that
    node. The nodes immediately below a parent node are its *children*. We sometimes
    distinguish between *immediate children* that are directly connected to a parent,
    and *distant children* that are at the same depth as immediate children, but connected
    to the parent through a sequence of other nodes. If we focus our attention on
    a specific node, then that node and all of its children taken together are called
    a *subtree*. Nodes that share the same immediate parent are called *siblings*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-10](#figure11-10) shows some of these ideas graphically.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![F11010](Images/F11010.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-10: Using familiar terms with trees. The green node’s parent is immediately
    above it, and its children are immediately below.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: With this vocabulary in mind, let’s return to our game of 20 Questions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Using Decision Trees
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An interesting quality of the 20 Questionstree shown in [Figure 11-8](#figure11-8)
    is that it’s *binary*; every parent node has exactly two children: one for yes,
    one for no. Binary trees are a particularly easy kind of tree for some algorithms
    to work with. If some nodes have more than two children, we say that the tree
    overall is *bushy*. We can always convert a bushy tree to a binary tree if we
    want. An example of a bushy tree that tries to guess the month of someone’s birthday
    is shown on the left of [Figure 11-11](#figure11-11), and the corresponding binary
    tree is shown on the right. Because we can easily go back and forth, we usually
    draw trees in whatever form is most clear and succinct for the discussion at hand.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 11-8](#figure11-8)中展示的20问题树的一个有趣特点是它是*二叉*的；每个父节点恰好有两个子节点：一个代表“是”，一个代表“否”。二叉树是某些算法特别容易处理的一种树结构。如果某些节点有超过两个子节点，我们称整个树为*灌木状*。如果需要，我们总是可以将一个灌木状的树转换为二叉树。一个尝试猜测某人生日月份的灌木状树示例显示在[图
    11-11](#figure11-11)的左侧，右侧则是相应的二叉树。因为我们可以轻松地在两者之间转换，所以通常会根据讨论的需要，绘制最清晰简洁的树形。
- en: We can use trees to classify data. When used this way, the trees are called
    *decision trees*. The full name of the approach is *categorical variable decision
    trees*. This is to distinguish it from those times we use decision trees to work
    with continuous variables, as in regression problems. Those are called *continuous
    variable decision trees*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用树来分类数据。当树用于这种方式时，称为*决策树*。这种方法的全称是*分类变量决策树*。这是为了区分我们在使用决策树处理连续变量时的情况，如回归问题。那种情况下的决策树被称为*连续变量决策树*。
- en: '![F11011](Images/F11011.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![F11011](Images/F11011.png)'
- en: 'Figure 11-11: Turning a bushy tree (left) into a binary tree (right)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-11：将一个灌木状树（左）转换为二叉树（右）
- en: Let’s stick with the categorical versions here. For simplicity, we’ll just refer
    to them as decision trees, or simply trees, from now on. An example of such a
    tree for sorting inputs into different classes is shown in [Figure 11-12](#figure11-12).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们继续使用分类版本。为了简化，从现在开始，我们只将其称为决策树，或者简称为树。一个用于将输入分类到不同类别的树的示例如[图 11-12](#figure11-12)所示。
- en: In [Figure 11-12](#figure11-12), we start with a root node containing samples
    of different classes, distinguished by their shapes (and colors). To construct
    the tree, we *split* the samples at each node into two groups using some kind
    of test, resulting in a decision. For instance, the test applied at the root node
    might be, “Is this shape rectangular?” The test for the left child might be, “Is
    this shape taller than it is wide?” We’ll soon see how to come up with such tests.
    The goal in this example is to keep splitting each node until we’re left with
    samples of only one class. At that point, we declare that node to be a leaf, and
    stop splitting. In [Figure 11-12](#figure11-12), we’ve split our starting data
    into five classes. It’s essential to remember the test we applied at each node.
    Now, when a new sample arrives, we can start at the root and apply the root’s
    test, then the test of the appropriate child, and so on. When we finally reach
    a leaf, we have determined the class for that sample.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 11-12](#figure11-12)中，我们从一个包含不同类别样本的根节点开始，这些样本通过形状（和颜色）来区分。为了构建树，我们在每个节点将样本分成两组，使用某种测试方式，得到一个决策。例如，在根节点应用的测试可能是：“这个形状是矩形吗？”左子节点的测试可能是：“这个形状比宽度高吗？”我们很快就会看到如何提出这些测试。这个示例的目标是不断拆分每个节点，直到只剩下属于一个类别的样本为止。此时，我们声明该节点为叶子节点，并停止拆分。在[图
    11-12](#figure11-12)中，我们已将起始数据分成了五个类别。记住我们在每个节点应用的测试非常重要。现在，当一个新样本到来时，我们可以从根节点开始，应用根节点的测试，然后是相应子节点的测试，依此类推。最终，当我们到达一个叶子节点时，就确定了该样本的类别。
- en: '![F11012](Images/F11012.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![F11012](Images/F11012.png)'
- en: 'Figure 11-12: A categorical decision tree. Each class is a different shape
    and color.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-12：一个分类决策树。每个类别都有不同的形状和颜色。
- en: Decision trees don’t start out fully built. Instead, we build the tree based
    on the samples in the training set. When we reach a leaf node during training,
    we test to see if the new training sample has the same class as all the other
    samples in that leaf. If it does, we add the sample to the leaf and we’re done.
    Otherwise, we come up with decision criteria based on some of the features that
    let us distinguish between this sample and the previous samples in the node. We
    then use this test to splitthe node. The test we come up with gets saved with
    the node, we create at least two children, and assign each sample to the appropriate
    child, as in [Figure 11-13](#figure11-13).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: When we’re done with training, evaluating new samples is easy. We just start
    at the root and work our way down the tree, following the appropriate branch at
    each node based on that node’s test with this sample’s features. When we land
    in a leaf, we report that the sample belongs to the class of the objects in that
    leaf.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![F11013](Images/F11013.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-13: Splitting a node by applying a test to its contents'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: This is an idealized process. In practice, our tests can be imperfect and our
    leaves might contain a mixture of objects of different classes if, for efficiency
    or memory reasons, we chose not to split some leaves any more. For example, if
    we land in a node that contains samples that are 80 percent from class A and 20
    percent from class B, we might report that the new sample has an 80 percent chance
    of being in A and a 20 percent chance of being in B. If we have to report just
    one class, we might report that it’s A 80 percent of the time, and B the other
    20 percent.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: This process of making a decision tree works with just one sample at a time.
    In the simplest version of this technique, we don’t consider the entire training
    set at once and try to find, say, the smallest or most balanced tree that classifies
    the samples. Instead, we consider one sample at a time and split the nodes in
    the tree as required to handle that sample. Then we do the same for the next sample,
    and the next, and so on, making decisions in the moment without caring about any
    other data yet to come. This makes for efficient training.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm makes decisions based only on the data it has seen before and
    the data currently under consideration. It doesn’t try to plan or strategize for
    the future based on what it has seen so far. We call this a *greedy* algorithm,
    since it’s focused on maximizing its immediate, short-term gains.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Decisions trees are sometimes preferred in practice over other classifiers because
    their results are *explainable*. When the algorithm assigns a class to a sample,
    we don’t have to unravel some complex mathematical or algorithmic process. Instead,
    we can fully explain the final result just by identifying each decision along
    the way. This can be important in real life, too.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we apply for a loan at a bank, but we’re turned down. When
    we ask for the reason, the bank can show us each test made along the way. We say
    that the workings of the algorithm are *transparent*. Note that this doesn’t mean
    that they’re fair or reasonable. The bank may have come up with tests that are
    biased against one or more social groups or that depend on what seem to be irrelevant
    criteria. Just because they can explain why they made their choice doesn’t make
    the process or the results satisfactory. Legislators in particular seem to prefer
    laws that enforce transparency, which is easy to demonstrate, over fairness, which
    is much harder. Transparency is nice to have, but it doesn’t mean a system is
    behaving the way we’d like it to.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees can make bad decisions because they are particularly prone to
    overfitting. Let’s see why.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting Trees
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s begin our discussion of overfitting in the tree-building process by considering
    a couple of examples.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: The data in [Figure 11-14](#figure11-14) shows a cleanly separated set of data
    representing two classes. A dataset that roughly follows this kind of geometry
    is often called a *two-moons dataset*, presumably because the semicircles reminded
    someone of crescent moons.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![F11014](Images/F11014.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-14: Our 600 starting data points for building a decision tree, arranged
    in a two-moons structure. These 2D points represent two classes, blue and orange.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Each step in building a tree potentially involves splitting a leaf, and thus
    replacing it with a node and two leaves, for a net increase to the tree of one
    internal node and one leaf. We often think about the size of our tree in terms
    of the number of leaves it contains.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-15](#figure11-15) shows the process of building a decision tree
    for this data.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![F11015](Images/F11015.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-15: Building a decision tree for the data in [Figure 11-14](#figure11-14).
    Notice how the tree starts with big chunks and refines them into smaller and more
    precise regions.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: In this figure, we drew the dataset over each image just for reference. In this
    example, each node corresponds to a box. We don’t show it here, but the tree begins
    with just a single root corresponding to a blue box that covers the entire region.
    Then the training process receives one of the orange points near the top of the
    orange curve. This isn’t in the blue class, so we split the root with a horizontal
    cut into two boxes, shown in the upper left. The next point that comes in is a
    blue point near the left part of the blue curve. This falls in the orange box,
    so we split that with a vertical cut into two boxes as shown, giving us a total
    of three leaves. The rest of the figure shows the evolving tree as more samples
    arrive.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the regions gradually refine as the tree grows in response to more
    training data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: This tree needs only 12 leaves to correctly classify every training sample.
    The final tree and the original data are shown together in [Figure 11-16](#figure11-16).
    This tree fits the data perfectly.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Note the two horizontal, thin rectangles. They enclose two orange samples at
    the top of the left side of the arc and manage to slip between the blue points
    (recall that the samples are points in the center of each circle). This is overfitting,
    because any future points that fall into those rectangles, despite the fact that
    they’re both almost completely in a blue region, will be classified as orange.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![F11016](Images/F11016.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-16: Our final tree with 12 leaves'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Because decision trees are so sensitive to each input sample, they have a profound
    tendency to overfit. In fact, decision trees almost always overfit, because every
    training sample can influence the tree’s shape. To see this, take a look at [Figure
    11-17](#figure11-17). Here we ran the same algorithm as for [Figure 11-16](#figure11-16)
    two times, but in each case, we used a different, randomly chosen 70 percent of
    the input data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![F11017](Images/F11017.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-17: Decision trees are very sensitive to their inputs. (Left) We
    randomly chose 70 percent of the samples from [Figure 11-14](#figure11-14) and
    fit a tree. (Right) The same process, but for a different randomly selected 70
    percent of the original samples.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: These two decision trees are similar, but definitely not identical. The tendency
    of decision trees to overfit is much more pronounced when the data isn’t so easily
    separated. Let’s look at an example of that now.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-18](#figure11-18) shows another pair of crescent moons, but this
    time we added lots of noise to the samples after they had their classes assigned.
    The two classes no longer have a clean boundary.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![F11018](Images/F11018.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-18: A noisy set of 600 samples for building decision trees'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a tree to this data starts out with big regions, but it rapidly turns
    into a complicated set of tiny boxes as the algorithm splits up nodes this way
    and that to match the noisy data. [Figure 11-19](#figure11-19) shows the result.
    In this case, it required 100 leaves for the tree to correctly classify the points.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![F11019](Images/F11019.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-19: The tree-building process. Note that the second row uses large
    numbers of leaves.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-20](#figure11-20) shows a close-up of the final tree and the original
    data.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![F11020](Images/F11020.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-20: Our noisy data fit with a tree with 100 leaves. Notice how many
    little boxes have been used to catch just an odd sample here and there.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: There’s a lot of overfitting here. Though we expect most of the samples in the
    lower right to be orange and most of those in the upper left to be blue, this
    tree has carved out a lot of exceptions based on this particular dataset. Future
    samples that fall into those little boxes are likely to be misclassified.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Let’s repeat our process of building trees using different, random 70 percent
    selections of the data in [Figure 11-18](#figure11-18). [Figure 11-21](#figure11-21)
    shows the results.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![F11021](Images/F11021.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-21: A couple of trees built with different sets of 70 percent of
    the samples in [Figure 11-18](#figure11-18)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: There are similarities, but these trees are significantly different, and lots
    of little bits exist only to classify a few samples. This is overfitting in action.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Although this may look pretty bad for the decision tree method, in Chapter 12
    we’ll see that by combining many simple decision trees into a group, or an *ensemble*,
    we can create robust, efficient classifiers that don’t suffer as much from overfitting.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: There are a few other ways we can control overfitting.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in [Figure 11-15](#figure11-15) and [Figure 11-19](#figure11-19),
    the first few steps of the tree’s growth tend to generate big, general shapes.
    It’s only when the tree gets very deep that we get the tiny boxes that are symptomatic
    of overfitting. One popular strategy to reducing overfitting is *depth limiting:*
    we simply limit the tree’s depth while it’s building. If a node is more than a
    given number of steps from the root, we just declare it a leaf and don’t split
    it any more. A different strategy is setting a minimum sample requirement so that
    we never split a node that has less than a certain number of samples, no matter
    how mixed they are.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Yet another approach to reducing overfitting is to reduce the size of the tree
    after it’s made in a process called *pruning*. This works by removing, or *trimming*,
    leaf nodes. We look at each leaf and characterize what would happen to the total
    error of the tree’s results if we removed that leaf. If the error is acceptable,
    we simply remove the leaf from the tree. If we remove all the children of a node,
    then it becomes a leaf itself, and a candidate for further pruning. Pruning a
    tree can make it shallower, which offers the additional benefit of also making
    it faster when we classify new data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Depth limiting, setting minimum sample requirements per node, and pruning all
    simplify the tree, but because they do so in different ways, they usually give
    us different results.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Splitting Nodes
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we leave decision trees, let’s return briefly to the node-splitting
    process, since many machine learning libraries offer us a choice of splitting
    algorithms to choose from. Here are two questions to ask when we consider a node:
    First, does it need to be split? Second, how should we split it? Let’s take these
    in order.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: When we ask if a node needs to be split, we usually consider to what extent
    all the samples in a given node are of the same class. We describe the uniformity
    of a node’s contents with a number called that node’s *purity*. If all the samples
    are in the same class, the node is completely pure. The more samples we have of
    other classes, the smaller the value of purity becomes. To test if a node needs
    splitting, we can check the purity against a threshold. If the node is too *impure*,
    meaning that the purity is below the threshold, we split it.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Now we can look at how to split the node. If our samples have many features,
    we can invent lots of different possible splitting tests. We can test the value
    of just one feature and ignore the others. We can look at groups of features and
    test on some aggregated values from them. We’re free to choose a completely different
    test at every node based on different features. This gives us a huge variety of
    possible tests to consider.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-22](#figure11-22) shows a node containing a mix of circles of different
    sizes and colors. Let’s try to get all the reddish objects in one child and all
    the bluish ones in another. When we just look at the data (usually the best first
    step with any new database), it seems like the reddish circles are the biggest.
    Let’s try using a test based on the radius of each circle. The figure shows the
    result of splitting on the radius using three different values.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![F11022](Images/F11022.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-22: Splitting a node according to different values of the radii of
    the circles within it'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the radius value of 70 produces the purest results, with all
    the blue objects in one child and all the red ones in the other. If we use this
    test for this node, we’ll remember which feature we’re splitting on (the radius)
    and what value to test for (70).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Since we could potentially split the node using a test based on any characteristic
    of the samples, we need some way to evaluate the results so we can pick the best
    test. Let’s look at two popular ways to test these results.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Recall from Chapter 6 that *entropy* is a measure of complexity, or how many
    bits it takes to communicate some piece of information. The *Information Gain
    (IG)* measure uses this idea by comparing the entropy of a node to that of the
    children produced by each candidate test.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate a test, IG adds together the entropies of all the new children produced
    by that test and compares that result to the entropy in the parent cell. The more
    pure a cell is, the lower its entropy, so if a test makes pure cells, the sum
    of their entropies is less than the entropy of their parent. After trying different
    ways to split a node, we choose the split that gives us the biggest reduction
    in entropy (or the biggest gain in information).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Another popular way to evaluate splitting tests is called the *Gini impurity*.
    The math used by this technique is designed to minimize the probability of misclassifying
    a sample. For example, suppose a leaf has 10 samples of class A and 90 samples
    of class B. If a new sample ends up at that leaf, and we report that it belongs
    to class B, there is a 10 percent chance that we are wrong. Gini impurity measures
    those errors at each leaf for multiple candidate split values. It then chooses
    the split that has the least chance of an erroneous classification.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Some libraries offer other measures for grading the quality of a potential split.
    As with so many other choices, we usually try out a few options and pick the one
    that works the best for the specific data we’re working with.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s consider our first parametric algorithm: the *support vector machine*
    (or *SVM*). We will use 2D data and just two classes for our illustrations (VanderPlas
    2016), but like most machine learning algorithms, the ideas are easily applied
    to data with any number of dimensions and classes.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The Basic Algorithm
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s begin with two blobs of points, one for each of two classes, shown in
    [Figure 11-23](#figure11-23).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![F11023](Images/F11023.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-23: Our starting dataset consists of two blobs of 2D samples.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: We want to find a boundary between these clusters. To keep things simple, let’s
    use a straight line. But which one? A lot of lines split these two groups. Three
    candidates are shown in [Figure 11-24](#figure11-24).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![F11024](Images/F11024.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-24: Three of the infinite number of lines that can separate our two
    clusters of samples'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Which of these lines should we pick? One way to think about this is to imagine
    new data that might come in. Generally speaking, we want to classify any new sample
    as belonging to the class of the sample that it is nearest.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate how well any given boundary line achieves this goal, let’s find
    its distance to the nearest sample of either class. We can use this distance to
    draw a symmetrical boundary around the line. [Figure 11-25](#figure11-25) shows
    the idea for a few different lines.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![F11025](Images/F11025.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-25: We can assign a quality to each line by finding the distance
    from that line to the nearest data point.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 11-25](#figure11-25), we’ve drawn a circle around the sample that’s
    closest to the line. In the leftmost figure, many new points that are closer to
    the lower-right cluster than the upper-left one would be incorrectly classified
    as part of the upper-left cluster. The same situation holds in the rightmost figure.
    In the center, the line is much better, but it’s now preferring the lower-right
    cluster a little. Since we want each new point to be assigned to the class of
    the sample it’s closest to, we’d like our line to go right through the middle
    of the two clusters.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm that finds this line is called a *support vector machine*, or
    *SVM* (Steinwart 2008). An SVM finds the line that is farthest from all the points
    in both clusters. In this context, the word *support* can be thought of as meaning
    “nearest,” *vector* is a synonym for “sample,” and *machine* is a synonym for
    “algorithm.” Thus, we can describe SVM as the “nearest sample algorithm.”
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: The best line for our two clusters in [Figure 11-23](#figure11-23), as calculated
    by SVM, is shown in [Figure 11-26](#figure11-26).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how SVM finds this line.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 11-26](#figure11-26) the circled samples are the nearest samples,
    or the *support vectors*. The algorithm’s first job is to locate these circled
    points. Once it finds them, the algorithm then finds the solid line near the middle
    of the figure. Of all the lines that separate the two sets of points, this is
    the line that’s farthest from every sample in each set, because it has the greatest
    distance to its support vectors. The dashed lines in [Figure 11-26](#figure11-26),
    like the circles around the support vectors, are just visual aids to help us see
    that the solid line in the center, found by the SVM, is the one that is as far
    as possible from all the samples. The distance from the solid line to the dashed
    lines that pass through the support vectors is called the *margin*. We can rephrase
    the idea by saying that the SVM algorithm finds the line with the largest margin.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![F11026](Images/F11026.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-26: The SVM algorithm finds the line that has the greatest distance
    from all the samples.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: What if the data is noisy, and the blobs overlap, as in [Figure 11-27](#figure11-27)?
    Now we can’t create a line surrounded by an empty zone. What’s the best line to
    draw through these overlapping sets of samples?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![F11027](Images/F11027.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-27: A new set of data where the blobs overlap'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: The SVM algorithm gives us control over a parameter that’s conventionally called
    *C*. This parameter controls how strict the algorithm is about letting points
    into the region between the margins. The larger the value of *C*, the more the
    algorithm demands an empty zone around the line. The smaller the value of *C*,
    the more points can appear in a zone around the line. We frequently need to search
    for the best value for *C* using trial and error. In practice, that usually means
    trying out lots of values and evaluating them with cross-validation.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-28](#figure11-28) shows our overlapping data with a *C* value of
    100,000.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![F11028](Images/F11028.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-28: The value of *C* tells SVM how sensitive to be to points that
    can intrude into the zone around the line that’s fit to the data. Here, *C* is
    100,000.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Let’s drop *C* way down to 0.01\. [Figure 11-29](#figure11-29) shows that this
    lets fewer points into the region around the line.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![F11029](Images/F11029.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-29: Lowering *C* to 0.01 lets in fewer points compared to [Figure
    11-28](#figure11-28).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The lines in [Figure 11-28](#figure11-28) and [Figure 11-29](#figure11-29) are
    different. Which one we prefer depends on what we want from our classifier. If
    we think the best boundary comes from the details near the zone where the points
    overlap, we want a small value of *C* so that we only look at the points near
    that boundary. If we think the overall shapes of the two collections of points
    is a better descriptor of that boundary, we want a larger value of *C* to include
    more of those farther-away points.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The SVM Kernel Trick
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A parametric algorithm is limited by the shapes it’s able to find. SVM, for
    instance, can only find *linear* shapes, like lines and planes. If we have data
    that can’t obviously be separated by such a shape, it may seem that SVM won’t
    be of much use. But there’s a clever trick that can sometimes let us use a linear
    boundary where it initially appears as if only a curved one could do the job.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we have the data of [Figure 11-30](#figure11-30), where there’s
    a blob of samples of one class surrounded by a ring of samples of another. There’s
    no way we can draw a straight line to separate these two sets.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![F11030](Images/F11030.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-30: A dataset of two classes'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Here comes the clever part. Suppose we temporarily add a third dimension to
    each point by elevating it by an amount based on that point’s distance from the
    center of the square. [Figure 11-31](#figure11-31) shows the idea.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![F11031](Images/F11031.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-31: If we push each point in [Figure 11-30](#figure11-30) upward
    by an amount based on its distance from the center of the pink blob, we get two
    distinct clouds of points, which we can separate with a plane.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in [Figure 11-31](#figure11-31), we can now draw a plane (the
    2D version of a straight line) between the two sets. In fact, we can use the very
    same idea of support vectors and margins as we did before to find the plane. [Figure
    11-32](#figure11-32) highlights the support vectors for the plane between the
    two clusters of points.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![F11032](Images/F11032.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-32: The support vectors for the plane'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Now all points above the plane can be placed into one class and all those below
    into the other.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: If we highlight the support vectors we found from [Figure 11-32](#figure11-32)
    in our original 2D plot, we get [Figure 11-33](#figure11-33).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![F11033](Images/F11033.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-33: Looking down on [Figure 11-32](#figure11-32)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: If we include the boundary created by the plane, we get [Figure 11-34](#figure11-34).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: In this case we found the right way to modify our data by looking at it and
    then coming up with a good 3D transformation of the data that let us split it
    apart. But when the data has many dimensions, we might not be able to visualize
    it well enough to even guess at a good transformation. Happily, we don’t have
    to find these transformations manually.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: One way to find a good transformation is to try lots and lots of them, and then
    select the one that works the best. The calculations required make this approach
    too slow to be practical, but fortunately, we can speed this up in a clever way.
    This idea focuses on a piece of math called the *kernel,* which lies at the heart
    of the SVM algorithm. Mathematicians sometimes honor a particularly clever or
    neat idea with the complementary term *trick.* In this case, rewriting the SVM
    math is called the *kernel trick* (Bishop 2006). The kernel trick lets the algorithm
    find the distances between transformed points without actually transforming them,
    which is a major efficiency boost. The kernel trick is used automatically by all
    major libraries, so we don’t even have to ask for it (Raschka 2015).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![F11034](Images/F11034.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-34: The data from [Figure 11-30](#figure11-30) with the support vectors,
    the dashed lines showing the margins, and the boundary created by the plane'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at a parametric classifier that’s often used when we need quick results,
    even if they’re not the most accurate.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: This classifier works quickly because it begins by making assumptions about
    the data. It is based on Bayes’ Rule, which we looked at in Chapter 4.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Recall that Bayes’ Rule begins with a *prior*, or a predetermined idea of what
    the result is likely to be. Normally when we use Bayes’ Rule, we refine the prior
    by evaluating new pieces of evidence, creating a *posterior* that then becomes
    our new prior. But what if we just commit to the prior ahead of time and then
    see where it leads us?
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: The *naive Bayes* classifier takes this approach. It’s called *naive* because
    the assumptions we make in our prior are not based on the contents of our data.
    That is, we make an uninformed, or naive, characterization of our data. We just
    assumethat the data has a certain structure. If we’re right, great, we get good
    results. The less well the data matches this assumption, the worse the results
    are. Naive Bayes is popular because this assumption turns out to be correct, or
    nearly correct, often enough that it’s worth taking a look. The interesting thing
    is that we never check to see if our assumption is justified. We just plow ahead
    as if we are certain.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'In one of the more common forms of naive Bayes, we assume that every feature
    of our samples follows a Gaussian distribution. Recall from Chapter 2 that this
    is the famous bell curve: a smooth, symmetrical shape with a central peak. That’s
    our prior. When we look at a specific feature across all of our samples, we simply
    try to match that as well as we can with a Gaussian curve.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: If our features really do follow Gaussian distributions, then this assumption
    produces a good fit. The great thing about naive Bayes is that this assumption
    seems to work well far more frequently than we might expect.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see it in action, starting with data that doessatisfy the prior. [Figure
    11-35](#figure11-35) shows a dataset that was created by drawing samples from
    two Gaussian distributions.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![F11035](Images/F11035.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-35: A set of 2D data for training with naive Bayes. There are two
    classes, red and blue.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: When we give this data to a naive Bayes classifier, it assumes that each set
    of features comes from a Gaussian. That is, it assumes that the x coordinates
    of the red points follow a Gaussian, and the y coordinates of the red points also
    follow a Gaussian. It assumes the same thing about the x and y features of the
    blue points. Then it tries to fit the best four Gaussians it can to that data,
    creating two 2D hills. The result is shown in [Figure 11-36](#figure11-36).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![F11036](Images/F11036.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-36: Naive Bayes fits a Gaussian to each of the x and y features of
    each class. Left: The Gaussian for the red class. Right: The Gaussian for the
    blue class.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: If we overlay the Gaussian blobs and the points and look directly down, as in
    [Figure 11-37](#figure11-37), we can see that they form a very close match. That’s
    no surprise, because we generated the data in a way that had exactly the distribution
    the naive Bayes classifier was expecting.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![F11037](Images/F11037.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-37: Our entire training set overlaid on the Gaussians of [Figure
    11-36](#figure11-36)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: To see how well the classifier works in practice, let’s split the training data,
    putting a randomly selected 70 percent of the points into a training set and the
    rest into a test set. Let’s train with this new training set and then draw the
    test set on top of the Gaussians, giving us [Figure 11-38](#figure11-38).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 11-38](#figure11-38), we drew all the points that were classified
    as belonging to the first class on the left, and all those in the second class
    on the right, maintaining their original colors. We can see that all of the test
    samples were correctly classified.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![F11038](Images/F11038.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-38: The test data after training with 70 percent of our starting
    data'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s try an example where we don’tsatisfy the prior that all features of
    all samples follow Gaussian distributions. [Figure 11-39](#figure11-39) shows
    our new starting data of two noisy crescent moons.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![F11039](Images/F11039.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-39: Some noisy crescent moon data'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: When we give these samples to the naive Bayes classifier, it assumes (as always)
    that the red x values, red y values, blue x values, and blue y values all come
    from Gaussian distributions. It finds the best Gaussians it can, shown in [Figure
    11-40](#figure11-40).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![F11040](Images/F11040.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-40: Fitting Gaussians to the crescent-moon data from [Figure 11-39](#figure11-39)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Of course, these are not a good match to our data, because they don’t satisfy
    the assumptions. Overlaying the data on the Gaussians in [Figure 11-41](#figure11-41)
    shows that the matches aren’t abysmal, but they’re pretty far off.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![F11041](Images/F11041.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-41: Our training data from [Figure 11-39](#figure11-39) overlaid
    on the Gaussians of [Figure 11-40](#figure11-40)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Like before, let’s now split the crescent moons into training and test sets,
    train on the 70 percent, and look at the predictions. In the left image of [Figure
    11-42](#figure11-42) we can see all the points that we assigned to the red class.
    As we would hope, this has most of the red points, but some of the points from
    the upper-left red moon are not classified as red, and some points from the lower-right
    blue moon are classified as red, because their value from this Gaussian is higher
    than their value from the other.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: On the right of [Figure 11-42](#figure11-42), we can see that the opposite situation
    holds for the other Gaussian. In other words, we correctly classified lots of
    the points, but we also misclassified points in each class.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: We shouldn’t be too surprised at the misclassifications because our data did
    not follow the assumptions made by the naive Bayes prior. What is amazing is how
    well the classifier did. In general, naive Bayes often does a good job on all
    kinds of data. This is probably because lots of real-world data comes from processes
    that are well described by Gaussians.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![F11042](Images/F11042.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-42: Predictions of test data from our naive Bayes classifier trained
    on the data in [Figure 11-39](#figure11-39)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Because naive Bayes is so fast, it’s common to apply it when we’re trying to
    get a feeling for our data. If it does a great job, we might not have to look
    at any more complex algorithms.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Classifiers
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We looked at four popular classification algorithms in this chapter. Most machine
    learning libraries offer all of these, along with many others. Very briefly, let’s
    look at the pros and cons of these four classifiers, starting with the nonparametric
    algorithms.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: The kNN method is flexible. It doesn’t explicitly represent boundaries, so it
    can handle any kind of complicated structure formed by the class samples in the
    training data. It’s fast to train, since it typically just saves each training
    sample. On the other hand, prediction is slow, because the algorithm has to search
    for the nearest neighbors for every sample we want to classify (there are many
    efficiency methods that speed up this search, but it still takes time). And because
    it’s saving every training sample, the algorithm can consume huge gulps of memory.
    If the training set is larger than the available memory, the operating system
    typically needs to start saving data on the hard drive (or other external storage),
    which can slow the algorithm down significantly.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are fast to train, and they’re also fast when making predictions.
    They can handle weird boundaries between classes, though this can require a deeper
    tree. They have a huge downside due to their appetite for overfitting (though
    as we mentioned, we will address this issue later by using collections of small
    trees, so all is not lost). Decision trees have a huge appeal in practice because
    they are easy to interpret. Sometimes people use decision trees, even when the
    results are inferior to other classifiers, because their decisions are transparent,
    or easy to understand. Note that this doesn’t mean that the choices are fair or
    even correct, just that they’re comprehensible to humans.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines are parametric algorithms that can make fast predictions.
    Once trained, they don’t need much memory, since they only store the boundaries
    between sample regions. And they can use the kernel trick to find classification
    boundaries that appear much more complicated than the straight lines (and flat
    planes, and higher-dimensional flat surfaces) that SVM produces. On the other
    hand, the training time grows with the size of the training set. The quality of
    the results is sensitive to the parameter *C* that specifies how many samples
    are allowed near the boundary. We can use cross-validation to try different values
    of *C* and pick out the best one.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes trains quickly and predicts quickly, and it’s not too hard to explain
    its results (though they’re a bit more abstract than decision trees or kNN results).
    The method has no parameters that we need to tune. If the classes we’re working
    with are well separated, then the naive Bayes prior often produces good results.
    The algorithm works particularly well when our data is Gaussian in nature. It
    also works well when the data has many features because the classes of such data
    are often separated in ways that play to the strengths of naive Bayes (VanderPlas
    2016). In practice, we often try naive Bayes early in the process of getting to
    know a dataset, because it’s fast to train and predict and can give us a feeling
    for the structure of our data. If the prediction quality is poor, we can then
    turn to a more expensive classifier (that is, one requiring more time or memory).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: The algorithms we’ve seen here are frequently used in practice, particularly
    when we’re first getting to know our data, because they’re usually straightforward
    to apply and visualize.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we covered two types of classifier. When a classifier has no
    preconceptions on the structure of the data it’s going to look at, we say it is
    nonparametric. The *k-*nearest neighbors algorithm is of this variety, assigning
    a class to a sample based on its most popular neighbor. Decision trees are also
    nonparametric, assigning classes based on a series of decisions that are learned
    from the training data.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, parametric classifiers have a preconceived notion of the
    structure of the data. A basic support vector machine looks for linear shapes,
    like lines or planes, that separate the training data by class. A naive Bayes
    classifier presumes that the data has a fixed distribution, usually Gaussian,
    and then does its best to fit that distribution to each feature in the data.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll see how to bundle together multiple classifiers to
    produce ensemble classifiers that outperform their individual components.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
