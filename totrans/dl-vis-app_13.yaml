- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classifiers
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we introduce four important classification algorithms, building
    on the basics of classification that we covered in Chapter 7\. We often use these
    algorithms to help us study and understand our data. In some cases, we can even
    build a final classification system from them. In other cases, we can use the
    understanding we gain from these methods to design a deep learning classifier,
    as discussed in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: We will usually illustrate our classifiers using 2D data and usually only two
    classes because that’s easy to draw and understand, but modern classifiers are
    capable of handling data with any number of dimensions (or features) and huge
    numbers of classes. Modern libraries let us apply most of these algorithms to
    our own data with just a handful of lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Classifiers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we get into specific algorithms, let’s break down the world of classifiers
    into two main approaches: *parametric* and *nonparametric*.'
  prefs: []
  type: TYPE_NORMAL
- en: In the parametric approach, we usually think of the algorithm as starting with
    a preconceived description of the data it’s working with, and it then searches
    for the best parameters of that description to make it fit. For instance, if we
    think that our data follows a normal distribution, we can look for the mean and
    standard deviation that best fit it.
  prefs: []
  type: TYPE_NORMAL
- en: In the nonparametric approach, we let the data lead the way, and we try to come
    up with some way to represent it only after we’ve analyzed it. For example, we
    may look at all of the data and attempt to find a boundary that splits it into
    two or more classes.
  prefs: []
  type: TYPE_NORMAL
- en: In reality, these two approaches are more conceptual than strict. For example,
    we can argue that simply choosing a particular kind of learning algorithm means
    that we’re making assumptions about our data. And we can argue that we’re always
    learning about the data itself by processing it. But these are useful generalizations,
    and we’ll use them to organize our discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by looking at two nonparametric classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: k-Nearest Neighbors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We begin with a nonparametric algorithm called *k-nearest neighbors*, or *kNN*.
    As usual, the letter *k* at the start refers not to a word but to a number. We
    can pick any integer that’s 1 or larger. Because we set this value before the
    algorithm runs, it’s a hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 7, we saw an algorithm called *k-means clustering*. Despite the similarity
    in names, that algorithm and *k*-nearest neighbor are different techniques. One
    key difference is that *k*-means clustering learns from unlabeled data, whereas
    kNN works with labeled data. In other words, *k*-means clustering and kNN fall
    into the classes of unsupervised and supervised learning, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: kNN is fast to train, because all it does is save a copy of every incoming sample
    into a database. The interesting part comes when training is complete, and a new
    sample arrives to be classified. The central idea of how kNN classifies a new
    sample is appealingly geometric, as shown in [Figure 11-1](#figure11-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![F11001](Images/F11001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-1: To find the class for a new sample, shown as a star, we find the
    most popular of its *k* neighbors.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 11-1](#figure11-1)(a) we have a sample point (a star) amid a bunch
    of other samples that represent three classes (circle, square, and triangle).
    To determine a class for our new sample, we look at the *k* nearest samples (or
    *neighbors*), and we count up their classes. Whichever class is most populous
    becomes the new sample’s class. We show which samples are considered for different
    values of *k* by a line to each of the *k* nearest samples. In [Figure 11-1](#figure11-1)(b)
    we’ve set *k* to 1, meaning we want to use the class of the nearest sample. In
    this case it’s a red circle, so this new sample is classified as a circle. In
    [Figure 11-1](#figure11-1)(c) we’ve set *k* to 9, so we look at the nine nearest
    points. Here we find 3 circles, 4 squares, and 2 triangles. Because there are
    more squares than any other class, the star is classified as a square. In [Figure
    11-1](#figure11-1)(d) we’ve set *k* to 25\. Now we have 6 circles, 13 squares,
    and 6 triangles, so again the star is classified as a square.
  prefs: []
  type: TYPE_NORMAL
- en: To sum this up, kNN accepts a new sample to evaluate, along with a value of
    *k*. It then finds the closest *k* samples to the new sample, and we assign the
    new sample to the class with the largest number of representatives among the *k*
    samples we found. There are various ways to break ties and handle exceptional
    cases, but that’s the basic idea.
  prefs: []
  type: TYPE_NORMAL
- en: Note that kNN does not create explicit boundaries between groups of points.
    There’s no notion here of regions or areas that samples belong to. We say that
    kNN is an *on-demand*, or *lazy*, algorithm because it does no processing of the
    samples during the learning stage. When learning, kNN just stashes the samples
    in its internal memory and it’s done.
  prefs: []
  type: TYPE_NORMAL
- en: kNN is attractive because it’s simple, and training it is usually exceptionally
    fast. On the other hand, kNN can require a lot of memory, because (in its basic
    form) it’s saving all the input samples. Using large amounts of memory can slow
    down the algorithm. Another problem is that the classification of new points is
    often slow (compared to other algorithms we’ll see) because of the cost of searching
    for neighbors. Every time we want to classify a new piece of data, we have to
    find its *k* nearest neighbors, which requires work. Of course, there are many
    ways to enhance the algorithm to speed this up, but it still remains a relatively
    slow way to classify. For applications where classification speed is important,
    like real-time systems and websites, the time required by kNN to produce each
    answer can take it out of the running.
  prefs: []
  type: TYPE_NORMAL
- en: Another problem with this technique is that it depends on having lots of neighbors
    nearby (after all, if the nearest neighbors are all very far away, then they don’t
    offer a good proxy for other examples that are like the one we’re trying to classify).
    This means we need a lot of training data. If we have lots of features (that is,
    our data has many dimensions) then kNN quickly succumbs to the curse of dimensionality,
    which we discussed in Chapter 7\. As the dimensionality of the space goes up,
    if we don’t also significantly increase the number of training samples, then the
    number of samples in any local neighborhood drops, making it harder for kNN to
    get a good collection of nearby points.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s put kNN to the test. In [Figure 11-2](#figure11-2) we show a “smile” dataset
    of 2D data that falls into two classes. In this figure, and the similar ones that
    follow, the data is made of points. Since points are hard to see, we’re drawing
    a filled circle around each point as a visual aid.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11002](Images/F11002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-2: A smile dataset of 2D points. There are two classes, blue and
    orange.'
  prefs: []
  type: TYPE_NORMAL
- en: Using kNN with different values of *k* gives us the results in [Figure 11-3](#figure11-3).
  prefs: []
  type: TYPE_NORMAL
- en: '![F11003](Images/F11003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-3: Classifying with the points in [Figure 11-2](#figure11-2) using
    kNN for different values of *k*'
  prefs: []
  type: TYPE_NORMAL
- en: Although kNN doesn’t produce explicit classification boundaries, we can see
    that when *k* is small and we compare our input point with only a few neighbors,
    the space is broken up into sections that share a rather rough border. As *k*
    gets larger and we use more neighbors, that border smooths out because we’re getting
    a better overall picture of the environment around the new sample.
  prefs: []
  type: TYPE_NORMAL
- en: To make things more interesting, let’s add some noise to our data so that the
    edges aren’t so easy to find. [Figure 11-4](#figure11-4) shows a noisy version
    of [Figure 11-2](#figure11-2).
  prefs: []
  type: TYPE_NORMAL
- en: '![F11004](Images/F11004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-4: A noisy version of the smile dataset from [Figure 11-2](#figure11-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The results for different values of *k* are shown in [Figure 11-5](#figure11-5).
  prefs: []
  type: TYPE_NORMAL
- en: '![F11005](Images/F11005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-5: Using kNN and [Figure 11-4](#figure11-4) to assign a class to
    points in the plane'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that in the presence of noise, small values of *k* lead to ragged
    borders. In this example, we have to get *k* up to 50 before we see fairly smooth
    boundaries. Also notice that as *k* increases, the smile shape contracts, since
    the perimeter gets eroded by the larger number of background points.
  prefs: []
  type: TYPE_NORMAL
- en: Because kNN doesn’t explicitly represent the boundaries between classes, it
    can handle any kind of boundaries, or any distribution of classes. To see this,
    let’s add some eyes to our smile, creating three disconnected sets of the same
    class. The resulting noisy data is in [Figure 11-6](#figure11-6).
  prefs: []
  type: TYPE_NORMAL
- en: '![F11006](Images/F11006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-6: A noisy dataset of a smile and two eyes'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting classifications for different values of *k* are shown in [Figure
    11-7](#figure11-7).
  prefs: []
  type: TYPE_NORMAL
- en: In this example, a value of about 20 for *k* looks best. Too small a value of
    *k* can give us ragged edges and noisy results, but too large a value of *k* can
    start to erode the features. As is so often the case, finding the best hyperparameter
    for this algorithm for any given dataset is a matter of repeated experimentation.
    We can use cross-validation to automatically score the quality of each result,
    which is particularly useful when there are many dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'kNN is a great nonparametric algorithm: it’s easy to understand and program,
    and when the dataset isn’t too big, training is extremely fast and classification
    of new data isn’t too slow. But when the dataset gets large, kNN becomes less
    appealing: memory requirements go up because every sample is kept, and classification
    gets slower because the search gets slower. These problems are shared by most
    nonparametric algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F11007](Images/F11007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-7: kNN doesn’t create boundaries between clusters of samples, so
    it works even when a class is broken up into pieces.'
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s consider another nonparametric classification method, called *decision
    trees*. This algorithm builds a data structure from the points in the sample set,
    which is then used to classify new points. Let’s begin by taking a look at this
    structure, and then we’ll see how to build it.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can illustrate the basic idea behind decision trees with the familiar parlor
    game called 20 Questions. In this game, one player (the chooser) thinks of a specific
    target object, which is often a person, place, or thing. The other player (the
    guesser) then asks a series of yes/no questions. If the guesser can correctly
    identify the target in 20 or fewer questions, they win. One reason the game endures
    is that it’s fun to narrow down the enormous number of possible people, places,
    and things to one specific instance with such a small number of simple questions
    (perhaps surprisingly, with 20 yes/no questions we can only distinguish just over
    a million distinct targets).
  prefs: []
  type: TYPE_NORMAL
- en: We can draw a typical game of 20 questions in graphical form, as in [Figure
    11-8](#figure11-8).
  prefs: []
  type: TYPE_NORMAL
- en: '![F11008](Images/F11008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-8: A tree for playing 20 questions. Note that after each decision
    there are exactly two choices, one each for “yes” and “no.”'
  prefs: []
  type: TYPE_NORMAL
- en: We call a structure like [Figure 11-8](#figure11-8) a *tree* because it looks
    something like an upside-down tree. Such trees have a bunch of associated terms
    that are worth knowing. We say that each splitting point in the tree is a *node*,
    and each line connecting nodes is a *link*, *edge*, or *branch*. Following the
    tree analogy, the node at the top is the *root*, and the nodes at the bottom are
    *leaves*, or *terminal nodes*. Nodes between the root and the leaves are called
    *internal nodes* or *decision nodes*.
  prefs: []
  type: TYPE_NORMAL
- en: If a tree has a perfectly symmetrical shape, we say the tree is *balanced*,
    otherwise it’s *unbalanced*. In practice, almost all trees are unbalanced when
    they’re made, but we can run algorithms to make them closer to being balanced
    if a particular application prefers that. We also say that every node has a *depth*,
    which is a number that gives the smallest number of nodes we must go through to
    reach the root. The root has a depth of 0, the nodes immediately below it have
    a depth of 1, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-9](#figure11-9) shows a tree with these labels.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F11009](Images/F11009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-9: Some terminology for a tree'
  prefs: []
  type: TYPE_NORMAL
- en: It’s also common to use the terms associated with family trees, though these
    abstract trees don’t require the union of two nodes to produce children. Every
    node (except the root) has a node above it. We call this the *parent* of that
    node. The nodes immediately below a parent node are its *children*. We sometimes
    distinguish between *immediate children* that are directly connected to a parent,
    and *distant children* that are at the same depth as immediate children, but connected
    to the parent through a sequence of other nodes. If we focus our attention on
    a specific node, then that node and all of its children taken together are called
    a *subtree*. Nodes that share the same immediate parent are called *siblings*.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-10](#figure11-10) shows some of these ideas graphically.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F11010](Images/F11010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-10: Using familiar terms with trees. The green node’s parent is immediately
    above it, and its children are immediately below.'
  prefs: []
  type: TYPE_NORMAL
- en: With this vocabulary in mind, let’s return to our game of 20 Questions.
  prefs: []
  type: TYPE_NORMAL
- en: Using Decision Trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An interesting quality of the 20 Questionstree shown in [Figure 11-8](#figure11-8)
    is that it’s *binary*; every parent node has exactly two children: one for yes,
    one for no. Binary trees are a particularly easy kind of tree for some algorithms
    to work with. If some nodes have more than two children, we say that the tree
    overall is *bushy*. We can always convert a bushy tree to a binary tree if we
    want. An example of a bushy tree that tries to guess the month of someone’s birthday
    is shown on the left of [Figure 11-11](#figure11-11), and the corresponding binary
    tree is shown on the right. Because we can easily go back and forth, we usually
    draw trees in whatever form is most clear and succinct for the discussion at hand.'
  prefs: []
  type: TYPE_NORMAL
- en: We can use trees to classify data. When used this way, the trees are called
    *decision trees*. The full name of the approach is *categorical variable decision
    trees*. This is to distinguish it from those times we use decision trees to work
    with continuous variables, as in regression problems. Those are called *continuous
    variable decision trees*.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11011](Images/F11011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-11: Turning a bushy tree (left) into a binary tree (right)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s stick with the categorical versions here. For simplicity, we’ll just refer
    to them as decision trees, or simply trees, from now on. An example of such a
    tree for sorting inputs into different classes is shown in [Figure 11-12](#figure11-12).
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 11-12](#figure11-12), we start with a root node containing samples
    of different classes, distinguished by their shapes (and colors). To construct
    the tree, we *split* the samples at each node into two groups using some kind
    of test, resulting in a decision. For instance, the test applied at the root node
    might be, “Is this shape rectangular?” The test for the left child might be, “Is
    this shape taller than it is wide?” We’ll soon see how to come up with such tests.
    The goal in this example is to keep splitting each node until we’re left with
    samples of only one class. At that point, we declare that node to be a leaf, and
    stop splitting. In [Figure 11-12](#figure11-12), we’ve split our starting data
    into five classes. It’s essential to remember the test we applied at each node.
    Now, when a new sample arrives, we can start at the root and apply the root’s
    test, then the test of the appropriate child, and so on. When we finally reach
    a leaf, we have determined the class for that sample.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11012](Images/F11012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-12: A categorical decision tree. Each class is a different shape
    and color.'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees don’t start out fully built. Instead, we build the tree based
    on the samples in the training set. When we reach a leaf node during training,
    we test to see if the new training sample has the same class as all the other
    samples in that leaf. If it does, we add the sample to the leaf and we’re done.
    Otherwise, we come up with decision criteria based on some of the features that
    let us distinguish between this sample and the previous samples in the node. We
    then use this test to splitthe node. The test we come up with gets saved with
    the node, we create at least two children, and assign each sample to the appropriate
    child, as in [Figure 11-13](#figure11-13).
  prefs: []
  type: TYPE_NORMAL
- en: When we’re done with training, evaluating new samples is easy. We just start
    at the root and work our way down the tree, following the appropriate branch at
    each node based on that node’s test with this sample’s features. When we land
    in a leaf, we report that the sample belongs to the class of the objects in that
    leaf.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11013](Images/F11013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-13: Splitting a node by applying a test to its contents'
  prefs: []
  type: TYPE_NORMAL
- en: This is an idealized process. In practice, our tests can be imperfect and our
    leaves might contain a mixture of objects of different classes if, for efficiency
    or memory reasons, we chose not to split some leaves any more. For example, if
    we land in a node that contains samples that are 80 percent from class A and 20
    percent from class B, we might report that the new sample has an 80 percent chance
    of being in A and a 20 percent chance of being in B. If we have to report just
    one class, we might report that it’s A 80 percent of the time, and B the other
    20 percent.
  prefs: []
  type: TYPE_NORMAL
- en: This process of making a decision tree works with just one sample at a time.
    In the simplest version of this technique, we don’t consider the entire training
    set at once and try to find, say, the smallest or most balanced tree that classifies
    the samples. Instead, we consider one sample at a time and split the nodes in
    the tree as required to handle that sample. Then we do the same for the next sample,
    and the next, and so on, making decisions in the moment without caring about any
    other data yet to come. This makes for efficient training.
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm makes decisions based only on the data it has seen before and
    the data currently under consideration. It doesn’t try to plan or strategize for
    the future based on what it has seen so far. We call this a *greedy* algorithm,
    since it’s focused on maximizing its immediate, short-term gains.
  prefs: []
  type: TYPE_NORMAL
- en: Decisions trees are sometimes preferred in practice over other classifiers because
    their results are *explainable*. When the algorithm assigns a class to a sample,
    we don’t have to unravel some complex mathematical or algorithmic process. Instead,
    we can fully explain the final result just by identifying each decision along
    the way. This can be important in real life, too.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we apply for a loan at a bank, but we’re turned down. When
    we ask for the reason, the bank can show us each test made along the way. We say
    that the workings of the algorithm are *transparent*. Note that this doesn’t mean
    that they’re fair or reasonable. The bank may have come up with tests that are
    biased against one or more social groups or that depend on what seem to be irrelevant
    criteria. Just because they can explain why they made their choice doesn’t make
    the process or the results satisfactory. Legislators in particular seem to prefer
    laws that enforce transparency, which is easy to demonstrate, over fairness, which
    is much harder. Transparency is nice to have, but it doesn’t mean a system is
    behaving the way we’d like it to.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees can make bad decisions because they are particularly prone to
    overfitting. Let’s see why.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting Trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s begin our discussion of overfitting in the tree-building process by considering
    a couple of examples.
  prefs: []
  type: TYPE_NORMAL
- en: The data in [Figure 11-14](#figure11-14) shows a cleanly separated set of data
    representing two classes. A dataset that roughly follows this kind of geometry
    is often called a *two-moons dataset*, presumably because the semicircles reminded
    someone of crescent moons.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11014](Images/F11014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-14: Our 600 starting data points for building a decision tree, arranged
    in a two-moons structure. These 2D points represent two classes, blue and orange.'
  prefs: []
  type: TYPE_NORMAL
- en: Each step in building a tree potentially involves splitting a leaf, and thus
    replacing it with a node and two leaves, for a net increase to the tree of one
    internal node and one leaf. We often think about the size of our tree in terms
    of the number of leaves it contains.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-15](#figure11-15) shows the process of building a decision tree
    for this data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F11015](Images/F11015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-15: Building a decision tree for the data in [Figure 11-14](#figure11-14).
    Notice how the tree starts with big chunks and refines them into smaller and more
    precise regions.'
  prefs: []
  type: TYPE_NORMAL
- en: In this figure, we drew the dataset over each image just for reference. In this
    example, each node corresponds to a box. We don’t show it here, but the tree begins
    with just a single root corresponding to a blue box that covers the entire region.
    Then the training process receives one of the orange points near the top of the
    orange curve. This isn’t in the blue class, so we split the root with a horizontal
    cut into two boxes, shown in the upper left. The next point that comes in is a
    blue point near the left part of the blue curve. This falls in the orange box,
    so we split that with a vertical cut into two boxes as shown, giving us a total
    of three leaves. The rest of the figure shows the evolving tree as more samples
    arrive.
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the regions gradually refine as the tree grows in response to more
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: This tree needs only 12 leaves to correctly classify every training sample.
    The final tree and the original data are shown together in [Figure 11-16](#figure11-16).
    This tree fits the data perfectly.
  prefs: []
  type: TYPE_NORMAL
- en: Note the two horizontal, thin rectangles. They enclose two orange samples at
    the top of the left side of the arc and manage to slip between the blue points
    (recall that the samples are points in the center of each circle). This is overfitting,
    because any future points that fall into those rectangles, despite the fact that
    they’re both almost completely in a blue region, will be classified as orange.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11016](Images/F11016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-16: Our final tree with 12 leaves'
  prefs: []
  type: TYPE_NORMAL
- en: Because decision trees are so sensitive to each input sample, they have a profound
    tendency to overfit. In fact, decision trees almost always overfit, because every
    training sample can influence the tree’s shape. To see this, take a look at [Figure
    11-17](#figure11-17). Here we ran the same algorithm as for [Figure 11-16](#figure11-16)
    two times, but in each case, we used a different, randomly chosen 70 percent of
    the input data.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11017](Images/F11017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-17: Decision trees are very sensitive to their inputs. (Left) We
    randomly chose 70 percent of the samples from [Figure 11-14](#figure11-14) and
    fit a tree. (Right) The same process, but for a different randomly selected 70
    percent of the original samples.'
  prefs: []
  type: TYPE_NORMAL
- en: These two decision trees are similar, but definitely not identical. The tendency
    of decision trees to overfit is much more pronounced when the data isn’t so easily
    separated. Let’s look at an example of that now.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-18](#figure11-18) shows another pair of crescent moons, but this
    time we added lots of noise to the samples after they had their classes assigned.
    The two classes no longer have a clean boundary.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F11018](Images/F11018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-18: A noisy set of 600 samples for building decision trees'
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a tree to this data starts out with big regions, but it rapidly turns
    into a complicated set of tiny boxes as the algorithm splits up nodes this way
    and that to match the noisy data. [Figure 11-19](#figure11-19) shows the result.
    In this case, it required 100 leaves for the tree to correctly classify the points.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11019](Images/F11019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-19: The tree-building process. Note that the second row uses large
    numbers of leaves.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-20](#figure11-20) shows a close-up of the final tree and the original
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F11020](Images/F11020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-20: Our noisy data fit with a tree with 100 leaves. Notice how many
    little boxes have been used to catch just an odd sample here and there.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s a lot of overfitting here. Though we expect most of the samples in the
    lower right to be orange and most of those in the upper left to be blue, this
    tree has carved out a lot of exceptions based on this particular dataset. Future
    samples that fall into those little boxes are likely to be misclassified.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s repeat our process of building trees using different, random 70 percent
    selections of the data in [Figure 11-18](#figure11-18). [Figure 11-21](#figure11-21)
    shows the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11021](Images/F11021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-21: A couple of trees built with different sets of 70 percent of
    the samples in [Figure 11-18](#figure11-18)'
  prefs: []
  type: TYPE_NORMAL
- en: There are similarities, but these trees are significantly different, and lots
    of little bits exist only to classify a few samples. This is overfitting in action.
  prefs: []
  type: TYPE_NORMAL
- en: Although this may look pretty bad for the decision tree method, in Chapter 12
    we’ll see that by combining many simple decision trees into a group, or an *ensemble*,
    we can create robust, efficient classifiers that don’t suffer as much from overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few other ways we can control overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in [Figure 11-15](#figure11-15) and [Figure 11-19](#figure11-19),
    the first few steps of the tree’s growth tend to generate big, general shapes.
    It’s only when the tree gets very deep that we get the tiny boxes that are symptomatic
    of overfitting. One popular strategy to reducing overfitting is *depth limiting:*
    we simply limit the tree’s depth while it’s building. If a node is more than a
    given number of steps from the root, we just declare it a leaf and don’t split
    it any more. A different strategy is setting a minimum sample requirement so that
    we never split a node that has less than a certain number of samples, no matter
    how mixed they are.
  prefs: []
  type: TYPE_NORMAL
- en: Yet another approach to reducing overfitting is to reduce the size of the tree
    after it’s made in a process called *pruning*. This works by removing, or *trimming*,
    leaf nodes. We look at each leaf and characterize what would happen to the total
    error of the tree’s results if we removed that leaf. If the error is acceptable,
    we simply remove the leaf from the tree. If we remove all the children of a node,
    then it becomes a leaf itself, and a candidate for further pruning. Pruning a
    tree can make it shallower, which offers the additional benefit of also making
    it faster when we classify new data.
  prefs: []
  type: TYPE_NORMAL
- en: Depth limiting, setting minimum sample requirements per node, and pruning all
    simplify the tree, but because they do so in different ways, they usually give
    us different results.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting Nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we leave decision trees, let’s return briefly to the node-splitting
    process, since many machine learning libraries offer us a choice of splitting
    algorithms to choose from. Here are two questions to ask when we consider a node:
    First, does it need to be split? Second, how should we split it? Let’s take these
    in order.'
  prefs: []
  type: TYPE_NORMAL
- en: When we ask if a node needs to be split, we usually consider to what extent
    all the samples in a given node are of the same class. We describe the uniformity
    of a node’s contents with a number called that node’s *purity*. If all the samples
    are in the same class, the node is completely pure. The more samples we have of
    other classes, the smaller the value of purity becomes. To test if a node needs
    splitting, we can check the purity against a threshold. If the node is too *impure*,
    meaning that the purity is below the threshold, we split it.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can look at how to split the node. If our samples have many features,
    we can invent lots of different possible splitting tests. We can test the value
    of just one feature and ignore the others. We can look at groups of features and
    test on some aggregated values from them. We’re free to choose a completely different
    test at every node based on different features. This gives us a huge variety of
    possible tests to consider.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-22](#figure11-22) shows a node containing a mix of circles of different
    sizes and colors. Let’s try to get all the reddish objects in one child and all
    the bluish ones in another. When we just look at the data (usually the best first
    step with any new database), it seems like the reddish circles are the biggest.
    Let’s try using a test based on the radius of each circle. The figure shows the
    result of splitting on the radius using three different values.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F11022](Images/F11022.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-22: Splitting a node according to different values of the radii of
    the circles within it'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the radius value of 70 produces the purest results, with all
    the blue objects in one child and all the red ones in the other. If we use this
    test for this node, we’ll remember which feature we’re splitting on (the radius)
    and what value to test for (70).
  prefs: []
  type: TYPE_NORMAL
- en: Since we could potentially split the node using a test based on any characteristic
    of the samples, we need some way to evaluate the results so we can pick the best
    test. Let’s look at two popular ways to test these results.
  prefs: []
  type: TYPE_NORMAL
- en: Recall from Chapter 6 that *entropy* is a measure of complexity, or how many
    bits it takes to communicate some piece of information. The *Information Gain
    (IG)* measure uses this idea by comparing the entropy of a node to that of the
    children produced by each candidate test.
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate a test, IG adds together the entropies of all the new children produced
    by that test and compares that result to the entropy in the parent cell. The more
    pure a cell is, the lower its entropy, so if a test makes pure cells, the sum
    of their entropies is less than the entropy of their parent. After trying different
    ways to split a node, we choose the split that gives us the biggest reduction
    in entropy (or the biggest gain in information).
  prefs: []
  type: TYPE_NORMAL
- en: Another popular way to evaluate splitting tests is called the *Gini impurity*.
    The math used by this technique is designed to minimize the probability of misclassifying
    a sample. For example, suppose a leaf has 10 samples of class A and 90 samples
    of class B. If a new sample ends up at that leaf, and we report that it belongs
    to class B, there is a 10 percent chance that we are wrong. Gini impurity measures
    those errors at each leaf for multiple candidate split values. It then chooses
    the split that has the least chance of an erroneous classification.
  prefs: []
  type: TYPE_NORMAL
- en: Some libraries offer other measures for grading the quality of a potential split.
    As with so many other choices, we usually try out a few options and pick the one
    that works the best for the specific data we’re working with.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s consider our first parametric algorithm: the *support vector machine*
    (or *SVM*). We will use 2D data and just two classes for our illustrations (VanderPlas
    2016), but like most machine learning algorithms, the ideas are easily applied
    to data with any number of dimensions and classes.'
  prefs: []
  type: TYPE_NORMAL
- en: The Basic Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s begin with two blobs of points, one for each of two classes, shown in
    [Figure 11-23](#figure11-23).
  prefs: []
  type: TYPE_NORMAL
- en: '![F11023](Images/F11023.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-23: Our starting dataset consists of two blobs of 2D samples.'
  prefs: []
  type: TYPE_NORMAL
- en: We want to find a boundary between these clusters. To keep things simple, let’s
    use a straight line. But which one? A lot of lines split these two groups. Three
    candidates are shown in [Figure 11-24](#figure11-24).
  prefs: []
  type: TYPE_NORMAL
- en: '![F11024](Images/F11024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-24: Three of the infinite number of lines that can separate our two
    clusters of samples'
  prefs: []
  type: TYPE_NORMAL
- en: Which of these lines should we pick? One way to think about this is to imagine
    new data that might come in. Generally speaking, we want to classify any new sample
    as belonging to the class of the sample that it is nearest.
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate how well any given boundary line achieves this goal, let’s find
    its distance to the nearest sample of either class. We can use this distance to
    draw a symmetrical boundary around the line. [Figure 11-25](#figure11-25) shows
    the idea for a few different lines.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11025](Images/F11025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-25: We can assign a quality to each line by finding the distance
    from that line to the nearest data point.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 11-25](#figure11-25), we’ve drawn a circle around the sample that’s
    closest to the line. In the leftmost figure, many new points that are closer to
    the lower-right cluster than the upper-left one would be incorrectly classified
    as part of the upper-left cluster. The same situation holds in the rightmost figure.
    In the center, the line is much better, but it’s now preferring the lower-right
    cluster a little. Since we want each new point to be assigned to the class of
    the sample it’s closest to, we’d like our line to go right through the middle
    of the two clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm that finds this line is called a *support vector machine*, or
    *SVM* (Steinwart 2008). An SVM finds the line that is farthest from all the points
    in both clusters. In this context, the word *support* can be thought of as meaning
    “nearest,” *vector* is a synonym for “sample,” and *machine* is a synonym for
    “algorithm.” Thus, we can describe SVM as the “nearest sample algorithm.”
  prefs: []
  type: TYPE_NORMAL
- en: The best line for our two clusters in [Figure 11-23](#figure11-23), as calculated
    by SVM, is shown in [Figure 11-26](#figure11-26).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how SVM finds this line.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 11-26](#figure11-26) the circled samples are the nearest samples,
    or the *support vectors*. The algorithm’s first job is to locate these circled
    points. Once it finds them, the algorithm then finds the solid line near the middle
    of the figure. Of all the lines that separate the two sets of points, this is
    the line that’s farthest from every sample in each set, because it has the greatest
    distance to its support vectors. The dashed lines in [Figure 11-26](#figure11-26),
    like the circles around the support vectors, are just visual aids to help us see
    that the solid line in the center, found by the SVM, is the one that is as far
    as possible from all the samples. The distance from the solid line to the dashed
    lines that pass through the support vectors is called the *margin*. We can rephrase
    the idea by saying that the SVM algorithm finds the line with the largest margin.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11026](Images/F11026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-26: The SVM algorithm finds the line that has the greatest distance
    from all the samples.'
  prefs: []
  type: TYPE_NORMAL
- en: What if the data is noisy, and the blobs overlap, as in [Figure 11-27](#figure11-27)?
    Now we can’t create a line surrounded by an empty zone. What’s the best line to
    draw through these overlapping sets of samples?
  prefs: []
  type: TYPE_NORMAL
- en: '![F11027](Images/F11027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-27: A new set of data where the blobs overlap'
  prefs: []
  type: TYPE_NORMAL
- en: The SVM algorithm gives us control over a parameter that’s conventionally called
    *C*. This parameter controls how strict the algorithm is about letting points
    into the region between the margins. The larger the value of *C*, the more the
    algorithm demands an empty zone around the line. The smaller the value of *C*,
    the more points can appear in a zone around the line. We frequently need to search
    for the best value for *C* using trial and error. In practice, that usually means
    trying out lots of values and evaluating them with cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-28](#figure11-28) shows our overlapping data with a *C* value of
    100,000.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F11028](Images/F11028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-28: The value of *C* tells SVM how sensitive to be to points that
    can intrude into the zone around the line that’s fit to the data. Here, *C* is
    100,000.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s drop *C* way down to 0.01\. [Figure 11-29](#figure11-29) shows that this
    lets fewer points into the region around the line.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11029](Images/F11029.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-29: Lowering *C* to 0.01 lets in fewer points compared to [Figure
    11-28](#figure11-28).'
  prefs: []
  type: TYPE_NORMAL
- en: The lines in [Figure 11-28](#figure11-28) and [Figure 11-29](#figure11-29) are
    different. Which one we prefer depends on what we want from our classifier. If
    we think the best boundary comes from the details near the zone where the points
    overlap, we want a small value of *C* so that we only look at the points near
    that boundary. If we think the overall shapes of the two collections of points
    is a better descriptor of that boundary, we want a larger value of *C* to include
    more of those farther-away points.
  prefs: []
  type: TYPE_NORMAL
- en: The SVM Kernel Trick
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A parametric algorithm is limited by the shapes it’s able to find. SVM, for
    instance, can only find *linear* shapes, like lines and planes. If we have data
    that can’t obviously be separated by such a shape, it may seem that SVM won’t
    be of much use. But there’s a clever trick that can sometimes let us use a linear
    boundary where it initially appears as if only a curved one could do the job.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we have the data of [Figure 11-30](#figure11-30), where there’s
    a blob of samples of one class surrounded by a ring of samples of another. There’s
    no way we can draw a straight line to separate these two sets.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11030](Images/F11030.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-30: A dataset of two classes'
  prefs: []
  type: TYPE_NORMAL
- en: Here comes the clever part. Suppose we temporarily add a third dimension to
    each point by elevating it by an amount based on that point’s distance from the
    center of the square. [Figure 11-31](#figure11-31) shows the idea.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11031](Images/F11031.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-31: If we push each point in [Figure 11-30](#figure11-30) upward
    by an amount based on its distance from the center of the pink blob, we get two
    distinct clouds of points, which we can separate with a plane.'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in [Figure 11-31](#figure11-31), we can now draw a plane (the
    2D version of a straight line) between the two sets. In fact, we can use the very
    same idea of support vectors and margins as we did before to find the plane. [Figure
    11-32](#figure11-32) highlights the support vectors for the plane between the
    two clusters of points.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11032](Images/F11032.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-32: The support vectors for the plane'
  prefs: []
  type: TYPE_NORMAL
- en: Now all points above the plane can be placed into one class and all those below
    into the other.
  prefs: []
  type: TYPE_NORMAL
- en: If we highlight the support vectors we found from [Figure 11-32](#figure11-32)
    in our original 2D plot, we get [Figure 11-33](#figure11-33).
  prefs: []
  type: TYPE_NORMAL
- en: '![F11033](Images/F11033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-33: Looking down on [Figure 11-32](#figure11-32)'
  prefs: []
  type: TYPE_NORMAL
- en: If we include the boundary created by the plane, we get [Figure 11-34](#figure11-34).
  prefs: []
  type: TYPE_NORMAL
- en: In this case we found the right way to modify our data by looking at it and
    then coming up with a good 3D transformation of the data that let us split it
    apart. But when the data has many dimensions, we might not be able to visualize
    it well enough to even guess at a good transformation. Happily, we don’t have
    to find these transformations manually.
  prefs: []
  type: TYPE_NORMAL
- en: One way to find a good transformation is to try lots and lots of them, and then
    select the one that works the best. The calculations required make this approach
    too slow to be practical, but fortunately, we can speed this up in a clever way.
    This idea focuses on a piece of math called the *kernel,* which lies at the heart
    of the SVM algorithm. Mathematicians sometimes honor a particularly clever or
    neat idea with the complementary term *trick.* In this case, rewriting the SVM
    math is called the *kernel trick* (Bishop 2006). The kernel trick lets the algorithm
    find the distances between transformed points without actually transforming them,
    which is a major efficiency boost. The kernel trick is used automatically by all
    major libraries, so we don’t even have to ask for it (Raschka 2015).
  prefs: []
  type: TYPE_NORMAL
- en: '![F11034](Images/F11034.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-34: The data from [Figure 11-30](#figure11-30) with the support vectors,
    the dashed lines showing the margins, and the boundary created by the plane'
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at a parametric classifier that’s often used when we need quick results,
    even if they’re not the most accurate.
  prefs: []
  type: TYPE_NORMAL
- en: This classifier works quickly because it begins by making assumptions about
    the data. It is based on Bayes’ Rule, which we looked at in Chapter 4.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that Bayes’ Rule begins with a *prior*, or a predetermined idea of what
    the result is likely to be. Normally when we use Bayes’ Rule, we refine the prior
    by evaluating new pieces of evidence, creating a *posterior* that then becomes
    our new prior. But what if we just commit to the prior ahead of time and then
    see where it leads us?
  prefs: []
  type: TYPE_NORMAL
- en: The *naive Bayes* classifier takes this approach. It’s called *naive* because
    the assumptions we make in our prior are not based on the contents of our data.
    That is, we make an uninformed, or naive, characterization of our data. We just
    assumethat the data has a certain structure. If we’re right, great, we get good
    results. The less well the data matches this assumption, the worse the results
    are. Naive Bayes is popular because this assumption turns out to be correct, or
    nearly correct, often enough that it’s worth taking a look. The interesting thing
    is that we never check to see if our assumption is justified. We just plow ahead
    as if we are certain.
  prefs: []
  type: TYPE_NORMAL
- en: 'In one of the more common forms of naive Bayes, we assume that every feature
    of our samples follows a Gaussian distribution. Recall from Chapter 2 that this
    is the famous bell curve: a smooth, symmetrical shape with a central peak. That’s
    our prior. When we look at a specific feature across all of our samples, we simply
    try to match that as well as we can with a Gaussian curve.'
  prefs: []
  type: TYPE_NORMAL
- en: If our features really do follow Gaussian distributions, then this assumption
    produces a good fit. The great thing about naive Bayes is that this assumption
    seems to work well far more frequently than we might expect.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see it in action, starting with data that doessatisfy the prior. [Figure
    11-35](#figure11-35) shows a dataset that was created by drawing samples from
    two Gaussian distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11035](Images/F11035.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-35: A set of 2D data for training with naive Bayes. There are two
    classes, red and blue.'
  prefs: []
  type: TYPE_NORMAL
- en: When we give this data to a naive Bayes classifier, it assumes that each set
    of features comes from a Gaussian. That is, it assumes that the x coordinates
    of the red points follow a Gaussian, and the y coordinates of the red points also
    follow a Gaussian. It assumes the same thing about the x and y features of the
    blue points. Then it tries to fit the best four Gaussians it can to that data,
    creating two 2D hills. The result is shown in [Figure 11-36](#figure11-36).
  prefs: []
  type: TYPE_NORMAL
- en: '![F11036](Images/F11036.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-36: Naive Bayes fits a Gaussian to each of the x and y features of
    each class. Left: The Gaussian for the red class. Right: The Gaussian for the
    blue class.'
  prefs: []
  type: TYPE_NORMAL
- en: If we overlay the Gaussian blobs and the points and look directly down, as in
    [Figure 11-37](#figure11-37), we can see that they form a very close match. That’s
    no surprise, because we generated the data in a way that had exactly the distribution
    the naive Bayes classifier was expecting.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11037](Images/F11037.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-37: Our entire training set overlaid on the Gaussians of [Figure
    11-36](#figure11-36)'
  prefs: []
  type: TYPE_NORMAL
- en: To see how well the classifier works in practice, let’s split the training data,
    putting a randomly selected 70 percent of the points into a training set and the
    rest into a test set. Let’s train with this new training set and then draw the
    test set on top of the Gaussians, giving us [Figure 11-38](#figure11-38).
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 11-38](#figure11-38), we drew all the points that were classified
    as belonging to the first class on the left, and all those in the second class
    on the right, maintaining their original colors. We can see that all of the test
    samples were correctly classified.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11038](Images/F11038.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-38: The test data after training with 70 percent of our starting
    data'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s try an example where we don’tsatisfy the prior that all features of
    all samples follow Gaussian distributions. [Figure 11-39](#figure11-39) shows
    our new starting data of two noisy crescent moons.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11039](Images/F11039.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-39: Some noisy crescent moon data'
  prefs: []
  type: TYPE_NORMAL
- en: When we give these samples to the naive Bayes classifier, it assumes (as always)
    that the red x values, red y values, blue x values, and blue y values all come
    from Gaussian distributions. It finds the best Gaussians it can, shown in [Figure
    11-40](#figure11-40).
  prefs: []
  type: TYPE_NORMAL
- en: '![F11040](Images/F11040.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-40: Fitting Gaussians to the crescent-moon data from [Figure 11-39](#figure11-39)'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, these are not a good match to our data, because they don’t satisfy
    the assumptions. Overlaying the data on the Gaussians in [Figure 11-41](#figure11-41)
    shows that the matches aren’t abysmal, but they’re pretty far off.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11041](Images/F11041.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-41: Our training data from [Figure 11-39](#figure11-39) overlaid
    on the Gaussians of [Figure 11-40](#figure11-40)'
  prefs: []
  type: TYPE_NORMAL
- en: Like before, let’s now split the crescent moons into training and test sets,
    train on the 70 percent, and look at the predictions. In the left image of [Figure
    11-42](#figure11-42) we can see all the points that we assigned to the red class.
    As we would hope, this has most of the red points, but some of the points from
    the upper-left red moon are not classified as red, and some points from the lower-right
    blue moon are classified as red, because their value from this Gaussian is higher
    than their value from the other.
  prefs: []
  type: TYPE_NORMAL
- en: On the right of [Figure 11-42](#figure11-42), we can see that the opposite situation
    holds for the other Gaussian. In other words, we correctly classified lots of
    the points, but we also misclassified points in each class.
  prefs: []
  type: TYPE_NORMAL
- en: We shouldn’t be too surprised at the misclassifications because our data did
    not follow the assumptions made by the naive Bayes prior. What is amazing is how
    well the classifier did. In general, naive Bayes often does a good job on all
    kinds of data. This is probably because lots of real-world data comes from processes
    that are well described by Gaussians.
  prefs: []
  type: TYPE_NORMAL
- en: '![F11042](Images/F11042.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-42: Predictions of test data from our naive Bayes classifier trained
    on the data in [Figure 11-39](#figure11-39)'
  prefs: []
  type: TYPE_NORMAL
- en: Because naive Bayes is so fast, it’s common to apply it when we’re trying to
    get a feeling for our data. If it does a great job, we might not have to look
    at any more complex algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Classifiers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We looked at four popular classification algorithms in this chapter. Most machine
    learning libraries offer all of these, along with many others. Very briefly, let’s
    look at the pros and cons of these four classifiers, starting with the nonparametric
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The kNN method is flexible. It doesn’t explicitly represent boundaries, so it
    can handle any kind of complicated structure formed by the class samples in the
    training data. It’s fast to train, since it typically just saves each training
    sample. On the other hand, prediction is slow, because the algorithm has to search
    for the nearest neighbors for every sample we want to classify (there are many
    efficiency methods that speed up this search, but it still takes time). And because
    it’s saving every training sample, the algorithm can consume huge gulps of memory.
    If the training set is larger than the available memory, the operating system
    typically needs to start saving data on the hard drive (or other external storage),
    which can slow the algorithm down significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are fast to train, and they’re also fast when making predictions.
    They can handle weird boundaries between classes, though this can require a deeper
    tree. They have a huge downside due to their appetite for overfitting (though
    as we mentioned, we will address this issue later by using collections of small
    trees, so all is not lost). Decision trees have a huge appeal in practice because
    they are easy to interpret. Sometimes people use decision trees, even when the
    results are inferior to other classifiers, because their decisions are transparent,
    or easy to understand. Note that this doesn’t mean that the choices are fair or
    even correct, just that they’re comprehensible to humans.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines are parametric algorithms that can make fast predictions.
    Once trained, they don’t need much memory, since they only store the boundaries
    between sample regions. And they can use the kernel trick to find classification
    boundaries that appear much more complicated than the straight lines (and flat
    planes, and higher-dimensional flat surfaces) that SVM produces. On the other
    hand, the training time grows with the size of the training set. The quality of
    the results is sensitive to the parameter *C* that specifies how many samples
    are allowed near the boundary. We can use cross-validation to try different values
    of *C* and pick out the best one.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes trains quickly and predicts quickly, and it’s not too hard to explain
    its results (though they’re a bit more abstract than decision trees or kNN results).
    The method has no parameters that we need to tune. If the classes we’re working
    with are well separated, then the naive Bayes prior often produces good results.
    The algorithm works particularly well when our data is Gaussian in nature. It
    also works well when the data has many features because the classes of such data
    are often separated in ways that play to the strengths of naive Bayes (VanderPlas
    2016). In practice, we often try naive Bayes early in the process of getting to
    know a dataset, because it’s fast to train and predict and can give us a feeling
    for the structure of our data. If the prediction quality is poor, we can then
    turn to a more expensive classifier (that is, one requiring more time or memory).
  prefs: []
  type: TYPE_NORMAL
- en: The algorithms we’ve seen here are frequently used in practice, particularly
    when we’re first getting to know our data, because they’re usually straightforward
    to apply and visualize.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we covered two types of classifier. When a classifier has no
    preconceptions on the structure of the data it’s going to look at, we say it is
    nonparametric. The *k-*nearest neighbors algorithm is of this variety, assigning
    a class to a sample based on its most popular neighbor. Decision trees are also
    nonparametric, assigning classes based on a series of decisions that are learned
    from the training data.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, parametric classifiers have a preconceived notion of the
    structure of the data. A basic support vector machine looks for linear shapes,
    like lines or planes, that separate the training data by class. A naive Bayes
    classifier presumes that the data has a fixed distribution, usually Gaussian,
    and then does its best to fit that distribution to each feature in the data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll see how to bundle together multiple classifiers to
    produce ensemble classifiers that outperform their individual components.
  prefs: []
  type: TYPE_NORMAL
