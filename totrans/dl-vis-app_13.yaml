- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Classifiers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: In this chapter, we introduce four important classification algorithms, building
    on the basics of classification that we covered in Chapter 7\. We often use these
    algorithms to help us study and understand our data. In some cases, we can even
    build a final classification system from them. In other cases, we can use the
    understanding we gain from these methods to design a deep learning classifier,
    as discussed in later chapters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了四种重要的分类算法，基于我们在第7章中涵盖的分类基础知识。我们通常使用这些算法来帮助我们研究和理解数据。在某些情况下，我们甚至可以根据这些算法构建最终的分类系统。在其他情况下，我们可以利用这些方法获得的理解来设计深度学习分类器，如后续章节中所讨论的那样。
- en: We will usually illustrate our classifiers using 2D data and usually only two
    classes because that’s easy to draw and understand, but modern classifiers are
    capable of handling data with any number of dimensions (or features) and huge
    numbers of classes. Modern libraries let us apply most of these algorithms to
    our own data with just a handful of lines of code.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常会使用二维数据并且通常只有两个类别来说明我们的分类器，因为这样便于绘制和理解，但现代分类器能够处理任意维度（或特征）和大量类别的数据。现代库使我们可以用少量代码将这些算法应用于我们自己的数据。
- en: Types of Classifiers
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类器的类型
- en: 'Before we get into specific algorithms, let’s break down the world of classifiers
    into two main approaches: *parametric* and *nonparametric*.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨具体算法之前，我们先将分类器的世界分为两种主要方法：*参数方法*和*非参数方法*。
- en: In the parametric approach, we usually think of the algorithm as starting with
    a preconceived description of the data it’s working with, and it then searches
    for the best parameters of that description to make it fit. For instance, if we
    think that our data follows a normal distribution, we can look for the mean and
    standard deviation that best fit it.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在参数方法中，我们通常认为算法以对其处理的数据的预先设定描述开始，然后搜索该描述的最佳参数以使其适配。例如，如果我们认为我们的数据符合正态分布，我们可以寻找最适合它的均值和标准差。
- en: In the nonparametric approach, we let the data lead the way, and we try to come
    up with some way to represent it only after we’ve analyzed it. For example, we
    may look at all of the data and attempt to find a boundary that splits it into
    two or more classes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在非参数方法中，我们让数据引导我们，在分析数据后，我们才尝试找到某种方式来表示它。例如，我们可能会查看所有数据，试图找到一个边界，将其分成两个或多个类别。
- en: In reality, these two approaches are more conceptual than strict. For example,
    we can argue that simply choosing a particular kind of learning algorithm means
    that we’re making assumptions about our data. And we can argue that we’re always
    learning about the data itself by processing it. But these are useful generalizations,
    and we’ll use them to organize our discussion.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这两种方法更像是概念性的，而不是严格的。例如，我们可以认为仅仅选择一种特定类型的学习算法就意味着我们在对数据做出假设。而我们也可以认为我们总是在通过处理数据来学习数据本身。但这些是有用的概括，我们将用它们来组织我们的讨论。
- en: Let’s start by looking at two nonparametric classifiers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从两个非参数分类器开始。
- en: k-Nearest Neighbors
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-最近邻
- en: We begin with a nonparametric algorithm called *k-nearest neighbors*, or *kNN*.
    As usual, the letter *k* at the start refers not to a word but to a number. We
    can pick any integer that’s 1 or larger. Because we set this value before the
    algorithm runs, it’s a hyperparameter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一种非参数算法开始，叫做*k-最近邻*，或称*kNN*。像往常一样，字母*k*代表的不是一个词，而是一个数字。我们可以选择任何大于或等于1的整数。因为在算法运行之前设置了这个值，所以它是一个超参数。
- en: In Chapter 7, we saw an algorithm called *k-means clustering*. Despite the similarity
    in names, that algorithm and *k*-nearest neighbor are different techniques. One
    key difference is that *k*-means clustering learns from unlabeled data, whereas
    kNN works with labeled data. In other words, *k*-means clustering and kNN fall
    into the classes of unsupervised and supervised learning, respectively.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我们看到了一个叫做*k-均值聚类*的算法。尽管名字相似，但该算法和*k*-最近邻是不同的技术。一个关键的区别是，*k*-均值聚类从未标记的数据中学习，而kNN则处理带标签的数据。换句话说，*k*-均值聚类和kNN分别属于无监督学习和监督学习。
- en: kNN is fast to train, because all it does is save a copy of every incoming sample
    into a database. The interesting part comes when training is complete, and a new
    sample arrives to be classified. The central idea of how kNN classifies a new
    sample is appealingly geometric, as shown in [Figure 11-1](#figure11-1).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: kNN训练速度很快，因为它所做的就是将每个传入的样本复制到数据库中。真正有趣的部分发生在训练完成后，当一个新样本到来需要分类时。kNN分类新样本的核心思想是几何上的，这一点在[图11-1](#figure11-1)中得到了很好的展示。
- en: '![F11001](Images/F11001.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![F11001](Images/F11001.png)'
- en: 'Figure 11-1: To find the class for a new sample, shown as a star, we find the
    most popular of its *k* neighbors.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图11-1：为了找到新样本（以星号表示）的类别，我们寻找其*k*个邻居中最常见的类别。
- en: In [Figure 11-1](#figure11-1)(a) we have a sample point (a star) amid a bunch
    of other samples that represent three classes (circle, square, and triangle).
    To determine a class for our new sample, we look at the *k* nearest samples (or
    *neighbors*), and we count up their classes. Whichever class is most populous
    becomes the new sample’s class. We show which samples are considered for different
    values of *k* by a line to each of the *k* nearest samples. In [Figure 11-1](#figure11-1)(b)
    we’ve set *k* to 1, meaning we want to use the class of the nearest sample. In
    this case it’s a red circle, so this new sample is classified as a circle. In
    [Figure 11-1](#figure11-1)(c) we’ve set *k* to 9, so we look at the nine nearest
    points. Here we find 3 circles, 4 squares, and 2 triangles. Because there are
    more squares than any other class, the star is classified as a square. In [Figure
    11-1](#figure11-1)(d) we’ve set *k* to 25\. Now we have 6 circles, 13 squares,
    and 6 triangles, so again the star is classified as a square.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图11-1](#figure11-1)(a)中，我们有一个样本点（一个星号），它位于代表三类（圆形、方形和三角形）的其他样本之间。为了确定新样本的类别，我们查看*k*个最近的样本（或*邻居*），并统计它们的类别。类别最多的那个类将成为新样本的类别。我们通过一条线连接每个*k*个最近样本，来显示在不同*k*值下考虑的样本。在[图11-1](#figure11-1)(b)中，我们将*k*设置为1，这意味着我们希望使用最近样本的类别。在这种情况下，它是一个红色的圆形，因此新样本被分类为圆形。在[图11-1](#figure11-1)(c)中，我们将*k*设置为9，因此我们查看9个最近的点。在这里，我们找到3个圆形，4个方形和2个三角形。因为方形的数量多于其他类别，所以星号被分类为方形。在[图11-1](#figure11-1)(d)中，我们将*k*设置为25。现在我们有6个圆形，13个方形和6个三角形，因此星号再次被分类为方形。
- en: To sum this up, kNN accepts a new sample to evaluate, along with a value of
    *k*. It then finds the closest *k* samples to the new sample, and we assign the
    new sample to the class with the largest number of representatives among the *k*
    samples we found. There are various ways to break ties and handle exceptional
    cases, but that’s the basic idea.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，kNN接受一个新的样本进行评估，并提供一个*k*值。然后，它会找到与新样本最近的*k*个样本，并将新样本分配给在找到的*k*个样本中，代表性最多的类别。处理平局和特殊情况的方式有很多种，但这就是基本思想。
- en: Note that kNN does not create explicit boundaries between groups of points.
    There’s no notion here of regions or areas that samples belong to. We say that
    kNN is an *on-demand*, or *lazy*, algorithm because it does no processing of the
    samples during the learning stage. When learning, kNN just stashes the samples
    in its internal memory and it’s done.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，kNN算法不会在各个点群体之间创建明确的边界。这里没有样本所属区域或区域的概念。我们称kNN为*按需*或*懒惰*算法，因为它在学习阶段不会对样本进行任何处理。学习时，kNN只是将样本存储在其内部内存中，然后结束。
- en: kNN is attractive because it’s simple, and training it is usually exceptionally
    fast. On the other hand, kNN can require a lot of memory, because (in its basic
    form) it’s saving all the input samples. Using large amounts of memory can slow
    down the algorithm. Another problem is that the classification of new points is
    often slow (compared to other algorithms we’ll see) because of the cost of searching
    for neighbors. Every time we want to classify a new piece of data, we have to
    find its *k* nearest neighbors, which requires work. Of course, there are many
    ways to enhance the algorithm to speed this up, but it still remains a relatively
    slow way to classify. For applications where classification speed is important,
    like real-time systems and websites, the time required by kNN to produce each
    answer can take it out of the running.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: kNN之所以具有吸引力，是因为它简单，并且训练速度通常非常快。另一方面，kNN可能需要大量内存，因为（在其基本形式下）它保存所有输入样本。使用大量内存可能会导致算法变慢。另一个问题是，新的数据点的分类通常很慢（与我们接下来看到的其他算法相比），这是由于寻找邻居的成本。每次我们想要对一个新数据进行分类时，都必须找到它的*k*个最近邻，这需要一定的计算。当然，有许多方法可以增强算法以加快这一过程，但它仍然是一个相对较慢的分类方法。对于分类速度至关重要的应用，例如实时系统和网站，kNN每次产生答案所需的时间可能会使其无法使用。
- en: Another problem with this technique is that it depends on having lots of neighbors
    nearby (after all, if the nearest neighbors are all very far away, then they don’t
    offer a good proxy for other examples that are like the one we’re trying to classify).
    This means we need a lot of training data. If we have lots of features (that is,
    our data has many dimensions) then kNN quickly succumbs to the curse of dimensionality,
    which we discussed in Chapter 7\. As the dimensionality of the space goes up,
    if we don’t also significantly increase the number of training samples, then the
    number of samples in any local neighborhood drops, making it harder for kNN to
    get a good collection of nearby points.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术的另一个问题是，它依赖于附近有大量邻居（毕竟，如果最近的邻居都非常远，那么它们就无法为我们尝试分类的其他相似样本提供很好的代理）。这意味着我们需要大量的训练数据。如果我们有很多特征（也就是我们的数据有很多维度），那么kNN很快就会遭遇维度灾难，这在第7章中我们已经讨论过。随着空间的维度增加，如果我们没有显著增加训练样本的数量，那么任何局部邻域中的样本数都会减少，这使得kNN很难得到一个好的邻近点集合。
- en: Let’s put kNN to the test. In [Figure 11-2](#figure11-2) we show a “smile” dataset
    of 2D data that falls into two classes. In this figure, and the similar ones that
    follow, the data is made of points. Since points are hard to see, we’re drawing
    a filled circle around each point as a visual aid.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来测试一下kNN。在[图11-2](#figure11-2)中，我们展示了一个“微笑”数据集，该数据集属于两类。在此图以及接下来类似的图中，数据由点组成。由于点难以观察，我们在每个点周围绘制一个实心圆圈作为视觉辅助。
- en: '![F11002](Images/F11002.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![F11002](Images/F11002.png)'
- en: 'Figure 11-2: A smile dataset of 2D points. There are two classes, blue and
    orange.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图11-2：一个二维点的微笑数据集。共有两类，蓝色和橙色。
- en: Using kNN with different values of *k* gives us the results in [Figure 11-3](#figure11-3).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同*k*值的kNN给出了[图11-3](#figure11-3)中的结果。
- en: '![F11003](Images/F11003.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![F11003](Images/F11003.png)'
- en: 'Figure 11-3: Classifying with the points in [Figure 11-2](#figure11-2) using
    kNN for different values of *k*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图11-3：使用不同*k*值的kNN对[图11-2](#figure11-2)中的点进行分类
- en: Although kNN doesn’t produce explicit classification boundaries, we can see
    that when *k* is small and we compare our input point with only a few neighbors,
    the space is broken up into sections that share a rather rough border. As *k*
    gets larger and we use more neighbors, that border smooths out because we’re getting
    a better overall picture of the environment around the new sample.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管kNN不会生成明确的分类边界，但我们可以看到，当*k*较小时，我们只与少数邻居进行比较时，空间被分割成一些具有相当粗糙边界的区域。随着*k*增大，我们使用更多的邻居时，那些边界会变得平滑，因为我们对新样本周围的环境有了更全面的了解。
- en: To make things more interesting, let’s add some noise to our data so that the
    edges aren’t so easy to find. [Figure 11-4](#figure11-4) shows a noisy version
    of [Figure 11-2](#figure11-2).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让事情变得更有趣，我们向数据中添加一些噪声，这样边界就不容易找到。[图11-4](#figure11-4)展示了[图11-2](#figure11-2)的一个噪声版本。
- en: '![F11004](Images/F11004.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![F11004](Images/F11004.png)'
- en: 'Figure 11-4: A noisy version of the smile dataset from [Figure 11-2](#figure11-2)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图11-4：来自[图11-2](#figure11-2)的噪声版本的微笑数据集
- en: The results for different values of *k* are shown in [Figure 11-5](#figure11-5).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 不同*k*值的结果显示在[图11-5](#figure11-5)中。
- en: '![F11005](Images/F11005.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![F11005](Images/F11005.png)'
- en: 'Figure 11-5: Using kNN and [Figure 11-4](#figure11-4) to assign a class to
    points in the plane'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-5：使用 kNN 和 [图 11-4](#figure11-4) 将类别分配给平面中的点
- en: We can see that in the presence of noise, small values of *k* lead to ragged
    borders. In this example, we have to get *k* up to 50 before we see fairly smooth
    boundaries. Also notice that as *k* increases, the smile shape contracts, since
    the perimeter gets eroded by the larger number of background points.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在存在噪声的情况下，较小的 *k* 值会导致边界不规则。在这个例子中，我们需要将 *k* 增加到 50 才能看到相对平滑的边界。还需要注意的是，随着
    *k* 增加，微笑形状会收缩，因为较多的背景点侵蚀了边缘。
- en: Because kNN doesn’t explicitly represent the boundaries between classes, it
    can handle any kind of boundaries, or any distribution of classes. To see this,
    let’s add some eyes to our smile, creating three disconnected sets of the same
    class. The resulting noisy data is in [Figure 11-6](#figure11-6).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 kNN 并没有明确表示类别之间的边界，它可以处理任何类型的边界或任何类别的分布。为了说明这一点，我们可以在微笑上添加一些眼睛，创建三个不相连的相同类别的数据集。生成的带噪声数据见
    [图 11-6](#figure11-6)。
- en: '![F11006](Images/F11006.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![F11006](Images/F11006.png)'
- en: 'Figure 11-6: A noisy dataset of a smile and two eyes'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-6：带噪声的数据集，包含一个微笑和两只眼睛
- en: The resulting classifications for different values of *k* are shown in [Figure
    11-7](#figure11-7).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 不同 *k* 值的结果分类见 [图 11-7](#figure11-7)。
- en: In this example, a value of about 20 for *k* looks best. Too small a value of
    *k* can give us ragged edges and noisy results, but too large a value of *k* can
    start to erode the features. As is so often the case, finding the best hyperparameter
    for this algorithm for any given dataset is a matter of repeated experimentation.
    We can use cross-validation to automatically score the quality of each result,
    which is particularly useful when there are many dimensions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，约 20 的 *k* 值看起来最好。过小的 *k* 值可能导致边缘不规则和结果噪声，但过大的 *k* 值可能会开始侵蚀特征。像往常一样，找到最佳的超参数对于任何给定数据集而言，都是通过反复实验来完成的。我们可以使用交叉验证来自动评分每个结果的质量，这在维度很多的情况下特别有用。
- en: 'kNN is a great nonparametric algorithm: it’s easy to understand and program,
    and when the dataset isn’t too big, training is extremely fast and classification
    of new data isn’t too slow. But when the dataset gets large, kNN becomes less
    appealing: memory requirements go up because every sample is kept, and classification
    gets slower because the search gets slower. These problems are shared by most
    nonparametric algorithms.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: kNN 是一个很棒的非参数算法：它容易理解和编程，而且当数据集不太大时，训练非常快速，分类新数据也不会太慢。但当数据集变大时，kNN 就不那么吸引人了：内存需求增加，因为每个样本都会被保留，分类变得更慢，因为搜索变得更慢。这些问题是大多数非参数算法共有的。
- en: '![F11007](Images/F11007.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![F11007](Images/F11007.png)'
- en: 'Figure 11-7: kNN doesn’t create boundaries between clusters of samples, so
    it works even when a class is broken up into pieces.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-7：kNN 不会在样本簇之间创建边界，因此即使一个类别被拆分成几个部分，它也能正常工作。
- en: Decision Trees
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树
- en: Let’s consider another nonparametric classification method, called *decision
    trees*. This algorithm builds a data structure from the points in the sample set,
    which is then used to classify new points. Let’s begin by taking a look at this
    structure, and then we’ll see how to build it.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑另一种非参数分类方法，称为 *决策树*。该算法从样本集中的点构建一个数据结构，然后用这个结构来分类新点。让我们首先看看这个结构，然后再看看如何构建它。
- en: Introduction to Trees
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 树的介绍
- en: We can illustrate the basic idea behind decision trees with the familiar parlor
    game called 20 Questions. In this game, one player (the chooser) thinks of a specific
    target object, which is often a person, place, or thing. The other player (the
    guesser) then asks a series of yes/no questions. If the guesser can correctly
    identify the target in 20 or fewer questions, they win. One reason the game endures
    is that it’s fun to narrow down the enormous number of possible people, places,
    and things to one specific instance with such a small number of simple questions
    (perhaps surprisingly, with 20 yes/no questions we can only distinguish just over
    a million distinct targets).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个熟悉的娱乐游戏“20个问题”来说明决策树的基本思想。在这个游戏中，一名玩家（选择者）想一个特定的目标物体，通常是一个人、地方或事物。另一名玩家（猜测者）接着问一系列是/否问题。如果猜测者能在20个问题内正确识别目标物体，他们就获胜。这个游戏经久不衰的原因之一是，它很有趣，通过如此少量的简单问题就能把庞大的可能对象范围缩小到一个特定实例（或许令人惊讶的是，20个是/否问题我们只能区分出超过一百万个不同的目标）。
- en: We can draw a typical game of 20 questions in graphical form, as in [Figure
    11-8](#figure11-8).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像[图11-8](#figure11-8)那样以图形形式绘制一个典型的20个问题游戏。
- en: '![F11008](Images/F11008.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![F11008](Images/F11008.png)'
- en: 'Figure 11-8: A tree for playing 20 questions. Note that after each decision
    there are exactly two choices, one each for “yes” and “no.”'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图11-8：用于玩20个问题的树形图。请注意，每次决策后都有两个选择，一个是“是”，另一个是“否”。
- en: We call a structure like [Figure 11-8](#figure11-8) a *tree* because it looks
    something like an upside-down tree. Such trees have a bunch of associated terms
    that are worth knowing. We say that each splitting point in the tree is a *node*,
    and each line connecting nodes is a *link*, *edge*, or *branch*. Following the
    tree analogy, the node at the top is the *root*, and the nodes at the bottom are
    *leaves*, or *terminal nodes*. Nodes between the root and the leaves are called
    *internal nodes* or *decision nodes*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称像[图11-8](#figure11-8)这样的结构为*树*，因为它看起来像一棵倒立的树。这种树有许多相关的术语，值得了解。我们说，树中的每个分裂点是一个*节点*，连接节点的每一条线是*链接*、*边*或*分支*。沿着树的类比，位于顶部的节点是*根节点*，位于底部的节点是*叶节点*，或称*终端节点*。位于根节点和叶节点之间的节点称为*内部节点*或*决策节点*。
- en: If a tree has a perfectly symmetrical shape, we say the tree is *balanced*,
    otherwise it’s *unbalanced*. In practice, almost all trees are unbalanced when
    they’re made, but we can run algorithms to make them closer to being balanced
    if a particular application prefers that. We also say that every node has a *depth*,
    which is a number that gives the smallest number of nodes we must go through to
    reach the root. The root has a depth of 0, the nodes immediately below it have
    a depth of 1, and so on.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一棵树具有完美对称的形状，我们称这棵树为*平衡*的，否则它就是*不平衡*的。实际上，几乎所有树在创建时都是不平衡的，但我们可以运行算法使它们更接近平衡，如果某个特定应用偏好这种情况的话。我们还可以说，每个节点都有一个*深度*，它是一个数字，表示到达根节点所需通过的最小节点数。根节点的深度为0，紧接在其下方的节点深度为1，以此类推。
- en: '[Figure 11-9](#figure11-9) shows a tree with these labels.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11-9](#figure11-9)显示了一个带有这些标签的树形图。'
- en: '![F11009](Images/F11009.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![F11009](Images/F11009.png)'
- en: 'Figure 11-9: Some terminology for a tree'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图11-9：树的一些术语
- en: It’s also common to use the terms associated with family trees, though these
    abstract trees don’t require the union of two nodes to produce children. Every
    node (except the root) has a node above it. We call this the *parent* of that
    node. The nodes immediately below a parent node are its *children*. We sometimes
    distinguish between *immediate children* that are directly connected to a parent,
    and *distant children* that are at the same depth as immediate children, but connected
    to the parent through a sequence of other nodes. If we focus our attention on
    a specific node, then that node and all of its children taken together are called
    a *subtree*. Nodes that share the same immediate parent are called *siblings*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通常也使用与家族树相关的术语，尽管这些抽象的树形结构不需要两个节点结合来产生子节点。每个节点（根节点除外）都有一个位于其上方的节点。我们称这个节点为该节点的*父节点*。紧接在父节点下方的节点是其*子节点*。我们有时会区分*直接子节点*（直接与父节点相连的节点）和*远程子节点*（与父节点处于相同深度，但通过一系列其他节点连接到父节点的节点）。如果我们将注意力集中在一个特定的节点上，那么该节点及其所有子节点一起称为*子树*。共享相同父节点的节点称为*兄弟节点*。
- en: '[Figure 11-10](#figure11-10) shows some of these ideas graphically.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11-10](#figure11-10)以图形方式展示了这些概念的一部分。'
- en: '![F11010](Images/F11010.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![F11010](Images/F11010.png)'
- en: 'Figure 11-10: Using familiar terms with trees. The green node’s parent is immediately
    above it, and its children are immediately below.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图11-10：使用树的熟悉术语。绿色节点的父节点在其上方，而其子节点在下方。
- en: With this vocabulary in mind, let’s return to our game of 20 Questions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这些词汇后，让我们回到我们的20个问题游戏。
- en: Using Decision Trees
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用决策树
- en: 'An interesting quality of the 20 Questionstree shown in [Figure 11-8](#figure11-8)
    is that it’s *binary*; every parent node has exactly two children: one for yes,
    one for no. Binary trees are a particularly easy kind of tree for some algorithms
    to work with. If some nodes have more than two children, we say that the tree
    overall is *bushy*. We can always convert a bushy tree to a binary tree if we
    want. An example of a bushy tree that tries to guess the month of someone’s birthday
    is shown on the left of [Figure 11-11](#figure11-11), and the corresponding binary
    tree is shown on the right. Because we can easily go back and forth, we usually
    draw trees in whatever form is most clear and succinct for the discussion at hand.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 11-8](#figure11-8)中展示的20问题树的一个有趣特点是它是*二叉*的；每个父节点恰好有两个子节点：一个代表“是”，一个代表“否”。二叉树是某些算法特别容易处理的一种树结构。如果某些节点有超过两个子节点，我们称整个树为*灌木状*。如果需要，我们总是可以将一个灌木状的树转换为二叉树。一个尝试猜测某人生日月份的灌木状树示例显示在[图
    11-11](#figure11-11)的左侧，右侧则是相应的二叉树。因为我们可以轻松地在两者之间转换，所以通常会根据讨论的需要，绘制最清晰简洁的树形。
- en: We can use trees to classify data. When used this way, the trees are called
    *decision trees*. The full name of the approach is *categorical variable decision
    trees*. This is to distinguish it from those times we use decision trees to work
    with continuous variables, as in regression problems. Those are called *continuous
    variable decision trees*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用树来分类数据。当树用于这种方式时，称为*决策树*。这种方法的全称是*分类变量决策树*。这是为了区分我们在使用决策树处理连续变量时的情况，如回归问题。那种情况下的决策树被称为*连续变量决策树*。
- en: '![F11011](Images/F11011.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![F11011](Images/F11011.png)'
- en: 'Figure 11-11: Turning a bushy tree (left) into a binary tree (right)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-11：将一个灌木状树（左）转换为二叉树（右）
- en: Let’s stick with the categorical versions here. For simplicity, we’ll just refer
    to them as decision trees, or simply trees, from now on. An example of such a
    tree for sorting inputs into different classes is shown in [Figure 11-12](#figure11-12).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们继续使用分类版本。为了简化，从现在开始，我们只将其称为决策树，或者简称为树。一个用于将输入分类到不同类别的树的示例如[图 11-12](#figure11-12)所示。
- en: In [Figure 11-12](#figure11-12), we start with a root node containing samples
    of different classes, distinguished by their shapes (and colors). To construct
    the tree, we *split* the samples at each node into two groups using some kind
    of test, resulting in a decision. For instance, the test applied at the root node
    might be, “Is this shape rectangular?” The test for the left child might be, “Is
    this shape taller than it is wide?” We’ll soon see how to come up with such tests.
    The goal in this example is to keep splitting each node until we’re left with
    samples of only one class. At that point, we declare that node to be a leaf, and
    stop splitting. In [Figure 11-12](#figure11-12), we’ve split our starting data
    into five classes. It’s essential to remember the test we applied at each node.
    Now, when a new sample arrives, we can start at the root and apply the root’s
    test, then the test of the appropriate child, and so on. When we finally reach
    a leaf, we have determined the class for that sample.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 11-12](#figure11-12)中，我们从一个包含不同类别样本的根节点开始，这些样本通过形状（和颜色）来区分。为了构建树，我们在每个节点将样本分成两组，使用某种测试方式，得到一个决策。例如，在根节点应用的测试可能是：“这个形状是矩形吗？”左子节点的测试可能是：“这个形状比宽度高吗？”我们很快就会看到如何提出这些测试。这个示例的目标是不断拆分每个节点，直到只剩下属于一个类别的样本为止。此时，我们声明该节点为叶子节点，并停止拆分。在[图
    11-12](#figure11-12)中，我们已将起始数据分成了五个类别。记住我们在每个节点应用的测试非常重要。现在，当一个新样本到来时，我们可以从根节点开始，应用根节点的测试，然后是相应子节点的测试，依此类推。最终，当我们到达一个叶子节点时，就确定了该样本的类别。
- en: '![F11012](Images/F11012.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![F11012](Images/F11012.png)'
- en: 'Figure 11-12: A categorical decision tree. Each class is a different shape
    and color.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-12：一个分类决策树。每个类别都有不同的形状和颜色。
- en: Decision trees don’t start out fully built. Instead, we build the tree based
    on the samples in the training set. When we reach a leaf node during training,
    we test to see if the new training sample has the same class as all the other
    samples in that leaf. If it does, we add the sample to the leaf and we’re done.
    Otherwise, we come up with decision criteria based on some of the features that
    let us distinguish between this sample and the previous samples in the node. We
    then use this test to splitthe node. The test we come up with gets saved with
    the node, we create at least two children, and assign each sample to the appropriate
    child, as in [Figure 11-13](#figure11-13).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树并不是一开始就完全构建好的。相反，我们根据训练集中的样本来构建树。当我们在训练过程中到达一个叶节点时，我们会测试新的训练样本是否与该叶节点中所有其他样本属于同一类别。如果是，我们将该样本添加到叶节点中，并完成操作。否则，我们基于一些特征制定决策标准，这些特征能帮助我们区分这个样本和节点中的其他样本。然后我们使用这个测试来拆分节点。我们制定的测试会与节点一起保存，至少创建两个子节点，并将每个样本分配到相应的子节点，就像在[图11-13](#figure11-13)中所示。
- en: When we’re done with training, evaluating new samples is easy. We just start
    at the root and work our way down the tree, following the appropriate branch at
    each node based on that node’s test with this sample’s features. When we land
    in a leaf, we report that the sample belongs to the class of the objects in that
    leaf.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们完成训练后，评估新样本变得很容易。我们只需从根节点开始，沿着每个节点根据该节点的测试结果，结合样本的特征，沿着合适的分支向下遍历。当我们到达叶节点时，我们报告该样本属于该叶节点中的对象类别。
- en: '![F11013](Images/F11013.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![F11013](Images/F11013.png)'
- en: 'Figure 11-13: Splitting a node by applying a test to its contents'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图11-13：通过对节点内容应用测试来拆分节点
- en: This is an idealized process. In practice, our tests can be imperfect and our
    leaves might contain a mixture of objects of different classes if, for efficiency
    or memory reasons, we chose not to split some leaves any more. For example, if
    we land in a node that contains samples that are 80 percent from class A and 20
    percent from class B, we might report that the new sample has an 80 percent chance
    of being in A and a 20 percent chance of being in B. If we have to report just
    one class, we might report that it’s A 80 percent of the time, and B the other
    20 percent.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个理想化的过程。在实际应用中，我们的测试可能并不完美，如果出于效率或内存考虑，我们选择不再拆分某些叶节点，那么这些叶节点中可能包含不同类别的对象的混合。例如，如果我们到达一个节点，该节点包含80%来自A类和20%来自B类的样本，我们可能会报告新样本有80%的概率属于A类，20%的概率属于B类。如果我们只能报告一个类别，我们可能会报告A类占80%的概率，而B类占20%。
- en: This process of making a decision tree works with just one sample at a time.
    In the simplest version of this technique, we don’t consider the entire training
    set at once and try to find, say, the smallest or most balanced tree that classifies
    the samples. Instead, we consider one sample at a time and split the nodes in
    the tree as required to handle that sample. Then we do the same for the next sample,
    and the next, and so on, making decisions in the moment without caring about any
    other data yet to come. This makes for efficient training.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种决策树的构建过程一次只处理一个样本。在这个技术的最简单版本中，我们不会一次性考虑整个训练集并尝试找到最小的或最平衡的树来对样本进行分类。相反，我们一次只考虑一个样本，并根据需要拆分树中的节点来处理该样本。然后我们对下一个样本进行相同的操作，以此类推，实时做出决策，而不关心任何还未出现的其他数据。这使得训练更加高效。
- en: This algorithm makes decisions based only on the data it has seen before and
    the data currently under consideration. It doesn’t try to plan or strategize for
    the future based on what it has seen so far. We call this a *greedy* algorithm,
    since it’s focused on maximizing its immediate, short-term gains.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法只根据它以前见过的数据和当前考虑的数据做出决策。它不会基于之前所见的内容为未来做出计划或策略。我们称这种算法为*贪婪*算法，因为它专注于最大化其即时的短期收益。
- en: Decisions trees are sometimes preferred in practice over other classifiers because
    their results are *explainable*. When the algorithm assigns a class to a sample,
    we don’t have to unravel some complex mathematical or algorithmic process. Instead,
    we can fully explain the final result just by identifying each decision along
    the way. This can be important in real life, too.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树有时在实践中优于其他分类器，因为它们的结果是*可解释的*。当算法为一个样本分配类别时，我们无需解开复杂的数学或算法过程。相反，我们可以通过识别每个决策过程来完全解释最终结果。这在现实生活中也可能非常重要。
- en: For example, suppose we apply for a loan at a bank, but we’re turned down. When
    we ask for the reason, the bank can show us each test made along the way. We say
    that the workings of the algorithm are *transparent*. Note that this doesn’t mean
    that they’re fair or reasonable. The bank may have come up with tests that are
    biased against one or more social groups or that depend on what seem to be irrelevant
    criteria. Just because they can explain why they made their choice doesn’t make
    the process or the results satisfactory. Legislators in particular seem to prefer
    laws that enforce transparency, which is easy to demonstrate, over fairness, which
    is much harder. Transparency is nice to have, but it doesn’t mean a system is
    behaving the way we’d like it to.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们向银行申请贷款，但被拒绝。当我们询问原因时，银行可以展示在整个过程中所做的每一个测试。我们称之为算法的*透明性*。需要注意的是，这并不意味着它们是公平或合理的。银行可能制定了一些对某些社会群体有偏见的测试，或者依赖于看似无关的标准。仅仅因为他们可以解释为什么做出选择，并不意味着这一过程或结果是令人满意的。立法者尤其偏好那些强制执行透明性的法律，这些法律容易证明，而公平则更加难以实现。透明性是一个可取的特性，但它并不意味着一个系统的行为是我们期望的那样。
- en: Decision trees can make bad decisions because they are particularly prone to
    overfitting. Let’s see why.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可能会做出糟糕的决策，因为它们特别容易出现过拟合。让我们看看为什么。
- en: Overfitting Trees
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合树
- en: Let’s begin our discussion of overfitting in the tree-building process by considering
    a couple of examples.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过考虑几个示例，开始讨论在构建树的过程中如何出现过拟合。
- en: The data in [Figure 11-14](#figure11-14) shows a cleanly separated set of data
    representing two classes. A dataset that roughly follows this kind of geometry
    is often called a *two-moons dataset*, presumably because the semicircles reminded
    someone of crescent moons.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-14](#figure11-14) 中的数据展示了一个干净分离的两类数据集。一个大致遵循这种几何结构的数据集通常被称为*两个月亮数据集*，可能是因为这些半圆形状让人联想到新月。'
- en: '![F11014](Images/F11014.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![F11014](Images/F11014.png)'
- en: 'Figure 11-14: Our 600 starting data points for building a decision tree, arranged
    in a two-moons structure. These 2D points represent two classes, blue and orange.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-14：我们构建决策树的 600 个起始数据点，排列成两个半月形结构。这些二维数据点代表了两个类别，蓝色和橙色。
- en: Each step in building a tree potentially involves splitting a leaf, and thus
    replacing it with a node and two leaves, for a net increase to the tree of one
    internal node and one leaf. We often think about the size of our tree in terms
    of the number of leaves it contains.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 构建树的每一步可能涉及拆分一个叶子节点，从而将其替换为一个节点和两个叶子节点，导致树中一个内部节点和一个叶子节点的净增加。我们通常通过树中叶子节点的数量来衡量树的大小。
- en: '[Figure 11-15](#figure11-15) shows the process of building a decision tree
    for this data.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-15](#figure11-15) 显示了为这些数据构建决策树的过程。'
- en: '![F11015](Images/F11015.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![F11015](Images/F11015.png)'
- en: 'Figure 11-15: Building a decision tree for the data in [Figure 11-14](#figure11-14).
    Notice how the tree starts with big chunks and refines them into smaller and more
    precise regions.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-15：为 [图 11-14](#figure11-14) 中的数据构建决策树。请注意，树是如何从大块区域开始，并逐步将它们细化为更小、更精确的区域。
- en: In this figure, we drew the dataset over each image just for reference. In this
    example, each node corresponds to a box. We don’t show it here, but the tree begins
    with just a single root corresponding to a blue box that covers the entire region.
    Then the training process receives one of the orange points near the top of the
    orange curve. This isn’t in the blue class, so we split the root with a horizontal
    cut into two boxes, shown in the upper left. The next point that comes in is a
    blue point near the left part of the blue curve. This falls in the orange box,
    so we split that with a vertical cut into two boxes as shown, giving us a total
    of three leaves. The rest of the figure shows the evolving tree as more samples
    arrive.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在此图中，我们在每个图像上绘制了数据集，仅供参考。在这个示例中，每个节点对应一个盒子。我们在这里没有显示，但树的起始部分仅由一个根节点组成，对应一个覆盖整个区域的蓝色盒子。然后，训练过程接收到一个靠近橙色曲线顶部的橙色点。这不属于蓝色类别，因此我们通过水平切割将根节点分成两个盒子，如左上方所示。接下来的点是一个靠近蓝色曲线左侧的蓝色点。这落入了橙色盒子，因此我们通过垂直切割将其分成两个盒子，如所示，最终得到了三个叶子节点。其余部分显示了随着更多样本到来，树形逐渐演变的过程。
- en: Notice how the regions gradually refine as the tree grows in response to more
    training data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当树形随着更多训练数据的到来而生长时，区域是如何逐渐细化的。
- en: This tree needs only 12 leaves to correctly classify every training sample.
    The final tree and the original data are shown together in [Figure 11-16](#figure11-16).
    This tree fits the data perfectly.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这棵树只需要 12 个叶子就能正确分类每个训练样本。[图 11-16](#figure11-16) 中同时展示了最终树和原始数据。该树完美地拟合了数据。
- en: Note the two horizontal, thin rectangles. They enclose two orange samples at
    the top of the left side of the arc and manage to slip between the blue points
    (recall that the samples are points in the center of each circle). This is overfitting,
    because any future points that fall into those rectangles, despite the fact that
    they’re both almost completely in a blue region, will be classified as orange.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这两个水平的细长矩形。它们将两个橙色样本包围在弧线左侧的顶部，并设法穿插在蓝色点之间（回想一下，样本是每个圆圈中心的点）。这是过拟合，因为尽管它们几乎完全位于蓝色区域内，但任何落入这些矩形的未来点都会被分类为橙色。
- en: '![F11016](Images/F11016.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![F11016](Images/F11016.png)'
- en: 'Figure 11-16: Our final tree with 12 leaves'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-16：我们的最终决策树，包含 12 个叶子
- en: Because decision trees are so sensitive to each input sample, they have a profound
    tendency to overfit. In fact, decision trees almost always overfit, because every
    training sample can influence the tree’s shape. To see this, take a look at [Figure
    11-17](#figure11-17). Here we ran the same algorithm as for [Figure 11-16](#figure11-16)
    two times, but in each case, we used a different, randomly chosen 70 percent of
    the input data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于决策树对每个输入样本都非常敏感，它们有一种强烈的过拟合倾向。实际上，决策树几乎总是会过拟合，因为每个训练样本都可能影响树的形状。要验证这一点，请看看
    [图 11-17](#figure11-17)。在这里，我们对 [图 11-16](#figure11-16) 使用相同的算法进行了两次运行，但每次我们都使用了不同的、随机选择的
    70% 输入数据。
- en: '![F11017](Images/F11017.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![F11017](Images/F11017.png)'
- en: 'Figure 11-17: Decision trees are very sensitive to their inputs. (Left) We
    randomly chose 70 percent of the samples from [Figure 11-14](#figure11-14) and
    fit a tree. (Right) The same process, but for a different randomly selected 70
    percent of the original samples.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-17：决策树对其输入非常敏感。（左）我们从 [图 11-14](#figure11-14) 随机选择了 70% 的样本并拟合了一棵树。（右）同样的过程，但对于从原始样本中随机选择的
    70% 样本。
- en: These two decision trees are similar, but definitely not identical. The tendency
    of decision trees to overfit is much more pronounced when the data isn’t so easily
    separated. Let’s look at an example of that now.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这两棵决策树相似，但显然不是完全相同。当数据不容易分离时，决策树过拟合的倾向会更加明显。现在我们来看一个这样的例子。
- en: '[Figure 11-18](#figure11-18) shows another pair of crescent moons, but this
    time we added lots of noise to the samples after they had their classes assigned.
    The two classes no longer have a clean boundary.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-18](#figure11-18) 展示了另一对弯月形状的样本，但这次我们在样本分配类别后加入了大量噪声。两个类别不再有清晰的边界。'
- en: '![F11018](Images/F11018.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![F11018](Images/F11018.png)'
- en: 'Figure 11-18: A noisy set of 600 samples for building decision trees'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-18：构建决策树的 600 个噪声样本集
- en: Fitting a tree to this data starts out with big regions, but it rapidly turns
    into a complicated set of tiny boxes as the algorithm splits up nodes this way
    and that to match the noisy data. [Figure 11-19](#figure11-19) shows the result.
    In this case, it required 100 leaves for the tree to correctly classify the points.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 将树拟合到这些数据从大区域开始，但随着算法不断拆分节点来适应噪声数据，它很快变成了一组复杂的微小矩形框。[图 11-19](#figure11-19)
    显示了结果。在这种情况下，树需要 100 个叶子来正确分类这些点。
- en: '![F11019](Images/F11019.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![F11019](Images/F11019.png)'
- en: 'Figure 11-19: The tree-building process. Note that the second row uses large
    numbers of leaves.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-19：构建决策树的过程。注意第二行使用了大量的叶子。
- en: '[Figure 11-20](#figure11-20) shows a close-up of the final tree and the original
    data.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-20](#figure11-20) 展示了最终树和原始数据的特写。'
- en: '![F11020](Images/F11020.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![F11020](Images/F11020.png)'
- en: 'Figure 11-20: Our noisy data fit with a tree with 100 leaves. Notice how many
    little boxes have been used to catch just an odd sample here and there.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-20：我们的噪声数据拟合了一棵包含 100 个叶子的树。注意，多少小矩形被用来捕捉偶尔的样本。
- en: There’s a lot of overfitting here. Though we expect most of the samples in the
    lower right to be orange and most of those in the upper left to be blue, this
    tree has carved out a lot of exceptions based on this particular dataset. Future
    samples that fall into those little boxes are likely to be misclassified.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多过拟合。尽管我们预计右下角的大部分样本应该是橙色，而左上角的大部分样本应该是蓝色，但这棵树根据这个特定的数据集划出了许多例外。未来落入这些小矩形框的样本可能会被错误分类。
- en: Let’s repeat our process of building trees using different, random 70 percent
    selections of the data in [Figure 11-18](#figure11-18). [Figure 11-21](#figure11-21)
    shows the results.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重复构建树的过程，使用不同的、随机选择的70%数据，在[图11-18](#figure11-18)中展示。结果见[图11-21](#figure11-21)。
- en: '![F11021](Images/F11021.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![F11021](Images/F11021.png)'
- en: 'Figure 11-21: A couple of trees built with different sets of 70 percent of
    the samples in [Figure 11-18](#figure11-18)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图11-21：使用[图11-18](#figure11-18)中不同70%的样本集构建的几棵树
- en: There are similarities, but these trees are significantly different, and lots
    of little bits exist only to classify a few samples. This is overfitting in action.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这些树有相似之处，但它们差异显著，许多小部分仅用于分类少量样本。这就是过拟合的表现。
- en: Although this may look pretty bad for the decision tree method, in Chapter 12
    we’ll see that by combining many simple decision trees into a group, or an *ensemble*,
    we can create robust, efficient classifiers that don’t suffer as much from overfitting.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这看起来对于决策树方法来说可能不太理想，但在第12章中我们将看到，通过将多个简单的决策树结合成一个群体，或称为*集成*，我们可以创建出稳健、高效的分类器，从而减少过拟合的影响。
- en: There are a few other ways we can control overfitting.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他方法可以控制过拟合。
- en: As we saw in [Figure 11-15](#figure11-15) and [Figure 11-19](#figure11-19),
    the first few steps of the tree’s growth tend to generate big, general shapes.
    It’s only when the tree gets very deep that we get the tiny boxes that are symptomatic
    of overfitting. One popular strategy to reducing overfitting is *depth limiting:*
    we simply limit the tree’s depth while it’s building. If a node is more than a
    given number of steps from the root, we just declare it a leaf and don’t split
    it any more. A different strategy is setting a minimum sample requirement so that
    we never split a node that has less than a certain number of samples, no matter
    how mixed they are.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[图11-15](#figure11-15)和[图11-19](#figure11-19)中看到的，树的生长初期通常会生成较大、较为通用的形状。只有当树变得非常深时，才会出现那些典型的过拟合现象——小的盒子。减少过拟合的一个常见策略是*限制深度*：我们在树构建过程中简单地限制树的深度。如果一个节点离根节点超过了设定的步数，我们就将其直接声明为叶子节点，不再进行分裂。另一种策略是设置最小样本要求，确保我们在分裂一个节点时，该节点至少包含一定数量的样本，无论这些样本有多么混杂。
- en: Yet another approach to reducing overfitting is to reduce the size of the tree
    after it’s made in a process called *pruning*. This works by removing, or *trimming*,
    leaf nodes. We look at each leaf and characterize what would happen to the total
    error of the tree’s results if we removed that leaf. If the error is acceptable,
    we simply remove the leaf from the tree. If we remove all the children of a node,
    then it becomes a leaf itself, and a candidate for further pruning. Pruning a
    tree can make it shallower, which offers the additional benefit of also making
    it faster when we classify new data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种减少过拟合的方法是在树构建完成后，通过一种叫做*剪枝*的过程来减小树的大小。其原理是通过移除或*修剪*叶子节点来实现。我们检查每个叶子节点，并判断如果移除该叶子节点，树的总误差会发生什么变化。如果误差仍在可接受范围内，我们就直接移除该叶子节点。如果移除一个节点的所有子节点，那么该节点就会变成一个叶子节点，并成为进一步剪枝的候选对象。剪枝树可以使树变得更浅，这还带来了一个额外的好处，那就是在对新数据进行分类时，它的速度也会更快。
- en: Depth limiting, setting minimum sample requirements per node, and pruning all
    simplify the tree, but because they do so in different ways, they usually give
    us different results.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 限制深度、设置每个节点的最小样本要求以及剪枝，都能简化树的结构，但由于它们是以不同的方式实现的，因此通常会给出不同的结果。
- en: Splitting Nodes
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 节点分裂
- en: 'Before we leave decision trees, let’s return briefly to the node-splitting
    process, since many machine learning libraries offer us a choice of splitting
    algorithms to choose from. Here are two questions to ask when we consider a node:
    First, does it need to be split? Second, how should we split it? Let’s take these
    in order.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们离开决策树之前，简要回顾一下节点分裂过程，因为许多机器学习库提供了不同的分裂算法供我们选择。当考虑一个节点时，有两个问题需要问：首先，是否需要分裂？其次，我们应该如何分裂它？我们按顺序来讨论这两个问题。
- en: When we ask if a node needs to be split, we usually consider to what extent
    all the samples in a given node are of the same class. We describe the uniformity
    of a node’s contents with a number called that node’s *purity*. If all the samples
    are in the same class, the node is completely pure. The more samples we have of
    other classes, the smaller the value of purity becomes. To test if a node needs
    splitting, we can check the purity against a threshold. If the node is too *impure*,
    meaning that the purity is below the threshold, we split it.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们问一个节点是否需要拆分时，通常会考虑节点中所有样本属于同一类别的程度。我们用一个叫做该节点*纯度*的数字来描述节点内容的均匀性。如果所有样本都属于同一类别，则该节点是完全纯净的。我们拥有其他类别样本的数量越多，纯度的值就越小。为了测试节点是否需要拆分，我们可以将纯度与阈值进行比较。如果节点的纯度过于*不纯*，即纯度低于阈值，则我们进行拆分。
- en: Now we can look at how to split the node. If our samples have many features,
    we can invent lots of different possible splitting tests. We can test the value
    of just one feature and ignore the others. We can look at groups of features and
    test on some aggregated values from them. We’re free to choose a completely different
    test at every node based on different features. This gives us a huge variety of
    possible tests to consider.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看一下如何拆分节点。如果我们的样本具有多个特征，我们可以设计许多不同的拆分测试。我们可以只测试一个特征的值，忽略其他特征。我们可以查看特征组，并基于它们的一些聚合值进行测试。我们可以自由地根据不同特征在每个节点上选择完全不同的测试方法。这为我们提供了大量可能的测试方案。
- en: '[Figure 11-22](#figure11-22) shows a node containing a mix of circles of different
    sizes and colors. Let’s try to get all the reddish objects in one child and all
    the bluish ones in another. When we just look at the data (usually the best first
    step with any new database), it seems like the reddish circles are the biggest.
    Let’s try using a test based on the radius of each circle. The figure shows the
    result of splitting on the radius using three different values.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-22](#figure11-22)展示了一个包含不同大小和颜色的圆圈的混合节点。让我们尝试将所有红色物体放入一个子节点，将所有蓝色物体放入另一个子节点。当我们仅查看数据时（通常是处理任何新数据库时的最佳第一步），似乎红色圆圈是最大的。让我们尝试使用基于圆圈半径的测试。图中展示了使用三个不同半径值进行拆分的结果。'
- en: '![F11022](Images/F11022.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![F11022](Images/F11022.png)'
- en: 'Figure 11-22: Splitting a node according to different values of the radii of
    the circles within it'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-22：根据圆圈的半径值拆分节点
- en: In this example, the radius value of 70 produces the purest results, with all
    the blue objects in one child and all the red ones in the other. If we use this
    test for this node, we’ll remember which feature we’re splitting on (the radius)
    and what value to test for (70).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，半径值为70的拆分产生了最纯净的结果，所有蓝色物体都在一个子节点中，所有红色物体都在另一个子节点中。如果我们使用这个测试来拆分该节点，我们将记住我们使用的特征（半径）以及要测试的值（70）。
- en: Since we could potentially split the node using a test based on any characteristic
    of the samples, we need some way to evaluate the results so we can pick the best
    test. Let’s look at two popular ways to test these results.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可能基于样本的任何特征使用测试来拆分节点，因此我们需要某种方式来评估结果，以便选择最佳的测试方法。让我们来看看两种常见的结果测试方法。
- en: Recall from Chapter 6 that *entropy* is a measure of complexity, or how many
    bits it takes to communicate some piece of information. The *Information Gain
    (IG)* measure uses this idea by comparing the entropy of a node to that of the
    children produced by each candidate test.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下第6章中提到的*熵*，它是复杂性的度量，或者说传达某些信息所需的比特数。*信息增益（IG）*度量通过比较节点的熵与每个候选测试所生成子节点的熵来使用这一概念。
- en: To evaluate a test, IG adds together the entropies of all the new children produced
    by that test and compares that result to the entropy in the parent cell. The more
    pure a cell is, the lower its entropy, so if a test makes pure cells, the sum
    of their entropies is less than the entropy of their parent. After trying different
    ways to split a node, we choose the split that gives us the biggest reduction
    in entropy (or the biggest gain in information).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估一个测试，IG将通过该测试生成的所有新子节点的熵加在一起，并将结果与父节点的熵进行比较。一个单元格越纯净，它的熵就越低，因此如果一个测试使得单元格变得纯净，则它们的熵总和会小于父节点的熵。经过不同的拆分测试后，我们选择那个能带来最大熵减少（或最大信息增益）的拆分方法。
- en: Another popular way to evaluate splitting tests is called the *Gini impurity*.
    The math used by this technique is designed to minimize the probability of misclassifying
    a sample. For example, suppose a leaf has 10 samples of class A and 90 samples
    of class B. If a new sample ends up at that leaf, and we report that it belongs
    to class B, there is a 10 percent chance that we are wrong. Gini impurity measures
    those errors at each leaf for multiple candidate split values. It then chooses
    the split that has the least chance of an erroneous classification.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种评估分裂测试流行的方法是 *基尼不纯度*。该技术使用的数学方法旨在最小化错误分类样本的概率。例如，假设某个叶子节点包含10个A类样本和90个B类样本。如果一个新样本进入该叶子节点，并且我们报告它属于B类，那么我们错误的概率为10%。基尼不纯度衡量每个叶子节点中多个候选分裂值的错误情况。然后，它选择错误分类机会最小的分裂。
- en: Some libraries offer other measures for grading the quality of a potential split.
    As with so many other choices, we usually try out a few options and pick the one
    that works the best for the specific data we’re working with.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一些库提供了其他用于评估潜在分裂质量的度量。像许多其他选择一样，我们通常会尝试几种选项，并选择最适合我们正在处理的特定数据的那个。
- en: Support Vector Machines
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持向量机
- en: 'Let’s consider our first parametric algorithm: the *support vector machine*
    (or *SVM*). We will use 2D data and just two classes for our illustrations (VanderPlas
    2016), but like most machine learning algorithms, the ideas are easily applied
    to data with any number of dimensions and classes.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑第一个参数化算法：*支持向量机*（或 *SVM*）。我们将使用二维数据并仅用两个类别进行说明（VanderPlas 2016），但像大多数机器学习算法一样，这些思想很容易应用于具有任意维度和类别的数据。
- en: The Basic Algorithm
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本算法
- en: Let’s begin with two blobs of points, one for each of two classes, shown in
    [Figure 11-23](#figure11-23).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从两个点簇开始，每个簇对应两个类别，如[图 11-23](#figure11-23)所示。
- en: '![F11023](Images/F11023.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![F11023](Images/F11023.png)'
- en: 'Figure 11-23: Our starting dataset consists of two blobs of 2D samples.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-23：我们的起始数据集由两个二维样本簇组成。
- en: We want to find a boundary between these clusters. To keep things simple, let’s
    use a straight line. But which one? A lot of lines split these two groups. Three
    candidates are shown in [Figure 11-24](#figure11-24).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望找到这两个簇之间的边界。为了简化问题，假设我们使用一条直线。但应该选择哪一条呢？有许多条直线可以分开这两个组。[图 11-24](#figure11-24)显示了三条候选线。
- en: '![F11024](Images/F11024.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![F11024](Images/F11024.png)'
- en: 'Figure 11-24: Three of the infinite number of lines that can separate our two
    clusters of samples'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-24：可以分隔我们两个样本簇的三条无限多的线之一。
- en: Which of these lines should we pick? One way to think about this is to imagine
    new data that might come in. Generally speaking, we want to classify any new sample
    as belonging to the class of the sample that it is nearest.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该选择哪一条线？一种思考方法是想象可能会有新的数据进来。一般来说，我们希望将任何新样本分类为与其最接近的样本所属的类别。
- en: To evaluate how well any given boundary line achieves this goal, let’s find
    its distance to the nearest sample of either class. We can use this distance to
    draw a symmetrical boundary around the line. [Figure 11-25](#figure11-25) shows
    the idea for a few different lines.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估任何给定的边界线如何实现这一目标，让我们找出它到任一类别最近样本的距离。我们可以利用这个距离在这条线周围画一个对称的边界。[图 11-25](#figure11-25)展示了几条不同线的想法。
- en: '![F11025](Images/F11025.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![F11025](Images/F11025.png)'
- en: 'Figure 11-25: We can assign a quality to each line by finding the distance
    from that line to the nearest data point.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-25：我们可以通过找到该线与最近数据点之间的距离，为每条线分配一个质量值。
- en: In [Figure 11-25](#figure11-25), we’ve drawn a circle around the sample that’s
    closest to the line. In the leftmost figure, many new points that are closer to
    the lower-right cluster than the upper-left one would be incorrectly classified
    as part of the upper-left cluster. The same situation holds in the rightmost figure.
    In the center, the line is much better, but it’s now preferring the lower-right
    cluster a little. Since we want each new point to be assigned to the class of
    the sample it’s closest to, we’d like our line to go right through the middle
    of the two clusters.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 11-25](#figure11-25)中，我们在最接近该线的样本周围画了一个圆圈。在最左侧的图中，许多新点会被错误地归类为左上簇的一部分，而它们实际上更接近右下簇。在最右侧的图中，情况相同。在中间，线条明显更好，但它现在略微偏向右下簇。由于我们希望每个新点都被分配到它最接近的样本的类别，因此我们希望我们的线能够恰好穿过两个簇的中间。
- en: The algorithm that finds this line is called a *support vector machine*, or
    *SVM* (Steinwart 2008). An SVM finds the line that is farthest from all the points
    in both clusters. In this context, the word *support* can be thought of as meaning
    “nearest,” *vector* is a synonym for “sample,” and *machine* is a synonym for
    “algorithm.” Thus, we can describe SVM as the “nearest sample algorithm.”
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: The best line for our two clusters in [Figure 11-23](#figure11-23), as calculated
    by SVM, is shown in [Figure 11-26](#figure11-26).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how SVM finds this line.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 11-26](#figure11-26) the circled samples are the nearest samples,
    or the *support vectors*. The algorithm’s first job is to locate these circled
    points. Once it finds them, the algorithm then finds the solid line near the middle
    of the figure. Of all the lines that separate the two sets of points, this is
    the line that’s farthest from every sample in each set, because it has the greatest
    distance to its support vectors. The dashed lines in [Figure 11-26](#figure11-26),
    like the circles around the support vectors, are just visual aids to help us see
    that the solid line in the center, found by the SVM, is the one that is as far
    as possible from all the samples. The distance from the solid line to the dashed
    lines that pass through the support vectors is called the *margin*. We can rephrase
    the idea by saying that the SVM algorithm finds the line with the largest margin.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![F11026](Images/F11026.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-26: The SVM algorithm finds the line that has the greatest distance
    from all the samples.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: What if the data is noisy, and the blobs overlap, as in [Figure 11-27](#figure11-27)?
    Now we can’t create a line surrounded by an empty zone. What’s the best line to
    draw through these overlapping sets of samples?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![F11027](Images/F11027.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-27: A new set of data where the blobs overlap'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: The SVM algorithm gives us control over a parameter that’s conventionally called
    *C*. This parameter controls how strict the algorithm is about letting points
    into the region between the margins. The larger the value of *C*, the more the
    algorithm demands an empty zone around the line. The smaller the value of *C*,
    the more points can appear in a zone around the line. We frequently need to search
    for the best value for *C* using trial and error. In practice, that usually means
    trying out lots of values and evaluating them with cross-validation.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-28](#figure11-28) shows our overlapping data with a *C* value of
    100,000.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![F11028](Images/F11028.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-28: The value of *C* tells SVM how sensitive to be to points that
    can intrude into the zone around the line that’s fit to the data. Here, *C* is
    100,000.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Let’s drop *C* way down to 0.01\. [Figure 11-29](#figure11-29) shows that this
    lets fewer points into the region around the line.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![F11029](Images/F11029.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-29: Lowering *C* to 0.01 lets in fewer points compared to [Figure
    11-28](#figure11-28).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The lines in [Figure 11-28](#figure11-28) and [Figure 11-29](#figure11-29) are
    different. Which one we prefer depends on what we want from our classifier. If
    we think the best boundary comes from the details near the zone where the points
    overlap, we want a small value of *C* so that we only look at the points near
    that boundary. If we think the overall shapes of the two collections of points
    is a better descriptor of that boundary, we want a larger value of *C* to include
    more of those farther-away points.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The SVM Kernel Trick
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A parametric algorithm is limited by the shapes it’s able to find. SVM, for
    instance, can only find *linear* shapes, like lines and planes. If we have data
    that can’t obviously be separated by such a shape, it may seem that SVM won’t
    be of much use. But there’s a clever trick that can sometimes let us use a linear
    boundary where it initially appears as if only a curved one could do the job.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we have the data of [Figure 11-30](#figure11-30), where there’s
    a blob of samples of one class surrounded by a ring of samples of another. There’s
    no way we can draw a straight line to separate these two sets.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![F11030](Images/F11030.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-30: A dataset of two classes'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Here comes the clever part. Suppose we temporarily add a third dimension to
    each point by elevating it by an amount based on that point’s distance from the
    center of the square. [Figure 11-31](#figure11-31) shows the idea.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![F11031](Images/F11031.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-31: If we push each point in [Figure 11-30](#figure11-30) upward
    by an amount based on its distance from the center of the pink blob, we get two
    distinct clouds of points, which we can separate with a plane.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in [Figure 11-31](#figure11-31), we can now draw a plane (the
    2D version of a straight line) between the two sets. In fact, we can use the very
    same idea of support vectors and margins as we did before to find the plane. [Figure
    11-32](#figure11-32) highlights the support vectors for the plane between the
    two clusters of points.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![F11032](Images/F11032.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-32: The support vectors for the plane'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Now all points above the plane can be placed into one class and all those below
    into the other.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: If we highlight the support vectors we found from [Figure 11-32](#figure11-32)
    in our original 2D plot, we get [Figure 11-33](#figure11-33).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![F11033](Images/F11033.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-33: Looking down on [Figure 11-32](#figure11-32)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: If we include the boundary created by the plane, we get [Figure 11-34](#figure11-34).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: In this case we found the right way to modify our data by looking at it and
    then coming up with a good 3D transformation of the data that let us split it
    apart. But when the data has many dimensions, we might not be able to visualize
    it well enough to even guess at a good transformation. Happily, we don’t have
    to find these transformations manually.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: One way to find a good transformation is to try lots and lots of them, and then
    select the one that works the best. The calculations required make this approach
    too slow to be practical, but fortunately, we can speed this up in a clever way.
    This idea focuses on a piece of math called the *kernel,* which lies at the heart
    of the SVM algorithm. Mathematicians sometimes honor a particularly clever or
    neat idea with the complementary term *trick.* In this case, rewriting the SVM
    math is called the *kernel trick* (Bishop 2006). The kernel trick lets the algorithm
    find the distances between transformed points without actually transforming them,
    which is a major efficiency boost. The kernel trick is used automatically by all
    major libraries, so we don’t even have to ask for it (Raschka 2015).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 找到一个好的变换方法的一种方式是尝试大量的变换，然后选择效果最好的一个。由于所需的计算量，这种方法往往过于缓慢，不具备实用性，但幸运的是，我们可以通过巧妙的方式加速这一过程。这个想法集中在一种叫做*核*的数学概念上，它是SVM算法的核心。数学家们有时会用*技巧*这个词来形容一个特别巧妙或简洁的想法。在这种情况下，重写SVM数学公式被称为*核技巧*（Bishop
    2006）。核技巧使得算法可以在不实际变换数据点的情况下计算变换后点之间的距离，这是一个巨大的效率提升。所有主要库都会自动使用核技巧，因此我们甚至不需要特别请求它（Raschka
    2015）。
- en: '![F11034](Images/F11034.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![F11034](Images/F11034.png)'
- en: 'Figure 11-34: The data from [Figure 11-30](#figure11-30) with the support vectors,
    the dashed lines showing the margins, and the boundary created by the plane'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图11-34：来自[图11-30](#figure11-30)的数据，带有支持向量、显示边界的虚线以及由平面所创建的边界
- en: Naive Bayes
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Let’s look at a parametric classifier that’s often used when we need quick results,
    even if they’re not the most accurate.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个参数化分类器，这种分类器通常在我们需要快速得到结果时使用，即使这些结果不是最准确的。
- en: This classifier works quickly because it begins by making assumptions about
    the data. It is based on Bayes’ Rule, which we looked at in Chapter 4.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分类器之所以快速，是因为它首先对数据做出假设。它基于贝叶斯定理，我们在第4章中已经讲过这个定理。
- en: Recall that Bayes’ Rule begins with a *prior*, or a predetermined idea of what
    the result is likely to be. Normally when we use Bayes’ Rule, we refine the prior
    by evaluating new pieces of evidence, creating a *posterior* that then becomes
    our new prior. But what if we just commit to the prior ahead of time and then
    see where it leads us?
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，贝叶斯定理是从*先验*开始的，或者说是对结果可能是什么的预先设想。通常，当我们使用贝叶斯定理时，我们通过评估新的证据来精化先验，创建一个*后验*，然后这个后验就成了我们的新先验。那么，如果我们提前就决定好先验，然后看看它会带我们走向何方呢？
- en: The *naive Bayes* classifier takes this approach. It’s called *naive* because
    the assumptions we make in our prior are not based on the contents of our data.
    That is, we make an uninformed, or naive, characterization of our data. We just
    assumethat the data has a certain structure. If we’re right, great, we get good
    results. The less well the data matches this assumption, the worse the results
    are. Naive Bayes is popular because this assumption turns out to be correct, or
    nearly correct, often enough that it’s worth taking a look. The interesting thing
    is that we never check to see if our assumption is justified. We just plow ahead
    as if we are certain.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*朴素贝叶斯*分类器采用了这种方法。之所以叫做*朴素*，是因为我们在先验中做出的假设并不是基于数据的内容。也就是说，我们对数据做出了一个未经信息验证的假设，或者说是一个“朴素”的假设。我们只是假设数据有某种结构。如果我们的假设是正确的，那就很好，我们能得到不错的结果。如果数据和这个假设不太匹配，结果就会变差。朴素贝叶斯之所以受欢迎，是因为这个假设经常是正确的，或者至少接近正确，因此值得一试。有趣的是，我们从来不检查我们的假设是否成立。我们只是继续前进，好像我们确信无疑一样。'
- en: 'In one of the more common forms of naive Bayes, we assume that every feature
    of our samples follows a Gaussian distribution. Recall from Chapter 2 that this
    is the famous bell curve: a smooth, symmetrical shape with a central peak. That’s
    our prior. When we look at a specific feature across all of our samples, we simply
    try to match that as well as we can with a Gaussian curve.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在朴素贝叶斯的一个常见形式中，我们假设每个样本的特征都遵循高斯分布。回想第2章中的内容，这就是著名的钟形曲线：一种平滑对称的形状，中央有一个峰值。这就是我们的先验。当我们查看所有样本中的某个特定特征时，我们只是尽可能地将其与高斯曲线进行匹配。
- en: If our features really do follow Gaussian distributions, then this assumption
    produces a good fit. The great thing about naive Bayes is that this assumption
    seems to work well far more frequently than we might expect.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的特征确实遵循高斯分布，那么这个假设会产生良好的拟合效果。朴素贝叶斯的一个优点是，这个假设似乎比我们预期的要有效得多。
- en: Let’s see it in action, starting with data that doessatisfy the prior. [Figure
    11-35](#figure11-35) shows a dataset that was created by drawing samples from
    two Gaussian distributions.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是如何工作的，从满足先验假设的数据开始。[图 11-35](#figure11-35)展示了一个通过从两个高斯分布中抽样生成的数据集。
- en: '![F11035](Images/F11035.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![F11035](Images/F11035.png)'
- en: 'Figure 11-35: A set of 2D data for training with naive Bayes. There are two
    classes, red and blue.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-35：用于朴素贝叶斯训练的一组二维数据。共有两个类别：红色和蓝色。
- en: When we give this data to a naive Bayes classifier, it assumes that each set
    of features comes from a Gaussian. That is, it assumes that the x coordinates
    of the red points follow a Gaussian, and the y coordinates of the red points also
    follow a Gaussian. It assumes the same thing about the x and y features of the
    blue points. Then it tries to fit the best four Gaussians it can to that data,
    creating two 2D hills. The result is shown in [Figure 11-36](#figure11-36).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这些数据输入到朴素贝叶斯分类器时，它假设每组特征来自一个高斯分布。也就是说，它假设红色点的 x 坐标符合高斯分布，红色点的 y 坐标也符合高斯分布。它对蓝色点的
    x 和 y 特征做同样的假设。然后，它尝试为这些数据拟合出最佳的四个高斯分布，生成两个二维的高山。结果如[图 11-36](#figure11-36)所示。
- en: '![F11036](Images/F11036.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![F11036](Images/F11036.png)'
- en: 'Figure 11-36: Naive Bayes fits a Gaussian to each of the x and y features of
    each class. Left: The Gaussian for the red class. Right: The Gaussian for the
    blue class.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-36：朴素贝叶斯为每个类别的 x 和 y 特征拟合高斯分布。左：红色类别的高斯分布。右：蓝色类别的高斯分布。
- en: If we overlay the Gaussian blobs and the points and look directly down, as in
    [Figure 11-37](#figure11-37), we can see that they form a very close match. That’s
    no surprise, because we generated the data in a way that had exactly the distribution
    the naive Bayes classifier was expecting.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将高斯分布的“斑点”和数据点重叠，并从上方直接观察，如[图 11-37](#figure11-37)所示，我们可以看到它们形成了非常接近的匹配。这个结果并不令人意外，因为我们生成数据时使用的分布正是朴素贝叶斯分类器所期望的分布。
- en: '![F11037](Images/F11037.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![F11037](Images/F11037.png)'
- en: 'Figure 11-37: Our entire training set overlaid on the Gaussians of [Figure
    11-36](#figure11-36)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-37：我们整个训练集与[图 11-36](#figure11-36)中的高斯分布重叠
- en: To see how well the classifier works in practice, let’s split the training data,
    putting a randomly selected 70 percent of the points into a training set and the
    rest into a test set. Let’s train with this new training set and then draw the
    test set on top of the Gaussians, giving us [Figure 11-38](#figure11-38).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看看分类器在实际中的表现如何，让我们将训练数据拆分，随机选择 70% 的点作为训练集，其余作为测试集。我们用这个新的训练集进行训练，然后将测试集绘制在高斯分布上，得到[图
    11-38](#figure11-38)。
- en: In [Figure 11-38](#figure11-38), we drew all the points that were classified
    as belonging to the first class on the left, and all those in the second class
    on the right, maintaining their original colors. We can see that all of the test
    samples were correctly classified.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 11-38](#figure11-38)中，我们将所有被分类为第一类的点绘制在左侧，所有第二类的点绘制在右侧，并保持它们原来的颜色。我们可以看到所有的测试样本都被正确分类。
- en: '![F11038](Images/F11038.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![F11038](Images/F11038.png)'
- en: 'Figure 11-38: The test data after training with 70 percent of our starting
    data'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-38：使用我们起始数据的 70% 进行训练后的测试数据
- en: Now let’s try an example where we don’tsatisfy the prior that all features of
    all samples follow Gaussian distributions. [Figure 11-39](#figure11-39) shows
    our new starting data of two noisy crescent moons.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试一个不满足所有样本特征遵循高斯分布先验假设的例子。[图 11-39](#figure11-39)显示了我们新的起始数据，包含两个噪声弯月形的数据。
- en: '![F11039](Images/F11039.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![F11039](Images/F11039.png)'
- en: 'Figure 11-39: Some noisy crescent moon data'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-39：一些噪声弯月形数据
- en: When we give these samples to the naive Bayes classifier, it assumes (as always)
    that the red x values, red y values, blue x values, and blue y values all come
    from Gaussian distributions. It finds the best Gaussians it can, shown in [Figure
    11-40](#figure11-40).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这些样本输入到朴素贝叶斯分类器时，它假设（和往常一样）红色 x 值、红色 y 值、蓝色 x 值和蓝色 y 值都来自高斯分布。它找到它能找到的最佳高斯分布，如[图
    11-40](#figure11-40)所示。
- en: '![F11040](Images/F11040.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![F11040](Images/F11040.png)'
- en: 'Figure 11-40: Fitting Gaussians to the crescent-moon data from [Figure 11-39](#figure11-39)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-40：将高斯分布拟合到[图 11-39](#figure11-39)中的弯月形数据
- en: Of course, these are not a good match to our data, because they don’t satisfy
    the assumptions. Overlaying the data on the Gaussians in [Figure 11-41](#figure11-41)
    shows that the matches aren’t abysmal, but they’re pretty far off.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些高斯分布与我们的数据并不匹配，因为它们不满足假设。将数据覆盖在[图 11-41](#figure11-41)中的高斯分布上，可以看到匹配并不糟糕，但也相差甚远。
- en: '![F11041](Images/F11041.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![F11041](Images/F11041.png)'
- en: 'Figure 11-41: Our training data from [Figure 11-39](#figure11-39) overlaid
    on the Gaussians of [Figure 11-40](#figure11-40)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-41：我们的训练数据来自[图 11-39](#figure11-39)，并叠加在[图 11-40](#figure11-40)的高斯分布上
- en: Like before, let’s now split the crescent moons into training and test sets,
    train on the 70 percent, and look at the predictions. In the left image of [Figure
    11-42](#figure11-42) we can see all the points that we assigned to the red class.
    As we would hope, this has most of the red points, but some of the points from
    the upper-left red moon are not classified as red, and some points from the lower-right
    blue moon are classified as red, because their value from this Gaussian is higher
    than their value from the other.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，接下来我们将新月形的数据分为训练集和测试集，在70%的数据上进行训练，并查看预测结果。在[图 11-42](#figure11-42)的左侧图像中，我们可以看到所有分配给红色类别的点。正如我们所希望的那样，这些大多数是红色点，但来自左上方红色新月的一些点没有被分类为红色，来自右下方蓝色新月的一些点被错误地分类为红色，因为它们来自这个高斯分布的值高于来自另一个的值。
- en: On the right of [Figure 11-42](#figure11-42), we can see that the opposite situation
    holds for the other Gaussian. In other words, we correctly classified lots of
    the points, but we also misclassified points in each class.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 11-42](#figure11-42)的右侧，我们可以看到另一种高斯分布的相反情况。换句话说，我们正确分类了许多点，但也在每个类别中都出现了误分类。
- en: We shouldn’t be too surprised at the misclassifications because our data did
    not follow the assumptions made by the naive Bayes prior. What is amazing is how
    well the classifier did. In general, naive Bayes often does a good job on all
    kinds of data. This is probably because lots of real-world data comes from processes
    that are well described by Gaussians.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应对误分类感到太惊讶，因为我们的数据并没有遵循朴素贝叶斯先验假设。令人惊讶的是分类器的表现如此出色。一般来说，朴素贝叶斯在各种数据上通常表现良好。这可能是因为许多现实世界的数据来源于可以很好地用高斯分布描述的过程。
- en: '![F11042](Images/F11042.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![F11042](Images/F11042.png)'
- en: 'Figure 11-42: Predictions of test data from our naive Bayes classifier trained
    on the data in [Figure 11-39](#figure11-39)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-42：我们在[图 11-39](#figure11-39)的数据上训练的朴素贝叶斯分类器的测试数据预测结果
- en: Because naive Bayes is so fast, it’s common to apply it when we’re trying to
    get a feeling for our data. If it does a great job, we might not have to look
    at any more complex algorithms.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 由于朴素贝叶斯非常快速，因此我们通常在试图了解数据时应用它。如果它表现出色，我们可能不需要再考虑更复杂的算法。
- en: Comparing Classifiers
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类器比较
- en: We looked at four popular classification algorithms in this chapter. Most machine
    learning libraries offer all of these, along with many others. Very briefly, let’s
    look at the pros and cons of these four classifiers, starting with the nonparametric
    algorithms.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们介绍了四种流行的分类算法。大多数机器学习库都提供了这些算法以及许多其他算法。简要地说，让我们看看这四种分类器的优缺点，从非参数算法开始。
- en: The kNN method is flexible. It doesn’t explicitly represent boundaries, so it
    can handle any kind of complicated structure formed by the class samples in the
    training data. It’s fast to train, since it typically just saves each training
    sample. On the other hand, prediction is slow, because the algorithm has to search
    for the nearest neighbors for every sample we want to classify (there are many
    efficiency methods that speed up this search, but it still takes time). And because
    it’s saving every training sample, the algorithm can consume huge gulps of memory.
    If the training set is larger than the available memory, the operating system
    typically needs to start saving data on the hard drive (or other external storage),
    which can slow the algorithm down significantly.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: kNN 方法非常灵活。它不显式地表示边界，因此可以处理训练数据中由类别样本形成的任何复杂结构。它的训练速度很快，因为通常只是保存每个训练样本。另一方面，预测速度较慢，因为算法必须为我们要分类的每个样本搜索最近的邻居（虽然有很多提高搜索效率的方法，但仍然需要时间）。并且因为它保存了每个训练样本，算法可能会消耗大量内存。如果训练集的大小超过了可用内存，操作系统通常需要开始将数据保存在硬盘（或其他外部存储）上，这可能会显著减慢算法的速度。
- en: Decision trees are fast to train, and they’re also fast when making predictions.
    They can handle weird boundaries between classes, though this can require a deeper
    tree. They have a huge downside due to their appetite for overfitting (though
    as we mentioned, we will address this issue later by using collections of small
    trees, so all is not lost). Decision trees have a huge appeal in practice because
    they are easy to interpret. Sometimes people use decision trees, even when the
    results are inferior to other classifiers, because their decisions are transparent,
    or easy to understand. Note that this doesn’t mean that the choices are fair or
    even correct, just that they’re comprehensible to humans.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树训练速度快，在做出预测时也很快。它们可以处理类之间的奇怪边界，尽管这可能需要更深的树。由于它们容易过拟合，决策树有一个巨大的缺点（不过正如我们所提到的，我们稍后会通过使用多个小树来解决这个问题，所以并非全是坏消息）。决策树在实践中非常有吸引力，因为它们容易解释。有时候即使结果不如其他分类器，人们仍然会使用决策树，因为它们的决策是透明的，或者说容易理解。需要注意的是，这并不意味着选择是公平或正确的，仅仅是它们对人类来说是可以理解的。
- en: Support vector machines are parametric algorithms that can make fast predictions.
    Once trained, they don’t need much memory, since they only store the boundaries
    between sample regions. And they can use the kernel trick to find classification
    boundaries that appear much more complicated than the straight lines (and flat
    planes, and higher-dimensional flat surfaces) that SVM produces. On the other
    hand, the training time grows with the size of the training set. The quality of
    the results is sensitive to the parameter *C* that specifies how many samples
    are allowed near the boundary. We can use cross-validation to try different values
    of *C* and pick out the best one.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机是参数化算法，可以快速做出预测。一旦训练好，它们不需要太多内存，因为它们只存储样本区域之间的边界。而且它们可以使用核技巧来找到看起来比直线（以及平面、甚至更高维度的平面）复杂得多的分类边界。另一方面，训练时间随着训练集大小的增加而增长。结果的质量对参数*C*非常敏感，*C*指定了允许多少样本靠近边界。我们可以使用交叉验证来尝试不同的*C*值并选出最佳的。
- en: Naive Bayes trains quickly and predicts quickly, and it’s not too hard to explain
    its results (though they’re a bit more abstract than decision trees or kNN results).
    The method has no parameters that we need to tune. If the classes we’re working
    with are well separated, then the naive Bayes prior often produces good results.
    The algorithm works particularly well when our data is Gaussian in nature. It
    also works well when the data has many features because the classes of such data
    are often separated in ways that play to the strengths of naive Bayes (VanderPlas
    2016). In practice, we often try naive Bayes early in the process of getting to
    know a dataset, because it’s fast to train and predict and can give us a feeling
    for the structure of our data. If the prediction quality is poor, we can then
    turn to a more expensive classifier (that is, one requiring more time or memory).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯训练迅速，预测也很快，而且它的结果也不难解释（尽管比起决策树或kNN结果来说，稍显抽象）。该方法没有需要调整的参数。如果我们处理的类分布较为分离，那么朴素贝叶斯的先验通常能产生很好的结果。当我们的数据呈高斯分布时，该算法效果尤其好。当数据有许多特征时，它也能很好地工作，因为这类数据的类别通常以能发挥朴素贝叶斯优势的方式分离（VanderPlas
    2016）。在实践中，我们通常会在了解数据集的初期尝试朴素贝叶斯，因为它训练和预测速度快，能让我们对数据的结构有一个初步的感觉。如果预测质量较差，我们可以转向更昂贵的分类器（即需要更多时间或内存的分类器）。
- en: The algorithms we’ve seen here are frequently used in practice, particularly
    when we’re first getting to know our data, because they’re usually straightforward
    to apply and visualize.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到的算法在实践中经常被使用，尤其是在我们初步了解数据时，因为它们通常很容易应用和可视化。
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered two types of classifier. When a classifier has no
    preconceptions on the structure of the data it’s going to look at, we say it is
    nonparametric. The *k-*nearest neighbors algorithm is of this variety, assigning
    a class to a sample based on its most popular neighbor. Decision trees are also
    nonparametric, assigning classes based on a series of decisions that are learned
    from the training data.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们介绍了两种类型的分类器。当一个分类器没有预设的数据结构假设时，我们称之为非参数化。*k-*最近邻算法属于这一类，它根据样本最常见的邻居来为样本分配类别。决策树也是非参数化的，它根据从训练数据中学习到的一系列决策来分配类别。
- en: On the other hand, parametric classifiers have a preconceived notion of the
    structure of the data. A basic support vector machine looks for linear shapes,
    like lines or planes, that separate the training data by class. A naive Bayes
    classifier presumes that the data has a fixed distribution, usually Gaussian,
    and then does its best to fit that distribution to each feature in the data.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，参数化分类器对数据的结构有一个先入为主的假设。一个基本的支持向量机寻找线性形状，如直线或平面，将训练数据按类别分开。朴素贝叶斯分类器假设数据具有固定的分布，通常是高斯分布，然后尽力将该分布拟合到数据的每个特征上。
- en: In the next chapter, we’ll see how to bundle together multiple classifiers to
    produce ensemble classifiers that outperform their individual components.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何将多个分类器结合起来，产生比单个组件更强大的集成分类器。
