<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><h2 class="h1" id="ch01"><span epub:type="pagebreak" id="page_3" class="calibre2"/><strong class="calibre3"><span class="big">1</span><br class="calibre18"/>REGRESSION MODELS</strong></h2>
<div class="imagec"><img alt="Image" src="../images/common.jpg" class="calibre14"/></div>
<p class="noindent">In this chapter, we’ll introduce <em class="calibre13">regression functions</em>. Such functions give the mean of one variable in terms of one or more others—for instance, the mean weight of children in terms of their age. All ML methods are <em class="calibre13">regression methods</em> in some form, meaning that they use the data we provide to estimate regression functions.</p>
<p class="indent">We’ll present our first ML method, k-nearest neighbors (k-NN), and apply it to real data. We’ll also weave in concepts that will recur throughout the book, such as dummy variables, overfitting, p-hacking, “dirty” data, and so on. We’ll introduce many of these concepts only briefly for the time being in order to give you a bird’s-eye view of what we’ll return to in detail later: ML is intuitive and coherent but easier to master if taken in stages. Reader, please be prepared for frequent statements like “We’ll cover one aspect for now, with further details later.”</p>
<p class="indent"><span epub:type="pagebreak" id="page_4"/>Before you begin, make sure you have R and the <code>qeML</code> and <code>regtools</code> packages, version 1.7 or newer for the latter, installed on your computer. (Run <span class="codestrong1">packageVersion('regtools')</span> to check.) All code displays in this book assume that the user has already made the calls to load the packages:</p>
<pre class="calibre16">library(regtools)
library(qeML)</pre>
<p class="indent">So, let’s look at our first example dataset.</p>
<h3 class="h2" id="ch01lev1">1.1 Example: The Bike Sharing Dataset</h3>
<p class="noindent">Before we introduce k-NN, we’ll need to have some data to work with. Let’s start with this dataset from the UC Irvine Machine Learning Repository, which contains the Capital Bikeshare system’s hourly and daily count of bike rentals between 2011 and 2012, with corresponding information on weather and other quantities. A more detailed description of the data is available at the UC Irvine Machine Learning Repository.<a href="footnote.xhtml#ch1fn1" id="ch1fn1b" class="calibre12"><sup class="calibre11">1</sup></a></p>
<p class="indent">The dataset is included as the <code>day</code> dataset in <code>regtools</code> by permission of the data curator. Note, though, that we will use a slightly modified version, <code>day1</code> (also included in <code>regtools</code>), in which the numeric weather variables are given in their original scale rather than transformed to the interval [0,1].</p>
<p class="indent">Our main interest will be in predicting total ridership for a day.</p>
<div class="sidebar">
<p class="boxtitle-d">SOME TERMINOLOGY</p>
<p class="noindent">Say we wish to predict ridership from temperature and humidity. Standard ML parlance refers to the variables used for prediction—in this case, temperature and humidity—as <em class="calibre13">features.</em></p>
<p class="noindents">If the variable to be predicted is numeric, say, ridership, there is no standard ML term for it. We’ll just refer to it as the <em class="calibre13">outcome</em> variable. But if the variable to be predicted is an R factor—that is, a categorical variable—it is called a <em class="calibre13">label</em>.</p>
<p class="noindents">For instance, later in this book we will analyze a dataset on diseases of human vertebrae. There are three possible outcomes or categories: normal (NO), disk hernia (DH), or spondylolisthesis (SL). The column in our dataset showing the class of each patient, NO, DH, or SL, would be the labels column.</p>
<p class="noindents">Our dataset, say, <code>day1</code> here, is called the <em class="calibre13">training set</em>. We use it to make predictions in future cases, in which the features are known but the outcome variable is unknown. We are predicting the latter.</p>
</div>
<h4 class="h3" id="ch01lev1sec1"><span epub:type="pagebreak" id="page_5" class="calibre2"/>1.1.1 Loading the Data</h4>
<p class="noindent">The data comes in hourly and daily forms, with the latter being the one in the <code>regtools</code> package. Load the data:</p>
<pre class="calibre16"><span class="codestrong">&gt; data(day1)</span></pre>
<p class="indent">With any dataset, it’s always a good idea to first take a look around. What variables are included in this data? What types are they, say, numeric or R factor? What are their typical values? One way to do this is to use R’s <code>head()</code> function to view the top of the data:</p>
<pre class="calibre16">&gt; <span class="codestrong">head(day1)</span>
  instant     dteday season yr mnth holiday
1       1 2011-01-01      1  0    1       0
2       2 2011-01-02      1  0    1       0
3       3 2011-01-03      1  0    1       0
4       4 2011-01-04      1  0    1       0
5       5 2011-01-05      1  0    1       0
6       6 2011-01-06      1  0    1       0
  weekday workingday weathersit     temp
1       6          0          2 8.175849
2       0          0          2 9.083466
3       1          1          1 1.229108
4       2          1          1 1.400000
5       3          1          1 2.666979
6       4          1          1 1.604356
      atemp      hum windspeed casual registered
1  7.999250 0.805833 10.749882    331        654
2  7.346774 0.696087 16.652113    131        670
3 -3.499270 0.437273 16.636703    120       1229
4 -1.999948 0.590435 10.739832    108       1454
5 -0.868180 0.436957 12.522300     82       1518
6 -0.608206 0.518261  6.000868     88       1518
   tot
1  985
2  801
3 1349
4 1562
5 1600
6 1606
&gt; <span class="codestrong">nrow(day1)</span>
[1] 731</pre>
<p class="indent">We see there are 731 rows (that is, 731 different days), with data on the date, nature of the date (such as <code>weekday</code>), and weather conditions (such as the temperature, <code>temp</code>, and humidity, <code>hum</code>). The last three columns measure ridership from casual users, registered users, and the total.</p>
<p class="indent">You can find more information on the dataset with the <code>?day1</code> command.</p>
<h4 class="h3" id="ch01lev1sec2"><span epub:type="pagebreak" id="page_6" class="calibre2"/>1.1.2 A Look Ahead</h4>
<p class="noindent">We will get to actual analysis of this data shortly. For now, here is a preview. Say we wish to predict total ridership for tomorrow, based on specific weather conditions and so on. How will we do that with k-NN?</p>
<p class="indent">We will search through our data, looking for data points that match or nearly match those same weather conditions and other variables. We will then average the ridership values among those data points, and that will be our predicted ridership for this new day.</p>
<p class="indent">Too simple to be true? No, not really; the above description is accurate. Of course, the old saying “The devil is in the details” applies, but the process is indeed simple. But first, let’s address some general issues.</p>
<h3 class="h2" id="ch01lev2">1.2 Machine Learning and Prediction</h3>
<p class="noindent">ML is fundamentally about prediction. Before we get into the details of our first ML method, we should be sure we know what “prediction” means.</p>
<p class="indent">Consider the bike sharing dataset. Early in the morning, the manager of the bike sharing service might want to predict the total number of riders for the day. The manager can do so by analyzing the relations between the features—the various weather conditions, the work status of the day (weekday, holiday), and so on. Of course, predictions are not perfect, but if they are in the ballpark of what turns out to be the actual number, they can be quite helpful. For instance, they can help the manager decide how many bikes to make available, with pumped-up tires and so on. (An advanced version would be to predict the demand for bikes at each station so that bikes could be reallocated accordingly.)</p>
<h4 class="h3" id="ch01lev2sec1">1.2.1 Predicting Past, Present, and Future</h4>
<p class="noindent">The famous baseball player and malapropist Yogi Berra once said, “Prediction is hard, especially about the future.” Amusing as this is, he had a point; in ML, prediction can refer not only to the future but also to the present or even the past. For example, a researcher may wish to estimate the mean wages workers made back in the 1700s. Or a physician may wish to make a diagnosis as to whether a patient has a particular disease, based on blood tests, symptoms, and so on, guessing their condition in the present, not the future. So when we in the ML field talk of “prediction,” don’t take the “pre-” too literally.</p>
<h4 class="h3" id="ch01lev2sec2">1.2.2 Statistics vs. Machine Learning in Prediction</h4>
<p class="noindent">A common misconception is that ML is concerned with prediction, while statisticians do <em class="calibre13">inference</em>—that is, confidence intervals and testing for quantities of interest—but prediction is definitely an integral part of the field of statistics.</p>
<p class="indent">There is sometimes a friendly rivalry between the statistics and ML communities, even down to a separate terminology for each (see <a href="app02.xhtml" class="calibre12">Appendix B</a>). <span epub:type="pagebreak" id="page_7"/>Indeed, statisticians sometimes use the term <em class="calibre13">statistical learning</em> to refer to the same methods known in the ML world as machine learning!</p>
<p class="indent">As a former statistics professor who has spent most of his career in a computer science department, I have a foot in both camps. I will present ML methods in computational terms, but with some insights informed by statistical principles.</p>
<div class="sidebar">
<p class="boxtitle-d">HISTORICAL NOTE</p>
<p class="noindents">Many of the methods treated in this book, which compose part of the backbone of ML, were originally developed in the statistics community. These include k-NN, decision trees or random forests, logistic regression, and L1/L2 shrinkage. These evolved from the linear models formulated way back in the 19th century, but which later statisticians felt were inadequate for some applications. The latter consideration sparked interest in methods that had less restrictive assumptions, leading to the invention first of k-NN and later of other techniques.</p>
<p class="noindents">On the other hand, two other prominent ML methods, support vector machines (SVMs) and neural networks, have been developed almost entirely outside of statistics, notably in university computer science departments. (Another method, <em class="calibre13">boosting</em>, began in computer science but has had major contributions from both factions.) Their impetus was not statistical at all. Neural networks, as we often hear in the media, were studied originally as a means to understand the workings of the human brain. SVMs were viewed simply in computer science algorithmic terms—given a set of data points of two classes, how can we compute the best line or plane separating them?</p>
</div>
<h3 class="h2" id="ch01lev3">1.3 Introducing the k-Nearest Neighbors Method</h3>
<p class="noindent">Our featured method in this chapter will be <em class="calibre13">k-nearest neighbors</em>, or <em class="calibre13">k-NN</em>. It’s arguably the oldest ML method, going back to the early 1950s, but it is still widely used today, especially in applications in which the number of features is small (for reasons that will become clear later). It’s also simple to explain and easy to implement—the perfect choice for this introductory chapter.</p>
<h4 class="h3" id="ch01lev3sec1">1.3.1 Predicting Bike Ridership with k-NN</h4>
<p class="noindent">Let’s first look at using k-NN to predict bike ridership from a single feature: temperature. Say the day’s temperature is forecast to be 28 degrees centigrade. How should we predict ridership for the day, using the 28 figure and our historical ridership dataset (our training set)? A person without a background in ML might suggest looking at all the days in our data, culling out those of temperature closest to 28 (there may be few or none with a temperature of exactly 28), and then finding the average ridership on those days. We would use that number as our predicted ridership for this day.</p>
<p class="indent">Actually, this intuition is correct! This, in fact, is the basis for many common ML methods, as we’ll discuss further in <a href="ch01.xhtml#ch01lev6" class="calibre12">Section 1.6</a> on the <span epub:type="pagebreak" id="page_8"/>regression function. For now, just know that k-NN takes the form of simply averaging over the similar cases—that is, over the neighboring data points. The quantity <em class="calibre13">k</em> is the number of neighbors we use. We could, say, take the 5 historical days with temperatures closest to 28, average the numbers for those days, and use the result to predict ridership on a new 28-degree day.</p>
<p class="indent">Later in this chapter, we will learn to use the <code>qe*</code>-series implementation of k-NN, <code>qeKNN()</code>. For now, though, let’s perform k-NN “manually,” so as to get a better understanding of the method.</p>
<p class="indent">Okay, ready to go! In this section we will make our first predictions.</p>
<h5 class="h4">1.3.1.1 R Subsetting Review</h5>
<p class="noindent">Before presenting the code, let’s review a few aspects of the R language. Recall that in R, the <code>#</code> symbol is for comments; that is, it is not part of the code itself but is for explanatory purposes. The comments in the code below and throughout the book are used as inline explanations of what the code is doing.</p>
<p class="indent">For the upcoming example, you’ll also need to remember how subsetting works in R. Take this snippet, for instance:</p>
<pre class="calibre16">&gt; <span class="codestrong">x &lt;- c(5,12,13,8,88)</span>
&gt; <span class="codestrong">x[c(2,4,5)]</span>
[1] 12  8 88</pre>
<p class="noindent">The expression <code>x[c(2,4,5)]</code> extracts the 2nd, 4th, and 5th elements from the vector <code>x</code>. Also recall that here we refer to 2, 4, and 5 as <em class="calibre13">subscripts</em> or <em class="calibre13">indices</em>.</p>
<h5 class="h4" id="ch01lev3sec1sec2">1.3.1.2 A First Prediction</h5>
<p class="noindent">Here, then, is our small, manually performed k-NN example:</p>
<pre class="calibre16">&gt; <span class="codestrong">data(day1)</span>
&gt; <span class="codestrong">tmps &lt;- day1$temp</span>
&gt; <span class="codestrong">dists &lt;- abs(tmps - 28)</span>  # distances of the temps to 28
&gt; <span class="codestrong">do5 &lt;- order(dists)[1:5]</span>  # which are the 5 closest?
&gt; <span class="codestrong">dists[do5]</span>  # and how close are they?
[1] 0.005849 0.033349 0.045000 0.045000 0.084151</pre>
<p class="indent">The distance between any two numbers is the absolute value of their difference: |25 − 32| = 7, so 25 is a distance 7 from 32. This is why we made the call to R’s <code>abs()</code> (absolute value) function. R’s <code>order()</code> function is like <code>sort()</code>, except that it shows us the indices of the sorted numbers. Here is an example:</p>
<pre class="calibre16">&gt; <span class="codestrong">x &lt;- c(12,5,8,88,13)</span>
&gt; <span class="codestrong">order(x)</span>
[1] 2 3 1 5 4</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_9"/>The values 2, 3, and so on are saying, “The first-smallest number in <code>x</code> was <code>x[2]</code>, the second-smallest was <code>x[3]</code>, and so on.” The line</p>
<pre class="calibre16">&gt; <span class="codestrong">do5 &lt;- order(dists)[1:5]</span>  # which are the 5 closest?</pre>
<p class="noindent">will place in <code>do5</code> the indices of rows in <code>day1</code> that have the 5-closest temperatures to 28. In this case, the temperatures are quite close to 28; the furthest is only 0.08 distant.</p>
<p class="indent">What were the ridership values on those days?</p>
<pre class="calibre16">&gt; <span class="codestrong">day1$tot[do5]</span>
[1] 7175 4780 4326 5687 3974</pre>
<p class="noindent">We then take the average of those values:</p>
<pre class="calibre16">&gt; <span class="codestrong">mean(day1$tot[do5])</span>
[1] 5188.4</pre>
<p class="indent">We can now predict that on a day with a temperature of 28 degrees, about 5,200 riders will use the bike sharing service.</p>
<p class="indent">There are some issues left hanging, notably: Why take the 5 days nearest in temperature to 28? Is 5 too small a sample, or is it sufficient to make an accurate prediction? This is a central issue in ML, to which we’ll return in <a href="ch01.xhtml#ch01lev7" class="calibre12">Section 1.7</a>.</p>
<h3 class="h2" id="ch01lev4">1.4 Dummy Variables and Categorical Variables</h3>
<p class="noindent">To work with this dataset, and do ML in general, you’ll need to be able to understand the several columns in the data that represent <em class="calibre13">dummy variables</em>. Dummy variables take on values 1 and 0 only, depending on whether they satisfy a particular condition. For instance, in the <code>workingday</code> column, 0 stands for “No” (the date in question is not a working day) and 1 stands for “Yes” (the date is a working day). The date 2011-01-05 has a 1 in the <code>workingday</code> column, meaning yes, this was a working day.</p>
<p class="indent">Dummy variables are sometimes more formally called <em class="calibre13">indicator variables</em> because they <em class="calibre13">indicate</em> whether a certain condition holds (code 1) or not (code 0). An alternative term popular in ML circles is <em class="calibre13">one-hot coding</em>.</p>
<p class="indent">Our bike sharing data also includes the categorical variables <code>mnth</code> and <code>weekday</code>. There is also a feature <code>weathersit</code> consisting of four categories (1 = clear, 2 = mist or cloudy, 3 = light snow or light rain, 4 = heavy rain or ice pellets or thunderstorm). That variable could be considered categorical as well.</p>
<p class="indent">One very common usage of dummy variables is coding of categorical data. In a marketing study, for instance, a factor of interest might be type of region of residence, say, Urban, Suburban, or Rural. Our original data might code these as 1, 2, or 3. However, those are just arbitrary codes, so, for example, there is no implication that Rural is 3 times as good as Urban. Yet ML algorithms may take it that way, which is not what we want.</p>
<p class="indent"><span epub:type="pagebreak" id="page_10"/>The solution, used throughout this book and throughout the ML field, is to use dummy variables. We could have a dummy variable for Urban (1 = yes, 0 = no) and one for Suburban. Rural-ness would then be coded by having both Urban and Suburban set to 0, so we don’t need a third dummy variable (having one might cause technical problems beyond the scope of this book). Of course, there is nothing special about using the first two values as dummies; we could have, say, one for Urban and one for Rural, without Suburban; the latter would then be indicated by 0 values in Urban and Rural.</p>
<p class="indent">In the current chapter, we focus on applications in which our outcome variable is numeric, such as total ridership in the bike sharing data. But in many applications, the outcome variable is categorical, such as our earlier example of predicting vertebral disease. In such settings, termed <em class="calibre13">classification applications</em>, the <em class="calibre13">Y</em> variable is categorical and must be converted to dummies.</p>
<p class="indent">Fortunately, most ML packages, including <code>qeML</code>, do conversions to dummies automatically, as we will see shortly.</p>
<h3 class="h2" id="ch01lev5">1.5 Analysis with qeKNN()</h3>
<p class="noindent">Now that we have a better sense of what’s going on under the hood, let’s try using the <code>qeKNN()</code> function to perform some k-NN analysis. As noted in the introduction, this book uses the <code>qe*</code>-series wrapper functions. For k-NN, this means <code>qeKNN()</code>. The latter <em class="calibre13">wraps</em>, or provides a simple interface for, <code>regtools</code>’s basic k-NN function, <code>kNN()</code>. In other words, <code>qeKNN()</code> calls <code>kNN()</code> but in a simpler, more convenient manner.</p>
<p class="indent">Before we start our analysis, we’ll introduce some “X” and “Y” notation to help us keep track of what we’re doing. Take notes—this will be important to remember for all subsequent chapters.</p>
<div class="sidebar">
<p class="boxtitle-d"><strong class="calibre3">FEATURES X AND OUTCOMES Y</strong></p>
<p class="noindents">The following informal shorthand used to refer to features and outcomes is pretty standard in both the ML and statistics fields:</p>
<ul class="calibre15">
<li class="noindent3">Traditionally, one collectively refers to the features as <em class="calibre13">X</em> and the outcome to be predicted as <em class="calibre13">Y</em>.</li>
<li class="noindent3"><em class="calibre13">X</em> is a set of columns in a data frame or matrix. If we are predicting ridership from temperature and humidity, <em class="calibre13">X</em> consists of those latter two columns in our data. <em class="calibre13">Y</em> here is the ridership column.</li></ul>
<p class="noindents">In 2-class classification applications, <em class="calibre13">Y</em> will typically be a dummy variable, a column of 1s and 0s. However, in multiclass classification applications, <em class="calibre13">Y</em> is a set of columns, one for each dummy variable. Equivalently, <em class="calibre13">Y</em> could be an R factor, stored in a single column.</p>
<br class="calibre1"/>
<p class="noindent"><span epub:type="pagebreak" id="page_11"/>One more bit of standard notation:</p>
<ul class="calibre15">
<li class="noindent3">The number of rows in <em class="calibre13">X</em>—that is, the number of data points—is typically denoted by <em class="calibre13">n</em>.</li>
<li class="noindent3">The number of columns in <em class="calibre13">X</em>—that is, the number of features—is typically denoted by <em class="calibre13">p</em>.</li></ul>
<p class="noindent">This is just a convenient shorthand. It’s easier, for instance, to say “X” rather than the more cumbersome “our feature set.” Again, <em class="calibre13">X</em>, <em class="calibre13">Y</em>, <em class="calibre13">n</em>, and <em class="calibre13">p</em> will appear throughout this book (and elsewhere, as they are standard in the ML field), so be sure to commit them to memory.</p>
</div>
<h4 class="h3" id="ch01lev5sec1">1.5.1 Predicting Bike Ridership with qeKNN()</h4>
<p class="noindent">For the bike sharing example, we’ll predict total ridership on any given day. Let’s start out using as features just the dummy for working day and the numeric weather variables.</p>
<p class="indent">Let’s extract those columns of the <code>day1</code> data frame. As we saw in <a href="ch01.xhtml#ch01lev1sec1" class="calibre12">Section 1.1.1</a>, they are in columns 8 and 10 through 13, with column 16 containing the outcome variable, the total ridership (<code>tot</code>). Thus we can obtain them via the following expression:</p>
<pre class="calibre16">day1[,c(8,10:13,16)]</pre>
<p class="noindent">This extracts the desired columns.</p>
<div class="note">
<p class="notet"><strong class="calibre3"><span class="notes">NOTE</span></strong></p>
<p class="notep"><em class="calibre13">Alternatively, you may prefer to use column names rather than numbers</em></p>
<pre class="calibre16"><span class="codeitalic">day1[c('workingday','temp','atemp','hum','windspeed','tot')]</span></pre>
<p class="notep"><em class="calibre13">in base R, or use the tidyverse or <span class="codeitalic1">data.table</span>, each of which works fine. Numeric data frame indexing is much easier to type, but use of column names may be clearer.</em></p>
<p class="notei"><em class="calibre13">As pointed out in the introduction, this is a book about ML that happens to use R as its vehicle of instruction rather than a book about R in ML. The way that individual readers handle data manipulation in R is not the focus of this book, so feel free to use your own preferred way to achieve the same results.</em></p>
</div>
<p class="indent">So, we form the sub-data frame, and as usual, take a look.</p>
<pre class="calibre16">&gt; <span class="codestrong">day1 &lt;- day1[,c(8,10:13,16)]</span>
&gt; <span class="codestrong">head(day1)</span>
  workingday     temp     atemp      hum windspeed  tot
1          0 8.175849  7.999250 0.805833 10.749882  985
2          0 9.083466  7.346774 0.696087 16.652113  801
3          1 1.229108 -3.499270 0.437273 16.636703 1349
4          1 1.400000 -1.999948 0.590435 10.739832 1562
5          1 2.666979 -0.868180 0.436957 12.522300 1600
6          1 1.604356 -0.608206 0.518261  6.000868 1606</pre>
<p class="indent"><span epub:type="pagebreak" id="page_12"/>Now let’s try a prediction. Say you are the manager this morning, and the day is a working day, with temperature 12.0, atemp 11.8, humidity 23 percent, and wind at 5 miles per hour. What is your prediction for the ridership?</p>
<p class="indent">All the <code>qe*</code>-series functions have a very simple call form:</p>
<pre class="calibre16">qeSomeMLmethod(<span class="codeitalic">your_data_frame</span>, Y_name, <span class="codeitalic">options_if_any</span>)</pre>
<p class="noindent">The option used in this example will be <em class="calibre13">k</em>, the number of nearest neighbors, which we’ll take to be 5. (If we do not specify <em class="calibre13">k</em> in the call, the default value is 25.)</p>
<pre class="calibre16">&gt; <span class="codestrong">knnout &lt;- qeKNN(day1,'tot',k=5)</span>  # fit the k-NN model
holdout set has 73 cases</pre>
<p class="noindent">We are applying the k-NN method, taking our “Y” (or “outcome,” as you’ll recall) to be the variable <code>tot</code> in the data frame <code>day1</code>, with 5 neighbors. We’ll discuss the holdout set shortly, but put that aside for now.</p>
<p class="indent">We saved the return value of <code>qeKNN()</code> in <code>knnout</code>. (Of course, you can use whatever name you wish.) It contains lots of information, but we won’t consider it at this point. The function <code>qeKNN()</code> did the prep work, enabling us to use <code>knnout</code> to do future predictions.</p>
<p class="indent">So, how exactly is that done? The general form for the <code>qe*</code>-series functions is likewise extremely simple:</p>
<pre class="calibre16">predict(output_from_qe_function, new_case)</pre>
<p class="noindent">Let’s say, as manager of the bike sharing business, we know that today is a workday, and the temperature, atemp, humidity, and wind speed will be 12.8, 11.8, 0.23, and 5, respectively. Here is our prediction:</p>
<pre class="calibre16">&gt; <span class="codestrong">today &lt;- data.frame(workingday=1, temp=12.8, atemp=11.8, hum=0.23,</span>
   <span class="codestrong">windspeed=5)</span>
&gt; <span class="codestrong">predict(knnout,today)</span>  # predict new case
     [,1]
[1,] 6321</pre>
<p class="noindent">We fed <code>knnout</code> into the <code>predict()</code> function, along with our specified prediction point, <code>today</code>, yielding the predicted number of riders: about 6,300. Observe that the argument <code>today</code> was a data frame with the same column names as in the original dataset <code>day1</code>. This is needed, both here and in many R ML packages, in order to match up the names in the prediction point with the names of the training set.</p>
<div class="note">
<p class="notet"><strong class="calibre3"><span class="notes">NOTE</span></strong></p>
<p class="notep"><em class="calibre13">If you run the above code yourself, you will likely get different output, due to randomness of the holdout set. There is a way to standardize the result so that different people obtain the same result; this will be explained in <a href="ch01.xhtml#ch01lev12sec3" class="calibre12">Section 1.12.3</a>.</em></p>
</div>
<p class="indent"><span epub:type="pagebreak" id="page_13"/>Let’s do another prediction, say, the same as above but with wind speed at 18:</p>
<pre class="calibre16">&gt; <span class="codestrong">anotherday &lt;- today</span>
&gt; <span class="codestrong">anotherday$windspeed &lt;- 18</span>
&gt; <span class="codestrong">predict(knnout,anotherday)</span>
     [,1]
[1,] 5000</pre>
<p class="noindent">People don’t seem to want to ride as much in the wind.</p>
<p class="indent">The second argument, such as <code>anotherday</code> above, can be any data frame with the same column names. For instance, we could have asked for both predictions together:</p>
<pre class="calibre16">&gt; <span class="codestrong">predict(knnout,rbind(today,anotherday))</span>
     [,1] [,2]
[1,] 6321 5000</pre>
<p class="indent">In <a href="#ch01lev8" class="calibre12">Section 1.8</a>, we will deepen our insight into k-NN by analyzing another dataset.</p>
<div class="sidebar">
<p class="boxtitle-d">TECHNICAL NOTE: PREDICT() AND GENERIC FUNCTIONS</p>
<p class="noindents">We saw that <code>qeKNN()</code> is paired with a <code>predict()</code> function. All functions in the <code>qe*</code>-series are similarly paired, which is a very common technique in R.</p>
<p class="noindents">While all these functions appear to share the same <code>predict()</code>, each one actually has its own separate prediction function, such as <code>predict.qeKNN()</code> in the case of <code>qeKNN()</code>. The function <code>predict()</code> itself is called a <em class="calibre13">generic function</em> in R. It simply <em class="calibre13">dispatches</em> calls, meaning it relays the original call to a function specific to the type of analysis we are doing. Thus a call to <code>predict()</code> on an object created by <code>qeKNN()</code> will actually be relayed to <code>predict.qeKNN()</code>, and so on.</p>
<p class="noindents">R has various generic functions, some of which you’ve probably already been using, perhaps without knowing it. Examples are <code>print()</code>, <code>plot()</code>, and <code>summary()</code>. We’ll see an example of <code>plot()</code> in <a href="ch09.xhtml#ch09lev6" class="calibre12">Section 9.6</a>.</p>
</div>
<h3 class="h2" id="ch01lev6">1.6 The Regression Function: The Basis of ML</h3>
<p class="noindent">To understand machine learning methods, you need to know <em class="calibre13">what</em> is being “learned.” The answer is something called the <em class="calibre13">regression function</em>. Directly or indirectly (often the latter), it is the basis for ML methods. It gives the mean value of one variable, holding another variable fixed. Let’s make this concrete.<span epub:type="pagebreak" id="page_14"/></p>
<div class="note">
<p class="notet"><strong class="calibre3"><span class="notes">NOTE</span></strong></p>
<p class="notep">Regression function <em class="calibre13">is a general statistical and ML term, much broader than the concept of linear regression that some readers may have learned in a statistics course.</em></p>
</div>
<p class="indent">Recall our example above, where we took as our predicted value for a 28-degree day the mean ridership of days near that temperature. If we were to predict the ridership on a 15-degree day, we would use for our prediction the mean ridership of all days with a temperature of 15, or near 15, and so on. Denoting the regression function by <em class="calibre13">r</em>(), the quantities of interest are <em class="calibre13">r</em>(28), <em class="calibre13">r</em>(15), and so on.</p>
<p class="indent">We say <em class="calibre13">r</em>() is the regression function of ridership on temperature. It is indeed a function; for every input (a temperature), we get an output (the associated mean ridership). And we use the function as our predictions; for instance, to predict ridership on a 15-degree day, we use an estimate of <em class="calibre13">r</em>(15).</p>
<p class="indent">But it’s an unknown function, not something familiar like <code>sqrt()</code>. So, we need to use our training data to infer values of the function. In ML parlance, we say that we “learn” this function, using our data, showing you where the L comes from in ML. (The M just means we use a computer or algorithm to do the learning.) For instance, in the above example, we <em class="calibre13">learn r</em>(28) by averaging the ridership values over the days with a temperature close to 28. The word <em class="calibre13">learn</em> is thus reflected in <em class="calibre13">train</em>, in the term <em class="calibre13">training data</em>.</p>
<p class="indent">It is customary to use the “hat” notation for “estimate of.” Thus we denote our estimate of <em class="calibre13">r</em>() by <img alt="Images" class="middle" src="../images/rcap.jpg"/>.</p>
<p class="indent">The regression function is also known as the <em class="calibre13">conditional mean</em>. In predicting ridership from temperature, <em class="calibre13">r</em>(28) is the mean ridership, subject to the <em class="calibre13">condition</em> that temperature is 28. That’s a subpopulation mean, which is quite different from the overall population mean ridership.</p>
<p class="indent">Let’s summarize these important points:</p>
<ul class="calibre15">
<li class="noindent3">The regression function <em class="calibre13">r</em>() gives the mean of our outcome variable as a function of our features.</li>
<li class="noindent3">We estimate <em class="calibre13">r</em>() from our training data. We call the estimate <img alt="Images" class="middle" src="../images/rcap.jpg"/>.</li>
<li class="noindent3">We use <img alt="Images" class="middle1" src="../images/rcap1.jpg"/> as the basis of our predictions.</li></ul>
<p class="indent">Any function has arguments. A regression function has as many arguments as we have features. Let’s take humidity as a second feature, for instance. To predict ridership for a day with temperature 28 and humidity 0.51, we would use the mean ridership in our dataset among days in which temperature and humidity are approximately 28 and 0.51. In regression function notation, that’s <em class="calibre13">r</em>(28, 0.51). In the example on <a href="ch01.xhtml#page_12" class="calibre12">page 12</a>, the value of interest was <em class="calibre13">r</em>(1, 12.8, 11.8, 0.23, 5).</p>
<p class="indent">As previously noted, the regression function forms the basis, directly or indirectly, in all predictive ML methods. It will come up repeatedly throughout the book.</p>
<h3 class="h2" id="ch01lev7"><span epub:type="pagebreak" id="page_15" class="calibre2"/>1.7 The Bias-Variance Trade-off</h3>
<p class="noindent">In the introduction, specifically in <a href="introduction.xhtml#ch00lev8" class="calibre12">Section 0.8</a>, we implored the reader:</p>
<p class="block">A page that is all prose—no math, no graphs, and no code—may be one of the most important pages in the book.</p>
<p class="noindent">The pages in this current section are prime examples of this, as the <em class="calibre13">Bias-Variance Trade-off</em> is one of the most famous topics in the field. My Google query yielded 18,400,000 results! It is an absolutely central issue in ML, which we will treat in depth in <a href="ch03.xhtml" class="calibre12">Chapter 3</a>. However, you should be aware of it from the beginning, so let’s give an overview.</p>
<p class="indent">The issue is, for example, the choice between greater or smaller values of <em class="calibre13">k</em>. Larger values of <em class="calibre13">k</em> have smaller variance but larger bias, and smaller values of <em class="calibre13">k</em> have the opposite effect. Let’s see how this works.</p>
<h4 class="h3" id="ch01lev7sec1">1.7.1 Analogy to Election Polls</h4>
<p class="noindent">First consider an analogy to election surveys. During an election campaign, voter polls will be taken to estimate the popularity of the various candidates. An analyst takes a random sample of the set of all telephone numbers and solicits opinions from those who pick up the phone.</p>
<p class="indent">Suppose we are interested in <em class="calibre13">p</em>, the proportion of the entire population that favors Candidate C. Since we just have a sample from that population, we can only estimate the value of <em class="calibre13">p</em> using the proportion <img alt="Images" class="middle2" src="../images/pcap.jpg"/> who like Candidate C in our sample. Accordingly, the poll results are accompanied by a <em class="calibre13">margin of error</em>, to recognize that the reported proportion <img alt="Images" class="middle2" src="../images/pcap.jpg"/> is only an estimate of <em class="calibre13">p</em>. (Those who have studied statistics may know that the margin of error is the radius of a 95 percent confidence interval.)</p>
<p class="indent">The margin of error gives an indication of the accuracy of our estimate. It measures sampling variability. A large value means that our estimate of <em class="calibre13">p</em> varies a lot from one sample to another; if the pollster were to call a new random sample of phone numbers, the value of our estimated <em class="calibre13">p</em> likely would be rather different, possibly quite different if the sample size is small. Of course, the pollster is not going to take a second sample, but the amount of sampling variability from one sample to the next tells us how reliable <img alt="Images" class="middle2" src="../images/pcap.jpg"/> is an estimate of <em class="calibre13">p</em>. The margin of error reflects that sampling variability, and if it is large, then our sample size was too small.</p>
<p class="indent">The key issue, then, is sampling variability, which is called the <em class="calibre13">variance</em> of <img alt="Images" class="middle2" src="../images/pcap.jpg"/>. It can be computed in the polling example from the margin of error.</p>
<p class="indent">A bias issue may also arise. Suppose the pollster has a list of landline phones but not cell phones. Many people, especially younger ones, don’t have a landline, so calling only those with landlines may bias our results.</p>
<h4 class="h3" id="ch01lev7sec2">1.7.2 Back to ML</h4>
<p class="noindent">Returning to ML, consider the bike sharing example in <a href="#ch01lev5sec1" class="calibre12">Section 1.5.1</a>, where we wished to have the value of <em class="calibre13">r</em>(1, 12.8, 11.8, 0.23, 5), which we wish to use <span epub:type="pagebreak" id="page_16"/>as our predicted value. We will obtain an estimate, <img alt="Images" class="middle1" src="../images/rcap1.jpg"/>(1, 12.8, 11.8, 0.23, 5), as the actual predicted value.</p>
<p class="indent">We treat the data on the number of riders per day as a sample from the (rather conceptual) population of all days, past, present, and future. Using the k-NN method (or any other ML method), we are only obtaining an estimate <img alt="Images" class="middle1" src="../images/rcap1.jpg"/> of the true population regression function <em class="calibre13">r</em>(). Forming a prediction from just the closest <em class="calibre13">k</em> = 5 neighbors works from a very small sample. Imagine the pollster sampling only 5 voters! In another sample, the closest days to our point to be predicted would be different, with different ridership values. <em class="calibre13">In other words, this is a variance issue.</em></p>
<p class="indent">On the other hand, using just <em class="calibre13">k</em> = 5, we found in <a href="ch01.xhtml#ch01lev3sec1sec2" class="calibre12">Section 1.3.1.2</a> that the 5 neighbors were all quite close to the prediction point. Suppose we look at the nearest <em class="calibre13">k</em> = 50 neighbors. In that case, we risk using data points far from the prediction point, which are thus not very similar to it. <em class="calibre13">This would create a bias problem.</em></p>
<p class="indent">In sum, larger values of <em class="calibre13">k</em> reduce variance but at the expense of increasing bias. We want to find a “Goldilocks” value for <em class="calibre13">k</em>—that is, one not too small and not too large.</p>
<p class="indent">However, note that computation time and the amount of memory the method requires are also important factors: if the best method takes too long to run or uses too much memory, we may end up choosing a different method.</p>
<h3 class="h2" id="ch01lev8">1.8 Example: The mlb Dataset</h3>
<p class="noindent">To cement your understanding of <code>qeKNN()</code> and introduce another example that we can refer to throughout the rest of the book, let’s try a similar operation on the <code>mlb</code> dataset. This dataset, provided courtesy of the UCLA Statistics Department, records the heights and weights of Major League Baseball players in inches and pounds, respectively. It’s included in <code>regtools</code>.</p>
<p class="indent">Let’s glance at the data first so we know what we’re working with:</p>
<pre class="calibre16">&gt; <span class="codestrong">data(mlb)</span>
&gt; <span class="codestrong">head(mlb)</span>
             Name Team       Position Height
1   Adam_Donachie  BAL        Catcher     74
2       Paul_Bako  BAL        Catcher     74
3 Ramon_Hernandez  BAL        Catcher     72
4    Kevin_Millar  BAL  First_Baseman     72
5     Chris_Gomez  BAL  First_Baseman     73
6   Brian_Roberts  BAL Second_Baseman     69
  Weight   Age PosCategory
1    180 22.99     Catcher
2    215 34.69     Catcher
3    210 30.78     Catcher
4    210 35.43   Infielder
5    188 35.71   Infielder
6    176 29.39   Infielder</pre>
<p class="indent"><span epub:type="pagebreak" id="page_17"/>Let’s predict the weight of a new player for whom it is only known that height and age are 72 and 24, respectively.</p>
<pre class="calibre16">&gt; <span class="codestrong">w &lt;- mlb[,c(4:6)]</span>  # extract height, weight, and age
&gt; <span class="codestrong">z &lt;- qeKNN(w,'Weight')</span>  # fit k-NN model
holdout set has  101 rows
&gt; <span class="codestrong">predict(z,data.frame(Height=72,Age=24))</span>
       [,1]
[1,] 182.56</pre>
<p class="noindent">Note again that we needed to specify the prediction point (72,24) in the same data frame form as <code>mlb</code>, the dataset on which we had fit the model.</p>
<h3 class="h2" id="ch01lev9">1.9 k-NN and Categorical Features</h3>
<p class="noindent">In the previous baseball player example, both of the features, height and age, were numeric. But what if we were to add a third feature, <code>Position</code>, a categorical variable? Since k-NN is distance-based, the features need to be numeric in order to compute distances between data points. How can we use k-NN with non-numeric features?</p>
<p class="indent">The answer, of course, is that the categorical variables (that is, R factors) should be converted to dummy variables. We could do this here via the <code>regtools</code> function <code>factorToDummies()</code>. However, as the <code>qe*</code>-series functions do this conversion internally when needed, we need not convert <code>Position</code> to dummies on our own. The <code>qeKNN()</code> function will also make a note in its output for later use by <code>predict()</code>, to make the same conversions when predicting.</p>
<p class="indent">For example, suppose we want to calculate another new player’s weight, using the categorical <code>Position</code> feature, in addition to height and age:</p>
<pre class="calibre16">&gt; <span class="codestrong">knnout1 &lt;- qeKNN(mlb[,3:6],'Weight',25)</span>  # extract Position, Height, Age
&gt; <span class="codestrong">predict(knnout1,data.frame(Height=72,Age=24,Position='Catcher'))</span>
     [,1]
[1,]  197</pre>
<p class="noindent">In our first prediction, not using <code>Position</code>, our predicted value was about 183 pounds. But if we know that the new player is a catcher, we see that, at least in this case, our predicted value increases to 197. This makes sense; catchers do tend to be heavyset so they can guard home plate.</p>
<p class="indent">The <code>qe*</code>-series functions identify categorical features by their status as R factors. In the example here, <code>Position</code> is indeed an R factor. As noted, the internals of <code>qeKNN()</code> and <code>predict.qeKNN()</code> will automatically do the conversion to dummies for us, which is a major convenience.</p>
<p class="indent">Typically, categorical features will already be expressed as factors in data frames. In some cases, a feature will be categorical but expressed in terms of numeric codes. If so, apply <code>as.factor()</code> to the feature to convert it to factor form.</p>
<h3 class="h2" id="ch01lev10"><span epub:type="pagebreak" id="page_18" class="calibre2"/>1.10 Scaling</h3>
<p class="noindent">A theme encountered in many ML methods is that of <em class="calibre13">scaling</em> our data. In your future ML projects, it’s good practice to keep scaling in mind. It’s used in many ML methods and may produce better results even if a method doesn’t require it.</p>
<p class="indent">Let’s go back to the baseball player data. Consider two players, one of height 70 and age 24 and one of height 72 and age 30. Taking these two pairs of numbers in an algebra context, the distance between them is the distance between the points (70,24) and (72,30) in the plane:</p>
<div class="imagec"><img alt="Image" src="../images/ch1eq1.jpg" class="calibre19"/></div>
<p class="noindent">Distances like this would be computed in k-NN. But what if height were converted to, say, meters, and age to months? The heights would be divided by 39.37, while age would be multiplied by 12:</p>
<pre class="calibre16">&gt; <span class="codestrong">sqrt((70/39.37-72/39.37)^2 + (12*24-12*30)^2)</span>
[1] 72.00002</pre>
<p class="noindent">That creates a problem in that age would dominate the computation, with height playing only a small role. Since height obviously is a major factor in predicting weight, the change in units would probably reduce our prediction ability.</p>
<p class="indent">The solution is to do away with units like inches, meters, and so on, which is called <em class="calibre13">scaling</em>. To scale our data, we first subtract the mean, giving everything mean 0, called <em class="calibre13">centering</em>. Then we divide each feature by its standard deviation, <em class="calibre13">scaling</em>. This gives everything a standard deviation of 1. Now all the features are unitless and commensurate. (Usually, when doing scaling, we also do centering, and the combined process is simply called scaling.)</p>
<p class="indent">To make this more concrete, as a former or current student, you may recall one of your professors converting examination scores in this way: “To get an A, you needed to be 1.5 standard deviations above the mean.” That professor was subtracting the mean exam score, then dividing by the standard deviation. The R function <code>scale()</code> performs this operation for us, but to illustrate, here is how we would do it on our own:</p>
<pre class="calibre16">&gt; <span class="codestrong">ht &lt;- mlb$Height</span>
&gt; <span class="codestrong">ht &lt;- (ht - mean(ht)) / sd(ht)</span>
&gt; <span class="codestrong">mlb$Height &lt;- ht</span></pre>
<p class="indent">The <code>qeKNN()</code> function has an argument <code>scaleX</code> for this purpose. Its default value is <code>TRUE</code>, so scaling was done by default in our above k-NN examples. In each <em class="calibre13">X</em> column (recall this means feature column) of our data, <code>qeKNN()</code> will transform that column by calling <code>scale()</code>. (Actually, we can scale all the <em class="calibre13">X</em> variables with a single <code>scale()</code> call.)</p>
<p class="indent">Of course, we must remember to do the same scaling—dividing by the same standard deviations—in the <em class="calibre13">X</em> values of new cases that we predict, such <span epub:type="pagebreak" id="page_19"/>as new days in our bike sharing example. The <code>qe*</code>-series functions make a note of this, which is then used by the paired <code>predict()</code> functions.</p>
<div class="note">
<p class="notet"><strong class="calibre3"><span class="notes">NOTE</span></strong></p>
<p class="notep"><em class="calibre13">Keep in mind that <span class="codeitalic1">scale()</span> won’t work on a vector in which all values are identical, as the standard deviation would be 0. We can check for this by calling the <span class="codeitalic1">regtools</span> function <span class="codeitalic1">constCols()</span>, which will report all constant columns in our data frame.</em></p>
</div>
<p class="indent">In the bike sharing data, <code>day1</code> uses unscaled data, for instructional purposes. The “official” version of the data, <code>day</code>, does scale. It does so in a different manner, though: it is scaled such that the values of the variables lie in the interval [0,1]. One way to do that is to transform by:</p>
<div class="imagec"><img alt="Image" src="../images/ch01eq02.jpg" class="calibre20"/></div>
<p class="noindent">This has an advantage over <code>scale()</code> by producing <em class="calibre13">bounded variables</em>—that is, numbers bounded by 0 and 1. By contrast, <code>scale()</code> produces variables in the interval (<em class="calibre13">–∞</em>, <em class="calibre13">∞</em>), and a variable with a small standard deviation will likely have very large scaled values, giving that variable undue influence in the analysis.</p>
<p class="indent">The <code>regtools</code> function <code>mmscale()</code> does the above mapping to [0,1]. Here is a small example:</p>
<pre class="calibre16">&gt; <span class="codestrong">x &lt;- data.frame(u=3:5,v=c(12,5,13))</span>
&gt; <span class="codestrong">x</span>
  u  v
1 3 12
2 4  5
3 5 13
&gt; <span class="codestrong">mmscale(x)</span>
     [,1]  [,2]
[1,]  0.0 0.875
[2,]  0.5 0.000
[3,]  1.0 1.000</pre>
<p class="noindent">The <code>u</code> column had a mean of 4, with a minimum and maximum of 3 and 5, respectively. Therefore, the 4 in the second row was replaced by (4 – 3) / (5 – 3) = 1/2, for example. As you can see, all the resulting values are in [0,1] and are unitless—that is, no inches or months.</p>
<h3 class="h2" id="ch01lev11">1.11 Choosing Hyperparameters</h3>
<p class="noindent">In the introduction, we mentioned that the number of nearest neighbors <em class="calibre13">k</em> is a <em class="calibre13">hyperparameter</em> or <em class="calibre13">tuning parameter</em>, a value chosen by the user that affects the predictive ability of the model. As noted in <a href="ch01.xhtml#ch01lev7" class="calibre12">Section 1.7</a>, <em class="calibre13">k</em> is a “Goldilocks” quantity that needs to be carefully set for best performance—not too small and not too large.</p>
<p class="indent">Finding the best <em class="calibre13">k</em> can be a challenge. Luckily, k-NN has only one hyperparameter, but as you will find later in the book, most ML methods have <span epub:type="pagebreak" id="page_20"/>several; in some advanced methods outside the scope of the book, there may even be a dozen or more. Choosing the “right” combination of values for several hyperparameters is especially difficult, but even choosing a good value for a single hyperparameter is nontrivial.</p>
<p class="indent">We take our first look below at ways to tackle the challenging problem of picking hyperparameters and then go into more details in <a href="ch03.xhtml" class="calibre12">Chapter 3</a>. Spend a little extra time on this section, since this issue will arise repeatedly in this book and throughout your ML career.</p>
<h4 class="h3" id="ch01lev11sec1">1.11.1 Predicting the Training Data</h4>
<p class="noindent">Almost all methods for choosing hyperparameters involve testing our model by predicting new cases. In the most basic form, we go back and predict our original training data. This sounds odd—we know the ridership values in the data, so why predict them? The idea is to try various values of <em class="calibre13">k</em> and see which one predicts our known data the best. That then would be the value of <em class="calibre13">k</em> that we use for predicting new <em class="calibre13">X</em> data in the future.</p>
<p class="indent">This is not ideal, and in practice a slight modification of this approach is used. We will use it too, but before presenting it, let’s go through an illustration of what can go wrong. Let’s predict the third data point in <code>day1</code> using the smallest possible value of <em class="calibre13">k</em>: 1.</p>
<pre class="calibre16">&gt; <span class="codestrong">kno &lt;- qeKNN(day1,'tot',1)</span>
&gt; <span class="codestrong">datapoint3X &lt;- day1[3,-6]</span>  # remote Y value
&gt; <span class="codestrong">predict(kno,datapoint3X)</span>
     [,1]
[1,] 1349
&gt; <span class="codestrong">day1[3,6]</span>
[1] 1349</pre>
<p class="noindent">We predicted exactly correctly! But wait a minute . . . that seems suspicious, and it is. The closest neighbor to data point 3 (the third row in our data) is itself! The distance from that point to itself is 0. Similarly, row 8 is the closest data point to row 8, row 56 is the closest data point to row 56, and so on. Of course we were 100 percent correct; we took the average of the 1-closest neighbor, thus just duplicating the <em class="calibre13">Y</em> value. The same analysis shows that even, say, <em class="calibre13">k</em> = 5 would give us overly optimistic prediction accuracy. One of the 5 neighbors would still be the original data point, thus biasing our prediction.</p>
<p class="indent">The takeaway is that when we evaluate the prediction accuracy of a given value of <em class="calibre13">k</em>, we should predict on a different dataset than the one to which we fit the k-NN method. But, you protest, we only have one dataset. How can we get another for properly assessing prediction accuracy? That’s the topic of the following section on holdout sets.</p>
<h3 class="h2" id="ch01lev12"><span epub:type="pagebreak" id="page_21" class="calibre2"/>1.12 Holdout Sets</h3>
<p class="noindent">Again, as noted in the previous section, to assess the predictive accuracy of our model, we need to try it out on “fresh” data, not the data it was fit on. But we don’t have any new data, so what can be done? Key to solving this conundrum are the notions of <em class="calibre13">holdout sets</em> and <em class="calibre13">cross-validation</em> covered in this section. They are central to ML and will recur throughout the book.</p>
<p class="indent">In the 731 data points in <code>day1</code>, we could randomly cull out, say, 100 of them. These will serve as our <em class="calibre13">holdout set</em>, or <em class="calibre13">test set</em>. The remaining 631 will temporarily be our training set. We fit the model to this training set, then see how well it predicts on the holdout set, which serves as “fresh” data not biased by training. (In examples here, the holdout size is 73, as the default size is 10 percent of the dataset size.)</p>
<p class="indent">Technically, our accuracy in future predictions will be best if we fit our model on the entire dataset, in which case we set <code>holdout=NULL</code>. However, in preliminary exploration, it’s important to have some idea as to how well our model works on new data. Thus it’s best to have a holdout set during the exploration phase, then refit the model on the entire data once we’ve chosen hyperparameters and so on.</p>
<p class="indent">Before we select our holdout set, if we are to evaluate quality of prediction, we need a criterion for evaluating prediction accuracy.</p>
<h4 class="h3" id="ch01lev12sec1">1.12.1 Loss Functions</h4>
<p class="noindent">For a data point in the holdout set, we could take the absolute difference between the actual value and predicted value, then average those absolute differences over all holdout points to get the Mean Absolute Prediction Error (MAPE). This is an example of a <em class="calibre13">loss function</em>, which is simply a criterion for goodness of prediction. We could take MAPE as that criterion, with smaller values being better.</p>
<p class="indent">Another popular loss function is Mean Squared Prediction Error, or MSPE, where we average the squares of the prediction errors rather than their absolute values. MSPE is the more common of the two, but I prefer to use MAPE, as MSPE overly accentuates large errors. Say we are predicting weight in the MLB data. Consider two cases in which we are in error by 12 and 15 pounds. Those two numbers are fairly similar, but their squares, 144 and 225, are quite different.</p>
<p class="indent">For classification applications, the most commonly used loss function is simply the overall probability of misclassification. We predict each <em class="calibre13">Y</em> in the holdout set, tally the number of times we are wrong, and divide by the size of the holdout set.</p>
<h4 class="h3" id="ch01lev12sec2">1.12.2 Holdout Sets in the qe*-Series</h4>
<p class="noindent">The <code>qe*</code>-series functions will automatically perform the above process of finding the MAPE, or overall probability of misclassification, then report the result in the output component <code>testAcc</code>. These functions will sense whether our application is a classification problem according to whether we specify <span epub:type="pagebreak" id="page_22"/><em class="calibre13">Y</em> (second argument in any <code>qe*</code>-series call) as an R factor. If so, then the misclassification probability will be computed instead of MAPE.</p>
<p class="indent">We’ll turn again to the bike sharing dataset for an example of finding <code>qeKNN</code>’s automatically generated MAPE:</p>
<pre class="calibre16">&gt; <span class="codestrong">knnout &lt;- qeKNN(day1,'tot',5)</span>
holdout set has  73 rows
&gt; <span class="codestrong">knnout$testAcc</span>
[1] 1203.644</pre>
<p class="noindent">Using 5 nearest neighbors, we make an average prediction error of about 1,200 riders. That’s not great, but better than the alternative, as follows.</p>
<p class="indent">Suppose we had no access to weather conditions and so on. How could we predict ridership? One natural idea would be to just use the overall mean:</p>
<pre class="calibre16">&gt; <span class="codestrong">meanTot &lt;- mean(day1$tot)</span>
&gt; <span class="codestrong">meanTot</span>
[1] 4504.349</pre>
<p class="noindent">In other words, every day, including the one at hand, we would predict about 4,500 riders. How well would we fare using this strategy?</p>
<pre class="calibre16">&gt; <span class="codestrong">mean(abs(day1$tot - meanTot))</span>
[1] 1581.793</pre>
<p class="noindent">If we were to always predict ridership using the overall mean riders per day, our average prediction error would be nearly 1,600. Using the weather variables and <code>workingday</code> as predictors does help bring MAPE down to 1,200.</p>
<p class="indent">Can we find a better value for <em class="calibre13">k</em> than 5? Let’s try, say, 10 and 25:</p>
<pre class="calibre16">&gt; <span class="codestrong">qeKNN(day1,'tot',10)$testAcc</span>
holdout set has  73 rows
[1] 1127.333
&gt; <span class="codestrong">qeKNN(day1,'tot',25)$testAcc</span>
holdout set has  73 rows
[1] 1131.108</pre>
<p class="noindent">Of the values we’ve tried, <em class="calibre13">k</em> = 10 seems to work best, though we must keep in mind the randomness of the holdout sets. Indeed, it is better to try several holdout sets for each candidate value of <em class="calibre13">k</em>, the topic of our next section.</p>
<h4 class="h3" id="ch01lev12sec3">1.12.3 Motivating Cross-Validation</h4>
<p class="noindent">When using <code>qeKNN</code>’s automatically generated MAPEs, it is important to remember that the software is choosing the holdout set at random. The holdout set size is only 73, a somewhat small sample—imagine our election pollster above sampling only 73 voters. Thus there will be considerable <span epub:type="pagebreak" id="page_23"/><em class="calibre13">sampling variation</em> between MAPE in one holdout set and another. We can solve this problem through performing <em class="calibre13">cross-validation</em>—that is, averaging the values of MAPE over multiple holdout sets.</p>
<p class="indent">To demonstrate sampling variation, let’s try running the same code a couple more times:</p>
<pre class="calibre16">&gt; <span class="codestrong">qeKNN(day1,'tot',10)$testAcc</span>
holdout set has  73 rows
[1] 1094.211
&gt; <span class="codestrong">qeKNN(day1,'tot',10)$testAcc</span>
holdout set has  73 rows
[1] 1157.5</pre>
<p class="indent">We can see that we cannot rely too much on that one MAPE value; the sample size of 73 is too small. There is a lot more to say on this issue, and as noted, we will resume this discussion in <a href="ch03.xhtml" class="calibre12">Chapter 3</a>. Suffice it to say now that we should look at many holdout sets and then perform cross-validation by averaging the resulting <code>testAcc</code> values.</p>
<p class="indent">By the way, we can control R’s random number generator by using the <code>set.seed()</code> function:</p>
<pre class="calibre16">&gt; <span class="codestrong">set.seed(9999)</span>
&gt; <span class="codestrong">qeKNN(day1,'tot',10)$testAcc</span>
holdout set has  73 rows
[1] 1210.373
&gt; <span class="codestrong">qeKNN(day1,'tot',10)$testAcc</span>
holdout set has  73 rows
[1] 1090.622
&gt; <span class="codestrong">set.seed(9999)</span>  # try it again
&gt; <span class="codestrong">qeKNN(day1,'tot',10)$testAcc</span>
holdout set has  73 rows
[1] 1210.373
&gt; <span class="codestrong">qeKNN(day1,'tot',10)$testAcc</span>
holdout set has  73 rows
[1] 1090.622</pre>
<p class="noindent">We will often do this in the book. It sets a certain sequence of random numbers, in case the reader wishes to run the code and check the results. By using the same seed, 9999 here, the same training and holdout sets will be generated as what I had here. (I just chose 9999 as a favorite; there is nothing special about it.)</p>
<p class="indent">Clearly, we will obtain more accurate results by generating several holdout sets, thus averaging the resulting MAPE values. This is called <em class="calibre13">cross-validation</em>, which will be discussed in detail in <a href="ch03.xhtml" class="calibre12">Chapter 3</a>.</p>
<h4 class="h3" id="ch01lev12sec4"><span epub:type="pagebreak" id="page_24" class="calibre2"/>1.12.4 Hyperparameters, Dataset Size, and Number of Features</h4>
<p class="noindent">Recall the discussion in <a href="ch01.xhtml#ch01lev7sec2" class="calibre12">Section 1.7.2</a> regarding a trade-off involving the choice of the number of nearest neighbors <em class="calibre13">k</em>:</p>
<ul class="calibre15">
<li class="noindent3">If we set <em class="calibre13">k</em> = 5, we will be averaging just 5 data points, which seems too few. This is a variance problem; averages of 5 <em class="calibre13">Y</em> values at a given <em class="calibre13">X</em> will vary a lot from one sample to another.</li>
<li class="noindent3">On the other hand, setting <em class="calibre13">k</em> = 50, we would likely have some points in the neighborhood that are far away from the point to be predicted and thus unrepresentative.
<p class="indent">For example, in the bike data, say we are predicting ridership on a 20-degree day, which is rather comfortable. Points in our training set with an uncomfortable temperature like 40 are not very relevant to the prediction at hand and would tend to make our prediction too low. This is a bias problem.</p>
</li>
</ul>
<p class="noindent">So, we have a trade-off. We want to make <em class="calibre13">k</em> large to achieve low variance, but setting a large <em class="calibre13">k</em> incurs the risk of substantial bias. But . . . what if our bike sharing dataset were to have, say, <em class="calibre13">n</em> = 73000 data points rather than 731? In that case, the 50th-nearest neighbor might actually be pretty close to the prediction point, solving our bias problem. Then we could afford to use a larger <em class="calibre13">k</em> so as to hold down variance. In other words:</p>
<p class="block">All else being equal, the larger <em class="calibre13">n</em> is, the larger we can make <em class="calibre13">k</em>.</p>
<p class="noindent">This still doesn’t tell us what specific <em class="calibre13">k</em> to choose. We’ll return to this issue in <a href="ch03.xhtml" class="calibre12">Chapters 3</a> and <a href="ch04.xhtml" class="calibre12">4</a>, but it is something to at least be aware of at this early stage.</p>
<p class="indent">The corresponding statement concerning the number of features <em class="calibre13">p</em> is:</p>
<p class="block">All else being equal, the larger <em class="calibre13">p</em> is, the larger we must make <em class="calibre13">k</em>.</p>
<p class="noindent">This is less intuitive than the previous statement involving <em class="calibre13">n</em>, but roughly this is the issue. Having more features means more variability in interpoint distances, thus increasing variance in predictions. To counter this, we need a larger <em class="calibre13">k</em>.</p>
<h3 class="h2" id="ch01lev13">1.13 Pitfall: p-Hacking and Hyperparameter Selection</h3>
<p class="noindent">In non-ML settings, <em class="calibre13">p-hacking</em> refers to the following pitfall in analyses of large studies. Say one is studying genetic impacts on some outcome, with very large numbers of genes involved. Even if no gene has a real impact, due to sampling variation, one of them will likely appear to have a “significant” impact just by accident. Though we won’t go into detail on how to solve this just yet, you should be aware of it from the beginning.</p>
<p class="indent">p-hacking also has major implications for the setting of hyperparameters in ML. Let’s say we have four tuning parameters in an ML method, and we try 10 values of each. That’s 10<sup class="calibre11">4</sup> = 10000 possible combinations. Even if all of them are equally effective, the odds are that one of them will accidentally <span epub:type="pagebreak" id="page_25"/>have a much better MAPE value. What seems to be the “best” setting for the hyperparameters may be illusory.</p>
<p class="indent">The <code>regtools</code> function <code>fineTuning()</code> takes steps to counter the possibility of p-hacking in searches for the best tuning parameter combination. We’ll cover more on this in <a href="ch07.xhtml" class="calibre12">Chapter 7</a>.</p>
<h3 class="h2" id="ch01lev14">1.14 Pitfall: Long-Term Time Trends</h3>
<p class="noindent">Before we move on to the next chapter, let’s cover three major pitfalls you may encounter while working with the methods introduced here that may influence the quality of your predictions: dirty data, missing data, and longterm trends in the data. We will deal with the last pitfall first.</p>
<p class="indent">In the few experiments we did with the MLB data, we found that the best MAPE value might be around 1,100. This seems rather large, but recall we are not using all of our data. Look again at the bike dataset in <a href="ch01.xhtml#ch01lev1sec1" class="calibre12">Section 1.1.1</a>. There are several variables involving timing of the observation: date, season, year, and month. To investigate whether the timing data can improve our MAPE value, let’s graph ridership against time. (The data is in chronological order.)</p>
<pre class="calibre16">&gt; <span class="codestrong">plot(day1$tot,type='l')</span>  # plotting type is 'l', lines between points</pre>
<p class="noindent">The result is in <a href="ch01.xhtml#ch01fig01" class="calibre12">Figure 1-1</a>. Clearly, there is both a seasonal trend (the dips are about a year apart) and an overall upward trend. The bike sharing service seems to have gotten much more popular over time. Statistical techniques that analyze data over the course of time are known as <em class="calibre13">time series methods</em>. They arise a lot in ML in various contexts. We’ll investigate this in <a href="ch13.xhtml" class="calibre12">Chapter 13</a>, but let’s give it a try here without new tools.</p>
<div class="image"><img alt="Image" id="ch01fig01" src="../images/ch01fig01.jpg" class="calibre21"/></div>
<p class="figcap"><em class="calibre13">Figure 1-1: Time trends, bike data</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_26"/>Column 1 in the bike sharing data, <code>instant</code>, is the day number, with the first day of the dataset having value 1, the second day with value 2, and so on, through the last day, 731.</p>
<p class="indent">Let’s add <code>instant</code> to our feature set. Recall our earlier selection of columns in <a href="ch01.xhtml#ch01lev5sec1" class="calibre12">Section 1.5.1</a>:</p>
<pre class="calibre16">&gt;<span class="codestrong"> day1[,c(8,10:13,16)]</span></pre>
<p class="noindent">The features we decided to explore at that time were in columns 8, 10 through 13, and 16. Now we want to also use <code>instant</code>, which is in column 1:</p>
<pre class="calibre16">&gt; <span class="codestrong">data(day1)</span>
&gt; <span class="codestrong">day2 &lt;- day1[,c(1,8,10:13,16)]</span>
&gt; <span class="codestrong">kno &lt;- qeKNN(day2,'tot',k=5)</span>  # rerun k-NN on the new data
holdout set has  73 rows
&gt; <span class="codestrong">kno$testAcc</span>
[1] 662.9836</pre>
<p class="noindent">Ah, much better! Our MAPE is down to about 663.</p>
<p class="indent">When using k-NN and the other methods covered in this book, keep in mind that conditions in the phenomena under study may vary through time, possibly becoming a major factor. In some cases, the time variable may not even be explicit but implied in the ordering of the records. Failure to explore this may result in a substantial deterioration in the quality of prediction, so keep an eye out for places where you may run into this pitfall.</p>
<h3 class="h2" id="ch01lev15">1.15 Pitfall: Dirty Data</h3>
<p class="noindent">Beyond accounting for long-term time trends, you may also encounter problems caused by dirty data. For an example of this, look at the entry in the bike sharing data for January 1, 2011.</p>
<pre class="calibre16">&gt; <span class="codestrong">head(day1)</span>
  instant     dteday season yr mnth holiday
1       1 2011-01-01      1  0    1       0</pre>
<p class="noindent">As you can see in the <code>holiday</code> column, the dataset claims this is not a holiday. But of course January 1 is a federal holiday in the United States. Also, although the documentation for the dataset states there are 4 values for the categorical variable <code>weathersit</code>, there actually are just values 1, 2, and 3:</p>
<pre class="calibre16">&gt; <span class="codestrong">table(day1$weathersit)</span>
<br class="calibre1"/>
  1   2   3
463 247  21</pre>
<p class="indent">Errors in data are quite common and, of course, an obstacle to good analysis. For instance, consider the New York City taxi data that will be discussed in depth in <a href="ch05.xhtml" class="calibre12">Chapter 5</a>, which contains pickup and dropoff locations, <span epub:type="pagebreak" id="page_27"/>trip times, and so on. One of the dropoff locations, if one believes the numbers, is in Antarctica! (You can take a look at <a href="https://data.cityofnewyork.us/Transportation/2018-Yellow-Taxi-Trip-Data/t29m-gskq" class="calibre12"><em class="calibre13">https://data.cityofnewyork.us/Transportation/2018-Yellow-Taxi-Trip-Data/t29m-gskq</em></a>.)</p>
<p class="indent">Whenever working with a new dataset, the analyst should do quite a bit of exploring—for example, with <code>hist()</code> and <code>table()</code>, as we’ve seen here. You should also be wary of <em class="calibre13">multivariate outliers</em>, meaning data points that are not extreme in any one of their components but, when viewed collectively, are unusual. For instance, suppose a person is recorded as having height 74 inches (29.1 cm) and age 6. Neither that height nor that age would be cause for concern individually (assume we have people of all ages in our data), but in combination it seems quite suspicious. In this case, k-NN is a useful tool! A data point like the 74-inch six-year-old described above would be unusually distant from the other points and might be exposed that way.</p>
<p class="indent">Alas, there is no formulaic way to detect anomalous data points. This mirrors the nature of ML in general, as we’ve emphasized. There is no magic “recipe.” This is an advanced topic in statistics and beyond the scope of this book.</p>
<h3 class="h2" id="ch01lev16">1.16 Pitfall: Missing Data</h3>
<p class="noindent">In R, the value NA means the data is not available—that is, missing. Datasets commonly include NAs, maybe many. How should we deal with this?</p>
<p class="indent">There are entire books devoted to the study of missing value analysis. We cannot cover the topic in any depth here, but we will briefly discuss one common method, <em class="calibre13">listwise deletion</em>, to at least introduce the issues at stake.</p>
<p class="indent">Say our data consists of people, and we have variables Age, Gender, Years of Education, and so on. If, for a particular person, Age is missing but the other variables are intact, this method would simply skip over that case. But there are two problems with this:</p>
<ul class="calibre15">
<li class="noindent3">If we have a large number of features, odds are that many cases will have at least one NA. This would mean throwing out a lot of precious data.</li>
<li class="noindent3">Skipping over cases with NAs may induce a bias. In survey data, for instance, the people who decline to respond to a certain question may be different from those who do respond to it, and this may affect the accuracy of our predictions.</li>
</ul>
<p class="noindent">Note that when we find missing values are coded numerically rather than as NAs, we should change such values to NAs.</p>
<p class="indent">As with dirty data, developing one’s personal approach to handling missing values comes with experience. Learn some tools and gradually develop your approach, which will largely be different for each person. The CRAN Task View series includes one on missing data.</p>
<h3 class="h2" id="ch01lev17"><span epub:type="pagebreak" id="page_28" class="calibre2"/>1.17 Direct Access to the regtools k-NN Code</h3>
<p class="noindent">As mentioned in the introduction, most of the code in this book uses popular R packages that are specific to each different ML method. We use the <code>qe*</code>-series of wrappers to interface those packages as a uniform, simple convenience, but of course one can also choose direct access.</p>
<p class="indent">One reason for doing so is that the functions in those standard R packages contain many advanced options not available via the wrappers. In the particular case of <code>qeKNN()</code>, but <em class="calibre13">not</em> for the other <code>qe*</code>-series functions, direct access may also be faster if one is doing just a single prediction.</p>
<p class="indent">In most applications, such specialized usage will not be necessary, but our book will show how to use the functions directly if the reader is interested. Note that we will not be able to demonstrate how to use those advanced options, as they are numerous and frequently involve advanced concepts. Instead, we will simply show how to use one of our existing examples with a direct call.</p>
<p class="indent">Here are the particulars for our k-NN code. The <code>qeKNN()</code> function wraps <code>regtools::kNN()</code>, which has arguments as follows:</p>
<pre class="calibre16">&gt; <span class="codestrong">library(regtools)</span>
&gt; <span class="codestrong">args(kNN)</span>
function (x, y, newx = x, kmax, scaleX = TRUE, PCAcomps = 0,
    expandVars = NULL, expandVals = NULL, smoothingFtn = mean,
    allK = FALSE, leave1out = FALSE, classif = FALSE, startAt1 = TRUE,
    saveNhbrs = FALSE, savedNhbrs = NULL)</pre>
<p class="noindent">Here <code>x</code> and <code>y</code> are our <em class="calibre13">X</em> and <em class="calibre13">Y</em>, <code>newx</code> is the <em class="calibre13">X</em> at which we wish to predict, and <code>kmax</code> is <em class="calibre13">k</em>.</p>
<p class="indent">Let’s retrace our steps from <a href="#ch01lev8" class="calibre12">Section 1.8</a>:</p>
<pre class="calibre16">&gt; <span class="codestrong">x &lt;- mlb[,c(4,6)]</span> # height, age
&gt; <span class="codestrong">y &lt;- mlb[,5]</span>  # weight
&gt; <span class="codestrong">newx &lt;- c(72,24)</span>  # ht, age of new player to be predicted re weight
&gt; <span class="codestrong">kmax &lt;- 25</span>   # k
&gt; <span class="codestrong">knnout &lt;- kNN(x,y,newx,kmax)</span>
&gt; <span class="codestrong">knnout$regests</span>  # predicted value
[1] 183.76</pre>
<p class="noindent">Note that the model fitting and prediction are combined into one step, though the latter can be postponed if desired.</p>
<p class="indent">The prediction here is slightly different from the earlier one, as the latter had a holdout set. If we suppress formation of a holdout set, we obtain the same result:</p>
<pre class="calibre16">&gt; <span class="codestrong">kno &lt;- qeKNN(mlb[,4:6],'Weight',k=25,holdout=NULL)</span>
&gt; <span class="codestrong">predict(kno,data.frame(Height=72,Age=24))</span>
       [,1]
[1,] 183.76</pre>
<p class="indent">More information on <code>kNN()</code> is available by typing <span class="codestrong1">?kNN</span> (not <code>?knn</code>).</p>
<h3 class="h2" id="ch01lev18"><span epub:type="pagebreak" id="page_29" class="calibre2"/>1.18 Conclusions</h3>
<p class="noindent">We are off to a good start! You should now have a good idea of both the k-NN method and several general ML concepts: the regression function, how k-NN estimates it, the general concept of the Bias-Variance Trade-off, how hyperparameters affect it, and how to use holdout sets to find a good point on that trade-off spectrum.</p>
<p class="indent">You are already armed with enough tools to experiment with some data analysis of your own. Please do!</p>
<p class="indent">The next chapter will introduce the issues in classification applications.<span epub:type="pagebreak" id="page_30"/></p>
</div></body></html>