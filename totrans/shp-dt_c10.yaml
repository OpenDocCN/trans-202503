- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multicore and Quantum Computing
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous chapters, we overviewed many tools that come from the fields
    of geometry and topology. We saw how these algorithms can impact network analytics,
    natural language processing, supervised learning, time-series analytics, visualization,
    and unsupervised learning. Many more algorithms rooted in topology exist, including
    hybrid algorithms that improve existing models such as convolutional or recurrent
    neural networks, and the field has the potential to contribute thousands more
    to the field of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: However, one of the major issues facing the development and application of topological
    and geometric machine learning algorithms is computational cost. While most will
    not, calculating certain network metrics can scale to network sizes of one million
    or one billion (or more!). Other classes of algorithms, such as persistent homology
    or manifold learning, won’t scale well; some will reach issues on a standard laptop
    at around 20,000 rows of data. Calculating a distance matrix for a large set of
    data will also require a lot of time and computing resources.
  prefs: []
  type: TYPE_NORMAL
- en: Yet there are a couple of potential options for practitioners who hope to scale
    these methods. In this chapter, we’ll review multicore approaches and quantum
    computing solutions. Solutions like the ones presented in this chapter are in
    their infancy, and as the algorithms in this book are adopted as big data algorithms,
    it’s likely standard packages will be developed for quantum topological data analysis
    (TDA) algorithms and distributed TDA algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Multicore Approaches to Topological Data Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One approach to algorithm scaling is to map pieces of the data to different
    computer cores or systems, compute the desired quantities on each core’s worth
    of data at the same time, and reassemble each core’s data quantity computations
    into a single dataset. For instance, suppose we want to calculate betweenness
    centrality for each vertex in a million-vertex network. Even if this is a sparse
    network without many edges, each betweenness centrality calculation will take
    a long time to compute. However, if we have access to a million cores, we can
    map the network to each core and compute betweenness centrality for one vertex
    in the network with each core. While the compute time may still be long, the total
    time needed to compute betweenness centrality for all one million vertices will
    be much less than what it would take computing each betweenness centrality one
    after another in a loop (as we did in prior chapters for much smaller networks).
    While a million cores isn’t possible for most organizations, the time savings
    from 10 cores or 30 cores can be a substantial speedup.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also possible to map only part of a dataset to each core to compute some
    sort of metric. For instance, say we have a dataset of 300,000 individuals who
    completed an online customer survey. Calculating the distance between all 300,000
    individuals’ responses would take a lot of computing resources. However, distance
    matrices are necessary in many manifold learning and topological data analysis
    algorithms; it’d be good to have a quicker way to calculate this. We can map different
    pieces of the data to different cores to compute smaller distances matrices that
    can be assembled into the full 300,000-by-300,000-distance matrix after the cores
    have computed each piece. Again, we’ll see large time savings.
  prefs: []
  type: TYPE_NORMAL
- en: In general, these multicore approaches fall under the umbrella of *distributed
    computing*, where multiple cores are leveraged to compute pieces of a problem
    to assemble when all cores have their solutions computed. Most cloud computing
    resources will have support for distributed computing, and some R and Python packages
    that can be run on the cloud support distributed computing options. However, it’s
    usually up to the machine learning or data engineer to define distributed computing
    steps into the algorithm being used. [Figure 10-1](#figure10-1) shows a simple
    example of how data might be parsed and sent to different cores.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c10/f010001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-1: A mapping of three sections of data to three cores that will perform
    a mathematical or algorithmic step in the machine learning pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 10-1](#figure10-1), we parse our dataset into three pieces to map
    to three cores that will compute the quantities we desire. In a simple big data
    case, this might involve computing the minimum and maximum values found in each
    column of the dataset. We’d then compute the minimum and maximum values across
    cores, save that value, and spit out the minimum and maximum values for each column
    from the full set of data. It may not help much if we have a dataset of only a
    few million rows, but it will speed up the computational process a lot if we’re
    dealing with trillions of rows of data.
  prefs: []
  type: TYPE_NORMAL
- en: Multicore approaches work well for network algorithms, such as those encountered
    in Chapters [2](c02.xhtml) through [4](c04.xhtml), and they have had some success
    with persistent homology and discrete exterior calculus. As long as the problem
    is local in nature or can be broken into smaller pieces for some steps, multicore
    approaches are viable. For instance, in robotics path-planning homotopy algorithms,
    we can break up the potential routes around obstacles and calculate some optimality
    for each route.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, few multicore versions of the algorithms in this book currently
    exist in R or Python. However, it is an area being actively explored in research,
    and if you’re familiar with multicore frameworks on big data platforms, you are
    encouraged to play around with ways to apply this approach to persistent homology,
    manifold learning, or other geometry-based algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Quantum Computing Approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another approach to scaling geometry-based algorithms is to implement them on
    a different type of computer that can leverage distributed computing natively.
    *Quantum computing* is a recent buzzword, and a lot of mystery and myths still
    surround the field, even within data science and software engineering. Progress
    has been faster than expected, but the technology is still in its early stages,
    with current systems having stark limitations and companies pouring money into
    hardware research and development. However, some question network algorithms already
    exist, and network science is one of the areas of machine learning that could
    benefit the most from quantum computing. In this section, we’ll go through some
    basics of quantum computing and list some of the quantum algorithms related to
    graph theory and network science that exist as of 2023.
  prefs: []
  type: TYPE_NORMAL
- en: To start, quantum computing hardware can take on several different forms, each
    of which has its advantages and disadvantages. The type of circuit in the computer
    dictates what sort of algorithms can be developed on the machine, and some types
    of circuits are more amenable to network analytics. Two types of hardware seem
    to be the focus of most research and development these days, and they’ll be the
    focus of this discussion.
  prefs: []
  type: TYPE_NORMAL
- en: The current systems have many limitations, including the need to cool the circuits,
    effects due to quantum scales (tunneling through energy barriers, fields created
    by interacting particles, and so on), and random error inherent in the qubits.
    Nevertheless, researchers have been able to explore potential new algorithms and
    quantum versions of existing algorithms through the quantum computers that exist
    and the simulation packages in Python. Graph theory and network algorithms, in
    particular, seem well suited to quantum computing, and the ability to search through
    combinatorics solutions simultaneously with qubits suggests that network science
    will get a boost when quantum computing scales to larger circuits.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Qubit-Based Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll start with the version of quantum hardware that is most like classical
    computers: the *qubit-based model*, which replaces classical bits with a quantum
    version of the bit, *qubits*, the quantum version of the 0 and 1 bits that underlie
    classical circuits. Rather than taking values of only 0 or 1 at a given time,
    qubits can exist simultaneously in the 1 and 0 state until the qubit is observed
    (where it will collapse to a 0 or a 1 state), and they can also rotate through
    computer gates to take fractional values. This means qubits can exist in many
    different states and be evolved through quantum gates into a final, optimized
    state. This makes them very flexible. [Figure 10-2](#figure10-2) shows the difference
    between bits and qubits.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c10/f010002r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-2: A plot comparing classical bits with qubits'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s quickly go through the two main types of hardware that use qubits. Don’t
    worry if you don’t understand every term here; just keep the high-level ideas
    in mind. Two types of chipsets use qubits. Both rely on quantum principles of
    qubits (such as superposition) to speed up computation and improve accuracy. The
    first is a *gate-based circuit*that’s similar to classical hardware, in which
    gate operations act on qubit states (such as the rotation gate). Gate-based circuits
    are the most common, and some examples include IBM’s machines and Rigetti’s machines;
    in general, gate-based algorithms speed up algorithms and provide easy benchmarking
    of algorithms. The other option, currently used by D-Wave on its machines, relies
    on quantum annealing (physical heating and cooling processes) through the changing
    of magnetic fields to act on qubit states rather than manipulating qubits through
    gates. In general, there aren’t as many performance and accuracy guarantees or
    bounds on this type of system as a gate-based quantum hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Qumodes-Based Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Besides the qubit model, the other dominant model is the *qumodes*-based circuit
    model, which is a photon-based circuit being developed by Xanadu, a Canadian company.
    Currently, software and algorithms based on photonic computing—which uses photons
    in place of qubits—are only simulations of the machine, but this type of simulation
    allows for the development of some interesting applications. This type of circuit
    employs wave functions, which are continuous distributions, in place of qubits
    (which collapse to a 1 or 0 when measured). Wave functions can then be squeezed,
    mapped, or operated on by other types of geometric transformations of the function
    without collapsing to a 1 or 0\. This computer doesn’t exist yet, but simulation
    programs do exist in Python (as of 2023), similar to simulation programs available
    for qubit circuits.
  prefs: []
  type: TYPE_NORMAL
- en: Using Quantum Network Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several quantum network science algorithms exist that relate to properties of
    graphs. Clique-finding algorithms are particularly useful in network science,
    and quantum clique-finding algorithms already exist. Maximal clique algorithms
    seem to enjoy a speedup on the very small problems they’ve been tested on.
  prefs: []
  type: TYPE_NORMAL
- en: One important caveat of quantum algorithms is their probabilistic nature. Rather
    than getting, say, a list of cliques in the output, a quantum algorithm will run
    multiple times, creating arrays of clique lists, which can be combined into probabilistic
    scores of clique existence in the network. This can be helpful in prioritizing
    cliques for further parts of a project or zeroing in on the cliques of most interest
    within a very large network, though the latter will require much larger circuits
    than exist today.
  prefs: []
  type: TYPE_NORMAL
- en: Quantum maximum flow and minimum cut algorithms also exist; these algorithms
    aim to partition the graph into communities with the fewest possible cuts that
    maximize information flow on the graph. Applications thus far have explored importance-scoring
    uses to rank edges and vertices by importance to the graph structure and communication
    potential. They show some promise for sparse graphs and provide a probabilistic
    framework for deriving importance scores.
  prefs: []
  type: TYPE_NORMAL
- en: A basic quantum maximum flow and minimum cut algorithm using the R package QuantumOps
    does exist, though the capability is limited to small, sparse graphs. Using a
    qubits approach, the problem is first translated to a *quantum approximation optimization
    algorithm*, or *QAOA*. A QAOA formulation is a combinatorial algorithm that relies
    on the superposition of qubit states and something called *unitary operators*
    to solve optimization problems. Unitary operators are a type of linear algebra
    operator with special properties that match well to the underlying quantum mechanics
    of qubit circuits. Because of the probabilistic nature of solutions, it’s best
    to run the algorithm multiple times for more exact answers. In this case, let’s
    run the algorithm 10 times (an arbitrary number large enough to generate usable
    results with a probabilistic solution) and explore this function in a bit more
    depth by using the script in [Listing 10-1](#listing10-1), which reloads Graph
    1 from [Listing 4-1](c04.xhtml#listing4-1).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 10-1: Running the maxcut algorithm on Graph 1, first seen in [Chapter
    4](c04.xhtml), 10 times'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the output from this algorithm contains 2⁷ items, with our original
    graph containing six vertices. In general, this version of quantum maximum cut
    algorithms will include 2^(^(*n*)^(+1)) items in the output, with problems scaling
    to larger graphs. The output is also in raw format with this algorithm, leaving
    the user to translate the output to the most likely cuts made. A couple of existing
    Python packages give a more usable output, but this package provides a way for
    you to explore some qubit-based computing simulations within R.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to maximum flow and minimum cut algorithms, there are quantum versions
    of random walk algorithms, and it would be possible to build a community-finding
    algorithm or PageRank-type algorithm from them. In fact, some quantum PageRank
    algorithms have been proposed in the past few years and studied theoretically.
    However, this type of application has not been explored much in practice thus
    far. It does provide an avenue for further research and potential applications
    to network science in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Other examples that are more tied to graph theory than network science include
    graph-coloring algorithms and algorithms focused on testing graph properties (such
    as isomorphism and connectivity). Quantum querying on graphs and quantum versions
    of Dijkstra’s algorithm or other shortest path algorithms are in their infancy,
    but they show promise in the future of quantum graph theory algorithms and quantum
    network science algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Though quantum computing is a nascent technology, graph theory and network science
    have already seen the potential gains from quantum algorithms, and it is likely
    more algorithms will be developed in the future. The extant algorithms generally
    run in Python for now, but it’s likely that new interfaces to quantum computers
    will exist in the future, as more companies develop quantum computers and provide
    access to researchers.
  prefs: []
  type: TYPE_NORMAL
- en: Speeding Up Algorithms with Quantum Computing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most algorithms do not have quantum computing or simulated quantum computing
    packages available yet. However, it’s possible to leverage basic mathematical
    tools that do exist in quantum computing and simulated quantum computing packages
    to assemble algorithms such as the ones in this book step-by-step. The R package
    QuantumOps has some of these tools available for us to explore, so let’s dive
    into an example of basic mathematics computations on a quantum system. Please
    note, this package only simulates algorithms that would be run on a quantum computer,
    so the speedups on your classical laptop won’t be what you’d see on a real quantum
    computer. However, the following example will show one way that quantum algorithms
    are being developed to run on quantum computers to speed up computation for problems
    that require a lot of computational power as the complexity grows.
  prefs: []
  type: TYPE_NORMAL
- en: There are many algorithms that find the greatest common denominator between
    two numbers, and it’s possible to do this within a quantum computing framework.
    The *greatest common denominator* refers to the largest number that will divide
    both numbers of interest; for instance, the largest number that divides both 12
    and 20 is 4\. We can factor 12 into its divisors (1, 2, 3, 4, 6, 12) and factor
    20 into its divisors (1, 2, 4, 5, 10, 20). We can then see that 4 is the largest
    number to occur in both sets of divisors. This is a simple problem for us to do
    by hand when the numbers are small; however, in many encryption applications,
    the numbers to factor or determine to be prime can involve 12 or more digits.
    Factoring these requires a computer.
  prefs: []
  type: TYPE_NORMAL
- en: The `gcd()` function in QuantumOps finds the greatest common denominator between
    two numbers the way they would be found on a quantum computer. Let’s try our example
    with 12 and 20 as our input numbers and see how this works with the `gcd()` function;
    see [Listing 10-2](#listing10-2).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 10-2: Finding the greatest common denominator of 12 and 20'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see an output of `4` from [Listing 10-2](#listing10-2)’s code, which
    corresponds to the greatest common denominator of 12 and 20 that we found by hand.
    This function can compute common denominators of much larger numbers that would
    be impractical to compute by hand. Let’s plug in two new, larger numbers (say,
    14,267 and 11,345) and find their greatest common denominator with this function
    by modifying the `gcd()` function in [Listing 10-2](#listing10-2) to take these
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: According to our output, the greatest common denominator between these two numbers
    is 1\. Neither of these numbers is a prime, but they do not share any divisors.
    This algorithm runs quickly for numbers this large, and if you are interested
    in cybersecurity applications of factoring algorithms, you are encouraged to try
    numbers on the scale that you are using in your applications to further explore
    this function and its capabilities. On quantum systems, it’s possible to run this
    algorithm for much larger numbers in a reasonable compute time. This means that
    security algorithms based on factoring will not perform well once quantum computing
    becomes more accessible outside of research settings.
  prefs: []
  type: TYPE_NORMAL
- en: The QuantumOps package does not include some of the more advanced mathematical
    tools upon which algorithms are built, but it’s possible to define dot products,
    norms, and other linear algebra tools important to distance metric design, as
    well as define other linear algebra tools underlying common machine learning tasks.
    On a real quantum computer, we’d be able to run algorithms much more quickly and
    for much larger problems than the ones we’ve considered. However, the QuantumOps
    package allows us to explore some of what does exist in the quantum algorithm
    research that is done on quantum computers.
  prefs: []
  type: TYPE_NORMAL
- en: Using Image Classifiers on Quantum Computers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the hot topics in machine learning today is *image classification*, in
    which machine learning is used to classify input images according to the category
    labels it is given. Typically, you’ll have a set of images with category labels
    associated to them that can be used to train a machine learning algorithm to identify
    characteristics (such as color, lines, circles, or more complicated patterns)
    that signal an image belongs to a certain class. For instance, consider the problem
    of labeling certain types of plants or animals based on pictures that may or may
    not contain one or more of the categories we’re hoping to automatically tag. Consider
    [Figure 10-3](#figure10-3), which depicts the flowering part of a cannonball tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c10/f010003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-3: A picture of a blooming cannonball tree at Fairchild Tropical
    Gardens (native to Central and South America)'
  prefs: []
  type: TYPE_NORMAL
- en: Now look at [Figure 10-4](#figure10-4), which depicts an elephant.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c10/f010004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-4: A picture of an elephant walking down a road in Kruger National
    Park, South Africa'
  prefs: []
  type: TYPE_NORMAL
- en: There are many challenges to classifying images such as these. In [Figure 10-3](#figure10-3),
    for instance, we don’t see the entire tree in the image (making it difficult to
    recognize as a tree rather than a bush or vine), and the tree is in bloom (meaning
    that other pictures of a cannonball tree may not have flowers in them). In [Figure
    10-4](#figure10-4), the elephant is walking away from the camera (meaning there
    is no animal face), the image has a lens filter applied (changing the natural
    color), and the image contains other types of things (plants, a road, and so on).
    Most real-world sets of images don’t contain full images of only the category
    of interest that are plainly visible in the same color scheme (think of all the
    images of a cat that come up when you try searching Google Images for a cat).
    A good classifier needs to generalize to a lot of different types of cats in different
    lighting with other things included in the image.
  prefs: []
  type: TYPE_NORMAL
- en: We might also be facing category imbalance among the set of images we’re using
    to train the algorithm. We might have a lot of pictures of orchids, tulips, jaguars,
    and kangaroos, but we may have relatively few pictures of black bat flowers, Gaboon
    vipers, or African dwarf sawsharks. For instance, we may have an entirely new
    plant that isn’t found in nature or a rare plant or flower for which many images
    don’t exist, such as the hybrid shown in [Figure 10-5](#figure10-5).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c10/f010005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-5: A picture of a genetically engineered new species of plant at
    Fairchild Tropical Gardens'
  prefs: []
  type: TYPE_NORMAL
- en: Typically, image classification algorithms involve pretrained or custom-built
    convolutional neural networks, which were discussed in [Chapter 1](c01.xhtml).
    To quickly review, convolutional neural networks (CNNs), a type of deep learning
    algorithm, find relevant features by optimizing maps from the input image layer
    to a series of different types of layers to the output layer containing the category
    labels. Within a CNN architecture, some layers find salient features within categories
    of images, which are pooled in other layers that find the best feature sets from
    prior layers and feed them into the next layer of feature-finding layers. It’s
    common for these architectures to involve many layers, and they usually require
    a large set of training data to perform well. Further, they require a lot of computational
    power.
  prefs: []
  type: TYPE_NORMAL
- en: Because quantum computing offers faster runtimes for many algorithms and can
    leverage superposition to broaden search capabilities, the merging of deep learning
    algorithms such as CNNs with quantum computing offers a synergy to find relevant
    features more quickly and with less input data. In many real-world applications,
    we don’t have access to every image that comes up on a Google search. We might
    have only a few hundred medical images of a rare condition or not have the computational
    power to optimize hundreds of parameters on an animal image set that includes
    thousands of pictures of hundreds of snake species that pose a threat to farmers
    or villages in the developing world.
  prefs: []
  type: TYPE_NORMAL
- en: However, many quantum neural network algorithms exist as of 2023, and many show
    competitive performance on problems such as image classification (though scaling
    is still an issue given the qubit number limitations of current quantum computers).
    We’ll explore one recently developed in South Africa to handle image classification
    problems similarly to CNNs, the circuit-centric quantum classifier that exists
    in the QuantumOps package for general usage and usage on a famous image analytics
    benchmark dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The details of circuit-centric quantum classifiers are a bit physics-heavy.
    It’s okay if you don’t follow all of this; we’ll see it in action later in this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: The basic approach of the circuit-centric quantum classifier is to learn the
    quantum gate parameters of the circuit through supervised learning, such as fitting
    hidden neural network–layer parameters in deep learning algorithms. The independent
    variables are coded into quantum systems amplitudes, which are manipulated by
    quantum gates. These single- and two-qubit gates are optimized through single-qubit
    measurement, which collapses the system from superposition into a single state.
    Gradients are learned by multiple runs of the algorithm, as in the approach in
    the quantum min cut and max flow algorithm earlier in this chapter. As with deep
    learning algorithms, we use dropout regularization fractions, which prune and
    add to the circuit during each iteration. The result is an optimized quantum architecture
    that modifies independent variable sets to create high-quality suggested dependent
    variable labels based on the training set independent and dependent variables.
  prefs: []
  type: TYPE_NORMAL
- en: While the exact details of how the quantum operators train the algorithm are
    beyond the scope of this book, the algorithm essentially modifies the quantum
    physics governing the processing of independent variables through the quantum
    circuit to produce accurate predictions of the dependent variable. One of the
    advantages of this approach is fewer parameters in the neural network architecture
    and training process to train compared to CNNs, which speeds up the fitting process
    and avoids the need for deep expertise in architecture design to obtain an architecture
    that fits the problem well.
  prefs: []
  type: TYPE_NORMAL
- en: The data we’ll explore comes from one of the most common image analytics benchmark
    datasets. The MNIST dataset contains tens of thousands of images of handwritten
    digits (0–9). These were collected from sets of 250 and 500 writers and combined
    into a single dataset. Because different people and different cultures have different
    ways of writing numbers (such as the slashed 7 in certain parts of Europe to distinguish
    it from a 1) and different people often have different slants to their writing
    (right, left, center, down, up, straight, and so on), classifying which digit
    is in the image is a more challenging problem than it first appears.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into an example that applies this circuit-centric quantum classifier
    in QuantumOps to the recognition of a single digit in the MNIST dataset with [Listing
    10-3](#listing10-3).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 10-3: Training a quantum classifier to recognize the digit 0'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 10-3](#listing10-3) creates a sample of images with the target class
    set to the digit 0 (classifying 0 versus any other number), a learning parameter
    of 1, no decay of the learning rate over each iteration, a low bias (which allows
    other parameters to update faster), and one training iteration. Depending on your
    machine, it may take a while to run, as the algorithm is simulating a quantum
    system; it may even require a machine with more computational power. With a CNN,
    we’d need many more parameters associated with the network (not to mention needing
    to tune the architecture for the number and type of hidden layers), more training
    time, and more computational power. With this code, we optimize 33 quantum gates
    and find a matrix of the entire optimized classifier circuit. This represents
    the quantum architecture that will parse handwritten digits corresponding to 0
    versus every other digit. We could repeat this process until we find quantum classifiers
    for each digit class in MNIST. On an actual quantum computer, this implementation
    would be much faster, as none of the circuitry would need to be simulated on a
    classical circuit.'
  prefs: []
  type: TYPE_NORMAL
- en: While other incarnations of quantum neural networks exist, most don’t have open
    source implementations yet. We encourage you to explore extant papers and tinker
    around with possible implementations of quantum neural networks on existing quantum
    systems. Most quantum computing companies have partnerships available for researchers
    in academia and industry to accelerate quantum algorithm design progress, and
    if you have access to a quantum computer, you can run the algorithms of this chapter
    (and other simulated packages) on a real quantum computer to leverage gains in
    computational speed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we explored distributed computing solutions to scale algorithms,
    such as our computationally intensive network and TDA algorithms. We also introduced
    quantum computing frameworks, simulated a quantum graph algorithm for finding
    min flow and max cut solutions, ran a simulation of quantum greatest common denominator
    algorithms, and explored a simulated quantum classifier in the spirit of CNNs
    to explore the potential of quantum. As quantum computers grow in size and availability
    to researchers and industry data scientists, it’s likely that you’ll have access
    to quantum computers in the future to implement these types of algorithms on real
    systems that will improve performance over classical algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Given the current pace of circuit design, other solutions may be developed in
    the future to help scale the algorithms in this book that struggle on big data
    within current computational infrastructure, and we hope that tools to scale algorithms
    will help accelerate the development of new tools from the fields of geometry
    and topology. The field of topological data analysis is rapidly growing. More
    research in the field may lead to novel algorithms that solve general or niche
    problems in machine learning and data analysis.
  prefs: []
  type: TYPE_NORMAL
