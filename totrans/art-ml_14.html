<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><h2 class="h1" id="ch10"><span epub:type="pagebreak" id="page_165" class="calibre2"/><strong class="calibre3"><span class="big">10</span><br class="calibre18"/>A BOUNDARY APPROACH: SUPPORT VECTOR MACHINES</strong></h2>
<div class="imagec"><img alt="Image" src="../images/common.jpg" class="calibre14"/></div>
<p class="noindent">Support vector machines (SVMs), together with neural networks (NNs), are arguably the two most “purist” of ML methods, motivated originally by artificial intelligence—that is, nonstatistical concepts. We’ll cover SVMs in this chapter and NNs in the next. SVMs are best known for classification applications. They can be used in regression settings as well, but we will focus on classification.</p>
<p class="indent">Keep in mind this chapter will be a tad more mathematical than the others. Staying true to the nonmath spirit of the book, though, equations will be kept to the absolute minimum. SVM is such a powerful, generally usable method that understanding a bit of math here is an excellent investment of time. Even reading the documentation of SVM software requires some understanding of the structural underpinnings of the method.<span epub:type="pagebreak" id="page_166"/></p>
<h3 class="h2" id="ch10lev1">10.1 Motivation</h3>
<p class="noindent">Everything about SVM involves boundary lines separating one class from another. To motivate that, we will first do a boundary analysis using the logistic model and then later bring in SVM. It will be important to keep in mind that throughout this section, we are simply exploring, to motivate SVM.</p>
<h4 class="h3" id="ch10lev1sec1"><em class="calibre22"><strong class="calibre3">10.1.1 Example: The Forest Cover Dataset</strong></em></h4>
<p class="noindent">Let’s revisit the forest cover data from <a href="ch05.xhtml#ch05lev4" class="calibre12">Section 5.4</a>. Here we will construct a motivational graph, so we will need to look at only a small subset of the data. First, to avoid the “black screen problem,” in which we have so many points that the graph becomes an amorphous mess, we will graph a random subset of just 500 data points. Second, to keep things to a visualizable two dimensions, we will use just two features.</p>
<p class="indent">The <span class="literal">qeML</span> package includes a dataset <span class="literal">forest500</span>, consisting of a random 500 rows of the original data. Now, what about the columns? We could try the Feature Ordering by Conditional Independence (FOCI) approach from <a href="ch04.xhtml#ch04lev5sec1" class="calibre12">Section 4.5.1</a>:</p>
<pre class="calibre16">&gt; <span class="codestrong">data(forest500)</span>
&gt; <span class="codestrong">qeFOCI(forest500,'V55')$sel</span>
    index names
 1:     1    V1
 2:     6    V6
...</pre>
<p class="indent">We might run the function again, as there is some randomness involved, but let’s go with the above for our example and, as always, get acquainted with the data:</p>
<pre class="calibre16">&gt; <span class="codestrong">f500 &lt;- forest500[,c(1,6,55)]</span>
&gt; <span class="codestrong">head(f500)</span>
     V1   V6 V55
1: 3438 1033   1
2: 3165 3961   2
3: 3020 5407   2
4: 3244  911   2
5: 2754 1463   2
6: 3008 1275   2
&gt; <span class="codestrong">table(f500$V55)</span>
  1   2   3   4   5   6   7
194 238  29   3   5  10  21</pre>
<p class="indent">As seen above, there are seven cover types, meaning this is a multiclass problem. Here we’ll look at a two-class version, in which we wish to predict whether we have cover type 1 versus all others. The <span class="literal">regtools::toSubFactor()</span> function is handy for that kind of thing.<span epub:type="pagebreak" id="page_167"/></p>
<pre class="calibre16">&gt; <span class="codestrong">f500$V55 &lt;- regtools::toSubFactor(f500$V55,list('1'))</span>
&gt; <span class="codestrong">head(f500)</span>
     V1   V6      V55
1: 3438 1033        1
2: 3165 3961 zzzOther
3: 3020 5407 zzzOther
4: 3244  911 zzzOther
5: 2754 1463 zzzOther
6: 3008 1275 zzzOther</pre>
<p class="indent">Let’s see what the data looks like:</p>
<pre class="calibre16">&gt; <span class="codestrong">plot(f500[,1:2],pch=ifelse(f500[,3] == '1',0,3))</span></pre>
<p class="indent">This code generates the plot shown in <a href="ch10.xhtml#ch10fig01" class="calibre12">Figure 10-1</a>.</p>
<div class="image"><img alt="Image" id="ch10fig01" src="../images/ch10fig01.jpg" class="calibre74"/></div>
<p class="figcap"><em class="calibre13">Figure 10-1: Forest cover data</em></p>
<p class="indent">We want to plot columns 1 and 2, hence the expression <span class="literal">f500[,c(1,2)]</span>. But we want to visually distinguish between the two classes, say, using squares and plus signs as symbols. In base-R graphics, plotting symbols are specified via the <span class="literal">pch</span> (point character) argument, and it turns out that the numerical codes were 0 and 3.<sup class="calibre11"><a id="ch10fn1b" class="calibre12"/><a href="footnote.xhtml#ch10fn1" class="calibre12">1</a></sup> The squares are the cover type 1 points, and pluses are non−type 1.</p>
<p class="indent">There seems to be no sharp tendency in the graph (that is, no trend of separation of the two groups). We see squares and pluses all over the graph. <span epub:type="pagebreak" id="page_168"/>However, the plus signs seem to fall to the left and more on the upward side, versus more to the right for the squares.</p>
<p class="indent">We would like to draw a line in <a href="ch10.xhtml#ch10fig01" class="calibre12">Figure 10-1</a>, such that most pluses are on one side of the line and most squares are on the other side. The reader can take a sneak peak at <a href="ch10.xhtml#ch10fig02" class="calibre12">Figure 10-2</a> to see where we are headed. But where did the line come from? Actually, one can use a logit model here. This should not be too surprising, as you will recall that the logit model has a linear form at its core.</p>
<p class="indent">Here is how such a line can be drawn. Let’s fit the model:</p>
<pre class="calibre16">&gt; <span class="codestrong">w &lt;- qeLogit(f500,'V55',holdout=NULL,yesYVal='1')</span></pre>
<p class="noindent">Normally, in prediction contexts we are not interested in the estimated logistic model coefficients <img alt="Image" class="middle11" src="../images/unch08equ07.jpg"/> . Here, however, we will want to draw a separating line in <a href="ch10.xhtml#ch10fig01" class="calibre12">Figure 10-1</a> using these coefficients. How do we obtain them from the output object <span class="literal">w</span>?</p>
<p class="indent">Recall from <a href="ch08.xhtml#ch08lev9sec3" class="calibre12">Section 8.9.3</a> that multiclass applications of the logit model use either a One vs. All (OVA) or an All vs. All (AVA) approach; <span class="literal">qeLogit()</span> uses OVA. It thus runs one logit model for each class, placing the <span class="literal">glm()</span> outputs in the <span class="literal">glmOuts</span> component of the <span class="literal">qeLogit()</span> output.</p>
<p class="indent">However, the two-class model is a little different. To avoid essentially running the same model twice—for the forest cover data, type 1 versus non−type 1—<span class="literal">qeLogit()</span> runs it just once. In other words, we’ll look at <span class="literal">w$glmOuts[[1]]</span>.</p>
<p class="indent">To obtain the coefficients, we turn to <span class="literal">coef()</span>, yet another generic function like the ones we’ve seen before, such as <span class="literal">print()</span> and <span class="literal">plot()</span>. This function extracts the estimated coefficients:</p>
<pre class="calibre16">&gt; <span class="codestrong">cf &lt;- coef(w$glmOuts[[1]])</span>
 (Intercept)           v1           v2
-1.684947e+01  5.389779e-03  1.469269e-05</pre>
<p class="indent">Now recall again that the logistic model routes the linear model into the logistic function, <em class="calibre13">ℓ</em>(<em class="calibre13">t</em>) = 1/(1 + <em class="calibre13">e</em><sup class="calibre11">−<em class="calibre13">t</em></sup>); the placeholder <em class="calibre13">t</em> is set to the linear form. The above output gives the estimated probability of type-1 cover for a location having feature values of <em class="calibre13">v</em>1 and <em class="calibre13">v</em>6 as:</p>
<p class="center" id="ch10equ01"><img alt="Image" src="../images/ch10equ01.jpg" class="calibre75"/></p>
<p class="noindent">Say we guess the location to have type-1 cover or not depending on whether the estimated probability in <a href="ch10.xhtml#ch10equ01" class="calibre12">Equation 10.1</a> is greater than 0.5 or not. Setting that equation to 0.5, things look formidable at first but become simple when we note the fact that <em class="calibre13">e</em><sup class="calibre11">0</sup> = 1. In other words, if the exponent</p>
<p class="center" id="ch10equ02"><img alt="Image" src="../images/ch10equ02.jpg" class="calibre76"/></p>
<p class="noindent">then the right-hand side of <a href="ch10.xhtml#ch10equ01" class="calibre12">Equation 10.1</a> is equal to 0.5, which is just what we want, a straight line forming the decision boundary.</p>
<p class="indent">So the line in <a href="ch10.xhtml#ch10equ02" class="calibre12">Equation 10.2</a> forms the boundary between predicting type-1 or non-type-1 cover. That is the equation of a straight line, which is <span epub:type="pagebreak" id="page_169"/>plotted in <a href="ch10.xhtml#ch10fig02" class="calibre12">Figure 10-2</a>. We superimposed that line onto <a href="ch10.xhtml#ch10fig01" class="calibre12">Figure 10-1</a> by using R’s <span class="literal">abline()</span> function, which plays exactly the role implied by the name—that is, adding a line to an existing plot:</p>
<pre class="calibre16"># arguments are intercept and slope
&gt; <span class="codestrong">abline(a=-cf[1]/cf[3], b=-cf[2]/cf[3])</span></pre>
<p class="noindent">The result is shown in <a href="ch10.xhtml#ch10fig02" class="calibre12">Figure 10-2</a>. It happens to be almost vertical, which is not surprising since the coefficient of <span class="literal">V6</span> is so small, but no matter. Data points to the right of the line are predicted to be of type-1 cover, with a non-type-1 prediction for those to the left.</p>
<div class="image"><img alt="Image" id="ch10fig02" src="../images/ch10fig02.jpg" class="calibre74"/></div>
<p class="figcap"><em class="calibre13">Figure 10-2: Forest cover data with a logistic boundary line</em></p>
<p class="indent">Clearly, quite a few data points are misclassified—that is, plus signs to the right of the line and squares to the left. We probably could reduce the number of misclassified points by increasing the number of features we use— we had only <em class="calibre13">p</em> = 2 features here—but there still would be some misclassified points.</p>
<p class="indent">This motivates the basic goal of SVM:</p>
<p class="block">We wish to find a line that separates our classes well and then use that line to predict new cases in the future by determining which side of the line they fall on. Our line typically will not fully separate our classes, so we will have some misclassification errors, just as with any ML method. But hopefully a carefully chosen line will serve us well.</p>
<p class="noindent">With <em class="calibre13">p</em> = 3 features, the line becomes a plane in three dimensions, difficult to visualize, and if we have more than three features, it is impossible to visualize. But by always keeping in mind the two-feature case and its geometric interpretation, we will have the intuition to use SVM effectively.<span epub:type="pagebreak" id="page_170"/></p>
<p class="indent">One more point before getting into the details: Why not simply use the above logit scheme to create our boundary line? What might be the advantage of using an SVM-produced line? The answer is that logit is very confining. It specifies a particular form for the regression function, involving the exponential function and so on as in <a href="ch10.xhtml#ch10equ01" class="calibre12">Equation 10.1</a>, and though this might be a good assumption in some applications, it might not be so in others.</p>
<p class="indent">By contrast, aside from the implicit assumption that the best interclass boundary is a straight line rather than some other curve (even this condition can be dropped, as we will see later), SVM makes no assumptions, so it is more flexible and may produce a better fit (just as is the case for k-NN, random forests, and so on, which make even fewer assumptions).</p>
<h3 class="h2" id="ch10lev2">10.2 Lines, Planes, and Hyperplanes</h3>
<p class="noindent">Let’s explore that geometric view a bit further.</p>
<p class="indent">The derivation above always holds with a logistic model; prediction of <em class="calibre13">Y</em> = 1 versus <em class="calibre13">Y</em> = 0 will always boil down to computing a linear function of the features. If we have <em class="calibre13">p</em> = 2 (that is, two features, such as <em class="calibre13">v</em>1 and <em class="calibre13">v</em> 6), the boundary between predicting <em class="calibre13">Y</em> = 1 and <em class="calibre13">Y</em> = 0 is a straight line of the form</p>
<p class="center" id="ch10equ03"><img alt="Image" src="../images/ch10equ03.jpg" class="calibre77"/></p>
<p class="noindent">as we saw in <a href="ch10.xhtml#ch10equ02" class="calibre12">Equation 10.2</a>. If <em class="calibre13">p</em> = 3, say, adding the <em class="calibre13">v</em> 8 feature, the boundary takes the form of a plane, with the form</p>
<p class="center" id="ch10equ04"><img alt="Image" src="../images/ch10equ04.jpg" class="calibre78"/></p>
<p class="indent">As noted above, this is hard to visualize. As also noted, for <em class="calibre13">p</em> &gt; 3, we can’t visualize the setting at all. But we are still working with a linear form in the features, whose behavior is like that of a line or plane. Since it is planelike, we call it a <em class="calibre13">hyperplane</em>. For technical accuracy, we’ll use the acronym LPH (line/plane/hyperplane) rather than merely saying “line,” but readers should always think in terms of lines in the <em class="calibre13">p</em> = 2 case to guide their intuition.</p>
<h3 class="h2" id="ch10lev3">10.3 Math Notation</h3>
<p class="noindent">Central to any discussion of SVM—including reading the documentation for SVM software—is the “dot product” notation. Though the name and math formula may sound intimidating, it’s just a way of stating things more succinctly. We first discuss how to change much of the SVM notation to vector form and then introduce dot products.</p>
<h4 class="h3" id="ch10lev3sec1"><em class="calibre22"><strong class="calibre3">10.3.1 Vector Expressions</strong></em></h4>
<p class="noindent">As can be seen from <a href="ch10.xhtml#ch10equ03" class="calibre12">Equations 10.3</a> and <a href="ch10.xhtml#ch10equ04" class="calibre12">10.4</a>, an LPH can be represented by its coefficient vector—for example, (<em class="calibre13">c</em><sub class="calibre27">1</sub>, <em class="calibre13">c</em><sub class="calibre27">2</sub>, <em class="calibre13">c</em><sub class="calibre27">3</sub>, <em class="calibre13">c</em><sub class="calibre27">4</sub>) in <a href="ch10.xhtml#ch10equ04" class="calibre12">Equation 10.4</a>. It’s customary in SVM to write things in terms of equality to 0, so, for instance, we rewrite <a href="ch10.xhtml#ch10equ04" class="calibre12">Equation 10.4</a> as<span epub:type="pagebreak" id="page_171"/></p>
<p class="center" id="ch10equ05"><img alt="Image" src="../images/ch10equ05.jpg" class="calibre79"/></p>
<p class="noindent">and set:</p>
<p class="center" id="ch10equ06"><img alt="Image" src="../images/ch10equ06.jpg" class="calibre80"/></p>
<p class="noindent">The vector <em class="calibre13">w</em> and the number <em class="calibre13">w</em><sub class="calibre27">0</sub> compose our description of the LPH.</p>
<p class="indent">So, we would summarize <a href="ch10.xhtml#ch10equ02" class="calibre12">Equation 10.2</a> by writing:</p>
<p class="center" id="ch10equ07"><img alt="Image" src="../images/ch10equ07.jpg" class="calibre81"/></p>
<h4 class="h3" id="ch10lev3sec2"><em class="calibre22"><strong class="calibre3">10.3.2 Dot Products</strong></em></h4>
<p class="noindent">The underlying theory of SVM makes heavy use of calculus and linear algebra. As noted, such mathematics is far beyond the scope of this book. However, it will be productive and easy to use some notation from that subject— just notation and nothing conceptual other than a little algebra.</p>
<p class="indent">Our goal will be to come up with a simple, compact way to determine on which side of the boundary line or LPH a new case falls so that we may easily predict its class.</p>
<p class="indent">The <em class="calibre13">dot product</em> between two vectors <em class="calibre13">u</em> = (<em class="calibre13">u</em><sub class="calibre27">1</sub>, . . . , <em class="calibre13">u</em><em class="calibre13"><sub class="calibre27">m</sub></em>) and <em class="calibre13">v</em> = (<em class="calibre13">v</em><sub class="calibre27">1</sub>, . . . , <em class="calibre13">v</em><em class="calibre13"><sub class="calibre27">m</sub></em>) is simply a sum of products:</p>
<p class="center" id="ch10equ08"><img alt="Image" src="../images/ch10equ08.jpg" class="calibre82"/></p>
<p class="noindent">For example, let’s take the dot product of <em class="calibre13">w</em> in <a href="ch10.xhtml#ch10equ07" class="calibre12">Equation 10.7</a> with the vector (1,−4):</p>
<p class="center" id="ch10equ09"><img alt="Image" src="../images/ch10equ09.jpg" class="calibre83"/></p>
<p class="noindent">It is helpful to recast <a href="ch10.xhtml#ch10equ01" class="calibre12">Equation 10.1</a> in our new dot product notation:</p>
<p class="center" id="ch10equ10"><img alt="Image" src="../images/ch10equ10.jpg" class="calibre84"/></p>
<p class="noindent">Note some algebraic properties:</p>
<ul class="calibre15">
<li class="noindent3"><em class="calibre13">e</em><sup class="calibre11">0</sup> = 1</li>
<li class="noindent3"><em class="calibre13">e</em><em class="calibre13"><sup class="calibre11">t</sup></em> &gt; 1 for <em class="calibre13">t</em> &gt; 0</li>
<li class="noindent3"><em class="calibre13">e</em><sup class="calibre11">−<em class="calibre13">t</em></sup> &lt; 1 for <em class="calibre13">t</em> &gt; 0</li>
<li class="noindent3"><img alt="Image" src="../images/unch10equ01.jpg" class="calibre85"/></li>
</ul>
<p class="noindent">Thus, in <a href="ch10.xhtml#ch10equ10" class="calibre12">Equation 10.10</a>,</p>
<p class="center" id="ch10equ11"><img alt="Image" src="../images/ch10equ11.jpg" class="calibre86"/></p>
<p class="noindent">and</p>
<p class="center" id="ch10equ12"><img alt="Image" src="../images/ch10equ12.jpg" class="calibre86"/></p>
<p class="noindent">So, faced with a new case to predict, we simply look at the sign—positive and negative—of <em class="calibre13">w</em> • (<em class="calibre13">v</em>1, <em class="calibre13">v</em>6) + <em class="calibre13">w</em><sub class="calibre27">0</sub>. If positive, the new case is more likely (probability more than 0.5) to have cover type 1, while in the negative case, that <span epub:type="pagebreak" id="page_172"/>probability is less than 0.5. In other words, we predict cover type 1 for the new case if <em class="calibre13">w</em> • (<em class="calibre13">v</em>1, <em class="calibre13">v</em>6) + <em class="calibre13">w</em><sub class="calibre27">0</sub> &gt; 0, and otherwise predict non−type 1.</p>
<p class="indent">This again is saying that we guess cover type 1 if our new case to be predicted falls to the right of the line or non−type 1 if it is on the left. And our SVM boundary is the vector <em class="calibre13">x</em> that makes:</p>
<p class="center" id="ch10equ13"><img alt="Image" src="../images/ch10equ13.jpg" class="calibre87"/></p>
<p class="indent">Again, this is just notation, but in math, it is often the case that convenient notation helps clarify things. Drawing lines is visualizable for applications with <em class="calibre13">p</em> = 2 features, but drawing planes is hard to visualize if <em class="calibre13">p</em> = 3, and if <em class="calibre13">p</em> &gt; 3, visualization is impossible. What’s nice about the dot product notation is that we know which way to guess the class of a new case by simply noting whether <em class="calibre13">w</em> • (<em class="calibre13">v</em>1, <em class="calibre13">v</em>6) + <em class="calibre13">w</em><sub class="calibre27">0</sub> is positive or negative.</p>
<p class="indent">By the way, SVM theorists also like to code the two classes as <em class="calibre13">Y</em> = +1 and <em class="calibre13">Y</em> = −1 rather than 1 or 0, as is standard in statistics. We’ll continue to use the latter coding in general but will turn to the former in this chapter.</p>
<h4 class="h3" id="ch10lev3sec3"><em class="calibre22"><strong class="calibre3">10.3.3 SVM as a Parametric Model</strong></em></h4>
<p class="noindent">We have referred to linear and logistic models as <em class="calibre13">parametric</em> in that the regression function is modeled as being determined by a finite number of values <em class="calibre13">β</em><sub class="calibre27">0</sub>, <em class="calibre13">β</em><sub class="calibre27">1</sub>, . . . , <em class="calibre13">β</em><em class="calibre13"><sub class="calibre27">p</sub></em> . This is in contrast to, for example, k-NN methods, which make no assumptions regarding the form of the regression function.</p>
<p class="indent">One implication of <a href="ch10.xhtml#ch10equ13" class="calibre12">Equation 10.13</a> is that SVM too is a parametric model. Instead of assuming a parametric form for the regression function, here we assume a parametric form for the boundary line between the two classes.</p>
<h3 class="h2" id="ch10lev4">10.4 SVM: The Basic Ideas—Separable Case</h3>
<p class="noindent">As noted earlier, the line we drew in <a href="ch10.xhtml#ch10fig02" class="calibre12">Figure 10-2</a> did not cleanly separate the two classes. There were plus signs and squares on both sides of the line. This is typical, not only for logit-produced lines but also for lines created by SVM—our focus here. However, the SVM method is easier to explain if we first consider datasets for which the two classes are cleanly separable, so most books begin with that case, as we will here.</p>
<p class="indent">For clarity, we will continue to focus on the two-class case, as with the type-1/non-type-1 cover example above. And again, we will continue looking at the case of <em class="calibre13">p</em> = 2 features, where the LPH is a line.</p>
<p class="indent">Note that all our references to “the data” here will be in terms of the training data. We find a boundary line for the training data and then predict future cases according to that line. Similarly, when we speak of separability of “the data,” we mean the training data.<span epub:type="pagebreak" id="page_173"/></p>
<h4 class="h3" id="ch10lev4sec1"><em class="calibre22"><strong class="calibre3">10.4.1 Example: The Anderson Iris Dataset</strong></em></h4>
<p class="noindent">Edgar Anderson’s data on iris flowers, included in R, has been the subject of countless examples in books, websites, and so on. There are three classes: <em class="calibre13">setosa</em>, <em class="calibre13">versicolor</em>, and <em class="calibre13">virginica</em>.</p>
<p class="indent">This dataset is included in R:</p>
<pre class="calibre16">&gt; <span class="codestrong">head(iris)</span>
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa</pre>
<p class="indent">Note that what we do in this section will be mainly for motivating the subsequent material. For day-to-day SVM computation, you’ll use <span class="literal">qeSVM()</span>. So, in the example here, in which we do some non-SVM analysis for motivational purposes, we will omit some of the code and algebra.</p>
<p class="indent">As mentioned, for this example, we’d like data in which the two classes are cleanly separated by a straight line. This is the case if we take our two iris classes as setosa and nonsetosa, with the features taken to be the <span class="literal">Sepal.Length</span> and <span class="literal">Petal.Width</span> columns.</p>
<p class="indent">Let’s first plot the data.</p>
<pre class="calibre16">&gt; <span class="codestrong">j2 &lt;- iris[,c(2,4,5)]</span>  # sepal width, petal width, species
# set up species code, 1 for setosa, 0 for nonsetosa
&gt; <span class="codestrong">head(j2)</span>
  Sepal.Width Petal.Width Species
1         3.5         0.2  setosa
2         3.0         0.2  setosa
3         3.2         0.2  setosa
4         3.1         0.2  setosa
5         3.6         0.2  setosa
6         3.9         0.4  setosa
&gt; <span class="codestrong">j2$Species &lt;- toSubFactor(j2$Species,'setosa')</span>
&gt; <span class="codestrong">j2[c(7,77),]</span>
   Sepal.Width Petal.Width  Species
7          3.4         0.3   setosa
77         2.8         1.4 zzzOther
&gt; <span class="codestrong">plot(j2[,1:2],pch=3*as.numeric(j2[,3]))</span></pre>
<p class="indent">This produces the graph in <a href="ch10.xhtml#ch10fig03" class="calibre12">Figure 10-3</a>.<span epub:type="pagebreak" id="page_174"/></p>
<div class="image"><img alt="Image" id="ch10fig03" src="../images/ch10fig03.jpg" class="calibre88"/></div>
<p class="figcap"><em class="calibre13">Figure 10-3: Setosa and nonsetosa</em></p>
<p class="indent">One can easily draw a line between the two classes—in fact, many lines. But, which line is optimal?</p>
<h4 class="h3" id="ch10lev4sec2"><em class="calibre22"><strong class="calibre3">10.4.2 Optimizing Criterion</strong></em></h4>
<p class="noindent">Choosing our boundary line amounts to choosing the coefficient vector <em class="calibre13">w</em> and the <em class="calibre13">w</em><sub class="calibre27">0</sub> term. In other words, the situation is similar to that of the linear and generalized linear models, where we choose the estimated coefficients <img alt="Image" class="middle11" src="../images/unch08equ07.jpg"/>. (Actually, <em class="calibre13">w</em> and <em class="calibre13">w</em><sub class="calibre27">0</sub> are estimates as well, but to avoid clutter, we do not use the hat notation.) Now recall that with a linear model, the way we choose our coefficients <img alt="Image" class="middle11" src="../images/unch08equ07.jpg"/> is through an optimization problem: we minimize a certain sum of squares.</p>
<p class="indent">SVM still minimizes a certain sum, but it uses a different loss function than squared error. Detailing it would take us too far into some arcane math with little, if any, benefit. Fortunately, the math has an easily grasped geometric version, which we will now discuss.</p>
<p class="indent">Toward this end, look at <a href="ch10.xhtml#ch10fig04" class="calibre12">Figure 10-4</a>. Here we have “roped off” our two classes (setosas above and nonsetosas below) into what are called <em class="calibre13">convex hulls</em>. Again, this is just for illustration purposes; the <span class="literal">qeSVM()</span> function will do the computation for us (and with a different method), and we will not compute convex hulls ourselves after this example. We thus omit the code. (One can use the function <span class="literal">mvtnorm::chull()</span>.)<span epub:type="pagebreak" id="page_175"/></p>
<div class="image"><img alt="Image" id="ch10fig04" src="../images/ch10fig04.jpg" class="calibre88"/></div>
<p class="figcap"><em class="calibre13">Figure 10-4: SVM convex hulls</em></p>
<p class="indent">It can be shown mathematically that the SVM boundary line is “halfway between” the two convex hulls. More precisely stated, one first finds the two points in the hulls that are closest to each other. Our boundary line is then the perpendicular bisector of the line segment between those two points. We draw that in <a href="ch10.xhtml#ch10fig05" class="calibre12">Figure 10-5</a>, along with two related, dashed lines, which define the “margin” of an SVM fit.</p>
<div class="image"><img alt="Image" id="ch10fig05" src="../images/ch10fig05.jpg" class="calibre88"/></div>
<p class="figcap"><em class="calibre13">Figure 10-5: SVM margin</em><span epub:type="pagebreak" id="page_176"/></p>
<p class="indent">Note the following, both here and in general:</p>
<ul class="calibre15">
<li class="noindent3">The region between the dashed lines is called the <em class="calibre13">margin</em>.</li>
<li class="noindent3">For separable data, there will be no data points <em class="calibre13">inside</em> the margin.</li>
<li class="noindent3">The points lying <em class="calibre13">on</em> the margin are called <em class="calibre13">support vectors</em> (the SV in SVM). For this dataset, we have three support vectors, one for the setosas at (2.7,1.0) and two for the nonsetosas at (2.3,0.3) and (3.5,0.6).</li>
<li class="noindent3">In terms of <em class="calibre13">w</em> and <em class="calibre13">w</em><sub class="calibre27">0</sub>, the value of <em class="calibre13">w</em><sub class="calibre27">0</sub> + <em class="calibre13">w</em> • <em class="calibre13">x</em> will be:
<ul class="none">
<li class="noindent4"><strong class="calibre5">−</strong>     0 for any point <em class="calibre13">x</em> on the boundary</li>
<li class="noindent4"><strong class="calibre5">−</strong>     +1 for any support vector in the <em class="calibre13">Y</em> = +1 class (setosa in this case)</li>
<li class="noindent4"><strong class="calibre5">−</strong>     −1 for any support vector in the <em class="calibre13">Y</em> = −1 class (nonsetosa in this case)</li>
<li class="noindent4"><strong class="calibre5">−</strong>     &gt; +1 for any nonsupport vector in the <em class="calibre13">Y</em> = +1 class</li>
<li class="noindent4"><strong class="calibre5">−</strong>     &lt; −1 for any nonsupport vector in the <em class="calibre13">Y</em> = −1 class</li>
</ul>
<p class="noindent">By the way, a look at <a href="ch10.xhtml#ch10equ13" class="calibre12">Equation 10.13</a> shows that the values of <em class="calibre13">w</em> and <em class="calibre13">w</em><sub class="calibre27">0</sub> are not unique. Multiplying both by, say, 8.8 would still result in 0 on the right-hand side of the equation. So, the convention is to choose them such that the value of <em class="calibre13">w</em><sub class="calibre27">0</sub> + <em class="calibre13">w</em> • <em class="calibre13">x</em> is +1 or −1 at the support points.</p></li>
<li class="noindent3">To predict <em class="calibre13">Y</em> for a new case in which <em class="calibre13">X</em> = <em class="calibre13">x</em><sub class="calibre27">new</sub>, we guess <em class="calibre13">Y</em> to be either +1 or −1, depending on whether <em class="calibre13">w</em><sub class="calibre27">0</sub> + <em class="calibre13">w</em> • <em class="calibre13">x</em><sub class="calibre27">new</sub> is greater than or less than 0. Note that even though our training data is assumed separable here, new data points may fall within the margin.</li>
</ul>
<p class="indent">Keep in mind fitting an SVM model amounts to choosing <em class="calibre13">w</em> and <em class="calibre13">w</em><sub class="calibre27">0</sub> . The above scheme can be shown to be optimal. But wait a minute—what does “optimal” even mean? The criterion for optimality generally used in SVM is this:</p>
<p class="block">We choose <em class="calibre13">w</em> and <em class="calibre13">w</em><sub class="calibre27">0</sub> in such a way that the margin will have the largest possible width.</p>
<p class="noindent">In other words, SVM seeks to not only separate the data points of the two classes but also render the two classes as far apart as possible, relative to the boundary. It finds a “buffer zone” between the two classes, maximizing the width of the zone. As noted, that buffer zone is called the margin.</p>
<p class="indent">The key idea is that a large margin in the training set means that the two classes are well separated, which hopefully means that new cases in the future will be correctly classified. It turns out that choosing <em class="calibre13">w</em> and <em class="calibre13">w</em><sub class="calibre27">0</sub> using convex hulls as above (even with <em class="calibre13">p</em> &gt; 2) does maximize the margin.<span epub:type="pagebreak" id="page_177"/></p>
<h5 class="h4" id="ch10lev4sec2sec1">10.4.2.1 Significance of the Support Vectors</h5>
<p class="noindent">The support vectors “support” the fit, in the sense that a change in any of them will change the fit; a change in the other data points will not change the fit (as long as it stays within the hull). Of course, we can also say that <em class="calibre13">adding</em> new points won’t change the fit either, as long as the new points are within a hull.</p>
<p class="indent">An oft-claimed benefit of SVMs is that (at least in the separable case) they tend to produce a sparse fit, in this case, meaning not that most of the components of <em class="calibre13">w</em> are 0s but rather that the essential dimensionality of the fit is low. Viewed another way, the claim is that the more support vectors one has, the greater the risk of overfitting. Evidence for this assertion is weak, I believe, though it may serve as one of several guides to one’s model-fitting process.</p>
<p class="indent">But that benefit may be illusory. As noted, the source of the sparsity is the dependence of <em class="calibre13">w</em> on a few data points (that is, the support vectors). But what if some of the support vectors are outliers (not representative of the data as a whole) or are even downright erroneous? Many real datasets do have some errors. Our fit then depends heavily on some questionable data.</p>
<p class="indent">Furthermore, this high sensitivity of <em class="calibre13">w</em> to just a few data points makes for high sampling variability; a different sample likely has a different set of support vectors. In other words, <em class="calibre13">w</em> has a high variance.</p>
<h3 class="h2" id="ch10lev5">10.5 Major Problem: Lack of Linear Separability</h3>
<p class="noindent">The margin, actually known as the <em class="calibre13">hard margin</em>, is only defined in situations in which some line (or LPH) exists that cleanly separates the data points of the two classes. As can be seen in our earlier graphs for the forest cover data, in most practical situations, no such line exists. Even with the iris data, no line separates the versicolor and virginica in columns 1 and 3:</p>
<pre class="calibre16">&gt; <span class="codestrong">plot(iris[,c(1,3)],pch=as.numeric(iris$Species))</span></pre>
<p class="noindent">See <a href="ch10.xhtml#ch10fig06" class="calibre12">Figure 10-6</a>; pluses and triangles are not cleanly separated.</p>
<p class="indent">There are two solutions to that problem: (a) using a <em class="calibre13">kernel</em> to transform the data into linear separability or (b) creating a <em class="calibre13">soft margin</em>, in which we allow some points to reside within the margin. Typically a combination of these two approaches is used. For instance, after doing a kernel transformation, we still may—in fact, probably will—find that no cleanly separating LPH exists, and thus we will need to resort to also allowing some exceptional points to lie within the margin. However, the fewer the exceptions, the better, so it’s best to use both approaches in combination rather than just going directly to a soft margin solution.<span epub:type="pagebreak" id="page_178"/></p>
<div class="image"><img alt="Image" id="ch10fig06" src="../images/ch10fig06.jpg" class="calibre21"/></div>
<p class="figcap"><em class="calibre13">Figure 10-6: Iris data; three classes</em></p>
<h4 class="h3" id="ch10lev5sec1"><em class="calibre22"><strong class="calibre3">10.5.1 Applying a “Kernel”</strong></em></h4>
<p class="noindent">Here we transform the data, say, by applying a polynomial transformation, and then find an LPH separator on the new data. The degree of the polynomial is then a hyperparameter.</p>
<h5 class="h4" id="ch10lev5sec1sec1">10.5.1.1 Motivating Illustration</h5>
<p class="noindent">To get a feeling as to why kernels can be useful, consider a favorite example in ML presentations: “doughnut-shaped” data. Let’s generate some. (The code here is rather arcane and can be safely skipped without affecting the sequel.)</p>
<pre class="calibre16"># generate 250 pairs of data points centered around (0,0)
&gt; <span class="codestrong">set.seed(9999)</span>
&gt; <span class="codestrong">z &lt;- matrix(rnorm(500),ncol=2)</span>
# form new data by taking only certain points from z
&gt; <span class="codestrong">plus1 &lt;- z[z[,1]^2 + z[,2]^2 &gt; 4,]</span>  # outer ring, class +1
&gt; <span class="codestrong">minus1 &lt;- z[z[,1]^2 + z[,2]^2 &lt; 2,]</span>  # inner disk, class -1
&gt; <span class="codestrong">plus1 &lt;- cbind(plus1,+1)</span>  # add in Y column
&gt; <span class="codestrong">minus1 &lt;- cbind(minus1,-1)</span>  # add in Y column
&gt; <span class="codestrong">head(plus1)</span>  # take a look
           [,1]       [,2] [,3]
[1,]  2.9038161  0.7172792    1
[2,] -0.4499405  2.0006861    1
[3,]  2.3329026  0.2288606    1
[4,] -0.2989460  2.3790936    1
[5,] -2.0778949  0.1488060    1
[6,] -0.9867098 -2.2020235    1
&gt; <span class="codestrong">head(minus1)</span>
           [,1]       [,2] [,3]
[1,]  1.0840991  0.670507239   -1
[2,]  0.8431089 -0.074557109   -1
[3,] -0.7730161 -0.009357795   -1
[4,]  0.9088839 -1.050183477   -1
[5,] -0.1882887 -1.348365272   -1
[6,]  0.9864382  0.936775923   -1
&gt; <span class="codestrong">pm1 &lt;- rbind(plus1,minus1)</span>  # combine into one dataset
&gt; <span class="codestrong">plot(pm1[,1:2],pch=pm1[,3]+2)</span>  # gives us pluses and circles</pre>
<p class="noindent"><span epub:type="pagebreak" id="page_179"/>The data is plotted in <a href="ch10.xhtml#ch10fig07" class="calibre12">Figure 10-7</a>. The two classes, drawn with pluses and circles, are clearly separable—but by a circle and not a straight line.</p>
<div class="image"><img alt="Image" id="ch10fig07" src="../images/ch10fig07.jpg" class="calibre89"/></div>
<p class="figcap"><em class="calibre13">Figure 10-7: “Doughnut” data</em></p>
<p class="indent">But we can fix that by adding a squared term:</p>
<pre class="calibre16">&gt; <span class="codestrong">pm2 &lt;- pm1[,1:2]^2</span>  # replace each X value by its square
&gt; <span class="codestrong">pm2 &lt;- cbind(pm2,pm1[,3])</span>  # tack on Y to the new datasets
&gt; <span class="codestrong">plot(pm2[,1:2],pch=pm2[,3]+2)</span></pre>
<p class="noindent">We took our original dataset and transformed it, replacing each data point by its square. That square is our new feature, replacing the old one. In the plot of the new data, <a href="ch10.xhtml#ch10fig08" class="calibre12">Figure 10-8</a>, the pluses and circles are easily separated by a straight line.<span epub:type="pagebreak" id="page_180"/></p>
<div class="image"><img alt="Image" id="ch10fig08" src="../images/ch10fig08.jpg" class="calibre21"/></div>
<p class="figcap"><em class="calibre13">Figure 10-8: “Doughnut” data, transformed</em></p>
<p class="indent">And that is what SVM users typically do: try to find a transformation of the data under which the transformed data will be linearly separable, or at least nearly so. Of course, the SVM software does all the work for us; we don’t transform the data by hand, as in the above illustration. This is done with a kernel, as seen in the next section.</p>
<h5 class="h4" id="ch10lev5sec1sec2">10.5.1.2 The Notion of a Kernel</h5>
<p class="noindent">We need to cover one more point before turning to an example on real data. What is a kernel? A <em class="calibre13">kernel</em> is a way of transforming our data, again with the goal of making our data separable. But it’s a little more specific than that; it’s a function <em class="calibre13">K</em>(<em class="calibre13">u</em>, <em class="calibre13">v</em>) whose inputs are two vectors.</p>
<p class="indent">This makes sense because we saw in <a href="ch10.xhtml#ch10lev3sec2" class="calibre12">Section 10.3.2</a> that dot products play a key role in SVM, and, in fact, even more so in the internal computations, which are not covered in this book. Accordingly, many kernels are functions of dot products.</p>
<p class="indent">An example is the <em class="calibre13">polynomial kernel</em>:</p>
<p class="center" id="ch10equ14"><img alt="Image" src="../images/ch10equ14.jpg" class="calibre90"/></p>
<p class="indent">The quantities <em class="calibre13">d</em> and <em class="calibre13">γ</em> are hyperparameters. In the quadratic case <em class="calibre13">d</em> = 2, we achieve essentially the same effect as in the previous section, where we squared the <em class="calibre13">X</em> values. Here we square dot products.</p>
<p class="indent">Also in common usage is the <em class="calibre13">radial basis function (RBF)</em>:</p>
<p class="center" id="ch10equ15"><img alt="Image" src="../images/ch10equ15.jpg" class="calibre91"/></p>
<p class="noindent">Here <em class="calibre13">γ</em> is a hyperparameter.<span epub:type="pagebreak" id="page_181"/></p>
<p class="indent">Once again, as with so many ML questions, the answer to the question “Which kernel is best?” is “It depends.” The type and size of the dataset, the number of features, and so on all make for variation in performance between kernels.</p>
<h4 class="h3" id="ch10lev5sec2"><em class="calibre22"><strong class="calibre3">10.5.2 Soft Margin</strong></em></h4>
<p class="noindent">As noted, it is rather uncommon to have linearly separable data. Nonseparability is the typical case. How can we handle it, in the sense of tolerating having a few points lying within the margin? Let’s consider two approaches.</p>
<h5 class="h4" id="ch10lev5sec2sec1">10.5.2.1 Geometric View</h5>
<p class="noindent">Instead of computing convex hulls, we could work with <em class="calibre13">reduced convex hulls</em>. Once again, the mathematical formulation is beyond the scope of this book, but the essence of the procedure is this:</p>
<ul class="calibre15">
<li class="noindent3">We replace the two original convex hulls with shrunken versions. (Not only will they be smaller in size, but their shape will tend to be “rounder,” or less oblong.)</li>
<li class="noindent3">The margin will be computed on the basis of the shrunken hulls.</li>
<li class="noindent3">Then, conduct “business as usual,” even though some of the training set data will then fall within the margin.</li>
</ul>
<p class="indent">Of course, the amount of shrinkage is, as always, a hyperparameter to be set by the user, possibly via cross-validation.</p>
<h5 class="h4" id="ch10lev5sec2sec2">10.5.2.2 Algebraic View</h5>
<p class="noindent">The geometric view is intuitive, but the more common approach is via “cost.” There is a cost hyperparameter, usually denoted by <em class="calibre13">C</em>. Here is how it works. Denote data point <em class="calibre13">i</em> in our training set by <em class="calibre13">X</em><em class="calibre13"><sub class="calibre27">i</sub></em>, the vector of features for that data point, and let <em class="calibre13">Y</em><em class="calibre13"><sub class="calibre27">i</sub></em> be the class label, either +1 or −1.</p>
<p class="indent">In the separable case, recall that, for data points on the margin boundary, <em class="calibre13">w</em><sub class="calibre27">0</sub> + <em class="calibre13">w</em> • <em class="calibre13">X<sub class="calibre27">i</sub></em> is equal to either +1 or −1. For points outside the margin,</p>
<p class="center" id="ch10equ16"><img alt="Image" src="../images/ch10equ16.jpg" class="calibre92"/></p>
<p class="noindent">depending on whether <em class="calibre13">Y<sub class="calibre27">i</sub></em> is +1 or −1. But there is a neat trick to state this requirement more compactly:</p>
<p class="center" id="ch10equ17"><img alt="Image" src="../images/ch10equ17.jpg" class="calibre93"/></p>
<p class="indent">In the case of a soft margin, we relax that a bit, allowing discrepancies from 1.0, say,</p>
<p class="center" id="ch10equ18"><img alt="Image" src="../images/ch10equ18.jpg" class="calibre94"/></p>
<p class="noindent">and</p>
<p class="center" id="ch10equ19"><img alt="Image" src="../images/ch10equ19.jpg" class="calibre95"/><span epub:type="pagebreak" id="page_182"/></p>
<p class="noindent">Let <em class="calibre13">d</em><em class="calibre13"><sub class="calibre27">i</sub></em> denote the discrepancy for data point <em class="calibre13">i</em> so that here <em class="calibre13">d</em><sub class="calibre27">3</sub> = 0.12 and <em class="calibre13">d</em><sub class="calibre27">8</sub> = −1.71. Set <em class="calibre13">d</em><em class="calibre13"><sub class="calibre27">i</sub></em> = 0 if data point <em class="calibre13">i</em> has no discrepancy—that is, if <em class="calibre13">Y</em><em class="calibre13"><sub class="calibre27">i</sub></em> (<em class="calibre13">w</em><sub class="calibre27">0</sub> + <em class="calibre13">w</em> • <em class="calibre13">X</em><em class="calibre13"><sub class="calibre27">i</sub></em>) ≥ 1.</p>
<p class="indent">Note that if 0 &lt; <em class="calibre13">d</em><em class="calibre13"><sub class="calibre27">i</sub></em> &lt; 1, then data point <em class="calibre13">i</em> is a margin violation, but it still is on the correct side of the decision boundary—that is, it will be correctly classified. But if <em class="calibre13">d</em><em class="calibre13"><sub class="calibre27">i</sub></em> &gt; 1, the point will be on the other side of the boundary and thus misclassified.</p>
<p class="indent">We control the total amount of discrepancy by stipulating that</p>
<p class="center" id="ch10equ20"><img alt="Image" src="../images/ch10equ20.jpg" class="calibre96"/></p>
<p class="noindent">where the hyperparameter <em class="calibre13">C</em> is our “discrepancy budget.” Again, the user sets <em class="calibre13">C</em>.</p>
<p class="indent">Note that the <em class="calibre13">d</em><em class="calibre13"><sub class="calibre27">i</sub></em> are not hyperparameters; they are by-products. The user chooses <em class="calibre13">C</em>. Each potential value of <em class="calibre13">w</em> and <em class="calibre13">w</em><sub class="calibre27">0</sub> then gives rise to the <em class="calibre13">d</em><em class="calibre13"><sub class="calibre27">i</sub></em>. The smaller the value we set for <em class="calibre13">C</em>, the fewer the number of data points within the margin—but the narrower the margin. We hope to have a wide margin; the SVM algorithm finds the values of <em class="calibre13">w</em> and <em class="calibre13">w</em><sub class="calibre27">0</sub> that maximize the width of the margin, subject to the constraint (<a href="ch10.xhtml#ch10equ20" class="calibre12">Equation 10.20</a>).</p>
<h3 class="h2" id="ch10lev6">10.6 Example: Forest Cover Data</h3>
<p class="noindent">Let’s give <span class="literal">qeSVM()</span> a try:</p>
<pre class="calibre16">&gt; <span class="codestrong">z &lt;- qeSVM(f500,'V55',holdout=NULL)</span></pre>
<p class="indent">We can then predict, say, for a new case similar to case 8 in our data, but changing the second feature value to 2,888:</p>
<pre class="calibre16">&gt; <span class="codestrong">newx &lt;- f500[8,1:2]</span>
&gt; <span class="codestrong">newx</span>
    V1   V6
8 3085 2977
&gt; <span class="codestrong">newx[2] &lt;- 2888</span>
&gt; <span class="codestrong">predict(z,newx)</span>
$predClasses
[1] "zzzOther"
<br class="calibre1"/>
$probs
          1  zzzOther
8 0.4829782 0.5170218</pre>
<p class="noindent">We predict non-type-1 cover, though just slightly more likely than type 1.</p>
<p class="indent">We’re using the default values here, which, among other things, set <span class="literal">kernel</span> to <span class="literal">radial</span>. If we wish to use just a soft margin (that is, no kernel transformation), we set <span class="literal">kernel</span> to <span class="literal">linear</span>. The hyperparameter <span class="literal">gamma</span>’s default value is 1.0. An optional hyperparameter, <span class="literal">cost</span>, is the value <em class="calibre13">C</em> in our earlier discussion of soft margins. By the way, <span class="literal">qeML()</span> wraps <span class="literal">svm</span> in the famous <span class="literal">e10171</span> package.<span epub:type="pagebreak" id="page_183"/></p>
<h3 class="h2" id="ch10lev7">10.7 And What About That Kernel Trick?</h3>
<p class="noindent">No presentation of SVM would be complete without discussing the famous <em class="calibre13">kernel trick</em>, as it is largely responsible for the success of SVM. As usual, we won’t delve into the mathematical details—assuming the reader has no burning desire to learn about reproducing kernel Hilbert spaces—but the principle itself has major practical implications.</p>
<p class="indent">To motivate this, let’s get an idea of how large our dataset can expand to when we transform via polynomials <em class="calibre13">without</em> using kernels. Our size measure in the transformed data will be the number of columns.</p>
<p class="indent">Let’s use the forest cover data again, together with the <span class="literal">polyreg</span> package, the latter being used in our counting data columns. (The <span class="literal">getPoly()</span> function in <span class="literal">polyreg</span> is used by the polynomial models in our <span class="literal">qe</span>-series.)</p>
<pre class="calibre16">&gt; <span class="codestrong">gpout &lt;- polyreg::getPoly(forest500,2)</span>  # input with n = 500, p = 54
P &gt; N. With polynomial terms and interactions, P is 1564.</pre>
<p class="noindent">The original dataset had only 54 features, but in the new form we have over 1,500 of them! We would have even more if <span class="literal">getPoly()</span> did not avoid creating duplicates—for example, the square of a dummy variable.</p>
<p class="indent">So, the new value of <em class="calibre13">p</em> was 1,564, for a dataset with only 500 rows, making it impossible to run. With the original data frame of more than 580,000 rows, the new size would be 581,012 × 1,564 = 908,702,768 elements. At 8 bytes per element, that would mean over 7GB of RAM! And it’s not just space but also time—the code would run forever.</p>
<p class="indent">And that was just for degree 2. Imagine degree-3 polynomials and so on! (Degree 3 turns out to generate 16,897 columns.) So, some kind of shortcut is badly needed. Kernel trick to the rescue!</p>
<p class="indent">The key point is that by using the kernel in <a href="ch10.xhtml#ch10equ14" class="calibre12">Equation 10.14</a>, we can avoid having to compute and store all those extra columns. In the forest cover data, we can stick to those original 54 features—the vectors <em class="calibre13">u</em> and <em class="calibre13">v</em> in that expression are each 54 elements long—rather than computing and storing the 1,564. We get the same calculations mathematically as if we were to take <em class="calibre13">u</em> and <em class="calibre13">v</em> to be 1,564-element vectors.</p>
<h3 class="h2" id="ch10lev8">10.8 “Warning: Maximum Number of Iterations Reached”</h3>
<p class="noindent">As with many other ML methods, the computation for SVM is iterative. However, unlike those other methods, SVM should not have convergence problems. The search space has the <em class="calibre13">convex</em> property, which basically says it is bowl-shaped and thus easy to find the minimum.</p>
<p class="indent">However, that presumes there is something to minimize. As we’ve seen with using soft margins or kernels, a line may not exist to cleanly separate the classes. For some particular pair of cost value and kernel (and the latter’s hyperparameters), it may be that no solution exists. In that case, we will of course have convergence issues, and we will have to try other combinations.<span epub:type="pagebreak" id="page_184"/></p>
<h3 class="h2" id="ch10lev9">10.9 Summary</h3>
<p class="noindent">This chapter has been a bit more mathematical than the others and perhaps a bit more abstract as well. But really, the basic principles are simple:</p>
<ul class="calibre15">
<li class="noindent3">SVM is primarily for classification problems. Using OVA or AVA pairing, SVM can handle any number of classes, but to keep things simple, let’s assume two classes here.</li>
<li class="noindent3">If we have <em class="calibre13">p</em> = 2 features, the basic goal is to find a line separating the two classes.</li>
<li class="noindent3">With <em class="calibre13">p</em> = 3, we wish to find a separating plane in three-dimensional space.</li>
<li class="noindent3">For the cases where <em class="calibre13">p</em> &gt; 3, we speak of a separating hyperplane. We cannot visualize these, and instead look at dot products with our hyperplane’s <em class="calibre13">w</em> vector. To classify a new case, we take the dot product of that case’s feature vector, add <em class="calibre13">w</em><sub class="calibre27">0</sub>, and decide the class based on whether the dot product is greater or less than 0.</li>
<li class="noindent3">We associate with the separating LPH a pair of LPHs that are parallel to the original one, thus creating the margin. The optimization criterion for choosing <em class="calibre13">w</em> is to maximize the width of the margin.</li>
<li class="noindent3">Typically the classes overlap, so there is no separating LPH, and we need to resort to artificial means to separate the classes. There are two ways to do this, usually used in combination:
<ul class="none">
<li class="noindent4"><strong class="calibre5">−</strong>     We can posit that a separating LPH does exist but is “curvy” rather than straight or flat. We transform the data, using a kernel, in an attempt to at least approximate some of that curviness.</li>
<li class="noindent4"><strong class="calibre5">−</strong>     We can, to various degrees set by the user, allow data points to reside within the margin.</li>
</ul></li>
</ul>
<p class="indent">As noted earlier, SVM is a more complicated tool than the ones we’ve seen earlier. But it is in wide usage, with many successes to its name, so the extra effort in this chapter is quite worthwhile.</p>
</div></body></html>