<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch01"><span epub:type="pagebreak" id="page_3"/><strong><span class="big">1</span><br/>EMBEDDINGS, LATENT SPACE, AND REPRESENTATIONS</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="noindent">In deep learning, we often use the terms <em>embedding vectors</em>, <em>representations</em>, and <em>latent space</em>. What do these concepts have in common, and how do they differ?</p>&#13;
<p class="indent">While these three terms are often used interchangeably, we can make subtle distinctions between them:</p>&#13;
<ul>&#13;
<li class="noindent">Embedding vectors are representations of input data where similar items are close to each other.</li>&#13;
<li class="noindent">Latent vectors are intermediate representations of input data.</li>&#13;
<li class="noindent">Representations are encoded versions of the original input.</li>&#13;
</ul>&#13;
<p class="indent">The following sections explore the relationship between embeddings, latent vectors, and representations and how each functions to encode information in machine learning contexts.</p>&#13;
<h3 class="h3" id="ch00lev5"><strong>Embeddings</strong></h3>&#13;
<p class="noindent">Embedding vectors, or <em>embeddings</em> for short, encode relatively high-dimensional data into relatively low-dimensional vectors.<span epub:type="pagebreak" id="page_4"/></p>&#13;
<p class="indent">We can apply embedding methods to create a continuous dense (non-sparse) vector from a (sparse) one-hot encoding. <em>One-hot encoding</em> is a method used to represent categorical data as binary vectors, where each category is mapped to a vector containing 1 in the position corresponding to the category’s index, and 0 in all other positions. This ensures that the categorical values are represented in a way that certain machine learning algorithms can process. For example, if we have a categorical variable Color with three categories, Red, Green, and Blue, the one-hot encoding would represent Red as [1, 0, 0], Green as [0, 1, 0], and Blue as [0, 0, 1]. These one-hot encoded categorical variables can then be mapped into continuous embedding vectors by utilizing the learned weight matrix of an embedding layer or module.</p>&#13;
<p class="indent">We can also use embedding methods for dense data such as images. For example, the last layers of a convolutional neural network may yield embedding vectors, as illustrated in <a href="ch01.xhtml#ch1fig1">Figure 1-1</a>.</p>&#13;
<div class="image"><img id="ch1fig1" src="../images/01fig01.jpg" alt="Image" width="909" height="369"/></div>&#13;
<p class="figcap"><em>Figure 1-1: An input embedding (left) and an embedding from a neural network (right)</em></p>&#13;
<p class="indent">To be technically correct, all intermediate layer outputs of a neural network could yield embedding vectors. Depending on the training objective, the output layer may also produce useful embedding vectors. For the sake of simplicity, the convolutional neural network in <a href="ch01.xhtml#ch1fig1">Figure 1-1</a> associates the second-to-last layer with embeddings.</p>&#13;
<p class="indent">Embeddings can have higher or lower numbers of dimensions than the original input. For instance, using embeddings methods for extreme expression, we can encode data into two-dimensional dense and continuous representations for visualization purposes and clustering analysis, as illustrated in <a href="ch01.xhtml#ch1fig2">Figure 1-2</a>.<span epub:type="pagebreak" id="page_5"/></p>&#13;
<div class="image"><img id="ch1fig2" src="../images/01fig02.jpg" alt="Image" width="897" height="403"/></div>&#13;
<p class="figcap"><em>Figure 1-2: Mapping words (left) and images (right) to a two-dimensional feature space</em></p>&#13;
<p class="indent">A fundamental property of embeddings is that they encode <em>distance</em> or <em>similarity</em>. This means that embeddings capture the semantics of the data such that similar inputs are close in the embeddings space.</p>&#13;
<p class="indent">For readers interested in a more formal explanation using mathematical terminology, an embedding is an injective and structure-preserving map between an input space <em>X</em> and the embedding space <em>Y</em>. This implies that similar inputs will be located at points in close proximity within the embedding space, which can be seen as the “structure-preserving” characteristic of the embedding.</p>&#13;
<h3 class="h3" id="ch00lev6"><strong>Latent Space</strong></h3>&#13;
<p class="noindent"><em>Latent space</em> is typically used synonymously with <em>embedding space</em>, the space into which embedding vectors are mapped.</p>&#13;
<p class="indent">Similar items can appear close in the latent space; however, this is not a strict requirement. More loosely, we can think of the latent space as any feature space that contains features, often compressed versions of the original input features. These latent space features can be learned by a neural network, such as an autoencoder that reconstructs input images, as shown in <a href="ch01.xhtml#ch1fig3">Figure 1-3</a>.</p>&#13;
<div class="image"><img id="ch1fig3" src="../images/01fig03.jpg" alt="Image" width="597" height="178"/></div>&#13;
<p class="figcap"><em>Figure 1-3: An autoencoder reconstructing the input image</em><span epub:type="pagebreak" id="page_6"/></p>&#13;
<p class="indent">The bottleneck in <a href="ch01.xhtml#ch1fig3">Figure 1-3</a> represents a small, intermediate neural network layer that encodes or maps the input image into a lower-dimensional representation. We can think of the target space of this mapping as a latent space. The training objective of the autoencoder is to reconstruct the input image, that is, to minimize the distance between the input and output images. In order to optimize the training objective, the autoencoder may learn to place the encoded features of similar inputs (for example, pictures of cats) close to each other in the latent space, thus creating useful embedding vectors where similar inputs are close in the embedding (latent) space.</p>&#13;
<h3 class="h3" id="ch00lev7"><strong>Representation</strong></h3>&#13;
<p class="noindent">A <em>representation</em> is an encoded, typically intermediate form of an input. For instance, an embedding vector or vector in the latent space is a representation of the input, as previously discussed. However, representations can also be produced by simpler procedures. For example, one-hot encoded vectors are considered representations of an input.</p>&#13;
<p class="indent">The key idea is that the representation captures some essential features or characteristics of the original data to make it useful for further analysis or processing.</p>&#13;
<h3 class="h3" id="ch00lev8"><strong>Exercises</strong></h3>&#13;
<p class="number"><strong>1-1.</strong> Suppose we’re training a convolutional network with five convolutional layers followed by three fully connected (FC) layers, similar to AlexNet (<em><a href="https://en.wikipedia.org/wiki/AlexNet">https://en.wikipedia.org/wiki/AlexNet</a></em>), as illustrated in <a href="ch01.xhtml#ch1fig4">Figure 1-4</a>.</p>&#13;
<div class="image"><img id="ch1fig4" src="../images/01fig04.jpg" alt="Image" width="769" height="293"/></div>&#13;
<p class="figcap"><em>Figure 1-4: An illustration of AlexNet</em></p>&#13;
<p class="indent">We can think of these fully connected layers as two hidden layers and an output layer in a multilayer perceptron. Which of the neural network layers can be utilized to produce useful embeddings? Interested readers can find more details about the AlexNet architecture and implementation in the original publication by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.</p>&#13;
<p class="number"><strong>1-2.</strong> Name some types of input representations that are not embeddings.<span epub:type="pagebreak" id="page_7"/></p>&#13;
<h3 class="h3" id="ch00lev9"><strong>References</strong></h3>&#13;
<ul>&#13;
<li class="noindent">The original paper describing the AlexNet architecture and implementation: Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, “ImageNet Classification with Deep Convolutional Neural Networks” (2012), <em><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks</a></em>.<span epub:type="pagebreak" id="page_8"/></li>&#13;
</ul>&#13;
</div>
</div>
</body></html>