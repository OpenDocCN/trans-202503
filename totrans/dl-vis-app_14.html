<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="297" id="Page_297"/>12</span><br/>
<span class="ChapterTitle">Ensembles</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">Anyone can make mistakes, even algorithms. Sometimes we might be pretty sure that our algorithms are giving us good answers, but for any number of reasons, we might harbor a bit of doubt. How can we increase our confidence in what the computer tells us?</p>
<p>This isn’t a new problem. The Apollo spacecraft of the 1960s and ’70s relied on one type of computer in the command module, which orbited the moon, and a different type of computer in the lunar module, which landed there. These computers were a critical part of almost every maneuver, so it was essential that the astronauts could trust their outputs. The computers were built with integrated circuits, which were relatively new at the time. The astronauts trusted their lives to their software and hardware, but there was always room for doubt. How could they guard against errors or malfunctions that could end the mission or even prove fatal?</p>
<p>The designers of these computers addressed that problem with redundancy: every circuit board was duplicated, not once, but twice, producing three copies in all. All three systems always ran in synchrony, a technique <span epub:type="pagebreak" title="298" id="Page_298"/>called <em>triple modular redundancy</em>. The computers took the same inputs and computed their own independent results. The output of the group was decided by majority vote (Ceruzzi 2015). That way, if any one of the three systems got damaged, the right answer would still emerge.</p>
<p>We can adopt and expand on this idea in machine learning. Like the Apollo engineers, we can make multiple learners and use them all simultaneously. In machine learning, groups of similar learners are called <em>ensembles</em>. And like the Apollo computers, the output of the ensemble is the most popular result from its members. But unlike Apollo’s identical software and hardware, we make each learner unique, usually by training it on slightly different data. This makes it unlikely that a mistake made by one learner will be made in exactly the same way by the others. In this way, the majority vote helps us weed out bad decisions.</p>
<p>In Chapter 11, we saw that decision trees easily overfit their training data, which can lead to errors when the system is deployed. In this chapter, we’ll see how to combine many such trees into an ensemble. The result is an algorithm that enjoys the simplicity and transparency of decision trees but greatly reduces their problems. Let’s begin with a brief discussion of how ensembles determine their final results.</p>
<h2 id="h1-500723c12-0001">Voting</h2>
<p class="BodyFirst">Making decisions is hard for computers and humans alike. In some human societies, we deal with individual imperfections in decision-making by aggregating the opinions of many people. Laws are passed by senates, financial decisions are made by boards, and individual leaders are elected by popular vote. The thinking in all of these cases is that we can avoid errors in judgment that are unique to a single individual if we instead use the consensus of multiple, independent voters. Although this doesn’t guarantee good decisions, it can sometimes help avoid problems caused by any one person’s idiosyncrasies, biases, or bad judgment.</p>
<p>Machines have biases, too. When we use learning algorithms to make decisions, their predictions are based on the data that they trained on. If that data contained biases, omissions, underrepresentations, overrepresentations, or any other kind of systemic error, those errors are baked into the learner as well. This can have profound implications in the real world. For instance, when we use machine learning to evaluate loans for homes or businesses, to determine admissions to colleges, or to prescreen job applicants, any unfairness or bias in our training data causes similar unfairness and bias in the decisions the system makes, and bad decisions from the past are repeated in the present and propagated into the future. </p>
<p>One way to reduce the effects of these problems is to create multiple learners trained with different datasets. For example, we might train each system with a different training set from a different source. Since such data is often hard to come by in practice, often we train with different subsets drawn from a common pool of training data instead.</p>
<p><span epub:type="pagebreak" title="299" id="Page_299"/>When we have trained a bunch of learners on these different datasets, we usually ask each one to evaluate each new input. Then we let the learners vote to determine a final result.</p>
<p>The typical way to do this is to use <em>plurality voting</em> (RangeVoting.org 2020). Put simply, each learner casts a single vote for its prediction, and whatever prediction receives the most votes is the winner (if there’s a tie, the computer can either randomly select one of the tied entries, or try another round of voting). Though plurality voting is not perfect, and there exist useful alternatives, it is simple, fast, and usually produces acceptable results in machine learning (NCSL 2020).</p>
<p>A popular variation of plurality voting is <em>weighted plurality voting</em>. Here, every vote gets a certain <em>weight</em>, which is just a number that tells us how much influence that vote has on the result. Another variation is to ask each voter to identify their <em>confidence</em> in their decision, so more confident voters can have more impact than those that are less sure.</p>
<p>With those terms in place, let’s now dig into making an ensemble of decision trees.</p>
<h2 id="h1-500723c12-0002">Ensembles of Decision Trees</h2>
<p class="BodyFirst">A great way to build on the strengths of decision trees, while reducing their drawbacks, is to combine them into ensembles. To keep the following discussion specific, let’s focus on using decision trees for classification.</p>
<p>Let’s look at three popular techniques for building decision tree ensembles that can significantly outperform their individual components. </p>
<h3 id="h2-500723c12-0001">Bagging</h3>
<p class="BodyFirst">The ensemble technique called <em>bagging</em> is a portmanteau of <em>bootstrap aggregating</em>. As the name suggests, this technique is based on the bootstrap idea we saw in Chapter 2. There, we saw how to use bootstrapping to estimate the quality of some statistical measure by evaluating lots of small subsets drawn from the starting data. In this case, let’s again create many small sets built from a training set, but now let’s use them to train a collection of decision trees.</p>
<p>Starting with our original training set of samples, we can build multiple new sets, or <em>bootstraps</em>, by picking items from the original, using sampling with replacement. This means that it’s possible to pick the same sample more than once. <a href="#figure12-1" id="figureanchor12-1">Figure 12-1</a> shows the idea. Remember that each sample comes with its assigned class (shown by color) so we can train with it.</p>
<p>In the center of <a href="#figure12-1">Figure 12-1</a>, we have a starting set of eight samples, belonging to five classes. By selecting samples from this set, we can make many new sets, in this case of four samples each. This is the first step of bagging. Since we’re sampling with replacement, it’s possible that any given sample might appear multiple times.</p>
<p>Now let’s create a decision tree for every bootstrap and train it on that data. We call the collection of those trees an <em>ensemble</em>.</p>
<span epub:type="pagebreak" title="300" id="Page_300"/><figure>
<img src="Images/f12001.png" alt="f12001" width="430" height="299"/>
<figcaption><p><a id="figure12-1">Figure 12-1</a>: Creating three bootstraps (top and bottom) from a set of samples (center)</p></figcaption>
</figure>
<p>When training is done and we’re evaluating a new sample, we give it to all the decision trees in the ensemble. Each tree produces one class prediction. We treat the predicted classes as votes in a plurality election, producing either a winner or a tie. Suppose that we have a small ensemble with just five trees. <a href="#figure12-2" id="figureanchor12-2">Figure 12-2</a> shows the process of evaluating a new sample after deployment, assigning it to one of four lettered classes.</p>
<figure>
<img src="Images/f12002.png" alt="f12002" width="659" height="217"/>
<figcaption><p><a id="figure12-2">Figure 12-2</a>: Using an ensemble to predict the class of a sample</p></figcaption>
</figure>
<p>Consider the left side of the figure. At the top, a new sample of unknown class arrives at our ensemble. The sample is given to every one of our decision trees (shown as triangles), and each one produces a predicted class, labeled A through D. Because each tree was trained on a different bootstrap, it’s a little different from all the others. On the right side of the figure, we run a plurality voting election with those predicted classes. In this example, the most popular class is B. That class wins, and is therefore the output of the ensemble.</p>
<p>We only need to specify two parameters to create this ensemble: how many samples we should use in each bootstrap, and how many trees we want to build. Analysis shows that adding more classifiers makes the ensemble’s predictions better, but at some point, adding more classifiers just makes it slower and the results stop improving. This is called <em>the law of diminishing returns in ensemble construction</em>. A good rule of thumb is to use about the same number of classifiers as classes of data (Bonab 2016), <span epub:type="pagebreak" title="301" id="Page_301"/>though we can use cross-validation to search for the best number of trees for any given dataset.</p>
<p>Before we leave bagging, let’s consider a couple of techniques that build on the basic idea. The central idea of each is to add extra randomization to our trees during training. </p>
<h3 id="h2-500723c12-0002">Random Forests</h3>
<p class="BodyFirst">As we saw in Chapter 11, when it’s time to split a decision tree’s node in two, we can choose any feature (or set of features) to create the test that directs elements into one child or the other. If we choose to split based on just one feature, then we need to choose which feature we want to use and what value of that feature to test for. To compare different tests, we can use the measurements we saw in Chapter 11, such as information gain or the Gini impurity. </p>
<p>When building a decision tree, we often look for the best test by considering every feature. But we can also use a technique called <em>feature bagging.</em> Before looking for the best test at a node, we first choose a random subset of the features of the samples at that node, using selection without replacement. Now we’re ready to look for the best test, based only on those features. We don’t even consider splits based on the features we’re ignoring.</p>
<p>Later, when we decide to split another node, we again choose a brand-new subset of features and again determine our new split using only those. The idea is shown in <a href="#figure12-3" id="figureanchor12-3">Figure 12-3</a>.</p>
<figure>
<img src="Images/F12003.png" alt="F12003" width="450" height="348"/>
<figcaption><p><a id="figure12-3">Figure 12-3</a>: Determining which feature (f1 to f5) to use when splitting a node by feature bagging, shown for two nodes with five samples each</p></figcaption>
</figure>
<p>On the left of <a href="#figure12-3">Figure 12-3</a>, we randomly select a set of three features from the five available, and search those for the best feature to split on. On the right we do it again, only we pick a different random set of three features, giving us a different test. The thinking here is that by randomly choosing only a few of the features, we can avoid making the same choice for this node in every tree we train, and thus we can increase the diversity of our decisions.</p>
<p><span epub:type="pagebreak" title="302" id="Page_302"/>When we build ensembles this way, we call the result a <em>random forest</em>. The <em>random</em> part of the name refers to our random choice of features at each node, and the word <em>forest</em> refers to the resulting collection of decision trees.</p>
<p>To create a random forest, we need to provide the same two parameters that we used for bagging: the size of each bootstrap and the number of trees to build. We also have to specify what fraction of the features to consider at each node. We can express this as a percentage of the number of features in the node. Alternatively, many libraries offer a variety of algorithms that pick that percentage for us.</p>
<h3 id="h2-500723c12-0003">Extra Trees</h3>
<p class="BodyFirst">Let’s look at a second way to randomize our construction of trees when building an ensemble. Normally when we split a node, we consider each feature it contains (or a random subset of them if we’re building a random forest), and we find the value of that feature that best splits the samples at that node into two children. As we mentioned, we compare different possible tests using a measure like information gain.</p>
<p>Instead of finding the best splitting point for each feature, let’s choose the splitting point randomly, based on the values that are in the node. The result of this change is an ensemble called <em>Extremely Randomized Trees</em>, or <em>Extra Trees</em>.</p>
<p>Although it may seem that this is destined to give us worse results for that tree, remember that decision trees are prone to overfitting. This random choice of the splitting point lets us trade off a little accuracy for reduced overfitting.</p>
<h2 id="h1-500723c12-0003">Boosting</h2>
<p class="BodyFirst">The techniques we just finished looking at are all specific to decision trees. Now let’s look at another method for building ensembles that is applicable to any kind of learner. This method is called <em>boosting </em>(Schapire 2012).</p>
<p>Boosting is a popular algorithm because it lets us combine a large number of small, fast, and inaccurate learners into a single accurate learner. To keep things concrete, let’s continue using decision trees as our example learners. We can make the discussion even simpler by focusing on binary classifiers, which assign every sample to one of only two classes. We’re going to build our ensemble out of simple classifiers that are just barely useful. To get going, let’s start with a thought experiment involving a completely useless classifier and then improve it just a little.</p>
<p>Imagine a dataset where the samples come from two classes. Also imagine a completely random binary classifier. Regardless of a sample’s features, the classifier assigns the sample to one of these two classes arbitrarily. If the samples are evenly split in the training set, we have a 50:50 chance of any sample being correctly labeled. We call this <em>random labeling</em>, because the odds of getting the right answer are up to chance.</p>
<p>Now suppose that we can tweak our binary classifier so that it does just barely better than chance. For example, <a href="#figure12-4" id="figureanchor12-4">Figure 12-4</a> shows a set of data of <span epub:type="pagebreak" title="303" id="Page_303"/>two classes, a binary classifier that is no better than chance, and a binary classifier that is just a tiny bit better than chance.</p>
<p>The learner in <a href="#figure12-4">Figure 12-4</a>(b) is no better than chance, with half of each class getting incorrectly classified. This is a useless classifier. The learner in part (c) is just slightly less useless than the classifier in part (b) because the slight tilt in the boundary line means it does just a little better than the useless classifier.</p>
<figure>
<img src="Images/F12004.png" alt="F12004" width="780" height="181"/>
<figcaption><p><a id="figure12-4">Figure 12-4</a>: (a) Our training data. (b) A random classifier. (c) A terrible, but not quite random, classifier.</p></figcaption>
</figure>
<p>We call the classifier in <a href="#figure12-4">Figure 12-4</a>(c) a <em>weak learner</em>. In this situation, a weak learner is any classifier that is even the slightest bit accurate. That is, it assigns the correct class more than 50 percent of the time, but perhaps only barely. The beauty of boosting is that we can use this weak learner as part of an ensemble that produces great results. </p>
<p>In fact, a weak binary learner is just as useful to us even if it does <em>worse </em>than chance. That’s because we have only two classes. If a classifier is below chance (that is, it assigns the wrong class more frequently than the right one), then we can just swap the output classes, and then it’s doing better than chance, rather than worse. The conclusion is that as long as a binary learner isn’t completely random, we are able to use it.</p>
<p>Weak classifiers are easy to make. Perhaps the most commonly used weak classifier is a decision tree that’s only one test deep. That is, the whole tree is made up of just a root node and its two children. This ridiculously small decision tree is often called a <em>decision stump</em>. Because it almost always does a better job than randomly assigning a class to each sample, it’s a fine example of a weak classifier. It’s small, fast, and a little better than random.</p>
<p>In contrast to a weak learner, a <em>strong learner</em> is a classifier that gets the correct label most of the time. The stronger the learner, the better its percentage of being right. </p>
<p>The idea behind boosting is to combine multiple weak classifiers into an ensemble that acts like a strong classifier. Note that our weakness condition is just a minimum threshold. We can combine lots of strong classifiers if we want to, though using weak ones is more common because they’re usually faster.</p>
<p>Let’s see how boosting works with an example. <a href="#figure12-5" id="figureanchor12-5">Figure 12-5</a> shows a training set of samples that belong to two different classes.</p>
<p>What might be some good classifiers for this data? A fast and easy classifier just draws a straight line through the 2D dataset. We can see that no straight line is going to split up this data because the circular samples surround the square samples on three sides.</p>
<span epub:type="pagebreak" title="304" id="Page_304"/><figure>
<img src="Images/F12005.png" alt="F12005" width="239" height="239"/>
<figcaption><p><a id="figure12-5">Figure 12-5</a>: A collection of samples we’re going to classify using boosting</p></figcaption>
</figure>
<p>Even though no single straight line can separate this data, we will see that multiple straight lines can, so let’s use straight lines as our weak classifiers. <a href="#figure12-6" id="figureanchor12-6">Figure 12-6</a> shows the boundary line for one such classifier. We’ll use A for the name of both the classifier and the boundary line it defines.</p>
<figure>
<img src="Images/F12006.png" alt="F12006" width="243" height="359"/>
<figcaption><p><a id="figure12-6">Figure 12-6</a>: Placing one line called A into our samples cuts the two big clusters apart. </p></figcaption>
</figure>
<p>In this classifier, everything on the side of A pointed to by the arrow is classified as square, and everything on the other side is classified as a circle. Using our measure of accuracy from Chapter 3, we find that the accuracy of this learner is given by (TP + TN) / (TP + TN + FP + FN) = (12 + 8) / (12 + 8 + 12 + 2) = 20 / 34, or about 59 percent. That’s a nice example of a weak learner: it’s better than chance (50 percent), but not a lot better.</p>
<p>To use boosting, we’ll want to add more lines (that is, additional weak learners) so that ultimately every region formed by the lines contains samples of a single class. After adding two more of these straight-line classifiers, we end up with <a href="#figure12-7" id="figureanchor12-7">Figure 12-7</a>.</p>
<p><span epub:type="pagebreak" title="305" id="Page_305"/>Line B has an accuracy of about 73 percent. C is terrible, with an accuracy of only about 12 percent. But as we noted before, that’s fine, because if we just swap the labels that C assigns (that is, just point the arrow in the other direction) it has an accuracy of about 88 percent!</p>
<p>The three lines, or boundaries, in <a href="#figure12-7">Figure 12-7</a> together create seven nonoverlapping regions. The figure also shows that each region contains only one class of samples. By looking at these regions, we can see a way to determine the class of a sample using just the outputs of the three classifiers.</p>
<figure>
<img src="Images/F12007.png" alt="F12007" width="375" height="376"/>
<figcaption><p><a id="figure12-7">Figure 12-7</a>: Two more lines added to <a href="#figure12-6">Figure 12-6</a>    </p></figcaption>
</figure>
<p>Let’s draw our three boundaries together. We will label each region with the learners that point toward it. The result is shown in <a href="#figure12-8" id="figureanchor12-8">Figure 12-8</a>.</p>
<figure>
<img src="Images/F12008.png" alt="F12008" width="694" height="344"/>
<figcaption><p><a id="figure12-8">Figure 12-8</a>: On the left, we show three lines named A, B, and C. On the right, each region is marked with the names of the learners that put that region on the positive side of their respective lines.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="306" id="Page_306"/>When our classifiers get a new sample, normally they each return a class. Instead, let’s set them up to return a 1 if the sample is on the positive side of that classifier’s boundary (that is, the side pointed to by the arrow in <a href="#figure12-8">Figure 12-8</a>), and 0 otherwise.</p>
<p>Now we can add up all the contributions from all three learners in each cell.</p>
<p>For example, consider the region at the top center, marked C in <a href="#figure12-8">Figure 12-8</a>. It’s on the positive side of learner C, earning it a score of 1. It’s on the negative side of both A and B, each of which thus contribute 0, so the sum of the three outputs is 1. The region at the bottom, marked AB, gets 1 from learners A and B, and 0 from C, giving it a total of 2. These scores are shown along with the other regions in <a href="#figure12-9" id="figureanchor12-9">Figure 12-9</a>.</p>
<figure>
<img src="Images/F12009.png" alt="F12009" width="375" height="372"/>
<figcaption><p><a id="figure12-9">Figure 12-9</a>: The composite score for each of the seven regions. Each letter in <a href="#figure12-8">Figure 12-8</a> earns that region a 1.</p></figcaption>
</figure>
<p>We’ve almost created our new classifier. There are two steps to go. First, we replace the 1 we arbitrarily assigned to each classifier’s output with a more useful value. Second, we find a threshold that turns the summed value in each region into a class.</p>
<p>Recall our discussion of weighted plurality voting from earlier in the chapter. If a region is on the positive side of a line, then that line is voting for that region. Rather than simply adding 1 from every classifier, we can assign each classifier its own voting weight. For example, if classifiers A, B, and C have voting weights 2, 3, and –4, and a point is on the positive side of A and C but not B, then A contributes 2, B contributes 0 (since the point is on the negative side of line B), and C contributes –4, for a total of 2 + 0 + –4 = –2.</p>
<p>The voting weight for each classifier is found for us by the boosting algorithm. Rather than going into those mechanics, let’s visualize the results of a specific set of weights for this dataset so we can see their effect. </p>
<p>In <a href="#figure12-10" id="figureanchor12-10">Figure 12-10</a> we show the regions that are affected by the score for each learner. A dark region gets that learner’s value, while a light region <span epub:type="pagebreak" title="307" id="Page_307"/>does not (so the learner’s value in light regions is 0). Here we use the weights 1.0, 1.5, and −2 for A, B, and C, respectively. Recall that line C was pointing in the “wrong” direction. Giving a negative value to C’s weight has the effect of reversing the decisions from classifier C.</p>
<figure>
<img src="Images/F12010.png" alt="F12010" width="692" height="235"/>
<figcaption><p><a id="figure12-10">Figure 12-10</a>: We assign a numerical value to each region that is classified as positive by each learner. The dark regions for each line get the weight associated with that line.</p></figcaption>
</figure>
<p>The sums of all of these scores are shown in <a href="#figure12-11" id="figureanchor12-11">Figure 12-11</a>. Blue regions have positive sums, and red regions have negative sums. These exactly correspond to where the circles and squares fell in our dataset. Any sample in a positive region is a square, and any sample in a negative region is a circle. </p>
<figure>
<img src="Images/F12011.png" alt="F12011" width="238" height="273"/>
<figcaption><p><a id="figure12-11">Figure 12-11</a>: Adding up the scores from <a href="#figure12-10">Figure 12-10</a></p></figcaption>
</figure>
<p>When a sample arrives, we send it to each classifier (that is, we test it against its corresponding line). For each classifier that finds the sample to be on the positive side of its line, we contribute its voting weight to a running sum. After adding up all the classifier outputs, we determine if the sample is positive or negative, which tells us which class the sample belongs to. We’ve correctly classified our data!</p>
<p>Let’s look at another example. <a href="#figure12-12" id="figureanchor12-12">Figure 12-12</a> shows a new set of data.</p>
<span epub:type="pagebreak" title="308" id="Page_308"/><figure>
<img src="Images/F12012.png" alt="F12012" width="242" height="242"/>
<figcaption><p><a id="figure12-12">Figure 12-12</a>: A set of data we’d like to classify using boosting</p></figcaption>
</figure>
<p>For this data, let’s try using four learners. <a href="#figure12-13" id="figureanchor12-13">Figure 12-13</a> shows the four weak learners that a boosting algorithm might find in order to partition this data.</p>
<figure>
<img src="Images/F12013.png" alt="F12013" width="294" height="302"/>
<figcaption><p><a id="figure12-13">Figure 12-13</a>: Four lines that let us classify the data of <a href="#figure12-12">Figure 12-12</a></p></figcaption>
</figure>
<p>As before, the algorithm also assigns weights to these learners. Let’s illustrate the results using weights −8, 2, 3, and 4 for learners A, B, C, and D respectively. <a href="#figure12-14" id="figureanchor12-14">Figure 12-14</a> shows which regions have those weights added to their overall score. Light-colored regions implicitly receive a value of 0. </p>
<figure>
<img src="Images/F12014.png" alt="F12014" width="844" height="215"/>
<figcaption><p><a id="figure12-14">Figure 12-14</a>: The regions corresponding to each learner</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="309" id="Page_309"/><a href="#figure12-15" id="figureanchor12-15">Figure 12-15</a> shows the sums of the contributions for each region. Again, a positive or negative sum distinguishes the two types of regions. We’ve found a way to combine four weak learners to correctly classify the points in <a href="#figure12-13">Figure 12-13</a>.</p>
<p>The beauty of boosting is that it takes classifiers that are simple and fast, but lousy, and by finding weights for them, it turns the ensemble into a single great classifier. </p>
<figure>
<img src="Images/F12015.png" alt="F12015" width="288" height="283"/>
<figcaption><p><a id="figure12-15">Figure 12-15</a>: The sums of the scores of each region from <a href="#figure12-14">Figure 12-14</a>. Positive regions are shown in blue, and they correctly classify the points in <a href="#figure12-13">Figure 12-13</a>.</p></figcaption>
</figure>
<p>The only hyperparameter we need to provide is how many classifiers we want. In boosting, as in bagging, a good rule of thumb is to start with about as many classifiers as there are classes (Bonab 2016). That means that our earlier examples started on the high side, since we used three or four classifiers for only two classes. But as in so many things in machine learning, the best value is found by trial and error.</p>
<p>Boosting made its first appearance as part of an algorithm called <em>Adaboost</em> (Freund 1997; Schapire 2013). Although it can work with any learning algorithm, boosting has been particularly popular with decision trees. In fact, it works very well with the decision stumps we mentioned previously (these are trees that have only a root node and its two immediate children). We can think of the lines we used in Figures 12-7 and 12-13 as decision stumps since they have just one test: Is a sample on the side of the line pointed to by the arrow, or is it not? </p>
<p>It’s worth noting that boosting is not a sure-fire way to improve all classification algorithms. The theory of boosting only covers binary classification, as in our earlier examples (Fumera 2008; Kak 2016). This is partly why boosting has been so popular and successful with decision tree classifiers.</p>
<h2 id="h1-500723c12-0004">Summary</h2>
<p class="BodyFirst">Ensembles are collections of diversified learners. The general idea is that we gather up multiple learners of similar type, but trained on different data, <span epub:type="pagebreak" title="310" id="Page_310"/>and let them all evaluate an input. We then let them vote for the class each one determines, and the winner of the most votes is reported as the class for that input. The thinking is that any errors in the individual learners are essentially voted away by the class agreed upon by the others.</p>
<p>Boosting is a way of using many weak learners as an ensemble to perform like a strong learner.</p>
<p>This discussion wraps up our discussion of machine learning techniques. Starting in Chapter 13, we look at the neural networks that power deep learning algorithms. We’ll see that the methods we introduced here are helpful in deep learning, because they help us understand our data and make the best choices of algorithms and networks to work with that data, and produce results that are useful to us. </p>
</section>
</div></body></html>