<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch08"><span epub:type="pagebreak" id="page_43"/><strong><span class="big">8</span><br/>THE SUCCESS OF TRANSFORMERS</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="noindent">What are the main factors that have contributed to the success of transformers?</p>&#13;
<p class="indent">In recent years, transformers have emerged as the most successful neural network architecture, particularly for various natural language processing tasks. In fact, transformers are now on the cusp of becoming state of the art for computer vision tasks as well. The success of transformers can be attributed to several key factors, including their attention mechanisms, ability to be parallelized easily, unsupervised pretraining, and high parameter counts.</p>&#13;
<h3 class="h3" id="ch00lev34"><strong>The Attention Mechanism</strong></h3>&#13;
<p class="noindent">The self-attention mechanism found in transformers is one of the key design components that make transformer-based LLMs so successful. However, transformers are not the first architecture to utilize attention mechanisms.</p>&#13;
<p class="indent">Attention mechanisms were first developed in the context of image recognition back in 2010, before being adopted to aid the translation of long sentences in recurrent neural networks. (<a href="ch16.xhtml">Chapter 16</a> compares the attention mechanisms found in recurrent neural networks and transformers in greater detail.)</p>&#13;
<p class="indent">The aforementioned attention mechanism is inspired by human vision, focusing on specific parts of an image (foveal glimpses) at a time to process <span epub:type="pagebreak" id="page_44"/>information hierarchically and sequentially. In contrast, the fundamental mechanism underlying transformers is a self-attention mechanism used for sequence-to-sequence tasks, such as machine translation and text generation. It allows each token in a sequence to attend to all other tokens, thus providing context-aware representations of each token.</p>&#13;
<p class="indent">What makes attention mechanisms so unique and useful? For the following illustration, suppose we are using an encoder network on a fixed-length representation of the input sequence or image—this can be a fully connected, convolutional, or attention-based encoder.</p>&#13;
<p class="indent">In a transformer, the encoder uses self-attention mechanisms to compute the importance of each input token relative to other tokens in the sequence, allowing the model to focus on relevant parts of the input sequence. Conceptually, attention mechanisms allow the transformers to attend to different parts of a sequence or image. On the surface, this sounds very similar to a fully connected layer where each input element is connected via a weight with the input element in the next layer. In attention mechanisms, the computation of the attention weights involves comparing each input element to all others. The attention weights obtained by this approach are dynamic and input dependent. In contrast, the weights of a convolutional or fully connected layer are fixed after training, as illustrated in <a href="ch08.xhtml#ch8fig1">Figure 8-1</a>.</p>&#13;
<div class="image"><img id="ch8fig1" src="../images/08fig01.jpg" alt="Image" width="848" height="511"/></div>&#13;
<p class="figcap"><em>Figure 8-1: The conceptual difference between model weights in fully connected layers (top) and attention scores (bottom)</em></p>&#13;
<p class="indent">As the top part of <a href="ch08.xhtml#ch8fig1">Figure 8-1</a> shows, once trained, the weights of fully connected layers remain fixed regardless of the input. In contrast, as shown at the bottom, self-attention weights change depending on the inputs, even after a transformer is trained.</p>&#13;
<p class="indent">Attention mechanisms allow a neural network to selectively weigh the importance of different input features, so the model can focus on the most <span epub:type="pagebreak" id="page_45"/>relevant parts of the input for a given task. This provides a contextual understanding of each word or image token, allowing for more nuanced interpretations, which is one of the aspects that can make transformers work so well.</p>&#13;
<h3 class="h3" id="ch00lev35"><strong>Pretraining via Self-Supervised Learning</strong></h3>&#13;
<p class="noindent">Pretraining transformers via self-supervised learning on large, unlabeled datasets is another key factor in the success of transformers. During pre-training, the transformer model is trained to predict missing words in a sentence or the next sentence in a document, for example. By learning to predict these missing words or the next sentence, the model is forced to learn general representations of language that can be fine-tuned for a wide range of downstream tasks.</p>&#13;
<p class="indent">While unsupervised pretraining has been highly effective for natural language processing tasks, its effectiveness for computer vision tasks is still an active area of research. (Refer to <a href="ch02.xhtml">Chapter 2</a> for a more detailed discussion of self-supervised learning.)</p>&#13;
<h3 class="h3" id="ch00lev36"><strong>Large Numbers of Parameters</strong></h3>&#13;
<p class="noindent">One noteworthy characteristic of transformers is their large model sizes. For example, the popular 2020 GPT-3 model consists of 175 billion trainable parameters, while other transformers, such as switch transformers, have trillions of parameters.</p>&#13;
<p class="indent">The scale and number of trainable parameters of transformers are essential factors in their modeling performance, particularly for large-scale natural language processing tasks. For instance, linear scaling laws suggest that the training loss decreases proportionally with an increase in model size, so a doubling of the model size can halve the training loss.</p>&#13;
<p class="indent">This, in turn, can lead to better performance on the downstream target task. However, it is essential to scale the model size and the number of training tokens equally. This means the number of training tokens should be doubled for every doubling of model size.</p>&#13;
<p class="indent">Since labeled data is limited, utilizing large amounts of data during un-supervised pretraining is vital.</p>&#13;
<p class="indent">To summarize, large model sizes and large datasets are critical factors in transformers’ success. Additionally, using self-supervised learning, the ability to pretrain transformers is closely tied to using large model sizes and large datasets. This combination has been critical in enabling the success of transformers in a wide range of natural language processing tasks.</p>&#13;
<h3 class="h3" id="ch00lev37"><strong>Easy Parallelization</strong></h3>&#13;
<p class="noindent">Training large models on large datasets requires vast computational resources, and it’s key that the computations can be parallelized to utilize these resources.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_46"/>Fortunately, transformers are easy to parallelize since they take a fixed-length sequence of word or image tokens as input. For instance, the self-attention mechanism used in most transformer architectures involves computing the weighted sum between a pair of input elements. Furthermore, these pair-wise token comparisons can be computed independently, as illustrated in <a href="ch08.xhtml#ch8fig2">Figure 8-2</a>, making the self-attention mechanism relatively easy to parallelize across different GPU cores.</p>&#13;
<div class="image"><img id="ch8fig2" src="../images/08fig02.jpg" alt="Image" width="653" height="336"/></div>&#13;
<p class="figcap"><em>Figure 8-2: A simplified self-attention mechanism without weight parameters</em></p>&#13;
<p class="indent">In addition, the individual weight matrices used in the self-attention mechanism (not shown in <a href="ch08.xhtml#ch8fig2">Figure 8-2</a>) can be distributed across different machines for distributed and parallel computing.</p>&#13;
<h3 class="h3" id="ch00lev38"><strong>Exercises</strong></h3>&#13;
<p class="number"><strong>8-1.</strong> As discussed in this chapter, self-attention is easily parallelizable, yet transformers are considered computationally expensive due to self-attention. How can we explain this contradiction?</p>&#13;
<p class="number"><strong>8-2.</strong> Since self-attention scores represent importance weights for the various input elements, can we consider self-attention to be a form of feature selection?<span epub:type="pagebreak" id="page_47"/></p>&#13;
<h3 class="h3" id="ch00lev39"><strong>References</strong></h3>&#13;
<ul>&#13;
<li class="noindent">An example of an attention mechanism in the context of image recognition: Hugo Larochelle and Geoffrey Hinton, “Learning to Combine Foveal Glimpses with a Third-Order Boltzmann Machine” (2010), <em><a href="https://dl.acm.org/doi/10.5555/2997189.2997328">https://dl.acm.org/doi/10.5555/2997189.2997328</a></em>.</li>&#13;
<li class="noindent">The paper introducing the self-attention mechanism with the original transformer architecture: Ashish Vaswani et al., “Attention Is All You Need” (2017), <em><a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></em>.</li>&#13;
<li class="noindent">Transformers can have trillions of parameters: William Fedus, Barret Zoph, and Noam Shazeer, “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity” (2021), <em><a href="https://arxiv.org/abs/2101.03961">https://arxiv.org/abs/2101.03961</a></em>.</li>&#13;
<li class="noindent">Linear scaling laws suggest that training loss decreases proportionally with an increase in model size: Jared Kaplan et al., “Scaling Laws for Neural Language Models” (2020), <em><a href="https://arxiv.org/abs/2001.08361">https://arxiv.org/abs/2001.08361</a></em>.</li>&#13;
<li class="noindent">Research suggests that in transformer-based language models, the training tokens should be doubled for every doubling of model size: Jordan Hoffmann et al., “Training Compute-Optimal Large Language Models” (2022), <em><a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a></em>.</li>&#13;
<li class="noindent">For more about the weights used in self-attention and cross-attention mechanisms, check out my blog post: “Understanding and Coding the Self-Attention Mechanism of Large Language Models from Scratch” at <em><a href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html">https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html</a></em>.<span epub:type="pagebreak" id="page_48"/></li>&#13;
</ul>&#13;
</div>
</div>
</body></html>