- en: '**14'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MASS STORAGE DEVICES AND FILESYSTEMS**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/comm1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The most prevalent I/O device on modern computers is probably the mass storage
    device. Whereas some PCs don’t have a display (they’re operated *headlessly*),
    or even a keyboard or mouse (they’re accessed remotely), almost every computer
    system recognizable as a PC has a mass storage device of some sort. This chapter
    will focus on the types of mass storage devices—hard drives, floppy disks, tape
    drives, flash drives, solid state drives, and more—as well as the special filesystem
    format they use to organize the data they store.
  prefs: []
  type: TYPE_NORMAL
- en: '**14.1 Disk Drives**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Almost all modern computer systems include some sort of disk drive unit to provide
    online mass storage. At one time, certain workstation vendors produced *diskless
    workstations*, but the relentless drop in price and increasing storage space of
    fixed (aka “hard”) disk and solid-state drive (SSD) units have all but obliterated
    the diskless computer system. Disk drives are so ubiquitous in modern systems
    that most people take them for granted. However, it’s dangerous for a programmer
    to take a disk drive for granted. Software constantly interacts with the disk
    drive as a medium for application file storage, so it’s very important to understand
    how disk drives operate if you want to write efficient code.
  prefs: []
  type: TYPE_NORMAL
- en: '***14.1.1 Floppy Disk Drives***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Floppy disks have all but disappeared from today’s PCs. Their limited storage
    capacity (typically 1.44MB) is far too small for modern applications and the data
    they produce. It’s hard to believe that at the beginning of the PC revolution
    a 143KB (that’s *kilo*bytes, not megabytes or gigabytes) floppy drive was considered
    a high-ticket item. However, floppy disk drives have failed to keep up with technological
    advances in the computer industry. Therefore, we won’t consider them further in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '***14.1.2 Hard Drives***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The fixed disk drive, more commonly known as the hard drive, is the most common
    mass storage device in use today (though, as of 2020, SSDs are rapidly replacing
    hard drives). The modern hard drive is truly an engineering marvel. Between 1982
    and 2020, the capacity of a single drive unit has increased over 2,400,000-fold,
    from 5MB to over 16TB (terabytes). At the same time, the minimum price for a new
    unit has dropped from $2,500 (US) to below $50\. No other component in the computer
    system has enjoyed such a radical increase in capacity and performance along with
    a comparable drop in price. (Semiconductor RAM probably comes in second: paying
    the 1982 price today would get you about 40,000 times the capacity.)'
  prefs: []
  type: TYPE_NORMAL
- en: While hard drives were decreasing in price and increasing in capacity, they
    were also becoming faster. In the early 1980s, a hard-drive subsystem was doing
    well to transfer 1MBps between the drive and the CPU’s memory; modern hard drives
    can transfer more than 2,500MBps.^([1](footnotes.xhtml#fn14_1a)) While this increase
    in performance isn’t as great as that of memory or CPUs, keep in mind that disk
    drives are mechanical units on which the laws of physics place greater limitations.
    In some cases, the dropping cost of hard drives has allowed system designers to
    improve their performance by using disk arrays (see “[RAID Systems](#sec14_1_3)”
    on page [388](#sec14_1_3) for details). By using certain hard-disk subsystems
    like disk arrays, you could achieve 2500MBps (or better) transfer rates, though
    it’s not especially cheap to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Hard drives are so named because their data is stored on a small, rigid disk
    that is usually made out of aluminum or glass and is coated with a magnetic material.
    Floppy disks, in contrast, store their information on a thin piece of flexible
    Mylar plastic.
  prefs: []
  type: TYPE_NORMAL
- en: In disk-drive terminology, the small aluminum or glass disk is known as a *platter*.
    Each platter has two surfaces, front and back (or top and bottom), both of which
    have the magnetic coating. During operation, the hard-drive unit spins this platter
    at a particular speed, which these days is usually 3,600; 5,400; 7,200; 10,000;
    or 15,000 revolutions per minute (RPM). Generally, though not always, the faster
    the platter spins, the faster the data is read from the disk and the higher the
    data transfer rate between the disk and the system. The smaller disk drives in
    laptop computers typically spin at much slower speeds, like 2,000 or 4,000 RPM,
    to conserve battery life and generate less heat.
  prefs: []
  type: TYPE_NORMAL
- en: 'A hard-disk subsystem contains two main active components: the disk platter(s)
    and the read/write head. The read/write head, when held stationary, floats above
    concentric circles, or *tracks*, on the disk surface. Each track is broken up
    into a sequence of sections known as *sectors* or *blocks*. The actual number
    of sectors varies by drive design, but a typical hard drive has between 32 and
    128 sectors per track (see [Figure 14-1](ch14.xhtml#ch14fig01)). Each sector typically
    holds between 256 and 4,096 bytes of data. Many disk-drive units let the OS choose
    between several different sector sizes, the most common being 512 bytes and 4,096
    bytes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-1: Tracks and sectors on a hard-disk platter*'
  prefs: []
  type: TYPE_NORMAL
- en: The disk drive records data when the read/write head sends a series of electrical
    pulses to the platter, which translates them into magnetic pulses that the platter’s
    magnetic surface retains. The frequency at which the disk controller can record
    these pulses is limited by the quality of the electronics, the read/write head
    design, and the quality of the magnetic surface.
  prefs: []
  type: TYPE_NORMAL
- en: The magnetic medium is capable of recording two adjacent bits on its disk surface
    and then differentiating between them during a later read operation. However,
    as you record bits closer and closer together, it becomes increasingly difficult
    to differentiate between them in the magnetic domain. *Bit density* is a measure
    of how closely a particular hard disk can pack data into its tracks—the higher
    the bit density, the more data you can squeeze onto a single track. However, recovering
    densely packed data requires faster and more expensive electronics.
  prefs: []
  type: TYPE_NORMAL
- en: The bit density has a big impact on the performance of the drive. If the drive’s
    platters are rotating at a fixed number of RPM, then the higher bit density, the
    more bits will rotate underneath the read/write head over a certain duration.
    Larger disk drives tend to be faster than smaller disk drives because they employ
    a higher bit density.
  prefs: []
  type: TYPE_NORMAL
- en: By moving the disk’s read/write head in a roughly linear path from the center
    of the disk platter to the outside edge, the system can position a single read/write
    head over any one of several thousand tracks. Yet the use of only one read/write
    head means that it will take a fair amount of time to move the head among the
    disk’s many tracks. Indeed, two of the most cited hard-disk performance parameters
    are the read/write head’s average seek time and track-to-track seek time.
  prefs: []
  type: TYPE_NORMAL
- en: The *average seek time* is half the amount of time it takes to move the read/write
    head from the edge of the disk to the center, or vice versa. A typical high-performance
    disk drive has an average seek time between 5 and 10 milliseconds. On the other
    hand, its *track-to-track seek time*—that is, the amount of time it takes to move
    the disk head from one track to the next—is on the order of 1 or 2 milliseconds.
    From these numbers, you can see that the acceleration and deceleration of the
    read/write head consumes a much greater percentage of the track-to-track seek
    time than of the average seek time. It takes only 20 times longer to traverse
    1,000 tracks than it does to move to the next track. And because moving the read/write
    heads from one track to the next is usually the most common operation, the track-to-track
    seek time is probably a better indication of the disk’s performance. Regardless
    of which metric you use, however, keep in mind that moving the disk’s read/write
    head is one of the most expensive operations you can do on a disk drive, so it’s
    something you want to minimize.
  prefs: []
  type: TYPE_NORMAL
- en: Because most hard-drive subsystems record data on both sides of a disk platter,
    there are two read/write heads associated with each platter—one for the top and
    one for the bottom. And because most hard drives incorporate multiple platters
    in their disk assembly in order to increase storage capacity (see [Figure 14-2](ch14.xhtml#ch14fig02)),
    a typical drive has multiple pairs of read/write heads.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-2: Multiple-platter hard-disk assembly*'
  prefs: []
  type: TYPE_NORMAL
- en: The various read/write heads are physically connected to the same actuator.
    Therefore, each head sits above the same track on its respective platter, and
    all the heads move across the disk surfaces as a unit. The set of all tracks over
    which the read/write heads are currently sitting is known as a *cylinder* (see
    [Figure 14-3](ch14.xhtml#ch14fig03)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-3: A hard-disk cylinder*'
  prefs: []
  type: TYPE_NORMAL
- en: Although using multiple heads and platters increases the cost of a hard-disk
    drive, it also improves the performance. The performance boost occurs when data
    the system needs isn’t located on the current track. In a hard-disk subsystem
    with only one platter, the read/write head would need to move to another track
    to locate the data. But in a subsystem with multiple platters, the next block
    of data to read is usually located within the same cylinder. And because the hard-disk
    controller can quickly switch between read/write heads electronically, doubling
    the number of platters in a disk subsystem nearly doubles the disk unit’s track-to-track
    seek performance because it winds up doing half the number of seek operations.
    Of course, increasing the number of platters also increases the unit’s capacity,
    which is another reason why high-capacity drives are often higher-performance
    drives as well.
  prefs: []
  type: TYPE_NORMAL
- en: With older disk drives, when the system wants to read a particular sector from
    a particular track on one of the platters, it commands the disk to position the
    read/write head over the appropriate track, and the disk drive then waits for
    the desired sector to rotate underneath. But by the time the head settles down,
    there’s a chance that the desired sector has just passed under the head, in which
    case the disk has to wait for almost one complete rotation before it can read
    the data. On average, the desired sector appears halfway across the disk. If the
    disk is rotating at 7,200 RPM (120 revolutions per second), it requires 8.333
    milliseconds for one complete rotation of the platter. Typically, 4.2 milliseconds
    will pass before the sector rotates underneath the head. This delay is known as
    the *[average rotational latency](gloss01.xhtml#gloss01_22)* of the drive, and
    it is usually equal to the time needed for one rotation, divided by 2.
  prefs: []
  type: TYPE_NORMAL
- en: To see how average rotational latency can be a problem, consider that an OS
    usually manipulates disk data in sector-sized chunks. For example, when reading
    data from a disk file, the OS typically requests that the disk subsystem read
    a sector of data and return that data. Upon receiving the data, the OS processes
    it and then very likely makes a request for additional data from the disk. But
    what happens when this second request is for data located on the next sector of
    the current track? Unfortunately, while the OS is processing the first sector’s
    data, the disk platters are still moving underneath the read/write heads. If the
    OS wants to read the next sector on the disk’s surface but doesn’t notify the
    drive immediately after reading the first sector, the second sector will rotate
    underneath the read/write head. When this happens, the OS will have to wait for
    almost a complete disk rotation before it can read the second sector. This is
    known as *blowing revs* (revolutions). If the OS (or application) is constantly
    blowing revs when reading data from a file, filesystem performance suffers dramatically.
    In early “single-tasking” OSes running on slower machines, blowing revs was an
    unpleasant fact of life. If a track had 64 sectors, it would often take 64 revolutions
    of the disk in order to read all the data on a single track.
  prefs: []
  type: TYPE_NORMAL
- en: To combat this problem, the disk-formatting routines for older drives allow
    the user to interleave sectors. *Interleaving* is the process of spreading out
    sectors within a track so that logically adjacent sectors are not physically adjacent
    on the disk surface (see [Figure 14-4](ch14.xhtml#ch14fig04)).
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of interleaving sectors is that once the OS reads a sector, it
    will take a full sector’s rotation time before the logically adjacent sector moves
    under the read/write head. This gives the OS time to do some processing and to
    issue a new disk I/O request before the desired sector moves underneath the head.
    However, in modern multitasking OSes, it’s difficult to guarantee that an application
    will gain control of the CPU so that it can respond before the next logical sector
    moves under the head, so interleaving isn’t very effective.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-4: Interleaving sectors*'
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, as well as improve disk performance in general, most
    modern disk drives include memory on the disk controller that allows it to read
    data from an entire track in one disk revolution. Once it caches the track data
    in memory, the controller can communicate disk read/write operations at RAM speed
    rather than at disk rotation speeds, which can dramatically improve performance.
    Reading the first sector from a track still exhibits rotational latency, but once
    the disk controller reads the entire track, the latency is all but eliminated
    for that track.
  prefs: []
  type: TYPE_NORMAL
- en: A typical track may have 64 sectors of 512 bytes each, for a total of 32KB per
    track. Because newer disks usually have between 8MB and 512MB of on-controller
    memory, the controller can buffer hundreds of tracks in its memory. Therefore,
    the disk controller cache improves not only the performance of disk read/write
    operations on a single track, but also overall disk performance. Note that the
    disk controller cache speeds up read operations *and* write operations. For example,
    the CPU can often write data to the disk controller’s cache memory within a few
    microseconds and then return to normal data processing while the disk controller
    moves the disk read/write heads into position. When the disk heads are finally
    in position at the appropriate track, the controller can write the data from the
    cache to the disk surface.
  prefs: []
  type: TYPE_NORMAL
- en: From an application designer’s perspective, advances in disk subsystem design
    have reduced the need to understand how disk-drive geometries (track and sector
    layouts) and disk-controller hardware affect the application’s performance. Despite
    these attempts to make the hardware transparent to the application, though, software
    engineers wanting to write great code must always remain cognizant of the disk
    drive’s underlying operation. For example, it’s valuable to know that sequential
    file operations are usually much faster than random-access operations because
    sequential operations require fewer head seeks. Also, if you know that a disk
    controller has an on-board cache, you can write file data in smaller blocks, doing
    other processing between the block operations, to give the hardware time to write
    the data to the disk surface. Though the techniques early programmers used to
    maximize disk performance don’t apply to modern hardware, by understanding how
    disks operate and how they store their data, you can avoid various pitfalls that
    produce slow code.
  prefs: []
  type: TYPE_NORMAL
- en: '***14.1.3 RAID Systems***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because a modern disk drive typically has between 8 and 16 heads, you might
    wonder if you could improve performance by simultaneously reading or writing data
    on multiple heads. While this is certainly possible, it really didn’t happen until
    SATA and larger disk caches came along. But there’s yet another way to improve
    disk drive performance using parallel read and write operations—the *redundant
    array of inexpensive disks (RAID)* configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RAID concept is quite simple: you connect multiple hard-disk drives to
    a special host controller card (sometimes known as an *adapter*), which simultaneously
    reads and writes the various disk drives. By hooking up two disk drives to a RAID
    controller card, you can read and write data about twice as fast as you could
    with a single disk drive. By hooking up four disk drives, you can improve average
    performance by almost a factor of 4.'
  prefs: []
  type: TYPE_NORMAL
- en: RAID controllers support different configurations depending on the purpose of
    the disk subsystem. So-called *[RAID 0](gloss01.xhtml#gloss01_209)* subsystems
    use multiple disk drives simply to increase the data transfer rate. If you connect
    two 150GB disk drives to a RAID controller, you’ll produce the equivalent of a
    300GB disk subsystem with double the data transfer rate. This is a typical configuration
    for personal RAID systems—those systems that are not installed on a file server.
  prefs: []
  type: TYPE_NORMAL
- en: Many high-end file-server systems are *RAID 1* (and higher) subsystems that
    store multiple copies of the data across the multiple disk drives, rather than
    increasing the data transfer rate between the system and the disk drive. In such
    configurations, should one disk fail, a copy of the data is still available on
    another disk drive. Some even higher-level RAID subsystems combine four or more
    disk drives to increase the data transfer rate and provide redundant data storage.
    This type of configuration usually appears on high-end, high-availability file
    server systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modern RAID system configurations can be categorized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**RAID 0** Interleaves data across all disks to increase performance (at the
    expense of reliability). This is known as *striping*. Requires a minimum of two
    disks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**RAID 1** Replicates data on pairs of drives to increase reliability (at the
    cost of performance; also cuts in half the total amount of storage available).
    Allows failure of at least one drive without data loss (depending on the drives
    that fail, could support two or more drive failures). Requires an even number
    of drives, with a minimum of two disks. This is known as *mirroring*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**RAID 5** Stores parity information on the drives. Faster than RAID 1, slower
    than RAID 0\. Allows failure of one drive without data loss. Requires a minimum
    of three drives. At three drives, 66 percent of the total storage is available
    for data; any drives you add beyond three increase data storage by the size of
    the added drive.'
  prefs: []
  type: TYPE_NORMAL
- en: '**RAID 6** Stores duplicate parity information across the drives. Faster than
    RAID 1, slower than RAID 0 and 5\. Allows failure of two drives without data loss.
    Requires a minimum of four drives. At four drives, half the total storage is available
    for data, but any drives you add beyond four increase system storage by the size
    of the added drive.'
  prefs: []
  type: TYPE_NORMAL
- en: '**RAID 10** Combination of RAID 1 + RAID 0\. Minimum four drives; expansion
    has to be in pairs of drives. Interleaved (striped) data across drives to speed
    up performance, plus redundant storage on pairs of drives for reliability. Faster
    than RAID 1 (but slower than RAID 0).'
  prefs: []
  type: TYPE_NORMAL
- en: '**RAID 50****, 60** Combination of RAID 5 + RAID 0 or RAID 6 + RAID 0.'
  prefs: []
  type: TYPE_NORMAL
- en: There are other RAID combinations (like 2, 3, and 4), but most are obsolete
    and you won’t find them in use in modern systems.
  prefs: []
  type: TYPE_NORMAL
- en: RAID systems enable you to dramatically increase disk subsystem performance
    without having to purchase exotic and expensive mass storage solutions. Though
    a software engineer can’t assume that every computer system in the world has a
    fast RAID subsystem available, for those applications that demand the absolute
    highest-performance storage subsystem, RAID (possibly using SSDs) could be a solution.
  prefs: []
  type: TYPE_NORMAL
- en: '***14.1.4 Optical Drives***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An optical drive uses a laser beam and a special photosensitive medium to record
    and play back digital data. Optical drives have a few advantages over hard-disk
    subsystems that use magnetic media:'
  prefs: []
  type: TYPE_NORMAL
- en: They are more shock resistant, so banging the disk drive around during operation
    won’t destroy the drive unit as easily as it would a hard disk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The medium is usually removable, allowing you to maintain an almost unlimited
    amount of offline or near-line storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They’re fairly high-capacity (though modern USB memory sticks and SD cards have
    greater capacities).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At one time, optical storage systems appeared to be the wave of the future
    because they offered very high storage capacity in a small space. Unfortunately,
    they have fallen out of favor in all but a few niche markets because they also
    have several drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: While their read performance is okay, their write speed is very slow—an order
    of magnitude slower than a hard drive and only a few times faster than a *floptical*
    (older combined magnetic/optical floppy) drive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although the optical medium is far more robust than the magnetic medium, the
    magnetic medium in a hard drive is usually sealed away from dirt, humidity, and
    abrasion. In contrast, optical media is easily accessible to someone who really
    wants to do damage to the disk’s surface.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seek times for optical-disk subsystems are much slower than for magnetic disks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optical disks have limited storage capacity, currently less than about 128GB
    (Blu-ray).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultimately, the low price and increasing capacity of USB flash drives killed
    off optical drives for personal computer use.
  prefs: []
  type: TYPE_NORMAL
- en: One area where optical-disk subsystems are still in use, however, is in *near-line
    storage subsystems*, which typically use a robotic jukebox to manage hundreds
    or thousands of optical disks. Although you could argue that a rack of high-capacity
    hard-disk drives would provide a more space-efficient storage solution, it would
    consume far more power, generate far more heat, and require a more sophisticated
    interface than an optical jukebox, which usually has only a single optical-drive
    unit and a robotic disk-selection mechanism. For archival storage, where the server
    system rarely needs access to any particular piece of data in the storage subsystem,
    a jukebox system is a very cost-effective solution.
  prefs: []
  type: TYPE_NORMAL
- en: If you wind up writing software that manipulates files on an optical-drive subsystem,
    the most important thing to remember is that read access is much faster than write
    access. You should try to use the optical system as a “read-mostly” device and
    avoid writing data as much as possible to the device. You should also avoid random
    access on an optical disk’s surface, as seek times are very slow.
  prefs: []
  type: TYPE_NORMAL
- en: CD, DVD, and Blu-ray drives are also optical drives. However, because of their
    widespread use, and their sufficiently different organization and performance
    when compared with standard optical drives, they warrant a separate discussion.
  prefs: []
  type: TYPE_NORMAL
- en: '***14.1.5 CD, DVD, and Blu-ray Drives***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CD-ROM was the first optical drive subsystem to gain wide acceptance in the
    personal computer market. CD-ROM disks were based on the audio CD digital recording
    standard, and they provided a large amount of storage (650MB) when compared to
    hard-disk-drive storage capacities at the time (typically 100MB). As time passed,
    of course, this relationship reversed. Still, CD-ROMs became the preferred distribution
    vehicle for most commercial applications, completely replacing the floppy-disk
    medium for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Although the CD-ROM format is a very inexpensive distribution medium in large
    quantities, often costing only a few cents per disk, it’s not appropriate for
    small production runs. The problem is that it typically costs several hundreds
    or thousands of dollars to produce a disk master (from which the run of CD-ROMs
    are made), meaning that CD-ROM is usually cost-effective only when the quantity
    of disks being produced is at least in the thousands.
  prefs: []
  type: TYPE_NORMAL
- en: The solution was a new CD medium, CD-Recordable (CD-R), which allowed the production
    of one-off CD-ROMs. CD-R uses a write-once optical disk technology, known euphemistically
    as *WORM* (write-once, read-many). When first introduced, CD-R disks cost about
    $10 to $15\. However, once the drives reached critical mass and media manufacturers
    began producing blank CD-R disks in huge quantities, their bulk retail price fell
    to about $0.25\. As a result, CD-R made it possible to distribute a fair amount
    of data in small quantities.
  prefs: []
  type: TYPE_NORMAL
- en: One obvious drawback to CD-R is the “write-once” limitation. To overcome it,
    the CD-Rewriteable (CD-RW) drive and medium were created. CD-RW, as its name suggests,
    supports both reading and writing. Unlike with optical disks, however, you can’t
    simply rewrite a single sector on CD-RW. Instead, to rewrite the data on a CD-RW
    disk, you must first erase the whole disk.
  prefs: []
  type: TYPE_NORMAL
- en: Although the 650MB of storage on a CD seemed like a gargantuan amount when CDs
    were first introduced, the old maxim that data and programs expand to fill up
    all available space certainly held true. Though CDs were ultimately expanded to
    700MB, various games (with embedded video), large databases, developer documentation,
    programmer development systems, clip art, stock photographs, and even regular
    applications reached the point where a single CD was woefully inadequate. The
    DVD-ROM (and later, DVD-R, DVD-RW, DVD+RW, and DVD-RAM) disk reduced this problem
    by offering between 3GB and 17GB of storage on a single disk. Except for the DVD-RAM
    format, you can view the DVD formats as faster, higher-capacity versions of the
    CD formats. There are some clear technical differences between the two, but most
    of them are transparent to the software. Today, Blu-ray optical discs deliver
    up to 128GB of storage (Blu-ray BDXL). However, electronic distribution via the
    internet has largely replaced physical media, so Blu-ray discs have never become
    as popular as distribution or storage media.
  prefs: []
  type: TYPE_NORMAL
- en: The CD and DVD formats were created for reading data in a continuous stream—*streaming*
    data—from the storage medium. The track-to-track head movement time required to
    read data stored on a hard disk creates a big gap in the streaming sequence, which
    is unacceptable for audio and video applications. CDs and DVDs record information
    on a single, very long track that forms a spiral across the surface of the whole
    disk. Thus, the CD or DVD player can continuously read the data simply by moving
    the laser beam along the disk’s single spiral track at a constant rate.
  prefs: []
  type: TYPE_NORMAL
- en: Although having a single track is great for streaming data, it does make it
    a bit more difficult to locate a specific sector on the disk. The CD or DVD drive
    can only approximate a sector’s position by mechanically positioning the laser
    beam to some point on the disk. Next, it must actually read data from the disk
    surface to determine where the laser is positioned, and then do some fine-tuning
    to locate the desired sector. As a result, searching for a specific sector on
    a CD or DVD disk can take an order of magnitude longer than searching for a specific
    sector on a hard disk.
  prefs: []
  type: TYPE_NORMAL
- en: The most important thing to remember for a programmer writing code that interacts
    with CD or DVD media is that random access is verboten. These media were designed
    for sequential streaming access, and seeking data on such media will hinder your
    application performance. If you’re using these disks to deliver your application
    and its data to the end user, you should have the user copy the data to a hard
    disk before use if high-performance random access is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '**14.2 Tape Drives**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tape drives were also popular mass storage devices. Traditionally, PC owners
    used tape drives to back up data stored on hard-disk drives back in the days when
    hard drives were much smaller. For many years, tape storage was far more cost-effective
    than hard-disk storage on a cost-per-megabyte basis. Indeed, at one time there
    was an order of magnitude difference in cost per megabyte between tape storage
    and magnetic disk storage. And because tape drives held more data than most hard-disk
    drives, they were more space-efficient too.
  prefs: []
  type: TYPE_NORMAL
- en: However, because of competition and technological advances in the hard-disk-drive
    marketplace, tapes have lost these advantages. Hard-disk drives now exceed 16TB
    in storage, and the optimum price point for hard disks is about $0.25 per gigabyte.
    Tape storage today costs far more per megabyte than hard-disk storage. Plus, only
    a few tape technologies allow you to store 250GB on a single tape, and those that
    do (such as Digital Linear Tape, or DLT) are extremely expensive. It’s not surprising
    that tape drives are seeing less and less use these days in home PCs and are typically
    found only in larger file server machines. Linear Tape-Open (LTO) drives extend
    the capacity to around 12TB (expected to increase to around 200TB in the future).
    Nevertheless, today a typical LTO-8 tape costs almost $130 (US), about half the
    price per megabyte of a hard drive.
  prefs: []
  type: TYPE_NORMAL
- en: Back in the days of mainframes, application programs interacted with tape drives
    in much the same way that today’s applications interact with hard-disk drives.
    A tape drive, however, is not an efficient random-access device. That is, although
    software can read a random set of blocks from a tape, it cannot do so with acceptable
    performance. Of course, in the days when most applications ran on mainframes,
    applications generally were not interactive, and CPUs were much slower; thus,
    the standard for “acceptable performance” was different.
  prefs: []
  type: TYPE_NORMAL
- en: In a tape drive, the read/write head is fixed, and the tape transport mechanism
    moves the tape past the head linearly, from the beginning of the tape to the end,
    or vice versa. If the beginning of the tape is currently positioned over the read/write
    head and you want to read data at the end of the tape, you have to move the entire
    tape past the head to get to the desired data. This can be very slow, requiring
    tens or even hundreds of seconds, depending on the length and format of the tape.
    Compare this with the tens of milliseconds it takes to reposition a hard disk’s
    read/write head (or the negligible time it takes to get data from an SSD). Therefore,
    to perform well on a tape drive, software must be written to account for the limitations
    of a sequential access device. In particular, data should be read or written sequentially
    on a tape.
  prefs: []
  type: TYPE_NORMAL
- en: Originally, data was written to tapes in blocks (much like sectors on a hard
    disk), and the drives were designed to allow quasi-random access to the tape’s
    blocks. If you’ve ever watched old movies that used the reel-to-reel drives, with
    the reels constantly stopping, starting, stopping, reversing, stopping, and continuing,
    you’ve seen “random access” in action. Such tape drives were very expensive because
    they required powerful motors, finely tooled tape-path mechanisms, and so on.
    As hard drives became larger and less expensive, applications stopped using tape
    as a data manipulation medium and used it only for offline storage (to back up
    data from hard disks).
  prefs: []
  type: TYPE_NORMAL
- en: Because sequential data access on tape does not require the heavy-duty mechanics
    of the original tape drives, tape-drive manufacturers sought to make a lower-cost
    product suitable for sequential access only. Their solution was the *streaming
    tape drive*, which was designed to keep the data constantly moving from the CPU
    to the tape, or vice versa. For example, while backing up the data from a hard
    disk to tape, a streaming tape drive treats the data like a video or audio recording
    and just lets the tape run, constantly writing the data from the hard disk to
    the tape. Because of the way streaming tape drives work, very few applications
    deal directly with the tape unit. Today, it’s very rare for anything other than
    a tape backup utility program, run by the system administrator, to access the
    tape hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '**14.3 Flash Storage**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interesting storage medium that has become popular because of its compact
    form factor^([2](footnotes.xhtml#fn14_2a)) is flash storage. The flash medium
    is actually a semiconductor device, based on *electrically erasable programmable
    read-only memory (EEPROM)* technology, which, despite its name, is both readable
    and writable. Unlike regular semiconductor memory, flash storage is *nonvolatile*,
    meaning it maintains its data even when disconnected from power. Like other semiconductor
    technologies, flash storage is purely electronic and doesn’t require any motors
    or other electromechanical devices for proper operation. Therefore, flash storage
    devices are more reliable and shock resistant, and they use far less power than
    mechanical storage solutions such as disk drives. This makes flash storage especially
    valuable in portable battery-powered devices like cell phones, tablets, laptop
    computers, electronic cameras, MP3 playback devices, and recorders.
  prefs: []
  type: TYPE_NORMAL
- en: Flash storage modules now provide in excess of 1TB of storage, and their optimal
    price point is about $0.15 (US) per gigabyte. This makes them comparable, per
    bit, to hard-disk storage.
  prefs: []
  type: TYPE_NORMAL
- en: Flash devices are sold in many different form factors. OEMs (original equipment
    manufacturers) can buy flash storage devices that look like other semiconductor
    chips and mount them directly on their circuit boards. However, most flash memory
    devices sold today are built into one of several standard forms, including SDHC
    cards, CompactFlash cards, smart-memory modules, memory sticks, USB/flash modules,
    or SSDs. For example, you might remove a CompactFlash card from your camera, insert
    it into a special CompactFlash card reader on your PC, and access your photographs
    just as you would files on a disk drive.
  prefs: []
  type: TYPE_NORMAL
- en: Memory in a flash storage module is organized in blocks of bytes, not unlike
    sectors on a hard disk. In contrast to regular semiconductor memory or RAM, however,
    you can’t write individual bytes in a flash storage module. Although you can generally
    *read* an individual byte from a flash storage device, to write to a particular
    byte you must first erase the entire block on which it resides. The block size
    varies by device, but most OSes treat these flash blocks like a disk sector for
    the purposes of reading and writing. Although the basic flash storage device itself
    could connect directly to the CPU’s memory bus, most common flash storage packages
    (such as CompactFlash cards and memory sticks) contain electronics that simulate
    a hard-disk interface, and you access the flash device just as you would a hard-disk
    drive.
  prefs: []
  type: TYPE_NORMAL
- en: One interesting aspect to flash memory devices, and EEPROM devices in general,
    is that they have a limited write lifetime. That is, you can write to a particular
    memory cell in a flash memory module only a certain number of times before that
    cell begins to have problems retaining the information. This was a big concern
    in early EEPROM/flash devices, because the average number of write cycles before
    failures began occurring was around 10,000\. That is, if some software wrote to
    the same memory block 10,000 times in a row, the EEPROM/flash device would probably
    develop a bad memory cell in that block, effectively rendering the entire chip
    useless. On the other hand, if the software wrote just once to 10,000 separate
    blocks, the device could still take 9,999 more writes to each memory cell. Therefore,
    the OSes of these early devices would try to spread out write operations across
    the entire device to minimize damage. Although modern flash devices still exhibit
    this problem, technological advances have reduced it almost to the point where
    we can ignore it. A modern flash memory cell supports an average of about a million
    write cycles before it will go bad. Furthermore, today’s OSes simply mark bad
    flash blocks, the same way they mark bad sectors on a disk, and will skip a block
    once they determine that it has gone bad.
  prefs: []
  type: TYPE_NORMAL
- en: Being electronic, flash devices do not exhibit rotational latency times at all,
    and they don’t exhibit much in the way of seek times either. There’s a tiny amount
    of time needed to write an address to a flash memory module, but it’s nothing
    compared to the head seek times on a hard disk. Despite this, flash memory is
    generally nowhere near as fast as typical RAM. Reading data from a flash device
    itself usually takes microseconds (rather than nanoseconds), and the interface
    between the flash memory device and the system may require additional time to
    set up a data transfer. In addition, it’s common to interface a flash storage
    module to a PC using a USB flash reader device, and this can further reduce the
    average read time per byte to hundreds of microseconds.
  prefs: []
  type: TYPE_NORMAL
- en: Write performance is even worse. To write a block of data to flash, you must
    write the data, read it back, compare it to the original data, and rewrite it
    if they don’t match. This process can take several tens or even hundreds of milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, flash memory modules are generally quite a bit slower than high-performance
    hard-disk subsystems. However, thanks mainly to demand from high-end digital camera
    users who want to be able to snap as many pictures as possible in a short time,
    technological advances are boosting their performance. Though flash memory performance
    probably won’t catch up with hard-disk performance any time soon, you can expect
    it to continue improving over time.
  prefs: []
  type: TYPE_NORMAL
- en: '**14.4 RAM Disks**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another interesting mass storage device is the RAM disk, a semiconductor solution
    that treats a large block of the computer system’s memory as though it were a
    disk drive, simulating blocks and sectors using memory arrays. The advantage of
    memory-based disks is that they are very high performance. RAM disks don’t suffer
    from the time delays associated with head seek time and rotational latency that
    you find on hard, optical, and floppy drives. Their interface to the CPU is also
    much faster, so data transfer times are very short, often running at the maximum
    bus speed. It’s hard to imagine a faster storage technology than a RAM disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'RAM disks, however, have two disadvantages: cost and volatility. The cost per
    byte of storage in a RAM disk system is very high. Indeed, byte for byte, semiconductor
    storage is as much as 10,000 times more expensive than magnetic hard-disk storage.
    As a result, RAM disks usually have low storage capacities, typically no more
    than several gigabytes. And RAM disks are volatile—they lose their memory unless
    they are powered at all times. This generally means that semiconductor disks are
    great for storing temporary files and files you’ll copy back to some permanent
    storage device before shutting down the system. They are not particularly well
    suited for maintaining important information over long periods of time.'
  prefs: []
  type: TYPE_NORMAL
- en: '**14.5 Solid-State Drives**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modern high-performance PCs use solid-state drives (SSDs). SSDs use flash memory
    (like USB sticks) with a high-performance interface to the system. But SSDs aren’t
    simply USB flash drives in different clothing. USB flash drives are designed for
    low cost per bit—except for certain camera applications (particularly 4K and 8K
    camcorders), speed is secondary to cost and capacity. A typical USB flash drive,
    for example, is quite a bit slower than a mediocre hard drive. SSDs, on the other
    hand, must be fast. Because of their solid-state design, they’re typically an
    order of magnitude faster than rotating magnetic media. With a RAID configuration,
    SSDs can actually achieve the performance limits of SATA interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: As this was being written, SSDs cost between 4 and 16 times as much as high-capacity
    hard drives (8TB drives and 1TB SSDs both cost about $100 US). However, the price-per-gigabyte
    gap has been closing. SSDs are rapidly replacing rotating magnetic drives, and
    rotating magnetic media will likely be relegated to the trash bin of history (much
    like tape drives). Before that point, why would anyone pay more for an SSD?
  prefs: []
  type: TYPE_NORMAL
- en: SSDs typically use a different underlying technology to store data and provide
    a much faster electronic interface to the PC. This is why an SSD tends to be much
    more expensive than a USB flash drive. That’s also why SSDs can achieve 2,500MBps
    data transfer rates, while high-quality memory cards are capable of only around
    100MBps (and USB flash/thumb drives are even worse).
  prefs: []
  type: TYPE_NORMAL
- en: From a programmer’s perspective, one of the big advantages of SSDs is that you
    no longer have to worry about seek times and other latency issues. SSDs tend to
    be true(r) random-access devices (at least when compared with hard drives). Accessing
    data at the beginning of the drive and then at the end takes only a little longer
    than accessing any pair of data elements elsewhere on the SSD.
  prefs: []
  type: TYPE_NORMAL
- en: There are a couple of disadvantages to SSDs, though. First of all, their write
    performance is usually much slower than their read performance (though writing
    to an SSD is still much faster than writing to a hard drive). Fortunately, data
    is read far more often than it is written, but this is something to consider when
    you’re working on software that writes data to a SSD. The second drawback is that
    SSDs wear out after a while. Writing to the same location over and over again
    will eventually cause the associated memory cell(s) to fail. Fortunately, modern
    OSes work around these failures. However, when you write applications that continuously
    overwrite file data, keep this issue in mind.
  prefs: []
  type: TYPE_NORMAL
- en: '**14.6 Hybrid Drives**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most modern hard drives contain an on-board RAM cache (to hold entire tracks
    of data to eliminate rotational latency, for example). Hybrid drives, such as
    Apple’s older Fusion Drive, combine a small SSD with a large hard drive—typically
    a 32GB to 128GB SSD and a 2TB magnetic disk, in Apple’s case. Frequently accessed
    data stays in the SSD cache, and is swapped out to the hard drive when space is
    needed for new data. This works the same way as caching in main memory, boosting
    the system performance to near-SSD speeds for data that is accessed regularly.
  prefs: []
  type: TYPE_NORMAL
- en: '**14.7 Filesystems on Mass Storage Devices**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very few applications access mass storage devices directly. That is, applications
    do not generally read and write tracks, sectors, or blocks on a mass storage device;
    instead, they open, read, write, and otherwise manipulate *files* on it. The OS’s
    *file manager* abstracts away the physical configuration of the underlying storage
    device and provides a convenient storage facility for multiple independent files
    on a single device.
  prefs: []
  type: TYPE_NORMAL
- en: On the earliest computer systems, applications were responsible for tracking
    the physical location of data on a mass storage device, because there was no file
    manager available to do so. They were able to maximize their performance by carefully
    considering the layout of data on the disk. For example, they could manually interleave
    data across various sectors on a track to give the CPU time to process it between
    reading and writing those sectors on the track. Such software was often many times
    faster than comparable software using a generic file manager. Later, when file
    managers were commonly available, some application authors still managed their
    files on a storage device for performance reasons. This was especially true back
    in the days of floppy disks, when low-level software written to manipulate data
    at the track and sector level often ran 10 times faster than the same application
    using a file manager system.
  prefs: []
  type: TYPE_NORMAL
- en: In theory, today’s software could benefit from this approach as well, but in
    practice you rarely see this kind of low-level disk access in modern applications,
    for several reasons. First, writing software that manipulates a mass storage device
    at such a low level locks you into using that particular device. That is, if your
    software manipulates a disk with 48 sectors per track, 12 tracks per cylinder,
    and 768 cylinders per drive, that software will not work optimally (if at all)
    on a drive with a different sector, track, and cylinder layout. Second, accessing
    the drive at a low level makes it difficult to share the device among different
    applications, something that can be especially costly on a multitasking system
    that may have several applications sharing the device at once. For example, if
    you’ve laid out your data on various sectors on a track to coordinate computation
    time with sector access, your work is lost when the OS interrupts your program
    and gives some other application its time slice—time you were counting on to do
    any necessary computations prior to the next data sector rotating under the read/write
    head. Third, some of the features of modern mass storage devices, such as on-board
    caching controllers and SCSI interfaces that present a storage device as a sequence
    of blocks rather than as something with a given track and sector geometry, eliminate
    any advantage that low-level software might have had. Fourth, modern OSes typically
    contain file buffering and block caching algorithms that provide good filesystem
    performance, obviating the need to operate at such a low level. Finally, low-level
    disk access is very complex, and writing such software is difficult.
  prefs: []
  type: TYPE_NORMAL
- en: '***14.7.1 Sequential Filesystems***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The earliest file manager systems stored files sequentially on the disk’s surface.
    That is, if each sector/block on the disk held 512 bytes and a file was 32KB long,
    that file would consume 64 consecutive sectors/blocks on the disk’s surface. To
    access that file at some future time, the file manager only needed to know the
    file’s starting block number and the number of blocks it occupied. The filesystem
    had to maintain these two pieces of information somewhere in nonvolatile storage.
    The obvious place was on the storage media itself, in a data structure known as
    the *directory*—an array of values starting at a specific disk location that the
    OS can reference when an application requests a particular file. The file manager
    can search through the directory for the file’s name and extract its starting
    block and length. With this information, the filesystem can provide the application
    with access to the file’s data.
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of the sequential filesystem is that it is very fast. The OS can
    read or write a single file’s data very rapidly if the file is stored in sequential
    blocks on the disk’s surface. But a sequential file organization has some serious
    problems, too. The biggest and most obvious drawback is that you can’t extend
    the size of a file once the file manager places another file at the next block
    on the disk. Disk fragmentation is another big problem. As applications create
    and delete many small and medium files, the disk fills up with short sequences
    of unused sectors that, individually, are too small for most files. On sequential
    filesystems, disks often had free space sufficient to hold some data, but they
    couldn’t use it because it was scattered in small pieces all over the disk’s surface.
    To solve this problem, users had to run disk compaction programs to coalesce all
    the free sectors and move them to the end of the disk by physically rearranging
    files on its surface. Another solution was to copy files from one full disk to
    another empty disk, collecting the many small, unused sectors together. Obviously,
    this was extra work that the user had to do—work that the OS should have been
    doing.
  prefs: []
  type: TYPE_NORMAL
- en: The sequential file storage scheme really falls apart when used with multitasking
    OSes. If two applications attempt to write file data to the disk concurrently,
    the filesystem must place the starting block of the second application’s file
    beyond the last block required by the first application’s file. As the OS has
    no way of determining how large the files can grow, each application has to tell
    the OS the maximum length of the file when the application first opens it. Unfortunately,
    many applications cannot determine in advance how much space they’ll need for
    their files, so they have to guess the size of the file when opening it. If the
    estimated file size is too small, either the program has to abort with a “file
    full” error, or the application has to create a larger file, copy the old data
    from the “full” file to the new file, and then delete the old file. As you can
    imagine, this is horribly inefficient and definitely not great code.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid such performance problems, many applications grossly overestimate the
    amount of space they need for their files. As a result, they wind up wasting disk
    space when the files don’t actually use all the data allocated to them, a form
    of *internal* fragmentation. Furthermore, if applications truncate their files
    when closing them, the resulting free sections returned to the OS tend to fragment
    the disk into the small, unusable blocks of free space described previously, a
    problem known as *external* fragmentation. For these reasons, sequential storage
    on the disk has been replaced by more sophisticated storage management schemes
    in modern OSes.
  prefs: []
  type: TYPE_NORMAL
- en: '***14.7.2 Efficient File Allocation Strategies***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most modern file allocation strategies allow files to be stored across arbitrary
    blocks on the disk. Because the filesystem can now place bytes of the file in
    any free block on the disk, the problems of external fragmentation and the limitation
    on file size are all but eliminated. As long as there’s at least one free block
    on the disk, you can expand the size of any file. However, with this flexibility
    comes some added complexity. In a sequential filesystem, it was easy to locate
    free space on the disk; by simply noting the starting block numbers and sizes
    of the files in a directory, the filesystem could locate a free block large enough
    to satisfy the current disk allocation request, if one was available. But with
    files stored across arbitrary blocks, scanning the directory and noting which
    blocks a file uses is far too expensive to compute, so the filesystem has to keep
    track of the free and used blocks. Most modern OSes use one of three data structures—a
    set, a table (array), or a list—to keep track of which sectors are free and which
    are not. Each scheme has its advantages and disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: '**14.7.2.1 Free-Space Bitmaps**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The free-space bitmap scheme uses a set data structure to maintain a set of
    free blocks on the disk drive. If a block is a member of that set, the file manager
    can remove it whenever it needs another block for a file. Because set membership
    is a Boolean relationship (a block is either in the set or it’s not), it takes
    exactly 1 bit to specify the set membership of each block.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, a file manager reserves a certain section of the disk to hold a bitmap
    that specifies which blocks on the disk are free. The bitmap consumes some integral
    number of blocks on the disk, with each block consumed representing a specific
    number of other blocks on the disk, which we can calculate by multiplying the
    block size (in bytes) by 8 (bits per byte). For example, if the OS uses 4,096-byte
    blocks on the disk, a bitmap consisting of a single block can track up to 32,768
    other blocks on the disk.
  prefs: []
  type: TYPE_NORMAL
- en: The disadvantage of the bitmap scheme is that as disks get large, so does the
    bitmap. For example, on a 120GB drive with 4,096-byte blocks, the bitmap will
    be almost 4MB long. While this is a small percentage of the total disk capacity,
    accessing a single bit in a bitmap this large can be clumsy. To find a free block,
    the OS has to do a linear search through this 4MB bitmap. Even if you keep the
    bitmap in system memory (which is a bit expensive, considering that you have to
    do it for each drive), searching through it every time you need a free sector
    is an expensive proposition. As a result, you don’t see this scheme used much
    on larger disk drives.
  prefs: []
  type: TYPE_NORMAL
- en: One advantage (and also a disadvantage) of the bitmap scheme is that the file
    manager uses it only to keep track of the free space on the disk, not which sectors
    belong to a given file. As a result, if the free-space bitmap is damaged somehow,
    nothing is permanently lost; you can easily reconstruct it by searching through
    all the disk directories and computing which sectors are being used by the files
    in those directories (the remaining sectors, obviously, are the free ones). Although
    this process is somewhat time-consuming, it’s nice to have the option if disaster
    strikes.
  prefs: []
  type: TYPE_NORMAL
- en: '**14.7.2.2 File Allocation Tables**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Another way to track disk-sector usage is with a table of sector pointers, or
    a *file allocation table (FAT)*. This scheme is widely used. Cementing its popularity,
    this is also the default file allocation scheme used on most USB flash drives.
    An interesting facet of the FAT structure is that it combines both free-space
    management and file-sector allocation management into the same data structure,
    ultimately saving space when compared to the bitmap scheme, which uses separate
    data structures for each. Furthermore, unlike the bitmap scheme, FAT doesn’t require
    an inefficient linear search to find the next available free sector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The FAT is really nothing more than an array of self-relative pointers (that
    is, indexes into itself), setting aside one pointer for each sector/block on the
    storage device. When a disk is initialized, the first several blocks on its surface
    are reserved for objects like the root directory and the FAT itself, and the remaining
    blocks on the disk are free. Somewhere in the root directory is a free-space pointer
    that specifies the next available free block on the disk. Assuming the free-space
    pointer initially contains the value `64`, implying that the next free block is
    block 64, the FAT entries at indexes 64, 65, 66, and so on, would contain the
    following values, assuming there are *n* blocks on the disk, numbered from 0 to
    *n* – 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **FAT index** | **FAT entry value** |'
  prefs: []
  type: TYPE_TB
- en: '| . . . | . . . |'
  prefs: []
  type: TYPE_TB
- en: '| 64 | `65` |'
  prefs: []
  type: TYPE_TB
- en: '| 65 | `66` |'
  prefs: []
  type: TYPE_TB
- en: '| 66 | `67` |'
  prefs: []
  type: TYPE_TB
- en: '| 67 | `68` |'
  prefs: []
  type: TYPE_TB
- en: '| . . . | . . . |'
  prefs: []
  type: TYPE_TB
- en: '| *n* – 2 | *n* – 1 |'
  prefs: []
  type: TYPE_TB
- en: '| *n* – 1 | `0` |'
  prefs: []
  type: TYPE_TB
- en: The entry at block 64 tells you the next available free block on the disk, 65\.
    Moving on to entry 65, you’ll find the value of the next available free block
    on the disk, `66`. The last entry in the FAT contains a `0` (block 0 contains
    meta-information for the entire disk partition and is never available).
  prefs: []
  type: TYPE_NORMAL
- en: 'Whenever an application needs one or more blocks to hold some new data on the
    disk’s surface, the file manager grabs the free-space pointer value and then continues
    going through the FAT entries for however many blocks are required to store the
    new data. For example, if each block is 4,096 bytes long and the current application
    is attempting to write 8,000 bytes to a file, the file manager will need to remove
    two blocks from the free-block list, following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Get the value of the free-space pointer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the value of the free-space pointer in order to determine the first free
    sector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue going through the FAT entries for the number of blocks required to
    store the application’s data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the FAT entry value of the last block where the application needs to
    store its data, and set the free-space pointer to this value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store a `0` over the FAT entry value of the last block that the application
    uses, marking the end to the list of blocks that the application needs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the original (as it was prior to these steps) value of the free-space
    pointer into the FAT as the pointer to the list of blocks that are now allocated
    for the application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After the block allocation in our earlier example, the application has blocks
    64 and 65 at its disposal, the free-space pointer contains `66`, and the FAT looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **FAT index** | **FAT entry value** |'
  prefs: []
  type: TYPE_TB
- en: '| . . . | . . . |'
  prefs: []
  type: TYPE_TB
- en: '| 64 | `65` |'
  prefs: []
  type: TYPE_TB
- en: '| 65 | `0` |'
  prefs: []
  type: TYPE_TB
- en: '| 66 | `67` |'
  prefs: []
  type: TYPE_TB
- en: '| 67 | `68` |'
  prefs: []
  type: TYPE_TB
- en: '| . . . | . . . |'
  prefs: []
  type: TYPE_TB
- en: '| *n* – 2 | *n* – 1 |'
  prefs: []
  type: TYPE_TB
- en: '| *n* – 1 | `0` |'
  prefs: []
  type: TYPE_TB
- en: 'This is not to imply that entries in the FAT *always* contain the index of
    the next entry in the table. As the file manager allocates and deallocates storage
    for files on the disk, these numbers tend to become scrambled. For example, if
    an application returns block 64 to the free list but holds on to block 65, the
    free-space pointer would contain the value `64`, and the FAT would have the following
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **FAT index** | **FAT entry value** |'
  prefs: []
  type: TYPE_TB
- en: '| . . . | . . . |'
  prefs: []
  type: TYPE_TB
- en: '| 64 | `66` |'
  prefs: []
  type: TYPE_TB
- en: '| 65 | `0` |'
  prefs: []
  type: TYPE_TB
- en: '| 66 | `67` |'
  prefs: []
  type: TYPE_TB
- en: '| 67 | `68` |'
  prefs: []
  type: TYPE_TB
- en: '| . . . | . . . |'
  prefs: []
  type: TYPE_TB
- en: '| *n* – 2 | *n* – 1 |'
  prefs: []
  type: TYPE_TB
- en: '| *n* – 1 | `0` |'
  prefs: []
  type: TYPE_TB
- en: As noted earlier, one advantage of the FAT data structure is that it combines
    both free-space management and file allocation management into a single data structure.
    This means that each file doesn’t have to carry around a list of the blocks its
    data occupies. Instead, it needs only a single pointer value specifying an index
    into the FAT where the first block of the file’s data can be found. You can find
    the remaining blocks containing the file’s data by stepping through the FAT.
  prefs: []
  type: TYPE_NORMAL
- en: One important advantage of the FAT scheme over the set (bitmap) scheme is that
    once the disk using a FAT filesystem is full, it doesn’t maintain information
    about which blocks are free. In contrast, the bitmap scheme consumes space on
    the disk to track free blocks even when there are none available. The FAT scheme
    replaces the entries originally used to track free blocks with the file-block
    pointers. When the disk is full, the values that originally maintained the free-block
    list are no longer consuming disk space because they’re all now tracking blocks
    in files. In this case, the free-space pointer contains `0` (to denote an empty
    free-space list) and all the FAT entries contain chains of block indexes for file
    data.
  prefs: []
  type: TYPE_NORMAL
- en: However, the FAT scheme does have a couple of disadvantages. First, unlike the
    bitmap in a set scheme filesystem, the table in a FAT filesystem represents a
    single point of failure. If the FAT is somehow destroyed, it can be very difficult
    to repair the disk and recover files; losing some free space on a disk is a problem,
    but losing track of where your files are on the disk is a *major* problem. Furthermore,
    because the disk head tends to spend more time in the FAT area of a storage device
    than in any other single area on the disk, the FAT is the most likely part of
    a hard disk to be damaged by a head crash, and the most likely part of a floppy
    or optical drive to exhibit excessive wear. This is a sufficiently big concern
    that some FAT filesystems provide an option to maintain an extra copy of the FAT
    on the disk.
  prefs: []
  type: TYPE_NORMAL
- en: Another problem with the FAT is that it’s usually located at a fixed place on
    the disk, typically at some low block number. In order to determine which block
    or blocks to read for a particular file, the disk heads must move to the FAT,
    so if the FAT is at the beginning of the disk, they’ll constantly be traveling
    to and from the FAT across large distances. This massive head movement not only
    is slow but tends to wear out the mechanical parts of the disk drive sooner. In
    newer versions of Microsoft OSes, the FAT-32 scheme eliminates part of this problem
    by allowing the FAT to be positioned somewhere other than the beginning of the
    disk, though still at a fixed location. Application file I/O performance can be
    quite low with a FAT filesystem unless the OS caches the FAT in main memory, which
    can be dangerous if the system crashes, because you could lose track of all file
    data whose FAT entries have not been written to disk.
  prefs: []
  type: TYPE_NORMAL
- en: The FAT scheme is also inefficient for doing random access on a file. To read
    from offset *m* to offset *n* in a file, the file manager must divide *n* by the
    block size to obtain the block offset into the file containing the byte at offset
    *n*, divide *m* by the block size to obtain its block offset, and then sequentially
    search through the FAT chain between these two blocks to find the sector(s) containing
    the desired data. This linear search can be expensive if the file is a large database
    with many thousands of blocks between the current block position and the desired
    block position.
  prefs: []
  type: TYPE_NORMAL
- en: Yet another problem with the FAT filesystem, though this one is rather esoteric,
    is that it doesn’t support sparse files. That is, you cannot write to byte 0 and
    byte 1,000,000 of a file without also allocating every byte of data between those
    two points on the disk surface. Some non-FAT file managers allocate only the blocks
    where an application has written data. For example, if an application writes data
    only to bytes 0 and 1,000,000 of a file, the file manager allocates only two blocks
    for the file. If the application attempts to read a block that hasn’t been previously
    allocated (for example, if the application in the current example attempts to
    read the byte at offset 500,000 without first writing to that location), the file
    manager simply returns `0`s for the read operation without actually using any
    space on the disk. But because of how a FAT is organized, you can’t create sparse
    files on the disk.
  prefs: []
  type: TYPE_NORMAL
- en: '**14.7.2.3 Lists of Blocks**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To overcome the limitations of the FAT filesystem, advanced OSes—such as Windows
    NT/2000/XP/7/8/10, macOS (APFS), and various flavors of Unix—use a list-of-blocks
    scheme instead. Indeed, the list scheme enjoys all the advantages of a FAT system
    (such as efficient, nonlinear free-block location, and efficient storage of the
    free-block list), and it solves many of FAT’s problems.
  prefs: []
  type: TYPE_NORMAL
- en: The list scheme begins by setting aside several blocks on the disk for the purpose
    of keeping (generally) 32- or 64-bit pointers to each free block on the disk.
    If each block on the disk holds 4,096 bytes, a block can hold 1,024 (or 512) pointers.
    Dividing the number of blocks on the disk by 1,024 (512) determines the number
    of blocks the free-block list will initially consume. As you’ll soon see, the
    system uses these blocks to store data once the disk fills up, so there’s no storage
    overhead associated with the blocks consumed by the free-block list.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a block in the free-block list contains 1,024 pointers (the following examples
    all assume 32-bit pointers), then the first 1,023 pointers contain the block numbers
    of free blocks on the disk. The file manager maintains two pointers on the disk:
    one that holds the block number of the current block containing free-block pointers,
    and one that holds an index into that current block. Whenever the filesystem needs
    a free block, it obtains the index for one from the free-block list by using these
    two pointers. Then the file manager increments the index into the free-block list
    to the next available entry in the list. When the index increments to 1,023 (the
    1,024th item in the free-block list), instead of using the pointer entry value
    at that index to locate a free block, the file manager uses it as the address
    of the next block containing a list of free-block pointers on the disk, and it
    uses the current block, containing a now-empty list of block pointers, as the
    free block. This is how the file manager reuses the blocks originally designated
    to hold the free-block list, rather than reusing the pointers in the free-block
    list to keep track of the blocks belonging to a given file, as FAT does. Once
    the file manager uses up all the free-block pointers in a given block, it uses
    that block for actual file data.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the FAT, the list scheme does not merge the free-block list and the file
    list into the same data structure. Instead, a separate data structure for each
    file holds the list of blocks associated with that file. Under typical Unix and
    Linux filesystems, the directory entry for the file actually holds the first 8
    to 16 entries in the list (see [Figure 14-5](ch14.xhtml#ch14fig05)). This allows
    the OS to track small files (up to 32KB or 64KB) without having to allocate any
    extra space on the disk.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-5: Block list for small files*'
  prefs: []
  type: TYPE_NORMAL
- en: Research on various flavors of Unix suggests that the vast majority of files
    are small, and embedding several pointers into the directory entry provides an
    efficient way to access small files. Of course, as time passes, the average file
    size seems to increase. But as it turns out, block sizes tend to increase as well.
    When the research was first done, the typical block size was 512 bytes, but today
    it’s 4,096 bytes. During that time, then, average file sizes could have increased
    by a factor of 8 without, on average, requiring any extra space in the directory
    entries.
  prefs: []
  type: TYPE_NORMAL
- en: For medium files, up to about 4MB, the OS will allocate a single block with
    1,024 pointers to the blocks that store the file’s data. The OS continues to use
    the pointers found in the directory entry for the first few blocks of the file,
    and then it uses a block on the disk to hold the next group of block pointers.
    Generally, the last pointer in the directory entry holds the location of this
    block (see [Figure 14-6](ch14.xhtml#ch14fig06)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-6: Block list for medium files*'
  prefs: []
  type: TYPE_NORMAL
- en: For files larger than about 4MB, the filesystem switches to a three-tiered block
    scheme, which works for file sizes up to 4GB. In this scheme, the last pointer
    in the directory entry stores the location of a block of 1,024 pointers, and each
    pointer in this block holds the location of an additional block of 1,024 pointers,
    with each pointer in *this* block storing the location of a block that contains
    actual file data. See [Figure 14-7](ch14.xhtml#ch14fig07) for the details.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/14fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14-7: Three-level block list for large files (up to 4GB)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'One advantage to this tree structure is that it readily supports sparse files:
    an application can write to block 0 and block 100 of a file without having to
    allocate data blocks for every block between those two points. By placing a special
    block pointer value (typically `0`) in the intervening entries in the block list,
    the OS can determine whether a block isn’t present in the file. Should an application
    attempt to read a missing block in the file, the OS can simply return all `0`s
    for the empty block. Of course, once the application writes data to a block that
    hadn’t been previously allocated, the OS must copy the data to the disk and fill
    in the appropriate block pointer in the block list.'
  prefs: []
  type: TYPE_NORMAL
- en: As disks became larger, the 4GB file limit imposed by this scheme began to create
    some problems for certain applications, such as video editors, large database
    applications, and web servers. One could easily extend this scheme 1,000 times—to
    4TB—by adding another level to the block-list tree. The only problem with this
    approach is that the more levels of indirection you have, the slower random file
    access becomes, because the OS may have to read several blocks from the disk in
    order to get a single block of data. (When it has one level, it makes sense to
    cache the block-pointer list in memory, but with two and three levels, it’s impractical
    to do this for every file). Another way to extend the maximum size 4GB at a time
    is to use multiple pointers to second-tier file blocks (for example, have all
    or most of the original 8 to 16 pointers in the directory point at second-tier
    block-list entries rather than directly at file data blocks). Although there’s
    no current convention for extending beyond three levels, rest assured that as
    the need arises, OS designers will develop schemes for accessing large files efficiently.
    For example, 64-bit OSes can use 64-bit pointers, rather than 32-bit pointers,
    and eliminate the 4GB limitation.
  prefs: []
  type: TYPE_NORMAL
- en: '**14.8 Writing Software That Manipulates Data on a Mass Storage Device**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Understanding how different mass storage devices behave is important if you
    want to write high-performance software that manipulates files on these devices.
    Although modern OSes attempt to isolate applications from the physical realities
    of mass storage, an OS can only do so much for you. Furthermore, because an OS
    can’t predict how your particular application will access files on a mass storage
    device, it can’t tailor file access optimizations to your application; instead,
    its optimizations are geared toward applications exhibiting *typical* file access
    patterns. The less typical your application’s file I/O is, then, the less likely
    you’ll get the best performance out of the system. In this section, we’ll look
    at how you can coordinate your file access activities with the OS to achieve the
    best performance.
  prefs: []
  type: TYPE_NORMAL
- en: '***14.8.1 File Access Performance***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although disk drives and most other mass storage devices are often thought of
    as “random access” devices, the fact is that mass storage access is usually more
    efficient when done sequentially. Sequential access on a disk drive is relatively
    efficient because the OS can move the read/write head one track at a time (assuming
    the file appears in sequential blocks on the disk). This is much faster than accessing
    one block on the disk, moving the read/write head to some other track, accessing
    another block, moving the head again, and so on. Therefore, you should avoid random
    file access in an application if at all possible.
  prefs: []
  type: TYPE_NORMAL
- en: You should also try to read or write large blocks of data on each file access
    rather than reading or writing small amounts more frequently. There are two reasons
    for this. First, OS calls are not fast, so if you make half as many calls by reading
    or writing twice as much data on each access, the application will often run twice
    as fast. Second, the OS must read or write whole disk blocks. If your block size
    is 4,096 bytes, but you just write 2,000 bytes to some block and then seek to
    some other position in the file outside that block, the OS will actually have
    to read the entire 4,096-byte block from the disk, merge in the 2,000 bytes, and
    then finally write the entire 4,096 bytes back to the disk. Contrast this with
    a write operation that writes a full 4,096 bytes—in this case, the OS wouldn’t
    have to read the data from the disk first; it would only have to write the block.
    Writing full blocks improves disk access performance by a factor of 2, because
    writing partial blocks requires the OS to first read the block, merge the data,
    and then write the block; writing whole blocks renders the read operation unnecessary.
    Even if your application doesn’t write data in increments that are even multiples
    of the disk’s block size, writing large blocks improves performance. If you write
    16,000 bytes to a file in one write operation, the OS will still have to write
    the last block of those 16,000 bytes using a read-merge-write operation, but it
    will write the first three blocks using only write operations.
  prefs: []
  type: TYPE_NORMAL
- en: If you start with a relatively empty disk, the OS generally attempts to write
    the data for new files in sequential blocks. This organization is probably most
    efficient for future file access. However, as the system’s users create and delete
    files on the disk, the blocks of data for individual files may be distributed
    nonsequentially. In a very bad case, the OS may wind up allocating a few blocks
    here and a few blocks there all across the disk’s surface. As a result, even sequential
    file access can behave like slow random file access. As discussed previously,
    this kind of file fragmentation can dramatically decrease filesystem performance.
    Unfortunately, there’s no way for an application to determine if its file data
    is fragmented across the disk surface and, even if it could, there’s little it
    could do about the situation. Although there are utilities available to *defragment*
    the blocks on the disk’s surface, an application generally can’t request their
    execution (and “defragger” utilities are quite slow anyway).
  prefs: []
  type: TYPE_NORMAL
- en: Although applications rarely get the opportunity to defragment their data files
    during normal program execution, there are some things you can do to reduce the
    probability of your data files becoming fragmented. The best advice you can follow
    is to always write file data in large chunks. Indeed, if you can write the whole
    file in a single write operation, do so. In addition to speeding up access to
    the OS, writing large amounts of data tends to result in the allocation of sequential
    blocks. When you write small blocks of data to the disk, other applications in
    a multitasking environment could also be writing to the disk concurrently. In
    this case, the OS may interleave the block allocation requests for the files being
    written by several different applications, making it unlikely that a particular
    file’s data will be written sequentially. It is important to try to write a file’s
    data in sequential blocks, even if you plan to access portions of that data randomly,
    since searching for random records in a file written sequentially generally requires
    far less head movement than searching for random records in a file whose blocks
    are scattered all over.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re going to create a file and then access its blocks of data repeatedly,
    whether randomly or sequentially, try to preallocate the blocks on the disk. If
    you know, for example, that your file’s data will not exceed 1MB, you could write
    a block of one million `0`s to the disk before your application starts manipulating
    the file. By doing so, you help ensure that the OS will write your file to sequential
    blocks on the disk. Though you pay an initial price to write all those `0`s (an
    operation you wouldn’t normally do, presumably), the savings in read/write head-seek
    times could easily make up for it. This scheme is especially useful if an application
    is reading or writing two or more files concurrently (which would almost guarantee
    the interleaving of the blocks for the various files).
  prefs: []
  type: TYPE_NORMAL
- en: '***14.8.2 Synchronous and Asynchronous I/O***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because most mass storage devices are mechanical, and, therefore, subject to
    mechanical delays, applications that use them extensively have to wait for them
    to complete read/write operations. Most disk I/O operations are *synchronous*,
    meaning that an application that makes a call to the OS must wait until that I/O
    request is complete before continuing subsequent operations.
  prefs: []
  type: TYPE_NORMAL
- en: This is why most modern OSes also provide an *[asynchronous I/O](gloss01.xhtml#gloss01_19)*
    capability, in which the OS begins the application’s request and then returns
    control to the application without waiting for the I/O operation to complete.
    While the I/O operation proceeds, the application promises not to do anything
    with the data buffer specified for it. However, the application can do computation
    and schedule additional I/O operations, because the OS will notify it when the
    original request completes. This is especially useful when you’re accessing files
    on multiple disk drives in the system, which is usually possible only with SCSI
    and other high-end drives.
  prefs: []
  type: TYPE_NORMAL
- en: '***14.8.3 The Implications of I/O Type***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another important consideration for writing software that manipulates mass
    storage devices is the type of I/O you’re performing. *Binary I/O* is usually
    faster than *formatted text I/O*, because of the format of the data written to
    disk. For example, suppose you have an array of 16 integer values that you want
    to write to a file. To achieve this, you could use either of the following two
    C/C++ code sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The second sequence looks like it would run slower than the first because it
    uses a loop, rather than a single call, to step through each element of the array.
    But although the extra execution overhead of the loop does have a small negative
    impact on the execution time of the write operation, this efficiency loss is minor
    compared to the real problem with the second sequence. Whereas the first code
    sequence writes out a 64-byte memory image consisting of 16 32-bit integers to
    the disk, the second code sequence converts each of the 16 integers to a string
    of characters and then writes each string to the disk. This integer-to-string
    conversion process is relatively slow. Furthermore, the `fprintf()` function has
    to interpret the format string (`"%d"`) at runtime, which incurs an additional
    delay.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of formatted I/O is that the resulting file is both human-readable
    and easily read by other applications. However, if you’re using a file to hold
    data that is of interest only to your application, a more efficient approach might
    be to write the data as a memory image.
  prefs: []
  type: TYPE_NORMAL
- en: '***14.8.4 Memory-Mapped Files***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Memory-mapped files use the OS’s virtual memory capabilities to map memory addresses
    in the application space directly to blocks on the disk. Modern OSes have highly
    optimized virtual memory subsystems, so piggy-backing file I/O on top of the virtual
    memory subsystem results in very efficient file access. Furthermore, memory-mapped
    file access is easy. When you open a memory-mapped file, the OS returns a memory
    pointer to some block of memory. By simply accessing the memory locations referenced
    by this pointer, just as you would any other in-memory data structure, you can
    access the file’s data. This makes file access almost trivial, while often improving
    file manipulation performance, especially when file access is random.
  prefs: []
  type: TYPE_NORMAL
- en: One of the reasons that memory-mapped files are so much more efficient than
    regular files is that the OS reads the list of blocks belonging to memory-mapped
    files only once. It then sets up the system’s memory management tables to point
    at each block belonging to the file. After opening the file, the OS rarely has
    to read any file metadata from the disk, which greatly reduces superfluous disk
    access during random file access. It also improves sequential file access, though
    to a lesser degree. The OS doesn’t constantly have to copy data between the disk,
    internal OS buffers, and application data buffers.
  prefs: []
  type: TYPE_NORMAL
- en: Memory-mapped file access does have some disadvantages. First, you can’t map
    gigantic files entirely into memory, at least on older PCs and OSes that have
    a 32-bit address bus and set aside a maximum of 4GB per application. Generally,
    it isn’t practical to use a memory-mapped access scheme for files larger than
    256MB, though this has changed as more CPUs with 64-bit addressing capabilities
    have become available. It’s also not a good idea to use memory-mapped files when
    an application is already using most of the RAM physically present in the system.
    Fortunately, these two situations are not typical, so they don’t limit the use
    of memory-mapped files much.
  prefs: []
  type: TYPE_NORMAL
- en: A more common and significant issue is that when you first create a memory-mapped
    file, you have to tell the OS the file’s maximum size. If it’s impossible to determine
    the file’s final size, you’ll have to overestimate it and then truncate the file
    when you close it. Unfortunately, this wastes system memory while the file is
    open. Memory-mapped files work well when you’re manipulating files in read-only
    mode or simply reading and writing data in an existing file without extending
    the file’s size. Fortunately, you can always create a file using traditional file
    access mechanisms and then use memory-mapped file I/O to access the file later.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, almost every OS does memory-mapped file access differently, so it’s
    unlikely that memory-mapped file I/O code will be portable between OSes. Nevertheless,
    the code to open and close memory-mapped files is quite short, and it’s easy enough
    to provide multiple copies of the code for the various OSes you need to support.
    Of course, actually accessing the file’s data consists of simple memory accesses,
    and that’s independent of the OS. For more information on memory-mapped files,
    consult your OS’s API reference. Given their convenience and performance, you
    should seriously consider using memory-mapped files whenever possible in your
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**14.9 For More Information**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Silberschatz, Abraham, Peter Baer Galvin, and Greg Gagne. *Operating System
    Concepts*. 8th ed. Hoboken, NJ: John Wiley & Sons, 2009.'
  prefs: []
  type: TYPE_NORMAL
