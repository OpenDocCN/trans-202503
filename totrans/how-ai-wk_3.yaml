- en: '**3'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CLASSICAL MODELS: OLD-SCHOOL MACHINE LEARNING**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Beginning piano students don’t start with Liszt’s “La Campanella,” but “Mary
    Had a Little Lamb” or “Twinkle, Twinkle, Little Star.” The simpler pieces contain
    the basics of playing the piano, and mastering the basics allows students to progress
    over time. This principle holds in most areas of study, including artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reach our ultimate goal of understanding modern AI, we must begin in the
    “simpler” world of classical machine learning. What holds for the classical models
    is generally true for more advanced neural networks. This chapter explores three
    classical models: nearest neighbors, random forests, and support vector machines.
    Understanding these will prepare us for the neural networks of [Chapter 4](ch04.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-1](ch03.xhtml#ch03fig01) shows the training samples for a made-up
    dataset with two features (*x*[0] and *x*[1]) and three classes (circles, squares,
    and triangles). We saw a similar plot in [Chapter 1](ch01.xhtml); see [Figure
    1-2](ch01.xhtml#ch01fig02). As with the iris dataset, every shape in the figure
    represents a sample from the training set. [Figure 3-1](ch03.xhtml#ch03fig01)
    is the tool we’ll use to understand the nearest neighbors classical model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch03fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-1: A made-up training set with three classes and two features*'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the previous chapter, [*nearest neighbor*](glossary.xhtml#glo71)
    classifiers are the simplest of models—so simple that there’s no model to train;
    the training data *is* the model. To assign a class label to a new, unknown input,
    find the training sample closest to the unknown sample and return that sample’s
    label. That’s all there is to it. Despite their simplicity, nearest neighbor classifiers
    are quite effective if the training data represents what the model will encounter
    in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: As a natural extension to the nearest neighbor model, locate the *k* training
    samples nearest the unknown sample. *k* is often a number like 3, 5, or 7, though
    it can be any number. This type of model uses a majority voting system, so the
    assigned class label is the one that’s most common among the *k* training samples.
    If there’s a tie, select the label randomly. For example, if the model is contemplating
    the 5-nearest neighbors to an unknown sample, and two are class 0 while another
    two are class 3, then assign the label by choosing randomly between 0 and 3; on
    average, you’ll make the correct choice 50 percent of the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the nearest neighbor concept to classify some unknown inputs. [Figure
    3-2](ch03.xhtml#ch03fig02) shows the training samples again, along with two unknown
    samples: the diamond and the pentagon. We want to assign these samples to one
    of the three classes: circle, square, or triangle. The nearest neighbor approach
    says to locate the training sample closest to each unknown sample. For the diamond,
    that’s the square to its upper left; for the pentagon, it appears to be the triangle
    to the upper right. Therefore, a nearest neighbor classifier assigns class square
    to the diamond and class triangle to the pentagon.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch03fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-2: Classifying unknown samples*'
  prefs: []
  type: TYPE_NORMAL
- en: I suspect you’ve noticed the lines connecting the unknown samples in [Figure
    3-2](ch03.xhtml#ch03fig02) to the three nearest training samples. These are the
    samples to use if *k* is 3\. In this case, the classifier would again assign class
    square to the diamond, because all three of the nearest training samples are squares.
    For the pentagon, two of the three nearest neighbors are triangles and one is
    a square, so it would also again assign class triangle to the pentagon.
  prefs: []
  type: TYPE_NORMAL
- en: This example uses two-dimensional feature vectors, *x*[0] and *x*[1], so we
    can visualize the process. We’re not restricted to models with only two features;
    we can have dozens or even hundreds. The idea of “nearest” (distance) still has
    mathematical meaning even when there are too many features to graph. Indeed, many
    mathematical concepts qualify as distance measures, and in practice, nearest neighbor
    classifiers may use any of the measures depending on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s return to [Chapter 1](ch01.xhtml)’s MNIST digits dataset.
    The samples are small, grayscale images of the digits 0 through 9 that we unravel
    into vectors of 784 elements. Therefore, each digit sample in the training set
    is a single point in a 784-dimensional space, just as in the previous example
    each sample was a point in a 2-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: The full MNIST dataset has 60,000 training examples, meaning the training space
    consists of 60,000 points scattered throughout the 784-dimensional space (not
    quite, but more on that soon). It also has 10,000 test samples that we can use
    to evaluate the nearest neighbor model. I trained 1-nearest neighbor models using
    all 60,000 training samples, then 6,000 samples, then 600, before ending with
    a mere 60\. Sixty samples in the training set implies about six examples of each
    digit. I say “about” because I sampled the training set randomly, so there might
    be eight of one digit and only three of another. In every case, I tested the model
    using all 10,000 test samples, thereby mimicking using the model in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 3-1](ch03.xhtml#ch03tab1) shows the model’s performance as the number
    of training examples changed.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 3-1:** Changing the Training Set Size'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Training set size** | **Accuracy (%)** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 60,000 | 97 |'
  prefs: []
  type: TYPE_TB
- en: '| 6,000 | 94 |'
  prefs: []
  type: TYPE_TB
- en: '| 600 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| 60 | 66 |'
  prefs: []
  type: TYPE_TB
- en: Recall that accuracy is the percentage of the test samples that the model classified
    correctly by assigning the correct digit label, 0 through 9\. When using the entire
    training set the model is correct 97 times out of 100, on average. Even when the
    training set is made 10 times smaller, the accuracy is still 94 percent. With
    600 training examples—about 60 per digit—the accuracy falls to 86 percent. It’s
    only when the training set shrinks to a mere six examples of each digit, on average,
    that the accuracy falls dramatically to 66 percent.
  prefs: []
  type: TYPE_NORMAL
- en: However, before we’re too harsh on our nearest neighbor model, remember that
    there are 10 digit classes, so random guessing will be correct, on average, about
    1 time in 10, for an accuracy of about 10 percent. In this light, even the 60-sample
    model is six times better than guessing randomly. Let’s explore this phenomenon
    a bit to see if we can gain some insight into why the nearest neighbor model does
    well with so little training data.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you’re alone in a basketball arena, sitting in the middle of the court.
    A speck of dust is suspended in the air somewhere in the arena. For convenience,
    the speck stays fixed in its position. Now imagine 59 more specks of dust inhabiting
    the air. Those 60 specks of dust are the 60 digit samples in our training set,
    and the arena is the three-dimensional world in which the digit image vectors
    live.
  prefs: []
  type: TYPE_NORMAL
- en: Now imagine a new speck of dust has appeared right in front of your nose. It’s
    a new digit vector you want to classify. The nearest neighbor model calculates
    the distance between that speck of dust and the 60 specks whose digit labels you
    know. The closest speck of dust to the new one is below the rim of the basket
    you’re facing, at a distance of 47 feet (14 meters). It’s a three, so the model
    returns a label of 3\. Is it reasonable to think that the closest speck represents
    the proper label for the unknown sample? After all, there are only 60 specks of
    dust in the whole arena.
  prefs: []
  type: TYPE_NORMAL
- en: We need to consider two competing effects to provide a reasonable answer to
    this question. First, we should answer “no” because it seems silly to believe
    that we can represent the giant volume of the arena with 60 specks of dust. There’s
    too little data in the training set to fill the arena’s space. This observation,
    known as the [*curse of dimensionality*](glossary.xhtml#glo25), refers to the
    fact that as the number of dimensions increases, so too, at a very rapid rate,
    does the number of samples needed to fill the space. In other words, the number
    of points increases rapidly, meaning the number of training samples necessary
    to represent the space increases rapidly—exponentially, to be more precise. The
    curse of dimensionality is one of the banes of classical machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: The curse of dimensionality says we should have no hope of properly classifying
    digits when we have only 60 training samples and 784 dimensions . . . yet our
    nearest neighbor classifier still works. Not very well, but better than random
    guessing. Why? The reason has to do with the digits dataset and how similar examples
    of the different classes are to each other. All examples of fives look like a
    5; if they didn’t, we wouldn’t recognize them as fives. Therefore, while there
    are 784 dimensions to the space of digits, most digits in a class will land relatively
    close to that class’s other digits. In other words, the specks of dust representing
    fives are likely clustered or grouped near each other, probably in a thin, tube-like
    region that snakes its way through the arena. The other digits are likely grouped
    similarly. Because of this, the nearest sample has a better chance of being from
    the same digit class than we initially suspected when considering the curse of
    dimensionality. Based on this observation, we upgrade our “no” answer to a wishy-washy
    “probably.”
  prefs: []
  type: TYPE_NORMAL
- en: We talk about this effect mathematically by saying that the digit data lies
    on a [*manifold*](glossary.xhtml#glo65) with an effective dimensionality that
    is well below the 784 dimensions of the vectors representing the digits. That
    data often lies on lower-dimensional manifolds is a boon if we can make use of
    that information. The nearest neighbor model uses the information because the
    training data is the model. Later in the book, when we discuss convolutional neural
    networks, we’ll understand that such models learn new ways to represent their
    inputs, which is akin to learning how to represent the lower-dimensional manifold
    on which the data lives.
  prefs: []
  type: TYPE_NORMAL
- en: Before we get too excited about how well our nearest neighbor classifier performs
    with the digits dataset, though, let’s bring ourselves back to reality by attempting
    to classify real images. The CIFAR-10 dataset consists of 50,000 small 32×32-pixel
    color images from 10 different classes, including a mix of vehicles, like airplanes,
    cars, and trucks, and animals, like dogs, cats, and birds. Unraveling each of
    these images creates a vector of 3,072 elements, so we’re asking our classifier
    to separate images in a 3,072-dimensional space. [Table 3-2](ch03.xhtml#ch03tab2)
    shows how it fares.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 3-2:** Classifying CIFAR-10 with Nearest Neighbor'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Training set size** | **Accuracy (%)** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 50,000 | 35.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 5,000 | 27.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 500 | 23.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | 17.5 |'
  prefs: []
  type: TYPE_TB
- en: As with MNIST, random guessing leads to an accuracy of 10 percent. While our
    classifier performs better than this with all variations of training set size,
    its best accuracy is little more than 35 percent—nowhere near the 97 percent achieved
    with MNIST. Sobering realizations like this led many in the machine learning community
    to lament that generic image classification might be beyond our grasp. Thankfully,
    it isn’t, but none of the classical machine learning models do it well.
  prefs: []
  type: TYPE_NORMAL
- en: If we think in terms of manifolds—the idea that data often lives in a lower-dimensional
    space than the dimensionality of the data itself—then these results aren’t surprising.
    CIFAR-10 contains real-world photographs, often referred to as natural images.
    Natural images are far more complex than simple images like MNIST digits, so we
    should expect them to exist in a higher-dimensional manifold and consequently
    be harder to learn to classify. As it happens, there are numerical approaches
    to estimating the true dimensionality of data. For MNIST, even though the images
    live in a 784-dimensional space, the data is closer to 11-dimensional. For CIFAR-10,
    the intrinsic dimensionality is closer to 21 dimensions, so we expect to need
    far more training data to perform on par with MNIST.
  prefs: []
  type: TYPE_NORMAL
- en: Nearest neighbor models aren’t used often these days. Two issues contribute
    to why. First, while training a nearest neighbor model is effectively instantaneous
    because there’s nothing to train, *using* a nearest neighbor model is slow because
    we have to calculate the distance between the unknown sample and each of the training
    set samples. This calculation time grows as the square of the number of samples
    in the training set. The more training data we have, the better we expect the
    model to perform, but the slower it runs. Double the size of the training set,
    and the search time increases by a factor of four.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decades of study of nearest neighbor classifiers have uncovered all manner
    of tricks to mitigate the time it takes to find the nearest neighbor, or nearest
    *k* neighbors, but the effect remains: increasing the number of training samples
    increases the time it takes to use the classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: The second issue is common to all classical machine learning models, as well
    as the traditional neural networks we’ll discuss in [Chapter 4](ch04.xhtml). These
    models are holistic, meaning they interpret their input vectors as a single entity
    without parts. This is *not* the right thing to do in many cases. For example,
    writing a four uses multiple strokes, and there are definite parts that distinguish
    the four from an eight. Classical machine learning models don’t explicitly learn
    about these parts or where they appear, or that they might appear in multiple
    locations. Modern convolutional neural networks, however, do learn these things.
  prefs: []
  type: TYPE_NORMAL
- en: In sum, nearest neighbor models are straightforward to understand and trivial
    to train, but slow to use and unable to explicitly understand structure in their
    inputs. Let’s change gears to contemplate the forest and the trees.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: We briefly explored decision trees, comprising a series of yes/no questions
    asked about an unknown sample, in [Chapter 1](ch01.xhtml). You begin at the root
    node and traverse the tree by answering the node’s question. If the answer is
    “yes,” move down one level to the left. If the answer is “no,” move down to the
    right. Continue answering questions until you reach a leaf (a node with no question),
    and assign the unknown sample whatever label is in the leaf node.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are deterministic; once constructed, they don’t change. Therefore,
    traditional decision tree algorithms return the same decision tree for the same
    training set. More often than not, the tree doesn’t work all that well. If that
    happens, is there anything we can do? Yes! We can grow a forest of trees.
  prefs: []
  type: TYPE_NORMAL
- en: But if decision trees are deterministic, won’t the forest be nothing more than
    the same tree, over and over, like a mass of clones? It will, if we don’t do anything
    clever along the way. Fortunately, humans are clever. Researchers realized around
    the year 2000 that introducing randomness produces a forest of unique trees, each
    with its own strengths and weaknesses, but collectively better than any single
    tree. A [*random forest*](glossary.xhtml#glo83) is a collection of decision trees,
    each randomly different from the others. The forest’s prediction is a combination
    of its trees’ predictions. Random forests are a manifestation of the wisdom of
    crowds.
  prefs: []
  type: TYPE_NORMAL
- en: Using randomness to build a classifier seems counterintuitive at first. If on
    Tuesday we present the model with sample X and it tells us that sample X is a
    member of class Y, then we don’t want it to tell us that it’s a member of class
    Z if we happen to present the same sample on Saturday. Fortunately, the randomness
    of a random forest doesn’t work that way. Give a trained forest sample X as input,
    and it always gives us class Y as output, even if it’s February 29.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three steps go into growing a random forest: bagging (also called bootstrapping),
    random feature selection, and ensembling. Bagging and random feature selection
    help combat overfitting, a concept mentioned in [Chapter 1](ch01.xhtml). Single
    decision trees are prone to overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: All three steps work together to grow a forest of decision trees whose combined
    outputs produce a (hopefully) better-performing model. Explainability is the price
    paid for this gain in power. A single decision tree explains itself by the series
    of questions and answers that produce its output. With dozens or hundreds of decision
    trees combining their output, explainability goes out the window, but we can live
    with that in many cases.
  prefs: []
  type: TYPE_NORMAL
- en: As I’ve already mentioned several times, the training set is key to conditioning
    the model. This remains true with random forests. We have as a starting point
    a training set. As we grow the forest, decision tree by decision tree, we use
    the existing training set to create tree-specific training sets unique to the
    current decision tree. This is where bagging comes in.
  prefs: []
  type: TYPE_NORMAL
- en: '[*Bagging*](glossary.xhtml#glo12) refers to constructing a new dataset from
    the current dataset by random sampling with replacement. The phrase “with replacement”
    means we might select a training sample more than once or not at all. This technique
    is used in statistics to understand a measurement’s bounds. We’ll use the following
    example dataset of test scores to figure out what that means:'
  prefs: []
  type: TYPE_NORMAL
- en: 95, 88, 76, 81, 92, 70, 86, 87, 72
  prefs: []
  type: TYPE_NORMAL
- en: One way to assess a class’s performance on the test is to calculate the average
    score by taking the sum of all the scores divided by the number of scores. The
    sum is 747, and there are 9 scores, giving us an average of 83.
  prefs: []
  type: TYPE_NORMAL
- en: Collectively, the test scores are a sample from a mythical parent process that
    generates test scores for the particular test taken. This isn’t a common way to
    think about test scores, but it’s a machine learning way to think about what a
    dataset represents. The test scores from another group of students represent another
    sample from the parent process for this test. If we have many classes’ worth of
    test scores, we can get an idea about the true average test score, or at least
    the range over which we expect to find that average score, with a high degree
    of confidence.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could give the test to many different classes to get multiple average scores,
    one per class, but instead we’ll use bagging to create new datasets from the collection
    of test scores we do have and look at their averages. To do that, we pick values
    from the collection of test scores at random, not caring if we’ve already picked
    this particular score or never pick that one. Here are six such bootstrapped datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: 86, 87, 87, 76, 81, 81, 88, 70, 95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 87, 92, 76, 87, 87, 76, 87, 92, 92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 95, 70, 87, 92, 70, 92, 72, 70, 72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 88, 86, 87, 70, 81, 72, 86, 95, 70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 86, 86, 92, 86, 87, 86, 70, 81, 87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 76, 88, 88, 88, 88, 72, 86, 95, 70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The respective averages of each are 83.4, 86.2, 80.0, 81.7, 84.6, and 83.4 percent.
    The lowest is 80.0 percent, and the highest is 86.2 percent. This gives us some
    reason to believe that a large number of samples will produce an average more
    or less in that range.
  prefs: []
  type: TYPE_NORMAL
- en: This is how a statistician might use bagging. For us, the critical part is the
    six new datasets bootstrapped from the original dataset. When growing a random
    forest, every time we need a new decision tree, we’ll first use bagging to produce
    a new dataset, then train the decision tree using that dataset, not the original.
    Notice that many of the six datasets have repeated values. For example, dataset
    1 used both 81 and 87 twice, but never 72\. This randomization of the given dataset
    helps create decision trees that behave differently from one another yet are aligned
    with what the original dataset represents.
  prefs: []
  type: TYPE_NORMAL
- en: The second trick a random forest uses is to train the decision tree on a randomly
    selected set of features. Let’s use the toy dataset in [Table 3-3](ch03.xhtml#ch03tab3)
    to understand what that means. As always, each row is a feature vector, a sample
    for which we know the proper class label. The columns are the values of that feature
    for each sample.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 3-3:** A Toy Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '| **#** | ***x***[**0**] | ***x***[**1**] | ***x***[**2**] | ***x***[**3**]
    | ***x***[**4**] | ***x***[**5**] |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0.52 | 0.95 | 0.81 | 0.78 | 0.97 | 0.36 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 0.89 | 0.37 | 0.66 | 0.55 | 0.75 | 0.45 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 0.49 | 0.98 | 0.49 | 0.39 | 0.42 | 0.24 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 0.43 | 0.51 | 0.90 | 0.78 | 0.19 | 0.22 |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | 0.51 | 0.16 | 0.11 | 0.48 | 0.34 | 0.54 |'
  prefs: []
  type: TYPE_TB
- en: '| **6** | 0.48 | 0.99 | 0.62 | 0.58 | 0.72 | 0.42 |'
  prefs: []
  type: TYPE_TB
- en: '| **7** | 0.80 | 0.84 | 0.72 | 0.26 | 0.93 | 0.23 |'
  prefs: []
  type: TYPE_TB
- en: '| **8** | 0.50 | 0.70 | 0.13 | 0.35 | 0.96 | 0.82 |'
  prefs: []
  type: TYPE_TB
- en: '| **9** | 0.70 | 0.54 | 0.62 | 0.72 | 0.14 | 0.53 |'
  prefs: []
  type: TYPE_TB
- en: What does this dataset represent? I have no idea; it’s made up. My cheeky answer
    is a good reminder that machine learning models don’t understand what their datasets
    represent. They process numbers without context. Is it a pixel value? The number
    of square feet in a house? The crime rate of a county per 100,000 people? It doesn’t
    matter to the machine learning model—it’s all just numbers.
  prefs: []
  type: TYPE_NORMAL
- en: This toy dataset consists of nine feature vectors, each with six features, *x*[0]
    through *x*[5]. The forest’s decision trees use a randomly selected subset of
    the six features. For example, say we randomly keep features *x*[0], *x*[4], and
    *x*[5]. [Table 3-4](ch03.xhtml#ch03tab4) shows the dataset now used to train the
    decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 3-4:** A Random Collection of Features'
  prefs: []
  type: TYPE_NORMAL
- en: '| **#** | ***x***[**0**] | ***x***[**4**] | ***x***[**5**] |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0.52 | 0.97 | 0.36 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 0.89 | 0.75 | 0.45 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 0.49 | 0.42 | 0.24 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 0.43 | 0.19 | 0.22 |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | 0.51 | 0.34 | 0.54 |'
  prefs: []
  type: TYPE_TB
- en: '| **6** | 0.48 | 0.72 | 0.42 |'
  prefs: []
  type: TYPE_TB
- en: '| **7** | 0.80 | 0.93 | 0.23 |'
  prefs: []
  type: TYPE_TB
- en: '| **8** | 0.50 | 0.96 | 0.82 |'
  prefs: []
  type: TYPE_TB
- en: '| **9** | 0.70 | 0.14 | 0.53 |'
  prefs: []
  type: TYPE_TB
- en: Each decision tree in the forest has been trained on a bootstrapped version
    of the dataset using only a subset of the available features. We’ve used randomness
    twice to grow a forest of trees that are all subtly different from each other,
    in both what data they’re trained on and which features they pay attention to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a forest, how do we use it? Enter the last of the three pieces:
    ensembling. Musically, an ensemble is a collection of musicians playing diverse
    instruments. The random forest is also an ensemble, with each decision tree a
    different musician playing a different instrument.'
  prefs: []
  type: TYPE_NORMAL
- en: A musical ensemble produces a single output, the music, by combining the notes
    played by each instrument. Likewise, a random forest produces a single output,
    a class label, by combining the labels produced by each decision tree, typically
    by voting like a *k*-nearest neighbors classifier. We assign the winning label
    to the input.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we want to use the random forest to classify sample X, and there
    are 100 trees in the random forest (already trained), we give each tree sample
    X. The trees know which subsets of sample X’s features to use to arrive at a leaf
    with a label. We now have 100 possible class labels, the output from the forest’s
    100 decision trees. If 78 of the trees assign sample X to class Y, the random
    forest proclaims sample X to be an instance of class Y.
  prefs: []
  type: TYPE_NORMAL
- en: The random assignment of features to trees, combined with bootstrapped datasets
    and ensemble voting, gives a random forest its power. Ensembling is an intuitively
    attractive idea that isn’t restricted to random forests. Nothing stops us from
    training multiple model types on the same dataset and then combining their predictions
    in some way to arrive at a joint conclusion about an input sample. Each of the
    models will have its own strengths and weaknesses. When combined, the strengths
    tend to enhance the output quality, making the sum greater than the parts.
  prefs: []
  type: TYPE_NORMAL
- en: We have one more classical machine learning model to investigate, the support
    vector machine (SVM). After that, we’ll pit the models against each other to gain
    intuition about how they behave and provide a baseline against which we can compare
    the performance of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand support vector machines is to understand four concepts: margins,
    support vectors, optimization, and kernels. The math is a bit hairy, even for
    math people, but we’ll set that aside and focus instead on gaining a conceptual
    understanding.'
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines are best understood visually, so we’ll begin with the
    example toy dataset in [Figure 3-3](ch03.xhtml#ch03fig03). This is a two-class
    dataset (circles and squares) with two-dimensional feature vectors, features *x*[0]
    and *x*[1].
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch03fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-3: A two-class toy dataset with two features, x[0] and x[1]*'
  prefs: []
  type: TYPE_NORMAL
- en: A classifier for this dataset is straightforward to construct because a line
    easily separates the dataset by class, with all the squares above it and to the
    right and all the circles below and to the left. But where should it go? There
    are an infinite number of lines that we might use. For example, we might pass
    the line just below all the squares. That line separates the classes, but if we
    encounter a sample from class square that lands just below the line when we use
    the classifier, we’ll make a mistake and assign the sample to class circle because
    it’s below the line we declared separates the classes. Similarly, if we place
    the line just above all the circles, we might call a new sample that’s actually
    a circle a square because it landed slightly above that line.
  prefs: []
  type: TYPE_NORMAL
- en: Given what we know based on the training data, we should place the separating
    line as far from each group as possible. Here’s where the concept of a margin
    comes into play. SVMs seek to maximize the margin between the two groups, meaning
    finding the place with the widest separation between classes. When they have the
    maximum margin, they place the boundary, here a line, in the middle of the margin
    because that’s the most sensible thing to do based on the information contained
    in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-4](ch03.xhtml#ch03fig04) shows the training data with three additional
    lines. The dashed lines define the margin, and the heavy continuous line marks
    the boundary placed by the SVM to maximize the distance between classes. This
    is the best position for the line to minimize labeling errors between the two
    classes. This, in a nutshell, is all an SVM does.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch03fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-4: The maximal margin separating line (heavy) and maximum margins
    (dashed)*'
  prefs: []
  type: TYPE_NORMAL
- en: The three other parts of an SVM—support vectors, optimization, and kernels—are
    used to find the margins and the separating line. In [Figure 3-4](ch03.xhtml#ch03fig04),
    notice that the dashed lines pass through some of the data points. These points
    are the support vectors that the algorithm finds to define the margin. Where do
    those support vectors come from? Recall that the figure’s points represent specific
    feature vectors in the training set. Support vectors are members of the training
    set found via an optimization algorithm. Optimization involves finding the best
    of something according to some criteria. The optimization algorithm used by an
    SVM locates the support vectors that define the maximum margin and, ultimately,
    the separating line. In [Chapter 1](ch01.xhtml), we used an optimization algorithm
    when we discussed fitting data to a curve, and we’ll use one again when training
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re almost there; we have only one SVM concept remaining: kernels. As opposed
    to the popcorn variety or the kernel at the heart of your computer’s operating
    system, mathematical kernels relate two things—here, two feature vectors. The
    example in [Figure 3-4](ch03.xhtml#ch03fig04) uses a linear kernel, meaning it
    uses the training data feature vectors as they are. Support vector machines admit
    many kinds of kernels to relate two feature vectors, but the linear kernel is
    the most common. Another kind, called a Gaussian kernel (or, even more verbose
    and impressive, a radial basis function kernel), often helps in situations where
    the linear kernel fails because the feature vectors are in a different kind of
    relationship to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: The kernel transforms the feature vectors into a different representation, an
    idea central to what convolutional neural networks do. One of the issues that
    made classical machine learning stumble for so long is that the data supplied
    to the models was too complex in its raw form for the model to make meaningful
    distinctions between classes. This is related to the idea of manifolds and intrinsic
    dimensionality introduced in our discussion of nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Classical machine learning practitioners spent considerable effort trying to
    minimize the number of features needed by a model, paring the features down to
    the minimal set necessary for the model to distinguish between classes. This approach
    was termed *feature selection* or *dimensionality reduction*, depending on the
    algorithm used. Similarly, especially with SVMs, they used kernels to map the
    given feature vectors to a new representation, making separating classes easier.
    These approaches were human-led endeavors; we selected the features or the kernels
    in the hopes that they’d make the problem more manageable. But, as we’ll learn,
    modern deep learning lets the data speak for itself when learning new representations
    of the information the data contains.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, training a support vector machine means locating good values for
    the parameters related to the kernel used. If the kernel is linear, as in the
    previous example, there’s only one value to find, universally called *C*. It’s
    a number, like 1 or 10, affecting how well the support vector machine performs.
    If using the Gaussian kernel, we have *C* and another parameter, known by the
    Greek letter *γ* (gamma). The art of training an SVM involves finding the magic
    values that work best for the dataset at hand.
  prefs: []
  type: TYPE_NORMAL
- en: The magic values used by a model are its [*hyperparameters*](glossary.xhtml#glo55).
    Neural networks have many hyperparameters; even more than SVMs. However, my experience
    has taught me that it’s often easier to tune a neural network—especially a modern
    deep neural network—than a support vector machine. I freely confess my bias here;
    others might disagree.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines are mathematically elegant, and practitioners use that
    elegance to tweak the hyperparameters and the kernel used, along with a suite
    of old-school data preparation approaches, to construct a well-performing model
    that works well on data in the wild. Every step of this process relies on the
    intuition and experience of the human building the model. If they’re knowledgeable
    and experienced, they’ll likely succeed if the dataset is amenable to such a model,
    but success isn’t assured. On the other hand, deep neural networks are big, kind
    of clunky, and live or die by the raw data they’re fed. That said, by coming to
    the problem with a minimal set of assumptions, neural networks can generalize
    over elements of the dataset that humans cannot fathom, which I think is often
    why modern neural networks can do what was previously believed to be next to impossible.
  prefs: []
  type: TYPE_NORMAL
- en: 'SVMs are binary classifiers: they distinguish between two classes, as in the
    dataset in [Figure 3-3](ch03.xhtml#ch03fig03). But sometimes we need to distinguish
    between more than two classes. How can we do that with an SVM?'
  prefs: []
  type: TYPE_NORMAL
- en: We have two options for generalizing SVMs to multiclass problems. Assume we
    have 10 classes in the dataset. The first generalization approach trains 10 SVMs,
    the first of which attempts to separate class 0 from the other nine classes. The
    second likewise attempts to separate class 1 from the remaining nine, and so on,
    giving us a collection of models, each trying to separate one class from all the
    others. To classify an unknown sample, we give the sample to each SVM and return
    the class label of the model that produced the largest decision function value—the
    [*metric*](glossary.xhtml#glo66), or measurement, the SVM uses to decide its confidence
    in its output. This option is known as [*one-versus-rest*](glossary.xhtml#glo78)
    or *one-versus-all*. It trains as many SVMs as there are classes.
  prefs: []
  type: TYPE_NORMAL
- en: The other option is [*one-versus-one*](glossary.xhtml#glo77), which trains a
    separate SVM for each possible pair of classes. The unknown sample is given to
    each model, and the class label that shows up most often is assigned to it. One-versus-one
    isn’t practical if the number of classes becomes too large. For example, for the
    10 classes in CIFAR-10, we’d need 45 different SVM machines. And if we tried this
    approach with the 1,000 classes in the ImageNet dataset, we’d be waiting a long
    time for the 499,500 different SVMs to train.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines were well suited to the computing power commonly available
    in the 1990s and early 2000s, which is why they held neural networks at bay for
    so long. However, with the advent of deep learning, there’s little reason to resort
    to an SVM (in my opinion).
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s test the three classical models explored in this chapter using an open
    source dataset consisting of dinosaur footprint outlines that comes from the 2022
    paper “A Machine Learning Approach for the Discrimination of Theropod and Ornithischian
    Dinosaur Tracks” by Jens N. Lallensack, Anthony Romilio, and Peter L. Falkingham.
    The footprint images were released under the Creative Commons CC BY 4.0 license,
    which allows reuse with attribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-5](ch03.xhtml#ch03fig05) contains samples from the dataset. Theropod
    footprints (think *T. rex*) are in the top row, and ornithischian footprints (think
    duckbilled dinos like hadrosaurs) are at the bottom. The images used by the models
    were inverted to be white on a black background, rescaled to 40×40 pixels, and
    unraveled to become 1,600-dimensional vectors. The dataset is small by modern
    standards, with 1,336 training samples and 335 test samples.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/ch03fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-5: Theropod (top) and ornithischian (bottom) footprints*'
  prefs: []
  type: TYPE_NORMAL
- en: 'I trained the following models:'
  prefs: []
  type: TYPE_NORMAL
- en: Nearest neighbor (*k* = 1, 3, 7)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A random forest with 300 trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A linear support vector machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A radial basis function support vector machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After training, I tested the models with the held-out test set. I also timed
    how long it took to train each model and to test each model after training. Using
    a model after training is [*inference*](glossary.xhtml#glo57), meaning I tracked
    the inference time on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*This isn’t a programming book, but if you’re familiar with programming, especially
    Python, feel free to contact me at* [rkneuselbooks@gmail.com](mailto:rkneuselbooks@gmail.com)
    *and I’ll send you the dataset and code.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 3-5](ch03.xhtml#ch03tab5) shows the results. Evaluating how well a model
    works is, as you might expect, a critical component of the machine learning process.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 3-5:** Classifying Dinosaur Footprints'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **ACC** | **MCC** | **Train** | **Test** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| RF300 | 83.3 | 0.65 | 1.5823 | 0.0399 |'
  prefs: []
  type: TYPE_TB
- en: '| RBF SVM | 82.4 | 0.64 | 0.9296 | 0.2579 |'
  prefs: []
  type: TYPE_TB
- en: '| 7-NN | 80.0 | 0.58 | 0.0004 | 0.0412 |'
  prefs: []
  type: TYPE_TB
- en: '| 3-NN | 77.6 | 0.54 | 0.0005 | 0.0437 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-NN | 76.1 | 0.50 | 0.0004 | 0.0395 |'
  prefs: []
  type: TYPE_TB
- en: '| Linear SVM | 70.7 | 0.41 | 2.8165 | 0.0007 |'
  prefs: []
  type: TYPE_TB
- en: 'The first column on the left identifies the model: from top to bottom, random
    forest, radial basis function support vector machine, nearest neighbors (with
    7, 3, and 1 neighbor), and linear support vector machine.'
  prefs: []
  type: TYPE_NORMAL
- en: The ACC and MCC columns are metrics calculated from the confusion matrix, the
    single most crucial part of the machine learning practitioner’s toolbox when evaluating
    a model (see [Chapter 1](ch01.xhtml)). For binary classifiers like the ones we
    have here, the confusion matrix counts the number of times a theropod test sample
    was correctly identified, the same for ornithischian test samples, and the number
    of times one was confused for the other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visually, the confusion matrix for a binary model looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Ornithischian** | **Theropod** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Ornithischian** | TN | FP |'
  prefs: []
  type: TYPE_TB
- en: '| **Theropod** | FN | TP |'
  prefs: []
  type: TYPE_TB
- en: 'The rows are the actual class label from the held-out test set. The columns
    are the labels assigned by the models. The cells are the counts of the number
    of times each combination of actual label and model-assigned label happened. The
    letters are the standard way to refer to what the numbers in the cells mean: TN
    is [*true negative*](glossary.xhtml#glo98), TP is [*true positive*](glossary.xhtml#glo99),
    FP is *false positive*, and FN is *false negative*. For the dinosaur footprint
    models, theropod is class 1, the “positive” class, making ornithischian class
    0, or the “negative” class.'
  prefs: []
  type: TYPE_NORMAL
- en: The number of times the model called an ornithischian footprint “ornithischian”
    is the TN count. Similarly, the TP count represents the number of times the model
    was right about a theropod footprint. The goal is to get TN and TP as high as
    possible while making FP and FN, the mistakes, as low as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Table 3-5](ch03.xhtml#ch03tab5), ACC refers to the accuracy: how many times
    was the classifier’s assigned label correct? While accuracy is the most natural
    metric to consider, it isn’t always the best, especially if the number of examples
    per class isn’t nearly equal. The random forest performed the best in terms of
    accuracy, correctly labeling more than 83 out of every 100 test images. The linear
    SVM was the worst; it was right only about 71 times out of 100\. Random guessing
    would be correct about 50 percent of the time because we have two classes, though,
    so even the linear SVM was learning from the footprint images. We define the accuracy
    in terms of the cells of the confusion matrix by adding TP and TN and dividing
    that sum by the sum of all four cells.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The MCC column, which stands for *Matthews correlation coefficient*, introduces
    a new metric. It’s a different combination of the four numbers in the confusion
    matrix. MCC is my favorite metric for classifiers, and it is increasingly understood
    to be the best single-number measure of how well a model performs. (These metrics
    apply to more advanced deep learning models as well.) [Table 3-5](ch03.xhtml#ch03tab5)
    is sorted by MCC, which, for this example, also happens to sort by ACC. For a
    binary model, the lowest possible MCC is –1, and the highest is 1\. Random guessing
    gives an MCC of 0\. An MCC of 1 means the model makes no mistakes. An MCC of –1,
    which never actually happens in practice, means that the model is perfectly wrong:
    in our case, it would label all theropod tracks ornithischian and all ornithischian
    tracks theropod. If you have a perfectly wrong classifier, swap the output labels
    to make it perfectly right.'
  prefs: []
  type: TYPE_NORMAL
- en: The Train and Test columns list times in seconds. The Train column tells us
    how long it took to train the model before using it. The nearest neighbor models
    take virtually no time, a mere fraction of a millisecond, because there’s nothing
    to train. Recall that a nearest neighbor model is the training set itself; there
    is no model to condition to approximate the data in some way.
  prefs: []
  type: TYPE_NORMAL
- en: The slowest model was the linear SVM. Curiously, the more complex radial basis
    function model trained in roughly one-third the time (a difference that can be
    attributed to how such models are implemented in code). The next slowest model
    to train was the random forest. This makes sense because there were 300 decision
    trees in the forest, and each of them had to be trained independently.
  prefs: []
  type: TYPE_NORMAL
- en: The inference time, in the Test column, was roughly the same between the nearest
    neighbor and random forest models. The SVM models were respectively slow (RBF)
    and very fast (linear), again reflecting differences in the implementation. Notice
    that the nearest neighbor models take longer to use than to train. This is the
    reverse of the usual scenario, especially for neural networks, as we’ll see later
    in the book. Typically, training is slow but needs to be done only once, while
    inference is fast. For nearest neighbor models, the larger the training set, the
    slower the inference time—a significant strike against them.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main things to take away from this exercise: a general understanding
    of the performance of the classical models, which we’ll use as a baseline against
    which to compare a neural network in [Chapter 4](ch04.xhtml), and that even classical
    models can do well on this particular dataset. Their performance was on par with
    that of human experts (meaning paleontologists), who also labeled the dinosaur
    footprint outlines. According to the original paper by Lallensack et al. from
    which the dinosaur dataset was taken, the human experts were correct only 57 percent
    of the time. They were also allowed to label tracks as “ambiguous,” a luxury the
    models don’t have; the models always make a class assignment, with no “I don’t
    know” option. We can coerce some model types into making such statements, but
    the classical models of this chapter are not well suited to that.'
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: Are the classical models symbolic AI or connectionism? Are they AI at all? Do
    they learn, or are they merely mathematical tricks? My answers to these questions
    follow.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.xhtml), I characterized the relationship between AI, machine
    learning, and deep learning as a series of nested concepts, with deep learning
    a form of machine learning and machine learning a form of AI (see [Figure 1-1](ch01.xhtml#ch01fig01)).
    This is the proper way to describe the relationship for most people, and it fits
    with [Chapter 2](ch02.xhtml)’s history. From this perspective, the classical models
    of this chapter are a form of AI.
  prefs: []
  type: TYPE_NORMAL
- en: But are the classical models symbolic AI or connectionist AI? I say neither.
    They are not symbolic AI because they don’t manipulate logical rules or statements,
    and they’re not connectionist because they don’t employ a network of simple units
    that learn their proper association as they work with the data. Instead, I consider
    these models to be a fancy form of curve fitting—the output of an algorithm employing
    an optimization process to produce a function that best characterizes the training
    data, and, hopefully, the data encountered by the model in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: For a support vector machine, the function is the structure of the model in
    terms of the support vectors it locates during its optimization process. A decision
    tree’s function is generated by a specific algorithm designed to repeatedly split
    the training data into smaller and smaller groups until a leaf is created that
    (usually) contains only examples from a single class. Random forests are merely
    collections of such functions working in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Tree classifiers are almost a form of genetic programming. [*Genetic programming*](glossary.xhtml#glo50)
    creates computer code by simulating evolution via natural selection, where improved
    fitness corresponds to “is a better solution to the problem.” Indeed, genetic
    programming is a kind of [*evolutionary algorithm*](glossary.xhtml#glo38), and
    evolutionary algorithms, along with [*swarm intelligence*](glossary.xhtml#glo93)
    algorithms, implement robust, generic optimization. Some people consider evolutionary
    algorithms and swarm intelligence to be AI. I don’t, though I frequently use them
    in my work. Swarms don’t learn; they search a space representing possible solutions
    to a problem.
  prefs: []
  type: TYPE_NORMAL
- en: Nearest neighbor models are even simpler; there is no function to create. If
    we have *all* the possible data generated by some parent process—that is, the
    thing creating the feature vectors that we’re trying to model—then we don’t need
    a model. To assign a class label to a feature vector, we simply look it up in
    the feature vector “phone book” and return the label we find there. Since we have
    all possible feature vectors with labels, there’s nothing to approximate, and
    any feature vector encountered in the wild will necessarily be in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Barring access to all possible feature vectors for the problem at hand, a nearest
    neighbor model uses the closest feature vector in the incomplete phone book represented
    by the training data.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, suppose we live in a town of 3,000 people, and all of them are
    in the phone book. (Are there still such things as phone books? If not, pretend.)
  prefs: []
  type: TYPE_NORMAL
- en: If we want to find Nosmo King’s phone number, we look in the book under “King”
    and scan until we hit “Nosmo,” and we have it. Suppose, however, that we don’t
    have a complete listing of all 3,000 people, but 300 selected at random. We still
    want to know Nosmo King’s phone number (class label), but it’s not in the phone
    book. However, there is a Burg R. King. There’s a good chance Burg is related
    to Nosmo because of the shared last name, so we return Burg’s phone number as
    Nosmo’s. Clearly, the more complete the phone book, the better the chance we’ll
    find our desired name or someone in that person’s household. That’s essentially
    all that a nearest neighbor model does.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: To recap, support vector machines, decision trees, and random forests use data
    to generate functions according to a carefully crafted algorithm designed by a
    human. That is neither symbolic AI nor connectionism to me, but curve fitting
    or, perhaps more accurately, optimization. Nearest neighbor models are even worse;
    in their case, there’s no function at all.
  prefs: []
  type: TYPE_NORMAL
- en: This doesn’t mean that AI is bogus, but it does mean that what practitioners
    have in mind when they talk about AI is likely different from what the general
    public considers “artificial intelligence.”
  prefs: []
  type: TYPE_NORMAL
- en: 'However, all is not lost. There is a machine learning model worthy of the connectionist
    label: the neural network. It’s at the heart of the AI revolution, and it’s capable
    of actually learning from data. So, let’s put classical models and symbolic AI
    aside and devote our attention to neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**KEY TERMS**'
  prefs: []
  type: TYPE_NORMAL
- en: bagging, curse of dimensionality, evolutionary algorithm, false negative, false
    positive, genetic programming, hyperparameters, inference, manifold, metric, nearest
    neighbor, one-versus-one, one-versus-rest, random forest, support vector machine,
    swarm intelligence, true negative, true positive
  prefs: []
  type: TYPE_NORMAL
