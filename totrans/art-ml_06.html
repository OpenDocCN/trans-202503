<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><h2 class="h2part" id="part02"><span epub:type="pagebreak" id="page_79" class="calibre2"/><strong class="calibre3">PART II</strong></h2>
<h2 class="h2parta"><strong class="calibre3">TREE-BASED METHODS</strong></h2>
<p class="noindent">Here we cover <em class="calibre13">tree methods</em>, which are extensions of the neighborhood concept. As we move into these and other sophisticated models, weâ€™ll bring in a more sophisticated tool for hyperparameter selection, the <code>qeML</code> function <code>qeFT()</code>.</p>
<p class="indent"><a href="ch05.xhtml" class="calibre12">Chapter 5</a> introduces decision trees, one of the earliest ML methods, which is still very popular today. This leads to two even more popular methods, random forests and gradient boosting, which are covered in <a href="ch06.xhtml" class="calibre12">Chapter 6</a>.<span epub:type="pagebreak" id="page_80"/></p>
</div></body></html>