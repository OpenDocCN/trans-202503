- en: '## **1'
  prefs: []
  type: TYPE_NORMAL
- en: INTRODUCTION TO DISASSEMBLY**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/com.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You may be wondering what to expect in a book dedicated to Ghidra. While obviously
    Ghidra-centric, this book is not intended to come across as *The Ghidra User’s
    Manual*. Instead, we intend to use Ghidra as the enabling tool for discussing
    reverse engineering techniques that you will find useful in analyzing a wide variety
    of software, ranging from vulnerable applications to malware. When appropriate,
    we will provide detailed steps in Ghidra for performing specific actions related
    to the task at hand. As a result, we will take a rather roundabout walk through
    Ghidra’s capabilities, beginning with the basic tasks you will want to perform
    upon initial examination of a file and leading up to advanced uses and customization
    of Ghidra for more challenging reverse engineering problems. We make no attempt
    to cover all of Ghidra’s features. We do, however, cover the features you will
    find most useful in meeting your reverse engineering challenges. This book will
    help make Ghidra the most potent weapon in your arsenal of tools.
  prefs: []
  type: TYPE_NORMAL
- en: Prior to diving into any Ghidra specifics, we will cover some of the basics
    of the disassembly process and review other tools available for reverse engineering
    compiled code. While these tools may not match the complete range of Ghidra’s
    capabilities, each does address specific subsets of Ghidra functionality and offers
    valuable insight into specific Ghidra features. The remainder of this chapter
    is dedicated to understanding the disassembly process from a high level.
  prefs: []
  type: TYPE_NORMAL
- en: '**Disassembly Theory**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Anyone who has spent any time at all studying programming languages has probably
    learned about the various generations of languages, but they are summarized here
    for those who may have been sleeping:'
  prefs: []
  type: TYPE_NORMAL
- en: '**First-generation languages** These are the lowest form of language, generally
    consisting of ones and zeros or a shorthand form, such as hexadecimal, and readable
    only by binary ninjas. Distinguishing data from instructions is difficult at this
    level because all the content looks the same. First-generation languages may also
    be referred to as *machine languages*, and in some cases *byte code*, while machine
    language programs are often referred to as *binaries*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Second-generation languages** Also called *assembly languages*, second-generation
    languages are a mere table lookup away from machine language and generally map
    specific bit patterns, or operation codes (opcodes), to short but memorable character
    sequences called *mnemonics*. These mnemonics help programmers remember the instructions
    with which they are associated. An *assembler* is a tool used by programmers to
    translate their assembly language programs into machine language suitable for
    execution. In addition to instruction mnemonics, a complete assembly language
    generally includes *directives* to the assembler that help dictate the memory
    layout of code and data in the final binary.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Third-generation languages** These languages take another step toward the
    expressive capability of natural languages by introducing keywords and constructs
    that programmers use as the building blocks for their programs. Third-generation
    languages are generally platform independent, though programs written using them
    may be platform dependent as a result of using features unique to a specific operating
    system. Often-cited examples of third-generation languages include FORTRAN, C,
    and Java. Programmers generally use compilers to translate their programs into
    assembly language or all the way to machine language (or some rough equivalent
    such as byte code).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fourth-generation languages** These exist but aren’t relevant to this book
    and are not discussed.'
  prefs: []
  type: TYPE_NORMAL
- en: '### **The What of Disassembly**'
  prefs: []
  type: TYPE_NORMAL
- en: In a traditional software development model, compilers, assemblers, and linkers
    are used by themselves or in combination to create executable programs. To work
    our way backward (or reverse engineer programs), we use tools to undo the assembly
    and compilation processes. Not surprisingly, such tools are called *disassemblers*
    and *decompilers*, and they do pretty much what their names indicate. A disassembler
    undoes the assembly process, so we should expect assembly language as the output
    (and therefore machine language as input). Decompilers aim to produce output in
    a high-level language when given assembly or even machine language as input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The promise of “source code recovery” will always be attractive in a competitive
    software market, and thus the development of usable decompilers remains an active
    research area in computer science. The following are just a few of the reasons
    that decompilation is difficult:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The compilation process is lossy.** At the machine language level, there
    are no variable or function names, and variable type information can be determined
    only by how the data is used rather than explicit type declarations. When you
    observe 32 bits of data being transferred, you’ll need to do some investigative
    work to determine whether those 32 bits represent an integer, a 32-bit floating
    point value, or a 32-bit pointer.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compilation is a many-to-many operation.** This means that a source program
    can be translated to assembly language in many different ways, and machine language
    can be translated back to source in many different ways. As a result, compiling
    a file and immediately decompiling it commonly yields a source file that is vastly
    different from the original.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Decompilers are language and library dependent.** Processing a binary produced
    by a Delphi compiler with a decompiler designed to generate C code can yield very
    strange results. Similarly, feeding a compiled Windows binary through a decompiler
    that has no knowledge of the Windows programming API may not yield anything useful.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A nearly perfect disassembly capability is needed in order to accurately
    decompile a binary.** Any errors or omissions in the disassembly phase will almost
    certainly propagate through to the decompiled code. Disassembled code can be verified
    for correctness against appropriate processor reference manuals; however, no canonical
    reference manuals are available to use in verifying the correctness of a decompiler’s
    output.'
  prefs: []
  type: TYPE_NORMAL
- en: Ghidra has a built-in decompiler, which is the subject of [Chapter 19](ch19.xhtml#ch19).
  prefs: []
  type: TYPE_NORMAL
- en: '### **The Why of Disassembly**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of disassembly tools is often to facilitate understanding of programs
    when source code is unavailable. Common situations in which disassembly is used
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of malware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of closed source software for vulnerabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of closed source software for interoperability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of compiler-generated code to validate compiler performance or correctness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Display of program instructions while debugging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The subsequent sections explain each situation in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: '***Malware Analysis***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unless you are dealing with script-based malware, malware authors seldom do
    you the favor of providing the source code to their creations. Lacking source
    code, you are faced with a very limited set of options for discovering exactly
    how the malware behaves. The two main techniques for malware analysis are dynamic
    analysis and static analysis. *Dynamic analysis* involves allowing the malware
    to execute in a carefully controlled environment (sandbox) while recording every
    observable aspect of its behavior by using any number of system instrumentation
    utilities. In contrast, *static analysis* attempts to understand the behavior
    of a program simply by reading through the program code, which, in the case of
    malware, generally consists solely of a disassembly listing and possibly a decompiler
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: '***Vulnerability Analysis***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the sake of simplification, let’s break the entire security-auditing process
    into three steps: vulnerability discovery, vulnerability analysis, and exploit
    development. The same steps apply whether you have source code or not; however,
    the level of effort increases substantially when all you have is a binary. The
    first step in the process is to discover a potentially exploitable condition in
    a program. This is often accomplished using dynamic techniques such as fuzzing,^([1](footnotes.xhtml#ch01fn1))
    but it can also be performed (usually with much more effort) via static analysis.
    Once a problem has been discovered, further analysis is often required to determine
    whether the problem is exploitable at all and, if so, under what conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying variables that can be manipulated to the attacker’s advantage is
    an important early step in vulnerability discovery. Disassembly listings provide
    the level of detail required to understand exactly how the compiler has chosen
    to allocate program variables. For example, it might be useful to know that a
    70-byte character array declared by a programmer was rounded up to 80 bytes when
    allocated by the compiler. Disassembly listings also provide the only means to
    determine exactly how a compiler has chosen to order all of the variables declared
    globally or within functions. Understanding the spatial relationships among variables
    is often essential when attempting to develop exploits. Ultimately, by using a
    disassembler and a debugger together, an exploit may be developed.
  prefs: []
  type: TYPE_NORMAL
- en: '***Software Interoperability***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When software is released in binary form only, it is very difficult for competitors
    to create software that can interoperate with it or to provide plugin replacements
    for that software. A common example is driver code released for hardware that
    is supported on only one platform. When a vendor is slow to support or, worse
    yet, refuses to support the use of its hardware with alternative platforms, substantial
    reverse engineering effort may be required in order to develop software drivers
    to support the hardware. In these cases, static code analysis is almost the only
    remedy and often must go beyond the software driver to understand embedded firmware.
  prefs: []
  type: TYPE_NORMAL
- en: '***Compiler Validation***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since the purpose of a compiler (or assembler) is to generate machine language,
    good disassembly tools are often required to verify that the compiler is doing
    its job in accordance with any design specifications. Analysts may also be interested
    in locating additional opportunities for optimizing compiler output and, from
    a security standpoint, ascertaining whether the compiler itself has been compromised
    to the extent that it may be inserting backdoors into generated code.
  prefs: []
  type: TYPE_NORMAL
- en: '***Debugging Displays***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Perhaps the single most common use of disassemblers is to generate listings
    within debuggers. Unfortunately, disassemblers embedded within debuggers tend
    to lack sophistication. They are generally incapable of batch disassembly and
    sometimes balk at disassembling when they cannot determine the boundaries of a
    function. This is one of the reasons it is best to use a debugger in conjunction
    with a high-quality disassembler to provide better situational awareness and context
    during debugging.
  prefs: []
  type: TYPE_NORMAL
- en: '**The How of Disassembly**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that you’re well versed in the purposes of disassembly, it’s time to move
    on to how the process actually works. Consider a typical daunting task faced by
    a disassembler: *Take these 100KB, distinguish code from data, convert the code
    to assembly language for display to a user, and please don’t miss anything along
    the way.* We could tack on any number of special requests, such as asking the
    disassembler to locate functions, recognize jump tables, and identify local variables,
    making the disassembler’s job that much more difficult.'
  prefs: []
  type: TYPE_NORMAL
- en: To accommodate all of our demands, any disassembler will need to pick and choose
    from a variety of algorithms as it navigates through the files we feed it. The
    quality of the generated disassembly listing will be directly related to the quality
    of the algorithms utilized and how well they have been implemented.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discuss two of the fundamental algorithms in use today for
    disassembling machine code. As we present these algorithms, we also point out
    their shortcomings in order to prepare you for situations in which your disassembler
    appears to fail. By understanding a disassembler’s limitations, you will be able
    to manually intervene to improve the overall quality of the disassembly output.
  prefs: []
  type: TYPE_NORMAL
- en: '***A Basic Disassembly Algorithm***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For starters, let’s develop a simple algorithm for accepting machine language
    as input and producing assembly language as output. In doing so, you will gain
    an understanding of the challenges, assumptions, and compromises that underlie
    an automated disassembly process:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step in the disassembly process is to identify a region of code to
    disassemble. This is not necessarily as straightforward as it may seem. Instructions
    are generally mixed with data, and it is important to distinguish between the
    two. In the most common case, disassembly of an executable file, the file will
    conform to a common format for executable files such as the *Portable Executable
    (PE)* format used on Windows and the *Executable and Linkable Format (ELF)* common
    on many Unix-based systems. These formats typically contain mechanisms (often
    in the form of hierarchical file headers) for locating the sections of the file
    that contain code and entry points into that code.^([2](footnotes.xhtml#ch01fn2))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given the address of an instruction, the next step is to read the value or values
    contained at that address (or file offset) and perform a table lookup to match
    the binary opcode value to its assembly language mnemonic. Depending on the complexity
    of the instruction set being disassembled, this may be a trivial process, or it
    may involve several additional operations such as understanding any prefixes that
    may modify the instruction’s behavior and determining any operands required by
    the instruction. For instruction sets with variable-length instructions, such
    as the Intel x86 instruction set, additional instruction bytes may need to be
    retrieved in order to completely disassemble a single instruction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once an instruction has been fetched and any required operands decoded, its
    assembly language equivalent is formatted and output as part of the disassembly
    listing. It may be possible to choose from more than one assembly language output
    syntax. For example, the two predominant formats for x86 assembly language are
    the Intel format and the AT&T format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Following the output of an instruction, we need to advance to the next instruction
    and repeat the previous process until we have disassembled every instruction in
    the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**X86 ASSEMBLY SYNTAX: AT&T VS. INTEL**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two main syntaxes are used for assembly source code: AT&T and Intel. Even though
    they are second-generation languages, the two vary greatly in syntax—from variable,
    constant, and register access, to segment and instruction size overrides, to indirection
    and offsets. The AT&T assembly syntax is distinguished by its use of the `%` symbol
    to prefix all register names, the use of `$` as a prefix for literal constants
    (also called *immediate operands*), and its operand ordering in which the source
    operand appears on the left and the destination operand appears on the right.
    Using AT&T syntax, the instruction to add 4 to the `EAX` register would be `add
    $0x4,%eax`. The GNU Assembler `(as)` and many other GNU tools, including `gcc`
    and `gdb`, utilize AT&T syntax by default.'
  prefs: []
  type: TYPE_NORMAL
- en: Intel syntax differs from AT&T in that it requires no register or literal prefixes,
    and the operand ordering is reversed such that the source operand appears on the
    right and the destination appears on the left. The same `add` instruction using
    the Intel syntax would be `add eax,0x4`. Assemblers utilizing Intel syntax include
    the Microsoft Assembler (MASM) and the Netwide Assembler (NASM).
  prefs: []
  type: TYPE_NORMAL
- en: Various algorithms exist for determining where to begin a disassembly, how to
    choose the next instruction to be disassembled, how to distinguish code from data,
    and how to determine when the last instruction has been disassembled. The two
    predominant disassembly algorithms are linear sweep and recursive descent.
  prefs: []
  type: TYPE_NORMAL
- en: '***Linear Sweep Disassembly***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The *linear sweep* disassembly algorithm takes a very straightforward approach
    to locating instructions to disassemble: where one instruction ends, another begins.
    As a result, the most difficult decisions faced are where to begin and when to
    stop. The usual solution is to assume that everything contained in sections of
    a program marked as code (typically specified by the program file’s headers) represents
    machine language instructions. Disassembly begins with the first byte in a code
    section and moves, in a linear fashion, through the section, disassembling one
    instruction after another until the end of the section is reached. No effort is
    made to understand the program’s control flow through recognition of nonlinear
    instructions such as branches.'
  prefs: []
  type: TYPE_NORMAL
- en: During the disassembly process, a pointer can be maintained to mark the beginning
    of the instruction currently being disassembled. As part of the disassembly process,
    the length of each instruction is computed and used to determine the location
    of the next instruction to be disassembled. Instruction sets with fixed-length
    instructions (MIPS, for example) are somewhat easier to disassemble, as locating
    subsequent instructions is straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of the linear sweep algorithm is that it provides complete
    coverage of a program’s code sections. One of the primary disadvantages of the
    linear sweep method is that it fails to account for data that may be comingled
    with code. This is evident in [Listing 1-1](ch01.xhtml#exa1_1), which shows the
    output of a function disassembled with a linear sweep disassembler.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 1-1: Linear sweep disassembly*'
  prefs: []
  type: TYPE_NORMAL
- en: This function contains a switch statement, and the compiler used in this case
    has elected to implement the switch by using a jump table to resolve case label
    targets. Furthermore, the compiler has elected to embed the jump table within
    the function itself. The `jmp` statement ➊ references an address table ➋. Unfortunately,
    the disassembler treats the address table as if it were a series of instructions
    and incorrectly generates the following assembly language representation.
  prefs: []
  type: TYPE_NORMAL
- en: If we treat successive 4-byte groups in the jump table ➋ as little-endian values,^([3](footnotes.xhtml#ch01fn3))
    we see that each represents a pointer to a nearby address that is in fact the
    destination for one of the various jumps (`004012e0`, `0040128b`, `00401290`,
    . . .). Thus, the `loopne` instruction ➋ is not an instruction at all. Instead,
    it indicates a failure of the linear sweep algorithm to properly distinguish embedded
    data from code.
  prefs: []
  type: TYPE_NORMAL
- en: Linear sweep is used by the disassembly engines contained in the GNU debugger
    (gdb), Microsoft’s WinDbg debugger, and the `objdump` utility.
  prefs: []
  type: TYPE_NORMAL
- en: '***Recursive Descent Disassembly***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The *recursive descent* disassembly algorithm takes a different approach to
    locating instructions: it focuses on the concept of control flow, which determines
    whether an instruction should be disassembled based on whether it is referenced
    by another instruction. To understand recursive descent, it is helpful to classify
    instructions according to how they affect the instruction pointer.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequential Flow Instructions**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Sequential flow instructions* pass execution to the instruction that immediately
    follows. Examples of sequential flow instructions include simple arithmetic instructions,
    such as `add`; register-to-memory transfer instructions, such as `mov`; and stack-manipulation
    operations, such as `push` and `pop`. For such instructions, disassembly proceeds
    as with linear sweep.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditional Branching Instructions**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Conditional branching instructions*, such as the x86 `jnz`, offer two possible
    execution paths. If the condition evaluates to true, the branch is taken, and
    the instruction pointer must be changed to reflect the target of the branch. However,
    if the condition is false, execution continues in a linear fashion, and a linear
    sweep methodology can be used to disassemble the next instruction. As it is generally
    not possible in a static context to determine the outcome of a conditional test,
    the recursive descent algorithm disassembles both paths, deferring disassembly
    of the branch target instruction by adding the address of the target instruction
    to a list of addresses to be disassembled at a later point.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unconditional Branching Instructions**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Unconditional branches* do not follow the linear flow model and therefore
    are handled differently by the recursive descent algorithm. As with the sequential
    flow instructions, execution can flow to only one instruction; however, that instruction
    need not immediately follow the branch instruction. In fact, as seen in [Listing
    1-1](ch01.xhtml#exa1_1), there is no requirement at all for an instruction to
    immediately follow an unconditional branch. Therefore, there is no reason to immediately
    disassemble the bytes that follow an unconditional branch.'
  prefs: []
  type: TYPE_NORMAL
- en: A recursive descent disassembler attempts to determine the target of the unconditional
    jump and continues disassembly at the target address. Unfortunately, some unconditional
    branches can cause problems for recursive descent disassemblers. When the target
    of a jump instruction depends on a runtime value, it may not be possible to determine
    the destination of the jump by using static analysis. The x86 instruction `jmp`
    `rax` demonstrates this problem. The `rax` register contains a value only when
    the program is actually running. Since the register contains no value during static
    analysis, we have no way to determine the target of the jump instruction, and,
    consequently, we have no way to determine where to continue the disassembly process.
  prefs: []
  type: TYPE_NORMAL
- en: '**Function Call Instructions**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Function call instructions* operate similarly to unconditional jump instructions
    (including the inability of the disassembler to determine the target of instructions
    such as `call rax`), with the additional expectation that execution usually returns
    to the instruction immediately following the call instruction after the function
    completes. In this regard, they are similar to conditional branch instructions
    in that they generate two execution paths. The target address of the call instruction
    is added to a list for deferred disassembly, while the instruction immediately
    following the call is disassembled in a manner similar to linear sweep.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recursive descent can fail if programs do not behave as expected when returning
    from called functions. For example, code in a function can deliberately manipulate
    the return address of that function so that upon completion, control returns to
    a location different from the one expected by the disassembler. A simple example
    is shown in the following incorrect listing, where function `badfunc` simply adds
    1 to the return address before returning to the caller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, control does not actually pass to the `add` instruction ➊ following
    the call to `badfunc`. A proper disassembly appears next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This listing more clearly shows the flow of the program in which function `badfunc`
    actually returns to the `mov` instruction ➊. It is important to understand that
    a linear sweep disassembler will also fail to properly disassemble this code,
    though for slightly different reasons.
  prefs: []
  type: TYPE_NORMAL
- en: '**Return Instructions**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In some cases, the recursive descent algorithm runs out of paths to follow.
    A function *return instruction* (x86 `ret`, for example) offers no information
    about which instruction will be executed next. If the program were actually running,
    an address would be taken from the top of the runtime stack, and execution would
    resume at that address. Disassemblers do not have the benefit of access to a stack.
    Instead, disassembly abruptly comes to a halt. It is at this point that the recursive
    descent disassembler turns to the list of addresses it has been setting aside
    for deferred disassembly. An address is removed from this list, and the disassembly
    process is continued from this address. This is the recursive process that lends
    the disassembly algorithm its name.
  prefs: []
  type: TYPE_NORMAL
- en: One of the principle advantages of the recursive descent algorithm is its superior
    ability to distinguish code from data. As a control flow-based algorithm, it is
    much less likely to incorrectly disassemble data values as code. The main disadvantage
    of recursive descent is the inability to follow indirect code paths, such as jumps
    or calls, which utilize tables of pointers to look up a target address. However,
    with the addition of some heuristics to identify pointers to code, recursive descent
    disassemblers can provide very complete code coverage and excellent recognition
    of code versus data. [Listing 1-2](ch01.xhtml#exa1_2) shows the output of Ghidra’s
    recursive descent disassembler used on the same switch statement shown earlier
    in [Listing 1-1](ch01.xhtml#exa1_1).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 1-2: Recursive descent disassembly*'
  prefs: []
  type: TYPE_NORMAL
- en: Note that this section of the binary has been recognized as a switch statement
    and formatted accordingly. An understanding of the recursive descent process will
    help us recognize situations in which Ghidra may produce less-than-optimal disassemblies
    and allow us to develop strategies to improve Ghidra’s output.
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Is deep understanding of disassembly algorithms essential when using a disassembler?
    No. Is it useful? Yes! Battling your tools is the last thing you want to spend
    time doing while reverse engineering. One of the many advantages of Ghidra is
    that, as an interactive disassembler, it offers you plenty of opportunity to guide
    and override its decisions. The net result is quite often a disassembly that is
    both thorough and accurate.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we review a variety of existing tools that prove useful
    in many reverse engineering situations. While not directly related to Ghidra,
    many of these tools have influenced Ghidra, and they help to explain the wide
    variety of informational displays available in the Ghidra user interface.
  prefs: []
  type: TYPE_NORMAL
