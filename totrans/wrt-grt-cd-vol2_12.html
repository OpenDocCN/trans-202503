<html><head></head><body>
		<h2 class="h2" id="ch12"><span epub:type="pagebreak" id="page_385"/><strong><span class="big">12</span></strong><br/><strong>ARITHMETIC AND LOGICAL EXPRESSIONS</strong></h2>&#13;
		<div class="image1">&#13;
			<img alt="image" src="../images/common01.jpg"/>&#13;
		</div>&#13;
		<p class="noindent">One of the major advantages that high-level languages provide over low-level languages is the use of algebraic arithmetic and logical expressions (hereafter, “arithmetic expressions”). HLL arithmetic expressions are an order of magnitude more readable than the sequence of machine instructions the compiler produces. However, the conversion process from arithmetic expressions into machine code is also one of the more difficult transformations to do efficiently, and a fair percentage of a typical compiler’s optimization phase is dedicated to handling it. Because of the difficulty with translation, this is one area where you can help the compiler. This chapter will describe:</p>&#13;
		<ul>&#13;
			<li>&#13;
				<p class="noindent">How computer architecture affects the computation of arithmetic expressions</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">The optimization of arithmetic expressions</p>&#13;
				</li>&#13;
			<li><span epub:type="pagebreak" id="page_386"/>&#13;
			<p class="noindent">Side effects of arithmetic expressions</p>&#13;
			</li>&#13;
			<li>&#13;
				<p class="noindent">Sequence points in arithmetic expressions</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Order of evaluation in arithmetic expressions</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Short-circuit and complete evaluation of arithmetic expressions</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">The computational cost of arithmetic expressions</p>&#13;
				</li>&#13;
		</ul>&#13;
		<p class="indent">Armed with this information, you’ll be able to write more efficient and more robust applications.</p>&#13;
		<h3 class="h3" id="ch00lev1sec100"><strong>12.1 Arithmetic Expressions and Computer Architecture</strong></h3>&#13;
		<p class="noindent">With respect to arithmetic expressions, we can classify traditional computer architectures into three basic types: stack-based machines, register-based machines, and accumulator-based machines. The major difference between these architectural types has to do with where the CPUs keep the operands for the arithmetic operations. Once the CPU fetches the data from these operands, the data is passed along to the arithmetic and logical unit, where the actual arithmetic or logical calculation occurs.<sup><a id="ch12fn_1"/><a href="footnotes.xhtml#ch12fn1">1</a></sup> We’ll explore each of these architectures in the following sections.</p>&#13;
		<h4 class="h4" id="ch00lev2sec148"><strong>12.1.1 Stack-Based Machines</strong></h4>&#13;
		<p class="noindent">Stack-based machines use memory for most calculations, employing a data structure called the <em>stack</em> in memory to hold all operands and results. Computer systems with a stack architecture offer some important advantages over other architectures:</p>&#13;
		<ul>&#13;
			<li>&#13;
				<p class="noindent">The instructions are often smaller in stack architectures because the instructions generally don’t have to specify any operands.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">It is usually easier to write compilers for stack architectures than for other machines because converting arithmetic expressions to a sequence of stack operations is very easy.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Temporary variables are rarely needed in a stack architecture, because the stack itself serves that purpose.</p>&#13;
				</li>&#13;
		</ul>&#13;
		<p class="indent">Unfortunately, stack machines also suffer from some serious disadvantages:</p>&#13;
		<ul>&#13;
			<li>&#13;
				<p class="noindent">Almost every instruction references memory (which is slow on modern machines). Though caches can help mitigate this problem, memory performance is still a major problem on stack machines.</p>&#13;
				</li>&#13;
			<li><span epub:type="pagebreak" id="page_387"/>&#13;
			<p class="noindentt">Even though conversion from HLLs to a stack machine is very easy, there’s less opportunity for optimization than there is with other architectures.</p>&#13;
			</li>&#13;
			<li>&#13;
				<p class="noindent">Because stack machines are constantly accessing the same data elements (that is, data on the <em>top of the stack</em>), pipelining and instruction parallelism is difficult to achieve.</p>&#13;
				</li>&#13;
		</ul>&#13;
		<div class="note">&#13;
			<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
			<p class="notep"><em>See</em> WGC1 <em>for details on pipelining and instruction parallelism.</em></p>&#13;
		</div>&#13;
		<p class="indent">With a stack you generally do one of three things: push new data onto it, pop data from it, or operate on the data that is currently sitting on the <em>top of stack</em> (and possibly the data immediately below that, or <em>next on stack</em>).</p>&#13;
		<h5 class="h5" id="ch00lev3sec74"><strong>12.1.1.1 Basic Stack Machine Organization</strong></h5>&#13;
		<p class="noindent">A typical stack machine maintains a couple of registers inside the CPU (see <a href="ch12.xhtml#ch12fig1">Figure 12-1</a>). In particular, you can expect to find a <em>program counter register</em> (like the 80x86’s RIP register) and a <em>stack pointer register</em> (like the 80x86 RSP register).</p>&#13;
		<div class="image" id="ch12fig1">&#13;
			<img alt="Image" src="../images/12fig01.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 12-1: Typical stack machine architecture</em></p>&#13;
		<p class="indent">The stack pointer register contains the memory address of the current top of stack (TOS) element in memory. The CPU increments or decrements the stack pointer register whenever a program places data onto the stack or removes data from the stack. On some architectures the stack expands from higher memory locations to lower memory locations; on other architectures, the stack grows from lower memory locations toward higher memory locations. Fundamentally, the direction of stack growth is irrelevant; all it really determines is whether the machine decrements the stack pointer register when placing data on the stack (if the stack grows toward lower memory addresses) or increments the stack pointer register (when the stack grows toward higher memory addresses).</p>&#13;
		<h5 class="h5" id="ch00lev3sec75"><span epub:type="pagebreak" id="page_388"/><strong>12.1.1.2 The push Instruction</strong></h5>&#13;
		<p class="noindent">To place data on the stack, you typically use the machine instruction <span class="literal">push</span>. This instruction generally takes a single operand that specifies the value to push onto the stack, like so:</p>&#13;
		<pre class="programs">&#13;
			push <span class="codeitalic1">memory or constant operand</span></pre>&#13;
		<p class="indent">Here are a couple of concrete examples:</p>&#13;
		<pre class="programs">&#13;
			push 10  ; Pushes the constant 10 onto the stack<br/>push mem ; Pushes the contents of memory location mem</pre>&#13;
		<p class="indent">A <span class="literal">push</span> operation typically increases the value of the stack pointer register by the size of its operand in bytes and then copies that operand to the memory location the stack pointer now specifies. For example, <a href="ch12.xhtml#ch12fig2">Figures 12-2</a> and <a href="ch12.xhtml#ch12fig3">12-3</a> illustrate what the stack looks like before and after a <span class="literal">push 10</span> operation.</p>&#13;
		<div class="image" id="ch12fig2">&#13;
			<img alt="Image" src="../images/12fig02.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 12-2: Before a <span class="codeitalic">push 10</span> operation</em></p>&#13;
		<div class="image" id="ch12fig3">&#13;
			<img alt="Image" src="../images/12fig03.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 12-3: After a <span class="codeitalic">push 10</span> operation</em></p>&#13;
		<h5 class="h5" id="ch00lev3sec76"><strong>12.1.1.3 The pop Instruction</strong></h5>&#13;
		<p class="noindent">To remove a data item from the top of a stack, you use a <span class="literal">pop</span> or <span class="literal">pull</span> instruction. (This book will use <span class="literal">pop</span>; just be aware that some architectures use <span class="literal">pull</span> instead.) A typical <span class="literal">pop</span> instruction might look as follows:</p>&#13;
		<pre class="programs">&#13;
			pop <span class="codeitalic1">memory location</span></pre>&#13;
		<div class="note">&#13;
			<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
			<p class="notep"><em>You cannot pop data into a constant. The <em>pop</em> operand must be a memory location.</em></p>&#13;
		</div>&#13;
		<p class="noindent">The <span class="literal">pop</span> instruction makes a copy of the data pointed at by the stack pointer and stores it into the destination memory location. Then it <span epub:type="pagebreak" id="page_389"/>decrements (or increments) the stack pointer register to point at the next lower item on the stack, or next on stack (NOS); see <a href="ch12.xhtml#ch12fig4">Figures 12-4</a> and <a href="ch12.xhtml#ch12fig5">12-5</a>.</p>&#13;
		<div class="image" id="ch12fig4">&#13;
			<img alt="Image" src="../images/12fig04.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 12-4: Before a <span class="codeitalic">pop mem</span> operation</em></p>&#13;
		<div class="image" id="ch12fig5">&#13;
			<img alt="Image" src="../images/12fig05.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 12-5: After a <span class="codeitalic">pop mem</span> operation</em></p>&#13;
		<p class="indent">Note that the value in stack memory that the <span class="literal">pop</span> instruction removes from the stack is still physically present in memory above the new TOS. However, the next time the program pushes data onto the stack, it will overwrite this value with the new value.</p>&#13;
		<h5 class="h5" id="ch00lev3sec77"><strong>12.1.1.4 Arithmetic Operations on a Stack Machine</strong></h5>&#13;
		<p class="noindent">The arithmetic and logical instructions found on a stack machine generally do not allow any operands. This is why stack machines are often called <em>zero-address machines</em>; the arithmetic instructions themselves do not encode any operand addresses. For example, consider an <span class="literal">add</span> instruction on a typical stack machine. This instruction will pop two values from the stack (TOS and NOS), compute their sum, and push the result back onto the stack (see <a href="ch12.xhtml#ch12fig6">Figures 12-6</a> and <a href="ch12.xhtml#ch12fig7">12-7</a>).</p>&#13;
		<div class="image" id="ch12fig6">&#13;
			<img alt="Image" src="../images/12fig06.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 12-6: Before an <span class="codeitalic">add</span> operation</em></p>&#13;
		<div class="image" id="ch12fig7">&#13;
			<span epub:type="pagebreak" id="page_390"/>&#13;
			<img alt="Image" src="../images/12fig07.jpg"/>&#13;
		</div>&#13;
		<p class="figcap"><em>Figure 12-7: After an <span class="codeitalic">add</span> operation</em></p>&#13;
		<p class="indent">Because arithmetic expressions are recursive in nature, and recursion requires a stack for proper implementation, it’s no surprise that converting arithmetic expressions to a sequence of stack machine instructions is relatively simple. Arithmetic expressions found in common programming languages use an <em>infix notation</em>, where the operator appears between two operands. For example, <span class="literal">a + b</span> and <span class="literal">c - d</span> are examples of infix notation because the operators (<span class="literal">+</span> and <span class="literal">-</span>) appear between the operands ([<span class="literal">a</span>, <span class="literal">b</span>] and [<span class="literal">c</span>, <span class="literal">d</span>]). Before you can do the conversion to stack machine instructions, you must convert these infix expressions into <em>postfix notation</em> (also known as <em>reverse polish notation</em>), where the operator immediately follows the operands to which it applies. For example, the infix expressions <span class="literal">a + b</span> and <span class="literal">c – d</span> would have the corresponding postfix forms <span class="literal">a b +</span> and <span class="literal">c d –</span>, respectively.</p>&#13;
		<p class="indent">Once you have an expression in postfix form, converting it to a sequence of stack machine instructions is very easy. You simply emit a <span class="literal">push</span> instruction for each operand and the corresponding arithmetic instruction for the operators. For example, <span class="literal">a b +</span> becomes:</p>&#13;
		<pre class="programs">&#13;
			push a<br/>push b<br/>add</pre>&#13;
		<p class="noindent">and <span class="literal">c d -</span> becomes:</p>&#13;
		<pre class="programs">&#13;
			push c<br/>push d<br/>sub</pre>&#13;
		<p class="noindent">assuming, of course, that <span class="literal">add</span> adds the top two items on the stack and <span class="literal">sub</span> subtracts the TOS from the value immediately below it on the stack.</p>&#13;
		<h5 class="h5" id="ch00lev3sec78"><strong>12.1.1.5 Real-World Stack Machines</strong></h5>&#13;
		<p class="noindent">A big advantage of the stack architecture is that it’s easy to write a compiler for such a machine. It’s also very easy to write an emulator for a stack-based machine. For these reasons, stack architectures are popular in <em>virtual machines (VMs)</em> such as the Java Virtual Machine, the UCSD Pascal p-machine, and the Microsoft Visual Basic, C#, and F# CIL. Although a few real-world stack-based CPUs do exist, such as a hardware implementation of the Java VM, they’re not very popular because of the performance limitations of memory access. Nonetheless, understanding the basics of a stack architecture is <span epub:type="pagebreak" id="page_391"/>important, because many compilers translate HLL source code into a stack-based form prior to emitting actual machine code. Indeed, in the worst (though rare) case, compilers are forced to emit code that emulates a stack-based machine when compiling complex arithmetic expressions.</p>&#13;
		<h4 class="h4" id="ch00lev2sec149"><strong>12.1.2 Accumulator-Based Machines</strong></h4>&#13;
		<p class="noindent">The simplicity of a stack machine instruction sequence hides an enormous amount of complexity. Consider the following stack-based instruction from the previous section:</p>&#13;
		<pre class="programs">add</pre>&#13;
		<p class="indent">This instruction looks simple, but it actually specifies a large number of operations:</p>&#13;
		<ul>&#13;
			<li>&#13;
				<p class="noindent">Fetch an operand from the memory location pointed to by the stack pointer.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Send the stack pointer’s value to the <em>ALU (arithmetic/logical unit)</em>.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Instruct the ALU to decrement the stack pointer’s value just sent to it.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Route the ALU’s value back to the stack pointer.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Fetch the operand from the memory location pointed to by the stack pointer.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Send the values from the previous step and the first step to the ALU.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Instruct the ALU to add those values.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Store the sum away in the memory location pointed to by the stack pointer.</p>&#13;
				</li>&#13;
		</ul>&#13;
		<p class="indent">The organization of a typical stack machine prevents many parallel operations that are possible with pipelining (see <em>WGC1</em> for more details on pipelining). So stack architectures are hit twice: typical instructions require many steps to complete, and those steps are difficult to execute in parallel with other operations.</p>&#13;
		<p class="indent">One big problem with the stack architecture is that it goes to memory for just about everything. For example, if you simply want to compute the sum of two variables and store this result in a third variable, you have to fetch the two variables and write them to the stack (four memory operations); then you have to fetch the two values from the stack, add them, and write their sum back to the stack (three memory operations); and finally, you have to pop the item from the stack and store the result into the destination memory location (two memory operations). That’s a total of nine memory operations. When memory access is slow, this is an expensive way to compute the sum of two numbers.</p>&#13;
		<p class="indent">One way to avoid this large number of memory accesses is to provide a general-purpose arithmetic register within the CPU. This is the idea behind an accumulator-based machine: you provide a single <em>accumulator</em> register, where the CPU computes temporary results rather than computing <span epub:type="pagebreak" id="page_392"/>temporary values in memory (on the stack). Accumulator-based machines are also known as <em>one-address</em> or <em>single-address machines</em>, because most instructions that operate on two operands use the accumulator as the default destination operand for the computation and require a single memory or constant operand to use as the source operand. A typical example of an accumulator machine is the 6502, which includes the following instructions:</p>&#13;
		<pre class="programs">&#13;
			LDA <span class="codeitalic1">constant or memory</span> ; Load accumulator register<br/>STA <span class="codeitalic1">memory</span>             ; Store accumulator register<br/>ADD <span class="codeitalic1">constant or memory</span> ; Add operand to accumulator<br/>SUB <span class="codeitalic1">constant or memory</span> ; Subtract operand from accumulator</pre>&#13;
		<p class="indent">Because one-address instructions require an operand that isn’t present in many of the zero-address instructions, individual instructions found on an accumulator-based machine tend to be larger than those found on a typical stack-based machine (because you have to encode the operand address as part of the instruction; see <em>WGC1</em> for details). However, programs are often smaller because fewer instructions are needed to do the same thing. Suppose, for example, you want to compute <span class="literal">x = y + z</span>. On a stack machine, you might use an instruction sequence like the following:</p>&#13;
		<pre class="programs">&#13;
			push y<br/>push z<br/>add<br/>pop x</pre>&#13;
		<p class="indent">On an accumulator machine, you’d probably use a sequence like this:</p>&#13;
		<pre class="programs">&#13;
			lda y<br/>add z<br/>sta x</pre>&#13;
		<p class="indent">Assuming that the <span class="literal">push</span> and <span class="literal">pop</span> instructions are roughly the same size as the accumulator machine’s <span class="literal">lda</span>, <span class="literal">add</span>, and <span class="literal">sta</span> instructions (a safe assumption), it’s clear that the stack machine’s instruction sequence is actually longer, because it requires more instructions. Even ignoring the extra instruction on the stack machine, the accumulator machine will probably execute the code faster, because it requires only three memory accesses (to fetch <span class="literal">y</span> and <span class="literal">z</span> and to store <span class="literal">x</span>), compared with the nine memory accesses the stack machine will require. Furthermore, the accumulator machine doesn’t waste any time manipulating the stack pointer register during computation.</p>&#13;
		<p class="indent">Even though accumulator-based machines generally have higher performance than stack-based machines (for reasons you’ve just seen), they’re not without their own problems. Having only one general-purpose register available for arithmetic operations creates a bottleneck in the system, resulting in <em>data hazards</em>. Many calculations produce temporary results that the application must write to memory in order to compute other components of the expression. This leads to extra memory accesses that could be avoided if the CPU provided additional accumulator registers. Thus, most modern <span epub:type="pagebreak" id="page_393"/>general-purpose CPUs do not use an accumulator-based architecture, but instead provide a large number of general-purpose registers.</p>&#13;
		<div class="note">&#13;
			<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
			<p class="notep"><em>See</em> WGC1 <em>for a discussion of data hazards.</em></p>&#13;
		</div>&#13;
		<p class="indent">Accumulator-based architectures were popular in early computer systems when the manufacturing process limited the number of features within the CPU, but today you rarely see them outside of low-cost embedded microcontrollers.</p>&#13;
		<h4 class="h4" id="ch00lev2sec150"><strong>12.1.3 Register-Based Machines</strong></h4>&#13;
		<p class="noindent">Of the three architectures discussed in this chapter, register-based machines are the most prevalent today because they offer the highest performance. By providing a fair number of on-CPU registers, this architecture spares the CPU from expensive memory accesses during the computation of complex expressions.</p>&#13;
		<p class="indent">In theory, a register-based machine could have as few as two general-purpose (arithmetic-capable) registers. In practice, about the only machines that fall into this category are the Motorola 680x processors, which most people consider to be a special case of the accumulator architecture with two separate accumulators. Register machines generally contain at least eight “general-purpose” registers (this number isn’t arbitrary; it’s the number of general-purpose registers found on the 80x86 CPU, the 8080 CPU, and the Z80 CPU, which are probably the minimalist examples of what a computer architect would call a “register-based” machine).</p>&#13;
		<p class="indent">Although some register-based machines (such as the 32-bit 80x86) have a small number of registers available, a general principle is “the more, the better.” Typical RISC machines, such as the PowerPC and ARM, have at least 16 general-purpose registers and often at least 32 registers. Intel’s Itanium processor, for example, provides 128 general-purpose integer registers. IBM’s CELL processor provides 128 registers in each of the processing units found on the device (each processing unit is a mini-CPU capable of certain operations); a typical CELL processor contains eight such processing units along with a PowerPC CPU core.</p>&#13;
		<p class="indent">The main reason for having as many general-purpose registers as possible is to avoid memory access. In an accumulator-based machine, the accumulator is a transient register used for calculations, but you can’t keep a variable’s value there for long periods of time, because you’ll need the accumulator for other purposes. In a register machine with a large number of registers, it’s possible to keep certain (often-used) variables in registers so you don’t have to access memory at all when using those variables. Consider the assignment statement <span class="literal">x := y+z</span>;. On a register-based machine (such as the 80x86), we could compute this result using the following HLA code:</p>&#13;
		<pre class="programs">&#13;
			// Note: Assume x is held in EBX, y is held in ECX,<br/>// and z is held in EDX:<br/><br/>mov( ecx, ebx );<br/>add( edx, ebx );</pre>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_394"/>Only two instructions and no memory accesses (for the variables) are required here. This is quite a bit more efficient than the accumulator- or stack-based architectures. From this example, you can see why the register-based architecture has become prevalent in modern computer systems.</p>&#13;
		<p class="indent">As you’ll see in the following sections, register machines are often described as either two-address machines or three-address machines, depending on the particular CPU’s architecture.</p>&#13;
		<h4 class="h4" id="ch00lev2sec151"><strong>12.1.4 Typical Forms of Arithmetic Expressions</strong></h4>&#13;
		<p class="noindent">Computer architects have studied typical source files extensively, and one thing they’ve discovered is that a large percentage of assignment statements take one of the following forms:</p>&#13;
		<pre class="programs">&#13;
			var = var2;<br/>var = constant;<br/>var = op var2;<br/>var = var op var2;<br/>var = var2 op var3;</pre>&#13;
		<p class="indent">Although other assignments do exist, the set of statements in a program that takes one of these forms is generally larger than any other group of assignment statements. Therefore, computer architects usually optimize their CPUs to efficiently handle these forms.</p>&#13;
		<h4 class="h4" id="ch00lev2sec152"><strong>12.1.5 Three-Address Architectures</strong></h4>&#13;
		<p class="noindent">Many machines use a <em>three-address architecture</em>. This means that an arithmetic statement supports three operands: two source operands and a destination operand. For example, most RISC CPUs offer an <span class="literal">add</span> instruction that will add together the values of two operands and store the result into a third operand:</p>&#13;
		<pre class="programs">&#13;
			add <span class="codeitalic1">source1</span>, <span class="codeitalic1">source2</span>, <span class="codeitalic1">dest</span></pre>&#13;
		<p class="indent">On such architectures, the operands are usually machine registers (or small constants), so typically you’d write this instruction as follows (assuming you use the names <em>R</em>0, <em>R</em>1, . . . , <em>Rn</em> to denote registers):</p>&#13;
		<pre class="programs">add r0, r1, r2   ; computes r2 := r0 + r1</pre>&#13;
		<p class="indent">Because RISC compilers attempt to keep variables in registers, this single instruction handles the last assignment statement given in the previous section:</p>&#13;
		<pre class="programs"><span class="codeitalic1">var</span> = <span class="codeitalic1">var2</span> op <span class="codeitalic1">var3</span>;</pre>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_395"/>Handling an assignment of the form:</p>&#13;
		<pre class="programs"><span class="codeitalic1">var</span> = <span class="codeitalic1">var</span> op <span class="codeitalic1">var2</span>;</pre>&#13;
		<p class="indent">is also relatively easy—just use the destination register as one of the source operands, like so:</p>&#13;
		<pre class="programs">add r0, r1, r0  ; computes r0 := r0 + r1</pre>&#13;
		<p class="indent">The drawback to a three-address architecture is that you must encode all three operands into each instruction that supports three operands. This is why three-operand instructions generally operate only upon register operands. Encoding three separate memory addresses can be quite expensive—just ask any VAX programmer. The DEC VAX computer system is a good example of a three-address CISC machine.</p>&#13;
		<h4 class="h4" id="ch00lev2sec153"><strong>12.1.6 Two-Address Architectures</strong></h4>&#13;
		<p class="noindent">The 80x86 architecture is known as a <em>two-address machine</em>. In this architecture, one of the source operands is also the destination operand. Consider the following 80x86/HLA <span class="literal">add</span> instruction:</p>&#13;
		<pre class="programs">add( ebx, eax );  ; computes eax := eax + ebx;</pre>&#13;
		<p class="indent">Two-address machines, such as the 80x86, can handle the first four forms of the assignment statement given earlier with a single instruction. The last form, however, requires two or more instructions and a temporary register. For example, to compute:</p>&#13;
		<pre class="programs"><span class="codeitalic1">var1</span> = <span class="codeitalic1">var2</span> + <span class="codeitalic1">var3</span>;</pre>&#13;
		<p class="indent">you’d need to use the following code (assuming <em>var2</em> and <em>var3</em> are memory variables and the compiler is keeping <em>var1</em> in the EAX register):</p>&#13;
		<pre class="programs">&#13;
			mov( <span class="codeitalic1">var2</span>, eax );<br/>add( <span class="codeitalic1">var3</span>, eax );  //Result (<span class="codeitalic1">var1</span>) is in EAX.</pre>&#13;
		<h4 class="h4" id="ch00lev2sec154"><strong>12.1.7 Architectural Differences and Your Code</strong></h4>&#13;
		<p class="noindent">One-address, two-address, and three-address architectures have the following hierarchy:</p>&#13;
		<p class="center"><strong>1Address</strong> ⊂ <strong>2Address</strong> ⊂ <strong>3Address</strong></p>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_396"/>That is, two-address machines are capable of doing anything a one-address machine can do, and three-address machines are capable of anything one-address or two-address machines can do. The proof is very simple:<sup><a id="ch12fn_2"/><a href="footnotes.xhtml#ch12fn2">2</a></sup></p>&#13;
		<ul>&#13;
			<li>&#13;
				<p class="noindent">To show that a two-address machine is capable of doing anything a one-address machine can do, simply choose one register on the two-address machine and use it as the “accumulator” when simulating a one-address architecture.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">To show that a three-address machine is capable of anything a two-address machine can do, simply use the same register for one of the source operands and the destination operand, thereby limiting yourself to two registers (operands/addresses) for all operations.</p>&#13;
				</li>&#13;
		</ul>&#13;
		<p class="indent">Given this hierarchy, you might think that if you limit the code you write so that it runs well on a one-address machine, you’ll get good results on all machines. In reality, most general-purpose CPUs available today are two- or three-address machines, so writing your code to favor a one-address machine may limit the optimizations that are possible on a two- or three-address machine. Furthermore, optimization quality varies so widely among compilers that backing up an assertion like this would be very difficult. You should probably try to create expressions that take one of the five forms given earlier (in “Typical Forms of Arithmetic Expressions” on <a href="ch12.xhtml#page_394">page 394</a>) if you want your compiler to produce the best possible code. Because most modern programs run on two- or three-address machines, the remainder of this chapter assumes that environment.</p>&#13;
		<h4 class="h4" id="ch00lev2sec155"><strong>12.1.8 Complex Expressions</strong></h4>&#13;
		<p class="noindent">Once your expressions get more complex than the five forms given earlier, the compiler will have to generate a sequence of two or more instructions to evaluate them. When compiling the code, most compilers internally translate a complex expression into a sequence of “three-address statements” that are semantically equivalent to it, as in the following example:</p>&#13;
		<pre class="programs">&#13;
			// complex = ( a + b ) * ( c - d ) - e/f;<br/><br/>temp1 = a + b;<br/>temp2 = c - d;<br/>temp1 = temp1 * temp2;<br/>temp2 = e / f;<br/>complex = temp1 - temp2;</pre>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_397"/>As you can see, these five statements are semantically equivalent to the complex expression appearing in the comment. The major difference in the computation is the introduction of two temporary values (<span class="literal">temp1</span> and <span class="literal">temp2</span>). Most compilers will attempt to use machine registers to maintain these temporary values.</p>&#13;
		<p class="indent">Because the compiler internally translates a complex instruction into a sequence of three-address statements, you may wonder if you can help it by converting complex expressions into three-address statements yourself. Well, it depends on your compiler. For many (good) compilers, breaking a complex calculation into smaller pieces may, in fact, thwart the compiler’s ability to optimize certain sequences. So, when it comes to arithmetic expressions, most of the time you should do your job (write the code as clearly as possible) and let the compiler do its job (optimize the result). However, if you can specify a calculation using a form that naturally converts to a two-address or three-address form, by all means do so. At the very least, it will have no effect on the code the compiler generates. At best, under some special circumstances, it could help the compiler produce better code. Either way, the resulting code will probably be easier to read and maintain if it is less complex.</p>&#13;
		<h3 class="h3" id="ch00lev1sec101"><strong>12.2 Optimization of Arithmetic Statements</strong></h3>&#13;
		<p class="noindent">Because HLL compilers were originally designed to let programmers use algebraic-like expressions in their source code, this is one area in computer science that has been well researched. Most modern compilers that provide a reasonable optimizer do a decent job of translating arithmetic expressions into machine code. You can usually assume that the compiler you’re using doesn’t need a whole lot of help with optimizing arithmetic expressions (and if it does, you might consider switching to a better compiler instead of trying to manually optimize the code).</p>&#13;
		<p class="indent">To help you appreciate the job the compiler is doing for you, this section discusses some of the typical optimizations you can expect from modern optimizing compilers. By understanding what a (decent) compiler does, you can avoid hand-optimizing those things that it is capable of handling.</p>&#13;
		<h4 class="h4" id="ch00lev2sec156"><strong>12.2.1 Constant Folding</strong></h4>&#13;
		<p class="noindent">Constant folding is an optimization that computes the value of constant expressions or subexpressions at compile time rather than emitting code to compute their result at runtime. For example, a Pascal compiler that supports this optimization would translate a statement of the form <span class="literal">i := 5 + 6;</span> to <span class="literal">i := 11;</span> prior to generating machine code for the statement. This saves it from emitting an <span class="literal">add</span> instruction that would have to execute at runtime. As another example, suppose you want to allocate an array containing 16MB of storage. One way to do this is as follows:</p>&#13;
		<pre class="programs">char bigArray[ 16777216 ]; // 16 MB of storage</pre>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_398"/>The only problem with this approach is that 16,777,216 is a magic number. It represents the value 2<sup>24</sup> and not some other arbitrary value. Now consider the following C/C++ declaration:</p>&#13;
		<pre class="programs">char bigArray[ 16*1024*1024 ]; // 16 MB of storage</pre>&#13;
		<p class="indent">Most programmers realize that 1,024 times 1,024 is a binary million, and 16 times this value corresponds to 16 mega-somethings. Yes, you need to recognize that the subexpression <span class="literal">16*1024*1024</span> is equivalent to 16,777,216. But this pattern is easier to recognize as 16MB (at least, when used within a character array) than <span class="literal">16777216</span> (or was it <span class="literal">16777214</span>?). In both cases the amount of storage the compiler allocates is exactly the same, but the second case is, arguably, more readable. Hence, it is better code.<sup><a id="ch12fn_3"/><a href="footnotes.xhtml#ch12fn3">3</a></sup></p>&#13;
		<p class="indent">Variable declarations aren’t the only place a compiler can use this optimization. Any arithmetic expression (or subexpression) containing constant operands is a candidate for constant folding. Therefore, if you can write an arithmetic expression more clearly by using constant expressions rather than computing the results by hand, you should definitely go for the more readable version and leave it up to the compiler to handle the constant calculation at compile time. If your compiler doesn’t support constant folding, you can certainly simulate it by performing all constant calculations manually. However, you should do this only as a last resort. Finding a better compiler is almost always a better choice.</p>&#13;
		<p class="indent">Some good optimizing compilers may take extreme steps when folding constants. For example, some compilers with a sufficiently high optimization level enabled will replace certain function calls, with constant parameters, to the corresponding constant value. For example, a compiler might translate a C/C++ statement of the form <span class="literal">sineR = sin(0);</span> to <span class="literal">sineR = 0;</span> during compilation (as the sine of zero radians is <span class="literal">0</span>). This type of constant folding, however, is not all that common, and you usually have to enable a special compiler mode to get it.</p>&#13;
		<p class="indent">If you ever have any questions about whether your particular compiler supports constant folding, have the compiler generate an assembly listing and look at its output (or view the disassembled output with a debugger). Here’s a trivial case written in C/C++ (compiled with Visual C++):</p>&#13;
		<pre class="programs">&#13;
			#include &lt;stdio.h&gt;<br/>int main(int argc, char **argv)<br/>{<br/>      int i = 16 * 1024 * 1024;<br/>      printf( "%d\n", i);<br/>       return 0;<br/>}<br/><br/><span epub:type="pagebreak" id="page_399"/>// Assembly output for sequence above (optimizations turned off!)<br/><br/>        mov     DWORD PTR i$[rsp], 16777216             ; 01000000H<br/><br/>        mov     edx, DWORD PTR i$[rsp]<br/>        lea     rcx, OFFSET FLAT:$SG7883<br/>        call    printf</pre>&#13;
		<p class="indent">Here’s a comparable program written in Java:</p>&#13;
		<pre class="programs">&#13;
			public class Welcome<br/>{<br/>      public static void main( String[] args )<br/>      {<br/>            int i = 16 * 1024 * 1024;<br/>            System.out.println( i );<br/>      }<br/>}<br/><br/>// JBC generated by the compiler:<br/><br/>javap -c Welcome<br/>Compiled from "Welcome.java"<br/>public class Welcome extends java.lang.Object{<br/>public Welcome();<br/>  Code:<br/>   0:   aload_0<br/><br/>        ; //Method java/lang/Object."&lt;init&gt;":()V<br/>   1:   invokespecial   #1<br/><br/>   4:   return<br/><br/>public static void main(java.lang.String[]);<br/>  Code:<br/>   0:   ldc #2; //int 16777216<br/>   2:   istore_1<br/><br/>        ; //Field java/lang/System.out:Ljava/io/PrintStream;<br/>   3:   getstatic   #3<br/><br/>   6:   iload_1<br/><br/>        ; //Method java/io/PrintStream.println:(I)V<br/>   7:   invokevirtual   #4   10:  return<br/><br/>}</pre>&#13;
		<p class="indent">Note that the <span class="literal">ldc #2</span> instruction pushes a constant from a constant pool onto the stack. The comment attached to this bytecode instruction explains that the Java compiler converted <span class="literal">16*1024*1024</span> into a single constant <span class="literal">16777216</span>. Java performs the constant folding at compile time rather than computing this product at runtime.</p>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_400"/>Here’s the comparable program in Swift, along with the assembly code emitted for the relevant portion<sup><a id="ch12fn_4"/><a href="footnotes.xhtml#ch12fn4">4</a></sup> of the main program:</p>&#13;
		<pre class="programs">&#13;
			import Foundation<br/><br/>var i:Int = 16*1024*1024<br/>print( "i=\(i)" )<br/><br/>// code produced by<br/>// "xcrun -sdk macosx<br/>//       swiftc -O -emit-assembly main.swift -o result.asm"<br/><br/>       movq    $16777216, _$S6result1iSivp(%rip)</pre>&#13;
		<p class="indent">As you can see, Swift also supports the constant folding optimization.</p>&#13;
		<h4 class="h4" id="ch00lev2sec157"><strong>12.2.2 Constant Propagation</strong></h4>&#13;
		<p class="noindent">Constant propagation is an optimization a compiler uses to replace a variable access by a constant value if the compiler determines that it’s possible. For example, a compiler that supports constant propagation will make the following optimization:</p>&#13;
		<pre class="programs">&#13;
			// original code:<br/><br/>    variable = 1234;<br/>    result = f( variable );<br/><br/>// code after constant propagation optimization<br/><br/>    variable = 1234;<br/>    result = f( 1234 );</pre>&#13;
		<p class="indent">In object code, manipulating immediate constants is often more efficient than manipulating variables; therefore, constant propagation often produces much better code. In some cases, constant propagation also allows the compiler to eliminate certain variables and statements altogether (in this example, the compiler could remove <span class="literal">variable = 1234;</span> if there are no later references to the variable object in the source code).</p>&#13;
		<p class="indent">In some cases, well-written compilers can do some outrageous optimizations involving constant folding. Consider the following C code:</p>&#13;
		<pre class="programs">&#13;
			#include &lt;stdio.h&gt;<br/>static int rtn3( void )<br/>{<br/>    return 3;<br/>}<br/><span epub:type="pagebreak" id="page_401"/>int main( void )<br/>{<br/>    printf( "%d", rtn3() + 2 );<br/>    return( 0 );<br/>}</pre>&#13;
		<p class="indent">Here’s the 80x86 output that GCC produces with the <span class="literal">-O3</span> (maximum) optimization option:</p>&#13;
		<pre class="programs">&#13;
			.LC0:<br/>        .string "%d"<br/>        .text<br/>        .p2align 2,,3<br/>.globl main<br/>        .type   main,@function<br/>main:<br/>        ; Build main's activation record:<br/><br/>        pushl   %ebp<br/>        movl    %esp, %ebp<br/>        subl    $8, %esp<br/>        andl    $-16, %esp<br/>        subl    $8, %esp<br/><br/>        ; Print the result of "rtn3()+5":<br/><br/>        pushl   $5      ; Via constant propagation/folding!<br/>        pushl   $.LC0<br/>        call    printf<br/>        xorl    %eax, %eax<br/>        leave<br/>        ret</pre>&#13;
		<p class="indent">A quick glance shows that the <span class="literal">rtn3()</span> function is nowhere to be found. With the <span class="literal">-O3</span> command-line option enabled, GCC figured out that <span class="literal">rtn3()</span> simply returns a constant, so it propagates that constant return result everywhere you call <span class="literal">rtn3()</span>. In the case of the <span class="literal">printf()</span> function call, the combination of constant propagation and constant folding yielded a single constant, <span class="literal">5</span>, that the code passes on to the <span class="literal">printf()</span> function.</p>&#13;
		<p class="indent">As with constant folding, if your compiler doesn’t support constant propagation you can simulate it manually, but only as a last resort. Again, finding a better compiler is almost always a better choice.</p>&#13;
		<p class="indent">You can turn on the compiler’s assembly language output to determine if your compiler support constant propagation. For example, here is Visual C++’s output (with the <span class="literal">/O2</span> optimization level turned on):</p>&#13;
		<pre class="programs">&#13;
			#include &lt;stdio.h&gt;<br/><br/><br/>int f(int a)<br/>{<br/><span epub:type="pagebreak" id="page_402"/>      return a + 1;<br/>}<br/><br/>int main(int argc, char **argv)<br/>{<br/>      int i = 16 * 1024 * 1024;<br/>      int j = f(i);<br/>      printf( "%d\n", j);<br/>}<br/><br/>// Assembly language output for the above code:<br/><br/>main    PROC                                            ; COMDAT<br/><br/>$LN6:<br/>        sub     rsp, 40                                 ; 00000028H<br/><br/>        mov     edx, 16777217                           ; 01000001H<br/>        lea     rcx, OFFSET FLAT:??_C@_02DPKJAMEF@?$CFd?$AA@<br/>        call    printf<br/><br/>        xor     eax, eax<br/>        add     rsp, 40                                 ; 00000028H<br/>        ret     0<br/>main    ENDP</pre>&#13;
		<p class="indent">As you can see, Visual C++ also eliminated the <span class="literal">f()</span> function as well as the <span class="literal">i</span> and <span class="literal">j</span> variables. It computed the function result (<span class="literal">i+1</span>) at compile time and substituted the constant <span class="literal">16777217</span> (<span class="literal">16*1024*1024 + 1</span>) for all the computations.</p>&#13;
		<p class="indent">Here’s an example using Java:</p>&#13;
		<pre class="programs">&#13;
			public class Welcome<br/>{<br/>      public static int f( int a ) { return a+1;}<br/>      public static void main( String[] args )<br/>      {<br/>            int i = 16 * 1024 * 1024;<br/>            int j = f(i);<br/>            int k = i+1;<br/>            System.out.println( j );<br/>            System.out.println( k );<br/>       }<br/>}<br/><br/>// JBC emitted for this Java source code:<br/><br/><br/><br/><br/><br/>javap -c Welcome<br/>Compiled from "Welcome.java"<br/><span epub:type="pagebreak" id="page_403"/>public class Welcome extends java.lang.Object{<br/>public Welcome();<br/>  Code:<br/>   0:   aload_0<br/><br/>        ; //Method java/lang/Object."&lt;init&gt;":()V<br/>   1:   invokespecial   #1<br/>   4:   return<br/><br/>public static int f(int);<br/>  Code:<br/>   0:   iload_0<br/>   1:   iconst_1<br/>   2:   iadd<br/>   3:   ireturn<br/><br/>public static void main(java.lang.String[]);<br/>  Code:<br/>   0:   ldc #2; //int 16777216<br/>   2:   istore_1<br/>   3:   iload_1<br/>   4:   invokestatic    #3; //Method f:(I)I<br/>   7:   istore_2<br/>   8:   iload_1<br/>   9:   iconst_1<br/>   10:  iadd<br/>   11:  istore_3<br/><br/>        ; //Field java/lang/System.out:Ljava/io/PrintStream;<br/>   12:  getstatic   #4<br/>   15:  iload_2<br/><br/>        ; //Method java/io/PrintStream.println:(I)V<br/>   16:  invokevirtual   #5<br/><br/>        ; //Field java/lang/System.out:Ljava/io/PrintStream;<br/>   19:  getstatic   #4<br/>   22:  iload_3<br/><br/>        ; //Method java/io/PrintStream.println:(I)V<br/>   23:  invokevirtual   #5<br/>   26:  return<br/><br/>}</pre>&#13;
		<p class="indent">A quick review of this Java bytecode shows that the Java compiler (<span class="literal">java version "1.6.0_65"</span>) does not support the constant propagation optimization. Not only did it not eliminate the <span class="literal">f()</span> function, but it also doesn’t eliminate variables <span class="literal">i</span> and <span class="literal">j</span>, and it passes the value of <span class="literal">i</span> to function <span class="literal">f()</span> rather than passing the appropriate constant. One could argue that Java’s bytecode interpretation dramatically affects performance, so a simple optimization such as constant propagation won’t impact performance that much.</p>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_404"/>Here’s the comparable program written in Swift, with the compiler’s assembly output:</p>&#13;
		<pre class="programs">&#13;
			import Foundation<br/><br/>func f( _ a:Int ) -&gt; Int<br/>{<br/>    return a + 1<br/>}<br/>let i:Int = 16*1024*1024<br/>let j = f(i)<br/>print( "i=\(i), j=\(j)" )<br/><br/>// Assembly output via the command:<br/>// xcrun -sdk macosx swiftc -O -emit-assembly main.swift -o result.asm<br/><br/>    movq    $16777216, _$S6result1iSivp(%rip)<br/>    movq    $16777217, _$S6result1jSivp(%rip)<br/>    .<br/>    .   // Lots of code that has nothing to do with the Swift source<br/>    .<br/>    movl    $16777216, %edi<br/>    callq   _$Ss26_toStringReadOnlyPrintableySSxs06CustomB11ConvertibleRzlFSi_Tg5<br/>    .<br/>    .<br/>    .<br/>    movl    $16777217, %edi<br/>    callq   _$Ss26_toStringReadOnlyPrintableySSxs06CustomB11ConvertibleRzlFSi_Tg5</pre>&#13;
		<p class="indent">The Swift compiler generates a tremendous amount of code in support of its runtime system, so you can hardly call Swift an <em>optimizing</em> compiler. That being said, the assembly code that it does generate demonstrates that Swift supports the constant propagation optimization. It eliminates the function <span class="literal">f()</span> and propagates the constants resulting from the calculations into the calls that print the values of <span class="literal">i</span> and <span class="literal">j</span>. It doesn’t eliminate <span class="literal">i</span> and <span class="literal">j</span> (probably because of some consistency issues regarding the runtime system), but it does propagate the constants through the compiled code.</p>&#13;
		<p class="indent">Given the excessive amount of code that the Swift compiler generates, it’s questionable whether this optimization is worthwhile. However, even with all the extra code (too much to print here, so feel free to look at it yourself), the output still runs faster than interpreted Java code.</p>&#13;
		<h4 class="h4" id="ch00lev2sec158"><strong>12.2.3 Dead Code Elimination</strong></h4>&#13;
		<p class="noindent">Dead code elimination is the removal of the object code associated with a particular source code statement if the program never again uses the result of that statement. Often, this is a result of a programming error. (After all, why would someone compute a value and not use it?) If a compiler encounters dead code in the source file, it may warn you to check the logic of your code. In some cases, however, earlier optimizations can produce dead code. For example, the constant propagation for the value variable <span epub:type="pagebreak" id="page_405"/>in the earlier example could result in the statement <span class="literal">variable = 1234;</span> being dead. Compilers that support dead code elimination will quietly remove the object code for this statement from the object file.</p>&#13;
		<p class="indent">As an example of dead code elimination, consider the following C program and its corresponding assembly code:</p>&#13;
		<pre class="programs">&#13;
			static int rtn3( void )<br/>{<br/>    return 3;<br/>}<br/><br/>int main( void )<br/>{<br/>    int i = rtn3() + 2;<br/><br/>    // Note that this program<br/>    // never again uses the value of i.<br/><br/>    return( 0 );<br/>}</pre>&#13;
		<p class="indent">Here’s the 32-bit 80x86 code GCC emits when supplied the <span class="literal">-O3</span> command-line option:</p>&#13;
		<pre class="programs">&#13;
			.file   "t.c"<br/>        .text<br/>        .p2align 2,,3<br/>.globl main<br/>        .type   main,@function<br/>main:<br/>        ; Build main's activation record:<br/><br/>        pushl   %ebp<br/>        movl    %esp, %ebp<br/>        subl    $8, %esp<br/>        andl    $-16, %esp<br/><br/>        ; Notice that there is no<br/>        ; assignment to i here.<br/><br/>        ; Return 0 as main's function result.<br/><br/>        xorl    %eax, %eax<br/>        leave<br/>        ret</pre>&#13;
		<p class="indent">Now consider the 80x86 output from GCC when optimization is not enabled:</p>&#13;
		<pre class="programs">&#13;
			.file   "t.c"<br/>        .text<br/>        .type   rtn3,@function<br/>rtn3:<br/><span epub:type="pagebreak" id="page_406"/>        pushl   %ebp<br/>        movl    %esp, %ebp<br/>        movl    $3, %eax<br/>        leave<br/>        ret<br/>.Lfe1:<br/>        .size   rtn3,.Lfe1-rtn3<br/>.globl main<br/>        .type   main,@function<br/>main:<br/>        pushl   %ebp<br/>        movl    %esp, %ebp<br/>        subl    $8, %esp<br/>        andl    $-16, %esp<br/>        movl    $0, %eax<br/>        subl    %eax, %esp<br/><br/>        ; Note the call and computation:<br/><br/>        call    rtn3<br/>        addl    $2, %eax<br/>        movl    %eax, -4(%ebp)<br/><br/>        ; Return 0 as the function result.<br/><br/>        movl    $0, %eax<br/>        leave<br/>        ret</pre>&#13;
		<p class="indent">In fact, one of the main reasons that program examples throughout this book call a function like <span class="literal">printf()</span> to display various values is to explicitly use those values to prevent dead code elimination from erasing the code we’re examining from the assembly output file. If you remove the final <span class="literal">printf()</span> from the C program in many of these examples, most of the assembly code will disappear because of dead code elimination.</p>&#13;
		<p class="indent">Here’s the output from the previous C++ code from Visual C++:</p>&#13;
		<pre class="programs">&#13;
			; Listing generated by Microsoft (R) Optimizing Compiler Version 19.00.24234.1<br/><br/>include listing.inc<br/><br/>INCLUDELIB LIBCMT<br/>INCLUDELIB OLDNAMES<br/><br/>PUBLIC  main<br/>; Function compile flags: /Ogtpy<br/>; File c:\users\rhyde\test\t\t\t.cpp<br/>_TEXT   SEGMENT<br/>main    PROC<br/><br/>        xor     eax, eax<br/><br/>        ret     0<br/><span epub:type="pagebreak" id="page_407"/>main    ENDP<br/>_TEXT   ENDS<br/>; Function compile flags: /Ogtpy<br/>; File c:\users\rhyde\test\t\t\t.cpp<br/>_TEXT   SEGMENT<br/>rtn3    PROC<br/><br/>        mov     eax, 3<br/><br/>        ret     0<br/>rtn3    ENDP<br/>_TEXT   ENDS<br/>END</pre>&#13;
		<p class="indent">Unlike GCC, Visual C++ did not eliminate the <span class="literal">rtn3()</span> function. However, it did remove the assignment to <span class="literal">i</span>—and the call to <span class="literal">rtn3()</span>—in the main program.</p>&#13;
		<p class="indent">Here’s the equivalent Java program and the JBC output:</p>&#13;
		<pre class="programs">&#13;
			public class Welcome<br/>{<br/>    public static int rtn3() { return 3;}<br/>    public static void main( String[] args )<br/>    {<br/>        int i = rtn3();<br/>    }<br/>}<br/><br/>// JBC output:<br/><br/>public class Welcome extends java.lang.Object{<br/>public Welcome();<br/>  Code:<br/>   0:   aload_0<br/><br/>        ; //Method java/lang/Object."&lt;init&gt;":()V<br/>   1:   invokespecial   #1<br/>   4:   return<br/><br/>public static int rtn3();<br/>  Code:<br/>   0:   iconst_3<br/>   1:   ireturn<br/><br/>public static void main(java.lang.String[]);<br/>  Code:<br/>   0:   invokestatic    #2; //Method rtn3:()I<br/>   3:   istore_1<br/>   4:   return<br/>}</pre>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_408"/>At first blush, it looks like Java does not support dead code elimination. However, the problem might be that our example code doesn’t trigger this optimization in the compiler. Let’s try something more obvious to the compiler:</p>&#13;
		<pre class="programs">&#13;
			public class Welcome<br/>{<br/>    public static int rtn3() { return 3;}<br/>    public static void main( String[] args )<br/>    {<br/>        if( false )<br/>        {   int i = rtn3();<br/>        }<br/>    }<br/>}<br/><br/>// Here's the output bytecode:<br/><br/>Compiled from "Welcome.java"<br/>public class Welcome extends java.lang.Object{<br/>public Welcome();<br/>  Code:<br/>   0:   aload_0<br/><br/>        ; //Method java/lang/Object."&lt;init&gt;":()V<br/>   1:   invokespecial   #1<br/>   4:   return<br/><br/>public static int rtn3();<br/>  Code:<br/>   0:   iconst_3<br/>   1:   ireturn<br/><br/>public static void main(java.lang.String[]);<br/>  Code:<br/>   0:   return<br/>}</pre>&#13;
		<p class="indent">Now we’ve given the Java compiler something it can chew on. The main program eliminates the call to <span class="literal">rtn3()</span> and the assignment to <span class="literal">i</span>. The optimization isn’t quite as smart as GCC’s or Visual C++’s optimization, but (at least) for some cases, it works. Unfortunately, without constant propagation, Java misses many opportunities for dead code elimination.</p>&#13;
		<p class="indent">Here’s the equivalent Swift code for the earlier example:</p>&#13;
		<pre class="programs">&#13;
			import Foundation<br/><br/>func rtn3() -&gt; Int<br/>{<br/>    return 3<br/>}<br/>let i:Int = rtn3()<br/><span epub:type="pagebreak" id="page_409"/>// Assembly language output:<br/><br/>_main:<br/>    pushq   %rbp<br/>    movq    %rsp, %rbp<br/>    movq    $3, _$S6result1iSivp(%rip)<br/>    xorl    %eax, %eax<br/>    popq    %rbp<br/>    retq<br/><br/>    .private_extern _$S6result4rtn3SiyF<br/>    .globl  _$S6result4rtn3SiyF<br/>    .p2align    4, 0x90<br/>_$S6result4rtn3SiyF:<br/>    pushq   %rbp<br/>    movq    %rsp, %rbp<br/>    movl    $3, %eax<br/>    popq    %rbp<br/>    retq</pre>&#13;
		<p class="indent">Note that Swift (at least for this example) does not support dead code elimination. However, let’s try the same thing we did with Java. Consider the following code:</p>&#13;
		<pre class="programs">&#13;
			import Foundation<br/><br/>func rtn3() -&gt; Int<br/>{<br/>    return 3<br/>}<br/>if false<br/>{<br/>    let i:Int = rtn3()<br/>}<br/><br/>// Assembly output<br/><br/>_main:<br/>    pushq   %rbp<br/>    movq    %rsp, %rbp<br/>    xorl    %eax, %eax<br/>    popq    %rbp<br/>    retq<br/><br/>    .private_extern _$S6result4rtn3SiyF<br/>    .globl  _$S6result4rtn3SiyF<br/>    .p2align    4, 0x90<br/>_$S6result4rtn3SiyF:<br/>    pushq   %rbp<br/>    movq    %rsp, %rbp<br/>    movl    $3, %eax<br/>    popq    %rbp<br/>    retq</pre>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_410"/>Compiling this code produces a list of warnings about the dead code, but the output demonstrates that Swift does support dead code elimination. Furthermore, because Swift supports constant propagation as well, it won’t miss as many opportunities for dead code elimination as Java (though Swift will need to mature a bit more before it catches up to GCC or Visual C++).</p>&#13;
		<h4 class="h4" id="ch00lev2sec159"><strong>12.2.4 Common Subexpression Elimination</strong></h4>&#13;
		<p class="noindent">Often, a portion of some expressions—a <em>subexpression</em>—may appear elsewhere in the current function. If there are no changes to the values of the variables appearing in the subexpression, the program doesn’t need to compute its value twice. Instead, it can save the subexpression’s value on the first evaluation and then use that value everywhere the subexpression appears again. For example, consider the following Pascal code:</p>&#13;
		<pre class="programs">&#13;
			complex := ( a + b ) * ( c - d ) - ( e div f );<br/>lessSo  := ( a + b ) - ( e div f );<br/>quotient := e div f;</pre>&#13;
		<p class="indent">A decent compiler might translate these to the following sequence of three-address statements:</p>&#13;
		<pre class="programs">&#13;
			temp1 := a + b;<br/>temp2 := c - d;<br/>temp3 := e div f;<br/>complex := temp1 * temp2;<br/>complex := complex - temp3;<br/>lessSo := temp1 - temp3;<br/>quotient := temp3;</pre>&#13;
		<p class="indent">Although the former statements use the subexpression <span class="literal">(a + b)</span> twice and the subexpression <span class="literal">(e div f)</span> three times, the three-address code sequence computes these subexpressions only once and uses their values when the common subexpressions appear later.</p>&#13;
		<p class="indent">As another example, consider the following C/C++ code:</p>&#13;
		<pre class="programs">&#13;
			#include &lt;stdio.h&gt;<br/><br/>static int i, j, k, m, n;<br/>static int <span class="codeitalic1">expr1</span>, <span class="codeitalic1">expr2</span>, <span class="codeitalic1">expr3</span>;<br/><br/>extern int <span class="codeitalic1">someFunc</span>( void );<br/><br/>int main( void )<br/>{<br/>    // The following is a trick to<br/>    // confuse the optimizer. When we call<br/>    // an external function, the optimizer<br/>    // knows nothing about the value this<br/>    // function returns, so it cannot optimize<br/>    // the values away. This is done to demonstrate<br/>    // the optimizations that this example is<br/><span epub:type="pagebreak" id="page_411"/>    // trying to show (that is, the compiler<br/>    // would normally optimize away everything<br/>    // and we wouldn't see the code the optimizer<br/>    // would produce in a real-world example without<br/>    // the following trick).<br/><br/>    i = <span class="codeitalic1">someFunc</span>();<br/>    j = <span class="codeitalic1">someFunc</span>();<br/>    k = <span class="codeitalic1">someFunc</span>();<br/>    m = <span class="codeitalic1">someFunc</span>();<br/>    n = <span class="codeitalic1">someFunc</span>();<br/><br/>    <span class="codeitalic1">expr1</span> = (i+j) * (k*m+n);<br/>    <span class="codeitalic1">expr2</span> = (i+j);<br/>    <span class="codeitalic1">expr3</span> = (k*m+n);<br/><br/>    printf( "%d %d %d", <span class="codeitalic1">expr1</span>, <span class="codeitalic1">expr2</span>, <span class="codeitalic1">expr3</span> );<br/>    return( 0 );<br/>}</pre>&#13;
		<p class="indent">Here’s the 32-bit 80x86 assembly file that GCC generates (with the <span class="literal">-O3</span> option) for the preceding C code:</p>&#13;
		<pre class="programs">&#13;
			.file   "t.c"<br/>        .section        .rodata.str1.1,"aMS",@progbits,1<br/>.LC0:<br/>        .string "%d %d %d"<br/>        .text<br/>        .p2align 2,,3<br/>.globl main<br/>        .type   main,@function<br/>main:<br/>        ; Build the activation record:<br/><br/>        pushl   %ebp<br/>        movl    %esp, %ebp<br/>        subl    $8, %esp<br/>        andl    $-16, %esp<br/><br/>        ; Initialize i, j, k, m, and n:<br/><br/>        call    <span class="codeitalic1">someFunc</span><br/>        movl    %eax, i<br/>        call    <span class="codeitalic1">someFunc</span><br/>        movl    %eax, j<br/>        call    <span class="codeitalic1">someFunc</span><br/>        movl    %eax, k<br/>        call    <span class="codeitalic1">someFunc</span><br/>        movl    %eax, m<br/>        call    <span class="codeitalic1">someFunc</span> ;n's value is in EAX.<br/><br/>        ; Compute EDX = k*m+n<br/>        ; and ECX = i+j<br/><span epub:type="pagebreak" id="page_412"/>        movl    m, %edx<br/>        movl    j, %ecx<br/>        imull   k, %edx<br/>        addl    %eax, %edx<br/>        addl    i, %ecx<br/><br/>        ; EDX is <span class="codeitalic1">expr3</span>, so push it<br/>        ; on the stack for printf<br/><br/>        pushl   %edx<br/><br/>        ; Save away n's value:<br/><br/>        movl    %eax, n<br/>        movl    %ecx, %eax<br/><br/>        ; ECX is <span class="codeitalic1">expr2</span>, so push it onto<br/>        ; the stack for printf:<br/><br/>        pushl   %ecx<br/><br/>        ; <span class="codeitalic1">expr1</span> is the product of the<br/>        ; two subexpressions (currently<br/>        ; held in EDX and EAX), so compute<br/>        ; their product and push the result<br/>        ; for printf.<br/><br/>        imull   %edx, %eax<br/>        pushl   %eax<br/><br/>        ; Push the address of the format string<br/>        ; for printf:<br/><br/>        pushl   $.LC0<br/><br/>        ; Save the variable's values and then<br/>        ; call printf to print the values<br/><br/>        movl    %eax, <span class="codeitalic1">expr1</span><br/>        movl    %ecx, <span class="codeitalic1">expr2</span><br/>        movl    %edx, <span class="codeitalic1">expr3</span><br/>        call    printf<br/><br/>        ; Return 0 as the main function's result:<br/><br/>        xorl    %eax, %eax<br/>        leave<br/>        ret</pre>&#13;
		<p class="indent">Note how the compiler maintains the results of the common subexpressions in various registers (see the comments in the assembly output for details).</p>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_413"/>Here’s the (64-bit) output from Visual C++:</p>&#13;
		<pre class="programs">&#13;
			_TEXT   SEGMENT<br/>main    PROC<br/><br/>$LN4:<br/>        sub     rsp, 40                                 ; 00000028H<br/><br/>        call    <span class="codeitalic1">someFunc</span><br/>        mov     DWORD PTR i, eax<br/><br/>        call    <span class="codeitalic1">someFunc</span><br/>        mov     DWORD PTR j, eax<br/><br/>        call    <span class="codeitalic1">someFunc</span><br/>        mov     DWORD PTR k, eax<br/><br/>        call    <span class="codeitalic1">someFunc</span><br/>        mov     DWORD PTR m, eax<br/><br/>        call    <span class="codeitalic1">someFunc</span><br/><br/>        mov     r9d, DWORD PTR m<br/><br/>        lea     rcx, OFFSET FLAT:$SG7892<br/>        imul    r9d, DWORD PTR k<br/>        mov     r8d, DWORD PTR j<br/>        add     r8d, DWORD PTR i<br/>        mov     edx, r8d<br/>        mov     DWORD PTR n, eax<br/>        mov     DWORD PTR <span class="codeitalic1">expr2</span>, r8d<br/>        add     r9d, eax<br/>        imul    edx, r9d<br/>        mov     DWORD PTR <span class="codeitalic1">expr3</span>, r9d<br/>        mov     DWORD PTR <span class="codeitalic1">expr1</span>, edx<br/>        call    printf<br/><br/>        xor     eax, eax<br/><br/>        add     rsp, 40                                 ; 00000028H<br/>        ret     0<br/>main    ENDP<br/>_TEXT   ENDS</pre>&#13;
		<p class="indent">Because of the extra registers available on the x86-64, Visual C++ was able to keep all the temporaries in registers and did an even better job of reusing precomputed values for common subexpressions.</p>&#13;
		<p class="indent">If the compiler you’re using doesn’t support common subexpression optimizations (you can determine this by examining the assembly output), chances are pretty good that its optimizer is subpar, and you should consider using a different compiler. However, in the meantime, you can always <span epub:type="pagebreak" id="page_414"/>explicitly code this optimization yourself. Consider this version of the former C code, which manually computes common subexpressions:</p>&#13;
		<pre class="programs">&#13;
			#include &lt;stdio.h&gt;<br/><br/>static int i, j, k, m, n;<br/>static int <span class="codeitalic1">expr1</span>, <span class="codeitalic1">expr2</span>, <span class="codeitalic1">expr3</span>;<br/>static int ijExpr, kmnExpr;<br/><br/>extern int <span class="codeitalic1">someFunc</span>( void );<br/><br/>int main( void )<br/>{<br/>    // The following is a trick to<br/>    // confuse the optimizer. By calling<br/>    // an external function, the optimizer<br/>    // knows nothing about the value this<br/>    // function returns, so it cannot optimize<br/>    // the values away because of constant propagation.<br/><br/>    i = <span class="codeitalic1">someFunc</span>();<br/>    j = <span class="codeitalic1">someFunc</span>();<br/>    k = <span class="codeitalic1">someFunc</span>();<br/>    m = <span class="codeitalic1">someFunc</span>();<br/>    n = <span class="codeitalic1">someFunc</span>();<br/><br/>    ijExpr = i+j;<br/>    kmnExpr = (k*m+n);<br/>    <span class="codeitalic1">expr1</span> = ijExpr * kmnExpr;<br/>    <span class="codeitalic1">expr2</span> = ijExpr;<br/>    <span class="codeitalic1">expr3</span> = kmnExpr;<br/><br/>    printf( "%d %d %d", <span class="codeitalic1">expr1</span>, <span class="codeitalic1">expr2</span>, <span class="codeitalic1">expr3</span> );<br/>    return( 0 );<br/>}</pre>&#13;
		<p class="indent">Of course, there was no reason to create the <span class="literal">ijExpr</span> and <span class="literal">kmnExpr</span> variables, as we could have simply used the <em>expr2</em> and <em>expr3</em> variables for this purpose. However, this code was written to make the changes to the original program as obvious as possible.</p>&#13;
		<p class="indent">Here’s the similar Java code:</p>&#13;
		<pre class="programs">&#13;
			public class Welcome<br/>{<br/>    public static int <span class="codeitalic1">someFunc</span>() { return 1;}<br/>    public static void main( String[] args )<br/>    {<br/>        int i = <span class="codeitalic1">someFunc</span>();<br/>        int j = <span class="codeitalic1">someFunc</span>();<br/>        int k = <span class="codeitalic1">someFunc</span>();<br/>        int m = <span class="codeitalic1">someFunc</span>();<br/>        int n = <span class="codeitalic1">someFunc</span>();<br/><span epub:type="pagebreak" id="page_415"/>        int <span class="codeitalic1">expr1</span> = (i + j) * (k*m + n);<br/>        int <span class="codeitalic1">expr2</span> = (i + j);<br/>        int <span class="codeitalic1">expr3</span> = (k*m + n);<br/>    }<br/>}<br/><br/>// JBC output<br/><br/>public class Welcome extends java.lang.Object{<br/>public Welcome();<br/>  Code:<br/>   0:   aload_0<br/><br/>        ; //Method java/lang/Object."&lt;init&gt;":()V<br/>   1:   invokespecial   #1<br/>   4:   return<br/><br/>public static int <span class="codeitalic1">someFunc</span>();<br/>  Code:<br/>   0:   iconst_1<br/>   1:   ireturn<br/><br/>public static void main(java.lang.String[]);<br/>  Code:<br/>   0:   invokestatic    #2; //Method <span class="codeitalic1">someFunc</span>:()I<br/>   3:   istore_1<br/>   4:   invokestatic    #2; //Method <span class="codeitalic1">someFunc</span>:()I<br/>   7:   istore_2<br/>   8:   invokestatic    #2; //Method <span class="codeitalic1">someFunc</span>:()I<br/>   11:  istore_3<br/>   12:  invokestatic    #2; //Method <span class="codeitalic1">someFunc</span>:()I<br/>   15:  istore  4<br/>   17:  invokestatic    #2; //Method <span class="codeitalic1">someFunc</span>:()I<br/>   20:  istore  5<br/>; iexpr1 = (i + j) * (k*m + n);<br/>   22:  iload_1<br/>   23:  iload_2<br/>   24:  iadd<br/>   25:  iload_3<br/>   26:  iload   4<br/>   28:  imul<br/>   29:  iload   5<br/>   31:  iadd<br/>   32:  imul<br/>   33:  istore  6<br/>; iexpr2 = (i+j)<br/>   35:  iload_1<br/>   36:  iload_2<br/>   37:  iadd<br/>   38:  istore  7<br/>; iexpr3 = (k*m + n)<br/>   40:  iload_3<br/>   41:  iload   4<br/>   43:  imul<br/>   44:  iload   5<br/><span epub:type="pagebreak" id="page_416"/>   46:  iadd<br/>   47:  istore  8<br/>   49:  return<br/>}</pre>&#13;
		<p class="indent">Notice that Java does not optimize common subexpressions; instead, it recomputes the subexpressions each time it encounters them. Therefore, you should manually compute the values of common subexpressions when writing Java code.</p>&#13;
		<p class="indent">Here’s a variant of the current example in Swift (along with the assembly output):</p>&#13;
		<pre class="programs">&#13;
			import Foundation<br/><br/>func <span class="codeitalic1">someFunc</span>() -&gt; UInt32<br/>{<br/>    return arc4random_uniform(100)<br/>}<br/>let i = <span class="codeitalic1">someFunc</span>()<br/>let j = <span class="codeitalic1">someFunc</span>()<br/>let k = <span class="codeitalic1">someFunc</span>()<br/>let m = <span class="codeitalic1">someFunc</span>()<br/>let n = <span class="codeitalic1">someFunc</span>()<br/><br/>let <span class="codeitalic1">expr1</span> = (i+j) * (k*m+n)<br/>let <span class="codeitalic1">expr2</span> = (i+j)<br/>let <span class="codeitalic1">expr3</span> = (k*m+n)<br/>print( "\(<span class="codeitalic1">expr1</span>), \(<span class="codeitalic1">expr2</span>), \(<span class="codeitalic1">expr3</span>)" )<br/><br/>// Assembly output for the above expressions:<br/><br/>; Code for the function calls:<br/><br/>    movl    $0x64, %edi<br/>    callq   arc4random_uniform<br/>    movl    %eax, %ebx  ; EBX = i<br/>    movl    %ebx, _$S6result1is6UInt32Vvp(%rip)<br/>    callq   _arc4random<br/>    movl    %eax, %r12d ; R12d = j<br/>    movl    %r12d, _$S6result1js6UInt32Vvp(%rip)<br/>    callq   _arc4random<br/>    movl    %eax, %r14d ; R14d = k<br/>    movl    %r14d, _$S6result1ks6UInt32Vvp(%rip)<br/>    callq   _arc4random<br/>    movl    %eax, %r15d ; R15d = m<br/>    movl    %r15d, _$S6result1ms6UInt32Vvp(%rip)<br/>    callq   _arc4random<br/>    movl    %eax, %esi  ; ESI = n<br/>    movl    %esi, _$S6result1ns6UInt32Vvp(%rip)<br/><br/>; Code for the expressions:<br/><br/>    addl    %r12d, %ebx ; R12d = i + j (which is <span class="codeitalic1">expr2</span>)<br/><span epub:type="pagebreak" id="page_417"/>    jb  LBB0_11         ; Branch if overflow occurs<br/><br/>    movl    %r14d, %eax ;<br/>    mull    %r15d<br/>    movl    %eax, %ecx  ; ECX = k*m<br/>    jo  LBB0_12         ; Bail if overflow<br/>    addl    %esi, %ecx  ; ECX = k*m + n (which is <span class="codeitalic1">expr3</span>)<br/>    jb  LBB0_13         ; Bail if overflow<br/><br/>    movl    %ebx, %eax<br/>    mull    %ecx        ; <span class="codeitalic1">expr1</span> = (i+j) * (k*m+n)<br/>    jo  LBB0_14         ; Bail if overflow<br/>    movl    %eax, _$S6result5expr1s6UInt32Vvp(%rip)<br/>    movl    %ebx, _$S6result5expr2s6UInt32Vvp(%rip)<br/>    movl    %ecx, _$S6result5expr3s6UInt32Vvp(%rip)</pre>&#13;
		<p class="indent">If you carefully read through this code, you can see the Swift compiler properly optimizes away the common subexpressions and computes each subexpression only once.</p>&#13;
		<h4 class="h4" id="ch00lev2sec160"><strong>12.2.5 Strength Reduction</strong></h4>&#13;
		<p class="noindent">Often, the CPU can directly compute some value using a different operator than the source code specifies, thereby replacing a more complex (or stronger) instruction with a simpler instruction. For example, a <span class="literal">shift</span> operation can implement multiplication or division by a constant that is a power of 2, and certain modulo (remainder) operations are possible using a bitwise <span class="literal">and</span> instruction (the <span class="literal">shift</span> and <span class="literal">and</span> instructions generally execute much faster than <span class="literal">multiply</span> and <span class="literal">divide</span> instructions). Most compiler optimizers are good at recognizing such operations and replacing the more expensive computation with a less expensive sequence of machine instructions. To see strength reduction in action, consider this C code and the 80x86 GCC output that follows it:</p>&#13;
		<pre class="programs">&#13;
			#include &lt;stdio.h&gt;<br/><br/>unsigned i, j, k, m, n;<br/><br/>extern unsigned <span class="codeitalic1">someFunc</span>( void );<br/>extern void preventOptimization( unsigned <span class="codeitalic1">arg1</span>, ... );<br/><br/>int main( void )<br/>{<br/>    // The following is a trick to<br/>    // confuse the optimizer. By calling<br/>    // an external function, the optimizer<br/>    // knows nothing about the value this<br/>    // function returns, so it cannot optimize<br/>    // the values away.<br/><br/>    i = <span class="codeitalic1">someFunc</span>();<br/>    j = i * 2;<br/>    k = i % 32;<br/><span epub:type="pagebreak" id="page_418"/>    m = i / 4;<br/>    n = i * 8;<br/><br/>    // The following call to "preventOptimization" is done<br/>    // to trick the compiler into believing the above results<br/>    // are used somewhere (GCC will eliminate all the code<br/>    // above if you don't actually use the computed result,<br/>    // and that would defeat the purpose of this example).<br/><br/>    preventOptimization( i,j,k,m,n);<br/>    return( 0 );<br/>}</pre>&#13;
		<p class="indent">Here’s the resulting 80x86 code generated by GCC:</p>&#13;
		<pre class="programs">&#13;
			.file   "t.c"<br/>        .text<br/>        .p2align 2,,3<br/>.globl main<br/>        .type   main,@function<br/>main:<br/>        ; Build main's activation record:<br/><br/>        pushl   %ebp<br/>        movl    %esp, %ebp<br/>        pushl   %esi<br/>        pushl   %ebx<br/>        andl    $-16, %esp<br/><br/>        ; Get i's value into EAX:<br/><br/>        call    <span class="codeitalic1">someFunc</span><br/><br/>        ; compute i*8 using the scaled-<br/>        ; indexed addressing mode and<br/>        ; the LEA instruction (leave<br/>        ; n's value in EDX):<br/><br/>        leal    0(,%eax,8), %edx<br/><br/>        ; Adjust stack for call to<br/>        ; preventOptimization:<br/><br/>        subl    $12, %esp<br/><br/>        movl    %eax, %ecx      ; ECX = i<br/>        pushl   %edx            ; Push n for call<br/>        movl    %eax, %ebx      ; Save i in k<br/>        shrl    $2, %ecx        ; ECX = i/4 (m)<br/>        pushl   %ecx            ; Push m for call<br/><br/>        andl    $31, %ebx       ; EBX = i % 32<br/>        leal    (%eax,%eax), %esi ;j=i*2<br/><span epub:type="pagebreak" id="page_419"/>        pushl   %ebx            ; Push k for call<br/>        pushl   %esi            ; Push j for call<br/>        pushl   %eax            ; Push i for call<br/>        movl    %eax, i         ; Save values in memory<br/>        movl    %esi, j         ; variables.<br/>        movl    %ebx, k<br/>        movl    %ecx, m<br/>        movl    %edx, n<br/>        call    preventOptimization<br/><br/>        ; Clean up the stack and return<br/>        ; 0 as main's result:<br/><br/>        leal    -8(%ebp), %esp<br/>        popl    %ebx<br/>        xorl    %eax, %eax<br/>        popl    %esi<br/>        leave<br/>        ret<br/>.Lfe1:<br/>        .size   main,.Lfe1-main<br/>        .comm   i,4,4<br/>        .comm   j,4,4<br/>        .comm   k,4,4<br/>        .comm   m,4,4<br/>        .comm   n,4,4</pre>&#13;
		<p class="indent">In this 80x86 code, note that GCC never emitted a multiplication or division instruction, even though the C code used these two operators extensively. GCC replaced each of these (expensive) operations with less expensive address calculations, shifts, and logical AND operations.</p>&#13;
		<p class="indent">This C example declared its variables as <span class="literal">unsigned</span> rather than as <span class="literal">int</span>. There’s a very good reason for this modification: strength reduction produces more efficient code for certain unsigned operands than it does for signed operands. This is a very important point: if you have a choice between using either signed or unsigned integer operands, always try to use unsigned values, because compilers can often generate better code when processing unsigned operands. To see the difference, here’s the previous C code rewritten using signed integers, followed by GCC’s 80x86 output:</p>&#13;
		<pre class="programs">&#13;
			#include &lt;stdio.h&gt;<br/><br/>int i, j, k, m, n;<br/><br/>extern int <span class="codeitalic1">someFunc</span>( void );<br/>extern void preventOptimization( int <span class="codeitalic1">arg1</span>, ... );<br/><br/>int main( void )<br/>{<br/>    // The following is a trick to<br/>    // confuse the optimizer. By calling<br/>    // an external function, the optimizer<br/>    // knows nothing about the value this<br/><span epub:type="pagebreak" id="page_420"/>    // function returns, so it cannot optimize<br/>    // the values away. That is, this prevents<br/>    // constant propagation from computing all<br/>    // the following values at compile time.<br/><br/>    i = <span class="codeitalic1">someFunc</span>();<br/>    j = i * 2;<br/>    k = i % 32;<br/>    m = i / 4;<br/>    n = i * 8;<br/><br/>    // The following call to "preventOptimization"<br/>    // prevents dead code elimination of all the<br/>    // preceding statements.<br/><br/>    preventOptimization( i,j,k,m,n);<br/>    return( 0 );<br/>}</pre>&#13;
		<p class="indent">Here is GCC’s (32-bit) 80x86 assembly output for this C code:</p>&#13;
		<pre class="programs">&#13;
			.file   "t.c"<br/>        .text<br/>        .p2align 2,,3<br/>        .globl main<br/>        .type   main,@function<br/>main:<br/>        ; Build main's activation record:<br/><br/>        pushl   %ebp<br/>        movl    %esp, %ebp<br/>        pushl   %esi<br/>        pushl   %ebx<br/>        andl    $-16, %esp<br/><br/>        ; Call <span class="codeitalic1">someFunc</span> to get i's value:<br/><br/>        call    someFunc<br/>        leal    (%eax,%eax), %esi ; j = i * 2<br/>        testl   %eax, %eax        ; Test i's sign<br/>        movl    %eax, %ecx<br/>        movl    %eax, i<br/>        movl    %esi, j<br/>        js      .L4<br/><br/>; Here's the code we execute if i is non-negative:<br/><br/>.L2:<br/>        andl    $-32, %eax       ; MOD operation<br/>        movl    %ecx, %ebx<br/>        subl    %eax, %ebx<br/>        testl   %ecx, %ecx       ; Test i's sign<br/>        movl    %ebx, k<br/>        movl    %ecx, %eax<br/><span epub:type="pagebreak" id="page_421"/>        js      .L5<br/>.L3:<br/>        subl    $12, %esp<br/>        movl    %eax, %edx<br/>        leal    0(,%ecx,8), %eax ; i*8<br/>        pushl   %eax<br/>        sarl    $2, %edx         ; Signed div by 4<br/>        pushl   %edx<br/>        pushl   %ebx<br/>        pushl   %esi<br/>        pushl   %ecx<br/>        movl    %eax, n<br/>        movl    %edx, m<br/>        call    preventOptimization<br/>        leal    -8(%ebp), %esp<br/>        popl    %ebx<br/>        xorl    %eax, %eax<br/>        popl    %esi<br/>        leave<br/>        ret<br/>        .p2align 2,,3<br/><br/>; For signed division by 4,<br/>; using a sarl operation, we need<br/>; to add 3 to i's value if i was<br/>; negative.<br/><br/>.L5:<br/>        leal    3(%ecx), %eax<br/>        jmp     .L3<br/>        .p2align 2,,3<br/><br/>; For signed % operation, we need to<br/>; first add 31 to i's value if it was<br/>; negative to begin with:<br/><br/>.L4:<br/>        leal    31(%eax), %eax<br/>        jmp     .L2</pre>&#13;
		<p class="indent">The difference in these two coding examples demonstrates why you should opt for unsigned integers (over signed integers) whenever you don’t absolutely need to deal with negative numbers.</p>&#13;
		<p class="indent">Attempting strength reduction manually is risky. While certain operations (like division) are almost always slower than others (like shifting to the right) on most CPUs, many strength reduction optimizations are not portable across CPUs. That is, substituting a left shift operation for multiplication may not always produce faster code when you compile for different CPUs. Some older C programs contain manual strength reductions that were originally added to improve performance. Today, those strength reductions can actually cause the programs to run slower than they should. Be very careful about incorporating strength reductions directly into your HLL code—this is one area where you should let the compiler do its job.</p>&#13;
		<h4 class="h4" id="ch00lev2sec161"><span epub:type="pagebreak" id="page_422"/><strong>12.2.6 Induction</strong></h4>&#13;
		<p class="noindent">In many expressions, particularly those appearing within a loop, the value of one variable in the expression is completely dependent on some other variable. As an example, consider the following <span class="literal">for</span> loop in Pascal:</p>&#13;
		<pre class="programs">&#13;
			for i := 0 to 15 do begin<br/><br/>    j := i * 2;<br/>    vector[ j ] := j;<br/>    vector[ j+1 ] := j + 1;<br/><br/>end;</pre>&#13;
		<p class="indent">A compiler’s optimizer may recognize that <span class="literal">j</span> is completely dependent on the value of <span class="literal">i</span> and rewrite this code as follows:</p>&#13;
		<pre class="programs">&#13;
			ij := 0;  {ij is the combination of i and j from<br/>           the previous code}<br/>while( ij &lt; 32 ) do begin<br/><br/>    vector[ ij ] := ij;<br/>    vector[ ij+1 ] := ij + 1;<br/>    ij := ij + 2;<br/><br/>end;</pre>&#13;
		<p class="indent">This optimization saves some work in the loop (specifically, the computation of <span class="literal">j := i * 2</span>).</p>&#13;
		<p class="indent">As another example, consider the following C code and the MASM output that Microsoft’s Visual C++ compiler produces:</p>&#13;
		<pre class="programs">&#13;
			extern unsigned vector[32];<br/><br/>extern void <span class="codeitalic1">someFunc</span>( unsigned v[] );<br/>extern void preventOptimization( int <span class="codeitalic1">arg1</span>, ... );<br/><br/>int main( void )<br/>{<br/><br/>    unsigned i, j;<br/><br/>    //  "Initialize" vector (or, at least,<br/>    //  make the compiler believe this is<br/>    //  what's going on):<br/><br/>    <span class="codeitalic1">someFunc</span>( vector );<br/><br/>    // For loop to demonstrate induction:<br/><br/>    for( i=0; i&lt;16; ++i )<br/>    {<br/><span epub:type="pagebreak" id="page_423"/>        j = i * 2;<br/>        vector[ j ] = j;<br/>        vector[ j+1 ] = j+1;<br/>    }<br/><br/>    // The following prevents dead code elimination<br/>    // of the former calculations:<br/><br/>    preventOptimization( vector[0], vector[15] );<br/>    return( 0 );<br/>}</pre>&#13;
		<p class="indent">Here’s the MASM (32-bit 80x86) output from Visual C++:</p>&#13;
		<pre class="programs">&#13;
			_main   PROC<br/><br/>        push    OFFSET _vector<br/>        call    _<span class="codeitalic1">someFunc</span><br/>        add     esp, 4<br/>        xor     edx, edx<br/><br/>        xor     eax, eax<br/>$LL4@main:<br/><br/>        lea     ecx, DWORD PTR [edx+1]      ; ECX = j+1<br/>        mov     DWORD PTR _vector[eax], edx ; EDX = j<br/>        mov     DWORD PTR _vector[eax+4], ecx<br/><br/>; Each time through the loop, bump j up by 2 (i*2)<br/><br/>        add     edx, 2<br/><br/>; Add 8 to index into vector, as we are filling two elements<br/>; on each loop.<br/><br/>        add     eax, 8<br/><br/>; Repeat until we reach the end of the array.<br/><br/>        cmp     eax, 128                                ; 00000080H<br/>        jb      SHORT $LL4@main<br/><br/>        push    DWORD PTR _vector+60<br/>        push    DWORD PTR _vector<br/>        call    _preventOptimization<br/>        add     esp, 8<br/><br/>        xor     eax, eax<br/><br/>        ret     0<br/>_main   ENDP<br/>_TEXT   ENDS</pre>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_424"/>As you can see in this MASM output, the Visual C++ compiler recognizes that <span class="literal">i</span> is not used in this loop. There are no calculations involving <span class="literal">i</span>, and it’s completely optimized away. Furthermore, there’s no <span class="literal">j = i * 2</span> computation. Instead, the compiler uses induction to determine that <span class="literal">j</span> increases by 2 on each iteration, and emits the code to do this rather than computing the value of <span class="literal">j</span> value from <span class="literal">i</span>. Finally, note that the compiler doesn’t index into the vector array. Instead, it marches a pointer through the array on each iteration of the loop—once again using induction to produce a faster and shorter code sequence than you’d get without this optimization.</p>&#13;
		<p class="indent">As for common subexpressions, you can manually incorporate induction optimization into your programs. The result is almost always harder to read and understand, but if your compiler’s optimizer fails to produce good machine code in a section of your program, manual optimization is always an option.</p>&#13;
		<p class="indent">Here’s the Java variation of this example and the JBC output:</p>&#13;
		<pre class="programs">&#13;
			public class Welcome<br/>{<br/>    public static void main( String[] args )<br/>    {<br/>          int[] vector = new int[32];<br/>          int j;<br/>          for (int i = 0; i&lt;16; ++i)<br/>          {<br/>            j = i * 2;<br/>            vector[j] = j;<br/>            vector[j + 1] = j + 1;<br/>          }<br/>    }<br/>}<br/><br/>// JBC:<br/><br/>Compiled from "Welcome.java"<br/>public class Welcome extends java.lang.Object{<br/>public Welcome();<br/>  Code:<br/>   0:   aload_0<br/><br/>        ; //Method java/lang/Object."&lt;init&gt;":()V<br/>   1:   invokespecial   #1<br/>   4:   return<br/><br/>public static void main(java.lang.String[]);<br/>  Code:<br/>; Create vector array:<br/><br/>   0:   bipush  16<br/>   2:   newarray int<br/>   4:   astore_1<br/><br/>; i = 0   -- for( int i=0;...;...)<br/><span epub:type="pagebreak" id="page_425"/>   5:   iconst_0<br/>   6:   istore_3<br/><br/>; If i &gt;= 16, exit loop  -- for(...;i&lt;16;...)<br/><br/>   7:   iload_3<br/>   8:   bipush  16<br/>   10:  if_icmpge   35<br/><br/>; j = i * 2<br/><br/>   13:  iload_3<br/>   14:  iconst_2<br/>   15:  imul<br/>   16:  istore_2<br/><br/>; vector[j] = j<br/><br/>   17:  aload_1<br/>   18:  iload_2<br/>   19:  iload_2<br/>   20:  iastore<br/><br/>; vector[j+1] = j + 1<br/><br/>   21:  aload_1<br/>   22:  iload_2<br/>   23:  iconst_1<br/>   24:  iadd<br/>   25:  iload_2<br/>   26:  iconst_1<br/>   27:  iadd<br/>   28:  iastore<br/><br/>; Next iteration of loop -- for(...;...; ++i )<br/><br/>   29:  iinc    3, 1<br/>   32:  goto    7<br/><br/>; exit program here.<br/><br/>   35:  return<br/>}</pre>&#13;
		<p class="indent">It’s probably obvious that Java doesn’t optimize this code at all. If you want better code, you’ll have to manually optimize it:</p>&#13;
		<pre class="programs">&#13;
			for ( j = 0; j &lt; 32; j = j + 2 )<br/>{<br/>    vector[j] = j;<br/>    vector[j + 1] = j + 1;<br/>}<br/><span epub:type="pagebreak" id="page_426"/>  Code:<br/>; Create array:<br/><br/>   0:   bipush  16<br/>   2:   newarray int<br/>   4:   astore_1<br/><br/>; for( int j = 0;...;...)<br/><br/>   5:   iconst_0<br/>   6:   istore_2<br/><br/>; if j &gt;= 32, bail -- for(...;j&lt;32;...)<br/><br/>   7:   iload_2<br/>   8:   bipush  32<br/>   10:  if_icmpge   32<br/><br/>; vector[j] = j<br/><br/>   13:  aload_1<br/>   14:  iload_2<br/>   15:  iload_2<br/>   16:  iastore<br/><br/>; vector[j + 1] = j + 1<br/><br/>   17:  aload_1<br/>   18:  iload_2<br/>   19:  iconst_1<br/>   20:  iadd<br/>   21:  iload_2<br/>   22:  iconst_1<br/>   23:  iadd<br/>   24:  iastore<br/><br/>; j += 2  -- for(...;...; j += 2 )<br/><br/>   25:  iload_2<br/>   26:  iconst_2<br/>   27:  iadd<br/>   28:  istore_2<br/>   29:  goto    7<br/><br/>   32:  return</pre>&#13;
		<p class="indent">As you can see, Java isn’t the best language choice if you’re interested in producing optimized runtime code. Perhaps Java’s authors felt that as a result of the interpreted bytecode execution, there was no real reason to try to optimize the compiler’s output, or perhaps they felt that optimization was the JIT compiler’s responsibility.</p>&#13;
		<h4 class="h4" id="ch00lev2sec162"><span epub:type="pagebreak" id="page_427"/><strong>12.2.7 Loop Invariants</strong></h4>&#13;
		<p class="noindent">The optimizations shown so far have all been techniques a compiler can use to improve code that is already well written. Handling loop invariants, by contrast, is a compiler optimization for fixing bad code. A <em>loop invariant</em> is an expression that does not change on each iteration of some loop. The following Visual Basic code demonstrates a trivial loop-invariant calculation:</p>&#13;
		<pre class="programs">&#13;
			i = 5<br/>for j = 1 to 10<br/>    k = i*2<br/>next j</pre>&#13;
		<p class="indent">The value of <span class="literal">k</span> does not change during the loop’s execution. Once the loop completes execution, <span class="literal">k</span>’s value is exactly the same as if the calculation of <span class="literal">k</span> had been moved before or after the loop. For example:</p>&#13;
		<pre class="programs">&#13;
			i = 5<br/>k = i * 2<br/>for j = 1 to 10<br/>next j<br/>rem At this point, k will contain the same<br/>rem value as in the previous example</pre>&#13;
		<p class="indent">The difference between these two code fragments, of course, is that the second example computes the value <span class="literal">k = i * 2</span> only once rather than on each iteration of the loop.</p>&#13;
		<p class="indent">Many compilers’ optimizers will spot a loop-invariant calculation and use <em>code motion</em> to move it outside the loop. As an example of this operation, consider the following C program and its corresponding output:</p>&#13;
		<pre class="programs">&#13;
			extern unsigned <span class="codeitalic1">someFunc</span>( void );<br/>extern void preventOptimization( unsigned <span class="codeitalic1">arg1</span>, ... );<br/><br/>int main( void )<br/>{<br/>    unsigned i, j, k, m;<br/><br/>    k = <span class="codeitalic1">someFunc</span>();<br/>    m = k;<br/>    for( i = 0; i &lt; k; ++i )<br/>    {<br/>        j = k + 2;    // Loop-invariant calculation<br/>        m += j + i;<br/>    }<br/>    preventOptimization( m, j, k, i );<br/>    return( 0 );<br/>}</pre>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_428"/>Here’s the 80x86 MASM code emitted by Visual C++:</p>&#13;
		<pre class="programs">&#13;
			_main   PROC NEAR ; COMDAT<br/>; File t.c<br/>; Line 5<br/>        push    ecx<br/>        push    esi<br/>; Line 8<br/>        call    _<span class="codeitalic1">someFunc</span><br/>; Line 10<br/>        xor     ecx, ecx  ; i = 0<br/>        test    eax, eax  ; see if k == 0<br/>        mov     edx, eax  ; m = k<br/>        jbe     SHORT $L108<br/>        push    edi<br/><br/>; Line 12<br/>; Compute j = k + 2, but only execute this<br/>; once (code was moved out of the loop):<br/><br/>        lea     esi, DWORD PTR [eax+2] ; j = k + 2<br/><br/>; Here's the loop the above code was moved<br/>; out of:<br/><br/>$L99:<br/>; Line 13<br/>        ; m(edi) = j(esi) + i(ecx)<br/><br/>        lea     edi, DWORD PTR [esi+ecx]<br/>        add     edx, edi<br/><br/>        ; ++i<br/>        inc     ecx<br/><br/>        ; While i &lt; k, repeat:<br/><br/>        cmp     ecx, eax<br/>        jb      SHORT $L99<br/><br/>        pop     edi<br/>; Line 15<br/>;<br/>; This is the code after the loop body:<br/><br/>        push    ecx<br/>        push    eax<br/>        push    esi<br/>        push    edx<br/>        call    _preventOptimization<br/>        add     esp, 16                                 ; 00000010H<br/>; Line 16<br/>        xor     eax, eax<br/>        pop     esi<br/><span epub:type="pagebreak" id="page_429"/>; Line 17<br/>        pop     ecx<br/>        ret     0<br/>$L108:<br/>; Line 10<br/>        mov     esi, DWORD PTR _j$[esp+8]<br/>; Line 15<br/>        push    ecx<br/>        push    eax<br/>        push    esi<br/>        push    edx<br/>        call    _preventOptimization<br/>        add     esp, 16                                 ; 00000010H<br/>; Line 16<br/>        xor     eax, eax<br/>        pop     esi<br/>; Line 17<br/>        pop     ecx<br/>        ret     0<br/>_main   ENDP</pre>&#13;
		<p class="indent">As you can see by reading the comments in the assembly code, the loop-invariant expression <span class="literal">j = k + 2</span> was moved out of the loop and executed prior to the start of the loop’s code, saving some execution time on each iteration of the loop.</p>&#13;
		<p class="indent">Unlike most optimizations, which you should leave up to the compiler if possible, you should move all loop-invariant calculations out of a loop unless there’s a justifiable reason for leaving them there. Loop-invariant calculations raise questions for someone reading your code (“Isn’t this supposed to change in the loop?”), because their presence actually makes the code harder to read and understand. If you want to leave the invariant code in the loop for some reason, be sure to comment your justification for anyone looking at your code later.</p>&#13;
		<h4 class="h4" id="ch00lev2sec163"><strong>12.2.8 Optimizers and Programmers</strong></h4>&#13;
		<p class="noindent">HLL programmers fall into three groups based on their understanding of these compiler optimizations:</p>&#13;
		<ul>&#13;
			<li>&#13;
				<p class="noindent">The first group is unaware of how compiler optimizations work, and they write their code without considering the effect that their code organization will have on the optimizer.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">The second group understands how compiler optimizations work, so they write their code to be more readable. They assume that the optimizer will handle issues such as converting multiplication and division to shifts (where appropriate) and preprocessing constant expressions. This second group places a fair amount of faith in the compiler’s ability to correctly optimize their code.</p>&#13;
				</li>&#13;
			<li><span epub:type="pagebreak" id="page_430"/>&#13;
			<p class="noindentt">The third group is also aware of the general types of optimizations that compilers can do, but they don’t trust the compilers to do the optimization for them. Instead, they manually incorporate those optimizations into their code.</p>&#13;
			</li>&#13;
		</ul>&#13;
		<p class="indent">Interestingly enough, compiler optimizers are actually designed for the first group of programmers, those who are ignorant of how the compiler operates. Therefore, a good compiler will usually produce roughly the same quality of code for all three types of programmers (at least with respect to arithmetic expressions). This is particularly true when you compile the same program across different compilers. However, keep in mind that this assertion is valid only for compilers that have decent optimization capabilities. If you have to compile your code on a large number of compilers and you can’t be confident that all of them have good optimizers, manual optimization may be one way to achieve consistently good performance across all compilers.</p>&#13;
		<p class="indent">Of course, the real question is, “Which compilers are good, and which are not?” It would be nice to provide a table or chart in this book that describes the optimization capabilities of all the different compilers you might encounter, but unfortunately, the rankings change as compiler vendors improve their products, so anything printed here would rapidly become obsolete.<sup><a id="ch12fn_5"/><a href="footnotes.xhtml#ch12fn5">5</a></sup> Fortunately, there are several websites that try to keep up-to-date comparisons of compilers.</p>&#13;
		<h3 class="h3" id="ch00lev1sec102"><strong>12.3 Side Effects in Arithmetic Expressions</strong></h3>&#13;
		<p class="noindent">You’ll definitely want to give a compiler some guidance with respect to side effects that may occur in an expression. If you don’t understand how compilers deal with side effects in arithmetic expressions, you might write code that doesn’t always produce correct results, particularly when moving source code between different compilers. Wanting to write the fastest or the smallest possible code is all well and good, but if it doesn’t produce the correct answer any optimizations you make on the code are all for naught.</p>&#13;
		<p class="indent">A <em>side effect</em> is any modification to the global state of a program outside the immediate result a piece of code is producing. The primary purpose of an arithmetic expression is to produce the expression’s result. Any other change to the system’s state in an expression is a side effect. The C, C++, C#, Java, Swift, and other C-based languages are especially guilty of allowing side effects in an arithmetic expression. For example, consider the following C code fragment:</p>&#13;
		<pre class="programs">i = i + *pi++ + (j = 2) * --k</pre>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_431"/>This expression exhibits four separate side effects:</p>&#13;
		<ul>&#13;
			<li>&#13;
				<p class="noindent">The decrement of <span class="literal">k</span> at the end of the expression</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">The assignment to <span class="literal">j</span> prior to using <span class="literal">j</span>’s value</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">The increment of the pointer <span class="literal">pi</span> after dereferencing <span class="literal">pi</span></p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">The assignment to <span class="literal">i</span><sup><a id="ch12fn_6"/><a href="footnotes.xhtml#ch12fn6">6</a></sup></p>&#13;
				</li>&#13;
		</ul>&#13;
		<p class="indent">Although few non–C-based languages provide as many ways to create side effects in arithmetic expressions as C does, most languages do allow you to create side effects within an expression via a function call. Side effects in functions are useful, for example, when you need to return more than a single value as a function result in languages that don’t directly support this capability. Consider the following Pascal code fragment:</p>&#13;
		<pre class="programs">&#13;
			var<br/>   k:<span class="codeitalic1">integer</span>;<br/>   m:<span class="codeitalic1">integer</span>;<br/>   n:<span class="codeitalic1">integer</span>;<br/><br/>function hasSideEffect( i:<span class="codeitalic1">integer</span>; var j:<span class="codeitalic1">integer</span> ):<span class="codeitalic1">integer</span>;<br/>begin<br/><br/>    k := k + 1;<br/>    hasSideEffect := i + j;<br/>    j := i;<br/><br/>end;<br/>        .<br/>        .<br/>        .<br/>    m := hasSideEffect( 5, n );</pre>&#13;
		<p class="indent">In this example, the call to the <span class="literal">hasSideEffect()</span> function produces two different side effects:</p>&#13;
		<ul>&#13;
			<li>&#13;
				<p class="noindent">The modification of the global variable <span class="literal">k</span>.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">The modification of the pass-by-reference parameter <span class="literal">j</span> (the actual parameter is <span class="literal">n</span> in this code fragment).</p>&#13;
				</li>&#13;
		</ul>&#13;
		<p class="indent">The real purpose of the function is to compute its return result. Any modification of global values or reference parameters constitutes a side effect of that function; hence, invoking that function within an expression produces side effects. Any language that allows you to modify global values (either directly or through parameters) from a function is capable of producing side effects within an expression; this concept is not limited to Pascal programs.</p>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_432"/>The problem with side effects in an expression is that most languages do not guarantee the order of evaluation of the components that make up an expression. Many novice programmers incorrectly assume that when they write an expression such as the following:</p>&#13;
		<pre class="programs">i := f(x) + g(x);</pre>&#13;
		<p class="noindent">the compiler will emit code that first calls function <span class="literal">f()</span> and then calls function <span class="literal">g()</span>. Very few programming languages, however, require this order of execution. That is, some compilers will indeed call <span class="literal">f()</span>, then <span class="literal">g()</span>, and add their return results together. Other compilers, however, will call <span class="literal">g()</span> first, then <span class="literal">f()</span>, and compute the sum of the function return results. That is, the compiler could translate this expression into either of the following simplified code sequences before actually generating native machine code:</p>&#13;
		<pre class="programs">&#13;
			{ Conversion #1 for "i := f(x) + g(x);" }<br/><br/>    <span class="codeitalic1">temp1</span> := f(x);<br/>    <span class="codeitalic1">temp2</span> := g(x);<br/>    i := <span class="codeitalic1">temp1</span> + <span class="codeitalic1">temp2</span>;<br/><br/>{ Conversion #2 for "i := f(x) + g(x);" }<br/><br/>    <span class="codeitalic1">temp1</span> := g(x);<br/>    <span class="codeitalic1">temp2</span> := f(x);<br/>    i := <span class="codeitalic1">temp2</span> + <span class="codeitalic1">temp1</span>;</pre>&#13;
		<p class="indent">These two different function call sequences could produce completely different results if <span class="literal">f()</span> or <span class="literal">g()</span> produces a side effect. For example, if function <span class="literal">f()</span> modifies the value of the <span class="literal">x</span> parameter you pass to it, the preceding sequence could produce different results.</p>&#13;
		<p class="indent">Note that issues such as precedence, associativity, and commutativity have no bearing on whether the compiler evaluates one subcomponent of an expression before another.</p>&#13;
		<p class="indent">For example, consider the following arithmetic expression and several possible intermediate forms for the expression:</p>&#13;
		<pre class="programs">&#13;
			    j := f(x) - g(x) * h(x);<br/><br/>{ Conversion #1 for this expression: }<br/><br/>    <span class="codeitalic1">temp1</span> := f(x);<br/>    <span class="codeitalic1">temp2</span> := g(x);<br/>    <span class="codeitalic1">temp3</span> := h(x);<br/>    <span class="codeitalic1">temp4</span> := <span class="codeitalic1">temp2</span> * <span class="codeitalic1">temp3</span><br/>    j := <span class="codeitalic1">temp1</span> - <span class="codeitalic1">temp4</span>;<br/><br/>{ Conversion #2 for this expression: }<br/><br/>    <span class="codeitalic1">temp2</span> := g(x);<br/>    <span class="codeitalic1">temp3</span> := h(x);<br/><span epub:type="pagebreak" id="page_433"/>    <span class="codeitalic1">temp1</span> := f(x);<br/>    <span class="codeitalic1">temp4</span> := <span class="codeitalic1">temp2</span> * <span class="codeitalic1">temp3</span><br/>    j := <span class="codeitalic1">temp1</span> - <span class="codeitalic1">temp4</span>;<br/><br/>{ Conversion #3 for this expression: }<br/><br/>    <span class="codeitalic1">temp3</span> := h(x);<br/>    <span class="codeitalic1">temp1</span> := f(x);<br/>    <span class="codeitalic1">temp2</span> := g(x);<br/>    <span class="codeitalic1">temp4</span> := <span class="codeitalic1">temp2</span> * <span class="codeitalic1">temp3</span><br/>    j := <span class="codeitalic1">temp1</span> - <span class="codeitalic1">temp4</span>;</pre>&#13;
		<p class="indent">Other combinations are also possible.</p>&#13;
		<p class="indent">The specifications for most programming languages explicitly leave the order of evaluation undefined. This may seem somewhat bizarre, but there’s a good reason for it: sometimes the compiler can produce better machine code by rearranging the order in which it evaluates certain subexpressions within an expression. Any attempt by the language designer to force a particular order of evaluation on a compiler’s implementer, therefore, could limit the range of optimizations possible.</p>&#13;
		<p class="indent">There are, of course, certain rules that most languages do enforce. Probably the most common rule is that all side effects within an expression will occur prior to the completion of that statement’s execution. For example, if the function <span class="literal">f()</span> modifies the global variable <span class="literal">x</span>, then the following statements will always print the value of <span class="literal">x</span> after <span class="literal">f()</span> modifies it:</p>&#13;
		<pre class="programs">&#13;
			i := f(x);<br/>writeln( "x=", x );</pre>&#13;
		<p class="indent">Another rule you can count on is that the assignment to a variable on the left-hand side of an assignment statement does not occur prior to the use of that same variable on the right-hand side of the expression. That is, the following code won’t store the result of the expression into variable <span class="literal">n</span> until it uses the previous value of <span class="literal">n</span> within the expression:</p>&#13;
		<pre class="programs">n := f(x) + g(x) - n;</pre>&#13;
		<p class="indent">Because the order of the production of side effects within an expression is undefined in most languages, the result of the following code is generally undefined (in Pascal):</p>&#13;
		<pre class="programs">&#13;
			function incN:<span class="codeitalic1">integer</span>;<br/>begin<br/>    incN := n;<br/>    n := n + 1;<br/>end;<br/>        .<br/>        .<br/>        .<br/>    n := 2;<br/>    writeln( incN + n*2 );</pre>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_434"/>The compiler is free to call the <span class="literal">incN()</span> function first (so <span class="literal">n</span> will contain <span class="literal">3</span> prior to executing the subexpression <span class="literal">n * 2</span>), or it can compute <span class="literal">n * 2</span> first and then call the <span class="literal">incN()</span> function. As a result, one compilation of this statement could produce the output <span class="literal">8</span>, while a different compilation might produce <span class="literal">6</span>. In both cases, <span class="literal">n</span> would contain <span class="literal">3</span> after the <span class="literal">writeln</span> statement is executed, but the order of computation of the expression in the <span class="literal">writeln</span> statement could vary.</p>&#13;
		<p class="indent">Don’t make the mistake of thinking you can run some experiments to determine the order of evaluation. At the very best, such experiments will tell you only the order a particular compiler uses. A different compiler may well compute subexpressions in a different order. In fact, the same compiler might also compute the components of a subexpression differently based on the context of that subexpression. This means that a compiler might compute the result using one ordering at one point in the program and using a different ordering somewhere else in the same program. This is why it’s dangerous to “determine” the ordering your particular compiler uses and rely on that ordering. Even if the compiler is consistent in the order it uses to compute side effects, the compiler vendor could change the ordering in a later version. If you must depend upon the order of evaluation, first break the expression down into a sequence of simpler statements whose computational order you can control. For example, if you really need to have your program call <span class="literal">f()</span> before <span class="literal">g()</span> in this statement:</p>&#13;
		<pre class="programs">i := f(x) + g(x);</pre>&#13;
		<p class="noindent">then you should write the code this way:</p>&#13;
		<pre class="programs"><span class="codeitalic1">temp1</span> := f(x);<br/><span class="codeitalic1">temp2</span> := g(x);<br/>i := <span class="codeitalic1">temp1</span> + <span class="codeitalic1">temp2</span>;</pre>&#13;
		<p class="indent">If you must control the order of evaluation within an expression, take special care to ensure that all side effects are computed at the appropriate time. To do this, you need to learn about sequence points.</p>&#13;
		<h3 class="h3" id="ch00lev1sec103"><strong>12.4 Containing Side Effects: Sequence Points</strong></h3>&#13;
		<p class="noindent">As noted earlier, most languages guarantee that the computation of side effects completes before certain points, known as <em>sequence points</em>, in your program’s execution. For example, almost every language guarantees that all side effects will be computed by the time the statement containing the expression completes execution. The end of a statement is an example of a sequence point.</p>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_435"/>The C programming language provides several important sequence points within expressions, in addition to the semicolon at the end of a statement. C defines sequence points between each of the following operators:</p>&#13;
		<pre class="programs"><span class="codeitalic1">expression1</span>, <span class="codeitalic1">expression2</span>                 (comma operator in an <span class="codeitalic1">expression</span>)<br/><span class="codeitalic1">expression1</span> &amp;&amp; <span class="codeitalic1">expression2</span>               (logical AND operator)<br/><span class="codeitalic1">expression1</span> || <span class="codeitalic1">expression2</span>               (logical OR operator)<br/><span class="codeitalic1">expression1</span> ? <span class="codeitalic1">expression2</span> : <span class="codeitalic1">expression3</span>  (conditional expression operator)</pre>&#13;
		<p class="indent">In these examples, C<sup><a id="ch12fn_7"/><a href="footnotes.xhtml#ch12fn7">7</a></sup> guarantees that all side effects in <span class="codeitalic">expression1</span> are completed before the computation of <span class="codeitalic">expression2</span> or <span class="codeitalic">expression3</span>. Note that for the conditional expression, C evaluates only one of <span class="codeitalic">expression2</span> or <span class="codeitalic">expression3</span> so the side effects of only one of these subexpressions ever occurs on a given execution of the conditional expression. Similarly, short-circuit evaluation may cause only <span class="codeitalic">expression1</span> to evaluate in the <span class="literal">&amp;&amp;</span> and <span class="literal">||</span> operations. So, take care when using the last three forms.</p>&#13;
		<p class="indent">To understand how side effects and sequence points can affect the operation of your program, consider the following example in C:</p>&#13;
		<pre class="programs">&#13;
			int array[6] = {0, 0, 0, 0, 0, 0};<br/>int i;<br/>    .<br/>    .<br/>    .<br/>i = 0;<br/>array[i] = i++;</pre>&#13;
		<p class="indent">Note that C does not define a sequence point across the assignment operator. Therefore, the language makes no guarantees about the value of the expression <span class="literal">i</span> it uses as an index. The compiler can choose to use the value of <span class="literal">i</span> before or after indexing into array. That the <span class="literal">++</span> operator is a post-increment operation implies only that <span class="literal">i++</span> returns the value of <span class="literal">i</span> prior to the increment; it doesn’t guarantee that the compiler will use the pre-increment value of <span class="literal">i</span> anywhere else in the expression. The bottom line is that the last statement in this example could be semantically equivalent to either of the following statements:</p>&#13;
		<pre class="programs">&#13;
			      array[0] = i++;<br/>-or-<br/>      array[1] = i++;</pre>&#13;
		<p class="indent">The C language definition allows either form; it doesn’t require the first form simply because the array index appears in the expression before the post-increment operator.</p>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_436"/>To control the assignment to <span class="literal">array</span> in this example, you have to ensure that no part of the expression depends upon the side effects of some other part of the expression. That is, you cannot both use the value of <span class="literal">i</span> at one point in the expression and apply the post-increment operator to <span class="literal">i</span> in another part of the expression, unless there is a sequence point between the two uses. Because there’s no such sequence point in this statement, the result is undefined by the C language standard.</p>&#13;
		<p class="indent">To guarantee that a side effect occurs at an appropriate point, you must have a sequence point between two subexpressions. For example, if you’d like to use the value of <span class="literal">i</span> prior to the increment as the index into the array, you could write the following code:</p>&#13;
		<pre class="programs">&#13;
			array [i] = i; //&lt;-semicolon marks a sequence point.<br/>++i;</pre>&#13;
		<p class="indent">To use the value of <span class="literal">i</span> after the increment operation as the array index, you could use code such as the following:</p>&#13;
		<pre class="programs">&#13;
			++i;               //&lt;-semicolon marks a sequence point.<br/>array[ i ] = i-1;</pre>&#13;
		<p class="indent">Note, by the way, that a decent compiler won’t increment <span class="literal">i</span> and then compute <span class="literal">i - 1</span>. It will recognize the symmetry here, grab the value of <span class="literal">i</span> prior to the increment, and use that value as the index into array. This is an example of where someone who is familiar with typical compiler optimizations could take advantage of this knowledge to write code that is more readable. A programmer who inherently mistrusts compilers and their ability to optimize well might write code like this:</p>&#13;
		<pre class="programs">&#13;
			j=i++;           //&lt;-semicolon marks a sequence point.<br/>array[ i ] = j;</pre>&#13;
		<p class="indent">An important distinction is that a sequence point does not specify exactly when a computation will take place, only that it will happen before crossing the sequence point. The side effect could have been computed much earlier in the code, at any point between the previous sequence point and the current one. Another takeaway is that sequence points do not force the compiler to complete some computations between a pair of sequence points if that computation does not produce any side effects. Eliminating common subexpressions, for example, would be a far less useful optimization if the compiler could only use the result of common subexpression computations between sequence points. The compiler is free to compute the result of a subexpression as far ahead as necessary as long as that subexpression produces no side effects. Similarly, a compiler can compute the result of a subexpression as late as it cares to, as long as that result doesn’t become part of a side effect.</p>&#13;
		<p class="indent">Because statement endings (that is, semicolons) are a sequence point in most languages, one way to control the computation of side effects is to manually break a complex expression down into a sequence of <span epub:type="pagebreak" id="page_437"/>three-address-like statements. For example, rather than relying on the Pascal compiler to translate an earlier example into three-address code using its own rules, you can explicitly write the code using whichever set of semantics you prefer:</p>&#13;
		<pre class="programs">&#13;
			{ Statement with an undefined result in Pascal }<br/><br/>    i := f(x) + g(x);<br/><br/>{ Corresponding statement with well-defined semantics }<br/><br/>    <span class="codeitalic1">temp1</span> := f(x);<br/>    <span class="codeitalic1">temp2</span> := g(x);<br/>    i := <span class="codeitalic1">temp1</span> + <span class="codeitalic1">temp2</span>;<br/><br/>{ Another version, also with well-defined but different semantics }<br/><br/>    <span class="codeitalic1">temp1</span> := g(x);<br/>    <span class="codeitalic1">temp2</span> := f(x);<br/>    i := <span class="codeitalic1">temp2</span> + <span class="codeitalic1">temp1</span>;</pre>&#13;
		<p class="indent">Again, operator precedence and associativity do not control when a computation takes place in an expression. Even though addition is left associative, the compiler may compute the value of the addition operator’s right operand before it computes the value of the addition operator’s left operand. Precedence and associativity control how the compiler arranges the computation to produce the result. They do not control when the program computes the subcomponents of the expression. As long as the final computation produces the results expected based on precedence and associativity, the compiler is free to compute the subcomponents in any order and at any time it pleases.</p>&#13;
		<p class="indent">Thus far, this section has implied that a compiler always computes the value of an assignment statement and completes that assignment (and any other side effects) upon encountering the semicolon at the end of the statement. Strictly speaking, this isn’t true. What many compilers do is ensure that all side effects occur between a sequence point and the next reference to the object changed by the side effect. For example, consider the following two statements:</p>&#13;
		<pre class="programs">&#13;
			j = i++;<br/>k = m*n + 2;</pre>&#13;
		<p class="indent">Although the first statement in this code fragment has a side effect, some compilers might compute the value (or portions thereof) of the second statement before completing the execution of the first statement. Many compilers will rearrange various machine instructions to avoid data hazards and other execution dependencies in the code that might hamper performance (for details on data hazards, see <em>WGC1</em>). The semicolon sitting between these two statements does not guarantee that all computations for the first statement are complete before the CPU begins any new computation; it guarantees only that the program computes any side effects that <span epub:type="pagebreak" id="page_438"/>precede the semicolon before executing any code that depends on them. Because the second statement does not depend upon the values of <span class="literal">j</span> or <span class="literal">i</span>, the compiler is free to start computing the second assignment prior to completing the first statement.</p>&#13;
		<p class="indent">Sequence points act as barriers. A code sequence must complete its execution before any subsequent code affected by the side effect can execute. A compiler cannot compute the value of a side effect before executing all the code up to the previous sequence point in the program. Consider the following two code fragments:</p>&#13;
		<pre class="programs">&#13;
			// Code fragment #1:<br/><br/>    i = j + k;<br/>    m = ++k;<br/><br/>// Code fragment #2:<br/><br/>    i = j + k;<br/>    m = ++n;</pre>&#13;
		<p class="indent">In code fragment 1, the compiler must not rearrange the code so that it produces the side effect <span class="literal">++k</span> prior to using <span class="literal">k</span> in the previous statement. The end-of-statement sequence point guarantees that the first statement in this example uses the value of <span class="literal">k</span> prior to any side effects produced in subsequent statements. In code fragment 2, however, the result of the side effect that <span class="literal">++n</span> produces does not affect anything in the <span class="literal">i = j + k;</span> statement, so the compiler is free to move the <span class="literal">++n</span> operation into the code that computes <span class="literal">i</span>’s value if doing so is more convenient or efficient.</p>&#13;
		<h3 class="h3" id="ch00lev1sec104"><strong>12.5 Avoiding Problems Caused by Side Effects</strong></h3>&#13;
		<p class="noindent">Because it’s often difficult to see the impact of side effects in your code, it’s a good idea to try to limit your program’s exposure to problems with side effects. Of course, the best way to do this is to eliminate side effects altogether in your programs. Unfortunately, that isn’t a realistic option. Many algorithms depend upon side effects for proper operation (functions returning multiple results via reference parameters or even global variables are good examples). You can, however, reduce unintended consequences of side effects by observing a few simple rules:</p>&#13;
		<ul>&#13;
			<li>&#13;
				<p class="noindent">Avoid placing side effects in Boolean expressions within program flow control statements such as <span class="literal">if</span>, <span class="literal">while</span>, and <span class="literal">do..until</span>.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">If a side effect exists on the right side of an assignment operator, try moving the side effect into its own statement before or after the assignment (depending on whether the assignment statement uses the value of the object before or after it applies the side effect).</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Avoid multiple assignments in the same statement; break them into separate statements.</p>&#13;
				</li>&#13;
			<li><span epub:type="pagebreak" id="page_439"/>&#13;
			<p class="noindent">Avoid calling more than one function (that might produce a side effect) in the same expression.</p>&#13;
			</li>&#13;
			<li>&#13;
				<p class="noindent">Avoid modifications to global objects (such as side effects) when writing functions.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Always document side effects thoroughly. For functions, you should note the side effect in the function’s documentation, as well as on every call to that function.</p>&#13;
				</li>&#13;
		</ul>&#13;
		<h3 class="h3" id="ch00lev1sec105"><strong>12.6 Forcing a Particular Order of Evaluation</strong></h3>&#13;
		<p class="noindent">As noted earlier, operator precedence and associativity do not control when a compiler may compute subexpressions. For example, if <span class="literal">X</span>, <span class="literal">Y</span>, and <span class="literal">Z</span> are each subexpressions (which could be anything from a single constant or variable reference to a complex expression in and of themselves), then an expression of the form <span class="literal">X / Y * Z</span> does not imply that the compiler computes the value for <span class="literal">X</span> before it computes the value for <span class="literal">Y</span> and <span class="literal">Z</span>. In fact, the compiler is free to compute the value for <span class="literal">Z</span> first, then <span class="literal">Y</span>, and finally <span class="literal">X</span>. Operator precedence and associativity require only that the compiler must compute the value of <span class="literal">X</span> and <span class="literal">Y</span> (in any order) before computing <span class="literal">X/Y</span>, and must compute the value of the subexpression <span class="literal">X/Y</span> before computing <span class="literal">(X / Y) * Z</span>. Of course, compilers can transform expressions via applicable algebraic transformations, but they’re generally careful about doing so, because not all standard algebraic transformations apply in limited-precision arithmetic.</p>&#13;
		<p class="indent">Although compilers can compute subexpressions in any order they choose (which is why side effects can create obscure problems), they usually avoid rearranging the order of actual computations. For example, mathematically, the following two expressions are equivalent following the standard rules of algebra (versus limited-precision computer arithmetic):</p>&#13;
		<pre class="programs">&#13;
			X / Y * Z<br/>Z * X / Y</pre>&#13;
		<p class="indent">In standard mathematics, this identity exists because the multiplication operator is <em>commutative</em>; that is, <em>A</em> × <em>B</em> is equal to <em>B</em> × <em>A</em>. Indeed, these two expressions will generally produce the same result as long as they are computed as follows:</p>&#13;
		<pre class="programs">&#13;
			(X / Y) * Z<br/>Z * (X / Y)</pre>&#13;
		<p class="indent">The parentheses are used here not to show precedence, but to group calculations that the CPU must perform as a unit. That is, the statements are equivalent to:</p>&#13;
		<pre class="programs">&#13;
			A = X / Y;<br/>B = Z<br/>C = A * B<br/>D = B * A</pre>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_440"/>In most algebraic systems, <span class="literal">C</span> and <span class="literal">D</span> should have the same value. To understand why <span class="literal">C</span> and <span class="literal">D</span> may not be equivalent, consider what happens when <span class="literal">X</span>, <span class="literal">Y</span>, and <span class="literal">Z</span> are all integer objects with the values <span class="literal">5</span>, <span class="literal">2</span>, and <span class="literal">3</span>, respectively:</p>&#13;
		<pre class="programs">&#13;
			    X / Y * Z<br/>=   5 / 2 * 3<br/>=   2 * 3<br/>=   6<br/><br/>    Z * X / Y<br/>=   3 * 5 / 2<br/>=   15 / 2<br/>=   7</pre>&#13;
		<p class="indent">Again, this is why compilers are careful about algebraically rearranging expressions. Most programmers realize that <span class="literal">X * (Y / Z)</span> is not the same thing as <span class="literal">(X * Y) / Z</span>. Most compilers realize this too. In theory, a compiler should translate an expression of the form <span class="literal">X * Y / Z</span> as though it were <span class="literal">(X * Y) / Z</span>, because the multiplication and division operators have the same precedence and are left associative. However, good programmers never rely on the rules of associativity to guarantee this. Although most compilers will correctly translate this expression as intended, the next engineer who comes along might not realize what’s going on. Therefore, explicitly including the parentheses to clarify the intended evaluation is a good idea. Better still, treat integer truncation as a side effect and break the expression down into its constituent computations (using three-address-like expressions) to ensure the proper order of evaluation.</p>&#13;
		<p class="indent">Integer arithmetic obviously obeys its own rules, and those of real algebra don’t always apply. However, don’t assume that floating-point arithmetic doesn’t suffer from the same set of problems. Any time you’re doing limited-precision arithmetic involving the possibility of rounding, truncation, overflow, or underflow—as is the case with floating-point arithmetic—standard real-arithmetic algebraic transformations may not be legal. In other words, applying arbitrary real-arithmetic transformations to a floating-point expression can introduce inaccuracies in the computation. Therefore, a good compiler won’t perform these types of transformations on real expressions. Unfortunately, some compilers do apply the rules of real arithmetic to floating-point operations. Most of the time, the results they produce are reasonably correct (within the limitations of the floating-point representation); in some special cases, however, they’re particularly bad.</p>&#13;
		<p class="indent">In general, if you must control the order of evaluation and when the program computes subcomponents of an expression, your only choice is to use assembly language. Subject to minor issues, such as out-of-order instruction execution, you can specify exactly when your software will compute various components of an expression when implementing the expression in assembly code. For very accurate computations, when the order of evaluation can affect the results you obtain, assembly language may be the safest approach. Although fewer programmers are capable of reading and understanding it, <span epub:type="pagebreak" id="page_441"/>there’s no question that it allows you to exactly specify the semantics of an arithmetic expression—what you read is what you get without any modification by the assembler. This simply isn’t true for most HLL systems.</p>&#13;
		<h3 class="h3" id="ch00lev1sec106"><strong>12.7 Short-Circuit Evaluation</strong></h3>&#13;
		<p class="noindent">For certain arithmetic and logical operators, if one component of the expression has a certain value, the value for the whole expression is automatically known regardless of the values of the expression’s remaining components. A classic example is the multiplication operator. If you have an expression <span class="literal">A * B</span> and you know that either <span class="literal">A</span> or <span class="literal">B</span> is <span class="literal">0</span>, there’s no need to compute the other component, because the result is already <span class="literal">0</span>. If the cost of computing the subexpressions is expensive relative to the cost of a comparison, then a program can save some time by testing the first component to determine if it needs to bother computing the second component. This optimization is known as <em>short-circuit evaluation</em> because the program skips over (“short-circuits” in electronics terminology) computing the remainder of the expression.</p>&#13;
		<p class="indent">Although a couple of arithmetic operations could employ short-circuit evaluation, the cost of checking for the optimization is usually more expensive than just completing the computation. Multiplication, for example, could use short-circuit evaluation to avoid multiplication by zero, as just described. However, in real programs, multiplication by zero occurs so infrequently that the cost of the comparison against zero in all the other cases generally overwhelms any savings achieved by avoiding multiplication by zero. For this reason, you’ll rarely see a language system that supports short-circuit evaluation for arithmetic operations.</p>&#13;
		<h4 class="h4" id="ch00lev2sec164"><strong>12.7.1 Using Short-Circuit Evaluation with Boolean Expressions</strong></h4>&#13;
		<p class="noindent">One type of expression that <em>can</em> benefit from short-circuit evaluation is a Boolean/logical expression. Boolean expressions are good candidates for short-circuit evaluation for three reasons:</p>&#13;
		<ul>&#13;
			<li>&#13;
				<p class="noindent">Boolean expressions produce only two results, <span class="literal">true</span> and <span class="literal">false</span>; therefore, it’s highly likely (50/50 chance, assuming random distribution) that one of the short-circuit “trigger” values will appear.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Boolean expressions tend to be complex.</p>&#13;
				</li>&#13;
			<li>&#13;
				<p class="noindent">Boolean expressions occur frequently in programs.</p>&#13;
				</li>&#13;
		</ul>&#13;
		<p class="indent">Because of these characteristics, you’ll find that many compilers use short-circuit evaluation when processing Boolean expressions.</p>&#13;
		<p class="indent">Consider the following two C statements:</p>&#13;
		<pre class="programs">&#13;
			A = B &amp;&amp; C;<br/>D = E || F;</pre>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_442"/>Note that if <span class="literal">B</span> is <span class="literal">false</span>, then <span class="literal">A</span> will be <span class="literal">false</span> regardless of <span class="literal">C</span>’s value. Similarly, if <span class="literal">E</span> is <span class="literal">true</span>, then <span class="literal">D</span> will be <span class="literal">true</span> regardless of <span class="literal">F</span>’s value. We can, therefore, compute the values for <span class="literal">A</span> and <span class="literal">D</span> as follows:</p>&#13;
		<pre class="programs">&#13;
			A = B;<br/>if( A )<br/>{<br/>    A = C;<br/>}<br/><br/>D = E;<br/>if( !D )<br/>{<br/>    D = F;<br/>}</pre>&#13;
		<p class="indent">Now this might seem like a whole lot of extra work (it’s certainly more typing!), but if <span class="literal">C</span> and <span class="literal">F</span> represent complex Boolean expressions, then this code sequence could possibly run much faster if <span class="literal">B</span> is usually <span class="literal">false</span> and <span class="literal">E</span> is usually <span class="literal">true</span>. Of course, if your compiler fully supports short-circuit evaluation, you’d never type this code; the compiler would generate the equivalent code for you.</p>&#13;
		<p class="indent">By the way, the converse of short-circuit evaluation is <em>complete Boolean evaluation</em>. In complete Boolean evaluation, the compiler emits code that always computes each subcomponent of a Boolean expression. Some languages (such as C, C++, C#, Swift, and Java) specify the use of short-circuit evaluation. A few languages (such as Ada) let the programmer specify whether to use short-circuit or complete Boolean evaluation. Most languages (such as Pascal) don’t define whether expressions will use short-circuit or complete Boolean evaluation—the language leaves the choice up to the implementer. Indeed, the same compiler could use complete Boolean evaluation for one instance of an expression and use short-circuit evaluation for another occurrence of that same expression in the same program. Unless you’re using a language that strictly defines the type of Boolean evaluation, you’ll have to check with your specific compiler’s documentation to determine how it processes Boolean expressions. (Remember to avoid compiler-specific mechanisms if there’s a chance you’ll have to compile your code with a different compiler in the future.)</p>&#13;
		<p class="indent">Look again at the expansions of the earlier Boolean expressions. It should be clear that the program won’t evaluate <span class="literal">C</span> and <span class="literal">F</span> if <span class="literal">A</span> is <span class="literal">false</span> and <span class="literal">D</span> is <span class="literal">true</span>. Therefore, the left-hand side of a conjunction (<span class="literal">&amp;&amp;</span>) or disjunction (<span class="literal">||</span>) operator can act as a gate, preventing the execution of the right-hand side of the expression. This is an important point and, indeed, many algorithms depend on this property for correct operation. Consider the following (very common) C statement:</p>&#13;
		<pre class="programs">&#13;
			if( ptr != NULL &amp;&amp; *ptr != '\0' )<br/>{<br/>    &lt;&lt; process current character pointed at by ptr &gt;&gt;<br/>}</pre>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_443"/>This example could fail if it used complete Boolean evaluation. Consider the case where the <span class="literal">ptr</span> variable contains <span class="literal">NULL</span>. With short-circuit evaluation the program will not compute the subexpression <span class="literal">*ptr !=</span> '<span class="literal">\0</span>'<span class="literal">;</span> because it realizes the result is always <span class="literal">false</span>. As a result, control immediately transfers to the first statement beyond the ending brace in this <span class="literal">if</span> statement. Consider, however, what would happen if this compiler utilized complete Boolean evaluation instead. After determining that <span class="literal">ptr</span> contains <span class="literal">NULL</span>, the program would still attempt to dereference <span class="literal">ptr</span>. Unfortunately, this attempt would probably produce a runtime error. Therefore, complete Boolean evaluation would cause this program to fail, even though it dutifully checks to make sure that access via pointer is legal.</p>&#13;
		<p class="indent">Another semantic difference between complete and short-circuit Boolean evaluation has to do with side effects. In particular, if a subexpression does not execute because of short-circuit evaluation, then that subexpression doesn’t produce any side effects. This behavior is incredibly useful but inherently dangerous. It is useful insofar as some algorithms absolutely depend upon short-circuit evaluation. It is dangerous because some algorithms also expect all the side effects to occur, even if the expression evaluates to <span class="literal">false</span> at some point. As an example, consider the following bizarre (but absolutely legal) C statement, which advances a “cursor” pointer to the next 8-byte boundary in a string or the end of the string (whichever comes first):</p>&#13;
		<pre class="programs">*++ptr &amp;&amp; *++ptr &amp;&amp; *++ptr &amp;&amp; *++ptr &amp;&amp; *++ptr &amp;&amp; *++ptr &amp;&amp; *++ptr &amp;&amp; *++ptr;</pre>&#13;
		<p class="indent">This statement begins by incrementing a pointer and then fetching a byte from memory (pointed to by <span class="literal">ptr</span>). If the byte fetched was <span class="literal">0</span>, execution of this expression/statement immediately stops, as the entire expression evaluates to <span class="literal">false</span> at that point. If the character fetched is not <span class="literal">0</span>, the process repeats up to seven more times. At the end of this sequence, either <span class="literal">ptr</span> points at a <span class="literal">0</span> byte or it points 8 bytes beyond the original position. The trick here is that the expression immediately terminates upon reaching the end of the string rather than mindlessly skipping beyond that point.</p>&#13;
		<p class="indent">Of course, there are complementary examples that demonstrate desirable behavior when side effects occur in Boolean expressions involving complete Boolean evaluation. The important thing to note is that no one scheme is correct or incorrect; it all depends on context. In different situations, a given algorithm may require the use of short-circuit Boolean evaluation or complete Boolean evaluation to produce correct results. If the definition of the language you’re using doesn’t explicitly specify which scheme to use, or you want to use the other one (such as complete Boolean evaluation in C), then you have to write your code such that it forces the evaluation scheme you prefer.</p>&#13;
		<h4 class="h4" id="ch00lev2sec165"><strong>12.7.2 Forcing Short-Circuit or Complete Boolean Evaluation</strong></h4>&#13;
		<p class="noindent">Forcing complete Boolean evaluation in a language where short-circuit evaluation is used (or may be used) is relatively easy. All you have to do is break the expression into individual statements, place the result of each subexpression into a variable, and then apply the conjunction and <span epub:type="pagebreak" id="page_444"/>disjunction operators to these temporary variables. For example, consider the following conversion:</p>&#13;
		<pre class="programs">&#13;
			// Complex expression:<br/><br/>if( (a &lt; f(x)) &amp;&amp; (b != g(y)) || predicate( a + b ))<br/>{<br/>    &lt;&lt;stmts to execute if this expression is true&gt;&gt;<br/>}<br/><br/>// Translation to a form that uses complete Boolean evaluation:<br/><br/><span class="codeitalic1">temp1</span> = a &lt; f(x);<br/><span class="codeitalic1">temp2</span> = b != g(y);<br/><span class="codeitalic1">temp3</span> = predicate( a + b );<br/>if( <span class="codeitalic1">temp1</span> &amp;&amp; <span class="codeitalic1">temp2</span> || <span class="codeitalic1">temp3</span> )<br/>{<br/>    &lt;&lt;stmts to execute if this expression is true&gt;&gt;<br/>}</pre>&#13;
		<p class="indent">The Boolean expression within the <span class="literal">if</span> statement still uses short-circuit evaluation. However, because this code evaluates the subexpressions prior to the <span class="literal">if</span> statement, this code ensures that all of the side effects produced by the <span class="literal">f()</span>, <span class="literal">g()</span>, and <span class="literal">predicate()</span> functions will occur.</p>&#13;
		<p class="indent">Suppose you want to go the other way. That is, what if your language supports only complete Boolean evaluation (or doesn’t specify the evaluation type), and you want to force short-circuit evaluation? This direction is a little more work than the converse, but it’s still not difficult.</p>&#13;
		<p class="indent">Consider the following Pascal code:<sup><a id="ch12fn_8"/><a href="footnotes.xhtml#ch12fn8">8</a></sup></p>&#13;
		<pre class="programs">&#13;
			if( ((a &lt; f(x)) and (b &lt;&gt; g(y))) or predicate( a + b )) then begin<br/><br/>    &lt;&lt;stmts to execute if the expression is true&gt;&gt;<br/><br/>end; (*if*)</pre>&#13;
		<p class="indent">To force short-circuit Boolean evaluation, you need to test the value of the first subexpression, and, only if it evaluates to <span class="literal">true</span>, evaluate the second subexpression (and the conjunction of the two expressions). You can do this with the following code:</p>&#13;
		<pre class="programs">&#13;
			boolResult := a &lt; f(x);<br/>if( boolResult ) then<br/>    boolResult := b &lt;&gt; g(y);<br/><br/>if( not boolResult ) then<br/>    boolResult := predicate( a+b );<br/><span epub:type="pagebreak" id="page_445"/><br/>if( boolResult ) then begin<br/><br/>    &lt;&lt;stmts to execute if the if's expression is true&gt;&gt;<br/><br/>end; (*if*)</pre>&#13;
		<p class="indent">This code simulates short-circuit evaluation by using <span class="literal">if</span> statements to block (or force) execution of the <span class="literal">g()</span> and <span class="literal">predicate()</span> functions based on the current state of the Boolean expression (kept in the <span class="literal">boolResult</span> variable).</p>&#13;
		<p class="indent">Converting an expression to force short-circuit evaluation or complete Boolean evaluation looks as though it requires far more code than the original forms. If you’re concerned about the efficiency of this translation, relax. Internally, the compiler translates those Boolean expressions to three-address code that is similar to the translation that you did manually.</p>&#13;
		<h4 class="h4" id="ch00lev2sec166"><strong>12.7.3 Comparing Short-Circuit and Complete Boolean Evaluation Efficiency</strong></h4>&#13;
		<p class="noindent">While you might have inferred from the preceding discussion that complete Boolean evaluation and short-circuit evaluation have equivalent efficiencies, that’s not the case. If you’re processing complex Boolean expressions or the cost of some of your subexpressions is rather high, then short-circuit evaluation is generally faster than complete Boolean evaluation. As to which form produces less object code, they’re roughly equivalent, and the exact difference will depend entirely upon the expression you’re evaluating.</p>&#13;
		<p class="indent">To understand the efficiency issues surrounding complete versus short-circuit Boolean evaluation, look at the following HLA code, which implements this Boolean expression using both forms:<sup><a id="ch12fn_9"/><a href="footnotes.xhtml#ch12fn9">9</a></sup></p>&#13;
		<pre class="programs">&#13;
			// Complex expression:<br/><br/> //  if( (a &lt; f(x)) &amp;&amp; (b != g(y)) || predicate( a+b ))<br/> //  {<br/> //      &lt;&lt;stmts to execute if the if's expression is true&gt;&gt;<br/> //  }<br/> //<br/> // Translation to a form that uses complete<br/> //  Boolean evaluation:<br/> //<br/> //  <span class="codeitalic1">temp1</span> = a &lt; f(x);<br/> //  <span class="codeitalic1">temp2</span> = b != g(y);<br/> //  <span class="codeitalic1">temp3</span> = predicate( a + b );<br/> //  if( <span class="codeitalic1">temp1</span> &amp;&amp; <span class="codeitalic1">temp2</span> || <span class="codeitalic1">temp3</span> )<br/> //  {<br/> //      &lt;&lt;stmts to execute if the expression evaluates true&gt;&gt;<br/> //  }<br/> //<br/> //<br/><span epub:type="pagebreak" id="page_446"/>// Translation into 80x86 assembly language code,<br/> // assuming all variables and return results are<br/> // unsigned 32-bit integers:<br/><br/>     f(x);            // Assume f returns its result in EAX<br/>     cmp( a, eax );   // Compare a with f(x)'s return result.<br/>     setb( bl );      // bl = a &lt; f(x)<br/>     g(y);            // Assume g returns its result in EAX<br/>     cmp( b, eax );   // Compare b with g(y)'s return result<br/>     setne( bh );     // bh = b != g(y)<br/>     mov( a, eax );   // Compute a + b to pass along to the<br/>     add( b, eax );   // predicate function.<br/>     predicate( eax );// al holds predicate's result (0/1)<br/>     and( bh, bl );   // bl = temp1 &amp;&amp; temp2<br/>     or( bl, al );    // al = (temp1 &amp;&amp; temp2) || temp3<br/>     jz skipStmts;    // 0 if false, not 0 if true.<br/><br/>       &lt;&lt;stmts to execute if the condition is true&gt;&gt;<br/><br/>skipStmts:</pre>&#13;
		<p class="indent">Here’s the same expression using short-circuit Boolean evaluation:</p>&#13;
		<pre class="programs">&#13;
			    //  if( (a &lt; f(x)) &amp;&amp; (b != g(y)) || predicate( a+b ))<br/>    //  {<br/>    //      &lt;&lt;stmts to execute if the if's expression evaluates true&gt;&gt;<br/>    //  }<br/><br/>        f(x);<br/>        cmp( a, eax );<br/>        jnb TryOR;          // If a is not less than f(x), try the OR clause<br/>        g(y);<br/>        cmp( b, eax );<br/>        jne DoStmts         // If b is not equal g(y) (and a &lt; f(x)), then do the body.<br/><br/>TryOR:<br/>        mov( a, eax );<br/>        add( b, eax );<br/>        predicate( eax );<br/>        test( eax, eax );   // EAX = 0?<br/>        jz SkipStmts;<br/><br/>DoStmts:<br/>        &lt;&lt;stmts to execute if the condition is true&gt;&gt;<br/>SkipStmts:</pre>&#13;
		<p class="indent">As you can see by simply counting statements, the version using short-circuit evaluation is slightly shorter (11 instructions versus 12). However, the short-circuit version will probably run much faster because half the time the code will evaluate only two of the three expressions. This code evaluates all three subexpressions only when the first subexpression, <span class="literal">a &lt; f(x)</span>, evaluates to <span class="literal">true</span> and the second expression, <span class="literal">b != g(y)</span>, evaluates to <span class="literal">false</span>. If the outcomes of these Boolean expressions are equally probable, this code <span epub:type="pagebreak" id="page_447"/>will test all three subexpressions 25 percent of the time. The remainder of the time it has to test only two subexpressions (50 percent of the time it will test <span class="literal">a &lt; f(x)</span> and <span class="literal">predicate(a + b)</span>, 25 percent of the time it will test <span class="literal">a &lt; f(x)</span> and <span class="literal">b != g(y)</span>, and the remaining 25 percent of the time it will need to test all three conditions).</p>&#13;
		<p class="indent">The interesting thing to note about these two assembly language sequences is that complete Boolean evaluation tends to maintain the state of the expression (<span class="literal">true</span> or <span class="literal">false</span>) in an actual variable, whereas short-circuit evaluation maintains the current state of the expression by the program’s position in the code. Take another look at the short-circuit example. Note that it does not maintain the Boolean results from each of the subexpressions anywhere other than the position in the code. For example, if you get to the <span class="literal">TryOR</span> label in this code, you know that the subexpression involving conjunction (logical AND) is <span class="literal">false</span>. Likewise, if the program executes the call to <span class="literal">g(y)</span>, you know that the first subexpression in the example, <span class="literal">a &lt; f(x)</span>, has evaluated to <span class="literal">true</span>. When you make it to the <span class="literal">DoStmts</span> label, you know that the entire expression has evaluated to <span class="literal">true</span>.</p>&#13;
		<p class="indent">If the execution time for the <span class="literal">f()</span>, <span class="literal">g()</span>, and <span class="literal">predicate()</span> functions is roughly the same in the current example, you can greatly improve the code’s performance with a nearly trivial modification:</p>&#13;
		<pre class="programs">&#13;
			    //  if( predicate( a + b ) || (a &lt; f(x)) &amp;&amp; (b != g(y)))<br/>    //  {<br/>    //      &lt;&lt;stmts to execute if the expression evaluates true&gt;&gt;<br/>    //  }<br/><br/>        mov( a, eax );<br/>        add( b, eax );<br/>        predicate( eax );<br/>        test( eax, eax );   // EAX = true (not zero)?<br/>        jnz DoStmts;<br/><br/>        f(x);<br/>        cmp( a, eax );<br/>        jnb SkipStmts;      // If a &gt;= f(x), try the OR clause<br/>        g(y);<br/>        cmp( b, eax );<br/>        je SkipStmts;       // If b != g(y) then do the body.<br/><br/>DoStmts:<br/>        &lt;&lt;stmts to execute if the condition is true&gt;&gt;<br/>SkipStmts:</pre>&#13;
		<p class="indent">Again, if you assume that the outcome of each subexpression is random and evenly distributed (that is, there is a 50/50 chance that each subexpression produces <span class="literal">true</span>), then this code will, on average, run about 50 percent faster than the previous version. Why? Moving the test for <span class="literal">predicate()</span> to the beginning of the code fragment means the code can now determine with one test whether it needs to execute the body. Because 50 percent of the time <span class="literal">predicate()</span> returns <span class="literal">true</span>, you can determine if you’re going to execute the loop body with a single test about half the time. In the earlier example, <span epub:type="pagebreak" id="page_448"/>it always took at least two tests to determine if we were going to execute the loop body.</p>&#13;
		<p class="indent">The two assumptions here (that the Boolean expressions are equally likely to produce <span class="literal">true</span> or <span class="literal">false</span> and that the costs of computing each subexpression are equal) rarely hold in practice. However, this means that you have an even greater opportunity to optimize your code, not less. For example, if the cost of calling the <span class="literal">predicate()</span> function is high (relative to the computation of the remainder of the expression), then you’ll want to arrange the expression so that it calls <span class="literal">predicate()</span> only when it absolutely must. Conversely, if the cost of calling <span class="literal">predicate()</span> is low compared to the cost of computing the other subexpressions, then you’ll want to call it first. The situation for the <span class="literal">f()</span> and <span class="literal">g()</span> functions is similar. Because the logical AND operation is commutative, the following two expressions are semantically equivalent (in the absence of side effects):</p>&#13;
		<pre class="programs">&#13;
			a &lt; f(x) &amp;&amp; b != g(y)<br/>b != g(y) &amp;&amp; a &lt; f(x)</pre>&#13;
		<p class="indent">When the compiler uses short-circuit evaluation, the first expression executes faster than the second if the cost of calling function <span class="literal">f()</span> is less than the cost of calling function <span class="literal">g()</span>. Conversely, if calling <span class="literal">f()</span> is more expensive than calling <span class="literal">g()</span>, then the second expression usually executes faster.</p>&#13;
		<p class="indent">Another factor that affects the performance of short-circuit Boolean expression evaluation is the likelihood that a given Boolean expression will return the same value on each call. Consider the following two templates:</p>&#13;
		<pre class="programs"><span class="codeitalic1">expr1</span> &amp;&amp; <span class="codeitalic1">expr2</span><br/><span class="codeitalic1">expr3</span> || <span class="codeitalic1">expr4</span></pre>&#13;
		<p class="indent">When working with conjunctions, try to place the expression that is more likely to return <span class="literal">true</span> on the right-hand side of the conjunction operator (<span class="literal">&amp;&amp;</span>). Remember, for the logical AND operation, if the first operand is <span class="literal">false</span>, a Boolean system employing short-circuit evaluation will not bother to evaluate the second operand. For performance reasons, you want to place the operand that is most likely to return <span class="literal">false</span> on the left-hand side of the expression. This will avoid the computation of the second operand more often than had you reversed the operands.</p>&#13;
		<p class="indent">The situation is reversed for disjunction (<span class="literal">||</span>). In this case, you’d arrange your operands so that <em>expr3</em> is more likely to return <span class="literal">true</span> than <em>expr4</em>. By organizing your disjunction operations this way, you’ll skip the execution of the right-hand expression more often than if you had swapped the operands.</p>&#13;
		<p class="indent">You cannot arbitrarily reorder Boolean expression operands if those expressions produce side effects, because the proper computation of those side effects may depend upon the exact order of the subexpressions. Rearranging the subexpressions may cause a side effect to happen that wouldn’t otherwise occur. Keep this in mind when you’re trying to improve performance by rearranging operands in a Boolean expression.</p>&#13;
		<h3 class="h3" id="ch00lev1sec107"><span epub:type="pagebreak" id="page_449"/><strong>12.8 The Relative Cost of Arithmetic Operations</strong></h3>&#13;
		<p class="noindent">Most algorithm analysis methodologies use a simplifying assumption that all operations take the same amount of time.<sup><a id="ch12fn_10"/><a href="footnotes.xhtml#ch12fn10">10</a></sup> This assumption is rarely correct, because some arithmetic operations are two orders of magnitude slower than other computations. For example, a simple integer addition operation is often much faster than an integer multiplication. Similarly, integer operations are usually much faster than the corresponding floating-point operations. For algorithm analysis purposes, it may be okay to ignore the fact that one operation may be <em>n</em> times faster than some other operation. For someone interested in writing great code, however, knowing which operators are the most efficient is important, especially when you have the option of choosing among them.</p>&#13;
		<p class="indent">Unfortunately, we can’t create a table of operators that lists their relative speeds. The performance of a given arithmetic operator will vary by CPU. Even within the same CPU family, you see a wide variance in performance for the same arithmetic operation. For example, shift and rotate operations are relatively fast on a Pentium III (relative, say, to an addition operation). On a Pentium 4, however, they’re considerably slower. These operations were faster on later Intel CPUs. So an operator such as the C/C++ <span class="literal">&lt;&lt;</span> or <span class="literal">&gt;&gt;</span> can be fast or slow, relative to an addition operation, depending upon which CPU it executes.</p>&#13;
		<p class="indent">That said, I can provide some general guidelines. For example, on most CPUs the addition operation is one of the most efficient arithmetic and logical operations around; few CPUs support faster arithmetic or logical operations than addition. Therefore, it’s useful to group various operations into classes based on their performance relative to an operation like addition (see <a href="ch12.xhtml#ch12tab1">Table 12-1</a> for an example).</p>&#13;
		<p class="tabcap" id="ch12tab1"><strong>Table 12-1:</strong> Relative Performances of Arithmetic Operations (Guidelines)</p>&#13;
		<table class="all">&#13;
			<colgroup>&#13;
				<col style="width:40%"/>&#13;
				<col style="width:65%"/>&#13;
			</colgroup>&#13;
			<tbody>&#13;
				<tr>&#13;
					<td class="table-h" style="vertical-align: top;">&#13;
						<p class="table"><strong>Relative performance</strong></p>&#13;
					</td>&#13;
					<td class="table-h" style="vertical-align: top;">&#13;
						<p class="table"><strong>Operations</strong></p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">Fastest</p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">Integer addition, subtraction, negation, logical AND, logical OR, logical XOR, logical NOT, and comparisons</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td/>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">Logical shifts</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td/>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">Logical rotates</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td/>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">Multiplication</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td/>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">Division</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td/>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">Floating-point comparisons and negation</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td/>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">Floating-point addition and subtraction</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td/>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">Floating-point multiplication</p>&#13;
					</td>&#13;
				</tr>&#13;
				<tr>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">Slowest</p>&#13;
					</td>&#13;
					<td style="vertical-align: top;">&#13;
						<p class="table">Floating-point division</p>&#13;
					</td>&#13;
				</tr>&#13;
			</tbody>&#13;
		</table>&#13;
		<p class="indent"><span epub:type="pagebreak" id="page_450"/>The estimates in <a href="ch12.xhtml#ch12tab1">Table 12-1</a> are not accurate for all CPUs, but they provide a “first approximation” from which you can work until you gain more experience with a particular processor. On many processors you’ll find anywhere between two and three orders of magnitude difference in the performances between the fastest and slowest operations. In particular, division tends to be quite slow on most processors (floating-point division is even slower). Multiplication is usually slower than addition, but again, the exact variance differs greatly between processors.</p>&#13;
		<p class="indent">If you absolutely need to do floating-point division, there’s little you can do to improve your application’s performance by using a different operation (although, in some cases, it is faster to multiply by the reciprocal). However, note that you can compute many integer arithmetic calculations using different algorithms. For example, a left shift is often less expensive than multiplication by 2. While most compilers automatically handle such “operator conversions” for you, compilers aren’t omniscient and can’t always figure out the best way to calculate some result. However, if you manually do the “operator conversion” yourself, you don’t have to rely on the compiler to get this right for you.</p>&#13;
		<h3 class="h3" id="ch00lev1sec108"><strong>12.9 For More Information</strong></h3>&#13;
		<p class="bib">Aho, Alfred V., Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman. <em>Compilers: Principles, Techniques, and Tools</em>. 2nd ed. Essex, UK: Pearson Education Limited, 1986.</p>&#13;
		<p class="bib">Barrett, William, and John Couch. <em>Compiler Construction: Theory and Practice</em>. Chicago: SRA, 1986.</p>&#13;
		<p class="bib">Fraser, Christopher, and David Hansen. <em>A Retargetable C Compiler: Design and Implementation</em>. Boston: Addison-Wesley Professional, 1995.</p>&#13;
		<p class="bib">Duntemann, Jeff. <em>Assembly Language Step-by-Step</em>. 3rd ed. Indianapolis: Wiley, 2009.</p>&#13;
		<p class="bib">Hyde, Randall. <em>The Art of Assembly Language</em>. 2nd ed. San Francisco: No Starch Press, 2010.</p>&#13;
		<p class="bib">Louden, Kenneth C. <em>Compiler Construction: Principles and Practice</em>. Boston: Cengage, 1997.</p>&#13;
		<p class="bib">Parsons, Thomas W. <em>Introduction to Compiler Construction</em>. New York: W. H. Freeman, 1992.</p>&#13;
		<p class="bib">Willus.com. “Willus.com’s 2011 Win32/64 C Compiler Benchmarks.” Last updated April 8, 2012. <em><a href="https://www.willus.com/ccomp_benchmark2.shtml">https://www.willus.com/ccomp_benchmark2.shtml</a></em>.</p>&#13;
	</body></html>