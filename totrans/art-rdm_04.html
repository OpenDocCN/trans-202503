<html><head></head><body>
<h2 class="h2" id="ch04"><span epub:type="pagebreak" id="page_103"/><strong><span class="big">4</span><br/>OPTIMIZE THE WORLD</strong></h2>
<div class="image1"><img alt="Image" src="../images/common.jpg"/></div>
<p class="noindenta"><em>Optimization</em> is the process of finding the best set of something, usually of parameters defining a function or an algorithm. In mathematics, optimization typically involves functions and uses their derivatives to locate minima or maxima. In this chapter, we’ll take a different approach that involves randomness. The algorithms we’ll use fall into two broad categories: swarm intelligence and evolutionary algorithms. Collectively, these are known as <em>metaheuristics</em>.</p>
<p class="indent">Optimizing with metaheuristics is more flexible than calculus-based optimization. What we’re optimizing doesn’t need to be a mathematical function; it could be an algorithm or another process. In fact, any problem that can be cast as locating the best position in a space, where the space represents the problem in some form, is amenable to swarm intelligence and evolutionary algorithms. I use both kinds of algorithms frequently for everything from curve fitting to evolving neural network architectures. Once you understand the process of formulating tasks as generic optimization problems, you’ll begin to see them everywhere.</p>
<p class="indent"><span epub:type="pagebreak" id="page_104"/>In this chapter, we’ll use swarm intelligence and evolutionary algorithms to fit data to a known function. Then, we’ll evolve the best fit function from scratch. We’ll begin, however, with a (very) short primer on swarm intelligence and evolutionary algorithms. We’ve already used an evolutionary algorithm, though it wasn’t named so at the time. The algorithm implemented in <a href="ch03.xhtml">Chapter 3</a> to explore natural selection and genetic drift is a genetic algorithm, one of the two kinds of evolutionary algorithms we’ll encounter in this chapter.</p>
<h3 class="h3" id="ch00lev1_24"><strong>Optimization with Randomness</strong></h3>
<p class="noindent">Imagine a large haystack where each position within the haystack corresponds to a possible solution to the problem. We wish to locate the part of the haystack that offers the best solution—that is, we want to find a needle. The question is: How do we go about finding it?</p>
<p class="indent">We’ll use the following generic algorithm to search the haystack:</p>
<ol>
<li class="noindent">A swarm (population) of “agents” are randomly scattered throughout the haystack.</li>
<li class="noindent">Each agent investigates its immediate location and assigns a number to how well that location solves the problem.</li>
<li class="noindent">The agents report their numbers to headquarters.</li>
<li class="noindent">After each agent has reported in, headquarters evaluates all the numbers and stores the best position currently known, updating it if, on this iteration, any agent finds a better position.</li>
<li class="noindent">Headquarters then orders each agent to a new position in the haystack based on the information received.</li>
<li class="noindent">The process repeats from step 2 until a best position has been found or we’ve run out of time (iterations).</li>
</ol>
<p class="indent">We have a lot of flexibility when it comes to possible implementations. Indeed, there are literally hundreds of published algorithms based on this approach. Many claim inspiration from nature, but such claims are often quite dubious and generally unnecessary.</p>
<p class="indent">Is this an example of a swarm intelligence algorithm or an evolutionary algorithm? It’s both. The difference between the two depends on what happens in step 4: the process headquarters uses to decide where the agents should go next. The distinction is vital for researchers but less so for us.</p>
<p class="indent">In a swarm intelligence algorithm, the agents, called <em>particles</em>, work collectively to locate new positions in the space to explore. They are actively aware of each other and “learn” from each particle’s experiences to move the swarm, as a whole, to ever better places in the space, thereby locating increasingly better solutions to the problem.</p>
<p class="indent">An evolutionary algorithm, on the other hand, applies techniques like crossover and mutation to breed new agents (organisms). In <a href="ch03.xhtml">Chapter 3</a>, we defined an organism’s fitness as the distance between its genome and the genome of an ideal organism for the current environment. Here, fitness is <span epub:type="pagebreak" id="page_105"/>a measure of how well the solution represented by the organism’s genome (position in the haystack) solves the problem. Generations of breeding fitter solutions, with a dash of random mutation, should move the population closer to the best solution to the problem.</p>
<p class="indent">In practical terms, all we need to know is that the two kinds of algorithms search a space to find the best position in it. We’ll configure our problems such that the best position is translatable into a best solution.</p>
<p class="indent">With the hundreds of swarm and evolutionary algorithms out there, which ones should we use? Each algorithm has strengths and weaknesses, and might work best for certain kinds of problems. You need to try several.</p>
<p class="indent">For this chapter, we’ll use five algorithms: two swarm intelligence, two evolutionary, and one that is so obvious many don’t consider it a swarm algorithm at all. We don’t have space to walk through each to understand the code; I’ll leave that as an exercise (as always, please contact me with questions). We’ll learn about the algorithms and the framework using them as we go.</p>
<p class="indent">The two swarm intelligence algorithms are <em>particle swarm optimization (PSO)</em> and <em>Jaya</em>. PSO is the grandfather of swarm intelligence algorithms, and many nature-inspired algorithms are PSO in disguise. Jaya is a newer algorithm that has no parameters to adjust—either it works well or it doesn’t. Although there are many flavors of PSO, we’ll use two here: canonical and bare-bones.</p>
<p class="indent">The two evolutionary algorithms are the <em>genetic algorithm (GA)</em>, a variation on what we used in <a href="ch03.xhtml">Chapter 3</a>, and <em>differential evolution (DE)</em>, another old-school and widely used technique. DE is one of my go-to algorithms, but it has the sometimes annoying habit of converging too quickly to local minima.</p>
<p class="indent">The last algorithm is <em>random optimization (RO)</em>. In RO, the particles don’t communicate; they conduct a local search and move to a new position whenever they find one, but are completely unaware of what other particles have discovered. Headquarters monitors each particle to track the best position found overall, but never issues orders based on that knowledge.</p>
<p class="indent">We learn best by doing, so let’s begin fitting a function to data.</p>
<h3 class="h3" id="ch00lev1_25"><strong>Fitting with Swarms</strong></h3>
<p class="noindent">A common task in science and engineering is to fit a function to a set of measurements, where <em>fit</em> means finding the best set of parameters for a known type of function—the set that makes the function approximate the data as well as possible. For this task, we know the functional form; we need only to learn the parameter values to tailor the function to the data. In the next section, we’ll start with the data and let the swarms tell us what the best-fit function and parameters are (hopefully!). I’m using the word <em>swarms</em> in a general sense to mean both the swarm of particles manipulated by a swarm intelligence algorithm and the population bred and evolved by an evolutionary algorithm. Again, the distinction is minor for us.</p>
<p class="indent"><span epub:type="pagebreak" id="page_106"/>Let’s start with a simple example: some data, a function, and the parameters that best fit the function to the data. The code for the example is in <em>curfit_example.py</em>. It generates a set of points from a quadratic function, with random noise added. Then, it uses NumPy’s <code>polyfit</code> routine to fit a quadratic: <em>ax</em><sup>2</sup> + <em>bx</em> + <em>c</em>. <a href="ch04.xhtml#ch04fig01">Figure 4-1</a> shows the plot and fit function.</p>
<div class="image"><img alt="Image" id="ch04fig01" src="../images/04fig01.jpg"/></div>
<p class="figcap"><em>Figure 4-1: Fitting a polynomial to some data</em></p>
<p class="indent">With the fit function, we can approximate <em>y</em> for any <em>x</em>, which is typically why we fit the data in the first place.</p>
<p class="indent">You may be wondering why we’d bother with swarms if <code>polyfit</code> can fit the data. Unfortunately, <code>polyfit</code> fits only polynomials, or functions that are sums of powers of <em>x</em>. If your function isn’t a polynomial, there are other functions you could use, like SciPy’s <code>curve_fit</code>. However, we’re not solely interested in curve fitting; we’re using it as a warm-up exercise. SciPy won’t be of much use for the other optimization problems we’ll explore later in this chapter and in the next.</p>
<h4 class="h4" id="ch00lev2_38"><em><strong>Curves</strong></em></h4>
<p class="noindent">Now that we have an idea of what curve fitting entails, let’s try it using a swarm. The code we want is in <em>curves.py</em>. We’ll use it first, then look at parts of it. I strongly recommend you read through the code to get a feel for things.</p>
<p class="indent">The code expects a datafile that contains the measured points along with the function to fit. We’ll use <em>curves.py</em> to fit the previous example. The input file we need is <em>curfit_example.txt</em>:</p>
<pre class="pre">3
p[0]*x**2+p[1]*x+p[2]
10.2772497 0.0000000
12.2926738 0.7142857
15.7968918 1.4285714<span epub:type="pagebreak" id="page_107"/>
11.9787533 2.1428571
7.5707351 2.8571429
0.2314503 3.5714286
-0.1762932 4.2857143
-9.0166104 5.0000000
-21.6965056 5.7142857
-50.3670945 6.4285714
-60.2153079 7.1428571
-88.6989830 7.8571429
-107.3679996 8.5714286
-145.8216296 9.2857143
-173.1300077 10.0000000</pre>
<p class="noindent">The first line is the number of parameters followed by the function to fit. The function is given as Python code where the fit parameters are elements of a vector, <code>p</code>, and the data points are represented by <code>x</code>.</p>
<p class="indent">We want to fit a function like <em>ax</em><sup>2</sup> + <em>bx</em> + <em>c</em>, a three-parameter function, so we use <code>p[0]*x**2+p[1]*x+p[2]</code>. If you want something like sin <em>x</em>, use <code>np.sin(x)</code> (use NumPy). Note that the data points are listed as <em>y</em> then <em>x</em>.</p>
<p class="indent">Let’s use <em>curves.py</em> and this file to fit the data using differential evolution:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 curves.py curfit_example.txt -10 20 20 1000 0 DE pcg64</span>
Minimum mean total squared error: 16.430381313  (curfit_example.txt)
Parameters:
 0:   -2.7702810873939598
 1:    9.8170736277919577
 2:    6.6657767196319488
(73 best updates, 20020 function calls, time: 1.618 seconds)</pre>
<p class="noindent">The output tells us several things, but look first at the parameters. These are the elements of <code>p</code>, the best set of parameters found. Compare them with <a href="ch04.xhtml#ch04fig01">Figure 4-1</a>. The fit is quite good.</p>
<p class="indent">The generic algorithm says that particles need to evaluate where they are in the haystack to determine how good a solution their current position represents. There are three parameters in the fit function; therefore, our haystack is a three-dimensional space, and the particles are initially scattered randomly throughout this space. Each point in the three-dimensional space corresponds to a <code>p</code> vector, a set of three parameters. The best position found during the search is reported by <em>curves.py</em>.</p>
<p class="indent">For every particle, at every position in the haystack, we calculate the value of the <em>objective function</em>, the fitness function that tells us about the quality of the solution at that position. For curve fitting, our objective function measures the mean squared error between the measured points, (<em>x</em>, <em>y</em>), and the <em>y</em> values the function returns for the same <em>x</em> positions. If <em>ŷ</em> = <em>f</em>(<em>x</em>, <em><strong>p</strong></em>) is the output of the function at <em>x</em> for some particle position, <em><strong>p</strong></em>, then the <em>mean squared error (MSE)</em> is:</p>
<div class="image1"><img alt="Image" id="ch04equ1" src="../images/f0107-01.jpg"/></div>
<p class="noindent"><span epub:type="pagebreak" id="page_108"/>The sum is over all the measured points, (<em>x<sub>i</sub></em>, <em>y<sub>i</sub></em>).</p>
<p class="indent">The closer the MSE is to zero, the better the function is at fitting the measured data, meaning the particle position giving us the smallest MSE is the best fit found. The swarm algorithm keeps adjusting particle positions until it finds the minimum or we run out of iterations.</p>
<p class="indent">The <em>curves.py</em> file accepts many command line parameters:</p>
<div class="bqparan">
<p class="noindentin"><span class="codestrong">curfit_example.txt</span>   Datafile</p>
<p class="noindentin"><span class="codestrong">-10</span>   Lower bound</p>
<p class="noindentin"><span class="codestrong">20</span>   Upper bound</p>
<p class="noindentin"><span class="codestrong">20</span>   Number of particles</p>
<p class="noindentin"><span class="codestrong">1000</span>   Number of iterations</p>
<p class="noindentin"><span class="codestrong">0</span>   Tolerance</p>
<p class="noindentin"><span class="codestrong">DE</span>   Algorithm (DE, Jaya, PSO, GA, RO)</p>
<p class="noindentin"><span class="codestrong">pcg64</span>   Randomness source</p>
</div>
<p class="indent">The first argument is the name of the file that has the measured data points. Again, the first line is the number of parameters in the fit, followed by the code to implement the fit function. The remainder of the file are the actual data points, <em>y</em> then <em>x</em>, one pair per line.</p>
<p class="indent">The next two arguments specify the bounds of the search. These limit the size of the space the swarm can move through. Specifying a scalar applies that value to all dimensions; otherwise, specify each dimension separated by <code>x</code>. In this case, we tell <em>curves.py</em> to limit its search space to the cube from (–10, –10, –10) to (20, 20, 20). The bounds are often helpful, but must enclose the actual best values; otherwise, the search will return only the best position within the given bounds.</p>
<p class="indent">The following argument, also <code>20</code>, specifies the size of the swarm, or the number of particles to scatter throughout the haystack. It’s generally better to have a smaller swarm and more iterations—the next parameter, here 1,000—but that’s only a rule of thumb; exceptions abound.</p>
<p class="indent">We are minimizing the MSE. The search stops early if the MSE is less than the given tolerance. By setting the tolerance to 0, we’re telling <em>curves.py</em> to search for 1,000 iterations of the swarm positions or to stop early if we find a position with no error. The last parameter is the randomness source for <code>RE</code>. A final parameter, the name of an output image file showing the data points and the fit, is also allowed.</p>
<p class="indent">Swarm algorithms are stochastic, meaning they change their output from run to run because they randomly assign initial particle positions and use random values during the search. For many problems, the changes are subtle and inconsequential, but sometimes the swarm simply gets lost. Therefore, it’s best to repeat searches several times, if possible, to be sure the results are meaningful.</p>
<p class="indent">To try the other swarm algorithms—PSO, Jaya, GA, and RO—specify them by name. I suspect you will find that PSO, Jaya, and even RO give results as good as DE. GA, however, is another story. The output, <span epub:type="pagebreak" id="page_109"/>numerically, is poor, though if you plot the result, it often looks at least somewhat reasonable. Does this mean GA is a flawed algorithm? No, it’s simply not well suited to this task. In general, GA is best for non-numerical optimization problems and problems with a higher number of dimensions (parameters). In this example, using GA means asking it to evolve a population of organisms with three genes each. That doesn’t leave much for evolution to work with.</p>
<p class="indent">Let’s look at one more curve fitting example. The function to fit is in <em>sinexp.txt</em>:</p>
<div class="image1"><img alt="Image" id="ch04equ2" src="../images/f0109-01.jpg"/></div>
<p class="noindent">This function is the sum of a sine and a normal curve and has five parameters: we’re in a five-dimensional search space and each particle is a point in this space. I can’t picture a five-dimensional haystack, but we’re looking for a needle in one regardless.</p>
<p class="indent">Let’s try <em>curves.py</em> using Jaya:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 curves.py sinexp.txt -3 23 20 1000 0 Jaya mt19937 fit.png</span>
Minimum mean total squared error: 0.000000015  (sinexp.txt)
Parameters:
 0:    1.9999892608106149
 1:    3.0000001706464414
 2:   20.0001115681148427
 3:    7.9999997934624147
 4:    0.6000128598331004
(137 best updates, 20020 function calls, time: 1.412 seconds)</pre>
<p class="indent">The first time I tried the code, the fit failed and returned a minimum MSE of 0.1656, which is orders of magnitude larger than the previous fit. A plot of the good result is in <a href="ch04.xhtml#ch04fig02">Figure 4-2</a>.</p>
<div class="image"><img alt="Image" id="ch04fig02" src="../images/04fig02.jpg"/></div>
<p class="figcap"><em>Figure 4-2: A fit to <a href="ch04.xhtml#ch04equ2">Equation 4.2</a> using Jaya</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_110"/>The search used 20 particles and 1,000 iterations as before. I limited the search space to the range –3 to 20 in all five dimensions. In this case, the dataset was generated directly from the function with parameter values of 2, 3, 20, 8, and 0.6, respectively. This explains the extremely low MSE: there is no noise in the measurements.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>You may encounter runtime warnings when using the code. These are the result of the swarm algorithm using parameter values that are too large for the exponential. Adding</em> <span class="codeitalic">-W ignore</span> <em>to the command line after</em> <span class="codeitalic">python3</span> <em>will suppress the warnings.</em></p>
</div>
<h4 class="h4" id="ch00lev2_39"><em><strong>The curves.py Code</strong></em></h4>
<p class="noindent">Let’s look at some code to get a feel for what the swarm algorithms are doing; it will also help you see how to put the pieces together. Start at the bottom of <em>curves.py</em>. The essence of the code, as you’ll see when you read through it, is the following:</p>
<pre class="pre">rng = RE(kind=kind)
b = Bounds(lower, upper, enforce="resample", rng=rng)
i = RandomInitializer(npart, ndim, bounds=b, rng=rng)
obj = Objective(X, Y, func)
swarm = DE(obj=obj, npart=npart, ndim=ndim, init=i, tol=tol, max_iter=niter, bounds=b, rng=rng)
swarm.Optimize()
res = swarm.Results()</pre>
<p class="indent">The first line configures an instance of <code>RE</code> using the source given on the command line (<code>kind</code>). The next four lines set up the search for differential evolution. The code is the same for Jaya, PSO, RO, and GA, with the addition of one more argument to the constructor in the case of PSO. Let’s go line by line; these are the steps to configure any swarm search using the framework, so we’ll see them again as we proceed.</p>
<p class="indent">First, swarm searches are bounded, so we need an instance of the <code>Bounds</code> class, or a subclass if we need to override one of its methods, typically <code>Validate</code>. The arguments are the lower and upper limits, the randomness source, and a parameter called <code>enforce</code> that’s set to <code>resample</code>. Referring back to the generic search algorithm, step 5 states that headquarters orders the agents to move to new positions based on the objective function values for their current positions. At times, these new positions might be outside the specified boundaries. The <code>enforce</code> parameter decides what to do in these cases. By setting it to <code>resample</code>, any particle dimension that is out of bounds is replaced by a randomly selected value along that dimension. The other option is <code>clip</code>, which clips the offending dimension to the minimum or maximum allowed. Most of the time, this isn’t what we want.</p>
<p class="indent">The <code>RandomInitializer</code> parameter provides an initializer to configure the swarm. It’s given the number of particles in the swarm (<code>npart</code>), the dimensionality of the search space (<code>ndim</code>), and the bounds configured in the previous line (<code>b</code>).</p>
<p class="indent">The search also needs to know how to evaluate the objective function, an instance of <code>Objective</code>. For the curve fit example, <code>X</code> and <code>Y</code> are the measured <span epub:type="pagebreak" id="page_111"/>points and <code>func</code> is the function to fit, all read from the datafile given on the command line. I’ll show you the objective function in a moment.</p>
<p class="indent">We’re now ready to create the swarm algorithm object (<code>swarm</code>), here an instance of <code>DE</code>. We give the objective function, number of particles, dimensions, initializer, and bounds, along with the randomness source. We also specify the tolerance (<code>tol</code>) and the number of iterations (<code>max_iter</code>).</p>
<p class="indent">Given all the configuration, using the swarm object is straightforward: call the <code>Optimize</code> method. When the call returns, the search is over. Call <code>Results</code> to return a dictionary with information about the search.</p>
<p class="indent">The most important elements of <code>res</code> are <code>gpos</code> and <code>gbest</code>. Both return lists tracing the collection of best positions the swarm found during its search. Therefore, the final element of these lists returns the best position (<code>gpos</code>) and the corresponding objective function value (<code>gbest</code>). The position is a vector with one value for each dimension of the search space; here each dimension is a parameter value for the function we’re fitting to the data. The <code>gbest</code> value is a scalar, the MSE for this set of parameters.</p>
<p class="indent">Let’s look at the <code>Objective</code> class. The object passed as the objective function must have, at a minimum, a method called <code>Evaluate</code>. The details aren’t very important, but because Python uses duck typing, any object with an <code>Evaluate</code> method that accepts a single argument is permitted. Here’s the code <em>curves.py</em> uses:</p>
<pre class="pre">class Objective:
    def __init__(self, x, y, func):
        self.x = x
        self.y = y
        self.func = func
        self.fcount = 0

    def Evaluate(self, p):
        self.fcount += 1
        x = self.x
        y = eval(self.func)
        return ((y - self.y)**2).mean()</pre>
<p class="indent">The constructor keeps references to the measured points, <code>x</code> and <code>y</code>, along with the string representing the function to fit (<code>func</code>). Many applications do not have ancillary information, in which case the constructor does nothing and need not be specified. Also, notice that <code>Objective</code> does not inherit from any other class, it needs only to implement <code>Evaluate</code> to be acceptable to the optimization framework.</p>
<p class="indent">The <code>Evaluate</code> method is called by the swarm algorithms. The argument, <code>p</code>, is the current position of a particle in the swarm, that is, a vector of possible parameter values. The first line increments <code>fcount</code>, an internal counter of the number of times <code>Evaluate</code> was called. The final value of <code>fcount</code> is displayed when <em>curves.py</em> exits.</p>
<p class="indent">The next line looks a little strange: it assigns the reference to the <em>x</em> data, <code>self.x</code>, to the local variable <code>x</code>. The following line uses Python’s <code>eval</code> function <span epub:type="pagebreak" id="page_112"/>to evaluate the function value; because <code>eval</code> uses both <code>x</code> and <code>p</code> as variable names, we need those names to exist in <code>Evaluate</code>—hence the <code>x = self.x</code>. The calculated function values are in <code>y</code>. These are the <em>ŷ</em> values in <a href="ch04.xhtml#ch04equ1">Equation 4.1</a>.</p>
<p class="indent">Finally, we calculate the MSE and return it to give the objective function value (or fitness value) for the supplied particle position in <code>p</code>. Notice there is no square root. I left it out to save a tiny bit of time. The smallest MSE is still the smallest even if the final square root is not applied.</p>
<p class="indent">When the swarm algorithm runs, it calls <code>Evaluate</code> thousands of times to map particle positions to MSE values. The swarm algorithms are utterly ignorant of <em>what</em> the objective function is measuring; all they know is that they pass a vector representing a particle position in a multidimensional space to the objective function, and it returns a scalar value where lower values are better than higher values. This makes the framework generic and applicable to a wide range of problems.</p>
<p class="indent">To recap, using the framework involves the following steps:</p>
<ol>
<li class="noindent">Determine how to map potential solutions to the problem to a position in a multidimensional space for the swarms to search.</li>
<li class="noindent">Use that mapping to create an objective function class supporting at least an <code>Evaluate</code> method to accept a candidate position vector and return a scalar representing the quality of the solution it represents. The framework always minimizes, so the smaller the return value, the better the solution. To maximize, return the negative of the fitness.</li>
<li class="noindent">Create a <code>Bounds</code> object to set the limits of the search space and what to do if those limits are exceeded.</li>
<li class="noindent">Create an initializer (<code>RandomInitializer</code>) to supply the initial positions of the swarm particles.</li>
<li class="noindent">Create an instance of the swarm class, <code>DE</code>, <code>PSO</code>, <code>Jaya</code>, <code>RO</code>, or <code>GA</code>.</li>
<li class="noindent">Perform the search by calling <code>Optimize</code>, and use <code>Results</code> to return the outcome.</li>
</ol>
<p class="indent">Our familiarity with the framework will grow with practice. For now, let’s review the swarm intelligence and evolutionary algorithms to understand how they differ from each other and where the randomness is. Randomness goes deeper than the initial configuration of the swarm particles in the search space; each algorithm depends critically on randomness for its operation.</p>
<h4 class="h4" id="ch00lev2_40"><em><strong>The Optimization Algorithms</strong></em></h4>
<p class="noindent">There are hundreds of swarm optimization algorithms out there, but what distinguishes one from another? The short answer is the method used for searching, or how headquarters orders the agents to new locations on each iteration.</p>
<p class="indent">The various algorithm approaches range from the most straightforward—RO, where the agents don’t communicate but wander from better position <span epub:type="pagebreak" id="page_113"/>to better position independently—to sophisticated algorithms incorporating information about agents’ current positions, histories, and associations into groups and neighborhoods.</p>
<p class="indent">In this section, I’ll summarize the essential operation of the five algorithms selected for the framework. In all cases, the essential operation is the same: scatter particles throughout the search space, evaluate the quality of each particle, decide where they go next, and repeat until a best position is found or time runs out. It’s the “decide where they go next” part that differentiates algorithms.</p>
<h5 class="h5"><strong>Random Optimization (RO)</strong></h5>
<p class="noindent">Swarm particles are represented by a vector of floating-point numbers, <em><strong>x</strong><sub>i</sub></em>, each component of which maps to a dimension of the search space. In other words, particles are points in the search space. On each iteration, particles construct a new position some distance from their current one and ask if the new position has a better fitness value, that is, if the objective function value is lower at the new position. If so, the particle moves to the new position; otherwise, it stays put. The new candidate position is</p>
<div class="image1"><img alt="Image" src="../images/f0113-01.jpg"/></div>
<p class="noindent">where <em>η</em> (eta) is a scale parameter (<em>η</em> = 0.1) and <em><strong>N</strong></em>(0, 1) is a vector of samples from a normal distribution with mean 0 and standard deviation 1. If <img alt="Image" class="inline" src="../images/pg113-01.jpg"/> has a lower objective function value, then <img alt="Image" class="inline" src="../images/pg113-02.jpg"/>; otherwise, the particle stays where it is for the next iteration. The particles make no use of what other particles have learned about the search space.</p>
<h5 class="h5"><strong>Jaya</strong></h5>
<p class="noindent">Jaya, Sanskrit for “victory,” is a swarm intelligence algorithm that has no adjustable parameters. Swarm algorithms depend on heuristics, so they often have adjustable parameters to improve their performance in different situations. Jaya does not. It works or it doesn’t.</p>
<p class="indent">On each iteration, the <em>i</em>th particle is updated via</p>
<div class="image1"><img alt="Image" src="../images/f0113-02.jpg"/></div>
<p class="noindent">where <em><strong>x</strong></em><sub>best</sub> and <em><strong>x</strong></em><sub>worst</sub> are the current best and worst positions of any particle in the swarm, and <em><strong>r</strong></em><sub>1</sub> and <em><strong>r</strong></em><sub>2</sub> are random vectors in [0, 1) per component. The vertical bars apply the absolute value to each component of the vector. In other words, Jaya moves particles toward the swarm’s best position and away from the swarm’s worst.</p>
<h5 class="h5"><strong>Particle Swarm Optimization (PSO)</strong></h5>
<p class="noindent">The update equations for PSO depend on the flavor. Our framework offers two: canonical and bare-bones. The <em>curves.py</em> file uses bare-bones, hence <code>bare=True</code> in the <code>PSO</code> constructor. However, it’s easier to begin with canonical PSO.</p>
<p class="indent">In canonical PSO, each particle (<em><strong>x</strong><sub>i</sub></em>) is associated with two other vectors. The first, <img alt="Image" class="inline" src="../images/xcap.jpg"/>, is the best position in the search space that <em>that</em> particle <span epub:type="pagebreak" id="page_114"/>has found, and the second, <em><strong>υ</strong><sub>i</sub></em>, is the particle’s velocity, which controls how quickly and in which direction the particle moves through the search space.</p>
<p class="indent">The canonical PSO update rule is accomplished in two steps. First, the velocity is updated:</p>
<div class="image1"><img alt="Image" src="../images/f0114-01.jpg"/></div>
<p class="indent">Here, <em>ω</em> is the inertia factor multiplying the current velocity. It’s a scalar, usually in [0.5, 1), with a typical initial value of 0.9. It decreases from iteration to iteration. This slows the particle as the search progresses, in theory, because the particle is likely moving closer to the best position. The second term calculates the difference between the particle’s best-known position so far, <img alt="Image" class="inline" src="../images/xcap.jpg"/>, and its current position, <em><strong>x</strong><sub>i</sub></em>. This value is multiplied, component by component, by <em><strong>c</strong></em><sub>1</sub> = <em>c</em><sub>1</sub><em><strong>U</strong></em>[0, 1), that is, a random vector in [0, 1) multiplied by a scalar, <em>c</em><sub>1</sub>. We use 1.49, a typical value. The last term in the velocity update calculates the difference between the swarm’s best-known position, <em><strong>g</strong></em>, and the particle’s current position, and multiplies it by vector <em><strong>c</strong></em><sub>2</sub> = <em>c</em><sub>2</sub><em><strong>U</strong></em>[0, 1). Usually, <em>c</em><sub>1</sub> = <em>c</em><sub>2</sub>.</p>
<p class="indent">Second, the particle position is updated using the newly calculated velocity:</p>
<p class="center"><em><strong>x</strong><sub>i</sub></em> ← <em><strong>x</strong><sub>i</sub></em> + <em><strong>υ</strong><sub>i</sub></em></p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>If you have a physics background and are, like me, bothered by the addition of a velocity and a position, imagine a</em> Δ t = 1 <em>multiplying</em> <strong>v</strong><em><sub>i</sub> where</em> Δ t <em>is the time step between iterations. Now the units are correct.</em></p>
</div>
<p class="indent">Bare-bones PSO, sometimes called BBPSO, does not use a velocity vector. Instead, particle positions are updated with samples from a normal distribution. If <em><strong>x</strong><sub>i</sub></em> is the vector representing particle <em>i</em>’s current position, then <em><strong>x</strong><sub>ij</sub></em> is the <em>j</em>th component of that vector. With that in mind, calculate</p>
<div class="image1"><img alt="Image" src="../images/f0114-03.jpg"/></div>
<p class="noindent">if <em>p ∼ U</em>[0, 1) &lt; <em>p<sub>b</sub></em>, otherwise</p>
<div class="image1"><img alt="Image" src="../images/f0114-04.jpg"/></div>
<p class="noindent">for each component (<em>j</em>) of each particle (<em>i</em>). Here, <img alt="Image" class="inline" src="../images/f0114-05.jpg"/> means draw a sample from a normal distribution with mean <img alt="Image" class="inline" src="../images/x-bar.jpg"/> and standard deviation <em>σ</em>. Typically, <em>p<sub>b</sub></em> = 0.5, so 50 percent of the time, on average, the particle’s <em>j</em>th component is calculated from the normal distribution, and 50 percent of the time it’s simply a copy of the corresponding component of the particle’s best position, <img alt="Image" class="inline" src="../images/xcap.jpg"/>.</p>
<h5 class="h5"><strong>Genetic Algorithm (GA)</strong></h5>
<p class="noindent">We know from our evolution experiments that a GA involves breeding (crossover) and random mutation. The code in <em>GA.py</em> follows this pattern, <span epub:type="pagebreak" id="page_115"/>but fits with the overall optimization framework. In particular, <em>GA.py</em> manipulates floating-point values by default, not integers. You can alter this behavior by subclassing <code>Bounds</code> and implementing a <code>Validate</code> method to force integer values.</p>
<p class="indent">The update rule for a particle, <em><strong>x</strong><sub>i</sub></em>, involves crossover with a randomly chosen mate, where in this case the mate is selected from the top 50 percent best performing particles—see the <code>top</code> parameter of the <code>GA</code> constructor. Additionally, the current best-particle position, that is, the fittest particle, is passed to the next generation unaltered.</p>
<p class="indent">Our evolution experiments bred every individual, generation to generation. Here, individuals breed only if a random value is below the <code>CR</code> probability, 0.8 by default. When an individual breeds, it is replaced by the offspring. Whether <em><strong>x</strong><sub>i</sub></em> breeds, there is a certain probability of a random mutation by assigning a randomly selected dimension a random value. Therefore, for any update, a particle may be replaced by its offspring, and it may undergo random mutation. The default mutation probability is 5 percent (<code>F</code> in the <code>GA</code> constructor).</p>
<p class="indent">As a rule of thumb, the GA seems to work best for problems that aren’t mathematical (like curve fitting) and involve a higher number of dimensions to give evolution a larger “genome” to manipulate. Whether this has implications for biological evolution, I don’t know; regardless, another hallmark of the GA is slow convergence. You often need an order of magnitude more iterations (or even more) to reach a solution similar to that found far more quickly by Jaya or DE. Let’s turn there now.</p>
<h5 class="h5"><strong>Differential Evolution (DE)</strong></h5>
<p class="noindent">DE was invented in 1995 by Price and Storn, the same year particle swarm optimization was invented by Kennedy and Eberhart. Like PSO, DE has stood the test of time and grown into a collection of similar approaches. DE is an evolutionary algorithm where particles are updated between iterations by a process involving crossover and mutation. However, unlike the straightforward crossover and mutation of the GA, DE replaces <em><strong>x</strong><sub>i</sub></em> with a new vector that is, in a sense, the offspring of <em>four</em> parents. DE isn’t modeled on nature.</p>
<p class="indent">To update particle <em><strong>x</strong><sub>i</sub></em>, first select three other members of the swarm, unique and not <em><strong>x</strong><sub>i</sub></em>. From these three, create a donor vector:</p>
<p class="center"><em><strong>υ</strong></em> = <em><strong>υ</strong></em><sub>1</sub> + <em>F</em>(<em><strong>υ</strong></em><sub>2</sub> – <em><strong>υ</strong></em><sub>3</sub>)</p>
<p class="noindent">Some variants of DE require that <em><strong>υ</strong></em><sub>1</sub> be the best performing member of the swarm instead of a randomly selected member. Here, <em>F</em> plays the role of mutation in the GA. In this case, the default value is <em>F</em> = 0.8.</p>
<p class="indent">The offspring of <em><strong>x</strong><sub>i</sub></em> and <em><strong>υ</strong></em> is created component by component (gene by gene) where, with probability <em>CR</em>, the corresponding component of <em><strong>υ</strong></em> is used; otherwise, the component of <em><strong>x</strong><sub>i</sub></em> is retained. The default value is <em>CR</em> = 0.5, meaning the offspring of <em><strong>x</strong><sub>i</sub></em> retains, on average, 50 percent of its existing values (genes).</p>
<p class="indent">There are so many variants of DE that a nomenclature has arisen to describe them. The code in <em>DE.py</em> defaults to “DE/rand/1/bin,” meaning the <span epub:type="pagebreak" id="page_116"/>donor vector uses three randomly selected vectors (“rand”), a single differential (<em><strong>υ</strong></em><sub>2</sub> – <em><strong>υ</strong></em><sub>3</sub>), and a <em>Bernoulli</em> crossover (“bin”). A Bernoulli trial is a coin flip where the probability of success is <em>p</em> and that of failure 1 – <em>p</em>. Here, <em>p</em> = <em>CR</em> is the crossover probability.</p>
<p class="indent">The <code>DE</code> class supports two additional types of selection and an additional type of crossover, should you wish to experiment with them. If one of the three vectors used to build the donor is always the current swarm best, then the label begins with “DE/best/1.” Additionally, a new selection mode is supported: DE/toggle/1, which toggles between “rand” and “best” every other update. Finally, Bernoulli crossover may be replaced with GA-style crossover, meaning the <code>DE</code> class supports six possible differential variants from the three different selection and two crossover types. Feel free to experiment with all of them. Do you notice anything different between them, especially how quickly the swarm converges? Hint: look at all the values in the <code>gbest</code> element of the dictionary returned by <code>DE</code>’s <code>Results</code> method combined with the <code>giter</code> element that tracks the iteration number for each new swarm best position (<code>gpos</code>) and objective function value (<code>gbest</code>).</p>
<p class="indent">The goal of this section was to fit a known function to a dataset by minimizing the MSE between the data points and the function value at those points. We were after the parameters of the function, as we already knew the form we wanted. This raises the question: What if all we have is the data and we don’t know the functional form? There are several ways to answer this. One is to use a machine learning model—after all, that’s what they are designed to do: learn a model (function) from a set of data. We’ll do this in <a href="ch05.xhtml">Chapter 5</a>. Another is to evolve a piece of code that approximates the data. Let’s give this approach a shot.</p>
<h3 class="h3" id="ch00lev1_26"><strong>Fitting Data</strong></h3>
<p class="noindent">Curve fitting had us searching for the parameters of a known function. In this section, all we have is the data and our goal is to evolve a piece of code approximating a function that fits the data. We still want <em>y</em> = <em>f</em>(<em>x</em>)—that is, for a given <em>x</em> we get an approximated <em>y</em>—but here <em>f</em>(<em>x</em>) is Python code. Evolving code is known as <em>genetic programming (GP)</em>, and it has a long history dating back to the early 1990s. A related term is <em>symbolic regression</em>.</p>
<p class="indent">As the name suggests, GP often uses the GA. Our implementation, however, uses the framework from the previous section so we can select any of the swarm intelligence and evolutionary algorithms. To use a swarm, we need to find a mapping between what we want (code) and a multidimensional space where each position in the space represents a possible solution. For curve fitting, the mapping was straightforward. If there were <em>n</em> parameters in the function, there were <em>n</em> parameters in the search space where each coordinate of a specific point was, literally, the parameter value.</p>
<p class="indent">Here, we have to be more clever. To identify the mapping, let’s think about how we want to represent the code of our function, and from there, the mapping might be somewhat easier to see.</p>
<p class="indent"><span epub:type="pagebreak" id="page_117"/>We want a function manipulating a scalar input value, <em>x</em>, to arrive at a scalar output value, <em>y</em>. So, we need math. We’ll make do with the standard arithmetic operations, plus negation, modulo, and powers.</p>
<p class="indent">Doing math implies mathematical expressions. Here things become murkier. Manipulating mathematical expressions is rather tricky, more tricky than we care to attack in a book like this. Traditional GP manipulates expressions using an evolutionary algorithm, complete with crossover and mutation, where crossover merges two expressions and mutation alters a term in the expression.</p>
<p class="indent">Fortunately, we can use a shortcut. If we have a stack and know about postfix notation, we have all we need to generate expressions and map code to a position in the search space. I’ll explain, but let’s make sure we’re on the same page when it comes to stacks and postfix notation.</p>
<h4 class="h4" id="ch00lev2_41"><em><strong>Introducing Stacks and Postfix Notation</strong></em></h4>
<p class="noindent">Imagine a stack of cafeteria trays. As new trays are added to the stack, they rest on top of all the existing trays. When someone needs a tray, they take the top tray, meaning the last tray added to the stack is the first tray removed from it. <em>Stacks</em> are like cafeteria trays (though cleaner).</p>
<p class="indent">Consider this example. We have three numbers: 1, 2, and 5. We also have a stack that is currently empty. The first number we <em>push</em> on the stack is 1, then we push 2, and finally 5. <a href="ch04.xhtml#ch04fig03">Figure 4-3</a> shows what the stack looks like step by step, from left to right.</p>
<div class="image"><img alt="Image" id="ch04fig03" src="../images/04fig03.jpg"/></div>
<p class="figcap"><em>Figure 4-3: Stack manipulation</em></p>
<p class="noindent">On the left, the stack is empty; then, moving right, we add 1, then 2, and finally 5. The stack is now three deep, with 1 at the bottom and 5 at the top.</p>
<p class="indent">Now it’s time to <em>pop</em> a value off the stack. What value do we get? In a queue, we would get 1, the first value in. For a stack, we get 5, the last value pushed. Pop the stack again and we get 2, and finally 1, leaving the stack empty. Values pop off a stack in reverse order compared to how they are pushed onto the stack.</p>
<p class="indent">Stacks are natural structures for manipulating expressions in <em>postfix</em> form, that is, expressions where the operands come first, followed by the operation. For example, infix notation, our usual way of writing expressions, might say <em>a</em> + <em>b</em>, but in postfix notation, this becomes <em>a b</em> +. Postfix notation, also called reverse Polish notation (RPN), was developed in 1924 by Polish mathematician Jan Łukasiewicz. Postfix notation doesn’t require parentheses to alter operator precedence. Instead, it builds the expression piece by piece. Combine postfix notation with a stack, and it becomes straightforward to evaluate arbitrary expressions. This is precisely what we want.</p>
<p class="indent"><span epub:type="pagebreak" id="page_118"/>To better understand what I mean, let’s translate the infix expression <em>y</em> = <em>a</em>(<em>b</em> + <em>c</em>) – <em>d</em> to postfix notation and implement it using a stack and pseudocode statements. In postfix notation, it becomes <em>a b c</em> + × <em>d</em> –. To evaluate it, move from left to right until you hit an operator, here +. The operands are the two variables to the left, <em>b</em> and <em>c</em>. Compute <em>b</em> + <em>c</em> and replace “<em>b c</em> +” with the result, <em>t</em><sub>0</sub>. The expression is now <em>a t</em><sub>0</sub> × <em>d</em> –. Repeat to find × with operands <em>a</em> and <em>t</em><sub>0</sub>. Compute the product and replace it with <em>t</em><sub>1</sub> to get <em>t</em><sub>1</sub> <em>d</em> –. Finally, evaluate <em>t</em><sub>1</sub> – <em>d</em> to get the value of the expression, <em>y</em>.</p>
<p class="indent">Let’s implement this process in code using a stack to hold values. Consider the following:</p>
<pre class="pre">push(a)  | a
push(b)  | a b
push(c)  | a b c
add      | a t0
mul      | t1
push(d)  | t1 d
sub      | y</pre>
<p class="indent">The values on the right show the stack after each instruction with <em>t</em><sub>0</sub> = <em>b</em> + <em>c</em>, <em>t</em><sub>1</sub> = <em>a</em> × <em>t</em><sub>0</sub>, and <em>y</em> = <em>t</em><sub>1</sub> – <em>d</em>. The expressions leave the answer, <em>y</em>, on the stack, and <code>push</code> places values on the stack. Binary operations like <code>add</code> pop two values off the stack, add them, and push the result back on the stack. Therefore, a linear sequence of statements and a stack are all we need to implement any function of <em>x</em> yielding <em>y</em>—at least, any function involving arithmetic operations, negation, and powers.</p>
<p class="indent">Functions written in this way become sequences of instructions with no loops. If we find a way to map the sequences to floating-point vectors, we’re in business.</p>
<h4 class="h4" id="ch00lev2_42"><em><strong>Mapping Code to Points</strong></em></h4>
<p class="noindent">To evolve code, we need the four basic arithmetic operations: addition (<code>add</code>), subtraction (<code>sub</code>), multiplication (<code>mul</code>), and division (<code>div</code>). We also need exponentiation (<code>pow</code>) and will throw in modulo for good measure (<code>mod</code>).</p>
<p class="indent">Postfix notation distinguishes the subtraction operator and negation, treating the latter as a different instruction, so we need negation (<code>neg</code>) as well, <em>x →</em> –<em>x</em>.</p>
<p class="indent">Finally, we need two more instructions: <code>halt</code> and <code>push</code>. If <code>halt</code> is executed, the code stops and ignores any subsequent instructions. The <code>push</code> instruction pushes <em>x</em> or a number (a constant) on the stack.</p>
<p class="indent">We have nine instructions. We want a sequence of instructions executed in order, which reminds me of a vector where each element is an instruction, and we execute the instructions one at a time from index 0 to the end of the vector.</p>
<p class="indent"><span epub:type="pagebreak" id="page_119"/>Each instruction becomes a value, for example, <code>add</code> is 1 and <code>sub</code> is 2, so if a particle has a 2 in a particular component, then that component encodes a subtraction instruction. Particle positions are floating-point numbers, not integers, so we’ll keep only the integer part of each, meaning a component with a floating-point value of 2.718 is interpreted as a 2, implying a subtraction instruction.</p>
<p class="indent"><a href="ch04.xhtml#ch04tab01">Table 4-1</a> has the mapping we’ll use for instructions (the integer part of a particle position).</p>
<p class="tabcap" id="ch04tab01"><strong>Table 4-1:</strong> Mapping Particle Positions to Instructions</p>
<table class="table-h">
<colgroup>
<col style="width:60%"/>
<col style="width:40%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><strong>Instruction</strong></th>
<th class="tab_th"><strong>Number</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1"><code>add</code></td>
<td class="bg1">1</td>
</tr>
<tr>
<td class="bg"><code>sub</code></td>
<td class="bg">2</td>
</tr>
<tr>
<td class="bg1"><code>mul</code></td>
<td class="bg1">3</td>
</tr>
<tr>
<td class="bg"><code>div</code></td>
<td class="bg">4</td>
</tr>
<tr>
<td class="bg1"><code>mod</code></td>
<td class="bg1">5</td>
</tr>
<tr>
<td class="bg"><code>pow</code></td>
<td class="bg">6</td>
</tr>
<tr>
<td class="bg1"><code>neg</code></td>
<td class="bg1">7</td>
</tr>
<tr>
<td class="bg"><code>push(x)</code></td>
<td class="bg">8</td>
</tr>
<tr>
<td class="bg1"><code>halt</code></td>
<td class="bg1">9</td>
</tr>
</tbody>
</table>
<p class="noindent">All that remains is to handle pushing constants on the stack. If we can’t do this, we’re stuck evolving expressions of only <em>x</em>, like <em>xx</em> + <em>x</em> – <em>x</em> – <em>x</em>, which will get us nowhere.</p>
<p class="indent">The instruction numbers begin with 1, not 0. This is intentional. Numbering this way leaves vector components in the range [0, 1) available as that range is not associated with an instruction.</p>
<p class="indent">Let’s use this range to push arbitrary numbers on the stack. When we run a search, we’ll specify a lowest and highest number, like –1 and 11. Then, we’ll map values in [0, 1) to [–1, 11). So, to push a constant of 3.1472 on the stack, we issue the instruction 0.3456 because:</p>
<p class="center"><em>a</em> + <em>f</em>(<em>b</em> – <em>a</em>) = –1 + 0.3456(11 – <sup>–</sup>1) = 3.1472</p>
<p class="noindent">Handling things this way lets us specify arbitrary constant values within the given range.</p>
<p class="indent">For example, <a href="ch04.xhtml#ch04tab02">Table 4-2</a> shows a segment of code generated by <em>gp.py</em>, the program we’re in the process of developing, along with the actual particle position values.</p>
<p class="tabcap" id="ch04tab02"><span epub:type="pagebreak" id="page_120"/><strong>Table 4-2:</strong> An Evolved Code Sample</p>
<table class="table-h">
<colgroup>
<col style="width:60%"/>
<col style="width:40%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><strong>Instruction</strong></th>
<th class="tab_th"><strong>Particle value</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1"><code>push(x)</code></td>
<td class="bg1">—</td>
</tr>
<tr>
<td class="bg"><code>push(x)</code></td>
<td class="bg">8.5251446</td>
</tr>
<tr>
<td class="bg1"><code>push(3.00482)</code></td>
<td class="bg1">0.6502409</td>
</tr>
<tr>
<td class="bg"><code>mul</code></td>
<td class="bg">3.3605457</td>
</tr>
<tr>
<td class="bg1"><code>push(7.07870)</code></td>
<td class="bg1">0.8539350</td>
</tr>
<tr>
<td class="bg"><code>push(-9.09650)</code></td>
<td class="bg">0.0451748</td>
</tr>
<tr>
<td class="bg1"><code>mod</code></td>
<td class="bg1">5.0708302</td>
</tr>
<tr>
<td class="bg"><code>add</code></td>
<td class="bg">1.3708454</td>
</tr>
<tr>
<td class="bg1"><code>halt</code></td>
<td class="bg1">9.7707617</td>
</tr>
<tr>
<td class="bg"><code>div</code></td>
<td class="bg">4.2693693</td>
</tr>
<tr>
<td class="bg1"><code>sub</code></td>
<td class="bg1">2.6309877</td>
</tr>
<tr>
<td class="bg"><code>div</code></td>
<td class="bg">4.6783009</td>
</tr>
<tr>
<td class="bg1"><code>pow</code></td>
<td class="bg1">6.5429319</td>
</tr>
</tbody>
</table>
<p class="indent">The task was to fit a noisy set of points representing a line. The evolved function fit the data quite nicely. The number limit in this case was –10 to 10, and I told the search (bare-bones PSO) to use 12 instructions. All evolved functions begin with <em>x</em> on the stack, here represented by the first <code>push(x)</code>. Also, when the function exits, it returns the top stack value as <em>y</em>. Any remaining stack values are ignored.</p>
<p class="indent">If the particle value is ≥ 1, the integer part specifies an instruction, so 8.525 <em>→</em> 8, which is the instruction to push <em>x</em>, just as 3.360 <em>→</em> 3 to indicate multiplication.</p>
<p class="indent">Look at the second particle vector component, 0.6502409, which, with the number limits, becomes:</p>
<p class="center">–10 + 0.6502409(10 – <sup>–</sup>10) = 3.00482</p>
<p class="indent">This number is multiplied by <em>x</em>, that is, the second and third instructions implement 3.00482<em>x</em>. The data points were generated by adding a small amount of random noise to the line 3<em>x</em> – 2. The evolved function immediately implements 3.00482<em>x</em>, which is quite encouraging.</p>
<p class="indent">The next three instructions push 7.07870 then –9.09650 on the stack before executing <code>mod</code>. This seems like a strange thing to do, but consider what Python does with the expression:</p>
<pre class="pre">&gt;&gt;&gt; <span class="codestrong1">7.07870 % -9.09650</span>
-2.0178000000000003</pre>
<p class="noindent">These instructions leave –2.0178 on the stack.</p>
<p class="indent">The next instruction is <code>add</code>. We add the top two stack values, which we just learned are 3.00482<em>x</em> and –2.0178. Interestingly, this is equivalent to the infix expression 3.00482<em>x</em> – 2.0178, and earlier I stated that the data points were generated from 3<em>x</em> – 2. The evolved code implements the expression used to create the data points in the first place.</p>
<p class="indent"><span epub:type="pagebreak" id="page_121"/>The instruction following <code>add</code> is <code>halt</code>, which causes the function to exit with the sum on the stack. Instructions after <code>halt</code> are never executed.</p>
<p class="indent">Fabulous! We have an approach, a way to map floating-point vectors to code to implement a function. It’s a bit odd, but we’ll run with it and see where we get. Our next task is to create <em>gp.py</em>.</p>
<h4 class="h4" id="ch00lev2_43"><em><strong>Creating gp.py</strong></em></h4>
<p class="noindent">If you haven’t already, read through <em>gp.py</em>. The essential framework pieces are all there, and we won’t discuss every line, so it will help to be familiar with it before we begin.</p>
<p class="indent">The code imports all the framework components from the earlier curve fitting exercise; defines some helper functions (<code>GetData</code>, <code>Number StrExpression</code>) and the objective function class before the main code, which interprets the command line; constructs framework objects; and runs the search. Let’s review <code>Number</code> and the objective function class here. The main code mirrors that of the curve fitting code.</p>
<p class="indent">The <code>Number</code> function transforms particle values in [0, 1) to the range specified on the command line when <em>gp.py</em> is executed. Specifically:</p>
<pre class="pre">def Number(f, gmin=-20.0, gmax=20.0):
    return gmin + f*(gmax-gmin)</pre>
<p class="noindent">It’s a direct implementation of the previous equation with <code>gmin</code> and <code>gmax</code> being the limits from the command line. These limits restrict the range of possible constants available to the evolved code; therefore, some experimentation might be necessary to find reasonable limits. For example, if you run a search and see constants at the limits, the specified range is likely too small, so double the size and try again. Remember, swarm intelligence and evolutionary algorithms are stochastic and heuristic. Parameters controlling their operation abound and must often be managed to produce good results.</p>
<p class="indent">A successful swarm search utilizes an objective function that drives the swarm toward good solutions, here a piece of code minimizing the MSE between the known data points and the output of the code at those data points. Therefore, our next port of call is the objective function class, <code>Objective</code>:</p>
<pre class="pre">class Objective:
    def __init__(self, x,y, gmin=-20.0, gmax=20.0):
        self.fcount = 0
        self.x = x.copy()
        self.y = y.copy()
        self.gmin = gmin
        self.gmax = gmax

    def Evaluate(self, p):
        self.fcount += 1
<span epub:type="pagebreak" id="page_122"/>
        y = np.zeros(len(self.x))
        for i in range(len(self.x)):
            y[i] = Expression(self.x[i],p, self.gmin, self.gmax)
            if (np.isnan(y[i])):
                y[i] = 1e9
        return ((y - self.y)**2).mean()</pre>
<p class="indent">There are two methods, a constructor and <code>Evaluate</code>. The constructor keeps the data points and the number limits passed on the command line. It also initializes <code>fcount</code>, which tracks the number of times the objective function is evaluated.</p>
<p class="indent">The <code>Evaluate</code> method accepts a particle position (<code>p</code>) and passes the <em>x</em>-coordinate of the data points through it to generate output vector <code>y</code>. It then returns the MSE between <code>y</code> and <code>self.y</code> as the objective function value.</p>
<p class="indent">Not every piece of code represented by a particle position is valid. It’s likely, especially early on in the search, that the randomly generated particle positions become failing blocks of code because they try impossible things like extracting values from an empty stack or dividing by zero. The NaN check in <code>Evaluate</code> captures such cases and ensures that a very high objective function value is returned.</p>
<p class="indent">The function <code>Expression</code> evaluates a particle position as code. It’s given the <em>x</em> values, the particle position (<code>p</code>), and the number range; see <a href="ch04.xhtml#ch04list01">Listing 4-1</a>.</p>
<pre class="pre">def Expression(x, expr, gmin=-20.0, gmax=20.0):
 <span class="ent">➊</span> def BinaryOp(s,op):
        b = s.pop()
        a = s.pop()
        if (op == 0):
            c = a + b
        elif (op == 1):
            c = a - b
        elif (op == 2):
            c = a * b
        elif (op == 3):
            c = a / b
        elif (op == 4):
            c = a % b
        elif (op == 5):
            c = a**b
        s.append(c)

    bad = 1e9
 <span class="ent">➋</span> s = [x]
    try:
     <span class="ent">➌</span> for e in expr:
            if (e &lt; 1.0):
                s.append(Number(e, gmin=gmin, gmax=gmax))<span epub:type="pagebreak" id="page_123"/>
            else:
                op = int(np.floor(e))
                if (op &lt; 7):
                    BinaryOp(s, op-1)
                elif (op == 7):
                    s.append(-s.pop())
                elif (op == 8):
                    s.append(x)
                elif (op == 9):
                    break
    except:
        return bad
    try:
     <span class="ent">➍</span> return s.pop()
    except:
        return bad</pre>
<p class="list" id="ch04list01"><em>Listing 4-1: Interpreting a particle position as code</em></p>
<p class="indent"><a href="ch04.xhtml#ch04list01">Listing 4-1</a> is the heart of <em>gp.py</em>. First, there is an embedded function, <code>BinaryOp</code> <span class="ent">➊</span>, which implements all binary operations like addition and exponentiation. The stack (<code>s</code>), a standard Python list, is popped twice to get the operands. Note the order: if we want <em>a</em> – <em>b</em> and <em>b</em> is the top stack item, then the first pop returns <em>b</em>, not <em>a</em>. The second argument indexes the operation. A more compact implementation might use Python’s <code>eval</code> function. Still, we need to be as fast as possible, so we opt for the verbose but significantly faster compound <code>if</code>.</p>
<p class="indent">The code initializes the stack with <em>x</em> <span class="ent">➋</span> and then begins a loop over the components of the particle position (<code>expr</code>) <span class="ent">➌</span>. Everything is inside a <code>try</code> block to catch any errors. Errors return <code>bad</code> as the function value.</p>
<p class="indent">If the particle component is less than 1.0, it pushes a constant, the output of <code>Number</code>, on the stack. Otherwise, the integer part of the value determines the operation. If less than 7, the appropriate binary operation is performed; otherwise, the instruction is either negation, push <em>x</em>, or halt, which breaks out of the loop, thereby ignoring the remaining particle components. Finally, the function returns the top stack item, if there is one, as the function value <span class="ent">➍</span>.</p>
<p class="indent">The remainder of <em>gp.py</em> is straightforward: parse the command line, create framework objects (<code>Bounds</code>, <code>RandomInitializer</code>, <code>Objective</code>), then, with the proper swarm object, call <code>Optimize</code> and <code>Results</code> to report how well the search went. If a final plot name is given, we generate it showing the data points and the fit.</p>
<p class="indent">Our curve fitting code used bare-bones PSO. This code uses both bare-bones and canonical PSO:</p>
<pre class="pre">elif (alg == "PSO"):
    swarm = PSO(obj=obj, npart=npart, ndim=ndim, init=i, tol=0, max_iter=niter, bounds=b, 
                rng=rng, vbounds=Bounds([-10]*ndim, [10]*ndim, enforce="clip", rng=rng), 
                inertia=LinearInertia(), ring=True, neighbors=6)
<span epub:type="pagebreak" id="page_124"/>
elif (alg == "BARE"):
    swarm = PSO(obj=obj, npart=npart, ndim=ndim, init=i, tol=0, max_iter=niter, bounds=b, 
                rng=rng, bare=True)</pre>
<p class="noindent">We can differentiate between the two by passing either <code>PSO</code> or <code>BARE</code> on the command line. Note that PSO uses options we haven’t seen before. One is <code>LinearInertia</code>, which linearly decreases <em>ω</em> during the search from 0.9 down to 0.4. Inertia is the coefficient multiplying the previous iteration’s velocity, per particle.</p>
<p class="indent">There are three additional options. Two are <code>ring</code> and <code>neighbors</code>, which work together. A variation of canonical PSO includes the concept of a <em>neighborhood</em>, a collection of particles that coordinate with each other. The practical effect of a neighborhood is to replace the global best position, <em><strong>g</strong></em>, with a neighborhood best. The arrangements of particles into neighborhoods is referred to as a <em>topology</em>. The <code>PSO</code> class supports a ring topology—the simplest. Imagine the particles forming a circle; then, for any particle, the <code>neighbors</code> particles to the left and right form the current particle’s neighborhood. As a challenge, try to modify <em>PSO.py</em> to accommodate von Neumann neighborhoods, which is frequently the best performing topology. Careful searching online will show you what a von Neumann topology entails.</p>
<p class="indent">The final new option is <code>vbounds</code>, which sets limits on the maximum velocity per particle component, much as <code>bounds</code> sets spatial limits on where the swarm can move within the search space. In the velocity case, <code>enforce</code> is <code>clip</code> to keep velocity components at the limits instead of resampling along that dimension.</p>
<p class="indent">The number of adjustable parameters makes it sometimes tricky to set up a successful canonical PSO search, even with neighborhoods. As a result, these values are more what you’d call “guidelines” than actual rules.</p>
<p class="indent">Now, let’s put <em>gp.py</em> through its paces to see what it can (and cannot) do for us.</p>
<h4 class="h4" id="ch00lev2_44"><em><strong>Evolving Fit Functions</strong></em></h4>
<p class="noindent">Let’s put <em>gp.py</em> to the test with several experiments.</p>
<h5 class="h5"><strong>Fitting a Line</strong></h5>
<p class="noindent">To run <em>gp.py</em>, use a command line like this one:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 gp.py data/x1_2n.txt -5 5 22 20 10000 bare minstd plot.png</span></pre>
<p class="indent">Illegal operations will likely happen early on in the search, so I recommend ignoring runtime errors by adding <code>-W ignore</code>. The command line uses the input file <em>x1_2n.txt</em>, which is the noisy line mentioned previously. Numbers are limited to [–5, 5), and the maximum program length is 22 instructions, though <code>halt</code> usually appears earlier.</p>
<p class="indent">The swarm has 20 particles and runs for 10,000 iterations using bare-bones PSO and the MINSTD randomness source. The result is written to <em>plot.png</em> with the code itself displayed:</p>
<span epub:type="pagebreak" id="page_125"/>
<pre class="pre">Minimum mean total squared error: 0.385596890  (x1_2n.txt)
    push(x)
    push(x)
    add
    push(x)
    add
    push(4.82483)
    push(2.39118)
    div
    sub
    halt
    add
    mod
    div
    sub
    sub
    sub
    div
    mul
    halt
    push(x)
    pow
    add
    div
(23 best updates, 200020 function calls, time: 257.076 seconds)</pre>
<p class="noindent">Note that <code>halt</code> appears as the ninth instruction (the initial <code>push(x)</code> is always present, so it’s not counted).</p>
<p class="indent"><a href="ch04.xhtml#ch04fig04">Figure 4-4</a> shows the original data points along with the fit function output.</p>
<div class="image"><img alt="Image" id="ch04fig04" src="../images/04fig04.jpg"/></div>
<p class="figcap"><em>Figure 4-4: The evolved fit to a noisy line</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_126"/>The fit is good, which is encouraging. If we can’t fit a line, we shouldn’t expect to fit more complex functions.</p>
<p class="indent">We now have two different solutions to fitting the line, but the evolutionary path followed to arrive at a solution was quite different. The first solution evolved</p>
<p class="center">(<em>x</em>)(3.00482) + (7.07870 mod – 9.09650) = 3.00482<em>x</em> – 2.0178</p>
<p class="noindent">but the second produced:</p>
<p class="center"><em>x</em> + <em>x</em> + <em>x</em> – (4.82483 / 2.39118) = 3<em>x</em> – 2.01776</p>
<p class="noindent">The second solution added <em>x</em> to itself three times instead of multiplying by a constant. Both solutions arrived at nearly identical intercepts, not by pushing a learned value but by implementing distinct binary operations with two learned values.</p>
<p class="indent">The <em>data</em> directory contains several datasets, many that are the output of <em>gpgen.py</em>, which you can use to create custom datasets, noisy polynomials up to degree five. Run <em>gpgen.py</em> without arguments to learn how it works. For now, let’s use a few of these datafiles to push <em>gp.py</em> to the limit.</p>
<h5 class="h5"><strong>Fitting a Quadratic</strong></h5>
<p class="noindent">We evolved the equation of a line easily enough. What about a quadratic?</p>
<pre class="pre">&gt; <span class="codestrong1">python3 gp.py data/x2_2n.txt -5 5 22 20 10000 bare minstd plot.png</span></pre>
<p class="noindent">For my run, this produced:</p>
<pre class="pre">Minimum mean total squared error: 0.263703051  (x2_2n.txt)
    push(x)
    push(x)
    mul
    push(-2.97844)
    sub
    push(x)
    sub
    push(x)
    sub
    halt
(98 best updates, 200020 function calls, time: 290.683 seconds)</pre>
<p class="noindent">Instructions after <code>halt</code> are ignored as they have no effect. I’ll do this consistently from now on. The resulting fit is in <a href="ch04.xhtml#ch04fig05">Figure 4-5</a>.</p>
<div class="image"><img alt="Image" id="ch04fig05" src="../images/04fig05.jpg"/></div>
<p class="figcap"><span epub:type="pagebreak" id="page_127"/><em>Figure 4-5: The evolved fit to a noisy quadratic</em></p>
<p class="indent">The evolved code is equivalent to:</p>
<p class="center">(<em>x</em><sup>2</sup> – <sup>–</sup>2.97844) – <em>x</em>) – <em>x</em>) = <em>x</em><sup>2</sup> – 2<em>x</em> + 2.97844</p>
<p class="noindent">The same dataset given to NumPy’s <code>polyfit</code> routine produces</p>
<p class="center"><em>x</em><sup>2</sup> – 2.02<em>x</em> + 2.99</p>
<p class="noindent">giving us growing confidence in the evolutionary search.</p>
<h5 class="h5"><strong>Fitting a Quartic</strong></h5>
<p class="noindent">The previous examples all use bare-bones PSO, which seems well suited to this task. Let’s try a different dataset, a quartic, along with different algorithms. Are all of them equally effective?</p>
<p class="indent">Specifically, we’ll fit the points in <em>x4_-2x3_3x2_-4x_5_50n.txt</em>, a noisy version of the quartic, <em>y</em> = <em>x</em><sup>4</sup> – 2<em>x</em><sup>3</sup> + 3<em>x</em><sup>2</sup> – 4<em>x</em> + 5. The only parameter changing from run to run is the optimization algorithm. For example, here’s the command line for bare-bones PSO:</p>
<pre class="pre">&gt; <span class="codestrong1">python3 gp.py data/x4_-2x3_3x2_-4x_5_50n.txt -25 25 22 25 15000 bare pcg64 plot.png</span></pre>
<p class="noindent">To use differential evolution, change <code>bare</code> to <code>DE</code> and run again. We’ll examine the results for a single run of each algorithm.</p>
<p class="indent"><span epub:type="pagebreak" id="page_128"/>The framework is designed for clarity, not speed. Since each particle evaluates the objective function independently of the others, opportunities for parallelization abound. Unfortunately, we take advantage of none of them, so patience is required to replicate the search for every algorithm: DE, bare-bones PSO, canonical PSO, Jaya, GA, and RO. Also, the framework does not use seed values, so your run of the code will produce different, though likely similar, output.</p>
<p class="indent"><a href="ch04.xhtml#ch04fig06">Figure 4-6</a> displays the fit for each algorithm, from DE on the upper left to RO on the lower right.</p>
<div class="image"><img alt="Image" id="ch04fig06" src="../images/04fig06.jpg"/></div>
<p class="figcap"><em>Figure 4-6: The fits for each algorithm</em></p>
<p class="indent">It’s evident that not every algorithm hit the mark. <a href="ch04.xhtml#ch04tab03">Table 4-3</a> shows us the equivalent equations they generated.</p>
<p class="tabcap" id="ch04tab03"><strong>Table 4-3:</strong> The Fit Equations Evolved by Each Algorithm</p>
<table class="table-h">
<colgroup>
<col style="width:40%"/>
<col style="width:60%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><strong>Algorithm</strong></th>
<th class="tab_th"><strong>Equivalent equation</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1">NumPy</td>
<td class="bg1"><em>y</em> = 1.01<em>x</em><sup>4</sup> – 2.01<em>x</em><sup>3</sup> + 2.76<em>x</em><sup>2</sup> – 3.34<em>x</em> + 5.76</td>
</tr>
<tr>
<td class="bg">Differential evolution</td>
<td class="bg"><em>y</em> = <em>x</em><sup>4</sup> – 2.19254<em>x</em><sup>3</sup> + 3<em>x</em><sup>2</sup></td>
</tr>
<tr>
<td class="bg1">Bare-bones PSO</td>
<td class="bg1"><em>y</em> = <em>x</em><sup>4</sup> – 2.23835<em>x</em><sup>3</sup></td>
</tr>
<tr>
<td class="bg">Canonical PSO</td>
<td class="bg"><em>y</em> = <em>x</em><sup>4.09896</sup></td>
</tr>
<tr>
<td class="bg1">Jaya</td>
<td class="bg1"><em>y</em> = –<em>x</em><sup>3</sup> + 19.36026<em>x</em><sup>2</sup></td>
</tr>
<tr>
<td class="bg">GA</td>
<td class="bg"><em>y</em> = 21.78212<em>x</em><sup>2</sup></td>
</tr>
<tr>
<td class="bg1">RO</td>
<td class="bg1"><img alt="Image" class="inline" src="../images/f0128-01.jpg"/></td>
</tr>
</tbody>
</table>
<p class="indent"><span epub:type="pagebreak" id="page_129"/>The first equation is the fit returned by NumPy’s <code>polyfit</code> routine. The data was generated from a quartic, so we expect the NumPy fit to be the best, and we’ll treat it as the gold standard.</p>
<p class="indent">Differential evolution produced the best-evolved fit function. Compare it to the NumPy fit. The DE fit recovered the first three terms of the polynomial, with coefficients in the same ballpark as the NumPy fit. Similarly, bare-bones PSO recovered the first two terms of the polynomial. Canonical PSO recovered only the first term, <em>x</em><sup>4</sup> (or thereabouts).</p>
<p class="indent">Jaya produced an exciting result. The two terms fight against each other, but their sum becomes a crude approximation of the dataset. As an exercise, try plotting –<em>x</em><sup>3</sup>, 19.36<em>x</em><sup>2</sup>, and their sum to see what I mean.</p>
<p class="indent">Both the GA and RO produced inferior output. The GA ended up with a quadratic, and whatever RO created fits only the left-hand set of dataset points, <em>x</em> &lt; –3 or so.</p>
<p class="indent">These results are from single runs. We know that swarm optimization algorithms are stochastic and vary from run to run. Perhaps we’re being a bit unfair, then. I ran the Jaya search five more times; here are the resulting equivalent equations:</p>
<div class="image1"><img alt="Image" src="../images/f0129-01.jpg"/></div>
<p class="noindent">The results imply that Jaya isn’t converging well to a local minimum. It either captures the essential <em>x</em><sup>4</sup> aspect of the data or stops at a quadratic.</p>
<p class="indent">Jaya isn’t the only algorithm we might be selling short. The GA and RO produced inferior results on our initial runs. What if we increase the swarm size and churn for more iterations? Intuitively, for these algorithms, it might make sense to do this. The more particles searching on their own, the more likely we’ll find a good position in the search space, so a larger swarm seems sensible for RO. For the GA, a larger population increases the size of the gene pool, so we should get better performance as well, much like the genetic drift example from <a href="ch03.xhtml">Chapter 3</a> where the larger population was better able to adapt to the environment after a catastrophe.</p>
<p class="indent">Running a swarm of 125 particles for 150,000 iterations using RO produced a function calculating 21.54962<em>x</em><sup>2</sup>, which is not encouraging as RO isn’t even capturing the quartic nature of the dataset. A run of the GA with 512 particles (organisms) for 30,000 iterations requiring all organisms to breed with a member of the best performing 20 percent (<code>top=0.2</code>) produced <a href="ch04.xhtml#ch04fig07">Figure 4-7</a>, which contains a graph showing the fit.</p>
<div class="image"><img alt="Image" id="ch04fig07" src="../images/04fig07.jpg"/></div>
<p class="figcap"><span epub:type="pagebreak" id="page_130"/><em>Figure 4-7: The genetic algorithm’s solution for 512 organisms and 30,000 generations</em></p>
<p class="indent">The evolved set of instructions are as follows:</p>
<pre class="pre2">push(x)

push(-21.60479)

mul

push(21.92950)

push(-24.20913)

sub

sub

push(-24.96894)

push(-11.43579)

neg

neg

push(-21.53894)

sub

push(x)

mod

mul

push(x)

push(x)

mul

push(23.76431)

mul

add

add</pre>
<p class="indent"><span epub:type="pagebreak" id="page_131"/>The function is quite strange, and used all 22 possible instructions (the initial <code>push(x)</code> is always present), which is also quite different from the results found with other algorithms. The equivalent function is</p>
<p class="center"><em>y</em> = 23.76431<em>x</em><sup>2</sup> – 21.60479<em>x</em> – 46.13863 – 24.96894(10.10315 mod <em>x</em>)</p>
<p class="noindent">which makes more sense in this form: a quadratic with an additional term using modulo, thereby accounting for the strange-looking oscillations on top of the general quadratic form.</p>
<h5 class="h5"><strong>Fitting a Normal Curve</strong></h5>
<p class="noindent">The previous examples attempted to evolve a function to match a polynomial. What happens if we try to fit a noisy normal (Gaussian) curve instead? The source function, before adding random noise, was:</p>
<div class="image1"><img alt="Image" src="../images/f0131-01.jpg"/></div>
<p class="noindent">The noisy data points are in <em>noisy_exp.txt</em>.</p>
<p class="indent">I ran three searches, once each for DE, bare-bones PSO, and Jaya. The searches all used 25 particles and 20,000 iterations. I restricted numbers to [–25, 25], and gave the evolved functions a maximum of 22 instructions.</p>
<p class="indent"><a href="ch04.xhtml#ch04fig08">Figure 4-8</a> shows the resulting fits.</p>
<div class="image"><img alt="Image" id="ch04fig08" src="../images/04fig08.jpg"/></div>
<p class="figcap"><em>Figure 4-8: Evolving a function to fit a noisy normal curve</em></p>
<p class="indent">The DE and bare-bones PSO results are virtually identical and overlap. They fit the dataset well. As we’ve seen with other experiments, Jaya gets close but doesn’t produce as nice a fit as the others.</p>
<p class="indent">So, what were the evolved functions? In this case, not only is the equivalent function illustrative, but so is the form of the code, so we’ll consider both together; see <a href="ch04.xhtml#ch04tab04">Table 4-4</a>.</p>
<p class="tabcap" id="ch04tab04"><span epub:type="pagebreak" id="page_132"/><strong>Table 4-4:</strong> Comparing the Evolved Programs by Algorithm</p>
<table class="table-h">
<colgroup>
<col style="width:25%"/>
<col style="width:40%"/>
<col style="width:35%"/>
</colgroup>
<thead>
<tr>
<th class="tab_th"><strong>DE</strong></th>
<th class="tab_th"><strong>Bare-bones PSO</strong></th>
<th class="tab_th"><strong>Jaya</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="bg1"><code>push(x)</code></td>
<td class="bg1"><code>push(x)</code></td>
<td class="bg1"><code>push(x)</code></td>
</tr>
<tr>
<td class="bg"><code>push(x)</code></td>
<td class="bg"><code>push(x)</code></td>
<td class="bg"><code>neg</code></td>
</tr>
<tr>
<td class="bg1"><code>push(0.35484)</code></td>
<td class="bg1"><code>push(x)</code></td>
<td class="bg1"><code>push(7.95565)</code></td>
</tr>
<tr>
<td class="bg"><code>push(x)</code></td>
<td class="bg"><code>push(2.80857)</code></td>
<td class="bg"><code>push(x)</code></td>
</tr>
<tr>
<td class="bg1"><code>push(x)</code></td>
<td class="bg1"><code>push(x)</code></td>
<td class="bg1"><code>pow</code></td>
</tr>
<tr>
<td class="bg"><code>mul</code></td>
<td class="bg"><code>neg</code></td>
<td class="bg"><code>push(x)</code></td>
</tr>
<tr>
<td class="bg1"><code>pow</code></td>
<td class="bg1"><code>push(x)</code></td>
<td class="bg1"><code>neg</code></td>
</tr>
<tr>
<td class="bg"><code>halt</code></td>
<td class="bg"><code>mul</code></td>
<td class="bg"><code>pow</code></td>
</tr>
<tr>
<td class="bg1"/>
<td class="bg1"><code>pow</code></td>
<td class="bg1"><code>halt</code></td>
</tr>
<tr>
<td class="bg"/>
<td class="bg"><code>halt</code></td>
<td class="bg"/>
</tr>
</tbody>
</table>
<p class="indent">The equivalent function evolved is <em>y</em> = 0.35484<sup><em>x</em><sup>2</sup></sup> (DE), <em>y</em> = 2.80857<sup>–<em>x</em><sup>2</sup></sup> (bare-bones PSO), and <em>y</em> = 7.95565<sup>–<em>x</em><sup>2</sup></sup> (Jaya). Again, the function we’re trying to recover from the noisy data is</p>
<div class="image1"><img alt="Image" src="../images/f0132-01.jpg"/></div>
<p class="noindent">where I’ve approximated <em>e</em> with the first five digits of its decimal expansion. Written this way, it’s clear that the bare-bones PSO search evolved almost exactly this function, so we should expect it to fit the data nicely.</p>
<p class="indent">The DE result seems odd at first. It’s an exponential function, but the base is 0.35484, not <em>e</em>, and the exponent is <em>x</em><sup>2</sup>, not –<em>x</em><sup>2</sup>. However, 1/<em>e</em> ≈ 0.36788, meaning DE evolved the same function as bare-bones PSO since</p>
<div class="image1"><img alt="Image" src="../images/f0132-02.jpg"/></div>
<p class="noindent">and 0.35484 is quite close to 0.36788. Finally, Jaya had the right idea, but didn’t converge to the correct base, 7.95565 &gt; <em>e</em>.</p>
<h3 class="h3" id="ch00lev1_27"><strong>Exercises</strong></h3>
<p class="noindent">Swarm algorithm applications are legion. Here are a few things you may wish to explore in more detail:</p>
<ul>
<li class="noindent">In the <em>curves</em> directory you’ll find a <em>NIST</em> directory. It contains example curve fit datafiles from the National Institute of Standards and Technology (NIST), part of the United States Department of Commerce. I formatted the <em>.txt</em> versions so that they’ll work with <em>curves.py</em>. The original versions end with <em>.dat</em>.
<p class="indent">These are challenging curve fitting test files meant to test high-performance curve fitting routines. Are any of the swarm algorithms up to the challenge? If so, which files can be fit, and which fail?</p></li>
<li class="noindent"><span epub:type="pagebreak" id="page_133"/>The <code>Results</code> method of a swarm object returns a dictionary, as we saw throughout the chapter. We can track the objective function value as a function of swarm iteration by using the <code>gbest</code> and <code>giter</code> dictionary values. The first is a list of each new global best objective function value, and the second is a list marking the iteration at which that value became the global best.
<p class="indent">Examine the code in <em>plot_gbest_giter.py</em> to learn how to plot these values to track the swarm’s learning during a search. Capture the corresponding lists for other searches using the examples in the chapter to make similar plots. Do the swarms all converge at the same rate for the same problem?</p></li>
<li class="noindent">The file <em>gaussian.py</em>, in the <em>micro</em> directory, runs a swarm search to minimize a two-dimensional function consisting of two inverted normal curves, that is, the function is <em>z</em> = <em>f</em>(<em>x</em>, <em>y</em>). The file <em>gaussians.png</em> shows a 3D plot of the function with two minima, one lower than the other. Since the function being minimized has two inputs, the search space is two-dimensional, making it possible to plot the position of every particle in the swarm and track them as they move during the search.
<p class="indent">Run <em>gaussian.py</em> without arguments to learn how to run a search and output images of the swarm at each step. Then, page through the images to watch the swarm search. The known best position is the empty box. The swarm’s current best position is the star. Change the algorithm to observe how each converges and traverses the space. Do the algorithms search the space in the same way? Do they converge on the global minimum, and, if so, at the same rate?</p></li>
<li class="noindent">The file <em>GWO.py</em>, also in the <em>micro</em> directory, implements the Grey Wolf Optimizer (GWO). GWO is a popular swarm intelligence algorithm, in theory, modeling the behavior of wolves as they hunt (I don’t buy it).
<p class="indent">Test GWO using <em>gaussian.py</em>, then adapt <em>curves.py</em> and <em>gp.py</em> to use it as well. A quick copy-paste is all that’s needed. How does GWO’s performance compare to DE, bare-bones PSO, and Jaya? It’s often claimed that GWO has no adjustable parameters, like Jaya. This is not strictly true. The <code>eta</code> parameter, which defaults to 2, can be adjusted, and this sometimes helps the search. If GWO isn’t performing well, adjust <code>eta</code>, perhaps to 3 or 4, and try again.</p></li>
<li class="noindent">Consider the code in <em>MiCRO.py</em>, also in the <em>micro</em> directory. It implements a tongue-in-cheek swarm algorithm loosely based on grazing cattle that I call <em>Minimally Conscious Random Optimization</em>. It’s meant to show how easy it is to create a “novel,” “nature-inspired” swarm algorithm. The idea behind MiCRO is that the swarm is a herd of cattle, mindlessly grazing in complete ignorance of each other. With a set probability on any iteration, an animal might look up and consider another animal’s position that’s better than its own. If this <span epub:type="pagebreak" id="page_134"/>happens, the animal jumps to the region around the better-off neighbor and continues to graze. So, the algorithm is RO with a slight probability of noticing a better performing neighbor; the swarm is <em>minimally conscious</em>.
<p class="indent">Explore how MiCRO performs using <em>gaussian.py</em>, then invent your own swarm algorithm using the code in <em>RO.py</em> and <em>MiCRO.py</em> as guides. Does your algorithm work? Does it work well? Is it really inspired by nature, or is the nature “inspiration” an after-the-fact justification?</p></li>
<li class="noindent">The code evolved by <em>gp.py</em> is restricted to arithmetic operations plus powers and modulo. Add sine, cosine, and tangent as available operators. Each consumes one item from the stack and returns one item to the stack.
<p class="indent">Try to fit the <em>cos.txt</em>, <em>sin.txt</em>, and <em>tan.txt</em> datasets in the <em>data</em> directory. Can the swarm algorithms do it?</p></li>
</ul>
<p class="indent">Recent work has demonstrated that GWO, along with several other popular nature-inspired algorithms, is not novel at all, but is older PSO ideas wrapped in often strained metaphors. That being the case, it’s fair to wonder why I included GWO here. The emphasis in this book is on practicality and ease of application. GWO is popular and works in terms of providing solutions to problems. In that sense, it doesn’t matter whether it’s novel. For the larger optimization field, it’s critically important to understand what is novel and what is not. I suspect, in the end, that many of the myriads of nature-inspired algorithms will prove to be alternate takes on well-known approaches. But, if GWO works, then it works, so we’ll keep it in our small collection of algorithms at the risk of alienating genuine optimization researchers.</p>
<p class="indent">The file <em>nature-inspired_algorithms.pdf</em> lists dozens of nature- and physics-inspired swarm optimization algorithms. The list is by no means exhaustive.</p>
<h3 class="h3" id="ch00lev1_28"><strong>Summary</strong></h3>
<p class="noindent">This chapter introduced us to swarm intelligence and evolutionary algorithms. We used a software framework to develop two applications: one to fit data to a known functional form, that is, traditional curve fitting, and one to evolve code from scratch to implement a best fit function. We learned how to use the framework and explored each of the swarm algorithms to develop our intuition about how they work and are best applied. Each swarm algorithm critically depends on randomness, from the initial configuration of the particles to each update step that moves the particles throughout the search space.</p>
<p class="indent">The experiments of this chapter fit data to functions by either locating the best parameters for a known functional form or evolving the function from scratch. Both attempts were successful, though not every algorithm performed equally well.</p>
<p class="indent"><span epub:type="pagebreak" id="page_135"/>DE proved an excellent match to these tasks, further justifying it as a goto algorithm. However, I was surprised to see bare-bones PSO do so well. Canonical PSO wasn’t as effective, but it has more parameters to tweak, so it might be made better with some experimentation (you can adjust <em>c</em><sub>1</sub>, <em>c</em><sub>2</sub>, <em>ω</em>, the inertia parameter change over iterations, and so on).</p>
<p class="indent">Jaya wasn’t a complete disappointment, but it performed similarly to other places where I’ve used it—not particularly good or bad. It was, at times, able to recover the essence of the functional form, but not the particulars, even allowing for many iterations. As a final example, I ran a search to fit the noisy normal function one more time using Jaya and 120,000 iterations, six times as many as before. The result was a worse fit than the first run with an equivalent function of</p>
<div class="image1"><img alt="Image" src="../images/f0135-01.jpg"/></div>
<p class="noindent">which looks vaguely similar to a normal curve but does not fit the data well.</p>
<p class="indent">RO is not, strictly speaking, a swarm algorithm because the particles don’t influence each other. Still, the examples here, combined with experience in other areas, make RO worth trying in many cases. We’ll encounter RO again in <a href="ch05.xhtml">Chapter 5</a>.</p>
<p class="indent">Curve fitting is not the GA’s cup of tea. We learned in <a href="ch03.xhtml">Chapter 3</a> that the GA is effective when simulating natural selection and genetic drift. Also, we barely explored some of its parameters, such as the fraction of top breeding organisms (<code>top</code>) or the particular mutation and crossover probabilities (<em>F</em> and <em>CR</em>, respectively).</p>
<p class="indent">We’re not through with swarm algorithms. Let’s leave curves and data-sets behind to apply swarm techniques to other areas, like image processing and in combination with a simulation.<span epub:type="pagebreak" id="page_136"/></p>
</body></html>