<html><head></head><body>
<h2 class="h2" id="ch05"><span epub:type="pagebreak" id="page_117"/><strong><span class="big">5</span><br/>COMPUTER ARCHITECTURE</strong></h2>
<div class="image1"><img src="../images/common.jpg" alt="Image"/></div>
<p class="noindent"><a href="ch04.xhtml#ch04">Chapter 4</a> walked through the design of a simple computer system and discussed how the CPU communicates with memory and I/O devices over the address and data buses. That’s not the end of the story, however. Many improvements over the years have made computers run faster while requiring less power and being easier to program. These improvements have added a lot of complexity to the designs.</p>
<p class="indent"><em>Computer architecture</em> refers to the arrangement of the various components into a computer—not to whether the box has Doric or Ionic columns or a custom shade of beige like the one American entrepreneur Steve Jobs (1955–2011) created for the original Macintosh computer. Many different <span epub:type="pagebreak" id="page_118"/>architectures have been tried over the years. What’s worked and what hasn’t makes for fascinating reading, and many books have been published on the subject.</p>
<p class="indent">This chapter focuses primarily on architectural improvements involving memory. A photomicrograph of a modern microprocessor shows that the vast majority of the chip area is dedicated to memory handling. It’s so important that it deserves a chapter of its own. We’ll also touch on a few other differences in architectures, such as instruction set design, additional registers, power control, and fancier execution units. And we’ll discuss support for <em>multitasking</em>, the ability to run multiple programs simultaneously, or at least to provide the illusion of doing so. Running multiple programs implies the existence of some sort of supervisory program called an <em>operating system (OS)</em> that controls their execution.</p>
<h3 class="h3" id="ch05lev1sec1"><strong>Basic Architectural Elements</strong></h3>
<p class="noindent">The two most common architectures are the <em>von Neumann</em> (named after Hungarian-American wizard John von Neumann, 1903–1957) and the <em>Harvard</em> (named after the Harvard Mark I computer, which was, of course, a Harvard architecture machine). We’ve already seen the parts; <a href="ch05.xhtml#ch05fig01">Figure 5-1</a> shows how they’re organized.</p>
<div class="image"><a id="ch05fig01"/><img src="../images/05fig01.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-1: The von Neumann and Harvard architectures</em></p>
<p class="indent">Notice that the only difference between them is the way the memory is arranged. All else being equal, the von Neumann architecture is slightly slower because it can’t access instructions and data at the same time, since there’s only one memory bus. The Harvard architecture gets around that but requires additional hardware for the second memory bus.</p>
<h4 class="h4" id="ch05lev2sec1"><strong><em>Processor Cores</em></strong></h4>
<p class="noindent">Both architectures in <a href="ch05.xhtml#ch05fig01">Figure 5-1</a> have a single CPU, which, as we saw in <a href="ch04.xhtml#ch04">Chapter 4</a>, is the combination of the ALU, registers, and execution unit. <em>Multiprocessor</em> systems with multiple CPUs debuted in the 1980s as a way to get higher performance than could be achieved with a single CPU. As it <span epub:type="pagebreak" id="page_119"/>turns out, though, it’s not that easy. Dividing up a single program so that it can be <em>parallelized</em> to make use of multiple CPUs is an unsolved problem in the general case, although it works well for some things such as particular types of heavy math. However, it’s useful when you’re running more than one program at the same time and was a lifesaver in the early days of graphical workstations, as the X Window System was such a resource hog that it helped to have a separate processor to run it.</p>
<p class="indent">Decreasing fabrication geometries lowers costs. That’s because chips are made on silicon wafers, and making things smaller means more chips fit on one wafer. Higher performance used to be achieved by making the CPU faster, which meant increasing the clock speed. But faster machines required more power, which, combined with smaller geometries, produced more heat-generating power per unit of area. Processors hit the <em>power wall</em> around 2000 because the power density couldn’t be increased without exceeding the melting point.</p>
<p class="indent">Salvation of sorts was found in the smaller fabrication geometries. The definition of CPU has changed; what we used to call a CPU is now called a <em>processor core</em>. <em>Multicore</em> processors are now commonplace. There are even systems, found primarily in data centers, with multiple multicore processors.</p>
<h4 class="h4" id="ch05lev2sec2"><strong><em>Microprocessors and Microcomputers</em></strong></h4>
<p class="noindent">Another orthogonal architectural distinction is based on mechanical packaging. <a href="ch05.xhtml#ch05fig01">Figure 5-1</a> shows CPUs connected to memory and I/O. When the memory and I/O are not in the same physical package as the processor cores, we call it a <em>microprocessor</em>, whereas when everything is on a single chip, we use the term <em>microcomputer</em>. These are not really well-defined terms, and there is a lot of fuzziness around their usage. Some consider a microcomputer to be a computer system built around a microprocessor and use the term <em>microcontroller</em> to refer to what I’ve just defined as a microcomputer.</p>
<p class="indent">Microcomputers tend to be less powerful machines than microprocessors because things like on-chip memory take a lot of space. We’re not going to focus on microcomputers much in this chapter because they don’t have the same complexity of memory issues. However, once you learn to program, it is worthwhile to pick up something like an Arduino, which is a small Harvard architecture computer based on an Atmel AVR microcomputer chip. Arduinos are great for building all sorts of toys and blinky stuff.</p>
<p class="indent">To summarize: microprocessors are usually components of larger systems, while microcomputers are what you find in things like your dishwasher.</p>
<p class="indent">There’s another variation called a <em>system on a chip (SoC)</em>. A passable but again fuzzy definition is that a SoC is a more complex microcomputer. Rather than having relatively simple on-chip I/O, a SoC can include things like Wi-Fi circuitry. SoCs are found in devices such as cell phones. There are even SoCs that include field-programmable gate arrays (FPGAs), which permit additional customization.</p>
<h3 class="h3" id="ch05lev1sec2"><span epub:type="pagebreak" id="page_120"/><strong>Procedures, Subroutines, and Functions</strong></h3>
<p class="noindent">Many engineers are afflicted with a peculiar variation of laziness. If there’s something they don’t want to do, they’ll put their energy into creating something that does it for them, even if that involves more work than the original task. One thing programmers want to avoid is writing the same piece of code more than once. There are good reasons for that besides laziness. Among them is that it makes the code take less space and, if there is a bug in the code, it only has to be fixed in one place.</p>
<p class="indent">The <em>function</em> (or <em>procedure</em> or <em>subroutine</em>) is a mainstay of code reuse. Those terms all mean the same thing as far as you’re concerned; they’re just regional differences in language. We’ll use <em>function</em> because it’s the most similar to what you may have learned in math class.</p>
<p class="indent">Most programming languages have similar constructs. For example, in JavaScript we could write the code shown in <a href="ch05.xhtml#ch05list01">Listing 5-1</a>.</p>
<pre>function<br/>
cube(x)<br/>
{<br/>
        return (x * x * x);<br/>
}</pre>
<p class="listing" id="ch05list01"><em>Listing 5-1: A sample JavaScript function</em></p>
<p class="indent">This code creates a function named <code>cube</code> that takes a single parameter named <code>x</code> and returns its cube. Keyboards don’t include the multiplication (×) symbol, so many programming languages use * for multiplication instead. Now we can write a program fragment like <a href="ch05.xhtml#ch05list02">Listing 5-2</a>.</p>
<pre>y = cube(3);</pre>
<p class="listing" id="ch05list02"><em>Listing 5-2: A sample JavaScript function call</em></p>
<p class="indent">The nice thing here is that we can invoke, or <em>call</em>, the <code>cube</code> function multiple times without having to write it again. We can find <code>cube(4) + cube(6)</code> without having to write the cubing code twice. This is a trivial example, but think about how convenient this capability would be for more complicated chunks of code.</p>
<p class="indent">How does this work? We need a way to run the function code and then get back to where we were. To get back, we need to know where we came from, which is the contents of the program counter (which you saw back in <a href="ch04.xhtml#ch04fig12">Figure 4-12</a> on <a href="ch04.xhtml#page_101">page 101</a>). <a href="ch05.xhtml#ch05tab01">Table 5-1</a> shows how to make a function call using the example instruction set we looked at in “<a href="ch04.xhtml#ch04lev1sec4">Instruction Set</a>” on <a href="ch04.xhtml#page_102">page 102</a>.</p>
<span epub:type="pagebreak" id="page_121"/>
<p class="tabcap" id="ch05tab01"><strong>Table 5-1:</strong> Making a Function Call</p>
<table class="topbot-d">
<colgroup>
<col style="width:10%"/>
<col style="width:20%"/>
<col style="width:20%"/>
<col style="width:50%"/>
</colgroup>
<thead>
<tr>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"><strong>Address</strong></p></td>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"><strong>Instruction</strong></p></td>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"><strong>Operand</strong></p></td>
<td style="vertical-align: top;" class="table-h"><p class="tab_th"><strong>Comments</strong></p></td>
</tr>
</thead>
<tbody>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">100</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">pca</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba"> </p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">Program counter → accumulator</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba">101</p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">add</p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">5 (immediate)</p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">Address for return (100 + 5 = 105)</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">102</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">store</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">200 (direct)</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">Store return address in memory</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba">103</p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">load</p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">3 (immediate)</p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">Put number to cube (3) in accumulator</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">104</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">bra</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">300 (direct)</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">Call the <code>cube</code> function</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba">105</p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba"> </p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba"> </p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">Continues here after function</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">...</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba"> </p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba"> </p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba"> </p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba">200</p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba"> </p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba"> </p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">Reserved memory location</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">...</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba"> </p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba"> </p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba"> </p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba">300</p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">...</p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">...</p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">The <code>cube</code> function</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-v"><p class="taba">...</p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba"> </p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba"> </p></td>
<td style="vertical-align: top;" class="table-v"><p class="taba">Remainder of <code>cube</code> function</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-ba"><p class="taba">310</p></td>
<td style="vertical-align: top;" class="table-ba"><p class="taba">bra</p></td>
<td style="vertical-align: top;" class="table-ba"><p class="taba">200 (indirect)</p></td>
<td style="vertical-align: top;" class="table-ba"><p class="taba">Branch to stored return address</p></td>
</tr>
</tbody>
</table>
<p class="indent">What’s happening here? We first calculate the address of where we want execution to continue after returning from the <code>cube</code> function. It takes us a few instructions to do that; plus, we need to load the number that must be cubed. That’s five instructions later, so we store that address in memory location 200. We branch off to the function, and when the function is done, we branch indirect through 200, so we end up at location 105. This process plays out as shown in <a href="ch05.xhtml#ch05fig02">Figure 5-2</a>.</p>
<div class="image"><a id="ch05fig02"/><img src="../images/05fig02.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-2: A function call flow</em></p>
<p class="indent">This is a lot of work for something that is done a lot, so many machines add helper instructions. For example, ARM processors have a <em>Branch with Link (BL)</em> instruction that combines the branch to the function with saving the address of the following instruction.</p>
<h3 class="h3" id="ch05lev1sec3"><span epub:type="pagebreak" id="page_122"/><strong>Stacks</strong></h3>
<p class="noindent">Functions aren’t limited to simple pieces of code such as the example we just saw. It’s common for functions to call other functions and for them to call themselves.</p>
<p class="indent">Wait, what was that? A function calling itself? That’s called <em>recursion</em>, and it’s really useful. Let’s look at an example. Your phone probably uses <em>JPEG (Joint Photographic Experts Group) compression</em> to reduce the file size of photos. To see how compression works, let’s start with a square black-and-white image, shown in <a href="ch05.xhtml#ch05fig03">Figure 5-3</a>.</p>
<div class="image"><a id="ch05fig03"/><img src="../images/05fig03.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-3: A crude smiley face</em></p>
<p class="indent">We can attack the compression problem using <em>recursive subdivision</em>: we look at the image, and if it’s not all one color, we divide it into four pieces, then check again, and so on until the pieces are one pixel in size.</p>
<p class="indent"><a href="ch05.xhtml#ch05list03">Listing 5-3</a> shows a <code>subdivide</code> function that processes a portion of the image. It’s written in <em>pseudocode</em>, an English-like programming language made up for examples. It <em>takes</em> the x- and y-coordinates of the lower-left corner of a square along with the <em>size</em> (we don’t need both the width and the height, since the image is a square). “Takes” is just shorthand for what’s called the <em>arguments to a function</em> in math.</p>
<pre>function<br/>
subdivide(x, y, size)<br/>
{<br/>
    IF (size ≠ 1 AND the pixels in the square are not all the same color) {<br/>
        half = size ÷ 2<br/>
        subdivide(x, y, half) lower          left quadrant<br/>
        subdivide(x, y + half, half)         upper left quadrant<br/>
        subdivide(x + half, y + half, half)  upper right quadrant<br/>
        subdivide(x + half, y, half)         lower right quadrant<br/>
    }<br/>
    ELSE {<br/>
        save the information about the square<br/>
    }<br/>
}</pre>
<p class="listing" id="ch05list03"><em>Listing 5-3: A subdivision function</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_123"/>The <code>subdivide</code> function partitions the image into same-colored chunks starting with the lower-left quadrant, then the upper left, upper right, and finally the lower right. <a href="ch05.xhtml#ch05fig04">Figure 5-4</a> shows things that need subdividing in gray and things that are solidly one color in black or white.</p>
<div class="image"><a id="ch05fig04"/><img src="../images/05fig04.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-4: Subdividing the image</em></p>
<p class="indent">What we have here looks like what computer geeks call a <em>tree</em> and what math geeks call a <em>directed acyclic graph (DAG)</em>. You follow the arrows. In this structure, arrows don’t go up, so there can’t be loops. Things with no arrows going out of them are called <em>leaf nodes</em>, and they’re the end of the line, like leaves are the end of the line on a tree branch. If you squint enough and count them in <a href="ch05.xhtml#ch05fig04">Figure 5-4</a>, you can see that there are 40 solid squares, which is fewer than the 64 squares in the original image, meaning there’s less information to store. That’s compression.</p>
<p class="indent">For some reason, probably because it’s easier to draw (or maybe because they rarely go outside), computer geeks always put the root of the tree at the top and grow it downward. This particular variant is called a <em>quadtree</em> because each node is divided into four parts. Quadtrees are <em>spatial data structures</em>. Hanan Samet has made these his life’s work and has written several excellent books on the subject.</p>
<p class="indent">There’s a problem with implementing functions as shown in the previous section. Because there’s only one place to store the return value, functions like this can’t call themselves because that value would get overwritten and we’d lose our way back.</p>
<p class="indent">We need to be able to store multiple return addresses in order to make recursion work. We also need a way to associate the return addresses with their corresponding function calls. Let’s see if we can find a pattern in how we subdivided the image. We went down the tree whenever possible and only went across when we ran out of downward options. This is called a <em>depth-first traversal</em>, as opposed to going across first and then down, which is a <em>breadth-first traversal</em>. Every time we go down a level, we need to remember our place so that we can go back. Once we go back, we no longer need to remember that place.</p>
<p class="indent">What we need is something like those gadgets that hold piles of plates in a cafeteria. When we call a function, we stick the return address on a plate and put it on top of the pile. When we return from the call, we remove that plate. In other words, it’s a <em>stack</em>. You can sound important by calling it <span epub:type="pagebreak" id="page_124"/>a <em>LIFO</em> (“last in, first out”) structure. We <em>push</em> things onto the stack, and <em>pop</em> them off. When we try to push things onto a stack that doesn’t have room, that’s called a <em>stack overflow</em>. Trying to pop things from an empty stack is a <em>stack underflow</em>.</p>
<p class="indent">We can do this in software. In our earlier function call example in <a href="ch05.xhtml#ch05tab01">Table 5-1</a>, every function could take its stored return address and push it onto a stack for later retrieval. Fortunately, most computers have hardware support for stacks because they’re so important. This support includes <em>limit registers</em> so that the software doesn’t have to constantly check for possible overflow. We’ll talk about how processors handle <em>exceptions</em>, such as exceeding limits, in the next section.</p>
<p class="indent">Stacks aren’t just used for return addresses. Our <code>subdivide</code> function included a <em>local variable</em> where we calculated half the size once and then used it eight times to make the program faster. We can’t just overwrite this every time we call the function. Instead, we store local variables on the stack too. That makes every function call independent of other function calls. The collection of things stored on the stack for each call is a <em>stack frame</em>. <a href="ch05.xhtml#ch05fig05">Figure 5-5</a> illustrates an example from our function in <a href="ch05.xhtml#ch05list03">Listing 5-3</a>.</p>
<div class="image"><a id="ch05fig05"/><img src="../images/05fig05.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-5: Stack frames</em></p>
<p class="indent">We follow the path shown by the heavy black squares. You can see that each call generates a new stack frame that includes both the return address and the local variable.</p>
<p class="indent">Several computer languages, such as <em>forth</em> and <em>PostScript</em>, are stack-based (see “<a href="ch05.xhtml#ch05sb01">Different Equation Notations</a>”), as are several classic HP calculators. <span epub:type="pagebreak" id="page_125"/>Stacks aren’t restricted to just computer languages, either. Japanese is stack-based: nouns get pushed onto the stack and verbs operate on them. Yoda’s cryptic utterances also follow this pattern.</p>
<div class="sidebar">
<p class="sidebart" id="ch05sb01">DIFFERENT EQUATION NOTATIONS</p>
<p class="spara">There are many different ways in which operators and operands can be arranged. You’re probably used to doing math using what’s called <em>infix notation</em>. Infix puts operators between operands, such as 4 + 8. Infix notation needs parentheses for grouping—for example, (1 + 2) × (3 + 4).</p>
<p class="sparai">Polish logician Jan Łuskasiewicz invented <em>prefix notation</em> in 1924. It’s also known as <em>Polish notation</em> because of his nationality. Polish notation puts the operator before the operands—for example, + 4 8. The advantage of Polish notion is that parentheses are not required. The preceding infix example would be written as × + 1 2 + 3 4.</p>
<p class="sparai">American mathematician Arthur Burks proposed <em>reverse Polish notation (RPN)</em>, also called <em>postfix notation</em>, in 1954. RPN puts the operator after the operands, as in 4 8 +, so the previous example would look like 1 2 + 3 4 + ×.</p>
<p class="sparai">RPN is easy to implement using stacks. Operands are pushed onto a stack. Operators pop operands off the stack, perform their operation, and then push the result back onto the stack.</p>
<p class="sparai">HP RPN calculators have an <small>ENTER</small> key that pushes an operand onto the stack in ambiguous situations; without it, there would be no way to know that 1 and 2 were separate operands instead of the number 12. Using such a calculator, we would solve the equation using the key sequence 1 <code>ENTER</code> 2 + 3 <code>ENTER</code> 4 + ×. An infix notation calculator would require more keystrokes.</p>
<p class="sparai">The example equation would look like <code>1 2 add 3 4 add mul</code> in the PostScript language. No special <small>ENTER</small> is required because the whitespace does the trick.</p>
</div>
<h3 class="h3" id="ch05lev1sec4"><strong>Interrupts</strong></h3>
<p class="noindent">Imagine that you’re in the kitchen whipping up a batch of chocolate chip cookies. You’re following a recipe, which is just a program for cooks. You’re the only one home, so you need to know if someone comes to the door. We’ll represent your activity using a <em>flowchart</em>, which is a type of diagram used to express how things work, as shown in <a href="ch05.xhtml#ch05fig06">Figure 5-6</a>.</p>
<span epub:type="pagebreak" id="page_126"/>
<div class="image"><a id="ch05fig06"/><img src="../images/05fig06.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-6: Home alone making cookies #1</em></p>
<p class="indent">This might work if someone really patient comes to the door. But let’s say that a package is being delivered that needs your signature. The delivery person isn’t going to wait 45 minutes, unless they can smell the cookies and are hoping to get some. Let’s try something different, like <a href="ch05.xhtml#ch05fig07">Figure 5-7</a>.</p>
<div class="image"><a id="ch05fig07"/><img src="../images/05fig07.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-7: Home alone making cookies #2</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_127"/>This technique is called <em>polling</em>. It works, but not very well. You’re less likely to miss your delivery, but you’re spending a lot of time checking the door.</p>
<p class="indent">We could divide up each of the cookie-making tasks into smaller subtasks and check the door in between them. That would improve your chances of receiving the delivery, but at some point you’d be spending more time checking the door than making cookies.</p>
<p class="indent">This is a common and important problem for which there is really no software solution. It’s not possible to make this work well by rearranging the structure of a program. What’s needed is some way to <em>interrupt</em> a running program so that it can respond to something external that needs attention. It’s time to add some hardware features to the execution unit.</p>
<p class="indent">Pretty much every processor made today includes an <em>interrupt</em> unit. Usually it has pins or electrical connections that generate an interrupt when wiggled appropriately. <em>Pin</em> is a colloquial term for an electrical connection to a chip. Chips used to have parts that looked like pins, but as devices and tools have gotten smaller, many other variants have emerged. Many processor chips, especially microcomputers, have <em>integrated peripherals</em> (on-chip I/O devices) that are connected to the interrupt system internally.</p>
<p class="indent">Here’s how it works. A peripheral needing attention generates an <em>interrupt request</em>. The processor (usually) finishes up with the currently executing instruction. It then puts the currently executing program on hold and veers off to execute a completely different program called an <em>interrupt handler</em>. The interrupt handler does whatever it needs to do, and the main program continues from where it left off. Interrupt handlers are functions.</p>
<p class="indent">The equivalent mechanism for the cookie project is a doorbell. You can happily make cookies until you’re interrupted by the doorbell, although it can be annoying to be interrupted by pollsters. There are a few things to consider. First is your <em>response time</em> to the interrupt. If you spend a long time gabbing with the delivery person, your cookies may burn; you need to make sure that you can service interrupts in a timely manner. Second, you need some way to save your <em>state</em> when responding to an interrupt so that you can go back to whatever you were doing after <em>servicing</em> it. For example, if the interrupted program had something in a register, the interrupt handler must save the contents of that register if it needs to use it and then restore it before returning to the main program.</p>
<p class="indent">The interrupt system uses a stack to save the place in the interrupted program. It is the interrupt handler’s job to save anything that it might need to use. This way, the handler can save the absolute minimum necessary so that it works fast.</p>
<p class="indent">How does the computer know where to find the interrupt handler? Usually, there’s a set of reserved memory addresses for interrupt vectors, one for each supported interrupt. An <em>interrupt vector</em> is just a pointer, the address of a memory location. It’s similar to a vector in math or physics—an arrow that says, “Go there from here.” When an interrupt occurs, the computer looks up that address and transfers control there.</p>
<p class="indent">Many machines include interrupt vectors for exceptions including stack overflow and using an invalid address such as one beyond the bounds of <span epub:type="pagebreak" id="page_128"/>physical memory. Diverting exceptions to an interrupt handler often allows the interrupt handler to fix problems so that the program can continue running.</p>
<p class="indent">Typically, there are all sorts of other special interrupt controls, such as ways to turn specific interrupts on and off. There is often a <em>mask</em> so that you can say things like “hold my interrupts while the oven door is open.” On machines with multiple interrupts, there is often some sort of <em>priority</em> ordering so that the most important things get handled first. That means that the handlers for lower-priority interrupts may themselves be interrupted. Most machines have one or more built-in <em>timers</em> that can be configured to generate interrupts.</p>
<p class="indent">Operating systems, discussed in the next section, often keep access to the <em>physical</em> (hardware) interrupts out of reach from most programs. They substitute some sort of <em>virtual</em> or software interrupt system. For example, the UNIX operating system has a <em>signal</em> mechanism. More recently developed systems call these <em>events</em>.</p>
<h3 class="h3" id="ch05lev1sec5"><strong>Relative Addressing</strong></h3>
<p class="noindent">What would it take to have multiple programs running at the same time? For starters, we’d have to have some sort of supervisor program that knew how to switch between them. We’ll call this program an operating system or operating system <em>kernel</em>. We’ll make a distinction between the OS and the programs it supervises by calling the OS a <em>system</em> program and everything else <em>user</em> programs, or <em>processes</em>. A simple OS might work something like <a href="ch05.xhtml#ch05fig08">Figure 5-8</a>.</p>
<div class="image"><a id="ch05fig08"/><img src="../images/05fig08.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-8: A simple operating system</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_129"/>The OS here is using a timer to tell it when to switch between user programs. This scheduling technique is called <em>time slicing</em> because it gives each program a slice of time in which to run. The user program <em>state</em> or <em>context</em> refers to the contents of the registers and any memory that the program is using, including the stack.</p>
<p class="indent">This works, but it’s pretty slow. It takes time to load a program. It would be much faster if you could load the programs into memory as space allows and keep them there, as shown in <a href="ch05.xhtml#ch05fig09">Figure 5-9</a>.</p>
<div class="image"><a id="ch05fig09"/><img src="../images/05fig09.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-9: Multiple programs in memory</em></p>
<p class="indent">In this example, user programs are loaded into memory one after another. But wait, how can this work? As explained back in “<a href="ch04.xhtml#ch04lev2sec5">Addressing Modes</a>” on <a href="ch04.xhtml#page_104">page 104</a>, our sample computer used <em>absolute addressing</em>, which means that the addresses in the instructions referred to specific memory locations. It’s not going to work to run a program that expects to be at address 1000 at a different address, such as 2000.</p>
<p class="indent">Some computers solve this problem by adding an <em>index register</em> (<a href="ch05.xhtml#ch05fig10">Figure 5-10</a>). This is a register whose contents are added to addresses to form <em>effective addresses</em>. If a user program expects to be run at address 1000, the OS could set the index register to 2000 before running it at address 3000.</p>
<div class="image"><a id="ch05fig10"/><img src="../images/05fig10.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-10: An index register</em></p>
<p class="indent">Another way to fix this is with <em>relative addressing</em>—which is not about sending a birthday card to your auntie. Instead of the addresses in instructions being relative to 0 (the beginning of memory in most machines), they can be relative to the address of their instruction. Go back and review <a href="ch04.xhtml#ch04tab04">Table 4-4</a> on <a href="ch04.xhtml#page_108">page 108</a>. You can see that the second instruction contains the address 100 (110100 in binary). With relative addressing, that would become +99, <span epub:type="pagebreak" id="page_130"/>since the instruction is at address 1 and address 100 is 99 away. Likewise, the last instruction is a branch to address 4, which would become a branch to –8 with relative addressing. This sort of stuff is a nightmare to do in binary, but modern language tools do all the arithmetic for us. Relative addressing allows us to <em>relocate</em> a program anywhere in memory.</p>
<h3 class="h3" id="ch05lev1sec6"><strong>Memory Management Units</strong></h3>
<p class="noindent">Multitasking has evolved from being a luxury to being a basic requirement now that everything is connected to the internet, because communications tasks are constantly running in the <em>background</em>—that is, in addition to what the user is doing. Index registers and relative addressing help, but they’re not enough. What happens if one of these programs contains bugs? For example, what if a bug in user program 2 (<a href="ch05.xhtml#ch05fig09">Figure 5-9</a>) causes it to overwrite something in user program 1—or even worse, in the OS? What if someone deliberately wrote a program to spy on or change other programs running on the system? We’d really like to isolate each program to make those scenarios impossible. To that end, most microprocessors now include <em>memory management unit (MMU)</em> hardware that provides this capability. MMUs are very complicated pieces of hardware.</p>
<p class="indent">Systems with MMUs make a distinction between <em>virtual addresses</em> and <em>physical addresses</em>. The MMU translates the virtual addresses used by programs into physical addresses used by memory, as shown in <a href="ch05.xhtml#ch05fig11">Figure 5-11</a>.</p>
<div class="image"><a id="ch05fig11"/><img src="../images/05fig11.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-11: MMU address translation</em></p>
<p class="indent">How is this different from an index register? Well, there’s not just one. And the MMUs aren’t the full width of the address. What’s happening here is that we’re splitting the virtual address into two parts. The lower part is identical to the physical address. The upper part undergoes <em>translation</em> via a piece of RAM called the <em>page table</em>, an example of which you can see in <a href="ch05.xhtml#ch05fig12">Figure 5-12</a>.</p>
<p class="indent">Memory is partitioned into 256-byte <em>pages</em> in this example. The page table contents control the actual location of each page in physical memory. This allows us to take a program that expects to start at address 1000 and put it at 2000, or anywhere else as long as it’s aligned on a <em>page boundary</em>. And although the virtual address space appears continuous to the program, it does not have to be mapped to contiguous physical memory pages. We could even move a program to a different place in physical memory while it’s running. We can provide one or more cooperating programs with <em>shared memory</em> by mapping portions of their virtual address space to the same physical memory. Note that the page table contents become part of a program’s context.</p>
<span epub:type="pagebreak" id="page_131"/>
<div class="image"><a id="ch05fig12"/><img src="../images/05fig12.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-12: Simple page table for a 16-bit machine</em></p>
<p class="indent">Now, if you’ve been paying attention, you might notice that the page table just looks like a piece of memory. And you’d be correct. And you’d expect me to tell you that it’s not that simple. Right again.</p>
<p class="indent">Our example uses 16-bit addresses. What happens if we have a modern machine with 64-bit addresses? If we split the address in half, we need 4 GiB of page table, and the page size would also be 4 GiB—not very useful since that’s more memory than many systems have. We could make the page size smaller, but that would increase the page table size. We need a solution.</p>
<p class="indent">The MMU in a modern processor has a limited page table size. The complete set of <em>page table entries</em> is kept in main memory, or on disk if memory runs out. The MMU loads a subset of the page table entries into its page table as needed.</p>
<p class="indent">Some MMU designs add further control bits to their page tables—for example, a <em>no-execute bit</em>. When this bit is set on a page, the CPU won’t execute instructions from that page. This prevents programs from executing their own data, which is a security risk. Another common control bit makes pages <em>read only</em>.</p>
<p class="indent">MMUs generate a <em>page fault</em> exception when a program tries to access an address that isn’t mapped to physical memory. This is useful, for example, in the case of stack overflow. Rather than having to abort the running program, the OS can have the MMU map some additional memory to grow the stack space and then resume the execution of the user program.</p>
<p class="indent"><span epub:type="pagebreak" id="page_132"/>MMUs make the distinction between von Neumann and Harvard architectures somewhat moot. Such systems have the single bus of the von Neumann architecture but can provide separate instruction and data memory.</p>
<h3 class="h3" id="ch05lev1sec7"><strong>Virtual Memory</strong></h3>
<p class="noindent">Operating systems manage the allocation of scarce hardware resources among competing programs. For example, we saw an OS manage access to the CPU itself in <a href="ch05.xhtml#ch05fig08">Figure 5-8</a>. Memory is also a managed resource. Operating systems use MMUs to provide <em>virtual memory</em> to user programs.</p>
<p class="indent">We saw earlier that the MMU can map a program’s virtual addresses to physical memory. But virtual memory is more than that. The page fault mechanism allows programs to think that they can have as much memory as they want, even if that exceeds the amount of physical memory. What happens when the requested memory exceeds the amount available? The OS moves the contents of memory pages that aren’t currently needed out to larger but slower mass storage, usually a disk. When a program tries to access memory that has been <em>swapped out</em>, the OS does whatever it needs to in order to make space and then copies the requested page back in. This is known as <em>demand paging</em>. <a href="ch05.xhtml#ch05fig13">Figure 5-13</a> shows a virtual memory system with one page swapped out.</p>
<div class="image"><a id="ch05fig13"/><img src="../images/05fig13.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-13: Virtual memory</em></p>
<p class="indent">System performance takes a big hit when swapping occurs, but it’s still better than not being able to run a program at all because of insufficient memory. Virtual memory systems use a number of tricks to minimize the performance hit. One of these is a <em>least recently used (LRU)</em> algorithm that tracks accesses to pages. The most frequently used pages are kept in physical memory; the least recently used are swapped out.</p>
<h3 class="h3" id="ch05lev1sec8"><span epub:type="pagebreak" id="page_133"/><strong>System and User Space</strong></h3>
<p class="noindent">Multitasking systems give each process the illusion that it’s the only program running on the computer. MMUs help to foster this illusion by giving each process its own address space. But this illusion is difficult to maintain when it comes to I/O devices. For example, the OS uses a timer device to tell it when to switch between programs in <a href="ch05.xhtml#ch05fig08">Figure 5-8</a>. The OS decides to set the timer to generate an interrupt once per second, but if one of the user programs changes it to interrupt once per hour, things won’t work as expected. Likewise, the MMU wouldn’t provide any serious isolation between programs if any user program could modify its configuration.</p>
<p class="indent">Many CPUs include additional hardware that addresses this problem. There is a bit in a register that indicates whether the computer is in <em>system</em> or <em>user</em> mode. Certain instructions, such as those that deal with I/O, are <em>privileged</em> and can be executed only in system mode. Special instructions called <em>traps</em> or <em>system calls</em> allow user mode programs to make requests of system mode programs, which means the operating system.</p>
<p class="indent">This arrangement has several advantages. First, it protects the OS from user programs and user programs from each other. Second, since user programs can’t touch certain things like the MMU, the OS can control resource allocation to programs. System space is where hardware exceptions are handled.</p>
<p class="indent">Any programs that you write for your phone, laptop, or desktop will run in user space. You need to get really good before you touch programs running in system space.</p>
<h3 class="h3" id="ch05lev1sec9"><strong>Memory Hierarchy and Performance</strong></h3>
<p class="noindent">Once upon a time, CPUs and memory worked at the same speed, and there was peace in the land. However, CPUs got faster and faster, and although memory got faster too, it couldn’t keep up. Computer architects have come up with all sorts of tricks to make sure that those fast CPUs aren’t sitting around waiting for memory.</p>
<p class="indent">Virtual memory and swapping introduce the notion of <em>memory hierarchy</em>. Although all memory looks the same to a user program, what happens behind the scenes greatly affects the system performance. Or, to paraphrase George Orwell, all memory accesses are equal, but some memory accesses are more equal than others.</p>
<p class="indent">Computers are fast. They can execute billions of instructions per second. But not much would get done if the CPU had to wait around for those instructions to arrive, or for data to be retrieved or stored.</p>
<p class="indent">We’ve seen that processors include some very fast, expensive memory called registers. Early computers had only a handful of registers, whereas some modern machines contain hundreds. But overall, the ratio of registers <span epub:type="pagebreak" id="page_134"/>to memory has gotten smaller. Processors communicate with <em>main memory</em>, usually DRAM, which is less than a tenth as fast as the processor. Mass storage devices such as disk drives may be a <em>millionth</em> as fast the processor.</p>
<p class="indent">Time for a food analogy courtesy of my friend Clem. Registers are like a refrigerator: there’s not a lot of space in there, but you can get to its contents quickly. Main memory is like a grocery store: it has a lot more space for stuff, but it takes a while to get there. Mass storage is like a warehouse: there’s even more space for stuff, but it’s much farther away.</p>
<p class="indent">Let’s milk this analogy some more. You often hit the fridge for one thing. When you make the trip to the store, you fill a few grocery bags. The warehouse supplies the store by the truckload. Computers are the same way. Small blocks of stuff are moved between the CPU and main memory. Larger blocks of stuff are moved between main memory and the disk. Check out <em>The Paging Game</em> by Jeff Berryman for a humorous explanation of how all this works.</p>
<p class="indent">Skipping a lot of gory details, let’s assume the CPU runs about 10 times the speed of main memory. That translates to a lot of time spent waiting for memory, so additional hardware (faster on-chip memory) was added for a pantry or <em>cache</em>. It’s much smaller than the grocery store, but much faster when running at full processor speed.</p>
<p class="indent">How do we fill the pantry from the grocery store? Way back in “<a href="ch03.xhtml#ch03lev2sec8">Random-Access Memory</a>” on <a href="ch03.xhtml#page_82">page 82</a>, we saw that DRAM performs best when accessing columns out of a row. When you examine the way programs work, you notice that they access sequential memory locations unless they hit a branch. And a fair amount of the data used by a program tends to clump together. This phenomenon is exploited to improve system performance. The CPU <em>memory controller</em> hardware fills the cache from consecutive columns in a row because, more often than not, data is needed from sequential locations. Rather than getting one box of cereal, we put several in sacks and bring them home. By using the highest-speed memory-access mode available, CPUs are usually ahead of the game even when there is a cache miss caused by a nonsequential access. A <em>cache miss</em> is not a contestant in a Miss Cache pageant; it’s when the CPU looks for something in the cache that isn’t there and has to fetch it from memory. Likewise, a <em>cache hit</em> is when the CPU finds what it’s looking for in the cache. You can’t have too much of a good thing.</p>
<p class="indent">There are several levels of cache memory, and they get bigger and slower as they get farther away from the CPU (even when they’re on the same chip). These are called the <em>L1</em>, <em>L2</em>, and <em>L3</em> caches, where the <em>L</em> stands for <em>level</em>. Yup, there’s the spare freezer in the garage plus the storeroom. And there’s a dispatcher that puts air traffic control to shame. There’s a whole army of logic circuitry whose job it is to pack and unpack grocery bags, boxes, and trucks of different sizes to make all this work. It actually takes up a good chunk of the chip real estate. The memory hierarchy is outlined in <a href="ch05.xhtml#ch05fig14">Figure 5-14</a>.</p>
<span epub:type="pagebreak" id="page_135"/>
<div class="image"><a id="ch05fig14"/><img src="../images/05fig14.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-14: Memory hierarchy</em></p>
<p class="indent">Additional complicated tweaks have improved performance even further. Machines include <em>branch prediction</em> circuitry that guesses the outcome of conditional branch instructions so that the correct data can be <em>prefetched</em> from memory and in the cache ready to go. There is even circuitry to handle <em>out-of-order execution</em>. This allows the CPU to execute instructions in the most efficient order even if it’s not the order in which they occur in a program.</p>
<p class="indent">Maintaining <em>cache coherency</em> is a particularly gnarly problem. Imagine a system that contains two processor chips, each with four cores. One of those cores writes data to a memory location—well, really to a cache, where it will eventually get into memory. How does another core or processor know that it’s getting the right version of the data from that memory location? The simplest approach is called <em>write through</em>, which means that writes go directly to memory and are not cached. But that eliminates many of the benefits of caching, so there’s a lot of additional cache-management hardware for this that is outside of the scope of this book.</p>
<h3 class="h3" id="ch05lev1sec10"><strong>Coprocessors</strong></h3>
<p class="noindent">A processor core is a pretty complicated piece of circuitry. You can free up processor cores for general computation by offloading common operations to simpler pieces of hardware called <em>coprocessors</em>. It used to be that coprocessors existed because there wasn’t room to fit everything on a single chip. For example, there were floating-point coprocessors for when there wasn’t space for floating-point instruction hardware on the processor itself. Today there are on-chip coprocessors for many things, including specialized graphics processing.</p>
<p class="indent">In this chapter we’ve talked about loading programs into memory to be run, which usually means that the programs are coming from some slow and cheap memory, such as a disk drive. And we’ve seen that virtual memory systems may be reading from and writing to disks as part of swapping. And we saw in “<a href="ch03.xhtml#ch03lev1sec3">Block Devices</a>” on <a href="ch03.xhtml#page_85">page 85</a> that disks aren’t byte-addressable—they transfer blocks of 512 or 4,096 bytes. This means there’s a lot of copying <span epub:type="pagebreak" id="page_136"/>between main memory and disk that’s straightforward, because no other computation is needed. Copying data from one place to another is one of the biggest consumers of CPU time. Some coprocessors do nothing but move data around. These are called <em>direct memory access (DMA)</em> units. They can be configured to do operations like “move this much stuff from here to there and let me know when you’re done.” CPUs offload a lot of grunt work onto DMA units, leaving the CPU free to do more useful operations.</p>
<h3 class="h3" id="ch05lev1sec11"><strong>Arranging Data in Memory</strong></h3>
<p class="noindent">We learned from the program in <a href="ch04.xhtml#ch04tab04">Table 4-4</a> that memory is used not only for the instructions but for data as well. In this case, it’s <em>static</em> data, meaning that the amount of memory needed is known when the program is written. We saw earlier in this chapter that programs also use memory for stacks. These data areas need to be arranged in memory so that they don’t collide.</p>
<p class="indent"><a href="ch05.xhtml#ch05fig15">Figure 5-15</a> illustrates the typical arrangement for both von Neumann and Harvard architecture machines without MMUs. You can see that the only difference is that instructions reside in separate memory on Harvard architecture machines.</p>
<div class="image"><a id="ch05fig15"/><img src="../images/05fig15.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-15: Memory arrangement</em></p>
<p class="indent">There’s one more way in which programs use memory. Most programs have to deal with <em>dynamic</em> data, which has a size that is unknown until the program is running. For example, an instant messaging system doesn’t know in advance how many messages it needs to store or how much storage will be needed for each message. Dynamic data is customarily piled into memory above the static area, called the <em>heap</em>, as shown in <a href="ch05.xhtml#ch05fig16">Figure 5-16</a>. The heap grows upward as more space is needed for dynamic data, while the stack grows downward. It’s important to make sure they don’t collide. There are a few minor variations on this theme; some processors reserve memory addresses at the beginning or end of memory for interrupt vectors and registers that control on-chip I/O devices.</p>
<span epub:type="pagebreak" id="page_137"/>
<div class="image"><a id="ch05fig16"/><img src="../images/05fig16.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-16: Memory arrangement with the heap</em></p>
<p class="indent">You’ll find this memory layout when using microcomputers, as they typically don’t have MMUs. When MMUs are involved, the instructions, data, and stack are mapped to different physical memory pages whose size can be adjusted as needed. But the same memory layout is used for the virtual memory presented to programs.</p>
<h3 class="h3" id="ch05lev1sec12"><strong>Running Programs</strong></h3>
<p class="noindent">You’ve seen that computer programs have a lot of pieces. In this section, you’ll learn how they all come together.</p>
<p class="indent">Earlier I said that programmers use functions for code reuse. That’s not the end of the story. There are many functions that are useful for more than one program—for example, comparing two text strings. It would be nice if we could just use these third-party functions rather than having to write our own every time. One way to do that is by grouping related functions into <em>libraries</em>. There are a large number of libraries available for everything from string handling to hairy math to MP3 decoding.</p>
<p class="indent">In addition to libraries, nontrivial programs are usually built in pieces. Though you could put the entirety of a program into a single file, there are several good reasons to break it up. Chief among these is that it makes it easier for several people to work on the same program at the same time.</p>
<p class="indent">But breaking programs up means we need some way to hook or <em>link</em> all the different pieces together. The way we accomplish this is by processing each program piece into an intermediate format designed for this purpose and then running a special <em>linker</em> program that makes all the connections. Many intermediate file formats have been developed over the years. <em>Executable and Linkable Format (ELF)</em> is currently the most popular flavor. This format includes sections similar to want ads. There might be something in the For Sale section that says, “I have a function named <code>cube</code>.” Likewise, we might see “I’m looking for a variable named <code>date</code>” in the Wanted section.</p>
<p class="indent">A linker is a program that <em>resolves</em> all the ads, resulting in a program that can actually be run. But of course, there are complications in the name of performance. It used to be that you treated libraries just like one of your <span epub:type="pagebreak" id="page_138"/>files full of functions and linked them in with the rest of your program. This was called <em>static linking</em>. Sometime in the 1980s, however, people noticed that lots of programs were using the same libraries. This was a great testament to the value of those libraries. But they added to the size of every program that used them, and there were many copies of the libraries using up valuable memory. Enter <em>shared libraries</em>. The MMU can be used to allow the same copy of a library to be shared by multiple programs, as illustrated in <a href="ch05.xhtml#ch05fig17">Figure 5-17</a>.</p>
<div class="image"><a id="ch05fig17"/><img src="../images/05fig17.jpg" alt="Image"/></div>
<p class="figcap"><em>Figure 5-17: A shared library</em></p>
<p class="indent">Keep in mind that the instructions from the shared library are common to the programs that use it. The library functions must be designed so that they use the heap and stack of the calling programs.</p>
<p class="indent">Programs have an <em>entry point</em>, which is the address of the first instruction in the program. Though it’s counterintuitive, that instruction is not the first one executed when a program is run. When all the pieces of a program are linked to form an <em>executable</em>, an additional <em>runtime library</em> is included. Code in this library runs before hitting the entry point.</p>
<p class="indent">The runtime library is responsible for setting up memory. That means establishing a stack and a heap. It also sets the initial values for items in the static data area. These values are stored in the executable and must be copied to the static data after acquiring that memory from the system.</p>
<p class="indent">The runtime library performs many more functions, especially for complicated languages. Fortunately, you don’t need to know any more about it right now.</p>
<h3 class="h3" id="ch05lev1sec13"><strong>Memory Power</strong></h3>
<p class="noindent">We’ve approached memory from a performance perspective so far. But there’s another consideration. Moving data around in memory takes <em>power</em>. That’s not a big deal for desktop computers. But it’s a huge issue for mobile devices. And although battery life isn’t an issue in data centers such as those used by large internet companies, using extra power on thousands of machines adds up.</p>
<p class="indent">Balancing power consumption and performance is challenging. Keep both in mind when writing code.</p>
<h3 class="h3" id="ch05lev1sec14"><span epub:type="pagebreak" id="page_139"/><strong>Summary</strong></h3>
<p class="noindent">You’ve learned that working with memory is not as simple as you might have thought after reading <a href="ch04.xhtml#ch04">Chapter 4</a>. You got a feel for how much additional complication gets added to simple processors in order to improve memory usage. You now have a pretty complete idea of what’s in a modern computer with the exception of I/O—which is the topic of <a href="ch06.xhtml#ch06">Chapter 6</a>.<span epub:type="pagebreak" id="page_140"/></p>
</body></html>