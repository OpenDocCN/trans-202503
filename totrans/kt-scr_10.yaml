- en: <hgroup>
  prefs: []
  type: TYPE_NORMAL
- en: 7 SORTING AND SEARCHING
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: </hgroup>
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/icon.jpg)'
  prefs: []
  type: TYPE_IMG
- en: One of the fundamental skills that any serious developer needs to learn is how
    to efficiently sort and search through a given dataset. This skill is invaluable
    for transforming raw data into actionable insights, whether you’re working with
    a simple array or with complex data structures spanning terabytes of multifield
    information extracted from the vast expanse of the internet.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting and searching are a dynamic duo that work hand in hand. *Sorting* organizes
    data into a specific order, which enables meaningful analysis of the dataset as
    a whole. Once the data is sorted, it becomes easier to identify patterns, trends,
    and outliers. Sorting also improves the speed and ease of *searching* for desired
    items within the dataset, especially when working with large volumes of data.
    Indeed, many search algorithms, such as binary search, interpolation search, and
    tree-based searches, rely on the organization achieved through sorting. Searching
    further complements sorting by enabling targeted analysis, allowing for the quick
    location of specific data points or subsets within the dataset. Together, sorting
    and searching streamline data exploration, optimize retrieval efficiency, and
    empower decision-making processes.
  prefs: []
  type: TYPE_NORMAL
- en: A wide array of sorting and search algorithms are available. In this chapter’s
    projects, we’ll focus on a selected group of algorithms that have broad applications
    in fields that require working with large datasets. By mastering these algorithms,
    you’ll be better equipped to tackle complex data challenges and make the most
    of sorting and searching capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sorting algorithms allow us to rearrange a collection of data elements into
    a specific order, such as numerical or alphabetical or based on any other desired
    criteria. We can sort various types of data, including numbers, strings, records
    (lines of data in a database), and complex objects. Sorting is a fundamental building
    block for a variety of operations, such as merging, joining, and aggregating datasets.
    It paves the way for efficient data manipulation, which is crucial in domains
    like database management, algorithms, and programming. By organizing data structures,
    sorting provides a structured framework that promotes clarity, consistency, and
    ease of use. This streamlined approach enhances data management and maintenance,
    particularly in scenarios where data must be updated or modified frequently.
  prefs: []
  type: TYPE_NORMAL
- en: Each sorting algorithm has its own advantages and disadvantages in terms of
    time complexity, space complexity, and stability. Before we get into specific
    algorithms, it’s important to review these concepts, as they’ll assist us in selecting
    the appropriate algorithm for a given problem.
  prefs: []
  type: TYPE_NORMAL
- en: '*Time complexity* refers to the estimation of the algorithm’s running time
    based on the input size, which is denoted by *n*. It provides insight into how
    the algorithm’s performance scales with larger datasets. Common notations like
    *O*(1), *O*(log *n*), *O*(*n*), *O*(*n* log *n*), *O*(*n*²), and *O*(2*^n*) indicate
    different growth rates of time complexity in increasing order. The smaller the
    growth rate, the quicker the algorithm is in sorting a collection of data.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Space complexity*, on the other hand, describes the amount of additional memory
    an algorithm requires to perform the sorting operation, beyond the memory it needs
    to store the data being sorted. Some algorithms may operate with minimal extra
    space, where the swapping of data elements happens *in place*. Others may require
    significant auxiliary memory to perform sorting operations efficiently. This is
    also called *out-of-place* sorting, where full or partial copies of the original
    dataset are needed to carry out the sorting operation. The smaller the space complexity,
    the more efficient (and scalable) that algorithm is in terms of memory requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Stability* is another important consideration. A sorting algorithm is stable
    if it maintains the relative order of elements with equal values. In certain situations,
    preserving the initial order of equal elements is required, and a stable algorithm
    becomes essential.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 7-1](chapter7.xhtml#tab7-1) shows these properties for a selected group
    of sorting algorithms that we’ll discuss in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7-1: Key Features of Selected Sorting Algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Time complexity |  | Space complexity | Stability |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Best | Average | Worst |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Insertion sort | O(n) | O(n2) | O(n2) |  | O(1) | Stable |'
  prefs: []
  type: TYPE_TB
- en: '| Merge sort | O(n log n) | O(n log n) | O(n log n) |  | O(n) | Stable |'
  prefs: []
  type: TYPE_TB
- en: '| Quick sort | O(n log n) | O(n log n) | O(n2) |  | O(log n)* | Unstable |'
  prefs: []
  type: TYPE_TB
- en: '| Heap sort | O(n log n) | O(n log n) | O(n log n) |  | O(1) | Unstable |'
  prefs: []
  type: TYPE_TB
- en: '| *The worst case can be O(n). |'
  prefs: []
  type: TYPE_TB
- en: Of the sorting algorithms listed in [Table 7-1](chapter7.xhtml#tab7-1), insertion
    sort is the simplest and most intuitive, but it isn’t the most efficient in terms
    of average time complexity. It tends to be slower than the other algorithms for
    larger datasets. Due to this limitation, insertion sort generally isn’t used as
    a standalone algorithm, but rather as part of a hybrid sorting scheme that combines
    multiple methods, depending on the characteristics of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Both merge sort and heap sort have similar time complexities, typically *O*(*n*
    log *n*). However, heap sort has an advantage in terms of space complexity because
    it’s an in-place algorithm, meaning it requires minimal additional memory beyond
    the input array. On the other hand, merge sort requires additional space proportional
    to the input size. If stability is a desired property, then merge sort becomes
    the preferred choice over heap sort. It’s a stable sorting algorithm, ensuring
    that elements with equal values maintain their original order.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, quick sort often performs better than other sorting algorithms,
    except when the data is already sorted or nearly sorted. Quick sort benefits from
    lower space complexity, and it has smaller overhead, or fewer hidden operations
    that don’t depend on the size of the input data. Many programming language libraries
    provide built-in functions for quick sort, making it easily accessible and widely
    used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project 27: Space-Efficient Sorting with Insertion Sort'
  prefs: []
  type: TYPE_NORMAL
- en: Insertion sort is a simple and intuitive sorting algorithm that works by building
    a sorted array one element at a time. The algorithm maintains a sorted subarray
    within the given array and extends it by inserting elements from the unsorted
    part of the array into the correct position in the sorted part. At the beginning,
    the first element of the array is considered to be a sorted subarray of size 1\.
    The algorithm then iterates through the remaining elements, one at a time, and
    inserts each element into its appropriate position within the sorted subarray.
  prefs: []
  type: TYPE_NORMAL
- en: To insert an element, the algorithm compares it with the elements in the sorted
    subarray from right to left. It shifts any larger elements one position to the
    right until it finds the correct position for the current element. Once the correct
    position is found, the element is inserted into that position. This process continues
    until all the elements in the array are processed, resulting in a fully sorted
    array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we have the unsorted array [8, 3, 4, 5, 1, 2]. Here’s how the insertion
    sort algorithm would process it:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.  Imagine that the given array is made up of two subarrays—a sorted array,
    which initially holds only the first element (8), and an unsorted array made up
    of the remaining elements.
  prefs: []
  type: TYPE_NORMAL
- en: '2.  Compare the second element of the array (index 1) with its preceding element
    (index 0) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: a.  Compare 3 with 8\. Since 3 is smaller, swap the elements.
  prefs: []
  type: TYPE_NORMAL
- en: b.  The array after the first pass is [3, 8, 4, 5, 1, 2].
  prefs: []
  type: TYPE_NORMAL
- en: 3.  Move to the next element (index 2) and compare it with the previous elements.
  prefs: []
  type: TYPE_NORMAL
- en: a.  Compare 4 with 8\. Since 4 is smaller, swap the elements.
  prefs: []
  type: TYPE_NORMAL
- en: b.  Compare 4 with 3\. Since 4 is greater, stop comparing.
  prefs: []
  type: TYPE_NORMAL
- en: c.  The array after the second pass is [3, 4, 8, 5, 1, 2].
  prefs: []
  type: TYPE_NORMAL
- en: '4.  Repeat this process for the remaining elements of the array. In the end,
    the array will be sorted in ascending order: [1, 2, 3, 4, 5, 8].'
  prefs: []
  type: TYPE_NORMAL
- en: As indicated in [Table 7-1](chapter7.xhtml#tab7-1), insertion sort has an average
    and worst-case time complexity of *O*(*n*²), where *n* is the number of elements
    in the array. However, it performs well for small lists or nearly sorted lists.
    It’s an in-place sorting algorithm with space complexity of *O*(1) for all cases,
    meaning it doesn’t require additional memory to perform the sorting.
  prefs: []
  type: TYPE_NORMAL
- en: The Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Implementing the insertion sort algorithm in Kotlin takes only a few lines of
    code. We’ll create a dedicated function called insertionSort() to handle all the
    necessary steps for sorting an array and call this function from main() to get
    the job done.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This code snippet implements the insertion sort algorithm to sort an array of
    numbers (in this case, integers) in ascending order. We create an array called
    arr that holds the initial unsorted array elements. The content of this array
    is printed to the console, allowing us to see the original order of the numbers.
    We then call the insertionSort() function to perform the sorting operation. It
    takes the array as input and modifies it in place, so you don’t have to return
    the sorted array to the calling function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the insertionSort() function, we iterate through the unsorted portion
    of the array by using a for loop ❶, starting from the second element (index 1)
    and continuing to the last element. For each element, we temporarily store the
    value in a variable called key. Next, we use a while loop ❷ to move from right
    to left through the sorted portion of the array, comparing key with each element.
    The while loop continues as long as two conditions are met: more elements remain
    to the left (checked using j > 0), and the current element is greater than key
    (checked using A[j-1] > key). Inside the while loop, if an element is greater
    than key, it’s shifted one position to the right. This makes space for key to
    be inserted at the correct sorted position.'
  prefs: []
  type: TYPE_NORMAL
- en: When the while loop ends, we assign the value of key to the current position
    in the array ❸, effectively inserting the element into the sorted portion of the
    array at the correct position. The for loop then moves to the next element, and
    the process repeats for all the elements in the array. Once the sorting is complete,
    the code prints the sorted array to the console, displaying the numbers in ascending
    order.
  prefs: []
  type: TYPE_NORMAL
- en: The Result
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you run the code without changing the given unsorted array, the output should
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The code can easily be tweaked to sort floating-point numbers by assembling
    an array of either Float or Double data types. I encourage you to modify the code
    to accept user input regarding the preferred sorting order—either ascending or
    descending. After that, you can implement a suitable function based on the user’s
    choice. Alternatively, you can use the same function with two subfunctions, which
    can be implemented using when(choice), one for sorting an array in ascending order
    and one for doing the opposite.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project 28: Faster Sorting with Merge Sort'
  prefs: []
  type: TYPE_NORMAL
- en: '*Merge sort* is a popular sorting algorithm that follows a divide-and-conquer
    approach. It works by recursively dividing an array into smaller subarrays until
    each subarray contains only one element. The subarrays are then merged back into
    longer arrays, placing the elements in the correct order in the process, eventually
    resulting in a fully sorted array. [Figure 7-1](chapter7.xhtml#fig7-1) illustrates
    this process for the same [8, 3, 4, 5, 1, 2] array we used in [Project 27](chapter7.xhtml#pre-27).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/Figure7-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-1: Visualizing the merge sort algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the given array is initially divided into two subarrays, and then
    notice how each of those subarrays is further divided into two subarrays, and
    so on. The subarrays are then sorted and merged. When we implement the algorithm
    by using a recursive function, we’ll first process entirely the left subarray
    of the first pair of subarrays—in this example, [8, 3, 4]—before moving on to
    the right subarray, [5, 1, 2]. Within each branch, we’ll divide the subarrays
    into individual elements and then reassemble the sorted subarrays. Eventually,
    the final pair of sorted left and right subarrays will be merged to generate the
    final sorted array.
  prefs: []
  type: TYPE_NORMAL
- en: Merge sort guarantees a consistent time complexity of *O*(*n* log *n*) in all
    cases, making it efficient for large datasets. It’s also a stable sorting algorithm,
    preserving the relative order of equal elements. However, it needs additional
    space for the merging step, making its space complexity *O*(*n*). Nonetheless,
    merge sort’s efficiency and stability make it a popular choice for sorting in
    various applications.
  prefs: []
  type: TYPE_NORMAL
- en: The Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We’ll follow a similar structure for the merge sort code as we did for the
    insertion sort: a main() function that kicks off the sorting process by passing
    an array to a mergeSort() function. This time, however, mergeSort() will recursively
    call itself until it reaches a stopping condition (when each subarray has only
    one element). To put everything back together, we’ll use a helper function called
    merge(), which handles the task of sorting and merging the subarrays.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the main() function, we begin by initializing an array called arr with a
    set of integer values. We also print the given array before proceeding so that
    we’ll be able to compare this with the sorted array once it’s generated. We then
    call the mergeSort() function ❶, which is responsible for carrying out the sorting
    process. This function takes an array arr as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within mergeSort(), we first check the length of the incoming array. If it’s
    less than 2, the subarray has a length of 1, so the function simply returns, and
    the splitting process stops. This is the all-important stopping condition that
    any recursive function needs. Next, we calculate the middle index of the array
    ❷ and create two subarrays: leftArray and rightArray. The former contains elements
    from index 0 up to but not including middle, while the latter contains elements
    from middle to the end of the array. To continue the process, the mergeSort()
    function recursively calls itself on both leftArray and rightArray ❸. As mentioned,
    this recursive step continues until the base case is reached—that is, when the
    length of the subarrays becomes 1. Finally, we call merge() to reassemble leftArray
    and rightArray into a single, sorted array.'
  prefs: []
  type: TYPE_NORMAL
- en: The merge() function accepts three parameters, leftArray, rightArray, and arr,
    representing the two subarrays to be merged and the original array that will be
    modified during the merging process. We start the function by initializing variables
    to keep track of the indices within the arrays; i is for traversing the original
    arr, l for the leftArray, and r for the rightArray. The actual merging and sorting
    occur within a while loop ❹ that continues as long as elements remain in both
    leftArray and rightArray to compare. During each iteration, the function compares
    the values at indices l and r in leftArray and rightArray, respectively. If the
    value in leftArray is smaller, it’s assigned to arr at index i, and the l index
    is incremented. Conversely, if the value in rightArray is smaller, it’s assigned
    to arr at index i, and the r index is incremented. Following each assignment,
    the i index is also incremented ❺.
  prefs: []
  type: TYPE_NORMAL
- en: The while loop concludes when either leftArray or rightArray has been fully
    processed. The remaining elements from the nonempty array are then assigned to
    arr to complete the merging process. We use two separate while loops for this
    task—one for leftArray and one for rightArray. Only one of these loops will actually
    execute.
  prefs: []
  type: TYPE_NORMAL
- en: The Result
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When you run the merge sort code for the given input array, it should produce
    the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: I encourage you to repeat the same exercise you did with insertion sort, allowing
    the user to choose the order of sorting (ascending or descending) and then modifying
    the code to sort accordingly. I also recommend that you think about arrays containing
    both positive and negative numbers. You might soon realize that by selectively
    multiplying the entire array by –1 before and after sorting, you can use the same
    code to sort an array in ascending or descending order instead of writing two
    separate functions!
  prefs: []
  type: TYPE_NORMAL
- en: 'Project 29: High-Efficiency Sorting with Quick Sort'
  prefs: []
  type: TYPE_NORMAL
- en: '*Quick sort* is a well-known and highly efficient in-place sorting algorithm
    that’s widely used in various real-world applications. It involves selecting a
    pivot element from the array and dividing the remaining elements into two subarrays,
    one for values less than the pivot and the other for values greater than the pivot.
    This mechanism places the pivot element itself in the correct position in the
    final sorted array, while the remaining elements end up on the appropriate side
    of the pivot. The process repeats recursively for the subarrays, selecting new
    pivot elements and further portioning the array, until everything is sorted.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a step-by-step example of how quick sort works, using the array [8,
    3, 4, 5, 1, 2]:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.  Choose a pivot element, which can be any element from the array. In this
    example, we’ll choose the last element, 2.
  prefs: []
  type: TYPE_NORMAL
- en: 2.  Partition the array into two subarrays, the left subarray with elements
    less than the pivot and the right subarray with elements greater than the pivot.
    In this case, the left subarray becomes [1], and the right becomes [4, 5, 8, 3].
    I’ll explain where this order comes from later in the project.
  prefs: []
  type: TYPE_NORMAL
- en: '3.  Recursively apply quick sort to the subarrays. For the left subarray, no
    further action is needed: it has only one element, so it’s already in its final
    position. For the right subarray, we now pick 3 as the pivot. This creates an
    empty left subarray as 3 is the smallest number. The right subarray now has [5,
    8, 4].'
  prefs: []
  type: TYPE_NORMAL
- en: 4.  Repeat step 3 until all subarrays are sorted, meaning each subarray has
    only one element or is empty.
  prefs: []
  type: TYPE_NORMAL
- en: '5.  Combine the sorted subarrays to get the final sorted array: [1, 2, 3, 4,
    5, 8].'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we always chose the last element of the array or subarray
    as the pivot element. Another option could have been to choose the first element
    as the pivot. For a wide range of inputs, choosing the first or last element as
    the pivot will work well, especially if the input data is randomly or uniformly
    distributed. However, if the array is already sorted or nearly sorted, pivoting
    around the first or last element will yield the worst-case time complexity of
    *O*(*n*²). To avoid this, you can use one of the following alternative techniques
    for choosing a pivot:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Choose a random element**'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly selecting a pivot element helps mitigate the inefficiency of choosing
    the first or the last element when the array is already mostly sorted. This approach
    can provide a good average-case performance since the pivot’s position is less
    predictable. It reduces the likelihood of encountering worst-case scenarios, resulting
    in better overall efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '**Choose the median of three**'
  prefs: []
  type: TYPE_NORMAL
- en: This strategy involves using the median value among the first, middle, and last
    elements of the array as the pivot. This approach aims to balance the pivot selection
    by choosing a value closer to the true median of the dataset. It helps improve
    the algorithm’s performance on a wide range of inputs, reducing the chance of
    worst-case behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to other sorting algorithms, quick sort is highly efficient for large
    datasets, and its average and worst-case time complexity are *O*(*n* log *n*)
    and *O*(*n*²), respectively. Quick sort has an average space complexity of *O*(log
    *n*), which can degenerate to *O*(*n*) when the input array is already sorted
    or nearly sorted and the first or the last element is chosen as the pivot (worst
    case).
  prefs: []
  type: TYPE_NORMAL
- en: The Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The code for quick sort is quite similar to that of merge sort in structure,
    as both algorithms rely on a divide-and-conquer approach. In the code, the main()
    function accepts an input array and passes it to the quickSort() function. Within
    quickSort(), we invoke a partition() helper function to determine the correct
    position for the pivot element. This allows us to divide the original array into
    a left array containing elements less than the pivot and a right array containing
    elements greater than or equal to the pivot. Finally, quickSort() is recursively
    called on these subarrays as long as start is less than end, which means at least
    two elements remain in the subarray.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the main() function, we call the quickSort() function by passing three parameter
    values: the array to be sorted (arr) and the indices for its first and last elements
    (start and end) ❶. As before, we print the array before and after sorting.'
  prefs: []
  type: TYPE_NORMAL
- en: In the quickSort() function, we start by checking whether the starting index
    of the incoming subarray is less than the ending index ❷. When this is no longer
    true, the subarray will have only one element, so the recursion of that branch
    will stop. Otherwise, we call the partition() helper function, which returns the
    final (sorted) position of the pivot element. We store this position as pivotIndex
    and use it to divide the original array into left and right subarrays. We then
    recursively call quicksort() on the left and right subarrays until the stopping
    condition is met.
  prefs: []
  type: TYPE_NORMAL
- en: The real sorting work happens inside the partition() function. After setting
    pivot to the value of the last element in the subarray, we use two index variables,
    i and j, to swap the positions of the elements inside a for loop. Both start at
    the beginning of the subarray, and then j steps through the subarray looking for
    elements with values less than pivot ❸. Each time one is found, the values at
    i and j are swapped, and then i is incremented. In effect, this moves elements
    less than the pivot to earlier in the array, and elements greater than the pivot
    to later in the array. Once the for loop is done, the pivot itself is swapped
    with the element at i ❹, which puts the pivot element into its final sorted position.
    Then the final value of i is returned so that two new subarrays can be formed
    on both sides of the final position of the last pivot element. The swaps themselves
    are relegated to a swap() helper function, which uses the temp variable to avoid
    overwriting the value at i. Apart from this one extra variable, the sorting happens
    in place.
  prefs: []
  type: TYPE_NORMAL
- en: The Result
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you run the code with the example array, the output should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: I mentioned earlier that I would explain how the order of the subarray elements
    is determined. This has to do with the swapping algorithm in the partition() function.
    During the first round of processing the [8, 3, 4, 5, 1, 2] array, for example,
    2 is the pivot, and the first element in the array less than the pivot is 1\.
    This element gets swapped with the 8 at the start of the array (accessed using
    index variable i), yielding an array of [1, 3, 4, 5, 8, 2]. Then the pivot itself
    (2) is swapped with the next element of the array (3—again accessed via i), yielding
    [1, 2, 4, 5, 8, 3].
  prefs: []
  type: TYPE_NORMAL
- en: I encourage you to manually step through the entire process of sorting the array
    with quick sort. You can refer to [Figure 7-2](chapter7.xhtml#fig7-2), which shows
    the original input array, the intermediate subarrays after each round of processing,
    and the final sorted array. By going through the comparisons and swaps yourself,
    you can visualize the partitioning and sorting process in a more tangible way.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/Figure7-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-2: The quick sort steps for [8, 3, 4, 5, 1, 2]'
  prefs: []
  type: TYPE_NORMAL
- en: You can also autogenerate the subarrays at each stage by printing the left and
    right arrays from inside the quicksort() function, just after the position of
    the pivot is determined.
  prefs: []
  type: TYPE_NORMAL
- en: Search Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Searching through a data structure is a fundamental operation in computer science.
    It helps us track down specific elements or retrieve information stored within
    a collection of data. While this task may seem trivial for a small amount of data,
    as the volume of data increases—up to large databases, filesystems, or even the
    whole internet—knowing how to choose the right search algorithm becomes paramount
    to keeping our digital life humming.
  prefs: []
  type: TYPE_NORMAL
- en: Search algorithms are intimately connected to the data structures they’re designed
    to search, since how the data is organized affects how efficiently a particular
    item can be found and accessed. For the purposes of the coming projects, we’ll
    focus on several algorithms that are used to search a graph, which is a type of
    data structure. Before we get to the algorithms themselves, however, it’s important
    to establish how graphs are structured.
  prefs: []
  type: TYPE_NORMAL
- en: What Is a Graph?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the field of graph theory, a graph is a mathematical structure consisting
    of a set of vertices (also known as *nodes*) and a set of edges (also known as
    *arcs* or *links*) that connect pairs of vertices. Vertices can represent any
    kind of objects, such as cities, people, or even more abstract concepts. Edges
    represent relationships or connections between the vertices. Mathematically, a
    graph is denoted by *G* and defined as *G* = (***V***, ***E***), where ***V***
    is a set of vertices or nodes, and ***E*** is a set of edges or links.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-3](chapter7.xhtml#fig7-3) depicts a simple graph consisting of five
    nodes and five edges. Each circle in the figure represents a vertex, and each
    line represents an edge. The nodes are named with sequential numbers for convenience.
    In real-world cases, most nodes would be names with strings, however. When node
    names are designated by whole numbers, we can treat them as either of type Int
    or of type String in the code.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/Figure7-3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-3: A simple graph with five nodes and five edges'
  prefs: []
  type: TYPE_NORMAL
- en: 'Graphs can be categorized into two main groups: undirected and directed. In
    an *undirected graph*, the edges allow movement between vertices in both directions.
    This type of graph is often used to represent scenarios like a road network, where
    traffic can flow both ways. By contrast, each edge in a *directed graph* has a
    specific direction associated with it, restricting the way you can move between
    vertices. For example, a directed graph can represent a water or power distribution
    network, where the flow always moves from areas of high pressure to areas of low
    pressure or from high voltage to low voltage, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: When the edges of a graph have weights associated with them, the graph is called
    a *weighted graph*. The weight in this case could be a proxy for cost, distance,
    or any other edge-related property. Weighted graphs can be either directed or
    undirected.
  prefs: []
  type: TYPE_NORMAL
- en: How to Search a Graph
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the coming projects, we’ll consider three different algorithms for searching
    a graph. The first, *depth-first search (DFS)*, is a technique that starts at
    a particular node and explores as far (or “as deep”) as possible along one branch
    before backtracking and exploring the next. In this way, it traverses the depth
    of a data structure before exploring its breadth. DFS is often implemented by
    using a *stack* data structure (we explored stacks in [Chapter 6](chapter6.xhtml)
    while developing the L-system simulator). This way, DFS can use the youngest node
    in the stack to extend the branch and explore each adjacent node at the end of
    a branch before backtracking and moving to the next branch. DFS is useful in many
    applications, including scheduling problems, detecting cycles in graphs, and solving
    puzzles with only one solution, such as a maze or a sudoku puzzle.
  prefs: []
  type: TYPE_NORMAL
- en: The next algorithm, *breadth-first search (BFS)*, takes the opposite approach
    of DFS, exploring the data structure level by level. It starts at a given node
    and visits all its immediate neighbors. Then it moves on to the next level, visiting
    all the neighbors’ neighbors, and so on. In this way, BFS prioritizes exploring
    the breadth of the entire data structure over the depth of any individual branch.
    As we’ll discuss in [Project 31](chapter7.xhtml#pre-31), BFS typically uses a
    queue data structure, allowing it to visit each level in order. It’s useful for
    finding the shortest path, web crawling, analyzing social networks, and exploring
    all reachable nodes in a graph while using the smallest number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: The choice between DFS and BFS depends on the specific problem and the characteristics
    of the data structure being searched. DFS is typically used when we want to conduct
    a deep exploration and potentially find a target item more quickly, while BFS
    is suitable for situations where we want to visit all nodes at a certain distance
    from the starting point or find the shortest path between nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The final algorithm we’ll explore is called *A* search* (pronounced “A-star
    search”). It excels in finding the shortest path in a graph or a maze by combining
    heuristic decision-making with real-time exploration to guide the search. The
    term *heuristic* refers to general decision-making strategies that rely on intuition,
    educated guesses, or common sense to arrive at a plausible solution or direction
    to explore. While heuristics can’t guarantee an optimal or perfect outcome, they
    often provide an advantage in situations where constraints such as limited information,
    time, or resources exist.
  prefs: []
  type: TYPE_NORMAL
- en: The A* algorithm’s heuristic is to consider both the cost of reaching a specific
    node and an estimate of the remaining effort required to reach the destination.
    In this way, A* is able to intelligently prioritize the most promising paths.
    This strategic approach, similar to having a GPS in a labyrinth, helps save time
    and effort in the search process. Due to its versatility, A* is frequently applied
    in fields such as pathfinding in video games, robotics, navigation systems, and
    various optimization problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project 30: Stack-Based Searching with Depth-First Search'
  prefs: []
  type: TYPE_NORMAL
- en: In this project, we’ll explore the core steps of depth-first search and implement
    them in Kotlin. We’ll employ the stack data structure in the code, although it’s
    worth noting the existence of other viable methods for implementing the core DFS
    algorithm. Later on, I’ll share some hints on an alternative approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a given graph (a network of nodes and edges), here are the steps to perform
    a DFS by using a stack:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.  Start by selecting a node as the starting node (it can be any node).
  prefs: []
  type: TYPE_NORMAL
- en: 2.  Push the starting node onto the stack.
  prefs: []
  type: TYPE_NORMAL
- en: 3.  While the stack is not empty, pop a node from the stack.
  prefs: []
  type: TYPE_NORMAL
- en: 4.  If the popped node is not yet visited, mark it as visited and push its neighbors
    to the stack; or else pop the next node from the stack.
  prefs: []
  type: TYPE_NORMAL
- en: 5.  Repeat steps 3 and 4 until the stack is empty.
  prefs: []
  type: TYPE_NORMAL
- en: Recall from [Chapter 6](chapter6.xhtml) that a stack follows the LIFO principle,
    whereby items are removed from the stack in the reverse order in which they were
    added. The LIFO principle allows the DFS algorithm to backtrack from the end of
    one branch before starting on a new, unvisited branch. This ensures an exhaustive
    search of the entire graph, although it would also be beneficial to include a
    stopping condition. When each node is visited, this condition would check if the
    desired goal of the search has been achieved, such as finding a specific object
    or completing a particular task. Once the goal is met, the search can be terminated
    early. For this project, we’ll use the graph shown in [Figure 7-3](chapter7.xhtml#fig7-3).
  prefs: []
  type: TYPE_NORMAL
- en: The time complexity of the DFS algorithm is *O*(*V* + *E*), where *V* is the
    number of vertices and *E* is the number of edges in the graph. The space complexity
    of DFS depends on the implementation (a stack versus a recursive function); the
    worst-case space complexity is *O*(*V*).
  prefs: []
  type: TYPE_NORMAL
- en: The Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s now examine the code that implements the core steps of DFS. We’ll use
    the code to traverse the entire example graph shown earlier in [Figure 7-3](chapter7.xhtml#fig7-3).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: First, we import the ArrayDeque class from java.util, which we’ll use to implement
    the stack. Next, we declare the main() function, which serves as the entry point
    of the program. It defines the graph as a map pairing each node ("0" through "4")
    with a set of all its neighboring nodes ❶. For example, node "2" is paired with
    the set ["0", "1", "4"], since it’s connected to those nodes. We print the graph
    to the console, then call the dfsStack() function to perform the search, passing
    the graph and a starting node as arguments ❷. Upon completion of the search, the
    list of visited nodes is returned, which is printed as the program’s final output.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the dfsStack() function, we create a mutable set called visited to keep
    track of the visited nodes and an ArrayDeque named stack to store the nodes during
    traversal. We push the starting node to the stack, then enter a while loop that
    iterates for as long as the stack is not empty ❸. In each iteration, the last
    node from the stack is removed by using pop() and assigned to the variable node.
    If the node hasn’t been visited before, we could perform additional operations
    or processing specific to the application at this point—for example, checking
    if the node matches our search criteria and breaking from the loop if it does.
    The node is then added to the visited set by using the add() function.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we add all neighboring nodes, retrieved from graph by using node as the
    key, to the stack via the push() function ❹. We use the nonnull assertion operator
    (!!) while adding graph[node] to the stack to avoid additional null safety checks
    that aren’t required for undirected graphs (every node will have at least one
    link or edge). The while loop terminates once the stack is empty, at which point
    the set of visited nodes is returned to main().
  prefs: []
  type: TYPE_NORMAL
- en: Note that we could have used the ArrayDeque class from kotlin.collections (as
    we did in [Chapter 6](chapter6.xhtml)) instead of ArrayDeque from java.util to
    implement the stack. In that case, we would replace push() with addLast() and
    pop() with the removeLast() function. I’ve chosen to use the Java version in part
    to illustrate an alternative stack implementation and in part because the ArrayDeque
    method names like push() and pop() fit naturally with the stack architecture.
    Both techniques follow the LIFO principle, meaning that the last element added
    to the stack is the first one removed.
  prefs: []
  type: TYPE_NORMAL
- en: The Result
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you run the code with the given graph, you should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The list of visited nodes [0, 3, 2, 4, 1] indicates the algorithm has traversed
    the entire graph. To see where this order comes from, and to better understand
    how the stack facilitates the DFS process, consider [Table 7-2](chapter7.xhtml#tab7-2),
    which shows the intermediate values at each step of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7-2: Anatomy of the Depth-First Search Using Stack'
  prefs: []
  type: TYPE_NORMAL
- en: '| Stage | Node | Node not visited? | Visited nodes | Neighbor nodes | Nodes
    on stack |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Initialization, with start of 0 | N/A | N/A | [] (empty) | N/A | [0] (start
    pushed to stack) |'
  prefs: []
  type: TYPE_TB
- en: '| Inside the while loop | 03024212001 | truetruefalsetruetruefalsetruefalsefalsefalsefalse
    | [0][0, 3]no change[0, 3, 2][0, 3, 2, 4]no change[0, 3, 2, 4, 1]no changeno changeno
    changeno change | [1, 2, 3][0]N/A[0, 1, 4][2]N/A[0, 2]N/AN/AN/AN/A | [1, 2, 3][1,
    2, 0][1, 2][1, 0, 1, 4][1, 0, 1, 2][1, 0, 1][1, 0, 0, 2][1, 0, 0][1, 0][1][] (empty;
    while loop terminates) |'
  prefs: []
  type: TYPE_TB
- en: Let’s take a look at a few rows from [Table 7-2](chapter7.xhtml#tab7-2) to understand
    how DFS works. In the first row, we see what happens during the initialization
    phase, before entering the while loop. We set the starting node to "0" and push
    it onto the stack. At this stage, node "0" hasn’t been marked as visited yet.
    Next, we move inside the while loop, where the rest of the processing happens.
    First, we pop the last node from the stack, which is "0" (this makes the stack
    momentarily empty). Since this node isn’t yet marked as visited, we add it to
    the list of visited nodes, which goes from [] to [0]. We then add all this node’s
    neighbors (accessed with graph["0"]) to the stack, which goes from [] to [1, 2,
    3].
  prefs: []
  type: TYPE_NORMAL
- en: The next time through the loop, "3" is popped from the stack, since it’s the
    last element. It hasn’t been visited yet, so it’s added to visited, and its only
    neighbor "0" is pushed to the stack. The process continues until the stack is
    found to be empty at the start of a while loop iteration. I strongly encourage
    you to go over the remaining rows of the table to get a hands-on feel for how
    the DFS algorithm works in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project 31: Queue-Based Searching with Breadth-First Search'
  prefs: []
  type: TYPE_NORMAL
- en: In this project we’ll continue our exploration of search algorithms by implementing
    breadth-first search. BFS guarantees that all nodes at the same level are visited
    before moving on to the next level. This process continues until all nodes in
    the graph have been visited. As in [Project 30](chapter7.xhtml#pre-30), we’ll
    use the ArrayDeque class from java.util to implement the BFS algorithm. This time,
    however, we’ll use the class as a *queue*, a data structure that adheres to the
    *first in, first out (FIFO)* principle. Whereas items are always added (“pushed”)
    or removed (“popped”) from the end of a stack, items are added (“enqueued”) at
    the end of a queue and removed (“dequeued”) from the beginning of the queue. This
    ensures that items are processed in the order in which they were added to the
    queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform a BFS, we’ll follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.  Select a node as the starting node (it can be any node).
  prefs: []
  type: TYPE_NORMAL
- en: 2.  Create a mutable list called visited and add the starting node to it.
  prefs: []
  type: TYPE_NORMAL
- en: 3.  Create an empty queue and enqueue (add) the starting node.
  prefs: []
  type: TYPE_NORMAL
- en: '4.  While the queue is not empty, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: a.  Dequeue the front node from the queue.
  prefs: []
  type: TYPE_NORMAL
- en: b.  Process the dequeued node as needed (perhaps printing its value or performing
    some operation).
  prefs: []
  type: TYPE_NORMAL
- en: c.  Enqueue all the unvisited neighbors of the dequeued node and mark them as
    visited.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the graph shown in [Figure 7-3](chapter7.xhtml#fig7-3) for this project
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: The time complexity of the BFS algorithm is *O*(*V* + *E*), where *V* is the
    number of vertices and *E* is the number of edges in the graph. The space complexity
    of the BFS algorithm is typically *O*(*V*). Both DFS and BFS therefore have the
    same time complexity, but their space complexity can vary depending on the implementation
    and the structure of the graph.
  prefs: []
  type: TYPE_NORMAL
- en: The Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The code for BFS closely resembles that of DFS, but I’ll highlight a few important
    distinctions as we discuss the program.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The main() function is essentially the same as that of the previous project.
    We define the input graph by using a map data structure and print the graph, displaying
    each node and its neighbors. We then call the bfsQueue() search function, passing
    the graph and the starting node as arguments ❶. The function returns the visited
    nodes, which are printed as the final output of the program.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the bfsQueue() function, we initialize a mutable list called visited
    to keep track of visited nodes as before, along with an ArrayDeque called queue
    to store the nodes to be visited. We then add the starting node to both the visited
    set and the queue, using the offer() method for the latter. Next, we initiate
    a while loop that continues until the queue becomes empty ❷. Within the loop,
    we dequeue a node from the front of the queue by using the poll() method, placing
    it in the node variable. We then iterate over each neighbor of the current node,
    obtained from the graph. If a neighbor hasn’t been visited (meaning it isn’t present
    in the visited set), it’s enqueued by using the offer() method and added to the
    visited set ❸. After processing all the neighbors, the loop continues until the
    queue becomes empty. The visited set is then returned, containing all the nodes
    visited during the search.
  prefs: []
  type: TYPE_NORMAL
- en: The Result
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the given graph, if you run the code without any changes, the code will
    produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Again, the list of visited nodes [0, 1, 2, 3, 4] indicates the algorithm has
    successfully traversed the entire graph. This time, the nodes are marked as visited
    in numerical order, a function of the FIFO principle of the queue. [Table 7-3](chapter7.xhtml#tab7-3)
    shows the intermediate values of the key variables as the process unfolds and
    how the BFS algorithm works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7-3: Anatomy of the Breadth-First Search Using a Queue'
  prefs: []
  type: TYPE_NORMAL
- en: '| Stage | Node | Neighbor nodes | next node | Node not visited? | Nodes on
    queue | Visited nodes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Initialization, with start of 0 | N/A | N/A | N/A | N/A | [0] | [0] |'
  prefs: []
  type: TYPE_TB
- en: '| Inside the while loop | 0 | [1, 2, 3] | 123 | truetruetrue | [1][1, 2][1,
    2, 3] | [0, 1][0, 1, 2][0, 1, 2, 3] |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | [0, 2] | 02 | falsefalse | [2, 3][2, 3] | no changeno change |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | [0, 1, 4] | 014 | falsefalsetrue | [3][3][3, 4] | no changeno change[0,
    1, 2, 3, 4] |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | [0] | 0 | false | [4] | no change |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | [2] | 2 | false | [] (empty; while loop terminates) | no change |'
  prefs: []
  type: TYPE_TB
- en: Let’s go over a few of the rows in [Table 7-3](chapter7.xhtml#tab7-3) to gain
    a better understanding of how the BFS algorithm is implemented. At the initialization
    stage, we identify node "0" as the start node and add it to both the visited list
    and the queue. Both of these lists now contain "0" (see the first row).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we move inside the while loop, which runs as long as queue is not empty.
    We start with the front node "0" and fetch its neighboring nodes, "1", "2", and
    "3". For each, we check that it hasn’t been visited before; when this is true,
    we add that node to both queue and visited. Since none of these nodes were visited,
    they’re all added to queue and visited when we’re done with node "0".
  prefs: []
  type: TYPE_NORMAL
- en: The process continues by pulling the next front node, "1". This time both its
    neighbors, "0" and "2", show up in the visited list, so nothing is added to queue
    or visited. Each time we remove a node from queue, the queue shrinks in size.
    In the final step, node "4" is pulled out, making queue empty, which breaks the
    while loop. The code returns the visited list as the final output.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing [Tables 7-2](chapter7.xhtml#tab7-2) and [7-3](chapter7.xhtml#tab7-3)
    will help you gain a deeper understanding of the unique features of the DFS and
    BFS algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project 32: Heuristic Searching with A*'
  prefs: []
  type: TYPE_NORMAL
- en: In this project, we’ll explore the A* search algorithm, an informed search algorithm
    that uses a heuristic function to guide the search. Its primary objective is to
    find the optimal path between two nodes in a graph by considering the cost of
    each path. To that end, it’s best suited for working with weighted graphs, where
    each edge has an associated score. [Figure 7-4](chapter7.xhtml#fig7-4) shows the
    graph we’ll use for the project.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/Figure7-4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-4: An example graph for [Project 32](chapter7.xhtml#pre-32) (start
    node = A, target node = J)'
  prefs: []
  type: TYPE_NORMAL
- en: The graph in the figure has 13 nodes (A through M) and 20 edges, making it significantly
    more substantial than the example graph we used in the previous projects. The
    values along the edges represent the cost of traveling between the two nodes connected
    by that edge. We’re assuming that the graph is undirected, so travel along an
    edge can go in either direction, and that the cost for each edge is *symmetric*,
    meaning it’s the same no matter the direction of travel. For this project, we’re
    interested in determining the lowest-cost route from node A (the start) to node
    J (the target).
  prefs: []
  type: TYPE_NORMAL
- en: 'As the A* algorithm traverses a graph, it uses two distinct functions to help
    make decisions. One calculates the *g-score*, the actual cost of traveling from
    the start node to the current node. The other calculates the *h-score*, the estimated
    or heuristic cost of traveling from the current node to the target node. Added
    together, these two scores give the *f-score*, the estimated total cost of the
    path:'
  prefs: []
  type: TYPE_NORMAL
- en: '*  f-score* = *g-score* + *h-score*'
  prefs: []
  type: TYPE_NORMAL
- en: One of the key strengths of the A* algorithm is its efficiency in finding the
    shortest path based on this informed approach. But for this to work, we need a
    good heuristic function.
  prefs: []
  type: TYPE_NORMAL
- en: '#### The Heuristic Function'
  prefs: []
  type: TYPE_NORMAL
- en: In the context of the A* search algorithm, a heuristic function, denoted as
    *h*(*n*), is a function that estimates the cost from the current node to the target
    node in a graph. The purpose of the heuristic function is to guide the search
    algorithm by providing an informed estimate of how far a node is from the target,
    which helps A* make more efficient decisions about which nodes to explore next.
  prefs: []
  type: TYPE_NORMAL
- en: An *admissible* heuristic function for the A* algorithm is a function that never
    overestimates the cost of reaching the goal from any node. With an admissible
    set of h-scores, A* is guaranteed to find the shortest or least costly path. However,
    not all sets of admissible h-scores are equally good. The algorithm’s performance
    depends on how close the h-scores are to the true costs. The more accurate the
    h-scores are, the faster the algorithm will find the optimal path.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another desirable property of the heuristic function is consistency. A *consistent*
    function satisfies this condition: the cost of reaching the goal from a node is
    always less than or equal to the cost of reaching the goal from any neighbor of
    that node, plus the cost of moving to that neighbor. Consistency implies admissibility
    but not vice versa. A consistent set of h-scores can make the A* algorithm more
    efficient, as it will expand fewer nodes and converge to the optimal solution
    very quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consistent h-scores may be hard or impossible to obtain for large and complex
    real-world problems. However, we can still estimate admissible h-scores that are
    of high quality by using various techniques, depending on the problem type. Here
    are some common approaches for generating heuristic functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ad hoc selection of h-scores**'
  prefs: []
  type: TYPE_NORMAL
- en: This method will work when the graph is small and it’s possible to make conservative
    guesses about the h-scores depending on the depth of a node. For example, one
    can set all h-scores to some arbitrary small value that’s guaranteed to be both
    admissible and consistent.
  prefs: []
  type: TYPE_NORMAL
- en: '**Domain knowledge**'
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, domain-specific knowledge can be used to craft heuristic functions.
    This requires an understanding of the problem and what makes a good heuristic
    based on expert insights. For example, in the case of solving an eight-piece sliding
    puzzle with a 3×3 grid, a practical heuristic is the Manhattan distance, determined
    by adding the horizontal and vertical distances between each tile’s current position
    and its target location.
  prefs: []
  type: TYPE_NORMAL
- en: '**Relaxation heuristics**'
  prefs: []
  type: TYPE_NORMAL
- en: This method involves simplifying a problem by temporarily ignoring certain constraints.
    Relaxation frequently results in an admissible heuristic because it tends to underestimate
    the actual cost. Take, for example, pathfinding problems, where one can use the
    Euclidean distance between two points as a heuristic, ignoring any obstacles that
    may lengthen the path.
  prefs: []
  type: TYPE_NORMAL
- en: '**Abstraction**'
  prefs: []
  type: TYPE_NORMAL
- en: This method involves simplifying the problem representation by grouping or abstracting
    specific elements within it. Abstraction can lead to admissible and consistent
    heuristics. Consider, for example, a navigation problem, where you could choose
    to abstract the map by representing cities as nodes and major highways as edges,
    while ignoring smaller streets.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pattern databases**'
  prefs: []
  type: TYPE_NORMAL
- en: In problems with large state spaces, where the graph includes numerous nodes
    and links (such as puzzle games), pattern databases can be employed to precompute
    heuristic values for subsets of the state space. These databases store the cost-to-goal
    for small subsets of the problem, and the heuristic for a given state is estimated
    as the sum of the costs associated with the relevant subsets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of the graph shown in [Figure 7-4](chapter7.xhtml#fig7-4), we’ll
    employ a combination of abstraction and ad hoc heuristic approaches to estimate
    a set of h-scores that are both admissible and consistent. Since we lack additional
    information about the nodes, such as their coordinates, we’ll begin with a simplifying
    assumption (abstraction): all edges or links within the graph have the same weight
    or cost. Furthermore, we’ll assume this weight equals the smallest weight found
    within the graph (ad hoc). Our approach can be summarized as a three-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.  Edge weight assumption: Assume that all edges within the graph have an
    identical weight, and set this value to the smallest weight found within the graph.'
  prefs: []
  type: TYPE_NORMAL
- en: '2.  Minimum links count: For each node, determine the minimum number of edges
    or links needed to traverse from that node to the target node.'
  prefs: []
  type: TYPE_NORMAL
- en: '3.  H-score estimation: The h-score for each node is estimated by multiplying
    the smallest weight determined in step 1 with the minimum number of links needed
    to reach the target node, as found in step 2.'
  prefs: []
  type: TYPE_NORMAL
- en: Given the relatively modest size of the graph, using this process to calculate
    h-scores is straightforward and quick. A brief examination of the weights reveals
    that the smallest one within the graph is 2 (for the link connecting nodes B and
    C). Now let’s consider nodes I and K, immediate neighbors of the target node J.
    Their h-scores will be 2 × 1 = 2, since both I and K are only one link away from
    the target. Similarly, h-scores for nodes E, F, G, H, and L, which are two links
    away from the target, can be estimated as 2 × 2 = 4\. Following this logical progression,
    the h-score for the starting node A, located farthest from the target, is estimated
    to be 2 × 4 = 8 because at least four links must be traversed to reach the target.
    Once these heuristic values are computed, you can easily incorporate them into
    the application’s getHScore() function, a lookup function that retrieves the h-score
    for a given node. (We’ll discuss this function later, along with the rest of the
    code.)
  prefs: []
  type: TYPE_NORMAL
- en: Given our approach of utilizing the minimum number of links necessary to traverse
    from a given node to the target, along with our use of the smallest weight present
    in the graph for h-score calculation, the resulting h-scores meet the criterion
    of admissibility. They never overestimate the cost of reaching the target. I invite
    you to verify that these h-scores also meet the criterion of consistency as defined
    earlier in the section. You can do this either manually or by writing a few lines
    of code.
  prefs: []
  type: TYPE_NORMAL
- en: The Algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given our heuristic function, here are the steps we’ll take to find the optimal
    path between two nodes by using the A* algorithm. This method assumes that at
    least one valid route exists between the starting node and the target:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.  Initialize two mutable maps to keep track of the visited and the unvisited
    nodes, respectively. The visited map starts empty; the unvisited map starts with
    all nodes in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 2.  Initialize each unvisited node’s g-score and f-score to infinity (or the
    maximum possible value of the corresponding type) and its previous node property
    to “none.”
  prefs: []
  type: TYPE_NORMAL
- en: 3.  Set the starting node’s g-score to 0 (as the journey starts here, no previous
    node exists to come from), calculate or look up its h-score, and set its f-score
    equal to its h-score (since g-score = 0). Leave its previous node property set
    to “none.”
  prefs: []
  type: TYPE_NORMAL
- en: '4.  While the unvisited map is not empty:'
  prefs: []
  type: TYPE_NORMAL
- en: a.  Select the node with the lowest f-score from the unvisited nodes and designate
    that as the current node. (The starting node will be the first current node.)
  prefs: []
  type: TYPE_NORMAL
- en: b.  If the current node is the target node, add the current node to the visited
    map and terminate the loop (the target has been reached).
  prefs: []
  type: TYPE_NORMAL
- en: c.  Otherwise (when the current node is not the target node), retrieve the current
    node’s neighbors from the graph.
  prefs: []
  type: TYPE_NORMAL
- en: d.  For each neighbor that has not already been visited, calculate a new g-score
    by adding the weight of the edge between the current node and the neighbor to
    the g-score of the current node. If this new g-score is lower than the neighbor’s
    existing g-score, update the neighbor’s attributes (g-score, f-score, previous
    node).
  prefs: []
  type: TYPE_NORMAL
- en: e.  Add the current node to the visited map and remove it from the unvisited
    map.
  prefs: []
  type: TYPE_NORMAL
- en: 5.  Once the loop ends, the visited map is returned, which contains information
    about the nodes explored during the search, their directional relationship (as
    captured in the “previous node” property), and the associated costs (g-scores
    and f-scores).
  prefs: []
  type: TYPE_NORMAL
- en: 6.  Use the information contained in the visited map to reconstruct the entire
    optimal path.
  prefs: []
  type: TYPE_NORMAL
- en: These steps outline the essence of the A* algorithm. They involve maintaining
    an open set of nodes to be explored and a closed set of nodes that have been visited,
    and calculating the cost of each node based on the actual cost from the starting
    node (g-score) and the estimated cost to the target node (h-score). By iteratively
    selecting the node with the lowest total cost (f-score), the algorithm efficiently
    finds the shortest path from the starting node to the target node.
  prefs: []
  type: TYPE_NORMAL
- en: The time complexity of the A* search algorithm depends on the nature of the
    problem and the quality of the heuristic function used. In the worst case, the
    time complexity of A* is *O*(*b**^d*), where *b* is the branching factor (the
    average number of edges per node) and *d* is the depth of the shallowest target
    node (the minimum number of edges or steps needed to reach the target from the
    starting node). The space complexity of the standard A* algorithm depends on the
    data structure used for the open and closed lists (for example, this could be
    implemented by using priority queues). In the worst case, the space complexity
    can be very high, also up to *O*(*b**^d*), due to the storage of nodes in the
    open and closed lists.
  prefs: []
  type: TYPE_NORMAL
- en: A well-chosen (admissible and consistent) heuristic can significantly improve
    the performance of A*, however, by efficiently guiding the algorithm to the target
    node, reducing the search space, and potentially making the actual time and space
    complexities much lower in practice. In the best-case scenario, when the heuristic
    function is perfect and the algorithm efficiently explores the most promising
    paths first, for example, A* can have a time complexity of *O*(*d*).
  prefs: []
  type: TYPE_NORMAL
- en: The Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The code for A* search is more involved than the code for DFS and BFS. For this
    reason, I’ll break it down into a number of segments, starting with the global
    declarations and the main() function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This application doesn’t require an import block, as the search algorithm can
    be implemented without relying on any specialized library functions. The sole
    global component is a data class that holds three key attributes of a node: its
    g-score, its f-score, and the previous node along the optimal path.'
  prefs: []
  type: TYPE_NORMAL
- en: In the main() function, we first define the graph shown in [Figure 7-4](chapter7.xhtml#fig7-4)
    as a Map ❶. Each node is specified in terms of its name ("A", "B", "C", and so
    on) along with an inner Map pairing each of the node’s neighbors with the weight
    of the edge leading to that neighbor. You can think of graph as a map of maps
    (similar to a list of lists) encapsulating all nodes in the network and their
    interconnections. Once the graph is defined, it’s printed by calling the displayGraph()
    function.
  prefs: []
  type: TYPE_NORMAL
- en: We next define the start and target nodes ("A" and "J" in this example) and
    call the aStar() function by passing the start and target nodes and the graph
    to be searched as arguments ❷. A call to this function returns a list of visited
    nodes (visitedList) as a Map of type <String, List<Any>>. This list represents
    a subset of nodes that the algorithm explored while trying to locate the optimal
    path. Crucially, A* search doesn’t need to visit all nodes in the graph, as it
    relies on heuristic information to zoom in on the region that includes the optimal
    solution. We use the displayList() function to print this visited list and then
    call the displayShortestPath() function, which reconstructs and displays the optimal
    path.
  prefs: []
  type: TYPE_NORMAL
- en: '##### The Display Functions'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a closer look at the various display helper functions called from
    the main() function, starting with the displayGraph() function, which prints the
    whole graph.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This function takes in graph as its sole argument, which as we’ve seen is a
    Map of type <String, Map<String, Int>>. It uses two for loops to print the elements
    of graph. The outer loop cycles through the nodes, one at a time, printing each
    one. The inner loop extracts and prints each of the current node’s neighbors,
    along with the associated edge weights (labeled as Cost in the output). You’ll
    see how the output looks later when we examine the results.
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll consider the displayList() function, which prints the characteristics
    of each visited node after the A* search is complete.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This simple function uses a for loop to extract and print the collection of
    visited nodes and their attributes. Each element in this list, which is presented
    as a Map object, has two components: the name of the visited node and a Node object
    with three data points linked to the node—its g-score (Int), f-score (Int), and
    the previous node (String). The latter is the node from which we would have departed
    to reach the current node, ensuring the minimum f-score for the current node.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, here’s the displayShortestPath() function, which takes in the list
    of visited nodes, the start node, and the target node, and identifies the optimal
    path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The function reconstructs the path in reverse, working backward from the target
    to the starting node. We start by initializing two variables to targetNode: currentNode,
    representing the current position in the path, and path, where the entire path
    is built up node by node. We then enter a while loop that iterates until currentNode
    becomes startNode. In the loop, we access currentNode from the list of visited
    nodes (supplied as a Map of type <String, Node>) and use its previousNode property
    to look up its previous node ❶. Next, we concatenate previousNode with the current
    value of path ❷ and update currentNode to previousNode for the next iteration.'
  prefs: []
  type: TYPE_NORMAL
- en: After the loop ends, we retrieve cost, the g-score of the target node, from
    the list of visited nodes, using targetNode as the key. We then print the reconstructed
    optimal path and its cost.
  prefs: []
  type: TYPE_NORMAL
- en: The aStar() Function and Its Helpers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s now dive into the core of the A* algorithm implemented in aStar() and
    its helper functions. This code very closely follows the steps outlined earlier
    for implementing the A* algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The algorithm begins by creating two mutable maps: visited and unvisited. At
    first, unvisited contains all the nodes in the graph, each initialized with the
    maximum possible g-score and f-score, and with a previous node of "none" ❶. The
    visited map, which is initially empty, keeps track of the nodes that have been
    visited. Next, the startNode in the unvisited map is updated to have a g-score
    of 0 and an f-score equivalent to its h-score ❷, which is retrieved with the getHScore()
    helper function. As shown here, this helper is implemented as a simple lookup
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: These scores were estimated by using the hybrid three-step process explained
    earlier. Note that the h-score for the target node "J" is 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to the aStar() function, we next display the list of unvisited nodes
    and enter a while loop that continues until the unvisited map is empty or the
    target node is reached ❸. Within the loop, currentNode is set to the unvisited
    node with the minimum f-score by using the getCurrentNode() helper function ❹.
    Here’s how that helper function is implemented by using Kotlin’s built-in .minByOrNull
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Back in aStar(), we check if currentNode is the same as targetNode ❺. If it
    is, we add the current node to the visited map and break the loop. Otherwise,
    for each neighbor of the current node not already in the visited map ❻, we calculate
    a new g-score by adding the edge weight to the current node’s g-score. If the
    new g-score is lower than the neighbor’s current g-score ❼, the neighbor’s attributes
    in the unvisited map are updated: its g-score is set to newGScore, its f-score
    is set to its new g-score, plus its h-score (again retrieved with the getHScore()
    function), and its previous node is set to currentNode.'
  prefs: []
  type: TYPE_NORMAL
- en: After processing all neighbors, the currentNode is added to the visited map
    and removed from the unvisited map. When the while loop terminates, the visited
    map is returned with all the information needed to reconstitute the optimal path.
  prefs: []
  type: TYPE_NORMAL
- en: The Result
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’re now ready to run the code and have a look at its console output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The output starts by printing the entire graph, node by node, along with each
    node’s neighbors and edge weights. Next, the initial state of the unvisited map
    is shown after updating the starting node’s attributes. Apart from node "A", each
    node should have the maximum possible g- and f-scores (2147483647) and a previous
    node of "none". Once the target node is reached, a message is printed before exiting
    the while loop. Then the final list of all visited nodes is printed. Looking over
    the list, we can see that not every node as been visited—nodes "L" and "M", representing
    a dead end, were skipped. Notice also that the target node’s g-score is the same
    as its f-score because its h-score is 0\. Also, as expected, all g-scores are
    less than or equal to their corresponding f-scores. This is because the f-score
    is the sum of the g-score and the h-score, and the latter is assumed to be greater
    than or equal to 0.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the terminal output shows the step-by-step process of reconstructing
    the optimal path, followed by the full path and its total associated cost.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this chapter, we explored some representative concepts and algorithms from
    two related domains: sorting and searching. These essential concepts and tools
    have extensive use in the realms of computer and data science, particularly in
    the context of information retrieval from databases, search engine performance
    optimization, data visualization, data mining, machine learning, and network routing.'
  prefs: []
  type: TYPE_NORMAL
- en: Within the domain of sorting, we implemented the insertion sort, merge sort,
    and quick sort algorithms and gained insight into their respective strengths,
    weaknesses, time and space complexities, and stability characteristics. In the
    searching domain, our projects revolved around navigating graph data structures.
    We implemented the depth-first search (DFS), breadth-first search (BFS), and A*
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout these projects, we harnessed the power of various Kotlin features,
    including both stack and queue data structures, as well as lists, maps, and more
    intricate constructs like maps of maps. Last but not least, by tackling the exercises,
    you’ll not only solidify your grasp of these core concepts but also raise your
    sorting and searching skills to a professional level.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Bhargava, Aditya Y. *Grokking Algorithms*. 2nd ed. Shelter Island, NY: Manning,
    2024.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.
    *Introduction to Algorithms*. 4th ed. Cambridge, MA: MIT Press, 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Even, Shimon. *Graph Algorithms*. 2nd ed., edited by Guy Even. New York: Cambridge
    University Press, 2012.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Heineman, George, Gary Pollice, and Stanley Selkow. *Algorithms in a Nutshell*.
    2nd ed. Sebastopol, CA: O’Reilly, 2016.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kopec, David. *Classic Computer Science Problems in Python*. Shelter Island,
    NY: Manning, 2019.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Skiena, Steven. *The Algorithm Design Manual*. 3rd ed. Cham, Switzerland: Springer
    Nature, 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Wengrow, Jay. *A Common-Sense Guide to Data Structures and Algorithms*. 2nd
    ed. Raleigh, NC: The Pragmatic Bookshelf, 2020.'
  prefs: []
  type: TYPE_NORMAL
