- en: '**D'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PITFALL: BEWARE OF “P-HACKING”!**'
  prefs: []
  type: TYPE_NORMAL
- en: In recent years there has been much concern over something that has acquired
    the name *p-hacking*. Though such issues have always been known and discussed,
    things really came to a head with the publication of John Ioannidis’s highly provocatively
    titled paper, “Why Most Published Research Findings Are False” (*PLOS Medicine*,
    August 30, 2005). One aspect of this controversy can be described as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Say we have 250 coins, and we suspect that some are unbalanced. (Any coin is
    unbalanced to at least some degree, but let’s put that aside.) We toss each coin
    100 times, and if a coin yields fewer than 40 or more than 60 heads, we will decide
    that it’s unbalanced. For those who know some statistics, this range was chosen
    so that a balanced coin would have only a 5 percent chance of straying more than
    10 heads away from 50 out of 100\. So, while this chance is only 5 percent for
    each particular coin, with 250 coins, the chances are high that at least one of
    them falls outside that [40,60] range, *even if none of the coins is unbalanced*.
    We will falsely declare some coins unbalanced. In reality, it was just a random
    accident that those coins look unbalanced.
  prefs: []
  type: TYPE_NORMAL
- en: 'Or, to give a somewhat frivolous example that still will make the point, say
    we are investigating whether there is any genetic component to sense of humor.
    Is there a humor gene? There are many, many genes to consider— many more than
    250, actually. Testing each one for relation to sense of humor is like checking
    each coin for being unbalanced: even if there is no humor gene, eventually just
    by accident we’ll stumble upon one that seems to be related to humor.'
  prefs: []
  type: TYPE_NORMAL
- en: In a complex scientific study, the analyst is testing many genes, or many risk
    factors for cancer, or many exoplanets for the possibility of life, or many economic
    inflation factors, and so on. The term *p-hacking* means that the analyst looks
    at so many different factors that one is likely to emerge as “statistically significant”
    even if no factor has any true impact. A common joke is that the analyst “beats
    the data until they confess,” alluding to a researcher testing so many factors
    that one finally comes out “significant.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Cassie Kozyrkov, head of decision intelligence at Google, said it quite well:'
  prefs: []
  type: TYPE_NORMAL
- en: What the mind does with inkblots, it also does with data. Complex datasets practically
    beg you to find false meaning in them.
  prefs: []
  type: TYPE_NORMAL
- en: '*This has major implications for ML analysis.* For instance, a popular thing
    in the ML community is to have competitions in which many analysts try their own
    tweaks on ML methods to outdo each other on a certain dataset. Typically these
    are classification problems, and “winning” means getting the lowest rate of misclassification.'
  prefs: []
  type: TYPE_NORMAL
- en: The trouble is, having 250 ML analysts attacking the same dataset is like having
    250 coins in our example above. Even if the 250 methods they try are all equally
    effective, one of them will emerge by accident as the victor, and it will be annointed
    as a “technological advance.”
  prefs: []
  type: TYPE_NORMAL
- en: Of course, it may well be that one of the 250 methods really is superior. But
    without careful statistical analysis of the 250 data points, it is not clear what’s
    real and what’s just accident. Note, too, that even if one of the 250 methods
    is in fact superior, there is a high probability that it won’t be the winner in
    the competition, again due to random variation.
  prefs: []
  type: TYPE_NORMAL
- en: The problem is exacerbated by the fact that a contestant will probably not even
    submit his entry if it appears unlikely to set a new record. This further biases
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, this concept is second nature to statisticians, but it is seldom
    mentioned in ML circles. An exception is the blog post “AI Competitions Don’t
    Produce Useful Models” by Lauren Oakden-Rayner, whose excellent graphic is reproduced
    in [Figure D-1](app04.xhtml#appdfig1) with Dr. Oakden-Rayner’s permission.^([1](footnote.xhtml#appdfn1))
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/app04fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure D-1: AI p-hacking*'
  prefs: []
  type: TYPE_NORMAL
- en: Rayner uses a simple statistical power analysis to analyze ImageNet, a contest
    in ML image classification. He reckons that at least those “new records” starting
    in 2014 are overfitting, or just noise. With more sophisticated statistical tools,
    a more refined analysis could be done, but the principle is clear.
  prefs: []
  type: TYPE_NORMAL
- en: This also has a big implication for the setting of tuning parameters. Let’s
    say we have four tuning parameters in an ML method, and we try 10 values of each.
    That’s 10⁴ = 10000 possible combinations, a lot more than 250! So again, what
    seems to be the “best” setting for the tuning parameters may be illusory.
  prefs: []
  type: TYPE_NORMAL
- en: The `regtools` function `fineTuning()` takes steps to counter the possibility
    of p-hacking in searches for the best tuning parameter combination.
  prefs: []
  type: TYPE_NORMAL
