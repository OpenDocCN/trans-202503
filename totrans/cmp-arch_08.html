<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="chn"><span epub:type="pagebreak" id="page_135"/><strong>6</strong></h2>&#13;
<h2 class="cht"><strong>SIMPLE MACHINES</strong></h2>&#13;
<div class="image1"><img src="../images/f0135-01.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="chq">In mechanical engineering, <em>simple machines</em> are a well-known set of standard designs including levers, axles, screws, and pulleys that each perform one function and can be put together to make larger machines. Analogously, computational simple machines are standard designs that are often used as subcomponents of computers. For example, the arithmetic logic unit in a modern CPU—exactly as in Babbage’s Analytical Engine—is made of many such simple machines that each perform one kind of arithmetic, such as addition, multiplication, or shifting.</p>&#13;
<p class="indent">This chapter introduces a range of simple machines as the next architecture level above logic gates. Then, in the next chapter, we’ll make use of these simple machines as components of a CPU. The simple machines we’ll discuss come in two main groups: <em>combinatorial machines</em>, which can be written as Boolean expressions, and <em>sequential machines</em>, which require feedback and sequential logic, extending Boolean logic with a temporal element. Feedback and sequential logic are needed to create memory.</p>&#13;
<h3 class="h3" id="lev121"><span epub:type="pagebreak" id="page_136"/>Combinatorial Logic</h3>&#13;
<p class="noindent"><em>Combinatorial logic</em> refers to those digital logic networks that can be described by regular Boolean logic, without considering the role of time. In this section, we’ll see examples of several combinatorial simple machines, which we’ll later rely on as we build up CPU structures.</p>&#13;
<h4 class="h4" id="lev122"><em>Bitwise Logical Operations</em></h4>&#13;
<p class="noindent">The individual logic gates of the previous chapter act on single bits of data: they usually take one or two single-bit inputs and yield a single-bit output. It’s simple to arrange multiple copies of a single gate, in parallel, thus creating an <em>array operator</em>, a simple machine that simultaneously performs the same operation on each bit of an input array to give an output array, as in <a href="ch06.xhtml#ch06fig1">Figure 6-1</a>.</p>&#13;
<div class="image"><img id="ch06fig1" src="../images/f0136-01.jpg" alt="Image" width="277" height="385"/></div>&#13;
<p class="figcap"><em>Figure 6-1: Some bitwise logical operations</em></p>&#13;
<p class="indent">Here, the input arrays <span class="literal">x</span> and <span class="literal">y</span> (or just <span class="literal">x</span> in the case of the NOT operation) pass through an array of identical gates, producing <span class="literal">z</span> as the output. These array operations are well known to low-level C programmers, as the C language includes them and assigns symbols to them. C compilers will ultimately execute these instructions using exactly this simple machine, if it’s present in the target CPU.</p>&#13;
<h4 class="h4" id="lev123"><em>Multi-input Logical Operations</em></h4>&#13;
<p class="noindent">We can create multi-input versions of AND gates from hierarchies of their two-input versions, as in the eight-input AND gate shown in <a href="ch06.xhtml#ch06fig2">Figure 6-2</a>.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_137"/><img id="ch06fig2" src="../images/f0137-01.jpg" alt="Image" width="462" height="320"/></div>&#13;
<p class="figcap"><em>Figure 6-2: An eight-input AND gate made from two-input AND gates (left) and its symbol (right)</em></p>&#13;
<p class="indent">This structure will output 1 if and only if all of its inputs are 1. The same structure works to create multi-input OR gates, which will output 1 if one or more of its inputs are 1.</p>&#13;
<h4 class="h4" id="lev124"><em>Shifters</em></h4>&#13;
<p class="noindent">In base 10, there’s a fast trick for multiplying integers by 10: just append a zero to the end. We can also multiply by a higher natural power of 10, 10<sup><em><sup>n</sup></em></sup>, by appending <em>n</em> zeros. Rather than thinking of it as appending a zero, think of it as shifting each digit one place to the left. Then the trick also works for multiplying non-integer numbers by powers of 10. We can similarly do easy and fast divides by powers of 10 by shifting the digits to the right. These tricks remove the need for the usual slower work of human pen-and-paper multiplication involving repeated single-digital multiplications, additions, and carries.</p>&#13;
<p class="indent">The same tricks work in binary for fast multiplication and division by integer powers of 2. To multiply or divide a number by 2<sup><em>n</em></sup>, shift the number’s bits <em>n</em> places to the left or right.</p>&#13;
<p class="indent"><a href="ch06.xhtml#ch06fig3">Figure 6-3</a> shows a simple machine that, when enabled, performs a left shift, thereby multiplying an input number by 2. The machine is enabled by setting the <em>S</em> (shift) input switch to true. If the <em>S</em> input isn’t enabled, the machine outputs the original input unchanged.</p>&#13;
<div class="image"><img id="ch06fig3" src="../images/f0137-02.jpg" alt="Image" width="742" height="372"/></div>&#13;
<p class="figcap"><em>Figure 6-3: A left-shifter made from logic gates</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_138"/>The shifter design is based on a sub-machine consisting of two ANDs, one NOT, and one OR. Each column (digit) of the number has a copy of this sub-machine, which either allows the column’s own bit to pass through unchanged, or takes the bit from the column to its right.</p>&#13;
<p class="indent">When you do multiplication by powers of two with an operation like <span class="literal">x&gt;&gt;2</span> using a high-level language like C, your CPU may contain a dedicated shifter that gets activated rather than its usual multiplication digital logic. This makes the multiplication operation go faster than one that isn’t by a power of two. This is an example of how knowing architecture enables you to write fast programs. You’ll often see speed-critical code designed to exploit this trick, such as games and media codecs enforcing values to be powers of two.</p>&#13;
<p class="notes"><strong><span class="nt">NOTE</span></strong></p>&#13;
<p class="noindent"><em>Shifting by more than one place can be done in several ways. You could reuse the same shifter network several times, which would save on transistors but take longer to run. Or you could use more transistors to implement many different switches that request different kinds of shift, and implement them immediately. Deciding whether to trade off transistors for speed in this way is a common architectural dilemma.</em></p>&#13;
<h4 class="h4" id="lev125"><em>Decoders and Encoders</em></h4>&#13;
<p class="noindent">Suppose you have a positive integer <em>x</em> represented as an <em>M</em>-bit binary number. Computers often need to convert this binary representation into an alternative 1-of-<em>N</em> representation, which has <em>N</em> = 2<em><sup>M</sup></em> bits, all 0 except for a 1 in the <em>x</em>th bit. For example, an <em>M</em> = 3 bit input such as 101 (coding the number 5<sub>10</sub>) would be converted to 00000100, which has 2<sup>3</sup> = 8 bits with only the fifth one high (counting the bits from left to right, starting from 0). A simple machine called a <em>decoder</em> can perform this conversion. <a href="ch06.xhtml#ch06fig4">Figure 6-4</a> shows a digital logic circuit for a 3-bit decoder.</p>&#13;
<div class="image"><img id="ch06fig4" src="../images/f0138-01.jpg" alt="Image" width="905" height="358"/></div>&#13;
<p class="figcap"><em>Figure 6-4: A 3-bit decoder</em></p>&#13;
<p class="indent">Each input is first copied and inverted. Then a set of AND gates are connected to either the uninverted or inverted versions of each input bit in connection patterns that model the patterns of binary number codings.</p>&#13;
<p class="indent">An <em>encoder</em> performs the inverse operation: it takes a 1-of-<em>N</em> representation as an input and transforms it into a binary number encoding.</p>&#13;
<h4 class="h4" id="lev126"><span epub:type="pagebreak" id="page_139"/><em>Multiplexers and Demultiplexers</em></h4>&#13;
<p class="noindent">We’ve seen that the Analytical Engine consisted of many subcomponents that were dynamically connected and disconnected as needed to perform computations. Making and breaking these connections in the Analytical Engine was done mechanically. For example, when we wanted to do some adding, the mechanisms physically brought the gears into contact between a register and the arithmetic logic unit (ALU). Or when we loaded data from RAM, a mechanism physically connected the desired RAM location to the bus. The digital logic version of this idea is multiplexing and demultiplexing.</p>&#13;
<p class="indent">A <em>multiplexer</em> enables us to select which one of multiple possible sources we wish to connect to a single output. For example, we might have eight registers and want to select one of them to connect to an ALU input. <a href="ch06.xhtml#ch06fig5">Figure 6-5</a> shows an eight-source multiplexer. It consists of a decoder together with eight data inputs, D<sub>0</sub> through D<sub>7</sub>, and additional AND and OR gates.</p>&#13;
<div class="image"><img id="ch06fig5" src="../images/f0139-01.jpg" alt="Image" width="917" height="466"/></div>&#13;
<p class="figcap"><em>Figure 6-5: A multiplexer</em></p>&#13;
<p class="indent">If we wish to connect a particular source, such as D<sub>3</sub>, to the output wire, we place its code, 011<sub>2</sub> for 3<sub>10</sub>, onto the decoder inputs C<sub>0</sub> to C<sub>2</sub>. The decoder sets the third line only to true, which is AND gated together with D<sub>3</sub> as a switch. The OR gates then copy D<sub>3</sub> onto the output wire, as all their other inputs are false.</p>&#13;
<p class="indent">A <em>demultiplexer</em> performs the opposite function to a multiplexer. It takes a single input wire and a code <em>n</em>, and sends a copy of the input signal to the <em>n</em>th of multiple output wires.</p>&#13;
<p class="indent">Multiplexers and demultiplexers are often used together, so we can choose which one of several possible sources to connect to which one of several possible destinations. In these cases, the shared wire is known as a <em>bus</em>.</p>&#13;
<h4 class="h4" id="lev127"><span epub:type="pagebreak" id="page_140"/><em>Adders</em></h4>&#13;
<p class="noindent">You saw in <a href="ch02.xhtml">Chapter 2</a> how to represent integers in binary. We can construct simple machines that use this representation to perform arithmetic operations, such as <em>adders</em> for performing addition.</p>&#13;
<p class="indent">Here’s an example of adding two binary numbers, 001100 and 011010:</p>&#13;
<div class="image"><img src="../images/f0140-01.jpg" alt="Image" width="335" height="122"/></div>&#13;
<p class="indenta">You can perform this addition by hand using the same algorithm as taught to children for decimal addition: starting from the rightmost column, compute the column sum by adding the digits from the input numbers for that column, writing the result underneath as the output sum for that column. If this creates a carry, for example from 1 + 1 = 10, write the lower-power column of the result (the 0 of 10) as the sum and carry the higher-power column of the result (the 1 of 10) to the next column, where it needs to be added as a third input. In the example, the first three columns (counting from the right) don’t produce carries, but the fourth and fifth columns do. (The carries are shown below the final sum.)</p>&#13;
<p class="indent">If you look back at the truth tables for AND and XOR in <a href="ch05.xhtml#ch05fig1">Figures 5-1</a> and <a href="ch05.xhtml#ch05fig4">5-4</a> and compare them to the work done during binary addition, you’ll see that as long as there’s no input carry (as is the case for the first four columns in the example), the results of XOR are identical to column-wise addition, while the results of AND are identical to the carry operation. We could thus use one XOR and one AND to form the simple machine known as a <em>half adder</em>, shown in <a href="ch06.xhtml#ch06fig6">Figure 6-6</a>, to compute the sums for columns when there’s no carry coming in.</p>&#13;
<div class="image"><img id="ch06fig6" src="../images/f0140-02.jpg" alt="Image" width="417" height="316"/></div>&#13;
<p class="figcap"><em>Figure 6-6: A half adder</em></p>&#13;
<p class="indent">By itself, the half adder isn’t very useful, as we don’t usually know if an input carry will also be present. However, if we combine two half adders together with an OR gate, as in <a href="ch06.xhtml#ch06fig7">Figure 6-7</a>, we obtain a more useful network called a <em>full adder</em>.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_141"/><img id="ch06fig7" src="../images/f0141-01.jpg" alt="Image" width="677" height="542"/></div>&#13;
<p class="figcap"><em>Figure 6-7: A full adder made from two half adders and an OR gate</em></p>&#13;
<p class="indent">The truth table for a full adder is shown in <a href="ch06.xhtml#ch06tab1">Table 6-1</a>.</p>&#13;
<p class="tabcap" id="ch06tab1"><strong>Table 6-1:</strong> Full Adder Truth Table</p>&#13;
<table class="allc">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<th style="vertical-align: top" class="borderb"><p class="tabtextc"><strong>X</strong></p></th>&#13;
<th style="vertical-align: top" class="borderb"><p class="tabtextc"><strong>Y</strong></p></th>&#13;
<th style="vertical-align: top" class="borderb"><p class="tabtextc"><strong>C<sub>in</sub></strong></p></th>&#13;
<th style="vertical-align: top" class="borderb"><p class="tabtextc"><strong>Sum</strong></p></th>&#13;
<th style="vertical-align: top" class="borderb"><p class="tabtextc"><strong>C<sub>out</sub></strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tabtextc">0</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">0</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tabtextc">0</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">0</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">0</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">0</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">0</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="tabtextc">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">1</p></td>&#13;
<td style="vertical-align: top"><p class="tabtextc">1</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">The full adder performs two single-bit additions in a row, the first for the main inputs (X and Y) and the second for the sum of the main inputs plus the incoming carry (C<sub>in</sub>). The net result is a single-column sum, as shown here:</p>&#13;
<div class="image"><img src="../images/f0141-02.jpg" alt="Image" width="145" height="127"/></div>&#13;
<p class="indenta">This is the full process needed to correctly find the binary digit sum for each column of binary addition. As well as adding the two binary digits from that column of the two input numbers, it also adds an incoming carried digit whenever it’s present. The full adder’s two outputs are the sum for the column (S) and the carry out for the column (C<sub>out</sub>).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_142"/>The full adder network is often represented by the single symbol shown in <a href="ch06.xhtml#ch06fig8">Figure 6-8</a>.</p>&#13;
<div class="image"><img id="ch06fig8" src="../images/f0142-01.jpg" alt="Image" width="245" height="216"/></div>&#13;
<p class="figcap"><em>Figure 6-8: The adder symbol</em></p>&#13;
<p class="indent">A full adder performs addition of a single column, but to actually add integers together we need to add many columns. One way to do this is to create one full adder for each column and connect the carry out from each column to the carry in of the next. This is known as a <em>ripple-carry adder</em>. <a href="ch06.xhtml#ch06fig9">Figure 6-9</a> shows a 3-bit example that calculates Z = X + Y.</p>&#13;
<div class="image"><img id="ch06fig9" src="../images/f0142-02.jpg" alt="Image" width="429" height="426"/></div>&#13;
<p class="figcap"><em>Figure 6-9: A ripple-carry adder computing the 3 bits of Z = X + Y</em></p>&#13;
<p class="indent">The subscripts say which power of 2 the column represents; for example, here X<sub>0</sub> is ones (as 2<sup>0</sup> = 1), X<sub>1</sub> is twos (as 2<sup>1</sup> = 2), and X<sub>2</sub> is fours (as 2<sup>2</sup> = 4). There’s an additional output from the final carry to indicate if an overflow has occurred. In some cases this would be interpreted as an error. In others it might be connected to further systems that together are able to handle larger numbers.</p>&#13;
<p class="indent">The adder symbol of <a href="ch06.xhtml#ch06fig8">Figure 6-8</a> can also be used to denote a multibit adder such as a ripple-carry adder, where the input and output lines are assumed to denote groups of wires rather than single wires.</p>&#13;
<div class="sidebar">&#13;
<p class="stitle"><span epub:type="pagebreak" id="page_143"/><strong>RIPPLE-CARRY VS. CARRY-SAVE ADDERS</strong></p>&#13;
<p class="stext">When you’re taught to do addition at school, you’re taught a serial adding algorithm, starting at the right side and moving across, with carry digits moving to the next step. The ripple-carry adder is a straight base 2 translation of this idea into digital logic.</p>&#13;
<p class="stext">Think about the efficiency of this process; assuming that both of the inputs are <em>n</em> digits long, we see that this method of addition will scale linearly with <em>n</em> as the length of digits increases the addition runs in roughly <em>O</em>(<em>n</em>) time.</p>&#13;
<p class="stext">But addition doesn’t have to be done or taught like this. Imagine that instead of teaching kids to add numbers together individually, they’re taught to work from the start as a team, each performing a smaller part of the addition. How would you get the numbers added together as quickly as possible in parallel? You’d probably give each kid one pair of column digits from the addition, have them each do their addition at the same time, then have them send their carry along to the person on their left. Then they each take the carry from their right and add it into their result to update it if needed, and sometimes update their carry output and pass it again to their left, until everyone is happy. This is called a <em>carry-save adder</em>.</p>&#13;
<p class="stext">Estimating the number of carry steps that need to be done in this kind of parallel addition is quite a challenge. Naively, around one-quarter of initial additions will produce a carry. But then you need to think about the probability of a second or third subsequent carry step as you later receive incoming carries.</p>&#13;
<p class="stext">To do this properly as a probabilistic estimate, you should take into account the distribution of digits involved in the addition. Most natural quantities have a lower probability of higher-value digits (5+ in decimal; 1 in binary) than low-value digits (up to 4 in decimal; 0 in binary). This is found both in physical and pure mathematics quantities (for example, digits of Planck’s constant, <em>π</em>, and <em>e</em>), though the reason why is quite complex.</p>&#13;
<p class="stext">Carry-save adders can do addition in <em>O</em>(log <em>n</em>) time. They’re still doing the same <em>O</em>(<em>n</em>) amount of total work as the ripple-carry adder, but performing more of the work in parallel, using more silicon. More silicon consumes more space and money, but in this case delivers faster performance. Again, trading silicon for time is a common architectural dilemma.</p>&#13;
<p class="stext">Carry-save adders are found in modern ALUs. They aren’t a new idea and in fact were featured in one of the Analytical Engine designs. This is one of the main reasons the machine was never built: Babbage kept going back to improve the efficiency of the carry mechanism, to the point of obsession. Had he stuck to one design, it may have been completed.</p>&#13;
</div>&#13;
<h4 class="h4" id="lev128"><em>Negators and Subtractors</em></h4>&#13;
<p class="noindent">If we use two’s complement data representation for integers, then negating a number (that is, multiplying it by –1) can be performed by flipping its bits and then adding 1 to the result. A machine that performs this operation is called a <em>negator</em>. <a href="ch06.xhtml#ch06fig10">Figure 6-10</a> shows a 3-bit negator.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_144"/><img id="ch06fig10" src="../images/f0144-01.jpg" alt="Image" width="653" height="499"/></div>&#13;
<p class="figcap"><em>Figure 6-10: A 3-bit negator</em></p>&#13;
<p class="indent">The thick wires in this figure are standard notation for bundles of multiple individual wires, in this case bundles of three wires. (Another common notation for bundles is to draw a diagonal slash through and write the number of wires next to it.) The switches in the bottom-left specify the input number, with the least significant bit first. The number 1 to add is encoded by power and ground inputs, again with the least significant bit first. The adder symbol here indicates not just a full adder but a 3-bit adder, such as a ripple-carry adder.</p>&#13;
<p class="indent">Once we have a negator, we can make a <em>subtractor</em>, a machine that subtracts one number from another. Single- and multi-bit subtractors are indicated with the symbol in <a href="ch06.xhtml#ch06fig11">Figure 6-11</a>.</p>&#13;
<div class="image"><img id="ch06fig11" src="../images/f0144-02.jpg" alt="Image" width="81" height="81"/></div>&#13;
<p class="figcap"><em>Figure 6-11: The subtractor symbol</em></p>&#13;
<p class="indent">We could make a two’s complement subtractor to calculate <em>c</em> = <em>a</em> – <em>b</em> by passing <em>b</em> through a negator and then using an adder to add the result to <em>a</em>.</p>&#13;
<h3 class="h3" id="lev129">From Combinatorial to Sequential Logic</h3>&#13;
<p class="noindent">The combinatorial circuits we’ve seen so far may be viewed as computing instantly. Each circuit corresponds exactly to a Boolean logic expression, which has a definite, mathematical truth value that corresponds to the output of the circuit. This output depends only on the input values, and the input-output pairs can be listed in a truth table.</p>&#13;
<p class="indent">We’ve seen that Shannon’s combinatorial logic circuits can be used to build many simple machines, like multiplexers and adders. Shannon proposed his logic gate theory in 1936, the same year as Church’s and Turing’s definitions of computation, and you might want to view Shannon’s logic gates as an additional competing model of computation from this year, if <span epub:type="pagebreak" id="page_145"/>you’re happy for a “program” to be a set of instructions for how to physically connect a bunch of logic gates, in a similar manner to programming the pre–virtual machine ENIAC.</p>&#13;
<p class="indent">However, Church computers need to be able to simulate any other machine (given enough memory), and we know that some other machines have <em>memory</em> for data storage. There’s no concept of memory in combinatorial logic circuits because memory means storage over time, and there’s no concept of time because these circuits can be viewed as acting instantly. Church computers need to have time and memory and be able to compute outputs that are functions not only of their current input but also of their state as derived from previous inputs.</p>&#13;
<p class="indent">We can extend Shannon’s logic gates with these additional concepts if we allow logic gate networks whose outputs are fed back into their inputs. Such networks weren’t allowed in Shannon’s original combinatorial logic, as they would have resulted in paradoxical Boolean expressions. For example, the circuit in <a href="ch06.xhtml#ch06fig12">Figure 6-12</a> appears to instantiate the Boolean statement X = NOT X. This Boolean statement says that if X is true, then X is false, but if X is false then X is true. What do you think this circuit would do in practice if you connected it? Perhaps it would oscillate or explode?</p>&#13;
<div class="image"><img id="ch06fig12" src="../images/f0145-01.jpg" alt="Image" width="167" height="90"/></div>&#13;
<p class="figcap"><em>Figure 6-12: A paradoxical circuit</em></p>&#13;
<p class="indent">In computer science, feedback is often thought of as evil or paradoxical, something to be avoided: many of the theorems in logic and computability theory are about how to destroy programs, proofs, and machines by feeding their output or descriptions of themselves into their inputs. But feedback is a big idea in computer science in general, and learning to control it and use it for good has been a major part of our success and our culture. Creating memory is one such positive and controlled use of feedback.</p>&#13;
<p class="indent">Let’s illustrate this idea using the example of a guitarist. Guitarists have a more practical worry about feedback, as their guitar strings can vibrate in sympathy with the sounds coming from their amplifiers. These vibrations, in turn, are amplified, and so on, leading to a terrible (or beautiful, depending on your musical point of view) single-frequency screeching sound. Consider exactly <em>when</em> this happens. It’s possible to put the same guitar in exactly the same place in front of the amp, and yet have the system remain completely silent if there’s no initial sound. The feedback emerges only if there’s some sound—even a small one—to make it begin. We could thus use this guitar-amp system to store 1 bit of information. We bring the guitar next to the amp very carefully so no sound is made and the system stays silent, representing a 0. If we later want to store a 1, we stroke the strings to begin the feedback, which continues forever, representing the 1. To change it back to 0, we could turn the amp off and on again.</p>&#13;
<p class="indent">The circuit in <a href="ch06.xhtml#ch06fig13">Figure 6-13</a> is an attempt to make a digital logic version of the same idea. If we try to map it to Boolean logic, it seems less paradoxical than the circuit from <a href="ch06.xhtml#ch06fig12">Figure 6-12</a>, appearing to instantiate the Boolean statement Q = G OR Q. (G is for <em>guitar</em>, and Q is a traditional symbol for <span epub:type="pagebreak" id="page_146"/><em>quiescence</em>, or system state.) You can just about convince yourself that this is stable for G = Q = 0 or for G = Q = 1.</p>&#13;
<div class="image"><img id="ch06fig13" src="../images/f0146-01.jpg" alt="Image" width="271" height="161"/></div>&#13;
<p class="figcap"><em>Figure 6-13: A guitar-like feedback circuit</em></p>&#13;
<p class="indent">However, this still doesn’t give us the concepts of time or memory, because Boolean logic is inherently static. To fully capture these concepts, we need to go beyond Boolean logic and Shannon gates, and consider a new type of logic gate having different states at different <em>times</em>. We need to distinguish states at times using <em>sequential logic</em>, such as writing <em>Q</em><sub><em>t</em></sub> ≠ Q<sub><em>t</em>-1</sub> for states at time <em>t</em> and just before time <em>t</em>. This would be foreign to Boole and Shannon, and indeed it’s an extension of their theories. It can be used to give meaning to digital logic circuits that their theories can’t handle, such as mapping <a href="ch06.xhtml#ch06fig12">Figure 6-12</a> to <em>X</em><sub><em>t</em></sub> = NOT X<sub><em>t</em>-1</sub> and <a href="ch06.xhtml#ch06fig13">Figure 6-13</a> to Q<sub><em>t</em></sub> = G OR Q<sub><em>t</em>-1</sub>. The latter is now an exact analog of the guitar feedback memory, with Q able to sustain a value of 1 copied from G even if G is later lowered to 0.</p>&#13;
<p class="indent">This still isn’t a very useful memory, because once Q has been set high there’s no way to reset it to low again. We need to add the equivalent of the amplifier power switch, A, as in <a href="ch06.xhtml#ch06fig14">Figure 6-14</a>.</p>&#13;
<div class="image"><img id="ch06fig14" src="../images/f0146-02.jpg" alt="Image" width="445" height="163"/></div>&#13;
<p class="figcap"><em>Figure 6-14: A guitar-and-amp-like feedback circuit</em></p>&#13;
<p class="indent">The <em>SR flip-flop</em> of <a href="ch06.xhtml#ch06fig15">Figure 6-15</a> is a variation on this idea made from two NAND gates, the most common universal gate.</p>&#13;
<div class="image"><img id="ch06fig15" src="../images/f0146-03.jpg" alt="Image" width="385" height="270"/></div>&#13;
<p class="figcap"><em>Figure 6-15: An SR flip-flop</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_147"/>S and R stand for <em>set</em> and <em>reset</em>. When S is high, it sets the output Q to 1. When R is high, it resets the output Q to 0. (This also has the advantage of making NOT Q available on the Q<sup>′</sup> output as a free by-product, which is sometimes useful.)</p>&#13;
<h3 class="h3" id="lev130">Clocked Logic</h3>&#13;
<p class="noindent">Sequential logic behavior can be unpredictable if we don’t have a clearly defined, discrete signal telling us when <em>t</em> has changed to <em>t</em> + 1. This can be done with a clock signal, traditionally called <em>clk</em>, that steadily oscillates between 0 and 1, as discussed in <a href="ch04.xhtml">Chapter 4</a>.</p>&#13;
<p class="indent">By tradition, the instant of the rising edge of clk is used as the instant that <em>t</em> increases by one; this is called a <em>tick</em>. We then design the temporal parts of our circuits to update their state at each tick. Copies of clk can be wired into many points across the system to make them all update simultaneously on each tick.</p>&#13;
<p class="indent">As with the combinatorial logic section, we’ll now walk through a series of clocked logic machines.</p>&#13;
<h4 class="h4" id="lev131"><em>Clocked Flip-Flops</em></h4>&#13;
<p class="noindent">Most sequential simple machines can be converted to clocked form by adding gates that AND their inputs with a clock signal. <a href="ch06.xhtml#ch06fig16">Figure 6-16</a> shows how to extend an SR flip-flop in this way.</p>&#13;
<div class="image"><img id="ch06fig16" src="../images/f0147-01.jpg" alt="Image" width="610" height="271"/></div>&#13;
<p class="figcap"><em>Figure 6-16: A clocked SR flip-flop</em></p>&#13;
<p class="indent">Only a single tick of high signal is needed in S or R to flip the state of the memory, which is then retained over time until a new S or R signal is received. Changes occur only during a clock tick, as the AND gates on the clock act to disable the S and R inputs at other times.</p>&#13;
<p class="indent">Clocked versions of simple machines are drawn with the clock input marked with a triangle, as in <a href="ch06.xhtml#ch06fig17">Figure 6-17</a>.</p>&#13;
<div class="image"><img id="ch06fig17" src="../images/f0147-02.jpg" alt="Image" width="121" height="122"/></div>&#13;
<p class="figcap"><em>Figure 6-17: The symbol for a clocked SR flip-flop</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_148"/>SR is the simplest type of flip-flop to understand, and for that reason it’s generally used to introduce the concept, but SR flip-flops aren’t typically used in practice. This is because they have undesirable, undefined behavior in cases where both inputs are 1. The <em>D-type flip-flop</em> has a modified design that fixes this issue; it’s widely used in practice. Unlike SR, it uses an inherently clock-based approach.</p>&#13;
<p class="indent">A D-type flip-flop (D for <em>data</em>) has only one data input and a clock input. At one point of the clock cycle, such as the rising edge, it captures the data on the D input. For the rest of the clock cycle, it outputs that value on its output Q. This stores the data for only one clock cycle—if you want to keep it for longer, you need to arrange external connections so that D<sub><em>t</em> +1</sub> = Q<sub><em>t</em></sub>. One of many possible implementations of a D-type flip-flop is shown in <a href="ch06.xhtml#ch06fig18">Figure 6-18</a>.</p>&#13;
<div class="image"><img id="ch06fig18" src="../images/f0148-01.jpg" alt="Image" width="688" height="339"/></div>&#13;
<p class="figcap"><em>Figure 6-18: A D-type flip-flop</em></p>&#13;
<p class="indent">The standard D-type flip-flop symbol is shown in <a href="ch06.xhtml#ch06fig19">Figure 6-19</a>.</p>&#13;
<div class="image"><img id="ch06fig19" src="../images/f0148-02.jpg" alt="Image" width="120" height="120"/></div>&#13;
<p class="figcap"><em>Figure 6-19: The D-type flip-flop symbol</em></p>&#13;
<p class="indent">Here, the standard triangle symbol is used for the clock input, and the negated output is shown by a circle, as used in NAND and NOR gate symbols.</p>&#13;
<h4 class="h4" id="lev132"><em>Counters</em></h4>&#13;
<p class="noindent">A <em>counter</em> is a digital logic version of Pascal’s calculator. We use a D-type flip-flop to store the value in each column, and wire its output to both its own data input (to refresh the storage) and also to the clock input of the <em>next</em> column’s flip-flop as a carry. This is shown in <a href="ch06.xhtml#ch06fig20">Figure 6-20</a>.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_149"/><img id="ch06fig20" src="../images/f0149-01.jpg" alt="Image" width="801" height="194"/></div>&#13;
<p class="figcap"><em>Figure 6-20: A 4-bit binary counter</em></p>&#13;
<p class="indent">If the input to the first column is a clock, then the counter will count the number of ticks that have taken place. If you take an output wire from one of the columns of the counter, you get a clock divider, which drops the clock frequency by a power of two. This is useful when you have a fast clock and want to create a slower clock from it, for example to use as a clock for slower pieces of hardware.</p>&#13;
<p class="indent">Alternatively, the input to the first column can be any arbitrary signal, such as a wire from a manual-controlled switch or some other event in a digital circuit, in which case the counter will count the number of these events that have taken place.</p>&#13;
<h4 class="h4" id="lev133"><em>Sequencers</em></h4>&#13;
<p class="noindent">A <em>sequencer</em> is a device that triggers a bunch of other devices at particular times. For example, a traffic light sequencer will turn on and off the different colored lights in a particular, repeating order. A sequencer can be made from a counter and a decoder, as in <a href="ch06.xhtml#ch06fig21">Figure 6-21</a>, which simply uses the counter’s output as an input to the decoder.</p>&#13;
<div class="image"><img id="ch06fig21" src="../images/f0149-02.jpg" alt="Image" width="988" height="591"/></div>&#13;
<p class="figcap"><em>Figure 6-21: An eight-state sequencer using a 3-bit counter and decoder</em></p>&#13;
<h4 class="h4" id="lev134"><span epub:type="pagebreak" id="page_150"/><em>Random-Access Memory</em></h4>&#13;
<p class="noindent"><em>Random-access memory (RAM)</em> is memory that consists of addresses, each containing a group of bits of data known as a <em>word</em>, and in which any address can be read and written at equal time cost. Babbage’s Analytical Engine features a mechanical RAM; let’s see how to build the same structure from digital logic as a simple machine.</p>&#13;
<p class="indent">Basic RAM has three groups of wires as its interface. First, <em>N</em> address wires carry a binary natural number representation specifying which of 2<sup><em>N</em></sup> addresses is of interest. Each address stores a word of length <em>M</em> so, second, a group of <em>M</em> data wires carry copies of words to or from the specified address of the RAM. Finally, a single control wire, called <em>write</em>, carries a single bit that controls whether the specified address is to be read or written.</p>&#13;
<p class="indent"><a href="ch06.xhtml#ch06fig22">Figure 6-22</a> shows a (toy-sized) RAM with <em>N</em> = 2 and <em>M</em> = 2. The address wires are labeled A0 and A1, and the data wires D0 and D1.</p>&#13;
<div class="image"><img id="ch06fig22" src="../images/f0150-01.jpg" alt="Image" width="1108" height="507"/></div>&#13;
<p class="figcap"><em>Figure 6-22: A simple RAM, with addressed words implemented as flip-flops. This toy example has a 2-bit address space of 2-bit words.</em></p>&#13;
<p class="indent">Each of the 2<sup>2</sup> = 4 addresses stores a 2-bit word. Each bit of each word is stored by a D-type flip-flop. The selection of address from the address wires is performed using a decoder.</p>&#13;
<div class="sidebar">&#13;
<p class="stitle"><strong>HARDWARE DESCRIPTION LANGUAGES</strong></p>&#13;
<p class="stext">The tools used in this book are focused on LogiSim and simulation. Large-scale architecture is, however, usually done via a stack of text-based languages such as netlists, Verilog, and Chisel. Let’s take a brief look at these formats in case you max out LogiSim and want to explore larger and more complex designs in your own projects.</p>&#13;
<p class="stextd"><span epub:type="pagebreak" id="page_151"/><strong>Mask Files</strong></p>&#13;
<p class="stext"><em>Mask files</em> are the very lowest level of chip description, containing the physical locations, sizes, and shapes of components such as transistors and wires. These are used to produce the masks needed for fabrication.</p>&#13;
<p class="stextd"><strong>Netlist Files</strong></p>&#13;
<p class="stext"><em>Netlist files</em> contain descriptions of connectivity between physical components and wires, but as abstract connectivity rather than a physical layout. You use a layout engine program to <em>place and route</em> the connections—that is, to transform a netlist file into a mask file. (This is an NP-hard problem, so layout programs use complex heuristics that were until recently closely guarded commercial secrets.)</p>&#13;
<p class="stextd"><strong>Verilog and VHDL Files</strong></p>&#13;
<p class="stext"><em>Verilog</em> and <em>VHDL</em> are text-based hardware description languages for designing electronic systems. In their most basic forms, they have a similar function to LogiSim, allowing you to instantiate and connect various electronic components. But instead of using a GUI, they use text files with a syntax similar to software programming languages. Unlike a language like C, however, which is imperative, Verilog and VHDL fundamentally describe static objects and relationships between them. In this sense, their structure is more like XML or a database, containing lists of facts rather than instructions to <em>do</em> things. For example, here’s a Verilog module representing a full adder:</p>&#13;
<pre>module FullAdder( input io_a,&#13;
                  input io_b,&#13;
                  input io_cin,&#13;
                  output io_sum,&#13;
                  output io_cout&#13;
                );&#13;
&#13;
  assign io_sum = io_a ^ io_b ^ io_cin;&#13;
  assign io_cout = io_a &amp; io_b | io_a &amp; io_cin | io_b &amp; io_cin;&#13;
endmodule</pre>&#13;
<p class="stext">Once you write a Verilog or VHDL description, a compiler turns it into a netlist. This compilation process is called <em>synthesis</em> because the logic expressed in the source code is synthesized from gates. Software simulators also exist that can be used to test Verilog or VHDL hardware designs without actually manufacturing the hardware.</p>&#13;
<p class="stext">While some people still write Verilog or VHDL by hand to design digital logic, it’s becoming more common to use higher-level tools such as LogiSim or Chisel (discussed next) that compile into Verilog or VHDL. Verilog also adds higher-level language constructions that enable some C-like imperative programming and get compiled to digital logic structures. LogiSim Evolution is able to export your designs as Verilog or VHDL, which enables you to compile them to netlists and use them to make real chips.</p>&#13;
<p class="stextd"><span epub:type="pagebreak" id="page_152"/><strong>Chisel</strong></p>&#13;
<p class="stext"><em>Chisel</em> is a high-level hardware language that was developed for general architecture design use. Chisel describes classes of hardware with object orientation; for example, you could create a <span class="literal">FullAdder</span> class to represent the class of full adders, which could be abstracted and inherited in the usual high-level object-oriented ways:</p>&#13;
<pre>class FullAdder extends Module {&#13;
  val io = IO(new Bundle {&#13;
    val a = Input(UInt(2.W))&#13;
    val b = Input(UInt(2.W))&#13;
    val cin = Input(UInt(2.W))&#13;
    val sum = Output(UInt(2.W))&#13;
    val cout = Output(UInt(2.W))&#13;
  })&#13;
  // Generate the sum&#13;
  val a_xor_b = io.a ^ io.b&#13;
  io.sum := a_xor_b ^ io.cin&#13;
  // Generate the carry&#13;
  val a_and_b = io.a &amp; io.b&#13;
  val b_and_cin = io.b &amp; io.cin&#13;
  val a_and_cin = io.a &amp; io.cin&#13;
  io.cout := a_and_b | b_and_cin | a_and_cin&#13;
}</pre>&#13;
<p class="stext">Chisel classes may have parameters for numbers of input and output wires, for example, to enable loops to generate <em>N</em> full adders to make a ripple adder.</p>&#13;
<p class="stext">Chisel is a hardware language, but it’s based closely on the very high-level Scala software language. Scala, in turn, is influenced heavily by lambda calculus, functional programming, and Java; these kinds of languages are not usually associated with hardware design, so bringing them in has enabled Chisel to operate at much higher levels than the old days of having to do hardware design in Verilog. You may benefit from taking regular Scala tutorials before attempting to work with Chisel.</p>&#13;
</div>&#13;
<h3 class="h3" id="lev135">Summary</h3>&#13;
<p class="noindent">Logic gates can be combined into networks to perform more complex functions. Simple machines are certain well-known types of networks that tend to appear again and again in architecture. Combinatorial logic machines—including shifters, encoders, multiplexers, and adders—use Shannon’s original theory, without relying on feedback or time. When feedback and clocks are also allowed, additional sequential and clocked logic simple machines can be created as well. These are able to retain data in memory over time. Flip-flops are simple machines storing 1 bit of memory. They can function as subcomponents of counters, sequencers, and RAM.</p>&#13;
<p class="indent">Now that we have a collection of simple machines, we can combine them in the next chapter to build a digital logic CPU.</p>&#13;
<h3 class="h3" id="lev136"><span epub:type="pagebreak" id="page_153"/>Exercises</h3>&#13;
<h4 class="h4a"><strong>Building Simple Machines in LogiSim Evolution</strong></h4>&#13;
<p class="noindent">As you work on the following exercises, keep in mind that you can create hierarchies of subcircuits in LogiSim. You might do this, for example, so that your shifter becomes available as a single component to use in higher-level networks. To create a subcircuit, click the <strong>+</strong> button. Then, to use the new component, go back to the main circuit and add it like any other component. Use pins for input and output inside the subcircuit if you want them to show in the external interface in the main circuit.</p>&#13;
<ol class="number">&#13;
<li class="tm">Build the left-shifter (<a href="ch06.xhtml#ch06fig3">Figure 6-3</a>), decoder (<a href="ch06.xhtml#ch06fig4">Figure 6-4</a>), and multiplexer (<a href="ch06.xhtml#ch06fig5">Figure 6-5</a>) shown earlier in this chapter.</li>&#13;
<li class="tm">Design and build a right-shifter, encoder, and demultiplexer. These perform the inverse functions of the left-shifter, decoder, and multiplexer, respectively.</li>&#13;
<li class="tm">Build and test an 8-bit ripple-carry adder. Use it to perform subtraction and addition, using two’s complement.</li>&#13;
<li class="tm">Build and test unclocked and clocked SR flip-flops, and a D-type flip-flop.</li>&#13;
<li class="tm">Build and test a counter, using a clock as its input.</li>&#13;
<li class="tm">Build a traffic light sequencer from a 2-bit counter and decoder. Use it to light red, amber, and green bulbs in the UK’s standard sequence, which goes: (1) red (stop); (2) red and amber together (get ready to go); (3) green (go); (4) amber (get ready to stop). This is roughly how the control unit of a CPU works.</li>&#13;
</ol>&#13;
<h4 class="h4a"><strong>Prebuilt LogiSim Modules</strong></h4>&#13;
<p class="noindent">LogiSim has prebuilt modules for many simple machines. For example, there’s a prebuilt RAM module found under Memory in the menu, as in <a href="ch06.xhtml#ch06fig23">Figure 6-23</a>.</p>&#13;
<div class="image"><img id="ch06fig23" src="../images/f0153-01.jpg" alt="Image" width="1116" height="351"/></div>&#13;
<p class="figcap"><em>Figure 6-23: An eight-address, 2-bit word RAM</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_154"/>This version has two control inputs, one for write enable and one for read enable. A NOT gate is used in the figure to create both from a single control line.</p>&#13;
<ol class="number">&#13;
<li class="tm">Explore LogiSim’s prebuilt modules that correspond to the machines you implemented in the previous exercises. Check that they give the same results as your own implementations.</li>&#13;
<li class="tm">Explore the RAM module shown in <a href="ch06.xhtml#ch06fig23">Figure 6-23</a>. Use the module options to specify the RAM’s word and address lengths. You can manually edit the RAM’s contents with a built-in hex editor by right-clicking and then clicking Edit Contents. A splitter, found in the Wiring menu, is used to bundle and unbundle groups of wires for data and address. A probe or LEDs can be used for output; constants, DIP switches, or pins can be used for input.</li>&#13;
</ol>&#13;
<h4 class="h4a"><strong>Challenging</strong></h4>&#13;
<ol class="number">&#13;
<li class="tm">Design and build a natural number multiplier in LogiSim. This can be done by following the usual multiplication algorithm that you were taught at school, but in binary. You can use shifters to multiply one of the inputs by all the different powers of two, then adders to add together those powers that are present in the second number. Use AND gates to enable and disable the relevant powers. As is often the case in architecture, you can choose whether to use multiple silicon copies of the required structures, or use a single copy plus timing logic to run it many times.</li>&#13;
<li class="tm">Extend your multiplier to work with negative integers, using two’s complement data representation.</li>&#13;
</ol>&#13;
<h4 class="h4a"><strong>More Challenging</strong></h4>&#13;
<p class="noindent">Design, build, and test an 8-bit carry-save adder in LogiSim. How much more efficient is it than a ripple-carry adder?</p>&#13;
<h3 class="h3" id="lev137">Further Reading</h3>&#13;
<p class="noindent">For full details on how to use LogiSim, including advanced features, see George Self, <em>LogiSim Evolution Lab Manual</em> (July 2019), <em><a href="https://www.icochise.com/docs/logisim.pdf">https://www.icochise.com/docs/logisim.pdf</a></em>.</p>&#13;
</div>
</div>
</body></html>