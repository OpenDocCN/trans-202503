<html><head></head><body>
<h2 class="h2" id="ch16"><span epub:type="pagebreak" id="page_265"/><span class="big">16</span><br/>CONFIGURATION AND SECRETS</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">Any high-quality application is designed so that key configuration items can be injected at runtime rather than being embedded in the source code. When we move our application components to containers, we need a way to tell the container runtime what configuration information to inject to ensure that our application components behave the way they should.</p>&#13;
<p class="indent">Kubernetes provides two primary resource types for injecting this configuration information: ConfigMap and Secret. These two resources are very similar in capability but have slightly different use cases.</p>&#13;
<h3 class="h3" id="ch00lev1sec66">Injecting Configuration</h3>&#13;
<p class="noindent">When we looked at container runtimes in <a href="part01.xhtml#part01">Part I</a>, we saw that we could pass environment variables to our containers. Of course, as Kubernetes manages the container runtime for us, we’ll first need to pass that information to Kubernetes, which will then pass it to the container runtime for us.</p>&#13;
<div class="note">&#13;
<p class="notet"><span epub:type="pagebreak" id="page_266"/><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on getting set up.</em></p>&#13;
</div>&#13;
<p class="indent">For simple configuration injection, we can provide environment variables directly from the Pod specification. We saw an example of this in Pod form when we created a PostgreSQL server in <a href="ch10.xhtml#ch10">Chapter 10</a>. Here’s a PostgreSQL Deployment with a similar configuration in its embedded Pod specification:</p>&#13;
<p class="noindent6"><em>pgsql.yaml</em></p>&#13;
<pre>---&#13;
kind: Deployment&#13;
apiVersion: apps/v1&#13;
metadata:&#13;
  name: postgres&#13;
spec:&#13;
  replicas: 1&#13;
  selector:&#13;
    matchLabels:&#13;
      app: postgres&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: postgres&#13;
    spec:&#13;
      containers:&#13;
      - name: postgres&#13;
        image: postgres&#13;
        env:&#13;
        - name: POSTGRES_PASSWORD&#13;
          value: "supersecret"</pre>&#13;
<p class="indent">When we provide environment variables directly in the Deployment, those environment variables are stored directly in the YAML file and in the cluster’s configuration for that Deployment. There are two important problems with embedding environment variables in this manner. First, we’re reducing flexibility because we can’t specify a new value for the environment variable without changing the Deployment YAML file. Second, the password is visible in plaintext directly in the Deployment YAML file. YAML files are often checked in to source control, so we’re going to have a hard time adequately protecting the password.</p>&#13;
<div class="box5">&#13;
<p class="boxtitle-d"><span epub:type="pagebreak" id="page_267"/><strong>GITOPS</strong></p>&#13;
<p class="noindents">The reason that the YAML files that define Kubernetes resources are often checked in to source control is that this is by far the best way to manage an application deployment. GitOps is a best practice by which all configuration is kept in a Git repository. This includes the cluster configuration, additional infrastructure components including load balancers, ingress controller, and storage plug-ins, as well as all of the information to build, assemble, and deploy applications. GitOps provides a log of changes to the cluster configuration, avoids configuration drift that can occur over time, and ensures consistency between development, test, and production environments. Not only that, but GitOps tools like FluxCD and ArgoCD can be used to watch changes to a Git repository and automatically pull the latest configuration to update a cluster.</p>&#13;
</div>&#13;
<p class="indent">Let’s first look at moving the configuration out of the Deployment; then we’ll consider how best to protect the password.</p>&#13;
<h4 class="h4" id="ch00lev2sec99">Externalizing Configuration</h4>&#13;
<p class="noindent">Embedding configuration in the Deployment makes the resource definition less reusable. If, for example, we wanted to deploy a PostgreSQL server for both test and production versions of our application, it would be useful to reuse the same Deployment to avoid duplication and to avoid configuration drift between the two versions. However, for security, we would not want to use the same password in both environments.</p>&#13;
<p class="indent">It’s better if we externalize the configuration by storing it in a separate resource and referring to it from the Deployment. To enable this, Kubernetes offers the <em>ConfigMap</em> resource. A ConfigMap specifies a set of key–value pairs that can be referenced when specifying a Pod. For example, we can define our PostgreSQL configuration this way:</p>&#13;
<p class="noindent6"><em>pgsql-cm.yaml</em></p>&#13;
<pre>---&#13;
kind: ConfigMap&#13;
apiVersion: v1&#13;
metadata:&#13;
  name: pgsql&#13;
data:&#13;
  POSTGRES_PASSWORD: "supersecret"</pre>&#13;
<p class="indent">By storing this configuration information in a ConfigMap, it is no longer directly part of the Deployment YAML file or the cluster configuration for the Deployment.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_268"/>After we’ve defined our ConfigMap, we can reference it in our Deployment, as demonstrated in <a href="ch16.xhtml#ch16list1">Listing 16-1</a>.</p>&#13;
<p class="noindent6"><em>pgsql-ext-cfg.yaml</em></p>&#13;
<pre>---&#13;
kind: Deployment&#13;
apiVersion: apps/v1&#13;
metadata:&#13;
  name: postgres&#13;
spec:&#13;
  replicas: 1&#13;
  selector:&#13;
    matchLabels:&#13;
      app: postgres&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: postgres&#13;
    spec:&#13;
      containers:&#13;
      - name: postgres&#13;
        image: postgres&#13;
        envFrom:&#13;
        - configMapRef:&#13;
            name: pgsql</pre>&#13;
<p class="caption" id="ch16list1"><em>Listing 16-1: PostgreSQL with ConfigMap</em></p>&#13;
<p class="indent">In place of the <span class="literal">env</span> field, we have an <span class="literal">envFrom</span> field that specifies one or more ConfigMaps to serve as environment variables for the container. All of the key–value pairs in the ConfigMap will become environment variables.</p>&#13;
<p class="indent">This has the same effect as specifying one or more environment variables directly in the Deployment, but our Deployment specification is now reusable. The Deployment will look for the identified ConfigMap in its own Namespace, so we can have multiple Deployments from the same specification in separate Namespaces, and each can be configured differently.</p>&#13;
<p class="indent">This use of Namespace isolation to prevent naming conflicts, together with the Namespace-scoped security controls we saw in <a href="ch11.xhtml#ch11">Chapter 11</a> and the Namespace-scoped quotas we saw in <a href="ch14.xhtml#ch14">Chapter 14</a>, allows a single cluster to be used for many different purposes, by many different groups, a concept known as <em>multitenancy</em>.</p>&#13;
<p class="indent">Let’s create this Deployment and see how Kubernetes injects the configuration. First, let’s create the actual Deployment:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/pgsql-ext-cfg.yaml</span> &#13;
deployment.apps/postgres created</pre>&#13;
<p class="indent">This command completes successfully because the Deployment has been created in the cluster, but Kubernetes will not be able to start any Pods because the ConfigMap is missing:</p>&#13;
<pre><span epub:type="pagebreak" id="page_269"/>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                       READY  STATUS                      RESTARTS  AGE&#13;
postgres-6bf595fcbc-s8dqz  0/1    CreateContainerConfigError  0         53s</pre>&#13;
<p class="indent">If we now create the ConfigMap, we see that the Pod is then created:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/pgsql-cm.yaml</span> &#13;
configmap/pgsql created&#13;
root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                        READY   STATUS    RESTARTS   AGE&#13;
postgres-6bf595fcbc-s8dqz   1/1     Running   0          2m41s</pre>&#13;
<p class="indent">It can take a minute or so for Kubernetes to determine that the ConfigMap is available and start the Pod. As soon as the Pod is running, we can verify that the environment variables were injected based on the data in the ConfigMap:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti <span class="codeitalic1">postgres-6bf595fcbc-s8dqz</span> -- /bin/sh -c env</span>&#13;
...&#13;
POSTGRES_PASSWORD=supersecret&#13;
...</pre>&#13;
<p class="indent">The command <span class="literal">env</span> prints out all of the environment variables associated with a process. Because Kubernetes provides the same environment variables to our <span class="literal">/bin/sh</span> process as it provided to our main PostgreSQL process, we know that the environment variable was set as expected. It’s important to note, however, that even though we can change the ConfigMap at any time, doing so will not cause the Deployment to update its Pods; the application will not automatically pick up any environment variable changes. Instead, we need to apply some configuration change to the Deployment to cause it to create new Pods.</p>&#13;
<p class="indent">Although the configuration has been externalized, we still are not protecting it. Let’s do that next.</p>&#13;
<h4 class="h4" id="ch00lev2sec100">Protecting Secrets</h4>&#13;
<p class="noindent">When protecting secrets, thinking through the nature of the protection that makes sense is important. For example, we might need to protect authentication information that our application uses to connect to a database. However, given that the application itself needs that information to make the connection, anyone who can inspect the inner details of the application is going to be able to extract those credentials.</p>&#13;
<p class="indent">As we saw in <a href="ch11.xhtml#ch11">Chapter 11</a>, Kubernetes provides fine-grained access control over each individual resource type in a given Namespace. To enable protection of secrets, Kubernetes provides a separate resource type, <em>Secret</em>. This way, access to secrets can be limited to only those users who require access, a principle known as <em>least privilege</em>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_270"/>One more advantage to the Secret resource type is that it uses base64 encoding for all of its data, with automatic decoding when the data is provided to the Pod, which simplifies the storage of binary data.</p>&#13;
<div class="box5">&#13;
<p class="boxtitle-d"><strong>ENCRYPTING SECRET DATA</strong></p>&#13;
<p class="noindents">By default, data stored in a Secret is base64 encoded but is not encrypted. It is possible to encrypt secret data, and doing so is good practice for a production cluster, but remember that the data must be decrypted so that it can be provided to the Pod. For this reason, anyone who can control what Pods exist in a namespace can access secret data, as can any cluster administrators who can access the underlying container runtime. This is true even if the secret data is encrypted when stored. Proper access controls are essential to keep a cluster secure.</p>&#13;
</div>&#13;
<p class="indent">A Secret definition looks almost identical to a ConfigMap definition:</p>&#13;
<p class="noindent6"><em>pgsql-secret.yaml</em></p>&#13;
<pre>---&#13;
kind: Secret&#13;
apiVersion: v1&#13;
metadata:&#13;
  name: pgsql&#13;
stringData:&#13;
  POSTGRES_PASSWORD: "supersecret"</pre>&#13;
<p class="indent">The one obvious difference is the resource type of Secret rather than ConfigMap. However, there is a subtle difference as well. When we define this Secret, we place the key–value pairs in a field called <span class="literal">stringData</span> rather than just <span class="literal">data</span>. This tells Kubernetes that we are providing unencoded strings. When it creates the Secret, Kubernetes will encode the strings for us:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/pgsql-secret.yaml</span> &#13;
secret/pgsql created&#13;
root@host01:~# <span class="codestrong1">kubectl get secret pgsql -o json | jq .data</span>&#13;
{&#13;
  "POSTGRES_PASSWORD": "c3VwZXJzZWNyZXQ="&#13;
}</pre>&#13;
<p class="indent">Even though we specified the data using the field <span class="literal">stringData</span> and an unencoded string, the actual Secret uses the field <span class="literal">data</span> and stores the value using base64 encoding. We can also do the base64 encoding ourselves. In that case, we place the value directly into the <span class="literal">data</span> field:</p>&#13;
<p class="noindent6"><em>pgsql-secret-2.yaml</em></p>&#13;
<pre>---&#13;
kind: Secret&#13;
apiVersion: v1&#13;
metadata:&#13;
  name: pgsql&#13;
<span epub:type="pagebreak" id="page_271"/>data:&#13;
  POSTGRES_PASSWORD: c3VwZXJzZWNyZXQ=</pre>&#13;
<p class="indent">This approach is necessary to define binary content for the Secret in order for us to be able to supply that binary content as part of a YAML resource definition.</p>&#13;
<p class="indent">We use a Secret in a Deployment definition in exactly the same way we use a ConfigMap:</p>&#13;
<p class="noindent6"><em>pgsql-ext-sec.yaml</em></p>&#13;
<pre>---&#13;
kind: Deployment&#13;
apiVersion: apps/v1&#13;
metadata:&#13;
  name: postgres&#13;
spec:&#13;
  replicas: 1&#13;
  selector:&#13;
    matchLabels:&#13;
      app: postgres&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: postgres&#13;
    spec:&#13;
      containers:&#13;
      - name: postgres&#13;
        image: postgres&#13;
        envFrom:&#13;
        - secretRef:&#13;
            name: pgsql</pre>&#13;
<p class="indent">The only change is the use of <span class="literal">secretRef</span> in place of <span class="literal">configMapRef</span>.</p>&#13;
<p class="indent">To test this, let’s apply this new Deployment configuration:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/pgsql-ext-sec.yaml</span> &#13;
deployment.apps/postgres configured</pre>&#13;
<p class="indent">From the perspective of our Pod, the behavior is exactly the same. Kubernetes handles the base64 decoding, making the decoded value visible to our Pod:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                        READY   STATUS        RESTARTS   AGE&#13;
postgres-6bf595fcbc-s8dqz   1/1     Terminating   0          12m&#13;
postgres-794ff85bbf-xzz49   1/1     Running       0          26s&#13;
root@host01:~# <span class="codestrong1">kubectl exec -ti <span class="codeitalic1">postgres-794ff85bbf-xzz49</span> -- /bin/sh -c env</span>&#13;
...&#13;
POSTGRES_PASSWORD=supersecret&#13;
...</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_272"/>As before, we use the <span class="literal">env</span> command to show that the <span class="literal">POSTGRES_PASSWORD</span> environment variable was set as expected. The Pod sees the same behavior whether we specify the environment variable directly or use a ConfigMap or Secret.</p>&#13;
<p class="indent">Before we move on, let’s delete this Deployment:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete deploy postgres</span>&#13;
deployment.apps "postgres" deleted</pre>&#13;
<p class="indent">Using ConfigMaps and Secrets, we have the ability to externalize environment variable configuration for our application so that our Deployment specification can be reusable and to facilitate fine-grained access control over secret data.</p>&#13;
<h3 class="h3" id="ch00lev1sec67">Injecting Files</h3>&#13;
<p class="noindent">Of course, environment variables are not the only way we commonly configure applications. We also need a way to provide configuration files. We can do that using the same ConfigMap and Secret resources we’ve seen already.</p>&#13;
<p class="indent">Any files we inject in this way override files that exist in the container image, which means that we can supply the container image with a sensible default configuration and then override that configuration with each container we run. This makes it much easier to reuse container images.</p>&#13;
<p class="indent">The ability to specify file content in a ConfigMap and then mount it in a container is immediately useful for configuration files, but we can also use it to update the NGINX web server example we showed in <a href="ch15.xhtml#ch15">Chapter 15</a>. As we’ll see, with this version we can declare our HTML content solely using Kubernetes resource YAML files, with no need for console commands to copy content into a PersistentVolume.</p>&#13;
<p class="indent">The first step is to define a ConfigMap with the HTML content we want to serve:</p>&#13;
<p class="noindent6"><em>nginx-cm.yaml</em></p>&#13;
<pre>---&#13;
kind: ConfigMap&#13;
apiVersion: v1&#13;
metadata:&#13;
  name: nginx&#13;
data:&#13;
  index.html: |&#13;
    &lt;html&gt;&#13;
      &lt;head&gt;&#13;
        &lt;title&gt;Hello, World&lt;/title&gt;&#13;
      &lt;/head&gt;&#13;
      &lt;body&gt;&#13;
        &lt;h1&gt;Hello, World from a ConfigMap!&lt;/h1&gt;&#13;
      &lt;/body&gt;&#13;
    &lt;/html&gt;</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_273"/>The key part of the key–value pair is used to specify the desired filename, in this case <em>index.html</em>. For ease of reading, we use a pipe character (<span class="literal">|</span>) to start a YAML multiline string. This string continues as long as the following lines are indented, or until the end of the YAML file. We can define multiple files in this way by just adding more keys to the ConfigMap.</p>&#13;
<p class="indent">In the Deployment we saw in <a href="ch16.xhtml#ch16list1">Listing 16-1</a>, we specified the ConfigMap as the source of environment variables. Here, we specify it as the source of a volume mount:</p>&#13;
<p class="noindent6"><em>nginx-deploy.yaml</em></p>&#13;
<pre>---&#13;
kind: Deployment&#13;
apiVersion: apps/v1&#13;
metadata:&#13;
  name: nginx&#13;
spec:&#13;
  replicas: 1&#13;
  selector:&#13;
    matchLabels:&#13;
      app: nginx&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: nginx&#13;
    spec:&#13;
      containers:&#13;
      - name: nginx&#13;
        image: nginx&#13;
        volumeMounts:&#13;
        - name: nginx-files&#13;
          mountPath: /usr/share/nginx/html&#13;
      volumes:&#13;
        - name: nginx-files&#13;
          configMap:&#13;
            name: nginx</pre>&#13;
<p class="indent">This volume definition looks similar to the one we saw in <a href="ch15.xhtml#ch15">Chapter 15</a>. As before, the volume specification comes in two parts. The <span class="literal">volume</span> field specifies where the volume comes from, in this case the ConfigMap. The <span class="literal">volumeMounts</span> allows us to specify the path in the container where the files should be made available. In addition to making it possible to use the same volume in multiple containers in a Pod, this also means that we can share the same syntax when mounting persistent volumes and when mounting the configuration as files in the container filesystem.</p>&#13;
<p class="indent">Let’s create the ConfigMap and then get this Deployment started:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/nginx-cm.yaml</span> &#13;
configmap/nginx created&#13;
<span epub:type="pagebreak" id="page_274"/>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/nginx-deploy.yaml</span>&#13;
deployment.apps/nginx created</pre>&#13;
<p class="indent">After the Pod is running, we can see that the file content is as expected, and NGINX is serving our HTML file:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">IP=$(kubectl get po -l app=nginx -o jsonpath='{..podIP}')</span>&#13;
root@host01:~# <span class="codestrong1">curl http://$IP</span>&#13;
&lt;html&gt;&#13;
  &lt;head&gt;&#13;
    &lt;title&gt;Hello, World&lt;/title&gt;&#13;
  &lt;/head&gt;&#13;
  &lt;body&gt;&#13;
    &lt;h1&gt;Hello, World from a ConfigMap!&lt;/h1&gt;&#13;
  &lt;/body&gt;&#13;
&lt;/html&gt;</pre>&#13;
<p class="indent">The output looks similar to what we saw in <a href="ch15.xhtml#ch15">Chapter 15</a> when we provided the HTML content as a PersistentVolume, but we were able to avoid the effort of attaching the PersistentVolume and then copying content into it. In practice, both approaches have value, as maintaining a ConfigMap with a large amount of data would be unwieldy.</p>&#13;
<p class="indent">To make the contents of the ConfigMap appear as files in a directory, Kubernetes is writing out the contents of the ConfigMap to the host filesystem and then mounting the directory from the host into the container. This means that the specific directory shows up as part of the output for the <span class="literal">mount</span> command inside the container:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti <span class="codeitalic1">nginx-58bc54b5cd-4lbkq</span> -- /bin/mount</span>&#13;
...&#13;
/dev/sda1 on /usr/share/nginx/html type ext4 (ro,relatime)&#13;
...</pre>&#13;
<p class="indent">The <span class="literal">mount</span> command reports that the directory <em>/usr/share/nginx/html</em> is a separately mounted path coming from the host’s primary disk <em>/dev/sda1</em>.</p>&#13;
<p class="indent">We’re finished with the NGINX Deployment, so go ahead and delete it:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete deploy nginx</span>&#13;
deployment.apps "nginx" deleted</pre>&#13;
<p class="indent">Next, let’s look at how ConfigMap and Secret information is stored in a typical Kubernetes cluster so that we can see where <span class="literal">kubelet</span> is getting this content.</p>&#13;
<h3 class="h3" id="ch00lev1sec68">Cluster Configuration Repository</h3>&#13;
<p class="noindent">Although it’s possible to run a Kubernetes cluster with different choices of configuration repository, most Kubernetes clusters use <span class="literal">etcd</span> as the backing store for all cluster configuration data. This includes not only the ConfigMap and Secret storage but also all of the other cluster resources and the <span epub:type="pagebreak" id="page_275"/>current cluster state. Kubernetes also uses <span class="literal">etcd</span> to elect a leader when running in a highly available configuration with multiple API servers.</p>&#13;
<p class="indent">Although <span class="literal">etcd</span> is generally stable and reliable, node failures can lead to cases in which the <span class="literal">etcd</span> cluster can’t reestablish itself and elect a leader. Our purpose in demonstrating <span class="literal">etcd</span> is not just to see how configuration data is stored, but also to provide some valuable background into an essential cluster component that an administrator might need to debug.</p>&#13;
<p class="indent">For all of our example clusters, <span class="literal">etcd</span> is installed on the same nodes as the API server, which is common in smaller clusters. In large clusters, running <span class="literal">etcd</span> on separate nodes to allow it to scale separately from the Kubernetes control plane is common.</p>&#13;
<p class="indent">To explore the contents of the <span class="literal">etcd</span> backing store, we’ll use <span class="literal">etcdctl</span>, a command line client designed for controlling and troubleshooting <span class="literal">etcd</span>.</p>&#13;
<h4 class="h4" id="ch00lev2sec101">Using etcdctl</h4>&#13;
<p class="noindent">We need to tell <span class="literal">etcdctl</span> where our <span class="literal">etcd</span> server instance is located and how to authenticate to it. For authentication, we’ll use the same client certificate that the API server uses.</p>&#13;
<p class="indent">For convenience, we can set environment variables that <span class="literal">etcdctl</span> will read, so we don’t need to pass in those values via the command line with every command.</p>&#13;
<p class="indent">Here are the environment variables we need:</p>&#13;
<p class="noindent6"><em>etcd-env</em></p>&#13;
<pre>export ETCDCTL_API=3&#13;
export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt&#13;
export ETCDCTL_CERT=/etc/kubernetes/pki/apiserver-etcd-client.crt&#13;
export ETCDCTL_KEY=/etc/kubernetes/pki/apiserver-etcd-client.key&#13;
export ETCDCTL_ENDPOINTS=https://192.168.61.11:2379</pre>&#13;
<p class="indent">These variables configure <span class="literal">etcdctl</span> as follows:</p>&#13;
<div class="bqparan">&#13;
<p class="noindent5"><span class="codestrong">ETCDCTL_API</span> Use version 3 of the <span class="literal">etcd</span> API. With recent versions of <span class="literal">etcd</span>, only version 3 is supported.</p>&#13;
<p class="noindent5"><span class="codestrong">ETCDCTL_CACERT</span> Verify the <span class="literal">etcd</span> host using the provided certificate authority.</p>&#13;
<p class="noindent5"><span class="codestrong">ETCDCTL_CERT</span> Authenticate to <span class="literal">etcd</span> using this certificate.</p>&#13;
<p class="noindent5"><span class="codestrong">ETCDCTL_KEY</span> Authenticate to <span class="literal">etcd</span> using this private key.</p>&#13;
<p class="noindent5"><span class="codestrong">ETCDCTL_ENDPOINTS</span> Connect to <span class="literal">etcd</span> at this URL. While <span class="literal">etcd</span> is running on all three nodes, we only need one node to talk to it.</p>&#13;
</div>&#13;
<p class="indent">In our example, these environment variables are conveniently stored in a script in <em>/opt</em> so that we can load them for use with upcoming commands:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">source /opt/etcd-env</span></pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_276"/>We can now use <span class="literal">etcdctl</span> commands to inspect the cluster and the configuration data it’s storing. Let’s begin by listing only the cluster members:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">etcdctl member list</span>&#13;
45a2b6125030fdde, started, host02, https://192.168.61.12:2380, https://192.168.61.12:2379&#13;
91007aab9448ce27, started, host03, https://192.168.61.13:2380, https://192.168.61.13:2379&#13;
bf7b9991d532ba78, started, host01, https://192.168.61.11:2380, https://192.168.61.11:2379</pre>&#13;
<p class="indent">As expected, each of the control plane nodes has an instance of <span class="literal">etcd</span>. For a highly available configuration, we need to run at least three instances, and we need a majority of those instances to be running for the cluster to be healthy. This <span class="literal">etcdctl</span> command is a good first step to determine whether the cluster has any failed nodes.</p>&#13;
<p class="indent">As long as the cluster is healthy, we can store and retrieve data. Within <span class="literal">etcd</span>, information is stored in key–value pairs. Keys are specified as paths in a hierarchy. We can list the paths that have content:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">etcdctl get / --prefix --keys-only</span>&#13;
...&#13;
/registry/configmaps/default/nginx&#13;
/registry/configmaps/default/pgsql&#13;
...&#13;
/registry/secrets/default/pgsql&#13;
...</pre>&#13;
<p class="indent">The <span class="literal">--prefix</span> flag tells <span class="literal">etcdctl</span> to get all keys that start with <span class="literal">/</span>, whereas <span class="literal">--keys-only</span> ensures that we print only the keys to prevent being overwhelmed with data. Still, a lot of information is returned, including all of the various Kubernetes resource types that we’ve described in this book. Also included are the ConfigMaps and Secrets we just created.</p>&#13;
<h4 class="h4" id="ch00lev2sec102">Deciphering Data in etcd</h4>&#13;
<p class="noindent">We can generally rely on Kubernetes to store the correct configuration data in <span class="literal">etcd</span>, and we can rely on <span class="literal">kubectl</span> to see the current cluster configuration. However, it is useful to know how the underlying data store works in case we need to inspect the configuration when the cluster is down or in an anomalous state.</p>&#13;
<p class="indent">To save storage space and bandwidth, both <span class="literal">etcd</span> and Kubernetes use the <span class="literal">protobuf</span> library, a language-neutral binary data format. Because we’re using <span class="literal">etcdctl</span> to retrieve data from <span class="literal">etcd</span>, we can ask it to return data in JSON format, instead; however, that JSON data will include an embedded <span class="literal">protobuf</span> structure with the data from Kubernetes, so we’ll need to decode that as well.</p>&#13;
<p class="indent">Let’s begin by examining the JSON format for a Kubernetes Secret in <span class="literal">etcd</span>. We’ll send the output through <span class="literal">jq</span> for formatting:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">etcdctl -w json get /registry/secrets/default/pgsql | jq</span>&#13;
{&#13;
<span epub:type="pagebreak" id="page_277"/>  "header": {&#13;
...&#13;
  },&#13;
  "kvs": [&#13;
    {&#13;
      "key": "L3JlZ2lzdHJ5L3NlY3JldHMvZGVmYXVsdC9wZ3NxbA==",&#13;
      "create_revision": 14585,&#13;
      "mod_revision": 14585,&#13;
      "version": 1,&#13;
      "value": "azhzAAoMCgJ2MRIGU2..."&#13;
    }&#13;
  ],&#13;
  "count": 1&#13;
}</pre>&#13;
<p class="indent">The <span class="literal">kvs</span> field has the key–value pair that Kubernetes stored for this Secret. The value for the key is a simple base64-encoded string:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">echo $(etcdctl -w json get /registry/secrets/default/pgsql \</span>&#13;
<span class="codestrong1">| jq -r '.kvs[0].key' | base64 -d)</span>&#13;
/registry/secrets/default/pgsql</pre>&#13;
<p class="indent">We use <span class="literal">jq</span> to extract just the key’s value and return it in raw format (without quotes), and then we use <span class="literal">base64</span> to decode the string.</p>&#13;
<p class="indent">Of course, the interesting part of this key–value pair is the value because it contains the actual Kubernetes Secret. Although the value is also base64 encoded, we need to do a bit more detangling to access its information.</p>&#13;
<p class="indent">After we decode the base 64 value, we’ll have a <span class="literal">protobuf</span> message. However, it has a magic prefix that Kubernetes uses to allow for future changes in the storage format. We can see that prefix if we look at the first few bytes of the decoded value:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">etcdctl -w json get /registry/secrets/default/pgsql \</span>&#13;
<span class="codestrong1">| jq -r '.kvs[0].value' | base64 -d | head --bytes=10 | xxd</span>&#13;
00000000: 6b38 7300 0a0c 0a02 7631                 k8s.....v1</pre>&#13;
<p class="indent">We use <span class="literal">head</span> to retrieve the first 10 bytes of the decoded value and then use <span class="literal">xxd</span> to see a hex dump. The first few bytes are <span class="literal">k8s</span> followed by an ASCII null character. The rest of the data, starting with byte 5, is the actual <span class="literal">protobuf</span> message.</p>&#13;
<p class="indent">Let’s run one more command to actually decode the <span class="literal">protobuf</span> message using the <span class="literal">protoc</span> tool:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">etcdctl -w json get /registry/secrets/default/pgsql \</span>&#13;
<span class="codestrong1">| jq -r '.kvs[0].value' | base64 -d | tail --bytes=+5 | protoc --decode_raw</span>&#13;
1 {&#13;
  1: "v1"&#13;
  2: "Secret"&#13;
}&#13;
<span epub:type="pagebreak" id="page_278"/>2 {&#13;
  1 {&#13;
    1: "pgsql"&#13;
    2: ""&#13;
    3: "default"&#13;
    4: ""&#13;
...&#13;
  }&#13;
  2 {&#13;
    1: "POSTGRES_PASSWORD"&#13;
    2: "supersecret"&#13;
  }&#13;
  3: "Opaque"&#13;
}&#13;
...</pre>&#13;
<p class="indent">The <span class="literal">protoc</span> tool is mostly used for generating source code to read and write <span class="literal">protobuf</span> messages, but it’s also handy for message decoding. As we can see, within the <span class="literal">protobuf</span> message is all of the data Kubernetes stores for this Secret, including the resource version and type, the resource name and namespace, and the data. This illustrates, as mentioned earlier, that access to the hosts on which Kubernetes runs provides access to all of the secret data in the cluster. Even if we configured Kubernetes to encrypt data before storing it in <span class="literal">etcd</span>, the encryption keys themselves need to be stored unencrypted in <span class="literal">etcd</span> so that the API server can use them.</p>&#13;
<h3 class="h3" id="ch00lev1sec69">Final Thoughts</h3>&#13;
<p class="noindent">With the ability to provide either environment variables or files to Pods, ConfigMaps and Secrets allow us to externalize the configuration of our containers, which makes it possible to reuse both Kubernetes resource definitions such as Deployments and container images in a variety of applications.</p>&#13;
<p class="indent">At the same time, we need to be aware of how Kubernetes stores this configuration data and how it provides it to containers. Anyone with the right role can access configuration data using <span class="literal">kubectl</span>; anyone with access to the host running the container can access it from the container runtime; and anyone with the right authentication information can access it directly from <span class="literal">etcd</span>. For a production cluster, it’s critical that all of these mechanisms are correctly secured.</p>&#13;
<p class="indent">So far, we’ve seen how Kubernetes stores built-in cluster resource data in <span class="literal">etcd</span>, but Kubernetes can also store any kind of custom resource data we might choose to declare. In the next chapter, we’ll explore how custom resource definitions enable us to add new behavior to a Kubernetes cluster in the form of operators.</p>&#13;
</body></html>