- en: '13'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: THE OPERATING SYSTEM
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The *operating system* (OS) is a special system software layer that sits between
    the computer hardware and application programs running on the computer (see [Figure
    13-1](ch13.xhtml#ch13fig1)). The OS software is persistent on the computer, from
    power-on to power-off. Its primary purpose is to *manage* the underlying hardware
    components to efficiently run program workloads and to make the computer *easy
    to use*.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-1: The OS is special system software between the user and the hardware.
    It manages the computer’s hardware and implements abstractions to make the hardware
    easier to use.*'
  prefs: []
  type: TYPE_NORMAL
- en: One of the ways in which the OS makes the computer hardware easy to use is in
    its support for initiating programs to run on the computer. Consider what happens
    when a user double-clicks an icon or types the name of a program executable at
    a shell prompt (e.g., `./a.out`) to start a program running on the underlying
    system. The OS handles all the details of this operation, such as loading the
    program from disk into RAM and initializing the CPU to start running the program
    instructions; the OS hides from users these types of low-level actions that are
    necessary to run the user’s program on the computer.
  prefs: []
  type: TYPE_NORMAL
- en: One example of how the OS makes efficient use of system resources is by implementing
    *multiprogramming*, which means allowing more than a single program to run on
    the computer at a time. Multiprogramming does not necessarily mean that all the
    programs are running simultaneously on the computer hardware. In fact, the set
    of running programs in the system is typically much larger than the number of
    CPU cores. Instead, it means that the OS shares hardware resources, including
    the CPU, among several programs running in the system. For example, when one program
    needs data that is currently on disk, the OS can put another program on the CPU
    while the first program waits for the data to become available. Without multiprogramming,
    the CPU would sit idle whenever the program running on the computer accesses slower
    hardware devices. To support multiprogramming, the OS needs to implement an abstraction
    of a running program, called a *process*. The process abstraction enables the
    OS to manage the set of multiple programs that are running on the system at any
    given time.
  prefs: []
  type: TYPE_NORMAL
- en: Some example operating systems include Microsoft’s Windows, Apple’s macOS and
    iOS, Oracle’s Solaris, and open-source Unix variants such as OpenBSD and Linux.
    We use Linux examples in this book. However, all of these other general-purpose
    operating systems implement similar functionality, albeit sometimes in different
    ways.
  prefs: []
  type: TYPE_NORMAL
- en: The Kernel
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The term *operating system* is often used to refer to a large set of system-level
    software that performs some kind of resource management and that implements “easy-to-use”
    abstractions of the underlying system. In this chapter, we focus on the operating
    system *kernel*; thus, when we use the term OS alone, we mean the OS kernel.
  prefs: []
  type: TYPE_NORMAL
- en: The OS kernel implements core OS functionality—the functionality necessary for
    any use of the system. This functionality includes managing the computer hardware
    layer to run programs, implementing and managing OS abstractions exported to users
    of the system (e.g., files are an OS abstraction on top of stored data), and implementing
    interfaces to the user applications layer and to the hardware device layer. The
    kernel implements *mechanisms* to enable the hardware to run programs and to implement
    its abstractions such as processes. Mechanisms are the “how” part of OS functionality.
    The kernel also implements *policies* for efficiently managing the computer hardware
    and for governing its abstractions. Policies dictate the “what,” “when,” and “to
    whom” part of OS functionality. For example, a mechanism implements initializing
    the CPU to run instructions from a particular process, and a policy decides which
    process gets to run next on the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'The kernel implements a programming interface for users of the system: the
    *system call interface*. Users and programs interact with the OS through its system
    call interface. For example, if a program wants to know the current time of day,
    it can obtain that information from the OS by invoking the `gettimeofday` system
    call system call.'
  prefs: []
  type: TYPE_NORMAL
- en: The kernel also provides an interface for interacting with hardware devices
    (the *device interface*). Typically, I/O devices such as hard disk drives (HDDs),
    keyboards, and solid-state drives (SSDs) interact with the kernel through this
    interface. These devices come with special device driver software that runs in
    the OS and handles transferring data to or from a specific device. The device
    driver software interacts with the OS through the OS’s device interface; a new
    device can be added to a computer system by loading its device driver code, written
    to conform to the OS’s device interface, into the OS. The kernel directly manages
    other hardware devices, such as the CPU and RAM. [Figure 13-2](ch13.xhtml#ch13fig2)
    shows the OS kernel layer between the user applications and the computer hardware,
    including its programming interface to users and its hardware device interface.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-2: The OS kernel: core OS functionality necessary to use the system
    and facilitate cooperation between I/O devices and users of the system*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the rest of this chapter, we examine the role the operating system plays
    in running programs and in efficiently managing system resources. Our discussion
    is primarily focused on the mechanism (the “how”) of the OS functionality and
    the implementation of two primary OS abstractions: a *process* (a running program)
    and *virtual memory* (a view of process memory space that is abstracted from its
    underlying physical storage in RAM or secondary storage).'
  prefs: []
  type: TYPE_NORMAL
- en: 13.1 How the OS Works and How It Runs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Part of the job of the OS is to support programs running on the system. To start
    a program running on a computer, the OS allocates a portion of RAM for the running
    program, loads the program’s binary executable from disk into RAM, creates and
    initializes OS state for the process associated with this running program, and
    initializes the CPU to start executing the process’s instructions (e.g., the CPU
    registers need to be initialized by the OS to fetch and execute the process’s
    instructions). [Figure 13-3](ch13.xhtml#ch13fig3) illustrates these steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-3: The steps that the OS takes to start a new program running on
    the underlying hardware*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Like user programs, the OS is also software that runs on the computer hardware.
    The OS, however, is special system software that manages all system resources
    and implements the interface for users of the computer system; it is necessary
    for using the computer system. Because the OS is software, its binary executable
    code runs on the hardware just like any other program: its data and instructions
    are stored in RAM and its instructions are fetched and executed by the CPU just
    like a user’s program instructions are. As a result, for the OS to run, its binary
    executable needs to be loaded into RAM and the CPU initialized to start running
    OS code. However, because the OS is responsible for the task of running code on
    the hardware, it needs some help to get started running.'
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.1 OS Booting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The process of the OS loading and initializing itself on the computer is known
    as *booting*—the OS “pulls itself up by its bootstraps,” or *boots* itself on
    the computer. The OS needs a little help to initially get loaded onto the computer
    and to begin running its boot code. To initiate the OS code to start running,
    code stored in computer firmware (nonvolatile memory in the hardware) runs when
    the computer first powers up; *BIOS* (Basic Input/Output System) and *UEFI* (Unified
    Extensible Firmware Interface) are two examples of this type of firmware. On power-up,
    BIOS or UEFI runs and does just enough hardware initialization to load the first
    chunk of the OS (its boot block) from disk into RAM and to start running boot
    block instructions on the CPU. Once the OS starts running, it loads the rest of
    itself from disk, discovers and initializes hardware resources, and initializes
    its data structures and abstractions to make the system ready for users.
  prefs: []
  type: TYPE_NORMAL
- en: '13.1.2 Getting the OS to Do Something: Interrupts and Traps'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After the OS finishes booting and initializing the system for use, it then just
    waits for something to do. Most operating systems are implemented as *interrupt-driven
    systems*, meaning that the OS doesn’t run until some entity needs it to do something—the
    OS is woken up (interrupted from its sleep) to handle a request.
  prefs: []
  type: TYPE_NORMAL
- en: Devices in the hardware layer may need the OS to do something for them. For
    example, a *network interface card* (NIC) is a hardware interface between a computer
    and a network. When the NIC receives data over its network connection, it interrupts
    (or wakes up) the OS to handle the received data (see [Figure 13-4](ch13.xhtml#ch13fig4)).
    For example, the OS may determine that the data received by the NIC is part of
    a web page that was requested by a web browser; it then delivers the data from
    the NIC to the waiting web browser process.
  prefs: []
  type: TYPE_NORMAL
- en: Requests to the OS also come from user applications when they need access to
    protected resources. For example, when an application wants to write to a file,
    it makes a *system call* to the OS, which wakes up the OS to perform the write
    on its behalf (see [Figure 13-4](ch13.xhtml#ch13fig4)). The OS handles the system
    call by writing the data to a file stored on disk.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-4: In an interrupt-driven system, user-level programs make system
    calls, and hardware devices issue interrupts to initiate OS actions.*'
  prefs: []
  type: TYPE_NORMAL
- en: Interrupts that come from the hardware layer, such as when a NIC receives data
    from the network, are typically referred to as hardware interrupts, or just *interrupts*.
    Interrupts that come from the software layer as the result of instruction execution,
    such as when an application makes a system call, are typically referred to as
    *traps*. That is, a system call “traps into the OS,” which handles the request
    on behalf of the user-level program. Exceptions from either layer may also interrupt
    the OS. For example, a hard disk drive may interrupt the OS if a read fails due
    to a bad disk block, and an application program may trigger a trap to the OS if
    it executes a divide instruction that divides by zero.
  prefs: []
  type: TYPE_NORMAL
- en: System calls are implemented using special trap instructions that are defined
    as part of the CPU’s instruction set architecture (ISA). The OS associates each
    of its system calls with a unique identification number. When an application wants
    to invoke a system call, it places the desired call’s number in a known location
    (the location varies according to the ISA) and issues a trap instruction to interrupt
    the OS. The trap instruction triggers the CPU to stop executing instructions from
    the application program and to start executing OS instructions that handle the
    trap (run the OS trap handler code). The trap handler reads the user-provided
    system call number and executes the corresponding system call implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of what a `write` system call might look like on an IA32
    Linux system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The first instruction (`movl $4, %eax`) puts the system call number for `write`
    (4) into register `eax`. The second instruction (`int $0x80`) triggers the trap.
    When the OS trap handler code runs, it uses the value in register `eax` (4) to
    determine which system call is being invoked and runs the appropriate trap handler
    code (in this case it runs the `write` handler code). After the OS handler runs,
    the OS continues the program’s execution at the instruction right after the trap
    instruction (`addl` in this example).
  prefs: []
  type: TYPE_NORMAL
- en: Unlike system calls, which come from executing program instructions, hardware
    interrupts are delivered to the CPU on an interrupt bus. A device places a signal,
    typically a number indicating the type of interrupt, on the CPU’s interrupt bus
    (see [Figure 13-5](ch13.xhtml#ch13fig5)). When the CPU detects the signal on its
    interrupt bus, it stops executing the current process’s instructions and starts
    executing OS interrupt handler code. After the OS handler code runs, the OS continues
    the process’s execution at the application instruction that was being executed
    when the interrupt occurred.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-5: A hardware device (disk) sends a signal to the CPU on the interrupt
    bus to trigger OS execution on its behalf.*'
  prefs: []
  type: TYPE_NORMAL
- en: If a user program is running on the CPU when an interrupt (or trap) occurs,
    the CPU runs the OS’s interrupt (or trap) handler code. When the OS is done handling
    an interrupt, it resumes executing the interrupted user program at the point it
    was interrupted.
  prefs: []
  type: TYPE_NORMAL
- en: Because the OS is software, and its code is loaded into RAM and run on the CPU
    just like user program code, the OS must protect its code and state from regular
    processes running in the system. The CPU helps by defining two execution modes.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. In *user mode*, a CPU executes only user-level instructions and accesses
    only the memory locations that the operating system makes available to it. The
    OS typically prevents a CPU in user mode from accessing the OS’s instructions
    and data. User mode also restricts which hardware components the CPU can directly
    access. In *kernel mode*, a CPU executes any instructions and accesses any memory
    location (including those that store OS instructions and data). It can also directly
    access hardware components and execute special instructions.
  prefs: []
  type: TYPE_NORMAL
- en: When OS code is run on the CPU, the system runs in kernel mode, and when user-level
    programs run on the CPU, the system runs in user mode. If the CPU is in user mode
    and receives an interrupt, the CPU switches to kernel mode, fetches the interrupt
    handler routine, and starts executing the OS handler code. In kernel mode, the
    OS can access hardware and memory locations that are not allowed in user mode.
    When the OS is done handling the interrupt, it restores the CPU state to continue
    executing user-level code at the point at which the program left off when interrupted
    and returns the CPU back to user mode (see [Figure 13-6](ch13.xhtml#ch13fig6)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-6: The CPU and interrupts. User code running on the CPU is interrupted
    (at time X on the time line), and OS interrupt handler code runs. After the OS
    is done handling the interrupt, user code execution is resumed (at time Y on the
    time line).*'
  prefs: []
  type: TYPE_NORMAL
- en: In an interrupt-driven system, interrupts can happen at any time, meaning that
    the OS can switch from running user code to interrupt handler code at any machine
    cycle. One way to efficiently support this execution context switch from user
    mode to kernel mode is to allow the kernel to run within the execution context
    of every process in the system. At boot time, the OS loads its code at a fixed
    location in RAM that is mapped into the top of the address space of every process
    (see [Figure 13-7](ch13.xhtml#ch13fig7)), and initializes a CPU register with
    the starting address of the OS handler function. On an interrupt, the CPU switches
    to kernel mode and executes OS interrupt handler code instructions that are accessible
    at the top addresses in every process’s address space. Because every process has
    the OS mapped to the same location at the top of its address space, the OS interrupt
    handler code is able to execute quickly in the context of any process that is
    running on the CPU when an interrupt occurs. This OS code can be accessed only
    in kernel mode, protecting the OS from user-mode accesses; during regular execution
    a process runs in user mode and cannot read or write to the OS addresses mapped
    into the top of its address space.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-7: Process address space: the OS kernel is mapped into the top of
    every process’s address space.*'
  prefs: []
  type: TYPE_NORMAL
- en: Although mapping the OS code into the address space of every process results
    in fast kernel code execution on an interrupt, many modern processors have features
    that expose vulnerabilities to kernel protections when the OS is mapped into every
    process like this. As of the January 2018 announcement of the Meltdown hardware
    exploit,^([1](ch13.xhtml#fn13_1)) operating systems are separating kernel memory
    and user-level program memory in ways that protect against this exploit but that
    also result in less efficient switching to kernel mode to handle interrupts.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2 Processes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the main abstractions implemented by the operating system is a *process*.
    A process represents an instance of a program running in the system, which includes
    the program’s binary executable code, data, and execution *context*. The context
    tracks the program’s execution by maintaining its register values, stack location,
    and the instruction it is currently executing.
  prefs: []
  type: TYPE_NORMAL
- en: Processes are necessary abstractions in *multiprogramming* systems, which support
    multiple processes existing in the system at the same time. The process abstraction
    is used by the OS to keep track of individual instances of programs running in
    the system, and to manage their use of system resources.
  prefs: []
  type: TYPE_NORMAL
- en: The OS provides each process with a “lone view” abstraction of the system. That
    is, the OS isolates processes from one another and gives each process the illusion
    that it’s controlling the entire machine. In reality, the OS supports many active
    processes and manages resource sharing among them. The OS hides the details of
    sharing and accessing system resources from the user, and the OS protects processes
    from the actions of other processes running in the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a user may simultaneously run two instances of a Unix shell program
    along with a web browser on a computer system. The OS creates three processes
    associated with these three running programs: one process for each separate execution
    of the Unix shell program, and one process for the web browser. The OS handles
    switching between these three processes running on the CPU, and it ensures that
    as a process runs on the CPU, only the execution state and system resources allocated
    to the process can be accessed.'
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.1 Multiprogramming and Context Switching
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multiprogramming enables the OS to make efficient use of hardware resources.
    For example, when a process running on the CPU needs to access data that are currently
    on disk, rather than have the CPU sit idle waiting for the data to be read into
    memory, the OS can give the CPU to another process and let it run while the read
    operation for the original process is being handled by the disk. By using multiprogramming,
    the OS can mitigate some of the effects of the memory hierarchy on its program
    workload by keeping the CPU busy executing some processes while other processes
    are waiting to access data in the lower levels of the memory hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: General-purpose operating systems often implement *timesharing*, which is multiprogramming
    wherein the OS schedules each process to take turns executing on the CPU for short
    time durations (known as a *time slice* or *quantum*). When a process completes
    its time slice on the CPU, the OS removes the process from the CPU and lets another
    run. Most systems define time slices to be a few milliseconds (10^(-3) seconds),
    which is a long time in terms of CPU cycles but is not noticeable to a human.
  prefs: []
  type: TYPE_NORMAL
- en: Timesharing systems further support the “lone view” of the computer system to
    the user; because each process frequently executes on the CPU for short bursts
    of time, the fact that they are all sharing the CPU is usually imperceptible to
    the user. Only when the system is very heavily loaded might a user notice the
    effects of other processes in the system. The Unix command `ps -A` lists all the
    processes running in the system—you may be surprised by how many there are. The
    `top` command is also useful for seeing the state of the system as it runs by
    displaying the set of processes that currently use the most system resources (such
    as CPU time and memory space).
  prefs: []
  type: TYPE_NORMAL
- en: In multiprogrammed and timeshared systems, processes run *concurrently*, meaning
    that their executions overlap in time. For example, the OS may start running process
    A on the CPU, and then switch to running process B for a while, and later switch
    back to running process A some more. In this scenario, processes A and B run concurrently
    because their execution on the CPU overlaps due to the OS switching between the
    two.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.1.1 Context Switching
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The *mechanism* behind multiprogramming determines how the OS swaps one process
    running on the CPU with another. The *policy* aspect of multiprogramming governs
    scheduling the CPU, or picking which process from a set of candidate processes
    gets to use the CPU next and for how long. We focus primarily on the mechanism
    of implementing multiprogramming. Operating systems textbooks cover scheduling
    policies in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'The OS performs *context switching*, or swapping process state on the CPU,
    as the primary mechanism behind multiprogramming (and time-sharing). There are
    two main steps to performing a CPU context switch:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The OS saves the context of the current process running on the CPU, including
    all of its register values (PC, stack pointers, general-purpose register, condition
    codes, etc.), its memory state, and some other state (for example, the state of
    system resources it uses, like open files).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The OS restores the saved context from another process on the CPU and starts
    the CPU running this other process, continuing its execution from the instruction
    where it left off.
  prefs: []
  type: TYPE_NORMAL
- en: One part of context switching that may seem impossible to accomplish is that
    the OS’s code that implements context switching must run on the CPU while it saves
    (restores) a process’s execution contexts from (to) the CPU; the instructions
    of the context switching code need to use CPU hardware registers to execute, but
    the register values from the process being context switched off the CPU need to
    be saved by the context switching code. Computer hardware provides some help to
    make this possible.
  prefs: []
  type: TYPE_NORMAL
- en: At boot time, the OS initialized the hardware, including initializing the CPU
    state, so that when the CPU switches to kernel mode on an interrupt, the OS interrupt
    handler code starts executing and the interrupted process’s execution state is
    protected from this execution. Together, the computer hardware and OS perform
    some of the initial saving of the user-level execution context, enough that the
    OS code can run on the CPU without losing the execution state of the interrupted
    process. For example, register values of the interrupted process need to be saved
    so that when the process runs again on the CPU, the process can continue from
    the point at which it left off, using its register values. Depending on the hardware
    support, saving the user-level process’s register values may be done entirely
    by the hardware or may be done almost entirely in software as the first part of
    the kernel’s interrupt handling code. At a minimum, the process’s program counter
    (PC) value needs to be saved so that its value is not lost when the kernel interrupt
    handler address is loaded into the PC.
  prefs: []
  type: TYPE_NORMAL
- en: After the OS starts running, it executes its full process context switching
    code, saving the full execution state of the process running on the CPU and restoring
    the saved execution state of another process onto the CPU. Because the OS runs
    in kernel mode it is able to access any parts of computer memory and can execute
    privileged instructions and access any hardware registers. As a result, its context
    switching code is able to access and save the CPU execution state of any process
    to memory, and it is able to restore from memory the execution state of any process
    to the CPU. OS context switching code completes by setting up the CPU to execute
    the restored process’s execution state, and by switching the CPU to user mode.
    Once switched to user mode, the CPU executes instructions, and uses execution
    state from the process that the OS context switched onto the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.2 Process State
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In multiprogrammed systems, the OS must track and manage the multiple processes
    existing in the system at any given time. The OS maintains information about each
    process, including:'
  prefs: []
  type: TYPE_NORMAL
- en: A *process id* (PID), which is a unique identifier for a process. The `ps` command
    lists information about processes in the system, including their PID values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The address space information for the process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The execution state of the process (e.g., CPU register values, stack location).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The set of resources allocated to the process (e.g., open files).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The current *process state*, which is a value that determines its eligibility
    for execution on the CPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over the course of its lifetime, a process moves through several states, which
    correspond to different categories of process execution eligibility. One way that
    the OS uses process state is to identify the set of processes that are candidates
    for being scheduled on the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'The set of process execution states are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Ready*: The process could run on the CPU but is not currently scheduled (it
    is a candidate for being context switched on to the CPU). Once a new process is
    created and initialized by the OS, it enters the ready state (it is ready for
    the CPU to start executing its first instruction). In a timesharing system, if
    a process is context switched off the CPU because its time slice is up, it is
    also placed in the *ready* state (it is ready for the CPU to execute its next
    instruction, but it used up its time slice and has to wait its turn to get scheduled
    again on the CPU).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Running*: The process is scheduled on the CPU and is actively executing instructions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Blocked*: The process is waiting for some event before it can continue being
    executed. For example, the process is waiting for some data to be read in from
    disk. Blocked processes are not candidates for being scheduled on the CPU. After
    the event on which the process is blocked occurs, the process moves to the *ready*
    state (it is ready to run again).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exited*: The process has exited but still needs to be completely removed from
    the system. A process exits due to its completing the execution of its program
    instructions, or by exiting with an error (e.g., it tries to divide by zero),
    or by receiving a termination request from another process. An exited process
    will never run again, but it remains in the system until final clean-up associated
    with its execution state is complete.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Figure 13-8](ch13.xhtml#ch13fig8) shows the lifetime of a process in the system,
    illustrating how it moves between different states. Note the transitions (arrows)
    from one state to another. For example, a process can enter the Ready state in
    one of three ways: first, if it is newly created by the OS; second, if it was
    blocked waiting for some event and the event occurs; and third, if it was running
    on the CPU and its time slice is over and the OS context switches it off to give
    another Ready process its turn on the CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-8: The states of a process during its lifetime*'
  prefs: []
  type: TYPE_NORMAL
- en: PROCESS RUNTIME
  prefs: []
  type: TYPE_NORMAL
- en: Programmers often use a process’s completion time as a metric to evaluate its
    performance. For noninteractive programs, a faster runtime typically indicates
    a better, or more optimal, implementation. For example, in comparing two programs
    that compute the prime factors of a large number, the one that correctly completes
    the task faster is preferable.
  prefs: []
  type: TYPE_NORMAL
- en: There are two different measures of the runtime of a process. The first is total
    *wall time* (or wall-clock time). Wall time is the duration between the start
    and completion of a process; it is the elapsed time from the process’s start to
    finish as measured by a clock hanging on a wall. Wall time includes the time that
    the process is in the Running state executing on the CPU, as well as time that
    the process is in the Blocked state waiting for an event like I/O as well as the
    time that the process spends in the Ready state waiting for its turn to be scheduled
    to run on the CPU. In multiprogrammed and timeshared systems, the wall time of
    a process can slow down due to other processes running concurrently on the system
    and sharing system resources.
  prefs: []
  type: TYPE_NORMAL
- en: The second measure of process runtime is total *CPU time* (or process time).
    CPU time measures just the amount of time the process spends in the Running state
    executing its instructions on the CPU. CPU time does not include the time the
    process spends in the Blocked or Ready states. As a result, a process’s total
    CPU time is not affected by other processes concurrently running on the system.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.3 Creating (and Destroying) Processes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An OS creates a new process when an existing process makes a system call requesting
    it to do so. In Unix, the `fork` system call creates a new process. The process
    calling `fork` is the *parent* process and the new process it creates is its *child*
    process. For example, if you run `a.out` in a shell, the shell process calls the
    `fork` system call to request that the OS create a new child process that will
    be used to run the `a.out` program. Another example is a web browser process that
    calls `fork` to create child processes to handle different browsing events. A
    web browser may create a child process to handle communication with a web server
    when a user loads a web page. It may create another process to handle user mouse
    input, and other processes to handle separate browser windows or tabs. A multiple-process
    web browser like this is able to continue handling user requests through some
    of its child browser processes, while at the same time some of its other child
    browser processes may be blocked waiting for remote web server responses or for
    user mouse clicks.
  prefs: []
  type: TYPE_NORMAL
- en: A *process hierarchy* of parent–child relationships exists between the set of
    processes active in the system. For example, if process *A* makes two calls to
    `fork`, two new child processes are created, *B* and *C*. If process C then calls
    `fork`, another new process, *D*, will be created. Process C is the child of A,
    and the parent of D. Processes B and C are siblings (they share a common parent
    process, process A). Process A is the ancestor of B, C, and D. This example is
    illustrated in [Figure 13-9](ch13.xhtml#ch13fig9).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-9: An example process hierarchy created by a parent process (A)
    calling `fork` twice to create two child processes (B and C). C’s call to `fork`
    creates its child process, D. To list the process hierarchy on Linux systems,
    run `pstree`, or `ps -Aef --forest`.*'
  prefs: []
  type: TYPE_NORMAL
- en: Since existing processes trigger process creation, a system needs at least one
    process to create any new processes. At boot time, the OS creates the first user-level
    process in the system. This special process, named `init`, sits at the very top
    of the process hierarchy as the ancestor of all other processes in the system.
  prefs: []
  type: TYPE_NORMAL
- en: fork
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `fork` system call is used to create a process. At the time of the fork,
    the child inherits its execution state from its parent. The OS creates a *copy*
    of the calling (parent) process’s execution state at the point when the parent
    calls `fork`. This execution state includes the parent’s address space contents,
    CPU register values, and any system resources it has allocated (e.g., open files).
    The OS also creates a new *process control struct*, an OS data structure for managing
    the child process, and it assigns the child process a unique PID. After the OS
    creates and initializes the new process, the child and parent are concurrent—they
    both continue running and their executions overlap as the OS context switches
    them on and off the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the child process is first scheduled by the OS to run on the CPU, it starts
    executing at the point at which its parent left off—at the return from the `fork`
    call. This is because `fork` gives the child a copy of its parent’s execution
    state (the child executes using its own copy of this state when it starts running).
    From the programmer’s point of view, *a call to* fork *returns twice*: once in
    the context of the running parent process, and once in the context of the running
    child process.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to differentiate the child and parent processes in a program, a call
    to `fork` returns different values to the parent and child. The child process
    always receives a return value of 0, whereas the parent receives the child’s PID
    value (or –1 if `fork` fails).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following code snippet shows a call to the `fork` system call
    that creates a new child process of the calling process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: After the call to `fork` creates a new child process, the parent and child processes
    both continue executing, in their separate execution contexts, at the return point
    of the `fork` call. Both processes assign the return value of `fork` to their
    `pid` variable and both call `printf`. The child process’s call prints out 0 and
    the parent process prints out the child’s PID value.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-10](ch13.xhtml#ch13fig10) shows an example of what the process hierarchy
    looks like after this code’s execution. The child process gets an exact copy of
    the parent process’s execution context at the point of the fork, but the value
    stored in its variable `pid` differs from its parent because `fork` returns the
    child’s PID value (14 in this example) to the parent process, and 0 to the child.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-10: A process (PID 12) calls `fork` to create a new child process.
    The new child process gets an exact copy of its parent’s address and execution
    state, but gets its own process identifier (PID 14). `fork` returns 0 to the child
    process and the child’s PID value (14) to the parent.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Often, the programmer wants the child and parent processes to perform different
    tasks after the `fork` call. A programmer can use the different return values
    from `fork` to trigger the parent and child processes to execute different code
    branches. For example, the following code snippet creates a new child process
    and uses the return value from `fork` to have the child and parent processes execute
    different code branches after the call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It is important to remember that as soon as they’re created, the child and parent
    processes run concurrently in their own execution contexts, modifying their separate
    copies of program variables and possibly executing different branches in the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following program^([2](ch13.xhtml#fn13_2)) that contains a call
    to `fork` with branching on the value of `pid` to trigger the parent and child
    processes to execute different code (this example also shows a call to `getpid`
    that returns the PID of the calling process):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When run, this program’s output might look like the following (assume that
    the parent’s PID is 12 and the child’s is 14):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In fact, the program’s output could look like any of the possible options shown
    in [Table 13-1](ch13.xhtml#ch13tab1) (and you will often see more than one possible
    ordering of output if you run the program multiple times). In [Table 13-1](ch13.xhtml#ch13tab1),
    the parent prints B:12 and the child B:14 in this example, but the exact PID values
    will vary from run to run.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 13-1:** All Six Possible Orderings of Example Program Output'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Option 1** | **Option 2** | **Option 3** | **Option 4** | **Option 5**
    | **Option 6** |'
  prefs: []
  type: TYPE_TB
- en: '| A | A | A | A | A | A |'
  prefs: []
  type: TYPE_TB
- en: '| Parent... | Parent... | Parent... | Child... | Child... | Child... |'
  prefs: []
  type: TYPE_TB
- en: '| Child... | Child... | B:12 | Parent... | Parent... | B:14 |'
  prefs: []
  type: TYPE_TB
- en: '| B:12 | B:14 | Child... | B:12 | B:14 | Parent... |'
  prefs: []
  type: TYPE_TB
- en: '| B:14 | B:12 | B:14 | B:14 | B:12 | B:12 |'
  prefs: []
  type: TYPE_TB
- en: These six different output orderings are possible because after the `fork` system
    call returns, the parent and child processes are concurrent and can be scheduled
    to run on the CPU in many different orderings, resulting in any possible interleaving
    of their instruction sequences. Consider the execution time line of this program,
    shown in [Figure 13-11](ch13.xhtml#ch13fig11). The dotted line represents concurrent
    execution of the two processes. Depending on when each is scheduled to run on
    the CPU, one could execute both its `printf` statements before the other, or the
    execution of their two `printf` statements could be interleaved, resulting in
    any of the possible outcomes shown in [Table 13-1](ch13.xhtml#ch13tab1). Because
    only one process, the parent, exists before the call to `fork`, A is always printed
    by the parent before any of the output after the call to `fork`.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-11: The execution time line of the program. Only the parent process
    exists before the call to `fork`. After `fork` returns, both run concurrently
    (shown in the dotted lines).*'
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.4 exec
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Usually a new process is created to execute a program that is different from
    that of its parent process. This means that `fork` is often called to create a
    process with the intention of running a new program from its starting point (i.e.,
    starting its execution from its first instruction). For example, if a user types
    `./a.out` in a shell, the shell process forks a new child process to run `a.out`.
    As two separate processes, the shell and the `a.out` process are protected from
    each other; they cannot interfere with each other’s execution state.
  prefs: []
  type: TYPE_NORMAL
- en: While `fork` creates the new child process, it does not cause the child to run
    `a.out`. To initialize the child process to run a new program, the child process
    calls one of the *exec* system calls. Unix provides a family of exec system calls
    that trigger the OS to overlay the calling process’s image with a new image from
    a binary executable file. In other words, an exec system call tells the OS to
    overwrite the calling process’s address space contents with the specified `a.out`
    and to reinitialize its execution state to start executing the very first instruction
    in the `a.out` program.
  prefs: []
  type: TYPE_NORMAL
- en: 'One example of an exec system call is `execvp`, whose function prototype is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `filename` parameter specifies the name of a binary executable program to
    initialize the process’s image, and `argv` contains the command line arguments
    to pass into the `main` function of the program when it starts executing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example code snippet that, when executed, creates a new child process
    to run `a.out`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `argv` variable is initialized to the value of the `argv` argument that
    is passed to the `main` function of `a.out`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`execvp` will figure out the value to pass to `argc` based on this `argv` value
    (in this case, 1).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-12](ch13.xhtml#ch13fig12) shows what the process hierarchy would
    look like after executing this code.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-12: When the child process calls `execvp` (left), the OS replaces
    its image with `a.out` (right) and initializes the child process to start running
    the `a.out` program from its beginning.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Something to note in the previous example code is its seemingly odd error message
    after the call to `execvp`: why would returning from an exec system call be an
    error? If the exec system call is successful, then the error detection and handling
    code immediately following it will never be executed because the process will
    now be executing code in the `a.out` program instead of this code (the process’s
    address space contents have been changed by exec). That is, when a call to an
    exec function is successful, the process doesn’t continue its execution at the
    return of the exec call. Because of this behavior, the following code snippet
    is equivalent to the previous one (however, that code is typically easier to understand):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 13.2.5 exit and wait
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To terminate, a process calls the `exit` system call, which triggers the OS
    to clean up most of the process’s state. After running the exit code, a process
    notifies its parent process that it has exited. The parent is responsible for
    cleaning up the exited child’s remaining state from the system.
  prefs: []
  type: TYPE_NORMAL
- en: Processes can be triggered to exit in several ways. First, a process may complete
    all of its application code. Returning from its `main` function leads to a process
    invoking the `exit` system call. Second, a process can perform an invalid action,
    such as dividing by zero or dereferencing a null pointer, that results in its
    exiting. Finally, a process can receive a *signal* from the OS or another process,
    telling it to exit (in fact, dividing by zero and null pointer dereferences result
    in the OS sending the process `SIGFPE` and `SIGSEGV` signals telling it to exit).
  prefs: []
  type: TYPE_NORMAL
- en: SIGNALS
  prefs: []
  type: TYPE_NORMAL
- en: A *signal* is a software interrupt that the OS delivers to a process. Signals
    are a method by which related processes can communicate with one another. The
    OS provides an interface for one process to send a signal to another, and for
    it to communicate with processes (to send a process a `SIGSEGV` signal when it
    dereferences a null pointer, for example).
  prefs: []
  type: TYPE_NORMAL
- en: When a process receives a signal, it is interrupted to run special signal handler
    code. A system defines a fixed number of signals to communicate various meanings,
    each differentiated by a unique signal number. The OS implements default signal
    handlers for each signal type, but programmers can register their own user-level
    signal handler code to override the default actions of most signals for their
    application.
  prefs: []
  type: TYPE_NORMAL
- en: “Signals” on [page 657](ch13.xhtml#lev2_231) contains more information about
    signals and signal handling.
  prefs: []
  type: TYPE_NORMAL
- en: If a shell process wants to terminate its child process running `a.out`, it
    can send the child a `SIGKILL` signal. When the child process receives the signal,
    it runs signal handler code for `SIGKILL` that calls `exit`, terminating the child
    process. If a user types CTRL-C in a Unix shell that is currently running a program,
    the child process receives a `SIGINT` signal. The default signal handler for `SIGINT`
    also calls `exit`, resulting in the child process exiting.
  prefs: []
  type: TYPE_NORMAL
- en: After executing the `exit` system call, the OS delivers a `SIGCHLD` signal to
    the process’s parent process to notify it that its child has exited. The child
    becomes a *zombie* process; it moves to the Exited state and can no longer run
    on the CPU. The execution state of a zombie process is partially cleaned up, but
    the OS still maintains a little information about it, including about how it terminated.
  prefs: []
  type: TYPE_NORMAL
- en: A parent process *reaps* its zombie child (cleans up the rest of its state from
    the system) by calling the `wait` system call. If the parent process calls `wait`
    before its child process exits, then the parent process blocks until it receives
    a `SIGCHLD` signal from the child. The `waitpid` system call is a version of `wait`
    that takes a PID argument, allowing a parent to block while waiting for the termination
    of a specific child process.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-13](ch13.xhtml#ch13fig13) shows the sequence of events that occur
    when a process exits.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-13: Process exit. Left: The child process calls the `exit` system
    call to clean up most of its execution state. Middle: After running `exit`, the
    child process becomes a zombie (it is in the Exited state and cannot run again),
    and its parent process is sent a `SIGCHLD` signal, notifying it that its child
    is exited. Right: The parent calls `waitpid` to reap its zombie child (cleans
    up the rest of the child’s state from the system).*'
  prefs: []
  type: TYPE_NORMAL
- en: Because the parent and child processes execute concurrently, the parent may
    call `wait` before its child exits, or the child can exit before the parent calls
    `wait`. If the child is still executing when the parent process calls `wait`,
    then the parent blocks until the child exits (the parent enters the Blocked state
    waiting for the `SIGCHLD` signal event to happen). The blocking behavior of the
    parent can be seen if you run a program (`a.out`) in the foreground of a shell—the
    shell program doesn’t print out a shell prompt until `a.out` terminates, indicating
    that the shell parent process is blocked on a call to `wait`, waiting until it
    receives a `SIGCHLD` from its child process running `a.out`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A programmer can also design the parent process code so that it will never
    block waiting for a child process to exit. If the parent implements a `SIGCHLD`
    signal handler that contains the call to `wait`, then the parent only calls `wait`
    when there is an exited child process to reap, and thus it doesn’t block on a
    `wait` call. This behavior can be seen by running a program in the background
    in a shell (`a.out &`). The shell program will continue executing, print out a
    prompt, and execute another command as its child runs `a.out`. Here’s an example
    of how you might see the difference between a parent blocking on `wait` vs. a
    nonblocking parent that only calls `wait` inside a `SIGCHLD` signal handler (make
    sure you execute a program that runs for long enough to notice the difference):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Following is an example code snippet containing `fork`, `exec`, `exit`, and
    `wait` system calls (with error handling removed for readability). This example
    is designed to test your understanding of these system calls and their effects
    on the execution of the processes. In this example, the parent process creates
    a child process and waits for it to exit. The child then forks another child to
    run the `a.out` program (the first child is the parent of the second child). It
    then waits for its child to exit.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 13-14](ch13.xhtml#ch13fig14) illustrates the execution time line of
    process create/running/blocked/exit events from executing the preceding example.
    The dotted lines represent times when a process’s execution overlaps with its
    child or descendants: the processes are concurrent and can be scheduled on the
    CPU in any order. Solid lines represent dependencies on the execution of the processes.
    For example, Child 1 cannot call `exit` until it has reaped its exited child process,
    Child 2\. When a process calls `wait`, it blocks until its child exits. When a
    process calls `exit`, it never runs again. The program’s output is annotated along
    each process’s execution time line at points in its execution when the corresponding
    `printf` statement can occur.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-14: The execution time line for the example program, showing a possible
    sequence of `fork`, `exec`, `wait`, and `exit` calls from the three processes.
    Solid lines represent dependencies in the order of execution between processes,
    and dotted lines concurrent execution points. Parent is the parent process of
    Child 1, and Child 1 is the parent of Child 2.*'
  prefs: []
  type: TYPE_NORMAL
- en: After the calls to `fork` are made in this program, the parent process and first
    child process run concurrently, thus the call to `wait` in the parent could be
    interleaved with any instruction of its child. For example, the parent process
    could call `wait` and block before its child process calls `fork` to create its
    child process. [Table 13-2](ch13.xhtml#ch13tab2) lists all possible outputs from
    running the example program.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 13-2:** All Possible Output Orderings from the Program'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Option 1** | **Option 2** | **Option 3** | **Option 4** |'
  prefs: []
  type: TYPE_TB
- en: '| A | A | A | A |'
  prefs: []
  type: TYPE_TB
- en: '| B | B | B | E |'
  prefs: []
  type: TYPE_TB
- en: '| C | C | E | B |'
  prefs: []
  type: TYPE_TB
- en: '| D | E | C | C |'
  prefs: []
  type: TYPE_TB
- en: '| E | D | D | D |'
  prefs: []
  type: TYPE_TB
- en: '| F | F | F | F |'
  prefs: []
  type: TYPE_TB
- en: The program outputs in [Table 13-2](ch13.xhtml#ch13tab2) are all possible because
    the parent runs concurrently with its descendant processes until it calls `wait`.
    Thus, the parent’s call to `printf("E\n")` can be interleaved at any point between
    the start and the exit of its descendant processes.
  prefs: []
  type: TYPE_NORMAL
- en: 13.3 Virtual Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The OS’s process abstraction provides each process with a virtual memory space.
    *Virtual memory* is an abstraction that gives each process its own private, logical
    address space in which its instructions and data are stored. Each process’s virtual
    address space can be thought of as an array of addressable bytes from address
    0 up to some maximum address. For example, on 32-bit systems the maximum address
    is 2^(32) – 1\. Processes cannot access the contents of one another’s address
    spaces. Some parts of a process’s virtual address space come from the binary executable
    file it’s running (e.g., the *text* portion contains program instructions from
    the `a.out` file). Other parts of a process’s virtual address space are created
    at runtime (e.g., the *stack*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating systems implement virtual memory as part of the *lone view* abstraction
    of processes. That is, each process only interacts with memory in terms of its
    own virtual address space rather than the reality of many processes simultaneously
    sharing the computer’s physical memory (RAM). The OS also uses its virtual memory
    implementation to protect processes from accessing one another’s memory spaces.
    As an example, consider the following simple C program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If two processes simultaneously execute this program, they each get their own
    copy of stack memory as part of their separate virtual address spaces. As a result,
    if one process executes `x = 6` it will have no effect on the value of `x` in
    the other process—each process has its own copy of `x`, in its private virtual
    address space, as shown in [Figure 13-15](ch13.xhtml#ch13fig15).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-15: Two executions of `a.out` results in two processes, each running
    isolated instances of the `a.out` program. Each process has its own private virtual
    address space, containing its copies of program instructions, global variables,
    and stack and heap memory space. For example, each may have a local variable `x`
    in the stack portion of their virtual address spaces.*'
  prefs: []
  type: TYPE_NORMAL
- en: A process’s virtual address space is divided into several sections, each of
    which stores a different part of the process’s memory. The top part (at the lowest
    addresses) is reserved for the OS and can only be accessed in kernel mode. The
    text and data parts of a process’s virtual address space are initialized from
    the program executable file (`a.out`). The text section contains the program instructions,
    and the data section contains global variables (the data portion is actually divided
    into two parts, one for initialized global variables and the other for uninitialized
    globals).
  prefs: []
  type: TYPE_NORMAL
- en: The stack and heap sections of a process’s virtual address space vary in size
    as the process runs. Stack space grows in response to the process making function
    calls, and shrinks as it returns from functions. Heap space grows when the process
    dynamically allocates memory space (via calls to `malloc`), and shrinks when the
    process frees dynamically allocated memory space (via calls to `free`). The heap
    and stack portions of a process’s memory are typically located far apart in its
    address space to maximize the amount of space either can use. Typically, the stack
    is located at the bottom of a process’s address space (near the maximum address),
    and grows upward into lower addresses as stack frames are added to the top of
    the stack in response to a function call.
  prefs: []
  type: TYPE_NORMAL
- en: ABOUT HEAP AND STACK MEMORY
  prefs: []
  type: TYPE_NORMAL
- en: The actual total capacity of heap and stack memory space does not typically
    change on every call to `malloc` and `free`, nor on every function call and return.
    Instead, these actions often only make changes to how much of the currently allocated
    heap and stack parts of the virtual memory space are actively being used by the
    process. Sometimes, however, these actions do result in changes to the total capacity
    of the heap or stack space.
  prefs: []
  type: TYPE_NORMAL
- en: The operating system is responsible for managing a process’s virtual address
    space, including changing the total capacity of heap and stack space. The system
    calls `brk`, `sbrk`, or `mmap` can be used to request that the OS change the total
    capacity of heap memory. C programmers do not usually invoke these system calls
    directly. Instead, C programmers call the standard C library function `malloc`
    (and `free`) to allocate (and free) heap memory space. Internally, the standard
    C library’s user-level heap manager may invoke one of these system calls to request
    that the OS change the size of heap memory space to satisfy a `malloc` request.
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.1 Memory Addresses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because processes operate within their own virtual address spaces, operating
    systems must make an important distinction between two types of memory addresses.
    *Virtual addresses* refer to storage locations in a process’s virtual address
    space, and *physical addresses* refer to storage locations in physical memory
    (RAM).
  prefs: []
  type: TYPE_NORMAL
- en: Physical Memory (RAM) and Physical Memory Addresses
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: From [Chapter 11](ch11.xhtml#ch11), we know that physical memory (RAM) can be
    viewed as an array of addressable bytes in which addresses range from 0 to a maximum
    address value based on the total size of RAM. For example, in a system with 2
    gigabytes (GB) of RAM, physical memory addresses range from 0 to 2^(31) – 1 (1
    GB is 2^(30) bytes, so 2 GB is 2^(31) bytes).
  prefs: []
  type: TYPE_NORMAL
- en: In order for the CPU to run a program, the program’s instructions and data must
    be loaded into RAM by the OS; the CPU cannot directly access other storage devices
    (e.g., disks). The OS manages RAM and determines which locations in RAM should
    store the virtual address space contents of a process. For example, if two processes,
    P1 and P2, run the earlier example program, then P1 and P2 have separate copies
    of the `x` variable, each stored at a different location in RAM. That is, P1’s
    `x` and P2’s `x` have different physical addresses. If the OS gave P1 and P2 the
    same physical address for their `x` variables, then P1 setting `x` to 6 would
    also modify P2’s value of `x`, violating the per-process private virtual address
    space.
  prefs: []
  type: TYPE_NORMAL
- en: At any point in time, the OS stores in RAM the address space contents from many
    processes as well as OS code that it may map into every process’s virtual address
    space (OS code is typically loaded starting at address 0x0 of RAM). [Figure 13-16](ch13.xhtml#ch13fig16)
    shows an example of the OS and three processes (P1, P2, and P3) loaded into RAM.
    Each process gets its own separate physical storage locations for its address
    space contents (e.g., even if P1 and P2 run the same program, they get separate
    physical storage locations for their variable `x`).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-16: Example RAM contents showing OS loaded at address 0x0, and processes
    loaded at different physical memory addresses in RAM. If P1 and P2 are running
    the same `a.out`, P1’s physical address for `x` is different from P2’s physical
    address for `x`.*'
  prefs: []
  type: TYPE_NORMAL
- en: Virtual Memory and Virtual Addresses
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Virtual memory is the per-process view of its memory space, and *virtual addresses*
    are addresses in the process’s view of its memory. If two process run the same
    binary executable, then they have exactly the same virtual addresses for function
    code and for global variables in their address spaces (the virtual addresses of
    dynamically allocated space in heap memory and of local variables on the stack
    may vary slightly between the two processes due to runtime differences in their
    two separate executions). In other words, both processes will have the same virtual
    addresses for the location of their `main` function, and the same virtual address
    for the location of a global variable `x` in their address spaces, as shown in
    [Figure 13-17](ch13.xhtml#ch13fig17).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-17: Example virtual memory contents for two processes running the
    same `a.out` file. P1 and P2 have the same virtual address for global variable
    `x`.*'
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.2 Virtual Address to Physical Address Translation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A program’s assembly and machine code instructions refer to virtual addresses.
    As a result, if two processes execute the same `a.out` program, the CPU executes
    instructions with identical virtual addresses to access corresponding parts of
    their two separate virtual address spaces. For example, supposing that `x` is
    at virtual address 0x24100, then assembly instructions to set `x` to 6 might look
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: At runtime the OS loads each of the processes’ `x` variables at different physical
    memory addresses (at different locations in RAM). This means that whenever the
    CPU executes a load or store instruction to memory that specify virtual addresses,
    the virtual address from the CPU must be translated to its corresponding physical
    address in RAM before reading or writing the bytes from RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Because virtual memory is an important and core abstraction implemented by operating
    systems, processors generally provide some hardware support for virtual memory.
    An OS can make use of this hardware-level virtual memory support to perform virtual
    to physical address translations quickly, avoiding having to trap to the OS to
    handle every address translation. A particular OS chooses how much of the hardware
    support for paging it uses in its implementation of virtual memory. There is often
    a trade-off in speed versus flexibility when choosing a hardware-implemented feature
    versus a software-implemented feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *memory management unit* (MMU) is the part of the computer hardware that
    implements address translation. Together, the MMU hardware and the OS translate
    virtual to physical addresses when applications access memory. The particular
    hardware/software split depends on the specific combination of hardware and OS.
    At its most complete, MMU hardware performs the full translation: it takes a virtual
    address from the CPU and translates it to a physical address that is used to address
    RAM (as shown in [Figure 13-18](ch13.xhtml#ch13fig18)). Regardless of the extent
    of hardware support for virtual memory, there will be some virtual-to-physical
    translations that the OS has to handle. In our discussion of virtual memory, we
    assume a more complete'
  prefs: []
  type: TYPE_NORMAL
- en: MMU that minimizes the amount of OS involvement required for address translation.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-18: The memory management unit (MMU) maps virtual to physical addresses.
    Virtual addresses are used in instructions executed by the CPU. When the CPU needs
    to fetch data from physical memory, the virtual address is first translated by
    the MMU to a physical addresses that is used to address RAM.*'
  prefs: []
  type: TYPE_NORMAL
- en: The OS maintains virtual memory mappings for each process to ensure that it
    can correctly translate virtual to physical addresses for any process that runs
    on the CPU. During a context switch, the OS updates the MMU hardware to refer
    to the swapped-on process’s virtual-to-physical memory mappings. The OS protects
    processes from accessing one another’s memory spaces by swapping the per-process
    address mapping state on a context switch—swapping the mappings on a context switch
    ensures that one process’s virtual addresses will not map to physical addresses
    storing another process’s virtual address space.
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.3 Paging
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although many virtual memory systems have been proposed over the years, paging
    is now the most widely used implementation of virtual memory. In a *paged virtual
    memory* system, the OS divides the virtual address space of each process into
    fixed-sized chunks called *pages*. The OS defines the page size for the system.
    Page sizes of a few kilobytes are commonly used in general-purpose operating systems
    today—4 KB (4,096 bytes) is the default page size on many systems.
  prefs: []
  type: TYPE_NORMAL
- en: Physical memory is similarly divided by the OS into page-sized chunks called
    *frames*. Because pages and frames are defined to be the same size, any page of
    a process’s virtual memory can be stored in any frame of physical RAM.
  prefs: []
  type: TYPE_NORMAL
- en: In a paging system, pages and frames are the same size, so any page of virtual
    memory can be loaded into (stored) in any physical frame of RAM; a process’s pages
    do not need to be stored in contiguous RAM frames (at a sequence of addresses
    all next to one another in RAM); and not every page of virtual address space needs
    to be loaded into RAM for a process to run.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-19](ch13.xhtml#ch13fig19) shows an example of how pages from a process’s
    virtual address space may map to frames of physical RAM.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-19: Paged virtual memory. Individual pages of a process’s virtual
    address space are stored in RAM frames. Any page of virtual address space can
    be loaded into (stored at) any frame of physical memory. In this example, P1’s
    virtual page 1000 is stored in physical frame 100, and its [page 500](ch09.xhtml#page_500)
    resides in frame 513\. P2’s virtual page 1000 is stored in physical frame 880,
    and its [page 230](ch04.xhtml#page_230) resides in frame 102.*'
  prefs: []
  type: TYPE_NORMAL
- en: Virtual and Physical Addresses in Paged Systems
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Paged virtual memory systems divide the bits of a virtual address into two
    parts: the high-order bits specify the *page number* on which the virtual address
    is stored, and the low-order bits correspond to the *byte offset* within the page
    (which byte from the top of the page corresponds to the address).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, paging systems divide physical addresses into two parts: the high-order
    bits specify the *frame number* of physical memory, and the low-order bits specify
    the *byte offset* within the frame. Because frames and pages are the same size,
    the byte offset bits in a virtual address are identical to the byte offset bits
    in its translated physical address. Virtual addresses differ from their translated
    physical addresses in their high-order bits, which specify the virtual page number
    and physical frame number.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-20: The address bits in virtual and physical addresses*'
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a (very tiny) system with 16-bit virtual addresses, 14-bit
    physical addresses, and 8-byte pages. Because the page size is eight bytes, the
    low-order three bits of physical and virtual addresses define the byte offset
    into a page or frame—three bits can encode eight distinct byte offset values,
    0–7 (2³ is 8). This leaves the high-order 13 bits of the virtual address for specifying
    the page number and the high-order 11 bits of the physical address for specifying
    frame number, as shown in the example in [Figure 13-21](ch13.xhtml#ch13fig21).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-21: Virtual and physical address bit divisions in an example system
    with 16-bit virtual addresses, 14-bit physical addresses, and a page size of 8
    bytes.*'
  prefs: []
  type: TYPE_NORMAL
- en: In the example in [Figure 13-21](ch13.xhtml#ch13fig21), virtual address 43357
    (in decimal) has a byte offset of 5 (0b101 in binary), the low-order 3 bits of
    the address, and a page number of 5419 (0b1010100101011), the high-order 13 bits
    of the address. This means that the virtual address is at byte 5 from the top
    of page 5419.
  prefs: []
  type: TYPE_NORMAL
- en: If this page of virtual memory is loaded into frame 43 (0b00000101011) of physical
    memory, then its physical address is 349 (0b00000101011101), where the low-order
    3 bits (0b101) specify the byte offset, and the high-order 11 bits (0b00000101011)
    specify the frame number. This means that the physical address is at byte 5 from
    the top of frame 43 of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Page Tables for Virtual-to-Physical Page Mapping
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Because every page of a process’s virtual memory space can map to a different
    frame of RAM, the OS must maintain mappings for every virtual page in the process’s
    address space. The OS keeps a per-process *page table* that it uses to store the
    process’s virtual page number to physical frame number mappings. The page table
    is a data structure implemented by the OS that is stored in RAM. [Figure 13-22](ch13.xhtml#ch13fig22)
    shows an example of how the OS may store two process’s page tables in RAM. The
    page table of each process stores the mappings of its virtual pages to their physical
    frames in RAM such that any pages of virtual memory can be stored in any physical
    frame of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-22: Every process has a page table containing its virtual page to
    physical frame mappings. Page tables, stored in RAM, are used by the system to
    translate process’s virtual addresses to physical addresses that are used to address
    locations in RAM. This example shows the separate page tables stored in RAM for
    processes P1 and P2, each page table with its own virtual page to physical frame
    mappings.*'
  prefs: []
  type: TYPE_NORMAL
- en: For each page of virtual memory, the page table stores one *page table entry*
    (PTE) that contains the frame number of physical memory (RAM) storing the virtual
    page. A PTE may also contain other information about the virtual page, including
    a *valid bit* that is used to indicate whether the PTE stores a valid mapping.
    If a page’s valid bit is zero, then the page of the process’s virtual address
    space is not currently loaded into physical memory.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-23: A page table entry (PTE) stores the frame number (23) of the
    frame of RAM in which the virtual page is loaded. We list the frame number (23)
    in decimal, although it is really encoded in binary in the PTE entry (0…010111).
    A valid bit of 1 indicates that this entry stores a valid mapping.*'
  prefs: []
  type: TYPE_NORMAL
- en: Using a Page Table to Map Virtual Addresses to Physical Addresses
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are four steps to translating a virtual address to a physical address
    (shown in [Figure 13-24](ch13.xhtml#ch13fig24)). The particular OS/hardware combination
    determines which of the OS or the hardware performs all or part of each step.
    We assume a full-featured MMU that performs as much of the address translation
    as possible in hardware in describing these steps, but on some systems the OS
    may perform parts of these steps.
  prefs: []
  type: TYPE_NORMAL
- en: '1\. First, the MMU divides the bits of the virtual address into two parts:
    for a page size of 2^(*k*) bytes, the low-order *k* bits (VA bits *k –* 1 to 0)
    encode the byte offset (*d*) into the page, and the high-order *n – k* bits (VA
    bits *n –* 1 to *k*) encode the virtual page number (*p*).'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Next, the page number value (*p*) is used by the MMU as an index into the
    page table to access the PTE for page *p*. Most architectures have a *page table
    base register* (PTBR) that stores the RAM address of the running process’s page
    table. The value in the PTBR is combined with the page number value (*p*) to compute
    the address of the PTE for page *p*.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. If the valid bit in the PTE is set (is 1), then the frame number in the
    PTE represents a valid VA to PA mapping. If the valid bit is 0, then a page fault
    occurs, triggering the OS to handle this address translation (we discuss the OS
    page fault handling later).
  prefs: []
  type: TYPE_NORMAL
- en: 4\. The MMU constructs the physical address using the frame number (*f*) bits
    from the PTE entry as the high-order bits, and the page offset (*d*) bits from
    the VA as the low-order bits of the physical address.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-24: A process’s page table is used to perform virtual to physical
    address translations. The PTBR stores the base address of the currently running
    process’s page table.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'An Example: Mapping VA to PA with a Page Table'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Consider an example (tiny) paging system for which the page size is 4 bytes,
    the virtual addresses are 6 bits (the high-order 4 bits are the page number and
    the low-order 2 bits are the byte offset), and the physical addresses are 7 bits.
  prefs: []
  type: TYPE_NORMAL
- en: Assume that the page table for process P1 in this system looks like [Table 13-3](ch13.xhtml#ch13tab3)
    (values are listed in both decimal and binary).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 13-3:** Process P1’s Page Table'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Entry** | **Valid** | **Frame #** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 (0b0000) | 1 | 23 (0b10111) |'
  prefs: []
  type: TYPE_TB
- en: '| 1 (0b0001) | 0 | 17 (0b10001) |'
  prefs: []
  type: TYPE_TB
- en: '| 2 (0b0010) | 1 | 11 (0b01011) |'
  prefs: []
  type: TYPE_TB
- en: '| 3 (0b0011) | 1 | 16 (0b10000) |'
  prefs: []
  type: TYPE_TB
- en: '| 4 (0b0100) | 0 | 8 (0b01000) |'
  prefs: []
  type: TYPE_TB
- en: '| 5 (0b0101) | 1 | 14 (0b01110) |'
  prefs: []
  type: TYPE_TB
- en: '| ⋮ | ⋮ | ⋮ |'
  prefs: []
  type: TYPE_TB
- en: '| 15 (0b1111) | 1 | 30 (0b11110) |'
  prefs: []
  type: TYPE_TB
- en: Using the information provided in this example suggests several important things
    about address sizes, parts of addresses, and address translation.
  prefs: []
  type: TYPE_NORMAL
- en: First, the size of (number of entries in) the page table is determined by the
    number of bits in the virtual address and the page size in the system. The high-order
    4 bits of each 6-bit virtual address specifies the page number, so there are 16
    (2⁴) total pages of virtual memory. Since the page table has one entry for each
    virtual page, there are a total of 16 page table entries in each process’s page
    table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, the size of each page table entry (PTE) depends on the number of bits
    in the physical address and the page size in the system. Each PTE stores a valid
    bit and a physical frame number. The valid bit requires a single bit. The frame
    number requires 5 bits because physical addresses are 7 bits and the page offset
    is the low-order 2 bits (to address the 4 bytes on each page), which leaves the
    5 high-order bits for the frame number. Thus, each PTE entry requires 6 bits:
    1 for the valid bit, and 5 for the frame number.'
  prefs: []
  type: TYPE_NORMAL
- en: Third, the maximum sizes of virtual and physical memory are determined by the
    number of bits in the addresses. Because virtual addresses are 6 bits, 2⁶ bytes
    of memory can be addressed, so each process’s virtual address space is 2⁶ (or
    64) bytes. Similarly, the maximum size of physical memory is 2⁷ (or 128) bytes.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the page size, the number of bits in virtual and physical addresses,
    and the page table determine the mapping of virtual to physical addresses. For
    example, if process P1 executes an instruction to load a value from its virtual
    address 0b001110, its page table is used to convert the virtual address to physical
    address 0b1000010, which is then used to access the value in RAM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The virtual address (VA) to physical address (PA) translation steps are:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Separate the VA bits into the page number (*p*) and byte offset (*d*) bits:
    the high-order four bits are the page number (0b0011 or [page 3](preface.xhtml#page_3))
    and the lower-order two bits are the byte offset into the page (0b10 or byte 2).'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Use the page number (3) as an index into the page table to read the PTE
    for virtual [page 3](preface.xhtml#page_3) (PT[3]: valid:1 frame#:16).'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Check the valid bit for a valid PTE mapping. In this case, the valid bit
    is 1, so the PTE contains a valid mapping, meaning that virtual memory [page 3](preface.xhtml#page_3)
    is stored in physical memory frame 16.
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Construct the physical address using the five-bit frame number from the
    PTE as the high-order address bits (0b10000), and the low-order two-bit offset
    from the virtual address (0b10) as the lower-order two bits: the physical address
    is 0b1000010 (in RAM frame 16 at byte offset 2).'
  prefs: []
  type: TYPE_NORMAL
- en: Paging Implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Most computer hardware provides some support for paged virtual memory, and together
    the OS and hardware implement paging on a given system. At a minimum, most architectures
    provide a page table base register (PTBR) that stores the base address of the
    currently running process’s page table. To perform virtual-to-physical address
    translations, the virtual page number part of a virtual address is combined with
    the value stored in the PTBR to find the PTE entry for the virtual page. In other
    words, the virtual page number is an index into the process’s page table, and
    its value combined with the PTBR value gives the RAM address of the PTE for page
    *p* (e.g., PTBR + *p* × (PTE size) is the RAM address of the PTE for page *p*).
    Some architectures may support the full page table lookup by manipulating PTE
    bits in hardware. If not, then the OS needs to be interrupted to handle some parts
    of page table lookup and accessing the PTE bits to translate virtual addresses
    to physical addresses.
  prefs: []
  type: TYPE_NORMAL
- en: On a context switch, the OS *saves and restores* the PTBR values of processes
    to ensure that when a process runs on the CPU it accesses its own virtual-to-physical
    address mappings from its own page table in RAM. This is one mechanism through
    which the OS protects processes’ virtual address spaces from one another; changing
    the PTBR value on context switch ensures that a process cannot access the VA–PA
    mappings of another process, and thus it cannot read or write values at physical
    addresses that store the virtual address space contents of any other processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'An Example: Virtual to Physical Address Mappings of Two Processes'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As an example, consider an example system ([Table 13-4](ch13.xhtml#ch13tab4))
    with eight-byte pages, seven-bit virtual addresses, and six-bit physical addresses.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 13-4:** Example Process Page Tables'
  prefs: []
  type: TYPE_NORMAL
- en: '| **P1’s Page Table** | **P2’s Page Table** |'
  prefs: []
  type: TYPE_TB
- en: '| **Entry** | **Valid** | **Frame #** | **Entry** | **Valid** | **Frame #**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 3 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 2 | 1 | 1 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 6 | 2 | 1 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ⋮ |  |  | ⋮ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 1 | 7 | 11 | 0 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ⋮ |  |  | ⋮ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Given the current state of the (partially shown) page tables of two processes
    (P1 and P2) in [Table 13-4](ch13.xhtml#ch13tab4), let’s compute the physical addresses
    for the following sequence of virtual memory addresses generated from the CPU
    (each address is prefixed by the process that is running on the CPU):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: First, determine the division of bits in virtual and physical addresses. Since
    the page size is eight bytes, the three low-order bits of every address encodes
    the page offset (*d*). Virtual addresses are seven bits. Thus, with three bits
    for the page offset, this leaves the four high-order bits for specifying the page
    number (*p*). Since physical addresses are six bits long and the low-order three
    are for the page offset, the high-order three bits specify the frame number.
  prefs: []
  type: TYPE_NORMAL
- en: Next, for each virtual address, use its page number bits (*p*) to look up in
    the process’s page table the PTE for page *p*. If the valid bit (*v*) in the PTE
    is set, then use the frame number (*f*) for the high-order bits of the PA. The
    low-order bits of the PA come from the byte-offset bits (*d*) of the VA.
  prefs: []
  type: TYPE_NORMAL
- en: The results are shown in [Table 13-5](ch13.xhtml#ch13tab5) (note which page
    table is being used for the translation of each address).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 13-5:** Address Mappings for the Example Sequence of Memory Accesses
    from Processes P1 and P2'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Process** | **Virtual address** | *p* | *d* | **PTE** | *f* | *d* | **Physical
    address** |'
  prefs: []
  type: TYPE_TB
- en: '| P1 | 0000100 | 0000 | 100 | `PT[0]: 1(*v*), 3(*f*)` | 011 | 100 | 011100
    |'
  prefs: []
  type: TYPE_TB
- en: '| P1 | 0000000 | 0000 | 000 | `PT[0]: 1(*v*), 3(*f*)` | 011 | 000 | 011000
    |'
  prefs: []
  type: TYPE_TB
- en: '| P1 | 0010000 | 0010 | 000 | `PT[2]: 1(*v*), 6(*f*)` | 110 | 000 | 110000
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Context switch P1 to P2** |'
  prefs: []
  type: TYPE_TB
- en: '| P2 | 0010000 | 0010 | 000 | `PT[2]: 1(*v*), 5(*f*)` | 101 | 000 | 101000
    |'
  prefs: []
  type: TYPE_TB
- en: '| P2 | 0001010 | 0001 | 010 | `PT[1]: 1(*v*), 4(*f*)` | 100 | 010 | 100010
    |'
  prefs: []
  type: TYPE_TB
- en: '| P2 | 1011001 | 1011 | 001 | `PT[11]: 0(*v*), 3(*f*)` | Page fault (valid
    bit 0) |'
  prefs: []
  type: TYPE_TB
- en: '| **Context switch P2 to P1** |'
  prefs: []
  type: TYPE_TB
- en: '| P1 | 1011001 | 1011 | 001 | `PT[11]: 1(*v*), 7(*f*)` | 111 | 001 | 111001
    |'
  prefs: []
  type: TYPE_TB
- en: As one example, consider the first address accesses by process P1\. When P1
    accesses its virtual address 8 (0b0000100), the address is divided into its page
    number 0 (0b0000) and its byte offset 4 (0b100). The page number, 0, is used to
    look up PTE entry 0, whose valid bit is 1, indicating a valid page mapping entry,
    and whose frame number is 3 (0b011). The physical address (0b011100) is constructed
    using the frame number (0b011) as the high-order bits and the page offset (0b100)
    as the low-order bits.
  prefs: []
  type: TYPE_NORMAL
- en: When process P2 is context switched on the CPU, its page table mappings are
    used (note the different physical addresses when P1 and P2 access the same virtual
    address 0b0010000). When P2 accesses a PTE entry with a 0 valid bit, it triggers
    a page fault to the OS to handle.
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.4 Memory Efficiency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the primary goals of the operating system is to efficiently manage hardware
    resources. System performance is particularly dependent on how the OS manages
    the memory hierarchy. For example, if a process accesses data that are stored
    in RAM, then the process will run much faster than if those data are on disk.
  prefs: []
  type: TYPE_NORMAL
- en: The OS strives to increase the degree of multiprogramming in the system in order
    to keep the CPU busy doing real work while some processes are blocked waiting
    for an event like disk I/O. However, because RAM is fixed-size storage, the OS
    must make decisions about which process to load in RAM at any point in time, possibly
    limiting the degree of multiprogramming in the system. Even systems with a large
    amount of RAM (10s or 100s of gigabytes) often cannot simultaneously store the
    full address space of every process in the system. As a result, an OS can make
    more efficient use of system resources by running processes with only parts of
    their virtual address spaces loaded in RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Virtual Memory Using RAM, Disk, and Page Replacement
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: From “Locality” on [page 552](ch11.xhtml#lev1_88), we know that memory references
    usually exhibit a very high degree of locality. In terms of paging, this means
    that processes tend to access pages of their memory space with a high degree of
    temporal or spatial locality. It also means that at any point in its execution,
    a process is not typically accessing large extents of its address space. In fact,
    processes typically never access large extents of their full address spaces. For
    example, processes typically do not use the full extent of their stack or heap
    memory space.
  prefs: []
  type: TYPE_NORMAL
- en: One way in which the OS can make efficient use of both RAM and CPU is to treat
    RAM as a cache for disk. In doing so, the OS allows processes to run in the system
    only having some of their virtual memory pages loaded into physical frames of
    RAM. Their other virtual memory pages remain on secondary storage devices such
    as disk, and the OS only brings them into RAM when the process accesses addresses
    on these pages. This is another part of the OS’s *virtual memory* abstraction—the
    OS implements a view of a single large physical “memory” that is implemented using
    RAM storage in combination with disk or other secondary storage devices. Programmers
    do not need to explicitly manage their program’s memory, nor do they need to handle
    moving parts in and out of RAM as their program needs it.
  prefs: []
  type: TYPE_NORMAL
- en: By treating RAM as a cache for disk, the OS keeps in RAM only those pages from
    processes’ virtual address spaces that are being accessed or have been accessed
    recently. As a result, processes tend to have the set of pages that they are accessing
    stored in fast RAM and the set of pages that they do not access frequently (or
    at all) stored on slower disk. This leads to more efficient use of RAM because
    the OS uses RAM to store pages that are actually being used by running processes,
    and doesn’t waste RAM space to store pages that will not be accessed for a long
    time or ever. It also results in more efficient use of the CPU by allowing more
    processes to simultaneously share RAM space to store their active pages, which
    can result in an increase in the number of ready processes in the system, reducing
    times when the CPU is idle due to all the processes waiting for some event like
    disk I/O.
  prefs: []
  type: TYPE_NORMAL
- en: 'In virtual memory systems, however, processes sometimes try to access a page
    that is currently not stored in RAM (causing a *page fault*). When a page fault
    occurs, the OS needs to read the page from disk into RAM before the process can
    continue executing. The MMU reads a PTE’s valid bit to determine whether it needs
    to trigger a page fault exception. When it encounters a PTE whose valid bit is
    zero, it traps to the OS, which takes the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The OS finds a free frame (e.g., frame *j*) of RAM into which it will load
    the faulted page.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. It next issues a read to the disk to load the page from disk into frame
    *j* of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. When the read from disk has completed, the OS updates the PTE entry, setting
    the frame number to *j* and the valid bit to 1 (this PTE for the faulted page
    now has a valid mapping to frame *j*).
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Finally, the OS restarts the process at the instruction that caused the
    page fault. Now that the page table holds a valid mapping for the page that faulted,
    the process can access the virtual memory address that maps to an offset in physical
    frame *j*.
  prefs: []
  type: TYPE_NORMAL
- en: To handle a page fault, the OS needs to keep track of which RAM frames are free
    so that it can find a free frame of RAM into which the page read from disk can
    be stored. Operating systems often keep a list of free frames that are available
    for allocating on a page fault. If there are no available free RAM frames, then
    the OS picks a frame and replaces the page it stores with the faulted page. The
    PTE of the replaced page is updated, setting its valid bit to 0 (this page’s PTE
    mapping is no longer valid). The replaced page is written back to disk if its
    in-RAM contents differ from its on-disk version; if the owning process wrote to
    the page while it was loaded in RAM, then the RAM version of the page needs to
    be written to disk before being replaced so that the modifications to the page
    of virtual memory are not lost. PTEs often include a *dirty bit* that is used
    to indicate if the in-RAM copy of the page has been modified (written to). During
    page replacement, if the dirty bit of the replaced page is set, then the page
    needs to be written to disk before being replaced with the faulted page. If the
    dirty bit is 0, then the on-disk copy of the replaced page matches the in-memory
    copy, and the page does not need to be written to disk when replaced.
  prefs: []
  type: TYPE_NORMAL
- en: Our discussion of virtual memory has primarily focused on the *mechanism* part
    of implementing paged virtual memory. However, there is an important *policy*
    part of paging in the OS’s implementation. The OS needs to run a *page replacement
    policy* when free RAM is exhausted in the system. A page replacement policy picks
    a frame of RAM that is currently being used and replaces its contents with the
    faulted page; the current page is *evicted* from RAM to make room for storing
    the faulted page. The OS needs to implement a good page replacement policy for
    selecting which frame in RAM will be written back to disk to make room for the
    faulted page. For example, an OS might implement the *least recently used* (LRU)
    policy, which replaces the page stored in the frame of RAM that has been accessed
    least recently. LRU works well when there is a high degree of locality in memory
    accesses. There are many other policies that an OS may choose to implement. See
    an OS textbook for more information about page replacement policies.
  prefs: []
  type: TYPE_NORMAL
- en: Making Page Accesses Faster
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Although paging has many benefits, it also results in a significant slowdown
    to every memory access. In a paged virtual memory system, every load and store
    to a virtual memory address requires *two* RAM accesses: the first reads the page
    table entry to get the frame number for virtual-to-physical address translation,
    and the second reads or writes the byte(s) at the physical RAM address. Thus,
    in a paged virtual memory system, every memory access is twice as slow as in a
    system that supports direct physical RAM addressing.'
  prefs: []
  type: TYPE_NORMAL
- en: One way to reduce the additional overhead of paging is to cache page table mappings
    of virtual page numbers to physical frame numbers. When translating a virtual
    address, the MMU first checks for the page number in the cache. If found, then
    the page’s frame number mapping can be grabbed from the cache entry, avoiding
    one RAM access for reading the PTE.
  prefs: []
  type: TYPE_NORMAL
- en: A *translation look-aside buffer* (TLB) is a hardware cache that stores (page
    number, frame number) mappings. It is a small, fully associative cache that is
    optimized for fast lookups in hardware. When the MMU finds a mapping in the TLB
    (a TLB hit), a page table lookup is not needed, and only one RAM access is required
    to execute a load or store to a virtual memory address. When a mapping is not
    found in the TLB (a TLB miss), then an additional RAM access to the page’s PTE
    is required to first construct the physical address of the load or store to RAM.
    The mapping associated with a TLB miss is added into the TLB. With good locality
    of memory references, the hit rate in the TLB is very high, resulting in fast
    memory accesses in paged virtual memory—most virtual memory accesses require only
    a single RAM access. [Figure 13-25](ch13.xhtml#ch13fig25) shows how the TLB is
    used in virtual-to-physical address mappings.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-25: The translation look-aside buffer (TLB) is a small hardware
    cache of virtual page to physical frame mappings. The TLB is first searched for
    an entry for page *p*. If found, then no page table lookup is needed to translate
    the virtual address to its physical address.*'
  prefs: []
  type: TYPE_NORMAL
- en: 13.4 Interprocess Communication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Processes are one of the primary abstractions implemented by the OS. Private
    virtual address spaces are an important abstraction in multiprogrammed systems
    and are one way in which the OS prevents processes from interfering with one another’s
    execution state. However, sometimes a user or programmer may want their application
    processes to communicate with one another (or to share some of their execution
    state) as they run.
  prefs: []
  type: TYPE_NORMAL
- en: Operating systems typically implement support for several types of interprocess
    communication, or ways in which processes can communicate or share their execution
    state. *Signals* are a very restricted form of interprocess communication by which
    one process can send a signal to another process to notify it of some event. Processes
    can also communicate using *message passing*, in which the OS implements an abstraction
    of a message communication channel that is used by a process to exchange messages
    with another process. Finally, the OS may support interprocess communication through
    *shared memory* that allows a process to share all or part of its virtual address
    space with other processes. Processes with shared memory can read or write to
    addresses in shared space to communicate with one another.
  prefs: []
  type: TYPE_NORMAL
- en: 13.4.1 Signals
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A *signal* is a software interrupt that is sent by one process to another process
    via the OS. When a process receives a signal, its current execution point is interrupted
    by the OS to run signal handler code. If the signal handler returns, the process’s
    execution continues from where it was interrupted to handle the signal. Sometimes
    the signal handler causes the process to exit, and thus it does not continue its
    execution from where it left off.
  prefs: []
  type: TYPE_NORMAL
- en: Signals are similar to hardware interrupts and traps but are different from
    both. Whereas a trap is a synchronous software interrupt that occurs when a process
    explicitly invokes a system call, signals are asynchronous—a process may be interrupted
    by the receipt of a signal at any point in its execution. Signals also differ
    from asynchronous hardware interrupts in that they are triggered by software rather
    than hardware devices.
  prefs: []
  type: TYPE_NORMAL
- en: A process can send another process a signal by executing the `kill` system call,
    which requests that the OS post a signal to another process. The OS handles posting
    the signal to the target process and setting its execution state to run the signal
    handler code associated with the particular posted signal.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: The name of the `kill` system call is potentially misleading as well as unfortunately
    violent. Although it can be (and often is) used to deliver a termination signal,
    it is also used to send any other type of signal to a process.
  prefs: []
  type: TYPE_NORMAL
- en: The OS itself also uses signals to notify processes of certain events. For example,
    the OS posts a `SIGCHLD` signal to a process when one of its child processes exits.
  prefs: []
  type: TYPE_NORMAL
- en: Systems define a fixed number of signals (e.g., Linux defines 32 different signals).
    As a result, signals provide a limited way in which processes can communicate
    with one another, as opposed to other interprocess communication methods such
    as messaging or shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 13-6](ch13.xhtml#ch13tab6) lists some of the defined signals. See the
    man page (`man 7 signal`) for additional examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 13-6:** Example Signals Used for Interprocess Communication'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Signal** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `SIGSEGV` | Segmentation fault (e.g., dereferencing a null pointer) |'
  prefs: []
  type: TYPE_TB
- en: '| `SIGINT` | Interrupt process (e.g., CTRL-C in terminal window to kill process)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `SIGCHLD` | Child process has exited (e.g., a child is now a zombie after
    running `exit`) |'
  prefs: []
  type: TYPE_TB
- en: '| `SIGALRM` | Notify a process when a timer goes off (e.g., `alarm(2)` every
    2 secs) |'
  prefs: []
  type: TYPE_TB
- en: '| `SIGKILL` | Terminate a process (e.g., `pkill -9 a.out`) |'
  prefs: []
  type: TYPE_TB
- en: '| `SIGBUS` | Bus error occurred (e.g., a misaligned memory address to access
    an `int` value) |'
  prefs: []
  type: TYPE_TB
- en: '| `SIGSTOP` | Suspend a process, move to Blocked state (e.g., CTRL-Z) |'
  prefs: []
  type: TYPE_TB
- en: '| `SIGCONT` | Continue a blocked process (move it to the Ready state; e.g.,
    `bg` or `fg`) |'
  prefs: []
  type: TYPE_TB
- en: 'When a process receives a signal, one of several default actions can occur:
    the process can terminate, the signal can be ignored, the process can be blocked,
    or the process can be unblocked.'
  prefs: []
  type: TYPE_NORMAL
- en: The OS defines a default action and supplies the default signal handler code
    for every signal number. Application programmers, however, can change the default
    action of most signals and can write their own signal handler code. If an application
    program doesn’t register its own signal handler function for a particular signal,
    then the OS’s default handler executes when the process receives a signal. For
    some signals, the OS-defined default action cannot be overridden by application
    signal handler code. For example, if a process receives a `SIGKILL` signal, the
    OS will always force the process to exit, and receiving a `SIGSTOP` signal will
    always block the process until it receives a signal to continue (`SIGCONT`) or
    to exit (`SIGKILL`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Linux supports two different system calls that can be used to change the default
    behavior of a signal or to register a signal handler on a particular signal: `sigaction`
    and `signal`. Because `sigaction` is POSIX compliant and more featureful, it should
    be used in production software. However, we use `signal` in our example code because
    it is easier to understand.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is an example program^([3](ch13.xhtml#fn13_3)) that registers signal
    handlers for `SIGALRM`, `SIGINT`, and `SIGCONT` signals using the `signal` system
    call (error handling is removed for readability):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'When run, the process receives a `SIGALRM` every 5 seconds (due to the call
    to `alarm` in `main` and `sigalarm_handler`). The `SIGINT` and `SIGCONT` signals
    can be triggered by running the `kill` or `pkill` commands in another shell. For
    example, if the process’s PID is 1234 and its executable is `a.out`, then the
    following shell command sends the process `SIGINT` and `SIGCONT` signals, triggering
    their signal handler functions to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Writing a SIGCHLD handler
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Recall that when a process terminates, the OS delivers a `SIGCHLD` signal to
    its parent process. In programs that create child processes, the parent process
    does not always want to block on a call to `wait` until its child processes exit.
    For example, when a shell program runs a command in the background, it continues
    to run concurrently with its child process, handling other shell commands in the
    foreground as the child process runs in the background. A parent process, however,
    needs to call `wait` to reap its zombie child processes after they exit. If not,
    the zombie processes will never die and will continue to hold on to some system
    resources. In these cases, the parent process can register a signal handler on
    `SIGCHLD` signals. When the parent receives a `SIGCHLD` from an exited child process,
    its handler code runs and makes calls to `wait` to reap its zombie children.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a code snippet showing the implementation of a signal handler
    function for `SIGCHLD` signals. This snippet also shows parts of a `main` function
    that register the signal handler function for the `SIGCHLD` signal (note that
    this should be done before any calls to `fork`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This example passes –1 as the PID to `waitpid`, which means “reap any zombie
    child process.” It also passes the `WNOHANG` flag, which means that the call to
    `waitpid` does not block if there are no zombie child processes to reap. Also
    note that `waitpid` is called inside a `while` loop that continues as long as
    it returns a valid PID value (as long as it reaps a zombie child process). It
    is important that the signal handler function calls `waitpid` in a loop because
    as it is running, the process could receive additional `SIGCHLD` signals from
    other exited child process. The OS doesn’t keep track of the number of `SIGCHLD`
    signals a process receives, it just notes that the process received a `SIGCHLD`
    and interrupts its execution to run the handler code. As a result, without the
    loop, the signal handler could miss reaping some zombie children.
  prefs: []
  type: TYPE_NORMAL
- en: The signal handler executes whenever the parent receives a `SIGCHLD` signal,
    regardless of whether the parent is blocked on a call to `wait` or `waitpid`.
    If the parent is blocked on a call to `wait` when it receives a `SIGCHLD`, it
    wakes up and runs the signal handler code to reap one or more of its zombie children.
    It then continues execution at the point in the program after the call to `wait`
    (it just reaped an exited child process). If, however, the parent is blocked on
    a call to `waitpid` for a specific child, then the parent may or may not continue
    to block after its signal handler code runs to reap an exited child. The parent
    process continues execution after its call to `waitpid` if the signal handler
    code reaped the child for which it was waiting. Otherwise, the parent continues
    to block on the call to `waitpid` to wait for the specified child to exit. A call
    to `waitpid` with a PID of a nonexistent child process (perhaps one that was previously
    reaped in the signal handler loop) does not block the caller.
  prefs: []
  type: TYPE_NORMAL
- en: 13.4.2 Message Passing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One way in which processes with private virtual address spaces can communicate
    is through *message passing*—by sending and receiving messages to one another.
    Message passing allows programs to exchange arbitrary data rather than just a
    small set of predefined messages like those supported by signals. And operating
    systems typically implement a few different types of message passing abstractions
    that processes can use to communicate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The message passing interprocess communication model consists of three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Processes allocate some type of message channel from the OS. Example message
    channel types include *pipes* for one-way communication, and *sockets* for two-way
    communication. There may be additional connection setup steps that processes need
    to take to configure the message channel.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Processes use the message channel to send and receive messages to one another.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Processes close their end of the message channel when they are done using
    it.
  prefs: []
  type: TYPE_NORMAL
- en: A *pipe* is a one-way communication channel for two processes running on the
    same machine. One-way means that one end of the pipe is for sending messages (or
    writing to) only, and the other end of the pipe is for receiving messages (or
    for reading from) only. Pipes are commonly used in shell commands to send the
    output from one process to the input of another process.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the following command entered at a bash shell prompt
    that creates a pipe between two processes (the `cat` process outputs the contents
    of file `foo.c` and the pipe (`|`) redirects that output to the input of the `grep`
    command that searches for the string “factorial” in its input):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: To execute this command, the bash shell process calls the `pipe` system call
    to request that the OS creates a pipe communication. The pipe will be used by
    the shell’s two child processes (`cat` and `grep`). The shell program sets up
    the `cat` process’s `stdout` to write to the write end of the pipe and the `grep`
    process’s `stdin` to read from the read end of the pipe, so that when the child
    processes are created and run, the `cat` process’s output will be sent as input
    to the `grep` process (see [Figure 13-26](ch13.xhtml#ch13fig26)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-26: Pipes are unidirectional communication channels for processes
    on the same system. In this example, the `cat` process sends the `grep` process
    information by writing to the write end of the pipe. The `grep` process receives
    this information by reading from the read end of the pipe.*'
  prefs: []
  type: TYPE_NORMAL
- en: While pipes transmit data from one process to another in only one direction,
    other message passing abstractions allow processes to communicate in both directions.
    A *socket* is a two-way communication channel, which means that each end of a
    socket can be used for both sending and receiving messages. Sockets can be used
    by communicating processes running on the same computer or running on different
    computers connected by a network (see [Figure 13-27](ch13.xhtml#ch13fig27)). The
    computers could be connected by a *local area network* (LAN), which connects computers
    in a small area, such as a network in a university computer science department.
    The communicating processes could also be on different LANs, connected to the
    internet. As long as there exists some path through network connections between
    the two machines, the processes can use sockets to communicate.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-27: Sockets are bidirectional communication channels that can be
    used by communicating processes on different machines connected by a network.*'
  prefs: []
  type: TYPE_NORMAL
- en: Because each individual computer is its own system (hardware and OS), and because
    the OS on one system does not know about or manage resources on the other system,
    message passing is the only way in which processes on different computers can
    communicate. To support this type of communication, operating systems need to
    implement a common message passing protocol for sending and receiving messages
    over a network. TCP/IP is one example of a messaging protocol that can be used
    to send messages over the internet. When a process wants to send a message to
    another, it makes a `send` system call, passing the OS a socket on which it wants
    to transmit, the message buffer and possibly additional information about the
    message or its intended recipient. The OS takes care of packing up the message
    in the message buffer and sending it out over the network to the other machine.
    When an OS receives a message from the network, it unpacks the message and delivers
    it to the process on its system that has requested to receive the message. This
    process may be in a Blocked state waiting for the message to arrive. In this case,
    receipt of the message makes the process Ready to run again.
  prefs: []
  type: TYPE_NORMAL
- en: There are many system software abstractions built on top of message passing
    that hide the message passing details from the programmer. However, any communication
    between processes on different computers must use message passing at the lowest
    levels (communicating through shared memory or signals is not an option for processes
    running on different systems). In [Chapter 15](ch15.xhtml#ch15), we discuss message
    passing and the abstractions built atop it in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 13.4.3 Shared Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Message passing using sockets is useful for bidirectional communication between
    processes running on the same machine and between processes running on different
    machines. However, when two processes are running on the same machine, they can
    take advantage of shared system resources to communicate more efficiently than
    by using message passing.
  prefs: []
  type: TYPE_NORMAL
- en: For example, an operating system can support interprocess communication by allowing
    processes to share all or part of their virtual address spaces. One process can
    read and write values to the shared portion of its address space to communicate
    with other processes sharing the same memory region.
  prefs: []
  type: TYPE_NORMAL
- en: One way that the OS can implement partial address space sharing is by setting
    entries in the page tables of two or more processes to map to the same physical
    frames. [Figure 13-28](ch13.xhtml#ch13fig28) illustrates an example mapping. To
    communicate, one process writes a value to an address on a shared page, and another
    process subsequently reads the value.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/13fig28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-28: The OS can support sharing pages of virtual address space by
    setting entries in the page tables of sharing processes to the same physical frame
    number (e.g., frame 100). Note that processes do not need to use the same virtual
    address to refer to the shared page of physical memory.*'
  prefs: []
  type: TYPE_NORMAL
- en: If the OS supports partial shared memory, then it implements an interface to
    the programmer for creating and attaching to shared pages (or shared regions/segments)
    of memory. In Unix systems, the system call `shmget` creates or attaches to a
    shared memory segment. Each shared memory segment corresponds to a contiguous
    set of virtual addresses whose physical mappings are shared with other processes
    attaching to the same shared memory segment.
  prefs: []
  type: TYPE_NORMAL
- en: Operating systems also typically support sharing a single, full virtual address
    space. A *thread* is the OS abstraction of an execution control flow. A process
    has a single thread of execution control flow in a single virtual address space.
    A multithreaded process has multiple concurrent threads of execution control flow
    in a single, shared virtual address space—all threads share the full virtual address
    space of their containing process.
  prefs: []
  type: TYPE_NORMAL
- en: Threads can easily share execution state by reading and writing to shared locations
    in their common address space. For example, if one thread changes the value of
    a global variable, all other threads see the result of that change.
  prefs: []
  type: TYPE_NORMAL
- en: On a multiprocessor systems (SMP or multicore), individual threads of a multithreaded
    process can be scheduled to run simultaneously, *in parallel*, on the multiple
    cores. In [Chapter 14](ch14.xhtml#ch14), we discuss threads and parallel multithreaded
    programming in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 13.5 Summary and Other OS Functionality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we examined what an operating system is, how it works, and
    the role it plays in running application programs on the computer. As the system
    software layer between the computer hardware and application programs, the OS
    efficiently manages the computer hardware and implements abstractions that make
    the computer easier to use. Operating systems implement two abstractions, processes
    and virtual memory, to support multiprogramming (allowing more than one program
    running on the computer system at a time). The OS keeps track of all the processes
    in the system and their state, and it implements context switching of processes
    running on the CPU cores. The OS also provides a way for processes to create new
    processes, to exit, and to communicate with one another. Through virtual memory,
    the OS implements the abstraction of a private virtual memory space for each process.
    The virtual memory abstraction protects processes from seeing the effects of other
    processes sharing the computer’s physical memory space. Paging is one implementation
    of virtual memory that maps individual pages of each process’s virtual address
    space to frames of physical RAM space. Virtual memory is also a way in which the
    OS makes more efficient use of RAM; by treating RAM as a cache for disk, it allows
    pages of virtual memory space to be stored in RAM or on disk.
  prefs: []
  type: TYPE_NORMAL
- en: Our focus in this chapter on the operating system’s role in running a program,
    including the abstractions and mechanisms it implements to efficiently run programs,
    is in no way complete. There are many other implementation options and details
    and policy issues related to processes and process management, and to virtual
    memory and memory management. Additionally, operating systems implement many other
    important abstractions, functionality, and policies for managing and using the
    computer. For example, the OS implements filesystem abstractions for accessing
    stored data, protection mechanisms and security policies to protect users and
    the system, and scheduling policies for different OS and hardware resources.
  prefs: []
  type: TYPE_NORMAL
- en: Modern operating systems also implement support for interprocess communication,
    networking, and parallel and distributed computing. In addition, most operating
    systems include *hypervisor* support, which virtualizes the system hardware and
    allows the host OS to run multiple virtual guest operating systems. Virtualization
    supports the host OS that manages the computer hardware in booting and running
    multiple other operating systems on top of itself, each with its own private virtualized
    view of the underlying hardware. The host operating system’s hypervisor support
    manages the virtualization, including protection and sharing of the underlying
    physical resources among the guest operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, most operating systems provide some degree of extensibility by which
    a user (often a system administrator) can tune the OS. For example, most Unix-like
    systems allow users (usually requiring root, or superuser, privileges) to change
    sizes of OS buffers, caches, swap partitions, and to select from a set of different
    scheduling policies in OS subsystems and hardware devices. Through these modifications,
    a user can tune the system for the type of application workloads they run. These
    types of operating systems often support *loadable kernel modules*, which are
    executable code that can be loaded into the kernel and run in kernel mode. Loadable
    kernel modules are often used to add additional abstractions or functionality
    into the kernel as well as for loading device driver code into the kernel that
    is used to handle managing a particular hardware device. For more breadth and
    depth of coverage of operating systems, we recommend reading an operating systems
    textbook, such as *Operating Systems: Three Easy Pieces*.^([4](ch13.xhtml#fn13_4))'
  prefs: []
  type: TYPE_NORMAL
- en: Notes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[1.](ch13.xhtml#rfn13_1) Meltdown and Spectre. *[https://meltdownattack.com/](https://meltdownattack.com/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[2.](ch13.xhtml#rfn13_2) Available at *[https://diveintosystems.org/book/C13-OS/_attachments/fork.c](https://diveintosystems.org/book/C13-OS/_attachments/fork.c)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3.](ch13.xhtml#rfn13_3) Available at *[https://diveintosystems.org/book/C13-OS/_attachments/signals.c](https://diveintosystems.org/book/C13-OS/_attachments/signals.c)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4.](ch13.xhtml#rfn13_4) Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau,
    *Operating Systems: Three Easy Pieces*, Arpaci-Dusseau Books, 2018.'
  prefs: []
  type: TYPE_NORMAL
