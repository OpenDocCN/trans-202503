<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch09"><span epub:type="pagebreak" id="page_49"/><strong><span class="big">9</span><br/>GENERATIVE AI MODELS</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="noindent">What are the popular categories of deep generative models in deep learning (also called <em>generative AI</em>), and what are their respective downsides?</p>&#13;
<p class="indent">Many different types of deep generative models have been applied to generating different types of media: images, videos, text, and audio. Beyond these types of media, models can also be repurposed to generate domain-specific data, such as organic molecules and protein structures. This chapter will first define generative modeling and then outline each type of generative model and discuss its strengths and weaknesses.</p>&#13;
<h3 class="h3" id="ch00lev40"><strong>Generative vs. Discriminative Modeling</strong></h3>&#13;
<p class="noindent">In traditional machine learning, there are two primary approaches to modeling the relationship between input data (<em>x</em>) and output labels (<em>y</em>): generative models and discriminative models. <em>Generative models</em> aim to capture the underlying probability distribution of the input data <em>p</em>(<em>x</em>) or the joint distribution <em>p</em>(<em>x</em>, <em>y</em>) between inputs and labels. In contrast, <em>discriminative models</em> focus on modeling the conditional distribution <em>p</em>(<em>y</em>|<em>x</em>) of the labels given the inputs.</p>&#13;
<p class="indent">A classic example that highlights the differences between these approaches is to compare the naive Bayes classifier and the logistic regression <span epub:type="pagebreak" id="page_50"/>classifier. Both classifiers estimate the class label probabilities <em>p</em>(<em>y</em>|<em>x</em>) and can be used for classification tasks. However, logistic regression is considered a discriminative model because it directly models the conditional probability distribution <em>p</em>(<em>y</em>|<em>x</em>) of the class labels given the input features without making assumptions about the underlying joint distribution of inputs and labels. Naive Bayes, on the other hand, is considered a generative model because it models the joint probability distribution <em>p</em>(<em>x</em>, <em>y</em>) of the input features <em>x</em> and the output labels <em>y</em>. By learning the joint distribution, a generative model like naive Bayes captures the underlying data generation process, which enables it to generate new samples from the distribution if needed.</p>&#13;
<h3 class="h3" id="ch00lev41"><strong>Types of Deep Generative Models</strong></h3>&#13;
<p class="noindent">When we speak of <em>deep</em> generative models or deep generative AI, we often loosen this definition to include all types of models capable of producing realistic-looking data (typically text, images, videos, and sound). The remainder of this chapter briefly discusses the different types of deep generative models used to generate such data.</p>&#13;
<h4 class="h4" id="ch00levsec13"><em><strong>Energy-Based Models</strong></em></h4>&#13;
<p class="noindent"><em>Energy-based models (EBMs)</em> are a class of generative models that learn an energy function, which assigns a scalar value (energy) to each data point. Lower energy values correspond to more likely data points. The model is trained to minimize the energy of real data points while increasing the energy of generated data points. Examples of EBMs include <em>deep Boltzmann machines (DBMs)</em>. One of the early breakthroughs in deep learning, DBMs provide a means to learn complex representations of data. You can think of them as a form of unsupervised pretraining, resulting in models that can then be fine-tuned for various tasks.</p>&#13;
<p class="indent">Somewhat similar to naive Bayes and logistic regression, DBMs and multilayer perceptrons (MLPs) can be thought of as generative and discriminative counterparts, with DBMs focusing on capturing the data generation process and MLPs focusing on modeling the decision boundary between classes or mapping inputs to outputs.</p>&#13;
<p class="indent">A DBM consists of multiple layers of hidden nodes, as shown in <a href="ch09.xhtml#ch9fig1">Figure 9-1</a>. As the figure illustrates, along with the hidden layers, there’s usually a visible layer that corresponds to the observable data. This visible layer serves as the input layer where the actual data or features are fed into the network. In addition to using a different learning algorithm than MLPs (contrastive divergence instead of backpropagation), DBMs consist of binary nodes (neurons) instead of continuous ones.<span epub:type="pagebreak" id="page_51"/></p>&#13;
<div class="image"><img id="ch9fig1" src="../images/09fig01.jpg" alt="Image" width="517" height="317"/></div>&#13;
<p class="figcap"><em>Figure 9-1: A four-layer deep Boltzmann machine with three stacks of hidden nodes</em></p>&#13;
<p class="indent">Suppose we are interested in generating images. A DBM can learn the joint probability distribution over the pixel values in a simple image dataset like MNIST. To generate new images, the DBM then samples from this distribution by performing a process called <em>Gibbs sampling</em>. Here, the visible layer of the DBM represents the input image. To generate a new image, the DBM starts by initializing the visible layer with random values or, alternatively, uses an existing image as a seed. Then, after completing several Gibbs sampling iterations, the final state of the visible layer represents the generated image.</p>&#13;
<p class="indent">DBMs played an important historical role as one of the first deep generative models, but they are no longer very popular for generating data. They are expensive and more complicated to train, and they have lower expressivity compared to the newer models described in the following sections, which generally results in lower-quality generated samples.</p>&#13;
<h4 class="h4" id="ch00levsec14"><em><strong>Variational Autoencoders</strong></em></h4>&#13;
<p class="noindent"><em>Variational autoencoders (VAEs)</em> are built upon the principles of variational inference and autoencoder architectures. <em>Variational inference</em> is a method for approximating complex probability distributions by optimizing a simpler, tractable distribution to be as close as possible to the true distribution. <em>Autoencoders</em> are unsupervised neural networks that learn to compress input data into a low-dimensional representation (encoding) and subsequently reconstruct the original data from the compressed representation (decoding) by minimizing the reconstruction error.</p>&#13;
<p class="indent">The VAE model consists of two main submodules: an encoder network and a decoder network. The encoder network takes, for example, an input image and maps it to a latent space by learning a probability distribution over the latent variables. This distribution is typically modeled as a Gaussian with parameters (mean and variance) that are functions of the input <span epub:type="pagebreak" id="page_52"/>image. The decoder network then takes a sample from the learned latent distribution and reconstructs the input image from this sample. The goal of the VAE is to learn a compact and expressive latent representation that captures the essential structure of the input data while being able to generate new images by sampling from the latent space. (See <a href="ch01.xhtml">Chapter 1</a> for more details on latent representations.)</p>&#13;
<p class="indent"><a href="ch09.xhtml#ch9fig2">Figure 9-2</a> illustrates the encoder and decoder submodules of an auto-encoder, where <em>x′</em> represents the reconstructed input <em>x</em>. In a standard variational autoencoder, the latent vector is sampled from a distribution that approximates a standard Gaussian distribution.</p>&#13;
<div class="image"><img id="ch9fig2" src="../images/09fig02.jpg" alt="Image" width="460" height="112"/></div>&#13;
<p class="figcap"><em>Figure 9-2: An autoencoder</em></p>&#13;
<p class="indent">Training a VAE involves optimizing the model’s parameters to minimize a loss function composed of two terms: a reconstruction loss and a Kullback–Leibler-divergence (KL-divergence) regularization term. The reconstruction loss ensures that the decoded samples closely resemble the input images, while the KL-divergence term acts as a surrogate loss that encourages the learned latent distribution to be close to a predefined prior distribution (usually a standard Gaussian). To generate new images, we then sample points from the latent space’s prior (standard Gaussian) distribution and pass them through the decoder network, which generates new, diverse images that look similar to the training data.</p>&#13;
<p class="indent">Disadvantages of VAEs include their complicated loss function consisting of separate terms, as well as their often low expressiveness. The latter can result in blurrier images compared to other models, such as generative adversarial networks.</p>&#13;
<h4 class="h4" id="ch00levsec15"><em><strong>Generative Adversarial Networks</strong></em></h4>&#13;
<p class="noindent"><em>Generative adversarial networks (GANs)</em> are models consisting of interacting subnetworks designed to generate new data samples that are similar to a given set of input data. While both GANs and VAEs are latent variable models that generate data by sampling from a learned latent space, their architectures and learning mechanisms are fundamentally different.</p>&#13;
<p class="indent">GANs consist of two neural networks, a generator and a discriminator, that are trained simultaneously in an adversarial manner. The generator takes a random noise vector from the latent space as input and generates a synthetic data sample (such as an image). The discriminator’s task is to distinguish between real samples from the training data and fake samples generated by the generator, as illustrated in <a href="ch09.xhtml#ch9fig3">Figure 9-3</a>.<span epub:type="pagebreak" id="page_53"/></p>&#13;
<div class="image"><img id="ch9fig3" src="../images/09fig03.jpg" alt="Image" width="760" height="392"/></div>&#13;
<p class="figcap"><em>Figure 9-3: A generative adversarial network</em></p>&#13;
<p class="indent">The generator in a GAN somewhat resembles the decoder of a VAE in terms of its functionality. During inference, both GAN generators and VAE decoders take random noise vectors sampled from a known distribution (for example, a standard Gaussian) and transform them into synthetic data samples, such as images.</p>&#13;
<p class="indent">One significant disadvantage of GANs is their unstable training due to the adversarial nature of the loss function and learning process. Balancing the learning rates of the generator and discriminator can be difficult and can often result in oscillations, mode collapse, or non-convergence. The second main disadvantage of GANs is the low diversity of their generated outputs, often due to mode collapse. Here, the generator is able to fool the discriminator successfully with a small set of samples, which are representative of only a small subset of the original training data.</p>&#13;
<h4 class="h4" id="ch00levsec16"><em><strong>Flow-Based Models</strong></em></h4>&#13;
<p class="noindent">The core concept of <em>flow-based models</em>, also known as <em>normalizing flows</em>, is inspired by long-standing methods in statistics. The primary goal is to transform a simple probability distribution (like a Gaussian) into a more complex one using invertible transformations.</p>&#13;
<p class="indent">Although the concept of normalizing flows has been a part of the statistics field for a long time, the implementation of early flow-based deep learning models, particularly for image generation, is a relatively recent development. One of the pioneering models in this area was the <em>non-linear independent components estimation (NICE)</em> approach. NICE begins with a simple probability distribution, often something straightforward like a normal distribution. You can think of this as a kind of “random noise,” or data with no particular shape or structure. NICE then applies a series of transformations to this simple distribution. Each transformation is designed to make the data <span epub:type="pagebreak" id="page_54"/>look more like the final target (for instance, the distribution of real-world images). These transformations are “invertible,” meaning we can always reverse them back to the original simple distribution. After several successive transformations, the simple distribution has morphed into a complex distribution that closely matches the distribution of the target data (such as images). We can now generate new data that looks like the target data by picking random points from this complex distribution.</p>&#13;
<p class="indent"><a href="ch09.xhtml#ch9fig4">Figure 9-4</a> illustrates the concept of a flow-based model, which maps the complex input distribution to a simpler distribution and back.</p>&#13;
<div class="image"><img id="ch9fig4" src="../images/09fig04.jpg" alt="Image" width="395" height="137"/></div>&#13;
<p class="figcap"><em>Figure 9-4: A flow-based model</em></p>&#13;
<p class="indent">At first glance, the illustration is very similar to the VAE illustration in <a href="ch09.xhtml#ch9fig2">Figure 9-2</a>. However, while VAEs use neural network encoders like convolutional neural networks, the flow-based model uses simpler decoupling layers, such as simple linear transformations. Additionally, while the decoder in a VAE is independent of the encoder, the data-transforming functions in the flow-based model are mathematically inverted to obtain the outputs.</p>&#13;
<p class="indent">Unlike VAEs and GANs, flow-based models provide exact likelihoods, which gives us insights into how well the generated samples fit the training data distribution. This can be useful in anomaly detection or density estimation, for example. However, the quality of flow-based models for generating image data is usually lower than GANs. Flow-based models also often require more memory and computational resources than GANs or VAEs since they must store and compute inverses of transformations.</p>&#13;
<h4 class="h4" id="ch00levsec17"><em><strong>Autoregressive Models</strong></em></h4>&#13;
<p class="noindent"><em>Autoregressive models</em> are designed to predict the next value based on current (and past) values. LLMs for text generation, like ChatGPT (discussed further in <a href="ch17.xhtml">Chapter 17</a>), are one popular example of this type of model.</p>&#13;
<p class="indent">Similar to generating one word at a time, in the context of image generation, autoregressive models like PixelCNN try to predict one pixel at a time, given the pixels they have seen so far. Such a model might predict pixels from top left to bottom right, in a raster scan order, or in any other defined order.</p>&#13;
<p class="indent">To illustrate how autoregressive models generate an image one pixel at a time, suppose we have an image of size <em>H × W</em> (where <em>H</em> is the height and <em>W</em> is the width), ignoring the color channel for simplicity’s sake. This image consists of <em>N</em> pixels, where <em>i</em> = 1, . . . , <em>N</em>. The probability of observing a particular image in the dataset is then <em>P</em>(<em>Image</em>) = <em>P</em>(<em>i</em><sub>1</sub>, <em>i</em><sub>2</sub>, . . . , <em>i<sub>N</sub></em>). Based <span epub:type="pagebreak" id="page_55"/>on the chain rule of probability in statistics, we can decompose this joint probability into conditional probabilities:</p>&#13;
<div class="image1"><img src="../images/f0055-01.jpg" alt="Image" width="757" height="78"/></div>&#13;
<p class="noindent">Here, <em>P</em>(<em>i</em><sub>1</sub>) is the probability of the first pixel, <em>P</em>(<em>i</em><sub>2</sub>|<em>i</em><sub>1</sub>) is the probability of the second pixel given the first pixel, <em>P</em>(<em>i</em><sub>3</sub>|<em>i</em><sub>1</sub>, <em>i</em><sub>2</sub>) is the probability of the third pixel given the first and second pixels, and so on.</p>&#13;
<p class="indent">In the context of image generation, an autoregressive model essentially tries to predict one pixel at a time, as described earlier, given the pixels it has seen so far. <a href="ch09.xhtml#ch9fig5">Figure 9-5</a> illustrates this process, where pixels <em>i</em><sub>1</sub>, . . . , <em>i</em><sub>53</sub> represent the context and pixel <em>i</em><sub>54</sub> is the next pixel to be generated.</p>&#13;
<div class="image"><img id="ch9fig5" src="../images/09fig05.jpg" alt="Image" width="271" height="270"/></div>&#13;
<p class="figcap"><em>Figure 9-5: Autoregressive pixel generation</em></p>&#13;
<p class="indent">The advantage of autoregressive models is that the next-pixel (or word) prediction is relatively straightforward and interpretable. In addition, autoregressive models can compute the likelihood of data exactly, similar to flow-based models, which can be useful for tasks like anomaly detection. Furthermore, autoregressive models are easier to train than GANs as they don’t suffer from issues like mode collapse and other training instabilities.</p>&#13;
<p class="indent">However, autoregressive models can be slow at generating new samples. This is because they have to generate data one step at a time (for example, pixel by pixel for images), which can be computationally expensive. Autoregressive models may also struggle to capture long-range dependencies because each output is conditioned only on previously generated outputs.</p>&#13;
<p class="indent">In terms of overall image quality, autoregressive models are therefore usually worse than GANs but are easier to train.</p>&#13;
<h4 class="h4" id="ch00levsec18"><em><strong>Diffusion Models</strong></em></h4>&#13;
<p class="noindent">As discussed in the previous section, flow-based models transform a simple distribution (such as a standard normal distribution) into a complex one (the target distribution) by applying a sequence of invertible and differentiable transformations (flows). Like flow-based models, <em>diffusion models</em> also <span epub:type="pagebreak" id="page_56"/>apply a series of transformations. However, the underlying concept is fundamentally different.</p>&#13;
<p class="indent">Diffusion models transform the input data distribution into a simple noise distribution over a series of steps using stochastic differential equations. Diffusion is a stochastic process in which noise is progressively added to the data until it resembles a simpler distribution, like Gaussian noise. To generate new samples, the process is then reversed, starting from noise and progressively removing it.</p>&#13;
<p class="indent"><a href="ch09.xhtml#ch9fig6">Figure 9-6</a> outlines the process of adding and removing Gaussian noise from an input image <em>x</em>. During inference, the reverse diffusion process is used to generate a new image <em>x</em>, starting with the noise tensor <em>z<sub>n</sub></em> sampled from a Gaussian distribution.</p>&#13;
<div class="image"><img id="ch9fig6" src="../images/09fig06.jpg" alt="Image" width="302" height="250"/></div>&#13;
<p class="figcap"><em>Figure 9-6: The diffusion process</em></p>&#13;
<p class="indent">While both diffusion models and flow-based models are generative models aiming to learn complex data distributions, they approach the problem from different angles. Flow-based models use deterministic invertible transformations, while diffusion models use the aforementioned stochastic diffusion process.</p>&#13;
<p class="indent">Recent projects have established state-of-the-art performance in generating high-quality images with realistic details and textures. Diffusion models are also easier to train than GANs. The downside of diffusion models, however, is that they are slower to sample from since they require running a series of sequential steps, similar to flow-based models and autoregressive models.</p>&#13;
<h4 class="h4" id="ch00levsec19"><em><strong>Consistency Models</strong></em></h4>&#13;
<p class="noindent"><em>Consistency models</em> train a neural network to map a noisy image to a clean one. The network is trained on a dataset of pairs of noisy and clean images and learns to identify patterns in the clean images that are modified by noise. Once the network is trained, it can be used to generate reconstructed images from noisy images in one step.</p>&#13;
<p class="indent">Consistency model training employs an <em>ordinary differential equation (ODE)</em> trajectory, a path that a noisy image follows as it is gradually denoised. The ODE trajectory is defined by a set of differential equations that describe how the noise in the image changes over time, as illustrated in <a href="ch09.xhtml#ch9fig7">Figure 9-7</a>.<span epub:type="pagebreak" id="page_57"/></p>&#13;
<div class="image"><img id="ch9fig7" src="../images/09fig07.jpg" alt="Image" width="566" height="214"/></div>&#13;
<p class="figcap"><em>Figure 9-7: Trajectories of a consistency model for image denoising</em></p>&#13;
<p class="indent">As <a href="ch09.xhtml#ch9fig7">Figure 9-7</a> demonstrates, we can think of consistency models as models that learn to map any point from a probability flow ODE, which smoothly converts data to noise, to the input.</p>&#13;
<p class="indent">At the time of writing, consistency models are the most recent type of generative AI model. Based on the original paper proposing this method, consistency models rival diffusion models in terms of image quality. Consistency models are also faster than diffusion models because they do not require an iterative process to generate images; instead, they generate images in a single step.</p>&#13;
<p class="indent">However, while consistency models allow for faster inference, they are still expensive to train because they require a large dataset of pairs of noisy and clean images.</p>&#13;
<h3 class="h3" id="ch00lev42"><strong>Recommendations</strong></h3>&#13;
<p class="noindent">Deep Boltzmann machines are interesting from a historical perspective since they were one of the pioneering models to effectively demonstrate the concept of unsupervised learning. Flow-based and autoregressive models may be useful when you need to estimate exact likelihoods. However, other models are usually the first choice when it comes to generating high-quality images.</p>&#13;
<p class="indent">In particular, VAEs and GANs have competed for years to generate the best high-fidelity images. However, in 2022, diffusion models began to take over image generation almost entirely. Consistency models are a promising alternative to diffusion models, but it remains to be seen whether they become more widely adopted to generate state-of-the-art results. The trade-off here is that sampling from diffusion models is generally slower since it involves a sequence of noise-removal steps that must be run in order, similar to autoregressive models. This can make diffusion models less practical for some applications requiring fast sampling.</p>&#13;
<h3 class="h3" id="ch00lev43"><strong>Exercises</strong></h3>&#13;
<p class="number"><strong>9-1.</strong> How would we evaluate the quality of the images generated by a generative AI model?</p>&#13;
<p class="number"><strong>9-2.</strong> Given this chapter’s description of consistency models, how would we use them to generate new images?<span epub:type="pagebreak" id="page_58"/></p>&#13;
<h3 class="h3" id="ch00lev44"><strong>References</strong></h3>&#13;
<ul>&#13;
<li class="noindent">The original paper proposing variational autoencoders: Diederik P. Kingma and Max Welling, “Auto-Encoding Variational Bayes” (2013), <em><a href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a></em>.</li>&#13;
<li class="noindent">The paper introducing generative adversarial networks: Ian J. Good-fellow et al., “Generative Adversarial Networks” (2014), <em><a href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</a></em>.</li>&#13;
<li class="noindent">The paper introducing NICE: Laurent Dinh, David Krueger, and Yoshua Bengio, “NICE: Non-linear Independent Components Estimation” (2014), <em><a href="https://arxiv.org/abs/1410.8516">https://arxiv.org/abs/1410.8516</a></em>.</li>&#13;
<li class="noindent">The paper proposing the autoregressive PixelCNN model: Aaron van den Oord et al., “Conditional Image Generation with PixelCNN Decoders” (2016), <em><a href="https://arxiv.org/abs/1606.05328">https://arxiv.org/abs/1606.05328</a></em>.</li>&#13;
<li class="noindent">The paper introducing the popular Stable Diffusion latent diffusion model: Robin Rombach et al., “High-Resolution Image Synthesis with Latent Diffusion Models” (2021), <em><a href="https://arxiv.org/abs/2112.10752">https://arxiv.org/abs/2112.10752</a></em>.</li>&#13;
<li class="noindent">The Stable Diffusion code implementation: <em><a href="https://github.com/CompVis/stable-diffusion">https://github.com/CompVis/stable-diffusion</a></em>.</li>&#13;
<li class="noindent">The paper originally proposing consistency models: Yang Song et al., “Consistency Models” (2023), <em><a href="https://arxiv.org/abs/2303.01469">https://arxiv.org/abs/2303.01469</a></em>.</li>&#13;
</ul>&#13;
</div>
</div>
</body></html>