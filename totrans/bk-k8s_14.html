<html><head></head><body>
<h2 class="h2" id="ch12"><span epub:type="pagebreak" id="page_205"/><span class="big">12</span><br/>CONTAINER RUNTIME</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">In the previous chapter, we saw how the control plane manages and monitors the state of the cluster. However, it is the container runtime, especially the <span class="literal">kubelet</span> service, that creates, starts, stops, and deletes containers to actually bring the cluster to the desired state. In this chapter, we’ll explore how <span class="literal">kubelet</span> is configured in our cluster and how it operates.</p>&#13;
<p class="indent">As part of this exploration, we’ll address how <span class="literal">kubelet</span> manages to host the control plane while also being dependent on it. Finally, we’ll look at node maintenance in a Kubernetes cluster, including how to shut down a node for maintenance, issues that can prevent a node from working correctly, how the cluster behaves if a node suddenly becomes unavailable, and how the node behaves when it loses its cluster connection.</p>&#13;
<h3 class="h3" id="ch00lev1sec51">Node Service</h3>&#13;
<p class="noindent">The primary service that turns a regular host into a Kubernetes node is <span class="literal">kubelet</span>. Because of its criticality to a Kubernetes cluster, we’ll look in detail at how it is configured and how it behaves.</p>&#13;
<div class="box5">&#13;
<p class="boxtitle-d"><span epub:type="pagebreak" id="page_206"/><strong>CONTAINERD AND CRI-O</strong></p>&#13;
<p class="noindents">The examples for this chapter provide automated scripts to launch a cluster using either of two container runtimes: <span class="literal">containerd</span> and CRI-O. We’ll primarily use the <span class="literal">containerd</span> installation, though we’ll briefly look at the configuration difference. The CRI-O cluster is there to allow you to experiment with a separate container runtime. It also illustrates the fact that <span class="literal">kubelet</span> hides this difference from the rest of the cluster, as the rest of the cluster configuration is unaffected by a container runtime change.</p>&#13;
</div>&#13;
<p class="indent">We installed <span class="literal">kubelet</span> as a package on all of our nodes when we set up our cluster in <a href="ch06.xhtml#ch06">Chapter 6</a>, and the automation has been setting it up similarly for each chapter thereafter.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on getting set up.</em></p>&#13;
</div>&#13;
<p class="indent">The <span class="literal">kubelet</span> package also includes a system service. Our operating system is using <span class="literal">systemd</span> to run services, so we can get service information using <span class="literal">systemctl</span>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">systemctl status kubelet</span>&#13;
  kubelet.service - kubelet: The Kubernetes Node Agent&#13;
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; ...&#13;
    Drop-In: /etc/systemd/system/kubelet.service.d&#13;
               10-kubeadm.conf&#13;
     Active: active (running) since ...</pre>&#13;
<p class="indent">The first time <span class="literal">kubelet</span> started, it didn’t have the configuration needed to join the cluster. When we ran <span class="literal">kubeadm</span>, it created the file <em>10-kubeadm.conf</em> shown in the preceding output. This file configures the <span class="literal">kubelet</span> service for the cluster by setting command line parameters.</p>&#13;
<p class="indent"><a href="ch12.xhtml#ch12list1">Listing 12-1</a> gives us a look at the command line parameters that are passed to the <span class="literal">kubelet</span> service.</p>&#13;
<pre>root@host01:~# <span class="codestrong1">strings /proc/$(pgrep kubelet)/cmdline</span>&#13;
/usr/bin/kubelet&#13;
--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf&#13;
--kubeconfig=/etc/kubernetes/kubelet.conf&#13;
--config=/var/lib/kubelet/config.yaml&#13;
--container-runtime=remote&#13;
--container-runtime-endpoint=/run/containerd/containerd.sock&#13;
--node-ip=192.168.61.11&#13;
--pod-infra-container-image=k8s.gcr.io/pause:3.4.1</pre>&#13;
<p class="caption" id="ch12list1"><em>Listing 12-1: Kubelet command line</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_207"/>The <span class="literal">pgrep kubelet</span> embedded command outputs the process ID of the <span class="literal">kubelet</span> service. We then use this to print the command line of the process using the <em>/proc</em> Linux virtual filesystem. We use <span class="literal">strings</span> to print this file rather than <span class="literal">cat</span> because each separate command line parameter is null-terminated and <span class="literal">strings</span> turns this into a nice multiline display.</p>&#13;
<p class="indent">The <span class="literal">kubelet</span> service needs three main groups of configuration options: <em>cluster configuration</em>, <em>container runtime configuration</em>, and <em>network configuration</em>.</p>&#13;
<h4 class="h4" id="ch00lev2sec81">Kubelet Cluster Configuration</h4>&#13;
<p class="noindent">The cluster configuration options tell <span class="literal">kubelet</span> how to communicate with the cluster and how to authenticate. When <span class="literal">kubelet</span> starts for the first time, it uses the <span class="literal">bootstrap-kubeconfig</span> shown in <a href="ch12.xhtml#ch12list1">Listing 12-1</a> to find the cluster, verify the server certificate, and authenticate using the bootstrap token we discussed in <a href="ch11.xhtml#ch11">Chapter 11</a>. This bootstrap token is used to submit a Certificate Signing Request (CSR) for this new node. The <span class="literal">kubelet</span> then downloads the signed client certificate from the API server and stores it in <em>/etc/kubernetes/kubelet.conf</em>, the location specified by the <span class="literal">kubeconfig</span> option. This <em>kubelet.conf</em> file follows the same format that is used to configure <span class="literal">kubectl</span> to talk to the API server, as we saw in <a href="ch11.xhtml#ch11">Chapter 11</a>. After <em>kubelet.conf</em> has been written, the bootstrap file is deleted.</p>&#13;
<p class="indent">The <em>/var/lib/kubelet/config.yaml</em> file specified in <a href="ch12.xhtml#ch12list1">Listing 12-1</a> also contains important configuration information. To pull metrics from <span class="literal">kubelet</span>, we need to set it up with its own server certificate, not just a client certificate, and we need to configure how it authenticates its own clients. Here is the relevant content from the configuration file, created by <span class="literal">kubeadm</span>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cat /var/lib/kubelet/config.yaml</span>&#13;
...&#13;
authentication:&#13;
  anonymous:&#13;
    enabled: false&#13;
  webhook:&#13;
    cacheTTL: 0s&#13;
    enabled: true&#13;
  x509:&#13;
    clientCAFile: /etc/kubernetes/pki/ca.crt&#13;
...</pre>&#13;
<p class="indent">The <span class="literal">authentication</span> section tells <span class="literal">kubelet</span> not to allow anonymous requests, but to allow both webhook bearer tokens as well as any client certificates signed by the cluster certificate authority. The YAML resource file we installed for the metrics server includes a ServiceAccount that is used in its Deployment, so it is automatically injected with credentials that it can use to authenticate to <span class="literal">kubelet</span> instances, as we saw in <a href="ch11.xhtml#ch11">Chapter 11</a>.</p>&#13;
<h4 class="h4" id="ch00lev2sec82"><span epub:type="pagebreak" id="page_208"/>Kubelet Container Runtime Configuration</h4>&#13;
<p class="noindent">The container runtime configuration options tell <span class="literal">kubelet</span> how to connect to the container runtime so that <span class="literal">kubelet</span> can manage containers on the local machine. Because <span class="literal">kubelet</span> expects the runtime to support the Container Runtime Interface (CRI) standard, only a couple of settings are needed, as shown in <a href="ch12.xhtml#ch12list1">Listing 12-1</a>.</p>&#13;
<p class="indent">The first key setting is <span class="literal">container-runtime</span>, which can be set to either <span class="literal">remote</span> or <span class="literal">docker</span>. Kubernetes predates the separation of the Docker engine from the <span class="literal">containerd</span> runtime, so it had legacy support for Docker that used a <em>shim</em> to emulate the standard CRI interface. Because we are using <span class="literal">containerd</span> directly and not via the Docker shim or Docker engine, we set this to <span class="literal">remote</span>.</p>&#13;
<p class="indent">Next, we specify the path to the container runtime using the <span class="literal">container-runtime-endpoint</span> setting. The value in this case is <em>/run/containerd/containerd.sock</em>. The <span class="literal">kubelet</span> connects to this Unix socket to send CRI requests and receive status.</p>&#13;
<p class="indent">The <span class="literal">container-runtime-endpoint</span> command line setting is the only difference needed to switch the cluster between <span class="literal">containerd</span> and CRI-O. Additionally, it is automatically detected by <span class="literal">kubeadm</span> when the node is initialized, so the only difference in the automated scripts is to install CRI-O rather than <span class="literal">containerd</span> prior to installing Kubernetes. If we look at the command line for <span class="literal">kubelet</span> in our CRI-O cluster, we see only one change in the command line options:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">strings /proc/$(pgrep kubelet)/cmdline</span>&#13;
...&#13;
--container-runtime-endpoint=/var/run/crio/crio.sock&#13;
...</pre>&#13;
<p class="indent">The rest of the command line options are identical to our <span class="literal">containerd</span> cluster.</p>&#13;
<p class="indent">Finally, we have one more setting that is relevant to the container runtime: <span class="literal">pod-infra-container-image</span>. This specifies the Pod infrastructure image. We saw this image in <a href="ch02.xhtml#ch02">Chapter 2</a> in the form of a <span class="literal">pause</span> process that was the owner of Linux namespaces created for our containers. In this case, this <span class="literal">pause</span> process will come from the container image <span class="literal">k8s.gcr.io/pause:3.4.1</span>.</p>&#13;
<p class="indent">It’s highly convenient to have a separate container to own the namespaces that are shared between the containers in a Pod. Because the <span class="literal">pause</span> process doesn’t really do anything, it is very reliable and isn’t likely to crash, so it can continue to own these shared namespaces even if the other containers in the Pod terminate unexpectedly.</p>&#13;
<p class="indent">The <span class="literal">pause</span> image clocks in at around 300kb, as we can see by running <span class="literal">crictl</span> on one of our nodes:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">crictl images</span>&#13;
IMAGE             TAG                 IMAGE ID            SIZE&#13;
,,,&#13;
k8s.gcr.io/pause  3.4.1               0f8457a4c2eca       301kB&#13;
...</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_209"/>Additionally, the <span class="literal">pause</span> process uses practically no CPU, so the effect on our nodes of having an extra process for every Pod is minimal.</p>&#13;
<h4 class="h4" id="ch00lev2sec83">Kubelet Network Configuration</h4>&#13;
<p class="noindent">Network configuration helps <span class="literal">kubelet</span> integrate itself into the cluster and to integrate Pods into the overall cluster network. As we saw in <a href="ch08.xhtml#ch08">Chapter 8</a>, the actual Pod network setup is performed by a network plug-in, but the <span class="literal">kubelet</span> has a couple of important roles as well.</p>&#13;
<p class="indent">Our <span class="literal">kubelet</span> command line includes one option relevant to the network configuration: <span class="literal">node-ip</span>. It’s an optional flag, and if it is not present, <span class="literal">kubelet</span> will try to determine the IP address it should use to communicate with the API server. However, specifying the flag directly is useful because it guarantees that our cluster works in cases for which nodes have multiple network interfaces (such as the Vagrant configuration in this book’s examples, where a separate internal network is used for cluster communication).</p>&#13;
<p class="indent">In addition to this one command line option, <span class="literal">kubeadm</span> places two important network settings in <em>/var/lib/kubelet/config.yaml</em>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cat /var/lib/kubelet/config.yaml</span>&#13;
...&#13;
clusterDNS:&#13;
- 10.96.0.10&#13;
clusterDomain: cluster.local&#13;
...</pre>&#13;
<p class="indent">These settings are used to provide the <em>/etc/resolv.conf</em> file to all containers. The <span class="literal">clusterDNS</span> entry provides the IP address of this DNS server, whereas the <span class="literal">clusterDomain</span> entry provides a default domain for searches so that we can distinguish between hostnames inside the cluster and hostnames on external networks.</p>&#13;
<p class="indent">Let’s take a quick look at how these values are provided to the Pod. We’ll begin by creating a Pod:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/pod.yaml</span> &#13;
pod/debug created</pre>&#13;
<p class="indent">After a few seconds, when the Pod is running, we can get a shell:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti debug -- /bin/sh</span>&#13;
/ #</pre>&#13;
<p class="indent">Notice that <em>/etc/resolv.conf</em> is a separately mounted file in our container:</p>&#13;
<pre>/ # <span class="codestrong1">mount | grep resolv</span>&#13;
/dev/sda1 on /etc/resolv.conf type ext4 ...</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_210"/>Its contents reflect the <span class="literal">kubelet</span> configuration:</p>&#13;
<pre>/ # <span class="codestrong1">cat /etc/resolv.conf</span> &#13;
search default.svc.cluster.local svc.cluster.local cluster.local &#13;
nameserver 10.96.0.10&#13;
options ndots:5</pre>&#13;
<p class="indent">This DNS configuration points to the DNS server that is part of the Kubernetes cluster core components, enabling the Service lookup we saw in <a href="ch09.xhtml#ch09">Chapter 9</a>. Depending on the DNS configuration in your network, you might see other items in the <span class="literal">search</span> list beyond what is shown here.</p>&#13;
<p class="indent">While we’re here, note also that <em>/run/secrets/kubernetes.io/serviceaccount</em> is also a separately mounted directory in our container. This directory contains the ServiceAccount information we saw in <a href="ch11.xhtml#ch11">Chapter 11</a> to enable authentication with the API server from within a container:</p>&#13;
<pre>/ # <span class="codestrong1">mount | grep run</span>&#13;
tmpfs on /run/secrets/kubernetes.io/serviceaccount type tmpfs (ro,relatime)</pre>&#13;
<p class="indent">In this case, the mounted directory is of type <span class="literal">tmpfs</span> because <span class="literal">kubelet</span> has created an in-memory filesystem to hold the authentication information.</p>&#13;
<p class="indent">Let’s finish by exiting the shell session and deleting the Pod (we no longer need it):</p>&#13;
<pre>/ # <span class="codestrong1">exit</span>&#13;
root@host01:~# <span class="codestrong1">kubectl delete pod debug</span></pre>&#13;
<p class="indent">This cleanup will make upcoming Pod listings clearer as we look at how the cluster reacts when a node stops working. Before we do that, we have one more key mystery to solve: how <span class="literal">kubelet</span> can host the control plane and also depend on it.</p>&#13;
<h3 class="h3" id="ch00lev1sec52">Static Pods</h3>&#13;
<p class="noindent">We have something of a chicken-or-egg problem with creating our cluster. We want <span class="literal">kubelet</span> to manage the control plane components as Pods because that makes it easier to monitor, maintain, and update the control plane components. However, <span class="literal">kubelet</span> is dependent on the control plane to determine what containers to run. The solution is for <span class="literal">kubelet</span> to support static Pod definitions that it pulls from the filesystem and runs automatically prior to having its control plane connection.</p>&#13;
<p class="indent">This static Pod configuration is handled in <em>/var/lib/kubelet/config.yaml</em>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cat /var/lib/kubelet/config.yaml</span> &#13;
...&#13;
staticPodPath: /etc/kubernetes/manifests&#13;
...</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_211"/>If we look in <em>/etc/kubernetes/manifests</em>, we see a number of YAML files. These files were placed by <span class="literal">kubeadm</span> and define the Pods necessary to run the control plane components for this node:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ls -1 /etc/kubernetes/manifests</span>&#13;
etcd.yaml&#13;
kube-apiserver.yaml&#13;
kube-controller-manager.yaml&#13;
kube-scheduler.yaml</pre>&#13;
<p class="indent">As expected, we see a YAML file for each of the three essential control plane services we discussed in <a href="ch11.xhtml#ch11">Chapter 11</a>. We also see a Pod definition for <span class="literal">etcd</span>, the component that stores the cluster’s state and helps elect a leader for our highly available cluster. We’ll look at <span class="literal">etcd</span> in more detail in <a href="ch16.xhtml#ch16">Chapter 16</a>.</p>&#13;
<p class="indent">Each of these files contains a Pod definition just like the ones we’ve already seen:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cat /etc/kubernetes/manifests/kube-apiserver.yaml</span> &#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
...&#13;
  name: kube-apiserver&#13;
  namespace: kube-system&#13;
spec:&#13;
  containers:&#13;
  - command:&#13;
    - kube-apiserver&#13;
...</pre>&#13;
<p class="indent">The <span class="literal">kubelet</span> service continually monitors this directory for any changes, and updates the corresponding static Pod accordingly, which makes it possible for <span class="literal">kubeadm</span> to upgrade the cluster’s control plane on a rolling basis without any downtime.</p>&#13;
<p class="indent">Cluster add-ons like Calico and Longhorn could also be run using this directory, but they instead use a DaemonSet to have the cluster run a Pod on each node. This makes sense, as a DaemonSet can be managed once for the whole cluster, guaranteeing a consistent configuration across all nodes.</p>&#13;
<p class="indent">This static Pod directory is different on our three control plane nodes, <em>host01</em> through <em>host03</em>, compared to our “normal” node, <em>host04</em>. To make <em>host04</em> a normal node, <span class="literal">kubeadm</span> omits the control plane static Pod files from <em>/etc/kubernetes/manifests</em>:</p>&#13;
<pre>root@host04:~# <span class="codestrong1">ls -1 /etc/kubernetes/manifests</span>&#13;
root@host04:~#</pre>&#13;
<p class="indent">Note that this command is run from <em>host04</em>, our sole normal node in this cluster.</p>&#13;
<h3 class="h3" id="ch00lev1sec53"><span epub:type="pagebreak" id="page_212"/>Node Maintenance</h3>&#13;
<p class="noindent">The controller manager component of the control plane continuously monitors nodes to ensure that they are still connected and healthy. The <span class="literal">kubelet</span> service has the responsibility of reporting node information, including node memory consumption, disk consumption, and connection to the underlying container runtime. If a node becomes unhealthy, the control plane will shift Pods to other nodes to maintain the requested scale for Deployments, and will not schedule any new Pods to the node until it is healthy again.</p>&#13;
<h4 class="h4" id="ch00lev2sec84">Node Draining and Cordoning</h4>&#13;
<p class="noindent">If we know that we need to perform maintenance on a node, such as a reboot, we can tell the cluster to transfer Pods off of the node and mark the node as unscheduleable. We do this using the <span class="literal">kubectl drain</span> command.</p>&#13;
<p class="indent">To see an example, let’s create a Deployment with eight Pods, making it likely that each of our nodes will get a Pod:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/deploy.yaml</span> &#13;
deployment.apps/debug created</pre>&#13;
<p class="indent">If we allow enough time for startup, we can see that the Pods are distributed across the nodes:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods -o wide</span>&#13;
NAME                     READY   STATUS    ... NODE   ...&#13;
debug-8677494fdd-7znxn   1/1     Running   ... host02 ...  &#13;
debug-8677494fdd-9dgvd   1/1     Running   ... host03 ...  &#13;
debug-8677494fdd-hv6mt   1/1     Running   ... host04 ...  &#13;
debug-8677494fdd-ntqjp   1/1     Running   ... host02 ...  &#13;
debug-8677494fdd-pfw5n   1/1     Running   ... host03 ...  &#13;
debug-8677494fdd-qbhmn   1/1     Running   ... host02 ...  &#13;
debug-8677494fdd-qp9zv   1/1     Running   ... host03 ...  &#13;
debug-8677494fdd-xt8dm   1/1     Running   ... host03 ...</pre>&#13;
<p class="indent">To minimize the size of our test cluster, our normal node <span class="literal">host04</span> is small in terms of resources, so in this example it gets only one of the Pods. But that’s sufficient to see what happens when we shut down the node. This process is somewhat random, so if you don’t see any Pods allocated to <span class="literal">host04</span>, you can delete the Deployment and try again or scale it down and then back up, as we do in the next example.</p>&#13;
<p class="indent">To shut down the node, we use the <span class="literal">kubectl drain</span> command:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl drain --ignore-daemonsets host04</span>&#13;
node/host04 cordoned&#13;
WARNING: ignoring DaemonSet-managed Pods: ...&#13;
...&#13;
pod/debug-8677494fdd-hv6mt evicted&#13;
node/host04 evicted</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_213"/>We need to provide the <span class="literal">--ignore-daemonsets</span> option because all of our nodes have Calico and Longhorn DaemonSets, and of course, those Pods cannot be transferred to another node.</p>&#13;
<p class="indent">The eviction will take a little time. When it’s complete, we can see that the Deployment has created a Pod on another node, which keeps our Pod count at eight:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods -o wide</span>&#13;
NAME                     READY   STATUS    ... NODE     ...&#13;
debug-8677494fdd-7znxn   1/1     Running   ... host02   ...&#13;
debug-8677494fdd-9dgvd   1/1     Running   ... host03   ...&#13;
debug-8677494fdd-ntqjp   1/1     Running   ... host02   ...&#13;
debug-8677494fdd-pfw5n   1/1     Running   ... host03   ...&#13;
debug-8677494fdd-qbhmn   1/1     Running   ... host02   ...&#13;
debug-8677494fdd-qfnml   1/1     Running   ... host01   ...&#13;
debug-8677494fdd-qp9zv   1/1     Running   ... host03   ...&#13;
debug-8677494fdd-xt8dm   1/1     Running   ... host03   ...</pre>&#13;
<p class="indent">Additionally, the node has been <em>cordoned</em>, thus no more Pods will be scheduled on it:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get nodes</span>&#13;
NAME     STATUS                     ROLES        ...&#13;
host01   Ready                      control-plane...&#13;
host02   Ready                      control-plane...&#13;
host03   Ready                      control-plane...&#13;
host04   Ready,SchedulingDisabled   &lt;none&gt;       ...</pre>&#13;
<p class="indent">At this point, it is safe to stop <span class="literal">kubelet</span> or the container runtime, to reboot the node, or even to delete it from Kubernetes entirely:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete node host04</span>&#13;
node "host04" deleted</pre>&#13;
<p class="indent">This deletion removes the node information from the cluster’s storage, but because the node still has a valid client certificate and all its configuration, a simple restart of the <span class="literal">kubelet</span> service on <span class="literal">host04</span> will add it back to the cluster. First let’s restart <span class="literal">kubelet</span>:</p>&#13;
<pre>root@host04:~# <span class="codestrong1">systemctl restart kubelet</span></pre>&#13;
<p class="indent">Be sure to do this on <span class="literal">host04</span>. Next, back on <span class="literal">host01</span>, if we wait for <span class="literal">kubelet</span> on <span class="literal">host04</span> to finish cleaning up from its previous run and to reinitialize, we can see it return in the list of nodes:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get nodes</span>&#13;
NAME     STATUS   ROLES        ...&#13;
host01   Ready    control-plane...&#13;
host02   Ready    control-plane...&#13;
<span epub:type="pagebreak" id="page_214"/>host03   Ready    control-plane...&#13;
host04   Ready    &lt;none&gt;       ...</pre>&#13;
<p class="indent">Note that the cordon has been removed and <span class="literal">host04</span> no longer shows a status that includes <span class="literal">SchedulingDisabled</span>. This is one way to remove the cordon. The other is to do it directly using <span class="literal">kubectl uncordon</span>.</p>&#13;
<h4 class="h4" id="ch00lev2sec85">Unhealthy Nodes</h4>&#13;
<p class="noindent">Kubernetes will also shift Pods on a node automatically if the node becomes unhealthy as a result of resource constraints such as insufficient memory or disk space. Let’s simulate a low-memory condition on <span class="literal">host04</span> so that we can see this in action.</p>&#13;
<p class="indent">First, we’ll need to reset the scale of our <span class="literal">debug</span> Deployment to ensure that new Pods are allocated onto <span class="literal">host04</span>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl scale deployment debug --replicas=1</span>&#13;
deployment.apps/debug scaled&#13;
root@host01:~# <span class="codestrong1">kubectl scale deployment debug --replicas=12</span>&#13;
deployment.apps/debug scaled</pre>&#13;
<p class="indent">We first scale the Deployment all the way down, and then we scale it back up. This way, we get more chances to schedule at least one Pod on <span class="literal">host04</span>. As soon as the Pods have had a chance to settle, we see Pods on <span class="literal">host04</span> again:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods -o wide</span>&#13;
NAME                     READY   STATUS    ... NODE     ...&#13;
...&#13;
debug-8677494fdd-j7cth   1/1     Running   ... host04   ...&#13;
debug-8677494fdd-jlj4v   1/1     Running   ... host04   ...&#13;
...</pre>&#13;
<p class="indent">We can check the current statistics for our nodes using <span class="literal">kubectl top</span>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl top nodes</span>&#13;
NAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   &#13;
host01   503m         25%    1239Mi          65%       &#13;
host02   518m         25%    1346Mi          71%       &#13;
host03   534m         26%    1382Mi          73%       &#13;
host04   288m         14%    542Mi           29%</pre>&#13;
<p class="indent">We have 2GB total on <span class="literal">host04</span>, and currently we’re using more than 500MiB. By default, <span class="literal">kubelet</span> will evict Pods when there is less than 100MiB of memory remaining. We could try to use up memory on the node to get below that default threshold, but it’s chancy because using up so much memory could make our node behave badly. Instead, let’s update the eviction limit. To do this, we’ll add lines to <em>/var/lib/kubelet/config.yaml</em> and then restart <span class="literal">kubelet</span>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_215"/>Here’s the additional configuration we’ll add to our <span class="literal">kubelet</span> config file:</p>&#13;
<p class="noindent6"><em>node-evict.yaml</em></p>&#13;
<pre>evictionHard:&#13;
  memory.available: "1900Mi"</pre>&#13;
<p class="indent">This tells <span class="literal">kubelet</span> to start evicting Pods if it has less than 1,900MiB available. For nodes in our example cluster, that will happen right away. Let’s apply this change:</p>&#13;
<pre>root@host04:~# <span class="codestrong1">cat /opt/node-evict.yaml &gt;&gt; /var/lib/kubelet/config.yaml</span>&#13;
root@host04:~# <span class="codestrong1">systemctl restart kubelet</span></pre>&#13;
<p class="indent">Be sure to run these commands on <span class="literal">host04</span>. The first command adds additional lines to the <span class="literal">kubelet</span> config file. The second command restarts <span class="literal">kubelet</span> so that it picks up the change.</p>&#13;
<p class="indent">If we check on the node status for <span class="literal">host04</span>, it will appear to still be ready:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get nodes</span>&#13;
NAME     STATUS   ROLES        ...&#13;
host01   Ready    control-plane...&#13;
host02   Ready    control-plane...&#13;
host03   Ready    control-plane...&#13;
host04   Ready    &lt;none&gt;       ...</pre>&#13;
<p class="indent">However, the node’s event log makes clear what is happening:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe node host04</span>&#13;
Name:               host04&#13;
...&#13;
  Normal   NodeHasInsufficientMemory  6m31s                ...&#13;
  Warning  EvictionThresholdMet       7s (x14 over 6m39s)  ...</pre>&#13;
<p class="indent">The node starts evicting Pods, and the cluster automatically creates new Pods on other nodes as needed to stay at the desired scale:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods -o wide</span>&#13;
NAME                     READY   STATUS        ... NODE     ...&#13;
debug-8677494fdd-4274k   1/1     Running       ... host01   ...&#13;
debug-8677494fdd-4pnzb   1/1     Running       ... host01   ...&#13;
debug-8677494fdd-5nw6n   1/1     Running       ... host01   ...&#13;
debug-8677494fdd-7kbp8   1/1     Running       ... host03   ...&#13;
debug-8677494fdd-dsnp5   1/1     Running       ... host03   ...&#13;
debug-8677494fdd-hgdbc   1/1     Running       ... host01   ...&#13;
debug-8677494fdd-j7cth   1/1     Running       ... host04   ...&#13;
debug-8677494fdd-jlj4v   0/1     OutOfmemory   ... host04   ...&#13;
debug-8677494fdd-lft7h   1/1     Running       ... host01   ...&#13;
debug-8677494fdd-mnk6r   1/1     Running       ... host01   ...&#13;
debug-8677494fdd-pc8q8   1/1     Running       ... host01   ...&#13;
<span epub:type="pagebreak" id="page_216"/>debug-8677494fdd-sr2kw   0/1     OutOfmemory   ... host04   ...&#13;
debug-8677494fdd-tgpb2   1/1     Running       ... host03   ...&#13;
debug-8677494fdd-vnjks   0/1     OutOfmemory   ... host04   ...&#13;
debug-8677494fdd-xn8t8   1/1     Running       ... host02   ...</pre>&#13;
<p class="indent">Pods allocated to <span class="literal">host04</span> show <span class="literal">OutOfMemory</span>, and they have been replaced with Pods on other nodes. The Pods are stopped on the node, but unlike the previous case for which we drained the node, the Pods are not automatically terminated. Even if the node recovers from its low-memory situation, the Pods will continue to show up in the list of Pods, stuck in the <span class="literal">OutOfMemory</span> state, until <span class="literal">kubelet</span> is restarted.</p>&#13;
<h4 class="h4" id="ch00lev2sec86">Node Unreachable</h4>&#13;
<p class="noindent">We have one more case to look at. In our previous two examples, <span class="literal">kubelet</span> could communicate with the control plane to update its status, allowing the control plane to act accordingly. But what happens if there is a network issue or sudden power failure and the node loses its connection to the cluster without being able to report that it is shutting down? In that case, the cluster will record the node status as unknown, and after a timeout, it will start shifting Pods onto other nodes.</p>&#13;
<p class="indent">Let’s simulate this. We’ll begin by restoring <span class="literal">host04</span> to its proper working order:</p>&#13;
<pre>root@host04:~# <span class="codestrong1">sed -i '/^evictionHard/,+2d' /var/lib/kubelet/config.yaml</span> &#13;
root@host04:~# <span class="codestrong1">systemctl restart kubelet</span></pre>&#13;
<p class="indent">Be sure to run these commands on <span class="literal">host04</span>. The first command removes the two lines we added to the <span class="literal">kubelet</span> config, whereas the second restarts <span class="literal">kubelet</span> to pick up the change. We now can rescale our Deployment again so that it is redistributed:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl scale deployment debug --replicas=1</span>&#13;
root@host01:~# <span class="codestrong1">kubectl scale deployment debug --replicas=12</span></pre>&#13;
<p class="indent">As before, after you’ve run these commands, allow a few minutes for the Pods to settle. Then, use <span class="codestrong">kubectl get pods -o wide</span> to verify that at least one Pod was allocated to <span class="literal">host04</span>.</p>&#13;
<p class="indent">We’re now ready to forcibly disconnect <span class="literal">host04</span> from the cluster. We’ll do this by adding a firewall rule:</p>&#13;
<pre>root@host04:~# <span class="codestrong1">iptables -I INPUT -s 192.168.61.10 -j DROP</span>&#13;
root@host04:~# <span class="codestrong1">iptables -I OUTPUT -d 192.168.61.10 -j DROP</span></pre>&#13;
<p class="indent">Be sure to run this on <span class="literal">host04</span>. The first command tells the firewall to drop all traffic coming from the IP address <span class="literal">192.168.61.10</span>, which is the highly available IP that is shared by all three control plane nodes. The second command tells the firewall to drop all traffic going to that same IP address.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_217"/>After a minute or so, <span class="literal">host04</span> will show a state of <span class="literal">NotReady</span>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get nodes</span>&#13;
NAME     STATUS     ROLES        ...&#13;
host01   Ready      control-plane...&#13;
host02   Ready      control-plane...&#13;
host03   Ready      control-plane...&#13;
host04   NotReady   &lt;none&gt;       ...</pre>&#13;
<p class="indent">And if we wait a few minutes, the Pods on <span class="literal">host04</span> will be shown as <span class="literal">Terminating</span> because the cluster gives up on those Pods and shifts them to other nodes:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods -o wide</span>&#13;
NAME                     READY   STATUS        ... NODE     ...&#13;
debug-8677494fdd-2wrn2   1/1     Running       ... host01   ...&#13;
debug-8677494fdd-4lz48   1/1     Running       ... host02   ...&#13;
debug-8677494fdd-78874   1/1     Running       ... host01   ...&#13;
debug-8677494fdd-7f8fw   1/1     Running       ... host01   ...&#13;
debug-8677494fdd-9vb5m   1/1     Running       ... host03   ...&#13;
debug-8677494fdd-b7vj6   1/1     Running       ... host03   ...&#13;
debug-8677494fdd-c2c4v   1/1     Terminating   ... host04   ...&#13;
debug-8677494fdd-c8tzv   1/1     Running       ... host03   ...&#13;
debug-8677494fdd-d2r6b   1/1     Terminating   ... host04   ...&#13;
debug-8677494fdd-d5t6b   1/1     Running       ... host01   ...&#13;
debug-8677494fdd-j7cth   1/1     Terminating   ... host04   ...&#13;
debug-8677494fdd-jjfsl   1/1     Terminating   ... host04   ...&#13;
debug-8677494fdd-nqb8z   1/1     Running       ... host03   ...&#13;
debug-8677494fdd-sskd5   1/1     Running       ... host02   ...&#13;
debug-8677494fdd-wz6c6   1/1     Terminating   ... host04   ...&#13;
debug-8677494fdd-x5b4w   1/1     Running       ... host02   ...&#13;
debug-8677494fdd-zfbml   1/1     Running       ... host01   ...</pre>&#13;
<p class="indent">However, because <span class="literal">kubelet</span> on <span class="literal">host04</span> can’t connect to the control plane, it is unaware that it should be shutting down its Pods. If we check to see what containers are running on <span class="literal">host04</span>, we still see multiple containers:</p>&#13;
<pre>root@host04:~# <span class="codestrong1">crictl ps</span>&#13;
CONTAINER           IMAGE          ...  STATE      NAME  ...&#13;
2129a1cb00607       16ea53ea7c652  ...  Running    debug ...&#13;
cfd7fd6142321       16ea53ea7c652  ...  Running    debug ...&#13;
0289ffa5c816d       16ea53ea7c652  ...  Running    debug ...&#13;
fb2d297d11efb       16ea53ea7c652  ...  Running    debug ...&#13;
...</pre>&#13;
<p class="indent">Not only are the Pods still running, but because of the way we cut off the connection, they are still able to communicate with the rest of the cluster. This is very important. Kubernetes will do its best to run the number of instances requested and to respond to errors, but it can only do that based on <span epub:type="pagebreak" id="page_218"/>the information it has available. In this case, because <span class="literal">kubelet</span> on <span class="literal">host04</span> can’t talk to the control plane, Kubernetes has no way of knowing that the Pods are still running. When building applications for a distributed system like a Kubernetes cluster, you should recognize that some types of errors can have surprising results, like partial network connectivity or a different number of instances compared to what is specified. In more advanced application architectures that include rolling updates, this can even lead to cases in which old versions of application components are still running unexpectedly. Be sure to build applications that are resilient in the face of these kinds of surprising behaviors.</p>&#13;
<h3 class="h3" id="ch00lev1sec54">Final Thoughts</h3>&#13;
<p class="noindent">Ultimately, to have a Kubernetes cluster, we need nodes that can run containers, and that means instances of <span class="literal">kubelet</span> connected to the control plane and a container runtime. In this chapter, we’ve inspected how to configure <span class="literal">kubelet</span> and how the cluster behaves when nodes leave or enter the cluster, either intentionally or through an outage.</p>&#13;
<p class="indent">One of the key themes of this chapter is the way that Kubernetes acts to keep the specified number of Pods running, even in the face of node issues. In the next chapter, we’ll see how that monitoring extends inside the container to its processes, ensuring that the processes run as expected. We’ll see how to specify probes that allow Kubernetes to monitor containers, and how the cluster responds when a container is unhealthy.</p>&#13;
</body></html>