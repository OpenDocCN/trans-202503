<html><head></head><body><section class="chapter" epub:type="chapter" id="traffic_shaping_with_queues_and_prioriti" title="Chapter&#xA0;7.&#xA0;Traffic Shaping with Queues and Priorities"><div class="titlepage"><div><div><h2 class="title">Chapter 7. Traffic Shaping with Queues and Priorities</h2></div></div></div><div class="mediaobject"><a id="med_id00012"/><img alt="Traffic Shaping with Queues and Priorities" src="httpatomoreillycomsourcenostarchimages2127149.png.jpg"/></div><p><a class="indexterm" id="iddle1979"/><a class="indexterm" id="iddle1980"/>In this chapter, we look at how to use traffic shaping to allocate bandwidth resources efficiently and according to a specified policy. If the term <span class="emphasis"><em>traffic shaping</em></span> seems unfamiliar, rest assured it means what you think it means: that you’ll be altering the way your network allocates resources in order to satisfy the requirements of your users and their applications. With a proper understanding of your network traffic and the applications and users that generate it, you can, in fact, go quite a bit of distance toward “bigger, better, faster, more” just by optimizing your network for the traffic that’s actually supposed to pass there.</p><p>A small but powerful arsenal of traffic-shaping tools is at your disposal; all of them work by introducing nondefault behavior into your network setup to bend the realities of your network according to your wishes. Traffic shaping for PF contexts currently comes in two flavors: the once experimental <a class="indexterm" id="iddle1674"/><a class="indexterm" id="iddle1709"/><a class="indexterm" id="iddle1990"/><span class="emphasis"><em>ALTQ</em></span> (short for <span class="emphasis"><em>alternate queuing</em></span>) framework, now considered old-style after some 15 years of service, and the newer OpenBSD <span class="emphasis"><em>priorities and queuing</em></span> system introduced in OpenBSD 5.5.</p><p>In the first part of the chapter, we introduce traffic shaping by looking at the features of the new OpenBSD priority and queuing system. If you’re about to set up on OpenBSD 5.5 or newer, you can jump right in, starting with the next section, <a class="xref" href="ch07.html#always-on_priority_and_queues_for_traffi" title="Always-On Priority and Queues for Traffic Shaping">Always-On Priority and Queues for Traffic Shaping</a>. This is also where the main traffic-shaping concepts are introduced with examples.</p><p>On OpenBSD 5.4 and earlier as well as other BSDs where the PF code wasn’t current with OpenBSD 5.5, traffic shaping was the domain of the ALTQ system. On OpenBSD, ALTQ was removed after one transitional release, leaving only the newer traffic-shaping system in place from OpenBSD 5.6 onward. If you’re interested in converting an existing ALTQ setup to the new system, you’ll most likely find <a class="xref" href="ch07.html#transitioning_from_altq_to_priorities_an" title="Transitioning from ALTQ to Priorities and Queues">Transitioning from ALTQ to Priorities and Queues</a> useful; this section highlights the differences between the older ALTQ system and the new system.</p><p>If you’re working with an operating system where the queues system introduced in OpenBSD 5.5 isn’t yet available, you’ll want to study the ALTQ traffic-shaping subsystem, which is described in <a class="xref" href="ch07.html#directing_traffic_with_altq" title="Directing Traffic with ALTQ">Directing Traffic with ALTQ</a>. If you’re learning traffic-shaping concepts and want to apply them to an ALTQ setup, please read the first part of this chapter before diving into ALTQ-specific configuration details.</p><div class="sect1" title="Always-On Priority and Queues for Traffic Shaping"><div class="titlepage"><div><div><h2 class="title" id="always-on_priority_and_queues_for_traffi" style="clear: both">Always-On Priority and Queues for Traffic Shaping</h2></div></div></div><p>Managing your bandwidth has a lot in common with balancing your checkbook or handling other resources that are either scarce or available in finite quantities. The resource is available in a constant supply with hard upper limits, and you need to allocate the resource with maximum <span class="emphasis"><em>efficiency</em></span>, according to the <span class="emphasis"><em>priorities</em></span> set out in your <span class="emphasis"><em>policy</em></span> or <span class="emphasis"><em>specification</em></span>.</p><p>OpenBSD 5.5 and newer offers several different options for managing your bandwidth resources via classification mechanisms in our PF rule sets. We’ll take a look at what you can do with pure <span class="emphasis"><em>traffic prioritization</em></span> first and then move on to how to subdivide your bandwidth resources by allocating defined subsets of your traffic to <span class="emphasis"><em>queues</em></span>.</p><div class="note" title="Note"><h3 class="title"><a id="ch07note01"/>Note</h3><p><span class="emphasis"><em>The always-on priorities were introduced as a teaser of sorts in OpenBSD 5.0. After several years in development and testing, the new queuing system was finally committed in time to be included in OpenBSD 5.5, which was released on May 1, 2014. If you’re starting your traffic shaping from scratch on OpenBSD 5.5 or newer or you’re considering doing so, this section is the right place to start. If you’re upgrading from an earlier OpenBSD version or transitioning from another ALTQ system to a recent OpenBSD, you’ll most likely find the following section, <a class="xref" href="ch07.html#transitioning_from_altq_to_priorities_an" title="Transitioning from ALTQ to Priorities and Queues">Transitioning from ALTQ to Priorities and Queues</a>, useful.</em></span></p></div><div class="sect2" title="Shaping by Setting Traffic Priorities"><div class="titlepage"><div><div><h3 class="title" id="shaping_by_setting_traffic_priorities">Shaping by Setting Traffic Priorities</h3></div></div></div><p><a class="indexterm" id="iddle1491"/><a class="indexterm" id="iddle1673"/><a class="indexterm" id="iddle1683"/><a class="indexterm" id="iddle1685"/><a class="indexterm" id="iddle1890"/><a class="indexterm" id="iddle1993"/><a class="indexterm" id="iddle2029"/>If you’re mainly interested in pushing certain kinds of traffic ahead of others, you may be able to achieve what you want by simply setting priorities: assigning a higher priority to some items so that they receive attention before others.</p><div class="sect3" title="The prio Priority Scheme"><div class="titlepage"><div><div><h4 class="title" id="prio_priority_scheme">The prio Priority Scheme</h4></div></div></div><p>Starting with OpenBSD 5.0, a priority scheme for classifying network traffic on a per-rule basis is available. The range of priorities is from 0 to 7, where 0 is lowest priority. Items assigned priority 7 will skip ahead of everything else, and the default value 3 is automatically assigned for most kinds of traffic. The priority scheme, which you’ll most often hear referred to as <code class="literal">prio</code> after the PF syntax keyword, is always enabled, and you can tweak your traffic by setting priorities via your <code class="literal">match</code> or <code class="literal">pass</code> rules.</p><p>For example, to speed up your outgoing SSH traffic to the max, you could put a rule like this in your configuration:</p><a id="pro_id00250"/><pre class="programlisting">pass proto tcp to port ssh set prio 7</pre><p>Then your SSH traffic would be served before anything else.</p><p>You could then examine the rest of your rule set and decide what traffic is more or less important, what you would like always to reach its destination, and what parts of your traffic you feel matter less.</p><p>To push your Web traffic ahead of everything else and bump up the priority for network time and name services, you could amend your configuration with rules like these:</p><a id="pro_id00251"/><pre class="programlisting">pass proto tcp to port { www https } set prio 7&#13;
pass proto { udp tcp } to port { domain ntp } set prio 6</pre><p>Or if you have a rule set that already includes rules that match criteria other than just the port, you could achieve much the same effect by writing your priority traffic shaping as <code class="literal">match</code> rules instead:</p><a id="pro_id00252"/><pre class="programlisting">match proto tcp to port { www https } set prio 7&#13;
match proto { udp tcp } to port { domain ntp } set prio 6</pre><p>In some networks, time-sensitive traffic, like Voice over Internet Protocol (VoIP), may need special treatment. For VoIP, a priority setup like this may improve phone conversation quality:</p><a id="pro_id00253"/><pre class="programlisting">voip_ports="{ 2027 4569 5036 5060 10000:20000 }"&#13;
match proto udp to port $voip_ports set prio 7</pre><p>But do check your VoIP application’s documentation for information on what specific ports it uses. In any case, using <code class="literal">match</code> rules like these can have a positive effect on your configuration in other ways, too: You can use <code class="literal">match</code> rules like the ones in the examples here to separate filtering <a class="indexterm" id="iddle1011"/><a class="indexterm" id="iddle1012"/><a class="indexterm" id="iddle1240"/><a class="indexterm" id="iddle1249"/><a class="indexterm" id="iddle1684"/><a class="indexterm" id="iddle1952"/><a class="indexterm" id="iddle2002"/>decisions—such as passing, blocking, or redirecting—from traffic-shaping decisions, and with that separation in place, you’re likely to end up with a more readable and maintainable configuration.</p><p>It’s also worth noting that parts of the OpenBSD network stack set default priorities for certain types of traffic that the developers decided was essential to a functional network. If you don’t set any priorities, anything with <code class="literal">proto carp</code> and a few other management protocols and packet types will go by priority 6, and all types of traffic that don’t receive a specific classification with a <code class="literal">set prio</code> rule will have a default priority of 3.</p></div><div class="sect3" title="The Two-Priority Speedup Trick"><div class="titlepage"><div><div><h4 class="title" id="two-priority_speedup_trick">The Two-Priority Speedup Trick</h4></div></div></div><p>In the examples just shown, we set different priorities for different types of traffic and managed to get specific types of traffic, such as VoIP and SSH, to move faster than others. But thanks to the design of TCP, which carries the bulk of your traffic, even a simple priority-shaping scheme has more to offer with only minor tweaks to the rule set.</p><p>As readers of RFCs and a few practitioners have discovered, the connection-oriented design of TCP means that for each packet sent, the sender will expect to receive an acknowledgment (ACK) packet back within a preset time or matching a defined “window” of sequence numbers. If the sender doesn’t receive the acknowledgment within the expected limit, she assumes the packet was lost in transit and arranges to resend the data.</p><p>One other important factor to consider is that by default, packets are handled in the order they arrive. This is known as <span class="emphasis"><em>first in, first out (FIFO)</em></span>, and it means that the essentially dataless ACK packets will be waiting their turn in between the larger data packets. On a busy or congested link, which is exactly where traffic shaping becomes interesting, waiting for ACKs and performing retransmissions can eat measurably into effective bandwidth and slow down all transfers. In fact, concurrent transfers in both directions can slow each other significantly more than the value of their expected data sizes.<sup>[<a class="footnote" href="#ftn.ch07fn01" id="ch07fn01">39</a>]</sup></p><p>Fortunately, a simple and quite popular solution to this problem is at hand: You can use priorities to make sure those smaller packets skip ahead. If you assign two priorities in a <code class="literal">match</code> or <code class="literal">pass</code> rule, like this:</p><a id="pro_id00254"/><pre class="programlisting">match out on egress set prio (5, 6)</pre><p>The first priority will be assigned to the regular traffic, while ACK packets and other packets with a low delay type of service (ToS) will be assigned the second priority and will be served faster than the regular packets.</p><p>When a packet arrives, PF detects the ACK packets and puts them on the higher-priority queue. PF also inspects the ToS field on arriving packets. Packets that have the ToS set to low delay to indicate that the sender wants <a class="indexterm" id="iddle1070"/><a class="indexterm" id="iddle1492"/><a class="indexterm" id="iddle1678"/><a class="indexterm" id="iddle1710"/><a class="indexterm" id="iddle1992"/>speedier delivery also get the high-priority treatment. When more than one priority is indicated, as in the preceding rule, PF assigns priority accordingly. Packets with other ToS values are processed in the order they arrive, but with ACK packets arriving faster, the sender spends less time waiting for ACKs and resending presumably lost data. The net result is that the available bandwidth is used more efficiently. (The <code class="literal">match</code> rule quoted here is the first one I wrote in order to get a feel for the new <code class="literal">prio</code> feature—on a test system, of course—soon after it was committed during the OpenBSD 5.0 development cycle. If you put that single <code class="literal">match</code> rule on top of an existing rule set, you’ll probably see that the link can take more traffic and more simultaneous connections before noticeable symptoms of congestion turn up.)</p><p>See whether you can come up with a way to measure throughout before and after you introduce the two-priorities trick to your traffic shaping, and note the difference before you proceed to the more complex traffic-shaping options.</p></div></div><div class="sect2" title="Introducing Queues for Bandwidth Allocation"><div class="titlepage"><div><div><h3 class="title" id="introducing_queues_for_bandwidth_allocat">Introducing Queues for Bandwidth Allocation</h3></div></div></div><p>We’ve seen that traffic shaping using only priorities can be quite effective, but there will be times when a priorities-only scheme will fall short of your goals. One such scenario occurs when you’re faced with requirements that would be most usefully solved by assigning a higher priority, and perhaps a larger bandwidth share, to some kinds of traffic, such as email and other high-value services, and correspondingly less bandwidth to others. Another such scenario would be when you simply want to apportion your available bandwidth in different-sized chunks to specific services and perhaps set hard upper limits for some types of traffic, while at the same time wanting to ensure that all traffic that you care about gets at least its fair share of available bandwidth. In cases like these, you leave the pure-priority scheme behind, at least as the primary tool, and start doing actual traffic shaping using <span class="emphasis"><em>queues</em></span>.</p><p>Unlike with the priority levels, which are always available and can be used without further preparations, in any rule, queues represent specific parts of your available bandwidth and can be used only after you’ve defined them in terms of available capacity. Queues are a kind of buffer for network packets. Queues are defined with a specific amount of bandwidth, or as a specific portion of available bandwidth, and you can allocate portions of each queue’s bandwidth share to subqueues, or queues within queues, which share the parent queue’s resources. The packets are held in a queue until they’re either dropped or sent according to the queue’s criteria and subject to the queue’s available bandwidth. Queues are attached to specific interfaces, and bandwidth is managed on a per-interface basis, with available bandwidth on a given interface subdivided into the queues you define.</p><p>The basic syntax for defining a queue follows this pattern:</p><a id="pro_id00255"/><pre class="programlisting">queue <span class="emphasis"><em>name</em></span> on <span class="emphasis"><em>interface</em></span> bandwidth <span class="emphasis"><em>number [ ,K,M,G]</em></span>&#13;
     queue <span class="emphasis"><em>name1</em></span> parent <span class="emphasis"><em>name</em></span> bandwidth <span class="emphasis"><em>number[ ,K,M,G]</em></span> default&#13;
     queue <span class="emphasis"><em>name2</em></span> parent <span class="emphasis"><em>name</em></span> bandwidth <span class="emphasis"><em>number[ ,K,M,G]</em></span>&#13;
     queue <span class="emphasis"><em>name3</em></span> parent <span class="emphasis"><em>name</em></span> bandwidth <span class="emphasis"><em>number[ ,K,M,G]</em></span></pre><p><a class="indexterm" id="iddle1075"/><a class="indexterm" id="iddle1966"/>The letters following the bandwidth number denote the unit of measurement: <span class="emphasis"><em><code class="literal">K</code></em></span> denotes kilobits; <span class="emphasis"><em><code class="literal">M</code></em></span> megabits; and <span class="emphasis"><em><code class="literal">G</code></em></span> gigabits. When you write only the bandwidth number, it’s interpreted as the number of bits per second. It’s possible to tack on other options to this basic syntax, as we’ll see in later examples.</p><div class="note" title="Note"><h3 class="title"><a id="ch07note02"/>Note</h3><p><span class="emphasis"><em>Subqueue definitions name their parent queue, and one queue needs to be the default queue that receives any traffic not specifically assigned to other queues.</em></span></p></div><p>Once queue definitions are in place, you integrate traffic shaping into your rule set by rewriting your <code class="literal">pass</code> or <code class="literal">match</code> rules to assign traffic to a specific queue.</p><div class="sidebar"><a id="whatapostrophes_your_total_usable_bandwi"/><div class="sidebar-title">What’s your Total Usable Bandwidth?</div><p>Once we start working with defined parts of total bandwidth rather than priorities that somehow share the whole, determining the exact value of your total usable bandwidth becomes interesting. It can be difficult to determine actual usable bandwidth on a specific interface for queuing. If you don’t specify a total bandwidth, the total bandwidth available will be used to calculate the allocations, but some types of interfaces cannot reliably report the actual bandwidth value. One common example of this discrepancy is where your gateway’s external interface is a 100 megabit (Mb) Ethernet interface, attached to a DSL line that offers only 8Mb download and 1Mb upload.<sup>[<a class="footnote" epub:type="noteref" href="#ftn.ch07fn01a" id="ch07fn01a">40</a>]</sup> The Ethernet interface will then confidently report 100Mb bandwidth, not the actual value of the Internet-facing connection.</p><p>For this reason, it usually makes sense to set the total bandwidth to a fixed value. Unfortunately, the value to use may not be exactly what your bandwidth supplier tells you is available because there will always be some overhead due to various technologies and implementations. For example, in typical TCP/IP over wired Ethernet, overhead can be as low as single-digit percentages, but TCP/IP over ATM has been known to have overhead of almost 20 percent. If your bandwidth supplier doesn’t provide the overhead information, you’ll need to make an educated guess at the starting value. In any case, remember that the total bandwidth available is never greater than the bandwidth of the weakest link in your network path.</p><p>Queues are supported only for outbound connections relative to the system doing the queuing. When planning your bandwidth management, consider the actual usable bandwidth to be equal to the weakest (lowest bandwidth) link in the connection’s path, even if your queues are set up on a different interface.</p></div><div class="sect3" title="The HFSC Algorithm"><div class="titlepage"><div><div><h4 class="title" id="hfsc_algorithm">The HFSC Algorithm</h4></div></div></div><p><a class="indexterm" id="iddle1072"/><a class="indexterm" id="iddle1074"/><a class="indexterm" id="iddle1321"/><a class="indexterm" id="iddle1680"/><a class="indexterm" id="iddle1682"/><a class="indexterm" id="iddle1712"/><a class="indexterm" id="iddle1714"/><a class="indexterm" id="iddle1722"/>Underlying any queue system you define using the queue system in OpenBSD 5.5 and later is the <span class="emphasis"><em>Hierarchical Fair Service Curve (HFSC)</em></span> algorithm. HFSC was designed to allocate resources fairly among queues in a hierarchy. One of its interesting features is that it imposes no limits until some part of the traffic reaches a volume that’s close to its preset limits. The algorithm starts shaping just before the traffic reaches a point where it deprives some other queue of its guaranteed minimum share.</p><div class="note" title="Note"><h3 class="title"><a id="ch07note03"/>Note</h3><p><span class="emphasis"><em>All sample configurations we present in this book assign traffic to queues in the outgoing direction because you can realistically control only traffic generated locally and, once limits are reached, any traffic-shaping system will eventually resort to dropping packets in order to make the endpoint back off. As we saw in the earlier examples, all well-behaved TCP stacks will respond to lost ACKs with slower packet rates.</em></span></p></div><p>Now that you know at least the basics of the theory behind the OpenBSD queue system, let’s see how queues work.</p></div><div class="sect3" title="Splitting Your Bandwidth into Fixed-Size Chunks"><div class="titlepage"><div><div><h4 class="title" id="splitting_your_bandwidth_into_fixed-size">Splitting Your Bandwidth into Fixed-Size Chunks</h4></div></div></div><p>You’ll often find that certain traffic should receive a higher priority than other traffic. For example, you’ll often want important traffic, such as mail and other vital services, to have a baseline amount of bandwidth available at all times, while other services, such as peer-to-peer file sharing, shouldn’t be allowed to consume more than a certain amount. To address these kinds of issues, queues offer a wider range of options than the pure-priority scheme.</p><p>The first queue example builds on the rule sets from earlier chapters. The scenario is that we have a small local network, and we want to let the users on the local network connect to a predefined set of services outside their own network while also letting users from outside the local network access a Web server and an FTP server somewhere on the local network.</p></div><div class="sect3" title="Queue Definition"><div class="titlepage"><div><div><h4 class="title" id="queue_definition">Queue Definition</h4></div></div></div><p>In the following example, all queues are set up with the root queue, called <code class="literal">main</code>, on the external, Internet-facing interface. This approach makes sense mainly because bandwidth is more likely to be limited on the external link than on the local network. In principle, however, allocating queues and running traffic shaping can be done on any network interface.</p><p>This setup includes a queue for a total bandwidth of 20Mb with six subqueues.</p><a id="pro_id00256"/><pre class="programlisting">queue main on $ext_if bandwidth 20M&#13;
      queue defq parent main bandwidth 3600K default&#13;
      queue ftp parent main bandwidth 2000K&#13;
      queue udp parent main bandwidth 6000K&#13;
      queue web parent main bandwidth 4000K&#13;
      queue ssh parent main bandwidth 4000K&#13;
        queue ssh_interactive parent ssh bandwidth 800K&#13;
        queue ssh_bulk parent ssh bandwidth 3200K&#13;
queue icmp parent main bandwidth 400K</pre><p><a class="indexterm" id="iddle1006"/><a class="indexterm" id="iddle1345"/><a class="indexterm" id="iddle1347"/><a class="indexterm" id="iddle1493"/><a class="indexterm" id="iddle1785"/><a class="indexterm" id="iddle1802"/><a class="indexterm" id="iddle1883"/>The subqueue <code class="literal">defq</code>, shown in the preceding example, has a bandwidth allocation of 3600K, or 18 percent of the bandwidth, and is designated as the default queue. This means any traffic that matches a <code class="literal">pass</code> rule but that isn’t explicitly assigned to some other queue ends up here.</p><p>The other queues follow more or less the same pattern, up to subqueue <code class="literal">ssh</code>, which itself has two subqueues (the two indented lines below it). Here, we see a variation on the trick of using two separate priorities to speed up ACK packets, and as we’ll see shortly, the rule that assigns traffic to the two SSH subqueues assigns different priorities. Bulk SSH transfers, typically SCP file transfers, are transmitted with a ToS indicating throughput, while interactive SSH traffic has the ToS flag set to low delay and skips ahead of the bulk transfers. The interactive traffic is likely to be less bandwidth consuming and gets a smaller share of the bandwidth, but it receives preferential treatment because of the higher-priority value assigned to it. This scheme also helps the speed of SCP file transfers because the ACK packets for the SCP transfers will be assigned a higher priority.</p><p>Finally, we have the <code class="literal">icmp</code> queue, which is reserved for the remaining 400K, or 2 percent, of the bandwidth from the top level. This guarantees a minimum amount of bandwidth for ICMP traffic that we want to pass but that doesn’t match the criteria that would have it assigned to the other queues.</p></div><div class="sect3" title="Rule Set"><div class="titlepage"><div><div><h4 class="title" id="rule_set">Rule Set</h4></div></div></div><p>To tie the queues into the rule set, we use the <code class="literal">pass</code> rules to indicate which traffic is assigned to the queues and their criteria.</p><a id="pro_id00257"/><pre class="programlisting">set skip on { lo, $int_if }&#13;
pass log quick on $ext_if proto tcp to port ssh \&#13;
    queue (ssh_bulk, ssh_interactive) set prio (5,7)&#13;
pass in quick on $ext_if proto tcp to port ftp queue ftp&#13;
pass in quick on $ext_if proto tcp to port www queue http&#13;
pass out on $ext_if proto udp queue udp&#13;
pass out on $ext_if proto icmp queue icmp&#13;
pass out on $ext_if proto tcp from $localnet to port $client_out ➊</pre><p>The rules for <code class="literal">ssh</code>, <code class="literal">ftp</code>, <code class="literal">www</code>, <code class="literal">udp</code>, and <code class="literal">icmp</code> assign traffic to their respective queues, and we note again that the <code class="literal">ssh</code> queue’s subqueues are assigned traffic with two different priorities. The last catchall rule ➊ passes all other outgoing traffic from the local network, lumping it into the default <code class="literal">defq</code> queue.</p><p>You can always let a block of <code class="literal">match</code> rules do the queue assignment instead in order to make the configuration even more flexible. With match rules in place, you move the filtering decisions to block, pass, or even redirect to a set of rules elsewhere.</p><a id="pro_id00258"/><pre class="programlisting">match log quick on $ext_if proto tcp to port ssh \&#13;
    queue (ssh_bulk, ssh_interactive) set prio (5,7)&#13;
match in quick on $ext_if proto tcp to port ftp queue ftp&#13;
match in quick on $ext_if proto tcp to port www queue http&#13;
match out on $ext_if proto udp queue udp&#13;
match out on $ext_if proto icmp queue icmp</pre><p><a class="indexterm" id="iddle1073"/><a class="indexterm" id="iddle1322"/><a class="indexterm" id="iddle1681"/><a class="indexterm" id="iddle1707"/><a class="indexterm" id="iddle1713"/><a class="indexterm" id="iddle1723"/>Note that with <code class="literal">match</code> rules performing the queue assignment, there’s no need for a final catchall to put the traffic that doesn’t match the other rules into the default queue. Any traffic that doesn’t match these rules and that’s allowed to pass will end up in the default queue.</p></div><div class="sect3" title="Upper and Lower Bounds with Bursts"><div class="titlepage"><div><div><h4 class="title" id="upper_and_lower_bounds_with_bursts">Upper and Lower Bounds with Bursts</h4></div></div></div><p>Fixed bandwidth allocations are nice, but network admins with traffic-shaping ambitions tend to look for a little more flexibility once they’ve gotten their feet wet. Wouldn’t it be nice if there were a regime with flexible bandwidth allocation, offering guaranteed lower and upper bounds for bandwidth available to each queue and variable allocations over time—and one that starts shaping only when there’s an actual need to do so?</p><p>The good news is that the OpenBSD queues can do just that, courtesy of the underlying HFSC algorithm discussed earlier. HFSC makes it possible to set up queuing regimes with guaranteed minimum allocations and hard upper limits, and you can even have allocations that include <code class="literal">burst</code> values to let available capacity vary over time.</p></div><div class="sect3" title="Queue Definition"><div class="titlepage"><div><div><h4 class="title" id="queue_definition-id00002">Queue Definition</h4></div></div></div><p>Working from a typical gateway configuration like the ones we’ve altered incrementally over the earlier chapters, we insert this queue definition early in the <span class="emphasis"><em>pf.conf</em></span> file:</p><a id="pro_id00259"/><pre class="programlisting">queue rootq on $ext_if bandwidth 20M&#13;
        queue main parent rootq bandwidth 20479K min 1M max 20479K qlimit 100&#13;
             queue qdef parent main bandwidth 9600K min 6000K max 18M default&#13;
             queue qweb parent main bandwidth 9600K min 6000K max 18M&#13;
             queue qpri parent main bandwidth 700K min 100K max 1200K&#13;
             queue qdns parent main bandwidth 200K min 12K burst 600K for 3000ms&#13;
        queue spamd parent rootq bandwidth 1K min 0K max 1K qlimit 300</pre><p>This definition has some characteristics that are markedly different from the previous one in <a class="xref" href="ch07.html#introducing_queues_for_bandwidth_allocat" title="Introducing Queues for Bandwidth Allocation">Introducing Queues for Bandwidth Allocation</a>. We start with this rather small hierarchy by splitting the top-level queue, <code class="literal">rootq</code>, into two. Next, we subdivide the <code class="literal">main</code> queue into several subqueues, all of which have a <code class="literal">min</code> value set—the guaranteed minimum bandwidth allocated to the queue. (The <code class="literal">max</code> value would set a hard upper limit on the queue’s allocation.) The <code class="literal">bandwidth</code> parameter also sets the allocation the queue will have available when it’s backlogged—that is, when it’s started to eat into its <code class="literal">qlimit</code>, or <span class="emphasis"><em>queue limit</em></span>, allocation.</p><p><a class="indexterm" id="iddle1007"/><a class="indexterm" id="iddle1429"/><a class="indexterm" id="iddle1494"/><a class="indexterm" id="iddle1786"/>The queue limit parameter works like this: In case of congestion, each queue by default has a pool of 50 slots, the queue limit, to keep packets around when they can’t be transmitted immediately. Here, the top-level queues, <code class="literal">main</code> and <code class="literal">spamd</code>, both have larger-than-default pools set by their <code class="literal">qlimit</code> setting: <code class="literal">100</code> for <code class="literal">main</code> and <code class="literal">300</code> for <code class="literal">spamd</code>. Cranking up these <code class="literal">qlimit</code> sizes means we’re a little less likely to drop packets when the traffic approaches the set limits, but it also means that when the traffic shaping kicks in, we’ll see increased latency for connections that end up in these larger pools.</p></div><div class="sect3" title="Rule Set"><div class="titlepage"><div><div><h4 class="title" id="rule_set-id00003">Rule Set</h4></div></div></div><p>The next step is to tie the newly created queues into the rule set. If you have a filtering regime in place already, the tie-in is simple—just add a few <code class="literal">match</code> rules:</p><a id="pro_id00260"/><pre class="programlisting">match out on $ext_if proto tcp to port { www https } \&#13;
    set queue (qweb, qpri) set prio (5,6)&#13;
match out on $ext_if proto { tcp udp } to port domain \&#13;
    set queue (qdns, qpri) set prio (6,7)&#13;
match out on $ext_if proto icmp \&#13;
    set queue (qdns, qpri) set prio (6,7)</pre><p>Here, the <code class="literal">match</code> rules once again do the ACK packet speedup trick with the high- and low-priority queue assignment, just as we saw earlier in the pure-priority-based system. The only exception is when we assign traffic to our lowest-priority queue (with a slight modification to an existing <code class="literal">pass</code> rule), where we really don’t want any speedup.</p><a id="pro_id00261"/><pre class="programlisting">pass in log on egress proto tcp to port smtp \&#13;
    rdr-to 127.0.0.1 port spamd set queue spamd set prio 0</pre><p>Assigning the <code class="literal">spamd</code> traffic to a minimal-sized queue with 0 priority here is intended to slow down the spammers on their way to our <code class="literal">spamd</code>. (See <a class="xref" href="ch06.html" title="Chapter 6. Turning the Tables for Proactive Defense">Chapter 6</a> for more on <code class="literal">spamd</code> and related matters.)</p><p>With the queue assignment and priority setting in place, it should be clear that the queue hierarchy here uses two familiar tricks to make efficient use of available bandwidth. First, it uses a variation of the high- and low-priority mix demonstrated in the earlier pure-priority example. Second, we speed up almost all other traffic, especially the Web traffic, by allocating a small but guaranteed portion of bandwidth for name service lookups. For the <code class="literal">qdns</code> queue, we set the <code class="literal">burst</code> value with a time limit: After <code class="literal">3000</code> milliseconds, the allocation goes down to a minimum of <code class="literal">12K</code> to fit within the total <code class="literal">200K</code> quota. Short-lived <code class="literal">burst</code> values like this can be useful to speed connections that transfer most of their payload during the early phases.</p><p>It may not be immediately obvious from this example, but HFSC requires that traffic be assigned only to <span class="emphasis"><em>leaf queues</em></span>, or queues without subqueues. That means it’s possible to assign traffic to <code class="literal">main</code>’s subqueues—<code class="literal">qpri</code>, <code class="literal">qdef</code>, <code class="literal">qweb</code>, and <code class="literal">qdns</code>—as well as <code class="literal">rootq</code>’s subqueue—<code class="literal">spamd</code>—as we just did with the <code class="literal">match</code> and <a class="indexterm" id="iddle1928"/><code class="literal">pass</code> rules, but not to <code class="literal">rootq</code> or <code class="literal">main</code> themselves. With all queue assignments in place, we can use <code class="literal">systat</code> queues to show the queues and their traffic:</p><a id="pro_id00262"/><pre class="programlisting">    6 users Load 0.31 0.28 0.34                   Tue May 19 21:31:54 2015&#13;
&#13;
QUEUE            BW SCH  PR PKTS BYTES DROP_P DROP_B QLEN BORR SUSP P/S B/S&#13;
rootq           20M            0     0      0      0    0&#13;
main            20M            0     0      0      0    0&#13;
 qdef            9M        48887   15M      0      0    0&#13;
 qweb            9M        18553 8135K      0      0    0&#13;
 qpri          600K        37549 2407K      0      0    0&#13;
 qdns          200K        15716 1568K      0      0    0&#13;
spamd            1K        10590 661K     126   8772   47</pre><p>The queues are shown indented to indicate their hierarchy, from root to leaf queues. The <code class="literal">main</code> queue and its subqueues—<code class="literal">qpri</code>, <code class="literal">qdef</code>, <code class="literal">qweb</code>, and <code class="literal">qdns</code>—are shown with their bandwidth allocations and number of bytes and packets passed. The <code class="literal">DROP_P</code> and <code class="literal">DROP_B</code> columns, which show the number of packets and bytes dropped, would appear if we had been forced to drop packets at this stage. <code class="literal">QLEN</code> is the number of packets waiting for processing, while the final two columns show live updates of packets and bytes per second.</p><p>For a more detailed view, use <code class="literal">pfctl -vvsq</code> to show the queues and their traffic:</p><a id="pro_id00263"/><pre class="programlisting">queue rootq on xl0 bandwidth 20M qlimit 50&#13;
  [ pkts:          0  bytes:          0  dropped pkts:      0 bytes:      0 ]&#13;
  [ qlength:   0/ 50 ]&#13;
  [ measured:     0.0 packets/s, 0 b/s ]&#13;
queue main parent rootq on xl0 bandwidth 20M, min 1M, max 20M qlimit 100&#13;
  [ pkts:          0  bytes:          0  dropped pkts:      0 bytes:      0 ]&#13;
  [ qlength:   0/100 ]&#13;
  [ measured:     0.0 packets/s, 0 b/s ]&#13;
queue qdef parent main on xl0 bandwidth 9M, min 6M, max 18M default qlimit 50&#13;
  [ pkts:       1051  bytes:     302813  dropped pkts:      0 bytes:      0 ]&#13;
  [ qlength:   0/ 50 ]&#13;
  [ measured:   2.6 packets/s, 5.64Kb/s ]&#13;
queue qweb parent main on xl0 bandwidth 9M, min 6M, max 18M qlimit 50&#13;
  [ pkts:       1937  bytes:    1214950  dropped pkts:      0 bytes:      0 ]&#13;
  [ qlength:   0/ 50 ]&#13;
  [ measured:     3.6 packets/s, 13.65Kb/s ]&#13;
queue qpri parent main on xl0 bandwidth 600K, max 1M qlimit 50&#13;
  [ pkts:       2169  bytes:     143302  dropped pkts:      0 bytes:      0 ]&#13;
  [ qlength:   0/ 50 ]&#13;
  [ measured:     6.6 packets/s, 3.55Kb/s ]&#13;
queue qdns parent main on xl0 bandwidth 200K, min 12K burst 600K for 3000ms qlimit 50&#13;
  [ pkts:        604 bytes:       65091  dropped pkts:      0 bytes:      0 ]&#13;
  [ qlength:    0/ 50 ]&#13;
  [ measured:      1.6 packets/s, 1.31Kb/s ]&#13;
queue spamd parent rootq on xl0 bandwidth 1K, max 1K qlimit 300&#13;
  [ pkts:        884 bytes:       57388  dropped pkts:      0 bytes: 0 ]&#13;
  [ qlength: 176/300 ]&#13;
  [ measured:     1.9 packets/s, 1Kb/s ]</pre><p><a class="indexterm" id="iddle1071"/><a class="indexterm" id="iddle1198"/><a class="indexterm" id="iddle1199"/><a class="indexterm" id="iddle1217"/><a class="indexterm" id="iddle1679"/><a class="indexterm" id="iddle1711"/>This view shows that the queues receive traffic roughly as expected with the site’s typical workload. Notice that only a few moments after the rule set has been reloaded, the <code class="literal">spamd</code> queue is already backed up more than halfway to its <code class="literal">qlimit</code> setting, which seems to indicate that the queues are reasonably dimensioned to actual traffic.</p><div class="note" title="Note"><h3 class="title"><a id="ch07note04"/>Note</h3><p><span class="emphasis"><em>Pay attention to each queue’s dropped packets (<code class="literal">dropped pkts:</code>) counter. If the number of packets dropped is high or increasing, then that could mean that one of the bandwidth allocation parameters needs adjusting or that some other network problem needs to be investigated.</em></span></p></div></div><div class="sect3" title="The DMZ Network, Now with Traffic Shaping"><div class="titlepage"><div><div><h4 class="title" id="dmz_networkcomma_now_with_traffic_shapin">The DMZ Network, Now with Traffic Shaping</h4></div></div></div><p>In <a class="xref" href="ch05.html" title="Chapter 5. Bigger or Trickier Networks">Chapter 5</a>, we set up a network with a single gateway and all externally visible services configured on a separate DMZ (demilitarized zone) network so that all traffic to the servers from both the Internet and the internal network had to pass through the gateway. That network schematic, illustrated in <a class="xref" href="ch05.html" title="Chapter 5. Bigger or Trickier Networks">Chapter 5</a>, is shown again in <a class="xref" href="ch07.html#network_with_dmz" title="Figure 7-1. Network with DMZ">Figure 7-1</a>. Using the rule set from <a class="xref" href="ch05.html" title="Chapter 5. Bigger or Trickier Networks">Chapter 5</a> as the starting point, we’ll add some queuing in order to optimize our network resources. The physical and logical layout of the network will not change.</p><div class="figure"><a id="network_with_dmz"/><div class="figure-contents"><div class="mediaobject"><a id="med_id00013"/><img alt="Network with DMZ" src="httpatomoreillycomsourcenostarchimages2127159.png.jpg"/></div></div><div class="figure-title">Figure 7-1. Network with DMZ</div></div><p>The most likely bottleneck for this network is the bandwidth for the connection between the gateway’s external interface and the Internet. Although the bandwidth elsewhere in our setup isn’t infinite, of course, the available bandwidth on any interface in the local network is likely to be less limiting than the bandwidth actually available for communication with the outside world. In order to make services available with the best possible performance, we need to set up the queues so that the bandwidth available at the site is made available to the traffic we want to allow. The interface bandwidth on the DMZ interface is likely either 100Mb or 1Gb, while the <span class="emphasis"><em>actual available bandwidth</em></span> for connections from outside the local network is considerably smaller. This consideration shows up in our queue definitions, where the actual bandwidth available for external traffic is the main limitation in the queue setup.</p><a id="pro_id00264"/><pre class="programlisting">queue ext on $ext_if bandwidth 2M&#13;
        queue ext_main parent ext bandwidth 500K default&#13;
        queue ext_web parent ext bandwidth 500K&#13;
        queue ext_udp parent ext bandwidth 400K&#13;
        queue ext_mail parent ext bandwidth 600K&#13;
&#13;
queue dmz on $dmz_if bandwidth 100M&#13;
        queue ext_dmz parent dmz bandwidth 2M&#13;
                queue ext_dmz_web parent ext_dmz bandwidth 800K default&#13;
                queue ext_dmz_udp parent ext_dmz bandwidth 200K&#13;
                queue ext_dmz_mail parent ext_dmz bandwidth 1M&#13;
        queue dmz_main parent dmz bandwidth 25M&#13;
        queue dmz_web parent dmz bandwidth 25M&#13;
        queue dmz_udp parent dmz bandwidth 20M&#13;
        queue dmz_mail parent dmz bandwidth 20M</pre><p>Notice that for each interface, there’s a root queue with a bandwidth limitation that determines the allocation for all queues attached to that interface. In order to use the new queuing infrastructure, we need to make some changes to the filtering rules, too.</p><div class="note" title="Note"><h3 class="title"><a id="ch07note05"/>Note</h3><p><span class="emphasis"><em>Because any traffic not explicitly assigned to a specific queue is assigned to the default queue for the interface, be sure to tune your filtering rules as well as your queue definitions to the actual traffic in your network.</em></span></p></div><p>The main part of the filtering rules could end up looking like this after adding the queues:</p><a id="pro_id00265"/><pre class="programlisting">pass in on $ext_if proto { tcp, udp } to $nameservers port domain \&#13;
    set queue ext_udp set prio (6,5)&#13;
pass in on $int_if proto { tcp, udp } from $localnet to $nameservers \&#13;
    port domain&#13;
pass out on $dmz_if proto { tcp, udp } to $nameservers port domain \&#13;
    set queue ext_dmz_udp set prio (6,5)&#13;
pass out on $dmz_if proto { tcp, udp } from $localnet to $nameservers \&#13;
    port domain set queue dmz_udp&#13;
pass in on $ext_if proto tcp to $webserver port $webports set queue ext_web&#13;
pass in on $int_if proto tcp from $localnet to $webserver port $webports&#13;
pass out on $dmz_if proto tcp to $webserver port $webports \&#13;
    set queue ext_dmz_web&#13;
pass out on $dmz_if proto tcp from $localnet to $webserver port $webports \&#13;
    set queue dmz_web&#13;
pass in log on $ext_if proto tcp to $mailserver port smtp&#13;
pass in log on $ext_if proto tcp from $localnet to $mailserver port smtp&#13;
pass in log on $int_if proto tcp from $localnet to $mailserver port $email&#13;
pass out log on $dmz_if proto tcp to $mailserver port smtp set queue ext_mail&#13;
pass in on $dmz_if proto tcp from $mailserver to port smtp set queue dmz_mail&#13;
pass out log on $ext_if proto tcp from $mailserver to port smtp \&#13;
    set queue ext_dmz_mail</pre><p><a class="indexterm" id="iddle1495"/><a class="indexterm" id="iddle1583"/><a class="indexterm" id="iddle1675"/><a class="indexterm" id="iddle1677"/><a class="indexterm" id="iddle1715"/><a class="indexterm" id="iddle1991"/>Notice that only traffic that will pass either the DMZ or the external interface is assigned to queues. In this configuration, with no externally accessible services on the internal network, queuing on the internal interface wouldn’t make much sense because that’s likely the part of the network with the least restricted available bandwidth. Also, as in earlier examples, there’s a case to be made for separating the queue assignments from the filtering part of the rule set by making a block of <code class="literal">match</code> rules responsible for queue assignment.</p></div></div><div class="sect2" title="Using Queues to Handle Unwanted Traffic"><div class="titlepage"><div><div><h3 class="title" id="using_queues_to_handle_unwanted_traffic">Using Queues to Handle Unwanted Traffic</h3></div></div></div><p>So far, we’ve focused on queuing as a way to make sure specific kinds of traffic are let through as efficiently as possible. Now, we’ll look at two examples that present a slightly different way to identify and handle unwanted traffic using various queuing-related tricks to keep miscreants in line.</p><div class="sect3" title="Overloading to a Tiny Queue"><div class="titlepage"><div><div><h4 class="title" id="overloading_to_a_tiny_queue">Overloading to a Tiny Queue</h4></div></div></div><p>In <a class="xref" href="ch06.html#turning_away_the_brutes" title="Turning Away the Brutes">Turning Away the Brutes</a>, we used a combination of state-tracking options and <code class="literal">overload</code> rules to fill a table of addresses for special treatment. The special treatment we demonstrated in <a class="xref" href="ch06.html" title="Chapter 6. Turning the Tables for Proactive Defense">Chapter 6</a> was to cut all connections, but it’s equally possible to assign <code class="literal">overload</code> traffic to a specific queue instead. For example, consider the rule from our first queue example, shown here.</p><a id="pro_id00266"/><pre class="programlisting">pass log quick on $ext_if proto tcp to port ssh flags S/SA \&#13;
    keep state queue (ssh_bulk, ssh_interactive) set prio (5,7)</pre><p>To create a variation of the overload table trick from <a class="xref" href="ch06.html" title="Chapter 6. Turning the Tables for Proactive Defense">Chapter 6</a>, add state-tracking options, like this:</p><a id="pro_id00267"/><pre class="programlisting">pass log quick on $ext_if proto tcp to port ssh flags S/SA \&#13;
    keep state (max-src-conn 15, max-src-conn-rate 5/3, \&#13;
    overload &lt;bruteforce&gt; flush global) queue (ssh_bulk, ssh_interactive) \&#13;
    set prio (5,7)</pre><p>Then, make one of the queues slightly smaller:</p><a id="pro_id00268"/><pre class="programlisting">queue smallpipe parent main bandwidth 512</pre><p><a class="indexterm" id="iddle1044"/><a class="indexterm" id="iddle1579"/><a class="indexterm" id="iddle1676"/><a class="indexterm" id="iddle1686"/><a class="indexterm" id="iddle1687"/><a class="indexterm" id="iddle1716"/><a class="indexterm" id="iddle1727"/><a class="indexterm" id="iddle1994"/>And assign traffic from miscreants to the small-bandwidth queue with this rule:</p><a id="pro_id00269"/><pre class="programlisting">pass inet proto tcp from &lt;bruteforce&gt; to port $tcp_services queue smallpipe</pre><p>As a result, the traffic from the bruteforcers would pass, but with a hard upper limit of 512 bits per second. (It’s worth noting that tiny bandwidth allocations may be hard to enforce on high-speed links due to the network stack’s timer resolution. If the allocation is small enough relative to the capacity of the link, packets that exceed the stated per-second maximum allocation may be transferred anyway, before the bandwidth limit kicks in.) It might also be useful to supplement rules like these with table-entry expiry, as described in <a class="xref" href="ch06.html#tidying_your_tables_with_pfctl" title="Tidying Your Tables with pfctl">Tidying Your Tables with pfctl</a>.</p></div><div class="sect3" title="Queue Assignments Based on Operating System Fingerprint"><div class="titlepage"><div><div><h4 class="title" id="queue_assignments_based_on_operating_sys">Queue Assignments Based on Operating System Fingerprint</h4></div></div></div><p><a class="xref" href="ch06.html" title="Chapter 6. Turning the Tables for Proactive Defense">Chapter 6</a> covered several ways to use <code class="literal">spamd</code> to cut down on spam. If running <code class="literal">spamd</code> isn’t an option in your environment, you can use a queue and rule set based on the knowledge that machines that send spam are likely to run a particular operating system. (Let’s call that operating system Windows.)</p><p>PF has a fairly reliable operating system fingerprinting mechanism, which detects the operating system at the other end of a network connection based on characteristics of the initial SYN packets at connection setup. The following may be a simple substitute for <code class="literal">spamd</code> if you’ve determined that legitimate mail is highly unlikely to be delivered from systems that run that particular operating system.</p><a id="pro_id00270"/><pre class="programlisting">pass quick proto tcp from any os "Windows" to $ext_if \&#13;
    port smtp set queue smallpipe</pre><p>Here, email traffic originating from hosts that run a particular operating system get no more than 512 bits per second of your bandwidth.</p></div></div></div><div class="sect1" title="Transitioning from ALTQ to Priorities and Queues"><div class="titlepage"><div><div><h2 class="title" id="transitioning_from_altq_to_priorities_an" style="clear: both">Transitioning from ALTQ to Priorities and Queues</h2></div></div></div><p>If you already have configurations that use ALTQ for traffic shaping and you’re planning a switch to OpenBSD 5.5 or newer, this section contains some pointers for how to manage the transition. The main points are these:</p><p><span class="emphasis"><em><span class="strong"><strong>The rules after transition are likely simpler.</strong></span></em></span> The OpenBSD 5.5 and newer traffic-shaping system has done away with the somewhat arcane ALTQ syntax with its selection of queuing algorithms, and it distinguishes clearly between queues and pure-priority shuffling. In most cases, your configuration becomes significantly more readable and maintainable after a conversion to the new traffic-shaping system.</p><p><span class="emphasis"><em><span class="strong"><strong>For simple configurations, set prio is enough.</strong></span></em></span> The simplest queue discipline in ALTQ was <code class="literal">priq</code>, or priority queues. The most common simple use <a class="indexterm" id="iddle1009"/><a class="indexterm" id="iddle1126"/><a class="indexterm" id="iddle1241"/><a class="indexterm" id="iddle1250"/><a class="indexterm" id="iddle1317"/><a class="indexterm" id="iddle1326"/><a class="indexterm" id="iddle1496"/><a class="indexterm" id="iddle1718"/><a class="indexterm" id="iddle1724"/><a class="indexterm" id="iddle2003"/>case was the two-priority speedup trick first illustrated by Daniel Hartmeier in the previously cited article. The basic two-priority configuration looks like this:</p><a id="pro_id00271"/><pre class="programlisting">ext_if="kue0"&#13;
&#13;
altq on $ext_if priq bandwidth 100Kb queue { q_pri, q_def }&#13;
      queue q_pri priority 7&#13;
      queue q_def priority 1 priq(default)&#13;
&#13;
pass out on $ext_if proto tcp from $ext_if queue (q_def, q_pri)&#13;
pass in on $ext_if proto tcp to $ext_if queue (q_def, q_pri)</pre><p>In OpenBSD 5.5 and newer, the equivalent effect can be achieved with no queue definitions. Instead, you assign two priorities in a <code class="literal">match</code> or <code class="literal">pass</code> rule, like this:</p><a id="pro_id00272"/><pre class="programlisting">match out on egress set prio (5, 6)</pre><p>Here, the first priority will be assigned to regular traffic, while ACK and other packets with a low-delay ToS will be assigned the second priority and will be served faster than the regular packets. The effect is the same as in the ALTQ example we just quoted, with the exception of defined bandwidth limits and the somewhat dubious effect of traffic shaping on incoming traffic.</p><p><span class="emphasis"><em><span class="strong"><strong>Priority queues can for the most part be replaced by set prio constructs.</strong></span></em></span> For pure-priority differentiation, applying <code class="literal">set prio</code> on a per <code class="literal">pass</code> or <code class="literal">match</code> rule basis is simpler than defining queues and assigning traffic and affects only the packet priority. ALTQ allowed you to define CBQ or HFSC queues that also had a priority value as part of their definition. Under the new queuing system, assigning priority happens only in <code class="literal">match</code> or <code class="literal">pass</code> rules, but if your application calls for setting both priority and queue assignment in the same rule, the new syntax allows for that, too:</p><a id="pro_id00273"/><pre class="programlisting">pass log quick on $ext_if proto tcp to port ssh \&#13;
      queue (ssh_bulk, ssh_interactive) set prio (5,7)</pre><p>The effect is similar to the previous behavior shown in <a class="xref" href="ch07.html#splitting_your_bandwidth_into_fixed-size" title="Splitting Your Bandwidth into Fixed-Size Chunks">Splitting Your Bandwidth into Fixed-Size Chunks</a>, and this variant may be particularly helpful during transition.</p><p><span class="emphasis"><em><span class="strong"><strong>Priorities are now always important. Keep in mind that the default is 3.</strong></span></em></span> It’s important to be aware that traffic priorities are always enabled since OpenBSD 5.0, and they need to be taken into consideration even when you’re not actively assigning priorities. In old-style configurations that employed the two-priority trick to speed up ACKs and by extension all traffic, the only thing that was important was that there were two different priorities in play. The low-delay packets would be assigned to the higher-priority queue, and the net effect would be that traffic would likely pass faster, with more efficient bandwidth use than with the default FIFO <a class="indexterm" id="iddle1021"/><a class="indexterm" id="iddle1022"/><a class="indexterm" id="iddle1089"/><a class="indexterm" id="iddle1565"/><a class="indexterm" id="iddle1981"/>queue. Now the default priority is 3, and setting the priority for a queue to 0, as a few older examples do, will mean that the traffic assigned that priority will be considered ready to pass only when there’s no higher-priority traffic left to handle.</p><p><span class="emphasis"><em><span class="strong"><strong>For actual bandwidth shaping, HFSC works behind the scenes.</strong></span></em></span> Once you’ve determined that your specification calls for slicing available bandwidth into chunks, the underlying algorithm is always HFSC. The variety of syntaxes for different types of queues is gone. HFSC was chosen for its flexibility as well as the fact that it starts actively shaping traffic only once the traffic approaches one of the limits set by your queuing configuration. In addition, it’s possible to create CBQ-like configurations by limiting the queue definitions to only bandwidth declarations. <a class="xref" href="ch07.html#splitting_your_bandwidth_into_fixed-size" title="Splitting Your Bandwidth into Fixed-Size Chunks">Splitting Your Bandwidth into Fixed-Size Chunks</a> (mentioned earlier) demonstrates a static configuration that implements CBQ as a subset of HFSC.</p><p><span class="emphasis"><em><span class="strong"><strong>You can transition from ALTQ via the oldqueue mechanism.</strong></span></em></span> OpenBSD 5.5 supports legacy ALTQ configurations with only one minor change to configurations: The <code class="literal">queue</code> keyword was needed as a reserved word for the new queuing system, so ALTQ queues need to be declared as <code class="literal">oldqueue</code> instead. Following that one change (a pure search and replace operation that you can even perform just before starting your operating system upgrade), the configuration will work as expected.</p><p><span class="emphasis"><em><span class="strong"><strong>If your setup is sufficiently complicated, go back to specifications and reimplement.</strong></span></em></span> The examples in this chapter are somewhat stylized and rather simple. If you have running configurations that have been built up incrementally over several years and have reached a complexity level, orders of magnitude larger than those described here, the new syntax may present an opportunity to define what your setup is for and produce a specification that is fit to reimplement in a cleaner and more maintainable configuration.</p><p>Going the <code class="literal">oldqueue</code> route and tweaking from there will work to some degree, but it may be easier to make the transition via a clean reimplementation from revised specification in a test environment where you can test whether your accumulated assumptions hold up in a the context of the new traffic-shaping system. Whatever route you choose for your transition, you’re more or less certain to end up with a more readable and maintainable configuration after your switch to OpenBSD 5.5 or newer.</p></div><div class="sect1" title="Directing Traffic with ALTQ"><div class="titlepage"><div><div><h2 class="title" id="directing_traffic_with_altq" style="clear: both">Directing Traffic with ALTQ</h2></div></div></div><p><span class="emphasis"><em>ALTQ</em></span> is the very flexible legacy mechanism for network traffic shaping, which was integrated into PF on OpenBSD<sup>[<a class="footnote" href="#ftn.ch07fn02" id="ch07fn02">41</a>]</sup> in time for the OpenBSD 3.3 release by Henning Brauer, who’s also the main developer of the priorities and queues system introduced in OpenBSD 5.5 (described in the previous sections of this chapter). OpenBSD 3.3 onward moved all ALTQ <a class="indexterm" id="iddle1023"/><a class="indexterm" id="iddle1037"/><a class="indexterm" id="iddle1323"/><a class="indexterm" id="iddle1497"/><a class="indexterm" id="iddle1688"/><a class="indexterm" id="iddle1717"/><a class="indexterm" id="iddle1728"/><a class="indexterm" id="iddle1959"/><a class="indexterm" id="iddle1982"/><a class="indexterm" id="iddle1987"/>configuration into <span class="emphasis"><em>pf.conf</em></span> to ease the integration of traffic shaping and filtering. PF ports to other BSDs were quick to adopt at least some optional ALTQ integration.</p><div class="note" title="Note"><h3 class="title"><a id="ch07note06"/>Note</h3><p><span class="emphasis"><em>OpenBSD 5.5 introduced a new queue system for traffic shaping with a radically different (and more readable) syntax that complements the always-on priority system introduced in OpenBSD 5.0. The new system is intended to replace ALTQ entirely after one transitional release. The rest of this chapter is useful only if you’re interested in learning about how to set up or maintain an ALTQ-based system.</em></span></p></div><div class="sect2" title="Basic ALTQ Concepts"><div class="titlepage"><div><div><h3 class="title" id="basic_altq_concepts">Basic ALTQ Concepts</h3></div></div></div><p>As the name suggests, ALTQ configurations are totally queue-centric. As in the more recent traffic-shaping system, ALTQ queues are defined in terms of bandwidth and attached to interfaces. Queues can be assigned priority, and in some contexts, they can have subqueues that receive a share of the parent queue’s bandwidth.</p><p>The general syntax for ALTQ queues looks like this:</p><a id="pro_id00274"/><pre class="programlisting">altq on <span class="emphasis"><em>interface type</em></span> [options ... ] <span class="emphasis"><em>main_queue</em></span> { <span class="emphasis"><em>sub_q1</em></span>, <span class="emphasis"><em>sub_q2</em></span> ..}&#13;
  queue <span class="emphasis"><em>sub_q1</em></span> [ options ... ]&#13;
  queue <span class="emphasis"><em>sub_q2</em></span> [ options ... ] { <span class="emphasis"><em>subA</em></span>, <span class="emphasis"><em>subB</em></span>, ... }&#13;
[...] &#13;
pass [ ... ] queue <span class="emphasis"><em>sub_q1</em></span> &#13;
pass [ ... ] queue <span class="emphasis"><em>sub_q2</em></span></pre><div class="note" title="Note"><h3 class="title"><a id="ch07note07"/>Note</h3><p><span class="emphasis"><em>On OpenBSD 5.5 and newer, ALTQ queues are denoted <code class="literal">oldqueue</code> instead of <code class="literal">queue</code> due to an irresolvable syntax conflict with the new queuing subsystem.</em></span></p></div><p>Once queue definitions are in place, you integrate traffic shaping into your rule set by rewriting your <code class="literal">pass</code> or <code class="literal">match</code> rules to assign traffic to a specific queue. Any traffic that you don’t explicitly assign to a specific queue gets lumped in with everything else in the default queue.</p></div><div class="sect2" title="Queue Schedulers, aka Queue Disciplines"><div class="titlepage"><div><div><h3 class="title" id="queue_schedulerscomma_aka_queue_discipli">Queue Schedulers, aka Queue Disciplines</h3></div></div></div><p>In the default networking setup, with no queuing, the TCP/IP stack and its filtering subsystem process the packets according to the FIFO discipline.</p><p>ALTQ offers three queue-scheduler algorithms, or <span class="emphasis"><em>disciplines</em></span>, that can alter this behavior slightly. The types are <code class="literal">priq</code>, <code class="literal">cbq</code>, and <code class="literal">hfsc</code>. Of these, <code class="literal">cbq</code> and <code class="literal">hfsc</code> queues can have several levels of subqueues. The <code class="literal">priq</code> queues are essentially flat, with only one queue level. Each of the disciplines has its own syntax specifics, and we’ll address those in the following sections.</p><div class="sect3" title="priq"><div class="titlepage"><div><div><h4 class="title" id="priq">priq</h4></div></div></div><p><span class="emphasis"><em>Priority-based queues</em></span> are defined purely in terms of priority within the total declared bandwidth. For <code class="literal">priq</code> queues, the allowed priority range is <a class="indexterm" id="iddle1025"/><a class="indexterm" id="iddle1032"/><a class="indexterm" id="iddle1040"/><a class="indexterm" id="iddle1041"/><a class="indexterm" id="iddle1043"/><a class="indexterm" id="iddle1067"/><a class="indexterm" id="iddle1266"/><a class="indexterm" id="iddle1573"/><a class="indexterm" id="iddle1719"/><a class="indexterm" id="iddle1989"/>0 through 15, where a higher value earns preferential treatment. Packets that match the criteria for higher-priority queues are serviced before the ones matching lower-priority queues.</p></div><div class="sect3" title="cbq"><div class="titlepage"><div><div><h4 class="title" id="cbq">cbq</h4></div></div></div><p><span class="emphasis"><em>Class-based queues</em></span> are defined as constant-sized bandwidth allocations, as a percentage of the total available or in units of kilobits, megabits, or gigabits per second. A <code class="literal">cbq</code> queue can be subdivided into queues that are also assigned priorities in the range 0 to 7, and again, a higher priority means preferential treatment.</p></div><div class="sect3" title="hfsc"><div class="titlepage"><div><div><h4 class="title" id="hfsc">hfsc</h4></div></div></div><p>The <code class="literal">hfsc</code> discipline uses the HFSC algorithm to ensure a “fair” allocation of bandwidth among the queues in a hierarchy. HFSC comes with the possibility of setting up queuing regimes with guaranteed minimum allocations and hard upper limits. Allocations can even vary over time, and you can even have fine-grained priority with a 0 to 7 range.</p><p>Because both the algorithm and the corresponding setup with ALTQ are fairly complicated, with a number of tunable parameters, most ALTQ practitioners tend to stick with the simpler queue types. Yet the ones who claim to understand HFSC swear by it.</p></div></div><div class="sect2" title="Setting Up ALTQ"><div class="titlepage"><div><div><h3 class="title" id="setting_up_altq">Setting Up ALTQ</h3></div></div></div><p>Enabling ALTQ may require some extra steps, depending on your choice of operating system.</p><div class="sect3" title="ALTQ on OpenBSD"><div class="titlepage"><div><div><h4 class="title" id="altq_on_openbsd">ALTQ on OpenBSD</h4></div></div></div><p>On OpenBSD 5.5, all supported queue disciplines are compiled into the GENERIC and GENERIC.MP kernels. Check that your OpenBSD version still supports ALTQ. If so, the only configuration you need to do involves editing your <span class="emphasis"><em>pf.conf</em></span>.</p></div><div class="sect3" title="ALTQ on FreeBSD"><div class="titlepage"><div><div><h4 class="title" id="altq_on_freebsd">ALTQ on FreeBSD</h4></div></div></div><p>On FreeBSD, make sure that your kernel has ALTQ and the ALTQ queue discipline options compiled in. The default FreeBSD GENERIC kernel doesn’t have ALTQ options enabled, as you may have noticed from the messages you saw when running the <span class="emphasis"><em>/etc/rc.d/pf</em></span> script to enable PF. The relevant options are as follows:</p><a id="pro_id00275"/><pre class="programlisting">options            ALTQ&#13;
options            ALTQ_CBQ        # Class Bases Queuing (CBQ)&#13;
options            ALTQ_RED        # Random Early Detection (RED)&#13;
options            ALTQ_RIO        # RED In/Out&#13;
options            ALTQ_HFSC       # Hierarchical Packet Scheduler (HFSC)&#13;
options            ALTQ_PRIQ       # Priority Queuing (PRIQ)&#13;
options            ALTQ_NOPCC      # Required for SMP build</pre><p><a class="indexterm" id="iddle1035"/><a class="indexterm" id="iddle1038"/><a class="indexterm" id="iddle1042"/><a class="indexterm" id="iddle1318"/><a class="indexterm" id="iddle1534"/><a class="indexterm" id="iddle1690"/><a class="indexterm" id="iddle1730"/><a class="indexterm" id="iddle1986"/>The <code class="literal">ALTQ</code> option is needed to enable ALTQ in the kernel, but on SMP systems, you also need the <code class="literal">ALTQ_NOPCC</code> option. Depending on which types of queues you’ll be using, you’ll need to enable at least one of these: <code class="literal">ALTQ_CBQ</code>, <code class="literal">ALTQ_PRIQ</code>, or <code class="literal">ALTQ_HFSC</code>. Finally, you can enable the congestion-avoidance techniques <span class="emphasis"><em>random early detection (RED)</em></span> and <span class="emphasis"><em>RED In/Out</em></span> with the <code class="literal">ALTQ_RED</code> and <code class="literal">ALTQ_RIO</code> options, respectively. (See the <span class="emphasis"><em>FreeBSD Handbook</em></span> for information on how to compile and install a custom kernel with these options.)</p></div><div class="sect3" title="ALTQ on NetBSD"><div class="titlepage"><div><div><h4 class="title" id="altq_on_netbsd">ALTQ on NetBSD</h4></div></div></div><p>ALTQ was integrated into the NetBSD 4.0 PF implementation and is supported in NetBSD 4.0 and later releases. NetBSD’s default GENERIC kernel configuration doesn’t include the ALTQ-related options, but the GENERIC configuration file comes with all relevant options commented out for easy inclusion. The main kernel options are these:</p><a id="pro_id00276"/><pre class="programlisting">options    ALTQ         # Manipulate network interfaces' output queues&#13;
options    ALTQ_CBQ     # Class-Based queuing&#13;
options    ALTQ_HFSC    # Hierarchical Fair Service Curve&#13;
options    ALTQ_PRIQ    # Priority queuing&#13;
options    ALTQ_RED     # Random Early Detection</pre><p>The <code class="literal">ALTQ</code> option is needed to enable ALTQ in the kernel. Depending on the types of queues you’ll be using, you must enable at least one of these: <code class="literal">ALTQ_CBQ</code>, <code class="literal">ALTQ_PRIQ</code>, or <code class="literal">ALTQ_HFSC</code>.</p><p>Using ALTQ requires you to compile PF into the kernel because the PF loadable module doesn’t support ALTQ functionality. (See the NetBSD PF documentation at <span class="emphasis"><em><a class="ulink" href="http://www.netbsd.org/Documentation/network/pf.html" target="_top">http://www.netbsd.org/Documentation/network/pf.html</a></em></span> for the most up-to-date information.)</p></div></div></div><div class="sect1" title="Priority-Based Queues"><div class="titlepage"><div><div><h2 class="title" id="priority-based_queues" style="clear: both">Priority-Based Queues</h2></div></div></div><p>The basic concept behind priority-based queues (<code class="literal">priq</code>) is fairly straightforward. Within the total bandwidth allocated to the main queue, only traffic priority matters. You assign queues a priority value in the range 0 through 15, where a higher value means that the queue’s requests for traffic are serviced sooner.</p><div class="sect2" title="Using ALTQ Priority Queues to Improve Performance"><div class="titlepage"><div><div><h3 class="title" id="using_altq_priority_queues_to_improve_pe">Using ALTQ Priority Queues to Improve Performance</h3></div></div></div><p>Daniel Hartmeier discovered a simple yet effective way to improve the throughput for his home network by using ALTQ priority queues. Like many people, he had his home network on an asymmetric connection, with total usable bandwidth low enough that he wanted better bandwidth utilization. In addition, when the line was running at or near capacity, oddities started appearing. One symptom in particular seemed to suggest room for improvement: Incoming traffic (downloads, incoming mail, and such) <a class="indexterm" id="iddle1010"/><a class="indexterm" id="iddle1036"/><a class="indexterm" id="iddle1242"/><a class="indexterm" id="iddle1251"/><a class="indexterm" id="iddle1498"/><a class="indexterm" id="iddle1689"/><a class="indexterm" id="iddle1729"/><a class="indexterm" id="iddle1734"/><a class="indexterm" id="iddle1747"/><a class="indexterm" id="iddle1944"/>slowed down disproportionately whenever outgoing traffic started—more than could be explained by measuring the raw amount of data transferred. It all came back to a basic feature of TCP.</p><p>When a TCP packet is sent, the sender expects acknowledgment (in the form of an ACK packet) from the receiver and will wait a specified time for it to arrive. If the ACK doesn’t arrive within that time, the sender assumes that the packet hasn’t been received and resends it. And because in a default setup, packets are serviced sequentially by the interface as they arrive, ACK packets, with essentially no data payload, end up waiting in line while the larger data packets are transferred.</p><p>If ACK packets could slip in between the larger data packets, the result would be more efficient use of available bandwidth. The simplest practical way to implement such a system with ALTQ is to set up two queues with different priorities and integrate them into the rule set. Here are the relevant parts of the rule set.</p><a id="pro_id00277"/><pre class="programlisting">ext_if="kue0"&#13;
&#13;
altq on $ext_if priq bandwidth 100Kb queue { q_pri, q_def }&#13;
      queue q_pri priority 7&#13;
      queue q_def priority 1 priq(default)&#13;
&#13;
pass out on $ext_if proto tcp from $ext_if queue (q_def, q_pri)&#13;
&#13;
pass in on $ext_if proto tcp to $ext_if queue (q_def, q_pri)</pre><p>Here, the priority-based queue is set up on the external interface with two subordinate queues. The first subqueue, <code class="literal">q_pri</code>, has a high-priority value of 7; the other subqueue, <code class="literal">q_def</code>, has a significantly lower-priority value of 1.</p><p>This seemingly simple rule set works by exploiting how ALTQ treats queues with different priorities. Once a connection is set up, ALTQ inspects each packet’s ToS field. ACK packets have the ToS delay bit set to low, which indicates that the sender wanted the speediest delivery possible. When ALTQ sees a low-delay packet and queues of differing priorities are available, it assigns the packet to the higher-priority queue. This means that the ACK packets skip ahead of the lower-priority queue and are delivered more quickly, which in turn means that data packets are serviced more quickly. The net result is better performance than a pure FIFO configuration with the same hardware and available bandwidth. (Daniel Hartmeier’s article about this version of his setup, cited previously, contains a more detailed analysis.)</p></div><div class="sect2" title="Using a match Rule for Queue Assignment"><div class="titlepage"><div><div><h3 class="title" id="using_a_match_rule_for_queue_assignment">Using a match Rule for Queue Assignment</h3></div></div></div><p>In the previous example, the rule set was constructed the traditional way, with the queue assignment as part of the <code class="literal">pass</code> rules. However, this isn’t the only way to do queue assignment. When you use <code class="literal">match</code> rules (available in OpenBSD 4.6 and later), it’s incredibly easy to retrofit this simple priority-queuing regime onto an existing rule set.</p><p><a class="indexterm" id="iddle1525"/><a class="indexterm" id="iddle1929"/>If you worked through the examples in <a class="xref" href="ch03.html" title="Chapter 3. Into the Real World">Chapter 3</a> and <a class="xref" href="ch04.html" title="Chapter 4. Wireless Networks Made Easy">Chapter 4</a>, your rule set probably has a <code class="literal">match</code> rule that applies <code class="literal">nat-to</code> on your outgoing traffic. To introduce priority-based queuing to your rule set, you first add the queue definitions and make some minor adjustments to your outgoing <code class="literal">match</code> rule.</p><p>Start with the queue definition from the preceding example and adjust the total bandwidth to local conditions, as shown in here.</p><a id="pro_id00278"/><pre class="programlisting">altq on $ext_if priq bandwidth $ext_bw queue { q_pri, q_def }&#13;
      queue q_pri priority 7&#13;
      queue q_def priority 1 priq(default)</pre><p>This gives the queues whatever bandwidth allocation you define with the <code class="literal">ext_bw</code> macro.</p><p>The simplest and quickest way to integrate the queues into your rule set is to edit your outgoing <code class="literal">match</code> rule to read something like this:</p><a id="pro_id00279"/><pre class="programlisting">match out on $ext_if from $int_if:network nat-to ($ext_if) queue (q_def, q_pri)</pre><p>Reload your rule set, and the priority-queuing regime is applied to all traffic that’s initiated from your local network.</p><p>You can use the <code class="literal">systat</code> command to get a live view of how traffic is assigned to your queues.</p><a id="pro_id00280"/><pre class="programlisting"><span class="strong"><strong>$ sudo systat queues</strong></span></pre><p>This will give you a live display that looks something like this:</p><a id="pro_id00281"/><pre class="programlisting">   2 users    Load 0.39 0.27 0.30                      Fri Apr 1 16:33:44 2015&#13;
&#13;
QUEUE                BW SCH  PR   PKTS  BYTES  DROP_P  DROP_B  QLEN BORRO SUSPE  P/S   B/S&#13;
q_pri                   priq  7  21705  1392K       0       0     0               12   803&#13;
q_def                   priq     12138  6759K       0       0     0                9  4620</pre><p>Looking at the numbers in the <code class="literal">PKTS</code> (packets) and <code class="literal">BYTES</code> columns, you see a clear indication that the queuing is working as intended.</p><p>The <code class="literal">q_pri</code> queue has processed a rather large number of packets in relation to the amount of data, just as we expected. The ACK packets don’t take up a lot of space. On the other hand, the traffic assigned to the <code class="literal">q_def</code> queue has more data in each packet, and the numbers show essentially the reverse packet numbers–to–data size ratio as in to the <code class="literal">q_pri</code> queue.</p><div class="note" title="Note"><h3 class="title"><a id="ch07note08"/>Note</h3><p><span class="emphasis"><em><code class="literal">systat</code> is a rather capable program on all BSDs, and the OpenBSD version offers several views that are relevant to PF and that aren’t found in the <code class="literal">systat</code> variants on the other systems as of this writing. We’ll be looking at <code class="literal">systat</code> again in the next chapter. In the meantime, read the man pages and play with the program. It’s a very useful tool for getting to know your system.</em></span></p></div></div><div class="sect2" title="Class-Based Bandwidth Allocation for Small Networks"><div class="titlepage"><div><div><h3 class="title" id="class-based_bandwidth_allocation_for_sma">Class-Based Bandwidth Allocation for Small Networks</h3></div></div></div><p><a class="indexterm" id="iddle1005"/><a class="indexterm" id="iddle1024"/><a class="indexterm" id="iddle1026"/><a class="indexterm" id="iddle1066"/><a class="indexterm" id="iddle1068"/><a class="indexterm" id="iddle1127"/><a class="indexterm" id="iddle1720"/><a class="indexterm" id="iddle1803"/><a class="indexterm" id="iddle1884"/><a class="indexterm" id="iddle1983"/>Maximizing network performance generally feels nice. However, you may find that your network has other needs. For example, it might be important for some traffic—such as mail and other vital services—to have a baseline amount of bandwidth available at all times, while other services—peer-to-peer file sharing comes to mind—shouldn’t be allowed to consume more than a certain amount. To address these kinds of requirements or concerns, ALTQ offers the class-based queue (<code class="literal">cbq</code>) discipline with a slightly larger set of options.</p><p>To illustrate how to use <code class="literal">cbq</code>, we’ll build on the rule sets from previous chapters within a small local network. We want to let the users on the local network connect to a predefined set of services outside their own network and let users from outside the local network access a Web server and an FTP server somewhere on the local network.</p><div class="sect3" title="Queue Definition"><div class="titlepage"><div><div><h4 class="title" id="queue_definition-id00004">Queue Definition</h4></div></div></div><p>All queues are set up on the external, Internet-facing interface. This approach makes sense mainly because bandwidth is more likely to be limited on the external link than on the local network. In principle, however, allocating queues and running traffic shaping can be done on any network interface. The example setup shown here includes a <code class="literal">cbq</code> queue for a total bandwidth of 2Mb with six subqueues.</p><a id="pro_id00282"/><pre class="programlisting">altq on $ext_if cbq bandwidth 2Mb queue { main, ftp, udp, web, ssh, icmp }&#13;
      queue main bandwidth 18% cbq(default borrow red)&#13;
      queue ftp bandwidth 10% cbq(borrow red)&#13;
      queue udp bandwidth 30% cbq(borrow red)&#13;
      queue web bandwidth 20% cbq(borrow red)&#13;
      queue ssh bandwidth 20% cbq(borrow red) { ssh_interactive, ssh_bulk }&#13;
            queue ssh_interactive priority 7 bandwidth 20%&#13;
            queue ssh_bulk priority 5 bandwidth 80%&#13;
      queue icmp bandwidth 2% cbq</pre><p>The subqueue <code class="literal">main</code> has 18 percent of the bandwidth and is designated as the default queue. This means any traffic that matches a <code class="literal">pass</code> rule but isn’t explicitly assigned to some other queue ends up here. The <code class="literal">borrow</code> and <code class="literal">red</code> keywords mean that the queue may “borrow” bandwidth from its parent queue, while the system attempts to avoid congestion by applying the RED algorithm.</p><p>The other queues follow more or less the same pattern up to the subqueue <code class="literal">ssh</code>, which itself has two subqueues with separate priorities. Here, we see a variation on the ACK priority example. Bulk SSH transfers, typically SCP file transfers, are transmitted with a ToS indicating throughput, while interactive SSH traffic has the ToS flag set to low delay and skips ahead of the bulk transfers. The interactive traffic is likely to be less bandwidth consuming and gets a smaller share of the bandwidth, but it receives <a class="indexterm" id="iddle1027"/><a class="indexterm" id="iddle1031"/><a class="indexterm" id="iddle1033"/><a class="indexterm" id="iddle1069"/><a class="indexterm" id="iddle1128"/><a class="indexterm" id="iddle1324"/><a class="indexterm" id="iddle1325"/><a class="indexterm" id="iddle1346"/><a class="indexterm" id="iddle1431"/><a class="indexterm" id="iddle1721"/><a class="indexterm" id="iddle1725"/><a class="indexterm" id="iddle1985"/>preferential treatment because of the higher-priority value assigned to it. This scheme also helps the speed of SCP file transfers because the ACK packets for the SCP transfers will be assigned to the higher-priority subqueue.</p><p>Finally, we have the <code class="literal">icmp</code> queue, which is reserved for the remaining 2 percent of the bandwidth from the top level. This guarantees a minimum amount of bandwidth for ICMP traffic that we want to pass but that doesn’t match the criteria for being assigned to the other queues.</p></div><div class="sect3" title="Rule Set"><div class="titlepage"><div><div><h4 class="title" id="rule_set-id00005">Rule Set</h4></div></div></div><p>To make it all happen, we use these <code class="literal">pass</code> rules, which indicate which traffic is assigned to the queues and their criteria:</p><a id="pro_id00283"/><pre class="programlisting">set skip on { lo, $int_if }&#13;
pass log quick on $ext_if proto tcp to port ssh queue (ssh_bulk, ssh_&#13;
interactive)&#13;
pass in quick on $ext_if proto tcp to port ftp queue ftp&#13;
pass in quick on $ext_if proto tcp to port www queue http&#13;
pass out on $ext_if proto udp queue udp&#13;
pass out on $ext_if proto icmp queue icmp&#13;
pass out on $ext_if proto tcp from $localnet to port $client_out</pre><p>The rules for <code class="literal">ssh</code>, <code class="literal">ftp</code>, <code class="literal">www</code>, <code class="literal">udp</code>, and <code class="literal">icmp</code> assign traffic to their respective queues. The last catchall rule passes all other traffic from the local network, lumping it into the default <code class="literal">main</code> queue.</p></div></div><div class="sect2" title="A Basic HFSC Traffic Shaper"><div class="titlepage"><div><div><h3 class="title" id="basic_hfsc_traffic_shaper">A Basic HFSC Traffic Shaper</h3></div></div></div><p>The simple schedulers we have looked at so far can make for efficient setups, but network admins with traffic-shaping ambitions tend to look for a little more flexibility than can be found in the pure-priority-based queues or the simple class-based variety. The HFSC queuing algorithm (<code class="literal">hfsc</code> in <span class="emphasis"><em>pf.conf</em></span> terminology) offers flexible bandwidth allocation, guaranteed lower and upper bounds for bandwidth available to each queue, and variable allocations over time, and it only starts shaping when there’s an actual need. However, the added flexibility comes at a price: The setup is a tad more complex than the other ALTQ types, and tuning your setup for an optimal result can be quite an interesting process.</p><div class="sect3" title="Queue Definition"><div class="titlepage"><div><div><h4 class="title" id="queue_definition-id00006">Queue Definition</h4></div></div></div><p>First, working from the same configuration we altered slightly earlier, we insert this queue definition early in the <span class="emphasis"><em>pf.conf</em></span> file:</p><a id="pro_id00284"/><pre class="programlisting">altq on $ext_if bandwidth $ext_bw hfsc queue { main, spamd }&#13;
       queue main bandwidth 99% priority 7 qlimit 100 hfsc (realtime 20%, linkshare 99%) \&#13;
            { q_pri, q_def, q_web, q_dns }&#13;
          queue q_pri bandwidth 3% priority 7 hfsc (realtime 0, linkshare 3% red )&#13;
          queue q_def bandwidth 47% priority 1 hfsc (default realtime 30% linkshare 47% red)&#13;
          queue q_web bandwidth 47% priority 1 hfsc (realtime 30% linkshare 47% red)&#13;
          queue q_dns bandwidth 3% priority 7 qlimit 100 hfsc (realtime (30Kb 3000 12Kb), \&#13;
                  linkshare 3%)&#13;
       queue spamd bandwidth 0% priority 0 qlimit 300 hfsc (realtime 0, upperlimit 1%, \&#13;
            linkshare 1%)</pre><p><a class="indexterm" id="iddle1034"/><a class="indexterm" id="iddle1327"/><a class="indexterm" id="iddle1499"/><a class="indexterm" id="iddle1708"/><a class="indexterm" id="iddle1726"/><a class="indexterm" id="iddle1745"/><a class="indexterm" id="iddle2015"/>The <code class="literal">hfsc</code> queue definitions take slightly different parameters than the simpler disciplines. We start off with this rather small hierarchy by splitting the top-level queue into two. At the next level, we subdivide the <code class="literal">main</code> queue into several subqueues, each with a defined priority. All the subqueues have a <code class="literal">realtime</code> value set—the guaranteed minimum bandwidth allocated to the queue. The optional <code class="literal">upperlimit</code> sets a hard upper limit on the queue’s allocation. The <code class="literal">linkshare</code> parameter sets the allocation the queue will have available when it’s backlogged—that is, when it’s started to eat into its <code class="literal">qlimit</code> allocation.</p><p>In case of congestion, each queue by default has a pool of 50 slots, the queue limit (<code class="literal">qlimit</code>), to keep packets around when they can’t be transmitted immediately. In this example, the top-level queues <code class="literal">main</code> and <code class="literal">spamd</code> both have larger-than-default pools set by their <code class="literal">qlimit</code> setting: <code class="literal">100</code> for <code class="literal">main</code> and <code class="literal">300</code> for <code class="literal">spamd</code>. Cranking up queue sizes here means we’re a little less likely to drop packets when the traffic approaches the set limits, but it also means that when the traffic shaping kicks in, we’ll see increased latency for connections that end up in these larger than default pools.</p><p>The queue hierarchy here uses two familiar tricks to make efficient use of available bandwidth:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>It uses a variation of the high- and low-priority mix demonstrated in the earlier pure-priority example.</p></li><li class="listitem"><p>We speed up almost all other traffic (and most certainly the Web traffic that appears to be the main priority here) by allocating a small but guaranteed portion of bandwidth for name service lookups. For the <code class="literal">q_dns</code> queue, we set up the <code class="literal">realtime</code> value with a time limit—after <code class="literal">3000</code> milliseconds, the <code class="literal">realtime</code> allocation goes down to <code class="literal">12Kb</code>. This can be useful to speed connections that transfer most of their payload during the early phases.</p></li></ul></div></div><div class="sect3" title="Rule Set"><div class="titlepage"><div><div><h4 class="title" id="rule_set-id00007">Rule Set</h4></div></div></div><p>Next, we tie the newly created queues into the rule set. If you have a filtering regime in place already, which we’ll assume you do, the tie-in becomes amazingly simple, accomplished by adding a few <code class="literal">match</code> rules.</p><a id="pro_id00285"/><pre class="programlisting">match out on $ext_if from $air_if:network nat-to ($ext_if) \&#13;
    queue (q_def, q_pri)&#13;
match out on $ext_if from $int_if:network nat-to ($ext_if) \&#13;
    queue (q_def, q_pri)&#13;
match out on $ext_if proto tcp to port { www https } queue (q_web, q_pri)&#13;
match out on $ext_if proto { tcp udp } to port domain queue (q_dns, q_pri)&#13;
match out on $ext_if proto icmp queue (q_dns, q_pri)</pre><p><a class="indexterm" id="iddle1008"/><a class="indexterm" id="iddle1039"/><a class="indexterm" id="iddle1065"/><a class="indexterm" id="iddle1193"/><a class="indexterm" id="iddle1930"/><a class="indexterm" id="iddle1988"/>Here, the <code class="literal">match</code> rules once again do the ACK packet speedup trick with the high- and low-priority queue assignment, just as you saw earlier in the pure-priority-based system. The only exception is when we assign traffic to our lowest-priority queue, where we really don’t care to have any speedup at all.</p><a id="pro_id00286"/><pre class="programlisting">pass in log on egress proto tcp to port smtp rdr-to 127.0.0.1 port spamd queue spamd</pre><p>This rule is intended to slow down the spammers a little more on their way to our <code class="literal">spamd</code>. With a hierarchical queue system in place, <code class="literal">systat queues</code> shows the queues and their traffic as a hierarchy, too.</p><a id="pro_id00287"/><pre class="programlisting">  2 users    Load 0.22 0.25 0.25                    Fri Apr 3 16:43:37 2015&#13;
&#13;
QUEUE         BW  SCH  PRIO    PKTS    BYTES  DROP_P  DROP_B QLEN  BORROW SUSPEN P/S    B/S&#13;
root_nfe0     20M hfsc    0       0        0       0       0    0                  0      0&#13;
 main         19M hfsc    7       0        0       0       0    0                  0      0&#13;
  q_pri      594K hfsc    7    1360    82284       0       0    0                 11    770&#13;
  q_def     9306K hfsc          158    15816       0       0    0                0.2     11&#13;
  q_web     9306K hfsc          914   709845       0       0    0                 50  61010&#13;
  q_dns      594K hfsc    7     196    17494       0       0    0                  3    277&#13;
  spamd         0 hfsc    0     431    24159       0       0    0                  2    174</pre><p>The root queue is shown as attached to the physical interface—as <code class="literal">nfe0</code> and <code class="literal">root_nfe0</code>, in this case. <code class="literal">main</code> and its subqueues—<code class="literal">q_pri</code>, <code class="literal">q_def</code>, <code class="literal">q_web</code>, and <code class="literal">q_dns</code>—are shown with their bandwidth allocations and number of bytes and packets passed. The <code class="literal">DROP_P</code> and <code class="literal">DROP_B</code> columns are where number of packets and bytes dropped, respectively, would appear if we had been forced to drop packets at this stage. The final two columns show live updates of packets per second and bytes per second.</p></div></div><div class="sect2" title="Queuing for Servers in a DMZ"><div class="titlepage"><div><div><h3 class="title" id="queuing_for_servers_in_a_dmz">Queuing for Servers in a DMZ</h3></div></div></div><p>In <a class="xref" href="ch05.html" title="Chapter 5. Bigger or Trickier Networks">Chapter 5</a>, we set up a network with a single gateway but with all externally visible services configured on a separate DMZ network. That way, all traffic to the servers from both the Internet and the internal network had to pass through the gateway (see <a class="xref" href="ch07.html#network_with_dmz" title="Figure 7-1. Network with DMZ">Figure 7-1</a>).</p><p>With the rule set from <a class="xref" href="ch05.html" title="Chapter 5. Bigger or Trickier Networks">Chapter 5</a> as our starting point, we’ll add some queuing in order to optimize our network resources. The physical and logical layout of the network will not change. The most likely bottleneck for this network is the bandwidth for the connection between the gateway’s external interface and the Internet at large. The bandwidth elsewhere in our setup isn’t infinite, of course, but the available bandwidth on any interface in the local network is likely to be less of a limiting factor than the bandwidth actually available for communication with the outside world. For services to be available with the best possible performance, we need to set up the queues so the bandwidth available at the site is made available to the traffic we want to allow.</p><p>In our example, it’s likely that the interface bandwidth on the DMZ interface is either 100Mb or 1Gb, while the <span class="emphasis"><em>actual available bandwidth</em></span> for connections from outside the local network is considerably smaller. This consideration shows up in our queue definitions, where you clearly see that the bandwidth available for external traffic is the main limitation in the queue setup.</p><a id="pro_id00288"/><pre class="programlisting">total_ext = 2Mb&#13;
total_dmz = 100Mb&#13;
altq on $ext_if cbq bandwidth $total_ext queue { ext_main, ext_web, ext_udp, \&#13;
    ext_mail, ext_ssh }&#13;
queue ext_main bandwidth 25% cbq(default borrow red) { ext_hi, ext_lo }&#13;
queue ext_hi priority 7 bandwidth 20%&#13;
queue ext_lo priority 0 bandwidth 80%&#13;
queue ext_web bandwidth 25% cbq(borrow red)&#13;
queue ext_udp bandwidth 20% cbq(borrow red)&#13;
queue ext_mail bandwidth 30% cbq(borrow red)&#13;
altq on $dmz_if cbq bandwidth $total_dmz queue { ext_dmz, dmz_main, dmz_web, \&#13;
    dmz_udp, dmz_mail }&#13;
queue ext_dmz bandwidth $total_ext cbq(borrow red) queue { ext_dmz_web, \&#13;
    ext_dmz_udp, ext_dmz_mail }&#13;
      queue ext_dmz_web bandwidth 40% priority 5&#13;
      queue ext_dmz_udp bandwidth 10% priority 7&#13;
      queue ext_dmz_mail bandwidth 50% priority 3&#13;
queue dmz_main bandwidth 25Mb cbq(default borrow red) queue { dmz_main_hi, \&#13;
    dmz_main_lo }&#13;
queue dmz_main_hi priority 7 bandwidth 20%&#13;
queue dmz_main_lo priority 0 bandwidth 80%&#13;
queue dmz_web bandwidth 25Mb cbq(borrow red)&#13;
queue dmz_udp bandwidth 20Mb cbq(borrow red)&#13;
queue dmz_mail bandwidth 20Mb cbq(borrow red)</pre><p>Notice that the <code class="literal">total_ext</code> bandwidth limitation determines the allocation for all queues where the bandwidth for external connections is available. In order to use the new queuing infrastructure, we need to make some changes to the filtering rules, too. Keep in mind that any traffic you don’t explicitly assign to a specific queue is assigned to the default queue for the interface. Thus, it’s important to tune your filtering rules as well as your queue definitions to the actual traffic in your network.</p><p>With queue assignment, the main part of the filtering rules could end up looking like this:</p><a id="pro_id00289"/><pre class="programlisting">pass in on $ext_if proto { tcp, udp } to $nameservers port domain \&#13;
    queue ext_udp&#13;
pass in on $int_if proto { tcp, udp } from $localnet to $nameservers \&#13;
    port domain&#13;
pass out on $dmz_if proto { tcp, udp } to $nameservers port domain \&#13;
    queue ext_dmz_udp&#13;
pass out on $dmz_if proto { tcp, udp } from $localnet to $nameservers \&#13;
    port domain queue dmz_udp&#13;
pass in on $ext_if proto tcp to $webserver port $webports queue ext_web&#13;
pass in on $int_if proto tcp from $localnet to $webserver port $webports&#13;
pass out on $dmz_if proto tcp to $webserver port $webports queue ext_dmz_web&#13;
pass out on $dmz_if proto tcp from $localnet to $webserver port $webports \&#13;
    queue dmz_web&#13;
pass in log on $ext_if proto tcp to $mailserver port smtp&#13;
pass in log on $ext_if proto tcp from $localnet to $mailserver port smtp&#13;
pass in log on $int_if proto tcp from $localnet to $mailserver port $email&#13;
pass out log on $dmz_if proto tcp to $mailserver port smtp queue ext_mail&#13;
pass in on $dmz_if from $mailserver to port smtp queue dmz_mail&#13;
pass out log on $ext_if proto tcp from $mailserver to port smtp \&#13;
    queue ext_dmz_mail</pre><p><a class="indexterm" id="iddle1028"/><a class="indexterm" id="iddle1030"/><a class="indexterm" id="iddle1582"/><a class="indexterm" id="iddle1984"/>Notice that only traffic that will pass either the DMZ interface or the external interface is assigned to queues. In this configuration, with no externally accessible services on the internal network, queuing on the internal interface wouldn’t make much sense because it’s likely the part of our network with the least restrictions on available bandwidth.</p></div><div class="sect2" title="Using ALTQ to Handle Unwanted Traffic"><div class="titlepage"><div><div><h3 class="title" id="using_altq_to_handle_unwanted_traffic">Using ALTQ to Handle Unwanted Traffic</h3></div></div></div><p>So far, we’ve focused on queuing as a method to make sure specific kinds of traffic are let through as efficiently as possible given the conditions that exist in and around your network. Now, we’ll look at two examples that present a slightly different approach to identify and handle unwanted traffic in order to demonstrate some queuing-related tricks you can use to keep miscreants in line.</p><div class="sect3" title="Overloading to a Tiny Queue"><div class="titlepage"><div><div><h4 class="title" id="overloading_to_a_tiny_queue-id00008">Overloading to a Tiny Queue</h4></div></div></div><p>Think back to <a class="xref" href="ch06.html#turning_away_the_brutes" title="Turning Away the Brutes">Turning Away the Brutes</a>, where we used a combination of state-tracking options and <code class="literal">overload</code> rules to fill up a table of addresses for special treatment. The special treatment we demonstrated in <a class="xref" href="ch06.html" title="Chapter 6. Turning the Tables for Proactive Defense">Chapter 6</a> was to cut all connections, but it’s equally possible to assign <code class="literal">overload</code> traffic to a specific queue instead.</p><p>Consider this rule from our class-based bandwidth example in <a class="xref" href="ch07.html#class-based_bandwidth_allocation_for_sma" title="Class-Based Bandwidth Allocation for Small Networks">Class-Based Bandwidth Allocation for Small Networks</a>.</p><a id="pro_id00290"/><pre class="programlisting">pass log quick on $ext_if proto tcp to port ssh flags S/SA \&#13;
   keep state queue (ssh_bulk, ssh_interactive)</pre><p>We could add state-tracking options, as shown in here.</p><a id="pro_id00291"/><pre class="programlisting">pass log quick on $ext_if proto tcp to port ssh flags S/SA \&#13;
   keep state (max-src-conn 15, max-src-conn-rate 5/3, \&#13;
   overload &lt;bruteforce&gt; flush global) queue (ssh_bulk, ssh_interactive)</pre><p>Then, we could make one of the queues slightly smaller.</p><a id="pro_id00292"/><pre class="programlisting">queue smallpipe bandwidth 1kb cbq</pre><p>Next, we could assign traffic from miscreants to the small-bandwidth queue with the following rule.</p><a id="pro_id00293"/><pre class="programlisting">pass inet proto tcp from &lt;bruteforce&gt; to port $tcp_services queue smallpipe</pre><p><a class="indexterm" id="iddle1029"/><a class="indexterm" id="iddle1578"/>It might also be useful to supplement rules like these with table-entry expiry, as described in <a class="xref" href="ch06.html#tidying_your_tables_with_pfctl" title="Tidying Your Tables with pfctl">Tidying Your Tables with pfctl</a>.</p></div><div class="sect3" title="Queue Assignments Based on Operating System Fingerprint"><div class="titlepage"><div><div><h4 class="title" id="queue_assignments_based_on_opera-id00009">Queue Assignments Based on Operating System Fingerprint</h4></div></div></div><p><a class="xref" href="ch06.html" title="Chapter 6. Turning the Tables for Proactive Defense">Chapter 6</a> covered several ways to use <code class="literal">spamd</code> to cut down on spam. If running <code class="literal">spamd</code> isn’t an option in your environment, you can use a queue and rule set based on the common knowledge that machines that send spam are likely to run a particular operating system.</p><p>PF has a fairly reliable operating system fingerprinting mechanism, which detects the operating system at the other end of a network connection based on characteristics of the initial SYN packets at connection setup. The following may be a simple substitute for <code class="literal">spamd</code> if you’ve determined that legitimate mail is highly unlikely to be delivered from systems that run that particular operating system.</p><a id="pro_id00294"/><pre class="programlisting">pass quick proto tcp from any os "Windows" to $ext_if port smtp queue smallpipe</pre><p>Here, email traffic originating from hosts that run a particular operating system get no more than 1KB of your bandwidth, with no borrowing.</p></div></div></div><div class="sect1" title="Conclusion: Traffic Shaping for Fun, and Perhaps Even Profit"><div class="titlepage"><div><div><h2 class="title" id="conclusion_traffic_shaping_for_funcomma" style="clear: both">Conclusion: Traffic Shaping for Fun, and Perhaps Even Profit</h2></div></div></div><p>This chapter has dealt with traffic-shaping techniques that can make your traffic move faster, or at least make preferred traffic pass more efficiently and according to your specifications. By now you should have at least a basic understanding of traffic-shaping concepts and how they apply to the traffic-shaping tool set you’ll be using on your systems.</p><p>I hope that the somewhat stylized (but functional) examples in this chapter have given you a taste of what’s possible with traffic shaping and that the material has inspired you to play with some of your own ideas of how you can use the traffic-shaping tools in your networks. If you pay attention to your network traffic and the underlying needs it expresses (see <a class="xref" href="ch09.html" title="Chapter 9. Logging, Monitoring, and Statistics">Chapter 9</a> and <a class="xref" href="ch10.html" title="Chapter 10. Getting Your Setup Just Right">Chapter 10</a> for more on studying network traffic in detail), you can use the traffic-shaping tools to improve the way your network serves its users. With a bit of luck, your users will appreciate your efforts and you may even enjoy the experience.</p></div><div class="footnotes" epub:type="footnotes"><br/><hr style="width: 100; align: left;"/><div class="footnote" id="ftn.ch07fn01"><p><sup>[<a class="para" href="#ch07fn01">39</a>] </sup>Daniel Hartmeier, one of the original PF developers, wrote a nice article about this problem, which is available at <span class="emphasis"><em><a class="ulink" href="http://www.benzedrine.cx/ackpri.html" target="_top">http://www.benzedrine.cx/ackpri.html</a></em></span>. Daniel’s explanations use the older ALTQ priority queues syntax but include data that clearly illustrates the effect of assigning two different priorities to help ACKs along.</p></div><div class="footnote" epub:type="footnote" id="ftn.ch07fn01a"><p><sup>[<a class="para" href="#ch07fn01a">40</a>] </sup>This really dates the book, I know. In a few years, these numbers will seem quaint.</p></div><div class="footnote" id="ftn.ch07fn02"><p><sup>[<a class="para" href="#ch07fn02">41</a>] </sup>The original research on ALTQ was presented in a paper for the USENIX 1999 conference. You can read Kenjiro Cho’s paper “Managing Traffic with ALTQ” online at <span class="emphasis"><em><a class="ulink" href="http://www.usenix.org/publications/library/proceedings/usenix99/cho.html" target="_top">http://www.usenix.org/publications/library/proceedings/usenix99/cho.html</a></em></span>. The code turned up in OpenBSD soon after through the efforts of Cho and Chris Cappucio.</p></div></div></section></body></html>