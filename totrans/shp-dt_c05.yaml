- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Geometry in Data Science
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学中的几何学
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: 'In this chapter, we’ll explore several tools from geometry: we’ll look at distance
    metrics and their use in *k*-nearest neighbor algorithms; we’ll discuss manifold
    learning algorithms that map high-dimensional data to potentially curved lower-dimensional
    manifolds; and we’ll see how to apply fractal geometry to stock market data. The
    motivation for this chapter follows, among other things, from the *manifold hypothesis*,
    which posits that real-world data often has a natural dimensionality lower than
    the dimensionality of the dataset collected. In other words, a dataset that has
    20 variables (that is, a dimensionality of 20) might have a better representation
    in a 12-dimensional space or an 8-dimensional space. Given the curse of dimensionality,
    representing data in lower-dimensional spaces is ideal (particularly when the
    original dimensionality of a dataset is large, as in genomics or proteomics data).
    Choosing the right distance measurements needed to create these representations
    has important implications for solution quality.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索几何学中的几个工具：我们将研究距离度量及其在*k*-最近邻算法中的应用；我们将讨论将高维数据映射到潜在弯曲的低维流形的流形学习算法；并且我们将看到如何将分形几何应用于股票市场数据。本章的动机来源于*流形假设*，该假设认为现实世界中的数据通常具有比收集到的数据集维度更低的自然维度。换句话说，一个具有20个变量（即20维度）的数据集，可能在12维空间或8维空间中有更好的表示。鉴于维度的诅咒，在低维空间中表示数据是理想的（尤其是当数据集的原始维度很大时，如基因组学或蛋白质组学数据）。选择合适的距离度量来创建这些表示对于解决方案的质量具有重要意义。
- en: Introduction to Distance Metrics in Data
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据中的距离度量简介
- en: Many machine learning algorithms depend on distance metrics, which provide a
    measure between points or objects in a space or manifold. Changes in choice of
    distance metric can impact machine learning performance dramatically, as we’ll
    see later in this chapter. *Distance metrics* provide a measure between points
    or objects in a space or manifold. This can be relatively straightforward like
    using a ruler to measure the distance between two points on a flat sheet of paper,
    as demonstrated in [Figure 5-1](#figure5-1).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习算法依赖于距离度量，它们提供了空间或流形中点或对象之间的度量。选择距离度量的变化可以显著影响机器学习的表现，正如我们将在本章后面看到的那样。*距离度量*提供了空间或流形中点或对象之间的度量。这可以像使用尺子测量平面纸上两点之间的距离那样简单，正如[图5-1](#figure5-1)中所演示的。
- en: '![](image_fi/503083c05/f05001r.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05001r.png)'
- en: 'Figure 5-1: A plot of two points on a sheet of paper and the line connecting
    them'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-1：纸面上两个点的图示及连接它们的直线
- en: However, measuring the distance between two points on a sphere using a ruler
    will surely be a bit more complicated.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用尺子来测量球面上两点之间的距离肯定会更为复杂。
- en: If you used a piece of string to limn out the shortest path connecting the two
    points on the sphere, as in [Figure 5-2](#figure5-2), you could mark the distance
    on the string and then use a ruler to measure that distance on the straightened-out
    string. This is akin to what is done with distances on manifolds, where *geodesics*
    (shortest paths between two points relative to the curved manifold) are lifted
    into the *tangent space* (a zero-curvature space defined by tangent lines, tangent
    planes, and higher-dimensional tangents) to measure distances.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你用一根绳子标出连接球面上两点的最短路径，如[图5-2](#figure5-2)所示，你可以在绳子上标记距离，然后用尺子测量拉直后的绳子上的距离。这类似于在流形上度量距离时所做的事情，其中*测地线*（相对于弯曲流形的两点之间的最短路径）被提升到*切空间*（由切线、切平面和更高维切线定义的零曲率空间）来测量距离。
- en: '![](image_fi/503083c05/f05002.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05002.png)'
- en: 'Figure 5-2: A plot of two points on a sphere, along with the geodesic connecting
    them'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-2：球面上两个点的图示，以及连接它们的测地线
- en: We’ll explore tangent spaces and their applications in machine learning in more
    depth in [Chapter 6](c06.xhtml), but for now, you can think of lifting the string
    to a large sheet of paper and measuring its length with a ruler to measure distance
    outside of the curved space, where it’s more difficult to establish a standard
    measurement. While geodesics and tangent spaces look counterintuitive, they follow
    from our knowledge of tangents in Euclidean geometry and derivatives in calculus.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第6章](c06.xhtml)中更深入地探讨切空间及其在机器学习中的应用，但目前你可以将其想象为将弦提升到一张大纸上，并用尺子测量其长度，以便在曲面外部测量距离，因为在这里更难建立标准的测量方式。虽然测地线和切空间看起来违反直觉，但它们源自我们对欧几里得几何中切线和微积分中导数的理解。
- en: However, there are other situations in which distances between two points are
    a bit more complicated. Consider walking from one house to another in the neighborhood,
    as shown in [Figure 5-3](#figure5-3).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有其他情况下，两点之间的距离会更加复杂。考虑从一个房屋走到另一个房屋，正如在[图5-3](#figure5-3)中所示。
- en: '![](image_fi/503083c05/f05003.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05003.png)'
- en: 'Figure 5-3: A plot of houses in a neighborhood, where one is walking between
    two houses'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-3：一个社区的房屋图，其中一个人在两栋房屋之间走动
- en: Unless one is able to walk through neighboring houses without running into exterior
    and interior walls (not to mention disgruntled neighbors!), it’s not possible
    to draw a straight line or geodesic between the houses that gives a direct route,
    as you can see in [Figure 5-4](#figure5-4).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 除非有人能够在邻近的房屋之间穿行，而不碰到外墙和内墙（更不用说遇到不满的邻居了！），否则无法画出一条在两栋房屋之间的直线或测地线来提供一条直接路径，正如在[图5-4](#figure5-4)中所看到的。
- en: '![](image_fi/503083c05/f05004.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05004.png)'
- en: 'Figure 5-4: A plot of houses in a neighborhood, where one attempts a straight
    line between houses'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-4：一个社区的房屋图，其中一个人尝试在房屋之间画一条直线
- en: Instead, it’s a lot more practical to take the sidewalks ([Figure 5-5](#figure5-5)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，走人行道要实际得多（见[图5-5](#figure5-5)）。
- en: '![](image_fi/503083c05/f05005.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05005.png)'
- en: 'Figure 5-5: A plot of houses in a neighborhood, where one walks on the sidewalks
    between houses'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-5：一个社区的房屋图，其中一个人在房屋之间的人行道上走动
- en: Distance is often discrete, rather than continuous, or lies on a manifold with
    curvature. Understanding the geometry of the data space in which the data points
    live can give a good indication of what distance metric is appropriate for the
    data. In the following section, we’ll go over some common distance metrics in
    machine learning, and then, in the sections after that, we’ll apply these distances
    to *k*-NN algorithms and dimensionality reduction algorithms.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 距离通常是离散的，而非连续的，或者位于具有曲率的流形上。理解数据空间的几何形状，可以很好地指示出适合数据的距离度量。在接下来的部分中，我们将介绍一些机器学习中常见的距离度量，之后的章节中，我们会将这些距离应用于*k*-NN算法和降维算法。
- en: Common Distance Metrics
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见的距离度量
- en: Given the nuances of measuring distance, it’s important to understand some of
    the more common distance metrics used in machine learning, including one we briefly
    encountered in [Chapter 4](c04.xhtml) (Wasserstein distance, used to compare persistent
    homology results). There are an infinite number of distance metrics, and some
    distance metrics have parameters that can give rise to an infinite number of variations.
    Thus, we cannot cover all possible distance metrics one could encounter in machine
    learning. We’ve left out some that are useful in recommender systems, such as
    cosine distance, as they are uncommon metrics within topological data analysis
    or network analysis applications. We’ll explore some of the more common ones;
    if you’re interested in going further, we suggest you explore the field of *metric
    geometry*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于度量距离的复杂性，理解一些机器学习中常用的距离度量非常重要，包括我们在[第4章](c04.xhtml)中简要接触到的一个（Wasserstein距离，用于比较持久同调结果）。距离度量有无限多种，有些距离度量还具有可以引起无限种变体的参数。因此，我们无法涵盖所有可能在机器学习中遇到的距离度量。我们已经排除了在推荐系统中有用的某些度量，如余弦距离，因为它们在拓扑数据分析或网络分析应用中并不常见。我们将探讨一些常见的度量；如果你有兴趣深入了解，我们建议你探索*度量几何*这一领域。
- en: Simulating a Small Dataset
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模拟一个小数据集
- en: Before we start exploring common distance metrics, let’s simulate some data
    with [Listing 5-1](#listing5-1).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始探索常见的距离度量之前，让我们通过[清单5-1](#listing5-1)来模拟一些数据。
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Listing 5-1: A script that simulates and plots a small dataset'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 清单5-1：模拟并绘制小数据集的脚本
- en: This script creates a dataset with three variables and plots points in a three-dimensional
    space. This should give a plot with points lying on the three axes ([Figure 5-6](#figure5-6)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本创建了一个包含三个变量的数据集，并在三维空间中绘制点。这将生成一个绘制在三个坐标轴上的点图（[图 5-6](#figure5-6)）。
- en: '![](image_fi/503083c05/f05006.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05006.png)'
- en: 'Figure 5-6: A plot of five points, all lying on axes defined by variables *a*,
    *b*, and *c*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-6：五个点的图，所有点都位于由变量*a*、*b*和*c*定义的坐标轴上
- en: This dataset includes the points shown in [Listing 5-2](#listing5-2), which
    we will use to calculate distances between points.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包括[清单 5-2](#listing5-2)中显示的点，我们将用它来计算点之间的距离。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Listing 5-2: A matrix of the five points in the simulated dataset with random
    variables *a*, *b*, and *c*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 5-2：包含模拟数据集中五个点的矩阵，这些点有随机变量*a*、*b*和*c*
- en: Now that we have a dataset generated, let’s look at some standard distance metrics
    that can be used to measure the distance between pairs of points in the dataset.
    R comes with a handy package, called the *stats* package (which comes with the
    base R installation), for calculating some of the common distance metrics used
    on data through the `dist()` function.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经生成了一个数据集，让我们看看一些可以用来衡量数据集中点对之间距离的标准距离度量。R自带了一个非常方便的包，叫做*stats*包（它是R基本安装的一部分），可以通过`dist()`函数计算一些常见的距离度量。
- en: Using Norm-Based Distance Metrics
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用基于范数的距离度量
- en: The first distances we’ll consider are related. The *norm* of a function or
    vector is a measurement of the “length” of that function or vector. The norm involves
    summing distance differences to a power and then applying that power’s root to
    the result. For the *Euclidean distance* between points, for example, the squares
    of differences are summed before taking the square root of the result. For single
    vectors (that is, a single data point), the norm will be a weighted distance from
    the origin, where the axes mutually intersect. You can think of this as the length
    of a straight line from the origin to the point being measured. Going back to
    the scatterplot of our points, this might be drawn like in [Figure 5-7](#figure5-7).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先考虑的距离是相关的。*范数*是一个函数或向量的“长度”度量。范数涉及将距离差异的幂次求和，然后对结果应用该幂次的根。例如，对于点之间的*欧几里得距离*，会先求出差异的平方和，然后对结果取平方根。对于单个向量（即单个数据点），范数将是从原点到该点的加权距离，其中坐标轴互相交叉。你可以把它想象成从原点到被测点的直线长度。回到我们点的散点图，可能像[图
    5-7](#figure5-7)所示那样绘制。
- en: '![](image_fi/503083c05/f05007.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05007.png)'
- en: 'Figure 5-7: A plot of the five points with a straight line pointing to one
    of the points in the set'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-7：一个包含五个点的图，图中有一条直线指向集合中的某个点
- en: The most common norm used to measure metric distance between points is probably
    the Euclidean distance mentioned earlier, given by the L²-norm, defined as the
    square root of squared distance between points where L is a placeholder for the
    vector (or vectors) and the exponent is the power of the norm (here, 2). This
    is the distance typically taught in high school geometry classes, and it is also
    referred to as the *Pythagorean distance*. We saw it in [Figure 5-4](#figure5-4),
    which showed a straight line of shortest distance between the houses (traveling
    as the bird flies above the houses). Statisticians typically use the square of
    the Euclidean distance metric when calculating squared errors in regression algorithms;
    for reasons we won’t delve into here, using the square of Euclidean distance is
    very natural.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 用于度量点之间度量距离的最常见范数可能是前面提到的欧几里得距离，由L²-范数给出，定义为点之间的平方距离的平方根，其中L是向量（或向量组）的占位符，指数是范数的幂次（这里是2）。这是高中的几何学课程中通常教授的距离，也被称为*毕达哥拉斯距离*。我们在[图
    5-4](#figure5-4)中看到了它，图中显示了房屋之间最短距离的直线（按鸟飞的路径从空中看）。统计学家在计算回归算法中的平方误差时通常使用欧几里得距离的平方；由于一些原因，我们在这里不再深入探讨，使用欧几里得距离的平方是非常自然的。
- en: Related to the L²-norm is the L¹-norm, or *Manhattan distance*. Manhattan distance
    calculations are much like the neighborhood example given in [Figure 5-5](#figure5-5).
    Manhattan distance is defined as the sum of point differences along each axis,
    with the axes’ point differences summed into a final tally of axis distances.
    Let’s say we have two points (0, 1) and (1, 0), which might represent whether
    a patient has a gene mutation in either gene of interest within a disease model.
    The Manhattan distance is (0 + 1) + (1 + 0), or the sum of point differences across
    all vector axes. In this example, we find the Manhattan distance is 2.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与L²范数相关的是L¹范数，或称*曼哈顿距离*。曼哈顿距离计算类似于[图 5-5](#figure5-5)中给出的邻域示例。曼哈顿距离定义为沿每个轴的点差的总和，轴的点差被求和到一个最终的轴距离总计中。假设我们有两个点(0,
    1)和(1, 0)，它们可能表示患者是否在疾病模型中某个感兴趣基因中存在基因突变。曼哈顿距离是(0 + 1) + (1 + 0)，即沿所有向量轴的点差之和。在这个例子中，我们发现曼哈顿距离为2。
- en: This metric is useful when working with count data or other discrete data formats,
    such as the example dataset generated earlier in this section. [Figure 5-5](#figure5-5)
    demonstrates this type of distance calculation, where the person needs to walk
    on the streets along the north-south and east-west axes. Manhattan distance and
    L¹-norms often come up in applications of Lasso and elastic net regression, where
    it is used to set beta coefficients to 0 if they are within a certain distance
    of the origin, thereby performing variable selection and creating a sparse model.
    This is useful in situations where the dimensionality of the independent variable
    set is high (such as in genomic data).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 该度量在处理计数数据或其他离散数据格式时非常有用，例如本节前面生成的示例数据集。[图 5-5](#figure5-5)演示了这种类型的距离计算，其中人物需要沿着南北轴和东西轴在人行道上行走。曼哈顿距离和L¹范数在Lasso回归和弹性网回归的应用中经常出现，用于将β系数设置为0，如果它们距离原点在某个范围内，从而执行变量选择并创建稀疏模型。这在自变量集维度较高（如基因组数据）的情况下非常有用。
- en: A generalization of both the L¹-norm and L²-norm is the *Minkowski distance*,
    which generalizes norms from L³-norms to L^∞-norms. The L^∞-norm is another special
    instance of norm-based distances, dubbed the *Chebyshev distance*. Mathematically,
    Chebyshev distance is the maximum distance between points along any axis. It is
    often used in problems involving planned movements of machinery or autonomous
    systems (like drones).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: L¹范数和L²范数的推广是*闵可夫斯基距离*，它将范数从L³范数推广到L^∞范数。L^∞范数是基于范数的距离的另一种特殊实例，称为*切比雪夫距离*。从数学角度来看，切比雪夫距离是沿任一轴之间的最大距离。它常用于涉及机械或自主系统（如无人机）计划运动的问题中。
- en: As the dimension of the norm increases, the Minkowski distance values typically
    decrease and stabilize. Thus, Minkowski distances with high-dimensional norms
    can act as distance smoothers that rein in strange or unusually large distance
    calculations found with the Manhattan or Euclidean distance calculations. Minkowski
    distance does impose a few conditions, including that the zero vector has a length
    of zero, that application of a positive scalar multiple to a vector does not change
    the vector’s direction, and that the shortest distance between two points is a
    straight line (known as the *triangle inequality condition*). In the `dist()`
    function of R, the dimension of the norm is given by the parameter `p`, with `p=1`
    corresponding to Manhattan distance, `p=2` corresponding to Euclidean distance,
    and so on.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 随着范数维度的增加，闵可夫斯基距离值通常会减少并趋于稳定。因此，具有高维范数的闵可夫斯基距离可以充当距离平滑器，抑制曼哈顿或欧几里得距离计算中发现的奇异或异常大的距离。闵可夫斯基距离确实有一些条件，包括零向量的长度为零，应用正标量倍数于向量时不会改变向量的方向，并且两点之间的最短距离是直线（称为*三角不等式条件*）。在R的`dist()`函数中，范数的维度由参数`p`给出，其中`p=1`对应曼哈顿距离，`p=2`对应欧几里得距离，依此类推。
- en: A special extension of Manhattan distance is the *Canberra distance*, which
    is a weighted sum of the L¹-norm. Technically, Canberra distance is computed by
    finding the absolute value of the distance between a pair of points divided that
    by the sum of the pair of points’ absolute values, which then is summed across
    point pairs. It can be a useful distance metric when dealing with outliers, intrusion
    detection, or mixed types of predictors (continuous and discrete measures). The
    example point in [Figure 5-7](#figure5-7) likely isn’t a statistical outlier,
    but it certainly lies in a different part of the data space than the other simulated
    points.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离的一个特殊扩展是*堪培拉距离*，它是L¹范数的加权和。从技术上讲，堪培拉距离是通过找到一对点之间的距离的绝对值，并将其除以这对点的绝对值之和，然后对点对求和来计算的。当处理离群值、入侵检测或混合类型预测变量（连续和离散测量）时，堪培拉距离可以是一个有用的度量。在[图5-7](#figure5-7)中的示例点可能不是统计学上的离群值，但它确实位于与其他模拟点不同的数据空间中。
- en: 'Let’s run these distances and compare the results on the dataset we simulated
    earlier in this section; add the following to the code in [Listing 5-1](#listing5-1):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行这些距离并将结果与本节前面模拟的数据集进行比较；将以下内容添加到[列表5-1](#listing5-1)中的代码：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This code computes distance metrics for Euclidean, Manhattan, Canberra, and
    Minkowski distances applied to our example dataset. Looking at Euclidean distance
    measurements between pairs of points in the simulated dataset, shown in [Table
    5-1](#table5-1), we see values with many decimal points for many pairs of points,
    owing to the square root involved in calculating the Euclidean distance.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码计算了应用于我们示例数据集的欧几里得距离、曼哈顿距离、堪培拉距离和闵可夫斯基距离的度量。通过查看模拟数据集中点对之间的欧几里得距离，如[表格5-1](#table5-1)所示，我们可以看到许多点对之间的值有很多小数位数，这是由于计算欧几里得距离时涉及的平方根。
- en: 'Table 5-1: Euclidean Distance Calculations Between Pairs of Points in [Listing
    5-2](#listing5-2)’s Matrix'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表格5-1：在[列表5-2](#listing5-2)的矩阵中，点对之间的欧几里得距离计算
- en: '| **Euclidean** | **1** | **2** | **3** | **4** | **5** |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| **欧几里得** | **1** | **2** | **3** | **4** | **5** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **1** | 0 | 2.44949 | 1.414214 | 2.236068 | 1.414214 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 0 | 2.44949 | 1.414214 | 2.236068 | 1.414214 |'
- en: '| **2** | 2.44949 | 0 | 1.414214 | 1 | 3.162278 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 2.44949 | 0 | 1.414214 | 1 | 3.162278 |'
- en: '| **3** | 1.414214 | 1.414214 | 0 | 1 | 2 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 1.414214 | 1.414214 | 0 | 1 | 2 |'
- en: '| **4** | 2.236068 | 1 | 1 | 0 | 3 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 2.236068 | 1 | 1 | 0 | 3 |'
- en: '| **5** | 1.414214 | 3.162278 | 2 | 3 | 0 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 1.414214 | 3.162278 | 2 | 3 | 0 |'
- en: Moving on to Manhattan distance ([Table 5-2](#table5-2)), the distances between
    pairs of points become whole numbers, as the calculation involves discrete steps
    along each axis separating the points.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 继续查看曼哈顿距离（[表格5-2](#table5-2)），点对之间的距离变为整数，因为计算过程中涉及沿每个轴的离散步长来分隔点。
- en: 'Table 5-2: Manhattan Distance Calculations Between Pairs of Points in [Listing
    5-2](#listing5-2)’s Matrix'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 表格5-2：在[列表5-2](#listing5-2)的矩阵中，点对之间的曼哈顿距离计算
- en: '| **Manhattan** | **1** | **2** | **3** | **4** | **5** |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| **曼哈顿** | **1** | **2** | **3** | **4** | **5** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **1** | 0 | 4 | 2 | 3 | 2 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 0 | 4 | 2 | 3 | 2 |'
- en: '| **2** | 4 | 0 | 2 | 1 | 4 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 4 | 0 | 2 | 1 | 4 |'
- en: '| **3** | 2 | 2 | 0 | 1 | 2 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 2 | 2 | 0 | 1 | 2 |'
- en: '| **4** | 3 | 1 | 1 | 0 | 3 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 3 | 1 | 1 | 0 | 3 |'
- en: '| **5** | 2 | 4 | 2 | 3 | 0 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 2 | 4 | 2 | 3 | 0 |'
- en: As expected, the Minkowski distance calculations match the Manhattan distance
    for `p=1` and Euclidean distance for `p=2`. In [Table 5-3](#table5-3), you can
    see Minkowski distances with `p=1`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，闵可夫斯基距离计算结果与`p=1`时的曼哈顿距离和`p=2`时的欧几里得距离一致。在[表格5-3](#table5-3)中，你可以看到`p=1`时的闵可夫斯基距离。
- en: 'Table 5-3: Minkowski Distance Calculations Between Pairs of Points in [Listing
    5-2](#listing5-2)’s Matrix'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表格5-3：在[列表5-2](#listing5-2)的矩阵中，点对之间的闵可夫斯基距离计算
- en: '| **Minkowski `p=1`** | **1** | **2** | **3** | **4** | **5** |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| **闵可夫斯基 `p=1`** | **1** | **2** | **3** | **4** | **5** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **1** | 0 | 4 | 2 | 3 | 2 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 0 | 4 | 2 | 3 | 2 |'
- en: '| **2** | 4 | 0 | 2 | 1 | 4 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 4 | 0 | 2 | 1 | 4 |'
- en: '| **3** | 2 | 2 | 0 | 1 | 2 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 2 | 2 | 0 | 1 | 2 |'
- en: '| **4** | 3 | 1 | 1 | 0 | 3 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 3 | 1 | 1 | 0 | 3 |'
- en: '| **5** | 2 | 4 | 2 | 3 | 0 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 2 | 4 | 2 | 3 | 0 |'
- en: The Canberra distance gives some similar and overlapping values with Manhattan
    distance. However, some distances are different (particularly pairs involving
    points 2 or 3), owing to the weighted parts of the distance calculation, as shown
    in [Table 5-4](#table5-4).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 堪培拉距离与曼哈顿距离给出了一些相似和重叠的值。然而，由于距离计算中的加权部分，某些距离是不同的（特别是涉及点 2 或 3 的对），如[表 5-4](#table5-4)所示。
- en: 'Table 5-4: Canberra Distance Calculations Between Pairs of Points in [Listing
    5-2](#listing5-2)’s Matrix'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-4：在[清单 5-2](#listing5-2)矩阵中的点对之间的堪培拉距离计算
- en: '| **Canberra** | **1** | **2** | **3** | **4** | **5** |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| **堪培拉** | **1** | **2** | **3** | **4** | **5** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **1** | 0 | 3 | 2 | 3 | 1.8 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 0 | 3 | 2 | 3 | 1.8 |'
- en: '| **2** | 3 | 0 | 3 | 3 | 3 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 3 | 0 | 3 | 3 | 3 |'
- en: '| **3** | 2 | 3 | 0 | 3 | 1.5 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 2 | 3 | 0 | 3 | 1.5 |'
- en: '| **4** | 3 | 3 | 3 | 0 | 3 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 3 | 3 | 3 | 0 | 3 |'
- en: '| **5** | 1.8 | 3 | 1.5 | 3 | 0 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 1.8 | 3 | 1.5 | 3 | 0 |'
- en: For some points in [Listing 5-2](#listing5-2)’s distance matrix calculations,
    these three distances give the same distance score for a pair of points (such
    as for points 4 and 5). However, some of the distances are quite different when
    we increase the value of `p` (such as points 1 and 2). If we’re using the distance
    metrics in a support vector machine classifier, we might end up with a very different
    line cutting our data into groups—or very different error rates.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在[清单 5-2](#listing5-2)的距离矩阵计算中，对于某些点，这三种距离为一对点（例如点 4 和 5）提供相同的距离评分。然而，当我们增加`p`值时，某些距离会发生显著变化（例如点
    1 和 2）。如果我们在支持向量机分类器中使用这些距离度量，我们可能会得到一个非常不同的线来划分数据，或者产生非常不同的错误率。
- en: 'There are other ways to modify or extend norm-based distances. One popular
    modification is like the Canberra distance: *Mahalanobis distance* applies a weighting
    scheme to Euclidean distance calculations before taking the square root of the
    result, such that Euclidean distance is weighted by the covariance matrix. If
    the covariance matrix is simply the identity matrix, Mahalanobis distance will
    collapse to Euclidean distance. If the covariance matrix is diagonal, the result
    is a standardized Euclidean distance. Thus, Mahalanobis distance provides a type
    of “centered” distance metric that can identify leverage points and outliers within
    a data sample. It’s often used in clustering and discriminant analyses, as outliers
    and leverage points can skew results.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他修改或扩展基于范数的距离的方法。一种常见的修改方式类似于堪培拉距离：*马哈拉诺比斯距离*在计算欧几里得距离之前，应用加权方案，然后再对结果取平方根，使得欧几里得距离通过协方差矩阵加权。如果协方差矩阵只是单位矩阵，马哈拉诺比斯距离将退化为欧几里得距离。如果协方差矩阵是对角矩阵，则结果是标准化的欧几里得距离。因此，马哈拉诺比斯距离提供了一种“中心化”的距离度量，可以识别数据样本中的杠杆点和异常值。它通常用于聚类和判别分析，因为异常值和杠杆点可能会扭曲结果。
- en: 'There’s a simple way to calculate Mahalanobis distance in R: the `mahalanobis()`
    function. Let’s add to our script again:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R 中计算马哈拉诺比斯距离有一个简单的方法：`mahalanobis()`函数。让我们再次添加到我们的脚本中：
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This code will calculate Mahalanobis distance with various centering strategies,
    yielding three different measures of leverage/weighted standard distance from
    a defined reference, detailed in [Table 5-5](#table5-5).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将计算使用不同中心化策略的马哈拉诺比斯距离，产生三个不同的杠杆/加权标准距离度量，参考[表 5-5](#table5-5)中的详细内容。
- en: 'Table 5-5: Mahalanobis Distance Results for the Individual Points from [Figure
    5-7](#figure5-7)’s Matrix'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-5：来自[图 5-7](#figure5-7)矩阵的个体点的马哈拉诺比斯距离结果
- en: '| **Mahalanobis** | **1** | **2** | **3** | **4** | **5** |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| **马哈拉诺比斯** | **1** | **2** | **3** | **4** | **5** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **Covariance only** | 6.857143 | 6.857143 | 0.857143 | 0 | 7.714286 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| **仅协方差** | 6.857143 | 6.857143 | 0.857143 | 0 | 7.714286 |'
- en: '| **Point 1** | 0 | 8 | 5.428571 | 6.857143 | 7.714286 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| **点 1** | 0 | 8 | 5.428571 | 6.857143 | 7.714286 |'
- en: '| **Column means** | 3.2 | 3.2 | 0.628571 | 2.057143 | 2.914286 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| **列的含义** | 3.2 | 3.2 | 0.628571 | 2.057143 | 2.914286 |'
- en: By using each point as a center, you can complete a full distance matrix similar
    to how the `dist()` function creates the distance matrix. You would simply loop
    through the individual points and append rows to a data frame.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将每个点作为中心，你可以完成一个类似于`dist()`函数创建的距离矩阵。你只需遍历各个点并将行附加到数据框中。
- en: A few interesting observations come out of the Mahalanobis distance calculations.
    When only covariance is used, the origin becomes the reference point for calculating
    distances, and point 4, which is located at the origin, has a Mahalanobis distance
    of 0\. However, when column means are used to center the data, this point jumps
    to a much farther away value. This suggests that point 4 is quite far away from
    the column means, though it is perfectly centered at the origin. Another interesting
    trend involves point 3, which is quite close to both the origin and the centered
    column means, which come out to (1.2, 0.2, 0.2) in this dataset. Point 3 is located
    at (1, 0, 0), which is both near the origin and near this centered column mean.
    The other points are relatively far from both the origin and the centered column
    means.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 从马哈拉诺比斯距离计算中可以得出一些有趣的观察结果。当只使用协方差时，原点成为计算距离的参考点，而位于原点的点4的马哈拉诺比斯距离为0。然而，当使用列均值来居中数据时，这个点的值跳跃到了一个更远的值。这表明，尽管点4在原点上完全居中，但它距离列均值相当远。另一个有趣的趋势是点3，它既靠近原点，也靠近居中的列均值，在这个数据集中居中的列均值为(1.2,
    0.2, 0.2)。点3位于(1, 0, 0)，既靠近原点，又靠近这个居中的列均值。其他点相对较远，无论是从原点还是从居中的列均值。
- en: 'We can add these column means to our plot of this data and visualize a bit
    of how Mahalanobis distance works by adding to our script again:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些列均值添加到数据的图中，并通过再次修改脚本来可视化马哈拉诺比斯距离是如何工作的：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code adds the column mean point to the original dataset to examine where
    the “middle” of the data should be located in the three-dimensional space; the
    code should yield a plot similar [Figure 5-8](#figure5-8).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将列均值点添加到原始数据集中，以检查数据在三维空间中应该位于哪里；该代码应生成一个类似于[图 5-8](#figure5-8)的图形。
- en: Examining [Figure 5-8](#figure5-8) and comparing it to [Figure 5-6](#figure5-6),
    we can see that a point has been placed off the axes that does seem to occupy
    a central location among the five points. Finding the central location of a dataset
    helps in several data science tasks, including finding stealth outliers (outliers
    without extreme values for any one variable but far from most points in the multivariate
    dataset) and calculating multivariate statistics. Some points are closer to this
    central location than others, as our Mahalanobis results suggest; these are points
    3 and 4 in our dataset, which are relatively close to the origin.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 检查[图 5-8](#figure5-8)并与[图 5-6](#figure5-6)进行比较，我们可以看到一个点被放置在轴外，似乎在五个点中占据了一个中心位置。找到数据集的中心位置有助于进行几个数据科学任务，包括发现隐性异常值（即没有任何一个变量极端值但在多变量数据集中远离大多数点的异常值）和计算多变量统计量。正如我们的马哈拉诺比斯结果所示，某些点离这个中心位置更近；这些点是我们数据集中的点3和点4，它们相对靠近原点。
- en: '![](image_fi/503083c05/f05008.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05008.png)'
- en: 'Figure 5-8: Mahalanobis distance with centering at column means for the individual
    points from [Listing 5-2](#listing5-2)’s matrix plotted with the column mean point
    shown off the axes'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-8：对于[Listing 5-2](#listing5-2)中矩阵中的各个点，通过列均值居中计算的马哈拉诺比斯距离图，其中列均值点显示在轴外
- en: These distance differences come up in many machine learning applications, and
    we’ll see how distance impacts machine learning performance and results when we
    apply these distances within a *k*-nearest neighbors problem later in this chapter.
    Performance can vary dramatically with a different choice of metric, and using
    the wrong metric can mislead model results and interpretation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些距离差异在许多机器学习应用中都会出现，我们将在本章稍后应用这些距离于*k*-最近邻问题时，看到距离如何影响机器学习的表现和结果。不同的度量选择会导致性能的剧烈变化，使用错误的度量可能会误导模型结果和解释。
- en: Comparing Diagrams, Shapes, and Probability Distributions
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较图形、形状和概率分布
- en: Norm-based distance metrics are not the only class of metrics possible in machine
    learning. As we saw in [Chapter 4](c04.xhtml), it’s possible to calculate distance
    between objects other than points, such as persistence diagrams. Roughly speaking,
    these types of metrics measure differences in probability distributions. We’ve
    already briefly used one of these, the Wasserstein distance, to compare persistence
    diagram distributions. Let’s take a closer look now.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 基于范数的距离度量并不是机器学习中唯一可能的度量类。如我们在[第 4 章](c04.xhtml)中所看到的，除了点之间的距离外，还可以计算其他对象之间的距离，如持久性图。粗略来说，这些度量衡量的是概率分布之间的差异。我们已经简要使用过其中的一种，即Wasserstein距离，用于比较持久性图分布。现在让我们更仔细地看看。
- en: Wasserstein Distance
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Wasserstein 距离
- en: In loose terms, *Wasserstein distance* compares the piles of probability weights
    stacked in two distributions. It’s often dubbed “earth-mover distance,” as the
    Wasserstein distance measures the cost and effort needed to move probability piles
    of one distribution to turn it into the comparison distribution. For more mathematically
    sophisticated readers, the *p*th Wasserstein distance can be calculated by taking
    the expected value of the joint distribution marginals to the *p*th power, finding
    the infimum over all join probability distributions of those random variables,
    and taking the *p*th root of the result. However, the details of this are beyond
    what is expected of readers, and we’ll stick with the intuition of earth-mover
    distance as we explore this metric. Let’s visualize two distributions of dirt
    piles to build some intuition behind this metric ([Figure 5-9](#figure5-9)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 粗略来说，*Wasserstein距离*比较了两种分布中堆积的概率权重堆。它通常被称为“地球搬运工距离”，因为Wasserstein距离衡量的是将一种分布的概率堆移到另一个分布所需的成本和努力。对于那些数学上更为精通的读者，*p*阶Wasserstein距离可以通过将联合分布的边际期望值提升到*p*次方，求出所有这些随机变量的联合概率分布的下确界，然后再对结果取*p*次方根来计算。然而，这些细节超出了本书的预期范围，我们将在探索这个度量时坚持“地球搬运工距离”的直观理解。让我们通过可视化两种泥土堆的分布来帮助理解这个度量（[图5-9](#figure5-9)）。
- en: '![](image_fi/503083c05/f05009.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05009.png)'
- en: 'Figure 5-9: Two distributions of dirt piles akin to the type of probability
    density functions that could be compared using Wasserstein distance metrics'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-9：两种泥土堆积物的分布，类似于可以使用Wasserstein距离度量进行比较的概率密度函数类型
- en: Pile 1 of [Figure 5-9](#figure5-9) has a large stack of dirt toward the left
    side that would need to be shoveled to the right piles if we were to transform
    Pile 1’s distribution of dirt to Pile 2’s. Our dirt mover will have to move quite
    a bit of dirt to transform Pile 1 into Pile 2\. However, if our Pile 2 had a distribution
    of dirt piles closer to that of Pile 1, as in [Figure 5-10](#figure5-10), the
    amount of work to turn Pile 1 into Pile 2 would be less for our dirt mover.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-9](#figure5-9)中的堆积物1在左侧有一大堆泥土，如果我们要将堆积物1的泥土分布转化为堆积物2的分布，那么这些泥土需要铲到右侧的堆积物中。我们的泥土搬运工将需要移动相当多的泥土才能将堆积物1变成堆积物2。然而，如果堆积物2的泥土分布更接近堆积物1，如[图5-10](#figure5-10)所示，那么我们的泥土搬运工所需的工作量就会减少。'
- en: '![](image_fi/503083c05/f05010.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05010.png)'
- en: 'Figure 5-10: Two distributions of dirt piles akin to the type of probability
    density functions that could be compared using Wasserstein distance metrics, which
    measure the amount of work needed to be done by our dirt mover to transform Pile
    1 into Pile 2'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-10：两种泥土堆积物的分布，类似于可以使用Wasserstein距离度量进行比较的概率密度函数类型，该度量衡量的是泥土搬运工将堆积物1转化为堆积物2所需的工作量
- en: To get from Distribution 1 to Distribution 2, you can think of someone shoveling
    the dirt. The amount of dirt shoveled corresponds to the Wasserstein distance.
    Probability density functions that are very similar will have a smaller Wasserstein
    distance; those that are very dissimilar will have a larger Wasserstein distance.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要从分布1到达分布2，你可以想象有人正在铲泥土。铲掉的泥土量对应着Wasserstein距离。非常相似的概率密度函数会有较小的Wasserstein距离；而那些非常不相似的概率密度函数则会有较大的Wasserstein距离。
- en: Thus, Wasserstein distance can be a nice metric to use in comparing probability
    distributions—comparing theoretical distributions to sample distributions to see
    if they match, comparing multiple sample distributions from the same or different
    populations to see if they match, or even understanding if it’s possible to use
    a simpler probability distribution in a machine learning function that approximates
    the underlying data such that calculations within a machine learning algorithm
    will be easier to compute.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Wasserstein距离是比较概率分布的一个很好的度量工具——可以用来比较理论分布与样本分布，看看它们是否匹配；也可以用来比较来自相同或不同群体的多个样本分布，看看它们是否匹配；甚至可以用来理解在机器学习函数中是否可能使用更简单的概率分布来近似底层数据，以便机器学习算法中的计算更容易进行。
- en: Although we won’t go into it here, some distributions—including count data,
    yes/no data, and even machine learning output structures like dendrograms—can
    be compared through other metrics. For now, let’s look at another way to compare
    probability distributions, this time with discrete distributions.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在这里不会深入探讨，但有些分布——包括计数数据、是/否数据，甚至是机器学习输出结构（如树状图）——可以通过其他度量进行比较。现在，让我们看看另一种比较概率分布的方法，这一次是对于离散分布。
- en: Entropy
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 熵
- en: One common class of distance metrics involves a property dubbed *entropy*. Information
    entropy, defined by the negative logarithm of the probability density function,
    measures the amount of nonrandomness at each point of the probability density
    function. By understanding how much information is contained in a distribution
    at each value, it’s possible to compare differences in information across distributions.
    This can be a handy tool for comparing complicated probability functions or output
    from machine learning algorithms, as well as deriving nonparametric statistical
    tests.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一类常见的距离度量涉及一个被称为 *熵* 的属性。信息熵由概率密度函数的负对数定义，衡量概率密度函数中每个点的非随机性程度。通过理解每个值中分布所包含的信息量，就可以比较不同分布之间的信息差异。这对于比较复杂的概率函数或机器学习算法的输出，以及推导非参数统计检验，可以是一个非常有用的工具。
- en: Binomial distributions come up often in data science. We might think that a
    random customer has no preference between two new interface designs (50 percent
    preferring A and 50 percent preferring B in an A/B test). We could estimate the
    chance that 10 or 20 or 10,000 customers prefer A over B and compare it to samples
    of actual customers providing us feedback. One assumption might be that we have
    different customer populations, including one that is very small. Of course, in
    practice, we don’t know the actual preference distributions of our customer populations
    and may not have enough data to compare the distributions mathematically via a
    proportions test. Leveraging a metric can help us derive a test.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 二项分布在数据科学中经常出现。我们可能认为一个随机顾客在两种新界面设计之间没有偏好（在 A/B 测试中，50% 的顾客偏好 A，50% 的顾客偏好 B）。我们可以估计
    10 个、20 个或 10,000 个顾客偏好 A 而非 B 的机会，并将其与提供反馈的实际顾客样本进行比较。一个假设可能是我们有不同的顾客群体，其中一个群体非常小。当然，在实际中，我们不知道顾客群体的实际偏好分布，并且可能没有足够的数据通过比例检验来数学地比较这些分布。利用某种度量可以帮助我们推导出检验。
- en: To understand this a bit more intuitively, let’s simulate two binomial probability
    distributions with the code in [Listing 5-3](#listing5-3).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更直观地理解这一点，让我们通过 [清单 5-3](#listing5-3) 中的代码来模拟两个二项概率分布。
- en: '[PRE5]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Listing 5-3: A script that simulates different binomial probability distributions'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 5-3：模拟不同二项概率分布的脚本
- en: '[Figure 5-11](#figure5-11) shows that these binomial distributions have very
    different density functions, with information stored in different parts of the
    distribution.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-11](#figure5-11) 显示了这两个二项分布具有非常不同的密度函数，信息存储在分布的不同部分。'
- en: '![](image_fi/503083c05/f05011.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05011.png)'
- en: 'Figure 5-11: Two binomial distributions’ density functions plotted for comparison'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-11：为比较绘制的两个二项分布的密度函数
- en: The black curve distribution, distribution *b***,** includes a wider spread
    of information over more values than distribution *a*, which concentrates its
    information nearer to zero (lighter gray curve). Entropy-based metrics can be
    used to quantify this difference in information storage between distributions*a*
    and *b*. R provides many tools and packages for quantifying and comparing information
    entropy. Let’s explore a bit further.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 黑色曲线分布，分布 *b*，**在比分布 *a*（灰色曲线）覆盖更多值的范围内包含了更广泛的信息。基于熵的度量可以用来量化分布 *a* 和 *b* 之间在信息存储上的差异。R
    提供了许多工具和包，用于量化和比较信息熵。让我们进一步探索。
- en: The philentropy package in R contains 46 different metrics for comparing probability
    distributions, including many metrics based on entropy. One of the more popular
    entropy metrics is the *Kullback–Leibler divergence*, which measures the relative
    entropy of two probability distributions. Technically speaking, the Kullback–Leibler
    divergence measures the expectation (sum for discrete distributions or integral
    for continuous distributions) of the logarithmic differences between two probability
    distributions. As such, it’s a measurement of information gain or loss. This allows
    us to convert information entropy into a distribution comparison tool, which is
    useful when we’re trying to compare differences between unknown or complicated
    probability distributions that might not be amenable to the usual statistical
    tools.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: R中的philentropy包包含46种不同的度量方法，用于比较概率分布，其中包括许多基于熵的度量方法。比较流行的熵度量之一是*Kullback–Leibler散度*，它度量两个概率分布的相对熵。从技术上讲，Kullback–Leibler散度度量的是两个概率分布之间对数差异的期望（离散分布为求和，连续分布为积分）。因此，它是信息增益或损失的度量。这使我们能够将信息熵转换为一种分布比较工具，当我们试图比较那些可能不适合常规统计工具的未知或复杂的概率分布时，这个工具非常有用。
- en: 'Let’s develop our intuition by returning to our two binomial distributions,
    *a*and *b*. We’ll calculate the Kullback–Leibler divergence between the two distributions
    by adding the following to [Listing 5-3](#listing5-3):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回到我们的两个二项分布*a*和*b*，来培养直觉。我们将通过在[列表 5-3](#listing5-3)中添加以下内容来计算这两个分布之间的Kullback–Leibler散度：
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This code calculates the Kullback–Leibler divergence for the two binomial distribution
    samples generated in [Listing 5-3](#listing5-3). In this set of simulated data,
    the Kullback–Leibler divergence is 398.5428; another simulation of these distributions
    might yield a different divergence measurement. However, using nonparametric tests,
    we can compare this divergence value with the random error component of one of
    our distributions to see whether there is a statistically significant difference
    of entropy between distributions *a*and *b*. We can add to our script to create
    a nonparametric statistical test using entropy differences:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码计算了在[列表 5-3](#listing5-3)中生成的两个二项分布样本的Kullback–Leibler散度。在这组模拟数据中，Kullback–Leibler散度为398.5428；另一组这些分布的模拟可能会得出不同的散度值。然而，通过使用非参数检验，我们可以将这个散度值与我们分布之一的随机误差成分进行比较，从而判断分布*a*和*b*之间的熵是否存在统计学上的显著差异。我们可以在脚本中添加内容，利用熵差异创建非参数统计检验：
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The confidence intervals from this test suggest confidence intervals of 1,427–1,475,
    which suggests that our distributions are significantly different. This is expected,
    as distributions *a*and *b*have very different values and ranges. Plotting the
    last distribution simulated ([Figure 5-12](#figure5-12)) shows that the new distribution
    is a much better match to *a*than *b* is.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本次测试的置信区间表明置信区间为1,427–1,475，这意味着我们的分布存在显著差异。这是可以预期的，因为分布*a*和*b*的值和范围非常不同。绘制最后一个模拟的分布（[图
    5-12](#figure5-12)）表明，新分布与*a*的匹配度远高于与*b*的匹配度。
- en: '![](image_fi/503083c05/f05012.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05012.png)'
- en: 'Figure 5-12: Three binomial distributions’ density functions plotted for comparison,
    with two coming from samples of the same distribution'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-12：三个二项分布的密度函数图，用于比较，其中两个来自相同分布样本的样本
- en: Using the Kullback–Leibler divergence, we’ve determined that *a*and *b*are different
    populations statistically. If we saw a confidence range including 104.2, we’d
    conclude that *a*and *b*likely come from the same population distribution. Statistical
    tests such as proportions tests exist to compare binomial distributions in practice,
    but some discrete distributions or sample sizes don’t have easy statistical comparisons
    (such as comparisons of predicted class distributions coming out of two convolutional
    neural network classifiers).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kullback–Leibler散度，我们已经确定*a*和*b*是统计学上不同的种群。如果我们看到包含104.2的置信区间，我们会得出结论：*a*和*b*可能来自同一个总体分布。实际中，像比例检验这样的统计检验存在用于比较二项分布，但有些离散分布或样本大小并没有容易的统计比较方法（例如，来自两个卷积神经网络分类器的预测类别分布的比较）。
- en: Comparison of Shapes
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 形状比较
- en: As we saw in [Chapter 4](c04.xhtml), sets of points and shapes, such as persistence
    diagrams, can be important data structures, and these objects can be measured
    and compared as well. The next three measures will deal with this situation in
    more depth. Let’s start with an example of two circles with differing radii, as
    shown in [Figure 5-13](#figure5-13).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第 4 章](c04.xhtml)中看到的，点集和形状（例如持久性图）可以是重要的数据结构，这些对象也可以被衡量和比较。接下来的三个度量将更深入地讨论这种情况。我们从两个半径不同的圆的例子开始，如[图
    5-13](#figure5-13)所示。
- en: '![](image_fi/503083c05/f05013r.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05013r.png)'
- en: 'Figure 5-13: Two circles of differing radii'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-13：两个半径不同的圆
- en: Now, let’s imagine these two circles are paths in a park, and someone is walking
    their dog on a leash, with the dog following the outer path and the owner following
    the inner path. [Figure 5-14](#figure5-14) shows a visual.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设这两个圆是公园中的小径，有人正在遛狗，狗沿外环行走，主人沿内环行走。[图 5-14](#figure5-14)显示了这一视觉效果。
- en: '![](image_fi/503083c05/f05014r.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05014r.png)'
- en: 'Figure 5-14: A dog and owner connected by a leash walking on different paths
    at a park'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-14：一只狗和主人通过牵引绳连接，在公园的不同小径上行走
- en: At some points, the owner and her dog are close together, and a small leash
    suffices to connect them. However, as they move counterclockwise, the distance
    between the owner and their dog increases, necessitating more leash to connect
    them; you can see this in [Figure 5-15](#figure5-15).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些时刻，主人和她的狗靠得很近，短牵引绳就足够将它们连接起来。然而，随着它们逆时针移动，主人和狗之间的距离增加，需要更长的牵引绳才能将它们连接起来；你可以在[图
    5-15](#figure5-15)中看到这一点。
- en: '![](image_fi/503083c05/f05015r.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05015r.png)'
- en: 'Figure 5-15: A dog and owner connected by a longer leash walking on different
    paths at a park'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-15：一只狗和主人通过更长的牵引绳连接，在公园的不同小径上行走
- en: One historically important metric that compares distances between points on
    different shapes is *Fréchet distance*. The version of Fréchet distance that we’ll
    consider here applies to discrete measurements (usually taken to be polygons).
    A grid graph is constructed from the polygons, and minmax paths are computed to
    find the maximum distance that two paths may be from each other. Many assumptions
    can be placed on the paths themselves and the synchronization of movement along
    those paths; the strictest requirements give rise to what’s called a *homotopic*
    Fréchet distance, which has applications in many robotics problems. We’ll return
    to homotopy applications in [Chapter 8](c08.xhtml).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一个历史上重要的度量是*弗雷歇距离*，它用于比较不同形状上点之间的距离。我们在这里考虑的弗雷歇距离版本适用于离散测量（通常被认为是多边形）。从多边形构建网格图，并计算最小最大路径，找到两个路径之间可能的最大距离。可以对路径本身及其沿路径的同步性做出许多假设；最严格的要求产生了所谓的*同伦*弗雷歇距离，这在许多机器人学问题中有应用。我们将在[第
    8 章](c08.xhtml)中回到同伦应用的问题。
- en: For now, in more lay terms, Fréchet distance is known as the dog-walking distance,
    and it has many uses in analytics. It can be used to measure not only the maximum
    distance between points on curves or shapes but the total distance between points
    on shapes or curves. Many R packages include functions to calculate one of the
    extant versions of Fréchet distance, including the TSdist time-series package,
    which is used in the following example. In this package, two time series are generated
    from ARMA(3, 2) distributions, with Series 3 containing 100 time points and Series
    4 containing 120 time points. Time series are important in tracking disease progression
    in groups of patients, tracking stock market changes over time, and tracking buyer
    behavior over time.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，用更通俗的说法，弗雷歇距离被称为“遛狗距离”，它在分析中有许多用途。它不仅可以用于测量曲线或形状上点之间的最大距离，还可以测量形状或曲线上点之间的总距离。许多
    R 包包括计算现有版本的弗雷歇距离的函数，包括以下示例中使用的 TSdist 时间序列包。在此包中，两个时间序列是从 ARMA(3, 2) 分布生成的，Series
    3 包含 100 个时间点，Series 4 包含 120 个时间点。时间序列在跟踪病人群体中的疾病进展、股市变化以及买方行为等方面非常重要。
- en: Let’s load these package-generated time series and plot them to visualize potential
    differences using the code in [Listing 5-4](#listing5-4).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载这些包生成的时间序列并绘制它们，使用[列表 5-4](#listing5-4)中的代码来可视化潜在的差异。
- en: '[PRE8]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Listing 5-4: A script that loads and examines two time-series datasets'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5-4：加载并检查两个时间序列数据集的脚本
- en: '[Figure 5-16](#figure5-16) shows two time series with distinct highs and lows
    over time.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-16](#figure5-16)显示了两条具有明显高低起伏的时间序列。'
- en: '![](image_fi/503083c05/f05016.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05016.png)'
- en: 'Figure 5-16: Plots of the two example time series'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-16：两个示例时间序列的绘图
- en: 'Notice the overlap between the time series isn’t perfect, and we’d expect our
    comparisons to show some differences between these time series. With Fréchet distance,
    it’s possible to measure both the maximum/minimum deviation (maximum/minimum leash
    length) between the time series and the sum of deviations across the full comparison
    set. We’ll examine both of these for Series 3 and Series 4 by adding the following
    code to our script:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，时间序列之间的重叠并不完全，我们预期我们的比较会显示这些时间序列之间的一些差异。使用Fréchet距离，我们可以衡量时间序列之间的最大/最小偏差（最大/最小链条长度）以及整个比较集的偏差总和。我们将通过向脚本中添加以下代码来检查第3序列和第4序列的这两个值：
- en: '[PRE9]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This code calculates the minimum, maximum, and sum of Fréchet distances between
    the time series, which should yield a value of 402.0 for the sum of distances
    between the time-series curves, a value of 13.7 for the maximum distance between
    points on the time series curves, and a value of 0.03 for the minimum distance
    between points on the time series curves. This suggests that the time series have
    approximately the same values at some comparison points and values that are very
    different at other points. The sum of distances between the time-series curves
    will converge to the integral taken with continuous time across the series; this
    calculation can give a good tool for calculating areas between functions using
    discrete and fairly quick approximations.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码计算了时间序列之间Fréchet距离的最小值、最大值和总和，应该会得出时间序列曲线之间距离总和为402.0，时间序列曲线之间点的最大距离为13.7，时间序列曲线之间点的最小距离为0.03。这表明，时间序列在某些比较点的值大致相同，而在其他点的值则有很大差异。时间序列曲线之间的距离总和将趋近于对时间序列进行连续时间积分的结果；这一计算可以成为计算函数之间面积的良好工具，使用离散且相对快速的近似方法。
- en: There are other ways to compare shapes besides Fréchet distance, though, and
    these are sometimes preferable. Let’s return to our two circles again and move
    them so that they are intersecting, such as in [Figure 5-17](#figure5-17).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，除了Fréchet距离，还有其他方法可以用来比较形状，这些方法有时是更可取的。让我们再次回到我们的两个圆，并将它们移动使它们相交，如[图5-17](#figure5-17)所示。
- en: '![](image_fi/503083c05/f05017r.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05017r.png)'
- en: 'Figure 5-17: Plot of two intersecting circles'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-17：两个相交圆的绘图
- en: We can think of points on each circle, much like we did to measure Fréchet distance.
    Let’s consider a point on circle B and its closest point on circle A (shown in
    [Figure 5-17](#figure5-17)). Each point on circle B will have a closest point
    on circle A, and these form a collection of closest points. The points chosen
    in [Figure 5-17](#figure5-17) are a special set of points. They are the farthest
    apart of any points in our set of closest points. This means that the maximum
    we’d have to travel to hop from one circle to the other occurs with those two
    points. This distance is called the *Hausdorff distance*, and it is found in a
    lot of applications in early computer vision and image matching tasks. These days,
    it is mainly used for sequence matching, graph matching, and other discrete object
    matching tasks.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将每个圆上的点想象成我们测量Fréchet距离时所做的那样。我们考虑圆B上的一个点以及它在圆A上的最近点（如[图5-17](#figure5-17)所示）。圆B上的每个点都会有一个在圆A上的最近点，这些点构成了一个最近点的集合。[图5-17](#figure5-17)中选择的点是一个特殊的点集。它们是我们最近点集合中最远的两个点。这意味着，从一个圆跳到另一个圆所需的最大距离就出现在这两个点之间。这个距离被称为*Hausdorff距离*，在早期的计算机视觉和图像匹配任务中有着广泛的应用。如今，它主要用于序列匹配、图形匹配以及其他离散对象匹配任务。
- en: However, one of the limitations of Hausdorff distance is that the sets being
    compared must exist in the same metric spaces. Thus, while we can compare points
    on circles, we cannot directly compare points on a circle to those on a sphere
    with Hausdorff distance or points on a Euclidean plane to points on a positively
    curved sphere. The two objects being compared must be in the same metric space.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Hausdorff距离的一个局限性是，所比较的集合必须存在于相同的度量空间中。因此，虽然我们可以比较圆上的点，但不能直接用Hausdorff距离比较圆上的点和球面上的点，或者欧几里得平面上的点和正曲率球面上的点。被比较的两个对象必须处于相同的度量空间中。
- en: Fortunately, a solution to this conundrum exists. We can simply measure the
    farthest shortest distance from two metric spaces when those metric spaces are
    mapped to a single metric space while preserving the original distances between
    points within each space (called an *isometric embedding*). So, we could project
    the sphere and its points to tangent space to compare with other Euclidean spaces.
    Or we could embed two objects in a higher-dimensional space similar to what is
    done in kernel applications. This extension of Hausdorff distance is dubbed *Gromov–Hausdorff
    distance*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这个难题有一个解决方案。我们可以简单地在将这两个度量空间映射到单一度量空间时，测量它们之间最远的最短距离，同时保持每个空间内点之间的原始距离（称为*等距嵌入*）。因此，我们可以将球体及其点投影到切平面上，以与其他欧几里得空间进行比较。或者，我们可以将两个物体嵌入到一个更高维的空间中，类似于内核应用中所做的那样。这种Hausdorff距离的扩展被称为*Gromov–Hausdorff距离*。
- en: Let’s build some intuition around this metric. Say we have a triangle and a
    tetrahedron, as in [Figure 5-18](#figure5-18).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们围绕这个度量建立一些直觉。假设我们有一个三角形和一个四面体，如图[5-18](#figure5-18)所示。
- en: '![](image_fi/503083c05/f05018r.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05018r.png)'
- en: 'Figure 5-18: A triangle and tetrahedron, which exist in different-dimensional
    Euclidean spaces'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-18：一个三角形和四面体，它们存在于不同维度的欧几里得空间中
- en: One solution to this problem depicted in [Figure 5-18](#figure5-18) is to simply
    bring the triangle into three-dimensional Euclidean space and calculate the distances
    between the objects in three-dimensional Euclidean space. Perhaps part of the
    triangle overlaps with the tetrahedron when it is embedded in three-dimensional
    space, as shown in [Figure 5-19](#figure5-19).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-18中描述的这个问题的一个解决方案是将三角形简单地带入三维欧几里得空间，并计算三维欧几里得空间中物体之间的距离。也许三角形的一部分在嵌入三维空间时与四面体重叠，如图[5-19](#figure5-19)所示。
- en: '![](image_fi/503083c05/f05019r.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05019r.png)'
- en: 'Figure 5-19: A triangle and tetrahedron, both mapped into three-dimensional
    Euclidean space'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-19：一个三角形和四面体，它们都被映射到三维欧几里得空间中
- en: It’s now possible for us to compute the farthest point sets of the closest ones
    for these two objects, likely occurring at one of the far tips of the triangle
    or tetrahedron in this example.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算这两个物体最远点集合与最近点集合的距离，这些点可能出现在三角形或四面体的某个尖端。
- en: R has a nice package for computing Gromov–Hausdorff distances (gromovlab), so
    we can easily implement this distance metric in R. Let’s first simulate a small
    sample from a two-dimensional disc with the code in [Listing 5-5](#listing5-5).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: R有一个很好的包用于计算Gromov–Hausdorff距离（gromovlab），因此我们可以轻松地在R中实现这个距离度量。首先，让我们使用[清单5-5](#listing5-5)中的代码模拟一个来自二维圆盘的小样本。
- en: '[PRE10]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Listing 5-5: A script that samples from a two-dimensional disc to examine Gromov–Hausdorff
    distance'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 清单5-5：从二维圆盘中采样的脚本，用于检查Gromov–Hausdorff距离
- en: This should give a rough disc shape when plotted; take a look at [Figure 5-20](#figure5-20).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制时，这应该会呈现出一个大致的圆盘形状；请查看图[5-20](#figure5-20)。
- en: '![](image_fi/503083c05/f05020.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05020.png)'
- en: 'Figure 5-20: A sample taken from a two-dimensional disc'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-20：从二维圆盘中采样的样本
- en: 'Now, let’s add to [Listing 5-5](#listing5-5) to simulate a sample of the same
    size from the same uniform distribution that was used to generate our two-dimensional
    disc sample:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在[清单5-5](#listing5-5)的基础上添加代码，从同样的均匀分布中模拟一个相同大小的样本，该分布用于生成我们的二维圆盘样本：
- en: '[PRE11]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The code we’ve added samples one of the line segments to give a one-dimensional
    space. This gives us two Euclidean spaces of differing dimension. Now, we can
    compute the distances between points in each sample’s native space (two-dimensional
    Euclidean space for the disc, one-dimensional Euclidean space for the line). From
    there, we can compute the Gromov–Hausdorff distance between our samples by adding
    the following code:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加的代码从其中一个线段采样，以给出一个一维空间。这为我们提供了两个维度不同的欧几里得空间。现在，我们可以计算每个样本的原生空间中的点之间的距离（对于圆盘是二维欧几里得空间，对于线段是一维欧几里得空间）。从这里，我们可以通过添加以下代码来计算我们的样本之间的Gromov–Hausdorff距离：
- en: '[PRE12]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This code allows us to compare distance matrices between points in the samples.
    For this random sample, the Gromov–Hausdorff distance is 5.8\. We could simulate
    a nonparametric test based on our metric as we did in [Listing 5-3](#listing5-3)
    to help us determine if the embeddings of our disc and our line are the same statistically.
    Changing the metric parameters may change the significant differences between
    embeddings or the quality of an embedding, as we saw earlier in this chapter when
    we compared Canberra, Manhattan, and Euclidean distances. Interested readers are
    encouraged to play around with the embedding parameters, set up their own nonparametric
    tests, and see how the results vary for Gromov–Hausdorff distances for our disc
    and line sample.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: The `lp` parameter allows one to use the norm-based metrics examined earlier
    in this chapter. For this particular comparison, we’ve used the Euclidean norm,
    as both samples lie in Euclidean spaces and the distance matrices ingested are
    defined by the Euclidean norm. Other norms, such as the Manhattan or Chebyshev,
    are possible and perhaps preferable for other problems, and the package is equipped
    to handle graphs and trees, as well as distance matrices. One thing to note about
    this particular package is that the algorithm searches through all possible isometric
    embeddings, so the compute time and memory needed may be large for some problems.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '*K*-Nearest Neighbors with Metric Geometry'
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Metric geometry shows up in many algorithms, including *k*-nearest neighbor
    (*k*-NN) analyses, which classify observations based on the classifications of
    objects near them. One way to understand this method is to consider a high school
    cafeteria with different cliques of students, as shown in [Figure 5-21](#figure5-21).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05021.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-21: A high school cafeteria with three distinct student cliques'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'In the cafeteria shown in [Figure 5-21](#figure5-21), three student cliques
    exist: a dark gray clique, a gray clique, and a light gray clique. Students tend
    to stay near their group of friends, as exhibited by students A, B, and C. These
    students are surrounded by their friends, and classifying them using an arbitrary
    number of students standing nearest them according to a distance metric (like
    Euclidean distance or number of floor tiles between students) would give a pretty
    accurate classification into student cliques.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: However, there are a few students, such as students D and E, who are located
    near other cliques or among all three cliques. Student E might be part of the
    popular clique (bottom center) and also part of the varsity athlete clique (top
    left), and student D might be popular, a varsity athlete, and a math team member
    (top right). Depending on how many students located near students D and E are
    considered in classifying them into a clique, they may belong to their main clique
    or be incorrectly reassigned to a new clique, of which they may fit but not consider
    their main clique. For instance, the closest 10 students may assign student E
    correctly to the popular group, while the closest 2 students would not.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有一些学生，比如 D 和 E，位于其他群体附近，或者位于所有三个群体之间。学生 E 可能是流行群体的一员（位于底部中央），同时也是大学运动员群体的一员（位于顶部左侧）；而学生
    D 可能是流行的学生、大学运动员以及数学队成员（位于顶部右侧）。根据在对学生 D 和 E 进行分类时，考虑的离他们最近的学生数量，他们可能会被分配到他们的主群体，也可能会被错误地重新分配到一个新的群体，尽管他们可能适合该群体，但并不认为这是他们的主群体。例如，最近的
    10 个学生可能会正确地将学生 E 分配到流行群体，而最近的 2 个学生则无法做到这一点。
- en: Thus, *k*-NN methods rely on both a neighborhood size (in this instance, the
    number of students nearest the student of interest) and a distance metric defining
    which students are closest to the student of interest. Let’s look at little more
    closely at how distance metric can impact *k*-NN classification accuracy with
    five nearest neighbors in a simulated dataset with three variables impacting classification
    and three noise variables, given in [Listing 5-6](#listing5-6), which uses the
    knnGarden package (and includes many of the distances covered in the simulated
    data analyzed with norm-based distance metrics earlier in this chapter). You’ll
    first need to download the package ([https://cran.r-project.org/web/packages/knnGarden/index.html](https://cran.r-project.org/web/packages/knnGarden/index.html))
    and install it locally.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*k*-NN 方法依赖于邻域大小（在本例中是距离目标学生最近的学生数量）和定义哪些学生最接近目标学生的距离度量方法。让我们更仔细地看看距离度量方法如何影响
    *k*-NN 分类准确性。在一个模拟数据集中，假设有三个变量影响分类，另外还有三个噪声变量，数据来自 [清单 5-6](#listing5-6)，并使用了
    knnGarden 包（该包包括了前面章节中使用基于规范的距离度量分析的许多距离）。你首先需要下载该包（[https://cran.r-project.org/web/packages/knnGarden/index.html](https://cran.r-project.org/web/packages/knnGarden/index.html)）并在本地安装。
- en: '[PRE13]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Listing 5-6: A script that generates and classifies a sample through *k*-NN
    classification with varying distance metric and five neighbors'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 5-6：一个生成并通过 *k*-NN 分类进行分类的脚本，使用不同的距离度量方法和五个邻居
- en: '[Listing 5-6](#listing5-6) creates a sample and then runs the *k*-NN algorithm
    to classify points based on different distance metrics, including Manhattan, Euclidean,
    and Canberra distances. In this particular simulation, all of our distances yield
    similar accuracies (Euclidean distance of 81 percent, Manhattan distance of 81
    percent, and Canberra distance of 82 percent). We can consider a larger neighborhood
    by modifying [Listing 5-6](#listing5-6) to include 20 nearest neighbors.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 5-6](#listing5-6) 创建一个示例，并运行 *k*-NN 算法，基于不同的距离度量方法（包括曼哈顿距离、欧几里得距离和堪培拉距离）对数据点进行分类。在这个特定的模拟中，所有的距离方法得到了相似的准确度（欧几里得距离
    81%、曼哈顿距离 81% 和堪培拉距离 82%）。我们可以通过修改 [清单 5-6](#listing5-6)，将最近邻数量增加到 20，从而考虑更大的邻域。'
- en: '[PRE14]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This script modifies the functions that calculate the *k*-NN model, with the
    changes marked in bold; we changed the parameter to `K=20`. With this particular
    simulated dataset, there are dramatic differences in classification accuracy when
    20 nearest neighbors are considered. Euclidean and Manhattan distances give a
    slightly worse accuracy of 78.5 percent, and Canberra distance gives a much worse
    accuracy of 57 percent. Neighborhood size matters quite a bit in accuracy for
    Canberra distance, but it plays a lesser role for Euclidean and Manhattan distances.
    Generally speaking, using larger numbers of nearest neighbors smooths the data,
    similarly to how a weighted average might. These results suggest that, for our
    Canberra distance, adding a larger number of nearest neighbors might be smoothing
    the data too much. However, our Manhattan and Euclidean distance runs don’t show
    this smoothing effect and retain their original high performance. As our example
    shows, the choice of distance metric can matter a lot in algorithm performance—or
    it can matter little. Distance metrics thus function like other parameter choices
    in algorithm design.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本修改了计算 *k*-NN 模型的函数，修改部分已用粗体标出；我们将参数更改为 `K=20`。对于这个特定的模拟数据集，当考虑20个最近邻时，分类准确率存在显著差异。欧几里得距离和曼哈顿距离的准确率略低，为78.5%，而堪培拉距离则给出了更差的准确率，只有57%。邻域大小对堪培拉距离的准确率有较大影响，但对欧几里得距离和曼哈顿距离的影响较小。一般来说，使用更多的最近邻有助于平滑数据，类似于加权平均的效果。结果表明，对于我们的堪培拉距离，增加更多的最近邻可能会使数据过度平滑。然而，曼哈顿距离和欧几里得距离的运行结果并没有表现出这种平滑效应，仍然保持其原有的高性能。正如我们的例子所示，距离度量的选择在算法性能中可能起着至关重要的作用，也可能几乎没有影响。因此，距离度量在算法设计中起着与其他参数选择类似的作用。
- en: '*k*-NN models are among the most closely tied to metric geometry and neighborhoods,
    though many other methods rely on distance metrics or neighborhood size. There
    are many recent papers that suggest a multiscale approach to algorithm neighborhood
    definition can improve algorithm performance, including applications in *k*-NN
    regression, deep learning image classification, and persistent graph and simplex
    algorithms (including persistent homology), and this nascent field has grown in
    recent years.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-NN 模型是与度量几何和邻域关系最为紧密的模型之一，尽管许多其他方法也依赖于距离度量或邻域大小。近期有许多论文提出，采用多尺度方法来定义算法邻域能够提升算法性能，包括在
    *k*-NN 回归、深度学习图像分类、持久图和单纯形算法（包括持久同调）中的应用，且这一新兴领域近年来不断发展壮大。'
- en: One branch of machine learning where the choice of distance metric matters a
    lot is in dimensionality reduction, where we’re mapping a high-dimensional dataset
    to a lower-dimensional space. For instance, imagine we have a genomic dataset
    for a group of patients including 300 gene loci of interest. That’s a bit too
    much to visualize for a stakeholder on a PowerPoint slide. However, if we find
    a good mapping to two-dimensional space, we can add a scatterplot of our data
    to the slide deck in a way that is much easier for humans to process.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多机器学习领域中，距离度量的选择尤为重要，尤其是在降维任务中，我们将一个高维数据集映射到低维空间。例如，假设我们有一个基因组数据集，包含一组患者的300个感兴趣的基因位点。这对于在
    PowerPoint 幻灯片中展示给相关人员来说，显得有些过于复杂。然而，如果我们能够找到一种好的映射到二维空间的方式，就可以在幻灯片中加入一个数据的散点图，使得人们更容易理解。
- en: Manifold Learning
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流形学习
- en: Many dimensionality reduction algorithms also involve distance metrics and *k*-NN
    calculations. One of the most common dimensionality reduction algorithms, *principal
    component analysis (PCA)*, helps wrangle high-dimensional data into lower-dimensional
    spaces using a linear mapping between the original high-dimensional space and
    a lower-dimensional target space. Essentially, PCA finds the ideal set of linear
    bases to account for the most variance (packing in most of the relevant information
    related to our data) with the fewest linear bases possible; this allows us to
    drop many of the data space’s bases that don’t contain much relevant information.
    This helps us visualize data that lives in more than three dimensions; it also
    decorrelates predictors being fed into a model.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 许多降维算法也涉及距离度量和 *k*-NN 计算。最常见的降维算法之一是 *主成分分析（PCA）*，它通过在原始高维空间和低维目标空间之间的线性映射，将高维数据整合到低维空间中。本质上，PCA
    寻找一组理想的线性基底，以便在保持最多方差的同时使用尽可能少的线性基底（将与数据相关的大部分信息压缩进来）；这样我们就能丢弃那些包含较少相关信息的数据空间基底。这有助于我们可视化存在三维以上的数据，也有助于去相关化输入到模型中的预测因子。
- en: However, as noted, PCA assumes that data lives in a geometrically flat space
    and is mapped to a lower-dimensional flat space. As we’ve seen, this isn’t always
    the case, and Euclidean metrics can give different distance results than other
    distance metrics. Recently, many attempts to relax different assumptions and generalize
    dimensionality reduction to manifolds have provided a new class of dimensionality
    reduction techniques, called *manifold learning*. Manifold learning allows for
    mappings to lower-dimensional spaces that might be curved and generalizations
    of PCA to include metrics other than Euclidean distance. A *manifold* is a space
    that is locally Euclidean, with Euclidean space being one example of a manifold,
    so some people refer to *manifold learning* as an umbrella to this more general
    framework.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如前所述，PCA 假设数据位于几何平坦的空间中，并映射到一个低维的平坦空间。正如我们所看到的，这并不总是如此，欧几里得度量可能会给出与其他距离度量不同的距离结果。近年来，许多放宽不同假设并将降维推广到流形的尝试提供了一类新的降维技术，称为*流形学习*。流形学习允许映射到可能弯曲的低维空间，并且将
    PCA 推广到包括非欧几里得距离的度量。*流形*是一个局部欧几里得空间，欧几里得空间就是流形的一种示例，因此有些人将*流形学习*视为这个更一般框架的总称。
- en: Using Multidimensional Scaling
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用多维尺度法
- en: One of the older manifold learning algorithms is *multidimensional scaling*
    *(MDS)*. MDS considers embeddings of points into a Euclidean space such that distances
    between points, which can be Euclidean distances, are preserved as best as possible;
    this is done through the minimization of a user-defined cost function. Defining
    distances and cost functions via Euclidean distance yields the same results as
    PCA. However, there is no need to limit oneself to Euclidean distance with MDS,
    and many other metrics might be more suitable for a given problem. Let’s explore
    this further with a small dataset and different distance matrices as input to
    our MDS algorithm; take a look at the code in [Listing 5-7](#listing5-7).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种较老的流形学习算法是*多维尺度法*（*MDS*）。MDS 将点嵌入欧几里得空间，旨在尽可能保持点之间的距离（可以是欧几里得距离）。这是通过最小化用户定义的代价函数来实现的。通过欧几里得距离定义距离和代价函数，得到的结果与
    PCA 相同。然而，MDS 不必局限于欧几里得距离，许多其他度量可能更适合特定问题。让我们通过一个小数据集和不同的距离矩阵作为 MDS 算法的输入，进一步探讨这个问题；请查看[清单
    5-7](#listing5-7)中的代码。
- en: '[PRE15]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Listing 5-7: A script that generates an example dataset and calculates distance
    matrices from the dataset'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 5-7：生成示例数据集并从数据集中计算距离矩阵的脚本
- en: Now that we have generated some data and have calculated three different distance
    metrics, let’s see how the choice of distance metric impacts MDS embeddings. Let’s
    compute the MDS embeddings and plot the results by adding to [Listing 5-7](#listing5-7).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经生成了一些数据，并计算了三种不同的距离度量，让我们看看距离度量的选择如何影响 MDS 嵌入。我们来计算 MDS 嵌入并通过添加到[清单 5-7](#listing5-7)来绘制结果。
- en: '[PRE16]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Our addition to [Listing 5-7](#listing5-7) should give plots that look different
    from each other. In this example, the plots (shown in [Figure 5-22](#figure5-22))
    do vary dramatically depending on the metric used, suggesting that different distance
    metrics result in embeddings to different spaces.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对[清单 5-7](#listing5-7)的补充应会产生不同的绘图结果。在这个例子中，绘图（如[图 5-22](#figure5-22)所示）确实会根据使用的度量产生显著不同的结果，这表明不同的距离度量会导致嵌入到不同的空间。
- en: '![](image_fi/503083c05/f05022.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05022.png)'
- en: 'Figure 5-22: A side-by-side view of MDS results, which vary by distance metric
    chosen'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-22：MDS 结果并排显示，结果随选择的距离度量而变化
- en: The plot results in [Figure 5-22](#figure5-22) suggest that Minkowski distance
    yields quite different results than Euclidean or Manhattan distances; many points
    are bunched together in the Minkowski-type MDS result, which suggests it may not
    distinguish between pairs of points as well as the other metrics. However, the
    differences between Euclidean and Manhattan distance MDS results are less dramatic,
    with points spread out a lot more than in the case of our Minkowski distance.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-22](#figure5-22)中的绘图结果表明，闵可夫斯基距离与欧几里得距离或曼哈顿距离的结果有很大不同；在闵可夫斯基类型的 MDS 结果中，许多点聚集在一起，这表明它可能无法像其他度量那样区分点对。然而，欧几里得和曼哈顿距离
    MDS 结果之间的差异较小，点分布比在闵可夫斯基距离情况下要广泛得多。'
- en: Extending Multidimensional Scaling with Isomap
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展多维尺度法与 Isomap
- en: Some manifold learning algorithms extend MDS to other types of spaces and distance
    calculations. *Isomap* extends MDS by replacing the distance matrix with one of
    geodesic distances between points calculated from a neighborhood graph. This replacement
    of distance calculations with geodesic distances allows for the use of distances
    that naturally exist on spaces that are not flat, such as spheres (for instance,
    geographic information system data) or organs in a human body examined through
    MRIs. Most commonly, distances are estimated by examining a point’s nearest neighbors.
    This gives Isomap a neighborhood flavor and a way to investigate the role of scaling
    through the variance of the nearest neighbor parameter.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一些流形学习算法将MDS扩展到其他类型的空间和距离计算。*Isomap*通过用从邻域图计算出的地理距离矩阵替换距离矩阵来扩展MDS。这种通过地理距离替换距离计算的方法允许在非平坦空间（例如球面，地理信息系统数据，或通过MRI检查的人体器官）上使用自然存在的距离。通常，通过检查点的最近邻来估算距离。这赋予了Isomap一个邻域的特征，并提供了一种通过最近邻参数的方差来研究缩放作用的方法。
- en: 'Let’s explore this modification by adding to [Listing 5-7](#listing5-7), which
    simulated a dataset and explored MDS. We’ll use Euclidean distance as a dissimilarity
    measure, though other distance metrics can be used much as they were with MDS.
    To understand the role of neighborhood size, we’ll create neighborhoods of 5,
    10, and 20 nearest neighbors:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过在[清单5-7](#listing5-7)中添加内容来探索这一修改，该内容模拟了一个数据集并探索了MDS。我们将使用欧几里得距离作为相异度度量，尽管其他距离度量也可以像MDS中一样使用。为了理解邻域大小的作用，我们将创建5、10和20个最近邻的邻域：
- en: '[PRE17]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This snippet of code applies Isomap to the generated dataset in [Listing 5-7](#listing5-7)
    using Euclidean distance. Other distance metrics can be used and may give different
    results, as shown in the MDS analyses. The results of the Isomap analyses suggest
    that neighborhood size doesn’t play a large role in determining results for this
    dataset, as shown by the scales for each coordinate in the [Figure 5-23](#figure5-23)
    plots.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将Isomap应用于[清单5-7](#listing5-7)中生成的数据集，并使用欧几里得距离。可以使用其他距离度量，这可能会给出不同的结果，正如MDS分析中所示。Isomap分析的结果表明，对于这个数据集，邻域大小在确定结果中的作用不大，正如[图5-23](#figure5-23)中的每个坐标的比例所示。
- en: '![](image_fi/503083c05/f05023.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05023.png)'
- en: 'Figure 5-23: A side-by-side view of Isomap results, which vary by number of
    nearest neighbors'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-23：Isomap结果的并排视图，结果随最近邻数量变化
- en: MDS and Isomap aim to preserve distance between points regardless of how far
    apart the points lie on the data manifold, resulting in global preservation of
    distance. Other global manifold learning algorithms, which preserve distances
    between points that are not in the same neighborhood, exist. If you’re interested,
    you can explore global algorithms such as kernel PCA, autoencoders, and diffusion
    mapping.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: MDS和Isomap的目标是保留点之间的距离，无论这些点在数据流形上的距离有多远，从而实现距离的全局保留。还有其他全局流形学习算法，能够保留不在同一邻域中的点之间的距离。如果你有兴趣，可以探索一些全局算法，如核PCA、自编码器和扩散映射。
- en: Capturing Local Properties with Locally Linear Embedding
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用局部线性嵌入捕捉局部性质
- en: Sometimes global properties of the manifold aren’t as important as local properties.
    In fact, from the classical definition of a manifold, local properties might sometimes
    be more interesting. For instance, when we’re looking for nearest neighbors to
    a point, points that are very far way geometrically probably won’t be nearest
    neighbors of that point, but points that are nearby could be nearest neighbors
    with information that needs to be preserved in a mapping between higher-dimensional
    and lower-dimensional spaces. Local manifold learning algorithms aim to preserve
    the local properties with less focus on preserving global properties in the mapping
    between spaces.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，流形的全局性质不像局部性质那么重要。实际上，从流形的经典定义来看，局部性质有时可能更有趣。例如，当我们寻找一个点的最近邻时，从几何上非常远的点可能不会是该点的最近邻，但附近的点可能是最近邻，并且这些信息需要在高维空间和低维空间之间的映射中被保留。局部流形学习算法旨在保留局部性质，而在空间映射中较少关注保留全局性质。
- en: '*Locally linear embedding* *(LLE)* is one such local manifold learning algorithm,
    and it is one of the more often used manifold learning algorithms. Roughly speaking,
    LLE starts with a nearest neighbor graph and then proceeds to create sets of weights
    for each point given its nearest neighbors. From there, the algorithm calculates
    the mapping according to a cost function and the preservation of the nearest neighbor
    weight sets for each point. This allows it to preserve important geometric information
    in the data that exists between points near each other on the manifold.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*局部线性嵌入*（*LLE*）就是这样一种局部流形学习算法，它是使用较为频繁的流形学习算法之一。粗略来说，LLE 从一个最近邻图开始，然后为每个点根据其最近邻生成一组权重。接着，算法根据一个代价函数计算映射，并保持每个点的最近邻权重集。这使得它能够保持流形上相邻点之间的几何信息。'
- en: 'Returning to our code in [Listing 5-7](#listing5-7), let’s add to our code
    and explore LLE mapping to a two-dimensional space with varying numbers of neighbors.
    For this package, you’ll need to download the package ([https://mran.microsoft.com/snapshot/2016-08-05/web/packages/TDAmapper/README.html](https://mran.microsoft.com/snapshot/2016-08-05/web/packages/TDAmapper/README.html))
    and locally install it:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们在[列表 5-7](#listing5-7)中的代码，接下来我们将对代码进行扩展，并探索将 LLE 映射到一个二维空间中，使用不同数量的邻居。对于这个包，你需要下载该包（[https://mran.microsoft.com/snapshot/2016-08-05/web/packages/TDAmapper/README.html](https://mran.microsoft.com/snapshot/2016-08-05/web/packages/TDAmapper/README.html)）并本地安装它：
- en: '[PRE18]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This piece of code applies the LLE algorithm to our dataset, varying the number
    of nearest neighbors considered in the algorithm calculations. Let’s examine the
    plots from this dataset to understand the role of nearest neighbors in this local
    algorithm ([Figure 5-24](#figure5-24)).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将 LLE 算法应用于我们的数据集，变更算法计算中考虑的最近邻数量。让我们通过观察这个数据集的图表，来理解最近邻在这个局部算法中的作用（见[图
    5-24](#figure5-24)）。
- en: '![](image_fi/503083c05/f05024.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05024.png)'
- en: 'Figure 5-24: A side-by-side view of LLE results, which vary by number of nearest
    neighbors'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-24：LLE 结果的并排视图，结果随最近邻数量的变化而变化
- en: As shown in [Figure 5-24](#figure5-24), neighborhood size greatly impacts LLE
    results and the spread of points in the new two-dimensional space. Given that
    the number of nearest neighbors impacts the size of the neighborhood preserved,
    higher values result in less-local versions of LLE, converting the algorithm into
    more of a global-type manifold learning algorithm. Good separation seems to occur
    at `K=20`, which is less local than `K=5` but still a fairly small neighborhood
    for a dataset with 100 points. A fully global algorithm exists if we set K to
    100, giving a two-dimensional plot with good separation and spread of points across
    the new space; you can see this in [Figure 5-25](#figure5-25).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 5-24](#figure5-24)所示，邻域大小对 LLE 结果以及新二维空间中点的分布有很大影响。由于最近邻的数量会影响保存的邻域大小，较大的值会导致
    LLE 变得更为全局，将该算法转化为更具全局性质的流形学习算法。在`K=20`时似乎能够获得较好的分离，这比`K=5`的局部性要差一些，但对于一个包含 100
    个点的数据集来说，仍然是一个相对较小的邻域。如果我们将 K 设置为 100，就会得到一个全局性的算法，生成一个二维图，点在新空间中具有良好的分离和扩展；你可以在[图
    5-25](#figure5-25)中看到这一点。
- en: '![](image_fi/503083c05/f05025.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05025.png)'
- en: 'Figure 5-25: A plot of LLE results using the entire sample as nearest neighbors'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-25：使用整个样本作为最近邻的 LLE 结果图
- en: Other local manifold learning algorithms exist, and some of these allow for
    a scaling parameters like LLE’s neighborhood size. If you’re interested, you can
    explore Laplacian eigenmaps, Hessian LLE, and local tangent space analysis.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他局部流形学习算法，其中一些允许像 LLE 的邻域大小那样调整缩放参数。如果你感兴趣，可以探索拉普拉斯特征映射、Hessian LLE 和局部切空间分析。
- en: Visualizing with t-Distributed Stochastic Neighbor Embedding
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 t-分布随机邻居嵌入进行可视化
- en: 'We’ve now seen how local algorithms can capture global properties through neighborhood
    size definition. Some manifold learning algorithms exist that explicitly capture
    both local and global properties. One of the more well-known algorithms is a visualization
    tool called *t-distributed stochastic neighbor embedding* *(t-SNE)*. The algorithm
    has two main stages: creating probability distributions over points in the high-dimensional
    space and then matching these distributions to ones in a lower-dimensional space
    by minimizing the Kullback–Leibler divergence over the two sets of distributions.
    Thus, rather than starting with a distance calculation between points, this algorithm
    focuses on matching distribution distances to find the optimal space.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经看到局部算法是如何通过邻域大小定义来捕捉全局特性的。有些流形学习算法明确地捕捉局部和全局特性。一个更著名的算法是一个名为*t-分布随机邻域嵌入*（*t-SNE*）的可视化工具。该算法有两个主要阶段：在高维空间中创建点的概率分布，然后通过最小化Kullback–Leibler散度将这些分布与低维空间中的分布进行匹配。因此，该算法并不是从点之间的距离计算开始，而是聚焦于匹配分布之间的距离，以找到最优空间。
- en: 'Instead of defining a neighborhood by *k*-nearest neighbors to a point, t-SNE
    defines a neighborhood by the kernel’s bandwidth over the data; this yields a
    parameter called *perplexity*, which can also be varied to understand the role
    of neighborhood size. Let’s return to the data generated in [Listing 5-7](#listing5-7)
    and see how this works in practice. Add the following code:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE并不是通过点的*k*-最近邻来定义邻域，而是通过数据中核函数的带宽来定义邻域；这会产生一个名为*困惑度*的参数，该参数也可以变化，以理解邻域大小的作用。让我们回到在[清单
    5-7](#listing5-7)中生成的数据，看看这在实际中是如何工作的。请添加以下代码：
- en: '[PRE19]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This piece of code runs t-SNE on the dataset generated with [Listing 5-7](#listing5-7),
    varying the perplexity parameter. The plots should produce something like [Figure
    5-26](#figure5-26), which shows more clumping in the lower-perplexity trial than
    in the trials with higher perplexity.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码在使用[清单 5-7](#listing5-7)生成的数据集上运行 t-SNE，改变了困惑度参数。绘图应该会生成类似于[图 5-26](#figure5-26)的内容，显示低困惑度试验中的聚类比高困惑度试验中的聚类更多。
- en: '![](image_fi/503083c05/f05026.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05026.png)'
- en: 'Figure 5-26: A side-by-side plot of t-SNE results with differing perplexity
    settings'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-26：困惑度设置不同的 t-SNE 结果并排绘图
- en: The plots for perplexity of 15 and 25 look fairly similar, and as we increase
    perplexity, the range of the coordinates in the lower-dimensional space drops.
    There may be projects where more spread in the data is useful for subsequent analyses
    or visualizing possible trends; other projects may yield better results with tighter
    data.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度为15和25的绘图看起来相当相似，并且随着困惑度的增加，低维空间中坐标的范围减小。某些项目可能在后续分析或可视化可能的趋势时，需要数据的分布更广；而其他项目则可能通过更紧密的数据得到更好的结果。
- en: In summary, the distance metrics in this chapter pop up regularly in machine
    learning applications. Manifold learning, in particular, can involve different
    choices of metric, neighborhood size, and type of space onto which the data space
    is mapped. Many good textbooks and papers exist that cover these algorithms and
    others like them in more detail. However, we hope that you’ve gained an overview
    of dimensionality reduction methods—particularly those that are intimately connected
    to metric geometry.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本章中的距离度量在机器学习应用中经常出现。流形学习尤其涉及度量、邻域大小和数据空间映射到的空间类型的不同选择。有许多优秀的教材和论文更详细地介绍了这些算法和类似的算法。然而，我们希望你已经获得了关于降维方法的概览——特别是那些与度量几何紧密相关的方法。
- en: Before moving on, let’s consider one final use of metric geometry.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们考虑度量几何的最后一个用途。
- en: Fractals
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分形
- en: Another tool connected to metric geometry involves a type of self-similarity
    in geometry objects called *fractals*. Essentially, fractals have a pattern within
    the same pattern within the same pattern within the same pattern, and so on. [Figure
    5-27](#figure5-27) has an example.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个与度量几何相关的工具涉及一种几何对象中的自相似性，称为*分形*。本质上，分形具有同样模式中的同样模式中的同样模式，依此类推。[图 5-27](#figure5-27)提供了一个示例。
- en: '![](image_fi/503083c05/f05027.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05027.png)'
- en: 'Figure 5-27: An example of a fractal. Note the self-similarity of the patterns
    at different scales.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-27：一个分形的示例。注意不同尺度下的图案具有自相似性。
- en: Fractals occur often in natural and man-made systems. For instance, coastlines,
    blood vessels, music scales, epidemic spread in confined spaces, stock market
    behavior, and word frequency and ranking all have self-similarity properties at
    different scales. Being able to measure fractal dimension allows us to better
    understand the degree of self-similarity of these phenomena. There are many fractal
    dimension estimators these days, but most rely on measuring variations in the
    area under a fractal curve through some sort of iterative approach that compares
    neighboring point sets’ areas.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 分形在自然和人为系统中经常出现。例如，海岸线、血管、音乐音阶、封闭空间内的疫情传播、股市行为以及单词频率和排名等都在不同尺度下具有自相似性特征。能够测量分形维度使我们能够更好地理解这些现象的自相似性程度。如今有很多分形维度估计器，但大多数都依赖通过某种迭代方法，比较相邻点集的面积变化，从而测量分形曲线下的面积。
- en: Going back to the fractal in [Figure 5-27](#figure5-27), we could consider adding
    boxes to find the area under each iterative curve and then comparing the relative
    values across scales considered, as in [Figure 5-28](#figure5-28).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[图 5-27](#figure5-27)中的分形，我们可以考虑加入盒子，计算每个迭代曲线下的面积，并将不同尺度下的相对值进行比较，如同在[图 5-28](#figure5-28)中展示的那样。
- en: '![](image_fi/503083c05/f05028.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05028.png)'
- en: 'Figure 5-28: An example of measuring area under a series of fractal curves
    at scale'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-28：在尺度上测量一系列分形曲线下的面积示例
- en: Now that we have some intuition around fractals, let’s consider an application
    of fractal dimension metrics. Stock markets are known to exhibit some degree of
    self-similar behavior over periods of time. Understanding market volatility is
    a major aspect of investing wisely, and one method used to predict coming market
    reversal points, such as crashes, is changing self-similarity. The closing prices
    of the Dow Jones Industrial Average (one of the American stock market indices),
    or DJIA, are widely available for free download. Here, we’ll consider simulated
    daily closing prices like DJIA data from the period of June 2019 to May 2020,
    during which the COVID freefall happened. [Figure 5-29](#figure5-29) shows a chart
    of closing prices over that time period.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对分形有了一些直观理解，让我们来考虑分形维度度量的一个应用。股市在一段时间内常常表现出某种程度的自相似行为。理解市场波动性是明智投资的重要方面，而预测即将到来的市场反转点（如股市崩盘）的一种方法是分析自相似性的变化。道琼斯工业平均指数（DJIA），即美国股市的一个指数，其收盘价数据可以免费下载。这里，我们将考虑模拟的
    2019 年 6 月至 2020 年 5 月期间的每日收盘价数据，该时期正值 COVID 自由落体阶段。[图 5-29](#figure5-29) 展示了该时间段内收盘价的图表。
- en: '![](image_fi/503083c05/f05029new.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05029new.png)'
- en: 'Figure 5-29: A plot of simulated DJIA closing prices from June 2019 to May
    2020\. Note the big drops starting in late February 2020, when COVID became a
    global issue.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-29：2019 年 6 月到 2020 年 5 月期间模拟的道琼斯工业平均指数（DJIA）收盘价图。请注意，自 2020 年 2 月下旬开始的大幅下跌，正值
    COVID 成为全球问题之时。
- en: If we were predicting future market behavior, we’d want to employ fractal analyses
    with tools from time-series data analysis, which are outside the scope of this
    book. However, we can get a feel for changes in self-similarity month by month
    easily by parsing the data into monthly series and calculating each monthly series’
    fractal dimension. From there, we can examine how fractal dimension correlates
    with other measures of volatility, such as the range of closing prices within
    a month; we should see a positive correlation. [Listing 5-8](#listing5-8) loads
    the data, parses it, calculates fractal dimension, calculates closing price range,
    and runs a correlation test between fractal dimension and range.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们预测未来的市场行为，我们将希望结合时间序列数据分析中的工具进行分形分析，而这些内容超出了本书的范围。然而，我们可以通过将数据解析为按月划分的系列并计算每个月系列的分形维度，轻松感知自相似性的变化。从那里，我们可以检查分形维度与其他波动性度量（如一个月内收盘价的波动范围）之间的相关性；我们应该能看到正相关。[清单
    5-8](#listing5-8) 加载数据，解析数据，计算分形维度，计算收盘价范围，并运行分形维度与范围之间的相关性测试。
- en: '[PRE20]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Listing 5-8: A script that loads the simulated DJIA closing data, calculates
    fractal dimension and range of closing prices, and runs a correlation test to
    determine the relationship between fractal dimension and closing price range'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 5-8：一个脚本，加载模拟的 DJIA 收盘数据，计算分形维度和收盘价范围，并进行相关性测试，以确定分形维度与收盘价范围之间的关系。
- en: You should find a correlation of around 0.55, or a moderate relationship between
    closing price fractal dimension and closing price range, that is around the 0.05
    significance level on the correlation test. Self-similarity does seem positively
    tied to one measure of market volatility. The fractal dimension varies by month,
    with some months’ dimensionality being close to 1 and others’ dimensionality being
    quite a bit higher. Impressively, the fractal dimension shoots up to 2 for March
    2020.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该找到一个约为0.55的相关性，或者说是收盘价分形维度与收盘价区间之间的中等关系，这个相关性大约在0.05的显著性水平上。自相似性似乎与市场波动性的一个衡量指标有正相关关系。分形维度随着月份变化，有些月份的维度接近1，而其他月份的维度则较高。令人印象深刻的是，2020年3月的分形维度飙升至2。
- en: Given that we only have 12 months’ worth of data going into our test, it’s worth
    noting that we still find evidence for a positive relationship between fractal
    dimension and range of closing prices. Interested readers with their own stock
    market data are encouraged to optimize the time frame windows and potential window
    overlap chosen to calculate the series of fractal dimensions on their own data,
    as well as investigate the correlations with other geometric tools used in stock
    market change point detection, such as Forman–Ricci curvature and persistent homology.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们仅有12个月的数据用于测试，值得注意的是，我们仍然发现分形维度与收盘价区间之间存在正相关关系。对股市数据感兴趣的读者可以自行优化计算分形维度序列时所选的时间框架窗口和潜在的窗口重叠，并进一步探讨与股市变动点检测中使用的其他几何工具（如Forman–Ricci曲率和持久同源性）的相关性。
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we’ve investigated metric geometry and its application in several
    important machine learning algorithms, including the *k*-NN algorithm and several
    manifold learning algorithms. We’ve witnessed how the choice of distance metric
    (and other algorithm parameters) can dramatically impact performance. We’ve also
    examined fractals and their relationship to stock market volatility. Measuring
    distances between points and distributions crops up in many areas of machine learning
    and impacts quality of machine learning results. [Chapter 5](c05.xhtml) barely
    scratches the surface of extant tools from metric geometry. You may want to consult
    the papers referenced in the R packages used in this chapter, as well as current
    machine learning publications in distance metrics.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了度量几何及其在几个重要机器学习算法中的应用，包括*k*-NN算法和几个流形学习算法。我们见证了距离度量（以及其他算法参数）的选择如何显著影响性能。我们还考察了分形及其与股市波动性的关系。计算点和分布之间的距离在许多机器学习领域中都有应用，并且影响着机器学习结果的质量。[第5章](c05.xhtml)仅仅触及了度量几何现有工具的表面。你可能想参考本章使用的R包中的相关文献，以及当前在距离度量领域的机器学习出版物。
