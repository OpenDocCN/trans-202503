- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Geometry in Data Science
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学中的几何学
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: 'In this chapter, we’ll explore several tools from geometry: we’ll look at distance
    metrics and their use in *k*-nearest neighbor algorithms; we’ll discuss manifold
    learning algorithms that map high-dimensional data to potentially curved lower-dimensional
    manifolds; and we’ll see how to apply fractal geometry to stock market data. The
    motivation for this chapter follows, among other things, from the *manifold hypothesis*,
    which posits that real-world data often has a natural dimensionality lower than
    the dimensionality of the dataset collected. In other words, a dataset that has
    20 variables (that is, a dimensionality of 20) might have a better representation
    in a 12-dimensional space or an 8-dimensional space. Given the curse of dimensionality,
    representing data in lower-dimensional spaces is ideal (particularly when the
    original dimensionality of a dataset is large, as in genomics or proteomics data).
    Choosing the right distance measurements needed to create these representations
    has important implications for solution quality.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索几何学中的几个工具：我们将研究距离度量及其在*k*-最近邻算法中的应用；我们将讨论将高维数据映射到潜在弯曲的低维流形的流形学习算法；并且我们将看到如何将分形几何应用于股票市场数据。本章的动机来源于*流形假设*，该假设认为现实世界中的数据通常具有比收集到的数据集维度更低的自然维度。换句话说，一个具有20个变量（即20维度）的数据集，可能在12维空间或8维空间中有更好的表示。鉴于维度的诅咒，在低维空间中表示数据是理想的（尤其是当数据集的原始维度很大时，如基因组学或蛋白质组学数据）。选择合适的距离度量来创建这些表示对于解决方案的质量具有重要意义。
- en: Introduction to Distance Metrics in Data
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据中的距离度量简介
- en: Many machine learning algorithms depend on distance metrics, which provide a
    measure between points or objects in a space or manifold. Changes in choice of
    distance metric can impact machine learning performance dramatically, as we’ll
    see later in this chapter. *Distance metrics* provide a measure between points
    or objects in a space or manifold. This can be relatively straightforward like
    using a ruler to measure the distance between two points on a flat sheet of paper,
    as demonstrated in [Figure 5-1](#figure5-1).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习算法依赖于距离度量，它们提供了空间或流形中点或对象之间的度量。选择距离度量的变化可以显著影响机器学习的表现，正如我们将在本章后面看到的那样。*距离度量*提供了空间或流形中点或对象之间的度量。这可以像使用尺子测量平面纸上两点之间的距离那样简单，正如[图5-1](#figure5-1)中所演示的。
- en: '![](image_fi/503083c05/f05001r.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05001r.png)'
- en: 'Figure 5-1: A plot of two points on a sheet of paper and the line connecting
    them'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-1：纸面上两个点的图示及连接它们的直线
- en: However, measuring the distance between two points on a sphere using a ruler
    will surely be a bit more complicated.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用尺子来测量球面上两点之间的距离肯定会更为复杂。
- en: If you used a piece of string to limn out the shortest path connecting the two
    points on the sphere, as in [Figure 5-2](#figure5-2), you could mark the distance
    on the string and then use a ruler to measure that distance on the straightened-out
    string. This is akin to what is done with distances on manifolds, where *geodesics*
    (shortest paths between two points relative to the curved manifold) are lifted
    into the *tangent space* (a zero-curvature space defined by tangent lines, tangent
    planes, and higher-dimensional tangents) to measure distances.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你用一根绳子标出连接球面上两点的最短路径，如[图5-2](#figure5-2)所示，你可以在绳子上标记距离，然后用尺子测量拉直后的绳子上的距离。这类似于在流形上度量距离时所做的事情，其中*测地线*（相对于弯曲流形的两点之间的最短路径）被提升到*切空间*（由切线、切平面和更高维切线定义的零曲率空间）来测量距离。
- en: '![](image_fi/503083c05/f05002.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05002.png)'
- en: 'Figure 5-2: A plot of two points on a sphere, along with the geodesic connecting
    them'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-2：球面上两个点的图示，以及连接它们的测地线
- en: We’ll explore tangent spaces and their applications in machine learning in more
    depth in [Chapter 6](c06.xhtml), but for now, you can think of lifting the string
    to a large sheet of paper and measuring its length with a ruler to measure distance
    outside of the curved space, where it’s more difficult to establish a standard
    measurement. While geodesics and tangent spaces look counterintuitive, they follow
    from our knowledge of tangents in Euclidean geometry and derivatives in calculus.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: However, there are other situations in which distances between two points are
    a bit more complicated. Consider walking from one house to another in the neighborhood,
    as shown in [Figure 5-3](#figure5-3).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05003.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-3: A plot of houses in a neighborhood, where one is walking between
    two houses'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Unless one is able to walk through neighboring houses without running into exterior
    and interior walls (not to mention disgruntled neighbors!), it’s not possible
    to draw a straight line or geodesic between the houses that gives a direct route,
    as you can see in [Figure 5-4](#figure5-4).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05004.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-4: A plot of houses in a neighborhood, where one attempts a straight
    line between houses'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Instead, it’s a lot more practical to take the sidewalks ([Figure 5-5](#figure5-5)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05005.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-5: A plot of houses in a neighborhood, where one walks on the sidewalks
    between houses'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Distance is often discrete, rather than continuous, or lies on a manifold with
    curvature. Understanding the geometry of the data space in which the data points
    live can give a good indication of what distance metric is appropriate for the
    data. In the following section, we’ll go over some common distance metrics in
    machine learning, and then, in the sections after that, we’ll apply these distances
    to *k*-NN algorithms and dimensionality reduction algorithms.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Common Distance Metrics
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given the nuances of measuring distance, it’s important to understand some of
    the more common distance metrics used in machine learning, including one we briefly
    encountered in [Chapter 4](c04.xhtml) (Wasserstein distance, used to compare persistent
    homology results). There are an infinite number of distance metrics, and some
    distance metrics have parameters that can give rise to an infinite number of variations.
    Thus, we cannot cover all possible distance metrics one could encounter in machine
    learning. We’ve left out some that are useful in recommender systems, such as
    cosine distance, as they are uncommon metrics within topological data analysis
    or network analysis applications. We’ll explore some of the more common ones;
    if you’re interested in going further, we suggest you explore the field of *metric
    geometry*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Simulating a Small Dataset
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we start exploring common distance metrics, let’s simulate some data
    with [Listing 5-1](#listing5-1).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Listing 5-1: A script that simulates and plots a small dataset'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: This script creates a dataset with three variables and plots points in a three-dimensional
    space. This should give a plot with points lying on the three axes ([Figure 5-6](#figure5-6)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本创建了一个包含三个变量的数据集，并在三维空间中绘制点。这将生成一个绘制在三个坐标轴上的点图（[图 5-6](#figure5-6)）。
- en: '![](image_fi/503083c05/f05006.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05006.png)'
- en: 'Figure 5-6: A plot of five points, all lying on axes defined by variables *a*,
    *b*, and *c*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-6：五个点的图，所有点都位于由变量*a*、*b*和*c*定义的坐标轴上
- en: This dataset includes the points shown in [Listing 5-2](#listing5-2), which
    we will use to calculate distances between points.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包括[清单 5-2](#listing5-2)中显示的点，我们将用它来计算点之间的距离。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Listing 5-2: A matrix of the five points in the simulated dataset with random
    variables *a*, *b*, and *c*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 5-2：包含模拟数据集中五个点的矩阵，这些点有随机变量*a*、*b*和*c*
- en: Now that we have a dataset generated, let’s look at some standard distance metrics
    that can be used to measure the distance between pairs of points in the dataset.
    R comes with a handy package, called the *stats* package (which comes with the
    base R installation), for calculating some of the common distance metrics used
    on data through the `dist()` function.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经生成了一个数据集，让我们看看一些可以用来衡量数据集中点对之间距离的标准距离度量。R自带了一个非常方便的包，叫做*stats*包（它是R基本安装的一部分），可以通过`dist()`函数计算一些常见的距离度量。
- en: Using Norm-Based Distance Metrics
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用基于范数的距离度量
- en: The first distances we’ll consider are related. The *norm* of a function or
    vector is a measurement of the “length” of that function or vector. The norm involves
    summing distance differences to a power and then applying that power’s root to
    the result. For the *Euclidean distance* between points, for example, the squares
    of differences are summed before taking the square root of the result. For single
    vectors (that is, a single data point), the norm will be a weighted distance from
    the origin, where the axes mutually intersect. You can think of this as the length
    of a straight line from the origin to the point being measured. Going back to
    the scatterplot of our points, this might be drawn like in [Figure 5-7](#figure5-7).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先考虑的距离是相关的。*范数*是一个函数或向量的“长度”度量。范数涉及将距离差异的幂次求和，然后对结果应用该幂次的根。例如，对于点之间的*欧几里得距离*，会先求出差异的平方和，然后对结果取平方根。对于单个向量（即单个数据点），范数将是从原点到该点的加权距离，其中坐标轴互相交叉。你可以把它想象成从原点到被测点的直线长度。回到我们点的散点图，可能像[图
    5-7](#figure5-7)所示那样绘制。
- en: '![](image_fi/503083c05/f05007.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/503083c05/f05007.png)'
- en: 'Figure 5-7: A plot of the five points with a straight line pointing to one
    of the points in the set'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-7：一个包含五个点的图，图中有一条直线指向集合中的某个点
- en: The most common norm used to measure metric distance between points is probably
    the Euclidean distance mentioned earlier, given by the L²-norm, defined as the
    square root of squared distance between points where L is a placeholder for the
    vector (or vectors) and the exponent is the power of the norm (here, 2). This
    is the distance typically taught in high school geometry classes, and it is also
    referred to as the *Pythagorean distance*. We saw it in [Figure 5-4](#figure5-4),
    which showed a straight line of shortest distance between the houses (traveling
    as the bird flies above the houses). Statisticians typically use the square of
    the Euclidean distance metric when calculating squared errors in regression algorithms;
    for reasons we won’t delve into here, using the square of Euclidean distance is
    very natural.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 用于度量点之间度量距离的最常见范数可能是前面提到的欧几里得距离，由L²-范数给出，定义为点之间的平方距离的平方根，其中L是向量（或向量组）的占位符，指数是范数的幂次（这里是2）。这是高中的几何学课程中通常教授的距离，也被称为*毕达哥拉斯距离*。我们在[图
    5-4](#figure5-4)中看到了它，图中显示了房屋之间最短距离的直线（按鸟飞的路径从空中看）。统计学家在计算回归算法中的平方误差时通常使用欧几里得距离的平方；由于一些原因，我们在这里不再深入探讨，使用欧几里得距离的平方是非常自然的。
- en: Related to the L²-norm is the L¹-norm, or *Manhattan distance*. Manhattan distance
    calculations are much like the neighborhood example given in [Figure 5-5](#figure5-5).
    Manhattan distance is defined as the sum of point differences along each axis,
    with the axes’ point differences summed into a final tally of axis distances.
    Let’s say we have two points (0, 1) and (1, 0), which might represent whether
    a patient has a gene mutation in either gene of interest within a disease model.
    The Manhattan distance is (0 + 1) + (1 + 0), or the sum of point differences across
    all vector axes. In this example, we find the Manhattan distance is 2.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: This metric is useful when working with count data or other discrete data formats,
    such as the example dataset generated earlier in this section. [Figure 5-5](#figure5-5)
    demonstrates this type of distance calculation, where the person needs to walk
    on the streets along the north-south and east-west axes. Manhattan distance and
    L¹-norms often come up in applications of Lasso and elastic net regression, where
    it is used to set beta coefficients to 0 if they are within a certain distance
    of the origin, thereby performing variable selection and creating a sparse model.
    This is useful in situations where the dimensionality of the independent variable
    set is high (such as in genomic data).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: A generalization of both the L¹-norm and L²-norm is the *Minkowski distance*,
    which generalizes norms from L³-norms to L^∞-norms. The L^∞-norm is another special
    instance of norm-based distances, dubbed the *Chebyshev distance*. Mathematically,
    Chebyshev distance is the maximum distance between points along any axis. It is
    often used in problems involving planned movements of machinery or autonomous
    systems (like drones).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: As the dimension of the norm increases, the Minkowski distance values typically
    decrease and stabilize. Thus, Minkowski distances with high-dimensional norms
    can act as distance smoothers that rein in strange or unusually large distance
    calculations found with the Manhattan or Euclidean distance calculations. Minkowski
    distance does impose a few conditions, including that the zero vector has a length
    of zero, that application of a positive scalar multiple to a vector does not change
    the vector’s direction, and that the shortest distance between two points is a
    straight line (known as the *triangle inequality condition*). In the `dist()`
    function of R, the dimension of the norm is given by the parameter `p`, with `p=1`
    corresponding to Manhattan distance, `p=2` corresponding to Euclidean distance,
    and so on.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: A special extension of Manhattan distance is the *Canberra distance*, which
    is a weighted sum of the L¹-norm. Technically, Canberra distance is computed by
    finding the absolute value of the distance between a pair of points divided that
    by the sum of the pair of points’ absolute values, which then is summed across
    point pairs. It can be a useful distance metric when dealing with outliers, intrusion
    detection, or mixed types of predictors (continuous and discrete measures). The
    example point in [Figure 5-7](#figure5-7) likely isn’t a statistical outlier,
    but it certainly lies in a different part of the data space than the other simulated
    points.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run these distances and compare the results on the dataset we simulated
    earlier in this section; add the following to the code in [Listing 5-1](#listing5-1):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This code computes distance metrics for Euclidean, Manhattan, Canberra, and
    Minkowski distances applied to our example dataset. Looking at Euclidean distance
    measurements between pairs of points in the simulated dataset, shown in [Table
    5-1](#table5-1), we see values with many decimal points for many pairs of points,
    owing to the square root involved in calculating the Euclidean distance.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5-1: Euclidean Distance Calculations Between Pairs of Points in [Listing
    5-2](#listing5-2)’s Matrix'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '| **Euclidean** | **1** | **2** | **3** | **4** | **5** |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0 | 2.44949 | 1.414214 | 2.236068 | 1.414214 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '| **2** | 2.44949 | 0 | 1.414214 | 1 | 3.162278 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| **3** | 1.414214 | 1.414214 | 0 | 1 | 2 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| **4** | 2.236068 | 1 | 1 | 0 | 3 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| **5** | 1.414214 | 3.162278 | 2 | 3 | 0 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: Moving on to Manhattan distance ([Table 5-2](#table5-2)), the distances between
    pairs of points become whole numbers, as the calculation involves discrete steps
    along each axis separating the points.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5-2: Manhattan Distance Calculations Between Pairs of Points in [Listing
    5-2](#listing5-2)’s Matrix'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '| **Manhattan** | **1** | **2** | **3** | **4** | **5** |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0 | 4 | 2 | 3 | 2 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
- en: '| **2** | 4 | 0 | 2 | 1 | 4 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
- en: '| **3** | 2 | 2 | 0 | 1 | 2 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
- en: '| **4** | 3 | 1 | 1 | 0 | 3 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
- en: '| **5** | 2 | 4 | 2 | 3 | 0 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: As expected, the Minkowski distance calculations match the Manhattan distance
    for `p=1` and Euclidean distance for `p=2`. In [Table 5-3](#table5-3), you can
    see Minkowski distances with `p=1`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5-3: Minkowski Distance Calculations Between Pairs of Points in [Listing
    5-2](#listing5-2)’s Matrix'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '| **Minkowski `p=1`** | **1** | **2** | **3** | **4** | **5** |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0 | 4 | 2 | 3 | 2 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| **2** | 4 | 0 | 2 | 1 | 4 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| **3** | 2 | 2 | 0 | 1 | 2 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| **4** | 3 | 1 | 1 | 0 | 3 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| **5** | 2 | 4 | 2 | 3 | 0 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: The Canberra distance gives some similar and overlapping values with Manhattan
    distance. However, some distances are different (particularly pairs involving
    points 2 or 3), owing to the weighted parts of the distance calculation, as shown
    in [Table 5-4](#table5-4).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5-4: Canberra Distance Calculations Between Pairs of Points in [Listing
    5-2](#listing5-2)’s Matrix'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '| **Canberra** | **1** | **2** | **3** | **4** | **5** |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0 | 3 | 2 | 3 | 1.8 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: '| **2** | 3 | 0 | 3 | 3 | 3 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: '| **3** | 2 | 3 | 0 | 3 | 1.5 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: '| **4** | 3 | 3 | 3 | 0 | 3 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: '| **5** | 1.8 | 3 | 1.5 | 3 | 0 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: For some points in [Listing 5-2](#listing5-2)’s distance matrix calculations,
    these three distances give the same distance score for a pair of points (such
    as for points 4 and 5). However, some of the distances are quite different when
    we increase the value of `p` (such as points 1 and 2). If we’re using the distance
    metrics in a support vector machine classifier, we might end up with a very different
    line cutting our data into groups—or very different error rates.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other ways to modify or extend norm-based distances. One popular
    modification is like the Canberra distance: *Mahalanobis distance* applies a weighting
    scheme to Euclidean distance calculations before taking the square root of the
    result, such that Euclidean distance is weighted by the covariance matrix. If
    the covariance matrix is simply the identity matrix, Mahalanobis distance will
    collapse to Euclidean distance. If the covariance matrix is diagonal, the result
    is a standardized Euclidean distance. Thus, Mahalanobis distance provides a type
    of “centered” distance metric that can identify leverage points and outliers within
    a data sample. It’s often used in clustering and discriminant analyses, as outliers
    and leverage points can skew results.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s a simple way to calculate Mahalanobis distance in R: the `mahalanobis()`
    function. Let’s add to our script again:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This code will calculate Mahalanobis distance with various centering strategies,
    yielding three different measures of leverage/weighted standard distance from
    a defined reference, detailed in [Table 5-5](#table5-5).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5-5: Mahalanobis Distance Results for the Individual Points from [Figure
    5-7](#figure5-7)’s Matrix'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '| **Mahalanobis** | **1** | **2** | **3** | **4** | **5** |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: '| **Covariance only** | 6.857143 | 6.857143 | 0.857143 | 0 | 7.714286 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
- en: '| **Point 1** | 0 | 8 | 5.428571 | 6.857143 | 7.714286 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
- en: '| **Column means** | 3.2 | 3.2 | 0.628571 | 2.057143 | 2.914286 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
- en: By using each point as a center, you can complete a full distance matrix similar
    to how the `dist()` function creates the distance matrix. You would simply loop
    through the individual points and append rows to a data frame.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: A few interesting observations come out of the Mahalanobis distance calculations.
    When only covariance is used, the origin becomes the reference point for calculating
    distances, and point 4, which is located at the origin, has a Mahalanobis distance
    of 0\. However, when column means are used to center the data, this point jumps
    to a much farther away value. This suggests that point 4 is quite far away from
    the column means, though it is perfectly centered at the origin. Another interesting
    trend involves point 3, which is quite close to both the origin and the centered
    column means, which come out to (1.2, 0.2, 0.2) in this dataset. Point 3 is located
    at (1, 0, 0), which is both near the origin and near this centered column mean.
    The other points are relatively far from both the origin and the centered column
    means.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'We can add these column means to our plot of this data and visualize a bit
    of how Mahalanobis distance works by adding to our script again:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code adds the column mean point to the original dataset to examine where
    the “middle” of the data should be located in the three-dimensional space; the
    code should yield a plot similar [Figure 5-8](#figure5-8).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Examining [Figure 5-8](#figure5-8) and comparing it to [Figure 5-6](#figure5-6),
    we can see that a point has been placed off the axes that does seem to occupy
    a central location among the five points. Finding the central location of a dataset
    helps in several data science tasks, including finding stealth outliers (outliers
    without extreme values for any one variable but far from most points in the multivariate
    dataset) and calculating multivariate statistics. Some points are closer to this
    central location than others, as our Mahalanobis results suggest; these are points
    3 and 4 in our dataset, which are relatively close to the origin.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05008.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-8: Mahalanobis distance with centering at column means for the individual
    points from [Listing 5-2](#listing5-2)’s matrix plotted with the column mean point
    shown off the axes'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: These distance differences come up in many machine learning applications, and
    we’ll see how distance impacts machine learning performance and results when we
    apply these distances within a *k*-nearest neighbors problem later in this chapter.
    Performance can vary dramatically with a different choice of metric, and using
    the wrong metric can mislead model results and interpretation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Diagrams, Shapes, and Probability Distributions
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Norm-based distance metrics are not the only class of metrics possible in machine
    learning. As we saw in [Chapter 4](c04.xhtml), it’s possible to calculate distance
    between objects other than points, such as persistence diagrams. Roughly speaking,
    these types of metrics measure differences in probability distributions. We’ve
    already briefly used one of these, the Wasserstein distance, to compare persistence
    diagram distributions. Let’s take a closer look now.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Wasserstein Distance
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In loose terms, *Wasserstein distance* compares the piles of probability weights
    stacked in two distributions. It’s often dubbed “earth-mover distance,” as the
    Wasserstein distance measures the cost and effort needed to move probability piles
    of one distribution to turn it into the comparison distribution. For more mathematically
    sophisticated readers, the *p*th Wasserstein distance can be calculated by taking
    the expected value of the joint distribution marginals to the *p*th power, finding
    the infimum over all join probability distributions of those random variables,
    and taking the *p*th root of the result. However, the details of this are beyond
    what is expected of readers, and we’ll stick with the intuition of earth-mover
    distance as we explore this metric. Let’s visualize two distributions of dirt
    piles to build some intuition behind this metric ([Figure 5-9](#figure5-9)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05009.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-9: Two distributions of dirt piles akin to the type of probability
    density functions that could be compared using Wasserstein distance metrics'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Pile 1 of [Figure 5-9](#figure5-9) has a large stack of dirt toward the left
    side that would need to be shoveled to the right piles if we were to transform
    Pile 1’s distribution of dirt to Pile 2’s. Our dirt mover will have to move quite
    a bit of dirt to transform Pile 1 into Pile 2\. However, if our Pile 2 had a distribution
    of dirt piles closer to that of Pile 1, as in [Figure 5-10](#figure5-10), the
    amount of work to turn Pile 1 into Pile 2 would be less for our dirt mover.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05010.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-10: Two distributions of dirt piles akin to the type of probability
    density functions that could be compared using Wasserstein distance metrics, which
    measure the amount of work needed to be done by our dirt mover to transform Pile
    1 into Pile 2'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: To get from Distribution 1 to Distribution 2, you can think of someone shoveling
    the dirt. The amount of dirt shoveled corresponds to the Wasserstein distance.
    Probability density functions that are very similar will have a smaller Wasserstein
    distance; those that are very dissimilar will have a larger Wasserstein distance.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Thus, Wasserstein distance can be a nice metric to use in comparing probability
    distributions—comparing theoretical distributions to sample distributions to see
    if they match, comparing multiple sample distributions from the same or different
    populations to see if they match, or even understanding if it’s possible to use
    a simpler probability distribution in a machine learning function that approximates
    the underlying data such that calculations within a machine learning algorithm
    will be easier to compute.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Although we won’t go into it here, some distributions—including count data,
    yes/no data, and even machine learning output structures like dendrograms—can
    be compared through other metrics. For now, let’s look at another way to compare
    probability distributions, this time with discrete distributions.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Entropy
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One common class of distance metrics involves a property dubbed *entropy*. Information
    entropy, defined by the negative logarithm of the probability density function,
    measures the amount of nonrandomness at each point of the probability density
    function. By understanding how much information is contained in a distribution
    at each value, it’s possible to compare differences in information across distributions.
    This can be a handy tool for comparing complicated probability functions or output
    from machine learning algorithms, as well as deriving nonparametric statistical
    tests.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Binomial distributions come up often in data science. We might think that a
    random customer has no preference between two new interface designs (50 percent
    preferring A and 50 percent preferring B in an A/B test). We could estimate the
    chance that 10 or 20 or 10,000 customers prefer A over B and compare it to samples
    of actual customers providing us feedback. One assumption might be that we have
    different customer populations, including one that is very small. Of course, in
    practice, we don’t know the actual preference distributions of our customer populations
    and may not have enough data to compare the distributions mathematically via a
    proportions test. Leveraging a metric can help us derive a test.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: To understand this a bit more intuitively, let’s simulate two binomial probability
    distributions with the code in [Listing 5-3](#listing5-3).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Listing 5-3: A script that simulates different binomial probability distributions'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-11](#figure5-11) shows that these binomial distributions have very
    different density functions, with information stored in different parts of the
    distribution.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05011.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-11: Two binomial distributions’ density functions plotted for comparison'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The black curve distribution, distribution *b***,** includes a wider spread
    of information over more values than distribution *a*, which concentrates its
    information nearer to zero (lighter gray curve). Entropy-based metrics can be
    used to quantify this difference in information storage between distributions*a*
    and *b*. R provides many tools and packages for quantifying and comparing information
    entropy. Let’s explore a bit further.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: The philentropy package in R contains 46 different metrics for comparing probability
    distributions, including many metrics based on entropy. One of the more popular
    entropy metrics is the *Kullback–Leibler divergence*, which measures the relative
    entropy of two probability distributions. Technically speaking, the Kullback–Leibler
    divergence measures the expectation (sum for discrete distributions or integral
    for continuous distributions) of the logarithmic differences between two probability
    distributions. As such, it’s a measurement of information gain or loss. This allows
    us to convert information entropy into a distribution comparison tool, which is
    useful when we’re trying to compare differences between unknown or complicated
    probability distributions that might not be amenable to the usual statistical
    tools.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s develop our intuition by returning to our two binomial distributions,
    *a*and *b*. We’ll calculate the Kullback–Leibler divergence between the two distributions
    by adding the following to [Listing 5-3](#listing5-3):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This code calculates the Kullback–Leibler divergence for the two binomial distribution
    samples generated in [Listing 5-3](#listing5-3). In this set of simulated data,
    the Kullback–Leibler divergence is 398.5428; another simulation of these distributions
    might yield a different divergence measurement. However, using nonparametric tests,
    we can compare this divergence value with the random error component of one of
    our distributions to see whether there is a statistically significant difference
    of entropy between distributions *a*and *b*. We can add to our script to create
    a nonparametric statistical test using entropy differences:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The confidence intervals from this test suggest confidence intervals of 1,427–1,475,
    which suggests that our distributions are significantly different. This is expected,
    as distributions *a*and *b*have very different values and ranges. Plotting the
    last distribution simulated ([Figure 5-12](#figure5-12)) shows that the new distribution
    is a much better match to *a*than *b* is.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05012.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-12: Three binomial distributions’ density functions plotted for comparison,
    with two coming from samples of the same distribution'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Using the Kullback–Leibler divergence, we’ve determined that *a*and *b*are different
    populations statistically. If we saw a confidence range including 104.2, we’d
    conclude that *a*and *b*likely come from the same population distribution. Statistical
    tests such as proportions tests exist to compare binomial distributions in practice,
    but some discrete distributions or sample sizes don’t have easy statistical comparisons
    (such as comparisons of predicted class distributions coming out of two convolutional
    neural network classifiers).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Comparison of Shapes
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we saw in [Chapter 4](c04.xhtml), sets of points and shapes, such as persistence
    diagrams, can be important data structures, and these objects can be measured
    and compared as well. The next three measures will deal with this situation in
    more depth. Let’s start with an example of two circles with differing radii, as
    shown in [Figure 5-13](#figure5-13).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05013r.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-13: Two circles of differing radii'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s imagine these two circles are paths in a park, and someone is walking
    their dog on a leash, with the dog following the outer path and the owner following
    the inner path. [Figure 5-14](#figure5-14) shows a visual.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05014r.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-14: A dog and owner connected by a leash walking on different paths
    at a park'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: At some points, the owner and her dog are close together, and a small leash
    suffices to connect them. However, as they move counterclockwise, the distance
    between the owner and their dog increases, necessitating more leash to connect
    them; you can see this in [Figure 5-15](#figure5-15).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05015r.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-15: A dog and owner connected by a longer leash walking on different
    paths at a park'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: One historically important metric that compares distances between points on
    different shapes is *Fréchet distance*. The version of Fréchet distance that we’ll
    consider here applies to discrete measurements (usually taken to be polygons).
    A grid graph is constructed from the polygons, and minmax paths are computed to
    find the maximum distance that two paths may be from each other. Many assumptions
    can be placed on the paths themselves and the synchronization of movement along
    those paths; the strictest requirements give rise to what’s called a *homotopic*
    Fréchet distance, which has applications in many robotics problems. We’ll return
    to homotopy applications in [Chapter 8](c08.xhtml).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: For now, in more lay terms, Fréchet distance is known as the dog-walking distance,
    and it has many uses in analytics. It can be used to measure not only the maximum
    distance between points on curves or shapes but the total distance between points
    on shapes or curves. Many R packages include functions to calculate one of the
    extant versions of Fréchet distance, including the TSdist time-series package,
    which is used in the following example. In this package, two time series are generated
    from ARMA(3, 2) distributions, with Series 3 containing 100 time points and Series
    4 containing 120 time points. Time series are important in tracking disease progression
    in groups of patients, tracking stock market changes over time, and tracking buyer
    behavior over time.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Let’s load these package-generated time series and plot them to visualize potential
    differences using the code in [Listing 5-4](#listing5-4).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Listing 5-4: A script that loads and examines two time-series datasets'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-16](#figure5-16) shows two time series with distinct highs and lows
    over time.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05016.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-16: Plots of the two example time series'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the overlap between the time series isn’t perfect, and we’d expect our
    comparisons to show some differences between these time series. With Fréchet distance,
    it’s possible to measure both the maximum/minimum deviation (maximum/minimum leash
    length) between the time series and the sum of deviations across the full comparison
    set. We’ll examine both of these for Series 3 and Series 4 by adding the following
    code to our script:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This code calculates the minimum, maximum, and sum of Fréchet distances between
    the time series, which should yield a value of 402.0 for the sum of distances
    between the time-series curves, a value of 13.7 for the maximum distance between
    points on the time series curves, and a value of 0.03 for the minimum distance
    between points on the time series curves. This suggests that the time series have
    approximately the same values at some comparison points and values that are very
    different at other points. The sum of distances between the time-series curves
    will converge to the integral taken with continuous time across the series; this
    calculation can give a good tool for calculating areas between functions using
    discrete and fairly quick approximations.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: There are other ways to compare shapes besides Fréchet distance, though, and
    these are sometimes preferable. Let’s return to our two circles again and move
    them so that they are intersecting, such as in [Figure 5-17](#figure5-17).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05017r.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-17: Plot of two intersecting circles'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: We can think of points on each circle, much like we did to measure Fréchet distance.
    Let’s consider a point on circle B and its closest point on circle A (shown in
    [Figure 5-17](#figure5-17)). Each point on circle B will have a closest point
    on circle A, and these form a collection of closest points. The points chosen
    in [Figure 5-17](#figure5-17) are a special set of points. They are the farthest
    apart of any points in our set of closest points. This means that the maximum
    we’d have to travel to hop from one circle to the other occurs with those two
    points. This distance is called the *Hausdorff distance*, and it is found in a
    lot of applications in early computer vision and image matching tasks. These days,
    it is mainly used for sequence matching, graph matching, and other discrete object
    matching tasks.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: However, one of the limitations of Hausdorff distance is that the sets being
    compared must exist in the same metric spaces. Thus, while we can compare points
    on circles, we cannot directly compare points on a circle to those on a sphere
    with Hausdorff distance or points on a Euclidean plane to points on a positively
    curved sphere. The two objects being compared must be in the same metric space.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, a solution to this conundrum exists. We can simply measure the
    farthest shortest distance from two metric spaces when those metric spaces are
    mapped to a single metric space while preserving the original distances between
    points within each space (called an *isometric embedding*). So, we could project
    the sphere and its points to tangent space to compare with other Euclidean spaces.
    Or we could embed two objects in a higher-dimensional space similar to what is
    done in kernel applications. This extension of Hausdorff distance is dubbed *Gromov–Hausdorff
    distance*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Let’s build some intuition around this metric. Say we have a triangle and a
    tetrahedron, as in [Figure 5-18](#figure5-18).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05018r.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-18: A triangle and tetrahedron, which exist in different-dimensional
    Euclidean spaces'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: One solution to this problem depicted in [Figure 5-18](#figure5-18) is to simply
    bring the triangle into three-dimensional Euclidean space and calculate the distances
    between the objects in three-dimensional Euclidean space. Perhaps part of the
    triangle overlaps with the tetrahedron when it is embedded in three-dimensional
    space, as shown in [Figure 5-19](#figure5-19).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05019r.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-19: A triangle and tetrahedron, both mapped into three-dimensional
    Euclidean space'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: It’s now possible for us to compute the farthest point sets of the closest ones
    for these two objects, likely occurring at one of the far tips of the triangle
    or tetrahedron in this example.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: R has a nice package for computing Gromov–Hausdorff distances (gromovlab), so
    we can easily implement this distance metric in R. Let’s first simulate a small
    sample from a two-dimensional disc with the code in [Listing 5-5](#listing5-5).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Listing 5-5: A script that samples from a two-dimensional disc to examine Gromov–Hausdorff
    distance'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: This should give a rough disc shape when plotted; take a look at [Figure 5-20](#figure5-20).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05020.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-20: A sample taken from a two-dimensional disc'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s add to [Listing 5-5](#listing5-5) to simulate a sample of the same
    size from the same uniform distribution that was used to generate our two-dimensional
    disc sample:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The code we’ve added samples one of the line segments to give a one-dimensional
    space. This gives us two Euclidean spaces of differing dimension. Now, we can
    compute the distances between points in each sample’s native space (two-dimensional
    Euclidean space for the disc, one-dimensional Euclidean space for the line). From
    there, we can compute the Gromov–Hausdorff distance between our samples by adding
    the following code:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This code allows us to compare distance matrices between points in the samples.
    For this random sample, the Gromov–Hausdorff distance is 5.8\. We could simulate
    a nonparametric test based on our metric as we did in [Listing 5-3](#listing5-3)
    to help us determine if the embeddings of our disc and our line are the same statistically.
    Changing the metric parameters may change the significant differences between
    embeddings or the quality of an embedding, as we saw earlier in this chapter when
    we compared Canberra, Manhattan, and Euclidean distances. Interested readers are
    encouraged to play around with the embedding parameters, set up their own nonparametric
    tests, and see how the results vary for Gromov–Hausdorff distances for our disc
    and line sample.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: The `lp` parameter allows one to use the norm-based metrics examined earlier
    in this chapter. For this particular comparison, we’ve used the Euclidean norm,
    as both samples lie in Euclidean spaces and the distance matrices ingested are
    defined by the Euclidean norm. Other norms, such as the Manhattan or Chebyshev,
    are possible and perhaps preferable for other problems, and the package is equipped
    to handle graphs and trees, as well as distance matrices. One thing to note about
    this particular package is that the algorithm searches through all possible isometric
    embeddings, so the compute time and memory needed may be large for some problems.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '*K*-Nearest Neighbors with Metric Geometry'
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Metric geometry shows up in many algorithms, including *k*-nearest neighbor
    (*k*-NN) analyses, which classify observations based on the classifications of
    objects near them. One way to understand this method is to consider a high school
    cafeteria with different cliques of students, as shown in [Figure 5-21](#figure5-21).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05021.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-21: A high school cafeteria with three distinct student cliques'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'In the cafeteria shown in [Figure 5-21](#figure5-21), three student cliques
    exist: a dark gray clique, a gray clique, and a light gray clique. Students tend
    to stay near their group of friends, as exhibited by students A, B, and C. These
    students are surrounded by their friends, and classifying them using an arbitrary
    number of students standing nearest them according to a distance metric (like
    Euclidean distance or number of floor tiles between students) would give a pretty
    accurate classification into student cliques.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: However, there are a few students, such as students D and E, who are located
    near other cliques or among all three cliques. Student E might be part of the
    popular clique (bottom center) and also part of the varsity athlete clique (top
    left), and student D might be popular, a varsity athlete, and a math team member
    (top right). Depending on how many students located near students D and E are
    considered in classifying them into a clique, they may belong to their main clique
    or be incorrectly reassigned to a new clique, of which they may fit but not consider
    their main clique. For instance, the closest 10 students may assign student E
    correctly to the popular group, while the closest 2 students would not.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Thus, *k*-NN methods rely on both a neighborhood size (in this instance, the
    number of students nearest the student of interest) and a distance metric defining
    which students are closest to the student of interest. Let’s look at little more
    closely at how distance metric can impact *k*-NN classification accuracy with
    five nearest neighbors in a simulated dataset with three variables impacting classification
    and three noise variables, given in [Listing 5-6](#listing5-6), which uses the
    knnGarden package (and includes many of the distances covered in the simulated
    data analyzed with norm-based distance metrics earlier in this chapter). You’ll
    first need to download the package ([https://cran.r-project.org/web/packages/knnGarden/index.html](https://cran.r-project.org/web/packages/knnGarden/index.html))
    and install it locally.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Listing 5-6: A script that generates and classifies a sample through *k*-NN
    classification with varying distance metric and five neighbors'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 5-6](#listing5-6) creates a sample and then runs the *k*-NN algorithm
    to classify points based on different distance metrics, including Manhattan, Euclidean,
    and Canberra distances. In this particular simulation, all of our distances yield
    similar accuracies (Euclidean distance of 81 percent, Manhattan distance of 81
    percent, and Canberra distance of 82 percent). We can consider a larger neighborhood
    by modifying [Listing 5-6](#listing5-6) to include 20 nearest neighbors.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This script modifies the functions that calculate the *k*-NN model, with the
    changes marked in bold; we changed the parameter to `K=20`. With this particular
    simulated dataset, there are dramatic differences in classification accuracy when
    20 nearest neighbors are considered. Euclidean and Manhattan distances give a
    slightly worse accuracy of 78.5 percent, and Canberra distance gives a much worse
    accuracy of 57 percent. Neighborhood size matters quite a bit in accuracy for
    Canberra distance, but it plays a lesser role for Euclidean and Manhattan distances.
    Generally speaking, using larger numbers of nearest neighbors smooths the data,
    similarly to how a weighted average might. These results suggest that, for our
    Canberra distance, adding a larger number of nearest neighbors might be smoothing
    the data too much. However, our Manhattan and Euclidean distance runs don’t show
    this smoothing effect and retain their original high performance. As our example
    shows, the choice of distance metric can matter a lot in algorithm performance—or
    it can matter little. Distance metrics thus function like other parameter choices
    in algorithm design.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '*k*-NN models are among the most closely tied to metric geometry and neighborhoods,
    though many other methods rely on distance metrics or neighborhood size. There
    are many recent papers that suggest a multiscale approach to algorithm neighborhood
    definition can improve algorithm performance, including applications in *k*-NN
    regression, deep learning image classification, and persistent graph and simplex
    algorithms (including persistent homology), and this nascent field has grown in
    recent years.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: One branch of machine learning where the choice of distance metric matters a
    lot is in dimensionality reduction, where we’re mapping a high-dimensional dataset
    to a lower-dimensional space. For instance, imagine we have a genomic dataset
    for a group of patients including 300 gene loci of interest. That’s a bit too
    much to visualize for a stakeholder on a PowerPoint slide. However, if we find
    a good mapping to two-dimensional space, we can add a scatterplot of our data
    to the slide deck in a way that is much easier for humans to process.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Manifold Learning
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many dimensionality reduction algorithms also involve distance metrics and *k*-NN
    calculations. One of the most common dimensionality reduction algorithms, *principal
    component analysis (PCA)*, helps wrangle high-dimensional data into lower-dimensional
    spaces using a linear mapping between the original high-dimensional space and
    a lower-dimensional target space. Essentially, PCA finds the ideal set of linear
    bases to account for the most variance (packing in most of the relevant information
    related to our data) with the fewest linear bases possible; this allows us to
    drop many of the data space’s bases that don’t contain much relevant information.
    This helps us visualize data that lives in more than three dimensions; it also
    decorrelates predictors being fed into a model.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: However, as noted, PCA assumes that data lives in a geometrically flat space
    and is mapped to a lower-dimensional flat space. As we’ve seen, this isn’t always
    the case, and Euclidean metrics can give different distance results than other
    distance metrics. Recently, many attempts to relax different assumptions and generalize
    dimensionality reduction to manifolds have provided a new class of dimensionality
    reduction techniques, called *manifold learning*. Manifold learning allows for
    mappings to lower-dimensional spaces that might be curved and generalizations
    of PCA to include metrics other than Euclidean distance. A *manifold* is a space
    that is locally Euclidean, with Euclidean space being one example of a manifold,
    so some people refer to *manifold learning* as an umbrella to this more general
    framework.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Using Multidimensional Scaling
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the older manifold learning algorithms is *multidimensional scaling*
    *(MDS)*. MDS considers embeddings of points into a Euclidean space such that distances
    between points, which can be Euclidean distances, are preserved as best as possible;
    this is done through the minimization of a user-defined cost function. Defining
    distances and cost functions via Euclidean distance yields the same results as
    PCA. However, there is no need to limit oneself to Euclidean distance with MDS,
    and many other metrics might be more suitable for a given problem. Let’s explore
    this further with a small dataset and different distance matrices as input to
    our MDS algorithm; take a look at the code in [Listing 5-7](#listing5-7).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Listing 5-7: A script that generates an example dataset and calculates distance
    matrices from the dataset'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have generated some data and have calculated three different distance
    metrics, let’s see how the choice of distance metric impacts MDS embeddings. Let’s
    compute the MDS embeddings and plot the results by adding to [Listing 5-7](#listing5-7).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Our addition to [Listing 5-7](#listing5-7) should give plots that look different
    from each other. In this example, the plots (shown in [Figure 5-22](#figure5-22))
    do vary dramatically depending on the metric used, suggesting that different distance
    metrics result in embeddings to different spaces.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05022.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-22: A side-by-side view of MDS results, which vary by distance metric
    chosen'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: The plot results in [Figure 5-22](#figure5-22) suggest that Minkowski distance
    yields quite different results than Euclidean or Manhattan distances; many points
    are bunched together in the Minkowski-type MDS result, which suggests it may not
    distinguish between pairs of points as well as the other metrics. However, the
    differences between Euclidean and Manhattan distance MDS results are less dramatic,
    with points spread out a lot more than in the case of our Minkowski distance.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Extending Multidimensional Scaling with Isomap
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some manifold learning algorithms extend MDS to other types of spaces and distance
    calculations. *Isomap* extends MDS by replacing the distance matrix with one of
    geodesic distances between points calculated from a neighborhood graph. This replacement
    of distance calculations with geodesic distances allows for the use of distances
    that naturally exist on spaces that are not flat, such as spheres (for instance,
    geographic information system data) or organs in a human body examined through
    MRIs. Most commonly, distances are estimated by examining a point’s nearest neighbors.
    This gives Isomap a neighborhood flavor and a way to investigate the role of scaling
    through the variance of the nearest neighbor parameter.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore this modification by adding to [Listing 5-7](#listing5-7), which
    simulated a dataset and explored MDS. We’ll use Euclidean distance as a dissimilarity
    measure, though other distance metrics can be used much as they were with MDS.
    To understand the role of neighborhood size, we’ll create neighborhoods of 5,
    10, and 20 nearest neighbors:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This snippet of code applies Isomap to the generated dataset in [Listing 5-7](#listing5-7)
    using Euclidean distance. Other distance metrics can be used and may give different
    results, as shown in the MDS analyses. The results of the Isomap analyses suggest
    that neighborhood size doesn’t play a large role in determining results for this
    dataset, as shown by the scales for each coordinate in the [Figure 5-23](#figure5-23)
    plots.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05023.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-23: A side-by-side view of Isomap results, which vary by number of
    nearest neighbors'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: MDS and Isomap aim to preserve distance between points regardless of how far
    apart the points lie on the data manifold, resulting in global preservation of
    distance. Other global manifold learning algorithms, which preserve distances
    between points that are not in the same neighborhood, exist. If you’re interested,
    you can explore global algorithms such as kernel PCA, autoencoders, and diffusion
    mapping.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Capturing Local Properties with Locally Linear Embedding
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes global properties of the manifold aren’t as important as local properties.
    In fact, from the classical definition of a manifold, local properties might sometimes
    be more interesting. For instance, when we’re looking for nearest neighbors to
    a point, points that are very far way geometrically probably won’t be nearest
    neighbors of that point, but points that are nearby could be nearest neighbors
    with information that needs to be preserved in a mapping between higher-dimensional
    and lower-dimensional spaces. Local manifold learning algorithms aim to preserve
    the local properties with less focus on preserving global properties in the mapping
    between spaces.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '*Locally linear embedding* *(LLE)* is one such local manifold learning algorithm,
    and it is one of the more often used manifold learning algorithms. Roughly speaking,
    LLE starts with a nearest neighbor graph and then proceeds to create sets of weights
    for each point given its nearest neighbors. From there, the algorithm calculates
    the mapping according to a cost function and the preservation of the nearest neighbor
    weight sets for each point. This allows it to preserve important geometric information
    in the data that exists between points near each other on the manifold.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to our code in [Listing 5-7](#listing5-7), let’s add to our code
    and explore LLE mapping to a two-dimensional space with varying numbers of neighbors.
    For this package, you’ll need to download the package ([https://mran.microsoft.com/snapshot/2016-08-05/web/packages/TDAmapper/README.html](https://mran.microsoft.com/snapshot/2016-08-05/web/packages/TDAmapper/README.html))
    and locally install it:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This piece of code applies the LLE algorithm to our dataset, varying the number
    of nearest neighbors considered in the algorithm calculations. Let’s examine the
    plots from this dataset to understand the role of nearest neighbors in this local
    algorithm ([Figure 5-24](#figure5-24)).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05024.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-24: A side-by-side view of LLE results, which vary by number of nearest
    neighbors'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [Figure 5-24](#figure5-24), neighborhood size greatly impacts LLE
    results and the spread of points in the new two-dimensional space. Given that
    the number of nearest neighbors impacts the size of the neighborhood preserved,
    higher values result in less-local versions of LLE, converting the algorithm into
    more of a global-type manifold learning algorithm. Good separation seems to occur
    at `K=20`, which is less local than `K=5` but still a fairly small neighborhood
    for a dataset with 100 points. A fully global algorithm exists if we set K to
    100, giving a two-dimensional plot with good separation and spread of points across
    the new space; you can see this in [Figure 5-25](#figure5-25).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05025.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-25: A plot of LLE results using the entire sample as nearest neighbors'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Other local manifold learning algorithms exist, and some of these allow for
    a scaling parameters like LLE’s neighborhood size. If you’re interested, you can
    explore Laplacian eigenmaps, Hessian LLE, and local tangent space analysis.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing with t-Distributed Stochastic Neighbor Embedding
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ve now seen how local algorithms can capture global properties through neighborhood
    size definition. Some manifold learning algorithms exist that explicitly capture
    both local and global properties. One of the more well-known algorithms is a visualization
    tool called *t-distributed stochastic neighbor embedding* *(t-SNE)*. The algorithm
    has two main stages: creating probability distributions over points in the high-dimensional
    space and then matching these distributions to ones in a lower-dimensional space
    by minimizing the Kullback–Leibler divergence over the two sets of distributions.
    Thus, rather than starting with a distance calculation between points, this algorithm
    focuses on matching distribution distances to find the optimal space.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of defining a neighborhood by *k*-nearest neighbors to a point, t-SNE
    defines a neighborhood by the kernel’s bandwidth over the data; this yields a
    parameter called *perplexity*, which can also be varied to understand the role
    of neighborhood size. Let’s return to the data generated in [Listing 5-7](#listing5-7)
    and see how this works in practice. Add the following code:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This piece of code runs t-SNE on the dataset generated with [Listing 5-7](#listing5-7),
    varying the perplexity parameter. The plots should produce something like [Figure
    5-26](#figure5-26), which shows more clumping in the lower-perplexity trial than
    in the trials with higher perplexity.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05026.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-26: A side-by-side plot of t-SNE results with differing perplexity
    settings'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: The plots for perplexity of 15 and 25 look fairly similar, and as we increase
    perplexity, the range of the coordinates in the lower-dimensional space drops.
    There may be projects where more spread in the data is useful for subsequent analyses
    or visualizing possible trends; other projects may yield better results with tighter
    data.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the distance metrics in this chapter pop up regularly in machine
    learning applications. Manifold learning, in particular, can involve different
    choices of metric, neighborhood size, and type of space onto which the data space
    is mapped. Many good textbooks and papers exist that cover these algorithms and
    others like them in more detail. However, we hope that you’ve gained an overview
    of dimensionality reduction methods—particularly those that are intimately connected
    to metric geometry.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on, let’s consider one final use of metric geometry.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Fractals
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another tool connected to metric geometry involves a type of self-similarity
    in geometry objects called *fractals*. Essentially, fractals have a pattern within
    the same pattern within the same pattern within the same pattern, and so on. [Figure
    5-27](#figure5-27) has an example.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05027.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-27: An example of a fractal. Note the self-similarity of the patterns
    at different scales.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Fractals occur often in natural and man-made systems. For instance, coastlines,
    blood vessels, music scales, epidemic spread in confined spaces, stock market
    behavior, and word frequency and ranking all have self-similarity properties at
    different scales. Being able to measure fractal dimension allows us to better
    understand the degree of self-similarity of these phenomena. There are many fractal
    dimension estimators these days, but most rely on measuring variations in the
    area under a fractal curve through some sort of iterative approach that compares
    neighboring point sets’ areas.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Going back to the fractal in [Figure 5-27](#figure5-27), we could consider adding
    boxes to find the area under each iterative curve and then comparing the relative
    values across scales considered, as in [Figure 5-28](#figure5-28).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05028.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-28: An example of measuring area under a series of fractal curves
    at scale'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have some intuition around fractals, let’s consider an application
    of fractal dimension metrics. Stock markets are known to exhibit some degree of
    self-similar behavior over periods of time. Understanding market volatility is
    a major aspect of investing wisely, and one method used to predict coming market
    reversal points, such as crashes, is changing self-similarity. The closing prices
    of the Dow Jones Industrial Average (one of the American stock market indices),
    or DJIA, are widely available for free download. Here, we’ll consider simulated
    daily closing prices like DJIA data from the period of June 2019 to May 2020,
    during which the COVID freefall happened. [Figure 5-29](#figure5-29) shows a chart
    of closing prices over that time period.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c05/f05029new.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-29: A plot of simulated DJIA closing prices from June 2019 to May
    2020\. Note the big drops starting in late February 2020, when COVID became a
    global issue.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: If we were predicting future market behavior, we’d want to employ fractal analyses
    with tools from time-series data analysis, which are outside the scope of this
    book. However, we can get a feel for changes in self-similarity month by month
    easily by parsing the data into monthly series and calculating each monthly series’
    fractal dimension. From there, we can examine how fractal dimension correlates
    with other measures of volatility, such as the range of closing prices within
    a month; we should see a positive correlation. [Listing 5-8](#listing5-8) loads
    the data, parses it, calculates fractal dimension, calculates closing price range,
    and runs a correlation test between fractal dimension and range.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Listing 5-8: A script that loads the simulated DJIA closing data, calculates
    fractal dimension and range of closing prices, and runs a correlation test to
    determine the relationship between fractal dimension and closing price range'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: You should find a correlation of around 0.55, or a moderate relationship between
    closing price fractal dimension and closing price range, that is around the 0.05
    significance level on the correlation test. Self-similarity does seem positively
    tied to one measure of market volatility. The fractal dimension varies by month,
    with some months’ dimensionality being close to 1 and others’ dimensionality being
    quite a bit higher. Impressively, the fractal dimension shoots up to 2 for March
    2020.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Given that we only have 12 months’ worth of data going into our test, it’s worth
    noting that we still find evidence for a positive relationship between fractal
    dimension and range of closing prices. Interested readers with their own stock
    market data are encouraged to optimize the time frame windows and potential window
    overlap chosen to calculate the series of fractal dimensions on their own data,
    as well as investigate the correlations with other geometric tools used in stock
    market change point detection, such as Forman–Ricci curvature and persistent homology.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we’ve investigated metric geometry and its application in several
    important machine learning algorithms, including the *k*-NN algorithm and several
    manifold learning algorithms. We’ve witnessed how the choice of distance metric
    (and other algorithm parameters) can dramatically impact performance. We’ve also
    examined fractals and their relationship to stock market volatility. Measuring
    distances between points and distributions crops up in many areas of machine learning
    and impacts quality of machine learning results. [Chapter 5](c05.xhtml) barely
    scratches the surface of extant tools from metric geometry. You may want to consult
    the papers referenced in the R packages used in this chapter, as well as current
    machine learning publications in distance metrics.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
