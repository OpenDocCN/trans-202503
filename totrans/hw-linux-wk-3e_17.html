<html><head></head><body>
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="401" id="Page_401"/>17</span><br/>
<span class="ChapterTitle">Virtualization</span></h1>
</header>
<figure class="opener">
<img src="image_fi/book_art/chapterart.png" alt=""/>
</figure>
<p class="ChapterIntro">The word <em>virtual</em> can be vague in computing systems. It’s used primarily to indicate an intermediary that translates a complex or fragmented underlying layer to a simplified interface that can be used by multiple consumers. Consider an example that we’ve already seen, virtual memory, which allows multiple processes to access a large bank of memory as if each had its own insulated bank of memory.</p>
<p>That definition is still a bit daunting, so it might be better to explain the typical purpose of virtualization: creating isolated environments so that you can get multiple systems to run without clashing.</p>
<p>Because virtual machines are relatively easy to understand at a higher level, that’s where we’ll start our tour of virtualization. However, the discussion will remain on that higher level, aiming to explain some of the many terms you may encounter when working with virtual machines, without getting into the vast sea of implementation specifics.</p>
<p><span epub:type="pagebreak" title="402" id="Page_402"/>We’ll go into a bit more technical detail on containers. They’re built with the technology you’ve already seen in this book, so you can see how these components can be combined. In addition, it’s relatively easy to interactively explore containers.</p>
<h2 id="h1-500402c17-0001">	17.1	Virtual Machines</h2>
<p class="BodyFirst">Virtual machines are based on the same concept as virtual memory, except with <em>all</em> of the machine’s hardware instead of just memory. In this model, you create an entirely new machine (processor, memory, I/O interfaces, and so on) with the help of software, and run a whole operating system in it—including a kernel. This type of virtual machine is more specifically called a <em>system virtual machine</em>, and it’s been around for decades. For example, IBM mainframes traditionally use system virtual machines to create a multiuser environment; in turn, users get their own virtual machine running CMS, a simple single-user operating system.</p>
<p>You can construct a virtual machine entirely in software (usually called an <em>emulator</em>) or by utilizing the underlying hardware as much as possible, as is done in virtual memory. For our purposes in Linux, we’ll look at the latter kind due to its superior performance, but note that a number of popular emulators support old computer and gaming systems, such as the Commodore 64 and Atari 2600.</p>
<p>The world of virtual machines is diverse, with a tremendous amount of terminology to wade through. Our exploration of virtual machines will focus primarily on how that terminology relates to what you might experience as a typical Linux user. We’ll also discuss some of the differences you might encounter in virtual hardware. </p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	Fortunately, using virtual machines is far simpler than describing them. For example, in VirtualBox, you can use the GUI to create and run a virtual machine or even use the command-line <var>VBoxManage</var> tool if you need to automate that process in a script. The web interfaces of cloud services also facilitate administration. Due to this ease of use, we’ll concentrate more on making sense of the technology and terminology of virtual machines than the operational details.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2-500402c17-0001">17.1.1	 Hypervisors</h3>
<p class="BodyFirst">Overseeing one or more virtual machines on a computer is a piece of software called a <em>hypervisor</em> or <em>virtual machine monitor (VMM)</em>, which works similarly to how an operating system manages processes. There are two types of hypervisors, and the way you use a virtual machine depends on the type. To most users, the <em>type 2 hypervisor</em> is the most familiar, because it runs on a normal operating system such as Linux. For example, VirtualBox is a type 2 hypervisor, and you can run it on your system without extensive modifications. You might have already used it while reading this book to test and explore different kinds of Linux systems.</p>
<p>On the other hand, a <em>type 1 hypervisor</em> is more like its own operating system (especially the kernel), built specifically to run virtual machines <span epub:type="pagebreak" title="403" id="Page_403"/>quickly and efficiently. This kind of hypervisor might occasionally employ a conventional companion system such as Linux to help with management tasks. Even though you might never run one on your own hardware, you interact with type 1 hypervisors all the time. All cloud computing services run as virtual machines under type 1 hypervisors such as Xen. When you access a website, you’re almost certainly hitting software running on such a virtual machine. Creating an instance of an operating system on a cloud service such as AWS is creating a virtual machine on a type 1 hypervisor.</p>
<p>In general, a virtual machine with its operating system is called a <em>guest</em>. The <em>host</em> is whatever runs the hypervisor. For type 2 hypervisors, the host is just your native system. For type 1 hypervisors, the host is the hypervisor itself, possibly combined with a specialized companion system.</p>
<h3 id="h2-500402c17-0002">17.1.2	 Hardware in a Virtual Machine</h3>
<p class="BodyFirst">In theory, it should be straightforward for the hypervisor to provide hardware interfaces for a guest system. For example, to provide a virtual disk device, you could create a big file somewhere on the host and provide access as a disk with standard device I/O emulation. This approach is a strict hardware virtual machine; however, it is inefficient. Making virtual machines practical for a variety of needs requires some changes.</p>
<p>Most of the differences you might encounter between real and virtual hardware are a result of a bridging that allows guests to access host resources more directly. Bypassing virtual hardware between the host and guest is known as <em>paravirtualization</em>. Network interfaces and block devices are among the most likely to receive this treatment; for example, a <em>/dev/xvd</em> device on a cloud computing instance is a Xen virtual disk, using a Linux kernel driver to talk directly to the hypervisor. Sometimes paravirtualization is used for the sake of convenience; for example, on a desktop-capable system such as VirtualBox, drivers are available to coordinate the mouse movement between the virtual machine window and the host environment.</p>
<p>Whatever the mechanism, the goal of virtualization is always to reduce the problem just enough so that the guest operating system can treat the virtual hardware as it would any other device. This ensures that all of the layers on top of the device function properly. For example, on a Linux guest system, you want a kernel to be able to access virtual disks as block devices so that you can partition and create filesystems on them with the usual tools.</p>
<h4 id="h3-500402c17-0001">Virtual Machine CPU Modes</h4>
<p class="BodyFirst">Most of the details about how virtual machines work are beyond the scope of this book, but the CPU deserves a mention because we’ve already talked about the difference between kernel mode and user mode. The specific names of these modes vary depending on the processor (for example, the x86 processors use a system called <em>privilege rings</em>), but the idea is always the same. In kernel mode, the processor can do almost anything; in user mode, some instructions are not allowed, and memory access is limited.</p>
<p><span epub:type="pagebreak" title="404" id="Page_404"/>The first virtual machines for the x86 architecture ran in user mode. This presented a problem, because the kernel running inside the virtual machine wants to be in kernel mode. To counter this, the hypervisor can detect and react to (“trap”) any restricted instructions coming from a virtual machine. With a little work, the hypervisor emulates the restricted instructions, enabling virtual machines to run in kernel mode on an architecture not designed for it. Because most of the instructions a kernel executes aren’t restricted, those run normally, and the performance impact is fairly minimal.</p>
<p>Soon after the introduction of this kind of hypervisor, processor manufacturers realized that there was a market for processors that could assist the hypervisor by eliminating the need for the instruction trap and emulation. Intel and AMD released these feature sets as VT-x and AMD-V, respectively, and most hypervisors now support them. In some cases, they are required.</p>
<p>If you want to learn more about virtual machines, start with Jim Smith and Ravi Nair’s <em>Virtual Machines: Versatile Platforms for Systems and Processes</em> (Elsevier, 2005). This also includes coverage of <em>process virtual machines</em>, such as the Java virtual machine (JVM), which we won’t discuss here.</p>
<h3 id="h2-500402c17-0003">17.1.3	 Common Uses of Virtual Machines</h3>
<p class="BodyFirst">In the Linux world, virtual machine use often falls into one of a few categories:</p>
<ol class="none">
<li><span class="RunInHead">Testing and trials</span>  There are many use cases for virtual machines when you need to try something outside of a normal or production operating environment. For example, when you’re developing production software, it’s essential to test software in a machine separate from the developer’s. Another use is to experiment with new software, such as a new distribution, in a safe and “disposable” environment. Virtual machines allow you to do this without having to purchase new hardware. </li>
<li><span class="RunInHead">Application compatibility</span>  When you need to run something under an operating system that differs from your normal one, virtual machines are essential.</li>
<li><span class="RunInHead">Servers and cloud services</span>  As mentioned earlier, all cloud services are built on virtual machine technology. If you need to run an internet server, such as a web server, the quickest way to do so is to pay a cloud provider for a virtual machine instance. Cloud providers also offer specialized servers, such as databases, which are just preconfigured software sets running on virtual machines.</li>
</ol>
<h3 id="h2-500402c17-0004">17.1.4	 Drawbacks of Virtual Machines</h3>
<p class="BodyFirst">For many years, virtual machines have been the go-to method of isolating and scaling services. Because you can create virtual machines through a <span epub:type="pagebreak" title="405" id="Page_405"/>few clicks or an API, it’s very convenient to create servers without having to install and maintain hardware. That said, some aspects remain troublesome in day-to-day operation:</p>
<ul>
<li><em>It can be cumbersome and time-consuming to install and/or configure the system and application</em>. Tools such as Ansible can automate this process, but it still takes a significant amount of time to bring up a system from scratch. If you’re using virtual machines to test software, you can expect this time to accumulate quickly.</li>
<li><em>Even when configured properly, virtual machines start and reboot relatively slowly</em>. There are a few ways around this, but you’re still booting a full Linux system.</li>
<li><em>You have to maintain a full Linux system, keeping current with updates and security on each virtual machine</em>. These systems have systemd and sshd, as well as any tools on which your application depends.</li>
<li><em>Your application might have some conflicts with the standard software set on a virtual machine</em>. Some applications have strange dependencies, and they don’t always get along well with the software found on a production machine. In addition, dependencies like libraries can change with an upgrade in the machine, breaking things that once worked.</li>
<li><em>Isolating your services on separate virtual machines can be wasteful and costly</em>. The standard industry practice is to run no more than one application service on a system, which is robust and easier to maintain. In addition, some services can be further segmented; if you run multiple websites, it’s preferable to keep them on different servers. However, this is at odds with keeping costs down, especially when you’re using cloud services, which charge per virtual machine instance.</li>
</ul>
<p>These problems are really no different from the ones you’d encounter running services on real hardware, and they aren’t necessarily impediments in small operations. However, once you start running more services, they’ll become more noticeable, costing time and money. This is when you might consider containers for your services.</p>
<h2 id="h1-500402c17-0002">	17.2	Containers</h2>
<p class="BodyFirst">Virtual machines are great for insulating an entire operating system and its set of running applications, but sometimes you need a lighter-weight alternative. Container technology is now a popular way to fulfill this need. Before we go into the details, let’s take a step back to see its evolution.</p>
<p>The traditional way of operating computer networks was to run multiple services on the same physical machine; for example, a name server could also act as an email server and perform other tasks. However, you shouldn’t really trust any software, including servers, to be secure or stable. To enhance the security of the system and to keep services from interfering with one another, there are some basic ways to put up barriers around server daemons, especially when you don’t trust one of them very much.</p>
<p><span epub:type="pagebreak" title="406" id="Page_406"/>One method of service isolation is using the <code>chroot()</code> system call to change the root directory to something other than the actual system root. A program can change its root to something like <em>/var/spool/my_service</em> and no longer be able to access anything outside that directory. In fact, there is a <code>chroot</code> program that allows you to run a program with a new root directory. This type of isolation is sometimes called a <em>chroot jail</em> because processes can’t (normally) escape it.</p>
<p>Another type of restriction is the resource limit (rlimit) feature of the kernel, which restricts how much CPU time a process can consume or how big its files can be.</p>
<p>These are the ideas that containers are built on: you’re altering the environment and restricting the resources with which processes run. Although there’s no single defining feature, a <em>container</em> can be loosely defined as a restricted runtime environment for a set of processes, the implication being that those processes can’t touch anything on the system outside that environment. In general, this is called <em>operating system–level virtualization</em>.</p>
<p>It’s important to keep in mind that a machine running one or more containers still has only one underlying Linux kernel. However, the processes inside a container can use the user-space environment from a Linux distribution different than the underlying system. </p>
<p>The restrictions in containers are built with a number of kernel features. Some of the important aspects of processes running in a container are:</p>
<ul>
<li>They have their own cgroups.</li>
<li>They have their own devices and filesystem.</li>
<li>They cannot see or interact with any other processes on the system.</li>
<li>They have their own network interfaces.</li>
</ul>
<p>Pulling all of those things together is a complicated task. It’s possible to alter everything manually, but it can be challenging; just getting a handle on the cgroups for a process is tricky. To help you along, many tools can perform the necessary subtasks of creating and managing effective containers. Two of the most popular are Docker and LXC. This chapter focuses on Docker, but we’ll also touch on LXC to see how it differs.</p>
<h3 id="h2-500402c17-0005">17.2.1	 Docker, Podman, and Privileges</h3>
<p class="BodyFirst">To run the examples in this book, you need a container tool. The examples here are built with Docker, which you can normally install with a distribution package without any trouble.</p>
<p>There is an alternative to Docker called Podman. The primary difference between the two tools is that Docker requires a server to be running when using containers, while Podman does not. This affects the way the two systems set up containers. Most Docker configurations require superuser privileges to access the kernel features used by its containers, and the <span epub:type="pagebreak" title="407" id="Page_407"/>dockerd daemon does the relevant work. In contrast, you can run Podman as a normal user, called <em>rootless</em> operation. When run this way, it uses different techniques to achieve isolation.</p>
<p>You can also run Podman as the superuser, causing it to switch over to some of the isolation techniques that Docker uses. Conversely, newer versions of dockerd support a rootless mode.</p>
<p>Fortunately, Podman is command line–compatible with Docker. This means you can substitute <code>podman</code> for <code>docker</code> in the examples here, and they’ll still work. However, there are differences in the implementations, especially when you’re running Podman in rootless mode, so those will be noted where applicable.</p>
<h3 id="h2-500402c17-0006">17.2.2	 A Docker Example</h3>
<p class="BodyFirst">The easiest way to familiarize yourself with containers is to get hands-on. The Docker example here illustrates the principal features that make containers work, but providing an in-depth user manual is beyond the scope of this book. You should have no trouble understanding the online documentation after reading this, and if you’re looking for an extensive guide, try Nigel Poulton’s <em>Docker Deep Dive </em>(author, 2016).</p>
<p>First you need to create an <em>image</em>, which comprises the filesystem and a few other defining features for a container to run with. Your images will nearly always be based on prebuilt ones downloaded from a repository on the internet.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	It’s easy to confuse images and containers. You can think of an image as the container’s filesystem; processes don’t run in an image, but they do run in containers. This is not quite accurate (in particular, when you change the files in a Docker container, you aren’t making changes to the image), but it’s close enough for now.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Install Docker on your system (your distribution’s add-on package is probably fine), make a new directory somewhere, change to that directory, and create a file called <em>Dockerfile</em> containing these lines:</p>
<pre><code>FROM alpine:latest
RUN apk add bash
CMD ["/bin/bash"]</code></pre>
<p>This configuration uses the lightweight Alpine distribution. The only change we’re making is adding the bash shell, which we’re doing not just for an added measure of interactive usability but also to create a unique image and see how that procedure works. It’s possible (and common) to use public images and make no changes to them whatsoever. In that case, you don’t need a Dockerfile.</p>
<p>Build the image with the following command, which reads the Dockerfile in the current directory and applies the identifier <code>hlw_test</code> to the image:</p>
<pre><code>$ <b>docker build -t hlw_test .</b></code></pre>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<span epub:type="pagebreak" title="408" id="Page_408"/><h2><span class="NoteHead">NOTE</span></h2>
<p>	You might need to add yourself to the <em>docker</em> group on your system to be able to run Docker commands as a regular user.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Be prepared for a lot of output. Don’t ignore it; reading through it this first time will help you understand how Docker works. Let’s break it up into the steps that correspond to the lines of the Dockerfile. The first task is to retrieve the latest version of the Alpine distribution container from the Docker registry:</p>
<pre><code>Sending build context to Docker daemon  2.048kB
Step 1/3 : FROM alpine:latest
latest: Pulling from library/alpine
cbdbe7a5bc2a: Pull complete 
Digest: sha256:9a839e63dad54c3a6d1834e29692c8492d93f90c59c978c1ed79109ea4b9a54
Status: Downloaded newer image for alpine:latest
 ---&gt; f70734b6a266</code></pre>
<p>Notice the heavy use of SHA256 digests and shorter identifiers. Get used to them; Docker needs to track many little pieces. In this step, Docker has created a new image with the identifier <code>f70734b6a266</code> for the basic Alpine distribution image. You can refer to that specific image later, but you probably won’t need to, because it’s not the final image. Docker will build more on top of it later. An image that isn’t intended to be a final product is called an <em>intermediate image</em>.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	The output is different when you’re using Podman, but the steps are the same.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>The next part of our configuration is the bash shell package installation in Alpine. As you read the following, you’ll probably recognize output that results from the <code>apk add bash</code> command (shown in bold):</p>
<pre><code>Step 2/3 : RUN apk add bash
 ---&gt; Running in 4f0fb4632b31
<code class="bold">fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/main/x86_64/APKINDEX.tar.gz</code>
<code class="bold">fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/community/x86_64/APKINDEX.tar.gz</code>
<code class="bold">(1/4) Installing ncurses-terminfo-base (6.1_p20200118-r4)</code>
<code class="bold">(2/4) Installing ncurses-libs (6.1_p20200118-r4)</code>
<code class="bold">(3/4) Installing readline (8.0.1-r0)</code>
<b>(4/4) Installing bash (5.0.11-r1)</b>
<b>Executing bash-5.0.11-r1.post-install</b>
<b>Executing busybox-1.31.1-r9.trigger</b>
<b>OK: 8 MiB in 18 packages</b>
Removing intermediate container 4f0fb4632b31
 ---&gt; 12ef4043c80a</code></pre>
<p>What’s not so obvious is <em>how</em> that’s happening. When you think about it, you probably aren’t running Alpine on your own machine here. So how can you run the <code>apk</code> command that belongs to Alpine already?</p>
<p><span epub:type="pagebreak" title="409" id="Page_409"/>The key is the line that says <code>Running in 4f0fb4632b31</code>. You haven’t asked for a container yet, but Docker has set up a new container with the intermediate Alpine image from the previous step. Containers have identifiers as well; unfortunately, they look no different from image identifiers. To add to the confusion, Docker calls the temporary container an <em>intermediate container</em>, which differs from an intermediate image. Intermediate images stay around after a build; intermediate containers do not.</p>
<p>After setting up the (temporary) container with ID <code>4f0fb4632b31</code>, Docker ran the <code>apk</code> command inside that container to install bash, and then saved the resulting changes to the filesystem into a new intermediate image with the ID <code>12ef4043c80a</code>. Notice that Docker also removes the container after completion.</p>
<p>Finally, Docker makes the final changes required to run a bash shell when starting a container from the new image:</p>
<pre><code>Step 3/3 : CMD ["/bin/bash"]
 ---&gt; Running in fb082e6a0728
Removing intermediate container fb082e6a0728
 ---&gt; 1b64f94e5a54
Successfully built 1b64f94e5a54
Successfully tagged hlw_test:latest</code></pre>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	Anything done with the <var>RUN</var> command in a Dockerfile happens during the image build, not afterward, when you start a container with the image. The <var>CMD</var> command is for the container runtime; this is why it occurs at the end.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>In this example, you now have a final image with the ID <code>1b64f94e5a54</code>, but because you tagged it (in two separate steps), you can also refer to it as <code>hlw_test</code> or <code>hlw_test:latest</code>. Run <code>docker images</code> to verify that your image and the Alpine image are present:</p>
<pre><code>$ <b>docker images</b>
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
hlw_test            latest              1b64f94e5a54        1 minute ago        9.19MB
alpine              latest              f70734b6a266        3 weeks ago         5.61MB</code></pre>
<h4 id="h3-500402c17-0002">Running Docker Containers</h4>
<p class="BodyFirst">You’re now ready to start a container. There are two basic ways to run something in a container with Docker: you can either create the container and then run something inside it (in two separate steps), or you can simply create and run in one step. Let’s jump right into it and start one with the image that you just built:</p>
<pre><code>$ <b>docker run -it hlw_test</b></code></pre>
<p>You should get a bash shell prompt where you can run commands in the container. That shell will run as the root user.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<span epub:type="pagebreak" title="410" id="Page_410"/><h2><span class="NoteHead">NOTE</span></h2>
<p>	If you forget the <var>-it</var> options (interactive, connect a terminal), you won’t get a prompt, and the container will terminate almost immediately. These options are somewhat unusual in everyday use (especially <var>-t</var>).</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>If you’re the curious type, you’ll probably want to take a look around the container. Run some commands, such as <code>mount</code> and <code>ps</code>, and explore the filesystem in general. You’ll quickly notice that although most things look like a typical Linux system, others do not. For example, if you run a complete process listing, you’ll get just two entries:</p>
<pre><code># <b>ps aux</b>
PID   USER     TIME  COMMAND
    1 root      0:00 /bin/bash
    6 root      0:00 ps aux</code></pre>
<p>Somehow, in the container, the shell is process ID 1 (remember, on a normal system, this is init), and nothing else is running except for the process listing that you’re executing.</p>
<p>At this point, it’s important to remember that these processes are simply ones that you can see on your normal (host) system. If you open another shell window on your host system, you can find a container process in a listing, though it will require a little searching. It should look like this:</p>
<pre><code>root     20189  0.2  0.0   2408  2104 pts/0    Ss+  08:36   0:00 /bin/bash</code></pre>
<p>This is our first encounter with one of the kernel features used for containers: Linux kernel <em>namespaces</em> specifically for process IDs. A process can create a whole new set of process IDs for itself and its children, starting at PID 1, and then they are able to see only those.</p>
<h4 id="h3-500402c17-0003">Overlay Filesystems</h4>
<p class="BodyFirst">Next, explore the filesystem in your container. You’ll find it’s somewhat minimal; this is because it’s based on the Alpine distribution. We’re using Alpine not just because it’s small, but also because it’s likely to be different from what you’re used to. However, when you take a look at the way the root filesystem is mounted, you’ll see it’s very different from a normal device-based mount:</p>
<pre><code>overlay on / type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/
C3D66CQYRP4SCXWFFY6HHF6X5Z:/var/lib/docker/overlay2/l/K4BLIOMNRROX3SS5GFPB
7SFISL:/var/lib/docker/overlay2/l/2MKIOXW5SUB2YDOUBNH4G4Y7KF<span class="CodeAnnotation" aria-label="annotation1">1</span>,upperdir=/
var/lib/docker/overlay2/d064be6692c0c6ff4a45ba9a7a02f70e2cf5810a15bcb2b728b00
dc5b7d0888c/diff,workdir=/var/lib/docker/overlay2/d064be6692c0c6ff4a45ba9a7a02
f70e2cf5810a15bcb2b728b00dc5b7d0888c/work)</code></pre>
<p>This is an <em>overlay filesystem</em>, a kernel feature that allows you to create a filesystem by combining existing directories as layers, with changes stored <span epub:type="pagebreak" title="411" id="Page_411"/>in a single spot. If you look on your host system, you’ll see it (and have access to the component directories), and you’ll also find where Docker attached the original mount.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	In rootless mode, Podman uses the FUSE version of the overlay filesystem. In this case, you won’t see this detailed information from the filesystem mounts, but you can get similar information by examining the fuse-overlayfs processes on the host system.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>In the mount output, you’ll see the <code>lowerdir</code>, <code>upperdir</code>, and <code>workdir</code> directory parameters. The lower directory is actually a colon-separated series of directories, and if you look them up on your host system, you’ll find that the last one <span class="CodeAnnotation" aria-label="annotation1">1</span> is the base Alpine distribution that was set up in the first step of the image build (just look inside; you’ll see the distribution root directory). If you follow the two preceding directories, you’ll see they correspond to the other two build steps. Therefore, these directories “stack” on top of each other in order from right to left.</p>
<p>The upper directory goes on top of those, and it’s also where any changes to the mounted filesystem appear. It doesn’t have to be empty when you mount it, but for containers, it doesn’t make much sense to put anything there to start. The work directory is a place for the filesystem driver to do its work before writing changes to the upper directory, and it must be empty upon mount.</p>
<p>As you can imagine, container images with many build steps have quite a few layers. This is sometimes a problem, and there are various strategies to minimize the number of layers, such as combining <code>RUN</code> commands and multistage builds. We won’t go into details about those here.</p>
<h4 id="h3-500402c17-0004">Networking</h4>
<p class="BodyFirst">Although you can choose to have a container run in the same network as the host machine, you normally want some kind of isolation in the network stack for safety. There are several ways to achieve this in Docker, but the default (and most common) is called a bridge network, using another kind of namespace—the network namespace (netns). Before running anything, Docker creates a new network interface (usually <em>docker0</em>) on the host system, typically assigned to a private network such as 172.17.0.0/16, so the interface in this case would be assigned to 172.17.0.1. This network is for communication between the host machine and its containers.</p>
<p>Then, when creating a container, Docker creates a new network namespace, which is almost completely empty. At first, the new namespace (which will be the one in the container) contains only a new, private loopback (<em>lo</em>) interface. To prepare the namespace for actual use, Docker creates a <em>virtual interface</em> on the host, which simulates a link between two actual network interfaces (each with its own device) and places one of those devices in the new namespace. With a network configuration using an address on the Docker network (172.17.0.0/16 in our case) on the device in the new namespace, processes can send packets on that network and be received on <span epub:type="pagebreak" title="412" id="Page_412"/>the host. This can be confusing, because different interfaces in different namespaces can have the same name (for example, the container’s can be <em>eth0</em>, as well as the host machine).</p>
<p>Because this uses a private network (and a network administrator probably wouldn’t want to route anything to and from these containers blindly), if left this way, the container processes using that namespace couldn’t reach the outside world. To make it possible to reach outside hosts, the Docker network on the host configures NAT.</p>
<p><a href="#figure17-1" id="figureanchor17-1">Figure 17-1</a> shows a typical setup. It includes the physical layer with the interfaces, as well as the internet layer of the Docker subnet and the NAT linking this subnet to the rest of the host machine and its outside connections.</p>
<figure>
<img src="image_fi/500402c17/f17001.png" alt="f17001"/>
<figcaption><p><a id="figure17-1">Figure 17-1</a>: Bridge network in Docker. The thick link represents the virtual interface pair bond.</p></figcaption></figure>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	You might need to examine the subnet of your Docker interface network. There can sometimes be clashes between it and the NAT-based network assigned by router hardware from telecommunications companies.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Rootless operation networking in Podman is different because setting up virtual interfaces requires superuser access. Podman still uses a new network namespace, but it needs an interface that can be set up to operate in user space. This is a TAP interface (usually at <em>tap0</em>), and in conjunction with a forwarding daemon called slirp4netns, container processes can reach the outside world. This is less capable; for example, containers cannot connect to one another.</p>
<p>There’s a lot more to networking, including how to expose ports in the container’s network stack for external services to use, but the network topology is the most important thing to understand.</p>
<h4 id="h3-500402c17-0005">Docker Operation</h4>
<p class="BodyFirst">At this point, we could continue with a discussion of the various other kinds of isolation and restrictions that Docker enables, but it would take a long <span epub:type="pagebreak" title="413" id="Page_413"/>time and you probably get the point by now. Containers don’t come from one particular feature, but rather a collection of them. A consequence is that Docker must keep track of all of the things we do when creating a container and must also be able to clean them up.</p>
<p>Docker defines a container as “running” as long as it has a process running. You can show the currently running containers with <code>docker ps</code>:</p>
<pre><code>$ <b>docker ps</b>
CONTAINER ID   IMAGE       COMMAND       CREATED       STATUS      PORTS    NAMES
bda6204cecf7   hlw_test    "/bin/bash"   8 hours ago   Up 8 hours           boring_lovelace
8a48d6e85efe   hlw_test    "/bin/bash"   20 hours ago  Up 20 hours          awesome_elion</code></pre>
<p>As soon as all of its processes terminate, Docker puts them in an exit state, but it still keeps the containers (unless you start with the <code>--rm</code> option). This includes the changes made to the filesystem. You can easily access the filesystem with <code>docker export</code>.</p>
<p>You need to be aware of this, because <code>docker ps</code> doesn’t show exited containers by default; you have to use the <code>-a</code> option to see everything. It’s really easy to accumulate a large pile of exited containers, and if the application running in the container creates a lot of data, you can run out of disk space and not know why. Use <code>docker rm</code> to remove a terminated container.</p>
<p>This also applies to old images. Developing an image tends to be a repetitive process, and when you tag an image with the same tag as an existing image, Docker doesn’t remove the original image. The old image simply loses that tag. If you run <code>docker images</code> to show all the images on your system, you can see all of the images. Here’s an example showing a previous version of an image without a tag:</p>
<pre><code>$ <b>docker images</b>
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
hlw_test            latest              1b64f94e5a54        43 hours ago        9.19MB
&lt;none&gt;              &lt;none&gt;              d0461f65b379        46 hours ago        9.19MB
alpine              latest              f70734b6a266        4 weeks ago         5.61MB</code></pre>
<p>Use <code>docker rmi</code> to remove an image. This also removes any unnecessary intermediate images that the image builds on. If you don’t remove images, they can add up over time. Depending on what’s in the images and how they are built, this can consume a significant amount of storage space on your system.</p>
<p>In general, Docker does a lot of meticulous versioning and checkpointing. This layer of management reflects a particular philosophy compared to tools like LXC, which you’ll see soon.</p>
<h4 id="h3-500402c17-0006">Docker Service Process Models</h4>
<p class="BodyFirst">One potentially confusing aspect of Docker containers is the lifecycle of the processes inside them. Before a process can completely terminate, its parent is supposed to collect (“reap”) its exit code with the <code>wait()</code> system call. However, in a container, there are some situations in which dead processes can remain because their parents don’t know how to react. Along with the <span epub:type="pagebreak" title="414" id="Page_414"/>way that many images are configured, this might lead you to conclude that you’re not supposed to run multiple processes or services inside a Docker container. This is not correct.</p>
<p>You can have many processes in a container. The shell we ran in our example starts a new child process when you run a command. The only thing that really matters is that when you have child processes, the parent cleans up upon their exit. Most parents do this, but in certain circumstances, you might run into a situation where one does not, especially if it doesn’t know that it has children. This can happen when there are multiple levels of process spawning, and the PID 1 inside the container ends up being the parent of a child that it doesn’t know about.</p>
<p>To remedy this, if you have a simple single-minded service that just spawns some processes and seems to leave lingering processes even when a container is supposed to terminate, you can add the <code>--init</code> option to <code>docker run</code>. This creates a very simple init process to run as PID 1 in the container and act as a parent that knows what to do when a child process terminates.</p>
<p>However, if you’re running multiple services or tasks inside a container (such as multiple workers for some job server), instead of starting them with a script, you might consider using a process management daemon such as Supervisor (supervisord) to start and monitor them. This not only provides the necessary system functionality, but also gives you more control over service processes.</p>
<p>On that note, if you’re thinking about this kind of model for a container, there’s a different option that you might consider, and it doesn’t involve Docker.</p>
<h3 id="h2-500402c17-0007">17.2.3	 LXC</h3>
<p class="BodyFirst">Our discussion has revolved around Docker not only because it’s the most popular system for building container images, but also because it makes it very easy to get started and jump into the layers of isolation that containers normally provide. However, there are other packages for creating containers, and they take different approaches. Of these, LXC is one of the oldest. In fact, the first versions of Docker were built on LXC. If you understood the discussion of how Docker does its work, you won’t have trouble with LXC technical concepts, so we won’t go over any examples. Instead, we’ll just explore some of the practical differences. </p>
<p>The term <em>LXC</em> is sometimes used to refer to the set of kernel features that make containers possible, but most people use it to refer specifically to a library and package containing a number of utilities for creating and manipulating Linux containers. Unlike Docker, LXC involves a fair amount of manual setup. For example, you have to create your own container network interface, and you need to provide user ID mappings.</p>
<p>Originally, LXC was intended to be as much of an entire Linux system as possible inside the container—init and all. After installing a special version of a distribution, you could install everything you needed for whatever you were running inside the container. That part isn’t too different from what you’ve seen with Docker, but there is more setup to do; with Docker, you just download a bunch of files and you’re ready to go.</p>
<p><span epub:type="pagebreak" title="415" id="Page_415"/>Therefore, you might find LXC more flexible in adapting to different needs. For example, by default, LXC doesn’t use the overlay filesystem that you saw with Docker, although you can add one. Because LXC is built on a C API, you can use this granularity in your own software application if necessary.</p>
<p>An accompanying management package called LXD can help you work through some of LXC’s finer, manual points (such as network creation and image management) and offers a REST API that you can use to access LXC instead of the C API.</p>
<h3 id="h2-500402c17-0008">17.2.4	 Kubernetes</h3>
<p class="BodyFirst">Speaking of management, containers have become popular for many kinds of web servers, because you can start a bunch of containers from a single image across multiple machines, providing excellent redundancy. Unfortunately, this can be difficult to manage. You need to perform tasks such as the following:</p>
<ul>
<li>Track which machines are able to run containers.</li>
<li>Start, monitor, and restart containers on those machines.</li>
<li>Configure container startup.</li>
<li>Configure the container networking as required.</li>
<li>Load new versions of container images and update all running containers gracefully.</li>
</ul>
<p>That isn’t a complete list, nor does it properly convey the complexity of each task. Software was begging to be developed for it, and among the solutions that appeared, Google’s Kubernetes has become dominant. Perhaps one of the largest contributing factors for this is its ability to run Docker container images.</p>
<p>Kubernetes has two basic sides, much like any client-server application. The server involves the machine(s) available to run containers, and the client is primarily a set of command-line utilities that launch and manipulate sets of containers. The configuration files for containers (and the groups they form) can be extensive, and you’ll quickly find that most of the work involved on the client side is creating the appropriate configuration.</p>
<p>You can explore the configuration on your own. If you don’t want to deal with setting up the servers yourself, use the Minikube tool to install a virtual machine running a Kubernetes cluster on your own machine.</p>
<h3 id="h2-500402c17-0009">17.2.5	 Pitfalls of Containers</h3>
<p class="BodyFirst">If you think about how a service like Kubernetes works, you’ll also realize that a system utilizing containers is not without its costs. At minimum, you still need one or more machines on which to run your containers, and this has to be a full-fledged Linux machine, whether it’s on real hardware or a virtual machine. There’s still a maintenance cost here, although it might be simpler to maintain this core infrastructure than a configuration that requires many custom software installations.</p>
<p><span epub:type="pagebreak" title="416" id="Page_416"/>That cost can take several forms. If you choose to administer your own infrastructure, that’s a significant investment of time, and still has hardware, hosting, and maintenance costs. If you instead opt to use a container service like a Kubernetes cluster, you’ll be paying the monetary cost of having someone else do the work for you.</p>
<p>When thinking of the containers themselves, keep in mind the following:</p>
<ul>
<li><em>Containers can be wasteful in terms of storage</em>. In order for any application to function inside a container, the container must include all the necessary support of a Linux operating system, such as shared libraries. This can become quite large, especially if you don’t pay particular attention to the base distribution that you choose for your containers. Then, consider your application itself: how big is it? This situation is mitigated somewhat when you’re using an overlay filesystem with several copies of the same container, because they share the same base files. However, if your application creates a lot of runtime data, the upper layers of all of those overlays can grow large.</li>
<li><em>You still have to think about other system resources, such as CPU time</em>. You can configure limits on how much containers can consume, but you’re still constrained by how much the underlying system can handle. There’s still a kernel and block devices. If you overload stuff, then your containers, the system underneath, or both will suffer.</li>
<li><em>You might need to think differently about where you store your data</em>. In container systems such as Docker that use overlay filesystems, the changes made to the filesystem during runtime are thrown away after the processes terminate. In many applications, all of the user data goes into a database, and then that problem is reduced to database administration. But what about your logs? Those are necessary for a well-functioning server application, and you still need a way to store them. A separate log service is a must for any substantial scale of production.</li>
<li><em>Most container tools and operation models are geared toward web servers</em>. If you’re running a typical web server, you’ll find a great deal of support and information about running web servers in containers. Kubernetes, in particular, has a lot of safety features for preventing runaway server code. This can be an advantage, because it compensates for how (frankly) poorly written most web applications are. However, when you’re trying to run another kind of service, it can sometimes feel like you’re trying to drive a square peg into a round hole.</li>
<li><em>Careless container builds can lead to bloat, configuration problems, and malfunction</em>. The fact that you’re creating an isolated environment doesn’t shield you from making mistakes in that environment. You might not have to worry so much about the intricacies of systemd, but plenty of other things still can go wrong. When problems arise in any kind of system, inexperienced users tend to add things in an attempt to make the problem go away, often haphazardly. This can continue (often blindly) until at last there’s a somewhat functional system—with many additional issues. You need to understand the changes you make.</li>
<li><span epub:type="pagebreak" title="417" id="Page_417"/><em>Versioning can be problematic</em>. We used the <code>latest</code> tag for the examples in this book. This is supposed to be the latest (stable) release of a container, but it also means that when you build a container based on the latest release of a distribution or package, something underneath can change and break your application. One standard practice is to use a specific version tag of a base container.</li>
<li><em>Trust can be an issue</em>. This applies particularly to images built with Docker. When you base your containers on those in the Docker image repository, you’re placing trust in an additional layer of management that they haven’t been altered to introduce even more security problems than usual, and that they’ll be there when you need them. This contrasts with LXC, where you’re encouraged to build your own to a certain degree.</li>
</ul>
<p>When considering these issues, you might think that containers have a lot of disadvantages compared to other ways of managing system environments. However, that’s not the case. No matter what approach you choose, these problems are present in some degree and form—and some of them are easier to manage in containers. Just remember that containers won’t solve every problem. For example, if your application takes a long time to start on a normal system (after booting), it will also start slowly in a container.</p>
<h2 id="h1-500402c17-0003">	17.3	Runtime-Based Virtualization</h2>
<p class="BodyFirst">A final kind of virtualization to mention is based on the type of environment used to develop an application. This differs from the system virtual machines and containers that we’ve seen so far, because it doesn’t use the idea of placing applications onto different machines. Instead, it’s a separation that applies only to a particular application.</p>
<p>The reason for these kinds of environments is that multiple applications on the same system can use the same programming language, causing potential conflicts. For example, Python is used in several places on a typical distribution and can include many add-on packages. If you want to use the system’s version of Python in your own package, you can run into trouble if you want a different version of one of the add-ons.</p>
<p>Let’s look at how Python’s virtual environment feature creates a version of Python with only the packages that you want. The way to start is by creating a new directory for the environment like this:</p>
<pre><code>$ <b>python3 -m venv test-venv</b></code></pre>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	By the time you read this, you might simply be able to type <var>python</var> instead of <var>python3</var>.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Now, look inside the new <em>test-venv</em> directory. You’ll see a number of system-like directories such as <em>bin</em>, <em>include</em>, and <em>lib</em>. To activate the virtual environment, you need to source (not execute) the <code>test-venv/bin/activate</code> script:</p>
<pre><code>$ <b>. test-env/bin/activate</b></code></pre>
<p><span epub:type="pagebreak" title="418" id="Page_418"/>The reason for sourcing the execution is that activation is essentially setting an environment variable, which you can’t do by running an executable. At this point, when you run Python, you get the version in <em>test-venv/bin</em> directory (which is itself only a symbolic link), and the <code>VIRTUAL_ENV</code> environment variable is set to the environment base directory. You can run <code>deactivate</code> to exit to the virtual environment.</p>
<p>It isn’t any more complicated than that. With this environment variable set, you get a new, empty packages library in <em>test-venv/lib</em>, and anything new you install when in the environment goes there instead of in the main system’s library.</p>
<p>Not all programming languages allow virtual environments in the way Python does, but it’s worth knowing about it, if for no other reason than to clear up some confusion about the word <em>virtual</em>.</p>
</section>


<section>
<header><h1 class="BackmatterTitle"><span epub:type="pagebreak" title="419" id="Page_419"/>Bibliography</h1></header>
<p class="Reference">Abrahams, Paul W., and Bruce Larson, <em>UNIX for the Impatient</em>, 2nd ed. Boston: Addison-Wesley Professional, 1995.</p>
<p class="Reference">Aho, Alfred V., Brian W. Kernighan, and Peter J. Weinberger, <em>The AWK Programming Language</em>. Boston: Addison-Wesley, 1988.</p>
<p class="Reference">Aho, Alfred V., Monica S. Lam, Ravi Sethi, and Jeffery D. Ullman, <em>Compilers: Principles, Techniques, and Tools</em>, 2nd ed. Boston: Addison-Wesley, 2006.</p>
<p class="Reference">Aumasson, Jean-Philippe, <em>Serious Cryptography: A Practical Introduction to Modern Encryption</em>. San Francisco: No Starch Press, 2017.</p>
<p class="Reference">Barrett, Daniel J., Richard E. Silverman, and Robert G. Byrnes, <em>SSH, The Secure Shell: The Definitive Guide</em>, 2nd ed. Sebastopol, CA: O’Reilly, 2005.</p>
<p class="Reference">Beazley, David M., <em>Python Distilled</em>. Addison-Wesley, 2021.</p>
<p class="Reference">Beazley, David M., Brian D. Ward, and Ian R. Cooke, “The Inside Story on Shared Libraries and Dynamic Loading.” <em>Computing in Science &amp; Engineering </em>3, no. 5 (September/October 2001): 90–97.</p>
<p class="Reference">Calcote, John, <em>Autotools: A Practitioner’s Guide to GNU Autoconf, Automake, and Libtool</em>, 2nd ed. San Francisco: No Starch Press, 2019.</p>
<p class="Reference">Carter, Gerald, Jay Ts, and Robert Eckstein, <em>Using Samba: A File and Print Server for Linux, Unix, and Mac OS X</em>, 3rd ed. Sebastopol, CA: O’Reilly, 2007.</p>
<p class="Reference"><span epub:type="pagebreak" title="420" id="Page_420"/>Christiansen, Tom, brian d foy, Larry Wall, and Jon Orwant, <em>Programming Perl: Unmatched Power for Processing and Scripting</em>, 4th ed. Sebastopol, CA: O’Reilly, 2012.</p>
<p class="Reference">chromatic, <em>Modern Perl</em>, 4th ed. Hillsboro, OR: Onyx Neon Press, 2016.</p>
<p class="Reference">Davies, Joshua. <em>Implementing SSL/TLS Using Cryptography and PKI</em>. Hoboken, NJ: Wiley, 2011.</p>
<p class="Reference">Friedl, Jeffrey E. F., <em>Mastering Regular Expressions</em>, 3rd ed. Sebastopol, CA: O’Reilly, 2006.</p>
<p class="Reference">Gregg, Brendan, <em>Systems Performance: Enterprise and the Cloud</em>, 2nd ed. Boston: Addison-Wesley, 2020.</p>
<p class="Reference">Grune, Dick, Kees van Reeuwijk, Henri E. Bal, Ceriel J. H. Jacobs, and Koen Langendoen, <em>Modern Compiler Design</em>, 2nd ed. New York: Springer, 2012.</p>
<p class="Reference">Hopcroft, John E., Rajeev Motwani, and Jeffrey D. Ullman, <em>Introduction to Automata Theory, Languages, and Computation</em>, 3rd ed. Upper Saddle River, NJ: Prentice Hall, 2006.</p>
<p class="Reference">Kernighan, Brian W., and Rob Pike, <em>The UNIX Programming Environment</em>. Upper Saddle River, NJ: Prentice Hall, 1984.</p>
<p class="Reference">Kernighan, Brian W., and Dennis M. Ritchie, <em>The C Programming Language</em>, 2nd ed. Upper Saddle River, NJ: Prentice Hall, 1988.</p>
<p class="Reference">Kochan, Stephen G., and Patrick Wood, <em>Unix Shell Programming</em>, 3rd ed. Indianapolis: SAMS Publishing, 2003.</p>
<p class="Reference">Levine, John R., <em>Linkers and Loaders</em>. San Francisco: Morgan Kaufmann, 1999.</p>
<p class="Reference">Lucas, Michael W., <em>SSH Mastery: OpenSSH, PuTTY, Tunnels, and Keys</em>, 2nd ed. Detroit: Tilted Windmill Press, 2018.</p>
<p class="Reference">Matloff, Norman, <em>The Art of R Programming: A Tour of Statistical Software Design</em>. San Francisco: No Starch Press, 2011.</p>
<p class="Reference">Mecklenburg, Robert, <em>Managing Projects with GNU Make</em>, 3rd ed. Sebastopol, CA: O’Reilly, 2005.</p>
<p class="Reference">Peek, Jerry, Grace Todino-Gonguet, and John Strang, <em>Learning the UNIX Operating System: A Concise Guide for the New User</em>, 5th ed. Sebastopol, CA: O’Reilly, 2001.</p>
<p class="Reference">Pike, Rob, Dave Presotto, Sean Dorward, Bob Flandrena, Ken Thompson, Howard Trickey, and Phil Winterbottom, “Plan 9 from Bell Labs.” Accessed February 1, 2020, <a href="https://9p.io/sys/doc/">https://9p.io/sys/doc/.</a></p>
<p class="Reference">Poulton, Nigel, <em>Docker Deep Dive</em>. Author, 2016.</p>
<p class="Reference">Quinlan, Daniel, Rusty Russell, and Christopher Yeoh, eds., “Filesystem Hierarchy Standard, Version 3.0.” Linux Foundation, 2015, <a href="https://refspecs.linuxfoundation.org/fhs.shtml">https://refspecs.linuxfoundation.org/fhs.shtml</a>.</p>
<p class="Reference"><span epub:type="pagebreak" title="421" id="Page_421"/>Raymond, Eric S., ed., <em>The New Hacker’s Dictionary</em>. 3rd ed. Cambridge, MA: MIT Press, 1996.</p>
<p class="Reference">Robbins, Arnold, <em>sed &amp; awk Pocket Reference</em>, 2nd ed. Sebastopol, CA: O’Reilly, 2002.</p>
<p class="Reference">Robbins, Arnold, Elbert Hannah, and Linda Lamb, <em>Learning the vi and Vim Editors: Unix Text Processing</em>, 7th ed. Sebastopol, CA: O’Reilly, 2008.</p>
<p class="Reference">Salus, Peter H., <em>The Daemon, the Gnu, and the Penguin</em>. Tacoma, WA: Reed Media Services, 2008.</p>
<p class="Reference">Samar, Vipin, and Roland J. Schemers III. “Unified Login with Pluggable Authentication Modules (PAM),” October 1995, Open Software Foundation (RFC 86.0), <a href="http://www.opengroup.org/rfc/rfc86.0.html" class="LinkURL">http://www.opengroup.org/rfc/rfc86.0.html</a>.</p>
<p class="Reference">Schwartz, Randal L., brian d foy, and Tom Phoenix, <em>Learning Perl: Making Easy Things Easy and Hard Things Possible</em>, 7th ed. Sebastopol, CA: O’Reilly, 2016.</p>
<p class="Reference">Shotts, William, <em>The Linux Command Line</em>, 2nd ed. San Francisco: No Starch Press, 2019.</p>
<p class="Reference">Silberschatz, Abraham, Peter B. Galvin, and Greg Gagne, <em>Operating System Concepts</em>, 10th ed. Hoboken, NJ: Wiley, 2018.</p>
<p class="Reference">Smith, Jim, and Ravi Nair, <em>Virtual Machines: Versatile Platforms for Systems and Processes</em>. Cambridge, MA: Elsevier, 2005.</p>
<p class="Reference">Stallman, Richard M., <em>GNU Emacs Manual</em>, 18th ed. Boston: Free Software Foundation, 2018.</p>
<p class="Reference">Stevens, W. Richard, Bill Fenner, and Andrew M. Rudoff, <em>Unix Network Programming, Volume 1: The Sockets Networking API</em>, 3rd ed. Boston: Addison-Wesley Professional, 2003.</p>
<p class="Reference">Tanenbaum, Andrew S., and Herbert Bos, <em>Modern Operating Systems</em>, 4th ed. Upper Saddle River, NJ: Prentice Hall, 2014.</p>
<p class="Reference">Tanenbaum, Andrew S., and David J. Wetherall, <em>Computer Networks</em>, 5th ed. Upper Saddle River, NJ: Prentice Hall, 2010.</p>
</section>
</body></html>