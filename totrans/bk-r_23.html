<html><head></head><body>
<h2 class="h2" id="ch19"><span epub:type="pagebreak" id="page_435"/><span class="big"><strong>19</strong></span><br/><strong>ANALYSIS OF VARIANCE</strong></h2>&#13;
<div class="image"><img src="../images/common-01.jpg" alt="image"/></div>&#13;
<p class="noindent"><em>Analysis of variance (ANOVA)</em>, in its simplest form, is used to compare multiple means in a test for equivalence. In that sense, it’s a straightforward extension of the hypothesis test comparing two means. There’s a continuous variable from which the means of interest are calculated, and there’s at least one categorical variable that tells you how to define the groups for those means. In this chapter, you’ll explore the ideas surrounding ANOVA and look at comparing means first split by one categorical variable (one-way ANOVA) and then split by multiple categorical variables (multiple-factor ANOVA).</p>&#13;
<h3 class="h3" id="ch19lev1sec59"><strong>19.1 One-Way ANOVA</strong></h3>&#13;
<p class="noindent">The simplest version of ANOVA is referred to as <em>one-way</em> or <em>one-factor</em> analysis. Simply put, the one-way ANOVA is used to test two or more means for equality. Those means are split by a categorical <em>group</em> or <em>factor</em> variable. ANOVA is often used to analyze experimental data to assess the impact of an intervention. You might, for example, be interested in comparing the mean weights of the chicks in the built-in <code>chickwts</code> data set, split according to the different food types they were fed.</p>&#13;
<h4 class="h4" id="ch19lev2sec168"><span epub:type="pagebreak" id="page_436"/><strong><em>19.1.1 Hypotheses and Diagnostic Checking</em></strong></h4>&#13;
<p class="noindentb">Say you have a categorical-nominal variable that splits a total of <em>N</em> numeric observations into <em>k</em> distinct groups, where <em>k</em> ≥ 2. You’re looking to statistically compare the <em>k</em> groups’ means, <em>μ<sub>1</sub></em>,...,<em>μ<sub>k</sub></em>, to see whether they can be claimed to be equal. The standard hypotheses are as follows:</p>&#13;
<p class="hang">H<sub>0</sub> : <em>μ<sub>1</sub></em> = <em>μ<sub>2</sub></em> = ... = <em>μ<sub>k</sub></em></p>&#13;
<p class="hang">H<sub>A</sub> : <em>μ<sub>1</sub></em>, <em>μ<sub>2</sub></em>, ... , <em>μ<sub>k</sub></em> are not all equal</p>&#13;
<p class="hangp">(alternatively, at least one mean differs).</p>&#13;
<p class="indentt">In fact, when <em>k</em> = 2, the two-sample <em>t</em>-test is equivalent to ANOVA; for that reason, ANOVA is most frequently employed when <em>k</em> &gt; 2.</p>&#13;
<p class="indentb">The following assumptions need to be satisfied in order for the results of the basic one-way ANOVA test to be considered reliable:</p>&#13;
<p class="noindenth"><strong>Independence</strong> The samples making up the <em>k</em> groups must be independent of one another, and the observations in each group must be independent and identically distributed (iid).</p>&#13;
<p class="noindenth"><strong>Normality</strong> The observations in each group should be normally distributed, or at least approximately so.</p>&#13;
<p class="noindenth"><strong>Equality of variances</strong> The variance of the observations in each group should be equal, or at least approximately so.</p>&#13;
<p class="indentt">If the assumptions of equality of variances or normality are violated, it doesn’t necessarily mean your results will be completely worthless, but it will impact the overall effectiveness of detecting a true difference in the means (refer to the discussion on statistical power in <a href="ch18.xhtml#ch18lev2sec167">Section 18.5.4</a>). It’s always a good idea to assess the validity of these assumptions before using ANOVA; I’ll do this informally for the upcoming example.</p>&#13;
<p class="indent">It’s also worth noting that you don’t need to have an equal number of observations in each group to perform the test (in which case it is referred to as <em>unbalanced</em>). However, having unbalanced groups does render the test more sensitive to potentially detrimental effects if your assumptions of equality of variances and normality are not sound.</p>&#13;
<p class="indent">Let’s return to the <code>chickwts</code> data for the example—the weights of chicks based on <em>k</em> = 6 different feeds. You’re interested in comparing the mean weights according to feed type to see whether they’re all equal. Use <code>table</code> to summarize the six sample sizes and use <code>tapply</code> (see, for example, <a href="ch13.xhtml#ch13lev2sec116">Section 13.2.1</a>) to get each group mean, as follows:</p>&#13;
<pre>R&gt; table(chickwts$feed)<br/><br/>   casein horsebean   linseed  meatmeal   soybean sunflower<br/>       12        10        12        11        14        12<br/>R&gt; chick.means &lt;- tapply(chickwts$weight,INDEX=chickwts$feed,FUN=mean)<br/>R&gt; chick.means<br/>   casein horsebean   linseed  meatmeal   soybean sunflower<br/> 323.5833  160.2000  218.7500  276.9091  246.4286  328.9167</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_437"/>Your skills from <a href="ch14.xhtml#ch14lev2sec125">Section 14.3.2</a> allow you to produce side-by-side box-plots of the distributions of weights. The next two lines give you the plot on the left of <a href="ch19.xhtml#ch19fig1">Figure 19-1</a>:</p>&#13;
<pre>R&gt; boxplot(chickwts$weight~chickwts$feed)<br/>R&gt; points(1:6,chick.means,pch=4,cex=1.5)</pre>&#13;
<div class="image"><img src="../images/f19-01.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch19fig1"/>Figure 19-1: Exploring the</em> <code>chickwts</code> <em>data. Left: Side-by-side boxplots of chick weight split by feed type, with the mean marked by</em> ×<em>. Right: Normal QQ plot of the mean-centered data of each feed group.</em></p>&#13;
<p class="indent">Because boxplots display the median, not the mean, the second line of code adds the feed-specific means (stored in the <code>chick.means</code> object you just created) to each box using <code>points</code>.</p>&#13;
<p class="indentb">Inspecting the left plot of <a href="ch19.xhtml#ch19fig1">Figure 19-1</a>, it certainly looks as though there’s a difference in the mean weights. Is any apparent difference statistically significant, though? To find out, the ANOVA test for this example concerns the following hypotheses:</p>&#13;
<p class="hang">H<sub>0</sub> : <em>μ</em><sub><small>casein</small></sub> = <em>μ</em><small><sub>horsebean</sub></small> = <em>μ</em><sub><small>linseed</small></sub> = <em>μ</em><small><sub>meatmeal</sub></small> = <em>μ</em><small><sub>soybean</sub></small> = <em>μ</em><small><sub>sunflower</sub></small></p>&#13;
<p class="hang">H<sub>A</sub> : The means are not all equal.</p>&#13;
<p class="indentt">Assuming independence of the data, before implementing the test, you must first check that the other assumptions are valid. To examine equality of variances, you can use the same informal rule of thumb as used in the two-sample <em>t</em>-test. That is, you can assume equality of variances if the ratio of the largest sample standard deviation to the smallest is less than 2. For the chick weights data, the following code will determine this:</p>&#13;
<pre>R&gt; chick.sds &lt;- tapply(chickwts$weight,INDEX=chickwts$feed,FUN=sd)<br/>R&gt; chick.sds<br/>   casein horsebean   linseed  meatmeal   soybean sunflower<br/> 64.43384  38.62584  52.23570  64.90062  54.12907  48.83638<br/>R&gt; max(chick.sds)/min(chick.sds)<br/>[1] 1.680238</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_438"/>This informal result indicates that it’s reasonable to make the assumption.</p>&#13;
<p class="indent">Next, consider the assumption of normality of the raw observations. This can be difficult to determine in many real-data examples. At the least, though, it’s worthwhile to inspect histograms and QQ plots for signs of non-normality. You already inspected histograms and QQ plots for all 71 weights in <a href="ch16.xhtml#ch16lev2sec142">Section 16.2.2</a>, but for an ANOVA, you need to do this with respect to the grouping of the observations (that is, not just “overall” for the whole set of weights regardless of groups).</p>&#13;
<p class="indent">To achieve this for the <code>chickwts</code> data, you need to first <em>mean-center</em> each weight by its respective sample mean. You can do this by taking the original vector of weights and subtracting from it the <code>chick.means</code> vector, but first you must rearrange and replicate the latter elements to correspond to the elements in the former. This is done by using <code>as.numeric</code> on the factor vector that represents feed type, giving the numeric value of the levels of <code>chickwts$feed</code> for each record in the original data frame. When that numeric vector is passed via the square brackets to <code>chick.means</code>, you get the correct group mean matched to each observation. As an exercise, you can inspect all the ingredients that go into creating the following <code>chick.meancen</code> object to satisfy yourself of what’s going on:</p>&#13;
<pre>R&gt; chick.meancen &lt;- chickwts$weight-chick.means[as.numeric(chickwts$feed)]</pre>&#13;
<p class="indent">In the context of the current analysis, these group-wise, mean-centered values are also referred to as <em>residuals</em>, a term you’ll come across frequently when you study regression methods in the next few chapters.</p>&#13;
<p class="indent">You can now assess normality of the observations as a whole using the residuals. To inspect a normal QQ plot, the relevant functions are <code>qqnorm</code> and <code>qqline</code>, which you first met in <a href="ch16.xhtml#ch16lev2sec142">Section 16.2.2</a>. The following two lines produce the image on the right of <a href="ch19.xhtml#ch19fig1">Figure 19-1</a>.</p>&#13;
<pre>R&gt; qqnorm(chick.meancen,main="Normal QQ plot of residuals")<br/>R&gt; qqline(chick.meancen)</pre>&#13;
<p class="indent">Based on this plot (the proximity of the plotted points to the perfect straight line), it doesn’t seem unreasonable to assume normality for these data, particularly when compared to QQ plots of generated normal data of the same sample size (an example was given on the left of <a href="ch16.xhtml#ch16fig9">Figure 16-9</a> on <a href="ch16.xhtml#page_355">page 355</a>).</p>&#13;
<p class="indent">Investigating the validity of any required assumptions is referred to as <em>diagnostic checking</em>. If you wanted to perform a more rigorous diagnostic check for an ANOVA, other visual diagnostics could involve inspecting QQ plots split by group (you’ll do this in an example in <a href="ch19.xhtml#ch19lev1sec61">Section 19.3</a>) or plotting the sample standard deviation for each group against the corresponding sample means. Indeed, there are also general hypothesis tests for normality (such as the Shapiro-Wilk test or Anderson-Darling test—you’ll see the former used in <a href="ch22.xhtml#ch22lev2sec215">Section 22.3.2</a>), as well as tests for equality of variances (such as <span epub:type="pagebreak" id="page_439"/>Levene’s test), but I’ll stick with the basic rule of thumb and visual checks in this example.</p>&#13;
<h4 class="h4" id="ch19lev2sec169"><strong><em>19.1.2 One-Way ANOVA Table Construction</em></strong></h4>&#13;
<p class="noindent">Turning your attention back to the left of <a href="ch19.xhtml#ch19fig1">Figure 19-1</a>, remember that the goal is to statistically evaluate the equality of the means marked by ×. This task will therefore require you to consider not only the variability <em>within</em> each of the <em>k</em> samples but the variability <em>between</em> the samples; this is why the test is referred to as an analysis of variance.</p>&#13;
<p class="indent">The test proceeds by first calculating various metrics associated with the overall variability and then calculating the within- and between-group variability. These figures involve sums of squared quantities and associated degrees of freedom values. All this culminates in a single test statistic and <em>p</em>-value targeting the aforementioned hypotheses. These ingredients are typically presented in a table, which is defined as follows.</p>&#13;
<p class="indent">Let <em>x</em><sub>1</sub>, ... , <em>x<sub>N</sub></em> represent all <em>N</em> observations, regardless of group; let <em>x</em><sub>1</sub><sub>(</sub><sub><em>j</em></sub><sub>)</sub>, ... , <em>x<sub>nj</sub>(j)</em> denote the specific group observations in group <em>j</em> = 1, ... , <em>k</em> such that <em>n<sub>1</sub></em> + ... + <em>n<sub>k</sub></em> = <em>N</em>. Let the “grand mean” of all observations be defined as <img class="middle" src="../images/f0439-01.jpg" alt="image"/>. The ANOVA table is then constructed, where SS stands for sum-of-squares, df stands for degrees of freedom, MS stands for mean square, <em>F</em> refers to the <em>F</em> test statistic, and <em>p</em> refers to the <em>p</em>-value.</p>&#13;
<table>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table_2a"><p class="table"> </p></td>&#13;
<td style="vertical-align: top;" class="table_2b"><p class="tablec">df</p></td>&#13;
<td style="vertical-align: top;" class="table_2b"><p class="tablec">SS</p></td>&#13;
<td style="vertical-align: top;" class="table_2b"><p class="tablec">MS</p></td>&#13;
<td style="vertical-align: top;" class="table_2b"><p class="tablec"><em>F</em></p></td>&#13;
<td style="vertical-align: top;" class="table_2"><p class="tablec"><em>p</em></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table_3a"><p class="table">Overall</p></td>&#13;
<td style="vertical-align: top;" class="table_3"><p class="tablec">1</p></td>&#13;
<td style="vertical-align: top;" class="table_3"><p class="tablec"><strong>(1)</strong></p></td>&#13;
<td style="vertical-align: top;" class="table_3"><p class="tablec"> </p></td>&#13;
<td style="vertical-align: top;" class="table_3"><p class="tablec"> </p></td>&#13;
<td style="vertical-align: top;" class="table_3b"><p class="tablec"> </p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table_3a"><p class="table">Group (or “<a href="ch04.xhtml#ch04lev1sec18">Factor</a>”)</p></td>&#13;
<td style="vertical-align: top;" class="table_3"><p class="tablec"><em>k</em> − 1</p></td>&#13;
<td style="vertical-align: top;" class="table_3"><p class="tablec"><strong>(2)</strong></p></td>&#13;
<td style="vertical-align: top;" class="table_3"><p class="tablec"><strong>(5)</strong></p></td>&#13;
<td style="vertical-align: top;" class="table_3"><p class="tablec"><strong>(5)÷(6)</strong></p></td>&#13;
<td style="vertical-align: top;" class="table_3b"><p class="tablec"><em>p</em>-value</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table_2a"><p class="table">Error (or “Residual”)</p></td>&#13;
<td style="vertical-align: top;" class="table_2b"><p class="tablec"><em>N</em> − <em>k</em></p></td>&#13;
<td style="vertical-align: top;" class="table_2b"><p class="tablec"><strong>(3)</strong></p></td>&#13;
<td style="vertical-align: top;" class="table_2b"><p class="tablec"><strong>(6)</strong></p></td>&#13;
<td style="vertical-align: top;" class="table_2b"><p class="tablec"> </p></td>&#13;
<td style="vertical-align: top;" class="table_2"><p class="tablec"> </p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table_2a1"><p class="table">TOTAL</p></td>&#13;
<td style="vertical-align: top;" class="table_4"><p class="tablec"><em>N</em></p></td>&#13;
<td style="vertical-align: top;" class="table_4"><p class="tablec"><strong>(4)</strong></p></td>&#13;
<td style="vertical-align: top;" class="table_4"><p class="tablec"> </p></td>&#13;
<td style="vertical-align: top;" class="table_4"><p class="tablec"> </p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec"> </p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">You calculate the values with these formulas:</p>&#13;
<ol>&#13;
<li><p class="noindents"><em>N<span class="ent">x̄</span></em><sup>2</sup></p></li>&#13;
<li><p class="noindents"><img src="../images/f0439-02.jpg" alt="image"/></p></li>&#13;
<li><p class="noindents"><strong>(4)–(2)–(1)</strong></p></li>&#13;
<li><p class="noindents"><img src="../images/f0439-03.jpg" alt="image"/></p></li>&#13;
<li><p class="noindents"><strong>(2)</strong>÷(<em>k</em> – 1)</p></li>&#13;
<li><p class="noindents"><strong>(3)</strong>÷(<em>N</em> – <em>k</em>)</p></li>&#13;
</ol>&#13;
<p class="indentb">There are three input sources that are assumed to make up the observed data, which, when added together, result in the TOTAL row. Let’s think about these in a little more detail:</p>&#13;
<p class="noindenth"><strong>Overall row</strong> This relates to the scale on which the data as a whole sit. It doesn’t affect the outcome of the hypothesis test (since you’re interested only in the relative differences between means) and is sometimes removed from the table, affecting the TOTAL values accordingly.</p>&#13;
<p class="noindenth"><span epub:type="pagebreak" id="page_440"/><strong>Group row/Factor row</strong> This relates to the data in the individual groups of interest, thereby accounting for the <em>between-group variability</em>.</p>&#13;
<p class="noindenth"><strong>Error row/Residual row</strong> This accounts for the random deviation from the estimated means of each group, thereby accounting for the <em>within-group variability</em>.</p>&#13;
<p class="noindenth"><strong>TOTAL row</strong> This represents the raw data, based on the previous three ingredients. It is used to find the Error SS by differencing.</p>&#13;
<p class="indentt">The three input sources each have a corresponding degrees of freedom (df) value in the first column and a sum-of-squares (SS) value attached to the df in the second column. Between- and within-group variability is averaged by dividing the SS by the df, giving the mean squared (MS) component for these two items. The test statistic, <em>F</em>, is found by dividing the mean squared group (MSG) effect by the mean squared error (MSE) effect. This test statistic follows the <em>F</em>-distribution (refer to <a href="ch16.xhtml#ch16lev2sec145">Section 16.2.5</a>), which itself requires a pair of degrees of freedom values ordered as df<sub>1</sub> (which represents the Group df, <em>k</em>−1) and df<sub>2</sub> (which represents the Error df, <em>N</em>−<em>k</em>). Like the chi-squared distribution, the <em>F</em>-distribution is unidirectional in nature, and the <em>p</em>-value is obtained as the upper-tail area from the test statistic <em>F</em>.</p>&#13;
<h4 class="h4" id="ch19lev2sec170"><strong><em>19.1.3 Building ANOVA Tables with the aov Function</em></strong></h4>&#13;
<p class="noindent">As you might expect, R allows you to easily construct an ANOVA table for the chick weight test using the built-in <code>aov</code> function as follows:</p>&#13;
<pre>R&gt; chick.anova &lt;- aov(weight~feed,data=chickwts)</pre>&#13;
<p class="indent">Then, the table is printed to the console screen using <code>summary</code>.</p>&#13;
<pre>R&gt; summary(chick.anova)<br/>            Df Sum Sq Mean Sq F value   Pr(&gt;F)<br/>feed         5 231129   46226   15.37 5.94e-10 ***<br/>Residuals   65 195556    3009<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</pre>&#13;
<p class="indent">There are several comments to make here. Note that you employ formula notation <code>weight~feed</code> to specify the measurement variable of interest, weight, as modeled by the categorical-nominal variable of interest, feed type. In this case, the variable names <code>weight</code> and <code>feed</code> are not required to be prefaced by <code>chickwts$</code> since the optional <code>data</code> argument has been passed the data frame of interest.</p>&#13;
<p class="indent">Remember from <a href="ch14.xhtml#ch14lev2sec125">Section 14.3.2</a> that for the notation in the expression <code>weight~feed</code>, the “outcome” of interest must always appear on the left of the <code>~</code> (this notation will become particularly relevant in <a href="ch20.xhtml#ch20">Chapters 20</a> to <a href="ch22.xhtml#ch22">22</a>).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_441"/>To actually view the table, you must apply the <code>summary</code> command to the object resulting from the call to <code>aov</code>. R omits the first and last rows (Overall and TOTAL) since these are not directly involved in calculating the <em>p</em>-value. Other than that, it’s easy to identify that the <code>feed</code> row refers to the Group row and the <code>Residuals</code> row refers to the Error row.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>By default, R annotates model-based</em> <code><span class="codeitalic">summary</code></span> <em>output like this with</em> significance stars<em>. These show intervals of significance, and the number of stars increases as the p-value decreases beyond a cutoff mark of 0.1. This can be useful when you’re examining more complicated analyses where multiple p-values are summarized, though not everyone likes this feature. If you want, you can turn off this feature in a given R session by entering</em> <code><span class="codeitalic">options(show.signif.stars=FALSE)</code></span> <em>at the prompt. Alternatively, you can turn off the feature directly in the call to</em> <code><span class="codeitalic">summary</code></span> <em>by setting the additional argument</em> <code><span class="codeitalic">signif.stars=FALSE</code></span><em>. In this book, I’ll leave them be.</em></p>&#13;
</div>&#13;
<p class="indent">From the contents of the ANOVA for this example, you can quickly confirm the calculations. Note that the MSE, 3009, was defined as the Error SS divided by the Error df. Indeed, in R, the same result is achieved manually (the table output has been rounded to the nearest integer).</p>&#13;
<pre>R&gt; 195556/65<br/>[1] 3008.554</pre>&#13;
<p class="indent">You can confirm all the other results in the table output using the relevant equations from earlier.</p>&#13;
<p class="indent">Interpreting a hypothesis test based on ANOVA follows the same rules as any other test. With the understanding of a <em>p</em>-value as “the probability that you observe the sample statistics at hand or something more extreme, if H<sub>0</sub> is true,” a small <em>p</em>-value indicates evidence against the null hypothesis. In the current example, a tiny <em>p</em>-value provides strong evidence against the null that the mean chick weights are the same for the different diets. In other words, you reject H<sub>0</sub> in favor of H<sub>A</sub>; the latter states that there is a difference.</p>&#13;
<p class="indent">In a similar fashion as in the chi-squared tests, rejection of the null in one-way ANOVA doesn’t tell you exactly where a difference lies, merely that there’s evidence one exists. Further scrutiny of the data in the individual groups is necessary to identify the offending means. At the simplest level, you could turn back to pairwise two-sample <em>t</em>-tests, in which case you could also use the MSE from the ANOVA table as an estimate of the pooled variance. The substitution is valid if the assumption of equal variance holds, and such a step is beneficial because the corresponding <em>t</em>-based sampling distribution will utilize the Error df (this is naturally higher than would otherwise be the case if the df was based on just the sample sizes of the two groups of specific interest).</p>&#13;
<div class="ex">&#13;
<p class="ext"><span epub:type="pagebreak" id="page_442"/><a id="ch19exc1"/><strong>Exercise 19.1</strong></p>&#13;
<p class="noindentz">Consider the following data:</p>&#13;
<table class="topbot2">&#13;
<thead>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table_th"><p class="tabler"><strong>Site I</strong></p></td>&#13;
<td style="vertical-align: top;" class="table_th"><p class="tabler"><strong>Site II</strong></p></td>&#13;
<td style="vertical-align: top;" class="table_th"><p class="tabler"><strong>Site III</strong></p></td>&#13;
<td style="vertical-align: top;" class="table_th"><p class="tabler"><strong>Site IV</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">93</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">85</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">100</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">96</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">120</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">45</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">75</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">58</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">65</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">80</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">65</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">95</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">105</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">28</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">40</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">90</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">115</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">75</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">73</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">65</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">82</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">70</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">65</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">80</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">99</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">65</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">50</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">85</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">87</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">55</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">30</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">95</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">100</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">50</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">45</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">82</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">90</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">40</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">50</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler"> </p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">78</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler"> </p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">45</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler"> </p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">95</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler"> </p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">55</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler"> </p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">93</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler"> </p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler"> </p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler"> </p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">88</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler"> </p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler"> </p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler"> </p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">110</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler"> </p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler"> </p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler"> </p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indentz">These figures provide the depths (in centimeters) at which important archaeological finds were made at four sites in New Mexico (see <a href="ref.xhtml#ref77">Woosley and Mcintyre, 1996</a>). Store these data in your R workspace, with one vector containing depth and the other vector containing the site of each observation.</p>&#13;
<ol type="a">&#13;
<li><p class="noindents">Produce side-by-side boxplots of the depths split by group, and use additional points to mark the locations of the sample means.</p></li>&#13;
<li><p class="noindents">Assuming independence, execute diagnostic checks for normality and equality of variances.</p></li>&#13;
<li><p class="noindents">Perform and conclude a one-way ANOVA test for evidence of a difference between the means.</p></li>&#13;
</ol>&#13;
<p class="noindentz">In <a href="ch14.xhtml#ch14lev1sec47">Section 14.4</a>, you looked at the data set providing measurements on petal and sepal sizes for three species of iris flowers. This is available in R as <code>iris</code>.</p>&#13;
<ol type="a" start="4">&#13;
<li><p class="noindents">Based on diagnostic checks for normality and equality of variances, decide which of the four outcome measurements (sepal length/width and petal length/width) would be suitable for ANOVA (using the species as the group variable).</p></li>&#13;
<li><p class="noindents">Carry out one-way ANOVA for any suitable measurement variables.</p></li>&#13;
</ol>&#13;
</div>&#13;
<h3 class="h3" id="ch19lev1sec60"><span epub:type="pagebreak" id="page_443"/><strong>19.2 Two-Way ANOVA</strong></h3>&#13;
<p class="noindent">In many studies, the numeric outcome variable you’re interested in will be categorized by more than just one grouping variable. In these cases, you would use the <em>multiple-factor</em> ANOVA rather than the one-way ANOVA. This technique is directly referred to by the number of grouping variables used, with two- and three-way ANOVA being the next and most common extensions.</p>&#13;
<p class="indent">Increasing the number of grouping variables complicates matters somewhat—performing just a one-way ANOVA for each variable separately is inadequate. In dealing with more than one categorical grouping factor, you must consider the <em>main effects</em> of each factor on the numeric outcome, while simultaneously accounting for the presence of the other grouping factor(s). That’s not all, though. It’s just as important to additionally investigate the idea of an <em>interactive effect</em>; if an interactive effect exists, then it suggests that the impact one of the grouping variables has on the outcome of interest, specified by its main effect, varies according to the levels of the other grouping variable(s).</p>&#13;
<h4 class="h4" id="ch19lev2sec171"><strong><em>19.2.1 A Suite of Hypotheses</em></strong></h4>&#13;
<p class="noindentb">For this explanation, denote your numeric outcome variable with <em>O</em> and your two grouping variables as <em>G<sub>1</sub></em> and <em>G</em><sub>2</sub>. In two-way ANOVA, the hypotheses should be set along the following lines:</p>&#13;
<p class="hang">H<sub>0</sub> : <em>G<sub>1</sub></em> has no main (marginal) effect on the mean of <em>O</em>.</p>&#13;
<p class="hangp"><em>G</em><sub>2</sub> has no main (marginal) effect on the mean of <em>O</em>.</p>&#13;
<p class="hangp">There is no interactive effect of <em>G<sub>1</sub></em> with <em>G<sub>2</sub></em> on the mean of <em>O</em>.</p>&#13;
<p class="hang">H<sub>A</sub> : Separately, each statement in H<sub>0</sub> is incorrect.</p>&#13;
<p class="indentt">You can see from these general hypotheses that you now have to obtain a <em>p</em>-value for each of the three components.</p>&#13;
<p class="indent">For the example, let’s use the built-in <code>warpbreaks</code> data frame (<a href="ref.xhtml#ref65">Tippett, 1950</a>), which provides the number of “warp break” imperfections (column <code>breaks</code>) observed in 54 pieces of yarn of equal length. Each piece of yarn is classified according to two categorical variables: <code>wool</code> (the type of yarn, with levels <code>A</code> and <code>B</code>) and <code>tension</code> (the level of tension applied to that piece—<code>L</code>, <code>M</code>, or <code>H</code> for low, medium, or high). Using <code>tapply</code>, you can inspect the mean number of warp breaks for each classification.</p>&#13;
<pre>R&gt; tapply(warpbreaks$breaks,INDEX=list(warpbreaks$wool,warpbreaks$tension),<br/>          FUN=mean)<br/>         L        M        H<br/>A 44.55556 24.00000 24.55556<br/>B 28.22222 28.77778 18.77778</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_444"/>You can supply more than one grouping variable to the <code>INDEX</code> argument as separate members of a list (any factor vectors given to this argument should be the same length as the first argument that specifies the data of interest). The results are returned as a matrix for two grouping variables, a 3D array for three grouping variables, and so on.</p>&#13;
<p class="indent">For some analyses, however, you might need the same information provided earlier in a different format. The <code>aggregate</code> function is similar to <code>tapply</code>, but it returns a data frame, the results in <em>stacked</em> format according to the specified grouping variables (as opposed to an array as returned by <code>tapply</code>). It’s called in much the same way. The first argument takes the data vector of interest. The second argument, <code>by</code>, should be a list of the desired grouping variables, and in <code>FUN</code>, you specify the function to operate on each subset.</p>&#13;
<pre>R&gt; wb.means &lt;- aggregate(warpbreaks$breaks,<br/>                         by=list(warpbreaks$wool,warpbreaks$tension),FUN=mean)<br/>R&gt; wb.means<br/>  Group.1 Group.2        x<br/>1       A       L 44.55556<br/>2       B       L 28.22222<br/>3       A       M 24.00000<br/>4       B       M 28.77778<br/>5       A       H 24.55556<br/>6       B       H 18.77778</pre>&#13;
<p class="indent">Here I’ve stored the result of the call to <code>aggregate</code> as the object <code>wb.means</code> for later use.</p>&#13;
<h4 class="h4" id="ch19lev2sec172"><strong><em>19.2.2 Main Effects and Interactions</em></strong></h4>&#13;
<p class="noindent">I mentioned earlier that you could perform just a one-way ANOVA on each grouping variable separately, but this, in general, isn’t a good idea. I’ll demonstrate this now with the <code>warpbreaks</code> data (a quick inspection of the relevant diagnostics shows no obvious cause for concern):</p>&#13;
<pre>R&gt; summary(aov(breaks~wool,data=warpbreaks))<br/>            Df Sum Sq Mean Sq F value Pr(&gt;F)<br/>wool         1    451   450.7   2.668  0.108<br/>Residuals   52   8782   168.9<br/>R&gt; summary(aov(breaks~tension,data=warpbreaks))<br/>            Df Sum Sq Mean Sq F value  Pr(&gt;F)<br/>tension      2   2034  1017.1   7.206 0.00175 **<br/>Residuals   51   7199   141.1<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</pre>&#13;
<p class="indent">This output tells you that if you ignore <code>tension</code>, there is no evidence to suggest that there is any difference in the mean number of imperfections <span epub:type="pagebreak" id="page_445"/>based on the type of <code>wool</code> alone (<em>p</em>-value 0.108). If you ignore <code>wool</code>, however, there <em>is</em> evidence to suggest a difference in warp breaks according to <code>tension</code> only.</p>&#13;
<p class="indent">The problem here is that by ignoring one of the variables, you lose the ability to detect differences (or, more generally, statistical relationships) that may occur at a finer level. For example, though the <code>wool</code> type alone seems to have no remarkable impact on the mean number of warp breaks, you cannot tell whether this would be the case if you just looked at <code>wool</code> types at one particular level of <code>tension</code>.</p>&#13;
<p class="indent">Instead, you investigate this kind of question using two-way ANOVA. The following executes a two-way ANOVA for the warp breaks data based only on the main effects of the two grouping variables:</p>&#13;
<pre>R&gt; summary(aov(breaks~wool+tension,data=warpbreaks))<br/>            Df Sum Sq Mean Sq F value  Pr(&gt;F)<br/>wool         1    451   450.7   3.339 0.07361 .<br/>tension      2   2034  1017.1   7.537 0.00138 **<br/>Residuals   50   6748   135.0<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</pre>&#13;
<p class="indent">Take a look at the formula. Specifying <code>wool+tension</code> to the right of the outcome variable and the <code>~</code> allows you to take both grouping variables into account at the same time. The results reveal a small drop in the size of the <em>p</em>-values now attached to each grouping variable; indeed, the <em>p</em>-value for <code>wool</code> is around 0.073, approaching the conventional cutoff significance level of <em>α</em> = 0.05. To interpret the results, you hold one grouping variable constant—if you focus on just one type of wool, there is still statistically significant evidence to suggest a difference in the mean number of warp breaks between the different <code>tension</code> levels. If you focus on just one level of <code>tension</code>, the evidence of a difference considering the two <code>wool</code> types has increased a little but is still not statistically significant (assuming the aforementioned <em>α</em> = 0.05).</p>&#13;
<p class="indent">There’s still a limitation with considering only main effects. While the previous analysis shows that there’s variation in the outcome between the different levels of the two categorical variables, it doesn’t address the possibility that a difference in the mean number of warp breaks might vary further according to precisely <em>which</em> level of either <code>tension</code> or <code>wool</code> is being used when holding the other variable constant. This relatively subtle yet important consideration is known as an <em>interaction</em>. Specifically, if there is an interactive effect present between <code>tension</code> and <code>wool</code> with respect to warp breaks, then this would imply that the magnitude and/or direction of the difference in the mean number of warp breaks <em>is not the same</em> at different levels of the two grouping factors.</p>&#13;
<p class="indent">To account for interactions, you make a slight adjustment to the two-way ANOVA model code.</p>&#13;
<pre><span epub:type="pagebreak" id="page_446"/>R&gt; summary(aov(breaks~wool+tension+wool:tension,data=warpbreaks))<br/>             Df Sum Sq Mean Sq F value   Pr(&gt;F)<br/>wool          1    451   450.7   3.765 0.058213 .<br/>tension       2   2034  1017.1   8.498 0.000693 ***<br/>wool:tension  2   1003   501.4   4.189 0.021044 *<br/>Residuals    48   5745   119.7<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</pre>&#13;
<p class="indent">You can explicitly specify the interaction as the main effects model formula <em>plus</em> the notation <code>wool:tension</code>, where the two grouping variables are separated by a <code>:</code>. (Note, in this setting, the <code>:</code> operator has nothing to do with the shortcut for creating an integer sequence first discussed in <a href="ch02.xhtml#ch02lev2sec21">Section 2.3.2</a>.)</p>&#13;
<p class="indent">You can see from the ANOVA table output that, statistically, there is evidence of an interactive effect; that is, the very nature of the difference in the means is dependent upon the factor levels themselves, even though that evidence is relatively weak. Of course, the <em>p</em>-value of around 0.021 tells you only that, overall, there might be an interaction but not the precise features of the interaction.</p>&#13;
<p class="indent">To help with this, you can interpret such a two-way interaction effect in more detail with an <em>interaction plot</em>, provided in R with <code>interaction.plot</code>.</p>&#13;
<pre>R&gt; interaction.plot(x.factor=wb.means[,2],trace.factor=wb.means[,1],<br/>                    response=wb.means$x,trace.label="wool",<br/>                    xlab="tension",ylab="mean warp breaks")</pre>&#13;
<p class="indent">When <code>interaction.plot</code> is called, the outcome means should be supplied to the argument <code>response</code>, and the vectors providing the corresponding levels of each of the two factors should be supplied to the arguments <code>x.factor</code> (for the variable on the horizontal axis that refers to moving between levels from the left to the right) and <code>trace.factor</code> (each level of which will produce a different line, referenced in an automatically produced legend; the title of this legend is passed to <code>trace.label</code>). It doesn’t matter which grouping variable is which; the appearance of the plot will change accordingly, but your interpretations will (should!) be the same. The result is shown in <a href="ch19.xhtml#ch19fig2">Figure 19-2</a>.</p>&#13;
<p class="indent">The two-way interaction plot displays the outcome variable on the vertical axis and splits the recorded means by the levels of the two grouping variables. This allows you to inspect the potential effect that varying the levels of the grouping variables has on the outcome. In general, when the lines (or segments thereof) are not parallel, it suggests an interaction could be present. Vertical separations between the plotted locations indicate the individual main effects of the grouping variables.</p>&#13;
<p class="indent">It turns out that the columns returned by a call to <code>aggregate</code> are actually perfectly suited to <code>interaction.plot</code>. As usual, you can specify the common graphical parameters, like those you initially encountered in <a href="ch07.xhtml#ch07lev1sec24">Section 7.2</a>, to control specific features of the plot and axis annotation. For <a href="ch19.xhtml#ch19fig2">Figure 19-2</a>, <span epub:type="pagebreak" id="page_447"/>you’ve specified that <code>x.factor</code> should be the second column of the <code>wb.means</code> matrix, meaning that the <code>tension</code> levels vary horizontally. The <code>trace.factor</code> here is the type of <code>wool</code>, so there are only two distinct lines corresponding to the two levels <code>A</code> and <code>B</code>. The <code>response</code> is that third column of <code>wb.means</code>, extracted using <code>$x</code> (take a look at the <code>wb.means</code> object; you’ll see the column containing the results of interest is labeled <code>x</code> by default after a call to <code>aggregate</code>).</p>&#13;
<div class="image"><img src="../images/f19-02.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch19fig2"/>Figure 19-2: An interaction plot for the full two-way ANOVA model of the</em> <code>warpbreaks</code> <em>data set</em></p>&#13;
<p class="indent">Considering the actual appearance of the plot in <a href="ch19.xhtml#ch19fig2">Figure 19-2</a>, it does indeed appear that the mean number of warp breaks for wool type <code>A</code> is higher if <code>tension</code> is low, but the nature of the difference changes if you move to a medium tension, where <code>B</code> has a higher point estimate than <code>A</code>. Moving to a high tension, type <code>A</code> again has a higher estimate of the mean number of breaks than <code>B</code>, though here the difference between <code>A</code> and <code>B</code> is nowhere near as big as it is at a low tension. (Note, however, that the interaction plot does not display any kind of standard error measurements, so you must remember that all point estimates of the means are subject to variability.)</p>&#13;
<p class="indent">Interactions are certainly not a concept unique to multiple-factor ANOVA; they form an important consideration in many different types of statistical models. For the moment, it’s good just to gain a basic appreciation of interactions.</p>&#13;
<h3 class="h3" id="ch19lev1sec61"><strong>19.3 Kruskal-Wallis Test</strong></h3>&#13;
<p class="noindent">When comparing multiple means, there may be situations when you’re unwilling to assume normality or have even found the assumption of normality invalid in diagnostic checks. In this case, you can use the <em>Kruskal-Wallis test</em>, an alternative to the one-way ANOVA that relaxes the dependence <span epub:type="pagebreak" id="page_448"/>on the necessity for normality. This method tests for “equality of distributions” of the measurements in each level of the grouping factor. If you make the usual assumption of equal variances across these groups, you can therefore think of this test as one that compares multiple medians rather than means.</p>&#13;
<p class="indentb">The hypotheses governing the test alter accordingly.</p>&#13;
<p class="hang">H<sub>0</sub> : Group medians are all equal.</p>&#13;
<p class="hang">H<sub>A</sub> : Group medians are not all equal</p>&#13;
<p class="hangp">(alternatively, at least one group median differs).</p>&#13;
<p class="indentt">The Kruskal-Wallis test is a <em>nonparametric</em> approach since it does not rely on quantiles of a standardized parametric distribution (in other words, the normal distribution) or any of its functions. In the same way that the ANOVA is a generalization of the two-sample <em>t</em>-test, the Kruskal-Wallis ANOVA is a generalization of the Mann-Whitney test for two medians. It’s also referred to as the Kruskal-Wallis <em>rank sum</em> test, and you use the chi-squared distribution to calculate the <em>p</em>-value.</p>&#13;
<p class="indent">Turn your attention to the data frame <code>survey</code>, located in the <code>MASS</code> package. These data record particular characteristics of 237 first-year undergraduate statistics students collected from a class at the University of Adelaide, South Australia. Load the required package first with a call to <code>library("MASS")</code> and then enter <code>?survey</code> at the prompt. You can read the help file to understand which variables are present in the data frame.</p>&#13;
<p class="indent">Suppose you’re interested to see whether the age of the students, <code>Age</code>, tends to differ with respect to four smoking categories reported in <code>Smoke</code>. An inspection of the relevant side-by-side boxplots and a normal QQ plot of the residuals (mean-centered observations with respect to each group) suggests a straightforward one-way ANOVA isn’t necessarily a good idea. The following code (which mimics the steps you saw in <a href="ch19.xhtml#ch19lev2sec168">Section 19.1.1</a>) produces the two images in <a href="ch19.xhtml#ch19fig3">Figure 19-3</a>, which show normality is questionable:</p>&#13;
<pre>R&gt; boxplot(Age~Smoke,data=survey)<br/>R&gt; age.means &lt;- tapply(survey$Age,survey$Smoke,mean)<br/>R&gt; age.meancen &lt;- survey$Age-age.means[as.numeric(survey$Smoke)]<br/>R&gt; qqnorm(age.meancen,main="Normal QQ plot of residuals")<br/>R&gt; qqline(age.meancen)</pre>&#13;
<p class="indent">With this possible violation of normality, you could therefore apply the Kruskal-Wallis test instead of the parametric ANOVA. A quick check for equality of variances further supports this, with the ratio of the largest to the smallest group standard deviations clearly being less than 2.</p>&#13;
<pre>R&gt; tapply(survey$Age,survey$Smoke,sd)<br/>   Heavy    Never    Occas    Regul<br/>6.332628 6.675257 5.861992 5.408822</pre>&#13;
<div class="image"><span epub:type="pagebreak" id="page_449"/><img src="../images/f19-03.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch19fig3"/>Figure 19-3: Side-by-side boxplots (left) and a normal QQ plot of the residuals (right) for the student age observations split by smoking status</em></p>&#13;
<p class="indent">In R, a Kruskal-Wallis test is performed using <code>kruskal.test</code>.</p>&#13;
<pre>R&gt; kruskal.test(Age~Smoke,data=survey)<br/><br/>        Kruskal-Wallis rank sum test<br/><br/>data:  Age by Smoke<br/>Kruskal-Wallis chi-squared = 3.9262, df = 3, p-value = 0.2695</pre>&#13;
<p class="indent">The syntax for this test is the same as for <code>aov</code>. As you might suspect from <a href="ch19.xhtml#ch19fig3">Figure 19-3</a>, the large <em>p</em>-value suggests there’s no evidence against the null hypothesis that states that the medians are all equal. In other words, there doesn’t seem to be an overall age difference between the students in the four smoking categories.</p>&#13;
<div class="ex">&#13;
<p class="ext"><a id="ch19exc2"/><strong>Exercise 19.2</strong></p>&#13;
<p class="noindentz">Bring up the <code>quakes</code> data frame again, which describes the locations, magnitudes, depths, and number of observation stations that detected 1,000 seismic events off the coast of Fiji.</p>&#13;
<ol type="a">&#13;
<li><p class="noindents">Use <code>cut</code> (see <a href="ch04.xhtml#ch04lev2sec48">Section 4.3.3</a>) to create a new factor vector defining the depths of each event according to the following three categories: (0,200], (200,400], and (400,680].</p></li>&#13;
<li><p class="noindents">Decide whether a one-way ANOVA or a Kruskal-Wallis test is more appropriate to use to compare the distributions of the number of detecting stations, split according to the three categories in (a).</p></li>&#13;
<li><p class="noindents"><span epub:type="pagebreak" id="page_450"/>Perform your choice of test in (b) (assume a <em>α</em> = 0.01 level of significance) and conclude.</p></li>&#13;
</ol>&#13;
<p class="noindentz">Load the <code>MASS</code> package with a call to <code>library("MASS")</code> if you haven’t already done so in the current R session. This package includes the ready-to-use <code>Cars93</code> data frame, which contains detailed data on 93 cars for sale in the United States in 1993 (<a href="ref.xhtml#ref43">Lock, 1993</a>; <a href="ref.xhtml#ref69">Venables and Ripley, 2002</a>).</p>&#13;
<ol type="a" start="4">&#13;
<li><p class="noindents">Use <code>aggregate</code> to compute the mean length of the 93 cars, split by two categorical variables: <code>AirBags</code> (type of airbags available—levels are <code>Driver &amp; Passenger</code>, <code>Driver only</code>, and <code>None</code>), and <code>Man.trans.avail</code> (whether the car comes in a manual transmission—levels are <code>Yes</code> and <code>No</code>).</p></li>&#13;
<li><p class="noindents">Produce an interaction plot using the results in (d). Does there appear to be an interactive effect of <code>AirBags</code> with <code>Man.trans.avail</code> on the mean length of these cars (if you consider only these variables)?</p></li>&#13;
<li><p class="noindents">Fit a full two-way ANOVA model for the mean lengths according to the two grouping variables (assume satisfaction of all relevant assumptions). Is the interactive effect statistically significant? Is there evidence of any main effects?</p></li>&#13;
</ol>&#13;
</div>&#13;
<h5 class="h5" id="ch19lev3sec87"><strong>Important Code in This Chapter</strong></h5>&#13;
<table class="topbot">&#13;
<thead>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table_th"><p class="table"><strong>Function/operator</strong></p></td>&#13;
<td style="vertical-align: top;" class="table_th"><p class="table"><strong>Brief description</strong></p></td>&#13;
<td style="vertical-align: top;" class="table_th"><p class="table"><strong>First occurrence</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>aov</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Produce ANOVA table</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch19.xhtml#ch19lev2sec170">Section 19.1.3</a>, <a href="ch19.xhtml#page_440">p. 440</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>aggregate</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Stacked statistics by factor</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch19.xhtml#ch19lev2sec171">Section 19.2.1</a>, <a href="ch19.xhtml#page_444">p. 444</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>interaction.plot</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Two-factor interaction plot</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch19.xhtml#ch19lev2sec172">Section 19.2.2</a>, <a href="ch19.xhtml#page_446">p. 446</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>kruskal.test</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Kruskal-Wallis test</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch19.xhtml#ch19lev1sec61">Section 19.3</a>, <a href="ch19.xhtml#page_449">p. 449</a></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
</body></html>