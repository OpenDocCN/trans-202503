<html><head></head><body>
<h2 class="h2" id="ch15"><span epub:type="pagebreak" id="page_735"/><span class="big">15</span><br/><strong>LOOKING AHEAD: OTHER PARALLEL SYSTEMS AND PARALLEL PROGRAMMING MODELS</strong></h2>&#13;
<div class="imagec"><img alt="image" src="../images/common.jpg"/></div>&#13;
<p class="noindents">In the previous chapter, we discussed shared memory parallelism and multithreaded programming. In this chapter, we introduce other parallel programming models and languages for different classes of architecture. Namely, we introduce parallelism for hardware accelerators focusing on graphics processing units (GPUs) and general-purpose computing on GPUs (GPGPU computing), using CUDA as an example; distributed memory systems and message passing, using MPI as an example; and cloud computing, using MapReduce and Apache Spark as examples.</p>&#13;
<h4 class="h4" id="lev2_256"><span epub:type="pagebreak" id="page_736"/>A Whole New World: Flynn’s Taxonomy of Architecture</h4>&#13;
<p class="noindent"><em>Flynn’s taxonomy</em> is commonly used to describe the ecosystem of modern computing architecture (<a href="ch15.xhtml#ch15fig1">Figure 15-1</a>).</p>&#13;
<div class="imagec" id="ch15fig1"><img alt="image" src="../images/15fig01.jpg"/></div>&#13;
<p class="figcap"><em>Figure 15-1: Flynn’s taxonomy classifies the ways in which a processor applies instructions.</em></p>&#13;
<p class="indent">The horizontal axis refers to the data stream, whereas the vertical axis refers to the instruction stream. A <em>stream</em> in this context is a flow of data or instructions. A <em>single stream</em> issues one element per time unit, similar to a queue. In contrast, <em>multiple streams</em> typically issue many elements per time unit (think of multiple queues). Thus, a single instruction stream (SI) issues a single instruction per time unit, whereas a multiple instruction stream (MI) issues many instructions per time unit. Likewise, a single data stream (SD) issues one data element per time unit, whereas a multiple data stream (MD) issues many data elements per time unit.</p>&#13;
<p class="indent">A processor can be classified into one of four categories based on the types of streams it employs.</p>&#13;
<p class="noindentt"><strong>SISD</strong>   Single instruction/single data systems have a single control unit processing a single stream of instructions, allowing it to execute only one instruction at a time. Likewise, the processor can process only a single stream of data or process one data unit at a time. Most commercially available processors prior to the mid-2000s were SISD machines.</p>&#13;
<p class="noindentt"><strong>MISD</strong>   Multiple instruction/single data systems have multiple instruction units performing on a single data stream. MISD systems were typically designed for incorporating fault tolerance in mission-critical systems, such as the flight control programs for NASA shuttles. That said, MISD machines are rarely used in practice anymore.</p>&#13;
<p class="noindentt"><strong>SIMD</strong>   Single instruction/multiple data systems execute the <em>same</em> instruction on multiple data simultaneously and in lockstep fashion. During “lockstep” execution, all instructions are placed into a queue, while data is distributed among different compute units. During execution, each compute unit executes the first instruction in the queue simultaneously, before simultaneously executing the next instruction in the queue, and then the next, <span epub:type="pagebreak" id="page_737"/>and so forth. The most well-known example of the SIMD architecture is the graphics processing unit. Early supercomputers also followed the SIMD architecture. We discuss GPUs more in the next section.</p>&#13;
<p class="noindentt"><strong>MIMD</strong>   Multiple instruction/multiple data systems represent the most widely used architecture class. They are extremely flexible and have the ability to work on multiple instructions or multiple data streams. Since nearly all modern computers use multicore CPUs, most are classified as MIMD machines. We discuss another class of MIMD systems, distributed memory systems, in “Distributed Memory Systems, Message Passing, and MPI” on <a href="ch15.xhtml#lev1_115">page 746</a>.</p>&#13;
<h3 class="h3" id="lev1_114">15.1 Heterogeneous Computing: Hardware Accelerators, GPGPU Computing, and CUDA</h3>&#13;
<p class="noindent"><em>Heterogeneous computing</em> is computing using multiple, different processing units found in a computer. These processing units often have different ISAs, some managed by the OS, and others not. Typically, heterogeneous computing means support for parallel computing using the computer’s CPU cores and one or more of its accelerator units such as <em>graphics processing units</em> (GPUs) or <em>field programmable gate arrays</em> (FPGAs).<sup><a href="ch15.xhtml#fn15_1" id="rfn15_1">1</a></sup></p>&#13;
<p class="indent">It is increasingly common for developers to implement heterogeneous computing solutions to large, data-intensive and computation-intensive problems. These types of problems are pervasive in scientific computing as well as in a more diverse range of applications to Big Data processing, analysis, and information extraction. By making use of the processing capabilities of both the CPU and the accelerator units that are available on a computer, a programmer can increase the degree of parallel execution in their application, resulting in improved performance and scalability.</p>&#13;
<p class="indent">In this section, we introduce heterogeneous computing using hardware accelerators to support general-purpose parallel computing. We focus on GPUs and the CUDA programming language.</p>&#13;
<h4 class="h4" id="lev2_257">15.1.1 Hardware Accelerators</h4>&#13;
<p class="noindent">In addition to the CPU, computers have other processing units that are designed to perform specific tasks. These units are not general-purpose processing units like the CPU, but are special-purpose hardware that is optimized to implement functionality that is specific to certain devices or that is used to perform specialized types of processing in the system. FPGAs, Cell processors, and GPUs are three examples of these types of processing units.</p>&#13;
<h5 class="h5" id="lev3_135">FPGAs</h5>&#13;
<p class="noindent">An FPGA is an integrated circuit that consists of gates, memory, and interconnection components. They are reprogrammable, meaning that they can be reconfigured to implement specific functionality in hardware, and they are often used to prototype application-specific integrated circuits (ASICs). <span epub:type="pagebreak" id="page_738"/>FPGAs typically require less power to run than a full CPU, resulting in energy-efficient operation. Some example ways in which FPGAs are integrated into a computer system include as device controllers, for sensor data processing, for cryptography, and for testing new hardware designs (because they are reprogrammable, designs can be implemented, debugged, and tested on an FPGA). FPGAs can be designed as a circuit with a high number of simple processing units. FPGAs are also low-latency devices that can be directly connected to system buses. As a result, they have been used to implement very fast parallel computation that consists of regular patterns of independent parallel processing on several data input channels. However, reprogramming FPGAs takes a long time, and their use is limited to supporting fast execution of specific parts of parallel workloads or for running a fixed program workload.<sup><a href="ch15.xhtml#fn15_2" id="rfn15_2">2</a></sup></p>&#13;
<h5 class="h5" id="lev3_136">GPUs and Cell Processors</h5>&#13;
<p class="noindent">A Cell processor is a multicore processor that consists of one general-purpose processor and multiple coprocessors that are specialized to accelerate a specific type of computation, such as multimedia processing. The Sony PlayStation 3 gaming system was the first Cell architecture, using the Cell coprocessors for fast graphics.</p>&#13;
<p class="indent">GPUs perform computer graphics computations—they operate on image data to enable high-speed graphics rendering and image processing. A GPU writes its results to a frame buffer, which delivers the data to the computer’s display. Driven by computer gaming applications, today sophisticated GPUs come standard in desktop and laptop systems.</p>&#13;
<p class="indent">In the mid 2000s, parallel computing researchers recognized the potential of using accelerators in combination with a computer’s CPU cores to support general-purpose parallel computing.</p>&#13;
<h4 class="h4" id="lev2_258">15.1.2 GPU Architecture Overview</h4>&#13;
<p class="noindent">GPU hardware is designed for computer graphics and image processing. Historically, GPU development has been driven by the video game industry. To support more detailed graphics and faster frame rendering, a GPU device consists of thousands of special-purpose processors, specifically designed to efficiently manipulate image data, such as the individual pixel values of a two-dimensional image, in parallel.</p>&#13;
<p class="indent">The hardware execution model implemented by GPUs is <em>single instruction</em>/<em>multiple thread</em> (SIMT), a variation of SIMD. SIMT is like multithreaded SIMD, where a single instruction is executed in lockstep by multiple threads running on the processing units. In SIMT, the total number of threads can be larger than the total number of processing units, requiring the scheduling of multiple groups of threads on the processors to execute the same sequence of instructions.</p>&#13;
<p class="indent">As an example, NVIDIA GPUs consist of several streaming multiprocessors (SMs), each of which has its own execution control units and memory space (registers, L1 cache, and shared memory). Each SM consists of several <span epub:type="pagebreak" id="page_739"/>scalar processor (SP) cores. The SM includes a warp scheduler that schedules <em>warps</em>, or sets of application threads, to execute in lockstep on its SP cores. In lockstep execution, each thread in a warp executes the same instruction each cycle but on different data. For example, if an application is changing a color image to grayscale, then each thread in a warp executes the same sequence of instructions at the same time to set a pixel’s RGB value to its grayscale equivalent. Each thread in the warp executes these instructions on a different pixel data value, resulting in multiple pixels of the image being updated in parallel. Because the threads are executed in lockstep, the processor design can be simplified so that multiple cores share the same instruction control units. Each unit contains cache memory and multiple registers that it uses to hold data as it’s manipulated in lockstep by the parallel processing cores.</p>&#13;
<p class="indent"><a href="ch15.xhtml#ch15fig2">Figure 15-2</a> shows a simplified GPU architecture that includes a detailed view of one of its SM units. Each SM consists of multiple SP cores, a warp scheduler, an execution control unit, an L1 cache, and shared memory space.</p>&#13;
<div class="imagec" id="ch15fig2"><img alt="image" src="../images/15fig02.jpg"/></div>&#13;
<p class="figcap"><em>Figure 15-2: An example of a simplified GPU architecture with 2,048 cores. This shows the GPU divided into 64 SM units, and the details of one SM consisting of 32 SP cores. The SM’s warp scheduler schedules thread warps on its SPs. A warp of threads executes in lockstep on the SP cores.</em></p>&#13;
<h4 class="h4" id="lev2_259">15.1.3 GPGPU Computing</h4>&#13;
<p class="noindent"><em>General Purpose GPU</em> (GPGPU) computing applies special-purpose GPU processors to general-purpose parallel computing tasks. GPGPU computing <span epub:type="pagebreak" id="page_740"/>combines computation on the host CPU cores with SIMT computation on the GPU processors. GPGPU computing performs best on parallel applications (or parts of applications) that can be constructed as a stream processing computation on a grid of multidimensional data.</p>&#13;
<p class="indent">The host operating system does not manage the GPU’s processors or memory. As a result, space for program data needs to be allocated on the GPU and the data copied between the host memory and the GPU memory by the programmer. GPGPU programming languages and libraries typically provide programming interfaces to GPU memory that hide some or all of the difficulty of explicitly managing GPU memory from the programmer. For example, in CUDA a programmer can include calls to CUDA library functions to explicitly allocate CUDA memory on the GPU and to copy data between CUDA memory on the GPU and host memory. A CUDA programmer can also use CUDA unified memory, which is CUDA’s abstraction of a single memory space on top of host and GPU memory. CUDA unified memory hides the separate GPU and host memory, and the memory copies between the two, from the CUDA programmer.</p>&#13;
<p class="indent">GPUs also provide limited support for thread synchronization, which means that GPGPU parallel computing performs particularly well for parallel applications that are either embarrassingly parallel or have large extents of independent parallel stream-based computation with very few synchronization points. GPUs are massively parallel processors, and any program that performs long sequences of independent identical (or mostly identical) computation steps on data may perform well as a GPGPU parallel application. GPGPU computing also performs well when there are few memory copies between host and device memory. If GPU–CPU data transfer dominates execution time, or if an application requires fine-grained synchronization, GPGPU computing may not perform well or provide much, if any, gain over a multithreaded CPU version of the program.</p>&#13;
<h4 class="h4" id="lev2_260">15.1.4 CUDA</h4>&#13;
<p class="noindent">CUDA (Compute Unified Device Architecture)<sup><a href="ch15.xhtml#fn15_3" id="rfn15_3">3</a></sup> is NVIDIA’s programming interface for GPGPU computing on its graphics devices. CUDA is designed for heterogeneous computing in which some program functions run on the host CPU, and others run on the GPU device. Programmers typically write CUDA programs in C or C++ with annotations that specify CUDA kernel functions, and they make calls to CUDA library functions to manage GPU device memory. A CUDA <em>kernel function</em> is a function that is executed on the GPU, and a CUDA <em>thread</em> is the basic unit of execution in a CUDA program. CUDA threads are scheduled in warps that execute in lockstep on the GPU’s SMs, executing CUDA kernel code on their part of data stored in GPU memory. Kernel functions are annotated with <code>__global__</code> to distinguish them from host functions. CUDA <code>__device__</code> functions are helper functions that can be called from a CUDA kernel function.</p>&#13;
<p class="indent">The memory space of a CUDA program is separated into host and GPU memory. The program must explicitly allocate and free GPU memory space to store program data manipulated by CUDA kernels. The CUDA programmer <span epub:type="pagebreak" id="page_741"/>must either explicitly copy data to and from the host and GPU memory, or use CUDA unified memory that presents a view of memory space that is directly shared by the GPU and host. Here is an example of CUDA’s basic memory allocation, memory deallocation, and explicit memory copy functions:</p>&#13;
<pre>&#13;
/* "returns" through pass-by-pointer param dev_ptr GPU memory of size bytes<br/>&#13;
 * returns cudaSuccess or a cudaError value on error<br/>&#13;
 */<br/>&#13;
cudaMalloc(void **dev_ptr, size_t size);<br/>&#13;
<br/>&#13;
/* free GPU memory<br/>&#13;
 * returns cudaSuccess or cudaErrorInvalidValue on error<br/>&#13;
 */<br/>&#13;
cudaFree(void *data);<br/>&#13;
<br/>&#13;
/* copies data from src to dst, direction is based on value of kind<br/>&#13;
 *   kind: cudaMemcpyHosttoDevice is copy from cpu to gpu memory<br/>&#13;
 *   kind: cudaMemcpyDevicetoHost is copy from gpu to cpu memory<br/>&#13;
 * returns cudaSuccess or a cudaError value on error<br/>&#13;
 */<br/>&#13;
cudaMemcpy(void *dst, const void *src, size_t count, cudaMemcpyKind kind);</pre>&#13;
<p class="indent">CUDA threads are organized into <em>blocks</em>, and the blocks are organized into a <em>grid</em>. Grids can be organized into one-, two-, or three-dimensional groupings of blocks. Blocks, likewise, can be organized into one-, two-, or three-dimensional groupings of threads. Each thread is uniquely identified by its thread (<em>x</em>,<em>y</em>,<em>z</em>) position in its containing block’s (<em>x</em>,<em>y</em>,<em>z</em>) position in the grid. For example, a programmer could define two-dimensional block and grid dimensions as the following:</p>&#13;
<pre>&#13;
dim3 blockDim(16,16);  // 256 threads per block, in a 16x16 2D arrangement<br/>&#13;
dim3 gridDim(20,20);   // 400 blocks per grid, in a 20x20 2D arrangement</pre>&#13;
<p class="indent">When a kernel is invoked, its blocks/grid and thread/block layout is specified in the call. For example, here is a call to a kernel function named <code>do_something</code> specifying the grid and block layout using <code>gridDim</code> and <code>blockDim</code> defined above (and passing parameters <code>dev_array</code> and 100):</p>&#13;
<pre>ret = do_something&lt;&lt;&lt;gridDim,blockDim&gt;&gt;&gt;(dev_array, 100);</pre>&#13;
<p class="indent"><a href="ch15.xhtml#ch15fig3">Figure 15-3</a> shows an example of a two-dimensional arrangement of thread blocks. In this example, the grid is a 3 × 2 array of blocks, and each block is a 4× 3 array of threads.</p>&#13;
<div class="imagec" id="ch15fig3"><span epub:type="pagebreak" id="page_742"/><img alt="image" src="../images/15fig03.jpg"/></div>&#13;
<p class="figcap"><em>Figure 15-3: The CUDA thread model. A grid of blocks of threads. Blocks and threads can be organized into one-, two-, or three-dimensional layouts. This example shows a grid of two-dimensional blocks, 3 × 2 blocks per grid, and each block has a two-dimensional set of threads, 4 × 3 threads per block).</em></p>&#13;
<p class="indent">A thread’s position in this layout is given by the (<em>x</em>,<em>y</em>) coordinate in its containing block (<code>threadId.x</code>, <code>threadId.y</code>) and by the (<em>x</em>,<em>y</em>) coordinate of its block in the grid (<code>blockIdx.x</code>, <code>blockIdx.y</code>). Note that block and thread coordinates are (<em>x</em>,<em>y</em>) based, with the <em>x</em>-axis being horizontal, and the <em>y</em>-axis vertical. The (0,0) element is in the upper left. The CUDA kernel also has variables that are defined to the block dimensions (<code>blockDim.x</code> and <code>blockDim.y</code>). Thus, for any thread executing the kernel, its (row, col) position in the two-dimensional array of threads in the two-dimensional array of blocks can be logically identified as follows:</p>&#13;
<pre>int row = blockIdx.y * blockDim.y + threadIdx.y;<br/>&#13;
int col = blockIdx.x * blockDim.x + threadIdx.x;</pre>&#13;
<p class="indent">Although not strictly necessary, CUDA programmers often organize blocks and threads to match the logical organization of program data. For example, if a program is manipulating a two-dimensional matrix, it often makes sense to organize threads and blocks into a two-dimensional arrangement. This way, a thread’s block (<em>x</em>,<em>y</em>) and its thread (<em>x</em>,<em>y</em>) within a block can be used to associate a thread’s position in the two-dimensional blocks of threads with one or more data values in the two-dimensional array.</p>&#13;
<h5 class="h5" id="lev3_137">Example CUDA Program: Scalar Multiply</h5>&#13;
<p class="noindent">As an example, consider a CUDA program that performs scalar multiplication of a vector:</p>&#13;
<pre>x = a * x    // where x is a vector and a is a scalar value</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_743"/>Because the program data comprises one-dimensional arrays, using a one-dimensional layout of blocks/grid and threads/block works well. This is not necessary, but it makes the mapping of threads to data easier.</p>&#13;
<p class="indent">When run, the main function of this program will do the following:</p>&#13;
<div class="number">&#13;
<p class="number">1. Allocate host-side memory for the vector <code>x</code> and initialize it.</p>&#13;
<p class="number">2. Allocate device-side memory for the vector <code>x</code> and copy it from host memory to GPU memory.</p>&#13;
<p class="number">3. Invoke a CUDA kernel function to perform vector scalar multiply in parallel, passing as arguments the device address of the vector <code>x</code> and the scalar value <code>a</code>.</p>&#13;
<p class="number">4. Copy the result from GPU memory to host memory vector <code>x</code>.</p>&#13;
</div>&#13;
<p class="indent">In the example that follows, we show a CUDA program that performs these steps to implement scalar vector multiplication. We have removed some error handling and details from the code listing, but the full solution is available online.<sup><a href="ch15.xhtml#fn15_4" id="rfn15_4">4</a></sup></p>&#13;
<p class="indent">The main function of the CUDA program performs the aforementioned steps:</p>&#13;
<pre>&#13;
#include &lt;cuda.h&gt;<br/>&#13;
<br/>&#13;
#define BLOCK_SIZE       64     /* threads per block */<br/>&#13;
#define N              10240    /* vector size */<br/>&#13;
<br/>&#13;
// some host-side init function<br/>&#13;
void init_array(int *vector, int size, int step);<br/>&#13;
<br/>&#13;
// host-side function: main<br/>&#13;
int main(int argc, char **argv) {<br/>&#13;
<br/>&#13;
  int *vector, *dev_vector, scalar;<br/>&#13;
<br/>&#13;
  scalar = 3;     // init scalar to some default value<br/>&#13;
  if(argc == 2) { // get scalar's value from a command line argument<br/>&#13;
    scalar = atoi(argv[1]);<br/>&#13;
  }<br/>&#13;
<br/>&#13;
  // 1. allocate host memory space for the vector (missing error handling)<br/>&#13;
  vector = (int *)malloc(sizeof(int)*N);<br/>&#13;
<br/>&#13;
  // initialize vector in host memory<br/>&#13;
  // (a user-defined initialization function not listed here)<br/>&#13;
  init_array(vector, N, 7);<br/>&#13;
<br/>&#13;
  // 2. allocate GPU device memory for vector (missing error handling)<br/>&#13;
  cudaMalloc(&amp;dev_vector, sizeof(int)*N);<br/>&#13;
<br/>&#13;
<span epub:type="pagebreak" id="page_744"/>  // 2. copy host vector to device memory (missing error handling)<br/>&#13;
  cudaMemcpy(dev_vector, vector, sizeof(int)*N, cudaMemcpyHostToDevice;)<br/>&#13;
<br/>&#13;
  // 3. call the CUDA scalar_multiply kernel<br/>&#13;
  // specify the 1D layout for blocks/grid (N/BLOCK_SIZE)<br/>&#13;
  //    and the 1D layout for threads/block (BLOCK_SIZE)<br/>&#13;
  scalar_multiply&lt;&lt;&lt;(N/BLOCK_SIZE), BLOCK_SIZE===(dev_vector, scalar);<br/>&#13;
<br/>&#13;
  // 4. copy device vector to host memory (missing error handling)<br/>&#13;
  cudaMemcpy(vector, dev_vector, sizeof(int)*N, cudaMemcpyDeviceToHost);<br/>&#13;
<br/>&#13;
  // ...(do something on the host with the result copied into vector)<br/>&#13;
<br/>&#13;
  // free allocated memory space on host and GPU<br/>&#13;
  cudaFree(dev_vector);<br/>&#13;
  free(vector);<br/>&#13;
<br/>&#13;
  return 0;<br/>&#13;
}</pre>&#13;
<p class="indent">Each CUDA thread executes the CUDA kernel function <code>scalar_multiply</code>. A CUDA kernel function is written from an individual thread’s point of view. It typically consists of two main steps: (1) the calling thread determines which portion of the data it is responsible for based on its thread’s position in its enclosing block and its block’s position in the grid; (2) the calling thread performs application-specific computation on its portion of the data. In this example, each thread is responsible for computing scalar multiplication on exactly one element in the array. The kernel function code first calculates a unique index value based on the calling thread’s block and thread identifier. It then uses this value as an index into the array of data to perform scalar multiplication on its array element (<code>array[index] =</code> <code>array[index] * scalar</code>). CUDA threads running on the GPU’s SM units each compute a different index value to update array elements in parallel.</p>&#13;
<pre>&#13;
/*<br/>&#13;
 * CUDA kernel function that performs scalar multiply<br/>&#13;
 * of a vector on the GPU device<br/>&#13;
 *<br/>&#13;
 * This assumes that there are enough threads to associate<br/>&#13;
 * each array[i] element with a signal thread<br/>&#13;
 * (in general, each thread would be responsible for a set of data elements)<br/>&#13;
 */<br/>&#13;
__global__ void scalar_multiply(int *array, int scalar) {<br/>&#13;
<br/>&#13;
  int index;<br/>&#13;
<br/>&#13;
  // compute the calling thread's index value based on<br/>&#13;
  // its position in the enclosing block and grid<br/>&#13;
<span epub:type="pagebreak" id="page_745"/>  index = blockIdx.x * blockDim.x + threadIdx.x;<br/>&#13;
<br/>&#13;
  // the thread's uses its index value is to<br/>&#13;
  // perform scalar multiply on its array element<br/>&#13;
  array[index] = array[index] * scalar;<br/>&#13;
}</pre>&#13;
<h5 class="h5" id="lev3_138">CUDA Thread Scheduling and Synchronization</h5>&#13;
<p class="noindent">Each CUDA thread block is run by a GPU SM unit. An SM schedules a warp of threads from the same thread block to run its processor cores. All threads in a warp execute the same set of instructions in lockstep, typically on different data. Threads share the instruction pipeline but get their own registers and stack space for local variables and parameters.</p>&#13;
<p class="indent">Because blocks of threads are scheduled on individual SMs, increasing the threads per block increases the degree of parallel execution. Because the SM schedules thread warps to run on its processing units, if the number of threads per block is a multiple of the warp size, then no SM processor cores are wasted in the computation. In practice, using a number of threads per block that is a small multiple of the number of processing cores of an SM works well.</p>&#13;
<p class="indent">CUDA guarantees that all threads from a single kernel call complete before any threads from a subsequent kernel call are scheduled. Thus, there is an implicit synchronization point between separate kernel calls. Within a single kernel call, however, thread blocks are scheduled to run the kernel code in any order on the GPU SMs. As a result, a programmer should not assume any ordering of execution between threads in different thread blocks. CUDA provides some support for synchronizing threads, but only for threads that are in the same thread block.</p>&#13;
<h4 class="h4" id="lev2_261">15.1.5 Other Languages for GPGPU Programming</h4>&#13;
<p class="noindent">There are other programming languages for GPGPU computing. OpenCL, OpenACC, and OpenHMPP are three examples of languages that can be used to program any graphics device (they are not specific to NVIDIA devices). OpenCL (Open Computing Language) has a similar programming model to CUDA’s; both implement a lower-level programming model (or implement a thinner programming abstraction) on top of the target architectures. OpenCL targets a wide range of heterogeneous computing platforms that include a host CPU combined with other compute units, which could include CPUs or accelerators such as GPUs and FPGAs. OpenACC (Open Accelerator) is a higher-level abstraction programming model than CUDA or OpenCL. It is designed for portability and programmer ease. A programmer annotates portions of their code for parallel execution, and the compiler generates parallel code that can run on GPUs. OpenHMPP (Open Hybrid Multicore Programming) is another language that provides a higher-level programming abstraction for heterogeneous programming.</p>&#13;
<h3 class="h3" id="lev1_115"><span epub:type="pagebreak" id="page_746"/>15.2 Distributed Memory Systems, Message Passing, and MPI</h3>&#13;
<p class="noindent"><a href="ch14.xhtml#ch14">Chapter 14</a> describes mechanisms like Pthreads (see “Hello Threading! Writing Your First Multithreaded Program” on <a href="ch14.xhtml#lev1_106">page 677</a>) and OpenMP (see “Implicit Threading with OpenMP” on <a href="ch14.xhtml#lev1_111">page 729</a>) that programs use to take advantage of multiple CPU cores on a <em>shared memory system</em>. In such systems, each core shares the same physical memory hardware, allowing them to communicate data and synchronize their behavior by reading from and writing to shared memory addresses. Although shared memory systems make communication relatively easy, their scalability is limited by the number of CPU cores in the system.</p>&#13;
<p class="indent">As of 2019, high-end commercial server CPUs generally provide a maximum of 64 cores. For some tasks, though, even a few hundred CPU cores isn’t close enough. For example, imagine trying to simulate the fluid dynamics of the Earth’s oceans or index the entire contents of the World Wide Web to build a search application. Such massive tasks require more physical memory and processors than any single computer can provide. Thus, applications that require a large number of CPU cores run on systems that forego shared memory. Instead, they execute on systems built from multiple computers, each with their own CPU(s) and memory, that communicate over a network to coordinate their behavior.</p>&#13;
<p class="indent">A collection of computers working together is known as a <em>distributed memory system</em> (or often just <em>distributed system</em>).</p>&#13;
<p class="note"><strong><span class="black">Warning</span> A NOTE ON CHRONOLOGY</strong></p>&#13;
<p class="note-w">Despite the order in which they’re presented in this book, systems designers built distributed systems long before mechanisms like threads or OpenMP existed.</p>&#13;
<p class="indent">Some distributed memory systems integrate hardware more closely than others. For example, a <em>supercomputer</em> is a high-performance system in which many <em>compute nodes</em> are tightly coupled (closely integrated) to a fast interconnection network. Each compute node contains its own CPU(s), GPU(s), and memory, but multiple nodes might share auxiliary resources like secondary storage and power supplies. The exact level of hardware sharing varies from one supercomputer to another.</p>&#13;
<p class="indent">On the other end of the spectrum, a distributed application might run on a loosely coupled (less integrated) collection of fully autonomous computers (<em>nodes</em>) connected by a traditional local area network (LAN) technology like Ethernet. Such a collection of nodes is known as a <em>commodity off-the-shelf</em> (COTS) cluster. COTS clusters typically employ a <em>shared-nothing architecture</em> in which each node contains its own set of computation hardware (i.e., CPU(s), GPU(s), memory, and storage). <a href="ch15.xhtml#ch15fig4">Figure 15-4</a> illustrates a shared-nothing distributed system consisting of two shared-memory computers.</p>&#13;
<div class="imagec" id="ch15fig4"><span epub:type="pagebreak" id="page_747"/><img alt="image" src="../images/15fig04.jpg"/></div>&#13;
<p class="figcap"><em>Figure 15-4: The major components of a shared-nothing distributed memory architecture built from two compute nodes</em></p>&#13;
<h4 class="h4" id="lev2_262">15.2.1 Parallel and Distributed Processing Models</h4>&#13;
<p class="noindent">Application designers often organize distributed applications using tried-and-true designs. Adopting application models like these helps developers reason about an application because its behavior will conform to well-understood norms. Each model has its unique benefits and drawbacks—there’s no one-size-fits-all solution. We briefly characterize a few of the more common models in the subsections that follow, but note that we’re not presenting an exhaustive list.</p>&#13;
<h5 class="h5" id="lev3_139">Client/Server</h5>&#13;
<p class="noindent">The <em>client/server model</em> is an extremely common application model that divides an application’s responsibilities among two actors: client processes and server processes. A server process provides a service to clients that ask for something to be done. Server processes typically wait at well-known addresses to receive incoming connections from clients. Upon making a connection, a client sends requests to the server process, which either satisfies those requests (e.g., by fetching a requested file) or reports an error (e.g., the file doesn’t exist or the client can’t be properly authenticated).</p>&#13;
<p class="indent">Although you may not have considered it, you access web pages via the client/server model! Your web browser (client) connects to a website (server) at a public address (e.g., <code><a href="http://diveintosystems.org">diveintosystems.org</a></code>) to retrieve the page’s contents.</p>&#13;
<h5 class="h5" id="lev3_140"><span epub:type="pagebreak" id="page_748"/>Pipeline</h5>&#13;
<p class="noindent">The <em>pipeline model</em> divides an application into a distinct sequence of steps, each of which can process data independently. This model works well for applications whose workflow involves linear, repetitive tasks over large data inputs. For example, consider the production of computer-animated films. Each frame of the film must be processed through a sequence of steps that transform the frame (e.g., adding textures or applying lighting). Because each step happens independently in a sequence, animators can speed up rendering by processing frames in parallel across a large cluster of computers.</p>&#13;
<h5 class="h5" id="lev3_141">Boss/Worker</h5>&#13;
<p class="noindent">In the <em>boss/worker model</em>, one process acts as a central coordinator and distributes work among the processes at other nodes. This model works well for problems that require processing a large, divisible input. The boss divides the input into smaller pieces and assigns one or more pieces to each worker. In some applications, the boss might statically assign each worker exactly one piece of the input. In other cases, the workers might repeatedly finish a piece of the input and then return to the boss to dynamically retrieve the next input chunk. Later in this section, we’ll present an example program in which a boss divides an array among many workers to perform scalar multiplication on an array.</p>&#13;
<p class="indent">Note that this model is sometimes called other names, like “master/ worker” or other variants, but the main idea is the same.</p>&#13;
<h5 class="h5" id="lev3_142">Peer-to-Peer</h5>&#13;
<p class="noindent">Unlike the boss/worker model, a <em>peer-to-peer</em> application avoids relying on a centralized control process. Instead, peer processes self-organize the application into a structure in which they each take on roughly the same responsibilities. For example, in the BitTorrent file sharing protocol, each peer repeatedly exchanges parts of a file with others until they’ve all received the entire file.</p>&#13;
<p class="indent">Lacking a centralized component, peer-to-peer applications are generally robust to node failures. On the other hand, peer-to-peer applications typically require complex coordination algorithms, making them difficult to build and rigorously test.</p>&#13;
<h4 class="h4" id="lev2_263">15.2.2 Communication Protocols</h4>&#13;
<p class="noindent">Whether they are part of a supercomputer or a COTS cluster, processes in a distributed memory system communicate via <em>message passing</em>, whereby one process explicitly sends a message to processes on one or more other nodes, which receive it. It’s up to the applications running on the system to determine how to utilize the network—some applications require frequent communication to tightly coordinate the behavior of processes across many nodes, whereas other applications communicate to divide up a large input among processes and then mostly work independently.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_749"/>A distributed application formalizes its communication expectations by defining a communication <em>protocol</em>, which describes a set of rules that govern its use of the network, including:</p>&#13;
<ul>&#13;
<li class="noindent">When a process should send a message</li>&#13;
<li class="noindent">To which process(es) it should send the message</li>&#13;
<li class="noindent">How to format the message</li>&#13;
</ul>&#13;
<p class="indent">Without a protocol, an application might fail to interpret messages properly or even deadlock (see “Deadlock” on <a href="ch14.xhtml#lev3_120">page 700</a>). For example, if an application consists of two processes, and each process waits for the other to send it a message, neither process will ever make progress. Protocols add structure to communication to reduce the likelihood of such failures.</p>&#13;
<p class="indent">To implement a communication protocol, applications require basic functionality for tasks like sending and receiving messages, naming processes (addressing), and synchronizing process execution. Many applications look to the Message Passing Interface for such functionality.</p>&#13;
<h4 class="h4" id="lev2_264">15.2.3 Message Passing Interface</h4>&#13;
<p class="noindent">The <em>Message Passing Interface</em> (MPI) defines (but does not itself implement) a standardized interface that applications can use to communicate in a distributed memory system. By adopting the MPI communication standard, applications become <em>portable</em>, meaning that they can be compiled and executed on many different systems. In other words, as long as an MPI implementation is installed, a portable application can move from one system to another and expect to execute properly, even if the systems have different underlying characteristics.</p>&#13;
<p class="indent">MPI allows a programmer to divide an application into multiple processes. It assigns each of an application’s processes a unique identifier, known as a <em>rank</em>, which ranges from 0 to <em>N –</em> 1 for an application with <em>N</em> processes. A process can learn its rank by calling the <code>MPI_Comm_rank</code> function, and it can learn how many processes are executing in the application by calling <code>MPI_Comm</code> <code>_size</code>. To send a message, a process calls <code>MPI_Send</code> and specifies the rank of the intended recipient. Similarly, a process calls <code>MPI_Recv</code> to receive a message, and it specifies whether to wait for a message from a specific node or to receive a message from any sender (using the constant <code>MPI_ANY_SOURCE</code> as the rank).</p>&#13;
<p class="indent">In addition to the basic send and receive functions, MPI also defines a variety of functions that make it easier for one process to communicate data to multiple recipients. For example, <code>MPI_Bcast</code> allows one process to send a message to every other process in the application with just one function call. It also defines a pair of functions, <code>MPI_Scatter</code> and <code>MPI_Gather</code>, that allow one process to divide up an array and distribute the pieces among processes (scatter), operate on the data, and then later retrieve all the data to coalesce the results (gather).</p>&#13;
<p class="indent">Because MPI <em>specifies</em> only a set of functions and how they should behave, each system designer can implement MPI’s functionality in a way that <span epub:type="pagebreak" id="page_750"/>matches the capabilities of their particular system. For example, a system with an interconnect network that supports broadcasting (sending one copy of a message to multiple recipients at the same time) might be able to implement MPI’s <code>MPI_Bcast</code> function more efficiently than a system without such support.</p>&#13;
<h4 class="h4" id="lev2_265">15.2.4 MPI Hello World</h4>&#13;
<p class="noindent">As an introduction to MPI programming, consider the “Hello World” program<sup><a href="ch15.xhtml#fn15_5" id="rfn15_5">5</a></sup> presented here:</p>&#13;
<pre>&#13;
#include &lt;stdio.h&gt;<br/>&#13;
#include &lt;unistd.h&gt;<br/>&#13;
#include "mpi.h"<br/>&#13;
<br/>&#13;
int main(int argc, char **argv) {<br/>&#13;
    int rank, process_count;<br/>&#13;
    char hostname[1024];<br/>&#13;
<br/>&#13;
    /* Initialize MPI. */<br/>&#13;
    MPI_Init(&amp;argc, &amp;argv);<br/>&#13;
<br/>&#13;
    /* Determine how many processes there are and which one this is. */<br/>&#13;
    MPI_Comm_size(MPI_COMM_WORLD, &amp;process_count);<br/>&#13;
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);<br/>&#13;
<br/>&#13;
    /* Determine the name of the machine this process is running on. */<br/>&#13;
    gethostname(hostname, 1024);<br/>&#13;
<br/>&#13;
    /* Print a message, identifying the process and machine it comes from. */<br/>&#13;
    printf("Hello from %s process %d of %d\n", hostname, rank, process_count);<br/>&#13;
<br/>&#13;
    /* Clean up. */<br/>&#13;
    MPI_Finalize();<br/>&#13;
<br/>&#13;
    return 0;<br/>&#13;
}</pre>&#13;
<p class="indent">When starting this program, MPI simultaneously executes multiple copies of it as independent processes across one or more computers. Each process makes calls to MPI to determine how many total processes are executing (with <code>MPI_Comm_size</code>) and which process it is among those processes (the process’s rank, with <code>MPI_Comm_rank</code>). After looking up this information, each process prints a short message containing the rank and name of the computer (<code>hostname</code>) it’s running on before terminating.</p>&#13;
<p class="note"><span epub:type="pagebreak" id="page_751"/><strong><span class="black">Note</span> RUNNING MPI CODE</strong></p>&#13;
<p class="note1">To run these MPI examples, you’ll need an MPI implementation like OpenMPI<sup><a href="ch15.xhtml#fn15_6" id="rfn15_6">6</a></sup> or MPICH<sup><a href="ch15.xhtml#fn15_7" id="rfn15_7">7</a></sup> installed on your system.</p>&#13;
<p class="indent">To compile this example, invoke the <code>mpicc</code> compiler program, which executes an MPI-aware version of GCC to build the program and link it against MPI libraries:</p>&#13;
<pre>$ <span class="codestrong1">mpicc -o hello_world_mpi hello_world_mpi.c</span></pre>&#13;
<p class="indent">To execute the program, use the <code>mpirun</code> utility to start up several parallel processes with MPI. The <code>mpirun</code> command needs to be told which computers to run processes on (<code>--hostfile</code>) and how many processes to run at each machine (<code>-np</code>). Here, we provide it with a file named <code>hosts.txt</code> that tells <code>mpirun</code> to create four processes across two computers, one named <code>lemon</code>, and another named <code>orange</code>:</p>&#13;
<pre>&#13;
$ <span class="codestrong1">mpirun -np 8 --hostfile hosts.txt ./hello_world_mpi</span><br/>&#13;
Hello from lemon process 4 of 8<br/>&#13;
Hello from lemon process 5 of 8<br/>&#13;
Hello from orange process 2 of 8<br/>&#13;
Hello from lemon process 6 of 8<br/>&#13;
Hello from orange process 0 of 8<br/>&#13;
Hello from lemon process 7 of 8<br/>&#13;
Hello from orange process 3 of 8<br/>&#13;
Hello from orange process 1 of 8</pre>&#13;
<p class="note"><strong><span class="black">Warning</span> MPI EXECUTION ORDER</strong></p>&#13;
<p class="note-w">You should <em>never</em> make any assumptions about the order in which MPI pro- cesses will execute. The processes start up on multiple machines, each of which has its own OS and process scheduler. If the correctness of your program requires that processes run in a particular order, you must ensure that the proper order occurs—for example, by forcing certain processes to pause until they receive a message.</p>&#13;
<h4 class="h4" id="lev2_266">15.2.5 MPI Scalar Multiplication</h4>&#13;
<p class="noindent">For a more substantive MPI example, consider performing scalar multiplication on an array. This example adopts the boss/worker model—one process divides the array into smaller pieces and distributes them among worker processes. Note that in this implementation of scalar multiplication, the boss process also behaves as a worker and multiplies part of the array after distributing sections to the other workers.</p>&#13;
<p class="indent">To benefit from working in parallel, each process multiplies just its local piece of the array by the scalar value, and then the workers all send the results back to the boss process to form the final result. At several points in the program, the code checks to see whether the rank of the process is zero.</p>&#13;
<pre>&#13;
<span epub:type="pagebreak" id="page_752"/>if (rank == 0) {<br/>&#13;
    /* This code only executes at the boss. */<br/>&#13;
}</pre>&#13;
<p class="indent">This check ensures that only one process (the one with rank 0) plays the role of the boss. By convention, MPI applications often choose rank 0 to perform one-time tasks because no matter how many processes there are, one will always be given rank 0 (even if just a single process is executing).</p>&#13;
<h5 class="h5" id="lev3_143">MPI Communication</h5>&#13;
<p class="noindent">The boss process begins by determining the scalar value and initial input array. In a real scientific computing application, the boss would likely read such values from an input file. To simplify this example, the boss uses a constant scalar value (10) and generates a simple 40-element array (containing the sequence 0 to 39) for illustrative purposes.</p>&#13;
<p class="indent">This program requires communication between MPI processes for three important tasks:</p>&#13;
<div class="number">&#13;
<p class="number">1. The boss sends the scalar value and the size of the array to <em>all</em> of the workers.</p>&#13;
<p class="number">2. The boss divides the initial array into pieces and sends a piece to each worker.</p>&#13;
<p class="number">3. Each worker multiplies the values in its piece of the array by the scalar and then sends the updated values back to the boss.</p>&#13;
</div>&#13;
<h5 class="h5" id="lev3_144">Broadcasting Important Values</h5>&#13;
<p class="noindent">To send the scalar value to the workers, the example program uses the <code>MPI_Bcast</code> function, which allows one MPI process to send the same value to all the other MPI processes with one function call:</p>&#13;
<pre>&#13;
/* Boss sends the scalar value to every process with a broadcast. */<br/>&#13;
MPI_Bcast(&amp;scalar, 1, MPI_INT, 0, MPI_COMM_WORLD);</pre>&#13;
<p class="indent">This call sends one integer (<code>MPI_INT</code>) starting from the address of the <code>scalar</code> variable from the process with rank 0 to every other process (<code>MPI_COMM</code> <code>_WORLD</code>). All the worker processes (those with nonzero rank) receive the broadcast into their local copy of the <code>scalar</code> variable, so when this call completes, every process knows the scalar value to use.</p>&#13;
<p class="note"><strong><span class="black">Note</span> MPI_BCAST BEHAVIOR</strong></p>&#13;
<p class="note1">Every process executes <code>MPI_Bcast</code>, but it behaves differently depending on the rank of the calling process. If the rank matches that of the fourth argument, then the caller assumes the role of the sender. All other processes that call <code>MPI_Bcast</code> act as receivers.</p>&#13;
<p class="indent">Similarly, the boss broadcasts the total size of the array to every other process. After learning the total array size, each process sets a <code>local_size</code> <span epub:type="pagebreak" id="page_753"/>variable by dividing the total array size by the number of MPI processes. The <code>local_size</code> variable represents how many elements each worker’s piece of the array will contain. For example, if the input array contains 40 elements and the application consists of eight processes, each process is responsible for a five-element piece of the array (40 / 8 = 5). To keep the example simple, it assumes that the number of processes evenly divides the size of the array:</p>&#13;
<pre>&#13;
/* Each process determines how many processes there are. */<br/>&#13;
MPI_Comm_size(MPI_COMM_WORLD, &amp;process_count);<br/>&#13;
<br/>&#13;
/* Boss sends the total array size to every process with a broadcast. */<br/>&#13;
MPI_Bcast(&amp;array_size, 1, MPI_INT, 0, MPI_COMM_WORLD);<br/>&#13;
<br/>&#13;
/* Determine how many array elements each process will get.<br/>&#13;
 * Assumes the array is evenly divisible by the number of processes. */<br/>&#13;
local_size = array_size / process_count;</pre>&#13;
<h5 class="h5" id="lev3_145">Distributing the Array</h5>&#13;
<p class="noindent">Now that each process knows the scalar value and how many values it’s responsible for multiplying, the boss must divide the array into pieces and distribute them among the workers. Note that in this implementation, the boss (rank 0) also participates as a worker. For example, with a 40-element array and eight processes (ranks 0–7), the boss should keep array elements 0–4 for itself (rank 0), send elements 5–9 to rank 1, elements 10–14 to rank 2, and so on. <a href="ch15.xhtml#ch15fig5">Figure 15-5</a> shows how the boss assigns pieces of the array to each MPI process.</p>&#13;
<div class="imagec" id="ch15fig5"><img alt="image" src="../images/15fig05.jpg"/></div>&#13;
<p class="figcap"><em>Figure 15-5: The distribution of a 40-element array among eight MPI processes (ranks 0–7)</em></p>&#13;
<p class="indent">One option for distributing pieces of the array to each worker combines <code>{MPI_Send}</code> calls at the boss with an <code>{MPI_Recv}</code> call at each worker:</p>&#13;
<pre>&#13;
if (rank == 0) {<br/>&#13;
    int i;<br/>&#13;
<br/>&#13;
    /* For each worker process, send a unique chunk of the array. */<br/>&#13;
    for (i = 1; i &lt; process_count; i++) {<br/>&#13;
        /* Send local_size ints starting at array index (i * local_size) */<br/>&#13;
        MPI_Send(array + (i * local_size), local_size, MPI_INT, i, 0,<br/>&#13;
<span epub:type="pagebreak" id="page_754"/>                 MPI_COMM_WORLD);<br/>&#13;
    }<br/>&#13;
} else {<br/>&#13;
    MPI_Recv(local_array, local_size, MPI_INT, 0, 0, MPI_COMM_WORLD,<br/>&#13;
             MPI_STATUS_IGNORE);<br/>&#13;
}</pre>&#13;
<p class="indent">In this code, the boss executes a loop that executes once for each worker process, in which it sends the worker a piece of the array. It starts sending data from the address of <code>array</code> at an offset of <code>(i * local_size)</code> to ensure that each worker gets a unique piece of the array. That is, the worker with rank 1 gets a piece of the array starting at index 5, rank 2 gets a piece of the array starting at index 10, etc., as shown in <a href="ch15.xhtml#ch15fig5">Figure 15-5</a>.</p>&#13;
<p class="indent">Each call to <code>MPI_Send</code> sends <code>local_size</code> (5) integers worth of data (20 bytes) to the process with rank i. The <code>0</code> argument toward the end represents a message tag, which is an advanced feature that this program doesn’t need—setting it to <code>0</code> treats all messages equally.</p>&#13;
<p class="indent">The workers all call <code>MPI_Recv</code> to retrieve their piece of the array, which they store in memory at the address to which <code>local_array</code> refers. They receive <code>local_size</code> (5) integers worth of data (20 bytes) from the node with rank 0. Note that <code>MPI_Recv</code> is a <em>blocking</em> call, which means that a process that calls it will pause until it receives data. Because the <code>MPI_Recv</code> call blocks, no worker will proceed until the boss sends its piece of the array.</p>&#13;
<h5 class="h5" id="lev3_146">Parallel Execution</h5>&#13;
<p class="noindent">After a worker has received its piece of the array, it can begin multiplying each array value by the scalar. Because each worker gets a unique subset of the array, they can execute independently, in parallel, without the need to communicate.</p>&#13;
<h5 class="h5" id="lev3_147">Aggregating Results</h5>&#13;
<p class="noindent">Finally, after workers complete their multiplication, they send the updated array values back to the boss, which aggregates the results. Using <code>MPI_Send</code> and <code>MPI_Recv</code>, this process looks similar to the array distribution code we looked at earlier, except the roles of sender and receiver are reversed:</p>&#13;
<pre>&#13;
if (rank == 0) {<br/>&#13;
    int i;<br/>&#13;
<br/>&#13;
    for (i = 1; i &lt; process_count; i++) {<br/>&#13;
        MPI_Recv(array + (i * local_size), local_size, MPI_INT, i, 0,<br/>&#13;
                 MPI_COMM_WORLD, MPI_STATUS_IGNORE);<br/>&#13;
    }<br/>&#13;
} else {<br/>&#13;
    MPI_Send(local_array, local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);<br/>&#13;
}</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_755"/>Recall that <code>MPI_Recv</code> <em>blocks</em> or pauses execution, so each call in the <code>for</code> loop causes the boss to wait until it receives a piece of the array from worker <em>i</em>.</p>&#13;
<h5 class="h5" id="lev3_148">Scatter/Gather</h5>&#13;
<p class="noindent">Although the <code>for</code> loops in the previous example correctly distribute data with <code>MPI_Send</code> and <code>MPI_Recv</code>, they don’t succinctly capture the <em>intent</em> behind them. That is, they appear to MPI as a series of send and receive calls without the obvious goal of distributing an array across MPI processes. Because parallel applications frequently need to distribute and collect data like this example array, MPI provides functions for exactly this purpose: <code>MPI_Scatter</code> and <code>MPI_Gather</code>.</p>&#13;
<p class="indent">These functions provide two major benefits: they allow the entire code blocks in the previous example to each be expressed as a single MPI function call, which simplifies the code, and they express the <em>intent</em> of the operation to the underlying MPI implementation, which may be able to better optimize their performance.</p>&#13;
<p class="indent">To replace the first loop in the previous example, each process could call <code>MPI_Scatter</code>:</p>&#13;
<pre>&#13;
/* Boss scatters chunks of the array evenly among all the processes. */<br/>&#13;
MPI_Scatter(array, local_size, MPI_INT, local_array, local_size, MPI_INT,<br/>&#13;
            0, MPI_COMM_WORLD);</pre>&#13;
<p class="indent">This function automatically distributes the contents of memory starting at <code>array</code> in pieces containing <code>local_size</code> integers to the <code>local_array</code> destination variable. The <code>0</code> argument specifies that the process with rank 0 (the boss) is the sender, so it reads and distributes the <code>array</code> source to other processes (including sending one piece to itself). Every other process acts as a receiver and receives data into its <code>local_array</code> destination.</p>&#13;
<p class="indent">After this single call, the workers can each multiply the array in parallel. When they finish, each process calls <code>MPI_Gather</code> to aggregate the results back in the boss’s <code>array</code> variable:</p>&#13;
<pre>&#13;
/* Boss gathers the chunks from all the processes and coalesces the<br/>&#13;
 * results into a final array. */<br/>&#13;
MPI_Gather(local_array, local_size, MPI_INT, array, local_size, MPI_INT,<br/>&#13;
           0, MPI_COMM_WORLD);</pre>&#13;
<p class="indent">This call behaves like the opposite of <code>MPI_Scatter</code>: this time, the <code>0</code> argument specifies that the process with rank 0 (the boss) is the receiver, so it updates the <code>array</code> variable, and workers each send <code>local_size</code> integers from their <code>local_array</code> variables.</p>&#13;
<h5 class="h5" id="lev3_149"><span epub:type="pagebreak" id="page_756"/>Full Code for MPI Scalar Multiply</h5>&#13;
<p class="noindent">Here’s a full MPI scalar multiply code listing that uses <code>MPI_Scatter</code> and <code>MPI_Gather</code>:<sup><a href="ch15.xhtml#fn15_8" id="rfn15_8">8</a></sup></p>&#13;
<pre>&#13;
#include &lt;stdio.h&gt;<br/>&#13;
#include &lt;stdlib.h&gt;<br/>&#13;
#include "mpi.h"<br/>&#13;
<br/>&#13;
#define ARRAY_SIZE (40)<br/>&#13;
#define SCALAR (10)<br/>&#13;
<br/>&#13;
/* In a real application, the boss process would likely read its input from a<br/>&#13;
 * data file.  This example program produces a simple array and informs the<br/>&#13;
 * caller of the size of the array through the array_size pointer parameter.*/<br/>&#13;
int *build_array(int *array_size) {<br/>&#13;
    int i;<br/>&#13;
    int *result = malloc(ARRAY_SIZE * sizeof(int));<br/>&#13;
<br/>&#13;
    if (result == NULL) {<br/>&#13;
        exit(1);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    for (i = 0; i &lt; ARRAY_SIZE; i++) {<br/>&#13;
        result[i] = i;<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    *array_size = ARRAY_SIZE;<br/>&#13;
    return result;<br/>&#13;
}<br/>&#13;
<br/>&#13;
/* Print the elements of an array, given the array and its size. */<br/>&#13;
void print_array(int *array, int array_size) {<br/>&#13;
    int i;<br/>&#13;
    for (i = 0; i &lt; array_size; i++) {<br/>&#13;
        printf("%3d ", array[i]);<br/>&#13;
    }<br/>&#13;
    printf("\n\n");<br/>&#13;
}<br/>&#13;
<br/>&#13;
/* Multiply each element of an array by a scalar value. */<br/>&#13;
void scalar_multiply(int *array, int array_size, int scalar) {<br/>&#13;
    int i;<br/>&#13;
    for (i = 0; i &lt; array_size; i++) {<br/>&#13;
        array[i] = array[i] * scalar;<br/>&#13;
    }<br/>&#13;
}<br/>&#13;
<br/>&#13;
int main(int argc, char **argv) {<br/>&#13;
<span epub:type="pagebreak" id="page_757"/>    int rank, process_count;<br/>&#13;
    int array_size, local_size;<br/>&#13;
    int scalar;<br/>&#13;
    int *array, *local_array;<br/>&#13;
<br/>&#13;
    /* Initialize MPI */<br/>&#13;
    MPI_Init(&amp;argc, &amp;argv);<br/>&#13;
<br/>&#13;
    /* Determine how many processes there are and which one this is. */<br/>&#13;
    MPI_Comm_size(MPI_COMM_WORLD, &amp;process_count);<br/>&#13;
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);<br/>&#13;
<br/>&#13;
    /* Designate rank 0 to be the boss.  It sets up the problem by generating<br/>&#13;
     * the initial input array and choosing the scalar to multiply it by. */<br/>&#13;
    if (rank == 0) {<br/>&#13;
        array = build_array(&amp;array_size);<br/>&#13;
        scalar = SCALAR;<br/>&#13;
<br/>&#13;
        printf("Initial array:\n");<br/>&#13;
        print_array(array, array_size);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    /* Boss sends the scalar value to every process with a broadcast.<br/>&#13;
     * Worker processes receive the scalar value by making this MPI_Bcast<br/>&#13;
     * call. */<br/>&#13;
    MPI_Bcast(&amp;scalar, 1, MPI_INT, 0, MPI_COMM_WORLD);<br/>&#13;
<br/>&#13;
    /* Boss sends the total array size to every process with a broadcast.<br/>&#13;
     * Worker processes receive the size value by making this MPI_Bcast<br/>&#13;
     * call. */<br/>&#13;
    MPI_Bcast(&amp;array_size, 1, MPI_INT, 0, MPI_COMM_WORLD);<br/>&#13;
<br/>&#13;
    /* Determine how many array elements each process will get.<br/>&#13;
     * Assumes the array is evenly divisible by the number of processes. */<br/>&#13;
    local_size = array_size / process_count;<br/>&#13;
<br/>&#13;
    /* Each process allocates space to store its portion of the array. */<br/>&#13;
    local_array = malloc(local_size * sizeof(int));<br/>&#13;
    if (local_array == NULL) {<br/>&#13;
        exit(1);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    /* Boss scatters chunks of the array evenly among all the processes. */<br/>&#13;
    MPI_Scatter(array, local_size, MPI_INT, local_array, local_size, MPI_INT,<br/>&#13;
                0, MPI_COMM_WORLD);<br/>&#13;
<br/>&#13;
    /* Every process (including boss) performs scalar multiplication over its<br/>&#13;
<span epub:type="pagebreak" id="page_758"/>     * chunk of the array in parallel. */<br/>&#13;
    scalar_multiply(local_array, local_size, scalar);<br/>&#13;
<br/>&#13;
    /* Boss gathers the chunks from all the processes and coalesces the<br/>&#13;
     * results into a final array. */<br/>&#13;
    MPI_Gather(local_array, local_size, MPI_INT, array, local_size, MPI_INT,<br/>&#13;
               0, MPI_COMM_WORLD);<br/>&#13;
<br/>&#13;
    /* Boss prints the final answer. */<br/>&#13;
    if (rank == 0) {<br/>&#13;
        printf("Final array:\n");<br/>&#13;
        print_array(array, array_size);<br/>&#13;
    }<br/>&#13;
<br/>&#13;
    /* Clean up. */<br/>&#13;
    if (rank == 0) {<br/>&#13;
        free(array);<br/>&#13;
    }<br/>&#13;
    free(local_array);<br/>&#13;
    MPI_Finalize();<br/>&#13;
<br/>&#13;
    return 0;<br/>&#13;
}</pre>&#13;
<p class="indent">In the <code>main</code> function, the boss sets up the problem and creates an array. If this were solving a real problem (e.g., a scientific computing application), the boss would likely read its initial data from an input file. After initializing the array, the boss needs to send information about the size of the array and the scalar to use for multiplication to all the other worker processes, so it broadcasts those variables to every process.</p>&#13;
<p class="indent">Now that each process knows the size of the array and how many processes there are, they can each divide to determine how many elements of the array they’re responsible for multiplying. For simplicity, this code assumes that the array is evenly divisible by the number of processes.</p>&#13;
<p class="indent">The boss then uses the <code>MPI_Scatter</code> function to send an equal portion of the array to each worker process (including itself). Now the workers have all the information they need, so they each perform multiplication over their portion of the array in parallel. Finally, as the workers complete their multiplication, the boss collects each worker’s piece of the array using <code>MPI_Gather</code> to report the final results.</p>&#13;
<p class="indent">Compiling and executing this program looks like this:</p>&#13;
<pre>&#13;
$ <span class="codestrong1">mpicc -o scalar_multiply_mpi scalar_multiply_mpi.c</span><br/>&#13;
<br/>&#13;
$ <span class="codestrong1">mpirun -np 8 --hostfile hosts.txt ./scalar_multiply_mpi</span><br/>&#13;
Initial array:<br/>&#13;
  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18 19<br/>&#13;
 20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38 39<br/>&#13;
<br/>&#13;
<span epub:type="pagebreak" id="page_759"/>Final array:<br/>&#13;
  0  10  20  30  40  50  60  70  80  90 100 110 120 130 140 150 160 170 180 190<br/>&#13;
200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390</pre>&#13;
<h4 class="h4" id="lev2_267">15.2.6 Distributed Systems Challenges</h4>&#13;
<p class="noindent">In general, coordinating the behavior of multiple processes in distributed systems is notoriously difficult. If a hardware component (e.g., CPU or power supply) fails in a shared memory system, the entire system becomes inoperable. In a distributed system though, autonomous nodes can fail independently. For example, an application must decide how to proceed if one node disappears and the others are still running. Similarly, the interconnection network could fail, making it appear to each process as if all the others failed.</p>&#13;
<p class="indent">Distributed systems also face challenges due to a lack of shared hardware, namely clocks. Due to unpredictable delays in network transmission, autonomous nodes cannot easily determine the order in which messages are sent. Solving these challenges (and many others) is beyond the scope of this book. Fortunately, distributed software designers have constructed several frameworks that ease the development of distributed applications. We characterize some of these frameworks in the next section.</p>&#13;
<h4 class="h4" id="lev2_268">MPI Resources</h4>&#13;
<p class="noindent">MPI is large and complex, and this section hardly scratches the surface. For more information about MPI, we suggest:</p>&#13;
<ul>&#13;
<li class="noindent">The Lawrence Livermore National Lab’s MPI tutorial, by Blaise Barney.<sup><a href="ch15.xhtml#fn15_9" id="rfn15_9">9</a></sup></li>&#13;
<li class="noindent">CSinParallel’s MPI Patterns.<sup><a href="ch15.xhtml#fn15_10" id="rfn15_10">10</a></sup></li>&#13;
</ul>&#13;
<h3 class="h3" id="lev1_116">15.3 To Exascale and Beyond: Cloud Computing, Big Data, and the Future of Computing</h3>&#13;
<p class="noindent">Advances in technology have made it possible for humanity to produce data at a rate never seen before. Scientific instruments such as telescopes, biological sequencers, and sensors produce high-fidelity scientific data at low cost. As scientists struggle to analyze this “data deluge,” they increasingly rely on sophisticated multinode supercomputers, which form the foundation of <em>high-performance computing</em> (HPC).</p>&#13;
<p class="indent">HPC applications are typically written in languages like C, C++, or Fortran, with multithreading and message passing enabled with libraries such as POSIX threads, OpenMP, and MPI. Thus far, the vast majority of this book has described architectural features, languages, and libraries commonly leveraged on HPC systems. Companies, national laboratories, and other organizations <span epub:type="pagebreak" id="page_760"/>interested in advancing science typically use HPC systems and form the core of the computational science ecosystem.</p>&#13;
<p class="indent">Meanwhile, the proliferation of internet-enabled devices and the ubiquity of social media have caused humanity to effortlessly produce large volumes of online multimedia, in the form of web pages, pictures, videos, tweets, and social media posts. It is estimated that 90% of all online data was produced in the past two years, and that society produces 30 terabytes of user data per second (or 2.5 exabytes per day). The deluge of <em>user data</em> offers companies and organizations a wealth of information about the habits, interests, and behavior of its users, and it facilitates the construction of data-rich customer profiles to better tailor commercial products and services. To analyze user data, companies typically rely on multinode data centers that share many of the hardware architecture components of typical supercomputers. However, these data centers rely on a different software stack designed specifically for internet-based data. The computer systems used for the storage and analysis of large-scale internet-based data are sometimes referred to as <em>high-end data analysis</em> (HDA) systems. Companies like Amazon, Google, Microsoft, and Facebook have a vested interest in the analysis of internet data, and form the core of the data analytics ecosystem. The HDA and data analytics revolution started around 2010, and now is a dominant area of cloud computing research.</p>&#13;
<p class="indent"><a href="ch15.xhtml#ch15fig6">Figure 15-6</a> highlights the key differences in software utilized by the HDA and HPC communities. Note that both communities use similar cluster hardware that follows a distributed memory model, where each compute node typically has one or more multicore processors and frequently a GPU. The cluster hardware typically includes a <em>distributed filesystem</em> that allows users and applications common access to files that reside locally on multiple nodes in the cluster.</p>&#13;
<div class="imagec" id="ch15fig6"><img alt="image" src="../images/15fig06.jpg"/></div>&#13;
<p class="figcap"><em>Figure 15-6: Comparison of HDA vs. HPC frameworks. Based on a figure by Jack Dongarra and Daniel Reed.<sup><a href="ch15.xhtml#fn15_11" id="rfn15_11">11</a></sup></em></p>&#13;
<p class="indent">Unlike supercomputers, which are typically built and optimized for HPC use, the HDA community relies on <em>data centers</em>, which consist of a large collection of general-purpose compute nodes typically networked together via <span epub:type="pagebreak" id="page_761"/>Ethernet. At a software level, data centers typically employ virtual machines, large distributed databases, and frameworks that enable high-throughput analysis of internet data. The term <em>cloud</em> refers to the data storage and computing power components of HDA data centers.</p>&#13;
<p class="indent">In this section, we take a brief look at cloud computing, some of the software commonly used to enable cloud computing (specifically MapReduce), and some challenges for the future. Please note that this section is not meant to be an in-depth look at these concepts; we encourage interested readers to explore the referenced sources for greater detail.</p>&#13;
<h4 class="h4" id="lev2_269">15.3.1 Cloud Computing</h4>&#13;
<p class="noindent"><em>Cloud computing</em> is the use or lease of the cloud for a variety of services. Cloud computing enables computing infrastructure to act as a “utility”: a few central providers give users and organizations access to (a seemingly infinite amount of) compute power through the internet, with users and organizations choosing to use as much as they want and paying according to their level of use. Cloud computing has three main pillars: software as a service (SaaS), infrastructure as a service (IaaS), and platform as a service (PaaS).<sup><a href="ch15.xhtml#fn15_12" id="rfn15_12">12</a></sup></p>&#13;
<h5 class="h5" id="lev3_150">Software as a Service</h5>&#13;
<p class="noindent"><em>Software as a service</em> (SaaS) refers to software provided directly to users through the cloud. Most people utilize this pillar of cloud computing without even realizing it. Applications that many people use daily (e.g., web mail, social media, and video streaming) depend upon cloud infrastructure. Consider the classic application of web mail. Users are able to log on and access their web mail from any device, send and receive mail, and seemingly never run out of storage space. Interested organizations can in turn “rent” cloud email services to provide email to their own clients and employees, without incurring the hardware and maintenance cost of running the service themselves. Services in the SaaS pillar are managed completely by cloud providers; organizations and users do not (beyond configuring a few settings, perhaps) manage any part of the application, data, software, or hardware infrastructure, all which would be necessary if they were trying to set up the service on their own hardware. Prior to the advent of cloud computing, organizations interested in providing web mail for their users would need their own infrastructure and dedicated IT support staff to maintain it. Popular examples of SaaS providers include Google’s G Suite and Microsoft’s Office 365.</p>&#13;
<h5 class="h5" id="lev3_151">Infrastructure as a Service</h5>&#13;
<p class="noindent"><em>Infrastructure as a service</em> (IaaS) allows people and organizations to “rent out” computational resources to meet their needs, usually in the form of accessing virtual machines that are either general purpose or preconfigured for a particular application. One classic example is Amazon’s Elastic Compute Cloud (EC2) service from Amazon Web Services (AWS). EC2 enables users to create fully customizable virtual machines. The term <em>elastic</em> in EC2 refers <span epub:type="pagebreak" id="page_762"/>to a user’s ability to grow or shrink their compute resource requests as needed, paying as they go. For example, an organization may use an IaaS provider to host its website or deploy its own series of custom-built applications to users. Some research labs and classrooms use IaaS services in lieu of lab machines, running experiments in the cloud or offering a virtual platform for their students to learn. In all cases, the goal is to eliminate the maintenance and capital needed to maintain a personal cluster or server for similar purposes. Unlike use cases in the SaaS pillar, use cases in the IaaS pillar require clients to configure applications, data, and in some cases the virtual machine’s OS itself. However, the host OS and hardware infrastructure is set up and managed by the cloud provider. Popular IaaS providers include Amazon AWS, Google Cloud Services, and Microsoft Azure.</p>&#13;
<h5 class="h5" id="lev3_152">Platform as a Service</h5>&#13;
<p class="noindent"><em>Platform as a service</em> (PaaS) allows individuals and organizations to develop and deploy their own web applications for the cloud, eliminating the need for local configuration or maintenance. Most PaaS providers enable developers to write their applications in a variety of languages and offer a choice of APIs to use. For example, Microsoft Azure’s service allows users to code web applications in the Visual Studio IDE and deploy their applications to Azure for testing. Google App Engine enables developers to build and test custom mobile applications in the cloud in a variety of languages. Heroku and CloudBees are other prominent examples. Note that developers have control over their applications and data only; the cloud provider controls the rest of the software infrastructure and all of the underlying hardware infrastructure.</p>&#13;
<h4 class="h4" id="lev2_270">15.3.2 MapReduce</h4>&#13;
<p class="noindent">Perhaps the most famous programming paradigm used on cloud systems is MapReduce.<sup><a href="ch15.xhtml#fn15_13" id="rfn15_13">13</a></sup> Although MapReduce’s origins lay in functional programming’s Map and Reduce operations, Google was the first to apply the concept to analyzing large quantities of web data. MapReduce enabled Google to perform web queries faster than its competitors, and enabled Google’s meteoric rise as the go-to web service provider and internet giant it is today.</p>&#13;
<h5 class="h5" id="lev3_153">Understanding Map and Reduce Operations</h5>&#13;
<p class="noindent">The <code>map</code> and <code>reduce</code> functions in the MapReduce paradigm are based on the mathematical operations of Map and Reduce from functional programming. In this section, we briefly discuss how these mathematical operations work by revisiting some examples presented earlier in the book.</p>&#13;
<p class="indent">The Map operation typically applies the same function to all the elements in a collection. Readers familiar with Python may recognize this functionality most readily in the list comprehension feature in Python. For example, the following two code snippets perform scalar multiplication in Python:</p>&#13;
<span epub:type="pagebreak" id="page_763"/>&#13;
<p class="margnote">Regular scalar multiply</p>&#13;
<pre>'''<br/>&#13;
    The typical way to perform<br/>&#13;
    scalar multiplication<br/>&#13;
'''<br/>&#13;
<br/>&#13;
# array is an array of numbers<br/>&#13;
# s is an integer<br/>&#13;
def scalarMultiply(array, s):<br/>&#13;
<br/>&#13;
    for i in range(len(array)):<br/>&#13;
        array[i] = array[i] * s<br/>&#13;
<br/>&#13;
    return array<br/>&#13;
<br/>&#13;
# call the scalarMultiply function:<br/>&#13;
myArray = [1, 3, 5, 7, 9]<br/>&#13;
result = scalarMultiply(myArray, 2)<br/>&#13;
<br/>&#13;
# prints [2, 6, 10, 14, 18]<br/>&#13;
print(result)</pre>&#13;
<p class="margnote">Scalar multiply with list comprehension</p>&#13;
<pre>'''<br/>&#13;
    Equivalent program that<br/>&#13;
    performs scalar multiplication<br/>&#13;
    with list comprehension<br/>&#13;
'''<br/>&#13;
<br/>&#13;
# multiplies two numbers together<br/>&#13;
def multiply(num1, num2):<br/>&#13;
    return num1 * num2<br/>&#13;
<br/>&#13;
# array is an array of numbers<br/>&#13;
# s is an integer<br/>&#13;
def scalarMultiply(array, s):<br/>&#13;
<br/>&#13;
    # using list comprehension<br/>&#13;
    return [multiply(x, s) for x in array]<br/>&#13;
<br/>&#13;
# call the scalarMultiply function:<br/>&#13;
myArray = [1, 3, 5, 7, 9]<br/>&#13;
result = scalarMultiply(myArray, 2)<br/>&#13;
<br/>&#13;
# prints [2, 6, 10, 14, 18]<br/>&#13;
print(result)</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_764"/>The list comprehension applies the same function (in this case, multiplying an array element with scalar value <code>s</code>) to every element <code>x</code> in <code>array</code>.</p>&#13;
<p class="indent">A single Reduce operation takes a collection of elements and combines them together into a single value using some common function. For example, the Python function <code>sum</code> acts similarly to a Reduce operation, as it takes a collection (typically a Python list) and combines all the elements together using addition. So, for example, applying addition to all the elements in the <code>result</code> array returned from the <code>scalarMultiply</code> function yields a combined sum of 50.</p>&#13;
<h5 class="h5" id="lev3_154">The MapReduce Programming Model</h5>&#13;
<p class="noindent">A key feature of MapReduce is its simplified programming model. Developers need to implement only two types of functions, <code>map</code> and <code>reduce</code>; the underlying MapReduce framework automates the rest of the work.</p>&#13;
<p class="indent">The programmer-written <code>map</code> function takes an input (<em>key</em>,<em>value</em>) pair and outputs a series of intermediate (<em>key</em>,<em>value</em>) pairs that are written to a distributed filesystem shared among all the nodes. A combiner that is typically defined by the MapReduce framework then aggregates (<em>key</em>,<em>value</em>) pairs by key, to produce (<em>key</em>,list(<em>value</em>)) pairs that are passed to the programmer-defined <code>reduce</code> function. The <code>reduce</code> function then takes as input a (<em>key</em>,list(<em>value</em>)) pair and combines all the values together through some programmer-defined operation to form a final (<em>key</em>,<em>value</em>), where the <em>value</em> in this output corresponds to the result of the reduction operation. The output from the <code>reduce</code> function is written to the distributed filesystem and usually output to the user.</p>&#13;
<p class="indent">To illustrate how to use the MapReduce model to parallelize a program, we discuss the Word Frequency program. The goal of Word Frequency is to determine the frequency of each word in a large text corpus.</p>&#13;
<p class="indent">A C programmer may implement the following <code>map</code> function for the Word Frequency program:<sup>13</sup></p>&#13;
<pre>&#13;
void map(char *key, char *value){<br/>&#13;
    // key is document name<br/>&#13;
    // value is string containing some words (separated by spaces)<br/>&#13;
    int i;<br/>&#13;
    int numWords = 0; // number of words found: populated by parseWords()<br/>&#13;
<br/>&#13;
    // returns an array of numWords words<br/>&#13;
    char *words[] = parseWords(value, &amp;numWords);<br/>&#13;
    for (i = 0; i &lt; numWords; i++) {<br/>&#13;
        // output (word, 1) key-value intermediate to file system<br/>&#13;
        emit(words[i], "1");<br/>&#13;
    }<br/>&#13;
}</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_765"/>This <code>map</code> function receives as input a string (<code>key</code>) that corresponds to the name of the file, and a separate string (<code>value</code>) that contains a component of file data. The function then parses words from the input <code>value</code> and emits each word (<code>words[i]</code>) separately with the string value <code>"1"</code>. The <code>emit</code> function is provided by the MapReduce framework and writes the intermediate (<em>key</em>,<em>value</em>) pairs to the distributed filesystem.</p>&#13;
<p class="indent">To complete the Word Frequency program, a programmer may implement the following <code>reduce</code> function:</p>&#13;
<pre>&#13;
void reduce(char *key, struct Iterator values) {<br/>&#13;
    // key is individual word<br/>&#13;
    // value is of type Iterator (a struct that consists of<br/>&#13;
    // a items array (type char **), and its associated length (type int))<br/>&#13;
    int numWords = values.length();  // get length<br/>&#13;
    char *counts[] = values.items(); // get counts<br/>&#13;
    int i, total = 0;<br/>&#13;
    for (i = 0; i &lt; numWords; i++) {<br/>&#13;
        total += atoi(counts[i]); // sum up all counts<br/>&#13;
    }<br/>&#13;
    char *stringTotal = itoa(total); // convert total to a string<br/>&#13;
    emit(key, stringTotal); // output (word, total) pair to file system<br/>&#13;
}</pre>&#13;
<p class="indent">This <code>reduce</code> function receives as input a string (<code>key</code>) that corresponds to a particular word, and an <code>Iterator</code> struct (again, provided by the MapReduce framework) that consists of an aggregated array of items associated with the key (<code>items</code>), and the length of that array (<code>length</code>). In the Word Frequency application, <code>items</code> corresponds to a list of counts. The function then extracts the number of words from the <code>length</code> field of the <code>Iterator</code> struct, and the array of counts from the <code>items</code> field. It then loops over all the counts, aggregating the values into the variable <code>total</code>. Since the <code>emit</code> function requires <code>char *</code> parameters, the function converts <code>total</code> to a string prior to calling <code>emit</code>.</p>&#13;
<p class="indent">After implementing <code>map</code> and <code>reduce</code>, the programmer’s responsibility ends. The MapReduce framework automates the rest of the work, including partitioning the input, generating and managing the processes that run the <code>map</code> function (map tasks), aggregating and sorting intermediate (<em>key</em>,<em>value</em>) pairs, generating and managing the separate processes that run the <code>reduce</code> function (reduce tasks), and generating a final output file.</p>&#13;
<p class="indent">For simplicity, in <a href="ch15.xhtml#ch15fig7">Figure 15-7</a> we illustrate how MapReduce parallelizes the opening lines of the popular Jonathan Coulton song “Code Monkey”: <em>code monkey get up get coffee, code monkey go to job</em>.</p>&#13;
<div class="imagec" id="ch15fig7"><span epub:type="pagebreak" id="page_766"/><img alt="image" src="../images/15fig07.jpg"/></div>&#13;
<p class="figcap"><em>Figure 15-7: Parallelization of the opening lines of the song “Code Monkey” using the MapReduce framework</em></p>&#13;
<p class="indent"><a href="ch15.xhtml#ch15fig7">Figure 15-7</a> gives an overview of this process. Prior to execution, the boss node first partitions the input into <em>M</em> parts, where <em>M</em> corresponds to the number of map tasks. In <a href="ch15.xhtml#ch15fig7">Figure 15-7</a>, <em>M</em> = 3, and the input file (<code>coulton.txt</code>) is split into three parts. During the map phase, the boss node distributes the map tasks among one or more worker nodes, with each map task executing independently and in parallel. For example, the first map task parses the snippet <em>code monkey get up</em> into separate words and emits the following four (<em>key</em>,<em>value</em>) pairs: (<code>code</code>,<code>1</code>), (<code>monkey</code>,<code>1</code>), (<code>get</code>,<code>1</code>), (<code>up</code>,<code>1</code>). Each map task then emits its intermediate values to a distributed filesystem that takes up a certain amount of storage on each node.</p>&#13;
<p class="indent">Prior to the start of the reduce phase, the framework aggregates and combines the intermediate (<em>key</em>,<em>value</em>) pairs into (<em>key</em>,list(<em>value</em>)) pairs. In <a href="ch15.xhtml#ch15fig7">Figure 15-7</a>, for example, the (<em>key</em>,<em>value</em>) pair (<code>get</code>,<code>1</code>) is emitted by two separate map tasks. The MapReduce framework aggregates these separate (<em>key</em>,<em>value</em>) pairs into the single (<em>key</em>,list(<em>value</em>)) pair (<code>get</code>,<code>[1,1]</code>). The aggregated intermediate pairs are written to the distributed filesystem on disk.</p>&#13;
<p class="indent">Next, the MapReduce framework directs the boss node to generate <em>R</em> reduce tasks. In <a href="ch15.xhtml#ch15fig7">Figure 15-7</a>, <em>R</em> = 8. The framework then distributes the tasks among its worker nodes. Once again, each reduce task executes independently and in parallel. In the reduce phase of this example, the (<em>key</em>,list(<em>value</em>)) pair (<code>get</code>,<code>[1,1]</code>) is reduced to the (<em>key</em>,<em>value</em>) pair (<code>get</code>,<code>2</code>). Each worker node appends the output of its set of reduce tasks to a final file, which is available to the user upon completion.</p>&#13;
<h5 class="h5" id="lev3_155">Fault Tolerance</h5>&#13;
<p class="noindent">Data centers typically contain thousands of nodes. Consequently, the rate of failure is high; consider that if an individual node in a data center has a 2% chance of hardware failure, there is a greater than 99.99% chance that some node in a 1,000-node data center will fail. Software written for data centers must therefore be <em>fault tolerant</em>, meaning that it must be able to continue operation in the face of hardware failures (or else fail gracefully).</p>&#13;
<p class="indent">MapReduce was designed with fault tolerance in mind. For any MapReduce run, there is one boss node and potentially thousands of worker nodes. <span epub:type="pagebreak" id="page_767"/>The chance that a worker node will fail is therefore high. To remedy this, the boss node pings individual worker nodes periodically. If the boss node does not receive a response from a worker node, the boss redistributes the worker’s assigned workload to a different node and re-executes the task.<sup>13</sup> If the boss node fails (a low probability given that it is only one node), the MapReduce job aborts and must be rerun on a separate node. Note that sometimes a worker node may fail to respond to the boss node’s pings because the worker is bogged down by tasks. MapReduce therefore uses the same pinging and work redistribution strategy to limit the effect of slow (or straggler) worker nodes.</p>&#13;
<h5 class="h5" id="lev3_156">Hadoop and Apache Spark</h5>&#13;
<p class="noindent">The development of MapReduce took the computing world by storm. However, Google’s implementation of MapReduce is closed source. As a result, engineers at Yahoo! developed Hadoop,<sup><a href="ch15.xhtml#fn15_14" id="rfn15_14">14</a></sup> an open source implementation of MapReduce, which was later adopted by the Apache Foundation. The Hadoop project consists of an ecosystem of tools for Apache Hadoop, including the Hadoop Distributed File System or HDFS (an open source alternative to Google File System), and HBase (modeled after Google’s BigTable).</p>&#13;
<p class="indent">Hadoop has a few key limitations. First, it is difficult to chain multiple MapReduce jobs together into a larger workflow. Second, the writing of intermediates to the HDFS proves to be a bottleneck, especially for small jobs (smaller than one gigabyte). Apache Spark<sup><a href="ch15.xhtml#fn15_15" id="rfn15_15">15</a></sup> was designed to address these issues, among others. Due to its optimizations and ability to largely process intermediate data in memory, Apache Spark is up to 100 times faster than Hadoop on some applications.<sup><a href="ch15.xhtml#fn15_16" id="rfn15_16">16</a></sup></p>&#13;
<h4 class="h4" id="lev2_271">15.3.3 Looking Toward the Future: Opportunities and Challenges</h4>&#13;
<p class="noindent">Despite the innovations in the internet data analytics community, the amount of data produced by humanity continues to grow. Most new data is produced in so-called <em>edge environments</em>, or near sensors and other data-generating instruments that are by definition on the other end of the network from commercial cloud providers and HPC systems. Traditionally, scientists and practitioners gather data and analyze it using a local cluster, or they move it to a supercomputer or data center for analysis. This “centralized” view of computing is no longer a viable strategy as improvements in sensor technology have exacerbated the data deluge.</p>&#13;
<p class="indent">One reason for this explosive growth is the proliferation of small internet-enabled devices that contain a variety of sensors. These <em>Internet of Things</em> (IoT) devices have led to the generation of large and diverse datasets in edge environments. Transferring large datasets from the edge to the cloud is difficult, as larger datasets take more time and energy to move. To mitigate the logistic issues of so-called “Big Data,” the research community has begun to create techniques that aggressively summarize data at each transfer point between the edge and the cloud.<sup><a href="ch15.xhtml#fn15_17" id="rfn15_17">17</a></sup> There is intense interest in the computing research community in creating infrastructure that is capable of processing, <span epub:type="pagebreak" id="page_768"/>storing, and summarizing data in edge environments in a unified platform; this area is known as <em>edge</em> (or <em>fog</em>) computing. Edge computing flips the traditional analysis model of Big Data; instead of analysis occurring at the supercomputer or data center (“last mile”), analysis instead occurs at the source of data production (“first mile”).</p>&#13;
<p class="indent">In addition to data movement logistics, the other cross-cutting concern for the analysis of Big Data is power management. Large, centralized resources such as supercomputers and data centers require a lot of energy; modern supercomputers require several megawatts (million watts) to power and cool. An old adage in the supercomputing community is that “a megawatt costs a megabuck”; in other words, it costs roughly $1 million annually to maintain the power requirement of one megawatt.<sup><a href="ch15.xhtml#fn15_18" id="rfn15_18">18</a></sup> Local data processing in edge environments helps mitigate the logistical issue of moving large datasets, but the computing infrastructure in such environments must likewise use the minimal energy possible. At the same time, increasing the energy efficiency of large supercomputers and data centers is paramount.</p>&#13;
<p class="indent">There is also interest in figuring out ways to converge the HPC and cloud computing ecosystems to create a common set of frameworks, infrastructure and tools for large-scale data analysis. In recent years, many scientists have used techniques and tools developed by researchers in the cloud computing community to analyze traditional HPC datasets, and vice versa. Converging these two software ecosystems will allow for the cross-pollination of research and lead to the development of a unified system that allows both communities to tackle the coming onslaught of data and potentially share resources. The Big Data Exascale Computing (BDEC) working group<sup><a href="ch15.xhtml#fn15_19" id="rfn15_19">19</a></sup> argues that instead of seeing HPC and cloud computing as two fundamentally different paradigms, it is perhaps more useful to view cloud computing as a “digitally empowered” phase of scientific computing, in which data sources are increasingly generated over the internet.<sup>17</sup> In addition, a convergence of culture, training, and tools is necessary to fully integrate the HPC and cloud computing software and research communities. BDEC also suggests a model in which supercomputers and data centers are “nodes” in a very large network of computing resources, all working in concert to deal with data flooding from multiple sources. Each node aggressively summarizes the data flowing to it, releasing it to a larger computational resource node only when necessary.</p>&#13;
<p class="indent">As the cloud computing and HPC ecosystems look for unification and gird themselves against an increasing onslaught of data, the future of computer systems brims with exciting possibilities. New fields like artificial intelligence and quantum computing are leading to the creation of new <em>domain-specific architectures</em> (DSAs) and <em>application-specific integrated circuits</em> (ASICS) that will be able to handle custom workflows more energy efficiently than before (see the TPU<sup><a href="ch15.xhtml#fn15_20" id="rfn15_20">20</a></sup> for one example). In addition, the security of such architectures, long overlooked by the community, will become critical as the data they analyze increases in importance. New architectures will also lead to new languages needed to program them, and perhaps even new operating systems to manage their various interfaces. To learn more about what the future <span epub:type="pagebreak" id="page_769"/>of computer architecture may look like, we encourage readers to peruse an article by the 2017 ACM Turing Award winners and computer architecture giants, John Hennessy and David Patterson.<sup><a href="ch15.xhtml#fn15_21" id="rfn15_21">21</a></sup></p>&#13;
<h3 class="h3" id="lev1_117">Notes</h3>&#13;
<p class="fnote"><a href="ch15.xhtml#rfn15_1" id="fn15_1">1.</a> Sparsh Mittal, “A Survey Of Techniques for Architecting and Managing Asymmetric Multicore Processors,” <em>ACM Computing Surveys</em> 48(3), February 2016.</p>&#13;
<p class="fnote"><a href="ch15.xhtml#rfn15_2" id="fn15_2">2.</a> “FPGAs and the Road to Reprogrammable HPC,” inside HPC, July 2019, <em><a href="https://insidehpc.com/2019/07/fpgas-and-the-road-to-reprogrammable-hpc/">https://insidehpc.com/2019/07/fpgas-and-the-road-to-reprogrammable-hpc/</a></em></p>&#13;
<p class="fnote"><a href="ch15.xhtml#rfn15_3" id="fn15_3">3.</a> “GPU Programming,” from CSinParallel: <em><a href="https://csinparallel.org/csinparallel/modules/gpu_programming.html">https://csinparallel.org/csinparallel/modules/gpu_programming.html</a></em>; CSinParallel has other GPU programming modules: <em><a href="https://csinparallel.org">https://csinparallel.org</a></em></p>&#13;
<p class="fnote"><a href="ch15.xhtml#rfn15_4" id="fn15_4">4.</a> <em><a href="https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_cuda.cu">https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_cuda.cu</a></em></p>&#13;
<p class="fnote"><a href="ch15.xhtml#rfn15_5" id="fn15_5">5.</a> <em><a href="https://diveintosystems.org/book/C15-Parallel/_attachments/hello_world_mpi.c">https://diveintosystems.org/book/C15-Parallel/_attachments/hello_world_mpi.c</a></em></p>&#13;
<p class="fnote"><a href="ch15.xhtml#rfn15_6" id="fn15_6">6.</a> <em><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a></em></p>&#13;
<p class="fnote"><a href="ch15.xhtml#rfn15_7" id="fn15_7">7.</a> <em><a href="https://www.mpich.org/">https://www.mpich.org/</a></em></p>&#13;
<p class="fnote"><a href="ch15.xhtml#rfn15_8" id="fn15_8">8.</a> Available at <em><a href="https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_mpi.c">https://diveintosystems.org/book/C15-Parallel/_attachments/scalar_multiply_mpi.c</a></em></p>&#13;
<p class="fnote"><a href="ch15.xhtml#rfn15_9" id="fn15_9">9.</a> <em><a href="https://hpc-tutorials.llnl.gov/mpi/">https://hpc-tutorials.llnl.gov/mpi/</a></em></p>&#13;
<p class="fnote1"><a href="ch15.xhtml#rfn15_10" id="fn15_10">10.</a> <em><a href="http://selkie.macalester.edu/csinparallel/modules/Patternlets/build/html/MessagePassing/MPI_Patternlets.html">http://selkie.macalester.edu/csinparallel/modules/Patternlets/build/html/MessagePassing/MPI_Patternlets.html</a></em></p>&#13;
<p class="fnote1"><a href="ch15.xhtml#rfn15_11" id="fn15_11">11.</a> D. A. Reed and J. Dongarra, “Exascale Computing and Big Data,” <em>Communications of the ACM</em> 58(7), 56–68, 2015.</p>&#13;
<p class="fnote1"><a href="ch15.xhtml#rfn15_12" id="fn15_12">12.</a> M. Armbrust et al., “A View of Cloud Computing,” <em>Communications of the ACM</em> 53(4), 50–58, 2010.</p>&#13;
<p class="fnote1"><a href="ch15.xhtml#rfn15_13" id="fn15_13">13.</a> Jeffrey Dean and Sanjay Ghemawat, “MapReduce: Simplified Data Processing on Large Clusters,” <em>Proceedings of the Sixth Conference on Operating Systems Design and Implementation</em>, Vol. 6, USENIX, 2004.</p>&#13;
<p class="fnote1"><a href="ch15.xhtml#rfn15_14" id="fn15_14">14.</a> <em><a href="https://hadoop.apache.org/">https://hadoop.apache.org/</a></em></p>&#13;
<p class="fnote1"><a href="ch15.xhtml#rfn15_15" id="fn15_15">15.</a> <em><a href="https://spark.apache.org/">https://spark.apache.org/</a></em></p>&#13;
<p class="fnote1"><a href="ch15.xhtml#rfn15_16" id="fn15_16">16.</a> DataBricks, “Apache Spark,” <em><a href="https://databricks.com/spark/about">https://databricks.com/spark/about</a></em></p>&#13;
<p class="fnote1"><a href="ch15.xhtml#rfn15_17" id="fn15_17">17.</a> M. Asch et al., “Big Data and Extreme-Scale Computing: Pathways to Convergence – Toward a shaping strategy for a future software and data ecosystem for scientific inquiry,” <em>The International Journal of High Performance Computing Applications</em> 32(4), 435–479, 2018.</p>&#13;
<p class="fnote1"><a href="ch15.xhtml#rfn15_18" id="fn15_18">18.</a> M. Halper, “Supercomputing’s Super Energy Needs, and What to Do About Them,” CACM News, <em><a href="https://cacm.acm.org/news/192296-supercomputings-super-energy-needs-and-what-to-do-about-them/fulltext">https://cacm.acm.org/news/192296-supercomputings-super-energy-needs-and-what-to-do-about-them/fulltext</a></em></p>&#13;
<p class="fnote1"><a href="ch15.xhtml#rfn15_19" id="fn15_19">19.</a> <em><a href="https://www.exascale.org/bdec/">https://www.exascale.org/bdec/</a></em></p>&#13;
<p class="fnote1"><a href="ch15.xhtml#rfn15_20" id="fn15_20">20.</a> N. P. Jouppi et al., “In-Datacenter Performance Analysis of a Tensor Processing Unit,” <em>Proceedings of the 44th Annual International Symposium on Computer Architecture</em>, ACM, 2017.</p>&#13;
<p class="fnote1"><a href="ch15.xhtml#rfn15_21" id="fn15_21">21.</a> J. Hennessy and D. Patterson, “A New Golden Age for Computer Architecture,” <em>Communications of the ACM</em> 62(2), 48–60, 2019.</p>&#13;
<span epub:type="pagebreak" id="page_770"/>&#13;
</body></html>