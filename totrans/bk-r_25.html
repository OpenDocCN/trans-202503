<html><head></head><body>
<h2 class="h2" id="ch21"><span epub:type="pagebreak" id="page_485"/><span class="big"><strong>21</strong></span><br/><strong>MULTIPLE LINEAR REGRESSION</strong></h2>&#13;
<div class="image"><img src="../images/common-01.jpg" alt="image"/></div>&#13;
<p class="noindent">Multiple linear regression is a straightforward generalization of the single-predictor models discussed in the previous chapter. It allows you to model your continuous response variable in terms of more than one predictor so you can measure the joint effect of several explanatory variables on the response variable. In this chapter, you’ll see how to model your response variable in this way, and you’ll use R to fit the model using least-squares. You’ll also explore other key statistical aspects of linear modeling in the R environment, such as transforming variables and including interactive effects.</p>&#13;
<p class="indent">Multiple linear regression represents an important part of the practice of statistics. It lets you control or adjust for multiple sources of influence on the value of the response, rather than just measuring the effect of one explanatory variable (in most situations, there is more than one contributor to the outcome measurements). At the heart of this class of methods is the intention to uncover potentially causal relationships between your response variable and the (joint) effect of any explanatory variables. In reality, causality itself is extremely difficult to establish, but you can strengthen any evidence of causality by using a well-designed study supported by sound <span epub:type="pagebreak" id="page_486"/>data collection and by fitting models that might realistically gauge the relationships present in your data.</p>&#13;
<h3 class="h3" id="ch21lev1sec67"><strong>21.1 Terminology</strong></h3>&#13;
<p class="noindentb">Before you look at the theory behind multiple regression models, it’s important to have a clear understanding of some terminology associated with variables.</p>&#13;
<p class="bull">• A <em>lurking variable</em> influences the response, another predictor, or both, but goes unmeasured (or is not included) in a predictive model. For example, say a researcher establishes a link between the volume of trash thrown out by a household and whether the household owns a trampoline. The potential lurking variable here would be the number of children in the household—this variable is more likely to be positively associated with an increase in trash and chances of owning a trampoline. An interpretation that suggests owning a trampoline is a cause of increased waste would be erroneous.</p>&#13;
<p class="bull">• The presence of a lurking variable can lead to spurious conclusions about causal relationships between the response and the other predictors, or it can mask a true cause-and-effect association; this kind of error is referred to as <em>confounding</em>. To put it another way, you can think of confounding as the entanglement of the effects of one or more predictors on the response.</p>&#13;
<p class="bull">• A <em>nuisance</em> or <em>extraneous variable</em> is a predictor of secondary or no interest that has the potential to confound relationships between other variables and so affect your estimates of the other regression coefficients. Extraneous variables are included in the modeling as a matter of necessity, but the specific nature of their influence on the response is not the primary interest of the analysis.</p>&#13;
<p class="indentt">These definitions will become clearer once you begin fitting and interpreting the regression models in <a href="ch21.xhtml#ch21lev1sec69">Section 21.3</a>. The main message I want to emphasize here, once more, is that correlation does not imply causation. If a fitted model finds a statistically significant association between a predictor (or predictors) and a response, it’s important to consider the possibility that lurking variables are contributing to the results and to attempt to control any confounding before you draw conclusions. Multiple regression models allow you to do this.</p>&#13;
<h3 class="h3" id="ch21lev1sec68"><strong>21.2 Theory</strong></h3>&#13;
<p class="noindent">Before you start using R to fit regression models, you’ll examine the technical definitions of a linear regression model with multiple predictors. Here, you’ll look at how the models work in a mathematical sense and get a glimpse of the calculations that happen “behind the scenes” when estimating the model parameters in R.</p>&#13;
<h4 class="h4" id="ch21lev2sec190"><span epub:type="pagebreak" id="page_487"/><strong><em>21.2.1 Extending the Simple Model to a Multiple Model</em></strong></h4>&#13;
<p class="noindent">Rather than having just one predictor, you want to determine the value of a continuous response variable <em>Y</em> given the values of <em>p</em> &gt; 1 independent explanatory variables <em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, . . ., <em>X</em><sub>p</sub>. The overarching model is defined as</p>&#13;
<div class="imagec"><a id="ch21eq1"/><img src="../images/e21-1.jpg" alt="image"/></div>&#13;
<p class="noindent">where <em>β</em><sub>0</sub>, . . . , <em>β</em><sub>p</sub> are the regression coefficients and, as before, you assume independent, normally distributed residuals є ~ N(0, ˙) around the mean.</p>&#13;
<p class="indent">In practice, you have <em>n</em> data records; each record provides values for each of the predictors <em>X</em><sub><em>j</em></sub> ; <em>j</em> = {1, . . ., <em>p</em>}. The model to be fitted is given in terms of the mean response, conditional upon a particular realization of the set of explanatory variables</p>&#13;
<div class="imagec"><img src="../images/f0487-01.jpg" alt="image"/></div>&#13;
<p class="indent">where the <img class="middle" src="../images/bj.jpg" alt="image"/> represent estimates of the regression coefficients.</p>&#13;
<p class="indent">In simple linear regression, where you have only one predictor variable, recall that the goal is to find the “line of best fit.” The idea of least-squares estimation for linear models with multiple independent predictors follows much the same motivation. Now, however, in an abstract sense you can think of the relationship between response and predictors as a multidimensional plane or surface. You want to find the surface that best fits your multivariate data in terms of minimizing the overall squared distance between itself and the raw response data.</p>&#13;
<p class="indent">More formally, for your <em>n</em> data records, the <img class="middle" src="../images/bj.jpg" alt="image"/> are found as the values that minimize the sum</p>&#13;
<div class="imagec"><a id="ch21eq2"/><img src="../images/e21-2.jpg" alt="image"/></div>&#13;
<p class="noindent">where <em>x</em> <sub><em>j</em></sub>,<sub><em>i</em></sub> is the observed value of individual <em>i</em> for explanatory variable <em>X</em><sub><em>j</em></sub> and <em>y<sub>i</sub></em> is their response value.</p>&#13;
<h4 class="h4" id="ch21lev2sec191"><strong><em>21.2.2 Estimating in Matrix Form</em></strong></h4>&#13;
<p class="noindent">The computations involved in minimizing this squared distance (21.2) are made much easier by a <em>matrix representation</em> of the data. When dealing with <em>n</em> multivariate observations, you can write <a href="ch21.xhtml#ch21eq1">Equation (21.1)</a> as follows,</p>&#13;
<p class="center"><strong><em>Y</em></strong> = <strong><em>X</em></strong> · + є,</p>&#13;
<p class="noindent">where <strong><em>Y</em></strong> and є denote <em>n</em> × 1 column matrices such that</p>&#13;
<div class="imagec"><img src="../images/f0487-02.jpg" alt="image"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_488"/>Here, <em>y<sub>i</sub></em> and є<sub><em>i</em></sub> refer to the response observation and random error term for the <em>i</em>th individual. The quantity <em><strong>β</strong></em> is a (<em>p</em> + 1) × 1 column matrix of the regression coefficients, and then the observed predictor data for all individuals and explanatory variables are stored in an <em>n</em> × (<em>p</em> + 1) matrix <strong><em>X</em></strong>, called the <em>design matrix</em>:</p>&#13;
<div class="imagec"><img src="../images/f0488-01.jpg" alt="image"/></div>&#13;
<p class="indent">The minimization of (21.2) providing the estimated regression coefficient values is then found with the following calculation:</p>&#13;
<div class="imagec"><a id="ch21eq3"/><img src="../images/e21-3.jpg" alt="image"/></div>&#13;
<p class="indentb">It’s important to note the following:</p>&#13;
<p class="bull">• The symbol · represents matrix multiplication, the superscript <sup><span class="ent">⊤</span></sup> represents the transpose, and <sup>−</sup><sup>1</sup> represents the inverse when applied to matrices (as per <a href="ch03.xhtml#ch03lev1sec14">Section 3.3</a>).</p>&#13;
<p class="bull">• Extending the size of <em>β</em> and <strong><em>X</em></strong> (note the leading column of 1s in <strong><em>X</em></strong>) to create structures of size <em>p</em> + 1 (as opposed to just the number of predictors <em>p</em>) allows for the estimation of the overall intercept <em>β</em><sub>0</sub>.</p>&#13;
<p class="bull">• As well as (21.3), the design matrix plays a crucial role in the estimation of other quantities, such as the standard errors of the coefficients.</p>&#13;
<h4 class="h4" id="ch21lev2sec192"><strong><em>21.2.3 A Basic Example</em></strong></h4>&#13;
<p class="noindent">You can manually estimate the <em>β<sub>j</sub></em> (<em>j</em> = 0, 1, . . ., <em>p</em>) in R using the functions covered in <a href="ch03.xhtml#ch03">Chapter 3</a>: <code>%*%</code> (matrix multiplication), <code>t</code> (matrix transposition), and <code>solve</code> (matrix inversion). As a quick demonstration, let’s say you have two predictor variables: <em>X<sub>1</sub></em> as continuous and <em>X<sub>2</sub></em> as binary. Your target regression equation is therefore <img class="middle" src="../images/f0488-01a.jpg" alt="image"/>. Suppose you collect the following data, where the response data, data for <em>X</em><sub>1</sub>, and data for <em>X</em><sub>2</sub>, for <em>n</em> = 8 individuals, are given in the columns <code>y</code>, <code>x1</code>, and <code>x2</code>, respectively.</p>&#13;
<pre>R&gt; demo.data &lt;- data.frame(y=c(1.55,0.42,1.29,0.73,0.76,-1.09,1.41,-0.32),<br/>                           x1=c(1.13,-0.73,0.12,0.52,-0.54,-1.15,0.20,-1.09),<br/>                           x2=c(1,0,1,1,0,1,0,1))<br/>R&gt; demo.data<br/>      y    x1 x2<br/>1  1.55  1.13  1<br/>2  0.42 -0.73  0<br/>3  1.29  0.12  1<br/>4  0.73  0.52  1<br/>5  0.76 -0.54  0<br/>6 -1.09 -1.15  1<br/>7  1.41  0.20  0<br/>8 -0.32 -1.09  1</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_489"/>To get your point estimates in <strong><em>β</em></strong> = [<em>β</em><sub>0</sub>, <em>β</em><sub>1</sub>, <em>β</em><sub>2</sub>]<sup><span class="ent">┬</span></sup> for the linear model, you first have to construct <strong><em>X</em></strong> and <strong><em>Y</em></strong> as required by (21.3).</p>&#13;
<pre>R&gt; Y &lt;- matrix(demo.data$y)<br/>R&gt; Y<br/>      [,1]<br/>[1,]  1.55<br/>[2,]  0.42<br/>[3,]  1.29<br/>[4,]  0.73<br/>[5,]  0.76<br/>[6,] -1.09<br/>[7,]  1.41<br/>[8,] -0.32<br/>R&gt; n &lt;- nrow(demo.data)<br/>R&gt; X &lt;- matrix(c(rep(1,n),demo.data$x1,demo.data$x2),nrow=n,ncol=3)<br/>R&gt; X<br/>     [,1]  [,2] [,3]<br/>[1,]    1  1.13    1<br/>[2,]    1 -0.73    0<br/>[3,]    1  0.12    1<br/>[4,]    1  0.52    1<br/>[5,]    1 -0.54    0<br/>[6,]    1 -1.15    1<br/>[7,]    1  0.20    0<br/>[8,]    1 -1.09    1</pre>&#13;
<p class="indent">Now all you have to do is execute the line corresponding to (21.3).</p>&#13;
<pre>R&gt; BETA.HAT &lt;- solve(t(X)<br/>R&gt; BETA.HAT<br/>           [,1]<br/>[1,]  1.2254572<br/>[2,]  1.0153004<br/>[3,] -0.6980189</pre>&#13;
<p class="indent">You’ve just used least-squares to fit your model based on the observed data in <code>demo.data</code>, which results in the estimates <img class="middle" src="../images/f0489-01.jpg" alt="image"/>, <img class="middle" src="../images/f0489-02.jpg" alt="image"/>, and <img class="middle" src="../images/f0489-03.jpg" alt="image"/>.</p>&#13;
<h3 class="h3" id="ch21lev1sec69"><span epub:type="pagebreak" id="page_490"/><strong>21.3 Implementing in R and Interpreting</strong></h3>&#13;
<p class="noindent">Ever helpful, R automatically builds the matrices and carries out all the necessary calculations when you instruct it to fit a multiple linear regression model. As in simple regression models, you use <code>lm</code> and just include any additional predictors when you specify the formula in the first argument. So that you can focus on the R syntax and on interpretation, I’ll focus only on <em>main effects</em> for the moment, and then you’ll explore more complex relationships later in the chapter.</p>&#13;
<p class="indent">When it comes to output and interpretation, working with multiple explanatory variables follows the same rules as you’ve seen in <a href="ch20.xhtml#ch20">Chapter 20</a>. Any numeric-continuous variables (or a categorical variable being treated as such) have a slope coefficient that provides a “per-unit-change” quantity. Any <em>k</em>-group categorical variables (factors, formally unordered) are dummy coded and provide <em>k</em> − 1 intercepts.</p>&#13;
<h4 class="h4" id="ch21lev2sec193"><strong><em>21.3.1 Additional Predictors</em></strong></h4>&#13;
<p class="noindent">Let’s first confirm the manual matrix calculations from a moment ago. Using the <code>demo.data</code> object, fit the multiple linear model and examine the coefficients from that object as follows:</p>&#13;
<pre>R&gt; demo.fit &lt;- lm(y~x1+x2,data=demo.data)<br/>R&gt; coef(demo.fit)<br/>(Intercept)          x1          x2<br/>  1.2254572   1.0153004  -0.6980189</pre>&#13;
<p class="indent">You’ll see that you obtain exactly the point estimates stored earlier in <code>BETA.HAT</code>.</p>&#13;
<p class="indent">With the response variable on the left as usual, you specify the multiple predictors on the right side of the <code>~</code> symbol; altogether this represents the formula argument. To fit a model with several main effects, use <code>+</code> to separate any variables you want to include. In fact, you’ve already seen this notation in <a href="ch19.xhtml#ch19lev2sec172">Section 19.2.2</a>, when investigating two-way ANOVA.</p>&#13;
<p class="indent">To study the interpretation of the parameter estimates of a multiple linear regression model, let’s return to the <code>survey</code> data set in the <code>MASS</code> package. In <a href="ch20.xhtml#ch20">Chapter 20</a>, you explored several simple linear regression models based on a response variable of student height, as well as stand-alone predictors of handspan (continuous) and sex (categorical, <em>k</em> = 2). You found that handspan was highly statistically significant, with the estimated coefficient suggesting an average increase of about 3.12 cm for each 1 cm increase in handspan. When you looked at the same <em>t</em>-test using sex as the explanatory variable, the model also suggested evidence against the null hypothesis, with “being male” adding around 13.14 cm to the mean height when compared to the mean for females (the category used as the reference level).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_491"/>What those models can’t tell you is the <em>joint effect</em> of sex and handspan on predicting height. If you include both predictors in a multiple linear model, you can (to some extent) reduce any confounding that might otherwise occur in the isolated fits of the effect of either single predictor on height.</p>&#13;
<pre>R&gt; survmult &lt;- lm(Height~Wr.Hnd+Sex,data=survey)<br/>R&gt; summary(survmult)<br/>Call:<br/>lm(formula = Height ~ Wr.Hnd + Sex, data = survey)<br/><br/>Residuals:<br/>     Min       1Q   Median       3Q      Max<br/>-17.7479  -4.1830   0.7749   4.6665  21.9253<br/><br/>Coefficients:<br/>            Estimate Std. Error t value Pr(&gt;|t|)<br/>(Intercept) 137.6870     5.7131  24.100  &lt; 2e-16 ***<br/>Wr.Hnd        1.5944     0.3229   4.937 1.64e-06 ***<br/>SexMale       9.4898     1.2287   7.724 5.00e-13 ***<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/><br/>Residual standard error: 6.987 on 204 degrees of freedom<br/>  (30 observations deleted due to missingness)<br/>Multiple R-squared:  0.5062, Adjusted R-squared:  0.5014<br/>F-statistic: 104.6 on 2 and 204 DF,  p-value: &lt; 2.2e-16</pre>&#13;
<p class="indent">The coefficient for handspan is now only about 1.59, almost half of its corresponding value (3.12 cm) in the stand-alone simple linear regression for height. Despite this, it’s still highly statistically significant in the presence of sex. The coefficient for sex has also reduced in magnitude when compared with its simple linear model and is also still significant in the presence of handspan. You’ll interpret these new figures in a moment.</p>&#13;
<p class="indent">As for the rest of the output, the <code>Residual standard error</code> still provides you with an estimate of the standard error of the random noise term є, and you’re also provided with an <code>R-squared</code> value. When associated with more than one predictor, the latter is formally referred to as the coefficient of <em>multiple</em> determination. The calculation of this coefficient, as in the single predictor setting, comes from the correlations between the variables in the model. I’ll leave the theoretical intricacies to more advanced texts, but it’s important to note that <code>R-squared</code> still represents the proportion of variability in the response that’s explained by the regression; in this example, it sits at around 0.51.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_492"/>You can continue to add explanatory variables in the same way if you need to do so. In <a href="ch20.xhtml#ch20lev2sec186">Section 20.5.2</a>, you examined smoking frequency as a stand-alone categorical predictor for height and found that this explanatory variable provided no statistical evidence of an impact on the mean response. But could the smoking variable contribute in a statistically significant way if you control for handspan and sex?</p>&#13;
<pre>R&gt; survmult2 &lt;- lm(Height~Wr.Hnd+Sex+Smoke,data=survey)<br/>R&gt; summary(survmult2)<br/><br/>Call:<br/>lm(formula = Height ~ Wr.Hnd + Sex + Smoke, data = survey)<br/><br/>Residuals:<br/>     Min       1Q   Median       3Q      Max<br/>-17.4869  -4.7617   0.7604   4.3691  22.1237<br/><br/>Coefficients:<br/>            Estimate Std. Error t value Pr(&gt;|t|)<br/>(Intercept) 137.4056     6.5444  20.996  &lt; 2e-16 ***<br/>Wr.Hnd        1.6042     0.3301   4.860 2.36e-06 ***<br/>SexMale       9.3979     1.2452   7.547 1.51e-12 ***<br/>SmokeNever   -0.0442     2.3135  -0.019    0.985<br/>SmokeOccas    1.5267     2.8694   0.532    0.595<br/>SmokeRegul    0.9211     2.9290   0.314    0.753<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/><br/>Residual standard error: 7.023 on 201 degrees of freedom<br/>  (30 observations deleted due to missingness)<br/>Multiple R-squared:  0.5085, Adjusted R-squared:  0.4962<br/>F-statistic: 41.59 on 5 and 201 DF,  p-value: &lt; 2.2e-16</pre>&#13;
<p class="indent">Since it’s a categorical variable with <em>k</em> &gt; 2 levels, <code>Smoke</code> is dummy coded (with heavy smokers as the default reference level), giving you three extra intercepts for the three nonreference levels of the variable; the fourth is incorporated into the overall intercept.</p>&#13;
<p class="indent">In the <code>summary</code> of the latest fit, you can see that while handspan and sex continue to yield very small <em>p</em>-values, smoking frequency suggests no such evidence against the hypotheses of zero coefficients. The smoking variable has had little effect on the values of the other coefficients compared with the previous model in <code>survmult</code>, and the <code>R-squared</code> coefficient of multiple determination has barely increased.</p>&#13;
<p class="indent">One question you might now ask is, if smoking frequency doesn’t benefit your ability to predict mean height in any substantial way, should you remove that variable from the model altogether? This is the primary goal of <em>model selection</em>: to find the “best” model for predicting the outcome, without <span epub:type="pagebreak" id="page_493"/>fitting one that is unnecessarily complex (by including more explanatory variables than is required). You’ll look at some common ways researchers attempt to achieve this in <a href="ch22.xhtml#ch22lev1sec73">Section 22.2</a>.</p>&#13;
<h4 class="h4" id="ch21lev2sec194"><strong><em>21.3.2 Interpreting Marginal Effects</em></strong></h4>&#13;
<p class="noindent">In multiple regression, the estimation of each predictor takes into account the effect of all other predictors present in the model. A coefficient for a specific predictor <em>Z</em> should therefore be interpreted as the change in the mean response for a one-unit increase in <em>Z</em>, while holding all other predictors constant.</p>&#13;
<p class="indentb">As you’ve determined that smoking frequency still appears to have no discernible impact on mean height when taking sex and handspan into consideration, return your focus to <code>survmult</code>, the model that includes only the explanatory variables of sex and handspan. Note the following:</p>&#13;
<p class="bull">• For students of the same sex (that is, focusing on either just males or just females), a 1 cm increase in handspan leads to an estimated increase of 1.5944 cm in mean height.</p>&#13;
<p class="bull">• For students of similar handspan, males on average will be 9.4898 cm taller than females.</p>&#13;
<p class="bull">• The difference in the values of the two estimated predictor coefficients when compared with their respective simple linear model fits, plus the fact that both continue to indicate evidence against the null hypothesis of “being zero” in the multivariate fit, suggests that confounding (in terms of the effect of both handspan and sex on the response variable of height) is present in the single-predictor models.</p>&#13;
<p class="indentt">The final point highlights the general usefulness of multiple regression. It shows that, in this example, if you use only single predictor models, the determination of the “true” impact that each explanatory variable has in predicting the mean response is misleading since some of the change in height is determined by sex, but some is also attributed to handspan. It’s worth noting that the coefficient of determination (refer to <a href="ch20.xhtml#ch20lev2sec179">Section 20.3.3</a>) for the <code>survmult</code> model is noticeably higher than the same quantity in either of the single-variate models, so you’re actually accounting for more of the variation in the response by using multiple regression.</p>&#13;
<p class="indent">The fitted model itself can be thought of as</p>&#13;
<div class="imagec"><a id="ch21eq4"/><img src="../images/e21-4.jpg" alt="image"/></div>&#13;
<p class="noindent">where “handspan” is the writing handspan supplied in centimeters and “sex” is supplied as either 1 (if male) or 0 (if female).</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The baseline (overall) intercept of around 137.687 cm represents the mean height of a female with a handspan of 0 cm—again, this is clearly not directly interpretable in the context of the application. For this kind of situation, some researchers center the offending continuous predictor (or predictors) on zero by subtracting the sample mean of all the observations on that predictor from each observation prior to fitting the model. The <span epub:type="pagebreak" id="page_494"/>centered predictor data are then used in place of the original (untranslated) data. The resulting fitted model allows you to use the mean value of the untranslated predictor (in this case handspan) rather than a zero value in order to directly interpret the intercept estimate</em> <img class="middle" src="../images/b0.jpg" alt="image"/>.</p>&#13;
</div>&#13;
<h4 class="h4" id="ch21lev2sec195"><strong><em>21.3.3 Visualizing the Multiple Linear Model</em></strong></h4>&#13;
<p class="noindent">As shown here, “being male” simply changes the overall intercept by around 9.49 cm:</p>&#13;
<pre>R&gt; survcoefs &lt;- coef(survmult)<br/>R&gt; survcoefs<br/>(Intercept)      Wr.Hnd     SexMale<br/> 137.686951    1.594446    9.489814<br/>R&gt; as.numeric(survcoefs[1]+survcoefs[3])<br/>[1] 147.1768</pre>&#13;
<p class="indent">Because of this, you could also write (21.4) as two equations. Here’s the equation for female students:</p>&#13;
<p class="center">“Mean height” = 137.687 + 1.594 × “handspan”</p>&#13;
<p class="indent">Here’s the equation for male students:</p>&#13;
<table>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">“Mean height”</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">=</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">(137.687 + 9.4898) + 1.594 × “handspan”</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td style="vertical-align: top;" class="table"><p class="table">=</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">147.177 + 1.594 × “handspan”</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">This is handy because it allows you to visualize the multivariate model in much the same way as you can the simple linear models. This code produces <a href="ch21.xhtml#ch21fig1">Figure 21-1</a>:</p>&#13;
<pre>R&gt; plot(survey$Height~survey$Wr.Hnd,<br/>        col=c("gray","black")[as.numeric(survey$Sex)],<br/>        pch=16,xlab="Writing handspan",ylab="Height")<br/>R&gt; abline(a=survcoefs[1],b=survcoefs[2],col="gray",lwd=2)<br/>R&gt; abline(a=survcoefs[1]+survcoefs[3],b=survcoefs[2],col="black",lwd=2)<br/>R&gt; legend("topleft",legend=levels(survey$Sex),col=c("gray","black"),pch=16)</pre>&#13;
<p class="indent">First, a scatterplot of the height and handspan observations, split by sex, is drawn. Then, <code>abline</code> adds the line corresponding to females and adds a second one corresponding to males, based on those two equations.</p>&#13;
<p class="indent">Although this plot might look like two separate simple linear model fits, one for each level of sex, it’s important to recognize that isn’t the case. You’re effectively looking at a representation of a multivariate model on a two-dimensional canvas, where the statistics that determine the fit of the two visible lines have been estimated “jointly,” in other words, when considering both predictors.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_495"/><img src="../images/f21-01.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch21fig1"/>Figure 21-1: Visualizing the observed data and fitted multiple linear model of student height modeled by handspan and sex</em></p>&#13;
<h4 class="h4" id="ch21lev2sec196"><strong><em>21.3.4 Finding Confidence Intervals</em></strong></h4>&#13;
<p class="noindent">As in <a href="ch20.xhtml#ch20">Chapter 20</a>, you can easily find confidence intervals for any of the regression parameters in multiple regression models with <code>confint</code>. Using <code>survmult2</code>, the object of the fitted model for student height including the smoking frequency predictor, the output of a call to <code>confint</code> looks like this:</p>&#13;
<pre>R&gt; confint(survmult2)<br/>                  2.5 %     97.5 %<br/>(Intercept) 124.5010442 150.310074<br/>Wr.Hnd        0.9534078   2.255053<br/>SexMale       6.9426040  11.853129<br/>SmokeNever   -4.6061148   4.517705<br/>SmokeOccas   -4.1312384   7.184710<br/>SmokeRegul  -4.8543683    6.696525</pre>&#13;
<p class="indent">Note that the <code>Wr.Hnd</code> and <code>SexMale</code> variables were shown to be statistically significant at the 5 percent level in the earlier model summary and that their 95 percent confidence levels do not include the null value of zero. On the other hand, all the coefficients for the dummy variables associated with the smoking frequency predictor are all nonsignificant, and their confidence intervals clearly include zero. This reflects the fact that the smoking variable isn’t, as a whole, considered statistically significant in this particular model.</p>&#13;
<h4 class="h4" id="ch21lev2sec197"><span epub:type="pagebreak" id="page_496"/><strong><em>21.3.5 Omnibus F-Test</em></strong></h4>&#13;
<p class="noindent">First encountered in <a href="ch20.xhtml#ch20lev2sec186">Section 20.5.2</a> in the context of multilevel predictors, you can think of the omnibus <em>F</em>-test more generally for multiple regression models as a test with the following hypotheses:</p>&#13;
<div class="imagec"><a id="ch21eq5"/><img src="../images/e21-5.jpg" alt="image"/></div>&#13;
<p class="indent">The test is effectively comparing the amount of error attributed to the “null” model (in other words, one with an intercept only) with the amount of error attributed to the predictors when all the predictors are present. In other words, the more the predictors are able to model the response, the more error they explain, giving you a more extreme <em>F</em> statistic and therefore a smaller <em>p</em>-value. The single result makes the test especially useful when you have many explanatory variables. The test works the same regardless of the mix of predictors you have in a given model: one or more might be continuous, discrete, binary, and/or categorical with <em>k</em> &gt; 2 levels. When multiple regression models are fitted, the amount of output alone can take time to digest and interpret, and care must be taken to avoid Type I errors (incorrect rejection of a true null hypothesis—refer to <a href="ch18.xhtml#ch18lev1sec58">Section 18.5</a>).</p>&#13;
<p class="indent">The <em>F</em>-test helps boil all that down, allowing you to conclude either of the following:</p>&#13;
<ol>&#13;
<li><p class="noindent">Evidence against H<sub>0</sub> if the associated <em>p</em>-value is smaller than your chosen significance level <em>α</em>, which suggests that your regression—your combination of the explanatory variables—does a significantly better job of predicting the response than if you removed <em>all</em> those predictors.</p></li>&#13;
<li><p class="noindent">No evidence against H<sub>0</sub> if the associated <em>p</em>-value is larger than <em>α</em>, which suggests that using the predictors has no tangible benefit over having an intercept alone.</p></li>&#13;
</ol>&#13;
<p class="indent">The downside is that the test doesn’t tell you which of the predictors (or which subset thereof) is having a beneficial impact on the fit of the model, nor does it tell you anything about their coefficients or respective standard errors.</p>&#13;
<p class="indent">You can compute the <em>F</em>-test statistic using the coefficient of determination, <em>R</em><sup>2</sup>, from the fitted regression model. Let <em>p</em> be the number of regression parameters requiring estimation, excluding the intercept <em>β</em><sub>0</sub>. Then,</p>&#13;
<div class="imagec"><a id="ch21eq6"/><img src="../images/e21-6.jpg" alt="image"/></div>&#13;
<p class="noindent">where <em>n</em> is the number of observations used in fitting the model (after records with missing values have been deleted). Then, under H<sub>0</sub> in (21.5), <img class="middle" src="../images/f.jpg" alt="image"/> follows an <em>F</em> distribution (see <a href="ch16.xhtml#ch16lev2sec145">Section 16.2.5</a> and also <a href="ch19.xhtml#ch19lev2sec169">Section 19.1.2</a>) with df<sub>1</sub> = <em>p</em>, df<sub>2</sub> = <em>n</em>− <em>p</em>−1 degrees of freedom. The <em>p</em>-value associated with (21.6) is yielded as the upper-tail area of that <em>F</em> distribution.</p>&#13;
<p class="indent">As a quick exercise to confirm this, turn your attention back to the fitted multiple regression model <code>survmult2</code> in <a href="ch21.xhtml#ch21lev2sec193">Section 21.3.1</a>, which is the model <span epub:type="pagebreak" id="page_497"/>for student height by handspan, sex, and smoking status from <code>survey</code>. You can extract the coefficient of multiple determination from the <code>summary</code> report (using the technique noted in <a href="ch20.xhtml#ch20lev2sec180">Section 20.3.4</a>).</p>&#13;
<pre>R&gt; R2 &lt;- summary(survmult2)$r.squared<br/>R&gt; R2<br/>[1] 0.508469</pre>&#13;
<p class="indent">This matches the multiple R-squared value from <a href="ch21.xhtml#ch21lev2sec193">Section 21.3.1</a>. Then, you can get <em>n</em> as the original size of the data set in <code>survey</code> minus any missing values (reported as 30 in the earlier <code>summary</code> output).</p>&#13;
<pre>R&gt; n &lt;- nrow(survey)-30<br/>R&gt; n<br/>[1] 207</pre>&#13;
<p class="indent">You get <em>p</em> as the number of estimated regression parameters (minus 1 for the intercept).</p>&#13;
<pre>R&gt; p &lt;- length(coef(survmult2))-1<br/>R&gt; p<br/>[1] 5</pre>&#13;
<p class="indent">You can then confirm the value of <em>n</em> − <em>p</em> − 1, which matches the <code>summary</code> output (<code>201 degrees of freedom</code>):</p>&#13;
<pre>R&gt; n-p-1<br/>[1] 201</pre>&#13;
<p class="indent">Finally, you find the test statistic <img class="middle" src="../images/f.jpg" alt="image"/> as dictated by (21.6), and you can use the <code>pf</code> function as follows to obtain the corresponding <em>p</em>-value for the test:</p>&#13;
<pre>R&gt; Fstat &lt;- (R2*(n-p-1))/((1-R2)*p)<br/>R&gt; Fstat<br/>[1] 41.58529<br/>R&gt; 1-pf(Fstat,df1=p,df2=n-p-1)<br/>[1] 0</pre>&#13;
<p class="indent">You can see that the omnibus <em>F</em>-test for this example gives a <em>p</em>-value that’s so small, it’s effectively zero. These calculations match the relevant results reported in the output of <code>summary(survmult2)</code> completely.</p>&#13;
<p class="indent">Looking back at the student height multiple regression fit based on handspan, sex, and smoking in <code>survmult2</code> in <a href="ch21.xhtml#ch21lev2sec193">Section 21.3.1</a>, it’s little surprise that with two of the predictors yielding small <em>p</em>-values, the omnibus <em>F</em>-test suggests strong evidence against H<sub>0</sub> based on (21.5). This highlights the “umbrella” nature of the omnibus test: although the smoking frequency variable itself doesn’t appear to contribute anything statistically important, <span epub:type="pagebreak" id="page_498"/>the <em>F</em>-test for that model still suggests <code>survmult2</code> should be preferred over a “no-predictor” model, because both handspan and sex are important.</p>&#13;
<h4 class="h4" id="ch21lev2sec198"><strong><em>21.3.6 Predicting from a Multiple Linear Model</em></strong></h4>&#13;
<p class="noindent">Prediction (or <em>forecasting</em>) for multiple regression follows the same rules as for simple regression. It’s important to remember that point predictions found for a particular <em>covariate profile</em>—the collection of predictor values for a given individual—are associated with the mean (or <em>expected value</em>) of the response; that confidence intervals provide measures for mean responses; and that prediction intervals provide measures for raw observations. You also have to consider the issue of interpolation (predictions based on <em>x</em> values that fall within the range of the originally observed covariate data) versus extrapolation (prediction from <em>x</em> values that fall outside the range of said data). Other than that, the R syntax for <code>predict</code> is identical to that used in <a href="ch20.xhtml#ch20lev1sec65">Section 20.4</a>.</p>&#13;
<p class="indent">As an example, using the model fitted on student height as a linear function of handspan and sex (in <code>survmult</code>), you can estimate the mean height of a male student with a writing handspan of 16.5 cm, together with a confidence interval.</p>&#13;
<pre>R&gt; predict(survmult,newdata=data.frame(Wr.Hnd=16.5,Sex="Male"),<br/>           interval="confidence",level=0.95)<br/>       fit      lwr      upr<br/>1 173.4851 170.9419 176.0283</pre>&#13;
<p class="indent">The result indicates that you have an expected value of about 173.48 cm and that you can be 95 percent confident the true value lies somewhere between 170.94 and 176.03 (rounded to 2 d.p.). In the same way, the mean height of a female with a handspan of 13 cm is estimated at 158.42 cm, with a 99 percent prediction interval of 139.76 to 177.07.</p>&#13;
<pre>R&gt; predict(survmult,newdata=data.frame(Wr.Hnd=13,Sex="Female"),<br/>           interval="prediction",level=0.99)<br/>       fit      lwr      upr<br/>1 158.4147 139.7611 177.0684</pre>&#13;
<p class="indent">There are in fact two female students in the data set with writing handspans of 13 cm, as you can see in <a href="ch21.xhtml#ch21fig1">Figure 21-1</a>. Using your knowledge of subsetting data frames, you can inspect these two records and select the three variables of interest.</p>&#13;
<pre>R&gt; survey[survey$Sex=="Female" &amp; survey$Wr.Hnd==13,c("Sex","Wr.Hnd","Height")]<br/>       Sex Wr.Hnd Height<br/>45  Female     13 180.34<br/>152 Female     13 165.00</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_499"/>Now, the second female’s height falls well inside the prediction interval, but the first female’s height is significantly higher than the upper limit. It’s important to realize that, technically, nothing has gone wrong here in terms of the model fitting and interpretation—it’s still possible that an observation can fall outside a prediction interval, even a wide 99 percent interval, though it’s perhaps improbable. There could be any number of reasons for this occurring. First, the model could be inadequate. For example, you might be excluding important predictors in the fitted model and therefore have less predictive power. Second, although the prediction is within the range of the observed data, it has occurred at one extreme end of the range, where it’s less reliable because your data are relatively sparse. Third, the observation itself may be tainted in some way—perhaps the individual recorded her handspan incorrectly, in which case her invalid observation should be removed prior to model fitting. It’s with this critical eye that a good statistician will appraise data and models; this is a skill that I’ll emphasize further as this chapter unfolds.</p>&#13;
<div class="ex">&#13;
<p class="ext"><a id="ch21exc1"/><strong>Exercise 21.1</strong></p>&#13;
<p class="noindentz">In the <code>MASS</code> package, you’ll find the data frame <code>cats</code>, which provides data on sex, body weight (in kilograms), and heart weight (in grams) for 144 household cats (see <a href="ref.xhtml#ref69">Venables and Ripley, 2002</a>, for further details); you can read the documentation with a call to <code>?cats</code>. Load the <code>MASS</code> package with a call to <code>library("MASS")</code>, and access the object directly by entering <code>cats</code> at the console prompt.</p>&#13;
<ol type="a">&#13;
<li><p class="noindents">Plot heart weight on the vertical axis and body weight on the horizontal axis, using different colors or point characters to distinguish between male and female cats. Annotate your plot with a legend and appropriate axis labels.</p></li>&#13;
<li><p class="noindents">Fit a least-squares multiple linear regression model using heart weight as the response variable and the other two variables as predictors, and view a model summary.</p>&#13;
<ol type="i">&#13;
<li><p class="noindents">Write down the equation for the fitted model and interpret the estimated regression coefficients for body weight and sex. Are both statistically significant? What does this say about the relationship between the response and predictors?</p></li>&#13;
<li><p class="noindents">Report and interpret the coefficient of determination and the outcome of the omnibus <em>F</em>-test.</p></li>&#13;
</ol></li>&#13;
<li><p class="noindents">Tilman’s cat, Sigma, is a 3.4 kg female. Use your model to estimate her mean heart weight and provide a 95 percent prediction interval.</p></li>&#13;
<li><p class="noindents"><span epub:type="pagebreak" id="page_500"/>Use <code>predict</code> to superimpose continuous lines based on the fitted linear model on your plot from (a), one for male cats and one for female. What do you notice? Does this reflect the statistical significance (or lack thereof) of the parameter estimates?</p></li>&#13;
</ol>&#13;
<p class="noindentz">The <code>boot</code> package (<a href="ref.xhtml#ref16">Davison and Hinkley, 1997</a>; <a href="ref.xhtml#ref09">Canty and Ripley, 2015</a>) is another library of R code that’s included with the standard installation but isn’t automatically loaded. Load <code>boot</code> with a call to <code>library("boot")</code>. You’ll find a data frame called <code>nuclear</code>, which contains data on the construction of nuclear power plants in the United States in the late 1960s (<a href="ref.xhtml#ref14">Cox and Snell, 1981</a>).</p>&#13;
<ol type="a" start="5">&#13;
<li><p class="noindents">Access the documentation by entering <code>?nuclear</code> at the prompt and examine the details of the variables. (Note there is a mistake for <code>date</code>, which provides the date that the construction permits were issued—it should read “measured in years since January 1 <strong>1900</strong> to the nearest month.”) Use <code>pairs</code> to produce a quick scatterplot matrix of the data.</p></li>&#13;
<li><p class="noindents">One of the original objectives was to predict the cost of further construction of these power plants. Create a fit and summary of a linear regression model that aims to model <code>cost</code> by <code>t1</code> and <code>t2</code>, two variables that describe different elapsed times associated with the application for and issue of various permits. Take note of the estimated regression coefficients and their significance in the fitted model.</p></li>&#13;
<li><p class="noindents">Refit the model, but this time also include an effect for the date the construction permit was issued. Contrast the output for this new model against the previous one. What do you notice, and what does this information suggest about the relationships in the data with respect to these predictors?</p></li>&#13;
<li><p class="noindents">Fit a third model for power plant cost, using the predictors for “date of permit issue,” “power plant capacity,” and the binary variable describing whether the plant was sited in the northeastern United States. Write down the fitted model equation and provide 95 percent confidence intervals for each estimated coefficient.</p></li>&#13;
</ol>&#13;
<p class="noindentz">The following table gives an excerpt of a historical data set compiled between 1961 and 1973. It concerns the annual murder rate in Detroit, Michigan; the data were originally presented and analyzed by Fisher (<a href="ref.xhtml#ref23">1976</a>) and are reproduced here from Harraway (<a href="ref.xhtml#ref30">1995</a>). In the data set you’ll find the number of murders, police officers, and gun licenses issued per 100,000 population, as well as the overall unemployment rate as a percentage of the overall population.</p>&#13;
<table class="topbot2">&#13;
<thead>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table2r"><p class="tablec"><span epub:type="pagebreak" id="page_501"/><strong>Murders</strong></p></td>&#13;
<td style="vertical-align: top;" class="table2r"><p class="tablec"><strong>Police</strong></p></td>&#13;
<td style="vertical-align: top;" class="table2r"><p class="tablec"><strong>Unemployment</strong></p></td>&#13;
<td style="vertical-align: top;" class="table2r"><p class="tablec"><strong>Guns</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">8.60</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">260.35</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">11.0</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">178.15</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">8.90</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">269.80</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">7.0</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">156.41</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">8.52</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">272.04</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">5.2</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">198.02</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">8.89</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">272.96</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">4.3</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">222.10</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">13.07</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">272.51</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">3.5</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">301.92</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">14.57</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">261.34</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">3.2</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">391.22</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">21.36</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">268.89</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">4.1</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">665.56</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">28.03</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">295.99</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">3.9</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">1131.21</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">31.49</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">319.87</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">3.6</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">837.60</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">37.39</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">341.43</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">7.1</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">794.90</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">46.26</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">356.59</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">8.4</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">817.74</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">47.24</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">376.69</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">7.7</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">583.17</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">52.33</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">390.19</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">6.3</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">709.59</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<ol type="a" start="9">&#13;
<li><p class="noindents">Create your own data frame in your R workspace and produce a scatterplot matrix. Which of the variables appears to be most strongly related to the murder rate?</p></li>&#13;
<li><p class="noindents">Fit a multiple linear regression model using the number of murders as the response and all other variables as predictors. Write down the model equation and interpret the coefficients. Is it reasonable to state that all relationships between the response and the predictors are causal?</p></li>&#13;
<li><p class="noindents">Identify the amount of variation in the response attributed to the joint effect of the three explanatory variables. Then refit the model excluding the predictor associated with the largest (in other words, “most nonsignificant”) <em>p</em>-value. Compare the new coefficient of determination with that of the previous model. Is there much difference?</p></li>&#13;
<li><p class="noindents">Use your model from (k) to predict the mean number of murders per 100,000 residents, with 300 police officers and 500 issued gun licenses. Compare this to the mean response if there were no gun licenses issued and provide 99 percent confidence intervals for both predictions.</p></li>&#13;
</ol>&#13;
</div>&#13;
<h3 class="h3" id="ch21lev1sec70"><strong>21.4 Transforming Numeric Variables</strong></h3>&#13;
<p class="noindent">Sometimes, the linear function as strictly defined by the standard regression equation, (21.1), can be inadequate when it comes to capturing relationships between a response and selected covariates. You might, for example, observe curvature in a scatterplot between two numeric variables to which a perfectly straight line isn’t necessarily best suited. To a certain extent, the <span epub:type="pagebreak" id="page_502"/>requirement that your data exhibit such linear behavior in order for a linear regression model to be appropriate can be relaxed by simply transforming (typically in a nonlinear fashion) certain variables before any estimation or model fitting takes place.</p>&#13;
<p class="indent"><em>Numeric transformation</em> refers to the application of a mathematical function to your numeric observations in order to rescale them. Finding the square root of a number and converting a temperature from Fahrenheit to Celsius are both examples of a numeric transformation. In the context of regression, transformation is generally applied only to continuous variables and can be done in any number of ways. In this section, you’ll limit your attention to examples using the two most common approaches: <em>polynomial</em> and <em>logarithmic</em> transformations. However, note that the appropriateness of the methods used to transform variables, and any modeling benefits that might occur, can only really be considered on a case-by-case basis.</p>&#13;
<p class="indent">Transformation in general doesn’t represent a universal solution to solving problems of nonlinearity in the trends in your data, but it can at least improve how faithfully a linear model is able to represent those trends.</p>&#13;
<h4 class="h4" id="ch21lev2sec199"><strong><em>21.4.1 Polynomial</em></strong></h4>&#13;
<p class="noindent">Following on from a comment made earlier, let’s say you observe a curved relationship in your data such that a straight line isn’t a sensible choice for modeling it. In an effort to fit your data more closely, a polynomial or <em>power</em> transformation can be applied to a specific predictor variable in your regression model. This is a straightforward technique that, by allowing <em>polynomial curvature</em> in the relationships, allows changes in that predictor to influence the response in more complex ways than otherwise possible. You achieve this by including additional terms in the model definition that represent the impact of progressively higher powers of the variable of interest on the response.</p>&#13;
<p class="indent">To clarify the concept of polynomial curvature, consider the following sequence between −4 and 4, as well as the simple vectors computed from it:</p>&#13;
<pre>R&gt; x &lt;- seq(-4,4,length=50)<br/>R&gt; y &lt;- x<br/>R&gt; y2 &lt;- x + x^2<br/>R&gt; y3 &lt;- x + x^2 + x^3</pre>&#13;
<p class="indent">Here, you’re taking the original value of <code>x</code> and calculating specific functionals of it. The vector <code>y</code>, as a copy of <code>x</code>, is clearly linear (in technical terms, this is a “polynomial of order 1”). You assign <code>y2</code> to take on an additionally squared valued of <code>x</code>, providing <em>quadratic</em> behavior—a polynomial of order 2. Lastly, the vector <code>y3</code> represents the results of a <em>cubic</em> function of the values of <code>x</code>, with the inclusion of <code>x</code> raised to the power of 3—a polynomial of order 3.</p>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_503"/>The following three lines of code produce, separately, the plots from left to right in <a href="ch21.xhtml#ch21fig2">Figure 21-2</a>.</p>&#13;
<pre>R&gt; plot(x,y,type="l")<br/>R&gt; plot(x,y2,type="l")<br/>R&gt; plot(x,y3,type="l")</pre>&#13;
<div class="image"><img src="../images/f21-02.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch21fig2"/>Figure 21-2: Illustrating linear (left), quadratic (middle), and cubic functions (right) of</em> <code>x</code></p>&#13;
<p class="indent">Perhaps a bit more generally, let’s say you have data for a continuous predictor, <em>X</em>, that you want to use to model your response, <em>Y</em> . Following estimation in the usual way, linearly, the simple model is <img class="middle" src="../images/f0503-01.jpg" alt="image"/>; a quadratic trend in <em>X</em> can be modeled via the multiple regression <img class="middle" src="../images/f0503-02.jpg" alt="image"/>; a cubic relationship can be captured by <img class="middle" src="../images/f0503-03.jpg" alt="image"/>; and so on. From the plots in <a href="ch21.xhtml#ch21fig2">Figure 21-2</a>, a good way to interpret the effects of including these extra terms is in the complexity of the curves that can be captured. At order 1, the linear relationship allows no curvature. At order 2, a quadratic function of any given variable allows one “bend.” At order 3, the model can cope with two bends in the relationship, and this continues if you keep adding terms corresponding to increasing powers of the covariate. The regression coefficients associated with these terms (all implied to be 1 in the code that produced the previous plots) are able to control the specific appearance (in other words, the <em>strength</em> and <em>direction</em>) of the curvature.</p>&#13;
<h5 class="h5" id="ch21lev3sec98"><strong>Fitting a Polynomial Transformation</strong></h5>&#13;
<p class="noindent">Return your attention to the built-in <code>mtcars</code> data set. Consider the <code>disp</code> variable, which describes engine displacement volume in cubic inches, against a response variable of miles per gallon. If you examine a plot of the data in <a href="ch21.xhtml#ch21fig3">Figure 21-3</a>, you can see that there does appear to be a slight yet noticeable curve in the relationship between displacement and mileage.</p>&#13;
<pre>R&gt; plot(mtcars$disp,mtcars$mpg,xlab="Displacement (cu. in.)",ylab="MPG")</pre>&#13;
<div class="image"><span epub:type="pagebreak" id="page_504"/><img src="../images/f21-03.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch21fig3"/>Figure 21-3: Scatterplot of miles per gallon and engine displacement, for the</em> <code>mtcars</code> <em>data</em></p>&#13;
<p class="indent">Is the straight line that a simple linear regression model would provide really the best way to represent this relationship? To investigate this, start by fitting that basic linear setup.</p>&#13;
<pre>R&gt; car.order1 &lt;- lm(mpg~disp,data=mtcars)<br/>R&gt; summary(car.order1)<br/><br/>Call:<br/>lm(formula = mpg ~ disp, data = mtcars)<br/><br/>Residuals:<br/>    Min      1Q  Median      3Q     Max<br/>-4.8922 -2.2022 -0.9631  1.6272  7.2305<br/><br/>Coefficients:<br/>             Estimate Std. Error t value Pr(&gt;|t|)<br/>(Intercept) 29.599855   1.229720  24.070  &lt; 2e-16 ***<br/>disp        -0.041215   0.004712  -8.747 9.38e-10 ***<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/><br/>Residual standard error: 3.251 on 30 degrees of freedom<br/>Multiple R-squared:  0.7183,        Adjusted R-squared:  0.709<br/>F-statistic: 76.51 on 1 and 30 DF,  p-value: 9.38e-10</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_505"/>This clearly indicates statistical evidence of a negative linear impact of displacement on mileage—for each additional cubic inch of displacement, the mean response decreases by about 0.041 miles per gallon.</p>&#13;
<p class="indent">Now, try to capture the apparent curve in the data by adding a quadratic term in <code>disp</code> to the model. You can do this in two ways. First, you could create a new vector in the workspace by simply squaring the <code>mtcars$disp</code> vector and then supplying the result to the formula in <code>lm</code>. Second, you could specify <code>disp^2</code> directly as an additive term in the formula. If you do it this way, it’s essential to wrap that particular expression in a call to <code>I</code> as follows:</p>&#13;
<pre>R&gt; car.order2 &lt;- lm(mpg~disp+I(disp^2),data=mtcars)<br/>R&gt; summary(car.order2)<br/><br/>Call:<br/>lm(formula = mpg ~ disp + I(disp^2), data = mtcars)<br/><br/>Residuals:<br/>    Min      1Q  Median      3Q     Max<br/>-3.9112 -1.5269 -0.3124  1.3489  5.3946<br/><br/>Coefficients:<br/>              Estimate Std. Error t value Pr(&gt;|t|)<br/>(Intercept)  3.583e+01  2.209e+00  16.221 4.39e-16 ***<br/>disp        -1.053e-01  2.028e-02  -5.192 1.49e-05 ***<br/>I(disp^2)    1.255e-04  3.891e-05   3.226   0.0031 **<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/><br/>Residual standard error: 2.837 on 29 degrees of freedom<br/>Multiple R-squared:  0.7927,        Adjusted R-squared:  0.7784<br/>F-statistic: 55.46 on 2 and 29 DF,  p-value: 1.229e-10</pre>&#13;
<p class="indent">Use of the <code>I</code> function around a given term in the formula is necessary when said term requires an arithmetic calculation—in this case, <code>disp^2</code>—before the model itself is actually fitted.</p>&#13;
<p class="indent">Turning to the fitted multiple regression model itself, you can see that the contribution of the squared component is statistically significant—the output corresponding to <code>I(disp^2)</code> shows a <em>p</em>-value of 0.0031. This implies that even if a linear trend is taken into account, the model that includes a quadratic component (which introduces a curve) is a better-fitting model. This conclusion is supported by a noticeably higher coefficient of determination compared to the first fit (0.7927 against 0.7183). You can see the fit of this quadratic curve in <a href="ch21.xhtml#ch21fig4">Figure 21-4</a> (code for which follows shortly).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_506"/>Here you might reasonably wonder whether you can improve the ability of the model to capture the relationship further by adding yet another higher-order term in the covariate of interest. To that end:</p>&#13;
<pre>R&gt; car.order3 &lt;- lm(mpg~disp+I(disp^2)+I(disp^3),data=mtcars)<br/>R&gt; summary(car.order3)<br/><br/>Call:<br/>lm(formula = mpg ~ disp + I(disp^2) + I(disp^3), data = mtcars)<br/><br/>Residuals:<br/>    Min      1Q  Median      3Q     Max<br/>-3.0896 -1.5653 -0.3619  1.4368  4.7617<br/><br/>Coefficients:<br/>              Estimate Std. Error t value Pr(&gt;|t|)<br/>(Intercept)  5.070e+01  3.809e+00  13.310 1.25e-13 ***<br/>disp        -3.372e-01  5.526e-02  -6.102 1.39e-06 ***<br/>I(disp^2)    1.109e-03  2.265e-04   4.897 3.68e-05 ***<br/>I(disp^3)   -1.217e-06  2.776e-07  -4.382  0.00015 ***<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/><br/>Residual standard error: 2.224 on 28 degrees of freedom<br/>Multiple R-squared: 0.8771,         Adjusted R-squared: 0.8639<br/>F-statistic: 66.58 on 3 and 28 DF,  p-value: 7.347e-13</pre>&#13;
<p class="indent">The output shows that a cubic component also offers a statistically significant contribution. However, if you were to continue adding higher-order terms, you’d find that fitting a polynomial of order 4 to these data isn’t able to improve the fit at all, with several coefficients being rendered nonsignificant (the order 4 fit isn’t shown).</p>&#13;
<p class="indent">So, letting <span class="ent">ŷ</span> be miles per gallon and <em>x</em> be displacement in cubic inches, and expanding the e-notation from the previous output, the fitted multiple regression model is</p>&#13;
<p class="center"><span class="ent">ŷ</span> = 50.7 − 0.3372<em>x</em> + 0.0011<em>x</em><sup>2</sup> − 0.000001<em>x</em><sup>3</sup>,</p>&#13;
<p class="noindent">which is precisely what the order 3 line in the left panel of <a href="ch21.xhtml#ch21fig4">Figure 21-4</a> reflects.</p>&#13;
<h5 class="h5" id="ch21lev3sec99"><strong>Plotting the Polynomial Fit</strong></h5>&#13;
<p class="noindent">To address the plot itself, you visualize the data and the first (simple linear) model in <code>car.order1</code> in the usual way. To begin <a href="ch21.xhtml#ch21fig4">Figure 21-4</a>, execute the following code:</p>&#13;
<pre>R&gt; plot(mtcars$disp,mtcars$mpg,xlab="Displacement (cu. in.)",ylab="MPG")<br/>R&gt; abline(car.order1)</pre>&#13;
<div class="image"><span epub:type="pagebreak" id="page_507"/><img src="../images/f21-04.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch21fig4"/>Figure 21-4: Three different models, polynomials of orders 1, 2, and 3, fitted to the “mileage per displacement” relationship from the</em> <code>mtcars</code> <em>data set. Left: Visible plot limits constrained to the data. Right: Visible plot limits widened considerably to illustrate unreliability in extrapolation.</em></p>&#13;
<p class="indent">It’s a little more difficult to add the line corresponding to either of the polynomial-termed models since <code>abline</code> is equipped to handle only straight-line trends. One way to do this is to make use of <code>predict</code> for each value in a sequence that represents the desired values of the explanatory variable. (I favor this approach because it also allows you to simultaneously calculate confidence and prediction bands if you want.) To add the line for the order 2 model only, first create the required sequence over the observed range of <code>disp</code>.</p>&#13;
<pre>R&gt; disp.seq &lt;- seq(min(mtcars$disp)-50,max(mtcars$disp)+50,length=30)</pre>&#13;
<p class="indent">Here, the sequence has been widened a little by minus and plus 50 to predict a small amount on either side of the scope of the original covariate data, so the curve meets the edges of the graph. Then you make the prediction itself and superimpose the fitted line.</p>&#13;
<pre>R&gt; car.order2.pred &lt;- predict(car.order2,newdata=data.frame(disp=disp.seq))<br/>R&gt; lines(disp.seq,car.order2.pred,lty=2)</pre>&#13;
<p class="indent">You use the same technique, followed by the final addition of the legend, for the order 3 polynomial.</p>&#13;
<pre>R&gt; car.order3.pred &lt;- predict(car.order3,newdata=data.frame(disp=disp.seq))<br/>R&gt; lines(disp.seq,car.order3.pred,lty=3)<br/>R&gt; legend("topright",lty=1:3,<br/>          legend=c("order 1 (linear)","order 2 (quadratic)","order 3 (cubic)"))</pre>&#13;
<p class="indent">The result of all this is on the left panel of <a href="ch21.xhtml#ch21fig4">Figure 21-4</a>. Even though you’ve used raw data from only one covariate, <code>disp</code>, the example illustrated here is considered multiple regression because more than one parameter <span epub:type="pagebreak" id="page_508"/>(in addition to the universal intercept <em>β</em><sub>0</sub>) required estimation in the order 2 and 3 models.</p>&#13;
<p class="indent">The different types of trend lines fitted to the mileage and displacement data clearly show different interpretations of the relationship. Visually, you could reasonably argue that the simple linear fit is inadequate at modeling the relationship between response and predictor, but it’s harder to come to a clear conclusion when choosing between the order 2 and order 3 versions. The order 2 fit captures the curve that tapers off as <code>disp</code> increases; the order 3 fit additionally allows for a bump (in technical terms a <em>saddle</em> or <em>inflection</em>), followed by a steeper downward trend in the same domain.</p>&#13;
<p class="indent">So, which model is “best”? In this case, the statistical significance of the parameters suggests that the order 3 model should be preferred. Having said that, there are other things to consider when choosing between different models, which you’ll think about more carefully in <a href="ch22.xhtml#ch22lev1sec73">Section 22.2</a>.</p>&#13;
<h5 class="h5" id="ch21lev3sec100"><strong>Pitfalls of Polynomials</strong></h5>&#13;
<p class="noindent">One particular drawback associated with polynomial terms in linear regression models is the instability of the fitted trend when trying to perform any kind of extrapolation. The right plot in <a href="ch21.xhtml#ch21fig4">Figure 21-4</a> shows the same three fitted models (MPG by displacement), but this time with a much wider scale for displacement. As you can see, the validity of these models is questionable. Though the order 2 and 3 models fit MPG acceptably within the range of the observed data, if you move even slightly outside the maximum threshold of observed displacement values, the predictions of the mean mileage go wildly off course. The order 2 model in particular becomes completely nonsensical, suggesting a rapid improvement in MPG once the engine displacement rises over 500 cubic inches. You must keep this natural mathematical behavior of polynomial functions in mind if you’re considering using higher-order terms in your regression models.</p>&#13;
<p class="indent">To create this plot, the same code that created the left plot can be used; you simply use <code>xlim</code> to widen the <em>x</em>-axis range and define the <code>disp.seq</code> object to a correspondingly wider sequence (in this case, I just set <code>xlim=c(10,1000)</code> with matching <code>from</code> and <code>to</code> limits in the creation of <code>disp.seq</code>).</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>Models like this are still referred to as</em> linear <em>regression models, which might seem a bit confusing since the fitted trends for higher-order polynomials are clearly nonlinear. This is because</em> linear regression <em>refers to the fact that the function defining the mean response is linear in terms of the regression parameters</em> <em>β</em><sub>0</sub>, <em>β</em><sub>1</sub>, . . ., <sub><em>β</em>p</sub><em>. As such, any transformation applied to individual variables doesn’t affect the linearity of the function with respect to the coefficients themselves.</em></p>&#13;
</div>&#13;
<h4 class="h4" id="ch21lev2sec200"><strong><em>21.4.2 Logarithmic</em></strong></h4>&#13;
<p class="noindent">In statistical modeling situations where you have positive numeric observations, it’s common to perform a log transformation of the data to dramatically reduce the overall range of the data and bring extreme <span epub:type="pagebreak" id="page_509"/>observations closer to a measure of centrality. In that sense, transforming to a logarithmic scale can help reduce the severity of heavily skewed data (see <a href="ch15.xhtml#ch15lev2sec136">Section 15.2.4</a>). In the context of regression modeling, log transformations can be used to capture trends where apparent curves “flatten off,” without the same kind of instability outside the range of the observed data that you saw with some of the polynomials.</p>&#13;
<p class="indent">If you need to refresh your memory on logarithms, turn back to <a href="ch02.xhtml#ch02lev2sec18">Section 2.1.2</a>; it suffices here to note that the logarithm is the power to which you must raise a base value in order to obtain an <em>x</em> value. For example, in 3<sup>5</sup> = 243, the logarithm is 5 and 3 is the base, expressed as log<sub>3</sub> 243 = 5. Because of the ubiquity of the exponential function in common probability distributions, statisticians almost exclusively work with the natural log (logarithm to the base <em>e</em>). From here, assume all mentions of the log transformation refer to the natural log.</p>&#13;
<p class="indent">To briefly illustrate the typical behavior of the log transformation, take a look at <a href="ch21.xhtml#ch21fig5">Figure 21-5</a>, achieved with the following:</p>&#13;
<pre>R&gt; plot(1:1000,log(1:1000),type="l",xlab="x",ylab="",ylim=c(-8,8))<br/>R&gt; lines(1:1000,-log(1:1000),lty=2)<br/>R&gt; legend("topleft",legend=c("log(x)","-log(x)"),lty=c(1,2))</pre>&#13;
<p class="indent">This plots the log of the integers 1 to 1000 against the raw values, as well as plotting the negative log. You can see the way in which the log-transformed values taper off and flatten out as the raw values increase.</p>&#13;
<div class="image"><img src="../images/f21-05.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch21fig5"/>Figure 21-5: The log function applied to integers 1 to 1000</em></p>&#13;
<h5 class="h5" id="ch21lev3sec101"><strong>Fitting the Log Transformation</strong></h5>&#13;
<p class="noindent">As noted, one use of the log transformation in regression is to allow this kind of curvature in situations when a perfectly straight line doesn’t suit the observed relationship. For an illustration, return to the <code>mtcars</code> examples and consider mileage as a function of both horsepower and transmission <span epub:type="pagebreak" id="page_510"/>type (variables <code>hp</code> and <code>am</code>, respectively). Create a scatterplot of MPG against horsepower, with different colors distinguishing between automatic and manual cars.</p>&#13;
<pre>R&gt; plot(mtcars$hp,mtcars$mpg,pch=19,col=c("black","gray")[factor(mtcars$am)],<br/>        xlab="Horsepower",ylab="MPG")<br/>R&gt; legend("topright",legend=c("auto","man"),col=c("black","gray"),pch=19)</pre>&#13;
<p class="indent">The plotted points shown in <a href="ch21.xhtml#ch21fig6">Figure 21-6</a> suggest that curved trends in horsepower may be more appropriate than straight-line relationships. Note that you have to explicitly coerce the binary numeric <code>mtcars$am</code> vector to a factor here in order to use it as a selector for the vector of two colors. You’ll add the lines in after fitting the linear model.</p>&#13;
<div class="image"><img src="../images/f21-06.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch21fig6"/>Figure 21-6: Scatterplot of MPG on horsepower, split by transmission type, with lines corresponding to a multiple linear regression using a log-scaled effect of horsepower superimposed</em></p>&#13;
<p class="indent">Let’s do so using the log transformation of horsepower to try to capture the curved relationship. Since, in this example, you also want to account for the potential of transmission type to affect the response, this is included as an additional predictor variable as usual.</p>&#13;
<pre>R&gt; car.log &lt;- lm(mpg~log(hp)+am,data=mtcars)<br/>R&gt; summary(car.log)<br/><br/>Call:<br/>lm(formula = mpg ~ log(hp) + am, data = mtcars)<br/><br/>Residuals:<br/>    Min      1Q  Median      3Q     Max<br/>-3.9084 -1.7692 -0.1432  1.4032  6.3865<br/><br/>Coefficients:<br/>            Estimate Std. Error t value Pr(&gt;|t|)<br/>(Intercept)  63.4842     5.2697  12.047 8.24e-13 ***<br/>log(hp)      -9.2383     1.0439  -8.850 9.78e-10 ***<br/>am            4.2025     0.9942   4.227 0.000215 ***<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/><br/>Residual standard error: 2.592 on 29 degrees of freedom<br/>Multiple R-squared:  0.827,        Adjusted R-squared:  0.8151<br/>F-statistic: 69.31 on 2 and 29 DF,  p-value: 8.949e-12</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_511"/>The output indicates jointly statistically significant effects of both log-horsepower and transmission type on mileage. Keeping transmission constant, the mean MPG drops by around 9.24 for each additional unit of log-horsepower. Having a manual transmission increases the mean MPG by roughly 4.2 (estimated in this order owing to the coding of <code>am</code>—<code>0</code> for automatic, <code>1</code> for manual; see <code>?mtcars</code>). The coefficient of determination shows 82.7 percent of the variation in the response is explained by this regression, suggesting a satisfactory fit.</p>&#13;
<h5 class="h5" id="ch21lev3sec102"><strong>Plotting the Log Transformation Fit</strong></h5>&#13;
<p class="noindent">To visualize the fitted model, you first need to calculate the fitted values for all desired predictor values. The following code creates a sequence of horsepower values (minus and plus 20 horsepower) and performs the required prediction for both transmission types.</p>&#13;
<pre>R&gt; hp.seq &lt;- seq(min(mtcars$hp)-20,max(mtcars$hp)+20,length=30)<br/>R&gt; n &lt;- length(hp.seq)<br/>R&gt; car.log.pred &lt;- predict(car.log,newdata=data.frame(hp=rep(hp.seq,2),<br/>                                                      am=rep(c(0,1),each=n)))</pre>&#13;
<p class="indent">In the above code, since you want to plot predictions for both possible values of <code>am</code>, when using <code>newdata</code> you need to replicate <code>hp.seq</code> twice. Then, when you provide values for <code>am</code> to <code>newdata</code>, one series of <code>hp.seq</code> is paired with an appropriately replicated <code>am</code> value of <code>0</code>, the other with <code>1</code>. The result of this is a vector of predictions of length twice that of <code>hp.seq</code>, <code>car.log.pred</code>, with the first <code>n</code> elements corresponding to automatic cars and the latter <code>n</code> to manuals.</p>&#13;
<p class="indent">Now you can add these lines to <a href="ch21.xhtml#ch21fig6">Figure 21-6</a> with the following:</p>&#13;
<pre>R&gt; lines(hp.seq,car.log.pred[1:n])<br/>R&gt; lines(hp.seq,car.log.pred[(n+1):(2*n)],col="gray")</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_512"/>By examining the scatterplot, you can see that the fitted model appears to do a good job of estimating the joint relationship between horsepower/transmission and MPG. The statistical significance of transmission type in this model directly affects the difference between the two added lines. If <code>am</code> weren’t significant, the lines would be closer together; in that case, the model would be suggesting that one curve would be sufficient to capture the relationship. As usual, extrapolation too far outside the range of the observed predictor data isn’t a great idea, though it’s less unstable for log-transformed trends than for polynomial functions.</p>&#13;
<h4 class="h4" id="ch21lev2sec201"><strong><em>21.4.3 Other Transformations</em></strong></h4>&#13;
<p class="noindent">Transformation can involve more than one variable of the data set and isn’t limited to just predictor variables either. In their original investigation into the <code>mtcars</code> data, Henderson and Velleman (<a href="ref.xhtml#ref32">1981</a>) also noted the presence of the same curved relationships you’ve uncovered between the response and variables such as horsepower and displacement. They argued that it’s preferable to use gallons per mile (GPM) instead of MPG as the response variable to improve linearity. This would involve modeling a transformation of MPG, namely, that GPM = 1/MPG.</p>&#13;
<p class="indent">The authors also commented on the limited influence that both horsepower and displacement have on GPM if the weight of the car is included in a fitted model, because of the relatively high correlations present among these three predictors (known as <em>multicollinearity</em>). To address this, the authors created a new predictor variable calculated as horsepower divided by weight. This measures, in their words, how “overpowered” a car is—and they proceeded to use that new predictor instead of horsepower or displacement alone. This is just some of the experimentation that took place in the search for an appropriate way to model these data.</p>&#13;
<p class="indent">To this end, however you choose to model your own data, the objective of transforming numeric variables should always be to fit a valid model that represents the data and the relationships more realistically and accurately. When reaching for this goal, there’s plenty of freedom in how you can transform numeric observations in applications of regression methods. For a further discussion on transformations in linear regression, <a href="ch07.xhtml#ch07">Chapter 7</a> of Faraway (<a href="ref.xhtml#ref21">2005</a>) provides an informative introduction.</p>&#13;
<div class="ex">&#13;
<p class="ext"><a id="ch21exc2"/><strong>Exercise 21.2</strong></p>&#13;
<p class="noindentz">The following table presents data collected in one of Galileo’s famous “ball” experiments, in which he rolled a ball down a ramp of different heights and measured how far it traveled from the base of the ramp. For more on this and other interesting examples, look at “Teaching Statistics with Data of Historic Significance” by Dickey and Arnold (<a href="ref.xhtml#ref17">1995</a>).</p>&#13;
<table class="topbot2">&#13;
<thead>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table2r"><p class="tablec"><span epub:type="pagebreak" id="page_513"/><strong>Initial height</strong></p></td>&#13;
<td style="vertical-align: top;" class="table2r"><p class="tablec"><strong>Distance</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">1000</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">573</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">800</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">534</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">600</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">495</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">450</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">451</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">300</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">395</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">200</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">337</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">100</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="tablec">253</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<ol type="a">&#13;
<li><p class="noindents">Create a data frame in R based on this table and plot the data points with distance on the <em>y</em>-axis.</p></li>&#13;
<li><p class="noindents">Galileo believed there was a quadratic relationship between initial height and the distance traveled.</p>&#13;
<ol type="i">&#13;
<li><p class="noindents">Fit an order 2 polynomial in height, with distance as the response.</p></li>&#13;
<li><p class="noindents">Fit a cubic (order 3) and a quartic (order 4) model for these data. What do they tell you about the nature of the relationship?</p></li>&#13;
</ol></li>&#13;
<li><p class="noindents">Based on your models from (b), choose the one that you think best represents the data and plot the fitted line on the raw data. Add 90 percent confidence bands for mean distance traveled to the plot.</p></li>&#13;
</ol>&#13;
<p class="noindentz">The contributed R package <code>faraway</code> contains a large number of data sets that accompany a linear regression textbook by Faraway (<a href="ref.xhtml#ref21">2005</a>). Install the package and then call <code>library("faraway")</code> to load it. One of the data sets is <code>trees</code>, which provides data on the dimensions of felled trees of a certain type (see, for example, <a href="ref.xhtml#ref04">Atkinson, 1985</a>).</p>&#13;
<ol type="a" start="4">&#13;
<li><p class="noindents">Access the data object at the prompt and plot volume against girth (the latter along the <em>x</em>-axis).</p></li>&#13;
<li><p class="noindents">Fit two models with <code>Volume</code> as the response: one quadratic model in <code>Girth</code> and the other based on log transformations of both <code>Volume</code> and <code>Girth</code>. Write down the model equations for each and comment on the similarity (or difference) of the fits in terms of the coefficient of determination and the omnibus <em>F</em>-test.</p></li>&#13;
<li><p class="noindents">Use <code>predict</code> to add lines to the plot from (d) for each of the two models from (e). Use different line types; add a corresponding legend. Also include 95 percent prediction intervals, with line types matching those of the fitted values (note that for the model that involves log transformation of the response and the predictor, any returned values from <code>predict</code> will themselves be on the log scale; you have to back-transform these to the original scale <span epub:type="pagebreak" id="page_514"/>using <code>exp</code> before the lines for that model can be superimposed). Comment on the respective fits and their estimated prediction intervals.</p></li>&#13;
</ol>&#13;
<p class="noindentz">Lastly, turn your attention back to the <code>mtcars</code> data frame.</p>&#13;
<ol type="a" start="7">&#13;
<li><p class="noindents">Fit and summarize a multiple linear regression model to determine mean MPG from horsepower, weight, and displacement.</p></li>&#13;
<li><p class="noindents">In the spirit of Henderson and Velleman (<a href="ref.xhtml#ref32">1981</a>), use <code>I</code> to refit the model in (g) in terms of GPM = 1/MPG. Which model explains a greater amount of variation in the response?</p></li>&#13;
</ol>&#13;
</div>&#13;
<h3 class="h3" id="ch21lev1sec71"><strong>21.5 Interactive Terms</strong></h3>&#13;
<p class="noindent">So far, you’ve looked only at the joint main effects of how predictors affect the outcome variable (and one-to-one transformations thereof). Now you’ll look at interactions between covariates. An <em>interactive effect</em> between predictors is an additional change to the response that occurs at particular combinations of the predictors. In other words, an interactive effect is present if, for a given covariate profile, the values of the predictors are such that they produce an effect that augments the stand-alone main effects associated with those predictors.</p>&#13;
<h4 class="h4" id="ch21lev2sec202"><strong><em>21.5.1 Concept and Motivation</em></strong></h4>&#13;
<p class="noindent">Diagrams such as those found in <a href="ch21.xhtml#ch21fig7">Figure 21-7</a> are often used to help explain the concept of interactive effects. These diagrams show your mean response value, <span class="ent">ŷ</span>, on the vertical axis, as usual, and a predictor value for the variable <em>x<sub>1</sub></em> on the horizontal axis. They also show a binary categorical variable <em>x</em><sub>2</sub>, which can be either zero or one. These hypothetical variables are labeled as such in the images.</p>&#13;
<div class="image"><img src="../images/f21-07.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch21fig7"/>Figure 21-7: Concept of an interactive effect between two predictors x</em><sub>1</sub> <em>and x</em><sub>2</sub><em>, on the mean response value</em> <span class="ent">ŷ</span><em>. Left: Only main effects of x</em><sub>1</sub> <em>and x<sub>2</sub></em> <em>influence</em> <span class="ent">ŷ</span><em>. Right: An interaction between x</em><sub>1</sub> <em>and x<sub>2</sub></em> <em>is needed in addition to their main effects in order to model</em> <span class="ent">ŷ</span>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_515"/>The left diagram shows the limit of the models you’ve considered so far in this chapter—that both <em>x<sub>1</sub></em> and <em>x<sub>2</sub></em> affect <span class="ent">ŷ</span> independently of each other. The right diagram, however, clearly shows that the effect of <em>x<sub>1</sub></em> on <span class="ent">ŷ</span> changes completely depending on the value of <em>x</em><sub>2</sub>. On the left, only main effects of <em>x<sub>1</sub></em> and <em>x<sub>2</sub></em> are needed to determine <span class="ent">ŷ</span>; on the right, main effects <em>and</em> an interactive effect between <em>x<sub>1</sub></em> and <em>x<sub>2</sub></em> are present.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>When estimating regression models, you always have to accompany interactions with the main effects of the relevant predictors, for reasons of interpretability. Since interactions are themselves best understood as an augmentation of the main effects, it makes no sense to remove the latter and leave in the former.</em></p>&#13;
</div>&#13;
<p class="indent">For a good example of an interaction, think about pharmacology. Interactive effects between medicines are relatively common, which is why health care professionals often ask about other medicines you might be taking. Consider statins—drugs commonly used to reduce cholesterol. Users of statins are told to avoid grapefruit juice because it contains natural chemical compounds that inhibit the efficacy of the enzyme responsible for the correct metabolization of the drug. If an individual is taking statins and not consuming grapefruit, you would expect a negative relationship between cholesterol level and statin use (think about “statin use” either as a continuous or as a categorical dosage variable)—as statin use increases or is affirmative, the cholesterol level decreases. On the other hand, for an individual on statins who <em>is</em> consuming grapefruit, the nature of the relationship between cholesterol level and statin use could easily be different—weakened negative, neutral, or even positive. If so, since the effect of the statins on cholesterol changes according to the value of another variable—whether or not grapefruit is consumed—this would be considered an interaction between those two predictors.</p>&#13;
<p class="indent">Interactions can occur between categorical variables, numeric variables, or both. It’s most common to find <em>two-way</em> interactions—interactions between exactly two predictors—which is what you’ll focus on in <a href="ch21.xhtml#ch21lev2sec203">Sections 21.5.2</a> to <a href="ch21.xhtml#ch21lev2sec205">21.5.4</a>. Three-way and higher-order interactive effects are technically possible but less common, partly because they are difficult to interpret in a real-world context. You’ll consider an example of these in <a href="ch21.xhtml#ch21lev2sec206">Section 21.5.5</a>.</p>&#13;
<h4 class="h4" id="ch21lev2sec203"><strong><em>21.5.2 One Categorical, One Continuous</em></strong></h4>&#13;
<p class="noindent">Generally, a two-way interaction between a categorical and a continuous predictor should be understood as effecting a change in the slope of the continuous predictor with respect to the nonreference levels of the categorical predictor. In the presence of a term for the continuous variable, a categorical variable with <em>k</em> levels will have <em>k</em> − 1 main effect terms, so there will be a further <em>k</em> − 1 interactive terms between all the alternative levels of the categorical variable and the continuous variable.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_516"/>The different slopes for <em>x<sub>1</sub></em> by category of <em>x<sub>2</sub></em> for <span class="ent">ŷ</span> can be seen clearly on the right of <a href="ch21.xhtml#ch21fig7">Figure 21-7</a>. In such a situation, in addition to the main effects for <em>x<sub>1</sub></em> and <em>x</em><sub>2</sub>, there would be one interactive term in the fitted model corresponding to <em>x<sub>2</sub></em> = 1. This defines the additive term needed to change the slope in <em>x<sub>1</sub></em> for <em>x<sub>2</sub></em> = 0 to the new slope in <em>x<sub>1</sub></em> for <em>x<sub>2</sub></em> = 1.</p>&#13;
<p class="indent">For an example, let’s access a new data set. In <a href="ch21.xhtml#ch21exc2">Exercise 21.2</a>, you looked at the <code>faraway</code> package (<a href="ref.xhtml#ref21">Faraway, 2005</a>) to access the <code>trees</code> data. In this package, you’ll also find the <code>diabetes</code> object—a cardiovascular disease data set detailing characteristics of 403 African Americans (originally investigated and reported in <a href="ref.xhtml#ref59">Schorling et al., 1997</a>; <a href="ref.xhtml#ref76">Willems et al., 1997</a>). Install <code>faraway</code> if you haven’t already and load it with <code>library("faraway")</code>. Restrict your attention to the total cholesterol level (<code>chol</code>—continuous), age of the individual (<code>age</code>—continuous), and body frame type (<code>frame</code>—categorical with <em>k</em> = 3 levels: <code>"small"</code> as the reference level, <code>"medium"</code>, and <code>"large"</code>). You can see the data in <a href="ch21.xhtml#ch21fig8">Figure 21-8</a>, which will be created momentarily.</p>&#13;
<p class="indent">You’ll look at modeling total cholesterol by age and body frame. It seems logical to expect that cholesterol is related to both age and body type, so it makes sense to also consider the possibility that the effect of age on cholesterol is different for individuals of different body frames. To investigate, let’s fit the multiple linear regression and include a two-way interaction between the two variables. In the call to <code>lm</code>, you specify the main effects first, using <code>+</code> as usual, and then specify an interactive effect of two predictors by using a colon (<code>:</code>) between them.</p>&#13;
<pre>R&gt; dia.fit &lt;- lm(chol~age+frame+age:frame,data=diabetes)<br/>R&gt; summary(dia.fit)<br/><br/>Call:<br/>lm(formula = chol ~ age + frame + age:frame, data = diabetes)<br/><br/>Residuals:<br/>    Min      1Q  Median      3Q     Max<br/>-131.90  -26.24   -5.33   22.17  226.11<br/><br/>Coefficients:<br/>                Estimate Std. Error t value Pr(&gt;|t|)<br/>(Intercept)     155.9636    12.0697  12.922  &lt; 2e-16 ***<br/>age               0.9852     0.2687   3.667  0.00028 ***<br/>framemedium      28.6051    15.5503   1.840  0.06661 .<br/>framelarge       44.9474    18.9842   2.368  0.01840 *<br/>age:framemedium  -0.3514     0.3370  -1.043  0.29768<br/>age:framelarge   -0.8511     0.3779  -2.252  0.02490 *<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/><br/>Residual standard error: 42.34 on 384 degrees of freedom<br/>  (13 observations deleted due to missingness)<br/>Multiple R-squared:  0.07891,       Adjusted R-squared: 0.06692<br/>F-statistic:  6.58 on 5 and 384 DF, p-value: 6.849e-06</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_517"/>Inspecting the estimated model parameters in the output, you can see a main effect coefficient for <code>age</code>, main effect coefficients for the two levels of <code>frame</code> (that aren’t the reference level), and two further terms for the interactive effect of <code>age</code> with those same nonreference levels.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>There’s actually a shortcut to doing this in R—the</em> cross-factor <em>notation. The same model shown previously could have been fitted by using</em> <code><span class="codeitalic">chol~age*frame</code></span> <em>in</em> <code><span class="codeitalic">lm</code></span><em>; the symbol</em> <code>*</code> <em>between two variables in a formula should be interpreted as “include an intercept, all main effects, and the interaction.” I’ll use this shortcut from now on.</em></p>&#13;
</div>&#13;
<p class="indent">The output shows the significance of <code>age</code> and some evidence to support the presence of a main effect of <code>frame</code>. There’s also slight indication of significance of the interaction, though it’s weak. Assessing significance in this case, where one predictor is categorical with <em>k</em> &gt; 2 levels, follows the same rule as noted in the discussion of multilevel variables in <a href="ch20.xhtml#ch20lev2sec186">Section 20.5.2</a>—if at least one of the coefficients is significant, the entire effect should be deemed significant.</p>&#13;
<p class="indent">The general equation for the fitted model can be written down directly from the output.</p>&#13;
<div class="imagec"><a id="ch21eq7"/><img src="../images/e21-7.jpg" alt="image"/></div>&#13;
<p class="indent">I’ve used a colon (:) to denote the interactive terms to mirror the R output.</p>&#13;
<p class="indent">For the reference level of the categorical predictor, body type “small,” the fitted model can be written down straight from the output.</p>&#13;
<p class="center">“Mean total cholesterol” = 155.9636 + 0.9852 × “age”</p>&#13;
<p class="indent">For a model with the main effects only, changing body type to “medium” or “large” would affect only the intercept—you know from <a href="ch20.xhtml#ch20lev1sec66">Section 20.5</a> that the relevant effect is simply added to the outcome. The presence of the interaction, however, means that <em>in addition</em> to the change in the intercept, the main effect slope of <code>age</code> must now also be changed according to the relevant interactive term. For an individual with a “medium” frame, the model is</p>&#13;
<table>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">“Mean total cholesterol”</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">=</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">155.9636 + 0.9852 × “age” + 28.6051 − 0.3514 × “age”</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td style="vertical-align: top;" class="table"><p class="table">=</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">184.5687 + (0.9852 − 0.3514) × “age”</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td style="vertical-align: top;" class="table"><p class="table">=</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">184.5687 + 0.6338 × “age”</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_518"/>and for an individual with a “large” frame, the model is</p>&#13;
<table>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">“Mean total cholesterol”</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">=</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">155.9636 + 0.9852 × “age” + 44.9474 − 0.8511 × “age”</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td style="vertical-align: top;" class="table"><p class="table">=</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">200.911 + (0.9852 − 0.8511) × “age”</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td style="vertical-align: top;" class="table"><p class="table">=</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">200.911 + 0.1341 × “age”</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">You can easily calculate these in R by accessing the coefficients of the fitted model object:</p>&#13;
<pre>R&gt; dia.coef &lt;- coef(dia.fit)<br/>R&gt; dia.coef<br/>    (Intercept)             age     framemedium     framelarge<br/>    155.9635868       0.9852028      28.6051035     44.9474105<br/>age:framemedium  age:framelarge<br/>     -0.3513906      -0.8510549</pre>&#13;
<p class="indent">Next, let’s sum the relevant components of this vector. Once you have the sums, you’ll be able to plot the fitted model.</p>&#13;
<pre>R&gt; dia.small &lt;- c(dia.coef[1],dia.coef[2])<br/>R&gt; dia.small<br/>(Intercept)         age<br/>155.9635868   0.9852028<br/>R&gt; dia.medium &lt;- c(dia.coef[1]+dia.coef[3],dia.coef[2]+dia.coef[5])<br/>R&gt; dia.medium<br/>(Intercept)         age<br/>184.5686904   0.6338122<br/>R&gt; dia.large &lt;- c(dia.coef[1]+dia.coef[4],dia.coef[2]+dia.coef[6])<br/>R&gt; dia.large<br/>(Intercept)         age<br/>200.9109973   0.1341479</pre>&#13;
<p class="indent">The three lines are stored as numeric vectors of length 2, with the intercept first and the slope second. This is the form required by the optional <code>coef</code> argument of <code>abline</code>, which allows you to superimpose these straight lines on a plot of the raw data. The following code produces <a href="ch21.xhtml#ch21fig8">Figure 21-8</a>.</p>&#13;
<pre>R&gt; cols &lt;- c("black","darkgray","lightgray")<br/>R&gt; plot(diabetes$chol~diabetes$age,col=cols[diabetes$frame],<br/>        cex=0.5,xlab="age",ylab="cholesterol")<br/>R&gt; abline(coef=dia.small,lwd=2)<br/>R&gt; abline(coef=dia.medium,lwd=2,col="darkgray")<br/>R&gt; abline(coef=dia.large,lwd=2,col="lightgray")<br/>R&gt; legend("topright",legend=c("small frame","medium frame","large frame"),<br/>          lty=1,lwd=2,col=cols)</pre>&#13;
<div class="image"><span epub:type="pagebreak" id="page_519"/><img src="../images/f21-08.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch21fig8"/>Figure 21-8: Fitted linear model, main effects, and interaction for mean total cholesterol by age and body frame</em></p>&#13;
<p class="indent">If you examine the fitted model in <a href="ch21.xhtml#ch21fig8">Figure 21-8</a>, it’s clear that inclusion of an interaction between age and body frame has allowed more flexibility in the way mean total cholesterol relates to the two predictors. The nonparallel nature of the three plotted lines reflects the concept illustrated in <a href="ch21.xhtml#ch21fig7">Figure 21-7</a>.</p>&#13;
<p class="indent">I walked through this to illustrate how the concept works, but in practice you don’t need to go through all of these steps to find the point estimates (and any associated confidence intervals). You can predict from a fitted linear model with interactions in the same way as for main-effect-only models through the use of <code>predict</code>.</p>&#13;
<h4 class="h4" id="ch21lev2sec204"><strong><em>21.5.3 Two Categorical</em></strong></h4>&#13;
<p class="noindent">You met the concept of interactions between two categorical explanatory variables in the introduction to two-way ANOVA in <a href="ch19.xhtml#ch19lev1sec60">Section 19.2</a>. There, you uncovered evidence of an interactive effect of wool type and tension on the mean number of warp breaks in lengths of yarn (based on the ready-to-use <code>warpbreaks</code> data frame). You then visualized the interaction with an interaction plot (<a href="ch19.xhtml#ch19fig2">Figure 19-2</a> on <a href="ch19.xhtml#page_447">page 447</a>), not unlike the diagrams in <a href="ch21.xhtml#ch21fig7">Figure 21-7</a>.</p>&#13;
<p class="indent">Let’s implement the same model as the last <code>warpbreaks</code> example in <a href="ch19.xhtml#ch19lev2sec172">Section 19.2.2</a> in an explicit linear regression format.</p>&#13;
<pre>R&gt; warp.fit &lt;- lm(breaks~wool*tension,data=warpbreaks)<br/>R&gt; summary(warp.fit)<br/><br/>Call:<br/>lm(formula = breaks ~ wool * tension, data = warpbreaks)<br/><br/>Residuals:<br/>     Min       1Q   Median        3Q       Max<br/>-19.5556  -6.8889  -0.6667    7.1944   25.4444<br/><br/>Coefficients:<br/>               Estimate Std. Error t value Pr(&gt;|t|)<br/>(Intercept)      44.556      3.647  12.218 2.43e-16 ***<br/>woolB           -16.333      5.157  -3.167 0.002677 **<br/>tensionM        -20.556      5.157  -3.986 0.000228 ***<br/>tensionH        -20.000      5.157  -3.878 0.000320 ***<br/>woolB:tensionM   21.111      7.294   2.895 0.005698 **<br/>woolB:tensionH   10.556      7.294   1.447 0.154327<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/><br/>Residual standard error: 10.94 on 48 degrees of freedom<br/>Multiple R-squared:  0.3778,        Adjusted R-squared:  0.3129<br/>F-statistic: 5.828 on 5 and 48 DF,  p-value: 0.0002772</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_520"/>Here I’ve used the cross-factor symbol <code>*</code>, rather than <code>wool + tension + wool:tension</code>. When both predictors in a two-way interaction are categorical, there will be a term for each nonreference level of the first predictor combined with all nonreference levels of the second predictor. In this example, <code>wool</code> is binary with only <em>k</em> = 2 levels and <code>tension</code> has <em>k</em> = 3; therefore, the only interaction terms present are the “medium” (<code>M</code>) and “high” (<code>H</code>) tension levels (“low”, <code>L</code>, is the reference level) with wool type <code>B</code> (<code>A</code> is the reference level). Therefore, altogether in the fitted model, there are terms for <code>B</code>, <code>M</code>, <code>H</code>, <code>B:M</code>, and <code>B:H</code>.</p>&#13;
<p class="indent">These results provide the same conclusion as the ANOVA analysis—there is indeed statistical evidence of an interactive effect between wool type and tension on mean breaks, on top of the contributing main effects of those predictors.</p>&#13;
<p class="indent">The general fitted model can be understood as</p>&#13;
<table>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">“Mean warp breaks”</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">=</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">44.556 − 16.333 × “wool type B”</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td/>&#13;
<td style="vertical-align: top;" class="table"><p class="table">− 20.556 × “medium tension”</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td/>&#13;
<td style="vertical-align: top;" class="table"><p class="table">− 20.000 × “high tension”</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td/>&#13;
<td style="vertical-align: top;" class="table"><p class="table">+ 21.111 × “wool type B : medium tension”</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td/>&#13;
<td style="vertical-align: top;" class="table"><p class="table">+ 10.556 × “wool type B : high tension”</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">The additional interaction terms work the same way as the main effects—when only categorical predictors are involved, the model can be seen as a series of additive terms to the overall intercept. Exactly which ones you use in any given prediction depends on the covariate profile of a given individual.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_521"/>Let’s have a quick series of examples: for wool A at low tension, the mean number of warp breaks is predicted as simply the overall intercept; for wool A at high tension, you have the overall intercept and the main effect term for high tension; for wool B at low tension, you have the overall intercept and the main effect for wool type B only; and for wool B at medium tension, you have the overall intercept, the main effect for wool type B, the main effect for medium tension, <em>and</em> the interactive term for wool B with medium tension.</p>&#13;
<p class="indent">You can use <code>predict</code> to estimate the mean warp breaks for these four scenarios; they’re accompanied here with 90 percent confidence intervals:</p>&#13;
<pre>R&gt; nd &lt;- data.frame(wool=c("A","A","B","B"),tension=c("L","H","L","M"))<br/>R&gt; predict(warp.fit,newdata=nd,interval="confidence",level=0.9)<br/>       fit      lwr      upr<br/>1 44.55556 38.43912 50.67199<br/>2 24.55556 18.43912 30.67199<br/>3 28.22222 22.10579 34.33866<br/>4 28.77778 22.66134 34.89421</pre>&#13;
<h4 class="h4" id="ch21lev2sec205"><strong><em>21.5.4 Two Continuous</em></strong></h4>&#13;
<p class="noindent">Finally, you’ll look at the situation when the two predictors are continuous. In this case, an interaction term operates as a modifier on the continuous plane that’s fitted using the main effects only. In a similar way to an interaction between a continuous and a categorical predictor, an interaction between two continuous explanatory variables allows the slope associated with one variable to be affected, but this time, that modification is made in a continuous way (that is, according to the value of the other continuous variable).</p>&#13;
<p class="indent">Returning to the <code>mtcars</code> data frame, consider MPG once more as a function of horsepower and weight. The fitted model, shown next, includes the interaction in addition to the main effects of the two continuous predictors. As you can see, there is a single estimated interactive term, and it is deemed significantly different from zero.</p>&#13;
<pre>R&gt; car.fit &lt;- lm(mpg~hp*wt,data=mtcars)<br/>R&gt; summary(car.fit)<br/><br/>Call:<br/>lm(formula = mpg ~ hp * wt, data = mtcars)<br/><br/>Residuals:<br/>    Min      1Q  Median      3Q     Max<br/>-3.0632 -1.6491 -0.7362  1.4211  4.5513<br/><br/>Coefficients:<br/>            Estimate Std. Error t value Pr(&gt;|t|)<br/>(Intercept) 49.80842    3.60516  13.816 5.01e-14 ***<br/>hp          -0.12010    0.02470  -4.863 4.04e-05 ***<br/>wt          -8.21662    1.26971  -6.471 5.20e-07 ***<br/>hp:wt        0.02785    0.00742   3.753 0.000811 ***<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/><br/>Residual standard error: 2.153 on 28 degrees of freedom<br/>Multiple R-squared:  0.8848,        Adjusted R-squared:  0.8724<br/>F-statistic: 71.66 on 3 and 28 DF,  p-value: 2.981e-13</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_522"/>The model is written as</p>&#13;
<table>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="tabler">“Mean MPG”</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">=</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">49.80842 − 0.12010 × “horsepower”</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td/>&#13;
<td style="vertical-align: top;" class="table"><p class="table">− 8.21662 × “weight”</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td/>&#13;
<td style="vertical-align: top;" class="table"><p class="table">+ 0.02785 × “horsepower : weight”</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td style="vertical-align: top;" class="table"><p class="table">=</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">49.80842 − 0.12010 × “horsepower”</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td/>&#13;
<td style="vertical-align: top;" class="table"><p class="table">− 8.21662 × “weight”</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td/>&#13;
<td style="vertical-align: top;" class="table"><p class="table">+ 0.02785 × “horsepower” × “weight”</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">The second version of the model equation provided here reveals for the first time an interaction expressed as the <em>product</em> of the values of the two predictors, which is exactly how the fitted model is used to predict the response. (Technically, this is the same as when at least one of the predictors is categorical—but the dummy coding simply results in zeros and ones for the respective terms, so multiplication just amounts to the presence or absence of a given term, as you’ve seen.)</p>&#13;
<p class="indent">You can interpret an interaction between two continuous predictors by considering the sign (+ or −) of the coefficient. Negativity suggests that as the values of the predictors increase, the response is reduced after computing the result of the main effects. Positivity, as is the case here, suggests that as the values of the predictors increase, the effect is an additional increase, an amplification, on the mean response.</p>&#13;
<p class="indent">Contextually, the negative main effects of <code>hp</code> and <code>wt</code> indicate that mileage is naturally reduced for heavier, more powerful cars. However, positivity of the interactive effect suggests that this impact on the response is “softened” as horsepower or weight is increased. To put it another way, the negative relationship imparted by the main effects is rendered “less extreme” as the values of the predictors get bigger and bigger.</p>&#13;
<p class="indent"><a href="ch21.xhtml#ch21fig9">Figure 21-9</a> contrasts the main-effects-only version of the model (obtained using <code>lm</code> with the formula <code>mpg~hp+wt</code>; not explicitly fitted in this section) with the interaction version of the model fitted just above as the object <code>car.fit</code>.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_523"/><img src="../images/f21-09.jpg" alt="image"/></div>&#13;
<p class="figt"><em><a id="ch21fig9"/>Figure 21-9: Response surfaces for mean MPG by horsepower and weight, for a main-effects-only model (left), and one that includes the two-way interaction between the continuous predictors (right)</em></p>&#13;
<p class="indent">The plotted <em>response surfaces</em> show the mean MPG on the vertical <em>z</em>-axis and the two predictor variables on the horizontal axes as marked. You can interpret the predicted mean MPG, based on a given horsepower and weight value, as a point on the surface. Note that both surfaces decrease in MPG (vertically along the <em>z</em>-axis) as you move to larger values of either predictor along the respective horizontal axes.</p>&#13;
<p class="indent">I’ll show how these plots are created in <a href="ch25.xhtml#ch25">Chapter 25</a>. For now, they serve simply to highlight the aforementioned “softening” impact of the interaction in <code>car.fit</code>. On the left, the main-effects-only model shows a flat plane decreasing according to the negative linear slopes in each predictor. On the right, however, the presence of the positive interactive term flattens this plane out, meaning the rate of decrease is slowed as the values of the predictor variables increase.</p>&#13;
<h4 class="h4" id="ch21lev2sec206"><strong><em>21.5.5 Higher-Order Interactions</em></strong></h4>&#13;
<p class="noindent">As mentioned, two-way interactions are the most common kind of interactions you’ll encounter in applications of regression methods. This is because for three-way or higher-order terms, you need a lot more data for a reliable estimation of interactive effects, and there are a number of interpretative complexities to overcome. Three-way interactions are far rarer than two-way effects, and four-way and above are rarer still.</p>&#13;
<p class="indent">In <a href="ch21.xhtml#ch21exc1">Exercise 21.1</a>, you used the <code>nuclear</code> data set found in the <code>boot</code> package (provided with the standard R installation), which includes data on the constructions of nuclear power plants in the United States. In the exercises, you focused mainly on date and time predictors related to construction permits to model the mean cost of construction for the nuclear power <span epub:type="pagebreak" id="page_524"/>plants. For the sake of this example, assume you don’t have the data on these predictors. Can the cost of construction be adequately modeled using only the variables that describe characteristics of the plant itself?</p>&#13;
<p class="indent">Load the <code>boot</code> package and access the <code>?nuclear</code> help page to find details on the variables: <code>cap</code> (continuous variable describing the capacity of the plant); <code>cum.n</code> (treated as continuous, describing the number of similar constructions the engineers had previously worked on); <code>ne</code> (binary, describing whether the plant was in the northeastern United States); and <code>ct</code> (binary, describing whether the plant had a cooling tower).</p>&#13;
<p class="indent">The following model is fitted with the final construction cost of the plant as the response; a main effect for capacity; and main effects of, and all two-way interactions and the three-way interaction among, <code>cum.n</code>, <code>ne</code>, and <code>ct</code>:</p>&#13;
<pre>R&gt; nuc.fit &lt;- lm(cost~cap+cum.n*ne*ct,data=nuclear)<br/>R&gt; summary(nuc.fit)<br/><br/>Call:<br/>lm(formula = cost ~ cap + cum.n * ne * ct, data = nuclear)<br/><br/>Residuals:<br/>     Min       1Q   Median       3Q      Max<br/>-162.475  -50.368   -8.833   43.370  213.131<br/><br/>Coefficients:<br/>             Estimate Std. Error t value Pr(&gt;|t|)<br/>(Intercept)  138.0336    99.9599   1.381 0.180585<br/>cap           0.5085      0.1127   4.513 0.000157 ***<br/>cum.n       -24.2433      6.7874  -3.572 0.001618 **<br/>ne         -260.1036    164.7650  -1.579 0.128076<br/>ct         -187.4904     76.6316  -2.447 0.022480 *<br/>cum.n:ne     44.0196     12.2880   3.582 0.001577 **<br/>cum.n:ct     35.1687      8.0660   4.360 0.000229 ***<br/>ne:ct       524.1194    200.9567   2.608 0.015721 *<br/>cum.n:ne:ct -64.4444     18.0213  -3.576 0.001601 **<br/>---<br/>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/><br/>Residual standard error: 107.3 on 23 degrees of freedom<br/>Multiple R-squared:  0.705,        Adjusted R-squared:  0.6024<br/>F-statistic: 6.872 on 8 and 23 DF,  p-value: 0.0001264</pre>&#13;
<p class="indent">In this code, you specify the higher-order interactions by extending the number of variables connected with a <code>*</code> (using <code>*</code> instead of <code>:</code> since you want to include all the lower-order effects of those three predictors as well).</p>&#13;
<p class="indent">In the estimated results, the main effect for <code>cap</code> is positive, showing that an increased power capacity is tied to an increased construction cost. All other main effects are negative, which at face value seems to imply that a <span epub:type="pagebreak" id="page_525"/>reduced construction cost is associated with more experienced engineers, plants constructed in the Northeast, and plants with a cooling tower. However, this isn’t an accurate statement since you haven’t yet considered the interactive terms in those predictors. All estimated two-way interactive effects are positive—having more experienced engineers means a higher construction cost in the Northeast regardless of whether there’s a cooling tower, and having more experienced engineers also means higher costs for plants <em>with</em> a cooling tower, regardless of region.</p>&#13;
<p class="indent">Cost is also dramatically increased for plants in the Northeast with a cooling tower, regardless of the experience of the engineer. All that being said, the negative three-way interaction suggests that the increased cost associated with more experienced engineers working in the Northeast <em>and</em> on a plant with a cooling tower is lessened somewhat after the main effects and two-way interactive effects are calculated.</p>&#13;
<p class="indent">At the least, this example highlights the complexities associated with interpreting model coefficients for higher-order interactions. It’s also possible that statistically significant high-order interactions crop up due to lurking variables that have gone unaccounted for, that is, that the significant interactions are a spurious manifestation of patterns in the data that simpler terms involving those missing predictors could explain just as well (if not better). In part, this motivates the importance of adequate <em>model selection</em>, which is up next in the discussion.</p>&#13;
<div class="ex">&#13;
<p class="ext"><a id="ch21exc3"/><strong>Exercise 21.3</strong></p>&#13;
<p class="noindentz">Return your attention to the <code>cats</code> data frame in package <code>MASS</code>. In the first few problems in <a href="ch21.xhtml#ch21exc1">Exercise 21.1</a>, you fitted the main-effect-only model to predict the heart weights of domestic cats by total body weight and sex.</p>&#13;
<ol type="a">&#13;
<li><p class="noindents">Fit the model again, and this time include an interaction between the two predictors. Inspect the model summary. What do you notice in terms of the parameter estimates and their significance when compared to the earlier main-effect-only version?</p></li>&#13;
<li><p class="noindents">Produce a scatterplot of heart weight on body weight, using different point characters or colors to distinguish the observations according to sex. Use <code>abline</code> to add two lines denoting the fitted model. How does this plot differ from the one in <a href="ch21.xhtml#ch21exc1">Exercise 21.1</a> (d)?</p></li>&#13;
<li><p class="noindents">Predict the heart weight of Tilman’s cat using the new model (remember that Sigma is a 3.4 kg female) accompanied by a 95 percent prediction interval. Compare it to the main-effects-only model from the earlier exercise.</p></li>&#13;
</ol>&#13;
<p class="noindentz"><span epub:type="pagebreak" id="page_526"/>In <a href="ch21.xhtml#ch21exc2">Exercise 21.2</a>, you accessed the <code>trees</code> data frame in the contributed <code>faraway</code> package. After loading the package, access the <code>?trees</code> help file; you’ll find the volume and girth measurements you used earlier, as well as data on the height of each tree.</p>&#13;
<ol type="a" start="4">&#13;
<li><p class="noindents">Without using any transformations of the data, fit and inspect a main-effects-only model for predicting volume from girth and height. Then, fit and inspect a second version of this model including an interaction.</p></li>&#13;
<li><p class="noindents">Repeat (d), but this time use the log transformation of all variables. What do you notice about the significance of the interaction between the untransformed and transformed models? What does this suggest about the relationships in the data?</p></li>&#13;
</ol>&#13;
<p class="noindentz">Turn back to the <code>mtcars</code> data set and remind yourself of the variables in the help file <code>?mtcars</code>.</p>&#13;
<ol type="a" start="6">&#13;
<li><p class="noindents">Fit a linear model for <code>mpg</code> based on a two-way interaction between <code>hp</code> and <code>factor(cyl)</code> and their main effects, as well as a main effect for <code>wt</code>. Produce a summary of the fit.</p></li>&#13;
<li><p class="noindents">Interpret the estimated coefficients for the interaction between horsepower and the (categorical) number of cylinders.</p></li>&#13;
<li><p class="noindents">Suppose you’re keen on purchasing a 1970s performance car. Your mother advises you to purchase a “practical and economical” car that’s capable of an average MPG value of at least 25. You see three vehicles advertised: car 1 is a four-cylinder, 100 horsepower car that weighs 2100 lbs; car 2 is an eight-cylinder, 210 horsepower car that weighs 3900 lbs; and car 3 is a six-cylinder, 200 horsepower car that weighs 2900 lbs.</p>&#13;
<ol type="i">&#13;
<li><p class="noindents">Use your model to predict the mean MPG for each of the three cars; provide 95 percent confidence intervals. Based on your point estimates only, which car would you propose to your mother?</p></li>&#13;
<li><p class="noindents">You still want the most gas-guzzling car you can own with your mother’s blessing, so you decide to be sneaky and base your decision on what the confidence intervals tell you instead. Does this change your choice of vehicle?</p></li>&#13;
</ol></li>&#13;
</ol>&#13;
</div>&#13;
<h5 class="h5" id="ch21lev3sec103"><strong>Important Code in This Chapter</strong></h5>&#13;
<table class="topbot">&#13;
<thead>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table2r"><p class="table"><strong>Function/operator</strong></p></td>&#13;
<td style="vertical-align: top;" class="table2r"><p class="table"><strong>Brief description</strong></p></td>&#13;
<td style="vertical-align: top;" class="table2r"><p class="table"><strong>First occurrence</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>I</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Include arithmetic term</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch21.xhtml#ch21lev2sec199">Section 21.4.1</a>, <a href="ch21.xhtml#page_505">p. 505</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>:</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Interaction term</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch21.xhtml#ch21lev2sec203">Section 21.5.2</a>, <a href="ch21.xhtml#page_516">p. 516</a></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><code>*</code></p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table">Cross-factor operator</p></td>&#13;
<td style="vertical-align: top;" class="table"><p class="table"><a href="ch21.xhtml#ch21lev2sec204">Section 21.5.3</a>, <a href="ch21.xhtml#page_519">p. 519</a></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
</body></html>