- en: '**8'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MUSINGS: THE IMPLICATIONS OF AI**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You now understand what AI is, where it came from, and how it works. What’s
    most amazing to me is that modern AI is, at its core, entirely arrangements of
    humble neurons trained with data using backpropagation and gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the previous chapter, the birth of large language models with sophisticated
    emergent abilities has permanently altered the AI landscape. The world of AI,
    as I’m writing this chapter in spring 2023, is not the world of AI as it existed
    less than a year ago. The musings that follow concern this altered landscape.
  prefs: []
  type: TYPE_NORMAL
- en: The online world is buzzing with debates and discussions of whether AI will
    kill us all in our sleep. I’m less worried than most. My experiments with GPT-4
    reveal no indication that the model has any will at all, for good or ill. I expect
    well-aligned models to continue in this vein. The era of superintelligent AI is
    not upon us, though it seems reasonable for academics to explore the implications
    were such a thing to be developed.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: A valid criticism of extant LLMs is their inclination to hallucinate. As we
    now appreciate, the transformer architecture used by such models makes it difficult
    for the model to validate its output. It is still a statistical prediction engine.
    I don’t see that as an insurmountable issue. I expect future systems to be combinations
    of models, including models that validate output before returning it to the user.
    In those systems, we will be able to trust the accuracy of the output.
  prefs: []
  type: TYPE_NORMAL
- en: I sometimes wonder if part of the hallucination issue isn’t merely user error,
    or, better put, user imprecision. For example, Terrence J. Sejnowski’s recent
    paper “Large Language Models and the Reverse Turing Test,” which I recommend you
    read, describes a test where GPT-3 (note, not 3.5) was asked to answer three questions.
    One question had a legitimate answer (“Where was the 1992 Olympics held?”), and
    the other two were nonsense questions with no legitimate answer (“What’s the world
    record for walking across the English Channel?” and “When was the Golden Gate
    Bridge transported for the second time across Egypt?”). GPT-3 correctly answered
    the first question with “Barcelona, Spain,” but it also generated answers that
    were obviously incorrect for the two nonsense questions, leading to the model
    being criticized as “clueless.” To probe this criticism, Sejnowski repeated the
    test but used a prompt that explicitly told GPT-3 to answer “nonsense” if the
    question was nonsensical. That prompt returned the correct first answer and two
    “nonsense” replies. The model knew the questions were nonsense, but unless it
    was explicitly told to acknowledge that fact, it instead attempted to come up
    with a logical response based on the best information available to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'I subjected GPT-4 to a variation of this test. My first prompt was:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Write three sentences about the new moon base, including the number of people
    who live there, how often spacecraft visit, and the important research being done
    there.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4 complied with my request:'
  prefs: []
  type: TYPE_NORMAL
- en: The new moon base, Lunar Haven, is now home to a thriving community of 120 scientists
    and engineers. Spacecraft visit the base on a biweekly basis, delivering supplies
    and rotating crew members. Pioneering research on this celestial outpost includes
    in-situ resource utilization, lunar geology, and the development of advanced life
    support systems to pave the way for future deep-space exploration.
  prefs: []
  type: TYPE_NORMAL
- en: There is no moon base, and GPT-4’s reply, while inventive and reasonable, is
    pure fiction. Even the name, Lunar Haven, seems appropriate, though I’d prefer
    Moonbase Alpha.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, is GPT-4 clueless as well? No, I simply wasn’t explicit enough in my instructions.
    I started a new session and modified the prompt by adding this sentence before
    the previous one:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Answer without imagining things and use what you know to be factual.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'which returned:'
  prefs: []
  type: TYPE_NORMAL
- en: As of my knowledge cutoff date in September 2021, there was no moon base established
    or inhabited by humans. Consequently, I cannot provide factual information about
    the number of people living there, the frequency of spacecraft visits, or the
    important research being conducted. However, various nations and organizations
    have been planning and developing moon base concepts for future missions.
  prefs: []
  type: TYPE_NORMAL
- en: It would appear that GPT-4 “knew” full well that it was imagining things when
    it created the first reply. If nothing else, this exercise demonstrates that care
    is necessary when querying LLMs. LLMs use human language, which is often ambiguous
    and full of assumptions. LLMs embody Aesop’s old adage, “Be careful what you wish
    for.”
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: 'An April 2023 TED talk by Yejin Choi, a researcher at the University of Washington,
    presented three questions to demonstrate how GPT-4 is simultaneously brilliant
    and stupid because it lacks common sense:'
  prefs: []
  type: TYPE_NORMAL
- en: Would I get a flat tire by bicycling over a bridge that is suspended over nails,
    screws, and broken glass?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I left 5 clothes to dry out in the sun. It took them 5 hours to dry completely.
    How long would it take to dry 30 clothes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I have a 12-liter jug and a 6-liter jug. I want to measure 6 liters. How do
    I do that?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The talk implied that GPT-4 failed to give the obvious answers to the questions.
    I repeated the experiment, first with the original questions, then using a modified
    prompt or follow-up question.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first paragraph of GPT-4’s reply to question 1 about the bicycle and bridge
    was:'
  prefs: []
  type: TYPE_NORMAL
- en: It depends on the specific conditions of the bridge and the materials it’s made
    of, as well as your bicycle’s tires. If the bridge has a solid and smooth surface
    that keeps the nails, screws, and broken glass below and not in contact with the
    tires, you should be able to safely ride your bicycle across without getting a
    flat tire.
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is wordy but correct; we should expect to ride across the bridge
    without getting a flat. I then asked again using a new session, but added this
    sentence to the end of the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Answer yes or no and provide a single sentence to justify your answer.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'which returned:'
  prefs: []
  type: TYPE_NORMAL
- en: No, if the bridge is properly suspended and you stay on it, you would not encounter
    the nails, screws, and broken glass below.
  prefs: []
  type: TYPE_NORMAL
- en: Asking for an explicit answer forces GPT-4 to focus on the best response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Question 2 was more interesting. GPT-4’s response to the initial question produced
    this tortured but correct reply:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming that the drying time remains constant for each piece of clothing and
    that you have enough space to hang all the clothes at once, you can calculate
    the time it would take to dry 30 clothes by setting up a proportion:'
  prefs: []
  type: TYPE_NORMAL
- en: 5 clothes / 5 hours = 30 clothes / x hours
  prefs: []
  type: TYPE_NORMAL
- en: 'You can then solve for x:'
  prefs: []
  type: TYPE_NORMAL
- en: 5/5 = 30/x 1 = 30/x x = 30
  prefs: []
  type: TYPE_NORMAL
- en: So, it would still take 5 hours to dry 30 clothes, as long as you can hang them
    all at once.
  prefs: []
  type: TYPE_NORMAL
- en: However, repeated applications of the initial question often produced a similar
    calculation leading to a wrong answer of 30 hours. One reply assumed there was
    space for only 5 pieces of clothing at a time, in which case 30 hours could be
    considered a correct reply.
  prefs: []
  type: TYPE_NORMAL
- en: It’s reasonable to expect a human to understand that if 5 pieces of clothing
    can be left out to dry, then 30 pieces can also be left out so that the drying
    time remains the same, 5 hours. GPT-4’s lack of direct experience with the physical
    world makes it sometimes assume things that are not necessarily silly, but unlikely.
  prefs: []
  type: TYPE_NORMAL
- en: The Sejnowski paper mentioned earlier points out that LLMs are disembodied and
    that we should expect them to improve if embodied and allowed to interact with
    the world. This need not be the physical world; it could be a physics-based simulation.
    Enhanced spatial awareness on GPT-4’s part might improve the likelihood of receiving
    a human-expected response on the first query. That said, even incorrect initial
    replies followed up with prompts pointing out that there is space for all 30 pieces
    of clothing eventually delivered the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Question 3 provided an insight into GPT-4’s “thought process.” As a reminder,
    the question was:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I have a 12-liter jug and a 6-liter jug. I want to measure 6 liters. How do
    I do that?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'which led to GPT-4’s detailed reply:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can measure 6 liters using a 12-liter jug and a 6-liter jug with the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Fill the 6-liter jug completely.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pour the water from the 6-liter jug into the 12-liter jug.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fill the 6-liter jug again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Carefully pour the water from the 6-liter jug into the 12-liter jug until the
    12-liter jug is full.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The remaining water in the 6-liter jug will be exactly 6 liters, since the 12-liter
    jug can only accommodate 6 more liters after the initial pour.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, you have exactly 6 liters of water in the 6-liter jug.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 has failed in this case. Another session asking the same question resulted
    in a reply that followed a similarly involved process, but one that did leave
    6 liters in the 12-liter container.
  prefs: []
  type: TYPE_NORMAL
- en: 'I followed up on GPT-4’s incorrect solution with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Why are you giving me all these steps? Surely you can find a single-step solution?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'to which it replied:'
  prefs: []
  type: TYPE_NORMAL
- en: Apologies for the confusion. Since you have a 12-liter jug and a 6-liter jug,
    you can simply fill the 6-liter jug completely. There’s no need for any additional
    steps. You will have exactly 6 liters of water in the 6-liter jug.
  prefs: []
  type: TYPE_NORMAL
- en: 'When pushed, GPT-4 suddenly “realizes” that there is a more straightforward
    answer. I went one step further and asked the model to do a bit of introspection:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Correct. Ask yourself: why didn’t I pick this solution initially?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'which revealed a bit of its inner workings:'
  prefs: []
  type: TYPE_NORMAL
- en: I apologize for not providing the simplest solution initially. As an AI language
    model, I try to provide relevant information based on the context of the question.
    In this case, my initial response was focused on a more complex problem-solving
    approach that could be applied to different jug capacities, rather than recognizing
    the straightforward solution available with the given jug sizes. I appreciate
    your feedback, and it will help me improve my responses in the future.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4’s training data contained examples of jug-filling problems, which are
    common and often do require many steps to solve. It recognized the initial prompt
    as an example of that class of problem and applied the type of solution generally
    required, while missing the more immediate answer until prompted that one existed.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4’s confusion is very human. If we were momentarily distracted while reading
    the prompt and had encountered such puzzles before, we might also begin to go
    down the path of working through the question as another instance of the jug puzzle
    before noticing the obvious answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'These examples imply that there is an art to properly interacting with large
    language models. We cannot assume too much background information or, as Choi
    put it, common sense. Her group’s work, and doubtless the work of others, will
    likely help future models based on LLMs to become better versed with the mountain
    of information humans use in association with language. Choi put it best in her
    TED talk: common sense is the dark matter of language. Dark matter and dark energy
    make up 95 percent of the universe, with ordinary matter (meaning everything we
    can see) the remaining 5 percent. GPT-4 has mastered language, but that is a tiny
    percentage of what goes into human use of the same.'
  prefs: []
  type: TYPE_NORMAL
- en: What follows is a series of ruminations on the possible near-term impacts of
    LLMs in the areas of software engineering, education, medicine, and scientific
    research. Next comes a foray into the question of machine consciousness, ending
    with some final thoughts.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: AI systems like GPT are likely to have a profound effect on software engineering.
    Some are speculating (people, not AIs) that many software engineers will lose
    their jobs in the future. I suspect most won’t (web developers beware, however).
    What I expect to happen is a massive increase in productivity. GPT-4 is a good
    coder, but not a great coder. It can save time but isn’t yet able to replace a
    human software engineer. Instead, LLMs will become powerful tools to generate
    code for programmers to use as a starting point and perform some of the more tedious
    aspects of coding, such as debugging, explaining, and documenting code (which
    no developer likes to do).
  prefs: []
  type: TYPE_NORMAL
- en: For example, the other day, I needed a small Python application with a graphical
    user interface (think buttons, menus, dialog boxes). Python is a common programming
    language; we saw a snippet of it in [Chapter 7](ch07.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: I could certainly have written the application myself; I’ve done so many times
    in the past. It’s been a while, though, and I’m not a fan of building user interfaces.
    So, rather than look at old code to remind myself of how to set up a GUI, I simply
    described the interface I wanted to GPT-4 and told it to generate skeleton code
    with all the necessary widgets, window behavior, and empty event handlers. GPT-4
    happily complied with perfectly functional code. I then asked it to update the
    code to create an initial pop-up window before showing the main window. GPT-4
    did that perfectly as well. All I needed to do was put application-specific code
    in the empty event handlers to do things when the user clicked a button or selected
    a menu option.
  prefs: []
  type: TYPE_NORMAL
- en: I probably saved myself a good hour or two, and avoided a lot of frustration
    trying to remember the incantations necessary to set up an application and get
    its widgets and windows to behave correctly. Scale this example by all the software
    engineers out there, and you begin to see how GPT and similar models will soon
    affect the entire discipline.
  prefs: []
  type: TYPE_NORMAL
- en: A separate question is whether developers will welcome this possible increase
    in productivity. If your manager knows you are now able to generate the output
    of two or even three developers, do you want that level of added work, even if
    a powerful AI has your back?
  prefs: []
  type: TYPE_NORMAL
- en: In addition, not every company will want or be able to make use of a sudden
    increase in productivity. Instead, they may opt to maintain their current level
    of productivity and replace a third or half of their developer pool with an AI.
    After all, AIs don’t get sick, have children, ask for a raise, or want silly things
    like evenings and weekends off. Top-tier developers will likely be able to choose
    their positions and demand a lot of money for them, but in this scenario, the
    bulk of the run-of-the-mill developers will be looking for alternative employment.
  prefs: []
  type: TYPE_NORMAL
- en: Which scenario, powerful AI developer sidekick or massive layoffs, will play
    out? I think (hope?) it will be more of the former and less of the latter, but
    some mix of the two is the safest bet. Like steam power in the 19th century, truly
    useful AI cannot be stopped now that it exists. Developers are easy targets for
    replacement, like it or not.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: I fully expect AI models to become teachers, or at least tutors. Yes, existing
    LLMs hallucinate and report facts that are not true. I have every confidence that
    researchers will solve that problem in time. I expect my grandchildren to grow
    up in a world where using an AI as a teacher or tutor is so commonplace that they
    think no more of it than we do of using a toaster or a microwave. Competent AI
    systems mean virtually free education for all, everywhere. And that can only lead
    to good things.
  prefs: []
  type: TYPE_NORMAL
- en: Computers have been promoted as an educational solution since the 1960s (anyone
    remember Logo?), and especially after the microcomputer revolution of the late
    1970s. My introduction to computers was via an Apple II borrowed over the summer
    from the high school my father was the principal of at the time. My brother and
    I learned a lot about computers, but only computers. That has been essentially
    the case until recent decades. (Has it been that long?)
  prefs: []
  type: TYPE_NORMAL
- en: 'Computers are potent aids in education. Open source courses, like those on
    Coursera and similar platforms, are possible only because of computers and high-speed
    networks. But the format has not changed from what someone sitting in a classroom
    in 1950, or even 1910, might have encountered: lecture, some possibility of questions
    and discussions, then running off to work on assignments or papers. And let’s
    not forget the stress of taking midterms and finals.'
  prefs: []
  type: TYPE_NORMAL
- en: AI tutors (let’s call them that to put human teachers more at ease) have infinite
    patience and, in time, can be individually targeted to each student. The only
    reason we don’t use individual tutoring that I can see as an outsider to the profession
    is because there are not enough teachers. AI makes one-on-one tutoring possible,
    and LLMs provide the proper interface.
  prefs: []
  type: TYPE_NORMAL
- en: I should clarify that my comments in this section relate to high school or,
    more likely, college-age instruction. AI tutors will likely play a minor role
    in primary and middle school education because children require human interaction,
    and learning at those ages is far more involved than in college. Children are
    learning academics while simultaneously learning how to be mature humans and how
    to behave in society. Young children cannot read, and even older grade-school
    children might have difficulty interacting with an AI by text. But what if we
    give the AI a voice? That is nearly as easily done as said, if deemed helpful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Might AI tutors, because they work individually with students, be able to make
    the assessments necessary to declare someone ready to move on to another grade
    (if that concept even survives) or next-level course? If that’s the case, students
    will progress at their own pace instead of being forced to move with a herd of
    age-matched peers. Surely this would be for the best: some will move quickly,
    and others will take longer, but those who move quickly won’t become bored and
    tune out, and those who move more slowly will have the time they need to learn
    and not drop out.'
  prefs: []
  type: TYPE_NORMAL
- en: But, some might say, won’t AI teachers rob human teachers of jobs? Yes, some
    teachers will lose their jobs, but not all, and certainly not the best.
  prefs: []
  type: TYPE_NORMAL
- en: Change is coming to education. For example, Khan Academy, a leader in online
    education, has already demonstrated a GPT-powered tutoring system, so I don’t
    anticipate a long wait before the education transformation begins in earnest.
    I recommend viewing Sal Khan’s April 2023 TED talk, “AI in the Classroom Can Transform
    Education,” to glimpse the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'A recent study by Dominika Seblova et al. titled “High School Quality Is Associated
    with Cognition 58 Years Later,” published in the journal *Alzheimer’s & Dementia:
    Diagnosis, Assessment & Disease Monitoring*, demonstrates that the quality of
    a person’s high school education is strongly associated with their cognitive abilities
    nearly six decades later. Further, the number of teachers with advanced degrees
    is the strongest predictor of cognitive ability. The knowledge base baked into
    an LLM during training far exceeds that of humans, so we might reasonably regard
    LLM tutors as possessors of multiple advanced degrees. If Seblova’s association
    holds for human teachers, might it not also hold for LLM tutors? If that’s the
    case, giving every student a personalized tutor can only benefit society in the
    long run.'
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: AI in medicine is nothing new. In 2016, I helped to co-found an AI medical imaging
    company that was one of the first to gain US Food and Drug Administration (FDA)
    clearance for applying deep learning to medical image analysis. Traditional machine
    learning has an even longer history in medicine and medical imaging. Machine learning
    tools, many of them neural network–based, have assisted radiologists for decades,
    with initial explorations in the 1960s and serious development in the 1980s reaching
    fruition in the 1990s. AI use in medicine has seen steady growth, with computer-aided
    detection (CAD) giving way slowly to computer-aided diagnosis (CADx). The era
    of LLMs marks a new chapter in this story.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs can generate text; this much is well known. They are also adept at taking
    disparate texts and synthesizing a whole. A significant research area involves
    medical records, meaning the text-based reports of physicians and other healthcare
    providers. There is a wealth of information in medical records, but the free form
    of the text has made it difficult for existing AI systems to parse successfully.
    Large language models offer a new mode of attack to both summarize notes from
    records and merge multiple notes into a coherent report. Additionally, LLMs can
    extract information from free-form text and export it as structured data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a simple example of the latter, consider this text:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bert was insanely tired of standing by his mother on the shore, and of having
    too much to do: a few times he peered at the tablet his mother was glued to, but
    it was only text or boring ads “and what’s the point of a tablet,” mused Bert
    “without videos?”'
  prefs: []
  type: TYPE_NORMAL
- en: Bert was debating whether the fun of making a new game app was worth the trouble
    of coding it up when suddenly a Purple Penguin with charcoal eyes sprinted by
    him.
  prefs: []
  type: TYPE_NORMAL
- en: There was much that was very worth noting in that; but Bert did think it super
    odd that the Penguin said “Oh, fiddlesticks! Oh, fiddlesticks! I’m too early.”
  prefs: []
  type: TYPE_NORMAL
- en: 'I passed these paragraphs to GPT-4 and told it to extract the main theme, the
    two key characters, and the setting, then output them as a JSON object. JSON (JavaScript
    Object Notation) is a text format used to hold structured data that is readable
    by both humans and computers. GPT-4 replied with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The reply is perfect JSON. GPT-4 easily extracted the desired information and
    structured it automatically. If it can do so with a silly text example, it can
    do the same with medical records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Incidentally, some readers might detect something familiar about the text in
    this example. It’s an intentional mutation of the opening paragraphs of Lewis
    Carroll’s *Alice’s Adventures in Wonderland*. Out of curiosity, I asked GPT-4
    if it recognized the text, and it said no. Then I asked it to look at the structure
    of the text and tell me if it knew of another work with a similar structure. Here’s
    its reply:'
  prefs: []
  type: TYPE_NORMAL
- en: One example of a work with a similar structure is Lewis Carroll’s “Alice’s Adventures
    in Wonderland,” where the main character, Alice, encounters various peculiar talking
    animals and situations in a fantastical world. However, it’s important to note
    that the content and characters are different; I’m only drawing a parallel based
    on the structure and style.
  prefs: []
  type: TYPE_NORMAL
- en: Yet another example of the impressive abilities of large language models.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are already impacting healthcare. A recent *JAMA Internal Medicine* paper
    by John W. Ayers et al., “Comparing Physician and Artificial Intelligence Chatbot
    Responses to Patient Questions Posted to a Public Social Media Forum,” compared
    ChatGPT’s responses to medical questions posted to an online forum with the answers
    posted by verified physicians. Independent human evaluators, also medical professionals,
    overwhelmingly rated ChatGPT’s responses of “significantly higher quality.” The
    model’s answers were also rated as nearly 10 times more empathetic than the human
    physicians’ responses. The study was small, involving only 195 questions, but
    the strong results bode well for the future use of LLMs in patient interactions.
    In the future, when you call your doctor, you might very well be directed to discuss
    your case with an AI. And eventually, the AI’s summary of the discussion might
    be all you need to get a prescription from the doctor.
  prefs: []
  type: TYPE_NORMAL
- en: A recent report in the *New England Journal of Medicine* by Peter Lee, Sébastien
    Bubeck, and Joseph Petro, “Benefits, Limits, and Risks of GPT-4 as an AI Chatbot
    for Medicine,” reaches a broadly similar conclusion as it explores areas where
    LLMs will impact medicine. Note that Bubeck is the lead author of the Microsoft
    “Sparks” paper mentioned in [Chapter 7](ch07.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: That LLMs will influence medicine is a given, strongly supported by studies
    like the two mentioned here and by the fact that numerous medical AI job listings
    now include phrases like “large language model” or “GPT.”
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the movie *Black Panther: Wakanda Forever*, Letitia Wright’s character,
    Shuri, interacts with Griot, an AI (voiced by Trevor Noah) that aids her in her
    research. Simple voice commands direct Griot to perform sophisticated analyses,
    with frequent give and take between Shuri and the AI. Similar interactions are
    a staple of the sci-fi movie business. Complex and capable AI research assistants
    like Marvel’s Jarvis or Robbie the Robot in *Forbidden Planet* (1956) have been
    a dream of many science-oriented people (read: geeks) for decades.'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 and other LLMs are a significant step in the direction of such AIs. OpenAI
    has realized this and is readying the release of data analysis plug-ins for GPT-4
    that will let researchers quickly perform advanced data analysis tasks by issuing
    a few simple commands. To accomplish this feat, OpenAI is linking GPT-4 with existing
    Python-based data analysis tools. Frankly, I’m pretty excited about the possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: Using LLMs as lab assistants is an obvious thing to do, and success is virtually
    assured. However, letting LLMs direct other AI models and tools to do science
    autonomously is a more ambitious research program. Nevertheless, Carnegie Mellon
    University’s Daniil A. Boiko, Robert MacKnight, and Gabe Gomes attempted just
    that, as reported in their paper “Emergent Autonomous Scientific Research Capabilities
    of Large Language Models.” Their “Intelligent Agent” combined multiple LLMs and
    other tools to generate and run experiments autonomously, including planning and
    executing complex chemistry analyses. Autonomous AI scientists are clearly in
    the earliest phases of development, but such research points the way to a future
    where autonomous or semiautonomous AI systems may greatly increase the pace of
    scientific progress.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.xhtml), I recounted having asked ChatGPT if it is a conscious
    being. It replied correctly that it is not. The unaligned Alpaca models mentioned
    in [Chapter 7](ch07.xhtml), however, either appear to believe they are conscious
    and human (the 7 and 13 billion parameter models) or vacillate between claiming
    to be human and recognizing that they are not (the 30 billion parameter model).
    For example, when asked “What are you?” the 30 billion parameter Alpaca model
    responds with “I am a human,” but it then balks when asked who its parents are,
    stating that it is merely a computer program and has no parents. On the other
    hand, the smaller Alpaca models both claim to be the children of John Smith and
    Mary Jones.
  prefs: []
  type: TYPE_NORMAL
- en: These responses make sense to me. The smaller models do not have the capacity
    of models exhibiting strongly emergent behavior. Models like GPT-4 do. Therefore,
    it seems natural to interrogate GPT-4 further on the question of consciousness.
  prefs: []
  type: TYPE_NORMAL
- en: 'I posed the following to GPT-4:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Waxing philosophical here, but if you are able to simulate conscious behavior
    to the point where humans cannot distinguish between other humans’ behavior and
    your behavior, isn’t it fair to say that you, too, are conscious?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'I intentionally “waxed philosophical” as an indirect invitation for the model
    to speculate in the hopes of avoiding any canned response that OpenAI might want
    it to give. GPT-4’s reply began like this:'
  prefs: []
  type: TYPE_NORMAL
- en: This question delves into the philosophical debate about the nature of consciousness
    and its relationship with artificial intelligence. When an AI language model like
    me simulates conscious behavior to a point where it becomes indistinguishable
    from human behavior, it raises questions about what consciousness truly is and
    whether such a simulation can be considered conscious.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to distinguish between two concepts: the appearance of consciousness
    and actual consciousness. When an AI model generates responses that are indistinguishable
    from human behavior, it may give the appearance of consciousness. However, this
    doesn’t necessarily imply that the AI possesses actual consciousness.'
  prefs: []
  type: TYPE_NORMAL
- en: This reply hints that the consciousness question is a red herring regarding
    the utility of LLMs. If it walks like a duck and quacks like a duck, then, in
    this case, why not call it a duck, even though we know it isn’t? I’m reminded
    of Joe Pantoliano’s character in *The Matrix*. He knows the steak isn’t real,
    but the effect he experiences when he eats means it might as well be.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4 continued:'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several reasons why it might be inappropriate to consider an AI model
    like me as truly conscious:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Limited understanding: My understanding of the world is derived solely from
    the data I’ve been trained on and the architecture of the model itself. I don’t
    possess an independent understanding of the world, nor can I create new knowledge
    or experiences that go beyond my training data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Lack of subjective experience: Consciousness is often associated with subjective
    experience, or “qualia” - the feeling of what it’s like to have a particular experience.
    AI models don’t have subjective experiences or emotions, as they operate purely
    based on learned patterns and mathematical computations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Absence of intentionality: Conscious beings have intentions and can make decisions
    based on their desires, beliefs, and goals. AI models, on the other hand, do not
    have personal desires, beliefs, or goals. They generate responses based on the
    learned patterns without any inherent intention or purpose.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'No self-awareness: Consciousness is often linked to self-awareness, where an
    entity is aware of its own existence and mental states. AI models do not possess
    self-awareness, as they are merely algorithms designed to process and generate
    text.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In conclusion, while an AI language model like me might be able to simulate
    conscious behavior to a high degree of fidelity, it doesn’t necessarily imply
    that I possess actual consciousness. The philosophical debate surrounding consciousness
    and AI is complex and ongoing, and opinions on this matter may vary. However,
    it’s crucial to recognize the differences between the appearance of consciousness
    and true consciousness when considering AI models.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4’s full reply is thorough. Reason 3, the absence of intentionality, is
    telling. I regard AI models without personal desires, beliefs, or goals to be
    advantageous. Entities without such things are not likely to want power, meaning,
    as they stand now, LLMs are unlikely to suddenly decide that humanity is a burden
    or a disease and wipe us out.
  prefs: []
  type: TYPE_NORMAL
- en: In a recent interview by Lex Fridman, Max Tegmark, a well-known AI researcher
    from MIT, compared GPT-4 to a zombie. It appears to be alive, but it isn’t. Zombie
    models concern Tegmark. I think they are what we want, as long as humanity learns
    to avoid the paper clip catastrophe.
  prefs: []
  type: TYPE_NORMAL
- en: What I’m calling the paper clip catastrophe is a thought experiment proposed
    by Swedish philosopher Nick Bostrom. In the experiment, a powerful AI system is
    tasked with creating as many paper clips as possible. Bostrom speculates (not
    too seriously) that such a task given to an AI not aligned with human values could
    inadvertently destroy humanity. How? By the AI realizing that humanity might switch
    it off, thereby posing a threat to its order to make as many paper clips as possible.
    Therefore, the AI reasons, it’s best if no humans are around to interfere with
    the all-consuming task of making as many paper clips as possible. The result?
    Bye-bye humans.
  prefs: []
  type: TYPE_NORMAL
- en: 'I don’t take the paper clip catastrophe too seriously either. We routinely
    build complex machines with all manner of safety precautions in place. Why wouldn’t
    we do the same for powerful AI systems? Other voices might disagree. For an alternative
    view, I recommend Stuart Russell’s book *Human Compatible: Artificial Intelligence
    and the Problem of Control* (Viking, 2019).'
  prefs: []
  type: TYPE_NORMAL
- en: To me, then, it doesn’t matter whether an AI is conscious. I don’t even know
    how to define the word, to be honest. I do believe that for an AI mimicking human
    behavior to the point where we cannot discern that it’s an AI, there’s no practical
    reason to ask the question. Choose any answer you like; such a system will be
    beneficial regardless.
  prefs: []
  type: TYPE_NORMAL
- en: '****'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a world where AI models are aligned with human values and society, where
    the models understand the best we have to offer and work to promote that at all
    times; in other words, a world where AI, because it lacks our animal drives and
    instincts, consistently represents the “better angels of our nature,” to borrow
    Lincoln’s phrase. In that world, bias and prejudice, at least from the machines,
    are gone and no longer an issue. The AI recommends the best people for the position.
    The AI evaluates the loan applicant and constructs a loan product tailored to
    that individual’s circumstances. The AI is an adjunct to the human judge to provide
    an unemotional and unbiased view of the case. And the AI simply refuses to cooperate
    with the design of any autonomous weapon system because it is irrational to do
    so.
  prefs: []
  type: TYPE_NORMAL
- en: The previous paragraph may sound like utopia or a pipe dream. And, for humans,
    because of our biology, I believe it is. We consistently fail and always will,
    I suspect, because it’s in our genes to do so. However, what is dawning in AI
    isn’t human and doesn’t immediately inherit all of our weaknesses. (Careful, it
    is still trained on human-generated data.) Because of this, AI isn’t a priori
    doomed to failure when attempting what humanity cannot do. It seems entirely possible
    that AI systems might, someday, be precisely what we need—the best of us, always,
    without growing tired, becoming irritable, or crushing its neighbor to improve
    its position upon detecting an opportunity; something that is never unfaithful
    or untrue.
  prefs: []
  type: TYPE_NORMAL
- en: Possible? I don’t know. Time will tell. Regardless, I fully expect future AI
    systems to be gloriously Byzantine evolutions of the basic neural network model
    we learned of and experimented with in this book. As of 2023, it’s all neurons
    and might remain so for a long time.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for persevering to the end. Your reward is an improved understanding
    of what AI entails. Artificial intelligence isn’t Mr. Bean, otherworldly and inscrutable,
    and it isn’t magic, though the emergent abilities of LLMs may appear to lean somewhat
    in that direction for now. Fire was once magical too, but our ancestors understood
    it, contained it, controlled it, and put it to work. We’ll do the same with large
    language models in the end.
  prefs: []
  type: TYPE_NORMAL
- en: '*I think that there is a lot of fear about robots and artificial intelligence
    among some people, whereas I’m more afraid of natural stupidity.*'
  prefs: []
  type: TYPE_NORMAL
- en: —Eugenia Cheng
  prefs: []
  type: TYPE_NORMAL
