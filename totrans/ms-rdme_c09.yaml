- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Going On-Call
  prefs: []
  type: TYPE_NORMAL
- en: Many companies ask engineers to go on-call. On-call engineers are the first
    line of defense for any unplanned work, be it production issues or ad hoc support
    requests. Separating deep work from operational work lets the majority of the
    team focus on development while on-call engineers focus only on unpredictable
    operational issues and support tasks. Effective on-call engineers are prized by
    their teammates and managers, and they grow quickly from the relationship-building
    and learning opportunities that on-call rotations provide.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covers the basic knowledge and skills you’ll need to participate
    in on-call, incident handling, and support. We’ll explain how on-call rotations
    work and teach you important on-call skills. Then, we’ll go in-depth on a real-world
    incident to give you a practical example of how an incident is handled. After
    incidents, we’ll teach you support best practices. The on-call experience can
    cause burnout, so we end with a warning about the temptation to be a hero.
  prefs: []
  type: TYPE_NORMAL
- en: If you are in a role where an on-call rotation does not exist, read this chapter
    anyway. On-call skills apply in any urgent situation.
  prefs: []
  type: TYPE_NORMAL
- en: How On-Call Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On-call developers rotate based on a schedule. The length of a rotation can
    be as short as a day, though more often it’s a week or two. Every qualified developer
    takes part in the rotation. Developers who are new to the team or lack necessary
    skills are often asked to “shadow” a few primary on-call rotations to learn the
    ropes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some schedules have a primary and a secondary on-call developer; the secondary
    acts as a backup when the primary is unavailable. (Needless to say, developers
    who routinely cause the secondary on-call to step in are not looked upon kindly.)
    Some organizations have a tiered response structure: the support team might get
    alerted first, and then the issue would get escalated to operations engineers,
    followed by the development team.'
  prefs: []
  type: TYPE_NORMAL
- en: Most of an on-call’s time is spent fielding ad hoc support requests such as
    bug reports, questions about how their team’s software behaves, and usage questions.
    On-calls triage these requests and respond to the most urgent.
  prefs: []
  type: TYPE_NORMAL
- en: However, every on-call will eventually be hit with an operational incident (critical
    problem with production software). An incident is reported to on-call by an alert
    from an automated monitoring system or by a support engineer who manually observes
    a problem. On-call developers must triage, mitigate, and resolve incidents.
  prefs: []
  type: TYPE_NORMAL
- en: On-call developers get paged when critical alerts fire. *Paging* is an anachronism
    from before cell phones—these days, an alert is routed through channels such as
    chat, email, phone calls, or text messages. Make sure you enter the alerting service’s
    phone number into your contacts if, like us, you don’t answer calls from unknown
    numbers!
  prefs: []
  type: TYPE_NORMAL
- en: All on-call rotations should begin and end with a handoff. The previous on-call
    developer summarizes any current operational incidents and provides context for
    any open tasks to the next on-call developer. If you’ve tracked your work well,
    handoffs are a nonevent.
  prefs: []
  type: TYPE_NORMAL
- en: Important On-Call Skills
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On-call can be a rapid-fire, high-stress experience. Luckily, you can apply
    a common set of skills to handle both incidents and support requests. You’ll need
    to make yourself available and be on the lookout for incidents. You’ll also need
    to prioritize work so the most critical items get done first. Clear communication
    will be essential, and you’ll need to write down what you’re doing as you go.
    In this section, we’ll give you some tips to help you grow these skills.
  prefs: []
  type: TYPE_NORMAL
- en: Make Yourself Available
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: “Your best ability is availability.” This old saying is key to a successful
    on-call. An on-call’s job is to respond to requests and alerts. Don’t ignore requests
    or try to hide. Expect to be interrupted, and accept that you can’t do as much
    deep work while on-call.
  prefs: []
  type: TYPE_NORMAL
- en: Some on-call developers are expected to be near a computer 24/7 (though this
    doesn’t mean staying awake all night waiting for the alerts to fire. It means
    you’re reachable, able to react, and can adjust your nonwork plans accordingly).
    Larger companies have “follow the sun” on-call rotations that shift to developers
    in different time zones as the day goes on. Figure out what on-call expectations
    are, and don’t get caught in a situation where you can’t respond.
  prefs: []
  type: TYPE_NORMAL
- en: 'Being available does not mean immediately dropping whatever you are doing to
    address the latest question or problem. For many requests, it’s perfectly acceptable
    to acknowledge that you’ve received the query and respond with an approximate
    time when you should be able to look at the problem: “I am currently assisting
    someone else; can I get back to you in 15 minutes?” A fast response is generally
    expected from the on-call engineer, but not necessarily a fast resolution.'
  prefs: []
  type: TYPE_NORMAL
- en: Pay Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Information relevant to on-call work comes in through many channels: chat,
    email, phone calls, text messages, tickets, logs, metrics, monitoring tools, and
    even meetings. Pay attention to these channels so you’ll have context when debugging
    and troubleshooting.'
  prefs: []
  type: TYPE_NORMAL
- en: Proactively read release notes and chat or email channels that list operational
    information like software deployments or configuration changes. Keep an eye on
    chat rooms in which operations teams discuss unusual observations and announce
    adjustments they are making. Read meeting notes, particularly operational scrum
    digests that track ongoing incidents and maintenance for the day. Keep operational
    dashboards up in the background or on a nearby TV so you can establish a baseline
    for normal behavior. When incidents do occur, you’ll be able to tell which graphs
    look odd.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a list of resources that you can rely on in an emergency: direct links
    to critical dashboards and runbooks for your services, instructions for accessing
    logs, important chat rooms, and troubleshooting guides. A separate “on-call” bookmark
    folder that you keep up-to-date will come in handy. Share your list with the team
    so others can use and improve it.'
  prefs: []
  type: TYPE_NORMAL
- en: Prioritize Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Work on the highest-priority tasks first. As tasks are finished or become blocked,
    work your way down the list from highest to lowest priority. As you work, alerts
    will fire, and new questions will come in. Quickly triage the interruption: either
    set it aside or begin working on it if it’s an emergency. If the new request is
    higher priority than your current task but isn’t critical, try to finish your
    current task, or at least get it to a good stopping point before context switching.'
  prefs: []
  type: TYPE_NORMAL
- en: Some support requests are extremely urgent, while others are fine getting done
    in a week. If you can’t tell how urgent a request is, ask what the impact of the
    request is. The impact will determine the priority. If you disagree with the requestor
    about an issue’s prioritization, discuss it with your manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'On-call work is categorized by priority: P0, P1, P2, and so on. Prioritizing
    work into categories helps define a task’s urgency. Category names and meaning
    vary by company, but P0 tasks are the big ones. Google Cloud’s support priority
    ladder offers one example of how priority levels may be defined ([https://cloud.google.com/support/docs/best-practice#setting_the_priority_and_escalating/](https://cloud.google.com/support/docs/best-practice#setting_the_priority_and_escalating/)):'
  prefs: []
  type: TYPE_NORMAL
- en: 'P1: Critical Impact—Service Unusable in Production'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P2: High Impact—Service Use Severely Impaired'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P3: Medium Impact—Service Use Partially Impaired'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P4: Low Impact—Service Fully Usable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service level indicators, objectives, and agreements also help prioritize operational
    work. *Service level indicators* *(SLIs**)* such as error rate, request latency,
    and requests per second are the easiest way to see if an application is healthy.
    *Service level objectives* *(SLOs**)* define SLI targets for healthy application
    behavior. If error rate is an SLI for an application, an SLO might be a request
    error rate less than 0.001 percent. *Service level agreements* *(SLAs**)* are
    agreements about what happens when an SLO is missed. (Companies that violate SLAs
    with their customers usually need to return money and may even face contract termination.)
    Learn the SLIs, SLOs, and SLAs for your applications. SLIs will point you to the
    most important metrics. SLOs and SLAs will help you prioritize incidents.
  prefs: []
  type: TYPE_NORMAL
- en: Communicate Clearly
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Clear communication is critical when dealing with operational tasks. Things
    happen quickly, and miscommunication can cause major problems. To communicate
    clearly, be polite, direct, responsive, and thorough.
  prefs: []
  type: TYPE_NORMAL
- en: Under a barrage of operational tasks and interruptions, developers get stressed
    and grumpy—it’s human nature. Be patient and polite when responding to support
    tasks. While it might be your 10th interruption of the day, it’s the requestor’s
    first interaction with you.
  prefs: []
  type: TYPE_NORMAL
- en: Communicate in concise sentences. It can feel uncomfortable to be direct, but
    being direct doesn’t mean being rude. Brevity ensures that your communication
    is read and understood. If you don’t know an answer, say so. If you do know the
    answer, speak up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Respond to requests quickly. Responses don’t have to be solutions. Tell the
    requestor that you’ve seen their request, and make sure you understand the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks for reaching out. To clarify: the login service is getting 503 response
    codes from the profile service? You’re not talking about auth, right? They’re
    two separate services, but confusingly named.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Post status updates periodically. Updates should include what you’ve found
    since your last update and what you’re planning on doing next. Every time you
    make an update, provide a new time estimate:'
  prefs: []
  type: TYPE_NORMAL
- en: I looked at the login service. I don’t see a spike in error rate, but I’ll take
    a look at the logs and get back to you. Expect an update in an hour.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Track Your Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Write down what you’re doing as you work. Each item that you work on while on-call
    should be in an issue tracker or the team’s on-call log. Track progress as you
    work by writing updates in each ticket. Include the final steps that mitigated
    or resolved the issue in the ticket so you’ll have the solution documented if
    the issue appears again. Tracking progress reminds you where you left off when
    you come back to a ticket after an interruption. The next on-call will be able
    to see the state of ongoing work by reading your issues, and anyone you ask for
    help can read the log to catch up. Logged questions and incidents also create
    a searchable knowledge base that future on-calls can refer to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some companies use chat channels like Slack for operational incidents and support.
    Chat is a good way to communicate, but chat transcripts are hard to read later,
    so make sure to summarize everything in a ticket or document. Don’t be afraid
    to redirect support requests to appropriate channels. Be direct: “I’ll start looking
    into this right now. Could you open a ticket so this is counted when we evaluate
    our support workload?”'
  prefs: []
  type: TYPE_NORMAL
- en: Close finished issues so dangling tickets don’t clutter on-call boards and skew
    on-call support metrics. Ask the requestor to confirm that their issue has been
    addressed before closing their ticket. If a requestor isn’t responding, say that
    you’re going to close the ticket in 24 hours due to lack of response; then do
    so.
  prefs: []
  type: TYPE_NORMAL
- en: Always include timestamps in your notes. Timestamps help operators correlate
    events across the system when debugging issues. Knowing that a service was restarted
    at 1 PM is useful when customers begin reporting latency at 1:05 PM.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Incidents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Incident handling is an on-call’s most important responsibility. Most developers
    think handling an incident is about fixing a production problem. Resolving the
    problem is important, but in a critical incident, the top objective is to mitigate
    the impact of the problem and restore service. The second objective is to capture
    information to later analyze how and why the problem happened. Determining the
    cause of the incident, proving it to be the culprit, and fixing the underlying
    problem is only your *third* priority.
  prefs: []
  type: TYPE_NORMAL
- en: 'Incident response is broken into these five steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Triage**: Engineers must find the problem, decide its severity, and determine
    who can fix it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Coordination**: Teams (and potentially customers) must be notified of the
    issue. If the on-call can’t fix the problem themselves, they must alert those
    who can.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Mitigation**: Engineers must get things stable as quickly as possible. Mitigation
    is not a long-term fix; you are just trying to “stop the bleeding.” Problems can
    be mitigated by rolling back a release, failing over to another environment, turning
    off misbehaving features, or adding hardware resources.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Resolution**: After the problem is mitigated, engineers have some time to
    breathe, think, and work toward a resolution. Engineers continue to investigate
    the problem to determine and address underlying issues. The incident is resolved
    once the immediate problem has been fixed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Follow-up**: An investigation is conducted into the root cause—why it happened
    in the first place. If the incident was severe, a formal postmortem, or retrospective,
    is conducted. Follow-up tasks are created to prevent the root cause (or causes)
    from happening again. Teams look for any gaps in process, tooling, or documentation.
    The incident is not considered done until all follow-up tasks have been completed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The phases of an incident can sound abstract. To make things clear, we’ll walk
    you through a real incident and point out the different phases as we go. The incident
    occurs when data fails to load into a data warehouse. *Data warehouses* are databases
    meant to serve analytical queries for reports and machine learning. This particular
    data warehouse is kept up-to-date by a stream of updates in a real-time messaging
    system. Connectors read messages from the streaming system and write them into
    the warehouse. The data in the data warehouse is used by teams across the company
    for both internal and customer-facing reports, machine learning, application debugging,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: Triage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Determine a problem’s priority by looking at its impact: How many people is
    it affecting, and how detrimental is it? Use your company’s prioritization categories
    and SLO/SLA definitions to prioritize the issue, with the help of SLIs and the
    metric that triggered the alert, if applicable.'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re having trouble determining issue severity, ask for help. Triage is
    not the time to prove you can figure things out on your own; time is of the essence.
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, triage is not the time to troubleshoot problems. Your users will continue
    to suffer while you troubleshoot. Save troubleshooting for the mitigation and
    resolution phases.
  prefs: []
  type: TYPE_NORMAL
- en: Coordination
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Coordination starts by figuring out who’s in charge. For lower-priority incidents,
    the on-call is in charge and will coordinate. For larger incidents, an *incident
    commander* will take charge. Commanders keep track of who is doing what and what
    the current state of the investigation is.
  prefs: []
  type: TYPE_NORMAL
- en: Once someone takes charge, all relevant parties must be notified of the incident.
    Contact everyone needed to mitigate or resolve the problem—other developers or
    SREs. Internal stakeholders such as technical account managers, product managers,
    and support specialists might need to be notified. Impacted users might need to
    be alerted through status pages, emails, Twitter alerts, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Many different conversations will happen in parallel, which makes it difficult
    to follow what’s happening. Large incidents have war rooms to help with communication.
    *War rooms* are virtual or physical spaces used to coordinate incident response.
    All interested parties join the war room to coordinate response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Track communication in written form in a central location: a ticketing system
    or chat. Communicating helps everyone track progress, saves you from constantly
    answering status questions, prevents duplicate work, and enables others to provide
    helpful suggestions. Share both your observations and your actions, and state
    what you are about to do before you do it. Communicate your work even if you are
    working alone—someone might join later and find the log helpful, and a detailed
    record will help to reconstruct the timeline afterward.'
  prefs: []
  type: TYPE_NORMAL
- en: Mitigation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Your goal in the mitigation phase is to reduce the problem’s impact. Mitigation
    isn’t about fixing the problem; it’s about reducing its severity. Fixing a problem
    can take a lot of time, while mitigating it can usually be done quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Incidents are commonly mitigated by rolling back a software release to a “last
    known good” version or by shifting traffic away from the problem. Depending on
    the situation, mitigation might involve turning off a feature flag, removing a
    machine from a pool, or rolling back a just-deployed service.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, the software you’re working with will have a runbook for the problem.
    *Runbooks* are predefined step-by-step instructions to mitigate common problems
    and perform actions such as restarts and rollbacks. Make sure you know where runbooks
    and troubleshooting guides can be found.
  prefs: []
  type: TYPE_NORMAL
- en: Capture what data you can as you work to mitigate the problem. Once mitigated,
    the problem might be hard to reproduce. Quickly saving telemetry data, stack traces,
    heap dumps, logs, and screenshots of dashboards will help with debugging and root-cause
    analysis later.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll often find gaps in metrics, tooling, and configuration while trying to
    mitigate the problem. Important metrics might be missing, incorrect permissions
    might be granted, or systems might be misconfigured. Quickly write down any gaps
    that you find—anything that would have made your life better while troubleshooting.
    Open tickets during the follow-up phase to address these gaps.
  prefs: []
  type: TYPE_NORMAL
- en: Resolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once mitigation is complete, the incident is no longer an emergency. You can
    take time to troubleshoot and resolve the underlying issues. In our example incident,
    the priority was dropped once the customer-facing streams recovered. This gave
    breathing room to the engineers so they could investigate the problem.
  prefs: []
  type: TYPE_NORMAL
- en: During the resolution phase, focus on the immediate technical problems. Focus
    on what is needed to recover without the temporary measures put in place during
    mitigation. Set aside larger technical and process problems for the follow-up
    phase.
  prefs: []
  type: TYPE_NORMAL
- en: Use the scientific method to troubleshoot technical problems. Chapter 12 of
    Google’s *Site Reliability Engineering* book offers a *hypothetico-deductive*
    model of the scientific method. Examine the problem, make a diagnosis, and then
    test and treat. If the treatment is successful, the problem is cured; if not,
    you reexamine and start again. The team in our example applied the scientific
    method when they formed a hypothesis that the connector was having deserialization
    issues and not dropping data. They looked at metric data and did their binary-search
    experiment to find the bad stream. If they came up empty-handed, the team would
    need to reformulate a new hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, you can quarantine a misbehaving program instance and examine its
    misbehavior. The engineers in our connector example did this when they isolated
    the bad stream to a separate connector. Your goal during resolution is to understand
    the symptoms of the problem and try to make it reproducible. Use all the operational
    data at your disposal: metrics, logs, stack traces, heap dumps, change notifications,
    issue tickets, and communications channels.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have a clear view of the symptoms, diagnose the problem by looking
    for the causes. Diagnosis is a search, and like any search, you can use search
    algorithms to troubleshoot. For small problems, a linear search—examining components
    front to back—is fine. Use divide and conquer or a binary search (also called
    *half-splitting*) on bigger systems. Find a point halfway through the call stack
    and see if the problem is upstream or downstream of the issue. If the problem
    is upstream, pick a new component halfway upstream; if it’s downstream, do the
    reverse. Keep iterating until you find the component where you believe the problem
    is occurring.
  prefs: []
  type: TYPE_NORMAL
- en: Next, test your theory. Testing isn’t treatment—you’re not fixing the problem
    yet. Instead, see if you can control the bad behavior. Can you reproduce it? Can
    you change a configuration to make the problem go away? If so, you’ve located
    the cause. If not, you’ve eliminated one potential cause—go back, reexamine, and
    formulate a new diagnosis to test. Once the team in the connector example believed
    they had narrowed the problem to a header deserialization issue, they tested their
    theory by disabling header decoding in the connector configuration.
  prefs: []
  type: TYPE_NORMAL
- en: After a successful test, you can decide on the best course of treatment. Perhaps
    a configuration change is all that’s needed. Often, a bug fix will need to be
    written, tested, and applied. Apply the treatment and verify that it’s working
    as expected. Keep an eye on metrics and logs until you’re convinced everything
    is stable.
  prefs: []
  type: TYPE_NORMAL
- en: Follow-Up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Incidents are a big deal, so they need follow-up. The goal is to learn from
    the incident and to prevent it from happening again. A *postmortem* document is
    written and reviewed, and tasks are opened to prevent recurrence.
  prefs: []
  type: TYPE_NORMAL
- en: The on-call engineer who dealt with the incident is responsible for drafting
    a postmortem document, which should capture what happened, what was learned, and
    what needs to be done to prevent the incident from happening again. There are
    many approaches and templates for writing a postmortem. One good example is Atlassian’s
    postmortem template ([https://www.atlassian.com/incident-management/postmortem/templates/](https://www.atlassian.com/incident-management/postmortem/templates/)).
    The template has sections and examples describing the lead-up, fault, impact,
    detection, response, recovery, timeline, root cause, lessons learned, and corrective
    actions needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'A critical section of any postmortem document is the *root-cause analysis (RCA)*
    section. Root-cause analysis is performed using the five whys. This technique
    is pretty simple: keep asking why. Take a problem and ask why it happened. When
    you get an answer, ask why again. Keep asking why until you get to the root cause.
    The “five” is anecdotal—most problems take about five iterations to get to the
    root cause.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem: Data Missing from Data Warehouse**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Why?** The connector wasn’t loading data into the data warehouse.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Why?** The connector couldn’t deserialize incoming messages.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Why?** The incoming messages had bad headers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Why?** The APM was inserting headers into the messages.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Why?** The APM defaulted to this behavior without developer knowledge.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this example, the root cause was the APM’s accidental message header configuration.
  prefs: []
  type: TYPE_NORMAL
- en: After a postmortem document is written, one of the managers or tech leads schedules
    a review meeting with all interested parties. The postmortem document author leads
    the review, and participants discuss each section in detail. The author adds missing
    information and new tasks as they’re discovered during discussion.
  prefs: []
  type: TYPE_NORMAL
- en: It’s easy to get upset and cast blame in high-stress situations. Do your best
    to provide constructive feedback. Point out areas for improvement, but avoid blaming
    individuals or teams for problems. “Peter didn’t disable message headers” assigns
    blame, while “Message header config changes aren’t going through code review”
    is an area for improvement. Don’t let postmortems turn into unhealthy vent fests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Good postmortem meetings also keep “solutioning” separate from the review meeting.
    *Solutioning*—figuring out how to solve a problem—takes a long time and distracts
    from the purpose of the meeting: to discuss problems and assign tasks. “The message
    had a bad header” is a problem, while “We should put bad messages in a dead letter
    queue” is a solution. Any solutioning should happen in follow-up tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: After a postmortem meeting, follow-up tasks must be completed. If tasks are
    assigned to you, work with your manager and the postmortem team to prioritize
    them properly. An incident can’t be closed until all remaining follow-up tasks
    have been finished.
  prefs: []
  type: TYPE_NORMAL
- en: Old postmortem documents are a great way to learn. Some companies even share
    their postmortem documents publicly as valuable resources for the whole community
    to learn from. Look at Dan Luu’s collection for inspiration ([https://github.com/danluu/post-mortems](https://github.com/danluu/post-mortems)).
    You might find postmortem reading groups at your company, too. Teams get together
    to review postmortem documents with a wider audience. Some teams even use old
    postmortem documents to simulate production issues to train new engineers.
  prefs: []
  type: TYPE_NORMAL
- en: Providing Support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When on-call engineers aren’t dealing with incidents, they spend time handling
    support requests. These requests come both from within the organization and from
    external customers, and they run the gamut from simple “Hey, how does this work?”
    questions to difficult troubleshooting problems. Most requests are bug reports,
    questions about business logic, or technical questions about how to use your software.
  prefs: []
  type: TYPE_NORMAL
- en: 'Support requests follow a pretty standard flow. When a request comes in, you
    should acknowledge that you’ve seen it and ask questions to make sure you understand
    the problem. Once you’ve got a grasp on the problem, give a time estimate on the
    next update: “I’ll get back to you by 5 PM with an update.” Next, start investigating,
    and update the requestor as you go. Follow the same mitigation and resolution
    strategies that we outlined earlier. When you think the issue is resolved, ask
    the requestor to confirm. Finally, close out the request. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[3:48 PM] Sumeet: I’m getting reports from customers saying page loads are
    slow.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[4:12 PM] Janet: Hi, Sumeet. Thanks for reporting this. Can I get a few more
    pieces of information? Can you give me one or two customer IDs that reported slowness
    and any particular pages they are seeing the issue on? Our dashboards aren’t showing
    any widespread latency.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[5:15 PM] Sumeet: Customer IDs 1934 and 12305\. Pages: the ops main page (/ops)
    and the APM chart (/ops/apm/dashboard). They’re saying loads are taking > 5 seconds.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[5:32 PM] Janet: Great, thanks. I’ll have an update for you by 10 tomorrow
    morning.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[8:15 AM] Janet: Okay, I think I know what’s going on. We had maintenance on
    the database that powers the APM dashboard yesterday afternoon. It impacted the
    ops main page since we show a roll-up there, too. The maintenance finished around
    8 PM. Can you confirm that the customer is no longer seeing the issue?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[9:34 AM] Sumeet: Awesome! Just confirmed with a couple of the customers who
    reported the issue. Page loads are now much better.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This example illustrates many practices covered in “Important On-Call Skills.”
    Janet, the on-call, is *paying attention* and *makes herself available*; she responds
    within a half hour of the first request. Janet *communicates clearly* by asking
    clarifying questions to understand the problem and its impact so she can properly
    *prioritize the issue*. She posts an ETA for the next update when she has enough
    information to investigate. Once Janet believes the problem is solved, she *tracks
    her work* by describing what happened and asks the requestor to confirm it’s not
    an issue anymore.
  prefs: []
  type: TYPE_NORMAL
- en: Support can feel like a distraction, since your “real” job is programming. Think
    of support as an opportunity to learn. You’ll get to see how your team’s software
    is used in the real world and the ways in which it fails or confuses users. Answering
    support requests will take you to parts of the code you were not familiar with;
    you’ll have to think hard and experiment. You will notice patterns that cause
    problems, which will help you create better software in the future. Support rotations
    will make you a better engineer. Plus, you get to help others and build relationships
    and your reputation. Fast, high-quality support responses do not go unnoticed.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t Be a Hero
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve spent quite a few words in this chapter encouraging you not to shy away
    from on-call responsibilities and ad hoc support requests. There is another extreme
    we want to warn you about: doing too much. On-call activities can feel gratifying.
    Colleagues routinely thank you for helping them with issues, and managers praise
    efficient incident resolution. However, doing too much can lead to burnout.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For some engineers, jumping into “firefighting” mode becomes a reflex as they
    become more experienced. Talented firefighting engineers can be a godsend to a
    team: everyone knows that when things get tough, all they need to do is ask the
    firefighter, and they’ll fix it. Depending on a firefighter is not healthy. Firefighters
    who are pulled into every issue effectively become permanently on-call. The long
    hours and high stakes will cause burnout. Firefighter engineers also struggle
    with their programming or design work because they are constantly being interrupted.
    And teams that rely on a firefighter won’t develop their own expertise and troubleshooting
    abilities. Firefighter heroics can also cause fixes for serious underlying problems
    to be deprioritized because the firefighter is always around to patch things up.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you feel that you are the only one who can fix a problem or that you are
    routinely involved in firefighting when not on-call, you might be becoming a “hero.”
    Talk to your manager or tech lead about ways to find better balance and get more
    people trained and available to step in. If there’s a hero on your team, see if
    you can learn from them and pick up some of the burden; let them know when you
    are okay struggling a bit: “Thanks, Jen. I actually want to try to figure this
    out on my own for a bit so I can skill up . . . Can I ask for your help in 30
    minutes if this is still a mystery?”'
  prefs: []
  type: TYPE_NORMAL
- en: Do’s and Don’ts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| **Do’s** | **Don’ts** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** add known “pager” numbers to your phone’s contacts. | **DON’T** ignore
    alerts. |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** use priority categories, SLIs, SLOs, and SLAs to prioritize incident
    response. | **DON’T** try to troubleshoot during triage. |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** triage, coordinate, mitigate, resolve, and follow up on critical incidents.
    | **DON’T** leave a problem unmitigated while you search for the root cause. |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** use the scientific method to troubleshoot. | **DON’T** cast blame
    during postmortems. |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** ask “the five whys” when following up on an incident. | **DON’T**
    hesitate to close nonresponsive support requests. |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** acknowledge support requests. | **DON’T** ask support requestors what
    their priority is; ask about the impact of the problem. |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** give time estimates and periodic updates. | **DON’T** be a hero who
    has to fix all the things. |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** confirm a problem is fixed before closing a support request ticket.
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| **DO** redirect support requests to the appropriate communication channels.
    |  |'
  prefs: []
  type: TYPE_TB
- en: Level Up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The five phases of incident response in our “Handling Incidents” section come
    from an *Increment* article, “What Happens When the Pager Goes Off?” ([https://increment.com/on-call/when-the-pager-goes-off/](https://increment.com/on-call/when-the-pager-goes-off/)).
    The article has more quotes and detail around how different companies handle incidents.
  prefs: []
  type: TYPE_NORMAL
- en: In more nascent operations settings, developers might need to define SLIs and
    SLOs themselves. If you find yourself responsible for SLIs and SLOs, we highly
    recommend Chapter 4 of Google’s *Site Reliability Engineering* book.
  prefs: []
  type: TYPE_NORMAL
- en: Chapters 11, 13, 14, and 15 of *Site Reliability Engineering* cover on-call,
    emergency response, incident handling, and postmortems. We’ve included the most
    important information for new engineers in our chapter, but Google’s book provides
    more detail if you want to go deeper.
  prefs: []
  type: TYPE_NORMAL
