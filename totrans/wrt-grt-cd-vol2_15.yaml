- en: '**15**'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**FUNCTIONS AND PROCEDURES**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: 'Since the beginning of the structured programming revolution in the 1970s,
    subroutines (procedures and functions) have been one of the primary tools software
    engineers use to organize, modularize, and otherwise structure their programs.
    Because procedure and function calls are used so frequently in code, CPU manufacturers
    have attempted to make them as efficient as possible. Nevertheless, these calls—and
    their associated returns—have costs that many programmers don’t consider when
    creating functions, and using them inappropriately can greatly increase a program’s
    size and execution time. This chapter discusses those costs and how to avoid them,
    covering the following subjects:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Function and procedure calls
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Macros and inline functions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameter passing and calling conventions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation records and local variables
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameter-passing mechanisms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Function return results
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By understanding these topics, you can avoid the efficiency pitfalls that are
    common in modern programs that make heavy use of procedures and functions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '**15.1 Simple Function and Procedure Calls**'
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s begin with some definitions. A *function* is a section of code that computes
    and returns some value—the function result. A *procedure* (or *void function*,
    in C/C++/Java/Swift terminology) simply accomplishes some action. Function calls
    generally appear within an arithmetic or logical expression, while procedure calls
    look like statements in the programming language. For the purpose of this discussion,
    you can generally assume that a procedure call and a function call are the same,
    and use the terms *function* and *procedure* interchangeably. For the most part,
    a compiler implements procedure and function calls identically.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '*Functions and procedures do have some differences, however. Namely, there
    are some efficiency issues related to function results, which we’ll consider in
    “Function Return Values” on [page 590](ch15.xhtml#page_590).*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'With most CPUs, you invoke procedures via an instruction similar to the 80x86
    `call` (`branch` and `link` on the ARM and PowerPC) and return to the caller using
    the `ret` (return) instruction. The `call` instruction performs three discrete
    operations:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: It determines the address of the instruction to execute upon returning from
    the procedure (this is usually the instruction immediately following `call`).
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It saves this address (commonly known as the *return address* or *link address*)
    into a known location.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It transfers control (via a jump mechanism) to the first instruction of the
    procedure.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Execution starts with the first instruction of the procedure and continues
    until the CPU encounters a `ret` instruction, which fetches the return address
    and transfers control to the machine instruction at that address. Consider the
    following C function:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here’s the conversion to PowerPC code by GCC:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here’s the 32-bit ARM version of this source code compiled by GCC:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And here’s the conversion of the same source code to 80x86 code by GCC:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是GCC将相同源代码转换为80x86代码的结果：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, the 80x86, ARM, and PowerPC devote considerable effort to building
    and managing activation records (see “The Stack Section” on [page 179](ch07.xhtml#page_179)).
    The important things to see in these two assembly language sequences are the `bl
    _func` and `blr` instructions in the PowerPC code; `bl func` and `bx lr` instructions
    in the ARM code; and the `call func` and `ret` instructions in the 80x86 code.
    These are the instructions that call the function and return from it.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，80x86、ARM和PowerPC都花费了相当大的努力来构建和管理激活记录（请参阅[第179页](ch07.xhtml#page_179)的“栈部分”）。在这两段汇编语言序列中，重要的是要注意PowerPC代码中的`bl
    _func`和`blr`指令；ARM代码中的`bl func`和`bx lr`指令；以及80x86代码中的`call func`和`ret`指令。这些是调用函数和从函数返回的指令。
- en: '**15.1.1 Return Address Storage**'
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**15.1.1 返回地址存储**'
- en: But where, exactly, does the CPU store the return address? In the absence of
    recursion and certain other program control constructs, the CPU could store the
    return address in any location that is large enough to hold the address and that
    will still contain that address when the procedure returns to its caller. For
    example, the program could choose to store the return address in a machine register
    (in which case the return operation would consist of an indirect jump to the address
    contained in that register). One problem with using registers, however, is that
    CPUs generally have a limited number of them. This means every register that holds
    a return address is unavailable for other purposes. For this reason, on CPUs that
    save the return address in a register, the applications usually move the return
    address to memory so they can reuse that register.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，CPU到底将返回地址存储在哪里呢？在没有递归和某些其他程序控制结构的情况下，CPU可以将返回地址存储在任何足够大的位置，只要这个位置在过程返回调用者时仍然包含该地址。例如，程序可以选择将返回地址存储在机器寄存器中（在这种情况下，返回操作将是间接跳转到寄存器中包含的地址）。然而，使用寄存器有一个问题，那就是CPU通常只有有限数量的寄存器。这意味着每个保存返回地址的寄存器都不能用于其他目的。因此，在那些将返回地址保存在寄存器中的CPU上，应用程序通常会将返回地址移动到内存中，以便可以重用该寄存器。
- en: Consider the PowerPC and ARM `bl` (branch and link) instruction. This instruction
    transfers control to the target address specified by its operand and copies the
    address of the instruction following `bl` into the LINK register. Inside a procedure,
    if no code modifies the value of the LINK register, the procedure can return to
    its caller by executing a PowerPC `blr` (branch to LINK register) or ARM `bx`
    (branch and exchange) instruction. In our trivial example, the `func()` function
    does not execute any code that modifies the value of the LINK register, so this
    is exactly how `func()` returns to its caller. However, if this function had used
    the LINK register for some other purpose, it would have been the procedure’s responsibility
    to save the return address so that it could restore the value prior to returning
    via a `blr` instruction at the end of the function call.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑PowerPC和ARM的`bl`（分支和链接）指令。该指令将控制转移到其操作数指定的目标地址，并将`bl`后面的指令地址复制到LINK寄存器中。在一个过程内部，如果没有代码修改LINK寄存器的值，则该过程可以通过执行PowerPC的`blr`（分支到LINK寄存器）或ARM的`bx`（分支和交换）指令来返回到调用者。在我们简单的示例中，`func()`函数没有执行任何修改LINK寄存器值的代码，因此这正是`func()`返回到其调用者的方式。然而，如果该函数将LINK寄存器用于其他目的，那么过程就需要负责保存返回地址，以便在函数调用结束时通过`blr`指令恢复返回地址。
- en: 'A more common place to keep return addresses is in memory. Although accessing
    memory on most modern processors is much slower than accessing a CPU register,
    keeping return addresses in memory allows a program to have a large number of
    nested procedure calls. Most CPUs actually use a *stack* to hold return addresses.
    For example, the 80x86 `call` instruction *pushes* the return address onto a stack
    data structure in memory, and the `ret` instruction *pops* this return address
    off the stack. Using a stack of memory locations to hold return addresses offers
    several advantages:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更常见的保存返回地址的地方是内存。尽管在大多数现代处理器上，访问内存的速度比访问CPU寄存器慢得多，但将返回地址保存在内存中允许程序有大量的嵌套过程调用。大多数CPU实际上使用*栈*来保存返回地址。例如，80x86的`call`指令*将*返回地址压入内存中的栈数据结构，而`ret`指令*将*这个返回地址从栈中弹出。使用内存位置的栈来保存返回地址有几个优点：
- en: Stacks, because of their *last-in, first-out (LIFO)* organization, fully support
    nested procedure calls and returns as well as recursive procedure calls and returns.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacks are memory efficient because they reuse the same memory locations for
    different procedure return addresses (rather than requiring a separate memory
    location to hold each procedure’s return address).
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though stack access is slower than register access, the CPU can generally
    access memory locations on the stack faster than separate return addresses elsewhere,
    because the CPU frequently accesses the stack and the stack contents tend to remain
    in the cache.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As discussed in [Chapter 7](ch07.xhtml#ch07), stacks are also great places to
    store activation records (such as parameters, local variables, and other procedure
    state information).
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a stack also incurs a few penalties, though. Most importantly, maintaining
    a stack generally requires dedicating a CPU register to keep track of it in memory.
    This could be a register that the CPU explicitly dedicates for this purpose (for
    example, the RSP register on the x86-64 or R14/SP on the ARM) or a general-purpose
    register on a CPU that doesn’t provide explicit hardware stack support (for example,
    applications running on the PowerPC processor family typically use R1 for this
    purpose).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: On CPUs that provide a hardware stack implementation and a `call`/`ret` instruction
    pair, making a procedure call is easy. As shown earlier in the 80x86 GCC example
    output, the program simply executes a `call` instruction to transfer control to
    the beginning of the procedure and then executes a `ret` instruction to return
    from the procedure.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: The PowerPC/ARM approach, using a “branch and link” instruction might seem less
    efficient than the `call`/`ret` mechanism. While it’s certainly true that the
    “branch and link” approach requires a little more code, it isn’t so clear that
    it’s slower than the `call`/`ret` approach. A `call` instruction is a complex
    instruction (accomplishing several independent tasks with a single instruction)
    and, as a result, typically requires several CPU clock cycles to execute. The
    execution of the `ret` instruction is similar. Whether the extra overhead is costlier
    than maintaining a software stack varies by CPU and compiler. However, a “branch
    and link” instruction and an indirect jump through the link address, without the
    overhead of maintaining the software stack, is usually faster than the corresponding
    `call`/`ret` instruction pair. If a procedure doesn’t call any other procedures
    and can maintain parameters and local variables in machine registers, it’s possible
    to skip the software stack maintenance instructions altogether. For example, the
    call to `func()` in the previous example is probably more efficient on the PowerPC
    and ARM than on the 80x86, because `func()` doesn’t need to save the LINK register’s
    value into memory—it simply leaves that value in LINK throughout the execution
    of the function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'Because many procedures are short and have few parameters and local variables,
    a good RISC compiler can often dispense with the software stack maintenance entirely.
    Therefore, for many common procedures, this RISC approach is faster than the CISC
    (`call`/`ret`) approach; however, that’s not to imply that it’s always better.
    The brief example in this section is a very special case. In our simple demonstration
    program, the function that this code calls—via the `bl` instruction—is near the
    `bl` instruction. In a complete application, `func()` might be *very* far away,
    and the compiler wouldn’t be able to encode the target address as part of the
    instruction. That’s because RISC processors (like the PowerPC and ARM) must encode
    their entire instruction within a single 32-bit value (which must include both
    the opcode and the displacement to the function). If `func()` is farther away
    than can be encoded in the remaining displacement bits (24, in the case of the
    PowerPC and ARM `bl` instructions), the compiler has to emit a sequence of instructions
    that will compute the address of the target routine and indirectly transfer control
    through that address. Most of the time, this shouldn’t be a problem. After all,
    few programs are so large that the functions would be outside this range (64MB,
    in the case of the PowerPC, ±32MB for the ARM). However, there’s a very common
    case where GCC (and other compilers, presumably) must generate this type of code:
    when the compiler doesn’t know the target address of the function, because it’s
    an external symbol that the linker must merge in after compilation is complete.
    Because the compiler doesn’t know where the routine will be sitting in memory
    (and also because most linkers work only with 32-bit addresses, not 24-bit displacement
    fields), the compiler must assume that the function’s address is out of range
    and emit the long version of the function call. Consider the following slight
    modification to the earlier example:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多过程较短，且参数和局部变量较少，一个好的RISC编译器通常能够完全省去软件栈的维护。因此，对于许多常见的过程，这种RISC方法比CISC（`call`/`ret`）方法更快；然而，这并不意味着它总是更好。本节中的简短示例是一个非常特殊的情况。在我们的简单演示程序中，这段代码调用的函数——通过`bl`指令——离`bl`指令很近。在一个完整的应用程序中，`func()`可能会*非常*远，而编译器将无法将目标地址编码为指令的一部分。这是因为RISC处理器（如PowerPC和ARM）必须将整个指令编码在一个32位值内（该值必须同时包括操作码和到函数的偏移量）。如果`func()`的距离超出了剩余偏移位（在PowerPC和ARM的`bl`指令中为24位）所能编码的范围，编译器必须发出一系列指令来计算目标例程的地址，并通过该地址间接转移控制权。大多数时候，这不应该是个问题。毕竟，很少有程序的函数会超出这个范围（在PowerPC中为64MB，ARM为±32MB）。然而，有一个非常常见的情况是GCC（以及其他编译器，可能也一样）必须生成这种类型的代码：当编译器不知道函数的目标地址时，因为它是一个外部符号，链接器必须在编译完成后将其合并进来。因为编译器不知道该例程将在内存中的哪个位置（而且大多数链接器仅处理32位地址，而不是24位的偏移量字段），编译器必须假设该函数的地址超出了范围，并生成长版本的函数调用。考虑一下对之前示例的轻微修改：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This code declares `func()` as an external function. Now look at the PowerPC
    code that GCC produces and compare it with the earlier code:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将`func()`声明为外部函数。现在看看GCC生成的PowerPC代码，并与之前的代码进行比较：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This code effectively winds up calling two functions in order to call `func()`.
    First, it calls a *stub* function (`L_func$stub`), which then transfers control
    to the actual `func()` routine. Clearly there is considerable overhead here. Without
    actually benchmarking the PowerPC code against the 80x86 code, it’s probably a
    safe bet that the 80x86 solution is a bit more efficient. (The 80x86 version of
    the GCC compiler emits the same code for the main program as in the earlier example,
    even when compiling in the external reference.) You’ll soon see that the PowerPC
    also generates stub functions for things other than external functions. Therefore,
    the CISC solution often is more efficient than the RISC solution (presumably,
    RISC CPUs make up the difference in performance in other areas).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码实际上调用了两个函数来调用`func()`。首先，它调用一个*存根*函数（`L_func$stub`），然后将控制权转交给实际的`func()`函数。显然，这里有相当大的开销。如果不对PowerPC代码与80x86代码进行实际的基准测试，推测80x86解决方案可能会更高效一些。（即使在编译外部引用时，80x86版本的GCC编译器也会生成与之前示例中相同的主程序代码。）很快你会发现，PowerPC也会为除外部函数之外的其他情况生成存根函数。因此，CISC解决方案通常比RISC解决方案更高效（大概RISC处理器在其他方面弥补了性能差距）。
- en: 'The Microsoft CLR also provides generic call and return functionality. Consider
    the following C# program with a static function `f()`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here’s the CIL code that the Microsoft C# compiler emits for functions `f()`
    and `Main()`:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As one last example, here’s a comparable Java program:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here’s the Java bytecode (JBC) output:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that the Microsoft CLR and Java VM both have several variants of call and
    invoke instructions. These simple examples demonstrate calls to static methods.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '**15.1.2 Other Sources of Overhead**'
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Of course, a typical procedure call and return involve overhead beyond the execution
    of the actual procedure `call` and `return` instructions. Prior to calling the
    procedure, the calling code must compute and pass any parameters to it. Upon entry
    into the procedure, the calling code may also need to complete the construction
    of the *activation record* (that is, allocate space for local variables). The
    costs of these operations vary by CPU and compiler. For example, if the calling
    code can pass parameters in registers rather than on the stack (or some other
    memory location), this is usually more efficient. Similarly, if the procedure
    can keep all its local variables in registers rather than in the activation record
    on the stack, accessing those local variables is much more efficient. This is
    one area where RISC processors have a considerable advantage over CISC processors.
    A typical RISC compiler can reserve several registers for passing parameters and
    local variables. (RISC processors typically have 16, 32, or more general-purpose
    registers, so setting aside several registers for this purpose is not outrageous.)
    For procedures that don’t call any other procedures (discussed in the next section),
    there’s no need to preserve these register values, so parameter and local variable
    access is very efficient. Even on CPUs with a limited number of registers (such
    as the 32-bit 80x86), it’s still possible to pass a small number of parameters,
    or maintain a few local variables, in registers. Many 80x86 compilers, for example,
    will keep up to three values (parameters or local variables) in the registers.
    Clearly, though, the RISC processors have an advantage here.^([1](footnotes.xhtml#ch15fn1))
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Armed with this knowledge, along with the background on activation records and
    stack frames from earlier in this book (see “The Stack Section” on [page 179](ch07.xhtml#page_179)),
    we can now discuss how to write procedures and functions that operate as efficiently
    as possible. The exact rules are highly dependent upon your CPU and the compiler
    you’re using, but some of the concepts are generic enough to apply to all programs.
    The following sections assume that you’re writing for an 80x86 or ARM CPU (as
    most of the world’s software runs on one of these two CPUs).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '**15.2 Leaf Functions and Procedures**'
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compilers can often generate better code for *leaf* procedures and functions—that
    is, those that don’t call other procedures or functions. The metaphor comes from
    a graphical representation of procedure/function invocations known as a *call
    tree*. A call tree consists of a set of circles (*nodes*) that represent the functions
    and procedures in a program. An arrow from one node to another implies that the
    first node contains a call to the second. [Figure 15-1](ch15.xhtml#ch15fig1) illustrates
    a typical call tree.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the main program directly calls procedure `prc1()` and functions
    `fnc1()` and `fnc2()`. Function `fnc1()` directly calls procedure `prc2()`. Function
    `fnc2()` directly calls procedures `prc2()` and `prc3()` as well as function `fnc3()`.
    The leaf procedures and functions in this call tree are `prc1()`, `prc2()`, `fnc3()`,
    and `prc3()`, which do not call any other procedures or functions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig01.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-1: A call tree*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'Working with leaf procedures and functions has an advantage: they do not need
    to save parameters passed to them in registers or preserve the values of local
    variables they maintain in registers. For example, if `main()` passes two parameters
    to `fnc1()` in the EAX and EDX registers, and `fnc1()` passes a different pair
    of parameters to `prc2()` in EAX and EDX, then `fnc1()` must first save the values
    it found in EAX and EDX before calling `prc2()`. The `prc2()` procedure, on the
    other hand, doesn’t have to save the values in EAX and EDX prior to some procedure
    or function call, because it doesn’t make such calls. In a similar vein, if `fnc1()`
    allocates any local variables in registers, then it will need to preserve those
    registers across a call to `prc2()`, because `prc2()` can use the registers for
    its own purposes. By contrast, if `prc2()` uses a register for a local variable,
    it never has to preserve the variable’s value, because it never calls any subroutines.
    Therefore, good compilers tend to generate better code for leaf procedures and
    functions because they don’t have to preserve the register values.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: One way to *flatten* the call tree is to take the code associated with procedures
    and functions in interior nodes and inline it into functions higher in the call
    tree. In [Figure 15-1](ch15.xhtml#ch15fig1), for example, if it is practical to
    move the code for `fnc1()` into `main()`, you don’t need to save and restore registers
    (among other operations). However, be sure you’re not sacrificing readability
    and maintainability when flattening the call tree. You want to avoid writing procedures
    and functions that simply call other procedures and functions without doing any
    work on their own, but you don’t want to destroy the modularity of your application’s
    design by expanding function and procedure calls throughout your code.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ve already seen that having a leaf function is handy when you’re using
    a RISC processor, like the PowerPC or ARM, that uses a “branch and link” instruction
    to make a subroutine call. The PowerPC and ARM LINK registers are good examples
    of registers that you have to preserve across procedure calls. Because a leaf
    procedure does not (normally) modify the value in the LINK register, no extra
    code is necessary in a leaf procedure to preserve that register’s value. To see
    the benefits of calling leaf functions on a RISC CPU, consider the following C
    code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'GCC emits the following PowerPC assembly code:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: There’s an important difference between the implementations of the `f()` and
    `g()` functions in this PowerPC code—`f()` has to preserve the value of the LINK
    register, whereas `g()` does not. Not only does this involve extra instructions,
    but it also involves accessing memory, which can be slow.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage to using leaf procedures, which isn’t obvious from the call
    tree, is that constructing their activation record requires less work. On the
    80x86, for example, a good compiler doesn’t have to preserve the value of the
    EBP register, load EBP with the activation record address, and then restore the
    original value by accessing local objects via the stack pointer register (ESP).
    On RISC processors, which maintain the stack manually, the savings can be significant.
    For such procedures, the overhead of the procedure call and return and activation
    record maintenance is greater than the actual work done by the procedure. Therefore,
    eliminating the activation record maintenance code could nearly double the speed
    of the procedure. For these and other reasons, you should try to keep your call
    trees as shallow as possible. The more leaf procedures your program uses, the
    more efficient it may become when you compile it with a decent compiler.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '**15.3 Macros and Inline Functions**'
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One offshoot of the structured programming revolution was that computer programmers
    were taught to write small, modular, and logically coherent functions.^([2](footnotes.xhtml#ch15fn2))
    A function that is logically coherent does one thing well. All of the statements
    in such a procedure or function are dedicated to doing the task at hand without
    producing any side computations or doing any extraneous operations. Years of software
    engineering research indicate that decomposing a problem into small components,
    and then implementing those, produces programs that are easier to read, maintain,
    and modify. Unfortunately, it’s easy to get carried away with this process and
    produce functions like the following Pascal example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'On the 80x86, it would probably take about three instructions to compute the
    sum of two values and store that sum into a memory variable. For example:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Contrast this with the code necessary to simply *call* the function `sum()`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Within the procedure `sum` (assuming a mediocre compiler), you might expect
    to find code like the following HLA sequence:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, using a function takes three times as many instructions to compute
    the sum of these two objects as the straight-line (no function call) code. Worse
    still, these nine instructions are generally slower than the three that make up
    the inline code. The inline code could run 5 to 10 times faster than the code
    with the function call.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，使用一个函数来计算这两个对象的和，比直接的（没有函数调用的）代码需要更多三倍的指令。更糟糕的是，这九条指令通常比构成内联代码的三条指令要慢。内联代码的运行速度可能比带有函数调用的代码快
    5 到 10 倍。
- en: The one redeeming quality about the overhead associated with a function or procedure
    call is that it’s fixed. It takes the same number of instructions to set up the
    parameters and the activation record whether the procedure or function body contains
    1 or 1,000 machine instructions. Although the overhead of a procedure call is
    huge when the procedure’s body is small, it’s inconsequential when the procedure’s
    body is large. Therefore, to reduce the impact of procedure/function call overhead
    in your programs, try to place larger procedures and functions and write shorter
    sequences as inline code.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 关于函数或过程调用的开销，唯一值得肯定的优点是它是固定的。不管过程或函数体包含 1 条还是 1,000 条机器指令，设置参数和激活记录所需的指令数是相同的。尽管当过程体较小时，过程调用的开销很大，但当过程体较大时，这种开销几乎可以忽略不计。因此，为了减少程序中过程/函数调用的开销影响，尽量将较大的过程和函数放置为内联代码，并编写较短的序列。
- en: Finding the optimum balance between the benefits of modular structure and the
    cost of too-frequent procedure calls can be difficult. Unfortunately, good program
    design often prevents us from increasing the size of our procedures and functions
    enough that the overhead of the call and return becomes insignificant. Sure, we
    could combine several functions and procedure calls into a single procedure or
    function, but this would violate several rules of programming style, and great
    code usually avoids such tactics. (One problem with the resulting programs is
    that few people can figure out how they work in order to optimize them.) However,
    if you can’t sufficiently lower the overhead of a procedure’s body by increasing
    the procedure’s size, you can still improve overall performance by reducing the
    overhead in other ways. As you’ve seen, one option is to use leaf procedures and
    functions. Good compilers emit fewer instructions for leaf nodes in the call tree,
    thereby reducing the call/return overhead. However, if the procedure’s body is
    short, you need a way to completely eliminate the procedure call/return overhead.
    Some languages accomplish this with *macros*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在模块化结构的好处与过于频繁的过程调用成本之间找到最佳平衡可能很困难。不幸的是，良好的程序设计通常会阻止我们将过程和函数的大小增加到足以使调用和返回的开销变得微不足道的程度。确实，我们可以将多个函数和过程调用合并为一个过程或函数，但这会违反几个编程风格的规则，而且优秀的代码通常避免使用这种策略。（这样产生的程序的一个问题是，很少有人能够弄清楚它们是如何工作的，从而优化它们。）然而，如果通过增加过程的大小不能足够降低过程体的开销，你仍然可以通过其他方式减少开销，从而提高整体性能。正如你所看到的，一个选项是使用叶子过程和函数。优秀的编译器对调用树中的叶节点生成更少的指令，从而减少调用/返回的开销。然而，如果过程体很短，你需要一种方法来完全消除过程调用/返回的开销。有些语言通过*宏*实现这一点。
- en: 'A *pure* macro expands the body of a procedure or function in place of its
    invocation. Because there’s no call/return to code elsewhere in the program, a
    macro expansion avoids the overhead associated with those instructions. Furthermore,
    macros also save considerable expense by using textual substitution for parameters
    rather than pushing the parameter data onto the stack or moving it into registers.
    The drawback to a macro is that the compiler expands the macro’s body for each
    invocation of the macro. If the macro body is large and you invoke it in many
    different places, the executable program can grow by a fair amount. Macros represent
    the classic time/space tradeoff: faster code at the expense of greater size. For
    this reason, you should use macros only to replace procedures and functions that
    have a small number of statements (say, between one and five), except in some
    rare cases where speed is paramount.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: A few languages (like C/C++) provide *inline* functions and procedures, which
    are a cross between a true function (or procedure) and a pure macro. Most languages
    that support inline functions and procedures do not guarantee that the compiler
    will expand the code inline. *Inline expansion*, or a call to an actual function
    in memory, is done at the compiler’s discretion. Most compilers won’t expand an
    inline function if its body is too large or if it has an excessive number of parameters.
    Furthermore, unlike pure macros, which don’t have any associated procedure call
    overhead, inline functions may still need to build an activation record in order
    to handle local variables, temporaries, and other requirements. Thus, even if
    the compiler does expand such a function inline, there may still be some overhead
    that you wouldn’t get with a pure macro.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the result of function inlining, consider the following C source file
    prepared for compilation by Microsoft Visual C++:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here’s the MASM-compatible assembly language code that MSVC emits when you
    specify a C compilation (versus a C++ compilation, which produces messier output):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you can see in this assembly output, there are no function calls to the `inlineFunc()`
    function. Instead, the compiler expanded this function in place in the `main()`
    function, at the point where the main program calls it. Although the `ilf2()`
    function was also declared inline, the compiler refused to expand it inline and
    treated it like a normal function (probably because of its size).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '**15.4 Passing Parameters to a Function or Procedure**'
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The number and type of parameters can also have a big impact on the efficiency
    of the code a compiler generates for your procedures and functions. Simply put,
    the more parameter data you pass, the more expensive the procedure or function
    call becomes. Often, programmers call generic functions (or design generic functions)
    that require you to pass several optional parameters whose values the function
    won’t use. This scheme can make functions more generally applicable to different
    applications, but—as you’ll see in this section—there’s a cost associated with
    that generality, so you might want to consider using a version of the function
    specific to your application if space or speed is an issue.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter-passing mechanism (for example, pass-by-reference or pass-by-value)
    also has an impact on the overhead associated with a procedure call and return.
    Some languages allow you to pass large data objects by value. (Pascal lets you
    pass strings, arrays, and records by value, and C/C++ allows you to pass structures
    by value; other languages vary depending on their design.) Whenever you pass a
    large data object by value, the compiler must emit machine code that makes a copy
    of that data into the procedure’s activation record. This can be time-consuming
    (especially when copying large arrays or structures). Furthermore, large objects
    probably won’t fit in the CPU’s register set, so accessing such data within a
    procedure or function is expensive. It’s usually more efficient to pass large
    data objects such as arrays and structures by reference than by value. The extra
    cost of accessing the data indirectly is usually saved many times over by not
    having to copy the data into the activation record. Consider the following C code,
    which passes a large structure by value to a C function:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here’s the PowerPC code that GCC emits:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As you can see, the call to function `f()` calls `memcpy` to transfer a copy
    of the data from the `main()` function’s local array to the `f()` function’s activation
    record. Again, copying memory is a slow process, and this code amply demonstrates
    that you should avoid passing large objects by value. Consider the same code when
    you pass the structure by reference:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here’s the conversion of this C source code to 32-bit ARM assembly by GCC:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Depending on your CPU and compiler, it may be slightly more efficient to pass
    small (scalar) data objects by value rather than by reference. For example, if
    you’re using an 80x86 compiler that passes parameters on the stack, you’ll need
    two instructions to pass a memory object by reference, but only a single instruction
    to pass that same object by value. So, although trying to pass large objects by
    reference is a good idea, the reverse is generally true for small objects. However,
    this is not a hard and fast rule; its validity varies based on the CPU and compiler
    you’re using.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'Some programmers may feel that it’s more efficient to pass data to a procedure
    or function via global variables. After all, if the data is already sitting in
    a global variable that’s accessible to the procedure or function, a call to that
    procedure or function won’t require any extra instructions to pass the data to
    the subroutine, therefore reducing the call overhead. While this seems like a
    big win, keep in mind that compilers have a difficult time optimizing programs
    that make excessive use of global variables. Although using globals may reduce
    the function/procedure call overhead, it may also prevent the compiler from handling
    other optimizations that would have been otherwise possible. Here’s a simple example
    using Microsoft Visual C++ that demonstrates this problem:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here’s the 32-bit MASM source code (with manual annotations) that the MSVC++
    compiler generates for this code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As you can see, the compiler’s ability to optimize around global variables can
    be easily thwarted by the presence of some seemingly unrelated code. In this example,
    the compiler cannot determine that the call to the external `geti()` function
    doesn’t modify the value of the `globalValue` variable. Therefore, the compiler
    can’t assume that `globalValue` still has the value `1` when it computes the inline
    function result for `usesGlobal()`. Exercise extreme caution when using global
    variables to communicate information between a procedure or function and its caller.
    Code that’s unrelated to the task at hand (such as the call to `geti()`, which
    probably doesn’t affect `globalValue`’s value) can prevent the compiler from optimizing
    code that uses global variables.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5 Activation Records and the Stack**'
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Because of how a stack works, the last procedure activation record the software
    creates will be the first activation record that the system deallocates. Since
    activation records hold procedure parameters and local variables, a *last-in,
    first-out (LIFO)* organization is a very intuitive way of implementing activation
    records. To see how it works, consider the following (trivial) Pascal program:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[Figure 15-2](ch15.xhtml#ch15fig2) shows the stack layout as this program executes.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: When the program begins execution, it first creates an activation record for
    the main program. The main program calls the `A` procedure (①). Upon entry into
    the `A` procedure, the code completes the construction of the activation record
    for `A`, effectively pushing it onto the stack. Once inside procedure `A`, the
    code calls procedure `B` (②). Note that `A` is still active while the code calls
    `B`, so `A`’s activation record remains on the stack. Upon entry into `B`, the
    system builds `B`’s activation record and pushes it onto the top of the stack
    (③). Once inside `B`, the code calls procedure `C`, and `C` builds its activation
    record on the stack and arrives at the comment `(* Stack snapshot here *)` (④).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig02.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-2: Stack layout after three nested procedure calls*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Because procedures keep their local variables and parameter values in their
    activation record, the lifetime of these variables extends from the point the
    system first creates the activation record until the system deallocates it when
    the procedure returns to its caller. In [Figure 15-2](ch15.xhtml#ch15fig2), notice
    that `A`’s activation record remains on the stack during the execution of the
    `B` and `C` procedures. Therefore, the lifetime of `A`’s parameters and local
    variables completely brackets the lifetimes of `B`’s and `C`’s activation records.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Now consider the following C/C++ code with a recursive function:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This program calls the `recursive()` function three times before it begins returning
    (the main program calls `recursive()` once with the parameter value `2`, and `recursive()`
    calls itself twice with the parameter values `1` and `0`). Because each recursive
    call to `recursive()` pushes another activation record before the current call
    returns, when this program finally hits the `if` statement in this code with `cnt`
    equal to `0`, the stack looks something like [Figure 15-3](ch15.xhtml#ch15fig3).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig03.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-3: Stack layout after three recursive procedure calls*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Because each procedure invocation has a separate activation record, each activation
    of the procedure will have its own copy of the parameters and local variables.
    While the code for a procedure or function is executing, it will access only those
    local variables and parameters in the activation record it has most recently created,^([3](footnotes.xhtml#ch15fn3))
    thereby preserving the values from previous calls.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.1 Breaking Down the Activation Record**'
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now that you’ve seen how procedures manipulate activation records on the stack,
    it’s time to take a look at the internal composition of a typical activation record.
    In this section we’ll use a typical activation record layout that you’ll see when
    executing code on an 80x86\. Although different languages, different compilers,
    and different CPUs lay out the activation record differently, these differences,
    if they exist at all, will be minor.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'The 80x86 maintains the stack and activation records using two registers: ESP/RSP
    (the stack pointer) and EBP/RBP (the frame pointer, which Intel calls the *base
    pointer*). The ESP/RSP register points at the current top of stack, and the EBP
    register points at the base address of an activation record.^([4](footnotes.xhtml#ch15fn4))
    A procedure can access objects within its activation record by using the indexed
    addressing mode (see “Indexed Addressing Mode” on [page 34](ch03.xhtml#page_34))
    and supplying a positive or negative offset from the value in the EBP/RBP register.
    Generally, a procedure allocates memory storage for local variables at negative
    offsets from EBP/RBP’s value, and for parameters at positive offsets from EBP/RBP.
    Consider the following Pascal procedure, which has both parameters and local variables:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[Figure 15-4](ch15.xhtml#ch15fig4) shows a typical activation record for this
    Pascal procedure (remember that the stack grows toward lower memory on the 32-bit
    80x86).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig04.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-4: A typical activation record*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: When you see the term *base* associated with a memory object, you might think
    that the base address is the lowest address of that object in memory. However,
    there’s no such requirement. The base address is simply the address in memory
    on which you base the offsets to particular fields of that object. As this activation
    record demonstrates, 80x86 activation record base addresses are actually in the
    middle of the record.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'The activation record is constructed in two phases. The first phase begins
    when the code calling the procedure pushes the parameters for the call onto the
    stack. For example, consider the following call to `HasBoth()` in the previous
    example:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here’s the HLA/x86 assembly code that might correspond to this call:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The three `push` instructions in this code sequence build the first three double
    words of the activation record, and the `call` instruction pushes a *return address*
    onto the stack, creating the fourth double word in the activation record. After
    the call, execution continues in the `HasBoth()` procedure itself, where the program
    continues to build the activation record.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: The first few instructions of the `HasBoth()` procedure are responsible for
    finishing the construction of the activation record. Immediately upon entry into
    `HasBoth()`, the stack takes the form shown in [Figure 15-5](ch15.xhtml#ch15fig5).^([5](footnotes.xhtml#ch15fn5))
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig05.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-5: Activation record upon entry to HasBoth()*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing the procedure’s code should do is to preserve the value in
    the 80x86 EBP register. On entry, EBP probably points at the base address of the
    caller’s activation record. On exit from `HasBoth()`, EBP needs to contain its
    original value. Therefore, upon entry, `HasBoth()` needs to push the current value
    of EBP on the stack in order to preserve EBP’s value. Next, the `HasBoth()` procedure
    needs to change EBP so that it points at the base address of the `HasBoth()` activation
    record. The following HLA/x86 code takes care of these two operations:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, the code at the beginning of `HasBoth()` needs to allocate storage
    for its local (automatic) variables. As you saw in [Figure 15-4](ch15.xhtml#ch15fig4),
    those variables sit below the frame pointer in the activation record. To prevent
    future pushes from wiping out the values in those local variables, the code has
    to set ESP to the address of the last double word of local variables in the activation
    record. To accomplish this, it simply subtracts the number of bytes of local variables
    from ESP via the following machine instruction:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The *standard entry sequence* for a procedure like `HasBoth()` consists of
    the three machine instructions just considered—`push(ebp);`, `mov(esp, ebp);`,
    and `sub(12, esp);`—which complete the construction of the activation record inside
    the procedure. Just before returning, the Pascal procedure is responsible for
    deallocating the storage associated with the activation record. The *standard
    exit sequence* usually takes the following form (in HLA) for a Pascal procedure:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The first instruction deallocates storage for the local variables shown in
    [Figure 15-4](ch15.xhtml#ch15fig4). Note that EBP is pointing at the old value
    of EBP; this value is stored at the memory address just above all the local variables.
    By copying the value in EBP to ESP, we move the stack pointer past all the local
    variables, effectively deallocating them. Now, the stack pointer points at the
    old value of EBP on the stack; therefore, the `pop` instruction in this sequence
    restores EBP’s original value and leaves ESP pointing at the return address on
    the stack. The `ret` instruction in the standard exit sequence does two things:
    it pops the return address from the stack (and, of course, transfers control to
    this address), and it removes 12 bytes of parameters from the stack. Because `HasBoth()`
    has three double-word parameters, popping 12 bytes from the stack removes those
    parameters.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.2 Assigning Offsets to Local Variables**'
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This `HasBoth()` example allocates local (automatic) variables in the order
    the compiler encounters them. A typical compiler maintains a *current offset*
    (the initial value of which will be 0) into the activation record for local variables.
    Whenever the compiler encounters a local variable, it subtracts the variable’s
    size from the current offset and then uses the result as the offset of the local
    variable (from EBP/RBP) in the activation record. For example, upon encountering
    the declaration for variable `a`, the compiler subtracts the size of `a` (4 bytes,
    assuming `a` is a 32-bit integer) from the current offset (0) and uses the result
    (–4) as the offset for `a`. Next, the compiler encounters variable `r` (which
    is also 4 bytes), sets the current offset to –8, and assigns this offset to `r`.
    This process repeats for each of the local variables in the procedure.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Although this is a typical way that compilers assign offsets to local variables,
    most languages give compiler implementers free rein to allocate local objects
    as they see fit. A compiler can rearrange the objects in the activation record
    if doing so is more convenient. This means you should avoid designing algorithms
    that depend on the previously mentioned allocation scheme, because some compilers
    do it differently.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Many compilers try to ensure that all local variables you declare have an offset
    that is a multiple of the object’s size. For example, suppose you have the following
    two declarations in a C function:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Normally, you’d expect that the compiler would attach an offset like –1 to the
    `c` variable and –5 to the (4-byte) `int` variable `i`. However, some CPUs (such
    as RISC CPUs) require the compiler to allocate double-word objects on a double-word
    boundary. Even on CPUs that don’t require this (for example, the 80x86), it may
    be faster to access a double-word variable if the compiler aligns it on a double-word
    boundary. For this reason (and as previous chapters have described), many compilers
    automatically add padding bytes between local variables so that each variable
    resides at a *natural* offset in the activation record. In general, bytes may
    appear at any offset, words are happiest on even address boundaries, and double
    words should have a memory address that is a multiple of 4.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Although an optimizing compiler might automatically handle this alignment for
    you, that comes with a cost—those extra padding bytes. As explained earlier, compilers
    are usually free to rearrange the variables in an activation record, but they
    don’t always do so. Therefore, if you intertwine the definitions for several byte,
    word, double-word, and other-sized objects in your local variable declarations,
    the compiler may wind up inserting several bytes of padding into the activation
    record. You can minimize this problem by attempting to group as many like-sized
    objects together as is reasonable in your procedures and functions. Consider the
    following C/C++ code on a 32-bit machine:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'An optimizing compiler may elect to insert 3 bytes of padding between each
    of these character variables and the (4-byte) integer variable that immediately
    follows. This means that the preceding code will have about 12 bytes of wasted
    space (3 bytes for each of the character variables). Now consider the following
    declarations in the same C code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In this example, the compiler won’t emit any extra padding bytes to the code.
    Why? Because characters (being 1 byte each) may begin at any address in memory.^([6](footnotes.xhtml#ch15fn6))
    Therefore, the compiler can place these character variables at offsets –1, –2,
    –3, and –4 within the activation record. Because the last character variable appears
    at an address that is a multiple of 4, the compiler doesn’t need to insert any
    padding bytes between `c3` and `i0` (`i0` will naturally appear at offset –8 in
    the preceding declarations).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, arranging your declarations so that all like-sized objects are
    next to one another can help your compiler produce better code. Don’t take this
    suggestion to an extreme, though. If a rearrangement would make your program more
    difficult to read or maintain, you should carefully consider whether it’s worthwhile
    in your program.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.3 Associating Offsets with Parameters**'
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As noted, compilers are given considerable leeway with respect to how they assign
    offsets to local (automatic) variables within a procedure. As long as the compiler
    uses these offsets consistently, the exact allocation algorithm it applies is
    almost irrelevant; in fact, it could use a different allocation scheme in different
    procedures of the same program. However, a compiler *doesn’t* have free rein when
    assigning offsets to parameters. It has to live with certain restrictions, because
    other code outside the procedure accesses those parameters. Specifically, the
    procedure and the calling code must agree on the layout of the parameters in the
    activation record so the calling code can build the parameter list. Note that
    the calling code might not be in the same source file, or even in the same programming
    language, as the procedure. To ensure interoperability between a procedure and
    whatever code calls that procedure, then, compilers must adhere to certain *calling
    conventions*. This section will explore the three common calling conventions for
    Pascal/Delphi and C/C++.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.3.1 The Pascal Calling Convention**'
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In Pascal (including Delphi) the standard parameter-passing convention is to
    push the parameters on the stack in the order of their appearance in the parameter
    list. Consider the following call to the `HasBoth()` procedure from the earlier
    example:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following assembly code implements this call:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: When assigning offsets to a procedure’s formal parameters, the compiler assigns
    the highest offset to the first parameter and the lowest offset to the last parameter.
    Because the old value of EBP is at offset 0 in the activation record and the return
    address is at offset 4, the last parameter in the activation record (when using
    the Pascal calling convention on the 80x86 CPU) will reside at offset 8 from EBP.
    Looking back at [Figure 15-4](ch15.xhtml#ch15fig4), you can see that parameter
    `k` is at offset +8, parameter `j` is at offset +12, and parameter `i` (the first
    parameter) is at offset +16 in the activation record.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The Pascal calling convention also stipulates that it is the procedure’s responsibility
    to remove the parameters the caller pushes when the procedure returns to its caller.
    As you saw earlier, the 80x86 CPU provides a variant of the `ret` instruction
    that lets you specify how many bytes of parameters to remove from the stack upon
    return. Therefore, a procedure that uses the Pascal calling convention will typically
    supply the number of parameter bytes as an operand to the `ret` instruction when
    returning to its caller.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.3.2 The C Calling Convention**'
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The C/C++/Java languages employ another very popular calling convention, generally
    known as the *cdecl calling convention* (or, simply, the *C calling convention*).
    There are two major differences between the C and Pascal calling conventions.
    First, calls to functions in C must push their parameters on the stack in the
    reverse order. That is, the first parameter must appear at the lowest address
    on the stack (assuming the stack grows downward), and the last parameter must
    appear at the highest address in memory. The second difference is that C requires
    the caller, rather than the function, to remove all parameters from the stack.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following version of `HasBoth()` written in C instead of Pascal:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[Figure 15-6](ch15.xhtml#ch15fig6) provides the layout for a typical `HasBoth`
    activation record (written in C on a 32-bit 80x86 processor).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig06.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-6: HasBoth() activation record in C*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Looking closely, you’ll see the difference between this figure and [Figure 15-4](ch15.xhtml#ch15fig4).
    The positions of the `i` and `k` variables are reversed in this activation record
    (it’s only a coincidence that `j` appears at the same offset in both).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the C calling convention reverses the order of the parameters and it’s
    the caller’s responsibility to remove all parameter values from the stack, the
    calling sequence for `HasBoth()` is a little different in C than in Pascal. Consider
    the following call to `HasBoth()`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Here’s the HLA assembly code for this call:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As a result of using the C calling convention, this code differs in two ways
    from the assembly code for the Pascal implementation. First, this example pushes
    the values of the actual parameters in the opposite order of the Pascal code;
    that is, it first computes `y+2` and pushes that value, then it pushes `x`, and
    finally it pushes the value `5`. The second difference is the inclusion of the
    `add(12,esp);` instruction immediately after the call. This instruction removes
    12 bytes of parameters from the stack upon return. The return from `HasBoth()`
    will use only the `ret` instruction, not the `ret n` instruction.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.3.3 Conventions for Passing Parameters in Registers**'
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As you’ve seen in these examples, passing parameters on the stack between two
    procedures or functions requires a fair amount of code. Good assembly language
    programmers have long known that it’s better to pass parameters in registers.
    Therefore, several 80x86 compilers following Intel’s ABI (application binary interface)
    rules may attempt to pass as many as three parameters in the EAX, EDX, and ECX
    registers.^([7](footnotes.xhtml#ch15fn7)) Most RISC processors specifically set
    aside a set of registers for passing parameters between functions and procedures.
    See “Registers to the Rescue” on [page 585](ch15.xhtml#page_585) for more information.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Most CPUs require that the stack pointer remain aligned on some reasonable
    boundary (for example, a double-word boundary), and CPUs that don’t absolutely
    require this may still benefit from it. Furthermore, many CPUs (the 80x86 included)
    can’t easily push certain small-sized objects, like bytes, onto the stack. Therefore,
    most compilers reserve a minimum number of bytes for a parameter (typically 4),
    regardless of its actual size. As an example, consider the following HLA procedure
    fragment:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The activation record for this procedure appears in [Figure 15-7](ch15.xhtml#ch15fig7).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig07.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-7: OneByteParm() activation record*'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the HLA compiler reserves 4 bytes for the `b` parameter even
    though `b` is only a single-byte variable. This extra padding ensures that the
    ESP register will remain aligned on a double-word boundary.^([8](footnotes.xhtml#ch15fn8))
    We can easily push the value of `b` onto the stack in the code that calls `OneByteParm()`
    using a 4-byte `push` instruction.^([9](footnotes.xhtml#ch15fn9))
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Even if your program could access the extra bytes of padding associated with
    the `b` parameter, doing so is never a good idea. Unless you’ve explicitly pushed
    the parameter onto the stack (for example, using assembly language code), there’s
    no guarantee about the data values that appear in the padding bytes. In particular,
    they may not contain 0\. Nor should your code assume that the padding is present
    or that the compiler pads such variables out to 4 bytes. Some 16-bit processors
    may require only a single byte of padding. Some 64-bit processors may require
    7 bytes of padding. Some compilers on the 80x86 may use 1 byte of padding, while
    others use 3 bytes. Unless you’re willing to live with code that only one compiler
    can compile (and code that could break when the next version of the compiler comes
    along), it’s best to ignore these padding bytes.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.4 Accessing Parameters and Local Variables**'
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once a subroutine sets up the activation record, accessing local (automatic)
    variables and parameters is easy. The machine code simply uses the indexed addressing
    mode to access such objects. Consider again the activation record in [Figure 15-4](ch15.xhtml#ch15fig4).
    The variables in the `HasBoth()` procedure have the offsets found in [Table 15-1](ch15.xhtml#ch15tab1).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 15-1:** Offsets to Local Variables and Parameters in `HasBoth()` (Pascal
    Version)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '| **Variable** | **Offset** | **Addressing mode example** |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: '| `i` | +16 | `mov( [ebp+16], eax );` |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: '| `j` | +12 | `mov( [ebp+12], eax );` |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '| `k` | +8 | `mov( [ebp+8], eax );` |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| `a` | –4 | `mov( [ebp-4], eax );` |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '| `r` | –8 | `mov( [ebp-8], eax );` |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| `c` | –9 | `mov( [ebp-9], al );` |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| `b` | –10 | `mov( [ebp-10], al );` |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| `w` | –12 | `mov( [ebp-12], ax );` |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: The compiler allocates static local variables in a procedure at a fixed address
    in memory. Static variables do not appear in the activation record, so the CPU
    accesses static objects using the direct addressing mode.^([10](footnotes.xhtml#ch15fn10))
    As [Chapter 3](ch03.xhtml#ch03) discussed, in 80x86 assembly language instructions
    that use the direct addressing mode need to encode the full 32-bit address as
    part of the machine instruction. Therefore, instructions that use the direct addressing
    mode are usually at least 5 bytes long (and often longer). On the 80x86, if the
    offset from EBP is –128 through +127, then a compiler can encode an instruction
    of the form `[ebp + constant`] in as few as 2 or 3 bytes. Such instructions will
    be more efficient than those that encode a full 32-bit address. The same principle
    applies on other processors, even if those CPUs provide different addressing modes,
    address sizes, and so on. Specifically, accessing local variables whose offset
    is relatively small is generally more efficient than accessing static variables
    or variables with larger offsets.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Because most compilers allocate offsets for local (automatic) variables as the
    compiler encounters them, the first 128 bytes of local variables will be the ones
    with the shortest offsets (at least, on the 80x86; this value may be different
    for other processors).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following two sets of local variable declarations (presumably
    appearing with some C function):'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Here’s a second version of these declarations:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Although these two declaration sections are semantically identical, there is
    a big difference in the code a compiler for the 32-bit 80x86 generates to access
    these variables. In the first declaration, the variable `string` appears at offset
    –256 within the activation record, `i` appears at offset –260, `j` appears at
    offset –264, and `c` appears at offset –265\. Because these offsets are outside
    the range –128 through +127, the compiler will have to emit machine instructions
    that encode a 4-byte offset constant rather than a 1-byte constant. Accordingly,
    the code associated with these declarations will be larger and may run slower.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Now consider the second declaration. In this example the programmer declares
    the scalar (non-array) objects first. Therefore, the variables have the following
    offsets: `i` at –4, `j` at –8, `c` at –9, and `string` at –265\. This turns out
    to be the optimal configuration for these variables (`i`, `j`, and `c` will use
    1-byte offsets; `string` will require a 4-byte offset).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'This example demonstrates another rule you should try to follow when declaring
    local (automatic) variables: declare smaller, scalar objects first within a procedure,
    followed by all the arrays, structures/records, and other large objects.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: As explained in “Associating Offsets with Parameters” on [page 570](ch15.xhtml#page_570),
    if you declare several local objects with differing sizes adjacent to one another,
    the compiler may need to insert padding bytes to keep the larger objects aligned
    at an appropriate memory address. While worrying about a few wasted bytes here
    and there may seem ridiculous on machines with a gigabyte (or more) of RAM, those
    few padding bytes could be just enough to push the offsets of certain local variables
    beyond –128, causing the compiler to emit 4-byte offsets rather than 1-byte offsets
    for those variables. This is one more reason you should try to declare like-sized
    local variables adjacent to one another.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'On RISC processors, such as the PowerPC or ARM, the range of possible offsets
    is usually much greater than ±128\. This is a good thing, because once you exceed
    the range of the activation record offset that a RISC CPU can encode directly
    into an instruction, parameter and local variable access gets very expensive.
    Consider the following C program:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here’s the PowerPC assembly output from GCC:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: This compilation was done under GCC without optimization to show what happens
    when your activation record grows to the point you can no longer encode activation
    record offsets into the instruction.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'To encode the address of `e`, whose offset is too large, we need these three
    instructions:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'instead of a single instruction that stores `R0` into the `a` variable, such
    as:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: While two extra instructions in a program of this size might seem insignificant,
    keep in mind that the compiler will generate these extra instructions for each
    such access. If you frequently access a local variable with a huge offset, the
    compiler may generate a significant number of extra instructions throughout your
    function or procedure.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, in a standard application running on a RISC, this problem seldom
    occurs because we rarely allocate local storage beyond the range that a single
    instruction can encode. Also, RISC compilers generally allocate scalar (non-array/non-structure)
    objects in registers rather than blindly allocating them at the next memory address
    in the activation record. For example, if you turn on GCC’s optimization with
    the `-O2` command-line switch, you’ll get the following PowerPC output:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: One thing that you’ll notice in this version with optimization enabled is that
    GCC did not allocate variables in the activation record as they were encountered.
    Instead, it placed most of the objects in registers (even array elements). Keep
    in mind that an optimizing compiler may very well rearrange all the local variables
    you declare.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'The ARM processor has similar limitations based on the size of the instruction
    opcode (32 bits). Here’s the (unoptimized) ARM output from GCC:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: While this is arguably better than the PowerPC code, there’s still considerable
    ugliness in the address computations because the ARM CPU cannot encode 32-bit
    constants as part of the instruction opcode. To understand why GCC emits such
    bizarre constants to compute offsets into the activation record, see the discussion
    of the ARM immediate operands in the section “The Immediate Addressing Mode” in
    Appendix C online.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'If you find the optimized PowerPC or ARM code a bit hard to follow, consider
    the following 80x86 GCC output for the same C program:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Of course, the 80x86 doesn’t have as many registers to use for passing parameters
    and holding local variables, so the 80x86 code has to allocate more locals in
    the activation record. Also, the 80x86 provides an offset range of –128 to +127
    bytes only around the EBP register, so a larger number of instructions have to
    use the 4-byte offset rather than the 1-byte offset. Fortunately, the 80x86 does
    allow you to encode a full 32-bit address as part of the instructions that access
    memory, so you don’t have to execute multiple instructions in order to access
    a variable stored a long distance away from where EBP points in the stack frame.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.5 Registers to the Rescue**'
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As the examples in the previous section demonstrate, RISC code suffers greatly
    when it has to deal with parameters and local variables whose offsets are not
    easy to represent within the confines of the instruction opcode. In real code,
    however, the situation is not so dire. Compilers are smart enough to use machine
    registers to pass parameters and hold local variables providing immediate access
    to those values. This dramatically reduces the number of instructions in typical
    functions.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Consider the (register-starved) 32-bit 80x86 CPU. As there are only eight general-purpose
    registers, two of which (ESP and EBP) have special purposes that limit their use,
    there aren’t a lot of registers available for passing parameters or holding local
    variables. Typical C compilers use EAX, ECX, and EDX to pass up to three parameters
    to a function. Functions return their result in the EAX register. The function
    must preserve the values of any other registers (EBX, ESI, EDI, and EBP) it uses.
    It’s fortunate that memory access to local variables and parameters inside the
    function is very efficient—given the limited register set, the 32-bit 80x86 will
    need to use memory often for this purpose.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: For most applications, the largest architectural improvement to the 64-bit x86-64
    over the 32-bit 80x86 was not 64-bit registers (or even addresses), but that the
    x86-64 added eight new general-purpose registers and eight new XMM registers that
    compilers could use for passing parameters and holding local variables. The Intel/AMD
    ABI for the x86-64 allows a compiler to pass up to six different arguments in
    registers to a function (without the caller explicitly saving those register values
    before using them). [Table 15-2](ch15.xhtml#ch15tab2) lists the available registers.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 15-2:** Ix86-64 Argument Passing via Registers'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '| **Register** | **Usage** |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: '| RDI | 1st argument |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| RSI | 2nd argument |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| RDX | 3rd argument |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| RCX | 4th argument |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| R8 | 5th argument |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| R9 | 6th argument |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| XMM0–XMM7 | Used to pass floating-point arguments |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| R10 | Can be used to pass static chain pointer |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| RAX | Used to pass argument count if there are a variable number of parameters
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: The 32-bit ARM (A32) ABI specifies up to four arguments appearing in registers
    R0 through R3\. As the A64 architecture has twice as many registers (32), the
    A64 ABI is a bit more generous, passing up to eight 64-bit integer/pointer arguments
    in R0 through R7 and up to eight additional floating-point parameters in V0 through
    V7.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: The PowerPC ABI, which has 32 general-purpose registers, sets aside R3 through
    R10 to pass up to eight arguments to a function. It also sets aside the F1 through
    F8 floating-point registers to pass floating-point arguments to a function.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: In addition to setting aside registers to hold function arguments, the various
    ABIs typically define various registers that a function can use to hold local
    variables or temporary values (without explicitly preserving the values held in
    those registers upon entry to the function). For example, the Windows ABI sets
    aside R11, XMM8 through XMM15, MMX0 through MMX7, the FPU registers, and RAX for
    temporary/local use. The ARM A32 ABI sets aside R4 through R8 and R10 through
    R11 for use as locals. The A64 ABI sets aside R9 through R15 for locals and temporaries.
    The PowerPC sets aside R14 through R30 and F14 through F31 for local variables.
    Once a compiler exhausts the registers the ABI defines for argument passing, most
    ABIs expect the calling code to pass additional parameters on the stack. Similarly,
    once a function uses all the registers set aside for local variables, additional
    local variable allocation occurs on the stack.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Of course, a compiler can use other registers for local and temporary values
    as well as those set aside by the CPU’s or OS’s ABI. However, it’s the compiler’s
    responsibility to preserve those register values across the function call.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '*An ABI is a* convention, *not a requirement by the underlying OS or hardware.
    Compiler writers (and assembly language programmers) who stick to a given ABI
    can expect that their object code modules will be able to interact with code written
    in other languages that adhere to that ABI. However, nothing stops a compiler
    writer from using whatever mechanism they choose.*'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.6 Java VM and Microsoft CLR Parameters and Locals**'
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because the Java VM and Microsoft CLR are both virtual stack machines, programs
    compiled to those two architectures always push function arguments onto the stack.
    Beyond that, the two virtual machine architectures diverge. The reason for the
    divergence is that the Java VM’s design supports efficient interpretation of Java
    bytecodes with JIT compilation improving performance as needed. The Microsoft
    CLR, on the other hand, does not support interpretation; instead, the CLR code
    (CIL) design supports efficient JIT compilation to optimized machine code.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The Java VM is a traditional stack architecture, with parameters, locals, and
    temporaries sitting on the stack. Other than the fact that there are no registers
    to use for such objects, Java’s memory organization is very similar to that of
    the 80x86/x86-64, PowerPC, and ARM CPUs. During JIT compilation, it can be difficult
    to figure out which values on the stack can be moved into registers and which
    local variables the Java compiler allocates on the stack can be allocated in registers.
    Optimizing such stack allocations to use registers can be very time-consuming,
    so it’s doubtful that the Java JIT compiler does this while the application is
    running (as doing so would greatly diminish the application’s runtime performance).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft’s CLR operates under a different philosophy. CIL is always JIT-compiled
    into native machine code. Furthermore, Microsoft’s intent is to have the JIT compiler
    produce *optimized* native machine code. While the JIT compiler rarely does as
    good a job as a traditional C/C++ compiler, it generally does a much better job
    than the Java JIT compiler. This is because the Microsoft CLR definition explicitly
    singles out parameter argument and local variable memory accesses. When the JIT
    compiler sees these special instructions, it can allocate those variables to registers
    rather than memory locations. As a result, CLR JIT-compiled code is often shorter
    and faster than Java VM JIT-compiled code (especially on RISC architectures).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '**15.6 Parameter-Passing Mechanisms**'
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most high-level languages provide at least two mechanisms for passing actual
    parameter data to a subroutine: pass-by-value and pass-by-reference.^([11](footnotes.xhtml#ch15fn11))
    In languages like Visual Basic, Pascal, and C++, declaring and using both types
    of parameters is so easy that a programmer may conclude that there’s little difference
    in efficiency between the two mechanisms. That’s a myth this section intends to
    dispel.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '*There are other parameter-passing mechanisms besides pass-by-value and pass-by-reference.
    FORTRAN and HLA, for example, support a mechanism known as pass-by-value/result
    (or pass-by-value/returned). Ada and HLA support pass-by-result. HLA and Algol
    support pass-by-name. This book won’t discuss these alternative parameter-passing
    mechanisms further, because you probably won’t see them very often. If you’d like
    more information, consult a good book on programming language design or the HLA
    documentation.*'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '**15.6.1 Pass-by-Value**'
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pass-by-value is the easiest parameter-passing mechanism to understand. The
    code that calls a procedure makes a copy of the parameter’s data and passes this
    copy to the procedure. For small values, passing a parameter by value generally
    requires little more than a `push` instruction (or, when passing parameters in
    the registers, an instruction that moves the value into a register). Therefore,
    this mechanism is often very efficient.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: One big advantage of pass-by-value parameters is that the CPU treats them just
    like a local variable within the activation record. Because you’ll rarely have
    more than 120 bytes of parameter data that you pass to a procedure, CPUs that
    provide a shortened displacement with the indexed addressing mode will be able
    to access most parameter values using a shorter (and, therefore, more efficient)
    instruction.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: The one case where passing a parameter by value can be inefficient is when you
    need to pass a large data structure, such as an array or record. The calling code
    needs to make a byte-for-byte copy of the actual parameter into the procedure’s
    activation record, as you saw in an earlier example. This can be a very slow process,
    say, if you decide to pass a million-element array to a subroutine by value. Therefore,
    you should avoid passing large objects by value unless absolutely necessary.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '**15.6.2 Pass-by-Reference**'
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The pass-by-reference mechanism passes the address of an object rather than
    its value. This has a couple of distinct advantages over pass-by-value. First,
    regardless of the parameter’s size, pass-by-reference parameters always consume
    the same amount of memory—the size of a pointer (which typically fits in a machine
    register). Second, pass-by-reference parameters allow you to modify the value
    of the actual parameter—which is impossible with pass-by-value parameters.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Pass-by-reference parameters are not without their drawbacks, though. Usually,
    accessing a reference parameter within a procedure is more expensive than accessing
    a value parameter, because the subroutine needs to dereference that address on
    each access of the object. This generally involves loading a register with the
    pointer in order to dereference the pointer using a register indirect addressing
    mode.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the following Pascal code:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Here’s the equivalent HLA/x86 assembly code:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Notice that this code requires two more instructions than a version that uses
    pass-by-value—specifically, the two instructions that load the addresses of `dest`
    and `passedByRef` into the EDX register. In general, only a single instruction
    is needed to access the value of a pass-by-value parameter. However, two instructions
    are needed to manipulate the value of a parameter when you pass it by reference
    (one instruction to fetch the address, and one to manipulate the data at that
    address). So, unless you need the semantics of pass-by-reference, try to use pass-by-value
    instead.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: The issues with pass-by-reference tend to diminish when your CPU has lots of
    available registers that it can use to maintain the pointer values. In that situation,
    the CPU can use a single instruction to fetch or store a value via a pointer maintained
    in the register.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '**15.7 Function Return Values**'
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most HLLs return function results in one or more CPU registers. Exactly which
    register the compiler uses depends on the data type, CPU, and compiler. For the
    most part, however, functions return their results in registers (assuming the
    return data fits in a machine register).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: On the 32-bit 80x86, most functions that return ordinal (integer) values return
    their function results in the AL, AX, or EAX register. Functions that return 64-bit
    values (`long long int`) generally return the function result in the EDX:EAX register
    pair (with EDX containing the HO double word of the 64-bit value). On 64-bit variants
    of the 80x86 family, 64-bit compilers return 64-bit results in the RAX register.
    On the PowerPC, most compilers follow the IBM ABI and return 8-, 16-, and 32-bit
    values in the R3 register. Compilers for the 32-bit versions of the PowerPC return
    64-bit ordinal values in the R4:R3 register pair (with R4 containing the HO word
    of the function result). Presumably, compilers running on 64-bit variants of the
    PowerPC can return 64-bit ordinal results directly in R3.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Generally, compilers return floating-point results in one of the CPU’s (or FPU’s)
    floating-point registers. On 32-bit variants of the 80x86 CPU family, most compilers
    return a floating-point result in the 80-bit ST0 floating-point register. Although
    the 64-bit versions of the 80x86 family also provide the same FPU registers as
    the 32-bit members, some operating systems, such as Windows64, typically use one
    of the SSE registers (XMM0) to return floating-point values. PowerPC systems generally
    return floating-point function results in the F1 floating-point register. Other
    CPUs return floating-point results in comparable locations.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'Some languages allow a function to return a nonscalar (aggregate) value. The
    exact mechanism that compilers use to return large function return results varies
    from compiler to compiler. However, a typical solution is to pass a function the
    address of some storage where the function can place the return result. As an
    example, consider the following short C++ program whose `func()` function returns
    a structure object:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Here’s the PowerPC code that GCC emits for this C++ program:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Here’s the 32-bit 80x86 code that GCC emits for this same function:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The takeaway from these 80x86 and PowerPC examples is that functions returning
    large objects often copy the function result data just prior to returning. This
    extra copying can take considerable time, especially if the return result is large.
    Instead of returning a large structure as a function result, as shown here, it’s
    usually better to explicitly pass a pointer to some destination storage to a function
    that returns a large result and then let the function do whatever copying is necessary.
    This often saves some time and code. Consider the following C code, which implements
    this policy:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Here’s the conversion to 80x86 code by GCC:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: As you can see, this approach is more efficient because the code doesn’t have
    to copy the data twice, once to a local copy of the data and once to the final
    destination variable.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '**15.8 For More Information**'
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Aho, Alfred V., Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman. *Compilers:
    Principles, Techniques, and Tools*. 2nd ed. Essex, UK: Pearson Education Limited,
    1986.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'Barrett, William, and John Couch. *Compiler Construction: Theory and Practice*.
    Chicago: SRA, 1986.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Dershem, Herbert, and Michael Jipping. *Programming Languages, Structures and
    Models*. Belmont, CA: Wadsworth, 1990.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'Duntemann, Jeff. *Assembly Language Step-by-Step*. 3rd ed. Indianapolis: Wiley,
    2009.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Fraser, Christopher, and David Hansen. *A Retargetable C Compiler: Design and
    Implementation*. Boston: Addison-Wesley Professional, 1995.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'Ghezzi, Carlo, and Jehdi Jazayeri. *Programming Language Concepts*. 3rd ed.
    New York: Wiley, 2008.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'Hoxey, Steve, Faraydon Karim, Bill Hay, and Hank Warren, eds. *The PowerPC
    Compiler Writer’s Guide*. Palo Alto, CA: Warthman Associates for IBM, 1996.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyde, Randall. *The Art of Assembly Language*. 2nd ed. San Francisco: No Starch
    Press, 2010.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '———. “Webster: The Place on the Internet to Learn Assembly.” *[http://plantation-productions.com/Webster/index.html](http://plantation-productions.com/Webster/index.html)*.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Intel. “Intel 64 and IA-32 Architectures Software Developer Manuals.” Updated
    November 11, 2019\. *[https://software.intel.com/en-us/articles/intel-sdm](https://software.intel.com/en-us/articles/intel-sdm)*.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'Ledgard, Henry, and Michael Marcotty. *The Programming Language Landscape*.
    Chicago: SRA, 1986.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'Louden, Kenneth C. *Compiler Construction: Principles and Practice*. Boston:
    Cengage, 1997.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'Louden, Kenneth C., and Kenneth A. Lambert. *Programming Languages: Principles
    and Practice*. 3rd ed. Boston: Course Technology, 2012.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'Parsons, Thomas W. *Introduction to Compiler Construction*. New York: W. H.
    Freeman, 1992.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'Pratt, Terrence W., and Marvin V. Zelkowitz. *Programming Languages, Design
    and Implementation*. 4th ed. Upper Saddle River, NJ: Prentice Hall, 2001.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'Sebesta, Robert. *Concepts of Programming Languages*. 11th ed. Boston: Pearson,
    2016.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
