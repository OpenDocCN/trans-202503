- en: '**15**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**FUNCTIONS AND PROCEDURES**'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the beginning of the structured programming revolution in the 1970s,
    subroutines (procedures and functions) have been one of the primary tools software
    engineers use to organize, modularize, and otherwise structure their programs.
    Because procedure and function calls are used so frequently in code, CPU manufacturers
    have attempted to make them as efficient as possible. Nevertheless, these calls—and
    their associated returns—have costs that many programmers don’t consider when
    creating functions, and using them inappropriately can greatly increase a program’s
    size and execution time. This chapter discusses those costs and how to avoid them,
    covering the following subjects:'
  prefs: []
  type: TYPE_NORMAL
- en: Function and procedure calls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Macros and inline functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameter passing and calling conventions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation records and local variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameter-passing mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Function return results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By understanding these topics, you can avoid the efficiency pitfalls that are
    common in modern programs that make heavy use of procedures and functions.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.1 Simple Function and Procedure Calls**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s begin with some definitions. A *function* is a section of code that computes
    and returns some value—the function result. A *procedure* (or *void function*,
    in C/C++/Java/Swift terminology) simply accomplishes some action. Function calls
    generally appear within an arithmetic or logical expression, while procedure calls
    look like statements in the programming language. For the purpose of this discussion,
    you can generally assume that a procedure call and a function call are the same,
    and use the terms *function* and *procedure* interchangeably. For the most part,
    a compiler implements procedure and function calls identically.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Functions and procedures do have some differences, however. Namely, there
    are some efficiency issues related to function results, which we’ll consider in
    “Function Return Values” on [page 590](ch15.xhtml#page_590).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'With most CPUs, you invoke procedures via an instruction similar to the 80x86
    `call` (`branch` and `link` on the ARM and PowerPC) and return to the caller using
    the `ret` (return) instruction. The `call` instruction performs three discrete
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: It determines the address of the instruction to execute upon returning from
    the procedure (this is usually the instruction immediately following `call`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It saves this address (commonly known as the *return address* or *link address*)
    into a known location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It transfers control (via a jump mechanism) to the first instruction of the
    procedure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Execution starts with the first instruction of the procedure and continues
    until the CPU encounters a `ret` instruction, which fetches the return address
    and transfers control to the machine instruction at that address. Consider the
    following C function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the conversion to PowerPC code by GCC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the 32-bit ARM version of this source code compiled by GCC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And here’s the conversion of the same source code to 80x86 code by GCC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the 80x86, ARM, and PowerPC devote considerable effort to building
    and managing activation records (see “The Stack Section” on [page 179](ch07.xhtml#page_179)).
    The important things to see in these two assembly language sequences are the `bl
    _func` and `blr` instructions in the PowerPC code; `bl func` and `bx lr` instructions
    in the ARM code; and the `call func` and `ret` instructions in the 80x86 code.
    These are the instructions that call the function and return from it.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.1.1 Return Address Storage**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: But where, exactly, does the CPU store the return address? In the absence of
    recursion and certain other program control constructs, the CPU could store the
    return address in any location that is large enough to hold the address and that
    will still contain that address when the procedure returns to its caller. For
    example, the program could choose to store the return address in a machine register
    (in which case the return operation would consist of an indirect jump to the address
    contained in that register). One problem with using registers, however, is that
    CPUs generally have a limited number of them. This means every register that holds
    a return address is unavailable for other purposes. For this reason, on CPUs that
    save the return address in a register, the applications usually move the return
    address to memory so they can reuse that register.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the PowerPC and ARM `bl` (branch and link) instruction. This instruction
    transfers control to the target address specified by its operand and copies the
    address of the instruction following `bl` into the LINK register. Inside a procedure,
    if no code modifies the value of the LINK register, the procedure can return to
    its caller by executing a PowerPC `blr` (branch to LINK register) or ARM `bx`
    (branch and exchange) instruction. In our trivial example, the `func()` function
    does not execute any code that modifies the value of the LINK register, so this
    is exactly how `func()` returns to its caller. However, if this function had used
    the LINK register for some other purpose, it would have been the procedure’s responsibility
    to save the return address so that it could restore the value prior to returning
    via a `blr` instruction at the end of the function call.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more common place to keep return addresses is in memory. Although accessing
    memory on most modern processors is much slower than accessing a CPU register,
    keeping return addresses in memory allows a program to have a large number of
    nested procedure calls. Most CPUs actually use a *stack* to hold return addresses.
    For example, the 80x86 `call` instruction *pushes* the return address onto a stack
    data structure in memory, and the `ret` instruction *pops* this return address
    off the stack. Using a stack of memory locations to hold return addresses offers
    several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Stacks, because of their *last-in, first-out (LIFO)* organization, fully support
    nested procedure calls and returns as well as recursive procedure calls and returns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacks are memory efficient because they reuse the same memory locations for
    different procedure return addresses (rather than requiring a separate memory
    location to hold each procedure’s return address).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though stack access is slower than register access, the CPU can generally
    access memory locations on the stack faster than separate return addresses elsewhere,
    because the CPU frequently accesses the stack and the stack contents tend to remain
    in the cache.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As discussed in [Chapter 7](ch07.xhtml#ch07), stacks are also great places to
    store activation records (such as parameters, local variables, and other procedure
    state information).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a stack also incurs a few penalties, though. Most importantly, maintaining
    a stack generally requires dedicating a CPU register to keep track of it in memory.
    This could be a register that the CPU explicitly dedicates for this purpose (for
    example, the RSP register on the x86-64 or R14/SP on the ARM) or a general-purpose
    register on a CPU that doesn’t provide explicit hardware stack support (for example,
    applications running on the PowerPC processor family typically use R1 for this
    purpose).
  prefs: []
  type: TYPE_NORMAL
- en: On CPUs that provide a hardware stack implementation and a `call`/`ret` instruction
    pair, making a procedure call is easy. As shown earlier in the 80x86 GCC example
    output, the program simply executes a `call` instruction to transfer control to
    the beginning of the procedure and then executes a `ret` instruction to return
    from the procedure.
  prefs: []
  type: TYPE_NORMAL
- en: The PowerPC/ARM approach, using a “branch and link” instruction might seem less
    efficient than the `call`/`ret` mechanism. While it’s certainly true that the
    “branch and link” approach requires a little more code, it isn’t so clear that
    it’s slower than the `call`/`ret` approach. A `call` instruction is a complex
    instruction (accomplishing several independent tasks with a single instruction)
    and, as a result, typically requires several CPU clock cycles to execute. The
    execution of the `ret` instruction is similar. Whether the extra overhead is costlier
    than maintaining a software stack varies by CPU and compiler. However, a “branch
    and link” instruction and an indirect jump through the link address, without the
    overhead of maintaining the software stack, is usually faster than the corresponding
    `call`/`ret` instruction pair. If a procedure doesn’t call any other procedures
    and can maintain parameters and local variables in machine registers, it’s possible
    to skip the software stack maintenance instructions altogether. For example, the
    call to `func()` in the previous example is probably more efficient on the PowerPC
    and ARM than on the 80x86, because `func()` doesn’t need to save the LINK register’s
    value into memory—it simply leaves that value in LINK throughout the execution
    of the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because many procedures are short and have few parameters and local variables,
    a good RISC compiler can often dispense with the software stack maintenance entirely.
    Therefore, for many common procedures, this RISC approach is faster than the CISC
    (`call`/`ret`) approach; however, that’s not to imply that it’s always better.
    The brief example in this section is a very special case. In our simple demonstration
    program, the function that this code calls—via the `bl` instruction—is near the
    `bl` instruction. In a complete application, `func()` might be *very* far away,
    and the compiler wouldn’t be able to encode the target address as part of the
    instruction. That’s because RISC processors (like the PowerPC and ARM) must encode
    their entire instruction within a single 32-bit value (which must include both
    the opcode and the displacement to the function). If `func()` is farther away
    than can be encoded in the remaining displacement bits (24, in the case of the
    PowerPC and ARM `bl` instructions), the compiler has to emit a sequence of instructions
    that will compute the address of the target routine and indirectly transfer control
    through that address. Most of the time, this shouldn’t be a problem. After all,
    few programs are so large that the functions would be outside this range (64MB,
    in the case of the PowerPC, ±32MB for the ARM). However, there’s a very common
    case where GCC (and other compilers, presumably) must generate this type of code:
    when the compiler doesn’t know the target address of the function, because it’s
    an external symbol that the linker must merge in after compilation is complete.
    Because the compiler doesn’t know where the routine will be sitting in memory
    (and also because most linkers work only with 32-bit addresses, not 24-bit displacement
    fields), the compiler must assume that the function’s address is out of range
    and emit the long version of the function call. Consider the following slight
    modification to the earlier example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This code declares `func()` as an external function. Now look at the PowerPC
    code that GCC produces and compare it with the earlier code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This code effectively winds up calling two functions in order to call `func()`.
    First, it calls a *stub* function (`L_func$stub`), which then transfers control
    to the actual `func()` routine. Clearly there is considerable overhead here. Without
    actually benchmarking the PowerPC code against the 80x86 code, it’s probably a
    safe bet that the 80x86 solution is a bit more efficient. (The 80x86 version of
    the GCC compiler emits the same code for the main program as in the earlier example,
    even when compiling in the external reference.) You’ll soon see that the PowerPC
    also generates stub functions for things other than external functions. Therefore,
    the CISC solution often is more efficient than the RISC solution (presumably,
    RISC CPUs make up the difference in performance in other areas).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Microsoft CLR also provides generic call and return functionality. Consider
    the following C# program with a static function `f()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the CIL code that the Microsoft C# compiler emits for functions `f()`
    and `Main()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As one last example, here’s a comparable Java program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the Java bytecode (JBC) output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that the Microsoft CLR and Java VM both have several variants of call and
    invoke instructions. These simple examples demonstrate calls to static methods.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.1.2 Other Sources of Overhead**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Of course, a typical procedure call and return involve overhead beyond the execution
    of the actual procedure `call` and `return` instructions. Prior to calling the
    procedure, the calling code must compute and pass any parameters to it. Upon entry
    into the procedure, the calling code may also need to complete the construction
    of the *activation record* (that is, allocate space for local variables). The
    costs of these operations vary by CPU and compiler. For example, if the calling
    code can pass parameters in registers rather than on the stack (or some other
    memory location), this is usually more efficient. Similarly, if the procedure
    can keep all its local variables in registers rather than in the activation record
    on the stack, accessing those local variables is much more efficient. This is
    one area where RISC processors have a considerable advantage over CISC processors.
    A typical RISC compiler can reserve several registers for passing parameters and
    local variables. (RISC processors typically have 16, 32, or more general-purpose
    registers, so setting aside several registers for this purpose is not outrageous.)
    For procedures that don’t call any other procedures (discussed in the next section),
    there’s no need to preserve these register values, so parameter and local variable
    access is very efficient. Even on CPUs with a limited number of registers (such
    as the 32-bit 80x86), it’s still possible to pass a small number of parameters,
    or maintain a few local variables, in registers. Many 80x86 compilers, for example,
    will keep up to three values (parameters or local variables) in the registers.
    Clearly, though, the RISC processors have an advantage here.^([1](footnotes.xhtml#ch15fn1))
  prefs: []
  type: TYPE_NORMAL
- en: Armed with this knowledge, along with the background on activation records and
    stack frames from earlier in this book (see “The Stack Section” on [page 179](ch07.xhtml#page_179)),
    we can now discuss how to write procedures and functions that operate as efficiently
    as possible. The exact rules are highly dependent upon your CPU and the compiler
    you’re using, but some of the concepts are generic enough to apply to all programs.
    The following sections assume that you’re writing for an 80x86 or ARM CPU (as
    most of the world’s software runs on one of these two CPUs).
  prefs: []
  type: TYPE_NORMAL
- en: '**15.2 Leaf Functions and Procedures**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compilers can often generate better code for *leaf* procedures and functions—that
    is, those that don’t call other procedures or functions. The metaphor comes from
    a graphical representation of procedure/function invocations known as a *call
    tree*. A call tree consists of a set of circles (*nodes*) that represent the functions
    and procedures in a program. An arrow from one node to another implies that the
    first node contains a call to the second. [Figure 15-1](ch15.xhtml#ch15fig1) illustrates
    a typical call tree.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the main program directly calls procedure `prc1()` and functions
    `fnc1()` and `fnc2()`. Function `fnc1()` directly calls procedure `prc2()`. Function
    `fnc2()` directly calls procedures `prc2()` and `prc3()` as well as function `fnc3()`.
    The leaf procedures and functions in this call tree are `prc1()`, `prc2()`, `fnc3()`,
    and `prc3()`, which do not call any other procedures or functions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-1: A call tree*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Working with leaf procedures and functions has an advantage: they do not need
    to save parameters passed to them in registers or preserve the values of local
    variables they maintain in registers. For example, if `main()` passes two parameters
    to `fnc1()` in the EAX and EDX registers, and `fnc1()` passes a different pair
    of parameters to `prc2()` in EAX and EDX, then `fnc1()` must first save the values
    it found in EAX and EDX before calling `prc2()`. The `prc2()` procedure, on the
    other hand, doesn’t have to save the values in EAX and EDX prior to some procedure
    or function call, because it doesn’t make such calls. In a similar vein, if `fnc1()`
    allocates any local variables in registers, then it will need to preserve those
    registers across a call to `prc2()`, because `prc2()` can use the registers for
    its own purposes. By contrast, if `prc2()` uses a register for a local variable,
    it never has to preserve the variable’s value, because it never calls any subroutines.
    Therefore, good compilers tend to generate better code for leaf procedures and
    functions because they don’t have to preserve the register values.'
  prefs: []
  type: TYPE_NORMAL
- en: One way to *flatten* the call tree is to take the code associated with procedures
    and functions in interior nodes and inline it into functions higher in the call
    tree. In [Figure 15-1](ch15.xhtml#ch15fig1), for example, if it is practical to
    move the code for `fnc1()` into `main()`, you don’t need to save and restore registers
    (among other operations). However, be sure you’re not sacrificing readability
    and maintainability when flattening the call tree. You want to avoid writing procedures
    and functions that simply call other procedures and functions without doing any
    work on their own, but you don’t want to destroy the modularity of your application’s
    design by expanding function and procedure calls throughout your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ve already seen that having a leaf function is handy when you’re using
    a RISC processor, like the PowerPC or ARM, that uses a “branch and link” instruction
    to make a subroutine call. The PowerPC and ARM LINK registers are good examples
    of registers that you have to preserve across procedure calls. Because a leaf
    procedure does not (normally) modify the value in the LINK register, no extra
    code is necessary in a leaf procedure to preserve that register’s value. To see
    the benefits of calling leaf functions on a RISC CPU, consider the following C
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'GCC emits the following PowerPC assembly code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: There’s an important difference between the implementations of the `f()` and
    `g()` functions in this PowerPC code—`f()` has to preserve the value of the LINK
    register, whereas `g()` does not. Not only does this involve extra instructions,
    but it also involves accessing memory, which can be slow.
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage to using leaf procedures, which isn’t obvious from the call
    tree, is that constructing their activation record requires less work. On the
    80x86, for example, a good compiler doesn’t have to preserve the value of the
    EBP register, load EBP with the activation record address, and then restore the
    original value by accessing local objects via the stack pointer register (ESP).
    On RISC processors, which maintain the stack manually, the savings can be significant.
    For such procedures, the overhead of the procedure call and return and activation
    record maintenance is greater than the actual work done by the procedure. Therefore,
    eliminating the activation record maintenance code could nearly double the speed
    of the procedure. For these and other reasons, you should try to keep your call
    trees as shallow as possible. The more leaf procedures your program uses, the
    more efficient it may become when you compile it with a decent compiler.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.3 Macros and Inline Functions**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One offshoot of the structured programming revolution was that computer programmers
    were taught to write small, modular, and logically coherent functions.^([2](footnotes.xhtml#ch15fn2))
    A function that is logically coherent does one thing well. All of the statements
    in such a procedure or function are dedicated to doing the task at hand without
    producing any side computations or doing any extraneous operations. Years of software
    engineering research indicate that decomposing a problem into small components,
    and then implementing those, produces programs that are easier to read, maintain,
    and modify. Unfortunately, it’s easy to get carried away with this process and
    produce functions like the following Pascal example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'On the 80x86, it would probably take about three instructions to compute the
    sum of two values and store that sum into a memory variable. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Contrast this with the code necessary to simply *call* the function `sum()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Within the procedure `sum` (assuming a mediocre compiler), you might expect
    to find code like the following HLA sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, using a function takes three times as many instructions to compute
    the sum of these two objects as the straight-line (no function call) code. Worse
    still, these nine instructions are generally slower than the three that make up
    the inline code. The inline code could run 5 to 10 times faster than the code
    with the function call.
  prefs: []
  type: TYPE_NORMAL
- en: The one redeeming quality about the overhead associated with a function or procedure
    call is that it’s fixed. It takes the same number of instructions to set up the
    parameters and the activation record whether the procedure or function body contains
    1 or 1,000 machine instructions. Although the overhead of a procedure call is
    huge when the procedure’s body is small, it’s inconsequential when the procedure’s
    body is large. Therefore, to reduce the impact of procedure/function call overhead
    in your programs, try to place larger procedures and functions and write shorter
    sequences as inline code.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the optimum balance between the benefits of modular structure and the
    cost of too-frequent procedure calls can be difficult. Unfortunately, good program
    design often prevents us from increasing the size of our procedures and functions
    enough that the overhead of the call and return becomes insignificant. Sure, we
    could combine several functions and procedure calls into a single procedure or
    function, but this would violate several rules of programming style, and great
    code usually avoids such tactics. (One problem with the resulting programs is
    that few people can figure out how they work in order to optimize them.) However,
    if you can’t sufficiently lower the overhead of a procedure’s body by increasing
    the procedure’s size, you can still improve overall performance by reducing the
    overhead in other ways. As you’ve seen, one option is to use leaf procedures and
    functions. Good compilers emit fewer instructions for leaf nodes in the call tree,
    thereby reducing the call/return overhead. However, if the procedure’s body is
    short, you need a way to completely eliminate the procedure call/return overhead.
    Some languages accomplish this with *macros*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A *pure* macro expands the body of a procedure or function in place of its
    invocation. Because there’s no call/return to code elsewhere in the program, a
    macro expansion avoids the overhead associated with those instructions. Furthermore,
    macros also save considerable expense by using textual substitution for parameters
    rather than pushing the parameter data onto the stack or moving it into registers.
    The drawback to a macro is that the compiler expands the macro’s body for each
    invocation of the macro. If the macro body is large and you invoke it in many
    different places, the executable program can grow by a fair amount. Macros represent
    the classic time/space tradeoff: faster code at the expense of greater size. For
    this reason, you should use macros only to replace procedures and functions that
    have a small number of statements (say, between one and five), except in some
    rare cases where speed is paramount.'
  prefs: []
  type: TYPE_NORMAL
- en: A few languages (like C/C++) provide *inline* functions and procedures, which
    are a cross between a true function (or procedure) and a pure macro. Most languages
    that support inline functions and procedures do not guarantee that the compiler
    will expand the code inline. *Inline expansion*, or a call to an actual function
    in memory, is done at the compiler’s discretion. Most compilers won’t expand an
    inline function if its body is too large or if it has an excessive number of parameters.
    Furthermore, unlike pure macros, which don’t have any associated procedure call
    overhead, inline functions may still need to build an activation record in order
    to handle local variables, temporaries, and other requirements. Thus, even if
    the compiler does expand such a function inline, there may still be some overhead
    that you wouldn’t get with a pure macro.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the result of function inlining, consider the following C source file
    prepared for compilation by Microsoft Visual C++:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the MASM-compatible assembly language code that MSVC emits when you
    specify a C compilation (versus a C++ compilation, which produces messier output):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in this assembly output, there are no function calls to the `inlineFunc()`
    function. Instead, the compiler expanded this function in place in the `main()`
    function, at the point where the main program calls it. Although the `ilf2()`
    function was also declared inline, the compiler refused to expand it inline and
    treated it like a normal function (probably because of its size).
  prefs: []
  type: TYPE_NORMAL
- en: '**15.4 Passing Parameters to a Function or Procedure**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The number and type of parameters can also have a big impact on the efficiency
    of the code a compiler generates for your procedures and functions. Simply put,
    the more parameter data you pass, the more expensive the procedure or function
    call becomes. Often, programmers call generic functions (or design generic functions)
    that require you to pass several optional parameters whose values the function
    won’t use. This scheme can make functions more generally applicable to different
    applications, but—as you’ll see in this section—there’s a cost associated with
    that generality, so you might want to consider using a version of the function
    specific to your application if space or speed is an issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter-passing mechanism (for example, pass-by-reference or pass-by-value)
    also has an impact on the overhead associated with a procedure call and return.
    Some languages allow you to pass large data objects by value. (Pascal lets you
    pass strings, arrays, and records by value, and C/C++ allows you to pass structures
    by value; other languages vary depending on their design.) Whenever you pass a
    large data object by value, the compiler must emit machine code that makes a copy
    of that data into the procedure’s activation record. This can be time-consuming
    (especially when copying large arrays or structures). Furthermore, large objects
    probably won’t fit in the CPU’s register set, so accessing such data within a
    procedure or function is expensive. It’s usually more efficient to pass large
    data objects such as arrays and structures by reference than by value. The extra
    cost of accessing the data indirectly is usually saved many times over by not
    having to copy the data into the activation record. Consider the following C code,
    which passes a large structure by value to a C function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the PowerPC code that GCC emits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the call to function `f()` calls `memcpy` to transfer a copy
    of the data from the `main()` function’s local array to the `f()` function’s activation
    record. Again, copying memory is a slow process, and this code amply demonstrates
    that you should avoid passing large objects by value. Consider the same code when
    you pass the structure by reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the conversion of this C source code to 32-bit ARM assembly by GCC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Depending on your CPU and compiler, it may be slightly more efficient to pass
    small (scalar) data objects by value rather than by reference. For example, if
    you’re using an 80x86 compiler that passes parameters on the stack, you’ll need
    two instructions to pass a memory object by reference, but only a single instruction
    to pass that same object by value. So, although trying to pass large objects by
    reference is a good idea, the reverse is generally true for small objects. However,
    this is not a hard and fast rule; its validity varies based on the CPU and compiler
    you’re using.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some programmers may feel that it’s more efficient to pass data to a procedure
    or function via global variables. After all, if the data is already sitting in
    a global variable that’s accessible to the procedure or function, a call to that
    procedure or function won’t require any extra instructions to pass the data to
    the subroutine, therefore reducing the call overhead. While this seems like a
    big win, keep in mind that compilers have a difficult time optimizing programs
    that make excessive use of global variables. Although using globals may reduce
    the function/procedure call overhead, it may also prevent the compiler from handling
    other optimizations that would have been otherwise possible. Here’s a simple example
    using Microsoft Visual C++ that demonstrates this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the 32-bit MASM source code (with manual annotations) that the MSVC++
    compiler generates for this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the compiler’s ability to optimize around global variables can
    be easily thwarted by the presence of some seemingly unrelated code. In this example,
    the compiler cannot determine that the call to the external `geti()` function
    doesn’t modify the value of the `globalValue` variable. Therefore, the compiler
    can’t assume that `globalValue` still has the value `1` when it computes the inline
    function result for `usesGlobal()`. Exercise extreme caution when using global
    variables to communicate information between a procedure or function and its caller.
    Code that’s unrelated to the task at hand (such as the call to `geti()`, which
    probably doesn’t affect `globalValue`’s value) can prevent the compiler from optimizing
    code that uses global variables.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5 Activation Records and the Stack**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Because of how a stack works, the last procedure activation record the software
    creates will be the first activation record that the system deallocates. Since
    activation records hold procedure parameters and local variables, a *last-in,
    first-out (LIFO)* organization is a very intuitive way of implementing activation
    records. To see how it works, consider the following (trivial) Pascal program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 15-2](ch15.xhtml#ch15fig2) shows the stack layout as this program executes.'
  prefs: []
  type: TYPE_NORMAL
- en: When the program begins execution, it first creates an activation record for
    the main program. The main program calls the `A` procedure (①). Upon entry into
    the `A` procedure, the code completes the construction of the activation record
    for `A`, effectively pushing it onto the stack. Once inside procedure `A`, the
    code calls procedure `B` (②). Note that `A` is still active while the code calls
    `B`, so `A`’s activation record remains on the stack. Upon entry into `B`, the
    system builds `B`’s activation record and pushes it onto the top of the stack
    (③). Once inside `B`, the code calls procedure `C`, and `C` builds its activation
    record on the stack and arrives at the comment `(* Stack snapshot here *)` (④).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-2: Stack layout after three nested procedure calls*'
  prefs: []
  type: TYPE_NORMAL
- en: Because procedures keep their local variables and parameter values in their
    activation record, the lifetime of these variables extends from the point the
    system first creates the activation record until the system deallocates it when
    the procedure returns to its caller. In [Figure 15-2](ch15.xhtml#ch15fig2), notice
    that `A`’s activation record remains on the stack during the execution of the
    `B` and `C` procedures. Therefore, the lifetime of `A`’s parameters and local
    variables completely brackets the lifetimes of `B`’s and `C`’s activation records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now consider the following C/C++ code with a recursive function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This program calls the `recursive()` function three times before it begins returning
    (the main program calls `recursive()` once with the parameter value `2`, and `recursive()`
    calls itself twice with the parameter values `1` and `0`). Because each recursive
    call to `recursive()` pushes another activation record before the current call
    returns, when this program finally hits the `if` statement in this code with `cnt`
    equal to `0`, the stack looks something like [Figure 15-3](ch15.xhtml#ch15fig3).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-3: Stack layout after three recursive procedure calls*'
  prefs: []
  type: TYPE_NORMAL
- en: Because each procedure invocation has a separate activation record, each activation
    of the procedure will have its own copy of the parameters and local variables.
    While the code for a procedure or function is executing, it will access only those
    local variables and parameters in the activation record it has most recently created,^([3](footnotes.xhtml#ch15fn3))
    thereby preserving the values from previous calls.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.1 Breaking Down the Activation Record**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now that you’ve seen how procedures manipulate activation records on the stack,
    it’s time to take a look at the internal composition of a typical activation record.
    In this section we’ll use a typical activation record layout that you’ll see when
    executing code on an 80x86\. Although different languages, different compilers,
    and different CPUs lay out the activation record differently, these differences,
    if they exist at all, will be minor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The 80x86 maintains the stack and activation records using two registers: ESP/RSP
    (the stack pointer) and EBP/RBP (the frame pointer, which Intel calls the *base
    pointer*). The ESP/RSP register points at the current top of stack, and the EBP
    register points at the base address of an activation record.^([4](footnotes.xhtml#ch15fn4))
    A procedure can access objects within its activation record by using the indexed
    addressing mode (see “Indexed Addressing Mode” on [page 34](ch03.xhtml#page_34))
    and supplying a positive or negative offset from the value in the EBP/RBP register.
    Generally, a procedure allocates memory storage for local variables at negative
    offsets from EBP/RBP’s value, and for parameters at positive offsets from EBP/RBP.
    Consider the following Pascal procedure, which has both parameters and local variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 15-4](ch15.xhtml#ch15fig4) shows a typical activation record for this
    Pascal procedure (remember that the stack grows toward lower memory on the 32-bit
    80x86).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-4: A typical activation record*'
  prefs: []
  type: TYPE_NORMAL
- en: When you see the term *base* associated with a memory object, you might think
    that the base address is the lowest address of that object in memory. However,
    there’s no such requirement. The base address is simply the address in memory
    on which you base the offsets to particular fields of that object. As this activation
    record demonstrates, 80x86 activation record base addresses are actually in the
    middle of the record.
  prefs: []
  type: TYPE_NORMAL
- en: 'The activation record is constructed in two phases. The first phase begins
    when the code calling the procedure pushes the parameters for the call onto the
    stack. For example, consider the following call to `HasBoth()` in the previous
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the HLA/x86 assembly code that might correspond to this call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The three `push` instructions in this code sequence build the first three double
    words of the activation record, and the `call` instruction pushes a *return address*
    onto the stack, creating the fourth double word in the activation record. After
    the call, execution continues in the `HasBoth()` procedure itself, where the program
    continues to build the activation record.
  prefs: []
  type: TYPE_NORMAL
- en: The first few instructions of the `HasBoth()` procedure are responsible for
    finishing the construction of the activation record. Immediately upon entry into
    `HasBoth()`, the stack takes the form shown in [Figure 15-5](ch15.xhtml#ch15fig5).^([5](footnotes.xhtml#ch15fn5))
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-5: Activation record upon entry to HasBoth()*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing the procedure’s code should do is to preserve the value in
    the 80x86 EBP register. On entry, EBP probably points at the base address of the
    caller’s activation record. On exit from `HasBoth()`, EBP needs to contain its
    original value. Therefore, upon entry, `HasBoth()` needs to push the current value
    of EBP on the stack in order to preserve EBP’s value. Next, the `HasBoth()` procedure
    needs to change EBP so that it points at the base address of the `HasBoth()` activation
    record. The following HLA/x86 code takes care of these two operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the code at the beginning of `HasBoth()` needs to allocate storage
    for its local (automatic) variables. As you saw in [Figure 15-4](ch15.xhtml#ch15fig4),
    those variables sit below the frame pointer in the activation record. To prevent
    future pushes from wiping out the values in those local variables, the code has
    to set ESP to the address of the last double word of local variables in the activation
    record. To accomplish this, it simply subtracts the number of bytes of local variables
    from ESP via the following machine instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The *standard entry sequence* for a procedure like `HasBoth()` consists of
    the three machine instructions just considered—`push(ebp);`, `mov(esp, ebp);`,
    and `sub(12, esp);`—which complete the construction of the activation record inside
    the procedure. Just before returning, the Pascal procedure is responsible for
    deallocating the storage associated with the activation record. The *standard
    exit sequence* usually takes the following form (in HLA) for a Pascal procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The first instruction deallocates storage for the local variables shown in
    [Figure 15-4](ch15.xhtml#ch15fig4). Note that EBP is pointing at the old value
    of EBP; this value is stored at the memory address just above all the local variables.
    By copying the value in EBP to ESP, we move the stack pointer past all the local
    variables, effectively deallocating them. Now, the stack pointer points at the
    old value of EBP on the stack; therefore, the `pop` instruction in this sequence
    restores EBP’s original value and leaves ESP pointing at the return address on
    the stack. The `ret` instruction in the standard exit sequence does two things:
    it pops the return address from the stack (and, of course, transfers control to
    this address), and it removes 12 bytes of parameters from the stack. Because `HasBoth()`
    has three double-word parameters, popping 12 bytes from the stack removes those
    parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.2 Assigning Offsets to Local Variables**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This `HasBoth()` example allocates local (automatic) variables in the order
    the compiler encounters them. A typical compiler maintains a *current offset*
    (the initial value of which will be 0) into the activation record for local variables.
    Whenever the compiler encounters a local variable, it subtracts the variable’s
    size from the current offset and then uses the result as the offset of the local
    variable (from EBP/RBP) in the activation record. For example, upon encountering
    the declaration for variable `a`, the compiler subtracts the size of `a` (4 bytes,
    assuming `a` is a 32-bit integer) from the current offset (0) and uses the result
    (–4) as the offset for `a`. Next, the compiler encounters variable `r` (which
    is also 4 bytes), sets the current offset to –8, and assigns this offset to `r`.
    This process repeats for each of the local variables in the procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Although this is a typical way that compilers assign offsets to local variables,
    most languages give compiler implementers free rein to allocate local objects
    as they see fit. A compiler can rearrange the objects in the activation record
    if doing so is more convenient. This means you should avoid designing algorithms
    that depend on the previously mentioned allocation scheme, because some compilers
    do it differently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many compilers try to ensure that all local variables you declare have an offset
    that is a multiple of the object’s size. For example, suppose you have the following
    two declarations in a C function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Normally, you’d expect that the compiler would attach an offset like –1 to the
    `c` variable and –5 to the (4-byte) `int` variable `i`. However, some CPUs (such
    as RISC CPUs) require the compiler to allocate double-word objects on a double-word
    boundary. Even on CPUs that don’t require this (for example, the 80x86), it may
    be faster to access a double-word variable if the compiler aligns it on a double-word
    boundary. For this reason (and as previous chapters have described), many compilers
    automatically add padding bytes between local variables so that each variable
    resides at a *natural* offset in the activation record. In general, bytes may
    appear at any offset, words are happiest on even address boundaries, and double
    words should have a memory address that is a multiple of 4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although an optimizing compiler might automatically handle this alignment for
    you, that comes with a cost—those extra padding bytes. As explained earlier, compilers
    are usually free to rearrange the variables in an activation record, but they
    don’t always do so. Therefore, if you intertwine the definitions for several byte,
    word, double-word, and other-sized objects in your local variable declarations,
    the compiler may wind up inserting several bytes of padding into the activation
    record. You can minimize this problem by attempting to group as many like-sized
    objects together as is reasonable in your procedures and functions. Consider the
    following C/C++ code on a 32-bit machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'An optimizing compiler may elect to insert 3 bytes of padding between each
    of these character variables and the (4-byte) integer variable that immediately
    follows. This means that the preceding code will have about 12 bytes of wasted
    space (3 bytes for each of the character variables). Now consider the following
    declarations in the same C code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the compiler won’t emit any extra padding bytes to the code.
    Why? Because characters (being 1 byte each) may begin at any address in memory.^([6](footnotes.xhtml#ch15fn6))
    Therefore, the compiler can place these character variables at offsets –1, –2,
    –3, and –4 within the activation record. Because the last character variable appears
    at an address that is a multiple of 4, the compiler doesn’t need to insert any
    padding bytes between `c3` and `i0` (`i0` will naturally appear at offset –8 in
    the preceding declarations).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, arranging your declarations so that all like-sized objects are
    next to one another can help your compiler produce better code. Don’t take this
    suggestion to an extreme, though. If a rearrangement would make your program more
    difficult to read or maintain, you should carefully consider whether it’s worthwhile
    in your program.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.3 Associating Offsets with Parameters**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As noted, compilers are given considerable leeway with respect to how they assign
    offsets to local (automatic) variables within a procedure. As long as the compiler
    uses these offsets consistently, the exact allocation algorithm it applies is
    almost irrelevant; in fact, it could use a different allocation scheme in different
    procedures of the same program. However, a compiler *doesn’t* have free rein when
    assigning offsets to parameters. It has to live with certain restrictions, because
    other code outside the procedure accesses those parameters. Specifically, the
    procedure and the calling code must agree on the layout of the parameters in the
    activation record so the calling code can build the parameter list. Note that
    the calling code might not be in the same source file, or even in the same programming
    language, as the procedure. To ensure interoperability between a procedure and
    whatever code calls that procedure, then, compilers must adhere to certain *calling
    conventions*. This section will explore the three common calling conventions for
    Pascal/Delphi and C/C++.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.3.1 The Pascal Calling Convention**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In Pascal (including Delphi) the standard parameter-passing convention is to
    push the parameters on the stack in the order of their appearance in the parameter
    list. Consider the following call to the `HasBoth()` procedure from the earlier
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following assembly code implements this call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: When assigning offsets to a procedure’s formal parameters, the compiler assigns
    the highest offset to the first parameter and the lowest offset to the last parameter.
    Because the old value of EBP is at offset 0 in the activation record and the return
    address is at offset 4, the last parameter in the activation record (when using
    the Pascal calling convention on the 80x86 CPU) will reside at offset 8 from EBP.
    Looking back at [Figure 15-4](ch15.xhtml#ch15fig4), you can see that parameter
    `k` is at offset +8, parameter `j` is at offset +12, and parameter `i` (the first
    parameter) is at offset +16 in the activation record.
  prefs: []
  type: TYPE_NORMAL
- en: The Pascal calling convention also stipulates that it is the procedure’s responsibility
    to remove the parameters the caller pushes when the procedure returns to its caller.
    As you saw earlier, the 80x86 CPU provides a variant of the `ret` instruction
    that lets you specify how many bytes of parameters to remove from the stack upon
    return. Therefore, a procedure that uses the Pascal calling convention will typically
    supply the number of parameter bytes as an operand to the `ret` instruction when
    returning to its caller.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.3.2 The C Calling Convention**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The C/C++/Java languages employ another very popular calling convention, generally
    known as the *cdecl calling convention* (or, simply, the *C calling convention*).
    There are two major differences between the C and Pascal calling conventions.
    First, calls to functions in C must push their parameters on the stack in the
    reverse order. That is, the first parameter must appear at the lowest address
    on the stack (assuming the stack grows downward), and the last parameter must
    appear at the highest address in memory. The second difference is that C requires
    the caller, rather than the function, to remove all parameters from the stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following version of `HasBoth()` written in C instead of Pascal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 15-6](ch15.xhtml#ch15fig6) provides the layout for a typical `HasBoth`
    activation record (written in C on a 32-bit 80x86 processor).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-6: HasBoth() activation record in C*'
  prefs: []
  type: TYPE_NORMAL
- en: Looking closely, you’ll see the difference between this figure and [Figure 15-4](ch15.xhtml#ch15fig4).
    The positions of the `i` and `k` variables are reversed in this activation record
    (it’s only a coincidence that `j` appears at the same offset in both).
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the C calling convention reverses the order of the parameters and it’s
    the caller’s responsibility to remove all parameter values from the stack, the
    calling sequence for `HasBoth()` is a little different in C than in Pascal. Consider
    the following call to `HasBoth()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the HLA assembly code for this call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: As a result of using the C calling convention, this code differs in two ways
    from the assembly code for the Pascal implementation. First, this example pushes
    the values of the actual parameters in the opposite order of the Pascal code;
    that is, it first computes `y+2` and pushes that value, then it pushes `x`, and
    finally it pushes the value `5`. The second difference is the inclusion of the
    `add(12,esp);` instruction immediately after the call. This instruction removes
    12 bytes of parameters from the stack upon return. The return from `HasBoth()`
    will use only the `ret` instruction, not the `ret n` instruction.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.3.3 Conventions for Passing Parameters in Registers**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As you’ve seen in these examples, passing parameters on the stack between two
    procedures or functions requires a fair amount of code. Good assembly language
    programmers have long known that it’s better to pass parameters in registers.
    Therefore, several 80x86 compilers following Intel’s ABI (application binary interface)
    rules may attempt to pass as many as three parameters in the EAX, EDX, and ECX
    registers.^([7](footnotes.xhtml#ch15fn7)) Most RISC processors specifically set
    aside a set of registers for passing parameters between functions and procedures.
    See “Registers to the Rescue” on [page 585](ch15.xhtml#page_585) for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most CPUs require that the stack pointer remain aligned on some reasonable
    boundary (for example, a double-word boundary), and CPUs that don’t absolutely
    require this may still benefit from it. Furthermore, many CPUs (the 80x86 included)
    can’t easily push certain small-sized objects, like bytes, onto the stack. Therefore,
    most compilers reserve a minimum number of bytes for a parameter (typically 4),
    regardless of its actual size. As an example, consider the following HLA procedure
    fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The activation record for this procedure appears in [Figure 15-7](ch15.xhtml#ch15fig7).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-7: OneByteParm() activation record*'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the HLA compiler reserves 4 bytes for the `b` parameter even
    though `b` is only a single-byte variable. This extra padding ensures that the
    ESP register will remain aligned on a double-word boundary.^([8](footnotes.xhtml#ch15fn8))
    We can easily push the value of `b` onto the stack in the code that calls `OneByteParm()`
    using a 4-byte `push` instruction.^([9](footnotes.xhtml#ch15fn9))
  prefs: []
  type: TYPE_NORMAL
- en: Even if your program could access the extra bytes of padding associated with
    the `b` parameter, doing so is never a good idea. Unless you’ve explicitly pushed
    the parameter onto the stack (for example, using assembly language code), there’s
    no guarantee about the data values that appear in the padding bytes. In particular,
    they may not contain 0\. Nor should your code assume that the padding is present
    or that the compiler pads such variables out to 4 bytes. Some 16-bit processors
    may require only a single byte of padding. Some 64-bit processors may require
    7 bytes of padding. Some compilers on the 80x86 may use 1 byte of padding, while
    others use 3 bytes. Unless you’re willing to live with code that only one compiler
    can compile (and code that could break when the next version of the compiler comes
    along), it’s best to ignore these padding bytes.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.4 Accessing Parameters and Local Variables**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once a subroutine sets up the activation record, accessing local (automatic)
    variables and parameters is easy. The machine code simply uses the indexed addressing
    mode to access such objects. Consider again the activation record in [Figure 15-4](ch15.xhtml#ch15fig4).
    The variables in the `HasBoth()` procedure have the offsets found in [Table 15-1](ch15.xhtml#ch15tab1).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 15-1:** Offsets to Local Variables and Parameters in `HasBoth()` (Pascal
    Version)'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Variable** | **Offset** | **Addressing mode example** |'
  prefs: []
  type: TYPE_TB
- en: '| `i` | +16 | `mov( [ebp+16], eax );` |'
  prefs: []
  type: TYPE_TB
- en: '| `j` | +12 | `mov( [ebp+12], eax );` |'
  prefs: []
  type: TYPE_TB
- en: '| `k` | +8 | `mov( [ebp+8], eax );` |'
  prefs: []
  type: TYPE_TB
- en: '| `a` | –4 | `mov( [ebp-4], eax );` |'
  prefs: []
  type: TYPE_TB
- en: '| `r` | –8 | `mov( [ebp-8], eax );` |'
  prefs: []
  type: TYPE_TB
- en: '| `c` | –9 | `mov( [ebp-9], al );` |'
  prefs: []
  type: TYPE_TB
- en: '| `b` | –10 | `mov( [ebp-10], al );` |'
  prefs: []
  type: TYPE_TB
- en: '| `w` | –12 | `mov( [ebp-12], ax );` |'
  prefs: []
  type: TYPE_TB
- en: The compiler allocates static local variables in a procedure at a fixed address
    in memory. Static variables do not appear in the activation record, so the CPU
    accesses static objects using the direct addressing mode.^([10](footnotes.xhtml#ch15fn10))
    As [Chapter 3](ch03.xhtml#ch03) discussed, in 80x86 assembly language instructions
    that use the direct addressing mode need to encode the full 32-bit address as
    part of the machine instruction. Therefore, instructions that use the direct addressing
    mode are usually at least 5 bytes long (and often longer). On the 80x86, if the
    offset from EBP is –128 through +127, then a compiler can encode an instruction
    of the form `[ebp + constant`] in as few as 2 or 3 bytes. Such instructions will
    be more efficient than those that encode a full 32-bit address. The same principle
    applies on other processors, even if those CPUs provide different addressing modes,
    address sizes, and so on. Specifically, accessing local variables whose offset
    is relatively small is generally more efficient than accessing static variables
    or variables with larger offsets.
  prefs: []
  type: TYPE_NORMAL
- en: Because most compilers allocate offsets for local (automatic) variables as the
    compiler encounters them, the first 128 bytes of local variables will be the ones
    with the shortest offsets (at least, on the 80x86; this value may be different
    for other processors).
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following two sets of local variable declarations (presumably
    appearing with some C function):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s a second version of these declarations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Although these two declaration sections are semantically identical, there is
    a big difference in the code a compiler for the 32-bit 80x86 generates to access
    these variables. In the first declaration, the variable `string` appears at offset
    –256 within the activation record, `i` appears at offset –260, `j` appears at
    offset –264, and `c` appears at offset –265\. Because these offsets are outside
    the range –128 through +127, the compiler will have to emit machine instructions
    that encode a 4-byte offset constant rather than a 1-byte constant. Accordingly,
    the code associated with these declarations will be larger and may run slower.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now consider the second declaration. In this example the programmer declares
    the scalar (non-array) objects first. Therefore, the variables have the following
    offsets: `i` at –4, `j` at –8, `c` at –9, and `string` at –265\. This turns out
    to be the optimal configuration for these variables (`i`, `j`, and `c` will use
    1-byte offsets; `string` will require a 4-byte offset).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This example demonstrates another rule you should try to follow when declaring
    local (automatic) variables: declare smaller, scalar objects first within a procedure,
    followed by all the arrays, structures/records, and other large objects.'
  prefs: []
  type: TYPE_NORMAL
- en: As explained in “Associating Offsets with Parameters” on [page 570](ch15.xhtml#page_570),
    if you declare several local objects with differing sizes adjacent to one another,
    the compiler may need to insert padding bytes to keep the larger objects aligned
    at an appropriate memory address. While worrying about a few wasted bytes here
    and there may seem ridiculous on machines with a gigabyte (or more) of RAM, those
    few padding bytes could be just enough to push the offsets of certain local variables
    beyond –128, causing the compiler to emit 4-byte offsets rather than 1-byte offsets
    for those variables. This is one more reason you should try to declare like-sized
    local variables adjacent to one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'On RISC processors, such as the PowerPC or ARM, the range of possible offsets
    is usually much greater than ±128\. This is a good thing, because once you exceed
    the range of the activation record offset that a RISC CPU can encode directly
    into an instruction, parameter and local variable access gets very expensive.
    Consider the following C program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the PowerPC assembly output from GCC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: This compilation was done under GCC without optimization to show what happens
    when your activation record grows to the point you can no longer encode activation
    record offsets into the instruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'To encode the address of `e`, whose offset is too large, we need these three
    instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'instead of a single instruction that stores `R0` into the `a` variable, such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: While two extra instructions in a program of this size might seem insignificant,
    keep in mind that the compiler will generate these extra instructions for each
    such access. If you frequently access a local variable with a huge offset, the
    compiler may generate a significant number of extra instructions throughout your
    function or procedure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, in a standard application running on a RISC, this problem seldom
    occurs because we rarely allocate local storage beyond the range that a single
    instruction can encode. Also, RISC compilers generally allocate scalar (non-array/non-structure)
    objects in registers rather than blindly allocating them at the next memory address
    in the activation record. For example, if you turn on GCC’s optimization with
    the `-O2` command-line switch, you’ll get the following PowerPC output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: One thing that you’ll notice in this version with optimization enabled is that
    GCC did not allocate variables in the activation record as they were encountered.
    Instead, it placed most of the objects in registers (even array elements). Keep
    in mind that an optimizing compiler may very well rearrange all the local variables
    you declare.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ARM processor has similar limitations based on the size of the instruction
    opcode (32 bits). Here’s the (unoptimized) ARM output from GCC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: While this is arguably better than the PowerPC code, there’s still considerable
    ugliness in the address computations because the ARM CPU cannot encode 32-bit
    constants as part of the instruction opcode. To understand why GCC emits such
    bizarre constants to compute offsets into the activation record, see the discussion
    of the ARM immediate operands in the section “The Immediate Addressing Mode” in
    Appendix C online.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you find the optimized PowerPC or ARM code a bit hard to follow, consider
    the following 80x86 GCC output for the same C program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Of course, the 80x86 doesn’t have as many registers to use for passing parameters
    and holding local variables, so the 80x86 code has to allocate more locals in
    the activation record. Also, the 80x86 provides an offset range of –128 to +127
    bytes only around the EBP register, so a larger number of instructions have to
    use the 4-byte offset rather than the 1-byte offset. Fortunately, the 80x86 does
    allow you to encode a full 32-bit address as part of the instructions that access
    memory, so you don’t have to execute multiple instructions in order to access
    a variable stored a long distance away from where EBP points in the stack frame.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.5 Registers to the Rescue**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As the examples in the previous section demonstrate, RISC code suffers greatly
    when it has to deal with parameters and local variables whose offsets are not
    easy to represent within the confines of the instruction opcode. In real code,
    however, the situation is not so dire. Compilers are smart enough to use machine
    registers to pass parameters and hold local variables providing immediate access
    to those values. This dramatically reduces the number of instructions in typical
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the (register-starved) 32-bit 80x86 CPU. As there are only eight general-purpose
    registers, two of which (ESP and EBP) have special purposes that limit their use,
    there aren’t a lot of registers available for passing parameters or holding local
    variables. Typical C compilers use EAX, ECX, and EDX to pass up to three parameters
    to a function. Functions return their result in the EAX register. The function
    must preserve the values of any other registers (EBX, ESI, EDI, and EBP) it uses.
    It’s fortunate that memory access to local variables and parameters inside the
    function is very efficient—given the limited register set, the 32-bit 80x86 will
    need to use memory often for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: For most applications, the largest architectural improvement to the 64-bit x86-64
    over the 32-bit 80x86 was not 64-bit registers (or even addresses), but that the
    x86-64 added eight new general-purpose registers and eight new XMM registers that
    compilers could use for passing parameters and holding local variables. The Intel/AMD
    ABI for the x86-64 allows a compiler to pass up to six different arguments in
    registers to a function (without the caller explicitly saving those register values
    before using them). [Table 15-2](ch15.xhtml#ch15tab2) lists the available registers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 15-2:** Ix86-64 Argument Passing via Registers'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Register** | **Usage** |'
  prefs: []
  type: TYPE_TB
- en: '| RDI | 1st argument |'
  prefs: []
  type: TYPE_TB
- en: '| RSI | 2nd argument |'
  prefs: []
  type: TYPE_TB
- en: '| RDX | 3rd argument |'
  prefs: []
  type: TYPE_TB
- en: '| RCX | 4th argument |'
  prefs: []
  type: TYPE_TB
- en: '| R8 | 5th argument |'
  prefs: []
  type: TYPE_TB
- en: '| R9 | 6th argument |'
  prefs: []
  type: TYPE_TB
- en: '| XMM0–XMM7 | Used to pass floating-point arguments |'
  prefs: []
  type: TYPE_TB
- en: '| R10 | Can be used to pass static chain pointer |'
  prefs: []
  type: TYPE_TB
- en: '| RAX | Used to pass argument count if there are a variable number of parameters
    |'
  prefs: []
  type: TYPE_TB
- en: The 32-bit ARM (A32) ABI specifies up to four arguments appearing in registers
    R0 through R3\. As the A64 architecture has twice as many registers (32), the
    A64 ABI is a bit more generous, passing up to eight 64-bit integer/pointer arguments
    in R0 through R7 and up to eight additional floating-point parameters in V0 through
    V7.
  prefs: []
  type: TYPE_NORMAL
- en: The PowerPC ABI, which has 32 general-purpose registers, sets aside R3 through
    R10 to pass up to eight arguments to a function. It also sets aside the F1 through
    F8 floating-point registers to pass floating-point arguments to a function.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to setting aside registers to hold function arguments, the various
    ABIs typically define various registers that a function can use to hold local
    variables or temporary values (without explicitly preserving the values held in
    those registers upon entry to the function). For example, the Windows ABI sets
    aside R11, XMM8 through XMM15, MMX0 through MMX7, the FPU registers, and RAX for
    temporary/local use. The ARM A32 ABI sets aside R4 through R8 and R10 through
    R11 for use as locals. The A64 ABI sets aside R9 through R15 for locals and temporaries.
    The PowerPC sets aside R14 through R30 and F14 through F31 for local variables.
    Once a compiler exhausts the registers the ABI defines for argument passing, most
    ABIs expect the calling code to pass additional parameters on the stack. Similarly,
    once a function uses all the registers set aside for local variables, additional
    local variable allocation occurs on the stack.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, a compiler can use other registers for local and temporary values
    as well as those set aside by the CPU’s or OS’s ABI. However, it’s the compiler’s
    responsibility to preserve those register values across the function call.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*An ABI is a* convention, *not a requirement by the underlying OS or hardware.
    Compiler writers (and assembly language programmers) who stick to a given ABI
    can expect that their object code modules will be able to interact with code written
    in other languages that adhere to that ABI. However, nothing stops a compiler
    writer from using whatever mechanism they choose.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**15.5.6 Java VM and Microsoft CLR Parameters and Locals**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because the Java VM and Microsoft CLR are both virtual stack machines, programs
    compiled to those two architectures always push function arguments onto the stack.
    Beyond that, the two virtual machine architectures diverge. The reason for the
    divergence is that the Java VM’s design supports efficient interpretation of Java
    bytecodes with JIT compilation improving performance as needed. The Microsoft
    CLR, on the other hand, does not support interpretation; instead, the CLR code
    (CIL) design supports efficient JIT compilation to optimized machine code.
  prefs: []
  type: TYPE_NORMAL
- en: The Java VM is a traditional stack architecture, with parameters, locals, and
    temporaries sitting on the stack. Other than the fact that there are no registers
    to use for such objects, Java’s memory organization is very similar to that of
    the 80x86/x86-64, PowerPC, and ARM CPUs. During JIT compilation, it can be difficult
    to figure out which values on the stack can be moved into registers and which
    local variables the Java compiler allocates on the stack can be allocated in registers.
    Optimizing such stack allocations to use registers can be very time-consuming,
    so it’s doubtful that the Java JIT compiler does this while the application is
    running (as doing so would greatly diminish the application’s runtime performance).
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft’s CLR operates under a different philosophy. CIL is always JIT-compiled
    into native machine code. Furthermore, Microsoft’s intent is to have the JIT compiler
    produce *optimized* native machine code. While the JIT compiler rarely does as
    good a job as a traditional C/C++ compiler, it generally does a much better job
    than the Java JIT compiler. This is because the Microsoft CLR definition explicitly
    singles out parameter argument and local variable memory accesses. When the JIT
    compiler sees these special instructions, it can allocate those variables to registers
    rather than memory locations. As a result, CLR JIT-compiled code is often shorter
    and faster than Java VM JIT-compiled code (especially on RISC architectures).
  prefs: []
  type: TYPE_NORMAL
- en: '**15.6 Parameter-Passing Mechanisms**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most high-level languages provide at least two mechanisms for passing actual
    parameter data to a subroutine: pass-by-value and pass-by-reference.^([11](footnotes.xhtml#ch15fn11))
    In languages like Visual Basic, Pascal, and C++, declaring and using both types
    of parameters is so easy that a programmer may conclude that there’s little difference
    in efficiency between the two mechanisms. That’s a myth this section intends to
    dispel.'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*There are other parameter-passing mechanisms besides pass-by-value and pass-by-reference.
    FORTRAN and HLA, for example, support a mechanism known as pass-by-value/result
    (or pass-by-value/returned). Ada and HLA support pass-by-result. HLA and Algol
    support pass-by-name. This book won’t discuss these alternative parameter-passing
    mechanisms further, because you probably won’t see them very often. If you’d like
    more information, consult a good book on programming language design or the HLA
    documentation.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**15.6.1 Pass-by-Value**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pass-by-value is the easiest parameter-passing mechanism to understand. The
    code that calls a procedure makes a copy of the parameter’s data and passes this
    copy to the procedure. For small values, passing a parameter by value generally
    requires little more than a `push` instruction (or, when passing parameters in
    the registers, an instruction that moves the value into a register). Therefore,
    this mechanism is often very efficient.
  prefs: []
  type: TYPE_NORMAL
- en: One big advantage of pass-by-value parameters is that the CPU treats them just
    like a local variable within the activation record. Because you’ll rarely have
    more than 120 bytes of parameter data that you pass to a procedure, CPUs that
    provide a shortened displacement with the indexed addressing mode will be able
    to access most parameter values using a shorter (and, therefore, more efficient)
    instruction.
  prefs: []
  type: TYPE_NORMAL
- en: The one case where passing a parameter by value can be inefficient is when you
    need to pass a large data structure, such as an array or record. The calling code
    needs to make a byte-for-byte copy of the actual parameter into the procedure’s
    activation record, as you saw in an earlier example. This can be a very slow process,
    say, if you decide to pass a million-element array to a subroutine by value. Therefore,
    you should avoid passing large objects by value unless absolutely necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.6.2 Pass-by-Reference**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The pass-by-reference mechanism passes the address of an object rather than
    its value. This has a couple of distinct advantages over pass-by-value. First,
    regardless of the parameter’s size, pass-by-reference parameters always consume
    the same amount of memory—the size of a pointer (which typically fits in a machine
    register). Second, pass-by-reference parameters allow you to modify the value
    of the actual parameter—which is impossible with pass-by-value parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Pass-by-reference parameters are not without their drawbacks, though. Usually,
    accessing a reference parameter within a procedure is more expensive than accessing
    a value parameter, because the subroutine needs to dereference that address on
    each access of the object. This generally involves loading a register with the
    pointer in order to dereference the pointer using a register indirect addressing
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the following Pascal code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the equivalent HLA/x86 assembly code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Notice that this code requires two more instructions than a version that uses
    pass-by-value—specifically, the two instructions that load the addresses of `dest`
    and `passedByRef` into the EDX register. In general, only a single instruction
    is needed to access the value of a pass-by-value parameter. However, two instructions
    are needed to manipulate the value of a parameter when you pass it by reference
    (one instruction to fetch the address, and one to manipulate the data at that
    address). So, unless you need the semantics of pass-by-reference, try to use pass-by-value
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: The issues with pass-by-reference tend to diminish when your CPU has lots of
    available registers that it can use to maintain the pointer values. In that situation,
    the CPU can use a single instruction to fetch or store a value via a pointer maintained
    in the register.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.7 Function Return Values**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most HLLs return function results in one or more CPU registers. Exactly which
    register the compiler uses depends on the data type, CPU, and compiler. For the
    most part, however, functions return their results in registers (assuming the
    return data fits in a machine register).
  prefs: []
  type: TYPE_NORMAL
- en: On the 32-bit 80x86, most functions that return ordinal (integer) values return
    their function results in the AL, AX, or EAX register. Functions that return 64-bit
    values (`long long int`) generally return the function result in the EDX:EAX register
    pair (with EDX containing the HO double word of the 64-bit value). On 64-bit variants
    of the 80x86 family, 64-bit compilers return 64-bit results in the RAX register.
    On the PowerPC, most compilers follow the IBM ABI and return 8-, 16-, and 32-bit
    values in the R3 register. Compilers for the 32-bit versions of the PowerPC return
    64-bit ordinal values in the R4:R3 register pair (with R4 containing the HO word
    of the function result). Presumably, compilers running on 64-bit variants of the
    PowerPC can return 64-bit ordinal results directly in R3.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, compilers return floating-point results in one of the CPU’s (or FPU’s)
    floating-point registers. On 32-bit variants of the 80x86 CPU family, most compilers
    return a floating-point result in the 80-bit ST0 floating-point register. Although
    the 64-bit versions of the 80x86 family also provide the same FPU registers as
    the 32-bit members, some operating systems, such as Windows64, typically use one
    of the SSE registers (XMM0) to return floating-point values. PowerPC systems generally
    return floating-point function results in the F1 floating-point register. Other
    CPUs return floating-point results in comparable locations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some languages allow a function to return a nonscalar (aggregate) value. The
    exact mechanism that compilers use to return large function return results varies
    from compiler to compiler. However, a typical solution is to pass a function the
    address of some storage where the function can place the return result. As an
    example, consider the following short C++ program whose `func()` function returns
    a structure object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the PowerPC code that GCC emits for this C++ program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the 32-bit 80x86 code that GCC emits for this same function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The takeaway from these 80x86 and PowerPC examples is that functions returning
    large objects often copy the function result data just prior to returning. This
    extra copying can take considerable time, especially if the return result is large.
    Instead of returning a large structure as a function result, as shown here, it’s
    usually better to explicitly pass a pointer to some destination storage to a function
    that returns a large result and then let the function do whatever copying is necessary.
    This often saves some time and code. Consider the following C code, which implements
    this policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the conversion to 80x86 code by GCC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this approach is more efficient because the code doesn’t have
    to copy the data twice, once to a local copy of the data and once to the final
    destination variable.
  prefs: []
  type: TYPE_NORMAL
- en: '**15.8 For More Information**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Aho, Alfred V., Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman. *Compilers:
    Principles, Techniques, and Tools*. 2nd ed. Essex, UK: Pearson Education Limited,
    1986.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Barrett, William, and John Couch. *Compiler Construction: Theory and Practice*.
    Chicago: SRA, 1986.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dershem, Herbert, and Michael Jipping. *Programming Languages, Structures and
    Models*. Belmont, CA: Wadsworth, 1990.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Duntemann, Jeff. *Assembly Language Step-by-Step*. 3rd ed. Indianapolis: Wiley,
    2009.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fraser, Christopher, and David Hansen. *A Retargetable C Compiler: Design and
    Implementation*. Boston: Addison-Wesley Professional, 1995.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ghezzi, Carlo, and Jehdi Jazayeri. *Programming Language Concepts*. 3rd ed.
    New York: Wiley, 2008.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hoxey, Steve, Faraydon Karim, Bill Hay, and Hank Warren, eds. *The PowerPC
    Compiler Writer’s Guide*. Palo Alto, CA: Warthman Associates for IBM, 1996.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyde, Randall. *The Art of Assembly Language*. 2nd ed. San Francisco: No Starch
    Press, 2010.'
  prefs: []
  type: TYPE_NORMAL
- en: '———. “Webster: The Place on the Internet to Learn Assembly.” *[http://plantation-productions.com/Webster/index.html](http://plantation-productions.com/Webster/index.html)*.'
  prefs: []
  type: TYPE_NORMAL
- en: Intel. “Intel 64 and IA-32 Architectures Software Developer Manuals.” Updated
    November 11, 2019\. *[https://software.intel.com/en-us/articles/intel-sdm](https://software.intel.com/en-us/articles/intel-sdm)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ledgard, Henry, and Michael Marcotty. *The Programming Language Landscape*.
    Chicago: SRA, 1986.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Louden, Kenneth C. *Compiler Construction: Principles and Practice*. Boston:
    Cengage, 1997.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Louden, Kenneth C., and Kenneth A. Lambert. *Programming Languages: Principles
    and Practice*. 3rd ed. Boston: Course Technology, 2012.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parsons, Thomas W. *Introduction to Compiler Construction*. New York: W. H.
    Freeman, 1992.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pratt, Terrence W., and Marvin V. Zelkowitz. *Programming Languages, Design
    and Implementation*. 4th ed. Upper Saddle River, NJ: Prentice Hall, 2001.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sebesta, Robert. *Concepts of Programming Languages*. 11th ed. Boston: Pearson,
    2016.'
  prefs: []
  type: TYPE_NORMAL
