<html><head></head><body>
<h2 class="h2" id="ch13"><span epub:type="pagebreak" id="page_219"/><span class="big">13</span><br/>HEALTH PROBES</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">Having a reliable application is about more than just keeping application components running. Application components also need to be able to respond to requests in a timely way and get data from and make requests of dependencies. This means that the definition of a “healthy” application component is different for each individual component.</p>&#13;
<p class="indent">At the same time, Kubernetes needs to know when a Pod and its containers are healthy so that it can route traffic to only healthy containers and replace failed ones. For this reason, Kubernetes allows configuration of custom health checks for containers and integrates those health checks into management of workload resources such as Deployment.</p>&#13;
<p class="indent">In this chapter, we’ll look at how to define health probes for our applications. We’ll look at both network-based health probes and probes that are internal to a container. We’ll see how Kubernetes runs these health probes and how it responds when a container becomes unhealthy.</p>&#13;
<h3 class="h3" id="ch00lev1sec55"><span epub:type="pagebreak" id="page_220"/>About Probes</h3>&#13;
<p class="noindent">Kubernetes supports three different types of probes:</p>&#13;
<div class="bqparan">&#13;
<p class="noindent5"><strong>Exec</strong> Run a command or script to check on a container.</p>&#13;
<p class="noindent5"><strong>TCP</strong> Determine whether a socket is open.</p>&#13;
<p class="noindent5"><strong>HTTP</strong> Verify that an HTTP GET succeeds.</p>&#13;
</div>&#13;
<p class="indent">In addition, we can use any of these three types of probes for any of three different purposes:</p>&#13;
<div class="bqparan">&#13;
<p class="noindent5"><strong>Liveness</strong> Detect and restart failed containers.</p>&#13;
<p class="noindent5"><strong>Startup</strong> Give extra time before starting liveness probes.</p>&#13;
<p class="noindent5"><strong>Readiness</strong> Avoid sending traffic to containers when they are not prepared for it.</p></div>&#13;
<p class="indent">Of these three purposes, the most important is the liveness probe because it runs during the primary life cycle of the container and can result in container restarts. We’ll look closely at liveness probes and use that knowledge to understand how to use startup and readiness probes.</p>&#13;
<h3 class="h3" id="ch00lev1sec56">Liveness Probes</h3>&#13;
<p class="noindent">A <em>liveness</em> probe runs continuously as soon as the container has started running. Liveness probes are created as part of the container definition, and a container that fails its liveness probe will be restarted automatically.</p>&#13;
<h4 class="h4" id="ch00lev2sec87">Exec Probes</h4>&#13;
<p class="noindent">Let’s begin with a simple liveness probe that runs a command inside the container. Kubernetes expects the command to finish before a timeout and return zero to indicate success, or a non-zero code to indicate a problem.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on getting set up.</em></p>&#13;
</div>&#13;
<p class="indent">Let’s illustrate this with an NGINX web server container. We’ll use this Deployment definition:</p>&#13;
<p class="noindent6"><em>nginx-exec.yaml</em></p>&#13;
<pre>------&#13;
kind: Deployment&#13;
apiVersion: apps/v1&#13;
metadata:&#13;
  name: nginx&#13;
spec:&#13;
  replicas: 1&#13;
  selector:&#13;
    matchLabels:&#13;
<span epub:type="pagebreak" id="page_221"/>      app: nginx&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: nginx&#13;
    spec:&#13;
      containers:&#13;
      - name: nginx&#13;
        image: nginx&#13;
        livenessProbe:&#13;
          exec:&#13;
            command: ["/usr/bin/curl", "-fq", "http://localhost"]&#13;
          initialDelaySeconds: 10&#13;
          periodSeconds: 5</pre>&#13;
<p class="indent">The <code>exec</code> section of the <code>livenessProbe</code> tells Kubernetes to run a command inside the container. In this case, <code>curl</code> is used with a <code>-q</code> flag so that it doesn’t print the page contents but just returns a zero exit code on success. Additionally, the <code>-f</code> flag causes <code>curl</code> to return a non-zero exit code for any HTTP error response (that is, any response code of 300 or above).</p>&#13;
<p class="indent">The <code>curl</code> command runs every 5 seconds based on the <code>periodSeconds</code>; it starts 10 seconds after the container is started, based on <code>initialDelaySeconds</code>.</p>&#13;
<p class="indent">The automated scripts for this chapter add the <em>nginx-exec.yaml</em> file to <em>/opt</em>. Create this Deployment as usual:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/nginx-exec.yaml</span> &#13;
deployment.apps/nginx created</pre>&#13;
<p class="indent">The resulting Pod status doesn’t look any different from a Pod without a liveness probe:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                     READY   STATUS    RESTARTS   AGE&#13;
nginx-68dc5f984f-jq5xl   1/1     Running   0          25s</pre>&#13;
<p class="indent">However, in addition to the regular NGINX server process, <code>curl</code> is being run inside the container every 5 seconds, verifying that it is possible to connect to the server. The detailed output from <code>kubectl describe</code> shows this configuration:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe deployment nginx</span>&#13;
Name:                   nginx&#13;
Namespace:              default&#13;
...&#13;
Pod Template:&#13;
  Labels:  app=nginx&#13;
  Containers:&#13;
   nginx:&#13;
...&#13;
<span epub:type="pagebreak" id="page_222"/>    Liveness:     exec [/usr/bin/curl -q http://localhost] delay=10s &#13;
    timeout=1s period=5s #success=1 #failure=3&#13;
...</pre>&#13;
<p class="indent">Because a liveness probe is defined, the fact that the Pod continues to show a <code>Running</code> status and no restarts indicates that the check is successful. The <code>#success</code> field shows that one successful run is sufficient for the container to be considered live, whereas the <code>#failure</code> value shows that three consecutive failures will cause the Pod to be restarted.</p>&#13;
<p class="indent">We used <code>-q</code> to discard the logs from <code>curl</code>, but even without that flag, any logs from a successful liveness probe are discarded. If we want to save the ongoing log information from a probe, we need to send it to a file or use a logging library to ship it across the network.</p>&#13;
<p class="indent">Before moving on to another type of probe, let’s see what happens if a liveness probe fails. We’ll patch the <code>curl</code> command to try to retrieve a nonexistent path on the server, which will cause <code>curl</code> to return a non-zero exit code, so our probe will fail.</p>&#13;
<p class="indent">We used a patch file in <a href="ch09.xhtml#ch09">Chapter 9</a> when we edited a Service type. Let’s do that again here to make the change:</p>&#13;
<p class="noindent6"><em>nginx-404.yaml</em></p>&#13;
<pre>---&#13;
spec:&#13;
  template:&#13;
    spec:&#13;
      containers:&#13;
     <span class="ent">➊</span> - name: nginx&#13;
          livenessProbe:&#13;
            exec:&#13;
              command: ["/usr/bin/curl", "-fq", "http://localhost/missing"]</pre>&#13;
<p class="indent">Although a patch file allows us to update only the specific fields we care about, in this case the patch file has several lines because we need to specify the full hierarchy, and we also must specify the name of the container we want to modify <span class="ent">➊</span>, so Kubernetes will merge this content into the existing definition for that container.</p>&#13;
<p class="indent">To patch the Deployment, use the <code>kubectl patch</code> command:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl patch deploy nginx --patch-file /opt/nginx-404.yaml</span> &#13;
deployment.apps/nginx patched</pre>&#13;
<p class="indent">Because we changed the Pod specification within the Deployment, Kubernetes needs to terminate the old Pod and create a new one:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                     READY   STATUS        RESTARTS   AGE&#13;
nginx-679f866f5b-7lzsb   1/1     Terminating   0          2m28s&#13;
nginx-6cb4b995cd-6jpd7   1/1     Running       0          3s</pre>&#13;
<p class="indent">Initially, the new Pod shows a <code>Running</code> status. However, if we check back again in about 30 seconds, we get an indication that the Pod has an issue:</p>&#13;
<pre><span epub:type="pagebreak" id="page_223"/>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                     READY   STATUS    RESTARTS   AGE&#13;
nginx-6cb4b995cd-6jpd7   1/1     Running   1          28s</pre>&#13;
<p class="indent">We didn’t change the initial delay or the period for our liveness probe, so the first probe started after 10 seconds and the probe runs every 5 seconds. It takes three failures to trigger a restart, so it’s not surprising that we see one restart after 25 seconds have elapsed.</p>&#13;
<p class="indent">The Pod’s event log indicates the reason for the restart:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe pod</span>&#13;
Name:         nginx-6cb4b995cd-6jpd7&#13;
...&#13;
Containers:&#13;
  nginx:&#13;
...&#13;
    Last State:     Terminated&#13;
...&#13;
Events:&#13;
  Type     Reason     Age                From     Message&#13;
  ----     ------     ----               ----     -------&#13;
...&#13;
  Warning  Unhealthy  20s (x9 over 80s)  kubelet  Liveness probe failed: ...&#13;
curl: (22) The requested URL returned error: 404 Not Found&#13;
...</pre>&#13;
<p class="indent">The event log helpfully provides the output from <code>curl</code> telling us the reason for the failed liveness probe. Kubernetes will continue to restart the container every 25 seconds as each new container starts running and then fails three consecutive liveness probes.</p>&#13;
<h4 class="h4" id="ch00lev2sec88">HTTP Probes</h4>&#13;
<p class="noindent">The ability to run a command within a container to check health allows us to perform custom probes. However, for a web server like this one, we can take advantage of the HTTP probe capability within Kubernetes, avoiding the need for <code>curl</code> inside our container image and also verifying connectivity from outside the Pod.</p>&#13;
<p class="indent">Let’s replace our NGINX Deployment with a new configuration that uses an HTTP probe:</p>&#13;
<p class="noindent6"><em>nginx-http.yaml</em></p>&#13;
<pre>---&#13;
kind: Deployment&#13;
apiVersion: apps/v1&#13;
metadata:&#13;
  name: nginx&#13;
spec:&#13;
  replicas: 1&#13;
<span epub:type="pagebreak" id="page_224"/>  selector:&#13;
    matchLabels:&#13;
      app: nginx&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: nginx&#13;
    spec:&#13;
      containers:&#13;
      - name: nginx&#13;
        image: nginx&#13;
        livenessProbe:&#13;
          httpGet:&#13;
            path: /&#13;
            port: 80</pre>&#13;
<p class="indent">With this configuration, we tell Kubernetes to connect to port 80 of our Pod and do an HTTP GET at the root path of <em>/</em>. Because our NGINX server is listening on port 80 and will serve a welcome file for the root path, we can expect this to work.</p>&#13;
<p class="indent">We’ve specified the entire Deployment rather than using a patch, so we’ll use <code>kubectl apply</code> to update the Deployment:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/nginx-http.yaml</span> &#13;
deployment.apps/nginx configured</pre>&#13;
<p class="indent">We could use a patch to make this change as well, but it would be more complex this time, because a patch file is merged into the existing configuration. As a result, we would require two commands: one to remove the existing liveness probe and one to add the new HTTP liveness probe. Better to just replace the resource entirely.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The <span class="codeitalic">kubectl patch</span> command is a valuable command for debugging, but production applications should have YAML resource files under version control to allow for change tracking and peer review, and the entire file should always be applied every time to ensure that the cluster reflects the current content of the repository.</em></p>&#13;
</div>&#13;
<p class="indent">Now that we’ve applied the new Deployment configuration, Kubernetes will make a new Pod:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                    READY   STATUS    RESTARTS   AGE&#13;
nginx-d75d4d675-wvhxl   1/1     Running   0          2m38s</pre>&#13;
<p class="indent">For an HTTP probe, <code>kubelet</code> has the responsibility of running an HTTP GET request on the appropriate schedule and confirming the result. By default, any HTTP return code in the 200 or 300 series is considered a successful response.</p>&#13;
<p class="indent">The NGINX server is logging all of its requests, so we can use the container logs to see the probes taking place:</p>&#13;
<pre><span epub:type="pagebreak" id="page_225"/>root@host01:~# <span class="codestrong1">kubectl logs</span> <span class="codestrong1"><span class="codeitalic1">nginx-d75d4d675-wvhxl</span></span>&#13;
...&#13;
... 22:23:31 ... "GET / HTTP/1.1" 200 615 "-" "kube-probe/1.21" "-"&#13;
... 22:23:41 ... "GET / HTTP/1.1" 200 615 "-" "kube-probe/1.21" "-"&#13;
... 22:23:51 ... "GET / HTTP/1.1" 200 615 "-" "kube-probe/1.21" "-"</pre>&#13;
<p class="indent">We didn’t specify <code>periodSeconds</code> this time, so <code>kubelet</code> is probing the server at the default rate of once every 10 seconds.</p>&#13;
<p class="indent">Let’s clean up the NGINX Deployment before moving on:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete deployment nginx</span>&#13;
deployment.apps "nginx" deleted</pre>&#13;
<p class="indent">We’ve looked at two of the three types of probes; let’s finish by looking at TCP.</p>&#13;
<h4 class="h4" id="ch00lev2sec89">TCP Probes</h4>&#13;
<p class="noindent">A database server such as PostgreSQL listens for network connections, but it does not use HTTP for communication. We can still create a probe for these kinds of containers using a TCP probe. It won’t provide the configuration flexibility of an HTTP or exec probe, but it will verify that a container in the Pod is listening for connections on the specified port.</p>&#13;
<p class="indent">Here’s a PostgreSQL Deployment with a TCP probe:</p>&#13;
<p class="noindent6"><em>postgres-tcp.yaml</em></p>&#13;
<pre>---&#13;
kind: Deployment&#13;
apiVersion: apps/v1&#13;
metadata:&#13;
  name: postgres&#13;
spec:&#13;
  replicas: 1&#13;
  selector:&#13;
    matchLabels:&#13;
      app: postgres&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: postgres&#13;
    spec:&#13;
      containers:&#13;
      - name: postgres&#13;
        image: postgres&#13;
        env:&#13;
        - name: POSTGRES_PASSWORD&#13;
          value: "supersecret"&#13;
        livenessProbe:&#13;
<span epub:type="pagebreak" id="page_226"/>          tcpSocket:&#13;
            port: 5432</pre>&#13;
<p class="indent">We saw the requirement for the <code>POSTGRES_PASSWORD</code> environment variable in <a href="ch10.xhtml#ch10">Chapter 10</a>. The only configuration that’s changed for this example is the <code>livenessProbe</code>. We specify a TCP socket of 5432, as this is the standard port for PostgreSQL.</p>&#13;
<p class="indent">As usual, we can create this Deployment and, after a while, observe that it’s running:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/postgres-tcp.yaml</span> &#13;
deployment.apps/postgres created&#13;
...&#13;
root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                       READY   STATUS    RESTARTS   AGE&#13;
postgres-5566ff748-jqp5d   1/1     Running   0          29s</pre>&#13;
<p class="indent">Again, it is the job of <code>kubelet</code> to perform the probe. It does this solely by making a TCP connection to the port and then disconnecting. PostgreSQL doesn’t emit any logging when this happens, so the only way we know that the probe is working is to check that the container continues to run and doesn’t show any restarts:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                       READY   STATUS    RESTARTS   AGE&#13;
postgres-5566ff748-jqp5d   1/1     Running   0          2m7s</pre>&#13;
<p class="indent">Before we move on, let’s clean up the Deployment:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl delete deploy postgres</span>&#13;
deployment.apps "postgres" deleted</pre>&#13;
<p class="indent">We’ve now looked at all three types of probes. And although we used these three types to create liveness probes, the same three types will work with both startup and readiness probes as well. The only difference is the change in the behavior of our cluster when a probe fails.</p>&#13;
<h3 class="h3" id="ch00lev1sec57">Startup Probes</h3>&#13;
<p class="noindent">Unhealthy containers can create all kinds of difficulties for an application, including lack of responsiveness, errors responding to requests, or bad data, so we want Kubernetes to respond quickly when a container becomes unhealthy. However, when a container is first started, it can take time before it is fully initialized. During that time, it might not be able to respond to liveness probes.</p>&#13;
<p class="indent">Because of that delay, we’re left with a need to have a long timeout before a container fails a probe, so we can give our container enough time for initialization. However, at the same time, we need to have a short timeout in order to detect a failed container quickly and restart it. The solution is <span epub:type="pagebreak" id="page_227"/>to configure a separate <em>startup probe</em>. Kubernetes will use the startup probe configuration until the probe is successful; then it will switch over to the liveness probe.</p>&#13;
<p class="indent">For example, we might configure our NGINX server Deployment as follows:</p>&#13;
<pre>...&#13;
spec:&#13;
...&#13;
  template:&#13;
...&#13;
    spec:&#13;
      containers:&#13;
      - name: nginx&#13;
        image: nginx&#13;
        livenessProbe:&#13;
          httpGet:&#13;
            path: /&#13;
            port: 80&#13;
        startupProbe:&#13;
          httpGet:&#13;
            path: /&#13;
            port: 80&#13;
          periodSeconds: &#13;
          initialDelaySeconds: 30&#13;
          periodSeconds: 10&#13;
          failureThreshold: 60</pre>&#13;
<p class="indent">Given this configuration, Kubernetes would start checking the container 30 seconds after startup. It would continue checking every 10 seconds until the probe is successful or until there are 60 failed attempts. The effect is that the container has 10 minutes to finish initialization and respond to a probe successfully. If the container does not have a successful probe in that time, it will be restarted.</p>&#13;
<p class="indent">As soon as the container has one successful probe, Kubernetes will switch to the configuration for <code>livenessProbe</code>. Because we didn’t override any timing parameters, this will transition to a probe every 10 seconds, with three consecutive failed probes leading to a restart. We give the container 10 minutes to be live initially, but after that we will allow no more than 30 seconds before restarting it.</p>&#13;
<p class="indent">The fact that the <code>startupProbe</code> is defined completely separately means that it is possible to create a different check for startup from the one used for liveness. Of course, it’s important to choose wisely so that the container doesn’t pass its startup probe before the liveness probe would also pass, because that would result in inappropriate restarts.</p>&#13;
<h3 class="h3" id="ch00lev1sec58"><span epub:type="pagebreak" id="page_228"/>Readiness Probes</h3>&#13;
<p class="noindent">The third probe purpose is to check the <em>readiness</em> of the Pod. The term <em>readiness</em> might seem redundant with the startup probe. However, even though completing initialization is an important part of readiness for a piece of software, an application component might not be ready to do work for many reasons, especially in a highly available microservice architecture where components can come and go at any time.</p>&#13;
<p class="indent">Rather than being used for initialization, readiness probes should be used for any case in which the container cannot perform any work because of a failure outside its control. It may be a temporary situation, as retry logic somewhere else could fix the failure. For example, an API that relies on an external database might fail its readiness probe if the database is unreachable, but that database might return to service at any time.</p>&#13;
<p class="indent">This also creates a valuable contrast with startup and liveness probes. As we examined earlier, Kubernetes will restart a container if it fails the configured number of startup or liveness probes. But it makes no sense to do that if the issue is a failed or missing external dependency, given that restarting the container won’t fix whatever is wrong externally.</p>&#13;
<p class="indent">At the same time, if a container is missing a required external dependency, it can’t do work, so we don’t want to send any work to it. In that situation, the best thing to do is to leave the container running and give it an opportunity to reestablish the connections it needs, but avoid sending any requests to it. In the meantime, we can hope that somewhere in the cluster another Pod for the same Deployment is working as expected, making our application as a whole resilient to a localized failure.</p>&#13;
<p class="indent">This is exactly how readiness probes work in Kubernetes. As we saw in <a href="ch09.xhtml#ch09">Chapter 9</a>, a Kubernetes Service continually watches for Pods that match its selector and configures load balancing for its cluster IP that routes traffic to those Pods. If a Pod reports itself as not ready, the Service will stop routing traffic to it, but <code>kubelet</code> will not trigger any other action such as a container restart.</p>&#13;
<p class="indent">Let’s illustrate this situation. We want to have individual control over Pod readiness, so we’ll use a somewhat contrived example rather than a real external dependency to determine readiness. We’ll deploy a set of NGINX Pods, this time with a corresponding Service:</p>&#13;
<p class="noindent6"><em>nginx-ready.yaml</em></p>&#13;
<pre>---&#13;
kind: Deployment&#13;
apiVersion: apps/v1&#13;
metadata:&#13;
  name: nginx&#13;
spec:&#13;
  replicas: 3&#13;
  selector:&#13;
    matchLabels:&#13;
      app: nginx&#13;
  template:&#13;
<span epub:type="pagebreak" id="page_229"/>    metadata:&#13;
      labels:&#13;
        app: nginx&#13;
    spec:&#13;
      containers:&#13;
      - name: nginx&#13;
        image: nginx&#13;
        livenessProbe:&#13;
          httpGet:&#13;
            path: /&#13;
            port: 80&#13;
        readinessProbe:&#13;
          httpGet:&#13;
            path: /ready&#13;
            port: 80&#13;
---&#13;
kind: Service&#13;
apiVersion: v1&#13;
metadata:&#13;
  name: nginx&#13;
spec:&#13;
  selector:&#13;
    app: nginx&#13;
  ports:&#13;
  - protocol: TCP&#13;
    port: 80&#13;
    targetPort: 80</pre>&#13;
<p class="indent">This Deployment keeps its <code>livenessProbe</code> as an indicator that NGINX is working correctly and adds a <code>readinessProbe</code>. The Service definition is identical to what we saw in <a href="ch09.xhtml#ch09">Chapter 9</a> and will route traffic to our NGINX Pods.</p>&#13;
<p class="indent">This file has already been written to <em>/opt</em>, so we can apply it to the cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl apply -f /opt/nginx-ready.yaml</span> &#13;
deployment.apps/nginx created&#13;
service/nginx created</pre>&#13;
<p class="indent">After these Pods are up and running, they stay running because the liveness probe is successful:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                     READY   STATUS    RESTARTS   AGE&#13;
nginx-67fb6485f5-2k2nz   0/1     Running   0          38s&#13;
nginx-67fb6485f5-vph44   0/1     Running   0          38s&#13;
nginx-67fb6485f5-xzmj5   0/1     Running   0          38s</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_230"/>In addition, the Service we created has been allocated a cluster IP:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get services</span>&#13;
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE&#13;
...&#13;
nginx        ClusterIP   10.101.98.80   &lt;none&gt;        80/TCP    3m1s</pre>&#13;
<p class="indent">However, we aren’t able to use that IP address to reach any Pods:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">curl http://<span class="codeitalic1">10.101.98.80</span></span>&#13;
curl: (7) Failed to connect to 10.101.98.80 port 80: Connection refused</pre>&#13;
<p class="indent">This is because, at the moment, there is nothing for NGINX to serve on the <em>/ready</em> path, so it’s returning <code>404</code>, and the readiness probe is failing. A detailed inspection of the Pod shows that it is not ready:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe pod</span>&#13;
Name:         nginx-67fb6485f5-2k2nz&#13;
...&#13;
Containers:&#13;
  nginx:&#13;
...&#13;
    Ready:          False&#13;
...</pre>&#13;
<p class="indent">As a result, the Service does not have any Endpoints to which to route traffic:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe service nginx</span>&#13;
Name:              nginx&#13;
...&#13;
Endpoints:         &#13;
...</pre>&#13;
<p class="indent">Because the Service has no Endpoints, it has configured <code>iptables</code> to reject all traffic:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">iptables-save | grep default/nginx</span>&#13;
-A KUBE-SERVICES -d 10.101.98.80/32 -p tcp -m comment --comment "default/nginx has no endpoints"  &#13;
  -m tcp --dport 80 -j REJECT --reject-with icmp-port-unreachable</pre>&#13;
<p class="indent">To fix this, we’ll need at least one Pod to become ready to ensure that NGINX has something to serve on the <em>/ready</em> path. We’ll use the container’s hostname to keep track of which Pod is serving our request.</p>&#13;
<p class="indent">To make one of our Pods ready, let’s first get the list of Pods again, just to have the Pod names handy:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
NAME                     READY   STATUS    RESTARTS   AGE&#13;
nginx-67fb6485f5-2k2nz   0/1     Running   0          10m&#13;
<span epub:type="pagebreak" id="page_231"/>nginx-67fb6485f5-vph44   0/1     Running   0          10m&#13;
nginx-67fb6485f5-xzmj5   0/1     Running   0          10m</pre>&#13;
<p class="indent">Now, we’ll choose one and make it report that it is ready:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti <span class="codeitalic1">nginx-67fb6485f5-2k2nz</span> -- \</span>&#13;
  <span class="codestrong1">cp -v /etc/hostname /usr/share/nginx/html/ready</span>&#13;
'/etc/hostname' -&gt; '/usr/share/nginx/html/ready'</pre>&#13;
<p class="indent">Our Service will start to show a valid Endpoint:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe svc nginx</span>&#13;
Name:              nginx&#13;
...&#13;
Endpoints:         172.31.239.199:80&#13;
...</pre>&#13;
<p class="indent">Even better, we can now reach an NGINX instance via the cluster IP, and the content corresponds to the hostname:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">curl http://<span class="codeitalic1">10.101.98.80</span>/ready</span>&#13;
nginx-67fb6485f5-2k2nz</pre>&#13;
<p class="indent">Note the <code>/ready</code> at the end of the URL so the response is the hostname. If we run this command many times, we’ll see that the hostname is the same every time. This is because the one Pod that is passing its readiness probe is handling all of the Service traffic.</p>&#13;
<p class="indent">Let’s make the other two Pods ready as well:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl exec -ti <span class="codeitalic1">nginx-67fb6485f5-vph44</span> -- \</span>&#13;
  <span class="codestrong1">cp -v /etc/hostname /usr/share/nginx/html/ready</span>&#13;
'/etc/hostname' -&gt; '/usr/share/nginx/html/ready'&#13;
root@host01:~# <span class="codestrong1">kubectl exec -ti <span class="codeitalic1">nginx-67fb6485f5-xzmj5</span> -- \</span>&#13;
  <span class="codestrong1">cp -v /etc/hostname /usr/share/nginx/html/ready</span>&#13;
'/etc/hostname' -&gt; '/usr/share/nginx/html/ready'</pre>&#13;
<p class="indent">Our Service now shows all three Endpoints:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe service nginx</span>&#13;
Name:              nginx&#13;
...&#13;
Endpoints:         172.31.239.199:80,172.31.239.200:80,172.31.89.210:80&#13;
...</pre>&#13;
<p class="indent">Running the <code>curl</code> command multiple times shows that the traffic is now being distributed across multiple Pods:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">for i in $(seq 1 5); do curl http://<span class="codeitalic1">10.101.98.80</span>/ready; done</span>&#13;
nginx-67fb6485f5-xzmj5&#13;
nginx-67fb6485f5-2k2nz&#13;
nginx-67fb6485f5-xzmj5&#13;
<span epub:type="pagebreak" id="page_232"/>nginx-67fb6485f5-vph44&#13;
nginx-67fb6485f5-vph44</pre>&#13;
<p class="indent">The embedded command <code>$(seq 1 5)</code> returns the numbers one through five, causing the <code>for</code> loop to run <code>curl</code> five times. If you run this same <code>for</code> loop several times, you will see a different distribution of hostnames. As described in <a href="ch09.xhtml#ch09">Chapter 9</a>, load balancing is based on a random uniform distribution wherein each endpoint has an equal chance of being selected for each new connection.</p>&#13;
<p class="indent">A good practice is to offer an HTTP readiness endpoint for each application that checks the current state of the application and its dependencies and returns an HTTP success code (such as <code>200</code>) if the component is healthy, and an HTTP error code (such as <code>500</code>) if not. Some application frameworks such as Spring Boot provide application state management that automatically exposes liveness and readiness endpoints.</p>&#13;
<h3 class="h3" id="ch00lev1sec59">Final Thoughts</h3>&#13;
<p class="noindent">Kubernetes offers the ability to check on our containers and make sure they are working as expected, not just that the process is running. These probes can include any arbitrary command run inside the container, verifying that a port is open for TCP connections, or that the container responds correctly to an HTTP request. To build resilient applications, we should define both a liveness probe and a readiness probe for each application component. The liveness probe is used to restart an unhealthy container; the readiness probe determines whether the Pod can handle Service traffic. Additionally, if a component needs extra time for initialization, we should also define a startup probe to make sure that give it the required initialization time while responding quickly to failure as soon as initialization is complete.</p>&#13;
<p class="indent">Of course, for our containers to run as expected, other containers in the cluster must also be well behaved, not using too many of the cluster’s resources. In the next chapter, we’ll look at how we can limit our containers in their use of CPU, memory, disk space, and network bandwidth, as well as how we can control the maximum amount of total resources available to a user. This ability to specify limits and quotas is important to ensure that our cluster can support multiple applications with reliable performance.</p>&#13;
</body></html>