<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" epub:prefix="index: http://www.index.com/" lang="en" xml:lang="en">
<head>
<title>Chapter 7: Tools for Topological Data Analysis</title>
<link href="NSTemplate_v1.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:ea5eeba6-9dea-4463-bfe4-b91d6c6b5861" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter chapter">
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_155" title="155"/><a class="XrefDestination" id="7"/><span class="XrefDestination" id="xref-503083c07-001"/>7</span><br/>
<span class="ChapterTitle"><a class="XrefDestination" id="TopologicalDataAnalysis"/><span class="XrefDestination" id="xref-503083c07-002"/>Tools for Topological Data Analysis</span></h1>
</header>
<figure class="opener">
<img alt="" src="image_fi/book_art/chapterart.png"/>
</figure>
<p class="ChapterIntro">In this chapter, we’ll explore algorithms that have a direct basis in topology and use them to understand the dataset of self-reported educational data encountered in <span class="xref" itemid="xref_target_Chapter 6"><a href="c06.xhtml">Chapter 6</a></span>. The branch of machine learning that includes topology-based algorithms is called <em>topological data analysis (TDA)</em>. You already saw some TDA in <span class="xref" itemid="xref_target_Chapter 4"><a href="c04.xhtml">Chapter 4</a></span>, where we used persistent homology to explore network differences. Persistent homology has gained a lot of attention in the machine learning community lately and has been used in psychometric data validation, image comparison analyses, pooling steps of convolutional neural networks, and comparisons of small samples of data. In this chapter, we’ll reexamine persistent homology and look at the Mapper algorithm (now commercialized by Ayasdi).</p>
<h2 id="h1-503083c07-0001"><span epub:type="pagebreak" id="Page_156" title="156"/><a class="XrefDestination" id="GroupingGiftedStudentsbySchoolingExperience"/><span class="XrefDestination" id="xref-503083c07-003"/>Finding Distinctive Groups with Unique Behavior</h2>
<p class="BodyFirst">Previously, we used persistent homology to distinguish different types of graphs. Recall from <span class="xref" itemid="xref_target_Chapter 4"><a href="c04.xhtml">Chapter 4</a></span> that persistent homology creates simplicial complexes from point cloud data, applies a series of thresholds to those simplicial complexes, and calculates a series of numbers related to topological features present within each thresholded simplicial complex. To compare objects, we can use Wasserstein distance to measure the differences in topological features across slices.</p>
<p>Persistent homology has many uses in industry today. <em>Subgroup mining</em>, where we look for distinctive groups with unique behavior in the data, is one prominent use. In particular, we’re often searching for connected components with the zeroth homology groups, or groups that are connected to each other geometrically (such as clusters in hierarchical clustering). In psychometric survey validation, for example, subgroup mining allows us to find distinct groups within the survey, such as different subtypes of depression within a survey measuring depression.</p>
<p>Let’s walk through a practical example of subgroup mining with persistent homology related to self-reported educational data from a social networking site. We’ll simulate data and compare persistent homology results using the TDAstats package in R and single-linkage hierarchical clustering using the <code>hclust()</code> function in R (see <a href="#listing7-1" id="listinganchor7-1">Listing 7-1</a>). We’ll return to <span class="xref" itemid="xref_target_Chapter 6"><a href="c06.xhtml">Chapter 6</a></span>’s example dataset of gifted Quora users self-reporting their school experiences (see the book files for this dataset). In this example, we’ll split the sample into sets of 11 individuals so that we can compare the persistent homology results statistically to ensure our measurements don’t vary across samples from our population of students. This provides a validation that our measurement is consistent across the population.</p>
<pre><code>#load data and set seed
mydata&lt;-read.csv("QuoraSample.csv")
set.seed(1)

#sample data to split into two datasets; remove the IQ scores
s&lt;-sample(1:22,11)
set1&lt;-mydata[s,-1]
set2&lt;-mydata[-s,-1]</code></pre>
<p class="CodeListingCaption"><a id="listing7-1">Listing 7-1</a>: A script that loads the educational dataset and splits it into two sets to be explored with persistent homology</p>
<p>Now that we have our dataset, we can apply persistent homology to understand the clusters. Specifically, we’re looking at the zeroth Betti numbers, which correspond to connected groups, and other topological features of the data—see <span class="xref" itemid="xref_target_Chapter 4"><a href="c04.xhtml">Chapter 4</a></span> for a refresher.</p>
<p>First, we need to compute the Manhattan distances between each student in the social network; we’ll use these to define the filtration. <span epub:type="pagebreak" id="Page_157" title="157"/>Manhattan distances are often a go-to distance metric for discrete data. Add the following to your script:</p>
<pre><code>#calculate Manhattan distance between pairs of scores
mm1&lt;-dist(set1,"manhattan",diag=T,upper=T)</code></pre>
<p>Next, we want to apply the persistent homology algorithm to the distance-based data to reveal the persistent features. Using the TDAstats package, we can then add code to compute the zeroth and first Betti numbers of this dataset, using a relatively low-filtration setting set as the largest scale for the approximation (this will give us larger clusters). Finally, we can plot the results in a persistence diagram and a plot of hierarchical clustering:</p>
<pre><code>#create the Vietoris-Rips complex (turn data into a simplicial complex)
library(TDAstats)
d1&lt;-calculate_homology(mm1,dim=1,format="cloud")
#plot persistence diagrams where circles=connected components, triangles=loops
plot_persist(d1)
#hierachical cluster
plot(hclust(mm1),main="Hierarchical Clustering Results, Distance Data")</code></pre>
<p>The <code>calculate_homology()</code> function converts the point-cloud data from the distance dataset to a simplicial complex; we can then apply a filtration to identify topological features appearing and disappearing across the filtration. There are other methods that can create simplicial complexes from data, but the Rips complex in this package is one of the easiest to compute.</p>
<p>Using the previous code, we’ve plotted two figures. The call to <code>plot_persist()</code> should give something like <a href="#figure7-1" id="figureanchor7-1">Figure 7-1</a>. You can see there that it appears one main group exists, along with possibly a subgroup or two at the lower filtration level; however, the subgroup may or may not be a significant feature, as it is near the diagonal.</p>
<figure>
<img alt="" class="" src="image_fi/503083c07/f07001.png"/>
<figcaption><p><a id="figure7-1">Figure 7-1</a>: A persistence diagram of the first set of educational experience data</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_158" title="158"/>When using the hierarchical clustering results (<a href="#figure7-2" id="figureanchor7-2">Figure 7-2</a>), it’s easy to see a main group and then several splits at smaller distance thresholds.</p>
<figure>
<img alt="" class="" src="image_fi/503083c07/f07002.png"/>
<figcaption><p><a id="figure7-2">Figure 7-2</a>: A dendrogram of the simulated data</p></figcaption>
</figure>
<p>If you cut the clusters at a height of 5, the dendrogram results suggest that two main subgroups exist. Let’s split the <code>set1</code> data according to the two main clusters found in the hierarchical clustering by adding to <a href="#listing7-1">Listing 7-1</a>, first examining the smaller cluster and then examining the larger cluster:</p>
<pre><code>#smaller cluster
mydata[c(1,9,10,17),]</code></pre>
<p>This should output the following:</p>
<pre><code>&gt; <b>mydata[c(1,9,10,17),]</b>
    IQ      Bullying Teacher.Hostility Boredom Depression Lack.of.Motivation
1  187             0                 1       0          0                  1
9  182             1                 1       0          0                  1
10 161             0                 0       1          0                  1
17 170             1                 0       0          0                  0
   Outside.Learning Put.in.Remedial.Course
1                 0                      0
9                 0                      1
10                0                      0
17                0                      0</code></pre>
<p>In this cluster of individuals, no depression or outside learning was reported. Some individuals did report bullying, teacher hostility, boredom, remediation, or lack of motivation. Let’s contrast that with the larger cluster found in our dendrogram:</p>
<pre><code>#larger cluster
mydata[c(3,4,6,8,12,16,18),]</code></pre>
<p><span epub:type="pagebreak" id="Page_159" title="159"/>This should output something like this:</p>
<pre><code>&gt; <b>mydata[c(3,4,6,8,12,16,18),]</b>
    IQ Bullying Teacher.Hostility Boredom Depression Lack.of.Motivation
3  155        0                 0       0          0                  0
4  155        0                 0       0          0                  0
6  174        0                 0       1          1                  1
8  170        0                 0       0          0                  0
12 175        0                 1       0          0                  0
16 160        0                 1       0          0                  0
18 185        0                 0       0          0                  1
   Outside.Learning Put.in.Remedial.Course
3                 1                      0
4                 1                      0
6                 1                      1
8                 0                      1
12                1                      1
16                1                      1
18                1                      0</code></pre>
<p>Compared to the first cluster, these individuals mostly report outside learning and no bullying. This seems to separate learning experiences while in school. Learning outside of school and not dealing with bullying may have relevance to learning outcomes and overall school experience for students.</p>
<p>One item of interest in this analysis is individual 6, who seems to be an outlier in the <a href="#figure7-2">Figure 7-2</a> dendrogram. This individual did not deal with bullying or teacher hostility but did deal with every other issue during their schooling. Outliers can be important and influential in analyses. Topology-based algorithms like persistent homology are often more robust to outliers than other algorithms and statistical models: extreme values or subgroups in the population won’t impact the results as dramatically when we use TDA as compared to other methods. For instance, in our gifted sample, individual 6 might impact k-means results more than the results of persistent homology.</p>
<p>Subgroup mining is one important use of persistent homology—both for identifying groups within the dataset and for identifying outliers that might impact optimization steps in more traditional clustering methods. Let’s continue by exploring another important use: as a measurement validation tool.</p>
<h2 id="h1-503083c07-0002"><a class="XrefDestination" id="ComparingSampleswithDistanceMetrics"/><span class="XrefDestination" id="xref-503083c07-004"/>Validating Measurement Tools</h2>
<p class="BodyFirst">Many methods exist to compare dendrograms or persistence diagrams; this is still an active area of research. Persistence diagrams need to be turned into metric spaces, which allows us to construct nonparametric tests with a compatible distance metric, which in turn lets us compare two diagrams and simulate random samples from the null distribution, which finally we <span epub:type="pagebreak" id="Page_160" title="160"/>can use to compare the test distance. All in all, this lets us validate measurement tools. In our example, we want to validate our measurement of school problems by comparing samples from the same population (our Quora sample). If persistent homology results are the same, our measurement tool is consistent, which is a key property of measurement design in the field of psychometrics.</p>
<p>For persistence diagrams, we typically use the Wasserstein distance, as it works well for comparing distributions and sets of points in finite samples. For dendrograms, Hausdorff and Gromov–Hausdorff distance are two good options, both of which measure the largest distance within a set of smallest distances between points on a shape. However, the Gromov–Hausdorff distance is more complicated and imposes more structural information, which makes it less ideal.</p>
<p>To compare the distances of another persistence diagram to the current one, let’s use the second set of individuals in our self-reported educational dataset, adding to <a href="#listing7-1">Listing 7-1</a>:</p>
<pre><code>#compute Manhattan distance for set 2
mm2&lt;-dist(set2,"manhattan",diag=T,upper=T)

#create the Vietoris-Rips complex
d2&lt;-calculate_homology(mm2,dim=1,format="cloud")

#plot persistence diagrams—circles=connected components, triangles=loops
plot_persist(d2)</code></pre>
<p>Note that we’ve changed the dataset being analyzed to the second set of individuals from the full sample. This creates a comparison set that should be part of the same population; in this example, there are more potential subgroups that come out in the analysis. The plot should look something like <a href="#figure7-3" id="figureanchor7-3">Figure 7-3</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c07/f07003.png"/>
<figcaption><p><a id="figure7-3">Figure 7-3</a>: Another persistence diagram of simulated data, this time with different parameters used to simulate data</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_161" title="161"/>In <a href="#figure7-3">Figure 7-3</a>, we see a few distinct groups similar to <a href="#figure7-1">Figure 7-1</a>’s sample. We also see some points corresponding to Betti number 1; however, given how close they are to the line, these points are likely noise. The farther from the diagonal line a point lies, the more likely it is a real feature in the dataset. These new Betti number features are different than our prior sample but likely not real features.</p>
<p>Computing the distance between the diagrams is easy with the TDAstats package. Add these lines to your script:</p>
<pre><code>#calculate distance between diagrams
w&lt;-phom.dist(d1,d2,limit.num=0)
w</code></pre>
<p>This computes the distance between the persistence diagrams for the zeroth and first homology groups shown in <a href="#figure7-1">Figure 7-1</a> and <a href="#figure7-3">Figure 7-3</a> and should yield a distance of approximately 10.73 (zeroth homology) and 0.44 (first homology), though the values may vary according to your version of R. Now it’s possible to compute the distances between random samples drawn from the original sample. The TDAstats package has a handy way of computing this within a function so that we don’t have to write the entire test ourselves. Let’s add these pieces to our script:</p>
<pre><code>#compute permutation test and examine p-values from output
permutation_test(d1,d2,iterations=25)</code></pre>
<p>This script will now compute a permutation test between the two samples’ features, yielding a test statistic and p-value for each homology level computed. As expected, our zeroth homology differences are not significant at a 95 percent confidence level (<em>p</em> = 0.08). Given that we don’t have any first homology features in our first sample, we do see a significant difference between the samples for our first homology differences; however, the statistic itself is 0, suggesting that this is an artificial finding.</p>
<p>While this example involves a convenience sample without an actual survey being administered, it does relate to how a real psychometric tool administered across population samples can be validated through persistent homology. We can also use this methodology to compare differences across different populations to explore how a measurement’s behavior changes across populations. Perhaps one sample of students had been accelerated (skipped one or more grades) and one had not. We might end up with very different self-reported experiences in school. In this case, the measurement tool might show very different behavior across the proposed accelerated and nonaccelerated groups.</p>
<h2 id="h1-503083c07-0003"><a class="XrefDestination" id="UsingtheMapperAlgorithm"/><span class="XrefDestination" id="xref-503083c07-005"/>Using the Mapper Algorithm for Subgroup Mining</h2>
<p class="BodyFirst">In data science, we often are faced with clustering problems where data has extreme variable scale differences, includes sparse or spread-out data, includes outliers, or has substantial group overlap. These scenarios can pose issues to common clustering algorithms like k-means (group overlap, <span epub:type="pagebreak" id="Page_162" title="162"/>in particular) or DBSCAN (sparse or spread-out data). The <em>Mapper algorithm</em>—which finds clusters through a multistage process based on binning, clustering, and pulling back the clusters into a graph or simplicial complex—is another useful clustering tool for subgroup mining. This algorithm ties together some of the concepts in Morse theory with the filtration concept in persistent homology to provide a topologically grounded clustering algorithm.</p>
<h3 id="h2-503083c07-0001"><a class="XrefDestination" id="VisualizingtheMapperAlgorithm"/><span class="XrefDestination" id="xref-503083c07-006"/>Stepping Through the Mapper Algorithm</h3>
<p class="BodyFirst">The basic steps of the Mapper algorithm involve filtering a point cloud using a scalar-valued function called a <em>Morse function</em>; we then separate data into overlapping bins, cluster data within each bin, and connect the clusters into a graph or simplicial complex, based on overlap of the clusters across bins. To visualize this, let’s consider a simple point cloud with a defined scalar-valued function; we’ll shade the object according to the results we get when applying the function to the point cloud. Take a look at the results in <a href="#figure7-4" id="figureanchor7-4">Figure 7-4</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c07/f07004r.png"/>
<figcaption><p><a id="figure7-4">Figure 7-4</a>: A multishaded object with a Morse function defined by a shade gradient</p></figcaption>
</figure>
<p>This shape can now be chunked into four overlapping bins. This allows us to see potentially interesting relationships between areas with slightly different Morse function values, which will become relevant when we apply a clustering algorithm, as in <a href="#figure7-5" id="figureanchor7-5">Figure 7-5</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c07/f07005r.png"/>
<figcaption><p><a id="figure7-5">Figure 7-5</a>: Binning results that chunk <a href="#figure7-4">Figure 7-4</a> by shade gradient</p></figcaption>
</figure>
<p>Now that we’ve binned our function (<a href="#figure7-5">Figure 7-5</a>), we can start clustering. The clustering across bins can get a little bit more complicated than simply applying a clustering algorithm. This clustering is needed to define the complex and the overlapping of clusters across bins. Clustering within each of these bins and combining results to understand connectivity of <span epub:type="pagebreak" id="Page_163" title="163"/>clusters across bins would give a final result. An advantage of the Mapper algorithm is that results can be easily visualized as a graph or simplex; the final result of our example would likely output something like <a href="#figure7-6" id="figureanchor7-6">Figure 7-6</a>, where two distinct groups evolve from a single point connecting them.</p>
<figure>
<img alt="" class="" src="image_fi/503083c07/f07006r.png"/>
<figcaption><p><a id="figure7-6">Figure 7-6</a>: The clusters defined by binning the results of <a href="#figure7-4">Figure 7-4</a></p></figcaption>
</figure>
<p>In practice, a distance metric—correlation, Euclidean distance, Hamming distance, and so on—is typically applied to the raw data before filtering as a way to process the point cloud data and create better filter functions prior to clustering. Clustering of the distance metric dataset can be done with a variety of algorithms, though single-linkage hierarchical clustering is usually used in practice. The coordinate systems used generally don’t matter for Mapper results or results from other TDA algorithms.</p>
<p>There are a few advantages of the Mapper algorithm over other clustering methods, as well as topological data analysis compared to other methods in general. Invariance under small perturbations (noise) in the data allows Mapper to be more robust than k-means, which is sensitive to different starting seeds and can come up with very different results for each run. (Note that Mapper is sensitive to parameter changes but fairly robust to repeated runs with added noise.) The compression or visualization of results allows for easy visualization of clustering results for high-dimensional data. The lack of dependence on coordinate systems allows us to compare data on different scales or collected from different platforms. In addition, Mapper can deal with cluster overlap, which poses significant challenges to k-means algorithms and their derivatives. Lastly, Mapper’s ability to handle sparsity and outliers gives it an advantage over DBSCAN. This makes it ideal for use on small datasets, datasets where predictors might outnumber observations, or messy data that is likely to contain a lot of noise.</p>
<h3 id="h2-503083c07-0002"><a class="XrefDestination" id="ImplementingtheMapperAlgorithm"/><span class="XrefDestination" id="xref-503083c07-007"/>Using TDAmapper to Find Cluster Structures in Data</h3>
<p class="BodyFirst">The TDAmapper R package provides an implementation of the Mapper algorithm that can handle many types of processed data. For this example, we’ll return again to the self-reported educational dataset from the sample of gifted Quora users, including seven main school issues (bullying, teacher hostility, boredom, depression, lack of motivation, outside learning, put in remediation courses) reported across 22 individuals who provided scores in the gifted range and discussed at least one of the issues of interest. The objective is to understand the relation between issues within this sample (somewhat like creating subscales within the measurement). This is binary <span epub:type="pagebreak" id="Page_164" title="164"/>data, so we’ll use inverse Hamming distance to obtain a distance matrix. Hamming distance measures bit-by-bit differences in binary strings to get a dissimilarity measurement. Other distances can be used on binary data, but Hamming distance works well to compare overall differences between individuals scored on binary variables.</p>
<p>Let’s load the data and prepare it for analysis in <a href="#listing7-2" id="listinganchor7-2">Listing 7-2</a>:</p>
<pre><code>#load data and clean
mydata&lt;-read.csv("QuoraSample.csv")
mydata&lt;-mydata[,-1]

#load Mapper, igraph, and distance packages
library(TDAmapper)
library(igraph)
library(e1071)

#process data to turn it into a distance matrix
df&lt;-data.frame()
for (j in 1:7){
  for (i in 1:7){
    df[i,j]&lt;-1/(hamming.distance(mydata[,i],mydata[,j]))
  }
}
df[df&gt;1]&lt;-1</code></pre>
<p class="CodeListingCaption"><a id="listing7-2">Listing 7-2</a>: A script that loads and processes the data to obtain a distance matrix</p>
<p>The code in <a href="#listing7-2">Listing 7-2</a> loads our dataset and packages needed for the analysis and then processes the data to obtain a distance matrix to feed into the Mapper algorithm. Other distances can be used on binary data, but Hamming distance works well to compare overall differences between individuals scored on binary variables.</p>
<p>Now let’s apply the Mapper algorithm. We’ll set Mapper to process the distance matrix using three intervals with 70 percent overlap and three bins for clustering. A higher overlap parameter on a small dataset will encourage connectivity between clusters found across bins; in practice, a setting between 30 to 70 percent usually gives good results. In addition, the small number of intervals and bins correspond to about half the number of instances to be clustered in this dataset, which usually works well in practice. Generally, it’s useful to use different parameter settings, as the results will vary depending on starting parameters; a few recent papers have suggested that the Mapper algorithm with nonvarying parameters is not wholly stable with respect to results. We’ll also set Filter values according to minimum and maximum Hamming distances. We can do both by adding these lines to the script in <a href="#listing7-2">Listing 7-2</a>:</p>
<pre><code>#apply mapper
j&lt;-mapper1D(as.matrix(df),num_intervals=3,percent_overlap=70, num_bins_when_clustering=3,
filter_values=c(0.025,0.05,0.075,0.1,0.125,0.15,0.2))
summary(j)
j$points_in_vertex</code></pre>
<p><span epub:type="pagebreak" id="Page_165" title="165"/>This code runs the Mapper algorithm on the data with the parameters set earlier. The summary gives a list of objects in the Mapper object regarding results. The summary of points within a vertex gives us information as to how these variables separate into clusters.</p>
<p>Exploring the Mapper object yields some insight into which issues cluster together. We can gather a lot of information from the Mapper object, but this exploration will be limited to understanding which points from the dataset ended up in which cluster (vertex) in the Mapper object. Let’s examine the output from our last addition to <a href="#listing7-2">Listing 7-2</a>:</p>
<pre><code>$points_in_vertex
$points_in_vertex [[1]]
[1] 1

$points_in_vertex [[2]]
[1] 2

$points_in_vertex [[3]]
[1] 3

$points_in_vertex [[4]]
[1] 4

$points_in_vertex [[5]]
[1] 5

$points_in_vertex [[6]]
[1] 3 4 5 6

$points_in_vertex [[7]]
[1] 4 5 6 7</code></pre>
<p>From the previous results, which show which variable shows up in which clusters, we can see that variables 1 and 2 (bullying and teacher hostility) tend to occur in isolation (points vertices 1 and 2), while other issues tend to occur in clusters (points in the remaining vertices). Given that these are authority-social and peer-social issues of social etiology rather than curriculum etiology, this makes some sense. How teachers interact and how students behave is typically independent of the curriculum, while issues stemming from lack of challenge in the classroom stem directly from a curriculum cause.</p>
<p>Adding to our script, we can plot in igraph to obtain a bit more insight into the connectivity of the clusters:</p>
<pre><code>#create graph from the mapper object's adjacency information
g1&lt;-graph.adjacency(j$adjacency,mode="undirected")
plot(g1)</code></pre>
<p>This code turns the Mapper’s overlapping cluster results into a graph object that can be plotted and analyzed to see how the clusters overlap with each other.</p>
<p><a href="#figure7-7" id="figureanchor7-7">Figure 7-7</a> shows the isolation of the socially stemming issues of teacher hostility and bullying by peers. The curriculum-based issues tend to overlap <span epub:type="pagebreak" id="Page_166" title="166"/>to some extent with lack of motivation and outside learning (items 5 and 6) being the strong ties between these clusters.</p>
<figure>
<img alt="" class="" src="image_fi/503083c07/f07007.png"/>
<figcaption><p><a id="figure7-7">Figure 7-7</a>: A network plot of the clusters found in the Quora sample analysis</p></figcaption>
</figure>
<p>One of the noted issues with Mapper is its instability with respect to overlap and binning of the filtration. For instance, changing the bin overlap to 20 percent results in the unconnected graph shown in <a href="#figure7-8" id="figureanchor7-8">Figure 7-8</a>.</p>
<figure>
<img alt="" class="" src="image_fi/503083c07/f07008.png"/>
<figcaption><p><a id="figure7-8">Figure 7-8</a>: A network plot of the Quora sample results with a different parameter defining bin overlap</p></figcaption>
</figure>
<p>Some recent papers suggest using multiple scales to stabilize the output; however, most exploration of this notion is purely theoretical at this point. In general, using a variety of overlap fractions can yield a general idea of cluster structures in the data.</p>
<h2 id="h1-503083c07-0004"><a class="XrefDestination" id="Summary"/><span class="XrefDestination" id="xref-503083c07-008"/>Summary</h2>
<p class="BodyFirst">In this chapter, we explored a few tools from topological data analysis. We compared data from samples of an educational population using persistent homology and explored educational experience groups within a self-selected sample of profoundly gifted individuals. TDA has grown quite a bit in recent years, and many problems can be solved with one or more tools from TDA. In the next chapter, we’ll explore one more popular tool from this growing field.</p>
</section>
</body>
</html>