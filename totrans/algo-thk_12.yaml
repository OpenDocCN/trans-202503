- en: '**A'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**A'
- en: ALGORITHM RUNTIME**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**算法运行时间**'
- en: '![Image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common01.jpg)'
- en: Each competitive programming problem that we solve in this book specifies a
    time limit on how long our program will be allowed to run. If our program exceeds
    the time limit, then the judge terminates our program with a “Time-Limit Exceeded”
    error. A time limit is designed to prevent algorithmically naive solutions from
    passing the test cases. The problem author has some model solutions in mind and
    sets the time limit as an arbiter of whether we have demonstrated those solution
    ideas. As such, in addition to being correct, we need our programs to be fast.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们解决的每个竞争编程问题都有一个规定的时间限制，限制了我们的程序允许运行的最长时间。如果我们的程序超时，评测系统就会终止程序并显示“超时”错误。时间限制的设计目的是防止算法上过于简单的解决方案通过测试用例。问题的作者已经有了某些模型解法，并通过设置时间限制来判断我们是否展示了这些解法的思想。因此，除了正确性之外，我们还需要让程序运行得足够快。
- en: The Case for Timing . . . and Something Else
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**时机的选择……以及其他一些因素**'
- en: Most books on algorithms do not use time limits when discussing runtime. Time
    limits and execution times do, however, appear frequently in this book. The primary
    reason is that such times can give us intuitive understanding of the efficiency
    of our programs. We can run a program and measure how long it takes. If our program
    is too slow, according to the time limit for the problem, then we know that we
    need to optimize the current code or find a wholly new approach. We don’t know
    what kind of computer the judge is using, but running the program on our own computer
    is still informative. Say that we run our program on our laptop and it takes 30
    seconds on some small test case. If the problem time limit is three seconds, we
    can be confident that our program is simply not fast enough.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数算法书籍在讨论运行时间时并不使用时间限制。然而，本书中经常出现时间限制和执行时间的概念。其主要原因是，这些时间可以帮助我们直观地理解程序的效率。我们可以运行程序并测量它所花费的时间。如果我们的程序太慢，超过了问题的时间限制，那么我们就知道我们需要优化当前的代码或找到一种全新的方法。我们不知道评测系统使用的是哪种计算机，但在我们自己的计算机上运行程序仍然有参考价值。假设我们在笔记本电脑上运行程序，发现它在某个小的测试用例上花了
    30 秒。如果问题的时间限制是 3 秒，那么我们可以确信我们的程序显然还不够快。
- en: 'An exclusive focus on execution times, however, is limiting. Here are five
    reasons why:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅仅专注于执行时间是有限的。以下是五个原因：
- en: '**Execution time depends on the computer.** As just suggested, timing our program
    tells us only how long our program takes on one computer. That’s very specific
    information, and it gives us little in the way of understanding what to expect
    when it is run on other computers. When working through the book, you may also
    notice that the time taken by a program varies from run to run, even on the same
    computer. For example, you might run a program on a test case and find that it
    takes 3 seconds; you might then run it again, on the same test case, and find
    that it takes 2.5 seconds or 3.5 seconds. The reason for this difference is that
    your operating system is managing your computing resources, shunting them around
    to different tasks as needed. The decisions that your operating system makes influence
    the runtime of your program.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**执行时间取决于计算机。** 正如前面所提到的，给我们的程序计时只告诉我们在一台计算机上运行程序所花费的时间。这是非常具体的信息，并且它几乎无法帮助我们理解在其他计算机上运行时会发生什么。在阅读本书时，你可能还会注意到，即使在同一台计算机上，程序的执行时间也会有所不同。例如，你可能在一个测试用例上运行程序，发现它需要
    3 秒钟；然后你可能再运行一次，同样的测试用例，结果却是 2.5 秒或 3.5 秒。造成这种差异的原因是操作系统正在管理你的计算资源，并根据需要将它们分配到不同的任务中。操作系统所做的决策会影响程序的运行时间。'
- en: '**Execution time depends on the test case.** Timing our program on a test case
    tells us only how long our program takes on that test case. Suppose that our program
    takes one second to run on a small test case. That may seem fast, but here’s the
    truth about small test cases: every reasonable solution for a problem will be
    able to solve those. If I ask you to sort a few numbers, or optimally schedule
    a few events, or whatever, you can quickly do it with the first correct idea that
    you have. What’s interesting, then, are large test cases. They are the ones where
    algorithmic ingenuity pays off. How long will our program take on a large test
    case or on a huge test case? We don’t know. We’d have to run our program on those
    test cases, too. Even if we did that, there could be specific kinds of test cases
    that trigger poorer performance. We may be led to believe that our program is
    faster than it is.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**执行时间取决于测试用例。** 在一个测试用例上对我们的程序进行计时，只能告诉我们程序在该测试用例上运行了多长时间。假设我们的程序在一个小测试用例上运行需要一秒钟，这看起来可能很快，但关于小测试用例的真相是：每个合理的解决方案都能解决这些问题。如果我让你排序几个数字，或优化地安排几个事件，或者做其他什么，你可以用你第一个正确的想法迅速完成。那么，真正有趣的，是大测试用例。它们是算法独创性体现的地方。我们的程序在大测试用例上或巨大的测试用例上需要多久才能完成？我们不知道。我们也需要在这些测试用例上运行程序。即便我们这样做，也可能会有特定类型的测试用例会导致较差的表现。我们可能会被误导，认为我们的程序比实际更快。'
- en: '**The program requires implementation.** We can’t time something that we don’t
    implement. Suppose that we’re thinking about a problem and come up with an idea
    for how to solve it. Is it fast? Although we could implement it to find out, it
    would be nice to know, in advance, whether or not the idea is likely to lead to
    a fast program. You would not implement a program that you knew, at the outset,
    would be incorrect. It would similarly be nice to know, at the outset, that a
    program would be too slow.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**程序需要实现。** 我们无法对一个没有实现的东西进行计时。假设我们在考虑一个问题，并想出了一个解决方案。它快吗？虽然我们可以通过实现它来了解，但提前知道这个想法是否可能导致一个快速的程序会更好。你不会实现一个你一开始就知道会错误的程序。同样，知道一个程序一开始就太慢，也会很好。'
- en: '**Timing doesn’t explain slowness.** If we find that our program is too slow,
    then our next task is to design a faster one. However, simply timing a program
    gives us no insight into why our program is slow. It just is. Further, if we manage
    to think up a possible improvement to our program, we’d need to implement it to
    see whether or not it helps.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**计时无法解释缓慢。** 如果我们发现程序太慢，那么接下来的任务就是设计一个更快的程序。然而，单纯地计时并不能为我们提供程序为何缓慢的洞见。它只是慢而已。此外，如果我们设法想出一个可能改进程序的办法，我们需要实现它才能看到它是否有效。'
- en: '**Execution time is not easily communicated.** For many of the reasons above,
    it’s difficult to use execution time to talk to other people about the efficiency
    of algorithms. “My program takes two seconds to run on this computer that I bought
    last year, on a test case with eight chickens and four eggs, using a program that
    I wrote in C. How about yours?”'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**执行时间不容易传达。** 基于上述许多原因，使用执行时间与他人讨论算法效率是困难的。“我的程序在去年购买的这台电脑上运行需要两秒钟，测试用例包含八只鸡和四个鸡蛋，使用我用C语言编写的程序。你的程序呢？”'
- en: 'Not to worry: computer scientists have devised a notation that addresses these
    shortcomings of timing. It’s independent of the computer, independent of test
    case, and independent of a particular implementation. It signals why a slow program
    is slow. It’s easily communicated. It’s called *big O*, and it’s coming right
    up.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不用担心：计算机科学家已经设计出一种符号来解决计时的这些不足之处。它与计算机无关，与测试用例无关，也与特定的实现无关。它揭示了为什么一个程序会变慢。它容易传达。它被称为*大
    O 符号*，接下来就会介绍。
- en: Big O Notation
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大 O 符号
- en: 'Big O is a notation that computer scientists use to concisely describe the
    efficiency of algorithms. It assigns each algorithm to one of a small number of
    efficiency classes. An efficiency class tells you how fast an algorithm is or,
    equivalently, how much work it does. The faster an algorithm, the less work it
    does; the slower an algorithm, the more work it does. Each algorithm belongs to
    an efficiency class; the efficiency class tells you how much work that algorithm
    does relative to the amount of input that it must process. To understand big O,
    we need to understand these efficiency classes. I’ll introduce three of them here:
    linear time, constant time, and quadratic time.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大 O 是计算机科学家用来简洁描述算法效率的记法。它将每个算法归入少数几个效率类别中的一个。效率类别告诉你一个算法有多快，或者等价地说，它做了多少工作。算法越快，做的工作越少；算法越慢，做的工作越多。每个算法都属于一个效率类别；效率类别告诉你相对于它必须处理的输入量，算法做了多少工作。要理解大
    O，我们需要了解这些效率类别。我在这里将介绍三种：线性时间、常数时间和平方时间。
- en: '*Linear Time*'
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*线性时间*'
- en: Suppose that we are provided an array of integers in increasing order, and we
    want to return its maximum integer. For example, given the array
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们提供一个递增顺序的整数数组，我们想返回其中的最大整数。例如，给定以下数组：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: we want to return `21`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望返回`21`。
- en: One way to do this is to keep track of the maximum value that we have found
    so far. Whenever we find a larger value than the maximum, we update the maximum.
    [Listing A-1](app01.xhtml#app01ex01) implements this idea.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是追踪到目前为止找到的最大值。每当我们找到比当前最大值更大的值时，就更新最大值。[清单 A-1](app01.xhtml#app01ex01)实现了这个思想。
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Listing A-1: Finding the maximum in an array of increasing integers*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 A-1：在递增整数数组中查找最大值*'
- en: 'The code sets `max` to the value at index `0` of `nums`, and then loops through
    the array, looking for larger values. Don’t worry that the first iteration of
    the loop compares `max` to itself: that’s just one iteration of unnecessary work.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将`max`设置为`nums`数组中索引为`0`的值，然后通过循环遍历数组，寻找更大的值。不要担心循环的第一次迭代将`max`与其自身进行比较：那只是一次不必要的工作。
- en: Rather than timing specific test cases, let’s think about the amount of work
    that this algorithm does as a function of the size of the array. Suppose that
    the array has five elements. What does our program do? It performs one variable
    assignment above the loop, then iterates five times in the loop, and then returns
    the result. If the array has 10 elements, then our program does similarly, except
    now it iterates 10 times in the loop rather than 5\. What about a million elements?
    Our program iterates a million times. Now we see that the assignment above the
    loop and return below the loop pale in comparison to the amount of work done by
    the loop. What matters, especially as the test case gets large, is the number
    of iterations of the loop.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与其为特定的测试用例计时，不如思考这个算法根据数组大小所做的工作量。假设数组有五个元素。我们的程序做了什么？它在循环前执行了一个变量赋值，然后在循环中迭代了五次，最后返回结果。如果数组有10个元素，那么我们的程序做的也类似，只不过这次它在循环中迭代了10次而不是5次。那么，如果有一百万个元素呢？我们的程序会迭代一百万次。现在我们可以看到，循环前的赋值和循环后的返回相比，循环所做的工作量更为庞大。尤其是当测试用例变得非常大时，关键是循环的迭代次数。
- en: 'If our array has *n* elements, then the loop iterates *n* times. In big O notation,
    we say that this algorithm is *O*(*n*). Interpret this as follows: for an array
    of *n* elements, the algorithm does work proportional to *n*. An *O*(*n*) algorithm
    is called a *linear-time algorithm* because there is a linear relationship between
    the problem size and the amount of work done. If we double the problem size, then
    we double the work done and thereby double the runtime. For example, if it takes
    one second to run on an array with two million elements, we can expect it to take
    about two seconds to run on an array of four million elements.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数组有*n*个元素，那么循环会迭代*n*次。在大 O 记法中，我们说这个算法是*O*(*n*)。可以这样理解：对于一个包含*n*个元素的数组，算法的工作量与*n*成正比。*O*(*n*)算法被称为*线性时间算法*，因为问题规模与所做工作的数量之间存在线性关系。如果我们将问题规模加倍，那么工作量也会加倍，从而使运行时间加倍。例如，如果在一个包含二百万个元素的数组上运行需要一秒钟，我们可以预期在一个包含四百万个元素的数组上运行需要大约两秒钟。
- en: Notice that we didn’t have to run the code to arrive at this insight. We didn’t
    even have to write the code out. (Well . . . yeah, I did write the code, but that
    was just to make the algorithm clear.) Saying that an algorithm is *O*(*n*) offers
    us the fundamental relationship between the problem size and the growth in runtime.
    It’s true no matter what computer we use or which test case we look at.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们不需要运行代码就能得出这个结论。我们甚至不需要写出代码。（好吧……是的，我确实写了代码，但那只是为了让算法更清晰。）说一个算法是 *O*(*n*)，向我们提供了问题规模和运行时增长之间的基本关系。无论我们使用什么计算机，或者查看哪个测试案例，这都是成立的。
- en: '*Constant Time*'
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*常数时间复杂度*'
- en: 'We know something about our arrays that we didn’t exploit yet: that the integers
    are in increasing order. The biggest integer will therefore be found at the end
    of the array. Let’s just return that directly, rather than eventually finding
    it through an exhaustive search of the array. [Listing A-2](app01.xhtml#app01ex02)
    presents this new idea.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道数组中有一些信息我们还没有利用：整数是按递增顺序排列的。因此，最大的整数一定在数组的末尾。我们可以直接返回它，而不是通过对数组进行穷举搜索最终找到它。[列表
    A-2](app01.xhtml#app01ex02)展示了这一新思路。
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*Listing A-2: Finding the maximum in an array of increasing integers*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 A-2：在递增整数数组中找到最大值*'
- en: How much work does this algorithm do as a function of the size of the array?
    Interestingly, array size no longer matters! The algorithm accesses and returns
    `nums[n - 1]`, the final element of the array, no matter if it has 5 elements
    or 10 or a million. The algorithm doesn’t care. In big O notation, we say that
    this algorithm is *O*(1). It’s called a *constant-time algorithm* because the
    amount of work it does is constant, not increasing as the problem size increases.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法根据数组大小做多少工作？有趣的是，数组的大小已经不再重要！无论数组有 5 个元素，10 个元素，还是一百万个元素，算法都只访问并返回 `nums[n
    - 1]`，即数组的最后一个元素。算法并不关心大小。在大 O 符号中，我们说这个算法是 *O*(1)。它被称为 *常数时间算法*，因为它所做的工作量是恒定的，随着问题规模的增大并不会增加。
- en: This is the best kind of algorithm. No matter how large our array, we can expect
    about the same runtime. It’s surely better than a linear-time algorithm, which
    gets slower as the problem size increases. Not many interesting problems can be
    solved by constant-time algorithms, though. For example, if we were given the
    array in arbitrary order, rather than increasing order, then constant-time algorithms
    are out. There’s no way we could look at a fixed number of array elements and
    hope to be guaranteed to find the maximum.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最好的算法类型。无论数组多大，我们都可以预期大致相同的运行时间。它显然比线性时间算法要好，后者随着问题规模的增大而变得更慢。不过，并不是所有有趣的问题都能通过常数时间算法解决。例如，如果我们给定的是一个无序的数组，而不是递增的数组，那么常数时间算法就不适用了。我们不可能仅查看固定数量的数组元素，就能保证找到最大值。
- en: '*Another Example*'
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*另一个例子*'
- en: 'Consider the algorithm in [Listing A-3](app01.xhtml#app01ex03): is it *O*(*n*)
    or *O*(1) or something else? (Notice that I’ve left out the function and variable
    definitions so that we’re not tempted to compile and run this.)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请看[列表 A-3](app01.xhtml#app01ex03)中的算法：它是 *O*(*n*) 还是 *O*(1) 或者其他什么？（注意，我没有列出函数和变量的定义，以避免我们想要编译和运行它。）
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Listing A-3: What kind of algorithm is this?*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 A-3：这是什么样的算法？*'
- en: Suppose that array `nums` has *n* elements. The first loop iterates *n* times,
    and the second loop iterates *n* times. That’s 2*n* iterations in total. As a
    first attempt, it’s natural to say that this algorithm is *O*(2*n*). While saying
    that is technically true, computer scientists would ignore the 2, simply writing
    *O*(*n*).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数组 `nums` 有 *n* 个元素。第一个循环执行 *n* 次，第二个循环也执行 *n* 次。总共有 2*n* 次迭代。作为第一次尝试，直观上可以说这个算法是
    *O*(2*n*)。虽然这么说技术上是正确的，但计算机科学家通常会忽略 2，直接写作 *O*(*n*)。
- en: This may seem weird, since this algorithm is twice as slow as the one in [Listing
    A-1](app01.xhtml#app01ex01), yet we declare both to be *O*(*n*). The reason comes
    down to a balancing act between simplicity and expressiveness of our notation.
    If we kept the 2, then we’d perhaps be more accurate, but we’d obscure the fact
    that this is a linear-time algorithm. Whether it’s 2*n* or 3*n* or anything times
    *n*, it’s fundamental linear runtime growth does not change.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来有点奇怪，因为这个算法的速度是[列表 A-1](app01.xhtml#app01ex01)中的算法的两倍，但我们却宣称它们都是 *O*(*n*)。原因在于我们的记法在简洁性和表现力之间的平衡。如果我们保留
    2，那么或许更准确，但这也会掩盖它是一个线性时间算法这一事实。无论是 2*n* 还是 3*n*，或者任何乘以 *n* 的数字，它的基本线性运行时增长是不会改变的。
- en: '*Quadratic Time*'
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*二次时间复杂度*'
- en: We have now seen linear-time algorithms (which are very fast in practice) and
    constant-time algorithms (which are even faster than linear-time algorithms).
    Now let’s look at something slower than linear time. The code is in [Listing A-4](app01.xhtml#app01ex04).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经看到了线性时间算法（在实际中非常快速）和常数时间算法（比线性时间算法还要快）。接下来让我们看一下比线性时间还要慢的算法。代码在[示例 A-4](app01.xhtml#app01ex04)中。
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Listing A-4: A quadratic-time algorithm*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 A-4：一个二次时间算法*'
- en: Compared to [Listing A-3](app01.xhtml#app01ex03), notice that the loops are
    now nested rather than sequential. Each iteration of the outer loop causes *n*
    iterations of the inner loop. The outer loop iterates *n* times. Therefore, the
    total number of iterations for the inner loop, and the number of times that we
    update `total`, is *n*². (The first iteration of the outer loop costs *n* work,
    the second costs *n* work, the third costs *n* work, and so on. The total is *n*
    + *n* + *n* + . . . + *n*, where the number of times we add *n* is *n*.)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 与[示例 A-3](app01.xhtml#app01ex03)相比，请注意，现在的循环是嵌套的，而不是顺序的。外层循环的每次迭代都会导致内层循环的*n*次迭代。外层循环迭代*n*次。因此，内层循环的总迭代次数以及更新`total`的次数是*n*²。（外层循环的第一次迭代需要*n*的工作量，第二次需要*n*的工作量，第三次需要*n*的工作量，以此类推。总数是*n*
    + *n* + *n* + …… + *n*，其中我们加上*n*的次数是*n*。）
- en: In big O notation, we say that this algorithm is *O*(*n*²). It’s called a *quadratic-time
    algorithm* because quadratic is the mathematical term referring to a power of
    2.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在大 O 符号中，我们说这个算法是*O*(*n*²)。它被称为*二次时间算法*，因为“二次”是指数学中2的幂次。
- en: 'Let’s now probe why quadratic-time algorithms are slower than linear-time algorithms.
    Suppose that we have a quadratic-time algorithm that takes *n*² steps. On a problem
    size of 5, it would take 5² = 25 steps; on a problem size of 10, it would take
    10² = 100 steps; and on a problem size of 20, it would take 20² = 400 steps. Notice
    what’s happening when we double the problem size: the work done *quadruples*.
    That’s far worse than linear-time algorithms, where doubling the problem size
    leads to only a doubling of work done.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探讨为什么二次时间算法比线性时间算法慢。假设我们有一个需要*n*²步的二次时间算法。在问题规模为5时，它需要5² = 25步；在问题规模为10时，它需要10²
    = 100步；在问题规模为20时，它需要20² = 400步。注意，当我们将问题规模加倍时，所做的工作是*四倍*增加的。这比线性时间算法要差得多，因为在后者中，问题规模加倍时，工作量只会加倍。
- en: Don’t be surprised that an algorithm that takes 2*n*² steps, 3*n*² steps, and
    so on is also classified as a quadratic-time algorithm. The big O notation hides
    what’s in front of the *n*² term, just as it hides what’s in front of the *n*
    term in a linear-time algorithm.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 不要惊讶，2*n*²步、3*n*²步等算法也被归类为二次时间算法。大 O 符号隐藏了*n*²项前面的部分，就像它隐藏了线性时间算法中*n*项前面的部分一样。
- en: 'What if we have an algorithm that we find takes 2*n*² + 6*n* steps? This, too,
    is a quadratic-time algorithm. We’re taking a quadratic runtime of 2*n*² and adding
    a linear runtime of 6*n* to it. The result is still a quadratic-time algorithm:
    the quadrupling behavior of the quadratic part quickly comes to dominate the doubling
    behavior of the linear part.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个算法，它需要2*n*² + 6*n*步呢？这也是一个二次时间算法。我们正在将2*n*²的二次运行时间与6*n*的线性运行时间相加。结果仍然是一个二次时间算法：二次部分的四倍增长很快就会主导线性部分的两倍增长。
- en: '*Big O in This Book*'
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*本书中的大 O 符号*'
- en: There’s much more that can be said about big O. It has a formal mathematical
    basis used by computer scientists to rigorously analyze the runtime of their algorithms.
    There are other efficiency classes besides the three that I’ve introduced here
    (and I’ll introduce the few others that appear in this book as needed). There
    is certainly more to learn if you are interested in going further, but what I’ve
    presented here is enough for our purposes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 关于大 O 符号还有很多可以讲的内容。它有一个正式的数学基础，计算机科学家用它来严格分析算法的运行时间。除了我在这里介绍的三种效率类别外，还有其他效率类别（如果需要，我会介绍本书中出现的其他几种）。如果你有兴趣深入了解，肯定还有更多的内容可以学习，但我在这里介绍的已经足够满足我们的需求了。
- en: Big O generally arises in this book on an as-needed basis. We may pursue an
    initial solution for a problem, only to find that we get a “Time-Limit Exceeded”
    error from the judge. In those cases, we need to understand where we went wrong,
    and the first step in such an analysis is to appreciate the way that our runtime
    grows as a function of problem size. A big O analysis not only confirms that slow
    code is slow, but it often uncovers the particular bottlenecks in our code. We
    can then use that enhanced understanding to design a more efficient solution.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的大O符号通常根据需要出现。我们可能会为一个问题寻求初步的解决方案，却发现收到评测系统的“超时”错误。在这种情况下，我们需要理解自己哪里出了问题，而这种分析的第一步就是理解我们的运行时间如何随着问题规模的变化而增长。大O分析不仅能确认代码运行缓慢的事实，而且通常能揭示出代码中的具体瓶颈。然后，我们可以利用这种更深入的理解来设计更高效的解决方案。
