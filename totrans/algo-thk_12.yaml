- en: '**A'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ALGORITHM RUNTIME**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Each competitive programming problem that we solve in this book specifies a
    time limit on how long our program will be allowed to run. If our program exceeds
    the time limit, then the judge terminates our program with a “Time-Limit Exceeded”
    error. A time limit is designed to prevent algorithmically naive solutions from
    passing the test cases. The problem author has some model solutions in mind and
    sets the time limit as an arbiter of whether we have demonstrated those solution
    ideas. As such, in addition to being correct, we need our programs to be fast.
  prefs: []
  type: TYPE_NORMAL
- en: The Case for Timing . . . and Something Else
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most books on algorithms do not use time limits when discussing runtime. Time
    limits and execution times do, however, appear frequently in this book. The primary
    reason is that such times can give us intuitive understanding of the efficiency
    of our programs. We can run a program and measure how long it takes. If our program
    is too slow, according to the time limit for the problem, then we know that we
    need to optimize the current code or find a wholly new approach. We don’t know
    what kind of computer the judge is using, but running the program on our own computer
    is still informative. Say that we run our program on our laptop and it takes 30
    seconds on some small test case. If the problem time limit is three seconds, we
    can be confident that our program is simply not fast enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'An exclusive focus on execution times, however, is limiting. Here are five
    reasons why:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Execution time depends on the computer.** As just suggested, timing our program
    tells us only how long our program takes on one computer. That’s very specific
    information, and it gives us little in the way of understanding what to expect
    when it is run on other computers. When working through the book, you may also
    notice that the time taken by a program varies from run to run, even on the same
    computer. For example, you might run a program on a test case and find that it
    takes 3 seconds; you might then run it again, on the same test case, and find
    that it takes 2.5 seconds or 3.5 seconds. The reason for this difference is that
    your operating system is managing your computing resources, shunting them around
    to different tasks as needed. The decisions that your operating system makes influence
    the runtime of your program.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Execution time depends on the test case.** Timing our program on a test case
    tells us only how long our program takes on that test case. Suppose that our program
    takes one second to run on a small test case. That may seem fast, but here’s the
    truth about small test cases: every reasonable solution for a problem will be
    able to solve those. If I ask you to sort a few numbers, or optimally schedule
    a few events, or whatever, you can quickly do it with the first correct idea that
    you have. What’s interesting, then, are large test cases. They are the ones where
    algorithmic ingenuity pays off. How long will our program take on a large test
    case or on a huge test case? We don’t know. We’d have to run our program on those
    test cases, too. Even if we did that, there could be specific kinds of test cases
    that trigger poorer performance. We may be led to believe that our program is
    faster than it is.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The program requires implementation.** We can’t time something that we don’t
    implement. Suppose that we’re thinking about a problem and come up with an idea
    for how to solve it. Is it fast? Although we could implement it to find out, it
    would be nice to know, in advance, whether or not the idea is likely to lead to
    a fast program. You would not implement a program that you knew, at the outset,
    would be incorrect. It would similarly be nice to know, at the outset, that a
    program would be too slow.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Timing doesn’t explain slowness.** If we find that our program is too slow,
    then our next task is to design a faster one. However, simply timing a program
    gives us no insight into why our program is slow. It just is. Further, if we manage
    to think up a possible improvement to our program, we’d need to implement it to
    see whether or not it helps.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Execution time is not easily communicated.** For many of the reasons above,
    it’s difficult to use execution time to talk to other people about the efficiency
    of algorithms. “My program takes two seconds to run on this computer that I bought
    last year, on a test case with eight chickens and four eggs, using a program that
    I wrote in C. How about yours?”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Not to worry: computer scientists have devised a notation that addresses these
    shortcomings of timing. It’s independent of the computer, independent of test
    case, and independent of a particular implementation. It signals why a slow program
    is slow. It’s easily communicated. It’s called *big O*, and it’s coming right
    up.'
  prefs: []
  type: TYPE_NORMAL
- en: Big O Notation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Big O is a notation that computer scientists use to concisely describe the
    efficiency of algorithms. It assigns each algorithm to one of a small number of
    efficiency classes. An efficiency class tells you how fast an algorithm is or,
    equivalently, how much work it does. The faster an algorithm, the less work it
    does; the slower an algorithm, the more work it does. Each algorithm belongs to
    an efficiency class; the efficiency class tells you how much work that algorithm
    does relative to the amount of input that it must process. To understand big O,
    we need to understand these efficiency classes. I’ll introduce three of them here:
    linear time, constant time, and quadratic time.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Linear Time*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose that we are provided an array of integers in increasing order, and we
    want to return its maximum integer. For example, given the array
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: we want to return `21`.
  prefs: []
  type: TYPE_NORMAL
- en: One way to do this is to keep track of the maximum value that we have found
    so far. Whenever we find a larger value than the maximum, we update the maximum.
    [Listing A-1](app01.xhtml#app01ex01) implements this idea.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing A-1: Finding the maximum in an array of increasing integers*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code sets `max` to the value at index `0` of `nums`, and then loops through
    the array, looking for larger values. Don’t worry that the first iteration of
    the loop compares `max` to itself: that’s just one iteration of unnecessary work.'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than timing specific test cases, let’s think about the amount of work
    that this algorithm does as a function of the size of the array. Suppose that
    the array has five elements. What does our program do? It performs one variable
    assignment above the loop, then iterates five times in the loop, and then returns
    the result. If the array has 10 elements, then our program does similarly, except
    now it iterates 10 times in the loop rather than 5\. What about a million elements?
    Our program iterates a million times. Now we see that the assignment above the
    loop and return below the loop pale in comparison to the amount of work done by
    the loop. What matters, especially as the test case gets large, is the number
    of iterations of the loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'If our array has *n* elements, then the loop iterates *n* times. In big O notation,
    we say that this algorithm is *O*(*n*). Interpret this as follows: for an array
    of *n* elements, the algorithm does work proportional to *n*. An *O*(*n*) algorithm
    is called a *linear-time algorithm* because there is a linear relationship between
    the problem size and the amount of work done. If we double the problem size, then
    we double the work done and thereby double the runtime. For example, if it takes
    one second to run on an array with two million elements, we can expect it to take
    about two seconds to run on an array of four million elements.'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we didn’t have to run the code to arrive at this insight. We didn’t
    even have to write the code out. (Well . . . yeah, I did write the code, but that
    was just to make the algorithm clear.) Saying that an algorithm is *O*(*n*) offers
    us the fundamental relationship between the problem size and the growth in runtime.
    It’s true no matter what computer we use or which test case we look at.
  prefs: []
  type: TYPE_NORMAL
- en: '*Constant Time*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We know something about our arrays that we didn’t exploit yet: that the integers
    are in increasing order. The biggest integer will therefore be found at the end
    of the array. Let’s just return that directly, rather than eventually finding
    it through an exhaustive search of the array. [Listing A-2](app01.xhtml#app01ex02)
    presents this new idea.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing A-2: Finding the maximum in an array of increasing integers*'
  prefs: []
  type: TYPE_NORMAL
- en: How much work does this algorithm do as a function of the size of the array?
    Interestingly, array size no longer matters! The algorithm accesses and returns
    `nums[n - 1]`, the final element of the array, no matter if it has 5 elements
    or 10 or a million. The algorithm doesn’t care. In big O notation, we say that
    this algorithm is *O*(1). It’s called a *constant-time algorithm* because the
    amount of work it does is constant, not increasing as the problem size increases.
  prefs: []
  type: TYPE_NORMAL
- en: This is the best kind of algorithm. No matter how large our array, we can expect
    about the same runtime. It’s surely better than a linear-time algorithm, which
    gets slower as the problem size increases. Not many interesting problems can be
    solved by constant-time algorithms, though. For example, if we were given the
    array in arbitrary order, rather than increasing order, then constant-time algorithms
    are out. There’s no way we could look at a fixed number of array elements and
    hope to be guaranteed to find the maximum.
  prefs: []
  type: TYPE_NORMAL
- en: '*Another Example*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider the algorithm in [Listing A-3](app01.xhtml#app01ex03): is it *O*(*n*)
    or *O*(1) or something else? (Notice that I’ve left out the function and variable
    definitions so that we’re not tempted to compile and run this.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing A-3: What kind of algorithm is this?*'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that array `nums` has *n* elements. The first loop iterates *n* times,
    and the second loop iterates *n* times. That’s 2*n* iterations in total. As a
    first attempt, it’s natural to say that this algorithm is *O*(2*n*). While saying
    that is technically true, computer scientists would ignore the 2, simply writing
    *O*(*n*).
  prefs: []
  type: TYPE_NORMAL
- en: This may seem weird, since this algorithm is twice as slow as the one in [Listing
    A-1](app01.xhtml#app01ex01), yet we declare both to be *O*(*n*). The reason comes
    down to a balancing act between simplicity and expressiveness of our notation.
    If we kept the 2, then we’d perhaps be more accurate, but we’d obscure the fact
    that this is a linear-time algorithm. Whether it’s 2*n* or 3*n* or anything times
    *n*, it’s fundamental linear runtime growth does not change.
  prefs: []
  type: TYPE_NORMAL
- en: '*Quadratic Time*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have now seen linear-time algorithms (which are very fast in practice) and
    constant-time algorithms (which are even faster than linear-time algorithms).
    Now let’s look at something slower than linear time. The code is in [Listing A-4](app01.xhtml#app01ex04).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing A-4: A quadratic-time algorithm*'
  prefs: []
  type: TYPE_NORMAL
- en: Compared to [Listing A-3](app01.xhtml#app01ex03), notice that the loops are
    now nested rather than sequential. Each iteration of the outer loop causes *n*
    iterations of the inner loop. The outer loop iterates *n* times. Therefore, the
    total number of iterations for the inner loop, and the number of times that we
    update `total`, is *n*². (The first iteration of the outer loop costs *n* work,
    the second costs *n* work, the third costs *n* work, and so on. The total is *n*
    + *n* + *n* + . . . + *n*, where the number of times we add *n* is *n*.)
  prefs: []
  type: TYPE_NORMAL
- en: In big O notation, we say that this algorithm is *O*(*n*²). It’s called a *quadratic-time
    algorithm* because quadratic is the mathematical term referring to a power of
    2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now probe why quadratic-time algorithms are slower than linear-time algorithms.
    Suppose that we have a quadratic-time algorithm that takes *n*² steps. On a problem
    size of 5, it would take 5² = 25 steps; on a problem size of 10, it would take
    10² = 100 steps; and on a problem size of 20, it would take 20² = 400 steps. Notice
    what’s happening when we double the problem size: the work done *quadruples*.
    That’s far worse than linear-time algorithms, where doubling the problem size
    leads to only a doubling of work done.'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t be surprised that an algorithm that takes 2*n*² steps, 3*n*² steps, and
    so on is also classified as a quadratic-time algorithm. The big O notation hides
    what’s in front of the *n*² term, just as it hides what’s in front of the *n*
    term in a linear-time algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we have an algorithm that we find takes 2*n*² + 6*n* steps? This, too,
    is a quadratic-time algorithm. We’re taking a quadratic runtime of 2*n*² and adding
    a linear runtime of 6*n* to it. The result is still a quadratic-time algorithm:
    the quadrupling behavior of the quadratic part quickly comes to dominate the doubling
    behavior of the linear part.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Big O in This Book*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There’s much more that can be said about big O. It has a formal mathematical
    basis used by computer scientists to rigorously analyze the runtime of their algorithms.
    There are other efficiency classes besides the three that I’ve introduced here
    (and I’ll introduce the few others that appear in this book as needed). There
    is certainly more to learn if you are interested in going further, but what I’ve
    presented here is enough for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Big O generally arises in this book on an as-needed basis. We may pursue an
    initial solution for a problem, only to find that we get a “Time-Limit Exceeded”
    error from the judge. In those cases, we need to understand where we went wrong,
    and the first step in such an analysis is to appreciate the way that our runtime
    grows as a function of problem size. A big O analysis not only confirms that slow
    code is slow, but it often uncovers the particular bottlenecks in our code. We
    can then use that enhanced understanding to design a more efficient solution.
  prefs: []
  type: TYPE_NORMAL
