<html><head></head><body>
<h2 class="h2" id="ch10"><span epub:type="pagebreak" id="page_93"/><strong><span class="big">10</span><br/>INTRODUCTION TO AVERAGING AND PARAMETER ESTIMATION</strong></h2>&#13;
<div class="image1"><img alt="Image" src="../images/common.jpg"/></div>&#13;
<p class="noindent">This chapter introduces you to <em>parameter estimation</em>, an essential part of statistical inference where we use our data to guess the value of an unknown variable. For example, we might want to estimate the probability of a visitor on a web page making a purchase, the number of jelly beans in a jar at a carnival, or the location and momentum of a particle. In all of these cases, we have an unknown value we want to estimate, and we can use information we have observed to make a guess. We refer to these unknown values as <em>parameters</em>, and the process of making the best guess about these parameters as parameter estimation.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_94"/>We’ll focus on <em>averaging</em>, which is the most basic form of parameter estimation. Nearly everyone understands that taking an average of a set of observations is the best way to estimate a true value, but few people really stop to ask why this works—if it really does at all. We need to prove that we can trust averaging, because in later chapters, we build it into more complex forms of parameter estimation.</p>&#13;
<h3 class="h3" id="ch10lev1sec1"><strong>Estimating Snowfall</strong></h3>&#13;
<p class="noindent">Imagine there was a heavy snow last night and you’d like to figure out exactly how much snow fell, in inches, in your yard. Unfortunately, you don’t have a snow gauge that will give you an accurate measurement. Looking outside, you see that the wind has blown the snow around a bit overnight, meaning it isn’t uniformly smooth. You decide to use a ruler to measure the depth at seven roughly random locations in your yard. You come up with the following measurements (in inches):</p>&#13;
<p class="equ">6.2, 4.5, 5.7, 7.6, 5.3, 8.0, 6.9</p>&#13;
<p class="indent">The snow has clearly shifted around quite a bit and your yard isn’t perfectly level either, so your measurements are all pretty different. Given that, how can we use these measurements to make a good guess as to the actual snowfall?</p>&#13;
<p class="indent">This simple problem is a great example case for parameter estimation. The parameter we’re estimating is the actual depth of the snowfall from the previous night. Note that, since the wind has blown the snow around and you don’t have a snow gauge, we can never know the <em>exact</em> amount of snow that fell. Instead, we have a collection of data that we can combine using probability, to determine the contribution of each observation to our estimate, in order to help us make the best possible guess.</p>&#13;
<h4 class="h4" id="ch10lev2sec1"><strong><em>Averaging Measurements to Minimize Error</em></strong></h4>&#13;
<p class="noindent">You first instinct is probably to average these measurements. In grade school, we learn to average elements by adding them up and dividing the sum by the total number of elements. So if there are <em>n</em> measurements, each labeled as <em>m<sub>i</sub></em> where <em>i</em> is the <em>i</em>th measurement, we get:</p>&#13;
<div class="equ-image"><img alt="Image" src="../images/f0094-01.jpg"/></div>&#13;
<p class="indent">If we plug in our data, we get the following solution:</p>&#13;
<div class="equ-image"><img alt="Image" src="../images/f0094-02.jpg"/></div>&#13;
<p class="indent">So, given our seven observations, our best guess is that about 6.31 inches of snow fell.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_95"/>Averaging is a technique embedded in our minds from childhood, so its application to this problem seems obvious, but in actuality, it’s hard to reason about why it works and what it has to do with probability. After all, each of our measurements is different, and all of them are likely different from the true value of the snow that fell. For many centuries, even great mathematicians feared that averaging data compounds all of these erroneous measurements, making for a very inaccurate estimate.</p>&#13;
<p class="indent">When we estimate parameters, it’s vital that we understand <em>why</em> we’re making a decision; otherwise, we risk using an estimate that may be unintentionally biased or otherwise wrong in a systematic way. One error commonly made in statistics is to blindly apply procedures without understanding them, which frequently leads to applying the wrong solution to a problem. Probability is our tool for reasoning about uncertainty, and parameter estimation is perhaps the most common process for dealing with uncertainty. Let’s dive a little deeper into averaging to see if we can become more confident that it is the correct path.</p>&#13;
<h4 class="h4" id="ch10lev2sec2"><strong><em>Solving a Simplified Version of Our Problem</em></strong></h4>&#13;
<p class="noindent">Let’s simplify our snowfall problem a bit: rather than imagining all possible depths of snow, imagine the snow falling into nice, uniform blocks so that your yard forms a simple two-dimensional grid. <a href="ch10.xhtml#ch10fig01">Figure 10-1</a> shows this perfectly even, 6-inch-deep snowfall, visualized from the side (rather than as a bird’s-eye view).</p>&#13;
<div class="image"><a id="ch10fig01"/><img alt="Image" src="../images/10fig01.jpg"/></div>&#13;
<p class="figcap"><em>Figure 10-1: Visualizing a perfectly uniform, discrete snowfall</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_96"/>This is the perfect scenario. We don’t have an unlimited number of possible measurements; instead, we sample our six possible locations, and each location has only one possible measurement—6 inches. Obviously, averaging works in this case, because no matter how we sample from this data, our answer will always be 6 inches.</p>&#13;
<p class="indent">Compare that to <a href="ch10.xhtml#ch10fig02">Figure 10-2</a>, which illustrates the data when we include the windblown snow against the left side of your house.</p>&#13;
<div class="image"><a id="ch10fig02"/><img alt="Image" src="../images/10fig02.jpg"/></div>&#13;
<p class="figcap"><em>Figure 10-2: Representing the snow shifted by the wind</em></p>&#13;
<p class="indent">Now, rather than having a nice, smooth surface, we’ve introduced some uncertainty into our problem. Of course, we’re cheating because we can easily count each block of snow and know exactly how much snow has fallen, but we can use this example to explore how we would reason about an uncertain situation. Let’s start investigating our problem by measuring each of the blocks in your yard:</p>&#13;
<p class="equ">8, 7, 6, 6, 5, 4</p>&#13;
<p class="indent">Next, we want to associate some probabilities with each value. Since we’re cheating and know the true value of the snowfall is 6 inches, we’ll also record the difference between the observation and the true value, known as the <em>error</em> value (see <a href="ch10.xhtml#ch10tab01">Table 10-1</a>).</p>&#13;
<p class="tabcap" id="ch10tab01"><span epub:type="pagebreak" id="page_97"/><strong>Table 10-1:</strong> Our Observations, and Their Frequencies and Differences from Truth</p>&#13;
<table class="topbot-d">&#13;
<colgroup>&#13;
<col style="width:30%"/>&#13;
<col style="width:50%"/>&#13;
<col style="width:25%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Observation</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Difference from truth</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Probability</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td class="table-v" style="vertical-align: top;"><p class="taba">8</p></td>&#13;
<td class="table-v" style="vertical-align: top;"><p class="taba">2</p></td>&#13;
<td class="table-v" style="vertical-align: top;"><p class="taba">1/6</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">7</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">1</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">1/6</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-v" style="vertical-align: top;"><p class="taba">6</p></td>&#13;
<td class="table-v" style="vertical-align: top;"><p class="taba">0</p></td>&#13;
<td class="table-v" style="vertical-align: top;"><p class="taba">2/6</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">5</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">–1</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">1/6</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-va" style="vertical-align: top;"><p class="taba">4</p></td>&#13;
<td class="table-va" style="vertical-align: top;"><p class="taba">–2</p></td>&#13;
<td class="table-va" style="vertical-align: top;"><p class="taba">1/6</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Looking at the distance from the true measurement for each possible observation, we can see that the probability of overestimating by a certain value is balanced out by the probability of an undervalued measurement. For example, there is a 1/6 probability of picking a measurement that is 2 inches higher than the true value, but there’s an equally probable chance of picking a measurement that is 2 inches <em>lower</em> than the true measurement. This leads us to our first key insight into why averaging works: errors in measurement tend to cancel each other out.</p>&#13;
<h4 class="h4" id="ch10lev2sec3"><strong><em>Solving a More Extreme Case</em></strong></h4>&#13;
<p class="noindent">With such a smooth distribution of errors, the previous scenario might not have convinced you that errors cancel out in more complex situations. To demonstrate how this effect still holds in other cases, let’s look at a much more extreme example. Suppose the wind has blown 21 inches of snow to one of the six squares and left only 3 inches at each of the remaining squares, as shown in <a href="ch10.xhtml#ch10fig03">Figure 10-3</a>.</p>&#13;
<div class="image"><a id="ch10fig03"/><img alt="Image" src="../images/10fig03.jpg"/></div>&#13;
<p class="figcap"><em>Figure 10-3: A more extreme case of wind shifting the snow</em></p>&#13;
<p class="indent">Now we have a very different distribution of snowfall. For starters, unlike the preceding example, none of the values we can sample from have the true level of snowfall. Also, our errors are no longer nicely distributed—we have a bunch of lower-than-anticipated measurements and one extremely high measurement. <a href="ch10.xhtml#ch10tab02">Table 10-2</a> shows the possible measurements, the difference from the true value, and the probability of each measurement.</p>&#13;
<p class="tabcap" id="ch10tab02"><span epub:type="pagebreak" id="page_98"/><strong>Table 10-2:</strong> Observations, Differences, and Probabilities for Our Extreme Example</p>&#13;
<table class="topbot-d">&#13;
<colgroup>&#13;
<col style="width:30%"/>&#13;
<col style="width:50%"/>&#13;
<col style="width:25%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Observation</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Difference from truth</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Probability</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">21</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">15</p></td>&#13;
<td class="table-b" style="vertical-align: top;"><p class="taba">1/6</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">3</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">–3</p></td>&#13;
<td class="table-ba" style="vertical-align: top;"><p class="taba">5/6</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">We obviously can’t just match up one observation’s error value with another’s and have them cancel out. However, we can use probability to show that even in this extreme distribution, our errors still cancel each other out. We can do this by thinking of each error measurement as a value that’s being voted on by our data. The probability of each error observed is how strongly we believe in that error. When we want to combine our observations, we can consider the probability of the observation as a value representing the strength of its vote toward the final estimate. In this case, the error of –3 inches is five times more likely than the error of 15 inches, so –3 gets weighted more heavily. So, if we were taking a vote, –3 would get five votes, whereas 15 would only get one vote. We combine all of the votes by multiplying each value by its probability and adding them together, giving us a <em>weighted sum</em>. In the extreme case where all the values are the same, we would just have 1 multiplied by the value observed and the result would just be that value. In our example, we get:</p>&#13;
<div class="equ-image"><img alt="Image" src="../images/f0098-01.jpg"/></div>&#13;
<p class="indent">The errors in each observation cancel out to 0! So, once again, we find that it doesn’t matter if none of the possible values is a true measurement or if the distribution of errors is uneven. When we weight our observations by our belief in that observation, the errors tend to cancel each other out.</p>&#13;
<h4 class="h4" id="ch10lev2sec4"><strong><em>Estimating the True Value with Weighted Probabilities</em></strong></h4>&#13;
<p class="noindent">We are now fairly confident that errors from our true measurements cancel out. But we still have a problem: we’ve been working with the errors from the true observation, but to use these we need to know the true value. When we don’t know the true value, all we have to work with are our observations, so we need to see if the errors still cancel out when we have the weighted sum of our original observations.</p>&#13;
<p class="indent">To demonstrate that our method works, we need some “unknown” true values. Let’s start with the following errors:</p>&#13;
<p class="equ">2, 1, –1, –2</p>&#13;
<p class="indent">Since the true measurement is unknown, we’ll represent it with the variable <em>t</em>, then add the error. Now we can weight each of these observations by its probability:</p>&#13;
<div class="equ-image"><img alt="Image" src="../images/f0098-02.jpg"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_99"/>All we’ve done here is add our error to our constant value <em>t</em>, which represents our true measure, then weight each of the results by its probability. We’re doing this to see if we can still get our errors to cancel out and leave us with just the value <em>t</em>. If so, we can expect errors to cancel out even when we’re just averaging raw observations.</p>&#13;
<p class="indent">Our next step is to apply the probability weight to the values in our terms to get one long summation:</p>&#13;
<div class="equ-image"><img alt="Image" src="../images/f0099-01.jpg"/></div>&#13;
<p class="indent">Now if we reorder these terms so that all the errors are together, we can see that our errors will still cancel out, and the weighted <em>t</em> value sums up to just <em>t,</em> our unknown true value:</p>&#13;
<div class="equ-image"><img alt="Image" src="../images/f0099-02.jpg"/></div>&#13;
<p class="indent">This shows that even when we define our measurements as an unknown true value <em>t</em> and add some error value, the errors still cancel out! We are left with just the <em>t</em> in the end. Even when we don’t know what our true measurement or true error is, when we average our values the errors tend to cancel out.</p>&#13;
<p class="indent">In practice, we typically can’t sample the entire space of possible measurements, but the more samples we have, the more the errors are going to cancel out and, in general, the closer our estimate will be to the true value.</p>&#13;
<h4 class="h4" id="ch10lev2sec5"><strong><em>Defining Expectation, Mean, and Averaging</em></strong></h4>&#13;
<p class="noindent">What we’ve arrived at here is formally called the <em>expectation</em> or <em>mean</em> of our data. It is simply the sum of each value weighted by its probability. If we denote each of our measurements as <em>x<sub>i</sub></em> and the probability of each measurement as <em>p<sub>i</sub></em>, we mathematically define the mean—which is generally represented by μ (the lowercase Greek letter mu)—as follows:</p>&#13;
<div class="equ-image"><img alt="Image" src="../images/f0099-03.jpg"/></div>&#13;
<p class="indent">To be clear, this is the <em>exact</em> same calculation as the averaging we learned in grade school, just with notation to make the use of probability more explicit. As an example, to average four numbers, in school we wrote it as:</p>&#13;
<div class="equ-image"><img alt="Image" src="../images/f0099-04.jpg"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_100"/>which is identical to writing:</p>&#13;
<div class="equ-image"><img alt="Image" src="../images/f0100-01.jpg"/></div>&#13;
<p class="indent">or we can just say <em>p<sub>i</sub></em> = 1/4 and write it as:</p>&#13;
<div class="equ-image"><img alt="Image" src="../images/f0100-02.jpg"/></div>&#13;
<p class="indent">So even though the mean is really just the average nearly everyone is familiar with, by building it up from the principles of probability, we see <em>why</em> averaging our data works. No matter how the errors are distributed, the probability of errors at one extreme is canceled out by probabilities at the other extreme. As we take more samples, the averages are more likely to cancel out and we start to approach the true measurement we’re looking for.</p>&#13;
<h3 class="h3" id="ch10lev1sec2"><strong>Means for Measurement vs. Means for Summary</strong></h3>&#13;
<p class="noindent">We’ve been using our mean to estimate a true measurement from a distribution of observations with some added error. But the mean is often used as a way to <em>summarize</em> a set of data. For example, we might refer to things like:</p>&#13;
<ul>&#13;
<li class="noindent">The mean height of a person</li>&#13;
<li class="noindent">The average price of a home</li>&#13;
<li class="noindent">The average age of a student</li>&#13;
</ul>&#13;
<p class="indent">In all of these cases, we aren’t using mean as a parameter estimate for a single true measurement; instead, we’re summarizing the properties of a population. To be precise, we’re estimating a parameter of some abstract property of these populations that may not even be real. Even though mean is a very simple and well-known parameter estimate, it can be easily abused and lead to strange results.</p>&#13;
<p class="indent">A fundamental question you should always ask yourself when averaging data is: “What exactly am I trying to measure and what does this value really mean?” For our snowfall example, the answer is easy: we’re trying to estimate how much snow actually fell last night before the wind blew it around. However, when we’re measuring the “average height,” the answer is less clear. There is no such thing as an average person, and the differences in heights we observe aren’t errors—they’re truly different heights. A person isn’t 5’5” because part of their height drifted onto a 6’3” person!</p>&#13;
<p class="indent">If you were building an amusement park and wanted to know what height restrictions to put on a roller coaster so that at least half of all visitors could ride it, then you have a real value you are trying to measure. However, in that case, the mean suddenly becomes less helpful. A better measurement to estimate is the probability that someone entering your park will be taller than <em>x</em>, where <em>x</em> is the minimum height to ride a roller coaster.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_101"/>All of the claims I’ve made in this chapter assume we are talking about trying to measure a specific value and using the average to cancel the errors out. That is, we’re using averaging as a form of parameter estimation, where our parameter is an actual value that we simply can never know. While averaging can also be useful to summarize large sets of data, we can no longer use the intuition of “errors canceling out” because the variation in the data is genuine, meaningful variation and not error in a measurement.</p>&#13;
<h3 class="h3" id="ch10lev1sec3"><strong>Wrapping Up</strong></h3>&#13;
<p class="noindent">In this chapter, you learned that you can trust your intuition about averaging out your measurements in order to make a best estimate of an unknown value. This is true because errors tend to cancel out. We can formalize this notion of averaging into the idea of the expectation or mean. When we calculate the mean, we are weighting all of our observations by the probability of observing them. Finally, even though averaging is a simple tool to understand, we should always identify and understand what we’re trying to determine by averaging; otherwise, our results may end up being invalid.</p>&#13;
<h3 class="h3" id="ch10lev1sec4"><strong>Exercises</strong></h3>&#13;
<p class="noindent">Try answering the following questions to see how well you understand averaging to estimate an unknown measurement. The solutions can be found at <em><a href="https://nostarch.com/learnbayes/">https://nostarch.com/learnbayes/</a></em>.</p>&#13;
<ol>&#13;
<li class="noindent">It’s possible to get errors that don’t quite cancel out the way we want. In the Fahrenheit temperature scale, 98.6 degrees is the normal body temperature and 100.4 degrees is the typical threshold for a fever. Say you are taking care of a child that feels warm and seems sick, but you take repeated readings from the thermometer and they all read between 99.5 and 100.0 degrees: warm, but not quite a fever. You try the thermometer yourself and get several readings between 97.5 and 98. What could be wrong with the thermometer?</li>&#13;
<li class="noindent">Given that you feel healthy and have traditionally had a very consistently normal temperature, how could you alter the measurements 100, 99.5, 99.6, and 100.2 to estimate if the child has a fever?<span epub:type="pagebreak" id="page_102"/></li>&#13;
</ol>&#13;
</body></html>