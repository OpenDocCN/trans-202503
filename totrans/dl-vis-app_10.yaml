- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training and Testing
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter we’ll look at *training*, the process of taking a system that’s
    been initialized with default or random values and gradually improving it so that
    it’s tuned to the data we want to understand. When we’re done training, we can
    estimate how well our system will evaluate new data it hasn’t seen before, a process
    known as *testing*.
  prefs: []
  type: TYPE_NORMAL
- en: We illustrate the ideas in this chapter using a supervised classifier, which
    we teach with labeled data. Most of the techniques we discuss are general and
    can be applied to almost all types of learners.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we train a classifier with supervised learning, every sample has an associated
    label describing the class we’ve manually assigned to it. The collection of all
    the samples we’re going to learn from, along with their labels, is called the
    *training set*. We’re going to present each of the samples in our training set
    to the classifier, one at a time. For each sample, we give the system the sample’s
    features and ask it to predict its class.
  prefs: []
  type: TYPE_NORMAL
- en: If the prediction is correct (that is, it matches the label we assigned), then
    we move on to the next sample. If the prediction is wrong, we feed the classifier’s
    output and the correct label back to the classifier. Using algorithms that we’ll
    see in later chapters, we modify the classifier’s internal parameters so that
    it’s more likely to predict the correct label if it sees this sample again.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-1](#figure8-1) shows this idea visually. We use the classifier to
    get a prediction and compare that to the label. If they disagree, we update the
    classifier. Then we move on to the next sample.'
  prefs: []
  type: TYPE_NORMAL
- en: '![f08001](Images/f08001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-1: A block diagram of training a classifier'
  prefs: []
  type: TYPE_NORMAL
- en: As our training process runs through this loop, one sample at a time, the classifier’s
    internal variables are nudged toward values that do an increasingly good job of
    predicting labels. Each time we run through the entire training set, we say that
    we’ve trained for one *epoch*. We usually run the system through many epochs so
    the system sees every sample many times. Typically, we keep training as long as
    the system is still learning and improving its performance on the training data,
    but we might stop if we run out of time, or if we run into problems like those
    we discuss later in this chapter and in Chapter 9.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now look at how to measure the classifier’s accuracy at predicting correct
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We begin with a system whose parameters are initialized with random numbers.
    Then we teach it using the samples in the training data. Once the system has been
    *released*, or *deployed*, into the real world, it encounters new, *real-world
    data* (or *deployment data*, *release data*, or *user data*). We’d like to know
    how well our classifier will perform on real-world data before we deploy it. We
    may not need perfect accuracy, but we usually want the system to meet or exceed
    some quality threshold that we already have in mind. How can we estimate the quality
    of our system’s predictions before it’s released?
  prefs: []
  type: TYPE_NORMAL
- en: We need our system to do really well on the training data, but if we judge the
    system’s accuracy based on just this data, we’ll usually be misled. This is an
    important principle in practice, so let’s look at it more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we’re going to use our supervised classifier to process pictures of
    dogs. For every image, it will assign a label identifying that dog’s breed. Our
    goal is to put the system online so people can drag a picture of their dog onto
    their browser, and have it come back either with the dog’s breed, or the catch-all
    “mixed breed.”
  prefs: []
  type: TYPE_NORMAL
- en: To train our system, let’s collect 1,000 photos of different purebred dogs,
    each labeled by an expert. Using [Figure 8-1](#figure8-1), we can show the system
    all 1,000 pictures, and then we show it all of them again, and again, over and
    over, one epoch after another. When doing so, we usually scramble the order of
    the images in each epoch so they don’t always arrive in an identical sequence.
    If our system is designed well, it gradually starts producing more and more accurate
    results, until it’s perhaps correctly identifying the breed of the dog in 99 percent
    of these training pictures.
  prefs: []
  type: TYPE_NORMAL
- en: This does *not* mean that our system is going to be 99 percent correct when
    we put it up on the web. The problem is that the system might be exploiting subtle
    relationships in the training data that aren’t true for data in general. For example,
    suppose our images of poodles look like [Figure 8-2](#figure8-2).
  prefs: []
  type: TYPE_NORMAL
- en: 'When we assembled our training set, we didn’t notice that all of the poodles
    had a little bob at the end of their tails and that none of the other dogs did.
    But the system noticed. That little idiosyncrasy in the data gave the system a
    way to easily classify poodles: instead of looking at the size of the dog’s legs,
    the shape of its nose, and other features, the system could just look for the
    bob on the end of the tail. Using that rule, it would correctly classify all of
    our training images of poodles. We sometimes say that the system is doing what
    we asked for (“identify poodles”), but not what we want (“based on most of the
    features of the dog in the picture, determine if it’s a poodle”). We often say
    that the system has learned to *cheat*, though that might be unfair. What it learned
    was a shortcut that gave us the results we asked for.'
  prefs: []
  type: TYPE_NORMAL
- en: '![f08002](Images/f08002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-2: Training data for a system to identify dog breeds from pictures.
    Top row: Our input poodle images. Bottom row: The feature that our system learned
    to identify a photo as a poodle is highlighted in red.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For another example, suppose that all the pictures of Yorkshire terriers (or
    Yorkies) in our training data were taken when the dogs were sitting on a couch,
    as in [Figure 8-3](#figure8-3). We hadn’t noticed this, nor another important
    fact: none of the other pictures had couches in them. The system may learn that
    if there’s a couch in the image, it can immediately classify the image as a picture
    of a Yorkshire terrier. This rule works perfectly for our training data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![f08003](Images/f08003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-3: Top row: Three Yorkies on couches. Bottom row: Our system has learned
    to recognize the couch, shown in red.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we then deploy our system and someone submits a picture of a Great Dane
    standing in front of a holiday decoration of big white balls on a string, or their
    Siberian husky on a couch, as in [Figure 8-4](#figure8-4). Our system notices
    the white ball at the end of the Great Dane’s tail and says that it’s a poodle,
    and it sees the couch, ignores the dog, and reports that the husky is a Yorkie.
  prefs: []
  type: TYPE_NORMAL
- en: '![f08004](Images/f08004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-4: Top: A Great Dane is standing in front of a holiday display of
    white balls on a string, and a Siberian husky is lying on a couch. Bottom: The
    system sees the white ball on the end of the Great Dane’s tail and tells us that
    the dog is a poodle, and it notices the couch and classifies the dog on it as
    a Yorkie.'
  prefs: []
  type: TYPE_NORMAL
- en: This isn’t just a theoretical concern. A famous example of this phenomenon describes
    a meeting in the 1960s where a presenter was demonstrating an early machine-learning
    system (Muehlhauser 2011). The details of the data are murky, but it seems that
    they had photos of stands of trees with a camouflaged tank in their midst, and
    stands of trees with no tank. The presenter claimed the system could pick out
    the image with the tank without fail. That would have been an incredible feat
    for the time.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the talk, an audience member stood up and observed that the photos
    with the tanks in them were all taken on sunny days, whereas the photos without
    the tanks were all taken on cloudy days. It seemed likely that the system had
    merely distinguished bright skies from dark skies, so the impressive (and accurate)
    results had nothing to do with tanks at all.
  prefs: []
  type: TYPE_NORMAL
- en: This is why looking at the performance on the training data isn’t good enough
    to predict performance in the real world. The system might learn about some weird
    idiosyncrasies in the training data and then use that as a rule, only to be foiled
    by new data that doesn’t happen to have those quirks. This is known formally as
    *overfitting*, though we often refer casually to it simply as *cheating.* We’ll
    look at overfitting more closely in Chapter 9.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve seen that we need some measure other than performance on the training
    set to predict how well our system is going to do if we deploy it. It would be
    great if there was an algorithm or formula that would take our trained classifier
    and tell us how good it is, but there isn’t. There’s no way for us to know how
    our system will perform without trying it out and seeing. Like natural scientists
    who must run experiments to see what actually happens in the real world, we also
    must run experiments to see how well our systems perform.
  prefs: []
  type: TYPE_NORMAL
- en: Test Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The best way anyone has found to determine how well a system will do on new,
    unseen data is to give it new, unseen data and see how well it does. There’s no
    shortcut to this kind of experimental verification.
  prefs: []
  type: TYPE_NORMAL
- en: We call this unseen set of data points, or samples, the *test data* or a *test
    set*. Like the training data, we hope that the test data is representative of
    the data that we’re going to see once our system has been released. The typical
    process is to train the system using the training data until it’s doing as well
    as we think it can do. Then we evaluate it on the test data, and that tells us
    how well it’s likely to do in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: If the system’s performance on the test data isn’t good enough, we need to improve
    it. Since training on more data is almost always a good way to improve performance,
    it’s a usually good idea to gather more data and train again. Another benefit
    of getting more data is that we can diversify our training set. For example, we
    might find dogs other than poodles with bobs on their tail, or we might find dogs
    other than Yorkies on couches. Then our classifier would have to find other ways
    to identify those dogs, and we’d avoid making mistakes due to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'An *essential* rule of the training and testing process is that *we never learn
    from the test data*. As tempting as it might be for us to put the test data into
    the training set so that the system has even more examples to learn from, doing
    so ruins the test data’s value as an objective way to measure the accuracy of
    our system. The problem with learning from the test data is that it just becomes
    part of the training set. That means we’re right back where we were before: the
    system can all too easily key in on idiosyncrasies in the test data. If we then
    use the test data to see how well the classifier works, it might predict the correct
    label for each sample, but it could be cheating. If we learn from the test data,
    it loses its special and valuable quality as a way to measure the performance
    of the system on new data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this reason, we split the test data off from the training data before we
    even begin to train, and we hold it aside. We only come back to the test data
    when training is over, and then we use it just one time to evaluate the quality
    of our system. If the system doesn’t do well enough on the test set, we can’t
    just train some more and then test again. Think of the test set as being like
    the final exam questions in a class: once they’ve been seen, they can’t be used
    again. If our system doesn’t perform well on the test data, we must start all
    over again with a system initialized with random values. Then we can train with
    more data, or for a longer time period. When training is done, we can use the
    test set again, because this newly trained system has never seen it before. If
    it again doesn’t perform well enough, we must start our training all over.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is important enough to repeat: we must never let the system see the test
    data in any way prior to its single use when training has completed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem of accidentally learning from the test data has its own name: *data
    leakage*, also called *data contamination*, or *contaminated data*. We have to
    constantly look out for this, because as our training procedures and classifiers
    become more sophisticated, data leakage can sneak in wearing different (and hard-to-notice)
    disguises. Data leakage can be avoided by practicing *data hygiene*: always make
    sure the test data is kept separate and that it is only used once, when training
    has been completed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We often create the test data by splitting our original data collection into
    two pieces: the training set and the test set. We commonly set up this split to
    give about 75 percent of the samples to the training set. Often samples are chosen
    randomly for each set, but more sophisticated algorithms can try to make sure
    that each collection is a good approximation of the complete input data. Most
    machine-learning libraries offer routines to perform this splitting process for
    us.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-5](#figure8-5) shows the idea.'
  prefs: []
  type: TYPE_NORMAL
- en: '![f08005](Images/f08005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-5: Splitting our input examples into a training set and a test set.
    The split is often about 75:25 or 70:30.'
  prefs: []
  type: TYPE_NORMAL
- en: Validation Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our discussion up to this point, we trained the system for a while, then
    we stopped and evaluated its performance using the test set. If the performance
    wasn’t good enough, we started training all over again.
  prefs: []
  type: TYPE_NORMAL
- en: There’s nothing wrong with that strategy except that it is a slow way to work.
    In practice, we often want a rough estimate of the system’s performance as we
    go along, so we can stop training when we think the system is going to give us
    the performance we want from the test set.
  prefs: []
  type: TYPE_NORMAL
- en: To make this estimate, we split the input data into three sets, rather than
    the two we’ve seen so far. We call this new set the *validation data*, or *validation
    set*. The validation data is yet another chunk of data that’s meant to be a good
    proxy of the real-world data we’ll see when we deploy the system. We typically
    make these three sets by assigning about 60 percent of the original data to the
    training set, 20 percent to the validation set, and the remaining 20 percent to
    the test set. [Figure 8-6](#figure8-6) shows the idea.
  prefs: []
  type: TYPE_NORMAL
- en: '![f08006](Images/f08006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-6: Splitting our input data into a training set, a validation set,
    and a test set'
  prefs: []
  type: TYPE_NORMAL
- en: Our new process will be to train the system for an epoch, running through the
    entire training set, and then we estimate its performance by asking it to make
    predictions for the validation set. We do this after every epoch, so we’re reusing
    the validation set. This causes data leakage, but we only use the validation data
    for informal estimates. We use the system’s performance on the validation set
    to get a general sense of how well it’s learning over time. When we think the
    system is doing well enough to deploy, we use the one-time test set to get a reliable
    performance estimate.
  prefs: []
  type: TYPE_NORMAL
- en: The validation set is also helpful when we use automated search techniques to
    try out many values of hyperparameters. Recall that hyperparameters are variables
    that we set before we run our system to control how it operates, such as how much
    it should update its internal values after an error, or even how complex our classifier
    should be. For each variation, we train on the training set and evaluate the system’s
    performance on the validation set. As we mentioned, we don’t learn from the validation
    set, but we do use it repeatedly. The results from the validation set are just
    an estimate of how well the system is doing, so we can decide when to stop training.
    When we think that performance is up to par, we break out the test set and use
    it once in order to get a reliable estimate of the system’s accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: This gives us a convenient way to repeatedly try out different hyperparameters
    and then choose the best ones based on how they do on the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: This approach to trying out different sets of hyperparameters is based on running
    a loop. Let’s look at a simplified version of that loop now.
  prefs: []
  type: TYPE_NORMAL
- en: To run our loop, we select a set of hyperparameters, train our system, and then
    evaluate its performance with the validation set. This estimates how well the
    system trained with those hyperparameters predicts new data. Next, we set that
    system aside and create a new system, initialized, as always, with random values.
    We apply the next set of hyperparameters, train, and use the validation set to
    evaluate this system’s performance. We repeat this process over and over, once
    for each set of hyperparameters. When we’ve run through all sets of hyperparameters,
    we select the system that seemed to provide the most accurate results, run the
    test set through it, and discover how good its predictions really are.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-7](#figure8-7) shows this whole process graphically.'
  prefs: []
  type: TYPE_NORMAL
- en: '![f08007](Images/f08007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-7: We use the validation set when we try out lots of different hyperparameter
    sets. Note that we still keep a separate test set, which we use just before deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: When the loop is done, we may be tempted to use the results from the validation
    data as our final evaluation of the system. After all, the classifier didn’t learn
    from that data, since it was only used for testing. It may seem that we can save
    ourselves the trouble of making a separate test set and then run it through the
    system to get a performance estimate.
  prefs: []
  type: TYPE_NORMAL
- en: But that would be working with leaked data, which would distort our conclusions.
    The source of this leakage is a bit sneaky and subtle, like many data contamination
    issues. The problem is that although the classifier didn’t learn from the validation
    data, our whole training and evaluation system did, because it used that data
    to pick the best hyperparameters for the classifier. In other words, even though
    the classifier didn’t explicitly learn from the validation data, that data influenced
    our choice of classifier. We chose a classifier that did best on the validation
    data, so we *already know* that it’s going to do a good job with it. In other
    words, our knowledge of the classifier’s performance on the validation data “leaked”
    into our selection process.
  prefs: []
  type: TYPE_NORMAL
- en: 'If this seems subtle or tricky, it is. This sort of thing is easy to overlook
    or miss, which is why we have to be vigilant about data contamination. Otherwise
    we risk thinking our system is better than it really is, and thus we deploy a
    system that isn’t good enough for our intended use. To get a good estimate for
    how our system performs on brand-new data that it has never seen before, there’s
    no shortcut: we need to test it on brand-new data that it has never seen before.
    That’s why we always save the test set for the very end.'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the last section, we took almost half our training data and set it aside
    for validation and testing. That’s fine when we have lots and lots of data. But
    what if our sample set is small and we can’t get more data? Maybe we’re working
    with photos of Pluto and its moons taken by the New Horizons spacecraft during
    its 2015 flyby, and we want to build a classifier we can install on future spacecraft
    to identify what kind of terrain they’re looking at. Our dataset is limited and
    it’s not going to get bigger: there are no new close-up photos of Pluto coming
    anytime soon. Every image that we have is precious, and we want to learn all we
    can from every photo we have. Setting some images aside just to determine how
    good our classifier is would be a huge price to pay.'
  prefs: []
  type: TYPE_NORMAL
- en: If we’re willing to accept an estimate of the system’s performance, rather than
    a reliable measure of it, then we don’t have to set aside a test set. We can indeed
    train on every piece of input data and still predict our performance on new data.
    The catch is that we’re only going to get back an estimate of the system’s accuracy,
    so it won’t be as reliable a measure as we’d get by using a real test set, but
    when samples are precious, that tradeoff can be worth it.
  prefs: []
  type: TYPE_NORMAL
- en: The technique that does this job is called *cross-validation* or *rotation validation*.
    There are different types of cross-validation algorithms, but they all share the
    same basic structure (Schneider 1997). We’re going to look at a version that doesn’t
    require us to create a dedicated test set.
  prefs: []
  type: TYPE_NORMAL
- en: The core idea is that we can run a loop that repeatedly trains the same system
    from scratch and then tests it. Each time through we’ll split the entire input
    data into a one-time training set and a one-time validation set. The key thing
    is that we’ll construct these sets differently each time through the loop. This
    lets us use all of our data for training (though, as we’ll see, not all at the
    same time).
  prefs: []
  type: TYPE_NORMAL
- en: We begin by building a fresh instance of our classifier. We split the input
    data into a temporary training set and temporary validation set. We train our
    system on the temporary training set and evaluate it with the temporary test set.
    This gives us a score for the classifier’s performance. Now we go through the
    loop again, but this time, we split the training data into different temporary
    training and test sets. When we’ve done this for every iteration through the loop,
    the average of all the scores is our estimate for the overall performance of our
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: A visual summary of cross-validation is shown in [Figure 8-8](#figure8-8).
  prefs: []
  type: TYPE_NORMAL
- en: '![f08008](Images/f08008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-8: Using cross-validation to evaluate our system’s performance'
  prefs: []
  type: TYPE_NORMAL
- en: By using cross-validation, we get to train with all of our training data (though
    not all of it on every pass through the loop), yet we still get an objective measurement
    of the system’s quality from a held-out test set. This algorithm doesn’t have
    data leakage issues because each time through the loop, we create a new classifier,
    and the temporary test set for that classifier contains data that is brand-new
    and unseen with respect to *that specific* classifier, so it’s fair to use it
    to evaluate that classifier’s performance. The penalty for this technique is that
    our final estimate of the system’s accuracy is not as reliable as what we’d get
    from a held-out test set.
  prefs: []
  type: TYPE_NORMAL
- en: A variety of different algorithms are available for constructing the temporary
    training and validation sets. Let’s look at a popular approach.
  prefs: []
  type: TYPE_NORMAL
- en: k-Fold Cross-Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Perhaps the most popular way to build the temporary datasets for cross-validation
    is called *k-fold cross-validation*. The letter *k* here is not the first letter
    of a word. Instead, it stands for an integer (for example, we might run “2-fold
    cross-validation” or “5-fold cross-validation”). Typically, the value of *k* is
    the number of times we want to go through the loop of [Figure 8-8](#figure8-8).
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm starts before the cross-validation loop begins. We take our training
    data and split it up into a series of equal-sized groups. Every sample is placed
    into exactly one group, and all groups are the same size (well, we allow one smaller
    group at the end if we can’t split the input into equal-sized pieces).
  prefs: []
  type: TYPE_NORMAL
- en: It would have been great if these groups were each called something like “group,”
    or “equal-sized piece,” but the word that’s come to describe this idea is *fold.*
    This word is used here in an unusual sense to mean the section of a page *between*
    creases (or ends). To picture this, imagine writing all the samples in the training
    set on a long piece of paper and then folding that up into a fixed number of equal
    pieces. Each time we bend the paper we’re making a crease, and the material between
    the creases is called a fold. [Figure 8-9](#figure8-9) shows the idea.
  prefs: []
  type: TYPE_NORMAL
- en: '![f08009](Images/f08009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-9: Creating the folds for *k*-fold cross-validation. Here we have
    four creases and five folds.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s build equal-sized folds from our training data. We can flatten out [Figure
    8-9](#figure8-9) to create the more typical picture of five folds, shown in [Figure
    8-10](#figure8-10).
  prefs: []
  type: TYPE_NORMAL
- en: '![f08010](Images/f08010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-10: Splitting our training set into five equally sized folds, named
    Fold 1 through Fold 5'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use these five folds to see how the loop proceeds. The first time through
    the loop, we treat the samples in folds 2 through 5 as our temporary training
    set, and the samples in fold 1 as our temporary test set. That is, we train the
    classifier with the samples in folds 2 through 5 and then evaluate it with the
    samples in fold 1.
  prefs: []
  type: TYPE_NORMAL
- en: The next time through the loop, starting with a fresh classifier initialized
    with random numbers, we use the samples in folds 1, 3, 4, and 5 for our temporary
    training set, and the samples in fold 2 for our temporary test set. We train and
    test as usual with these two sets, and continue with the remaining folds. [Figure
    8-11](#figure8-11) shows the idea visually.
  prefs: []
  type: TYPE_NORMAL
- en: '![f08011](Images/f08011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-11: In each pass through the loop, we choose one fold for testing
    (in blue) and use the others (in red) for training. If we go through the loop
    more than five times, we repeat the pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: We can choose to repeat the loop as many times as we like, just repeating the
    cycle of fold selections (or mixing up the data so the sets always have different
    contents).
  prefs: []
  type: TYPE_NORMAL
- en: In an optional final step, we can train a fresh classifier with all of our data.
    This means we can’t get an estimate for its performance. But if we watch the training
    carefully, and look out for overfitting (discussed in the next chapter), we can
    usually assume that the system that was trained on all the data is at least as
    good as the worst performance we obtained from cross-validation (and we hope it
    will be at least a little bit better).
  prefs: []
  type: TYPE_NORMAL
- en: This makes cross-validation a great option when our data is limited. We do have
    to repeat the train-test cycle many times, and our final performance measure is
    only an estimate, both of which are downsides, but we gain the ability to train
    with all of our data, squeezing every little bit of information out of our input
    set and using it to make our classifier better.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed *k*-fold cross-validation with a classifier, but the algorithm
    is broadly applicable to almost any kind of learner.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This chapter was all about training a deep learning system and deciding if
    it’s working well enough to be deployed. We focused on a classifier, but the general
    thinking holds for any such system. We saw that we split the data into two pieces:
    a training set and a test set. We learned about the problems of overfitting and
    data leakage, and we also saw how we can use a validation set to get a rough idea
    of how well the system is learning after every epoch. Finally, we looked at cross
    validation, a technique typically used with small dataset, to estimate a system’s
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll take a closer look at overfitting and underfitting.
  prefs: []
  type: TYPE_NORMAL
