- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Training and Testing
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 训练与测试
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: In this chapter we’ll look at *training*, the process of taking a system that’s
    been initialized with default or random values and gradually improving it so that
    it’s tuned to the data we want to understand. When we’re done training, we can
    estimate how well our system will evaluate new data it hasn’t seen before, a process
    known as *testing*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将讨论*训练*，即将一个已用默认或随机值初始化的系统，逐渐改进，以便它能够针对我们想要理解的数据进行调整。训练完成后，我们可以估计系统在评估新数据时的表现，即它如何处理之前未见过的数据，这一过程称为*测试*。
- en: We illustrate the ideas in this chapter using a supervised classifier, which
    we teach with labeled data. Most of the techniques we discuss are general and
    can be applied to almost all types of learners.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个监督分类器来说明这一章中的概念，通过标注数据来教它。我们讨论的大部分技术都是通用的，可以应用于几乎所有类型的学习器。
- en: Training
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: When we train a classifier with supervised learning, every sample has an associated
    label describing the class we’ve manually assigned to it. The collection of all
    the samples we’re going to learn from, along with their labels, is called the
    *training set*. We’re going to present each of the samples in our training set
    to the classifier, one at a time. For each sample, we give the system the sample’s
    features and ask it to predict its class.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用监督学习训练一个分类器时，每个样本都有一个与之相关的标签，用来描述我们手动分配给它的类别。所有我们要学习的样本以及它们的标签集合被称为*训练集*。我们将把训练集中的每个样本逐一呈现给分类器。对于每个样本，我们提供样本的特征并要求系统预测它的类别。
- en: If the prediction is correct (that is, it matches the label we assigned), then
    we move on to the next sample. If the prediction is wrong, we feed the classifier’s
    output and the correct label back to the classifier. Using algorithms that we’ll
    see in later chapters, we modify the classifier’s internal parameters so that
    it’s more likely to predict the correct label if it sees this sample again.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果预测正确（即与我们分配的标签一致），我们就进入下一个样本。如果预测错误，我们将分类器的输出和正确的标签反馈给分类器。通过我们将在后续章节中看到的算法，我们修改分类器的内部参数，使其更有可能在再次看到该样本时预测正确的标签。
- en: '[Figure 8-1](#figure8-1) shows this idea visually. We use the classifier to
    get a prediction and compare that to the label. If they disagree, we update the
    classifier. Then we move on to the next sample.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-1](#figure8-1) 直观地展示了这个概念。我们使用分类器得到一个预测，并将其与标签进行比较。如果它们不一致，我们会更新分类器。然后我们继续下一个样本。'
- en: '![f08001](Images/f08001.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![f08001](Images/f08001.png)'
- en: 'Figure 8-1: A block diagram of training a classifier'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-1：训练分类器的框图
- en: As our training process runs through this loop, one sample at a time, the classifier’s
    internal variables are nudged toward values that do an increasingly good job of
    predicting labels. Each time we run through the entire training set, we say that
    we’ve trained for one *epoch*. We usually run the system through many epochs so
    the system sees every sample many times. Typically, we keep training as long as
    the system is still learning and improving its performance on the training data,
    but we might stop if we run out of time, or if we run into problems like those
    we discuss later in this chapter and in Chapter 9.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的训练过程一次处理一个样本时，分类器的内部变量会逐渐调整到能够更好地预测标签的值。每当我们遍历整个训练集一次，我们就说我们训练了一个*周期*。通常，我们会让系统经历多个周期，让系统多次看到每个样本。通常情况下，我们会继续训练，直到系统不再学习并且在训练数据上的表现有所改善，但如果时间耗尽，或者遇到我们在本章及第九章后面讨论的问题时，我们可能会停止训练。
- en: Let’s now look at how to measure the classifier’s accuracy at predicting correct
    labels.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何衡量分类器预测正确标签的准确性。
- en: Testing the Performance
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试性能
- en: We begin with a system whose parameters are initialized with random numbers.
    Then we teach it using the samples in the training data. Once the system has been
    *released*, or *deployed*, into the real world, it encounters new, *real-world
    data* (or *deployment data*, *release data*, or *user data*). We’d like to know
    how well our classifier will perform on real-world data before we deploy it. We
    may not need perfect accuracy, but we usually want the system to meet or exceed
    some quality threshold that we already have in mind. How can we estimate the quality
    of our system’s predictions before it’s released?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个参数初始化为随机数的系统开始。然后，我们通过训练数据中的样本来教它。一旦系统被*发布*，或*部署*到现实世界，它将遇到新的*现实世界数据*（或*部署数据*、*发布数据*、*用户数据*）。我们希望在系统发布之前，能够知道它在现实世界数据上的表现如何。我们可能不需要完美的准确性，但通常希望系统达到或超过我们心中的某个质量标准。在系统发布之前，我们如何估计其预测质量呢？
- en: We need our system to do really well on the training data, but if we judge the
    system’s accuracy based on just this data, we’ll usually be misled. This is an
    important principle in practice, so let’s look at it more detail.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要系统在训练数据上表现得很好，但如果仅仅根据这些数据来评估系统的准确性，通常会误导我们。这是实践中的一个重要原则，所以让我们更详细地看一下。
- en: Suppose we’re going to use our supervised classifier to process pictures of
    dogs. For every image, it will assign a label identifying that dog’s breed. Our
    goal is to put the system online so people can drag a picture of their dog onto
    their browser, and have it come back either with the dog’s breed, or the catch-all
    “mixed breed.”
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们打算使用我们的监督分类器来处理狗的图片。对于每一张图片，它将分配一个标签来识别狗的品种。我们的目标是将系统放到网上，让人们可以将自己狗的图片拖到浏览器上，然后得到该狗的品种，或者一个笼统的“混合品种”。
- en: To train our system, let’s collect 1,000 photos of different purebred dogs,
    each labeled by an expert. Using [Figure 8-1](#figure8-1), we can show the system
    all 1,000 pictures, and then we show it all of them again, and again, over and
    over, one epoch after another. When doing so, we usually scramble the order of
    the images in each epoch so they don’t always arrive in an identical sequence.
    If our system is designed well, it gradually starts producing more and more accurate
    results, until it’s perhaps correctly identifying the breed of the dog in 99 percent
    of these training pictures.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的系统，让我们收集1,000张不同纯种狗的照片，每张照片都由专家标注。通过使用[图8-1](#figure8-1)，我们可以将所有1,000张图片展示给系统，然后反复展示这些图片，一遍又一遍，每一个周期接一个周期。在这样做时，我们通常会打乱每个周期中的图片顺序，确保它们不会总是按照相同的顺序出现。如果我们的系统设计得好，它会逐渐开始产生越来越准确的结果，直到它可能在99%的训练图片中正确识别出狗的品种。
- en: This does *not* mean that our system is going to be 99 percent correct when
    we put it up on the web. The problem is that the system might be exploiting subtle
    relationships in the training data that aren’t true for data in general. For example,
    suppose our images of poodles look like [Figure 8-2](#figure8-2).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这并*不*意味着我们的系统在我们把它放到网上时会有99%的准确率。问题在于，系统可能在利用训练数据中微妙的关系，而这些关系在一般的数据中并不成立。例如，假设我们的贵宾犬图片像[图8-2](#figure8-2)这样。
- en: 'When we assembled our training set, we didn’t notice that all of the poodles
    had a little bob at the end of their tails and that none of the other dogs did.
    But the system noticed. That little idiosyncrasy in the data gave the system a
    way to easily classify poodles: instead of looking at the size of the dog’s legs,
    the shape of its nose, and other features, the system could just look for the
    bob on the end of the tail. Using that rule, it would correctly classify all of
    our training images of poodles. We sometimes say that the system is doing what
    we asked for (“identify poodles”), but not what we want (“based on most of the
    features of the dog in the picture, determine if it’s a poodle”). We often say
    that the system has learned to *cheat*, though that might be unfair. What it learned
    was a shortcut that gave us the results we asked for.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们组建训练集时，我们没有注意到所有的贵宾犬尾巴末端都有一个小的球形，而其他狗都没有。但系统注意到了。数据中的这个小特征为系统提供了一种轻松分类贵宾犬的方法：系统不需要查看狗腿的大小、鼻子的形状等特征，而只需检查尾巴末端的小球。使用这个规则，它将正确分类所有我们的贵宾犬训练图片。我们有时会说系统做到了我们要求的事（“识别贵宾犬”），但没有做我们想要的事（“根据图片中大部分特征判断是否是贵宾犬”）。我们常说系统学会了*作弊*，尽管这可能不公平。它学会了一个捷径，给我们带来了我们所要求的结果。
- en: '![f08002](Images/f08002.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![f08002](Images/f08002.png)'
- en: 'Figure 8-2: Training data for a system to identify dog breeds from pictures.
    Top row: Our input poodle images. Bottom row: The feature that our system learned
    to identify a photo as a poodle is highlighted in red.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-2：用于识别狗狗品种的训练数据。上排：我们的输入贵宾犬图片。下排：我们的系统学习到的将图片识别为贵宾犬的特征，用红色标出。
- en: 'For another example, suppose that all the pictures of Yorkshire terriers (or
    Yorkies) in our training data were taken when the dogs were sitting on a couch,
    as in [Figure 8-3](#figure8-3). We hadn’t noticed this, nor another important
    fact: none of the other pictures had couches in them. The system may learn that
    if there’s a couch in the image, it can immediately classify the image as a picture
    of a Yorkshire terrier. This rule works perfectly for our training data.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是，假设我们训练数据中所有约克夏犬（或Yorkies）的图片都是在狗狗坐在沙发上时拍摄的，如[图 8-3](#figure8-3)所示。我们之前没有注意到这一点，也没有注意到另一个重要事实：其他所有图片中都没有沙发。系统可能会学到，如果图像中有沙发，它就能立即将这张图像分类为约克夏犬的图片。这条规则对于我们的训练数据来说是完全有效的。
- en: '![f08003](Images/f08003.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![f08003](Images/f08003.png)'
- en: 'Figure 8-3: Top row: Three Yorkies on couches. Bottom row: Our system has learned
    to recognize the couch, shown in red.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-3：上排：三只约克夏犬坐在沙发上。下排：我们的系统已经学会识别沙发，并用红色标出。
- en: Suppose we then deploy our system and someone submits a picture of a Great Dane
    standing in front of a holiday decoration of big white balls on a string, or their
    Siberian husky on a couch, as in [Figure 8-4](#figure8-4). Our system notices
    the white ball at the end of the Great Dane’s tail and says that it’s a poodle,
    and it sees the couch, ignores the dog, and reports that the husky is a Yorkie.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将系统部署后，有人提交了一张大丹犬站在一串大白球装饰前的照片，或者他们的西伯利亚哈士奇坐在沙发上的照片，如[图 8-4](#figure8-4)所示。我们的系统看到了大丹犬尾巴末端的白色球，告诉我们那是一只贵宾犬，并看到了沙发，忽略了狗，报告说那只哈士奇是约克夏犬。
- en: '![f08004](Images/f08004.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![f08004](Images/f08004.png)'
- en: 'Figure 8-4: Top: A Great Dane is standing in front of a holiday display of
    white balls on a string, and a Siberian husky is lying on a couch. Bottom: The
    system sees the white ball on the end of the Great Dane’s tail and tells us that
    the dog is a poodle, and it notices the couch and classifies the dog on it as
    a Yorkie.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-4：上排：一只大丹犬站在一串白色球形装饰品前面，一只西伯利亚哈士奇躺在沙发上。下排：系统看到了大丹犬尾巴末端的白色球，告诉我们那是贵宾犬，并注意到沙发，将躺在沙发上的狗分类为约克夏犬。
- en: This isn’t just a theoretical concern. A famous example of this phenomenon describes
    a meeting in the 1960s where a presenter was demonstrating an early machine-learning
    system (Muehlhauser 2011). The details of the data are murky, but it seems that
    they had photos of stands of trees with a camouflaged tank in their midst, and
    stands of trees with no tank. The presenter claimed the system could pick out
    the image with the tank without fail. That would have been an incredible feat
    for the time.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅仅是一个理论问题。有一个著名的例子描述了1960年代的一次会议，会上一个演讲者展示了一个早期的机器学习系统（Muehlhauser 2011）。数据的具体细节不清楚，但似乎他们有一些树丛中有伪装坦克的照片，以及没有坦克的树丛照片。演讲者声称该系统能够毫无失败地识别出带有坦克的图像。对于那个时代来说，这将是一项令人难以置信的壮举。
- en: At the end of the talk, an audience member stood up and observed that the photos
    with the tanks in them were all taken on sunny days, whereas the photos without
    the tanks were all taken on cloudy days. It seemed likely that the system had
    merely distinguished bright skies from dark skies, so the impressive (and accurate)
    results had nothing to do with tanks at all.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在演讲结束时，一位观众站起来指出，带有坦克的照片都是在阳光明媚的日子拍摄的，而没有坦克的照片则是在阴天拍摄的。看起来系统只是区分了明亮的天空和阴暗的天空，因此那些令人印象深刻（且准确）的结果其实与坦克毫无关系。
- en: This is why looking at the performance on the training data isn’t good enough
    to predict performance in the real world. The system might learn about some weird
    idiosyncrasies in the training data and then use that as a rule, only to be foiled
    by new data that doesn’t happen to have those quirks. This is known formally as
    *overfitting*, though we often refer casually to it simply as *cheating.* We’ll
    look at overfitting more closely in Chapter 9.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么仅仅查看训练数据上的表现不足以预测在现实世界中的表现。系统可能会学到一些训练数据中的奇怪特征，并将其作为规则来使用，结果却被没有这些特征的新数据所破坏。这种现象在正式上称为*过拟合*，但我们通常称其为*作弊*。我们将在第9章更详细地讨论过拟合问题。
- en: We’ve seen that we need some measure other than performance on the training
    set to predict how well our system is going to do if we deploy it. It would be
    great if there was an algorithm or formula that would take our trained classifier
    and tell us how good it is, but there isn’t. There’s no way for us to know how
    our system will perform without trying it out and seeing. Like natural scientists
    who must run experiments to see what actually happens in the real world, we also
    must run experiments to see how well our systems perform.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，我们需要一种除了训练集上的性能之外的度量方式来预测我们部署系统后它的表现。如果有一种算法或公式可以拿我们的训练好的分类器并告诉我们它有多好，那该有多好，但事实并没有。我们无法知道系统的表现如何，除非通过尝试并观察。就像自然科学家必须通过实验来了解现实世界中的实际情况一样，我们也必须通过实验来看看我们的系统表现如何。
- en: Test Data
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试数据
- en: The best way anyone has found to determine how well a system will do on new,
    unseen data is to give it new, unseen data and see how well it does. There’s no
    shortcut to this kind of experimental verification.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，任何人发现的最佳方法是给系统提供新的、看不见的数据，看看它的表现如何，这样我们就能知道系统在新数据上的表现。没有捷径可以走这种实验验证。
- en: We call this unseen set of data points, or samples, the *test data* or a *test
    set*. Like the training data, we hope that the test data is representative of
    the data that we’re going to see once our system has been released. The typical
    process is to train the system using the training data until it’s doing as well
    as we think it can do. Then we evaluate it on the test data, and that tells us
    how well it’s likely to do in the real world.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这一组看不见的数据点或样本称为*测试数据*或*测试集*。像训练数据一样，我们希望测试数据能够代表我们系统发布后将要遇到的数据。典型的过程是使用训练数据训练系统，直到系统表现达到我们认为的最佳水平。然后我们在测试数据上进行评估，这能告诉我们系统在现实世界中的表现如何。
- en: If the system’s performance on the test data isn’t good enough, we need to improve
    it. Since training on more data is almost always a good way to improve performance,
    it’s a usually good idea to gather more data and train again. Another benefit
    of getting more data is that we can diversify our training set. For example, we
    might find dogs other than poodles with bobs on their tail, or we might find dogs
    other than Yorkies on couches. Then our classifier would have to find other ways
    to identify those dogs, and we’d avoid making mistakes due to overfitting.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统在测试数据上的表现不够好，我们需要改进它。由于在更多数据上训练几乎总是提高性能的好方法，通常收集更多数据并重新训练是一个不错的主意。获取更多数据的另一个好处是可以使我们的训练集更加多样化。例如，我们可能会发现除贵宾犬以外的其他犬种尾巴上有小球，或者可能会发现除约克夏犬以外的其他犬种在沙发上。这样我们的分类器就必须找到其他方法来识别这些狗，我们也能避免由于过拟合而犯错。
- en: 'An *essential* rule of the training and testing process is that *we never learn
    from the test data*. As tempting as it might be for us to put the test data into
    the training set so that the system has even more examples to learn from, doing
    so ruins the test data’s value as an objective way to measure the accuracy of
    our system. The problem with learning from the test data is that it just becomes
    part of the training set. That means we’re right back where we were before: the
    system can all too easily key in on idiosyncrasies in the test data. If we then
    use the test data to see how well the classifier works, it might predict the correct
    label for each sample, but it could be cheating. If we learn from the test data,
    it loses its special and valuable quality as a way to measure the performance
    of the system on new data.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试过程中的*基本*规则是*我们永远不能从测试数据中学习*。尽管将测试数据放入训练集，让系统有更多的示例来学习是很诱人的，但这么做会破坏测试数据作为衡量系统准确性的客观工具的价值。从测试数据中学习的问题在于它变成了训练集的一部分。这意味着我们又回到了最初的状态：系统很容易专注于测试数据中的独特特征。如果我们随后使用测试数据来看分类器的表现，它可能会正确预测每个样本的标签，但可能是在作弊。如果我们从测试数据中学习，它就失去了作为衡量系统在新数据上表现的特殊和宝贵的质量。
- en: 'For this reason, we split the test data off from the training data before we
    even begin to train, and we hold it aside. We only come back to the test data
    when training is over, and then we use it just one time to evaluate the quality
    of our system. If the system doesn’t do well enough on the test set, we can’t
    just train some more and then test again. Think of the test set as being like
    the final exam questions in a class: once they’ve been seen, they can’t be used
    again. If our system doesn’t perform well on the test data, we must start all
    over again with a system initialized with random values. Then we can train with
    more data, or for a longer time period. When training is done, we can use the
    test set again, because this newly trained system has never seen it before. If
    it again doesn’t perform well enough, we must start our training all over.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这个原因，我们在开始训练之前就将测试数据从训练数据中拆分出来，并将其保留下来。我们只有在训练结束后才会回到测试数据，然后只使用一次来评估系统的质量。如果系统在测试集上的表现不够好，我们不能只是继续训练然后再进行测试。可以将测试集看作是课堂上的期末考试题目：一旦被看到，它们就不能再使用。如果系统在测试数据上的表现不好，我们必须从头开始，使用一个初始化为随机值的系统。然后，我们可以使用更多的数据或训练更长的时间。当训练完成后，我们可以再次使用测试集，因为这个新训练的系统之前从未接触过它。如果它再次表现不够好，我们必须重新开始训练。
- en: 'This is important enough to repeat: we must never let the system see the test
    data in any way prior to its single use when training has completed.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这点很重要，值得重复：我们绝不能在训练完成之前以任何方式让系统接触到测试数据。
- en: 'The problem of accidentally learning from the test data has its own name: *data
    leakage*, also called *data contamination*, or *contaminated data*. We have to
    constantly look out for this, because as our training procedures and classifiers
    become more sophisticated, data leakage can sneak in wearing different (and hard-to-notice)
    disguises. Data leakage can be avoided by practicing *data hygiene*: always make
    sure the test data is kept separate and that it is only used once, when training
    has been completed.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 无意中从测试数据中学习的问题有一个专门的名称：*数据泄漏*，也叫做*数据污染*或*污染数据*。我们必须时刻警惕这个问题，因为随着我们的训练过程和分类器变得越来越复杂，数据泄漏可能会以不同（且难以察觉）的伪装悄然进入。通过执行*数据卫生*可以避免数据泄漏：始终确保测试数据被单独保存，并且只有在训练完成后才使用一次。
- en: 'We often create the test data by splitting our original data collection into
    two pieces: the training set and the test set. We commonly set up this split to
    give about 75 percent of the samples to the training set. Often samples are chosen
    randomly for each set, but more sophisticated algorithms can try to make sure
    that each collection is a good approximation of the complete input data. Most
    machine-learning libraries offer routines to perform this splitting process for
    us.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常通过将原始数据集拆分为两个部分来创建测试数据：训练集和测试集。我们通常将约75%的样本分配给训练集。样本通常是随机选择的，但更复杂的算法可以尝试确保每个集合都能较好地近似完整的输入数据。大多数机器学习库都提供了可以为我们执行这一拆分过程的功能。
- en: '[Figure 8-5](#figure8-5) shows the idea.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-5](#figure8-5)展示了这一思想。'
- en: '![f08005](Images/f08005.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![f08005](Images/f08005.png)'
- en: 'Figure 8-5: Splitting our input examples into a training set and a test set.
    The split is often about 75:25 or 70:30.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-5：将输入示例拆分为训练集和测试集。通常的拆分比例为75:25或70:30。
- en: Validation Data
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证数据
- en: In our discussion up to this point, we trained the system for a while, then
    we stopped and evaluated its performance using the test set. If the performance
    wasn’t good enough, we started training all over again.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在到目前为止的讨论中，我们训练了系统一段时间，然后停止并使用测试集评估其性能。如果性能不够好，我们就重新开始训练。
- en: There’s nothing wrong with that strategy except that it is a slow way to work.
    In practice, we often want a rough estimate of the system’s performance as we
    go along, so we can stop training when we think the system is going to give us
    the performance we want from the test set.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个策略没有问题，唯一的问题是它是一种工作效率较慢的方法。在实际操作中，我们通常希望在进行过程中对系统的性能进行大致估计，这样当我们认为系统能够在测试集上达到我们想要的性能时，就可以停止训练。
- en: To make this estimate, we split the input data into three sets, rather than
    the two we’ve seen so far. We call this new set the *validation data*, or *validation
    set*. The validation data is yet another chunk of data that’s meant to be a good
    proxy of the real-world data we’ll see when we deploy the system. We typically
    make these three sets by assigning about 60 percent of the original data to the
    training set, 20 percent to the validation set, and the remaining 20 percent to
    the test set. [Figure 8-6](#figure8-6) shows the idea.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做出这个估计，我们将输入数据分为三个数据集，而不是之前看到的两个数据集。我们将这个新数据集称为*验证数据*，或*验证集*。验证数据是另一块数据，它旨在成为我们在部署系统时将遇到的现实世界数据的良好代理。我们通常通过将大约
    60% 的原始数据分配给训练集，20% 分配给验证集，剩余的 20% 分配给测试集来生成这三个数据集。[图 8-6](#figure8-6)展示了这一思路。
- en: '![f08006](Images/f08006.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![f08006](Images/f08006.png)'
- en: 'Figure 8-6: Splitting our input data into a training set, a validation set,
    and a test set'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-6：将我们的输入数据分为训练集、验证集和测试集
- en: Our new process will be to train the system for an epoch, running through the
    entire training set, and then we estimate its performance by asking it to make
    predictions for the validation set. We do this after every epoch, so we’re reusing
    the validation set. This causes data leakage, but we only use the validation data
    for informal estimates. We use the system’s performance on the validation set
    to get a general sense of how well it’s learning over time. When we think the
    system is doing well enough to deploy, we use the one-time test set to get a reliable
    performance estimate.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新过程是训练系统一个周期，遍历整个训练集，然后通过要求它对验证集进行预测来估计其性能。我们在每个周期后都这样做，因此我们在重复使用验证集。这会导致数据泄漏，但我们只使用验证数据进行非正式的估计。我们使用系统在验证集上的表现来大致了解它随着时间的推移学习得怎么样。当我们认为系统表现足够好以进行部署时，我们使用一次性测试集来获得可靠的性能估计。
- en: The validation set is also helpful when we use automated search techniques to
    try out many values of hyperparameters. Recall that hyperparameters are variables
    that we set before we run our system to control how it operates, such as how much
    it should update its internal values after an error, or even how complex our classifier
    should be. For each variation, we train on the training set and evaluate the system’s
    performance on the validation set. As we mentioned, we don’t learn from the validation
    set, but we do use it repeatedly. The results from the validation set are just
    an estimate of how well the system is doing, so we can decide when to stop training.
    When we think that performance is up to par, we break out the test set and use
    it once in order to get a reliable estimate of the system’s accuracy.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用自动化搜索技术尝试不同的超参数值时，验证集也非常有用。回想一下，超参数是我们在运行系统之前设置的变量，用来控制系统的操作方式，例如在出错后应更新多少内部值，或者我们分类器的复杂度应该有多大。对于每一个变化，我们在训练集上进行训练，并在验证集上评估系统的表现。正如我们所提到的，我们并不从验证集中学习，但我们确实会反复使用它。来自验证集的结果仅仅是我们系统表现的估计，这样我们就可以决定何时停止训练。当我们认为系统的表现足够好时，我们会拿出测试集，并仅使用一次，以获得系统准确性的可靠估计。
- en: This gives us a convenient way to repeatedly try out different hyperparameters
    and then choose the best ones based on how they do on the validation set.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一种方便的方式，可以反复尝试不同的超参数，然后根据它们在验证集上的表现选择最佳的参数。
- en: This approach to trying out different sets of hyperparameters is based on running
    a loop. Let’s look at a simplified version of that loop now.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这种尝试不同超参数集的方法是基于运行一个循环的。现在让我们看看这个循环的简化版本。
- en: To run our loop, we select a set of hyperparameters, train our system, and then
    evaluate its performance with the validation set. This estimates how well the
    system trained with those hyperparameters predicts new data. Next, we set that
    system aside and create a new system, initialized, as always, with random values.
    We apply the next set of hyperparameters, train, and use the validation set to
    evaluate this system’s performance. We repeat this process over and over, once
    for each set of hyperparameters. When we’ve run through all sets of hyperparameters,
    we select the system that seemed to provide the most accurate results, run the
    test set through it, and discover how good its predictions really are.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行我们的循环，我们选择一组超参数，训练系统，然后用验证集评估其性能。这可以估计用这些超参数训练的系统在预测新数据时的表现。接下来，我们将该系统搁置一旁，创建一个新的系统，像往常一样初始化为随机值。我们应用下一组超参数，进行训练，并使用验证集评估这个系统的表现。我们一次次地重复这个过程，每次使用不同的超参数集。当所有超参数集都运行完毕后，我们选择那个表现最准确的系统，通过它运行测试集，并发现它的预测究竟有多准确。
- en: '[Figure 8-7](#figure8-7) shows this whole process graphically.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-7](#figure8-7) 以图形方式展示了整个过程。'
- en: '![f08007](Images/f08007.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![f08007](Images/f08007.png)'
- en: 'Figure 8-7: We use the validation set when we try out lots of different hyperparameter
    sets. Note that we still keep a separate test set, which we use just before deployment.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-7：当我们尝试不同的超参数集时，我们使用验证集。请注意，我们仍然保留一个单独的测试集，只有在部署前才使用它。
- en: When the loop is done, we may be tempted to use the results from the validation
    data as our final evaluation of the system. After all, the classifier didn’t learn
    from that data, since it was only used for testing. It may seem that we can save
    ourselves the trouble of making a separate test set and then run it through the
    system to get a performance estimate.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当循环结束时，我们可能会倾向于使用验证数据的结果作为系统的最终评估。毕竟，分类器并没有从那些数据中学习，因为它只是用来测试的。看起来我们可以省去制作一个单独的测试集的麻烦，然后将它运行在系统上，以获得性能估计。
- en: But that would be working with leaked data, which would distort our conclusions.
    The source of this leakage is a bit sneaky and subtle, like many data contamination
    issues. The problem is that although the classifier didn’t learn from the validation
    data, our whole training and evaluation system did, because it used that data
    to pick the best hyperparameters for the classifier. In other words, even though
    the classifier didn’t explicitly learn from the validation data, that data influenced
    our choice of classifier. We chose a classifier that did best on the validation
    data, so we *already know* that it’s going to do a good job with it. In other
    words, our knowledge of the classifier’s performance on the validation data “leaked”
    into our selection process.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 但那样做相当于在使用泄漏数据，这会扭曲我们的结论。数据泄漏的来源有些狡猾和微妙，就像许多数据污染问题一样。问题在于，尽管分类器没有从验证数据中学习，我们的整个训练和评估系统却学到了，因为它用了这些数据来选择最佳的超参数。换句话说，即使分类器没有明确从验证数据中学习，这些数据也影响了我们选择分类器的过程。我们选择了在验证数据上表现最好的分类器，因此我们*已经知道*它会在验证数据上表现得很好。换句话说，我们对分类器在验证数据上表现的了解“泄漏”到了我们的选择过程中。
- en: 'If this seems subtle or tricky, it is. This sort of thing is easy to overlook
    or miss, which is why we have to be vigilant about data contamination. Otherwise
    we risk thinking our system is better than it really is, and thus we deploy a
    system that isn’t good enough for our intended use. To get a good estimate for
    how our system performs on brand-new data that it has never seen before, there’s
    no shortcut: we need to test it on brand-new data that it has never seen before.
    That’s why we always save the test set for the very end.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这看起来有点微妙或棘手，那是因为确实如此。这类问题很容易被忽视或漏掉，这也是为什么我们必须警惕数据污染。否则，我们就有可能错误地认为系统比实际情况更好，从而部署一个不适合预期用途的系统。要想准确估计我们的系统在从未见过的新数据上的表现，没有捷径可走：我们需要在从未见过的新数据上进行测试。这就是为什么我们总是将测试集留到最后使用的原因。
- en: Cross-Validation
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证
- en: 'In the last section, we took almost half our training data and set it aside
    for validation and testing. That’s fine when we have lots and lots of data. But
    what if our sample set is small and we can’t get more data? Maybe we’re working
    with photos of Pluto and its moons taken by the New Horizons spacecraft during
    its 2015 flyby, and we want to build a classifier we can install on future spacecraft
    to identify what kind of terrain they’re looking at. Our dataset is limited and
    it’s not going to get bigger: there are no new close-up photos of Pluto coming
    anytime soon. Every image that we have is precious, and we want to learn all we
    can from every photo we have. Setting some images aside just to determine how
    good our classifier is would be a huge price to pay.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们将几乎一半的训练数据预留出来用于验证和测试。这对于数据量非常大的时候是没问题的。但如果我们的样本集很小，而且我们无法获得更多数据呢？也许我们在处理的是新地平线号探测器2015年飞掠冥王星时拍摄的冥王星及其卫星的照片，我们希望构建一个分类器，可以安装在未来的航天器上，以识别它们所观察的地形。我们的数据集是有限的，并且不会变大：近期不会有新的冥王星近距离照片出现。我们所拥有的每一张图片都是宝贵的，我们希望从每张照片中学到尽可能多的东西。如果将一些图片预留出来，仅仅为了确定我们的分类器效果，那将是一个巨大的代价。
- en: If we’re willing to accept an estimate of the system’s performance, rather than
    a reliable measure of it, then we don’t have to set aside a test set. We can indeed
    train on every piece of input data and still predict our performance on new data.
    The catch is that we’re only going to get back an estimate of the system’s accuracy,
    so it won’t be as reliable a measure as we’d get by using a real test set, but
    when samples are precious, that tradeoff can be worth it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意接受系统性能的估计，而不是可靠的衡量标准，那么我们就不必专门预留一个测试集。我们实际上可以在每一份输入数据上进行训练，并仍然预测我们在新数据上的表现。问题是，我们只会得到系统准确性的估计，因此它不会像使用真实测试集那样可靠，但当样本非常珍贵时，这种权衡可能是值得的。
- en: The technique that does this job is called *cross-validation* or *rotation validation*.
    There are different types of cross-validation algorithms, but they all share the
    same basic structure (Schneider 1997). We’re going to look at a version that doesn’t
    require us to create a dedicated test set.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这个任务的技术叫做*交叉验证*或*旋转验证*。有多种类型的交叉验证算法，但它们都有相同的基本结构（Schneider 1997）。我们将查看一种不需要我们创建专门测试集的版本。
- en: The core idea is that we can run a loop that repeatedly trains the same system
    from scratch and then tests it. Each time through we’ll split the entire input
    data into a one-time training set and a one-time validation set. The key thing
    is that we’ll construct these sets differently each time through the loop. This
    lets us use all of our data for training (though, as we’ll see, not all at the
    same time).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 核心思想是我们可以运行一个循环，反复从头训练相同的系统并进行测试。每次循环时，我们将整个输入数据分成一次性的训练集和一次性的验证集。关键在于每次循环时，我们都会以不同的方式构建这些数据集。这使我们能够使用所有的数据进行训练（尽管，正如我们将看到的，并不是同时使用所有数据）。
- en: We begin by building a fresh instance of our classifier. We split the input
    data into a temporary training set and temporary validation set. We train our
    system on the temporary training set and evaluate it with the temporary test set.
    This gives us a score for the classifier’s performance. Now we go through the
    loop again, but this time, we split the training data into different temporary
    training and test sets. When we’ve done this for every iteration through the loop,
    the average of all the scores is our estimate for the overall performance of our
    classifier.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先构建一个全新的分类器实例。我们将输入数据分成一个临时的训练集和临时的验证集。我们在临时训练集上训练系统，并使用临时测试集进行评估。这将给我们一个分类器性能的得分。接下来，我们再次进行循环，但这一次，我们将训练数据分成不同的临时训练集和测试集。当我们完成每次循环的迭代后，所有得分的平均值将作为我们对分类器整体性能的估计。
- en: A visual summary of cross-validation is shown in [Figure 8-8](#figure8-8).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证的视觉摘要显示在[图 8-8](#figure8-8)中。
- en: '![f08008](Images/f08008.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![f08008](Images/f08008.png)'
- en: 'Figure 8-8: Using cross-validation to evaluate our system’s performance'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-8：使用交叉验证来评估我们系统的性能
- en: By using cross-validation, we get to train with all of our training data (though
    not all of it on every pass through the loop), yet we still get an objective measurement
    of the system’s quality from a held-out test set. This algorithm doesn’t have
    data leakage issues because each time through the loop, we create a new classifier,
    and the temporary test set for that classifier contains data that is brand-new
    and unseen with respect to *that specific* classifier, so it’s fair to use it
    to evaluate that classifier’s performance. The penalty for this technique is that
    our final estimate of the system’s accuracy is not as reliable as what we’d get
    from a held-out test set.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用交叉验证，我们可以用所有的训练数据来训练（尽管每次循环中并不是所有数据都会参与训练），同时我们依然能够从留出的测试集获得系统质量的客观测量。这个算法没有数据泄漏的问题，因为每次通过循环时，我们都会创建一个新的分类器，而该分类器的临时测试集包含的是与*该特定*分类器全新且未见过的数据，因此可以公平地使用它来评估该分类器的性能。这种技术的缺点是，我们对系统准确性的最终估计不像从留出的测试集中得到的估计那么可靠。
- en: A variety of different algorithms are available for constructing the temporary
    training and validation sets. Let’s look at a popular approach.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种不同的算法可以用来构建临时的训练集和验证集。让我们来看一下一个流行的方法。
- en: k-Fold Cross-Validation
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-折交叉验证
- en: Perhaps the most popular way to build the temporary datasets for cross-validation
    is called *k-fold cross-validation*. The letter *k* here is not the first letter
    of a word. Instead, it stands for an integer (for example, we might run “2-fold
    cross-validation” or “5-fold cross-validation”). Typically, the value of *k* is
    the number of times we want to go through the loop of [Figure 8-8](#figure8-8).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 也许最流行的构建交叉验证临时数据集的方法叫做*k-fold交叉验证*。这里的字母*k*不是某个单词的首字母，而是表示一个整数（例如，我们可能会运行“2-fold交叉验证”或“5-fold交叉验证”）。通常，*k*的值是我们希望在[图
    8-8](#figure8-8)中循环的次数。
- en: The algorithm starts before the cross-validation loop begins. We take our training
    data and split it up into a series of equal-sized groups. Every sample is placed
    into exactly one group, and all groups are the same size (well, we allow one smaller
    group at the end if we can’t split the input into equal-sized pieces).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法在交叉验证循环开始之前就已经启动。我们将训练数据分割成一系列等大小的组。每个样本仅被放入一个组中，所有组的大小相同（当然，如果不能将输入数据分成完全相等的部分，我们允许最后一个组稍微小一点）。
- en: It would have been great if these groups were each called something like “group,”
    or “equal-sized piece,” but the word that’s come to describe this idea is *fold.*
    This word is used here in an unusual sense to mean the section of a page *between*
    creases (or ends). To picture this, imagine writing all the samples in the training
    set on a long piece of paper and then folding that up into a fixed number of equal
    pieces. Each time we bend the paper we’re making a crease, and the material between
    the creases is called a fold. [Figure 8-9](#figure8-9) shows the idea.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些组分别被称为“组”或“等大小的块”，那会很好，但描述这个概念的词是*fold*。这个词在这里的使用是为了表示页面上*折痕*（或末端）之间的部分。为了更好地理解这个概念，可以想象将训练集中的所有样本写在一张长纸上，然后将其折叠成固定数量的等份。每次我们折叠纸张时，就形成一个折痕，而折痕之间的部分称为fold。[图
    8-9](#figure8-9)展示了这个概念。
- en: '![f08009](Images/f08009.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![f08009](Images/f08009.png)'
- en: 'Figure 8-9: Creating the folds for *k*-fold cross-validation. Here we have
    four creases and five folds.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-9：为*k*-fold交叉验证创建folds。这里有四个折痕和五个fold。
- en: Let’s build equal-sized folds from our training data. We can flatten out [Figure
    8-9](#figure8-9) to create the more typical picture of five folds, shown in [Figure
    8-10](#figure8-10).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从训练数据中构建等大小的fold。我们可以将[图 8-9](#figure8-9)展开，创建出[图 8-10](#figure8-10)中更典型的五个fold的图像。
- en: '![f08010](Images/f08010.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![f08010](Images/f08010.png)'
- en: 'Figure 8-10: Splitting our training set into five equally sized folds, named
    Fold 1 through Fold 5'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-10：将我们的训练集拆分为五个大小相等的fold，分别命名为Fold 1到Fold 5
- en: Let’s use these five folds to see how the loop proceeds. The first time through
    the loop, we treat the samples in folds 2 through 5 as our temporary training
    set, and the samples in fold 1 as our temporary test set. That is, we train the
    classifier with the samples in folds 2 through 5 and then evaluate it with the
    samples in fold 1.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这五个fold来看看循环是如何进行的。在第一次循环中，我们将fold 2到fold 5中的样本作为临时训练集，而将fold 1中的样本作为临时测试集。也就是说，我们使用fold
    2到fold 5中的样本来训练分类器，然后用fold 1中的样本来评估它。
- en: The next time through the loop, starting with a fresh classifier initialized
    with random numbers, we use the samples in folds 1, 3, 4, and 5 for our temporary
    training set, and the samples in fold 2 for our temporary test set. We train and
    test as usual with these two sets, and continue with the remaining folds. [Figure
    8-11](#figure8-11) shows the idea visually.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 下一次循环时，从一个初始化为随机数的新分类器开始，我们使用第1、3、4和5折的样本作为临时训练集，使用第2折的样本作为临时测试集。我们照常用这两组数据进行训练和测试，并继续使用其余的折叠。[图8-11](#figure8-11)直观地展示了这一过程。
- en: '![f08011](Images/f08011.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![f08011](Images/f08011.png)'
- en: 'Figure 8-11: In each pass through the loop, we choose one fold for testing
    (in blue) and use the others (in red) for training. If we go through the loop
    more than five times, we repeat the pattern.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图8-11：在每次循环中，我们选择一个折叠进行测试（蓝色），其余的折叠（红色）用于训练。如果我们循环超过五次，我们将重复这一模式。
- en: We can choose to repeat the loop as many times as we like, just repeating the
    cycle of fold selections (or mixing up the data so the sets always have different
    contents).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择重复循环任意多次，只需重复折叠选择的周期（或者混合数据，使得每次集合的内容不同）。
- en: In an optional final step, we can train a fresh classifier with all of our data.
    This means we can’t get an estimate for its performance. But if we watch the training
    carefully, and look out for overfitting (discussed in the next chapter), we can
    usually assume that the system that was trained on all the data is at least as
    good as the worst performance we obtained from cross-validation (and we hope it
    will be at least a little bit better).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个可选的最终步骤中，我们可以使用所有数据来训练一个新的分类器。这意味着我们无法得到其性能的估计。但如果我们仔细观察训练过程，并留意过拟合（将在下一章讨论），我们通常可以假设，使用所有数据训练的系统至少和我们从交叉验证中得到的最差表现一样好（并且我们希望它会稍微好一点）。
- en: This makes cross-validation a great option when our data is limited. We do have
    to repeat the train-test cycle many times, and our final performance measure is
    only an estimate, both of which are downsides, but we gain the ability to train
    with all of our data, squeezing every little bit of information out of our input
    set and using it to make our classifier better.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得交叉验证在数据有限时成为一个很好的选择。我们确实需要重复多次训练-测试周期，并且我们的最终性能度量只是一个估计，这两者都是缺点，但我们可以利用所有数据进行训练，最大化地提取输入集中的每一点信息，并将其用来提高分类器的表现。
- en: We discussed *k*-fold cross-validation with a classifier, but the algorithm
    is broadly applicable to almost any kind of learner.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了带分类器的*k*折交叉验证，但该算法可以广泛应用于几乎任何类型的学习器。
- en: Summary
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'This chapter was all about training a deep learning system and deciding if
    it’s working well enough to be deployed. We focused on a classifier, but the general
    thinking holds for any such system. We saw that we split the data into two pieces:
    a training set and a test set. We learned about the problems of overfitting and
    data leakage, and we also saw how we can use a validation set to get a rough idea
    of how well the system is learning after every epoch. Finally, we looked at cross
    validation, a technique typically used with small dataset, to estimate a system’s
    performance.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要讲解了训练深度学习系统，并判断它是否足够好以进行部署。我们聚焦于分类器，但这种思路对任何此类系统都适用。我们看到我们将数据分为两部分：训练集和测试集。我们了解了过拟合和数据泄露的问题，并且还看到了如何使用验证集来大致了解系统在每个训练周期后的学习效果。最后，我们介绍了交叉验证，这是一种通常用于小数据集的技术，用于估计系统的性能。
- en: In the next chapter, we’ll take a closer look at overfitting and underfitting.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将更详细地探讨过拟合和欠拟合问题。
