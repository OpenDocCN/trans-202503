<html><head></head><body><div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="675" id="Page_675"/>23</span><br/>
<span class="ChapterTitle">Creative Applications</span></h1>
</header>
<figure class="opener">
<img src="Images/chapterart.png" alt="" width="206" height="206"/>
</figure>
<p class="ChapterIntro">We’ve reached the end of the book. Before we go, let’s relax and have some fun. In this chapter we look at some creative ways to use neural networks to create art. We explore two image-based applications: <em>deep dreaming,</em> which turns images into wild, psychedelic art, and <em>neural style transfer,</em> which allows us to transform photographs into what appear to be paintings in the styles of different artists. At the very end, we take a quick dip into <em>text generation </em>and use deep learning to generate even more of this book.</p>
<h2 id="h1-500723c23-0001">Deep Dreaming</h2>
<p class="BodyFirst">In deep dreaming, we use some ideas that were invented to help us visualize filters in convolutional networks, but we use them to make art. The result is that we modify images to excite different filters, causing those images to explode in psychedelic patterns.</p>
<h3 id="h2-500723c23-0001"><span epub:type="pagebreak" title="676" id="Page_676"/>Stimulating Filters</h3>
<p class="BodyFirst">In Chapter 17, we made images, or visualizations, of the filters in a convolutional neural network. Both deep dreaming and style transfer build on that visualization technique, so let’s look at it a little more closely. We can make our discussion specific by again using VGG16 as we did in Chapter 17 (Simonyan and Zisserman 2020), though we could substitute just about any CNN image classifier. Our only interest here is in the convolution stages, so although we will use the whole network as described in Chapter 17, the drawings in this chapter show just the convolution and pooling layers, as shown in <a href="#figure23-1" id="figureanchor23-1">Figure 23-1</a>.</p>
<figure>
<img src="Images/f23001.png" alt="f23001" width="844" height="96"/>
<figcaption><p><a id="figure23-1">Figure 23-1</a>: A simplified diagram of VGG16, showing just the convolution and pooling layers</p></figcaption>
</figure>
<p>We’re leaving out the last few stages of VGG16 because their job is to help the network predict the proper class of the output. In this application, we don’t care about the network’s output. Our only interest here is running an image through the network so that the filters in the convolution layers will evaluate their inputs. Our goal is to modify a starting image so it excites some chosen layers as much as possible. For example, if a few pixels are darker in the middle, that may cause the filter that looks for eyes to respond a little bit. Our goal is to modify those pixels so that they excite that filter more and more, which means that they look more and more like eyes.</p>
<p>We don’t need any new tools to do this. All we have to do is pick which filter outputs we want to maximize. We can pick just one filter, or several filters from different parts of the network. Our choice of which filters to use is entirely personal and artistic. Typically, we hunt around, trying out different filters, until we see our input images changing in a way that we like.</p>
<p>Let’s see the steps. Suppose we pick one filter on each of three different layers, as in <a href="#figure23-2" id="figureanchor23-2">Figure 23-2</a>. We start things off by providing the network with an image, which it processes. </p>
<p>We take the feature map from the first filter we’ve chosen, add up all of its values, and determine how much influence this sum will have by multiplying it by a weight that we choose. Though we’re using the word <em>weight,</em> this isn’t a weight inside the network. It’s just a value we use to control the impact of each filter in the deep dream process. We sum up and weight the other filters we’ve chosen. Now we add up those results. This gives us a single number, telling us how strongly our chosen filters are responding to the input image, weighted by how much influence we want to give to each layer’s filters. We call this number the <em>multifilter loss,</em> or the <em>multilayer loss.</em></p>
<span epub:type="pagebreak" title="677" id="Page_677"/><figure>
<img src="Images/f23002.png" alt="f23002" width="833" height="506"/>
<figcaption><p><a id="figure23-2">Figure 23-2</a>: The deep dream algorithm uses a loss built from multiple layers </p></figcaption>
</figure>
<p>Now comes the tricky part: the multifilter loss becomes the network’s “error.” In previous chapters, we used the error to drive backprop, which computed gradients for all of the network’s weights, starting at the final layer and working our way backward to the first. Then we used those gradients to modify the network’s weights to minimize the error. But that’s not what we do here. Instead, we want the error (the filter responses) to be as big as we can make them. And we don’t want to do this by changing the network, since we’re not training it. We’re going to <em>freeze</em> the network, so its weights can’t change. Instead, we’re going to modify the colors of the pixels themselves.</p>
<p>So, starting with this error, we use backprop as usual to find the gradients on all the weights in the network, but when we reach the first hidden layer, we take one more step backward to the input layer, which holds the pixels themselves. Then we use backprop as usual to find the gradients for the pixels. After all, changing the input pixels causes the values computed by the network to change, and thus causes a change to our error. Just as we can use backprop to learn how to change the network’s weights to reduce the error in a typical training setup, we can use the same backprop algorithm to find out how to change the pixel values to increase this error.</p>
<p>Now, as usual, we apply the optimization step. Since we’re not training and the network is frozen, we don’t touch the network weights. But we do use the gradients on the pixels to modify their color values so that they <em>maximize </em>the error, or more strongly stimulate our selected filters.</p>
<p>The result is that the pixel colors change just a little, in such a way that filters respond even more, creating a bigger error, which we use to find new <span epub:type="pagebreak" title="678" id="Page_678"/>gradients on the pixels, causing them to excite the filters even more, and around it goes, with the picture changing more and more each time we repeat the loop.</p>
<p>Since this is an artistic process, we usually watch the output after every update (or every few updates), and stop it when we like what we’re seeing.</p>
<h3 id="h2-500723c23-0002">Running Deep Dreaming</h3>
<p class="BodyFirst">Let’s put this algorithm to work, using the frog in <a href="#figure23-3" id="figureanchor23-3">Figure 23-3</a> as our starting point.</p>
<figure>
<img src="Images/f23003.png" alt="f23003" width="350" height="184"/>
<figcaption><p><a id="figure23-3">Figure 23-3</a>: A calm and thoughtful frog</p></figcaption>
</figure>
<p><a href="#figure23-4" id="figureanchor23-4">Figure 23-4</a> shows some “dreams” from our frog image, using some filters (and weights on them) that we chose by trial and error. </p>
<figure>
<img src="Images/f23004.png" alt="f23004" width="675" height="558"/>
<figcaption><p><a id="figure23-4">Figure 23-4</a>: Some deep dream results from the starting frog image (in the upper-left corner)</p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="679" id="Page_679"/>We’re seeing lots of eyes in <a href="#figure23-4">Figure 23-4</a> because some of our selected filters responded to eyes. If we had selected filters that responded to, say, horses and shoes, then we would expect to see lots of horses and shoes in our images.</p>
<p><a href="#figure23-5" id="figureanchor23-5">Figure 23-5</a> shows the results starting from an image of a dog. The changes to the image are mostly of a finer texture because the dog image is about 1,000 pixels on each side, more than four times the size of the images that the network trained on. The image in the lower-right used a version of the dog image that was scaled to the same size as the network’s training data, 224 by 224. </p>
<figure>
<img src="Images/f23005.png" alt="f23005" width="694" height="567"/>
<figcaption><p><a id="figure23-5">Figure 23-5</a>: Deep dreaming about a dog</p></figcaption>
</figure>
<p>The original name for this technique was <em>inceptionism </em>(Mordvintsev, Olah, and Tyka 2015) in honor of the movie <em>Inception,</em> but it has come to be more frequently known as <em>deep dreaming</em>. The name is a poetic suggestion that the network is “dreaming” about the original image, and the image we get back shows us where the network’s dream went. Deep dreaming has become popular not only because of the wild images it creates, but because it’s not hard to implement using modern deep learning libraries (Bonaccorso 2020). </p>
<p>Many variations on this basic algorithm have been explored (Tyka 2015), but they only scratch the surface. We can imagine schemes to automatically determine the weights on the layers or even apply weights to the individual filters on each layer. We can “mask” the activation maps before <span epub:type="pagebreak" title="680" id="Page_680"/>we add them up so that some areas (like the background) are ignored, or we can mask the updates to the pixels so that some pixels in the original image are not changed at all in response to one set of layer outputs, but are instead allowed to change a lot in response to some other set of layer outputs. We can even apply different combinations of layers and weights to different regions of the input image. The deep dreaming approach to making art has lots of room left for new discoveries. </p>
<p>There’s no “right” or “best” way to do deep dreaming. It’s a creative exercise in which we follow our aesthetics, hunches, or wild guesses to hunt for images that appeal to us. It can be hard to predict what’s going to come out from any particular combination of network, layers, and weights, so the process rewards patience and a lot of experimenting. </p>
<h2 id="h1-500723c23-0002">Neural Style Transfer</h2>
<p class="BodyFirst">We can use a variation on the deep dreaming technique to do something remarkable: transfer one artist’s style onto another image. This process is called <em>neural style transfer</em>. </p>
<h3 id="h2-500723c23-0003">Representing Style</h3>
<p class="BodyFirst">Cultures often celebrate the idiosyncratic visual style of artists. Let’s focus on paintings. What characterizes the style of a painting? That’s a big question, because “style” can include someone’s world view, which influences choices as diverse as their subject matter, composition, materials, and tools. Let’s focus strictly on visual appearance. Even narrowed down this way, it’s hard to precisely identify what “style” means for a painting, but we might say that it refers to how colors and shapes are used to create forms, and the types and distributions of those forms across the canvas (Art Story Foundation 2020; Wikipedia 2020). </p>
<p>Rather than try to refine this description, let’s see if we can find something that seems like it’s in the ballpark, while also being something we can formalize in terms of the layers and filters of a deep convolutional network. </p>
<p>Our goal in this section is to take a picture we want to modify, called the <em>base image</em>, and a second picture whose style we want to match, called the <em>style reference</em>. For example, our frog could be our base image, and any painting could be the style reference. We want to use these to create a new image, called the <em>generated image</em>, which has the content of the base image expressed in the style of the style reference. </p>
<p>To get started, we will make an assertion that comes out of nowhere. We say that we can characterize the style of an image (such as a painting) by looking at the layer activations it produces and finding pairs of layers that activate in roughly the same way. This idea comes from a seminal paper published in 2015 (Gatys, Ecker, and Bethge 2015). Without getting into the details, the process begins by running the style reference through a deep convolutional network. As with deep dreaming, we ignore its output, and instead focus on just the convolution filters. </p>
<p><span epub:type="pagebreak" title="681" id="Page_681"/>All of the activation maps in a given layer have the same size, so we can easily compare them to one another. Let’s start with the first activation map in a layer (that is, the first filter’s output). We can compare that to the activation map produced by the second filter and produce a score for the pair. If the two maps are very similar (that is, the filters are firing in the same places), we assign the pair a high score, and if the maps are very different, we give that pair a low score. We then compare the first map to the third map and compute their score, then compare the first and the fourth map, compute their score, and so on. Then we can start with the second map and compare it to every other map in the layer. We can organize the results in a grid, with as many cells on each side as there are filters in the layer. The value in each cell of the grid tells us the score for that pair of layers. This grid is called a <em>Gram matrix.</em> Let’s make one such Gram matrix for every layer.</p>
<p>Now we can restate our notion of style more formally: the style of an image is represented by the Gram matrices for all the layers. That is, each style produces its own particular form of Gram matrices.</p>
<p>Let’s see if this claim is true. <a href="#figure23-6" id="figureanchor23-6">Figure 23-6</a> shows a famous self-portrait by Pablo Picasso from 1907. There’s a ton of style here, such as big blocks of color and thick dark lines. Let’s run this through VGG16 and save the Gram matrix at each layer. We’ll call those the <em>style matrices</em>, and save them as the representation of the style for this image.</p>
<figure>
<img src="Images/f23006.png" alt="f23006" width="222" height="285"/>
<figcaption><p><a id="figure23-6">Figure 23-6</a>: A 1907 self-portrait by Pablo Picasso </p></figcaption>
</figure>
<p>If the Gram matrices represent style, then we can use them to modify a starting image of random noise. We run the noisy input image through the network and compute its Gram matrices, which we call the <em>image matrices.</em> If the style matrices really do somehow represent the style of the Picasso image, then if we can change the colors of the pixels in the noisy image so that eventually the image matrices come close to the style matrices, the noisy image should take on the style of the painting.</p>
<p>Let’s do just that. We’ll run the noise through the network, compute the image matrix at each layer, and compare that to the style matrix we saved for that layer. We’ll add up the differences between these two matrices so the more different they are, the bigger the result. Then we’ll add <span epub:type="pagebreak" title="682" id="Page_682"/>together these differences for all the layers, and this is the error for our network. As with deep dreaming, we use this error to compute the gradients for the entire network, including the pixels at the start, but we only modify the colors of the pixels. Unlike deep dreaming, our goal now is to minimize the error, and thereby change the pixels so that their colors produce Gram matrices that are like those of the style reference.</p>
<p><a href="#figure23-7" id="figureanchor23-7">Figure 23-7</a> shows the result of this process. For this visualization, we computed each layer’s error as the sum of the differences of all the matrices in all layers up to and including that layer. </p>
<figure>
<img src="Images/f23007.png" alt="f23007" width="668" height="570"/>
<figcaption><p><a id="figure23-7">Figure 23-7</a>: The result of getting noise to match the Gram matrices in VGG16</p></figcaption>
</figure>
<p>This is remarkable. By the time we get to the three convolution layers in Block 3, we’re generating abstracts that are very similar to our original style reference in <a href="#figure23-6">Figure 23-6</a>. The splotches of color show similar gradual changes in color. There are dark lines between some regions of different colors, and we can even see brushstroke textures. </p>
<p>The Gram matrices have indeed captured the style of Picasso’s painting. But why? The anticlimactic answer is that nobody really knows (Li et al. 2017). We have different ways to write down the mathematics of what the Gram matrices are measuring, but that doesn’t help us understand why this technique captures this elusive idea we call style. Neither the original paper on neural style transfer (Gatys, Ecker, and Bethge 2015), nor a somewhat more detailed follow-up (Gatys, Ecker, and Bethge 2016) explains how the authors hit on this idea or why it works so well. </p>
<h3 id="h2-500723c23-0004"><span epub:type="pagebreak" title="683" id="Page_683"/>Representing Content</h3>
<p class="BodyFirst">In deep dreaming, we started with an image and manipulated it by changing its pixels. If we try the same thing with neural transfer and start with an image rather than noise, the image quickly gets lost. The effect of minimizing the differences between the Gram matrices causes big changes to the input image, moving it toward the style we want, but losing the content of the image in the process.</p>
<p>A solution to this problem is to still start with noise, because it works so well (as shown by <a href="#figure23-7">Figure 23-7</a>), but retain the essence of the original image by adding a second error term. In addition to imposing a <em>style loss</em> that punishes the input for being a poor match to the style reference (as measured by the difference in the Gram matrices), we also impose a <em>content loss</em> that punishes the input for being too much unlike the base image (the picture we want to stylize). By starting with noise and adding together these two error terms (usually with different emphasis), we cause the pixels in the noise to change so that they simultaneously more closely match the colors of the picture we want to modify and the style in which we want it to be shown.</p>
<p>Gathering the content loss is easy. We take our base image, like our frog in <a href="#figure23-3">Figure 23-3</a>, and run it through the network. Then we save the activation map of every filter. From then on, any time we feed a new image to the network, the content loss is just the difference between the filter responses for that input and the responses we got from our base image. After all, if all the filters respond to an input the same way as the starting image, then the input is the starting image (or something very close to it).</p>
<h3 id="h2-500723c23-0005">Style and Content Together</h3>
<p class="BodyFirst">To recap, we feed our style reference through the network and save the Gram matrix for every pair of filters after each layer. Next, we find a base picture we’d like to stylize, run that through the network, and save the feature maps produced by every filter. </p>
<p>Using this saved data, we can create a stylized version of our picture. We start with noise and feed it to the network. The block diagram of the whole process is shown in <a href="#figure23-8" id="figureanchor23-8">Figure 23-8</a>. </p>
<p>Let’s start with the content loss. In the light blue rounded-corner rectangle at the far left, we gather up the feature maps from the filters on the first convolution layer and compute the difference between these maps and the ones we saved from the base image (such as the frog). We do the same with the feature maps from the second convolution layer. We can do this for all the layers, but for this figure and the examples that follow we stopped after two (this is another personal choice, guided by experimentation). We add together all of these differences, or content losses, and scale their sum by some value that lets us control how much the content of the picture should influence the changes we ultimately make to the colors of the input image.</p>
<span epub:type="pagebreak" title="684" id="Page_684"/><figure>
<img src="Images/f23008.png" alt="f23008" width="832" height="779"/>
<figcaption><p><a id="figure23-8">Figure 23-8</a>: A block diagram of neural style transfer</p></figcaption>
</figure>
<p>Now we address the style. For each layer, in the light yellow rounded-corner rectangles, we compute the Gram matrix that tells us how much each filter’s output corresponds with every other filter’s output. We then compare those matrices with the style matrices we saved earlier. We add up all of these differences to get the style loss and scale them by some value that let us control how much influence the style has when we modify the pixel colors.</p>
<p>The sum of the content and style losses is our error. As with deep dreaming, we compute the gradients throughout the network and all the way back to the pixels. And again, we leave the weights in the network untouched. Unlike deep dreaming, we modify the values of the pixels to <em>minimize</em> this total error, because we want the input to match the content <span epub:type="pagebreak" title="685" id="Page_685"/>and style information we previously saved. The result is that the original noise slowly changes so that it is simultaneously more like the original image and also has the filter relationships of the style.</p>
<p>As with deep dreaming, implementing neural style transfer is straightforward with modern deep learning libraries (Chollet 2017; Majumdar 2020). </p>
<h3 id="h2-500723c23-0006">Running Style Transfer</h3>
<p class="BodyFirst">Let’s see how well this works in practice. We again used VGG16 as our network and followed the process summarized in <a href="#figure23-8">Figure 23-8</a>.</p>
<p><a href="#figure23-9" id="figureanchor23-9">Figure 23-9</a> shows nine images, each with a distinctive style. These are our style references.</p>
<figure>
<img src="Images/f23009.png" alt="f23009" width="844" height="676"/>
<figcaption><p><a id="figure23-9">Figure 23-9</a>: Nine images with different styles, which serve as our style references. From left to right and top down, they are <em>Starry Night</em>, by Vincent van Gogh, <em>The Shipwreck of the Minotaur</em>, by J. M. W. Turner, <em>The Scream</em>, by Edvard Munch, <em>Seated Female Nude</em>, by Pablo Picasso, <em>Self-Portrait 1907</em>, by Pablo Picasso, <em>Nighthawks</em>, by Edward Hopper, <em>Sergeant Croce</em>, by the author, <em>Water Lilies, Yellow and Lilac</em>, by Claude Monet, and <em>Composition VII</em> by Wassily Kandinsky.  </p></figcaption>
</figure>
<p><span epub:type="pagebreak" title="686" id="Page_686"/>Let’s apply these styles to our old friend the frog. <a href="#figure23-10" id="figureanchor23-10">Figure 23-10</a> shows the results. </p>
<figure>
<img src="Images/f23010.png" alt="f23010" width="844" height="641"/>
<figcaption><p><a id="figure23-10">Figure 23-10</a>: Applying the nine styles in <a href="#figure23-9">Figure 23-9</a> to a photograph of a frog (top) </p></figcaption>
</figure>
<p>Wow. That worked great. These images bear close examination, because they have a lot of detail. At a first glance, we can see that the color palette of each style reference has been transferred to the frog photo. But notice the textures and edges, and how blocks of color are shaped. These images are not just color shifted frogs, or some kind of overlay or blend of two images. Instead, these are high-quality, detailed images of the frog in the different styles. To see this more clearly, <a href="#figure23-11" id="figureanchor23-11">Figure 23-11</a> shows the same zoomed-in region from each frog. </p>
<p>These images are significantly different, and all match the style they’re based on.</p>
<p>Let’s see another example. <a href="#figure23-12" id="figureanchor23-12">Figure 23-12</a> shows our styles applied to a photograph of a town.</p>
<span epub:type="pagebreak" title="687" id="Page_687"/><figure>
<img src="Images/f23011.png" alt="f23011" width="844" height="629"/>
<figcaption><p><a id="figure23-11">Figure 23-11</a>: Details for the nine styled frogs of <a href="#figure23-10">Figure 23-10</a> </p></figcaption>
</figure>
<p>These images are all the more remarkable when we remember that every one of them started out as random noise. For each image, we weighted the content loss by 0.025 and the style loss by 1, so the style had 40 times more influence on the changes to the pixels than the content did. In these examples, a little bit of content went a long way. </p>
<p>As Figures 23-10 through 23-12 show, the basic algorithm of neural style transfer produces terrific results. The technique has been extended and modified in many ways to improve the flexibility of the algorithm, the types of results it produces, and the range of control that artists can apply to create the results they want (Jing et al. 2018). It’s even been applied to video and spherical images that completely surround a viewer (Ruder, Dosovitskiy, and Brox 2018). </p>
<p>As with deep dreaming, neural style transfer is a general algorithm that allows for a lot of variation and exploration. There are surely many interesting and beautiful artistic effects waiting to be discovered. </p>
<span epub:type="pagebreak" title="688" id="Page_688"/><figure>
<img src="Images/f23012.png" alt="f23012" width="844" height="817"/>
<figcaption><p><a id="figure23-12">Figure 23-12</a>: Applying our nine styles of <a href="#figure23-9">Figure 23-9</a> to a photograph of a town seen from above (top)</p></figcaption>
</figure>
<h2 id="h1-500723c23-0003">Generating More of This Book</h2>
<p class="BodyFirst">Just for fun, we ran the text of the first edition of this book (except for this section) through an RNN that generates new text word by word, as discussed in Chapter 19. The full text contained about 427,000 words, drawn from a vocabulary of about 10,300 words. To learn this text, we used a network built from two layers of LSTMs, with 128 cells each.</p>
<p>The algorithm develops its output autoregressively, by finding the next most likely word based on the text it has created so far, then the next most likely word, then the next, and so on, until we stop it. Generating text this <span epub:type="pagebreak" title="689" id="Page_689"/>way is like the game of creating messages by repeatedly choosing just one of the three or four words suggested by a cell phone when you are typing a text (Lowensohn 2014). </p>
<p>Here are a few sentences manually selected from the output after 250 iterations. They are included here exactly as generated, including punctuation. </p>
<ul>
<li><span class="CustomCharStyle">The responses of the samples in all the red circles share two numbers, like the bottom of the last step, when their numbers would influence the input with respect to its category. </span></li>
<li><span class="CustomCharStyle">The gradient depends on the loss are little pixels on the wall. </span></li>
<li><span class="CustomCharStyle">Let’s look at the code for different dogs in this syllogism. </span></li>
</ul>
<p>It’s surprising how close these come to making sense! </p>
<p>Whole sentences are fun, but some of the most entertaining and poetic bits came out soon after the start of training, when the system was generating only fragments. Here are some manually selected excerpts after just 10 epochs, again presented exactly as they were produced: </p>
<ul>
<li><span class="CustomCharStyle">Set of of apply, we + the information. </span></li>
<li><span class="CustomCharStyle">Suppose us only parametric. </span></li>
<li><span class="CustomCharStyle">The usually quirk (alpha train had we than that to use them way up).</span></li>
</ul>
<p>These are mostly incoherent, but from these synthetic phrases we can distill a grain of truth: one of the primary goals of this book has definitely been to “<span class="CustomCharStyle">+ the information</span>.”</p>
<p>RNNs are great, but transformer-based generators are even better. We fine-tuned a medium-sized instance of the GPT-2 generator on the current edition of this book (again, except for this section). Here are a few hand-picked fragments of output, selected for their creative range (the second set appear to be figure captions). </p>
<ul>
<li><span class="CustomCharStyle">This is the neural network that’s been hailed as the queen of artificial neurons. It’s no surprise that her name is Christine, but it does speak volumes about the state of the field.</span></li>
<li><span class="CustomCharStyle">We can chain together several of these versions into a single tensor of a classifier that is essentially a jack-in-the-box. </span></li>
<li><span class="CustomCharStyle">Let’s use a small utility to take a shortcut that will let us make word predictions on the fly, even if we aren’t the right person online or offline.</span></li>
<li><span class="CustomCharStyle">In this view, a 1 in this range is a perfect integer, while a 0 in this range is a hyper-realized string of numbers. The approach we adopted in Chapter 6 is to treat all 0’s as incomplete, and all 1’s as incomplete, since we still have some information about the system we’re evaluating.</span></li>
<li><span class="CustomCharStyle"><a href="c10.xhtml#figure10-7">Figure 10-7</a>: A grid of negative images that don’t have labels.</span></li>
<li><span class="CustomCharStyle"><a href="c10.xhtml#figure10-10">Figure 10-10</a>: A deep learning system learns how to create and remove license plates from a dataset.</span></li>
</ul>
<h2 id="h1-500723c23-0004"><span epub:type="pagebreak" title="690" id="Page_690"/>Summary</h2>
<p class="BodyFirst">We started this chapter by looking at deep dreaming, a method for manipulating an image to stimulate chosen filters in a network and create wild, psychedelic images. Then we looked at neural style transfer. Using this technique, we slowly change an input of random noise to become simultaneously more like an input image and like a style reference, such as works by various painters. Finally, we used a little RNN and a transformer to produce new text from the manuscript of this book. It’s fun to make new text that seems familiar or reasonable on first glance! </p>
<h2 id="h1-500723c23-0005">Final Thoughts</h2>
<p class="BodyFirst">This book has covered only the basic ideas of deep learning. The field is developing at a startling pace. Every year new breakthroughs seem to defy all expectations of what computers can recognize, analyze, predict, and synthesize.</p>
<p>Just keeping up with new work can be a full-time job. One way to stay up on new developments is to watch a website called the arXiv (pronounced “archive”) server at <a href="https://arxiv.org/" class="LinkURL">https://arxiv.org/</a>, or more specifically, the machine learning section at <a href="https://arxiv.org/list/cs.LG/recent" class="LinkURL">https://arxiv.org/list/cs.LG/recent</a><em>.</em> This site hosts preprints of new papers before they appear in journals and conferences. But even watching arXiv can be overwhelming, so many people use the arXiv Sanity Preserver at <a href="http://www.arxiv-sanity.com" class="LinkURL">http://www.arxiv-sanity.com</a> and the Semantic Sanity project at <a href="https://s2-sanity.apps.allenai.org/cold-start" class="LinkURL">https://s2-sanity.apps.allenai.org/cold-start</a>. Both sites help filter the repository for just those papers involving specific keywords and ideas.</p>
<p>In this book we’ve focused on the technical background of deep learning. It’s important to keep in mind that when we use these systems in ways that can affect people, we need to take into account much more than just algorithms (O’Neil 2016). Because of their efficiency, deep learning systems are being widely and rapidly deployed, often with little oversight or consideration of their impact on the societies they affect.</p>
<p>Deep learning systems are being used today to influence or determine job offers, school admissions, jail sentences, personal and business loans, and even the interpretation of medical tests. Deep learning systems control what people see in their news and social media feeds, choosing items not to build a healthy and well-informed society, but to increase the profits of the organizations delivering those feeds (Orlowski 2020). Deep learning systems in “smart speakers” and “smart displays” (which also contain microphones and cameras) listen and watch people in their homes without pause, sometimes sending their captured data up to a remote server for analysis. Cultures used to fear such constant surveillance, but now people pay for these devices and willingly put them in their previously private spaces, such as their homes and bedrooms. Deep learning is being used to single out potentially troublesome children in schools, evaluate the honesty of people answering questions at border crossings, and identify individuals at protests and other gatherings by recognizing their faces. Mistakes made by these <span epub:type="pagebreak" title="691" id="Page_691"/>algorithms can vary from annoying to fundamentally life-changing. Even when the results are not incorrect, these systems are increasingly making profound decisions that affect our public and private lives.</p>
<p>Deep learning systems are only as good as their training and their algorithms, and time after time, we find that biases, prejudices, and outright errors in the training data are perpetuated and even enforced by the resulting algorithms. Such systems fall far short of the kinds of accuracy and fairness we expect when dealing with humans and other living creatures. Our algorithms completely lack empathy and compassion. They have no sense of exceptional circumstances, and have no conception of the joy or suffering their decisions can cause. They cannot understand love, kindness, fear, hope, gratitude, sorrow, generosity, wisdom, or any of the other qualities that we value in one another. They cannot admire, cry, smile, grieve, celebrate, or regret. </p>
<p>Deep learning holds great promise to help us as individuals and societies. But any tool can be used as a weapon, benefiting the owners of that tool to the disadvantage of those affected. Machine learning systems are often effectively invisible, so any systematic errors can go undetected for long periods. Even once problems are uncovered, it can require enormous social and political action to hold accountable the organizations that benefit by selling or using these systems, and even more effort to bring about change.</p>
<p>Another peril of machine learning systems is their insatiable need for enormous amounts of training data. This creates a market for organizations that do nothing but collect, organize, and sell previously private information about people’s lives, from their friendships and family relationships to where and when they like to travel, what food they like to eat, what medications they’re taking, and what their DNA reveals about them. This data can be used to harass, intimidate, threaten, and harm individuals.</p>
<p>The demand for massive quantities of data also means that as an organization grows larger (and often less accountable), the more data it can gather, and the more powerful its algorithms grow, making their decisions more influential, which means they can gather more data, and thereby solidify the organization’s power in a feedback loop. Every imperfection in such a system becomes magnified, and because of their scale, negative effects on individuals can go entirely unnoticed by the people running these systems. In general, the only competition to such organizations will come from other organizations of equivalent size, offering systems with their own enormous databases containing their own biases and errors, leading to the battles of the biased behemoths we’re familiar with today. This continuous and increasing concentration of power is a dangerous force in a free society when not subject to significant and strongly enforced control and regulation. Sadly, such controls are rarely to be seen today.</p>
<p>Deep learning has produced algorithms that make it possible to take any person’s appearance, from an entertainer to a politician, and produce images, audio, and video that seem to realistically portray that person saying or doing anything the person wielding the software desires. Societies have come to rely on captured audio, photographs, and video to enforce contracts, reconcile conflicts, exalt or shame a public person, influence <span epub:type="pagebreak" title="692" id="Page_692"/>elections, and serve as evidence in courts. The end of that era is very near, returning us to a time before reliable photographs, recordings, and video, when hearsay, memory, and opinion replace objective historical recordings. Without reliable audio and visual evidence documenting what someone actually said or did, the loudest, richest, or most persuasive voices in the room will determine many outcomes in public opinion, elections, and courts of law, because objective facts will be increasingly harder to find or trust. </p>
<p>Deep learning is a fascinating field, and we’re just starting to understand how it will affect our culture and society. Learning algorithms will surely continue to grow in scope and impact. They have the chance to create enormous good by helping people be happier, enabling societies to be more fair and supportive, and creating healthier and more diverse personal, social, political, and physical environments. It’s important to strive for these positive outcomes, even when they curtail corporate profits or governmental control.</p>
<p>We should remember to always use deep learning, like all of our tools, to bring out the best in humanity, and make the world a better place for everyone.</p>
</section>
</div></body></html>