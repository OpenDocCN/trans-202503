- en: '4'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '4'
- en: NETWORK NAMESPACES
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 网络命名空间
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: Understanding container networking is the biggest challenge in building modern
    applications based on containerized microservices. First, networking is complicated
    even without introducing containers. Multiple levels of abstraction are involved
    just in sending a simple `ping` from one physical server to another. Second, containers
    introduce additional complexity because each has its own set of virtual network
    devices to make it look like a separate machine. Not only that, but a container
    orchestration framework like Kubernetes then adds another layer of complexity
    by adding an “overlay” network through which containers can communicate even when
    they are running on different hosts.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 理解容器网络是构建基于容器化微服务的现代应用程序时面临的最大挑战。首先，即使没有引入容器，网络也是复杂的。仅仅从一台物理服务器发送一个简单的`ping`到另一台物理服务器，就涉及了多个抽象层。其次，容器引入了额外的复杂性，因为每个容器都有自己的一组虚拟网络设备，使其看起来像一个独立的机器。更重要的是，像Kubernetes这样的容器编排框架通过增加一个“覆盖”网络，使得容器即使运行在不同的主机上也能进行通信，从而增加了更多的复杂性。
- en: In this chapter, we will look in detail at how container networking operates.
    We will look at a container’s virtual network devices, including how each network
    device is assigned a separate IP address that can reach the host. We’ll see how
    containers on the same host are connected to one another through a bridge device
    and how container devices are configured to route traffic. Finally, we’ll examine
    how address translation is used to enable containers to connect to other hosts
    without exposing container networking internals on the host’s network.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细了解容器网络是如何工作的。我们将查看容器的虚拟网络设备，包括每个网络设备如何分配一个可以访问主机的独立IP地址。我们还将看到，同一主机上的容器如何通过桥接设备连接到彼此，以及容器设备如何配置以路由流量。最后，我们将探讨如何使用地址转换，使容器能够连接到其他主机，而不会暴露容器网络内部结构到主机的网络上。
- en: Network Isolation
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络隔离
- en: In [Chapter 2](ch02.xhtml#ch02), we discussed how isolation is important to
    system reliability because processes generally can’t affect something they cannot
    see. This is one important reason for network isolation in containers. Another
    reason is ease of configuration. To run a process that acts as a server, such
    as a web server, we need to choose one or more network interfaces on which that
    server will listen, and we need to choose a port number on which it will listen.
    We can’t have two processes listening on the same port on the same interface.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](ch02.xhtml#ch02)中，我们讨论了隔离对于系统可靠性的重要性，因为进程通常不能影响它们看不见的东西。这是容器网络隔离的重要原因之一。另一个原因是配置的简便性。要运行一个作为服务器的进程，比如一个Web服务器，我们需要选择一个或多个网络接口来监听该服务器，并且需要选择一个端口号来监听。我们不能让两个进程在同一个接口的相同端口上监听。
- en: As a result, it’s common for a process that acts as a server to provide a way
    to configure which port it should use to listen for connections. However, that
    still requires us to know what other servers are out there and what ports they
    are using so that we can ensure there are no conflicts. That would be impossible
    with a container orchestration framework like Kubernetes because new processes
    can show up at any time, from different users, with a need to listen on any port
    number.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作为服务器的进程通常会提供一种配置方式，让我们指定其监听连接的端口。然而，这仍然要求我们了解其他服务器的情况以及它们使用的端口，从而确保没有冲突。这对于像Kubernetes这样的容器编排框架来说几乎是不可能的，因为新的进程可以随时出现，来自不同的用户，并且可能需要监听任何端口。
- en: The way to get around this is to provide separate virtual network interfaces
    for each container. That way, a process in a container can choose any port it
    wants—it will be listening on a different network interface from a process in
    a different container. Let’s see a quick example.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是为每个容器提供独立的虚拟网络接口。这样，容器中的进程可以选择任何它想要的端口——它将监听一个与另一个容器中进程不同的网络接口。我们来看一个简短的例子。
- en: '**NOTE**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The example repository for this book is at* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples).
    *See “Running Examples” on [page xx](ch00.xhtml#ch00lev1sec2) for details on getting
    set up.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*本书的示例仓库位于* [https://github.com/book-of-kubernetes/examples](https://github.com/book-of-kubernetes/examples)。*有关设置的详细信息，请参见[第xx页](ch00.xhtml#ch00lev1sec2)中的“运行示例”。*'
- en: 'We’ll run two instances of an NGINX web server; each instance will listen on
    port 80\. As before, we’ll use CRI-O and `crictl`, but we’ll use a script to cut
    down on the typing:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行两个实例的 NGINX Web 服务器；每个实例将在端口 80 上监听。与以前一样，我们将使用 CRI-O 和 `crictl`，但我们将使用一个脚本来减少输入：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `source` before `nginx.sh` is important; it ensures that the script is run
    in a way that makes the environment variables it sets available in our shell for
    future commands. Inside *nginx.sh* are the usual `crictl runp`, `crictl create`,
    and `crictl start` commands we’ve used in previous chapters. The YAML files are
    also very similar to examples we’ve seen before; the only difference is that we
    use a container image that has NGINX installed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *nginx.sh* 之前的 `source` 很重要；它确保脚本以一种方式运行，使得它设置的环境变量在我们的 shell 中对未来的命令可用。在
    *nginx.sh* 中是我们在前几章中使用过的常规命令 `crictl runp`、`crictl create` 和 `crictl start`。YAML
    文件也与我们以前看到的示例非常相似；唯一的区别是我们使用了安装有 NGINX 的容器镜像。
- en: 'Let’s verify that we have two NGINX servers running:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证我们有两个 NGINX 服务器正在运行：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can also verify that both NGINX servers are listening on port 80, the standard
    port for web servers:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以验证两个 NGINX 服务器都在监听端口 80，这是 Web 服务器的标准端口：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We look at the open port by printing */proc/net/tcp* because we need to run
    this command inside the NGINX container, where we don’t have standard Linux commands
    like `netstat` or `ss`. As we saw in [Chapter 2](ch02.xhtml#ch02), in a container
    we have a separate `mnt` namespace providing a separate filesystem for each container,
    so only the executables available in that separate filesystem can be run in that
    namespace.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过打印 */proc/net/tcp* 我们查看开放的端口，因为我们需要在 NGINX 容器内运行这个命令，而我们没有标准的 Linux 命令，如 `netstat`
    或 `ss`。正如我们在 [第二章](ch02.xhtml#ch02) 中看到的，在容器中，我们有一个单独的 `mnt` 命名空间为每个容器提供单独的文件系统，因此只有在该单独文件系统中可用的可执行文件才能在该命名空间中运行。
- en: 'The port shown in both cases is `0050` in hexadecimal, which is port 80 in
    decimal. If these two processes were running together on the same system without
    network isolation, they wouldn’t both be able to listen on port 80, but in this
    case, the two NGINX instances have separate network interfaces. To explore this
    further, let’s start up a new BusyBox container:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下显示的端口是 `0050`，这是十六进制中的端口 80 在十进制中的表示。如果这两个进程在没有网络隔离的同一系统上运行，它们都无法同时监听端口
    80，但在这种情况下，这两个 NGINX 实例有单独的网络接口。为了进一步探索这一点，让我们启动一个新的 BusyBox 容器：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'BusyBox is now running in addition to our two NGINX containers:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在除了我们的两个 NGINX 容器外，BusyBox 也在运行：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s start a shell inside the container:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在容器内部启动一个 shell：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Listing 4-1](ch04.xhtml#ch04list1) shows the container’s network devices and
    addresses.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4-1](ch04.xhtml#ch04list1) 显示了容器的网络设备和地址。'
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*Listing 4-1: BusyBox network*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 4-1: BusyBox 网络*'
- en: Ignoring the standard loopback device, we see a network device with `10.85.0.4`
    for an IP address. This does not correspond at all with the IP address of the
    host, which is `192.168.61.11`; it is on a different network entirely. Because
    our container is on a separate network, we might not expect to be able to `ping`
    the underlying host system from inside the container, but it works, as [Listing
    4-2](ch04.xhtml#ch04list2) demonstrates.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略标准的环回设备，我们看到一个网络设备，其 IP 地址为 `10.85.0.4`。这与主机的 IP 地址 `192.168.61.11` 根本不对应；它在完全不同的网络上。由于我们的容器位于单独的网络上，我们可能不希望能够从容器内部
    `ping` 底层主机系统，但这是有效的，正如 [列表 4-2](ch04.xhtml#ch04list2) 所示。
- en: '[PRE7]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*Listing 4-2: BusyBox ping test*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 4-2: BusyBox ping 测试*'
- en: For traffic to get from our container to the host network, there must be an
    entry in the routing table to make that happen. As [Listing 4-3](ch04.xhtml#ch04list3)
    illustrates, we can verify this by using the `ip` command.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要使流量从我们的容器到主机网络，路由表中必须有一个条目来实现这一点。正如 [列表 4-3](ch04.xhtml#ch04list3) 所示，我们可以使用
    `ip` 命令验证这一点。
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*Listing 4-3: BusyBox routes*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 4-3: BusyBox 路由*'
- en: As expected, there is a default route. When we sent the `ping`, our BusyBox
    container reached out to `10.85.0.1`, which then had the ability to send the `ping`
    onward until it reached `192.168.61.11`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 预期地，存在一个默认路由。当我们发送 `ping` 时，我们的 BusyBox 容器连接到 `10.85.0.1`，然后有能力将 `ping` 转发直至到达
    `192.168.61.11`。
- en: 'We’ll leave all three containers running to explore them further, but let’s
    exit our BusyBox shell to get back to the host:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保持这三个容器继续运行以进一步探索它们，但让我们退出 BusyBox shell 返回主机：
- en: '[PRE9]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The view of the network from inside the container shows why our two NGINX servers
    are both able to listen on port 80\. As mentioned earlier, only one process can
    listen on a port for a particular interface, but of course, if each NGINX server
    has a separate network interface, there is no conflict.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从容器内部查看网络可以解释为什么我们的两个NGINX服务器都能监听80端口。如前所述，只有一个进程能够监听特定接口的端口，但当然，如果每个NGINX服务器都有一个单独的网络接口，就不会发生冲突。
- en: Network Namespaces
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络命名空间
- en: CRI-O is using Linux network namespaces to create this isolation. We explored
    network namespaces briefly in [Chapter 2](ch02.xhtml#ch02); in this chapter, we’ll
    look at them in more detail.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: CRI-O使用Linux网络命名空间来创建这种隔离。在[第2章](ch02.xhtml#ch02)中，我们简要地探讨了网络命名空间；在本章中，我们将更详细地讨论它们。
- en: 'First, let’s use the `lsns` command to list the network namespaces that CRI-O
    has created for our containers:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用`lsns`命令列出CRI-O为我们的容器创建的网络命名空间：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In addition to the root network namespace that is used for all the processes
    that aren’t in a container, we see three network namespaces, one for each Pod
    we’ve created.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于所有不在容器中的进程的根网络命名空间外，我们还看到三个网络命名空间，每个命名空间对应一个我们创建的Pod。
- en: When we use CRI-O with `crictl`, the network namespace actually belongs to the
    Pod. The `pause` process that is listed here exists so that the namespaces can
    continue to exist even as containers come and go inside the Pod.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用CRI-O与`crictl`时，网络命名空间实际上属于Pod。这里列出的`pause`进程存在的目的是为了让命名空间在Pod内的容器进出时能够持续存在。
- en: 'In the previous example, there are four network namespaces. The first one is
    the root namespace that was created when our host booted. The other three were
    created for each of the containers we have started so far: two NGINX containers
    and one BusyBox container.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，有四个网络命名空间。第一个是我们主机启动时创建的根命名空间。其他三个是为我们启动的每个容器创建的：两个NGINX容器和一个BusyBox容器。
- en: Inspecting Network Namespaces
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 检查网络命名空间
- en: 'To learn about how network namespaces work and manipulate them, we’ll use the
    `ip netns` command to list network namespaces:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解网络命名空间是如何工作的并进行操作，我们将使用`ip netns`命令列出网络命名空间：
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This command looks in a different configuration location to find network namespaces,
    so only the three container namespaces are listed.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令会在不同的配置位置查找网络命名空间，因此只列出了三个容器命名空间。
- en: 'We want to capture the network namespace for our BusyBox container. It’s one
    of the three listed, and we can guess that it is the one labeled `(id: 2)` because
    we created it last, but we can also use `crictl` and `jq` to extract the information
    we need:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '我们希望获取我们BusyBox容器的网络命名空间。它是三个列出的命名空间之一，我们可以猜测它是标记为`(id: 2)`的那个，因为我们最后创建了它，但我们也可以使用`crictl`和`jq`来提取我们需要的信息：'
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If you run `crictl inspectp $B1P_ID` by itself, you’ll see a wealth of information
    about the BusyBox Pod. Out of all that information, we want only the information
    about the network namespace, so we use `jq` to extract that information in three
    steps. First, it reaches down into the JSON data to pull out all of the namespaces
    associated with this Pod. It then selects only the namespace that has a `type`
    field of `network`. Finally, it extracts the `path` field for that namespace and
    stores it in the environment variable `NETNS_PATH`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果单独运行`crictl inspectp $B1P_ID`，你将看到关于BusyBox Pod的大量信息。在所有这些信息中，我们只需要关于网络命名空间的信息，因此我们使用`jq`分三步提取这些信息。首先，它会深入到JSON数据中，提取与此Pod相关的所有命名空间。然后，它只选择具有`type`字段为`network`的命名空间。最后，它提取该命名空间的`path`字段，并将其存储在环境变量`NETNS_PATH`中。
- en: The value that `crictl` returns is the full path to the network namespace under
    */var/run*. For our upcoming commands, we want only the value of the namespace,
    so we use `basename` to strip off the path. Also, because this information will
    be a lot more usable if we assign it to an environment variable, we do that, and
    then we use `echo` to print the value so that we can confirm it all worked.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`crictl`返回的值是网络命名空间在*/var/run*下的完整路径。对于接下来的命令，我们只需要命名空间的值，所以我们使用`basename`来去掉路径部分。此外，由于如果将这些信息分配给环境变量会更易于使用，我们这么做了，然后使用`echo`打印出该值，以便我们确认一切正常。'
- en: Of course, for interactive debugging, you can often just scroll through the
    entire contents of `crictl inspectp` (for Pods) and `crictl inspect` (for containers)
    and pick out the values you want. But this approach of extracting data with `jq`
    is very useful in scripting or in reducing the amount of output to scan through
    manually.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，对于交互式调试，您通常可以仅滚动浏览整个`crictl inspectp`（用于Pods）和`crictl inspect`（用于容器）的内容，并选择您想要的值。但是使用`jq`提取数据的这种方法在脚本编写或减少手动扫描输出量方面非常有用。
- en: 'Now that we’ve extracted the network namespace for BusyBox from `crictl`, let’s
    see what processes are assigned to that namespace:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们从`crictl`中提取了BusyBox的网络命名空间，让我们看看分配给该命名空间的进程有哪些：
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If we just ran `ip netns pids $NETNS`, we would get a list of the process IDs
    (PIDs), but no extra information. We take that output and send it to `ps --pid`,
    which makes it possible for us to see the name of the commands. As expected, we
    see a `pause` process and the `sleep` process that we specified when we ran the
    BusyBox container.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只运行`ip netns pids $NETNS`，我们将得到一个进程ID（PID）列表，但没有额外的信息。我们将该输出发送到`ps --pid`，这样我们就可以看到命令的名称。正如预期的那样，我们看到了我们在运行BusyBox容器时指定的`pause`进程和`sleep`进程。
- en: In the previous section, we used `crictl exec` to run a shell inside the container,
    which enabled us to see what network interfaces were available in that network
    namespace. Now that we know the ID of the network namespace, we can use `ip netns
    exec` to run commands individually from within a network namespace. Running `ip
    netns exec` is very powerful in that it is not limited to just networking commands,
    but could be any process such as a web server. However, note that this is not
    the same as fully running inside the container, because we are not entering any
    of the container’s other namespaces (for example, the `pid` namespace used for
    process isolation).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用`crictl exec`在容器内运行了一个Shell，这使我们能够看到该网络命名空间中可用的网络接口。现在我们知道了网络命名空间的ID，我们可以使用`ip
    netns exec`从网络命名空间内单独运行命令。使用`ip netns exec`非常强大，因为它不仅限于网络命令，还可以是任何进程，比如Web服务器。但请注意，这与完全在容器内部运行不同，因为我们没有进入容器的任何其他命名空间（例如用于进程隔离的`pid`命名空间）。
- en: 'Next, let’s try the `ip addr` command from within the BusyBox network namespace:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们在BusyBox网络命名空间内尝试`ip addr`命令：
- en: '[PRE14]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The list of network devices and IP addresses that we see here matches what we
    saw when we ran commands inside our BusyBox container in [Listing 4-1](ch04.xhtml#ch04list1).
    CRI-O is creating these network devices and placing them in the network namespace.
    (We will see how CRI-O was configured to perform container networking when we
    look at Kubernetes networking in [Chapter 8](ch08.xhtml#ch08).) For now, let’s
    look at how we can create our own devices and namespaces for network isolation.
    This will also show us how to debug container networking when something isn’t
    working correctly.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里看到的网络设备和IP地址列表与我们在[清单4-1](ch04.xhtml#ch04list1)内部运行BusyBox容器时看到的内容相匹配。 CRI-O正在创建这些网络设备并将它们放置在网络命名空间中。（当我们查看第8章节关于Kubernetes网络时，我们将看到CRI-O是如何配置执行容器网络的。）现在，让我们看看如何创建自己的设备和网络命名空间以进行网络隔离。这也将向我们展示在容器网络出现问题时如何进行调试。
- en: Creating Network Namespaces
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建网络命名空间
- en: 'We can create a network namespace with a single command:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用一个命令创建一个网络命名空间：
- en: '[PRE15]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This new namespace immediately shows up in the list:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的命名空间立即出现在列表中：
- en: '[PRE16]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This namespace isn’t very useful yet; it has a loopback interface but nothing
    else:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命名空间目前还不是很有用；它有一个回环接口，但没有其他内容：
- en: '[PRE17]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In addition, even the loopback interface is down, so it couldn’t be used. Let’s
    quickly fix that:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，即使是回环接口也是关闭的，因此无法使用。让我们快速修复它：
- en: '[PRE18]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The loopback interface is now up, and it has the typical IP address of `127.0.0.1`.
    A basic loopback `ping` will now work in this network namespace:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 回环接口现在已经启动，并且具有`127.0.0.1`的典型IP地址。现在，在这个网络命名空间中基本的回环`ping`将会起作用：
- en: '[PRE19]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The ability to `ping` the loopback network interface is a useful first test
    for any networking stack, as it shows the ability to send and receive packets.
    So, we now have a basic working network stack in our new network namespace, but
    it still isn’t terribly useful because a loopback interface by itself can’t talk
    to anything else on our system. We need to add another network device in this
    network namespace in order to establish connectivity to the host and the rest
    of the network.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`ping`回环网络接口的能力是任何网络堆栈的有用初步测试，因为它显示了发送和接收数据包的能力。因此，我们现在在新的网络命名空间中拥有一个基本的工作网络堆栈，但它仍然不是特别有用，因为回环接口本身无法与系统上的其他任何东西通信。我们需要在此网络命名空间中添加另一个网络设备，以便与主机和其他网络建立连接。'
- en: To do this, we’ll create a *virtual Ethernet* (veth) device. You can think of
    a veth as a virtual network cable. Like a network cable, it has two ends, and
    whatever goes in one end comes out the other end. For this reason, the term *veth
    pair* is often used.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将创建一个*虚拟以太网*（veth）设备。你可以将veth视为一根虚拟网络电缆。像网络电缆一样，它有两个端口，任何从一个端口进入的东西都会从另一个端口出来。因此，通常使用术语*veth对*。
- en: 'We start with a command that creates the veth pair:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个创建veth对的命令开始：
- en: '[PRE20]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This command does three things:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令做了三件事：
- en: Creates a veth device called `myveth-host`
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`myveth-host`的veth设备
- en: Creates a veth device called `myveth-myns`
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`myveth-myns`的veth设备
- en: Places the device `myveth-myns` in the network namespace `myns`
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将设备`myveth-myns`放置到网络命名空间`myns`中
- en: 'The host side of the veth pair appears in the regular list of network devices
    on the host:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: veth对的主机端出现在主机的常规网络设备列表中：
- en: '[PRE21]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This output shows `myveth-host` and also that it is connected to a device in
    the network namespace `myns`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出显示了`myveth-host`，并且它连接到了网络命名空间`myns`中的设备。
- en: If you run this command for yourself and look at the complete list of host network
    devices, you will notice additional `veth` devices connected to each of the container
    network namespaces. These were created by CRI-O when we deployed NGINX and BusyBox.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你自己运行此命令并查看主机网络设备的完整列表，你会注意到每个容器网络命名空间都有额外的`veth`设备。这些设备是CRI-O在我们部署NGINX和BusyBox时创建的。
- en: 'Similarly, we can see that our `myns` network namespace has a new network interface:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以看到我们的`myns`网络命名空间有了一个新的网络接口：
- en: '[PRE22]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As before, this interface is currently down. We need to bring up both sides
    of the veth pair before we can start communicating. We also need to assign an
    IP address to the `myveth-myns` side to enable it to communicate:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如之前所述，这个接口当前是关闭的。我们需要启动veth对的两端，才能开始通信。我们还需要为`myveth-myns`端分配一个IP地址，以使其能够通信：
- en: '[PRE23]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'A quick check confirms that we’ve successfully configured an IP address and
    brought up the network:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一个快速检查确认我们已经成功配置了IP地址并启动了网络：
- en: '[PRE24]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In addition to the loopback interface, we now see an additional interface with
    the IP address `10.85.0.254`. What happens if we try to `ping` this new IP address?
    It turns out we can indeed `ping` it, but only from within the network namespace:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了回环接口，我们现在还看到一个具有IP地址`10.85.0.254`的附加接口。如果我们尝试`ping`这个新的IP地址，会发生什么呢？事实证明，我们确实可以`ping`它，但只能在网络命名空间内部进行：
- en: '[PRE25]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The first `ping` command, run using `ip netns exec` so that it runs within the
    network namespace, shows a successful response ➊. However, the second `ping` command,
    run without `ip netns exec`, shows that no packets were received ➋. The problem
    is that we have successfully created a network interface inside our network namespace,
    and we have the other end of the veth pair on our host network, but we haven’t
    connected up a corresponding network device on the host, so there’s no host network
    interface that can talk to the interface in the network namespace.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个`ping`命令通过`ip netns exec`运行，以便在网络命名空间内运行，显示了成功的响应➊。然而，第二个`ping`命令没有通过`ip
    netns exec`运行，显示没有接收到数据包➋。问题在于，我们已经成功创建了一个网络命名空间中的网络接口，并且veth对的另一端在主机网络上，但我们没有在主机上连接相应的网络设备，因此没有主机网络接口可以与网络命名空间中的接口通信。
- en: At the same time, when we ran a `ping` test from our BusyBox container in [Listing
    4-2](ch04.xhtml#ch04list2), we were able to `ping` the host with no trouble. Clearly,
    there must be more configuration that CRI-O did for us when it created our containers.
    Let’s explore that in the next section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，当我们从BusyBox容器中运行`ping`测试时，在[清单4-2](ch04.xhtml#ch04list2)中，我们能够顺利地`ping`主机。显然，CRI-O在创建容器时为我们进行了更多配置。让我们在下一节中探讨这一点。
- en: Bridge Interfaces
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 桥接接口
- en: 'The host side of the veth pair currently isn’t connected to anything, so it
    isn’t surprising that our network namespace can’t talk to the outside world yet.
    To fix that, let’s look at one of the veth pairs that CRI-O created:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: veth 对的主机端目前没有连接到任何设备，因此我们还不能与外界进行通信也就不足为奇。为了修复这个问题，让我们来看一下 CRI-O 创建的其中一个 veth
    对：
- en: '[PRE26]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Unlike the interface we created, this interface specifies `master cni0`, which
    shows that it belongs to a *network bridge*. A network bridge exists to connect
    multiple interfaces together. You can think of it as an Ethernet switch because
    it routes traffic from one network interface to another based on the media access
    control (MAC) address of the interfaces.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们创建的接口不同，这个接口指定了 `master cni0`，表明它属于一个 *网络桥接器*。网络桥接器用于将多个接口连接在一起。你可以将它视为一个以太网交换机，因为它根据接口的媒体访问控制（MAC）地址来路由流量。
- en: 'We can see the bridge `cni0` in the list of network devices on the host:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在主机的网络设备列表中看到桥接器 `cni0`：
- en: '[PRE27]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The bridge is a little smarter than a typical Ethernet switch in that it provides
    some firewall and routing capabilities. It also has an IP address of `10.85.0.1`.
    This IP address is the same as we saw with the default route for our BusyBox container
    in [Listing 4-3](ch04.xhtml#ch04list3), so we’ve started to solve the mystery
    of how our BusyBox container is able to talk to hosts outside of its own network.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个桥接器比典型的以太网交换机更智能，它提供了一些防火墙和路由功能。它的 IP 地址也是 `10.85.0.1`。这个 IP 地址与我们在 [Listing
    4-3](ch04.xhtml#ch04list3) 中看到的 BusyBox 容器默认路由相同，因此我们已经开始解开 BusyBox 容器能够与其网络外部主机通信的谜团。
- en: Adding Interfaces to a Bridge
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向桥接器添加接口
- en: 'To inspect the bridge and add devices to it, we’ll use the `brctl` command.
    Let’s inspect the bridge first:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查桥接器并向其添加设备，我们将使用 `brctl` 命令。首先，让我们检查桥接器：
- en: '[PRE28]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The bridge `cni0` has three interfaces on it, corresponding to the host side
    of the veth pair for each of the three containers we have running (two NGINX and
    one BusyBox). Let’s take advantage of this existing bridge to set up network connectivity
    to the namespace we created:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 桥接器 `cni0` 上有三个接口，分别对应我们运行的三个容器的 veth 对的主机端（两个 NGINX 和一个 BusyBox）。我们可以利用这个现有的桥接器来为我们创建的网络命名空间设置网络连接：
- en: '[PRE29]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The host side of our veth pair is now connected to the bridge, which means
    that we can now `ping` into the namespace from the host:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们 veth 对的主机端已连接到桥接器，这意味着我们现在可以从主机使用 `ping` 命令测试与命名空间的连接：
- en: '[PRE30]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The fact that a packet was received ➊ shows that we set up a working connection.
    We should be pleased that it worked, but if we want to really understand this,
    we can’t be satisfied with saying that we can `ping` this interface “from the
    host.” We need to be more specific as to exactly how traffic is flowing.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包被接收 ➊ 的事实表明我们已经建立了一个有效的连接。我们应该为它的成功感到高兴，但如果我们真的想理解这一点，我们不能仅仅满足于说“我们可以从主机
    ping 这个接口”。我们需要更具体地了解流量是如何流动的。
- en: Tracing Traffic
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跟踪流量
- en: 'Let’s actually trace this traffic to see what’s happening when we run the `ping`
    command. We will use `tcpdump` to print out the traffic. First, let’s start a
    `ping` command in the background to create some traffic to trace:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实际跟踪一下这个流量，看看在我们运行 `ping` 命令时发生了什么。我们将使用 `tcpdump` 打印流量。首先，让我们在后台启动一个 `ping`
    命令，以便产生一些流量来跟踪：
- en: '[PRE31]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We send the output to */dev/null* so that it doesn’t clutter up our session.
    Now, let’s use `tcpdump` to see the traffic:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将输出发送到 */dev/null*，以免它干扰到我们的会话。现在，让我们使用 `tcpdump` 来查看流量：
- en: '[PRE32]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We use `timeout` to prevent `tcpdump` from running indefinitely, and we also
    use `killall` afterward to stop the `ping` command and discontinue it running
    in the background.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `timeout` 来防止 `tcpdump` 无限运行，之后我们还会使用 `killall` 来停止 `ping` 命令并终止其在后台的运行。
- en: 'The output shows that the `ping` is originating from the bridge interface,
    which has IP address `10.85.0.1`. This is because of the host’s routing table:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示 `ping` 来自桥接接口，该接口的 IP 地址是 `10.85.0.1`。这是因为主机的路由表设置：
- en: '[PRE33]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: When CRI-O created the bridge and configured its IP address, it also set up
    a route so that all traffic destined for the `10.85.0.0/16` network (that is,
    all traffic from `10.85.0.0` through `10.85.255.255`) would use `cni0`. This is
    enough information for the `ping` command to know where to send its packet, and
    the bridge handles the rest.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当 CRI-O 创建了桥接器并配置了其 IP 地址时，它还设置了一条路由，确保所有目标为 `10.85.0.0/16` 网络的流量（即从 `10.85.0.0`
    到 `10.85.255.255` 的所有流量）都会通过 `cni0`。这足以让 `ping` 命令知道如何发送数据包，桥接器处理剩下的工作。
- en: 'The fact that the `ping` is coming from the bridge interface of `10.85.0.1`
    rather than the host interface of `192.168.61.11` actually makes a big difference,
    as we can see if we try to run the `ping` the other way around. Let’s try to `ping`
    from within the namespace to the host network:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，`ping`来自`10.85.0.1`网桥接口而不是`192.168.61.11`主机接口，实际上有很大的区别，我们可以通过尝试从命名空间向主机网络运行`ping`来看到这一点。让我们尝试从命名空间内部向主机网络进行`ping`：
- en: '[PRE34]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The issue here is that the interface in our network namespace doesn’t know
    how to reach the host network. The bridge is available and willing to route traffic
    onto the host network, but we haven’t configured the necessary route to use it.
    Let’s do that now:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题是我们的网络命名空间中的接口不知道如何到达主机网络。网桥是可用的，并愿意将流量路由到主机网络，但我们尚未配置必要的路由来使用它。让我们现在来做这个：
- en: '[PRE35]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And now the `ping` works:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在`ping`命令有效了：
- en: '[PRE36]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This illustrates an important rule to remember when debugging network problems:
    it’s very easy to jump to conclusions about what network traffic is really being
    sent and received. There is often no substitute for using tracing to see what
    the traffic really looks like.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明了在调试网络问题时需要记住的一个重要规则：很容易就会对网络流量的实际发送和接收情况下结论。往往需要使用跟踪工具来查看实际的流量情况，没有什么可以替代这一点。
- en: '**IP ADDRESSES ON THE HOST**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**主机上的IP地址**'
- en: This approach is not the only one that results in connectivity from the host
    into the network namespace. We also could have assigned an IP address directly
    to the host side of the veth pair. However, even though that would have enabled
    communication from the host into our network namespace, it wouldn’t provide a
    way for multiple network namespaces to communicate with one another. Using a bridge
    interface, as CRI-O does, enables the interconnection of all of the containers
    on a host, making them all appear to be on the same network.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法并不是唯一能实现从主机到网络命名空间的连接性的方法。我们还可以直接为veth对的主机端分配IP地址。然而，即使这样做可以使主机能够与我们的网络命名空间通信，但它不会提供多个网络命名空间之间进行通信的方法。使用桥接接口，正如CRI-O所做的那样，能够在主机上互连所有容器，使它们看起来都在同一个网络上。
- en: This also explains why we didn’t assign an IP address to the host side of the
    veth pair. When working with bridges, only the bridge interface gets an IP address.
    Interfaces added to the bridge do not.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这也解释了为什么我们没有给veth对的主机端分配IP地址。在使用网桥时，只有网桥接口会获得IP地址。添加到网桥的接口不会获得IP地址。
- en: 'With that last change, it would seem like we’ve matched the network configuration
    of our containers, but we are still missing the ability to communicate with the
    broader network outside of `host01`. We can demonstrate this by trying to `ping`
    from our network namespace to `host02`, which is on the same internal network
    as `host01` and has the IP address `192.168.61.12`. If we try a `ping` from our
    BusyBox container, it works:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行最后一次更改后，看起来我们已经匹配了我们的容器的网络配置，但我们仍然缺少与`host01`之外的更广泛网络通信的能力。我们可以通过尝试从我们的网络命名空间向`host02`进行`ping`来演示这一点，`host02`位于与`host01`相同的内部网络上，并具有IP地址`192.168.61.12`。如果我们尝试从我们的BusyBox容器进行`ping`，它会成功：
- en: '[PRE37]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The `ping` output reports that a packet was received. However, if we try the
    same command using the network namespace we created, it doesn’t work:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`ping`输出报告收到一个数据包。但是，如果我们尝试使用我们创建的网络命名空间执行相同的命令，它却不起作用：'
- en: '[PRE38]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This command reports that no packets were received.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令报告未接收到任何数据包。
- en: Really, we ought to be surprised that the `ping` from our BusyBox container
    worked. After all, `host02` doesn’t know anything about the BusyBox container,
    or the `cni0` bridge interface, or the `10.85.0.0/16` network that the containers
    are in. How is it possible for `host02` to exchange a ping with our BusyBox container?
    To understand that, we need to look at network masquerade.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们应该感到惊讶的是，我们的BusyBox容器的`ping`命令确实有效了。毕竟，`host02`并不知道任何关于BusyBox容器、`cni0`网桥接口或容器所在的`10.85.0.0/16`网络的信息。`host02`如何可能与我们的BusyBox容器进行`ping`通信？要理解这一点，我们需要看看网络伪装。
- en: Masquerade
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 伪装
- en: '*Masquerade*, also known as Network Address Translation (NAT), is used every
    day in networking. For example, most home connections to the internet are provided
    with only a single IP address that is addressable from the internet, but many
    devices within the home network need an internet connection. It is the job of
    a router to make it appear that all traffic from that network is originating from
    a single IP address. It does this by rewriting the *source* IP address of outgoing
    traffic while tracking all outgoing connections so that it can rewrite the *destination*
    IP address of any replies.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*伪装*，也称为网络地址转换（NAT），在网络中每天都会使用。例如，大多数家庭连接到互联网时，只有一个可以从互联网访问的IP地址，但家庭网络内的许多设备也需要连接互联网。路由器的工作就是让所有来自该网络的流量看起来都是从单一IP地址发出的。它通过重写出站流量的*源*IP地址，并跟踪所有出站连接，以便它可以重写任何回复的*目标*IP地址来实现这一点。'
- en: '**NOTE**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The kind of NAT that we are talking about here is technically known as Source
    NAT (SNAT). Don’t get hung up on the name, though; for it to work correctly, any
    reply packets must have their destination rewritten. The term Source in this case
    just means that the source address is what’s rewritten when a new connection is
    initiated.*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们在这里讨论的NAT类型在技术上被称为源NAT（SNAT）。不过不要过于纠结于这个名称；为了让它正常工作，任何回复数据包必须将其目标地址重新写入。这里的“源”一词意味着，当发起新连接时，源地址是被重写的。*'
- en: Masquerading sounds like just what we need to connect our containers running
    in the `10.85.0.0/16` network to the host network, `192.168.61.0/24`, and in fact
    it is exactly how it worked. When we sent a ping from our BusyBox container, the
    source IP address was rewritten such that the ping appeared to come from the `host01`
    IP `192.168.61.11`. When `host02` responded, it sent its reply to `192.168.61.11`,
    but the destination was rewritten so that it was actually sent to the BusyBox
    container.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 伪装听起来正是我们需要的，用于将运行在`10.85.0.0/16`网络中的容器连接到主机网络`192.168.61.0/24`，实际上它确实是这样工作的。当我们从BusyBox容器发送ping时，源IP地址被重写，使得ping看起来是来自`host01`的IP
    `192.168.61.11`。当`host02`回应时，它将回复发送到`192.168.61.11`，但是目标地址被重写，最终实际上是发送到了BusyBox容器。
- en: 'Let’s trace the `ping` traffic all the way through to demonstrate:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们追踪一下`ping`流量，直到整个过程完成，以便演示：
- en: '[PRE39]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: When the `ping` originates from within our BusyBox container, it has a source
    IP address of `10.85.0.4` ➊. This address is rewritten, making the `ping` appear
    to be coming from the host IP `192.168.61.11` ➋. Of course, `host02` knows how
    to respond to a `ping` coming from that address, so the `ping` is answered ➌.
    At this point, the other half of the masquerade takes effect, and the destination
    is rewritten to `10.85.0.4` ➍. The result is that the BusyBox container is able
    to send a packet to a separate host and get a reply.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当`ping`从我们的BusyBox容器中发起时，它的源IP地址是`10.85.0.4` ➊。这个地址被重写，使得`ping`看起来是来自主机IP `192.168.61.11`
    ➋。当然，`host02`知道如何响应来自该地址的`ping`，所以`ping`得到了回复 ➌。此时，伪装的另一部分开始生效，目标地址被重写为`10.85.0.4`
    ➍。最终，BusyBox容器能够向一个独立主机发送数据包并接收到回复。
- en: 'To complete the setup for our network namespace, we need a similar rule to
    masquerade traffic coming from `10.85.0.254`. We can start by using `iptables`
    to look at the rules that CRI-O created when it configured the containers:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成我们网络命名空间的设置，我们需要一个类似的规则来伪装来自`10.85.0.254`的流量。我们可以从使用`iptables`查看CRI-O创建的规则开始，看看它在配置容器时做了什么：
- en: '[PRE40]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Masquerading starts when the connection is initiated; in this case, when traffic
    has a source address in the `10.85.0.0/16` network. For this reason, the `POSTROUTING`
    chain is used, because it sees all outgoing traffic. There is a rule in the `POSTROUTING`
    chain for each container; each rule invokes a `CNI` chain for that container.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 伪装从连接发起时开始；在这种情况下，当流量的源地址位于`10.85.0.0/16`网络时就会开始。为此，使用了`POSTROUTING`链，因为它会处理所有出站流量。`POSTROUTING`链中有一条针对每个容器的规则；每条规则都会调用该容器的`CNI`链。
- en: For brevity, only one of the three `CNI` chains is shown. The other two are
    identical. The `CNI` chain first does an `ACCEPT` for all traffic that is local
    to the container network, so this traffic won’t be masqueraded. It then sets up
    masquerade for all traffic (except `224.0.0.0/4`, which is multicast traffic that
    cannot be masqueraded because there is no way to properly route replies).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，只展示了三个`CNI`链中的一个。其他两个是相同的。`CNI`链首先会接受所有本地容器网络的流量，因此这些流量不会被伪装。然后，它为所有流量设置伪装（除了`224.0.0.0/4`，这是无法伪装的多播流量，因为无法正确路由回复）。
- en: 'What’s missing from this configuration is a matching setup for traffic from
    `10.85.0.254`, the IP address we assigned to the interface in our network namespace.
    Let’s add that. First, create a new chain in the `nat` table:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置中缺少的是来自`10.85.0.254`的流量的匹配设置，`10.85.0.254`是我们在网络命名空间中分配给接口的 IP 地址。让我们添加这个设置。首先，在`nat`表中创建一个新的链：
- en: '[PRE41]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, add a rule to accept all traffic for the local network:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，添加一个规则来接受本地网络的所有流量：
- en: '[PRE42]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now all remaining traffic (except multicast) should be masqueraded:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有剩余的流量（除了组播）应该都被伪装：
- en: '[PRE43]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'And finally, tell `iptables` to use this chain for any traffic coming from
    `10.85.0.254`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，告诉`iptables`对于来自`10.85.0.254`的任何流量使用这个链：
- en: '[PRE44]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can verify that we did all that correctly by listing the rules again:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过重新列出规则来验证我们是否正确完成了所有操作：
- en: '[PRE45]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'It looks like we have the configuration we need, as this configuration matches
    the way the virtual network devices were configured for the BusyBox container.
    To make sure, let’s try a `ping` to `host02` again:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们已经得到了所需的配置，因为这个配置与我们为 BusyBox 容器配置虚拟网络设备的方式相匹配。为了确认，让我们再次尝试对`host02`进行`ping`：
- en: '[PRE46]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Success! We’ve fully replicated the network isolation and connectivity that
    CRI-O is providing our containers.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们已经完全复制了 CRI-O 为我们的容器提供的网络隔离和连接性。
- en: Final Thoughts
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最后的想法
- en: Container networking looks deceptively simple when running containers. Each
    container is provided with its own set of network devices, avoiding the need to
    worry about port conflicts and reducing the effect that one container can have
    on another. However, as we’ve seen in this chapter, this “simple” network isolation
    requires some complex configuration to enable not just isolation, but also connectivity
    to other containers and other networks. In [Part II](part02.xhtml#part02), after
    we properly introduce Kubernetes, we’ll return to container networking and show
    how the complexity only increases when we need to connect containers running on
    different hosts and load balance traffic across multiple container instances.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 容器网络在运行容器时看起来 deceptively 简单。每个容器都被提供一组自己的网络设备，避免了端口冲突的问题，也减少了一个容器对另一个容器的影响。然而，正如我们在本章中所看到的，这种“简单”的网络隔离需要一些复杂的配置，不仅仅是隔离，还需要实现容器之间以及容器与其他网络之间的连接性。在[第二部分](part02.xhtml#part02)中，当我们正确引入
    Kubernetes 后，我们将回到容器网络，并展示当我们需要连接在不同主机上运行的容器并在多个容器实例之间负载均衡流量时，复杂性如何增加。
- en: For now, we have one more key topic to address with containers before we can
    move on to Kubernetes. We need to understand how container storage works, including
    the container image that is used as the base filesystem when a new container is
    started as well as the temporary storage that a running container uses. In the
    next chapter, we’ll investigate how container storage makes application deployment
    easier and how a layered filesystem is used to save on storage and improve efficiency.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，在我们进入 Kubernetes 之前，还有一个关键主题需要处理，就是容器存储的工作原理。我们需要理解容器存储是如何工作的，包括当启动一个新的容器时，作为基础文件系统使用的容器镜像，以及正在运行的容器使用的临时存储。在下一章，我们将探讨容器存储是如何简化应用部署的，以及如何通过使用分层文件系统来节省存储空间并提高效率。
