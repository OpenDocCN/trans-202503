<html><head></head><body>
<h2 class="h2" id="ch06"><span epub:type="pagebreak" id="page_87"/><span class="big">6</span><br/>WHY KUBERNETES MATTERS</h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">Containers enable us to transform the way we package and deploy application components, but orchestration of containers in a cluster enables the real advantage of a containerized microservice architecture. As described in <a href="ch01.xhtml#ch01">Chapter 1</a>, the main benefits of modern application architecture are scalability, reliability, and resiliency, and all three of those benefits require a container orchestration environment like Kubernetes in order to run many instances of containerized application components across many different servers and networks.</p>&#13;
<p class="indent">In this chapter, we’ll begin by looking at some cross-cutting concerns that exist when running containers across multiple servers in a cluster. We’ll then describe the core Kubernetes concepts designed to address those concerns. With that introduction complete, we’ll spend the bulk of the chapter actually installing a Kubernetes cluster, including important add-on components like networking and storage.</p>&#13;
<h3 class="h3" id="ch00lev1sec27"><span epub:type="pagebreak" id="page_88"/>Running Containers in a Cluster</h3>&#13;
<p class="noindent">The need to distribute our application components across multiple servers is not new to modern application architecture. To build a scalable and reliable application, we have always needed to take advantage of multiple servers to handle the application’s load and preclude a single point of failure. The fact that we are now running these components in containers does not change the need for multiple servers; we are still ultimately using CPUs and we are still ultimately dependent on hardware.</p>&#13;
<p class="indent">At the same time, a container orchestration environment brings challenges that may not have existed with other kinds of application infrastructure. When the container is the smallest individual module around which we build our system, we end up with application components that are much more self-contained and “opaque” from the perspective of our infrastructure. This means that instead of having a static application architecture through which we choose in advance what application components are assigned to specific servers, with Kubernetes, we try to make it possible for any container to run anywhere.</p>&#13;
<h4 class="h4" id="ch00lev2sec39">Cross-Cutting Concerns</h4>&#13;
<p class="noindent">The ability to run any container anywhere maximizes our flexibility, but it adds complexity to Kubernetes itself. Kubernetes does not know in advance what containers it will be asked to run, and the container workload is continuously changing as new applications are deployed or applications experience changes in load. To rise to this challenge, Kubernetes needs to account for the following design parameters that apply to all container orchestration software, no matter what containers are running:</p>&#13;
<div class="bqparan">&#13;
<p class="noindent5"><strong>Dynamic scheduling</strong> New containers must be allocated to a server, and allocations can change due to configuration changes or failures.</p>&#13;
<p class="noindent5"><strong>Distributed state</strong> The entire cluster must keep information about what containers are running and where, even during hardware or network failures.</p>&#13;
<p class="noindent5"><strong>Multitenancy</strong> It should be possible to run multiple applications in a single cluster, with isolation for security and reliability.</p>&#13;
<p class="noindent5"><strong>Hardware isolation</strong> Clusters must run in cloud environments and on regular servers of various types, isolating containers from the differences in these environments.</p>&#13;
</div>&#13;
<p class="indent">The best term to use to refer to these design parameters is <em>cross-cutting concern</em>, because they apply to any kind of containerized software that we might need to deploy, and even to the Kubernetes infrastructure itself. These parameters work together with the container orchestration requirements we saw in <a href="ch01.xhtml#ch01">Chapter 1</a> and ultimately drive the Kubernetes architecture and key design decisions.</p>&#13;
<h4 class="h4" id="ch00lev2sec40"><span epub:type="pagebreak" id="page_89"/>Kubernetes Concepts</h4>&#13;
<p class="noindent">To address these cross-cutting concerns, the Kubernetes architecture allows anything to come and go at any time. This includes not only the containerized applications deployed to Kubernetes, but also the fundamental software components of Kubernetes itself, and even the underlying hardware such as servers, network connections, and storage.</p>&#13;
<h5 class="h5" id="ch00lev3sec3">Separate Control Plane</h5>&#13;
<p class="noindent">Obviously, for Kubernetes to be a container orchestration environment, it requires the ability to run containers. This ability is provided by a set of worker machines called <em>nodes</em>. Each node runs a <em>kubelet</em> service that interfaces with the underlying container runtime to start and monitor containers.</p>&#13;
<p class="indent">Kubernetes also has a set of core software components that manage the worker nodes and their containers, but these software components are deployed separately from the worker nodes. These core Kubernetes software components are together referred to as the <em>control plane</em>. Because the control plane is separate from the worker nodes, the worker nodes can run the control plane, gaining the benefits of containerization for the Kubernetes core software components. A separate control plane also means that Kubernetes itself has a microservice architecture, which allows customization of each Kubernetes cluster. For example, one control plane component, the <em>cloud controller manager</em>, is used only when deploying Kubernetes to a cloud provider, and it’s customized based on the cloud provider used. This design provides hardware isolation for application containers and the rest of the Kubernetes control plane, while still allowing us to take advantage of the specific features of each cloud provider.</p>&#13;
<h5 class="h5" id="ch00lev3sec4">Declarative API</h5>&#13;
<p class="noindent">One critical component of the Kubernetes control plane is the <em>API server</em>. The API server provides an interface for cluster control and monitoring that other cluster users and control plane components use. In defining the API, Kubernetes could have chosen an <em>imperative</em> style, in which each API endpoint is a command such as “run a container” or “allocate storage.” Instead, the API is <em>declarative</em>, providing endpoints such as <em>create</em>, <em>patch</em>, <em>get</em>, and <em>delete</em>. The effect of these commands is to create, read, update, and delete <em>resources</em> from the cluster configuration—the specific configuration of each resource tells Kubernetes what we want the cluster to do.</p>&#13;
<p class="indent">This declarative API is essential to meet the cross-cutting concerns of dynamic scheduling and distributed state. Because a declarative API simply reports or updates cluster configuration, reacting to server or network failures that might cause a command to be missed is very easy. Consider an example in which the API server connection is lost just after an <code>apply</code> command is issued to change the cluster configuration. When the connection is restored, the client can simply query the cluster configuration and determine whether the command was received successfully. Or, even easier, the client can just issue the same <code>apply</code> command again, knowing that as long as the cluster configuration ends up as desired, Kubernetes will be trying to do <span epub:type="pagebreak" id="page_90"/>the “right thing” to the actual cluster. This core principle is known as <em>idempotence</em>, meaning it is safe to issue the same command multiple times because it will be applied at most once.</p>&#13;
<h5 class="h5" id="ch00lev3sec5">Self-Healing</h5>&#13;
<p class="noindent">Building on the declarative API, Kubernetes is designed to be <em>self-healing</em>. This means that the control plane components continually monitor both the cluster configuration and the actual cluster state and try to bring them into alignment. Every resource in the cluster configuration has an associated status and event log reflecting how the configuration has actually caused a change in the cluster state.</p>&#13;
<p class="indent">The separation of configuration and state makes Kubernetes very resilient. For example, a resource representing containers may be in a <code>Running</code> state if the containers have been scheduled and are actually running. If the Kubernetes control plane loses connection to the server on which the containers are running, it can immediately set the status to <code>Unknown</code> and then work to either reestablish connection or treat the node as failed and reschedule the containers.</p>&#13;
<p class="indent">At the same time, using a declarative API and self-healing approach has important implications. Because the Kubernetes API is declarative, a “success” response to a command means only that the cluster configuration was updated. It does not mean that the actual state of the cluster was updated, as it might take time to achieve the requested state, or there might be issues that prevent the cluster from achieving that state. As a result, we cannot assume that just because we created the appropriate resources, the cluster is running the containers we expect. Instead, we must watch the status of the resources and explore the event log to diagnose any issues that the Kubernetes control plane had in making the actual cluster state match the configuration we specified.</p>&#13;
<h3 class="h3" id="ch00lev1sec28">Cluster Deployment</h3>&#13;
<p class="noindent">With some core Kubernetes concepts under our belts, we’ll use the <code>kubeadm</code> Kubernetes administration tool to deploy a highly available Kubernetes cluster across multiple virtual machines.</p>&#13;
<div class="box5">&#13;
<p class="boxtitle-d"><strong>CHOOSING A KUBERNETES DISTRIBUTION</strong></p>&#13;
<p class="noindents">Rather than using a particular Kubernetes distribution as we did in <a href="ch01.xhtml#ch01">Chapter 1</a>, we’ll deploy a “vanilla” Kubernetes cluster using the generic upstream repository. This approach gives us the best opportunity to follow along with the cluster deployment and will make it easier to explore the cluster in-depth in the next several chapters. However, when you’re ready to deploy a Kubernetes cluster of your own, especially for production work, consider using a prebuilt Kubernetes distribution for ease of management and built-in security. The Cloud <span epub:type="pagebreak" id="page_91"/>Native Computing Foundation (CNCF) publishes a set of conformance tests that you can use to ensure that the Kubernetes distribution you choose is conformant to the Kubernetes specification.</p>&#13;
</div>&#13;
<p class="indent">Our Kubernetes cluster will be split across four virtual machines, labeled <code>host01</code> through <code>host04</code>. Three of these, <code>host01</code> through <code>host03</code>, will run control plane components, whereas the fourth will act solely as a worker node. We’ll have three control plane nodes because that is the smallest number required to run a highly available cluster. Kubernetes uses a voting scheme to provide failover, and at least three control plane nodes are required; this allows the cluster to detect which side should keep running in the event of a network failure. Also, to keep the cluster as small as possible for our examples, we’ll configure Kubernetes to run regular containers on the control plane nodes even though we would avoid doing that for a production cluster.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The example repository for this book is at</em> <a href="https://github.com/book-of-kubernetes/examples">https://github.com/book-of-kubernetes/examples</a>. <em>See “Running Examples” on <a href="ch00.xhtml#ch00lev1sec2">page xx</a> for details on getting set up.</em></p>&#13;
</div>&#13;
<p class="indent">Start by following the instructions for this chapter to get all four virtual machines up and running, either in Vagrant or AWS. The automated provisioning will set up all four machines with <code>containerd</code> and <code>crictl</code>, so we don’t need to do it manually. The automated provisioning script will also set up either <code>kube-vip</code> or an AWS network load balancer to provide required high-availability functionality, as discussed below.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>You can install Kubernetes automatically using the</em> <span class="codeitalic">extra</span> <em>provisioning script provided with this chapter’s examples. See the README file for this chapter for instructions.</em></p>&#13;
</div>&#13;
<p class="indent">You’ll need to run commands on each of the four virtual machines, so you might want to open terminal tabs for each one. However, the first series of commands needs to be run on all of the hosts, so the automation sets up a command called <code>k8s-all</code> to do that from <code>host01</code>. You can explore the content of this script in <em>/usr/local/bin/k8s-all</em> or by looking at the <em>k8s</em> Ansible role in the <em>setup</em> directory of the examples.</p>&#13;
<h4 class="h4" id="ch00lev2sec41">Prerequisite Packages</h4>&#13;
<p class="noindent">The first step is to make sure the <code>br_netfilter</code> kernel module is enabled and set to load on boot. Kubernetes uses advanced features of the Linux firewall to handle networking across the cluster, so we need this module. Run these two commands:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">k8s-all modprobe br_netfilter</span>&#13;
...&#13;
root@host01:~# <span class="codestrong1">k8s-all "echo 'br_netfilter' &gt; /etc/modules-load.d/k8s.conf"</span></pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_92"/>The first command ensures that the module is installed for the currently running kernel, and the second command adds it to the list of modules to run on boot. The slightly odd quoting in the second command ensures that the shell redirection happens on the remote hosts.</p>&#13;
<p class="indent">Next, in <a href="ch06.xhtml#ch06list1">Listing 6-1</a>, we’ll set some Linux kernel parameters to enable advanced network features that are also needed for networking across the cluster by using the <code>sysctl</code> command:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">k8s-all sysctl -w net.ipv4.ip_forward=1 \</span>&#13;
  <span class="codestrong1">net.bridge.bridge-nf-call-ip6tables=1 \</span>&#13;
  <span class="codestrong1">net.bridge.bridge-nf-call-iptables=1</span></pre>&#13;
<p class="caption" id="ch06list1"><em>Listing 6-1: Kernel settings</em></p>&#13;
<p class="indent">This command enables the following Linux kernel network features:</p>&#13;
<div class="bqparan">&#13;
<p class="noindent5"><span class="codestrong">net.ipv4.ip_forward</span> Transfer packets from one network interface to another (for example, from an interface inside a container’s network namespace to a host network).</p>&#13;
<p class="noindent5"><span class="codestrong">net.bridge.bridge-nf-call-ip6tables</span> Run IPv6 bridge traffic through the <code>iptables</code> firewall.</p>&#13;
<p class="noindent5"><span class="codestrong">net.bridge.bridge-nf-call-iptables</span> Run IPv4 bridge traffic through the <code>iptables</code> firewall.</p>&#13;
</div>&#13;
<p class="indent">The need for the last two items will become clear in <a href="ch09.xhtml#ch09">Chapter 9</a> when we discuss how Kubernetes provides networking for Services.</p>&#13;
<p class="indent">These <code>sysctl</code> changes in <a href="ch06.xhtml#ch06list1">Listing 6-1</a> do not persist after a reboot. The automated scripts do handle making the changes persistent, so if you reboot your virtual machines, either run the <code>extra</code> provisioning script, or run these commands again.</p>&#13;
<p class="indent">We’ve now finished configuring the Linux kernel to support our Kubernetes deployment and are almost ready for the actual install. First we need to install some prerequisite packages:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">k8s-all apt install -y apt-transport-https \</span>&#13;
  <span class="codestrong1">open-iscsi nfs-common</span></pre>&#13;
<p class="indent">The <code>apt-transport-https</code> package ensures that <code>apt</code> can support connecting to repositories via secure HTTP. The other two packages are needed for one of the cluster add-ons that we’ll install after our cluster is up and running.</p>&#13;
<h4 class="h4" id="ch00lev2sec42">Kubernetes Packages</h4>&#13;
<p class="noindent">We can now add the Kubernetes repository to install the <code>kubeadm</code> tool that will set up our cluster. First, add the GPG key used to check the package signatures:</p>&#13;
<pre><span epub:type="pagebreak" id="page_93"/>root@host01:~# <span class="codestrong1">k8s-all "curl -fsSL \</span>&#13;
  <span class="codestrong1">https://packages.cloud.google.com/apt/doc/apt-key.gpg | \</span>&#13;
  <span class="codestrong1">gpg --dearmor -o /usr/share/keyrings/google-cloud-keyring.gpg"</span></pre>&#13;
<p class="indent">This command uses <code>curl</code> to download the GPG key. It then uses <code>gpg</code> to reformat it, and then it writes the result to <em>/usr/share/keyrings</em>. The command line flags <code>fsSL</code> put <code>curl</code> in a mode that behaves better for chained commands, including avoiding unnecessary output, following server redirects, and terminating with an error if there is a problem.</p>&#13;
<p class="indent">Next, we add the repository configuration:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">k8s-all "echo 'deb [arch=amd64' \</span>&#13;
  <span class="codestrong1">'signed-by=/usr/share/keyrings/google-cloud-keyring.gpg]' \</span>&#13;
  <span class="codestrong1">'https://apt.kubernetes.io/ kubernetes-xenial main' &gt; \</span>&#13;
  <span class="codestrong1">/etc/apt/sources.list.d/kubernetes.list"</span></pre>&#13;
<p class="indent">As before, the quoting is essential to ensure that the command is passed correctly via SSH to all the other hosts in the cluster. The command configures <code>kubernetes-xenial</code> as the distribution; this distribution is used for any version of Ubuntu, starting with the older Ubuntu Xenial.</p>&#13;
<p class="indent">After we have created this new repository, we then need to run <code>apt update</code> on all hosts to download the list of packages:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">k8s-all apt update</span>&#13;
...</pre>&#13;
<p class="indent">Now we can install the packages we need using <code>apt</code>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">source /opt/k8sver</span>&#13;
root@host01:~# <span class="codestrong1">k8s-all apt install -y kubelet=$K8SV kubeadm=$K8SV kubectl=$K8SV</span></pre>&#13;
<p class="indent">The <code>source</code> command loads a file with a variable to install a specific Kubernetes version. This file is created by the automated scripts and ensures that we use a consistent Kubernetes version for all chapters. You can update the automated scripts to choose which Kubernetes version to install.</p>&#13;
<p class="indent">The <code>apt</code> command installs the following three packages along with some dependencies:</p>&#13;
<div class="bqparan">&#13;
<p class="noindent5"><span class="codestrong">kubelet</span> Service for all worker nodes that interfaces with the container engine to run containers as scheduled by the control plane</p>&#13;
<p class="noindent5"><span class="codestrong">kubeadm</span> Administration tool that we’ll use to install Kubernetes and maintain our cluster</p>&#13;
<p class="noindent5"><span class="codestrong">kubectl</span> Command line client that we’ll use to inspect our Kubernetes cluster and to create and delete resources</p>&#13;
</div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_94"/>The <code>kubelet</code> package starts its service immediately, but because we haven’t installed the control plane yet, the service will be in a failed state at first:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">systemctl status kubelet</span>&#13;
  kubelet.service - kubelet: The Kubernetes Node Agent&#13;
...&#13;
   Main PID: 75368 (code=exited, status=1/FAILURE)</pre>&#13;
<p class="indent">We need to control the version of the packages we just installed because we want to upgrade all of the components of our cluster together. To protect ourselves from accidentally updating these packages, we’ll hold them at their current version:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">k8s-all apt-mark hold kubelet kubeadm kubectl</span></pre>&#13;
<p class="indent">This command prevents the standard <code>apt full-upgrade</code> command from updating these packages. Instead, if we upgrade our cluster, we’ll need to specify the exact version that we want by using <code>apt install</code>.</p>&#13;
<h4 class="h4" id="ch00lev2sec43">Cluster Initialization</h4>&#13;
<p class="noindent">The next command, <code>kubeadm init</code>, initializes the control plane and provides the <code>kubelet</code> worker node service configuration for all the nodes. We’ll run <code>kubeadm init</code> on one node in our cluster and then use <code>kubeadm join</code> on each of the other nodes so that they join the existing cluster.</p>&#13;
<p class="indent">To run <code>kubeadm init</code>, we first create a YAML configuration file. This approach has a few advantages. It greatly shortens the number of command line flags that we need to remember, and it lets us keep the cluster configuration in a repository, giving us configuration control over the cluster. We then can update the YAML file and rerun <code>kubeadm</code> to make cluster configuration changes.</p>&#13;
<p class="indent">The automation scripts for this chapter have populated a YAML configuration file in <em>/etc/kubernetes</em>, so it’s ready to use. The following shows the contents of that file:</p>&#13;
<p class="noindent6"><em>kubeadm-init.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: kubeadm.k8s.io/v1beta3&#13;
kind: InitConfiguration&#13;
bootstrapTokens:&#13;
- groups:&#13;
  - system:bootstrappers:kubeadm:default-node-token&#13;
  token: 1d8fb1.2875d52d62a3282d&#13;
  ttl: 2h0m0s&#13;
  usages:&#13;
  - signing&#13;
  - authentication&#13;
nodeRegistration:&#13;
  kubeletExtraArgs:&#13;
    node-ip: 192.168.61.11&#13;
<span epub:type="pagebreak" id="page_95"/>  taints: []&#13;
localAPIEndpoint:&#13;
  advertiseAddress: 192.168.61.11&#13;
certificateKey: "5a7e07816958efb97635e9a66256adb1"&#13;
---&#13;
apiVersion: kubeadm.k8s.io/v1beta3&#13;
kind: ClusterConfiguration&#13;
kubernetesVersion: 1.21.4&#13;
apiServer:&#13;
  extraArgs:&#13;
    service-node-port-range: 80-32767&#13;
networking:&#13;
  podSubnet: "172.31.0.0/16"&#13;
controlPlaneEndpoint: "192.168.61.10:6443"&#13;
---&#13;
apiVersion: kubelet.config.k8s.io/v1beta1&#13;
kind: KubeletConfiguration&#13;
serverTLSBootstrap: true</pre>&#13;
<p class="indent">This YAML file has three documents, separated by dashes (<code>---</code>). The first document is specific to initializing the cluster, the second has more generic configuration, and the third is used to provide settings for <code>kubelet</code> across all the nodes. Let’s look at the purpose of each of these configuration items:</p>&#13;
<div class="bqparan">&#13;
<p class="noindent5"><span class="codestrong">apiVersion / kind</span> Tells Kubernetes about the purpose of each YAML document, so it can validate the contents.</p>&#13;
<p class="noindent5"><span class="codestrong">bootstrapTokens</span> Configures a secret that other nodes can use to join the cluster. The <code>token</code> should be kept secret in a production cluster. It is set to expire automatically after two hours, so if we want to join more nodes later, we’ll need to make another one.</p>&#13;
<p class="noindent5"><span class="codestrong">nodeRegistration</span> Configuration to pass to the <code>kubelet</code> service running on <code>host01</code>. The <code>node-ip</code> field ensures that <code>kubelet</code> registers the correct IP address with the API server so that the API server can communicate with it. The <code>taints</code> field ensures that regular containers can be scheduled onto control plane nodes.</p>&#13;
<p class="noindent5"><span class="codestrong">localAPIEndpoint</span> The local IP address that the API server should use. Our virtual machine has multiple IP addresses, and we want the API server listening on the correct network.</p>&#13;
<p class="noindent5"><span class="codestrong">certificateKey</span> Configures a secret that other nodes will use to gain access to the certificates for the API server. It’s needed so that all of the API server instances in our highly available cluster can use the same certificate. Keep it secret in a production cluster.</p>&#13;
<p class="noindent5"><span class="codestrong">networking</span> All containers in the cluster will get an IP address from the <code>podSubnet</code>, no matter what host they run on. Later, we’ll install a network driver that will ensure that every container on all hosts in the cluster can communicate.</p>&#13;
<p class="noindent5"><span epub:type="pagebreak" id="page_96"/><span class="codestrong">controlPlaneEndpoint</span> The API server’s external address. For a highly available cluster, this IP address needs to reach any API server instance, not just the first one.</p>&#13;
<p class="noindent5"><span class="codestrong">serverTLSBootstrap</span> Instructs <code>kubelet</code> to use the controller manager’s certificate authority to request server certificates.</p>&#13;
</div>&#13;
<p class="indent">The <code>apiVersion</code> and <code>kind</code> fields will appear in every Kubernetes YAML file. The <code>apiVersion</code> field defines a group of related Kubernetes resources, including a version number. The <code>kind</code> field then selects the specific resource type within that group. This not only allows the Kubernetes project and other vendors to add new groups of resources over time, but it also allows updates to existing resource specifications while maintaining backward compatibility.</p>&#13;
<div class="box5">&#13;
<p class="boxtitle-d"><strong>HIGHLY AVAILABLE CLUSTERS</strong></p>&#13;
<p class="noindents">The <code>controlPlaneEndpoint</code> field is used to configure the most important requirement for a highly available cluster: an IP address that reaches all of the API servers. We need to establish this IP address immediately when we initialize the cluster because it is used to generate certificates with which clients will verify the API server’s identity. The best way to provide a cluster-wide IP address depends on where the cluster is running; for example, in a cloud environment, using the provider’s built-in capability, such as an Elastic Load Balancer (ELB) in Amazon Web Services or an Azure Load Balancer, is best.</p>&#13;
<p class="noindents">Because of the nature of the two different environments, the examples for this book use <code>kube-vip</code> when running with Vagrant, and ELB when running in Amazon Web Services. The top-level <em>README.md</em> file in the example documentation has more details. The installation and configuration is done automatically so there’s nothing more to configure. We can just use <code>192.168.61.10:6443</code> and expect traffic to get to any of the API server instances running on <code>host01</code> through <code>host03</code>.</p>&#13;
</div>&#13;
<p class="indent">Because we have the cluster configuration ready to go in a YAML file, the <code>kubeadm init</code> command to initialize the cluster is simple. We run this command solely on <code>host01</code>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">/usr/bin/kubeadm init \</span>&#13;
  <span class="codestrong1">--config /etc/kubernetes/kubeadm-init.yaml --upload-certs</span></pre>&#13;
<p class="indent">The <code>--config</code> option points to the YAML configuration file (<em>kubeadm-init.yaml</em>) that we looked at earlier, and the <code>--upload-certs</code> option tells <code>kubeadm</code> that it should upload the API server’s certificates to the cluster’s distributed storage. The other control plane nodes then can download those certificates when they join the cluster, allowing all API server instances to use the same certificates so that clients will trust them. The certificates are encrypted using the <code>certificateKey</code> we provided, which means that the other nodes will need this key to decrypt them.</p>&#13;
<p class="indent">The <code>kubeadm init</code> command initializes the control plane’s components on <code>host01</code>. These components are run in containers and managed by the <code>kubelet</code> <span epub:type="pagebreak" id="page_97"/>service, which makes them easy to upgrade. Several container images will be downloaded, so the command might take a while, depending on the speed of your virtual machines and your internet connection.</p>&#13;
<h4 class="h4" id="ch00lev2sec44">Joining Nodes to the Cluster</h4>&#13;
<p class="noindent">The <code>kubeadm init</code> command prints out a <code>kubeadm join</code> command that we can use to join other nodes to the cluster. However, the automation scripts have already prestaged a configuration file to each of the other nodes to ensure that they join as the correct type of node. The servers <code>host02</code> and <code>host03</code> will join as additional control plane nodes, whereas <code>host04</code> will join solely as a worker node.</p>&#13;
<p class="indent">Here’s the YAML configuration file for <code>host02</code> with its specific settings:</p>&#13;
<p class="noindent6"><em>kubeadm-join.yaml (host02)</em></p>&#13;
<pre>---&#13;
apiVersion: kubeadm.k8s.io/v1beta3&#13;
kind: JoinConfiguration&#13;
discovery:&#13;
  bootstrapToken:&#13;
    apiServerEndpoint: 192.168.61.10:6443&#13;
    token: 1d8fb1.2875d52d62a3282d&#13;
    unsafeSkipCAVerification: true&#13;
  timeout: 5m0s&#13;
nodeRegistration:&#13;
  kubeletExtraArgs:&#13;
    cgroup-driver: containerd&#13;
    node-ip: 192.168.61.12&#13;
  taints: []&#13;
  ignorePreflightErrors:&#13;
    - DirAvailable--etc-kubernetes-manifests&#13;
controlPlane:&#13;
  localAPIEndpoint:&#13;
    advertiseAddress: 192.168.61.12&#13;
  certificateKey: "5a7e07816958efb97635e9a66256adb1"</pre>&#13;
<p class="indent">This resource has a type of <code>JoinConfiguration</code>, but most of the fields are the same as the <code>InitConfiguration</code> in the <em>kubeadm-init.yaml</em> file. Most important, the <code>token</code> and <code>certificateKey</code> match the secret we set up earlier, so this node will be able to validate itself with the cluster and decrypt the API server certificates.</p>&#13;
<p class="indent">One difference is the addition of <code>ignorePreflightErrors</code>. This section appears only when we are installing <code>kube-vip</code>, as in that case we need to prestage the configuration file for <code>kube-vip</code> to the <em>/etc/kubernetes/manifests</em> directory, and we need to tell <code>kubeadm</code> that it is okay for that directory to already exist.</p>&#13;
<p class="indent">Because we have this YAML configuration file, the <code>kubeadm join</code> command is simple. Run it on <code>host02</code>:</p>&#13;
<pre>root@host02:~# <span class="codestrong1">/usr/bin/kubeadm join --config /etc/kubernetes/kubeadm-join.yaml</span></pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_98"/>As before, this command runs the control plane components as containers using the <code>kubelet</code> service on this node, so it will take some time to download the container images and start the containers.</p>&#13;
<p class="indent">When it finishes, run the exact same command on <code>host03</code>:</p>&#13;
<pre>root@host03:~# <span class="codestrong1">/usr/bin/kubeadm join --config /etc/kubernetes/kubeadm-join.yaml</span></pre>&#13;
<p class="indent">The automation script set up the YAML file with the correct IP address for each host, so the differences in configuration between each of the hosts is already accounted for.</p>&#13;
<p class="indent">When this command completes, we’ll have created a highly available Kubernetes cluster, with the control plane components running on three separate hosts. However, we do not yet have any regular worker nodes. Let’s fix that issue.</p>&#13;
<p class="indent">We’ll begin by joining <code>host04</code> as a regular worker node and running exactly the same <code>kubeadm join</code> command on <code>host04</code>, but the YAML configuration file will be a little different. Here’s that file:</p>&#13;
<p class="noindent6"><em>kubeadm-join.yaml (host04)</em></p>&#13;
<pre>---&#13;
apiVersion: kubeadm.k8s.io/v1beta3&#13;
kind: JoinConfiguration&#13;
discovery:&#13;
  bootstrapToken:&#13;
    apiServerEndpoint: 192.168.61.10:6443&#13;
    token: 1d8fb1.2875d52d62a3282d&#13;
    unsafeSkipCAVerification: true&#13;
  timeout: 5m0s&#13;
nodeRegistration:&#13;
  kubeletExtraArgs:&#13;
    cgroup-driver: containerd&#13;
    node-ip: 192.168.61.14&#13;
  taints: []</pre>&#13;
<p class="indent">This YAML file is missing the <code>controlPlane</code> field, so <code>kubeadm</code> configures it as a regular worker node rather than a control plane node.</p>&#13;
<p class="indent">Now let’s join <code>host04</code> to the cluster:</p>&#13;
<pre>root@host04:~# <span class="codestrong1">/usr/bin/kubeadm join --config /etc/kubernetes/kubeadm-join.yaml</span></pre>&#13;
<p class="indent">This command completes a little faster because it doesn’t need to download the control plane container images and run them. We now have four nodes in the cluster, which we can verify by running <code>kubectl</code> back on <code>host01</code>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">export KUBECONFIG=/etc/kubernetes/admin.conf</span>&#13;
root@host01:~# <span class="codestrong1">kubectl get nodes</span>&#13;
NAME     STATUS     ROLES        ...&#13;
host01   NotReady   control-plane...&#13;
host02   NotReady   control-plane...&#13;
<span epub:type="pagebreak" id="page_99"/>host03   NotReady   control-plane...&#13;
host04   NotReady   &lt;none&gt;       ...</pre>&#13;
<p class="indent">The first command sets an environment variable to tell <code>kubectl</code> what configuration file to use. The <em>/etc/kubernetes/admin.conf</em> file was created automatically by <code>kubeadm</code> when it initialized <code>host01</code> as a control plane node. That file tells <code>kubectl</code> what address to use for the API server, what certificate to use to verify the secure connection, and how to authenticate.</p>&#13;
<p class="indent">The four nodes currently should be reporting a status of <code>NotReady</code>. Let’s run the <code>kubectl describe</code> command to get the node details:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl describe node host04</span>&#13;
Name:               host04&#13;
...&#13;
Conditions:&#13;
  Type   Status ... Message&#13;
  ----   ------ ... -------&#13;
  Ready  False  ... container runtime network not ready...&#13;
...</pre>&#13;
<p class="indent">We haven’t yet installed a network driver for our Kubernetes cluster, and as a result, all of the nodes are reporting a status of <code>NotReady</code>, which means that they won’t accept regular application workloads. Kubernetes communicates this by placing a <em>taint</em> in the node’s configuration. A taint restricts what can be scheduled on a node. We can list the taints on the nodes using <code>kubectl</code>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get node -o json | \</span>&#13;
  <span class="codestrong1">jq '.items[]|.metadata.name,.spec.taints[]'</span>&#13;
"host01"&#13;
{&#13;
  "effect": "NoSchedule",&#13;
  "key": "node.kubernetes.io/not-ready"&#13;
}&#13;
"host02"&#13;
{&#13;
  "effect": "NoSchedule",&#13;
  "key": "node.kubernetes.io/not-ready"&#13;
}&#13;
"host03"&#13;
{&#13;
  "effect": "NoSchedule",&#13;
  "key": "node.kubernetes.io/not-ready"&#13;
}&#13;
"host04"&#13;
<span epub:type="pagebreak" id="page_100"/>{&#13;
  "effect": "NoSchedule",&#13;
  "key": "node.kubernetes.io/not-ready"&#13;
}</pre>&#13;
<p class="indent">We select an output format of <code>json</code> so that we can use <code>jq</code> to print just the information we need. Because all the nodes have a status of <code>NotReady</code>, they have a <code>not-ready</code> taint set to <code>NoSchedule</code>, which prevents the Kubernetes scheduler from scheduling containers onto them.</p>&#13;
<p class="indent">By specifying <code>taints</code> as an empty array in the <code>kubeadm</code> configuration, we prevented the three control plane nodes from having an additional control plane taint. In a production cluster, this taint keeps application containers separate from the control plane containers for security reasons, so we would leave it in place. For our example cluster, though, it would mean that we need multiple extra virtual machines to act as worker nodes, which we don’t want.</p>&#13;
<p class="indent">The command <code>kubectl taint</code> would allow us to remove the <code>not-ready</code> taint manually, but the correct approach is to install a network driver as a cluster add-on so that the nodes will properly report <code>Ready</code>, enabling us to run containers on them.</p>&#13;
<h3 class="h3" id="ch00lev1sec29">Installing Cluster Add-ons</h3>&#13;
<p class="noindent">We’ve installed <code>kubelet</code> on four separate nodes and installed the control plane on three of those nodes and joined them to our cluster. For the rest, we’ll use the control plane to install cluster add-ons. These add-ons are similar to regular applications that we would deploy. They consist of Kubernetes resources and run in containers, but they provide essential services to the cluster that our applications will use.</p>&#13;
<p class="indent">To get a basic cluster up and running, we need to install three types of add-ons: a <em>network driver</em>, a <em>storage driver</em>, and an <em>ingress controller</em>. We will also install a fourth optional add-on, a <em>metrics server</em>.</p>&#13;
<h4 class="h4" id="ch00lev2sec45">Network Driver</h4>&#13;
<p class="noindent">Kubernetes networking is based on the Container Network Interface (CNI) standard. Anyone can build a new network driver for Kubernetes by implementing this standard, and as a result, several choices are available for Kubernetes network drivers. We’ll demonstrate different network plug-ins in <a href="ch08.xhtml#ch08">Chapter 8</a>, but most of the clusters in this book use the Calico network driver because it is the default choice for many Kubernetes platforms.</p>&#13;
<p class="indent">First, download the primary YAML configuration file for Calico:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">cd /etc/kubernetes/components</span>&#13;
root@host01:/etc/kubernetes/components# <span class="codestrong1">curl -L -O $calico_url</span>&#13;
...</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_101"/>The <code>-L</code> option tells <code>curl</code> to follow any HTTP redirects, whereas the <code>-O</code> option tells <code>curl</code> to save the content in a file using the same filename as in the URL. The value of the <code>calico_url</code> environment variable is set in the <code>k8s-ver</code> script that also specified the Kubernetes version. This is essential, as Calico is sensitive to the specific version of Kubernetes we’re running, so it’s important to choose values that are compatible.</p>&#13;
<p class="indent">The primary YAML configuration is written to the local file <em>tigera-operator.yaml</em>. This refers to the fact that the initial installation is a Kubernetes Operator, which then creates all of the other cluster resources to install Calico. We’ll explore operators in <a href="ch17.xhtml#ch17">Chapter 17</a>.</p>&#13;
<p class="indent">In addition to this primary YAML configuration, the automated scripts for this chapter have added a file called <em>custom-resources.yaml</em> that provides necessary configuration for our example cluster. We now can tell the Kubernetes API server to apply all the resources in these files to the cluster:</p>&#13;
<pre>root@host01:/etc/kubernetes/components# <span class="codestrong1">kubectl apply -f tigera-operator.yaml</span>&#13;
...&#13;
root@host01:/etc/kubernetes/components# <span class="codestrong1">kubectl apply -f custom-resources.yaml</span></pre>&#13;
<p class="indent">Kubernetes takes a few minutes to download container images and start containers, and then Calico will be running in our cluster and our nodes should report a status of <code>Ready</code>:</p>&#13;
<pre>root@host01:/etc/kubernetes/components# <span class="codestrong1">kubectl get nodes</span>&#13;
NAME     STATUS   ROLES                ...&#13;
host01   Ready    control-plane,master ...&#13;
host02   Ready    control-plane,master ...&#13;
host03   Ready    control-plane,master ...&#13;
host04   Ready    &lt;none&gt;               ...</pre>&#13;
<p class="indent">Calico works by installing a <em>DaemonSet</em>, a Kubernetes resource that tells the cluster to run a specific container or set of containers on every node. The Calico containers then provide network services for any containers running on that node. However, that raises an important question. When we installed Calico in our cluster, all of our nodes had a taint that told Kubernetes not to schedule containers on them. How was Calico able to run its containers on all the nodes? The answer is <em>tolerations</em>.</p>&#13;
<p class="indent">A toleration is a configuration setting applied to a resource that instructs Kubernetes it can be scheduled on a node despite a taint possibly being present. Calico specifies a toleration when it adds its DaemonSet to the cluster, as we can see with <code>kubectl</code>:</p>&#13;
<pre>root@host01:/etc/kubernetes/components# <span class="codestrong1">kubectl -n calico-system \</span>&#13;
  <span class="codestrong1">get daemonsets -o json | \</span>&#13;
  <span class="codestrong1">jq '.items[].spec.template.spec.tolerations[]'</span>&#13;
{&#13;
  "key": "CriticalAddonsOnly",&#13;
  "operator": "Exists"&#13;
}&#13;
<span epub:type="pagebreak" id="page_102"/>{&#13;
  "effect": "NoSchedule",&#13;
  "operator": "Exists"&#13;
}&#13;
{&#13;
  "effect": "NoExecute",&#13;
  "operator": "Exists"&#13;
}</pre>&#13;
<p class="indent">The <code>-n</code> option selects the <code>calico-system</code> <em>Namespace</em>. Namespaces are a way to keep Kubernetes resources separate from one another on a cluster, for security reasons as well as to avoid naming collisions. Also, as before, we request JSON output and use <code>jq</code> to select only the field we’re interested in. If you want to see the entire configuration for the resource, use <code>-o=json</code> without <code>jq</code> or use <code>-o=yaml</code>.</p>&#13;
<p class="indent">This DaemonSet has three tolerations, and the second one provides the behavior we need. It tells the Kubernetes scheduler to go ahead and schedule it even on nodes that have a <code>NoSchedule</code> taint. Calico then can get itself started before the node is ready, and once it’s running, the node changes its status to <code>Ready</code> so that normal application containers can be scheduled. The control plane components needed a similar toleration in order to run on nodes before they show <code>Ready</code>.</p>&#13;
<h4 class="h4" id="ch00lev2sec46">Installing Storage</h4>&#13;
<p class="noindent">The cluster nodes are ready, so if we deployed a regular application, its containers would run. However, applications that require persistent storage would fail to start because the cluster doesn’t yet have a storage driver. Like network drivers, several storage drivers are available for Kubernetes. The Container Storage Interface (CSI) provides the standard that storage drivers need to meet to work with Kubernetes. We’ll use Longhorn, a storage driver from Rancher; it’s easy to install and doesn’t require any underlying hardware like extra block devices or access to cloud-based storage.</p>&#13;
<p class="indent">Longhorn makes use of the iSCSI and NFS software we installed earlier. It expects all of our nodes to have the <code>iscsid</code> service enabled and running, so let’s make sure that’s true on all our nodes:</p>&#13;
<pre>root@host01:/etc/kubernetes/components# <span class="codestrong1">k8s-all systemctl enable --now iscsid</span></pre>&#13;
<p class="indent">We now can install Longhorn on the cluster. The process for installing Longhorn looks a lot like Calico. Start by downloading the Longhorn YAML configuration:</p>&#13;
<pre>root@host01:/etc/kubernetes/components# <span class="codestrong1">curl -LO $longhorn_url</span></pre>&#13;
<p class="indent">The <code>longhorn_url</code> environment variable is also set by the <code>k8s-ver</code> script, which allows us to ensure compatibility.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_103"/>Install Longhorn using <code>kubectl</code>:</p>&#13;
<pre>root@host01:/etc/kubernetes/components# <span class="codestrong1">kubectl apply -f longhorn.yaml</span></pre>&#13;
<p class="indent">As before, <code>kubectl apply</code> ensures that the resources in the YAML file are applied to the cluster, creating or updating them as necessary. The <code>kubectl apply</code> command supports URLs as the source of the resource it applies to the cluster, but for these three installs, we run a separate <code>curl</code> command because it’s convenient to have a local copy of what was applied to the cluster.</p>&#13;
<p class="indent">Longhorn is now installed on the cluster, which we’ll verify as we explore the cluster in the rest of this chapter.</p>&#13;
<h4 class="h4" id="ch00lev2sec47">Ingress Controller</h4>&#13;
<p class="noindent">We now have networking and storage, but the networking allows access to containers only from within our cluster. We need another service that exposes our containerized applications outside the cluster. The easiest way to do that is to use an ingress controller. As we’ll describe in <a href="ch09.xhtml#ch09">Chapter 9</a>, an ingress controller watches the Kubernetes cluster for <em>Ingress</em> resources and routes network traffic.</p>&#13;
<p class="indent">We begin by downloading the ingress controller YAML configuration:</p>&#13;
<pre>root@host01:/etc/kubernetes/components# <span class="codestrong1">curl -Lo ingress-controller.yaml</span>&#13;
  <span class="codestrong1">$ingress_url</span></pre>&#13;
<p class="indent">As in our earlier example, the <code>ingress_url</code> environment variable is set by the <code>k8s-ver</code> script so that we can ensure compatibility. In this case, the URL ends in the generic path of <em>deploy.yaml</em>, so we use <code>-o</code> to provide a filename to <code>curl</code> to make clear the purpose of the downloaded YAML file.</p>&#13;
<p class="indent">Install the ingress controller using <code>kubectl</code>:</p>&#13;
<pre>root@host01:/etc/kubernetes/components# <span class="codestrong1">kubectl apply -f ingress-controller.yaml</span></pre>&#13;
<p class="indent">This creates a lot of resources, but there are two main parts: an NGINX web server that actually performs routing of HTTP traffic, and a component that watches for changes in Ingress resources in the cluster and configures NGINX accordingly.</p>&#13;
<p class="indent">There’s one more step we need. As installed, the ingress controller tries to request an external IP address to allow traffic to reach it from outside the cluster. Because we’re running a sample cluster with no access to external IP addresses, this won’t work. Instead, we’ll be accessing our ingress controller using port forwarding from our cluster hosts. At the moment, our ingress controller is set up for this port forwarding, but it’s using a random port. We would like to select the port to be sure that we know where to find the ingress controller. At the same time, we’ll also add an annotation so that this ingress controller will be the default for this cluster.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_104"/>To apply the port changes, we’re going to provide our Kubernetes cluster an with extra YAML configuration with just the changes we need. Here’s that YAML:</p>&#13;
<p class="noindent6"><em>ingress-patch.yaml</em></p>&#13;
<pre>---&#13;
apiVersion: v1&#13;
kind: Service&#13;
metadata:&#13;
  name: ingress-nginx-controller&#13;
  namespace: ingress-nginx&#13;
spec:&#13;
  ports:&#13;
    - port: 80&#13;
      nodePort: 80&#13;
    - port: 443&#13;
      nodePort: 443</pre>&#13;
<p class="indent">This file specifies the name and Namespace of the Service to ensure that Kubernetes knows where to apply these changes. It also specifies the <code>port</code> configuration we’re updating, along with the <code>nodePort</code>, which is the port on our cluster nodes that will be used for port forwarding. We’ll look at NodePort service types and port forwarding in more detail in <a href="ch09.xhtml#ch09">Chapter 9</a>.</p>&#13;
<p class="indent">To patch the service, we use the <code>kubectl patch</code> command:</p>&#13;
<pre>root@host01:/etc/kubernetes/components# <span class="codestrong1">kubectl patch -n ingress-nginx \</span>&#13;
  <span class="codestrong1">service/ingress-nginx-controller --patch-file ingress-patch.yaml</span>&#13;
service/ingress-nginx-controller patched</pre>&#13;
<p class="indent">To apply the annotation, use the <code>kubectl annotate</code> command:</p>&#13;
<pre>root@host01:/etc/kubernetes/components# <span class="codestrong1">kubectl annotate -n ingress-nginx \</span>&#13;
  <span class="codestrong1">ingressclass/nginx ingressclass.kubernetes.io/is-default-class="true"</span>&#13;
ingressclass.networking.k8s.io/nginx annotated</pre>&#13;
<p class="indent">Kubernetes reports the change to each resource as we make it, so we know that our changes have been applied.</p>&#13;
<h4 class="h4" id="ch00lev2sec48">Metrics Server</h4>&#13;
<p class="noindent">Our final add-on is a <em>metrics server</em> that collects utilization metrics from our nodes, enabling the use of autoscaling. To do this, it needs to connect to the <code>kubelet</code> instances in our cluster. For security, it needs to verify the HTTP/S certificate when it connects to a <code>kubelet</code>. This is why we configured <code>kubelet</code> to request a certificate signed by the controller manager rather than allowing the <code>kubelet</code> to generate self-signed certificates.</p>&#13;
<p class="indent">During setup, <code>kubelet</code> created a certificate request on each node, but the requests were not automatically approved. Let’s find these requests:</p>&#13;
<pre>root@host01:/etc/kubernetes/components# <span class="codestrong1">kubectl get csr</span>&#13;
NAME      ... SIGNERNAME                                  ... CONDITION&#13;
<span epub:type="pagebreak" id="page_105"/>csr-sgrwz ... kubernetes.io/kubelet-serving               ... Pending&#13;
csr-agwb6 ... kubernetes.io/kube-apiserver-client-kubelet ... Approved,Issued&#13;
csr-2kwwk ... kubernetes.io/kubelet-serving               ... Pending&#13;
csr-5496d ... kubernetes.io/kube-apiserver-client-kubelet ... Approved,Issued&#13;
csr-hm6lj ... kubernetes.io/kube-apiserver-client-kubelet ... Approved,Issued&#13;
csr-jbfmx ... kubernetes.io/kubelet-serving               ... Pending&#13;
csr-njjr7 ... kubernetes.io/kube-apiserver-client-kubelet ... Approved,Issued&#13;
csr-v7tcs ... kubernetes.io/kubelet-serving               ... Pending&#13;
csr-vr27n ... kubernetes.io/kubelet-serving               ... Pending</pre>&#13;
<p class="indent">Each <code>kubelet</code> has a client certificate that it uses to authenticate to the API server; these were automatically approved during bootstrap. The requests we need to approve are for <code>kubelet-serving</code> certificates, which are used when clients such as our metrics server connect to <code>kubelet</code>. As soon as the request is approved, the controller manager signs the certificate. The <code>kubelet</code> then collects the certificate and starts using it.</p>&#13;
<p class="indent">We can approve all of these requests at once by querying for the name of all of the <code>kubelet-serving</code> requests and then passing those names to <code>kubectl certificate approve</code>:</p>&#13;
<pre>root@host01:/etc/kubernetes/components# <span class="codestrong1">kubectl certificate approve \$(kubectl</span>&#13;
  <span class="codestrong1">get csr --field-selector spec.signerName=kubernetes.io/kubelet-serving -o name)</span>&#13;
certificatesigningrequest.certificates.k8s.io/csr-sgrwz approved&#13;
...</pre>&#13;
<p class="indent">We now can install our metrics server by downloading and applying its YAML configuration:</p>&#13;
<pre>root@host01:/etc/kubernetes/components# <span class="codestrong1">curl -Lo metrics-server.yaml \$metrics_url</span>&#13;
root@host01:/etc/kubernetes/components# <span class="codestrong1">kubectl apply -f metrics-server.yaml</span>&#13;
...&#13;
root@host01:/etc/kubernetes/components# <span class="codestrong1">cd</span>&#13;
root@host01:~#</pre>&#13;
<p class="indent">This component is the last one we need to install, so we can leave this directory. With these cluster add-ons, we now have a complete, highly available Kubernetes cluster.</p>&#13;
<h3 class="h3" id="ch00lev1sec30">Exploring a Cluster</h3>&#13;
<p class="noindent">Before deploying our first application onto this brand-new Kubernetes cluster, let’s explore what’s running on it. The commands we use here will come in handy later as we debug our own applications and a cluster that isn’t working correctly.</p>&#13;
<p class="indent">We’ll use <code>crictl</code>, the same command we used to explore running containers in <a href="part01.xhtml#part01">Part I</a>, to see what containers are running on <code>host01</code>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">crictl ps</span>&#13;
CONTAINER       ... STATE    NAME                       ...&#13;
<span epub:type="pagebreak" id="page_106"/>25c63f29c1442   ... Running  longhorn-csi-plugin        ...&#13;
2ffdd044a81d8   ... Running  node-driver-registrar      ...&#13;
94468050de89c   ... Running  csi-provisioner            ...&#13;
119fbf417f1db   ... Running  csi-attacher               ...&#13;
e74c1a2a0c422   ... Running  kube-scheduler             ...&#13;
d1ad93cdbc686   ... Running  kube-controller-manager    ...&#13;
76266a522cc3d   ... Running  engine-image-ei-611d1496   ...&#13;
fc3cd1679e33e   ... Running  replica-manager            ...&#13;
48e792a973105   ... Running  engine-manager             ...&#13;
e658baebbc295   ... Running  longhorn-manager           ...&#13;
eb51d9ec0f2fc   ... Running  calico-kube-controllers    ...&#13;
53e7e3e4a3148   ... Running  calico-node                ...&#13;
772ac45ceb94e   ... Running  calico-typha               ...&#13;
4005370021f5f   ... Running  kube-proxy                 ...&#13;
26929cde3a264   ... Running  kube-apiserver             ...&#13;
9ea4c2f5af794   ... Running  etcd                       ...</pre>&#13;
<p class="indent">The control plane node is very busy, as this list includes Kubernetes control plane components, Calico components, and Longhorn components. Running this command on all the nodes and sorting out what containers are running where and for what purpose would be confusing. Fortunately, <code>kubectl</code> provides a clearer picture, although knowing that we can get down to these lower-level details and see exactly what containers are running on a given node is nice.</p>&#13;
<p class="indent">To explore the cluster with <code>kubectl</code>, we need to know how the cluster resources are organized into Namespaces. As mentioned previously, Kubernetes Namespaces provide security and avoid name collisions. To ensure idempotence, Kubernetes needs each resource to have a unique name. By dividing resources into Namespaces, we allow multiple resources to have the same name while still enabling the API server to know exactly which resource we mean, which also supports multitenancy, one of our cross-cutting concerns.</p>&#13;
<p class="indent">Even though we just set up the cluster, it’s already populated with several Namespaces:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get namespaces</span>&#13;
NAME              STATUS   AGE&#13;
calico-system     Active   50m&#13;
default           Active   150m&#13;
kube-node-lease   Active   150m&#13;
kube-public       Active   150m&#13;
kube-system       Active   150m&#13;
longhorn-system   Active   16m&#13;
tigera-operator   Active   50m</pre>&#13;
<p class="indent">As we run <code>kubectl</code> commands, they will apply to the <code>default</code> Namespace unless we use the <code>-n</code> option to specify a different Namespace.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_107"/>To see what containers are running, we ask <code>kubectl</code> to get the list of Pods. We look at Kubernetes Pods in much more detail in <a href="ch07.xhtml#ch07">Chapter 7</a>. For now, just know that a Pod is a group of one or more containers, much like the Pods that we created with <code>crictl</code> in <a href="part01.xhtml#part01">Part I</a>.</p>&#13;
<p class="indent">If we try to list Pods in the <code>default</code> Namespace, we can see that there aren’t any yet:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods</span>&#13;
No resources found in default namespace.</pre>&#13;
<p class="indent">So far, as we installed cluster infrastructure components, they’ve been created in other Namespaces. That way, when we configure normal user accounts, we can prevent those users from viewing or editing the cluster infrastructure. The Kubernetes infrastructure components were all installed into the <code>kube-system</code> Namespace:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n kube-system get pods</span>&#13;
NAME                             READY   STATUS    ...&#13;
coredns-558bd4d5db-7krwr         1/1     Running   ...&#13;
...&#13;
kube-apiserver-host01            1/1     Running   ...&#13;
...</pre>&#13;
<p class="indent">We cover the control plane components in <a href="ch11.xhtml#ch11">Chapter 11</a>. For now, let’s explore just one of the control plane Pods, the API server running on <code>host01</code>. We can get all of the details for this Pod using <code>kubectl describe</code>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n kube-system describe pod kube-apiserver-host01</span>&#13;
Name:                 kube-apiserver-host01&#13;
Namespace:            kube-system&#13;
...&#13;
Node:                 host01/192.168.61.11&#13;
...&#13;
Status:               Running&#13;
Containers:&#13;
  kube-apiserver:&#13;
    Container ID:  containerd://26929cde3a264e...&#13;
...</pre>&#13;
<p class="indent">The Namespace and name together uniquely identify this Pod. We also see the node on which the Pod is scheduled, its status, and details about the actual containers, including a container ID that we can use with <code>crictl</code> to find the container in the underlying <code>containerd</code> runtime.</p>&#13;
<p class="indent">Let’s also verify that Calico deployed into our cluster as expected:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n calico-system get pods</span>&#13;
NAME                                       READY   STATUS    ...&#13;
calico-kube-controllers-7f58dbcbbd-ch7zt   1/1     Running   ...&#13;
calico-node-cp88k                          1/1     Running   ...&#13;
calico-node-dn4rj                          1/1     Running   ...&#13;
<span epub:type="pagebreak" id="page_108"/>calico-node-xnkmg                          1/1     Running   ...&#13;
calico-node-zfscp                          1/1     Running   ...&#13;
calico-typha-68b99cd4bf-7lwss              1/1     Running   ...&#13;
calico-typha-68b99cd4bf-jjdts              1/1     Running   ...&#13;
calico-typha-68b99cd4bf-pjr6q              1/1     Running   ...</pre>&#13;
<p class="indent">Earlier we saw that Calico installed a DaemonSet resource. Kubernetes has used the configuration in this DaemonSet to automatically create a <code>calico-node</code> Pod for each node. Like Kubernetes itself, Calico also uses a separate control plane to handle overall configuration of the network, and the other Pods provide that control plane.</p>&#13;
<p class="indent">Finally, we’ll see the containers that are running for Longhorn:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl -n longhorn-system get pods</span>&#13;
NAME                                       READY   STATUS    RESTARTS   AGE&#13;
engine-image-ei-611d1496-8q58f             1/1     Running   0          31m&#13;
...&#13;
longhorn-csi-plugin-8vkr6                  2/2     Running   0          31m&#13;
...&#13;
longhorn-manager-dl9sb                     1/1     Running   1          32m&#13;
...</pre>&#13;
<p class="indent">Like Calico, Longhorn uses DaemonSets so that it can run containers on every node. These containers provide storage services to the other containers on the node. Longhorn also includes a number of other containers that serve as a control plane, including providing the CSI implementation that Kubernetes uses to tell Longhorn to create storage when needed.</p>&#13;
<p class="indent">We put a lot of effort into setting up this cluster, so it would be a shame to end this chapter without running at least one application on it. In the next chapter, we will look at many different ways to run containers, but let’s quickly run a simple NGINX web server in our Kubernetes cluster:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl run nginx --image=nginx</span>&#13;
pod/nginx created</pre>&#13;
<p class="indent">That may look like an imperative command, but under the hood, <code>kubectl</code> is creating a Pod resource using the name and container image we specified, and then it’s applying that resource on the cluster. Let’s inspect the default Namespace again:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">kubectl get pods -o wide</span>&#13;
NAME    READY   STATUS    ... IP               NODE  ...&#13;
nginx   1/1     Running   ... 172.31.89.203   host02 ...</pre>&#13;
<p class="indent">We used <code>-o wide</code> to see extra information about the Pod, including its IP address and where it was scheduled, which can be different each time the Pod is created. In this case, the Pod was scheduled to <code>host02</code>, showing that we were successful in allowing regular application containers to be deployed to our control plane nodes. The IP address comes from the Pod CIDR we configured, and Calico automatically assigns it.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_109"/>Calico also handles routing traffic so that we can reach the Pod from any container in the cluster as well as from the host network. Let’s verify that, starting with a regular <code>ping</code>:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">ping -c 1 <span class="codeitalic1">172.31.89.203</span></span>&#13;
PING 172.31.89.203 (172.31.89.203) 56(84) bytes of data.&#13;
64 bytes from 172.31.89.203: icmp_seq=1 ttl=63 time=0.848 ms&#13;
&#13;
--- 172.31.89.203 ping statistics ---&#13;
1 packets transmitted, 1 received, 0% packet loss, time 0ms&#13;
rtt min/avg/max/mdev = 0.848/0.848/0.848/0.000 ms</pre>&#13;
<p class="indent">Use your Pod’s IP address in the place of the one shown here.</p>&#13;
<p class="indent">We can also use <code>curl</code> to verify that the NGINX web server is working:</p>&#13;
<pre>root@host01:~# <span class="codestrong1">curl http://<span class="codeitalic1">172.31.89.203</span></span>&#13;
...&#13;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;&#13;
...</pre>&#13;
<p class="indent">The Kubernetes cluster is working and ready for us to deploy applications. Kubernetes will take advantage of all of the nodes in the cluster to load balance our applications and provide resiliency in the event of any failures.</p>&#13;
<h3 class="h3" id="ch00lev1sec31">Final Thoughts</h3>&#13;
<p class="noindent">In this chapter, we’ve explored how Kubernetes is architected with the flexibility to allow cluster components to come and go at any time. This applies not only to containerized applications but also to the cluster components, including control plane microservices and the underlying servers and networks the cluster uses. We were able to bootstrap a cluster and then dynamically add nodes to it, configure those nodes to accept certain types of containers, and then dynamically add networking and storage drivers using the Kubernetes cluster itself to run and monitor them. Finally, we deployed our first container to a Kubernetes cluster, allowing it to automatically schedule the container onto an available node, using our network driver to access the container from the host network.</p>&#13;
<p class="indent">Now that we have a highly available cluster, we can look at how to deploy an application to Kubernetes. We’ll explore some key Kubernetes resources that we need to create a scalable, reliable application. This process will provide a foundation for exploring Kubernetes in detail, including understanding what happens when our applications don’t run as expected and how to debug issues with our application or the Kubernetes cluster.<span epub:type="pagebreak" id="page_110"/></p>&#13;
</body></html>