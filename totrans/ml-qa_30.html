<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="h2" id="ch25"><span epub:type="pagebreak" id="page_163"/><strong><span class="big">25</span><br/>CONFIDENCE INTERVALS</strong></h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="noindent">What are the different ways to construct confidence intervals for machine learning classifiers?</p>&#13;
<p class="indent">There are several ways to construct confidence intervals for machine learning models, depending on the model type and the nature of your data. For instance, some methods are computationally expensive when working with deep neural networks and are thus more suitable to less resource-intensive machine learning models. Others require larger datasets to be reliable.</p>&#13;
<p class="indent">The following are the most common methods for constructing confidence intervals:</p>&#13;
<ul>&#13;
<li class="noindent">Constructing normal approximation intervals based on a test set</li>&#13;
<li class="noindent">Bootstrapping training sets</li>&#13;
<li class="noindent">Bootstrapping the test set predictions</li>&#13;
<li class="noindent">Confidence intervals from retraining models with different random seeds</li>&#13;
</ul>&#13;
<p class="indent">Before reviewing these in greater depth, let’s briefly review the definition and interpretation of confidence intervals.<span epub:type="pagebreak" id="page_164"/></p>&#13;
<h3 class="h3" id="ch00lev123"><strong>Defining Confidence Intervals</strong></h3>&#13;
<p class="noindent">A <em>confidence interval</em> is a type of method to estimate an unknown population parameter. A <em>population parameter</em> is a specific measure of a statistical population, for example, a mean (average) value or proportion. By “specific” measure, I mean there is a single, exact value for that parameter for the entire population. Even though this value may not be known and often needs to be estimated from a sample, it is a fixed and definite characteristic of the population. A <em>statistical population</em>, in turn, is the complete set of items or individuals we study.</p>&#13;
<p class="indent">In a machine learning context, the population could be considered the entire possible set of instances or data points that the model may encounter, and the parameter we are often most interested in is the true generalization accuracy of our model on this population.</p>&#13;
<p class="indent">The accuracy we measure on the test set estimates the true generalization accuracy. However, it’s subject to random error due to the specific sample of test instances we happened to use. This is where the concept of a confidence interval comes in. A 95 percent confidence interval for the generalization accuracy gives us a range in which we can be reasonably sure that the true generalization accuracy lies.</p>&#13;
<p class="indent">For instance, if we take 100 different data samples and compute a 95 percent confidence interval for each sample, approximately 95 of the 100 confidence intervals will contain the true population value (such as the generalization accuracy), as illustrated in <a href="ch25.xhtml#ch25fig1">Figure 25-1</a>.</p>&#13;
<div class="image"><img id="ch25fig1" src="../images/25fig01.jpg" alt="Image" width="1107" height="389"/></div>&#13;
<p class="figcap"><em>Figure 25-1: The concept of 95 percent confidence intervals</em></p>&#13;
<p class="indent">More concretely, if we were to draw 100 different representative test sets from the population (for instance, the entire possible set of instances that the model may encounter) and compute the 95 percent confidence interval for the generalization accuracy from each test set, we would expect about 95 of these intervals to contain the true generalization accuracy.</p>&#13;
<p class="indent">We can display confidence intervals in several ways. It is common to use a bar plot representation where the top of the bar represents the parameter value (for example, model accuracy) and the whiskers denote the upper and <span epub:type="pagebreak" id="page_165"/>lower levels of the confidence interval (left chart of <a href="ch25.xhtml#ch25fig2">Figure 25-2</a>). Alternatively, the confidence intervals can be shown without bars, as in the right chart of <a href="ch25.xhtml#ch25fig2">Figure 25-2</a>.</p>&#13;
<div class="image"><img id="ch25fig2" src="../images/25fig02.jpg" alt="Image" width="913" height="339"/></div>&#13;
<p class="figcap"><em>Figure 25-2: Two common plotting variants to illustrate confidence intervals</em></p>&#13;
<p class="indent">This visualization is functionally useful in a number of ways. For instance, when confidence intervals for two model performances do <em>not</em> overlap, it’s a strong visual indicator that the performances are significantly different. Take the example of statistical significance tests, such as t-tests: if two 95 percent confidence intervals do not overlap, it strongly suggests that the difference between the two measurements is statistically significant at the 0.05 level.</p>&#13;
<p class="indent">On the other hand, if two 95 percent confidence intervals overlap, we cannot automatically conclude that there’s no significant difference between the two measurements. Even when confidence intervals overlap, there can still be a statistically significant difference.</p>&#13;
<p class="indent">Alternatively, to provide more detailed information about the exact quantities, we can use a table view to express the confidence intervals. The two common notations are summarized in <a href="ch25.xhtml#ch25tab1">Table 25-1</a>.</p>&#13;
<p class="tabcap" id="ch25tab1"><strong>Table 25-1:</strong> Confidence Intervals</p>&#13;
<table class="all">&#13;
<colgroup>&#13;
<col style="width:15%"/>&#13;
<col style="width:40%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:20%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent">Model</p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent">Dataset A</p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent">Dataset B</p></th>&#13;
<th class="table-h" style="vertical-align: top"><p class="noindent">Dataset C</p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">89.1% <em>±</em> 1.7%</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">. . .</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">. . .</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent">2</p></td>&#13;
<td style="vertical-align: top"><p class="noindent">79.5% <em>±</em> 2.2%</p></td>&#13;
<td style="vertical-align: top"><p class="noindent">. . .</p></td>&#13;
<td style="vertical-align: top"><p class="noindent">. . .</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">3</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">95.2% <em>±</em> 1.6%</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">. . .</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">. . .</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="table-h" style="vertical-align: top"><p class="noindent"><strong>Model</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top"><p class="noindent"><strong>Dataset A</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top"><p class="noindent"><strong>Dataset B</strong></p></td>&#13;
<td class="table-h" style="vertical-align: top"><p class="noindent"><strong>Dataset C</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">1</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">89.1% (87.4%, 90.8%)</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">. . .</p></td>&#13;
<td style="vertical-align: top" class="gray"><p class="noindent">. . .</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="noindent">2</p></td>&#13;
<td style="vertical-align: top"><p class="noindent">79.5% (77.3%, 81.7%)</p></td>&#13;
<td style="vertical-align: top"><p class="noindent">. . .</p></td>&#13;
<td style="vertical-align: top"><p class="noindent">. . .</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="tab_th" style="vertical-align: top"><p class="noindent">3</p></td>&#13;
<td class="tab_th" style="vertical-align: top"><p class="noindent">95.2% (93.6%, 96.8%)</p></td>&#13;
<td class="tab_th" style="vertical-align: top"><p class="noindent">. . .</p></td>&#13;
<td class="tab_th" style="vertical-align: top"><p class="noindent">. . .</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">The <em>±</em> notation is often preferred if the confidence interval is <em>symmetric</em>, meaning the upper and lower endpoints are equidistant from the estimated parameter. Alternatively, the lower and upper confidence intervals can be written explicitly.<span epub:type="pagebreak" id="page_166"/></p>&#13;
<h3 class="h3" id="ch00lev124"><strong>The Methods</strong></h3>&#13;
<p class="noindent">The following sections describe the four most common methods of constructing confidence intervals.</p>&#13;
<h4 class="h4" id="ch00levsec28"><em><strong>Method 1: Normal Approximation Intervals</strong></em></h4>&#13;
<p class="noindent">The normal approximation interval involves generating the confidence interval from a single train-test split. It is often considered the simplest and most traditional method for computing confidence intervals. This approach is especially appealing in the realm of deep learning, where training models is computationally costly. It’s also desirable when we are interested in evaluating a specific model, instead of models trained on various data partitions like in <em>k</em>-fold cross-validation.</p>&#13;
<p class="indent">How does it work? In short, the formula for calculating the confidence interval for a predicted parameter (for example, the sample mean, denoted as <img class="middle" src="../images/x-bar.jpg" alt="Image" width="15" height="26"/>), assuming a normal distribution, is expressed as <img class="middle" src="../images/x-bar.jpg" alt="Image" width="15" height="26"/> <em>± z × SE</em>.</p>&#13;
<p class="indent">In this formula, <em>z</em> represents the <em>z</em>-score, which indicates a particular value’s number of standard deviations from the mean in a standard normal distribution. <em>SE</em> represents the standard error of the predicted parameter (in this case, the sample mean).</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>Most readers will be familiar with</em> z<em>-score tables that are usually found in the back of introductory statistics textbooks. However, a more convenient and preferred way to obtain</em> z<em>-scores is to use functions like SciPy’s</em> <span class="codeitalic">stats.zscore</span> <em>function, which computes the</em> z<em>-scores for given confidence levels.</em></p>&#13;
</div>&#13;
<p class="indent">For our scenario, the sample mean, denoted as <img class="middle" src="../images/x-bar.jpg" alt="Image" width="15" height="26"/>, corresponds to the test set accuracy, ACC<sub>test</sub>, a measure of successful predictions in the context of a binomial proportion confidence interval.</p>&#13;
<p class="indent">The standard error can be calculated under a normal approximation as follows:</p>&#13;
<div class="image1"><img src="../images/f0166-01.jpg" alt="Image" width="367" height="67"/></div>&#13;
<p class="indent">In this equation, <em>n</em> signifies the size of the test set. Substituting the standard error back into the previous formula, we obtain the following:</p>&#13;
<div class="image1"><img src="../images/f0166-02.jpg" alt="Image" width="444" height="67"/></div>&#13;
<p class="indent">Additional code examples to implement this method can also be found in the <em>supplementary/q25_confidence-intervals</em> subfolder in the supplementary code repository at <em><a href="https://github.com/rasbt/MachineLearning-QandAI-book">https://github.com/rasbt/MachineLearning-QandAI-book</a></em>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_167"/>While the normal approximation interval method is very popular due to its simplicity, it has some downsides. First, the normal approximation may not always be accurate, especially for small sample sizes or for data that is not normally distributed. In such cases, other methods of computing confidence intervals may be more accurate. Second, using a single train-test split does not provide information about the variability of the model performance across different splits of the data. This can be an issue if the performance is highly dependent on the specific split used, which may be the case if the dataset is small or if there is a high degree of variability in the data.</p>&#13;
<h4 class="h4" id="ch00levsec29"><em><strong>Method 2: Bootstrapping Training Sets</strong></em></h4>&#13;
<p class="noindent">Confidence intervals serve as a tool for approximating unknown parameters. However, when we are restricted to just one estimate, such as the accuracy derived from a single test set, we must make certain assumptions to make this work. For example, when we used the normal approximation interval described in the previous section, we assumed normally distributed data, which may or may not hold.</p>&#13;
<p class="indent">In a perfect scenario, we would have more insight into our test set sample distribution. However, this would require access to many independent test datasets, which is typically not feasible. A workaround is the bootstrap method, which resamples existing data to estimate the sampling distribution.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>In practice, when the test set is large enough, the normal distribution approximation will hold, thanks to the central limit theorem. This theorem states that the sum (or average) of a large number of independent, identically distributed random variables will approach a normal distribution, regardless of the underlying distribution of the individual variables. It is difficult to specify what constitutes a large-enough test set. However, under stronger assumptions than those of the central limit theorem, we can at least estimate the rate of convergence to the normal distribution using the Berry–Esseen theorem, which gives a more quantitative estimate of how quickly the convergence in the central limit theorem occurs.</em></p>&#13;
</div>&#13;
<p class="indent">In a machine learning context, we can take the original dataset and draw a random sample <em>with replacement</em>. If the dataset has size <em>n</em> and we draw a random sample with replacement of size <em>n</em>, this implies that some data points will likely be duplicated in this new sample, whereas other data points are not sampled at all. We can then repeat this procedure for multiple rounds to obtain multiple training and test sets. This process is known as <em>out-of-bag bootstrapping</em>, illustrated in <a href="ch25.xhtml#ch25fig3">Figure 25-3</a>.<span epub:type="pagebreak" id="page_168"/></p>&#13;
<div class="image"><img id="ch25fig3" src="../images/25fig03.jpg" alt="Image" width="1094" height="487"/></div>&#13;
<p class="figcap"><em>Figure 25-3: Out-of-bag bootstrapping evaluates models on resampled training sets.</em></p>&#13;
<p class="indent">Suppose we constructed <em>k</em> training and test sets. We can now take each of these splits to train and evaluate the model to obtain <em>k</em> test set accuracy estimates. Considering this distribution of test set accuracy estimates, we can take the range between the 2.5th and 97.5th percentile to obtain the 95 percent confidence interval, as illustrated in <a href="ch25.xhtml#ch25fig4">Figure 25-4</a>.</p>&#13;
<div class="image"><img id="ch25fig4" src="../images/25fig04.jpg" alt="Image" width="672" height="588"/></div>&#13;
<p class="figcap"><em>Figure 25-4: The distribution of test accuracies from 1,000 bootstrap samples, including a 95 percent confidence interval</em></p>&#13;
<p class="indent">Unlike the normal approximation interval method, we can consider this out-of-bag bootstrap approach to be more agnostic to the specific distribution. Ideally, if the assumptions for the normal approximation are satisfied, both methodologies would yield identical outcomes.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_169"/>Since bootstrapping relies on resampling the existing test data, its downside is that it doesn’t bring in any new information that could be available in a broader population or unseen data. Therefore, it may not always be able to generalize the performance of the model to new, unseen data.</p>&#13;
<p class="indent">Note that we are using the bootstrap sampling approach in this chapter instead of obtaining the train-test splits via <em>k</em>-fold cross-validation, because of the bootstrap’s theoretical grounding via the central limit theorem discussed earlier. There are also more advanced out-of-bag bootstrap methods, such as the .632 and .632+ estimates, which are reweighting the accuracy estimates.</p>&#13;
<h4 class="h4" id="ch00levsec30"><em><strong>Method 3: Bootstrapping Test Set Predictions</strong></em></h4>&#13;
<p class="noindent">An alternative approach to bootstrapping training sets is to bootstrap test sets. The idea is to train the model on the existing training set as usual and then to evaluate the model on bootstrapped test sets, as illustrated in <a href="ch25.xhtml#ch25fig5">Figure 25-5</a>. After obtaining the test set performance estimates, we can then apply the percentile method described in the previous section.</p>&#13;
<div class="image"><img id="ch25fig5" src="../images/25fig05.jpg" alt="Image" width="683" height="351"/></div>&#13;
<p class="figcap"><em>Figure 25-5: Bootstrapping the test set</em></p>&#13;
<p class="indent">Contrary to the prior bootstrap technique, this method uses a trained model and simply resamples the test set (instead of the training sets). This approach is especially appealing for evaluating deep neural networks, as it doesn’t require retraining the model on the new data splits. However, a disadvantage of this approach is that it doesn’t assess the model’s variability toward small changes in the training data.</p>&#13;
<h4 class="h4" id="ch00levsec31"><em><strong>Method 4: Retraining Models with Different Random Seeds</strong></em></h4>&#13;
<p class="noindent">In deep learning, models are commonly retrained using various random seeds since some random weight initializations may lead to much better models than others. How can we build a confidence interval from these experiments? If we assume that the sample means follow a normal distribution, we can employ a previously discussed method where we calculate the confidence interval around a sample mean, denoted as <img class="middle" src="../images/x-bar.jpg" alt="Image" width="15" height="26"/>, as follows:</p>&#13;
<div class="image1"><img src="../images/f0169-01.jpg" alt="Image" width="123" height="21"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_170"/>Since in this context we often work with a relatively modest number of samples (for instance, models from 5 to 10 random seeds), assuming a <em>t</em> distribution is deemed more suitable than a normal distribution. Therefore, we substitute the <em>z</em> value with a <em>t</em> value in the preceding formula. (As the sample size increases, the <em>t</em> distribution tends to look more like the standard normal distribution, and the critical values [<em>z</em> and <em>t</em>] become increasingly similar.)</p>&#13;
<p class="indent">Furthermore, if we are interested in the average accuracy, denoted as <img class="middle" src="../images/f0170-01.jpg" alt="Image" width="65" height="20"/>, we consider ACC<sub>test, <em>j</em></sub> corresponding to a unique random seed <em>j</em> as a sample. The number of random seeds we evaluate would then constitute the sample size <em>n</em>. As such, we would calculate:</p>&#13;
<div class="image1"><img src="../images/f0170-02.jpg" alt="Image" width="201" height="29"/></div>&#13;
<p class="noindent">Here, SE is the standard error, calculated as <img class="middle" src="../images/f0170-03.jpg" alt="Image" width="110" height="21"/>, while</p>&#13;
<div class="image1"><img src="../images/f0170-04.jpg" alt="Image" width="297" height="87"/></div>&#13;
<p class="noindent">is the average accuracy, which we compute over the <em>r</em> random seeds. The standard deviation SD is calculated as follows:</p>&#13;
<div class="image1"><img src="../images/f0170-05.jpg" alt="Image" width="401" height="84"/></div>&#13;
<p class="indent">To summarize, calculating the confidence intervals using various random seeds is another effective alternative. However, it is primarily beneficial for deep learning models. It proves to be costlier than both the normal approximation approach (method 1) and bootstrapping the test set (method 3), as it necessitates retraining the model. On the bright side, the outcomes derived from disparate random seeds provide us with a robust understanding of the model’s stability.</p>&#13;
<h3 class="h3" id="ch00lev125"><strong>Recommendations</strong></h3>&#13;
<p class="noindent">Each possible method for constructing confidence intervals has its unique advantages and disadvantages. The normal approximation interval is cheap to compute but relies on the normality assumption about the distribution. The out-of-bag bootstrap is agnostic to these assumptions but is substantially more expensive to compute. A cheaper alternative is bootstrapping the test only, but this involves bootstrapping a smaller dataset and may be misleading for small or nonrepresentative test set sizes. Lastly, constructing confidence intervals from different random seeds is expensive but can give us additional insights into the model’s stability.<span epub:type="pagebreak" id="page_171"/></p>&#13;
<h3 class="h3" id="ch00lev126"><strong>Exercises</strong></h3>&#13;
<p class="number1"><strong>25-1.</strong> As mentioned earlier, the most common choice of confidence level is 95 percent confidence intervals. However, 90 percent and 99 percent are also common. Are 90 percent confidence intervals smaller or wider than 95 percent confidence intervals, and why is this the case?</p>&#13;
<p class="number1"><strong>25-2.</strong> In “Method 3: Bootstrapping Test Set Predictions” on <a href="ch25.xhtml#ch00levsec30">page 169</a>, we created test sets by bootstrapping and then applied the already trained model to compute the test set accuracy on each of these datasets. Can you think of a method or modification to obtain these test accuracies more efficiently?</p>&#13;
<h3 class="h3" id="ch00lev127"><strong>References</strong></h3>&#13;
<ul>&#13;
<li class="noindent">A detailed discussion of the pitfalls of concluding statistical significance from nonoverlapping confidence intervals: Martin Krzywinski and Naomi Altman, “Error Bars” (2013), <em><a href="https://www.nature.com/articles/nmeth.2659">https://www.nature.com/articles/nmeth.2659</a></em>.</li>&#13;
<li class="noindent">A more detailed explanation of the binomial proportion confidence interval: <em><a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval">https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval</a></em>.</li>&#13;
<li class="noindent">For a detailed explanation of normal approximation intervals, see Section 1.7 of my article: “Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning” (2018), <em><a href="https://arxiv.org/abs/1811.12808">https://arxiv.org/abs/1811.12808</a></em>.</li>&#13;
<li class="noindent">Additional information on the central limit theorem for independent and identically distributed random variables: <em><a href="https://en.wikipedia.org/wiki/Central_limit_theorem">https://en.wikipedia.org/wiki/Central_limit_theorem</a></em>.</li>&#13;
<li class="noindent">For more on the Berry–Esseen theorem: <em><a href="https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem">https://en.wikipedia.org/wiki/Berry–Esseen_theorem</a></em>.</li>&#13;
<li class="noindent">The .632 bootstrap addresses a pessimistic bias of the regular outof-bag bootstrapping approach: Bradley Efron, “Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation” (1983), <em><a href="https://www.jstor.org/stable/2288636">https://www.jstor.org/stable/2288636</a></em>.</li>&#13;
<li class="noindent">The .632+ bootstrap corrects an optimistic bias introduced in the .632 bootstrap: Bradley Efron and Robert Tibshirani, “Improvements on Cross-Validation: The .632+ Bootstrap Method” (1997), <em><a href="https://www.jstor.org/stable/2965703">https://www.jstor.org/stable/2965703</a></em>.</li>&#13;
<li class="noindent">A deep learning research paper that discusses bootstrapping the test set predictions: Benjamin Sanchez-Lengeling et al., “Machine Learning for Scent: Learning Generalizable Perceptual Representations of Small Molecules” (2019), <em><a href="https://arxiv.org/abs/1910.10685">https://arxiv.org/abs/1910.10685</a></em>.<span epub:type="pagebreak" id="page_172"/></li>&#13;
</ul>&#13;
</div>
</div>
</body></html>