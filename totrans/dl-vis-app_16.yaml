- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural Networks
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: Deep learning algorithms are based on building a network of connected computational
    elements. The fundamental unit of such networks is a small bundle of computation
    called an *artificial neuron*, though it’s often referred to simply as a *neuron*.
    The artificial neuron was inspired by human neurons, which are the nerve cells
    that make up our brain and central nervous system and are largely responsible
    for our cognitive abilities.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we see what artificial neurons look like and how to arrange
    them into networks. We then group them into layers, which create deep learning
    networks. We also look at various ways to configure the outputs of these artificial
    neurons so that they produce the most useful results.
  prefs: []
  type: TYPE_NORMAL
- en: Real Neurons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In biology, the term *neuron* is applied to a wide variety of complex cells
    distributed throughout every human body. These cells all have similar structure
    and behavior, but they’re specialized for many different tasks. Neurons are sophisticated
    pieces of biology that use a mix of chemistry, physics, electricity, timing, proximity,
    and other means to perform their behaviors and communicate with one another (Julien
    2011; Khanna 2018; Lodish et al. 2000; Purves et al. 2001). A highly simplified
    sketch of a neuron is shown in [Figure 13-1](#figure13-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![F13001](Images/F13001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-1: A sketch of a highly simplified biological neuron (in red) with
    a few major structures identified. This neuron’s outputs are communicated to another
    neuron (in blue), only partially shown (adapted from Wikipedia 2020b).'
  prefs: []
  type: TYPE_NORMAL
- en: Neurons are information processing machines. One type of information arrives
    in the form of chemicals called *neurotransmitters* that temporarily *bind*, or
    attach, onto *receptor sites* located on the neuron (Goldberg 2015). Let’s sketch
    out what happens next in the broadest possible terms.
  prefs: []
  type: TYPE_NORMAL
- en: The chemicals that bind to the receptor sites cause electrical signals to travel
    into the body of the neuron. Each of these signals can be either positive or negative.
    All of the electrical signals arriving at the neuron’s body over a short interval
    of time are added together and then compared to a *threshold*. If the total exceeds
    that threshold, a new signal is sent along the axon to another part of the neuron,
    causing specific amounts of neurotransmitters to be released into the environment.
    These molecules then bind with other neurons, and the process repeats.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, information is propagated and modified as it flows through the
    densely connected network of neurons in the brain and central nervous system.
    If two neurons are physically close enough to each other that one can receive
    the neurotransmitters released by the other, we say that the neurons are *connected*,
    even though they may not be actually touching. There is some evidence that the
    particular pattern of connections between neurons is as essential to cognition
    and identity as the neurons themselves (Sporns, Tononi, and Kötter 2005; Seung
    2013). A map of an individual’s neuronal connections is called their *connectome*.
    Connectomes are as unique as fingerprints or iris patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Although real neurons and their surrounding environment are tremendously complex
    and subtle, the basic mechanism described here has an appealing elegance. Responding
    to this, some scientists have attempted to emulate or duplicate the brain by creating
    enormous numbers of simplified neurons and their environment, in hardware or software,
    hoping that interesting behavior will emerge (Furber 2012; Timmer 2014). So far,
    this has not delivered results that most people would call intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: But we can connect up simplified neurons in specific ways to produce great results
    on a wide range of problems. Those are the types of structures that will be our
    focus in this chapter, and the rest of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial Neurons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The “neurons” we use in machine learning are inspired by real neurons in the
    same way that a stick figure drawing is inspired by a human body. There’s a resemblance,
    but only in the most general sense. Almost all of the details are lost along the
    way, and we’re left with something that’s more of a reminder of the original,
    rather than even a simplified copy.
  prefs: []
  type: TYPE_NORMAL
- en: This has led to some confusion, particularly in the popular press, where “neural
    network” is sometimes used as a synonym for “electronic brain,” and from there,
    it’s only a short step to general intelligence, consciousness, emotions, and perhaps
    world domination and the elimination of human life. In reality, the neurons we
    use are so abstracted and simplified from real neurons that many people prefer
    instead to call them by the more generic name of *units*. But for better or worse,
    the word *neuron*, the phrase *neural net*, and all the related language are apparently
    here to stay, so we use them in this book as well.
  prefs: []
  type: TYPE_NORMAL
- en: The Perceptron
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The history of artificial neurons may be said to begin in 1943, with the publication
    of a paper that presented a massively simplified abstraction of a neuron’s basic
    functions in mathematical form, and described how multiple instances of this object
    could be connected into a *network*, or *net*. The big contribution of this paper
    was that it proved mathematically that such a network could implement any idea
    expressed in the language of mathematical logic (McCulloch and Pitts 1943). Since
    mathematical logic is the basis of machine calculation, that means neurons could
    perform mathematics. This was a big deal, because it provided a bridge between
    the fields of math, logic, computing, and neurobiology.
  prefs: []
  type: TYPE_NORMAL
- en: Building on that insight, in 1957 the *perceptron* was proposed as a simplified
    mathematical model of a neuron (Rosenblatt 1962). [Figure 13-2](#figure13-2) is
    a block diagram of a single perceptron with four inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![f13002](Images/f13002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-2: A four-input perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: Every input to a perceptron is represented by a single floating-point number.
    Each input is multiplied by a corresponding floating-point number called a *weight*.
    The results of these multiplications are all added together. Finally, we compare
    the result to a threshold value. If the result of the summation is greater than
    0, the perceptron produces an output of +1, otherwise it’s −1 (in some versions,
    the outputs are 1 and 0, rather than +1 and −1).
  prefs: []
  type: TYPE_NORMAL
- en: Though the perceptron is a vastly simplified version of a real neuron, it’s
    proven to be a terrific building block for deep learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: The history of the perceptron is an interesting part of the culture of machine
    learning, so let’s look at just a couple of its key events; more complete versions
    may be found online (Estebon 1997; Wikipedia 2020a).
  prefs: []
  type: TYPE_NORMAL
- en: After the principles of the perceptron had been verified in software, a perceptron-based
    computer was built at Cornell University in 1958\. It was a rack of wire-wrapped
    boards the size of a refrigerator, called the Mark I Perceptron (Wikipedia 2020c).
    The device was built to process images, using a grid of 400 photocells that could
    digitize an image at a resolution of 20 by 20 pixels (the word *pixel* hadn’t
    yet been coined). The weight applied to each input of the perceptron was set by
    turning a knob that controlled an electrical component called a potentiometer.
    To automate the learning process, electric motors were attached to the potentiometers
    so the device could literally turn its own knobs to adjust its weights and thereby
    change its calculations, and thus its output. The theory guaranteed that, with
    the right data, the system could learn to separate two different classes of inputs
    that could be split with a straight line.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, not many interesting problems involve sets of data that are separated
    by a straight line, and it proved hard to generalize the technique to more complicated
    arrangements of data. After a few years of stalled progress, a book proved that
    the original perceptron technique was fundamentally limited (Minsky and Papert
    1969). It showed that the lack of progress wasn’t due to a lack of imagination,
    but the result of theoretical limits built into the structure of a perceptron.
    Most interesting problems, and even some very simple ones, were provably beyond
    the ability of a perceptron to solve.
  prefs: []
  type: TYPE_NORMAL
- en: This result seemed to signal the end of perceptrons for many people, and a popular
    consensus formed that the perceptron approach was a dead end. Enthusiasm, interest,
    and funding all dried up, and most people directed their research to other problems.
    This period, which lasted roughly between the 1970s and 1990s, was called the
    *AI winter*.
  prefs: []
  type: TYPE_NORMAL
- en: But despite a widespread interpretation that the perceptron book had closed
    the door on perceptrons in general, in fact it had only shown the limitations
    of how they’d been used up to that time. Some people thought that writing off
    the whole idea was an overreaction and that perhaps the perceptron could still
    be a useful tool if applied in a different way. It took roughly a decade and a
    half, but this point of view eventually bore fruit when researchers combined perceptrons
    into larger structures and showed how to train them (Rumelhart, Hinton, and Williams
    1986). These combinations easily surpassed the limitations of any single unit.
    A series of papers then showed that careful arrangements of multiple perceptrons,
    beefed up with a few minor changes, could solve complex and interesting problems.
  prefs: []
  type: TYPE_NORMAL
- en: This discovery rekindled interest in the field, and soon research with perceptrons
    became a hot topic once again, producing a steady stream of interesting results
    that have led to the deep learning systems we use today. Perceptrons remain a
    core component of many modern deep learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: Modern Artificial Neurons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The neurons we use in modern neural networks are only slightly generalized
    from the original perceptrons. There are two changes: one at the input, and one
    at the output. These modified structures are still sometimes called perceptrons,
    but there’s rarely any confusion because the new versions are used almost exclusively.
    More commonly, they’re just called *neurons*. Let’s look at these two changes.'
  prefs: []
  type: TYPE_NORMAL
- en: The first change to the perceptron of [Figure 13-2](#figure13-2) is to provide
    each neuron with one more input, which we call the *bias*. This is a number that
    doesn’t come from the output of a previous neuron. Instead, it’s a number that’s
    directly added into the sum of all the weighted inputs. Every neuron has its own
    bias. [Figure 13-3](#figure13-3) shows our original perceptron, but with the bias
    term included.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13003](Images/F13003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-3: The perceptron of [Figure 13-2](#figure13-2), but now with a bias
    term'
  prefs: []
  type: TYPE_NORMAL
- en: Our second change to the perceptron of [Figure 13-2](#figure13-2) is at the
    output. The perceptron in that figure tests the sum against a threshold of 0,
    and then produces either a −1 or 1 (or 0 or 1). We generalize this by replacing
    the testing step with a mathematical function that takes the sum (including the
    bias) as input and returns a new floating-point value as output. Because the output
    of a real neuron is called its *activation*, we call this function that calculates
    the artificial neuron’s output the *activation function*. The little test shown
    in [Figure 13-2](#figure13-2) is an activation function, but one that’s rarely
    used anymore. Later in this chapter we’ll survey a variety of activation functions
    that have proved to be popular and useful in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing the Neurons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s identify a convention that’s used by most drawings of artificial neurons.
    In [Figure 13-3](#figure13-3) we showed the weights explicitly, and we also included
    the multiplication steps to show how the weights multiply the inputs. This takes
    a lot of room on the page. When we draw diagrams with a lot of neurons, all of
    these details can make for a cluttered and dense figure. So instead, in virtually
    all neural network diagrams, the weights and their multiplications are implied.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is important, and bears repeating: in neural network diagrams, the weights,
    and the steps where they multiply the inputs, are not drawn. Instead, we’re supposed
    to know that they are there and mentally include them in the diagram. If we show
    the weights at all, we typically label the lines from the inputs with the name
    of the weight. [Figure 13-4](#figure13-4) shows [Figure 13-3](#figure13-3) drawn
    in this style.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F13004](Images/F13004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-4: A neuron is often drawn with the weights on the arrows.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 13-4](#figure13-4), we also changed the threshold test at the end
    to a little picture. This is a drawing of a function called a *step*, and it’s
    meant to give us a visual reminder that any activation function can go into that
    spot. Basically, a number goes into that step, and a new number comes out, determined
    by whichever function we choose for the job.
  prefs: []
  type: TYPE_NORMAL
- en: We usually simplify things again. This time we omit the bias by pretending it’s
    one of the inputs. This not only makes the diagram simpler, but it makes the math
    simpler as well, which, in this case, also leads to more efficient algorithms.
    This simplification is called the *bias trick* (the word *trick* comes from mathematics,
    where it’s a complimentary term sometimes used for a clever simplification of
    a problem). Rather than change the value of the bias, we set the bias to always
    be 1, and change the weight applied to it before it gets summed up with the other
    inputs. [Figure 13-5](#figure13-5) shows this change in labeling. Though the bias
    term is always 1 and only its weight can change, we usually ignore the distinction
    and just talk about the value of the bias.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13005](Images/F13005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-5: The bias trick in action. Rather than show the bias term explicitly,
    as in [Figure 13-4](#figure13-4), we pretend it’s another input with its own weight.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We want our artificial neuron diagrams to be as simple as possible because
    when we start building up networks we’ll be showing lots of neurons at once, so
    most of these diagrams take two additional steps of simplification. First, they
    don’t show the bias at all. We’re supposed to remember that the bias is included
    (along with its weight), but it’s not shown. Second, the weights are often omitted
    as well, as in [Figure 13-6](#figure13-6). This is unfortunate, because the weights
    are the most important part of the neuron for us. The reason for this is that
    they are the only things we can change during training. Despite being left out
    of most drawings, they’re so essential that we repeat the key idea yet again:
    *even though we don’t show the weights explicitly, the weights are always there.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![F13006](Images/F13006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-6: A typical drawing of an artificial neuron. The bias term and the
    weights are not shown, but they are definitely present.'
  prefs: []
  type: TYPE_NORMAL
- en: Like real neurons, artificial neurons can be wired up in networks, where each
    input comes from the output of another neuron. When we connect neurons together
    into networks, we draw “wires” to connect one neuron’s output to one or more other
    neurons’ inputs. [Figure 13-7](#figure13-7) shows this idea visually.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13007](Images/F13007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-7: A piece of a larger network of artificial neurons. Each neuron
    receives its inputs from other neurons. The dashed lines show connections to and
    from outside this little cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: This is a *neural network*. Usually the goal of a network like [Figure 13-7](#figure13-7)
    is to produce one or more values as outputs. We’ll see later how we can interpret
    the numbers at the outputs in meaningful ways.
  prefs: []
  type: TYPE_NORMAL
- en: Even though we’ve said that we usually don’t draw the weights, in discussions,
    sometimes it’s useful to refer to individual weights. Let’s look at a common convention
    for weight names. [Figure 13-8](#figure13-8) shows six neurons. For convenience,
    we’ve labeled each neuron with a letter. Each weight corresponds to how the output
    of one specific neuron is changed on its way to another specific neuron. Each
    of these connections is shown as a line in the figure. To name a weight, we combine
    the name of the output neuron with the input neuron. For example, the weight that
    multiplies the output of A before it’s used by D is called AD.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13008](Images/F13008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-8: The weights are named by combining the names of the output and
    input neurons.'
  prefs: []
  type: TYPE_NORMAL
- en: From a structural point of view, it makes no difference whether we draw the
    weights inside each neuron, or on the wires that carry values to it. Various authors
    assume one or the other if it makes their discussion easier to follow, but we
    can always take the other viewpoint if we like.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 13-8](#figure13-8), we named the weight from neuron A to neuron D
    as AD. Some authors flip this around and write DA, because it’s a more direct
    match to how we often write the equations. It’s always worth a moment to check
    which order is being used in diagrams like this.
  prefs: []
  type: TYPE_NORMAL
- en: Feed-Forward Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 13-7](#figure13-7) showed a neural network with no apparent structure.
    A key feature of deep learningis that we arrange our neurons into *layers*. Typically,
    the neurons on each layer get their inputs only from the previous layer and send
    their outputs only to the following layer, and neurons do not communicate with
    other neurons on the same layer (there are, as always, exceptions to these rules).'
  prefs: []
  type: TYPE_NORMAL
- en: This organization allows us to process data in stages, with each layer of neurons
    building on the work done by the previous stage. By analogy, consider an office
    tower of many floors. The people on any given floor receive their work only from
    the people on the floor immediately below them, and they pass their work on only
    to the people on the floor immediately above them. In this analogy, each floor
    is a layer, and the people are the neurons on that layer.
  prefs: []
  type: TYPE_NORMAL
- en: We say that this type of arrangement processes the data *hierarchically*. There
    is some evidence that the human brain is structured to handle some tasks hierarchically,
    including the processing of sensory data like vision and hearing (Meunier et al.
    2009; Serre 2014). But here again, the connection between our computer models
    and real biology is much closer to inspiration than emulation.
  prefs: []
  type: TYPE_NORMAL
- en: It’s amazing that hooking up neurons in a series of layers produces anything
    useful. As we saw earlier, a single artificial neuron can hardly manage to do
    anything. It takes a bunch of numerical inputs, weights them, adds the results
    together, and then passes that result through a little function. This process
    can identify a straight line that splits a couple of clumps of data, and not much
    else. But if we assemble many thousands of these little units into layers and
    use some clever ideas to train them, then, working together, they’re capable of
    recognizing speech, identifying faces in photographs, and even beating humans
    at games of logic and skill.
  prefs: []
  type: TYPE_NORMAL
- en: The key to this is organization. Over time people have developed a number of
    ways to organize layers of neurons, resulting in a collection of common layer
    structures. The most common network structure arranges the neurons so that information
    flows in only one direction. We call this a *feed-forward network* because the
    data is flowing forward, with earlier neurons feeding, or delivering values to,
    later neurons. The art of designing a deep learning system lies in choosing the
    right sequence of layers, and the right hyperparameters, to create the basic architecture.
    To build a useful architecture for any given application, we need to understand
    how the neurons relate to one another. Let’s now look at how collections of neurons
    communicate, and how to set up the initial weights before learning begins.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Network Graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We usually represent neural networks as *graphs*. The study of graphs is so
    large that it is considered a field of mathematics in its own right, called *graph
    theory* (Trudeau 1994). Here, we’re going to stick to the basic ideas of graphs,
    because that’s all we need to organize our neural networks. Though we know we’ll
    usually be working with layers, let’s start out with some general graphs first,
    such as those shown in [Figure 13-9](#figure13-9).
  prefs: []
  type: TYPE_NORMAL
- en: A graph is made up of *nodes* (also called *vertices* or *elements*), here shown
    as circles. In this book, nodes are usually neurons, and throughout this book,
    we occasionally refer to one or more neurons in a network like this as nodes.
    The nodes are connected by arrows called *edges* (also called *arcs*, *wires*,
    or simply *lines*). The arrowhead is often left off when the direction of information
    flow is consistent in the drawing, which is almost always left to right or bottom
    to top. Information flows along the edges, carrying the output of one node to
    the inputs of others. Since information flows in only one direction on each edge,
    we sometimes call this kind of graph a *directed graph.*
  prefs: []
  type: TYPE_NORMAL
- en: The general idea is that we start things off by putting data into the input
    node or nodes, and then it flows through the edges, visiting nodes where it is
    transformed or changed, until it reaches the output node or nodes. No data ever
    returns to a node once it has left. In other words, information only flows forward,
    and there are no loops, or *cycles*. This kind of graph is like a little factory.
    Raw materials come in one end, and pass through machines that manipulate and combine
    them, ultimately producing one or more finished products at the end.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13009](Images/F13009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-9: Two neural networks drawn as graphs. Data flows from node to node
    along the edges, following the arrows. When the edges are not labeled with an
    arrow, data usually flows left-to-right or bottom-to-top. (a) Mostly left-to-right
    flow. (b) Mostly bottom-to-top flow.'
  prefs: []
  type: TYPE_NORMAL
- en: We say that a node near the inputs in [Figure 13-9](#figure13-9)(a) is *before*
    a node nearer to the outputs, which comes *after* it. In [Figure 13-9](#figure13-9)(b),
    we’d say a node near the inputs is *below* a node near the outputs, which is *above*
    it. Sometimes this below/above language is used even when the graph is drawn left
    to right, which can be confusing. It can help to think of *below* as “closer to
    the inputs,” and *above* as “closer to the outputs.”
  prefs: []
  type: TYPE_NORMAL
- en: We also sometimes say that if data flows from one node to another (let’s say
    it flows from A to B), then node A is an *ancestor* or *parent* of B, and node
    B is a *descendant* or *child* of A.
  prefs: []
  type: TYPE_NORMAL
- en: A common rule in neural networks is that there are no loops. This means that
    data coming out of a node can never make its way back into that same node, no
    matter how circuitous a path it follows. The formal name for this kind of graph
    is a *directed acyclic graph* (or *DAG*, pronounced to rhyme with “drag”). The
    word *directed* here means that the edges have arrows (which may only be implied,
    as we mentioned earlier). The word *acyclic* means there are no *cycles*, or loops.
    As always, there are exceptions to the rules, but they’re rare. We’ll see one
    such exception when we discuss recurrent neural networks (RNNs) in Chapter 19.
  prefs: []
  type: TYPE_NORMAL
- en: DAGs are popular in many fields, including machine learning, because they are
    significantly easier to understand, analyze, and design than arbitrary graphs
    that have loops. Including loops can introduce *feedback*, where a node’s output
    is returned to its input. Anyone who’s moved a live microphone too close to a
    speaker is familiar with how quickly feedback can grow out of control. The acyclic
    nature of a DAG naturally avoids the feedback problem, which saves us from dealing
    with this complex issue.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that a graph or network in which data only flows forward from inputs
    to outputs is called *feed-forward*. In Chapter 14, we’ll see that a key step
    in training neural networks involves temporarily flipping the arrows around, sending
    a particular type of information from the output nodes back to the input nodes.
    Although the normal flow of data is still feed-forward, when we push data through
    it backward, generally we call that a *feed-backward*, *backward-flow*, or *reverse-feed*
    algorithm. We reserve the word *feedback* for situations in which a loop in the
    graph can enable a node to receive its own output as input. As we’ve said, we
    generally avoid feedback in neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting graphs like those in [Figure 13-9](#figure13-9) usually means picturing
    the information as it flows along the edges, from one node to the next. But this
    picture only makes sense if we make some conventional assumptions. Let’s look
    at those now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Though we often use the word *flow* in various forms when referring to how
    data moves through the graph, this isn’t like the flow of water through pipes.
    Water flowing through pipes is a *continuous* process: new molecules of water
    flow through the pipes at every moment. The graphs we work with (and the neural
    networks they represent) are *discrete*: information arrives one chunk at a time,
    like text messages.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall from [Figure 13-5](#figure13-5) that we can draw a neural network by
    placing a weight on each edge (rather than inside a neuron). We call this style
    of the network a *weighted graph.* As we saw in [Figure 13-6](#figure13-6), we
    rarely draw the weights explicitly, but they are implied. It is always the case
    that in any neural network graph, even if no weights are explicitly shown, we
    are to understand that a unique weight is on each edge and as a value moves from
    one neuron to another along that edge that value is multiplied by the weight.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the Weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Teaching a neural network involves gradually improving the weights. The process
    begins when we assign initial values to the weights. How should we pick these
    starting values? It turns out that, in practice, how we initialize the weights
    can have a big effect on how quickly our network learns.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have developed theories for good ways to pick the initial values
    for the weights, and the various algorithms that have proved most useful are each
    named after the lead authors on the publications that describe them. The *LeCun
    Uniform*, *Glorot Uniform* (or *Xavier Uniform*), and *He Uniform* algorithms
    are all based on selecting initial values from a uniform distribution (LeCun et
    al. 1998; Glorot and Bengio 2010; He et al. 2015). It probably won’t be much of
    a surprise that the similarly named *LeCun Normal*, *Glorot Normal* (or *Xavier
    Normal*), and *He Normal* initialization methods draw their values from a normal
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: We don’t need to get into the math behind these algorithms. Happily, modern
    deep learning libraries offer each of these schemes, plus variations on them.
    Often the technique used by the library by default works great, so we rarely need
    to explicitly choose how to initialize the weights.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of the many possible ways to organize neurons in a network, placing them in
    a series of layers has proven to be both flexible and extremely powerful. Typically,
    neurons within a layer aren’t connected to one another. Their inputs come from
    the previous layer, and their outputs go to the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, the phrase *deep learning* comes from this structure. If we imagine
    many layers drawn side by side, we might call the network “wide.” If they were
    drawn vertically and we stood at the bottom looking up, we might call it “tall.”
    If we stood at the top and looked down, we might call it “deep.” And that’s all
    that *deep learning* means: a network made of a series of layers that we often
    draw vertically.'
  prefs: []
  type: TYPE_NORMAL
- en: A result of organizing neurons in layers is that we can analyze data hierarchically.
    The early layers process the raw input data, and each subsequent layer is able
    to use information from neurons on the previous layer to process larger chunks
    of data. For example, when considering a photograph, the first layer usually looks
    at the individual pixels. The next layer looks at groups of pixels, the one after
    that looks at groups of those groups, and so on. Early layers might notice that
    some pixels are darker than others, whereas later layers might notice that a clump
    of pixels looks like an eye, and a much later layer might notice the collection
    of shapes that reveal that the whole image shows a tiger.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-10](#figure13-10) shows an example of a deep learning architecture
    using three layers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F13010](Images/F13010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-10: A deep learning network'
  prefs: []
  type: TYPE_NORMAL
- en: When we draw the layers vertically, as in [Figure 13-10](#figure13-10), the
    inputs are almost always drawn at the bottom, and the outputs where we collect
    our results are almost always drawn at the top.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 13-10](#figure13-10), all three layers contain neurons. In practical
    systems, we usually use lots of other kinds of layers, which we might group together
    as *support layers*. We’ll see many such layers in later chapters. When we count
    the number of layers in a network, we usually don’t count these support layers.
    [Figure 13-10](#figure13-10) would be described as a deep network of three layers.
  prefs: []
  type: TYPE_NORMAL
- en: The topmost layer that contains neurons (Layer 3 in [Figure 13-10](#figure13-10))
    is called the *output layer*.
  prefs: []
  type: TYPE_NORMAL
- en: We would probably expect that Layer 1 in [Figure 13-10](#figure13-10) would
    be called the *input layer*, but it’s not. In a quirk of terminology, the term
    *input layer* is applied to the bottom of the network, labeled “Inputs” in [Figure
    13-10](#figure13-10). There’s no processing in this “layer.” Instead it’s just
    the memory where the input values reside. The input layer is an example of a support
    layer because it has no neurons, and therefore isn’t included when we count the
    layers in a network. The number of layers we count is called the network’s *depth*.
  prefs: []
  type: TYPE_NORMAL
- en: If we imagine standing above the top of [Figure 13-10](#figure13-10) and looking
    down, we only see the output layer. If we imagine we are below the bottom and
    looking up, we only see the input layer. The layers in between aren’t visible
    to us. Each of these layers between the input and output is called a *hidden layer*.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the stack is drawn left to right, as in [Figure 13-11](#figure13-11).
  prefs: []
  type: TYPE_NORMAL
- en: '![F13011](Images/F13011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-11: The same deep network of [Figure 13-10](#figure13-10), but drawn
    with data flowing left to right'
  prefs: []
  type: TYPE_NORMAL
- en: Even when drawn this way, we still use terms that refer to the vertical orientation.
    Authors might say that Layer 2 is “above” Layer 1, and “below” Layer 3\. We can
    always keep things straight regardless of how the diagram is drawn if we think
    of “above” or “higher” as referring to a layer closer to the outputs, and “below”
    or “lower” as meaning closer to the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Fully Connected Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *fully connected layer* (also called an *FC*, *linear*, or *dense* layer)
    is a set of neurons that each receive an input from *every* neuron on the previous
    layer. For example, if there are three neurons in a dense layer, and four neurons
    in the preceding layer, then each neuron in the dense layer has four inputs, one
    from each neuron in the preceding layer, for a total of 3 × 4 = 12 connections,
    each with an associated weight.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-12](#figure13-12)(a) shows a diagram of a fully connected layer
    with three neurons, coming after a layer with four neurons.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F13012](Images/F13012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-12: A fully connected layer. (a) The colored neurons make up a fully
    connected layer. Each of the neurons in this layer receives an input from every
    neuron in the previous layer. (b) Our schematic symbol for a fully connected layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-12](#figure13-12)(b) shows a schematic shorthand that we’ll use
    for fully connected layers. The idea is that two neurons are at the top and bottom
    of the symbol, and the vertical and diagonal lines are the four connections between
    them. Next to the symbol, we identify how many neurons are in the layer, as we’ve
    done here with the number 3\. When it’s relevant, this is also where we identify
    that layer’s activation function. If a layer is made up of only dense layers,
    it is sometimes called a *fully connected network*, or, in a throwback to earlier
    terminology, a *multilayer perceptron* *(MLP)*.'
  prefs: []
  type: TYPE_NORMAL
- en: In later chapters, we’ll see many other types of layers that help us organize
    our neurons in useful ways. For example, *convolution layers* and *pooling layers*
    have proven very useful for image processing tasks, and we’ll give them a lot
    of attention.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve seen that a deep learning system is built from a sequence of layers. And
    though the output of any neuron is a single number, we often want to talk about
    the output of an entire layer at once. The key idea that characterizes this collection
    of output numbers is its shape. Let’s see what that means.
  prefs: []
  type: TYPE_NORMAL
- en: If the layer contains a single neuron, the layer’s output is just a single number.
    We might describe this as an array, or a list, with one element. Mathematically,
    we can call this a *zero-dimensional array*. The number of dimensions in an array
    tells us how many indices we need to use to identify an element. Since a single
    number needs no indices, that array has zero dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: If we have multiple neurons in a layer, then we can describe their collective
    output as a list of all the values. Since we need one index to identify a particular
    output value in this list, this is a one-dimensional (1D) array. [Figure 13-13](#figure13-13)(a)
    shows such an array containing 12 elements.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13013](Images/F13013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-13: Three tensors, each with 12 elements. (a) A 1D tensor is a list.
    (b) A 2D tensor is a grid. (c) A 3D tensor is a volume. In all cases, and in higher-dimensional
    cases as well, there are no holes and no elements stick out from the block.'
  prefs: []
  type: TYPE_NORMAL
- en: We frequently organize our data into other box-like shapes. For instance, if
    the input to our system is a black and white image, it can be represented as a
    2D array, as in [Figure 13-13](#figure13-13)(b), indexed by x and y positions.
    If it’s a color image, then it can be represented as a 3D array, indexed by x
    position, y position, and color channel. A 3D shape is shown in [Figure 13-13](#figure13-13)(c).
  prefs: []
  type: TYPE_NORMAL
- en: 'We frequently call a 1D shape an *array*, *list*, or *vector*. To describe
    a 2D shape we often use the terms *grid* or *matrix*, and we can describe a 3D
    shape as a *volume* or *block*. We will often use arrays with even more dimensions.
    Rather than create a mountain of new terms, we use a single term for any collection
    of numbers arranged in a box shape with any number of dimensions: a *tensor* (pronounced
    ten′-sir).'
  prefs: []
  type: TYPE_NORMAL
- en: A tensor is merely a block of numbers with a given number of dimensions and
    a size in each dimension. It has no holes and no bits sticking out. The term *tensor*
    has a more complex meaning in some fields of math and physics, but in machine
    learning, we use this word to mean a collection of numbers organized into a multidimensional
    block. Taken together, the number of dimensions and the size in each dimension
    provide the *shape* of the tensor.
  prefs: []
  type: TYPE_NORMAL
- en: We often refer to a network’s *input tensor* (meaning all the input values),
    and its *output tensor* (meaning all the output values). The outputs of internal
    (or hidden) layers have no special name, so we usually say something like “the
    tensor produced by layer 3” to refer to the multidimensional array of numbers
    coming out of the neurons on layer 3.
  prefs: []
  type: TYPE_NORMAL
- en: Preventing Network Collapse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier we promised to return to activation functions. Let’s look at them now.
  prefs: []
  type: TYPE_NORMAL
- en: Each activation function, while a small piece of the overall structure, is critical
    to a successful neural network. Without activation functions, the neurons in a
    network combine, or *collapse*, into the equivalent of a single neuron. And, as
    we saw earlier, one neuron has very little computational power.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how a network collapses when it doesn’t have activation functions.
    [Figure 13-14](#figure13-14) shows a little network with two inputs (A and B),
    and five neurons (E through G) on three layers. Every neuron receives an input
    from every neuron on the previous layer, and each connection has a weight, giving
    us a total of ten weights, shown in red.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13014](Images/F13014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-14: A little network of two inputs, five neurons, and ten weights'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s suppose for the moment that these neurons don’t have activation functions.
    Then we can write the output of each neuron as a weighted sum of its inputs, as
    in [Figure 13-15](#figure13-15). In this figure, we’re using the mathematical
    convention of leaving out the multiplication sign when possible, so 2A is shorthand
    for 2 × A.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13015](Images/F13015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-15: Each neuron is labeled with the value of its output.'
  prefs: []
  type: TYPE_NORMAL
- en: The outputs of C and D depend only on A and B. Similarly, the outputs of E and
    F only depend on the outputs of C and D, which means that they, too, ultimately
    depend only on A and B. The same argument holds for G. If we start with the expression
    for G, plug in the values for E and F, and then plug in the values for C and D,
    we get a big expression in terms of A and B. If we do that and simplify, we find
    that the output of G is 78A + 86B. We can write this as a single neuron with two
    new weights, as shown in [Figure 13-16](#figure13-16).
  prefs: []
  type: TYPE_NORMAL
- en: This output of G in [Figure 13-16](#figure13-16) is exactly the same as the
    output of G in [Figure 13-14](#figure13-14). Our whole network has collapsed into
    a single neuron!
  prefs: []
  type: TYPE_NORMAL
- en: No matter how big or complicated our neural network is, if it has no activation
    functions, then it will always be equivalent to a single neuron. This is bad news
    if we want our network to be able to do anything more than what one neuron can
    do.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13016](Images/F13016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-16: This network’s output is exactly the same as the output in [Figure
    13-14](#figure13-14).'
  prefs: []
  type: TYPE_NORMAL
- en: In mathematical language, we say that our fully connected network collapsed
    because it only used addition and multiplication, which are in the category of
    *linear functions*. Linear functions can combine as we just saw, but *nonlinear
    functions* are fundamentally different and don’t combine this way. By designing
    activation functions to use *nonlinear* operations, we prevent this kind of collapse.
    We sometimes call an activation function a *nonlinearity*.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different types of activation functions, each producing different
    results. Generally speaking, the variety is there because in some situations,
    some functions can run into numerical trouble, making training run more slowly
    than it should, or even cease altogether. If that happens, we can substitute an
    alternative activation function that avoids the problem (though of course it has
    its own weak points).
  prefs: []
  type: TYPE_NORMAL
- en: In practice, a handful of activation functions are all we usually use. When
    reading the literature and looking at other people’s networks, we sometimes see
    the rarer activation function. Let’s survey the functions that by most major libraries
    usually provide and then gather together the most common ones.
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An *activation function* (sometimes also called a *transfer function*, or a
    *non-linearity*) takes a floating-point number as input and returns a new floating-point
    number as output. We can define these functions by drawing them as little graphs,
    without any equations or code. The horizontal, or X, axis is the input value,
    and the vertical, or Y, axis is the output value. To find the output for any input,
    we locate the input along the X axis, and move directly upward until we hit the
    curve. That’s the output value.
  prefs: []
  type: TYPE_NORMAL
- en: In theory, we can apply a different activation function to every neuron in our
    network, but in practice, we usually assign the same activation function to all
    the neurons in each layer.
  prefs: []
  type: TYPE_NORMAL
- en: Straight-Line Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s first look at activation functions that are made up of one or more straight
    lines. [Figure 13-17](#figure13-17) shows a few “curves” that are just straight
    lines.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the leftmost example in [Figure 13-17](#figure13-17). If we pick
    any point on the X axis, and go vertically up until we hit the line, the value
    of that intersection on the Y axis is the same as the value on the X axis. The
    output, or y value, of this curve is always the same as the input, or x value.
    We call this the *identity function*.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13017](Images/F13017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-17: Straight-line functions. The leftmost function is called the
    identity function.'
  prefs: []
  type: TYPE_NORMAL
- en: The other curves in [Figure 13-17](#figure13-17) are also straight lines, but
    they’re tilted to different slopes. We call any curve that’s just a single straight
    line a *linear function*, or even (slightly confusingly) a *linear curve*.
  prefs: []
  type: TYPE_NORMAL
- en: These activation functions do not prevent network collapse. When the activation
    function is a single straight line, then mathematically, it’s only doing multiplication
    and addition, and that means it’s a linear function and the network can collapse.
    These straight-line activation functions usually appear only in two specific situations.
  prefs: []
  type: TYPE_NORMAL
- en: The first application is on a network’s output neurons. There’s no risk of collapse
    since there are no neurons after the output. The top of [Figure 13-18](#figure13-18)
    shows the idea.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13018](Images/F13018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-18: Using the identity as an activation function. Top: The identity
    function on an output neuron. Bottom: Using an identity function to insert a step
    of processing between the summation step and a nonlinear activation function for
    any neuron.'
  prefs: []
  type: TYPE_NORMAL
- en: The second situation where we use a straight-line activation function is when
    we want to insert some processing between the summation step in a neuron and its
    activation function. In this case, we apply the identity function to the neuron,
    perform the processing step, and then perform the nonlinear activation function,
    as shown at the bottom of [Figure 13-18](#figure13-18).
  prefs: []
  type: TYPE_NORMAL
- en: Since we generally want nonlinear activation functions, we need to get away
    from a single straight line. All of the following activation functions are nonlinear
    and prevent network collapse.
  prefs: []
  type: TYPE_NORMAL
- en: Step Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We don’t want a straight line, but we can’t pick just any curve. Our curve needs
    to be single-valued. As we discussed in Chapter 5, this means that if we look
    upward from any value of x along the X axis, there’s only one value of y above
    us. An easy variation on a linear function is to start with a straight line and
    break it up into several pieces. They don’t even have to join. In the language
    of Chapter 5, this means that they don’t have to be continuous.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-19](#figure13-19) shows an example of this approach. We call this
    a *stair-step function*. In this example, it outputs the value 0 if the input
    is from 0 to just less than 0.2, but then the output is 0.2 if the input value
    is from 0.2 to just less than 0.4, and so on. These abrupt jumps don’t violate
    our rule that the curve has only one y output value for each input x value.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F13019](Images/F13019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-19: This curve is made up of multiple straight lines. A filled circle
    tells us that the y value there is valid, whereas an open circle tells us that
    there is no curve at that point.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest stair-step function has only a single step. This is a frequent
    special case, so it gets its own name: the *step function*. The original perceptron
    of [Figure 13-2](#figure13-2) used a step function as its activation function.
    A step function is usually drawn as in [Figure 13-20](#figure13-20)(a). It has
    one value until some *threshold* and then it has some other value.'
  prefs: []
  type: TYPE_NORMAL
- en: Different people have different preferences for what happens when the input
    has precisely the value of the threshold. In [Figure 13-20](#figure13-20)(a) we’re
    showing that the value at the threshold is the value of the right side of the
    step, as shown by the solid dot.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13020](Images/F13020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-20: A step function has two fixed values, one each to the left and
    right of a threshold value of x.'
  prefs: []
  type: TYPE_NORMAL
- en: Often authors are casual about what happens when the input is exactly at the
    transition, and draw the picture as in [Figure 13-20](#figure13-20)(b) in order
    to stress the “step” of the function. This is an ambiguous way to draw the curve
    because we don’t know what value is intended when the input is precisely at the
    threshold, but it’s a common kind of drawing (often we don’t care which value
    is used at the threshold, so we can choose whatever we prefer).
  prefs: []
  type: TYPE_NORMAL
- en: A couple of popular versions of the step have their own names. The *unit step*
    is 0 to the left of the threshold, and 1 to the right. [Figure 13-21](#figure13-21)
    shows this function.
  prefs: []
  type: TYPE_NORMAL
- en: If the threshold value of a unit step is 0, then we give it the more specific
    name of the *Heaviside step*, also shown in [Figure 13-21](#figure13-21).
  prefs: []
  type: TYPE_NORMAL
- en: '![F13021](Images/F13021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-21: Left: The unit step has a value of 0 to the left of the threshold,
    and 1 to the right. Right: The Heaviside step is a unit step where the threshold
    is 0.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if we have a Heaviside step (so the threshold is at 0) but the value
    to the left is −1 rather than 0, we call this the *sign function*, shown in [Figure
    13-22](#figure13-22). There’s a popular variation of the sign function where input
    values that are exactly 0 are assigned an output value of 0\. Both variations
    are commonly called “the sign function,” so when the difference matters, it’s
    worth paying attention to figure out which one is being referred to.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13022](Images/F13022.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-22: Two versions of the sign function. Left: Values less than 0 are
    assigned an output of −1, all others are 1\. Right: Like the left, except that
    an input of exactly 0 gets the value 0.'
  prefs: []
  type: TYPE_NORMAL
- en: Piecewise Linear Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If a function is made up of several pieces, each of which is a straight line,
    we call it *piecewise linear*. This is still a nonlinear function as long as the
    pieces, taken together, don’t form a single straight line.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the most popular activation function is a piecewise linear function
    called a *rectifier*, or *rectified linear unit*, which is abbreviated *ReLU*
    (note that the e is lowercase). The name comes from an electronics part called
    a rectifier, which can be used to prevent negative voltages from passing from
    one part of a circuit to another (Kuphaldt 2017). When the voltage goes negative,
    the physical rectifier clamps it to 0, and our rectified linear unit does the
    same thing with the numbers that are fed into it.
  prefs: []
  type: TYPE_NORMAL
- en: The ReLU’s graph is shown in [Figure 13-23](#figure13-23). It’s made up of two
    straight lines, but thanks to the kink, or bend, this is nota linear function.
    If the input is less than 0, then the output is 0\. Otherwise, the output is the
    same as the input.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13023](Images/F13023.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-23: The ReLU, or rectified linear unit. It outputs 0 for all negative
    inputs, otherwise the output is the input.'
  prefs: []
  type: TYPE_NORMAL
- en: The ReLU activation function is popular because it’s a simple and fast way to
    include a nonlinearity at the end of our artificial neurons. But there’s a potential
    problem. As we’ll see in Chapter 14, if changes in the input don’t lead to changes
    in the output, a network can stop learning. And the ReLU has an output of 0 for
    every negative value. If our input changes from, say, –3 to –2, then the output
    of ReLU stays at 0\. Fixing this problem has led to the development of the ReLU
    variations that follow.
  prefs: []
  type: TYPE_NORMAL
- en: Despite this issue, ReLU (or leaky ReLU, which we’ll see next) often performs
    well in practice, and people often use it as their default choice when building
    a new network, particularly for fully connected layers. Beyond the fact that these
    activation functions work well in practice, there are good mathematical reasons
    for wanting to use ReLU (Limmer and Stanczak 2017), though we won’t explore them
    here.
  prefs: []
  type: TYPE_NORMAL
- en: The *leaky ReLU* changes the response for negative values. Rather than output
    a 0 for any negative value, this functions outputs the input, scaled down by a
    factor of 10\. [Figure 13-24](#figure13-24) shows this function.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13024](Images/F13024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-24: The leaky ReLU is like the ReLU, but it returns a scaled-down
    value of x when x is negative.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there’s no need to always scale down the negative values by a factor
    of 10\. A *parametric ReLU* lets us choose by how much negative amounts are scaled,
    as shown in [Figure 13-25](#figure13-25).
  prefs: []
  type: TYPE_NORMAL
- en: When using a parametric ReLU, the essential thing is to never select a factor
    of exactly 1.0, because then we lose the kink, the function becomes a straight
    line, and any neuron we apply this to collapses with those that immediately follow
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13025](Images/F13025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-25: A parametric ReLU is like a leaky ReLU, but the slope for values
    of x that are less than 0 can be specified.'
  prefs: []
  type: TYPE_NORMAL
- en: Another variation on the basic ReLU is the *shifted ReLU*, which just moves
    the bend down and left. [Figure 13-26](#figure13-26) shows an example.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13026](Images/F13026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-26: The shifted ReLU moves the bend in the ReLU function down and
    left.'
  prefs: []
  type: TYPE_NORMAL
- en: We can generalize the various flavors of ReLU with an activation function called
    *maxout* (Goodfellow et al. 2013). Maxout allows us to define a set of lines.
    The output of the function at each point is the largest valueamong all the lines,
    evaluated at that point. [Figure 13-27](#figure13-27) shows maxout with just two
    lines, forming a ReLU, as well as two other examples that use more lines to create
    more complex shapes.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13027](Images/F13027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-27: The maxout function lets us build up a function from multiple
    straight lines. The heavy red line is the output of maxout for each set of lines.'
  prefs: []
  type: TYPE_NORMAL
- en: Another variation on the basic ReLU is to add a small random value to the input
    before running it through a standard ReLU. This function is called a *noisy ReLU*.
  prefs: []
  type: TYPE_NORMAL
- en: Smooth Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we’ll see in Chapter 14, a key step in teaching neural networks involves
    computing derivatives for the outputs of neurons, which necessarily involve their
    activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: The activation functions that we saw in the last section (except for the linear
    functions) create their nonlinearities by using multiple straight lines with at
    least one kink in the collection. Mathematically, there is no derivative at the
    kink between a pair of straight lines, and therefore the function is not linear.
  prefs: []
  type: TYPE_NORMAL
- en: If these kinks prevent the computation of derivatives, which are necessary for
    teaching a network, why are functions like ReLU useful at all, let alone so popular?
    It turns out that standard mathematical tools can finesse the sharp corners like
    those in ReLU and still produce a derivative (Oppenheim and Nawab 1996). These
    tricks don’t work on all functions, but one of the principles that guided the
    development of the functions we saw earlier is that they allow these methods to
    be used.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative to using multiple straight lines and then patching up the problems
    is to use smooth functions that inherently have a derivative everywhere. That
    is, they’re smooth everywhere. Let’s look at a few popular and smooth activation
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: The *softplus* function simply smooths out the ReLU, as shown in [Figure 13-28](#figure13-28).
  prefs: []
  type: TYPE_NORMAL
- en: '![F13028](Images/F13028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-28: The softplus function is a smoothed version of the ReLU.'
  prefs: []
  type: TYPE_NORMAL
- en: We can smooth out the shifted ReLU as well. This is called the *exponential
    ReLU*, or *ELU* (Clevert, Unterthiner, and Hochreiter 2016). It’s shown in [Figure
    13-29](#figure13-29).
  prefs: []
  type: TYPE_NORMAL
- en: '![F13029](Images/F13029.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-29: The exponential ReLU, or ELU'
  prefs: []
  type: TYPE_NORMAL
- en: Another way to smooth out the ReLU is called *swish* (Ramachandran, Zoph, and
    Le 2017). [Figure 13-30](#figure13-30) shows what this looks like. In essence
    it’s a ReLU, but with a small, smooth bump just left of 0, which then flattens
    out.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13030](Images/F13030.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-30: The swish activation function'
  prefs: []
  type: TYPE_NORMAL
- en: Another popular smooth activation function is the *sigmoid*, also called the
    *logistic function* or *logistic curve*. This is a smoothed-out version of the
    Heaviside step. The name *sigmoid* comes from the resemblance of the curve to
    an S shape, while the other names refer to its mathematical interpretation. [Figure
    13-31](#figure13-31) shows this function.
  prefs: []
  type: TYPE_NORMAL
- en: Closely related to the sigmoid is another mathematical function called the *hyperbolic
    tangent*. It’s much like the sigmoid, only negative values are sent to –1 rather
    than to 0\. The name comes from the curve’s origins in trigonometry. It’s a big
    name, so it’s usually written simply as *tanh*. This is shown in [Figure 13-32](#figure13-32).
  prefs: []
  type: TYPE_NORMAL
- en: We say that the sigmoid and tanh functions both *squash* their entire input
    range from negative to positive infinity into a small range of output values.
    The sigmoid squashes all inputs to the range [0, 1], while tanh squashes them
    to [−1, 1].
  prefs: []
  type: TYPE_NORMAL
- en: '![F13031](Images/F13031.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-31: The S-shaped sigmoid function is also called the logistic function
    or logistic curve. It has a value of 0 for very negative inputs, and a value of
    1 for very positive inputs. For inputs in the range of about −6 to 6, it smoothly
    transitions between the two.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F13032](Images/F13032.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-32: The hyperbolic tangent function, written tanh, is S-shaped like
    the sigmoid of [Figure 13-31](#figure13-31). The key differences are that it returns
    a value of −1 for very negative inputs, and the transition zone is a bit narrower.'
  prefs: []
  type: TYPE_NORMAL
- en: The two are shown on top of one another in [Figure 13-33](#figure13-33).
  prefs: []
  type: TYPE_NORMAL
- en: '![F13033](Images/F13033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-33: The sigmoid function (orange) and tanh function (teal), both
    plotted for the range −8 to 8'
  prefs: []
  type: TYPE_NORMAL
- en: Another smooth activation function uses a sine wave, as shown in [Figure 13-34](#figure13-34)
    (Sitzmann 2020). This squashes the outputs to the range [–1, 1] like tanh, but
    it doesn’t saturate (or stop changing) for inputs that are far from 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![F13034n](Images/F13034n.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-34: A sine wave activation function'
  prefs: []
  type: TYPE_NORMAL
- en: Activation Function Gallery
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Figure 13-35](#figure13-35) summarizes the activation functions we’ve discussed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F13035n](Images/F13035n.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-35: A gallery of popular activation functions'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Activation Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'ReLU used to be the most popular activation function, but in recent years,
    the leaky ReLU has been gaining in popularity. This is a result of practice: networks
    with leaky ReLU often learn faster.'
  prefs: []
  type: TYPE_NORMAL
- en: The reason is that ReLU has a problem, which we mentioned earlier. When a ReLU’s
    input is negative, its output is 0\. If the input is a large negative number,
    then changing it by a small amount still results in a negative input to ReLU and
    an unchanged output of 0\. This means that the derivative is also zero. As we’ll
    see in Chapter 14, when a neuron’s derivative goes to zero, not only does it stop
    learning, but it also makes it more likely that the neurons that precede it in
    the network will stop learning as well. Because a neuron whose output never changes
    no longer participates in learning, we sometimes use rather drastic language and
    say that the neuron has *died*. The leaky ReLU has been gaining in popularity
    over ReLU because, by providing an output that isn’t the same for every negative
    input, its derivative is not 0, and thus it does not die. The sine wave function
    also has a non-zero derivative almost everywhere (except at the very top and bottom
    of each wave).
  prefs: []
  type: TYPE_NORMAL
- en: After ReLU and leaky ReLU, sigmoid and tanh are probably the next most popular
    functions. Their appeal is that they’re smooth, and the outputs are bounded to
    [0, 1] or [–1, 1]. Experience has shown that networks learn most efficiently when
    all the values flowing through it are in a limited range.
  prefs: []
  type: TYPE_NORMAL
- en: There is no firm theory to tell us which activation function works best in a
    specific layer of a specific network. We usually start by making the same choices
    that have worked in other, similar networks that we’ve seen, and then we try alternatives
    if learning goes too slowly.
  prefs: []
  type: TYPE_NORMAL
- en: A few rules of thumb give us a good starting point in many situations. Generally
    speaking, we often apply ReLU or leaky ReLU to most neurons on hidden layers,
    particularly fully connected layers. For regression networks, we often use no
    activation function on the final layer (or if we must supply one, we use a linear
    activation function, which amounts to the same thing), because we care about the
    specific output value. When we’re classifying with just two classes, we have just
    a single output value. Here we often apply a sigmoid to push the output clearly
    to one class or the other. For classification networks with more than two classes,
    we almost always use a somewhat different kind of activation function, which we’ll
    look at next.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There’s an operation that we typically apply only to the output neurons of a
    classifier neural network, and even then, only if there are two or more output
    neurons. It’s not an activation function in the sense that we’ve been using the
    term because it takes as input the outputs of *all* the output neurons simultaneously.
    It processes them together and then produces a new output value for each neuron.
    Though it’s not quite an activation function, it’s close enough in spirit to activation
    functions to merit including it in this discussion.
  prefs: []
  type: TYPE_NORMAL
- en: The technique is called *softmax*. The purpose of softmax is to turn the raw
    numbers that come out of a classification network into class probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that softmax takes the place of any activation function
    we’d otherwise apply to those output neurons. That is, we give them no activation
    function (or, equivalently, apply the linear function) and then run those outputs
    into softmax.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mechanics of this process are involved with the mathematics of how the
    network computes its predictions, so we won’t go into those details here. The
    general idea is shown in [Figure 13-36](#figure13-36): *scores* come in, and *probabilities*
    come out.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F13036n](Images/F13036n.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-36: The softmax function takes all the network’s outputs and modifies
    them simultaneously. The result is that the scores are turned into probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Each output neuron presents a value, or score, that corresponds to how much
    the network thinks the input is of that class. In [Figure 13-36](#figure13-36)
    we’re assuming that we have three classes in our data, named A, B, and C, so each
    of the three output neurons gives us a score for its class. The larger the score,
    the more certain the system is that the input belongs to that class.
  prefs: []
  type: TYPE_NORMAL
- en: If one class has a larger score than some other class, it means the network
    thinks that class is more likely. That’s useful. But the scores aren’t designed
    to be compared in any other convenient way. For instance, if the score for A is
    twice that of B, it doesn’t mean that A is twice as likely as B. It just means
    that A is more likely. Because making comparisons like “twice as likely” is so
    useful, we use softmax to turn the output scores into probabilities. Now, if the
    softmax output of A is twice that of B, then indeed A is twice as probable as
    B. That’s such a useful way to look at the network’s output that we almost always
    use softmax at the end of a classification network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Any set of numbers that we want to treat as probabilities must satisfy two
    criteria: the values all lie between 0 and 1, and they add up to 1\. If we just
    modify each output of the network independently, we don’t know the other values,
    so we can’t make sure they added up to anything in particular. When we hand all
    the outputs to softmax, it can simultaneously adjust all the values so that they
    sum to 1\.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at softmax in action. Consider the top-left graph of [Figure 13-37](#figure13-37).
  prefs: []
  type: TYPE_NORMAL
- en: '![F13037](Images/F13037.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13-37: The softmax function takes all the network’s outputs and modifies
    them simultaneously. The result is that the scores are turned into probabilities.
    Top row: Scores from a classifier. Bottom row: Results of running the scores in
    the top row through softmax. Note that the graphs in the upper row use different
    vertical scales.'
  prefs: []
  type: TYPE_NORMAL
- en: The top left of [Figure 13-37](#figure13-37) shows the outputs for a classifier
    with six output neurons, which we’ve labeled A through F. In this example, all
    six of these values are between 0 and 1\. From this graph, we can see that the
    value for class B is 0.1 and the value for class C is 0.8\. As we’ve discussed,
    it is a mistake to conclude from this that the input is 8 times more likely to
    be in class C than class B, because these are scores and not probabilities. We
    can say that class C is more likely than class B, but anything more requires some
    math. To usefully compare these outputs to one another, we can apply softmax to
    carry out that math, and change them into probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: We show the output of softmax in the graph in the lower left. These are the
    probabilities of the input belonging to each of the six classes. It’s interesting
    to note that the big values, like C and F, get scaled down by a lot, but the small
    values, like B, are hardly scaled at all. This is a natural result of how scores
    between 0 and 1 turn into probabilities. But the ordering of the bars by size
    is still the same as it was for the scores (with C the largest, then F, then D,
    and so on). From the probabilities produced by softmax in the lower figure, we
    can see that class C has a probability of about 0.25, and class B has a probability
    of about 0.15\. We can conclude that the input is a little more than 1.5 times
    more probable to be in class C than class B.
  prefs: []
  type: TYPE_NORMAL
- en: The middle and right columns of [Figure 13-37](#figure13-37) show the outputs
    for two other hypothetical networks and inputs, before and after softmax. The
    three examples show that the output of softmax depends on whether the inputs are
    all less than 1\. The input ranges in [Figure 13-37](#figure13-37), reading left
    to right, are [0, 0.8], [0, 8], and [0, 3]. Softmax always preserves the ordering
    of its inputs (that is, if we sort the inputs from largest to smallest, they match
    a similar sort on the outputs). But when some input values are greater than 1,
    the largest value tends to stand out more. We say that softmax *exaggerates* the
    influence of the output with the largest value. Sometimes we also say that softmax
    *crushes* the other values, making the largest one dominate the others more obviously.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-37](#figure13-37) shows that the input range makes a big difference
    in the output of softmax. Softmax also has an interesting behavior depending on
    whether the inputs values are all less than 1, all greater than 1, or mixed.'
  prefs: []
  type: TYPE_NORMAL
- en: At the far left of [Figure 13-37](#figure13-37) all of the inputs are all less
    than 1, in the range [0, 0.8].
  prefs: []
  type: TYPE_NORMAL
- en: In the middle column, the inputs are all greater than 1, in the range [0, 8].
    Notice that in the output, the value of D (corresponding to the 8) clearly dominates
    all of the other values. Softmax has exaggerated the differences among the outputs,
    making it easier to pick out D as the largest.
  prefs: []
  type: TYPE_NORMAL
- en: On the far right of [Figure 13-37](#figure13-37) we have values both less and
    greater than 1, in the range [0, 3]. Here the exaggeration effect is somewhere
    between the left column, where all inputs are less than 1, and the middle column,
    where all inputs are greater than 1.
  prefs: []
  type: TYPE_NORMAL
- en: In all cases, though, softmax gives us back probabilities that are each between
    0 and 1, and sum up to 1\. The ordering of the inputs is always preserved, so
    the sequence of largest to smallest input is also the largest to smallest output.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Real, biological neurons are sophisticated nerve cells that process information
    using a fabulously complex range of chemical, electrical, and mechanical processes.
    They serve as the inspiration for a simple bit of computation that we call an
    artificial neuron, despite the enormous gulf between the computer version and
    its biological namesake. An artificial neuron multiplies each input value by a
    corresponding weight, adds the results, then passes that through an activation
    function. We can assemble artificial neurons into networks. Typically, those networks
    are DAGs: they are directed (information flows in only one direction), they are
    acyclic (no neuron ever receives its own output as an input), and they are graphs
    (the neurons are connected to one another). Input data enters at one end, and
    the network’s results appear at the other.'
  prefs: []
  type: TYPE_NORMAL
- en: We saw how, if we’re not careful in constructing our network, the entire network
    can collapse into a single neuron. We prevent this by using the activation function,
    a small function that takes each neuron’s output and turns it into a new number.
    These functions are designed to be nonlinear, meaning that they cannot be described
    merely by operations such as addition and multiplication. It is this nonlinearity
    that prevents the network from being equivalent to a single neuron. We concluded
    the chapter by looking at some of the more common activation functions, and how
    softmax can turn the numbers we get from a neural network into class probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The only difference between untrained deep learning systems and those that have
    been trained and are ready for deployment is in the value of the weights. The
    goal of training, or learning, is to find values for the weights so that the network’s
    output is correct for as many samples as possible. Since the weights start out
    with random numbers, we need some principled way to find these new, useful values.
    In Chapters 14 and 15, we’ll see how neural networks learn by looking at the two
    key algorithms that gradually improve the starting weights, transforming a network’s
    outputs into accurate, useful results.
  prefs: []
  type: TYPE_NORMAL
