- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Optimizers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: Training neural networks is frequently a time-consuming process. Anything that
    speeds it up is a welcome addition to our toolkit. This chapter is about a family
    of tools that are designed to speed up learning by improving the efficiency of
    gradient descent. The goals are to make gradient descent run faster and avoid
    some of the problems that can cause it to get stuck. These tools also automate
    some of the work of finding the best learning rate, including algorithms that
    can adjust that rate automatically over time. Collectively, these algorithms are
    called *optimizers*. Each optimizer has its strengths and weaknesses, so it’s
    worth becoming familiar with them so we can make good choices when training a
    neural network.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络通常是一个耗时的过程。任何能够加速这一过程的方法，都是我们工具箱中的一个受欢迎的补充。本章介绍了一类旨在通过提高梯度下降效率来加速学习的工具。其目标是让梯度下降运行得更快，并避免一些可能导致其陷入困境的问题。这些工具还自动化了寻找最佳学习率的部分工作，包括能够随着时间推移自动调整学习率的算法。这些算法统称为*优化器*。每个优化器都有其优缺点，因此熟悉它们是值得的，这样我们在训练神经网络时才能做出明智的选择。
- en: Let’s begin by drawing some pictures that enable us to visualize error and how
    it changes as we learn. These pictures will help us build some intuition for the
    algorithms yet to come.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先画一些图，帮助我们可视化误差以及它在学习过程中如何变化。这些图将帮助我们为接下来要介绍的算法建立一些直觉。
- en: Error as a 2D Curve
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 误差作为二维曲线
- en: It’s often helpful to think of the errors in our systems in terms of geometrical
    ideas. We frequently plot error as a 2D curve.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 将系统中的误差从几何学的角度进行思考，通常会非常有帮助。我们经常将误差绘制为二维曲线。
- en: To get familiar with this 2D error, let’s consider the task of splitting two
    classes of samples represented as dots arranged on a line. Dots at negative values
    are in one class, and dots at zero and above are in the other, as shown in [Figure
    15-1](#figure15-1).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了熟悉这种二维误差，我们考虑将表示为线上的点的两个类别样本进行划分的任务。负值处的点属于一个类别，零及以上的点属于另一个类别，如[图 15-1](#figure15-1)所示。
- en: '![F15001](Images/F15001.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![F15001](Images/F15001.png)'
- en: 'Figure 15-1: Two classes of dots on a line. Dots to the left of 0 are in class
    0, shown in blue, and the others are in class 1, shown in beige.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-1：线上的两个类别的点。位于 0 左侧的点属于类别 0，显示为蓝色，其他点属于类别 1，显示为米色。
- en: Let’s build a classifier for these samples. In this example, the boundary consists
    of just a single number. All samples to the left of that number are assigned to
    class 0, and all those to the right are assigned to class 1\. If we imagine moving
    this dividing point along the line, we can count up the number of samples that
    are misclassified and call that our error. We can summarize the results as a plot,
    where the X axis shows us each potential splitting point, and the error associated
    with that point is plotted as a dot above it. [Figure 15-2](#figure15-2) shows
    the result.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为这些样本构建一个分类器。在这个例子中，边界只由一个数字构成。所有位于该数字左侧的样本都被分配到类别 0，所有位于右侧的样本都被分配到类别 1。如果我们想象将这个划分点沿着线移动，我们可以统计被误分类的样本数量，并将其作为我们的误差。我们可以将结果总结为一个图表，其中
    X 轴展示了每个潜在的分割点，而与该点相关的误差则作为一个点绘制在其上。[图 15-2](#figure15-2)展示了这一结果。
- en: '![F15002](Images/F15002.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![F15002](Images/F15002.png)'
- en: 'Figure 15-2: Plotting the error function for a simple classifier'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-2：绘制简单分类器的误差函数
- en: We can smooth out the error curve of [Figure 15-2](#figure15-2) as shown in
    [Figure 15-3](#figure15-3).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将[图 15-2](#figure15-2)中的误差曲线平滑化，如[图 15-3](#figure15-3)所示。
- en: '![F15003](Images/F15003.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![F15003](Images/F15003.png)'
- en: 'Figure 15-3: A smoothed version of [Figure 15-2](#figure15-2)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-3：平滑后的[图 15-2](#figure15-2)版本
- en: For this particular set of random data, we see that the error is 0 when we’re
    at 0, or just a little to the left of it. This tells us that regardless of where
    we start, we want to end up with our divider just to the left of 0.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这组特定的随机数据，我们看到当我们处于 0 或者稍微偏左时，误差为 0。这告诉我们，无论从哪里开始，我们都希望最后将分隔线放在 0 的左侧。
- en: Our goal is to find a way to locate the smallest value of any error curve. When
    we can do that, we can apply the technique to all the weights of a neural network
    and thus reduce the whole network’s error.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到一种方法，定位任何误差曲线的最小值。当我们能够做到这一点时，我们就可以将这一技术应用于神经网络的所有权重，从而减少整个网络的误差。
- en: Adjusting the Learning Rate
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整学习率
- en: When we teach a system using gradient descent, the critical parameter is the
    learning rate, usually written with the lowercase Greek letter *η* (eta). This
    is often a value in the range 0.01 to 0.0001\. Larger values lead to faster learning,
    but they can lead us to miss valleys by jumping right over them. Smaller values
    of *η* (nearing 0, but always positive) lead to slower learning and can find narrow
    valleys, but they can also get stuck in gentle valleys even when there are much
    deeper ones nearby. [Figure 15-4](#figure15-4) recaps these phenomena graphically.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用梯度下降法来训练系统时，关键参数是学习率，通常用希腊字母 *η*（eta）表示。它通常的取值范围是0.01到0.0001。较大的值会导致更快的学习，但也可能会直接跳过深谷而错过它们。较小的
    *η* 值（接近0，但始终为正数）会导致学习较慢，并能够找到较窄的谷底，但也可能会停滞在较浅的谷底，即使附近存在更深的谷底。[图 15-4](#figure15-4)通过图形方式回顾了这些现象。
- en: An important idea shared by many optimizers is that we can improve learning
    by changing the learning rate as we go. The general thinking is analogous to hunting
    for buried metal on a beach using a metal detector. We start by taking big steps
    as we walk across the beach, but when the detector goes off, we take smaller and
    smaller steps to pinpoint the metal object’s location. In the same way, we usually
    take big steps along the error curve early in the learning process while we’re
    hunting for a valley. As time goes on, we hope that we found that valley, and
    we can now take smaller and smaller steps as we approach its lowest point.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 许多优化器共享一个重要的理念，即通过在学习过程中逐渐调整学习率来提高学习效果。这个思路类似于使用金属探测器在海滩上寻找埋藏的金属物品。我们开始时大步走过海滩，但当探测器响起时，我们会逐步缩小步伐，以精确定位金属物品的位置。以此类推，在学习过程的早期，我们通常会在误差曲线中大步前进，寻找谷底。随着时间的推移，我们希望找到了这个谷底，此时我们可以逐渐减小步伐，朝着最低点前进。
- en: '![F15004](Images/F15004.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![F15004](Images/F15004.png)'
- en: 'Figure 15-4: The influence of the learning rate, *η*. (a) When *η* is too large,
    we can jump right over a deep valley and miss it. (b) When *η* is too small, we
    can slowly descend into a local minimum, and miss the deeper valley.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-4：学习率 *η* 的影响。 (a) 当 *η* 太大时，我们可能会跳过一个深谷而错过它。 (b) 当 *η* 太小时，我们可能会慢慢下降到局部最小值，从而错过更深的谷底。
- en: We can illustrate our optimizers with a simple error curve containing a single
    isolated valley with the shape of a negative Gaussian, shown in [Figure 15-5](#figure15-5).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用一个简单的误差曲线来说明我们的优化器，该曲线包含一个孤立的谷底，其形状为负高斯分布，如[图 15-5](#figure15-5)所示。
- en: '![F15005](Images/F15005.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![F15005](Images/F15005.png)'
- en: 'Figure 15-5: Our error curve for looking at optimizers'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-5：我们查看优化器时的误差曲线
- en: Some gradients for this error curve are shown in [Figure 15-6](#figure15-6)
    (we’re actually showing the negative gradients).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该误差曲线的一些梯度在[图 15-6](#figure15-6)中展示（实际上我们展示的是负梯度）。
- en: '![F15006](Images/F15006.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![F15006](Images/F15006.png)'
- en: 'Figure 15-6: Our error curve and its negative gradients (scaled down by a factor
    of 0.25) at some locations'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-6：我们的误差曲线及其在某些位置的负梯度（缩小了0.25倍）
- en: The gradients in [Figure 15-6](#figure15-6) have been scaled down to 25 percent
    of their actual length for clarity. We can see that for this curve, the gradient
    is negative for input values that are less than 0 and positive for input values
    that are greater than 0\. When the input is 0, we’re at the very bottom of the
    bowl, so the gradient there is 0, drawn as just a single dot.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-6](#figure15-6)中的梯度已被缩小至实际长度的25%，以便清晰显示。我们可以看到，对于这条曲线，输入值小于0时梯度为负，输入值大于0时梯度为正。当输入为0时，我们处在碗底，所以此时梯度为0，表示为一个点。'
- en: Constant-Sized Updates
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常量大小的更新
- en: Let’s start our investigation of the effect of the learning rate by seeing what
    happens when we use a constant learning rate. In other words, we always scale
    the gradient by a value of *η* that stays fixed, or constant, during the whole
    training process.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过观察使用常量学习率时会发生什么，来开始研究学习率的影响。换句话说，我们始终使用一个固定的 *η* 值来调整梯度，该值在整个训练过程中保持不变。
- en: '[Figure 15-7](#figure15-7) shows the basic steps of updating with a fixed *η*.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-7](#figure15-7)展示了使用固定 *η* 进行更新的基本步骤。'
- en: '![F15007](Images/F15007.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![F15007](Images/F15007.png)'
- en: 'Figure 15-7: Finding the step for basic gradient descent'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-7：寻找基本梯度下降的步长
- en: Suppose we’re looking at a particular weight in a neural network. Let’s pretend
    that the weight begins with a value of w1, and we updated it once, so it now has
    the value w2, shown in [Figure 15-7](#figure15-7)(a). Its corresponding error
    is the point on the error curve directly above it, marked B. We want to update
    the weight again to a new and better value that we’ll call w3.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在查看神经网络中的一个特定权重。假设该权重初始值为w1，我们更新了一次它，现在它的值变为w2，如[图15-7](#figure15-7)(a)所示。其对应的误差是误差曲线上直接位于其上方的点，标记为B。我们希望再次更新该权重，得到一个新的、更好的值，称为w3。
- en: To update the weight, we find its gradient on the error surface at the point
    B, shown as the arrow labeled *g*. We scale the gradient by the learning rate
    *η* to get a new arrow labeled *ηg*. Because *η* is between 0 and 1, *ηg* is a
    new arrow that points in the same direction as *g* but is either the same size
    as *g* or smaller.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新权重，我们找到误差面上B点的梯度，表示为箭头*g*。我们通过学习率*η*来缩放该梯度，得到一个新箭头，标记为*ηg*。因为*η*介于0和1之间，*ηg*是一个与*g*方向相同但大小要么相同要么更小的新箭头。
- en: In [Figure 15-7](#figure15-7), the arrow we show for the gradient *g* is actually
    the *opposite*, or negative, of the gradient. The positive and negative gradients
    point in opposite directions along the same line, so people tend to refer to simply
    *the gradient* when the choice of positive or negative can be understood from
    context. We’ll follow that convention in this chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图15-7](#figure15-7)中，我们显示的梯度箭头*g*实际上是梯度的*相反*方向，或负梯度。正梯度和负梯度沿同一条线指向相反的方向，因此当正负梯度的选择能够从上下文中理解时，人们通常直接称之为*梯度*。我们将在本章中遵循这一约定。
- en: To find w3, the new value of the weight, we add the scaled gradient to w2\.
    In pictures, this means we place the tail of the arrow *ηg* at B, as in [Figure
    15-7](#figure15-7)(b). The horizontal position of the tip of that arrow is the
    new value of the weight, w3, and its value, directly above it on the error surface,
    is marked C. In this case, we stepped a bit too far and increased our error by
    a little.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到w3，即权重的新值，我们将缩放后的梯度加到w2上。用图示表示，这意味着我们将箭头*ηg*的尾部放置在B点，如[图15-7](#figure15-7)(b)所示。该箭头尖端的水平位置就是权重的新值w3，而它的值，直接位于其上方的误差面上，标记为C。在这种情况下，我们走得有点太远，导致误差略微增加。
- en: Let’s look at this technique in practice using an error curve with a single
    valley. [Figure 15-8](#figure15-8) shows a starting point in the upper left. The
    gradient here is small, so we move to the right a small amount. The error at that
    new point is a little less than the error we started with.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个具有单一谷底的误差曲线来看这个技巧的实际应用。[图15-8](#figure15-8)展示了左上角的起始点。这里的梯度很小，所以我们向右移动了一小步。此时新点的误差比我们开始时的误差稍微小一些。
- en: For these figures, we’ve chosen *η* = 1/8, or 0.125\. This is an unusually large
    value of *η* for constant-sized gradient descent, where we often use a value of
    1/100 or less. We chose this large value because it makes for clearer pictures.
    Smaller values work in similar ways, just more slowly. We aren’t showing values
    on the axes for these graphs to avoid visual clutter, since we’re more interested
    in the nature of what happens rather than the numbers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些图示，我们选择了*η* = 1/8，即0.125。这是一个非常大的*η*值，通常在恒定步长的梯度下降法中，我们常使用1/100或更小的值。我们选择这个较大的值是因为它能使图示更清晰。较小的值也能以类似的方式工作，只是速度更慢。我们没有在这些图表的坐标轴上显示数值，以避免视觉上的混乱，因为我们更关注发生的现象，而不是具体的数字。
- en: '![F15008](Images/F15008.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![F15008](Images/F15008.png)'
- en: 'Figure 15-8: Learning with a constant learning rate'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图15-8：使用恒定学习率的学习过程
- en: Rather than move from our first point by the entire gradient, we’re moving only
    1/8 of its length. This move takes us to a steeper part of the curve, where the
    gradient is larger, so the next update moves a little farther. Each step of learning
    is shown with a new color, which we use to draw the gradient from the previous
    location and then the new point.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不是通过整个梯度移动到第一个点，而是只移动了梯度长度的1/8。这一移动使我们到达了曲线的一个更陡峭部分，那里梯度较大，因此下一个更新会移动得更远。每一步学习都会用一种新的颜色显示，我们用这种颜色绘制上一个位置的梯度，然后是新点。
- en: We show a close-up of six steps in [Figure 15-9](#figure15-9), starting after
    the first step in [Figure 15-8](#figure15-8). We also show the error for each
    point.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[图15-9](#figure15-9)中展示了六步的特写，从[图15-8](#figure15-8)中的第一步开始。我们还显示了每个点的误差。
- en: '![F15009](Images/F15009.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![F15009](Images/F15009.png)'
- en: 'Figure 15-9: Left: A close-up of the final image in [Figure 15-8](#figure15-8).
    Right: The error associated with each of these six points.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-9：左：接近[图 15-8](#figure15-8)中的最终图像。右：与这六个点相关的误差。
- en: Will this process ever reach the bottom of the bowl and get down to 0 error?
    [Figure 15-10](#figure15-10) shows the first 15 steps in this process.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会最终到达碗底并将误差降至0吗？[图 15-10](#figure15-10)展示了该过程的前15步。
- en: '![F15010](Images/F15010.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![F15010](Images/F15010.png)'
- en: 'Figure 15-10: Left: The first 15 steps of learning with a constant learning
    rate. Right: The errors of these 15 points.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-10：左：使用恒定学习率的前15步。右：这些15个点的误差。
- en: We get near the bottom and then head up the hill on the right side. But that’s
    okay because the gradient here points down and to the left, so we head back down
    the valley until we overshoot the bottom again, and end up somewhere on the left
    side, then we turn around and overshoot again and end up on the right side, and
    so on, back and forth. We’re *bouncing around* the bottom of the bowl.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接近底部，然后沿右侧的山坡向上走。但这没关系，因为这里的梯度指向下方和左侧，所以我们又往下走进谷底，直到再次超过底部，最终出现在左侧，然后我们掉头再度超过底部，最终出现在右侧，如此反复。我们正在*在碗底反弹*。
- en: It doesn’t look like we’ll ever get to 0\. The problem is particularly bad in
    this symmetrical valley, as the error jumps back and forth between the left and
    right sides of the minimum. But this type of behavior happens a lot when we use
    a constant learning rate. The bouncing around is happening because when we’re
    near the bottom of a valley, we want to take small steps, but because our learning
    rate is a constant, we’re taking steps that are too big.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们永远也无法到达0。这个问题在对称的谷底特别严重，因为误差在最小值的左右两侧反复跳动。但是，当我们使用恒定学习率时，这种行为很常见。反弹现象的发生是因为我们靠近谷底时希望采取小步伐，但由于学习率是恒定的，我们采取的步伐过大。
- en: We might wonder if the bouncing problem of [Figure 15-10](#figure15-10) was
    caused by too large a learning rate. [Figure 15-11](#figure15-11) shows how things
    go for the first 15 steps of some smaller values of *η.*
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会想，[图 15-10](#figure15-10)中的反弹问题是否是由学习率过大引起的。[图 15-11](#figure15-11)显示了对于某些较小的*η*值，前15步的情况。
- en: '![F15011](Images/F15011.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![F15011](Images/F15011.png)'
- en: 'Figure 15-11: Taking 15 steps with our small learning rates. Top row: Learning
    rates of 0.025 (left column), 0.05 (middle column), and 0.1 (right column). Bottom
    row: Errors for the points in the top row.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-11：用小学习率进行15步。上排：学习率分别为0.025（左列）、0.05（中列）和0.1（右列）。下排：上排对应点的误差。
- en: As we can see from [Figure 15-11](#figure15-11), taking smaller steps doesn’t
    solve the bouncing problem, though the bounces are smaller. On the other hand,
    increasing the learning rate makes the bouncing problem worse, as shown in [Figure
    15-12](#figure15-12).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图 15-11](#figure15-11)中我们可以看到，采取较小的步伐并不能解决反弹问题，尽管反弹幅度变小了。另一方面，增加学习率会使反弹问题更加严重，正如[图
    15-12](#figure15-12)所示。
- en: '![f15012](Images/f15012.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![f15012](Images/f15012.png)'
- en: 'Figure 15-12: Top row: Learning rates of 0.5 (left column), 0.75 (middle column),
    and 1.0 (right column). Bottom row: Errors for the points in the top row.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-12：上排：学习率分别为0.5（左列）、0.75（中列）和1.0（右列）。下排：上排对应点的误差。
- en: Larger learning rates can also cause us to jump out of a nice valley with a
    low minimum. In [Figure 15-13](#figure15-13), starting at the green dot, we jump
    right over the rest of the valley we’re in (and would like to stay in) and into
    a new valley with a much larger minimum.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的学习率也可能导致我们跳出一个具有较低最小值的好谷底。在[图 15-13](#figure15-13)中，从绿色点出发，我们直接跳过了当前所在的谷底（并希望停留其中），进入了一个具有较大最小值的新谷底。
- en: '![F15013](Images/F15013.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![F15013](Images/F15013.png)'
- en: 'Figure 15-13: A large step overshoots the valley and ends up in a different
    valley with a higher minimum.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-13：大步伐超过谷底，最终进入一个具有更高最小值的不同谷底。
- en: Sometimes a big jump like this can help us move from a shallow valley to a deeper
    one, but for such a large learning rate, we’ll probably jump around valleys a
    lot, never finding a minimum. It seems like a challenge to find just one learning
    rate that moves at a reasonable speed but won’t overshoot valleys or get trapped
    bouncing around in the bottom. A nice alternative is to change the learning rate
    as we go.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有时像这样的较大跳跃可以帮助我们从一个较浅的山谷移动到更深的山谷，但对于如此大的学习率，我们可能会在山谷之间反复跳动，永远找不到最小值。找到一个既能以合理速度移动，又不会过度跳跃或困在底部反弹的学习率似乎是一个挑战。一个不错的替代方法是随着训练的进行逐步调整学习率。
- en: Changing the Learning Rate over Time
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随着时间变化调整学习率
- en: We can use a large value of *η* near the start of our learning so we don’t crawl
    along, and a small value near the end so we don’t end up bouncing around the bottom
    of a bowl.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在学习初期使用较大的 *η*，以避免进展缓慢，而在学习后期使用较小的 *η*，以避免在碗底部反复跳动。
- en: An easy way to start big and gradually get smaller is to multiply the learning
    rate by some number that’s almost 1 after every update step. Let’s use 0.99 as
    a multiplier and suppose that the starting learning rate is 0.1\. Then after the
    first step, it will be 0.1 × 0.99 = 0.099\. On the next step, it would be 0.099
    × 0.99 = 0.09801\. [Figure 15-14](#figure15-14) shows what happens to *η* when
    we do this for many steps using a few different values for the multiplier.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的开始时较大并逐渐变小的方法是每次更新后将学习率乘以一个接近 1 的数值。我们可以使用 0.99 作为乘数，假设初始学习率为 0.1。然后在第一步之后，它将变为
    0.1 × 0.99 = 0.099。在下一步，它将变为 0.099 × 0.99 = 0.09801。[图 15-14](#figure15-14)展示了当我们在多个步骤中使用不同的乘数时，*η*
    会发生什么变化。
- en: The easiest way to write the equation of these curves involves using exponents,
    so this kind of curve is called an *exponential decay* curve. The value by which
    we multiply *η* on every step is called the *decay parameter*. This is usually
    a number very close to 1\.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 编写这些曲线方程最简单的方法是使用指数，因此这种曲线被称为 *指数衰减* 曲线。我们在每一步乘以的值称为 *衰减参数*。这个值通常是一个接近 1 的数值。
- en: '![F15014](Images/F15014.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![F15014](Images/F15014.png)'
- en: 'Figure 15-14: Starting with a learning rate of *η* = 1, the various curves
    show how the learning rate drops after multiplying it by a given value after each
    update.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-14：从学习率 *η* = 1 开始，各种曲线显示了在每次更新后，学习率如何在乘以给定的值后下降。
- en: Let’s apply this gradual reduction of the learning rate to gradient descent
    on our error curve. Once again, we start with a learning rate of 1/8\. To make
    the effect of the decay parameter easily visible, let’s set it to the unusually
    low value of 0.8\. This means each step will only be 80 percent as long as the
    step before it. [Figure 15-15](#figure15-15) shows the result for the first 15
    steps.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这种学习率逐渐减少的方法应用于梯度下降中的误差曲线。我们再次从学习率 1/8 开始。为了使衰减参数的效果更加明显，我们将它设置为不寻常的低值 0.8。这意味着每一步将只有前一步的
    80%。[图 15-15](#figure15-15)展示了前 15 步的结果。
- en: '![F15015](Images/F15015.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![F15015](Images/F15015.png)'
- en: 'Figure 15-15: The first 15 steps using a shrinking learning rate'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-15：使用递减学习率的前 15 步
- en: Let’s compare this with our “bouncing” result from using a constant step size.
    [Figure 15-16](#figure15-16) shows the results for the constant and shrinking
    step sizes together for 15 steps.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个与使用常数步长时的“反弹”结果进行比较。[图 15-16](#figure15-16)展示了常数步长和递减步长的结果对比，进行 15 步更新。
- en: '![F15016](Images/F15016.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![F15016](Images/F15016.png)'
- en: 'Figure 15-16: On the left is the constant step size from [Figure 15-10](#figure15-10),
    and on the right is the decaying step size from [Figure 15-15](#figure15-15).
    Notice how the shrinking learning rate helps us efficiently settle into the minimum
    of the valley.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-16：左侧是[图 15-10](#figure15-10)中的常数步长，右侧是[图 15-15](#figure15-15)中的递减步长。注意到递减学习率帮助我们有效地在山谷的最小值处稳定下来。
- en: The shrinking step size does a beautiful job of landing us in the bottom of
    the bowl and keeping us there.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 递减步长成功地帮助我们稳定地达到碗底并保持在那里。
- en: Decay Schedules
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 衰减计划
- en: The decay technique is attractive, but it comes with some new challenges. First,
    we have to choose a value for the decay parameter. Second, we might not want to
    apply the decay after every update. To address these issues, we can try some other
    strategies for reducing the learning rate.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 衰减技术很有吸引力，但它带来了一些新挑战。首先，我们需要选择一个衰减参数的值。其次，我们可能不想在每次更新后都应用衰减。为了解决这些问题，我们可以尝试其他一些减少学习率的策略。
- en: Any given approach to changing the learning rate over time is called a *decay
    schedule* (Bengio 2012).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Decay schedules are usually expressed in epochs, rather than samples. We train
    on all the samples in our training set, and only then consider changing the learning
    rate before we train on all the samples again.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: The simplest decay schedule is to always apply decay to the learning rate after
    every epoch, as we just saw. [Figure 15-17](#figure15-17)(a) shows this schedule.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Another common scheduling method is to put off any decay at all for a while
    so our weights have a chance to get away from their starting random values and
    into something that might be close to finding a minimum. Then we apply whatever
    schedule we’ve picked. [Figure 15-17](#figure15-17)(b) shows this *delayed exponential
    decay* approach, putting off the exponential decay schedule of [Figure 15-17](#figure15-17)(a)
    for a few epochs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to apply the decay only every once in a while. The *interval
    decay* approach shown in [Figure 15-17](#figure15-17)(c), also called *fixed-step
    decay*, reduces the learning rate after every fixed number of epochs, say every
    4th or 10th. This way we don’t risk getting too small too fast.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![F15017](Images/F15017.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-17: Decay schedules for reducing the size of the learning rate over
    time. (a) Exponential decay, where the learning rate is reduced after every epoch.
    (b) Delayed exponential decay. (c) Interval decay, where the learning rate is
    reduced after every fixed number of epochs (here, 4). (d) Error-based decay, where
    the learning rate is reduced when the error stops dropping.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Yet another option is to monitor the error of our network. As long as the error
    is going down, we stick with whatever learning rate we have now. When the network
    stops learning, we apply the decay so it can take smaller steps and hopefully
    work its way into a deeper part of the error landscape. This *error-based decay*
    is shown in [Figure 15-17](#figure15-17)(d).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: We can easily cook up a lot of alternatives, such as applying decay only when
    the error decreases by a certain amount or certain percentage, or perhaps updating
    the learning rate by just subtracting a small value from it rather than multiplying
    it by a number close to 1 (as long as we stop at some positive value—if the learning
    rate went to 0, the system would stop learning, and if the learning rate went
    negative, the system would increase the error, rather than decrease it).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: We can even increase the learning rate over time if we want. The *bold driver*
    method looks at how the total loss is changing after each epoch (Orr 1999a; Orr
    1999b). If the error is going down, then we *increase* the learning rate a little,
    say 1 percent to 5 percent. The thinking is that if things are going well, and
    the error is dropping, we can take big steps. But if the error has gone up by
    more than just a little, then we slash the learning rate, cutting it by half.
    This way we can stop any increases immediately, before they can carry us too far
    away from the decreasing error we were previously enjoying.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意，甚至可以随着时间的推移增加学习率。*强力驱动*方法会查看每个训练周期后总损失的变化（Orr 1999a; Orr 1999b）。如果误差在下降，我们就*增加*学习率一点，比如从1%增加到5%。其思路是，如果一切进展顺利，且误差在下降，我们就可以采取大步伐。但如果误差上升得超过一点，我们就大幅降低学习率，将其减半。这样，我们可以立即停止任何增幅，以免它们把我们带离之前下降的误差。
- en: Learning rate schedules have the drawback that we have to pick their parameters
    in advance (Darken, Chang, and Moody 1992). We think of these parameters as *hyperparameters*,
    just like the learning rate itself. Most deep learning libraries offer routines
    that automatically search ranges of values for us to help us find the best values
    of one or more hyperparameters.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率调度的缺点是我们必须提前选择它们的参数（Darken, Chang, 和 Moody 1992）。我们将这些参数视为*超参数*，就像学习率本身一样。大多数深度学习库提供了自动搜索值范围的例程，帮助我们找到一个或多个超参数的最佳值。
- en: Generally speaking, simple strategies for adjusting the learning rate usually
    work well, and most machine-learning libraries let us pick one of them with little
    fuss (Karpathy 2016).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，调整学习率的简单策略通常效果很好，而且大多数机器学习库让我们可以轻松选择其中之一（Karpathy 2016）。
- en: Some kind of learning rate reduction is a common feature in most machine learning
    systems. We want to learn quickly in the early stages, moving in big steps over
    the landscape, looking for the lowest minimum we can find. Then we reduce the
    learning rate, enabling us to gradually take smaller steps and land in the very
    lowest part of whatever valley we’ve found.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习系统都具备某种形式的学习率衰减。我们希望在早期阶段快速学习，大步走过整个数据景观，寻找能找到的最低点。然后我们减少学习率，使得我们能够逐渐采取更小的步伐，最终停留在我们找到的山谷的最低处。
- en: It’s natural to wonder if there’s a way to control the learning rate that doesn’t
    depend on a schedule that we set up before we start training. Surely, we can somehow
    detect when we’re near a minimum, or in a bowl, or bouncing around, and automatically
    adjust the learning rate in response.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 自然会有人想知道是否有一种方法，可以控制学习率，而不依赖于我们在训练开始前设置的调度。我们一定能以某种方式检测到我们接近最小值、处于一个盆地中，或在四处跳动，并自动调整学习率以作出响应。
- en: An even more interesting question is to consider that maybe we don’t want to
    apply the same learning rate adjustments to all of our weights. It would be nice
    to be able to tune our updates so that each weight is learning at a rate that
    works best for it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 更有趣的问题是，或许我们并不希望对所有权重应用相同的学习率调整。能够调整更新，让每个权重以最适合它的学习速率进行学习，应该是件不错的事。
- en: Let’s look at some variations on gradient descent that address those ideas.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些针对梯度下降的变体，来解决这些问题。
- en: Updating Strategies
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新策略
- en: In the following sections, we compare the performance of three different ways
    to enhance gradient descent. In these examples, we use a small, but real, two-class
    classification problem.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将比较三种不同方式来增强梯度下降的表现。在这些例子中，我们使用一个小的，但真实的二分类问题。
- en: '[Figure 15-18](#figure15-18) shows our familiar dataset of two fuzzy crescent
    moons. The classes for these points are shown by color. These 300 samples are
    our reference data for the rest of this chapter.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15-18](#figure15-18)展示了我们熟悉的两个模糊的新月形数据集。这些点的类别通过颜色显示。300个样本是我们本章其余部分的参考数据。'
- en: In order to compare different networks, we need to train them until the error
    has reached a minimum, or seems to have stopped improving. We can show the results
    of our training with plots that graph the error after each epoch. Because of the
    wide variation in algorithms, the number of epochs in these graphs vary over a
    large range.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较不同的网络，我们需要训练它们直到误差达到最小值，或者似乎不再改善。我们可以通过绘制每个训练周期后的误差图表来展示训练结果。由于算法之间的差异，这些图表中的周期数变化范围较大。
- en: '![F15018](Images/F15018.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![F15018](Images/F15018.png)'
- en: 'Figure 15-18: The data we use for the rest of this chapter. The 300 points
    form two classes of 150 points each.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: To classify our points, we’ll use a neural network with three fully connected
    hidden layers (of 12, 13, and 13 nodes), and an output layer of 2 nodes, giving
    us the probability for each of the two classes. We’ll use ReLU on each hidden
    layer, and softmax at the end. Whichever class has the larger probability at the
    output is taken as our network’s prediction. For consistency, when we need a constant
    learning rate, we use a value of *η*= 0.01\. The network is shown in [Figure 15-19](#figure15-19).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![F15019](Images/F15019.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-19: Our network of four fully connected layers'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Batch Gradient Descent
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s begin by updating the weights just once per epoch, after we’ve evaluated
    all the samples. This is *batch gradient descent* (also called *epoch gradient
    descent*). In this approach, we run the entire training set through our system,
    accumulating the errors. Then we update all of the weights once using the combined
    information from all the samples.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-20](#figure15-20) shows the error from a typical training run using
    batch gradient descent.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![F15020](Images/F15020.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-20: The error for a training run using batch gradient descent'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: The broad features are reassuring. The error drops quite a bit at the beginning,
    suggesting that the network is starting on a steep section of the error surface.
    Then the curve becomes much shallower. The error surface here might be a nearly
    flat region of a shallow saddle or a region that’s nearly a plateau but has just
    a bit of slope to it, because the error does continue to drop slowly. Eventually
    the algorithm finds another steep region and follows it all the way down to 0.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Batch gradient descent looks very smooth, but to get down to near 0 error for
    this network and data requires about 20,000 epochs, which can take a long time.
    Let’s get a closer look at what happens from one epoch to the next by zooming
    in on the first 400 epochs, shown in [Figure 15-21](#figure15-21).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: It seems that batch gradient descent really is moving smoothly. That makes sense,
    because it’s using the error from all the samples on each update.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Batch gradient descent usually produces a smooth error curve, but it has some
    issues in practice. If we have more samples than can fit in our computer’s memory,
    then the costs of *paging*, or retrieving data from slower storage media, can
    become substantial enough to make training impractically slow. This can be a problem
    in some real situations when we work with enormous datasets of millions of samples.
    It can take a great deal of time to read samples from slower memory (or even a
    hard drive) over and over. There are solutions to this problem, but they can involve
    a lot of work.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![F15021](Images/F15021.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-21: A close-up of the first 400 epochs of batch gradient descent
    shown in [Figure 15-20](#figure15-20)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Closely related to this memory issue is that we must keep all the samples around
    and available so that we can run through them over and over, once per epoch. We
    sometimes say that batch gradient descent is an *offline algorithm*, meaning that
    it works strictly from information that it has stored and has access to. We can
    imagine disconnecting the computer from all networks, and it could continue to
    learn from all of our training data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相关的内存问题是，我们必须保留所有样本并使其随时可用，以便每个周期都能循环使用它们。我们有时会说批量梯度下降是一个*离线算法*，意味着它严格依赖于其存储和可访问的信息。我们可以想象将计算机与所有网络断开连接，它仍然可以从所有训练数据中继续学习。
- en: Stochastic Gradient Descent
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: Let’s go to the other extreme and update our weights after every sample. This
    is called *stochastic gradient descent*, or, more commonly, just *SGD*. Recall
    that the word *stochastic* is roughly a synonym for *random*. This word is used
    because we present the network with the training samples in a random order, so
    we can’t predict how the weights are going to change from one sample to the next.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们转向另一个极端，每个样本处理后就更新一次权重。这就是所谓的*随机梯度下降*，或更常见的简称*SGD*。回想一下，*随机*一词大致相当于*随机*，它的使用是因为我们将训练样本以随机顺序呈现给网络，因此无法预测权重如何从一个样本变到下一个样本。
- en: Since we update after every sample, our dataset of 300 samples requires us to
    update the weights 300 times over the course of each epoch. This is going to cause
    the error to jump around a lot as each sample pulls the weights one way and then
    another. Since we’re only plotting the error on an epoch-by-epoch basis, we don’t
    see this small-scale wiggling. But we still see a lot of variation epoch by epoch.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们每处理一个样本就更新一次权重，我们的数据集有300个样本，这就意味着每个周期内我们需要更新300次权重。这会导致误差剧烈波动，因为每个样本都会推动权重发生一次变化，接着又会发生反向变化。由于我们仅按每个周期绘制误差，我们看不到这种小范围的波动。但我们仍然能看到每个周期之间存在较大的变化。
- en: '[Figure 15-22](#figure15-22) shows the error of our network learning from this
    data using SGD.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15-22](#figure15-22)展示了我们使用SGD从该数据中学习的网络误差。'
- en: '![F15022](Images/F15022.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![F15022](Images/F15022.png)'
- en: 'Figure 15-22: Stochastic gradient descent, or SGD'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图15-22：随机梯度下降，或SGD
- en: The graph has the same general shape as the one for batch gradient descent in
    [Figure 15-20](#figure15-20), which makes sense since both training runs use the
    same network and data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图与[图15-20](#figure15-20)中批量梯度下降的图形大致相同，这是有道理的，因为两个训练过程使用了相同的网络和数据。
- en: The huge spike at around epoch 225 shows just how unpredictable SGD can be.
    Something in the sequencing of the samples and the way the network’s weights were
    updated caused the error to soar from nearly 0 to nearly 1\. In other words, it
    went from finding the right class for almost every sample to being dead wrong
    on almost every sample, and then back to being right again (though this recovery
    took a few epochs, as shown by the small curve to the right of the spike). If
    we were watching the errors as learning progresses, we might be inclined to stop
    the training session at the spike. If we use an automatic algorithm to watch the
    error, it may also stop it there. Yet just a few epochs after that spike, the
    system has recovered and we’re back to nearly 0\. The algorithm has definitely
    earned the word *stochastic* in its name.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在大约第225个周期时，出现了一个巨大的峰值，显示了SGD的不确定性。样本的顺序以及网络权重更新的方式导致了误差从接近0飙升至接近1。换句话说，网络从几乎正确分类每个样本，到几乎错误地分类每个样本，再到恢复正确（尽管这个恢复花了几个周期，正如峰值右侧的小曲线所示）。如果我们在学习过程中观察误差，可能会在这个峰值时倾向于停止训练。如果我们使用自动算法来观察误差，它也可能在这里停止。然而，就在这个峰值之后的几个周期内，系统已经恢复过来，我们又回到了接近0的误差。这个算法的确展现了“*随机*”这一特性。
- en: We can see from the plot that SGD got down to about 0 error in just 400 epochs.
    We cut off [Figure 15-22](#figure15-22) after that, since the curve stayed at
    0 from then on. Compare this to the roughly 20,000 epochs required by batch gradient
    descent in [Figure 15-20](#figure15-20). This increase in efficiency over batch
    gradient descent is typical (Ruder 2017).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中我们可以看到，SGD在仅仅400个周期内就将误差降到了接近0。之后我们切掉了[图15-22](#figure15-22)，因为从那时起曲线一直保持在0。与此相比，[图15-20](#figure15-20)中的批量梯度下降大约需要20,000个周期。这种相较于批量梯度下降的效率提升是典型的（Ruder
    2017）。
- en: But let’s compare apples to apples. How many times did each algorithm update
    the weights? Batch gradient descent updates the weights after each batch, so the
    20,000 epochs means it did 20,000 updates. SGD does an update after every one
    of our 300 samples. So in 400 epochs it performed 300 × 400 = 120,000 updates,
    six times more than batch gradient descent. The moral is that the amount of time
    we actually spend waiting for results isn’t completely predicted by the number
    of epochs, since the time per epoch can vary considerably.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: We call SGD an *online algorithm*, because it doesn’t require the samples to
    be stored or even to be consistent from one epoch to the next. It just handles
    each sample as it arrives and updates the network immediately.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: SGD produces noisy results, as we can see in [Figure 15-22](#figure15-22). This
    is both good and bad. The upside of this is that SGD can jump from one region
    of the error surface to another as it searches for minima. But the downside is
    that SGD can overshoot a deep minimum and spend its time searching around inside
    of some valley with a larger error. Reducing the learning rate over time definitely
    helps with the jumping problem, but the progress is still typically noisy.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Noise in the error curve can be a problem because it makes it hard for us to
    know when the system is learning and when it starts overfitting. We can look at
    a sliding window of many epochs, but we may only know that we’ve overshot the
    minimum error long after it happened.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Mini-Batch Gradient Descent
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can find a nice middle ground between the extremes of batch gradient descent,
    which updates once per epoch, and stochastic gradient descent, which updates after
    every sample. This compromise is called *mini-batch gradient descent*, or sometimes
    *mini-batch SGD*. Here, we update the weights after some fixed number of samples
    has been evaluated. This number is almost always considerably smaller than the
    batch size (the number of samples in the training set). We call this smaller number
    the *mini-batch size*, and a set of that many samples drawn from the training
    set is a *mini-batch*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: The mini-batch size is frequently a power of 2 between about 32 and 256, and
    often it is chosen to fully use the parallel capabilities of our GPU, if we have
    one. But that’s just for efficiency purposes. We can use any size of mini-batch
    that we like.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-23](#figure15-23) shows the results of using a mini-batch of 32
    samples.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: This is indeed a nice blend of the two algorithms. The curve is smooth, like
    batch gradient descent, but not perfectly so. It drops down to 0 in about 5,000
    epochs, between the 400 needed by SGD and the 20,000 of batch gradient descent.
    [Figure 15-24](#figure15-24) shows a close-up of the first 400 steps.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![F15023](Images/F15023.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-23: Mini-batch gradient descent'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![F15024](Images/F15024.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-24: A close-up of the first 400 epochs of [Figure 15-23](#figure15-23),
    showing the deep plunge at the very beginning of training'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: How many updates did mini-batch SGD perform? We have 300 samples, and we used
    a mini-batch size of 32, so there are 10 mini-batches per epoch. (Ideally, we’d
    like the mini-batches to precisely divide the size of the input, but in practice,
    we can’t control the size of our datasets. This often leaves us with a partial
    mini-batch at the end.) So 10 updates per epoch, times 5,000 epochs, gives us
    50,000 updates. This is also nicely between the 20,000 updates of batch gradient
    descent and the 120,000 updates of SGD.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Mini-batch gradient descent is less noisy than SGD, which makes it attractive
    for tracking the error. The algorithm can take advantage of huge efficiency gains
    by using the GPU for calculations, evaluating all the samples in a mini-batch
    in parallel. It’s faster than batch gradient descent, and more attractive in practice
    than SGD.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: For all these reasons, mini-batch SGD is popular in practice, with “plain” SGD
    and batch gradient descent being used relatively infrequently. In fact, most of
    the time when the term *SGD* is used in the literature, or even just *gradient
    descent*, it’s understood that the authors mean mini-batch SGD (Ruder 2017). To
    make things a little more confusing, the term *batch* is often used instead of
    *mini-batch*. Because epoch-based gradient descent is used so rarely these days,
    references to batch gradient descent and batches almost always refer to mini-batch
    gradient descent and mini-batches.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent Variations
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mini-batch gradient descent is an important algorithm, but it’s not perfect.
    Let’s review some of the challenges of mini-batch gradient descent, and a few
    ways to address them. Following convention, from here on we refer to mini-batch
    gradient descent as SGD. (The organization of this section is inspired by Ruder
    2017.)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Our first challenge is to specify what value of the learning rate *η* we want
    to use, which is notoriously hard to pick ahead of time. As we’ve seen, a value
    that’s too small can result in long learning times and getting stuck in shallow
    local minima, but a value that’s too big can cause us to overshoot deep local
    minima and then get stuck bouncing around inside a minimum when we do find it.
    If we try to avoid the problem by using a decay schedule to change *η* over time,
    we still have to pick the starting value of *η* and then the schedule’s hyperparameters
    as well.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: We also have to pick the size of the mini-batch. This is rarely an issue, because
    we usually choose whatever value produces calculations that are most closely matched
    to the structure of our GPU or other hardware.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider some improvements. Right now, we’re updating all the weights
    with a one-update-rate-fits-all approach. Instead, we can find a unique learning
    rate for each weight in the system so we’re not just moving it in the best direction,
    but we’re moving it by the best amount. We’ll see examples of this in the following
    pages.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Another improvement begins with the recognition that sometimes when the error
    surface forms a saddle, the surface can be shallow in all directions, so locally,
    it’s almost (but not quite) a plateau. This can slow our progress to an excruciating
    crawl. Research has shown that deep learning systems often have plenty of saddles
    in their error landscapes (Dauphin et al. 2014). It would be nice if there was
    a way to get unstuck in these situations, or better yet, to avoid getting stuck
    in them in the first place. The same thing goes for plateaus: we’d like to avoid
    getting stuck in the flat regions where the gradient drops to 0\. To do so, we
    want to avoid the regions where the gradient drops to 0, except of course for
    the minima we’re seeking.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some variations of gradient descent that address these issues.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Momentum
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s consider two weights at the same time. We can plot their values on an
    XY plane, and above them show the error that results from training the system
    with those values for those weights. Let’s think of our error surface as a landscape.
    Now we can picture our task of minimizing error as following a drop of water that’s
    looking for the lowest point.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-25](#figure15-25) repeats a figure from Chapter 5 that shows an
    example of this way of thinking about the training process.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![F15025](Images/F15025.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-25: A drop of water rolling down an error surface. This is a repeat
    of a figure from Chapter 5.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Instead of water, let’s think of this as a little ball rolling down the error
    surface. We know from the physical world that a real ball rolling down a hill
    in this way has some *inertia*, which describes its resistance to a change in
    its motion. If it’s rolling along in a given direction at a certain speed, it
    will continue to move that way unless something interferes with it.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: A related idea is the ball’s *momentum*, which is a bit more abstract from a
    physical point of view. Although they’re distinct ideas, sometimes deep learning
    discussions casually refer to inertia as momentum, and the algorithm we’re about
    to look at uses that language.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: This idea is what keeps the ball in [Figure 15-25](#figure15-25) moving across
    the plateau after it has come down from the peak and passed into the saddle near
    the middle of the figure. If the ball’s motion was determined strictly by the
    gradient, when it hit the plateau near the middle of the figure, it would stop
    (or if it was a near-plateau, the ball would slow to a crawl). But the ball’s
    momentum (or more properly, its inertia) keeps it rolling onward.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we’re near the left side of [Figure 15-26](#figure15-26). As we roll
    down the hill, we reach the plateau starting at around −0.5.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![F15026](Images/F15026.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-26: An error curve with a plateau between a hill and valley'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: With regular gradient descent, we stop on the plateau since the gradient is
    zero, as shown in the left of [Figure 15-27](#figure15-27). But if we include
    some momentum, as shown on the right, the ball keeps going for a while. It does
    slow down, but if we’re lucky, it continues to roll far enough to find the next
    valley.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: The technique of *momentum gradient descent* (Qian 1999) is based on this idea.
    For each step, once we calculate how much we want each weight to change, we add
    in a small amount of its change from the previousstep. If the change on a given
    step is 0, or nearly 0, but we had some larger change on the last step, we use
    some of that prior motion now, which pushes us along over the plateau.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![F15027](Images/F15027.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-27: Gradient descent on the error curve of [Figure 15-26](#figure15-26).
    Left: Gradient descent with decay. Right: Gradient descent with decay and momentum.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-28](#figure15-28) shows the idea visually.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![F15028](Images/F15028.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-28: Finding the step for gradient descent with momentum'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: We suppose that some weight had an error A. We updated that weight to value
    w2, with an error B. We now want to find the next value for the weight, w3, which
    will have error C. To find C, we find the change that we applied to point A. That
    is, we find the previous motion applied to A. This is the momentum, labeled *m*,
    shown in [Figure 15-28](#figure15-28)(a).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: We multiply the momentum, *m*, by a scaling factor usually referred to with
    the lowercase Greek letter *γ* (gamma). Sometimes this is called the *momentum
    scaling factor*, and it’s a value from 0 to 1\. Multiplying *m* by this value
    gives us a new arrow *γm* that points in the same direction as *m* but is the
    same length or shorter. We then find the scaled gradient, *ηg*, at B, as we did
    before, shown in [Figure 15-28](#figure15-28)(b). Now we have all we need. We
    add together the scaled momentum, *γm*, and the scaled gradient, *ηg*, to B, which
    we do graphically by placing the tail of *γm* at the head of *ηg*, as in [Figure
    15-28](#figure15-28)(c).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Let’s apply this rule and see how the weight and error change over time. [Figure
    15-29](#figure15-29) shows our symmetrical valley from before, and sequential
    steps of training. In this figure, we use both an exponential decay schedule and
    momentum. This is just like our sequence from [Figure 15-15](#figure15-15), but
    now the change applied to each step also includes momentum, or a scaled version
    of the change from the previous step. We can see this by looking at the two lines
    that emerge from each point (one for the gradient, the other for the momentum).
    This total then becomes the new change.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![F15029](Images/F15029.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-29: Learning with both an exponential decay schedule and momentum'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: On each step, we first find the gradient and multiply it by the current value
    of the learning rate *η*, as before. Then we find the previous change, scale it
    by *γ*, and add both of those changes to the current position of the weight. That
    combination gives us the change in this step.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-30](#figure15-30) shows a close-up of the sixth step in the grid,
    along with the error at each point along the way.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'An interesting thing happened here: when the ball reached the right side of
    the valley, it continued to roll up, even though the gradients pointed down. That’s
    just what we’d expect of a real ball. We can see it slowing down, and then eventually
    it comes back down the slope, overshooting the bottom, but by less than before,
    then slowing and coming back down again, and so on.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![F15030](Images/F15030.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-30: The final step in [Figure 15-29](#figure15-29), along with the
    error for each point'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: If we use too much momentum, our ball can fly right up the other side and out
    of the bowl altogether, but if we use too little momentum, our ball may not get
    across the plateaus it encounters along the way. [Figure 15-31](#figure15-31)
    shows our error curve from [Figure 15-26](#figure15-26). Here we used trial and
    error to find a value of *γ* to scale the momentum so that our ball gets through
    the plateau but can still settle into the minimum at the bottom of the bowl.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![F15031](Images/F15031.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-31: Using enough momentum to cross a plateau, but not so much that
    the ball is unable to settle nicely into the bottom of the minimum'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Finding the right amount of momentum to use is another task where we need to
    use our experience and intuition, along with trial and error, to help us understand
    the behavior of our specific network and the data we’re working with. We can also
    search for it using hyperparameter searching algorithms.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: To put this all together, we find the gradient, scale it by the current learning
    rate *η*, add in the previous change scaled by *γ*, and that gives us our new
    position. If we set *γ* to 0, then we add in none of the last step, and we have
    “normal” (or “vanilla”) gradient descent. If *γ* is set to 1, then we add in the
    entirety of the last change. Often we use a value of around 0.9\. In Figures 15-29
    and [Figure 15-31](#figure15-31), we set gamma to 0.7 to better illustrate the
    process.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-32](#figure15-32) shows the result of 15 steps of learning with
    both learning rate decay and momentum. The ball starts on the left, rolls down
    and then far up the right side, then it rolls down again and rolls up the left
    side, and so on, climbing a little less each time.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![F15032](Images/F15032.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-32: Learning with momentum and a decaying learning rate for 15 points'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Momentum helps us get over flat plateaus and out of shallow places in saddles.
    It has the additional benefit of helping us zip down steep slopes, so even with
    a small learning rate, we can pick up some efficiency.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-33](#figure15-33) shows the error for a training run on our dataset
    of [Figure 15-18](#figure15-18) consisting of two crescent moons.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![F15033](Images/F15033.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-33: Error curve for training with our two-crescent data using mini-batch
    gradient descent with momentum. We got to zero error in a little more than 600
    epochs.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Here we’re using mini-batch gradient descent (or SGD) with momentum. It’s noisier
    than the mini-batch curve of [Figure 15-23](#figure15-23) because the momentum
    sometimes carries us past where we want to be, causing a spike in the error. The
    error when using mini-batch SGD alone in [Figure 15-23](#figure15-23) for our
    data took about 5,000 epochs to reach about 0 error. With momentum, we get there
    in a little over 600 epochs. Not bad!
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Momentum clearly helps us learn more quickly, which is a great thing. But momentum
    brings us a new problem: choosing the momentum value *γ*. As we mentioned, we
    can pick this value using experience and intuition or use a hyperparametersearch
    for the value that gives us the best results.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Nesterov Momentum
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Momentum let us reach into the past for information to help us train. Now let’s
    reach into the future. The key idea is that instead of using only the gradient
    at the location where we currently are, we also use the gradient at the location
    where we expect that we’re *going to be*. Then we can use some of that “gradient
    from the future” to help us now.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Because we can’t really predict the future, we estimatewhere we’re going to
    be on the next step and use the gradient there. The thinking is that if the error
    surface is relatively smooth, and our estimate is pretty good, then the gradient
    we find at our estimated next position is close to the gradient where we’d actually
    end up if we just moved using standard gradient descent, with or without momentum.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Why is it useful to use the gradient from the future? Suppose we’re rolling
    down one side of a valley and approaching the bottom. On the next step, we overshoot
    the bottom and end up somewhere on the other wall. As we saw before, momentum
    carries us up that wall for a few steps, slowing as we lose momentum, until we
    turn around and come back down. But if we can predictthat we’ll be on the far
    side, we can include some of the gradient at that point in our calculations now.
    So instead of moving so far to the right and up the hill, that leftward push from
    the future causes us to move by a little less distance, so we don’t overshoot
    so far and end up closer to the bottom of the valley.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: In other words, if the next move we’re going to make is in the same direction
    as the last one, we take a larger step now. If the next move is going to move
    us backward, we take a smaller step.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Let’s break this down into steps so we don’t get mixed up between estimates
    and realities. [Figure 15-34](#figure15-34) shows the process.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: As before, we imagine that we started with our weight at position A, and after
    the most recent update, we ended up at B, as shown in [Figure 15-34](#figure15-34)(a).
    As with momentum, we find the change applied at point A to bring us to B (the
    arrow *m*) and we scale that by *γ*.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Now comes the new part, starting in [Figure 15-34](#figure15-34)(b). Rather
    than finding the gradient at B, we first add the scaled momentum to B to get the
    “predicted” error P. This is our guess for where we will end up on the error surface
    after the next step. As shown [Figure 15-34](#figure15-34)(c), we find the gradient
    *g* at point P and scale it as usual to get *ηg*. Now we find the new point C
    in [Figure 15-34](#figure15-34)(d) by adding the scaled momentum *γm* and the
    scaled gradient *ηg* to B.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![F15034](Images/F15034.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-34: Gradient descent with Nesterov momentum'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Notice we’re not using the gradient at point B at all. We just combine a scaled
    version of the momentum that got us to B and a scaled version of the gradient
    at our predicted point P.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Notice too that the point C in [Figure 15-34](#figure15-34)(d) is closer to
    the bottom of the bowl than point P, where we’d have ended up with normal momentum.
    By looking into the future and seeing that we’d be on the other side of the valley,
    we are able to use that left-pointing gradient to prevent rolling far up the far
    side.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: In honor of the researcher who developed this method, it’s called *Nesterov
    momentum*, or the *Nesterov accelerated gradient* (Nesterov 1983). It’s basically
    a souped-up version of the momentum technique we saw earlier. Though we still
    have to pick a value for *γ,* we don’t have to pick any new parameters. This is
    a nice example of an algorithm that gives us increased performance without requiring
    more work on our end.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-35](#figure15-35) shows the result of Nesterov momentum for 15 steps.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![F15035](Images/F15035.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-35: Running Nesterov momentum for 15 steps. It finds the bottom of
    the valley in about seven steps and then stays there.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-36](#figure15-36) shows the error curve for our standard test case
    using Nesterov momentum. This uses the exact same model and parameters as the
    momentum-only results in [Figure 15-33](#figure15-33), but it’s both less noisy
    and more efficient, getting down to about 0 error at roughly epoch 425, rather
    than the roughly 600 required by regular momentum alone.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![f15036](Images/f15036.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-36: Error for mini-batch SGD with Nesterov momentum. The system reaches
    zero error around epoch 600\. The graph shows 1,000 epochs.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Any time we use momentum it’s worth considering Nesterov momentum instead. It
    requires no additional parameters from us, but it usually learns more quickly
    and with less noise.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Adagrad
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve seen two types of momentum that help push us through plateaus and reduce
    overshooting. We’ve been using the same learning rate when we update all the weights
    in our network. Earlier in this chapter, we mentioned the idea of using a learning
    rate *η* that’s tailored individually for each weight.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Several related algorithms use this idea. Their names all begin with *Ada*,
    standing for “adaptive.”
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with an algorithm called *Adagrad*, which is short for *adaptive
    gradient learning* (Duchi, Hazan, and Singer 2011). As the name implies, the algorithm
    adapts (or changes) the size of the gradient for each weight. In other words,
    Adagrad gives us a way to perform learning-rate decay on a weight-by-weight basis.
    For each weight, Adagrad takes the gradient that we use in that update step, squares
    it, and adds that into a running sum for that weight. Then the gradient is divided
    by a value derived from this sum, giving us the value that’s then used for the
    update.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Because each step’s gradient is squared before it’s added in, the value that’s
    added into the sum is always positive. As a result, this running sum gets larger
    and larger over time. To keep it from growing out of control, we divide each change
    by that growing sum, so the changes to each weight get smaller and smaller over
    time.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: This sounds a lot like learning rate decay. As time goes on, the changes to
    the weights get smaller. The difference here is that the slowdown in learning
    is being computed uniquely for each weight based on its history.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Because Adagrad is effectively automatically computing a learning rate for every
    weight on the fly, the learning rate we use to kick things off isn’t as critical
    as it was for earlier algorithms. This is a huge benefit, since it frees us from
    the task of fine-tuning that error rate. We often set the learning rate *η* to
    a small value like 0.01 and let Adagrad handle things from there.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-37](#figure15-37) shows the performance of Adagrad on our test data.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: This has the same general shape as most of our other curves, but it takes a
    very long time to get to 0\. Because the sum of the gradients gets larger over
    time, eventually we’ll find that dividing each new gradient by a value related
    to that sum gives us gradients that approach 0\. The increasingly small updates
    are why the error curve for Adagrad descends so very slowly as it tries to get
    rid of that last remaining error.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: We can fix that without too much work.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![F15037](Images/F15037.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-37: The performance of Adagrad on our test setup'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Adadelta and RMSprop
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The problem with Adagrad is that the gradient we apply to each weight for its
    update step just keeps getting smaller and smaller. That’s because the running
    sum just gets larger and larger.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Instead of summing up all the squared gradients since the beginning of training,
    suppose we keep a *decaying sum* of these gradients. We can think of this as a
    running list of the most recent gradients for each weight. Each time we update
    the weights, we tack the new gradient onto the end of the list and drop the oldest
    one off the start. To find the value we use to divide the new gradient, we add
    up all the values in the list, but we first multiply them all by a number based
    on their position in the list. Recent values get multiplied by a large value,
    while the oldest ones get multiplied by a very small value. This way our running
    sum is most heavily determined by recent gradients, though it is influenced to
    a lesser degree by the older gradients (Ruder 2017).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: In this way, the running sum of the gradients (and thus the value we divided
    new gradients by) can go up and down based on the gradients we’ve applied recently.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm is called *Adadelta* (Zeiler 2012). The name comes from “adaptive,”
    like Adagrad, and the *delta* refers to the Greek letter *δ* (delta), which mathematicians
    often use to refer to change. This algorithm adaptively changes how much the weights
    are updated on each step using each one’s weighted running sum.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Since Adadelta adjusts the learning rates on the weights individually, any weight
    that’s been on a steep slope for a while will slow down so it doesn’t go flying
    off, but when that weight is on a flatter section, it’s allowed to take bigger
    steps.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Like Adagrad, we often start the learning rate at a value around 0.01, and then
    let the algorithm adjust it from then on.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-38](#figure15-38) shows the results of Adadelta on our test setup.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![F15038](Images/F15038.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-38: The results of training with Adadelta on our test data'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: This compares favorably to Adagrad’s performance in [Figure 15-37](#figure15-37).
    It’s nice and smooth and reaches 0 at around epoch 2,500, much sooner than Adagrad’s
    8,000 epochs.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Adadelta has the downside of requiring another parameter, which is unfortunately
    also called gamma (*γ*). It’s roughly related to the parameter *γ* used by the
    momentum algorithms, but they’re sufficiently different that it’s best to consider
    them distinct ideas that happen to have been given the same name. The value of
    *γ* here tells us how much we scale down the gradients in our history list over
    time. A large value of *γ* “remembers” values from farther back than smaller values
    and will let them contribute to the sum. A smaller value of *γ* just focuses on
    recent gradients. Often we set this *γ* to around 0.9.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: There’s actually another parameter in Adadelta, named with the Greek letter
    *ε* (epsilon). This is a detail that’s used to keep the calculations numerically
    stable. Most libraries will set this to a default value that’s carefully selected
    by the programmers to make things work as well as possible, so it should never
    be changed unless there’s a specific need.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: An algorithm that’s very similar to Adadelta, but that uses slightly different
    mathematics, is called *RMSprop* (Hinton, Srivastava, and Swersky 2015). The name
    comes from the fact that it uses a root-mean-squared operation, often abbreviated
    RMS, to determine the adjustment that is added (or *propagated*, hence the “prop”
    in the name) to the gradients.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: RMSprop and Adadelta were invented around the same time, and work in similar
    ways. RMSprop also uses a parameter to control how much it “remembers,” and this
    parameter, too, is named *γ*. Again, a good starting value is around 0.9.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Adam
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous algorithms share the idea of saving a list of squared gradients
    with each weight. They then create a scaling factor by adding up the values in
    this list, perhaps after scaling them. The gradient at each update step is divided
    by this total. Adagrad gives all the elements in the list equal weight when it
    builds its scaling factor, while Adadelta and RMSprop treat older elements as
    less important, and thus they contribute less to the overall total.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Squaring the gradient before putting it into the list is useful mathematically,
    but when we square a number, the result is always positive. This means that we
    lose track of whether that gradient in our list was positive or negative, which
    is useful information to have. So, to avoid losing this information, we can keep
    a second list of the gradients without squaring them. Then we can use both lists
    to derive our scaling factor.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: This is the approach of an algorithm called *adaptive moment estimation*, or
    more commonly *Adam* (Kingma and Ba 2015).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-39](#figure15-39) shows how Adam performs.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![F15039](Images/F15039.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-39: The Adam algorithm on our test set'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: The output is great. It’s only slightly noisy and hits about 0 error at around
    epoch 900, much sooner than Adagrad or Adadelta. The downside is that Adam has
    two parameters, which we must set at the start of learning. The parameters are
    named for the Greek letter *β* (beta) and are called “beta 1” and “beta 2,” written
    *β*1 and *β*2\. The authors of the paper on Adam suggest setting *β*1 to 0.9,
    and *β*2 to 0.999, and these values indeed often work well.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an Optimizer
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This has not been a complete list of all the optimizers that have been proposed
    and studied. There are many others, with more coming all the time, and each has
    its own strengths and weaknesses. Our goal was to give an overview of some of
    the most popular techniques and to understand how they achieve their speedups.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-40](#figure15-40) summarizes our two-moon results for SGD with Nesterov
    momentum and the three adaptive algorithms of Adagrad, Adadelta, and Adam.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![F15040](Images/F15040.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-40: The loss, or error, over time for four of the algorithms just
    covered. This graph shows only the first 4,000 epochs.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: In this simple test case, mini-batch SGD with Nesterov momentum is the clear
    winner, with Adam coming in a close second. In more complicated situations, the
    adaptive algorithms typically perform better.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Across a wide variety of datasets and networks, the final three adaptive algorithms
    that we discussed (Adadelta, RMSprop, and Adam) often perform very similarly (Ruder
    2017). Studies have found that Adam does a slightly better job than the others
    in some circumstances, so that’s usually a good place to start (Kingma and Ba
    2015).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Why are there so many optimizers? Wouldn’t it be wise to find the best one and
    stick with that? It turns out that not only do we not know of a “best” optimizer,
    but there can’t be a best optimizer for all situations*.* No matter what optimizer
    we put forth as the “best,” we can prove that it’s always possible to find some
    situation in which another optimizer would be better. This result is famously
    known by its colorful name, the *No Free Lunch Theorem* (Wolpert 1996; Wolpert
    and Macready 1997). This guarantees us that no optimizer will always perform better
    than any other.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Note that the No Free Lunch Theorem doesn’t say that all optimizers are equal.
    As we’ve seen in our tests in this chapter, different optimizers do perform differently.
    The theorem only tells us that no one optimizer will *always* beat the others.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Though no one optimizer is the best choice for all possible training situations,
    we can find the best optimizer for any specific combination of network and data.
    Most deep learning libraries offer routines that carry out an automated search
    that can try out multiple optimizers and run through multiple parameter choices
    for each one. Whether we choose our optimizer and its values by ourselves or as
    the result of a search, we need to keep in mind that the best choices can vary
    from one network and set of data to the next. As soon as we make a big change
    to either, we should consider checking to see if a better optimizer would give
    us more efficient training. As a practical guide, many people start out with Adam,
    using its default parameters.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: No matter what optimizer we choose, our network can suffer from overfitting.
    As we discussed in Chapter 9, overfitting is a natural result of training for
    too long. The problem is that the network learns the training data so well that
    it becomes tuned to just that data and performs poorly on new data once it’s released.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Techniques that delay the onset of overfitting are called *regularization* methods.
    They allow us to train for more epochs before overfitting has too great an impact,
    which means our networks have more training time in which to improve their performance.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A popular regularization method is called *dropout*. It is usually applied in
    a deep network in the form of a *dropout layer* (Srivastava et al. 2014). The
    dropout layer is called an *accessory layer* or a *supplemental layer*, because
    it doesn’t do any computation of its own. We call it a layer, and draw it as one,
    because it’s convenient conceptually, and lets us include dropout in drawings
    of networks. But we don’t consider it a real layer (hidden or otherwise), and
    we don’t count it when we describe how many layers make up a particular network.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Dropout is a placeholder that tells the network to run an algorithm on the previous
    layer. It’s also only active during training. When the network is deployed, dropout
    layers are disabled or removed.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: The job of the dropout layer is to temporarily disconnect some of the neurons
    on the previous layer. We give it a parameter that describes the percentageof
    neurons that should be affected, and at the start of each batch, it randomly chooses
    that percentage of neurons on the preceding layer and temporarily disconnects
    their inputs and outputs from the network. Since they’re disconnected, these neurons
    don’t participate in any forward calculations, they’re not included in backprop,
    and the weights coming into them are not updated by the optimizer. When the batch
    is done and the rest of the weights have been updated, the chosen neurons and
    all of their connections are restored.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: At the start of the next batch, the layer again chooses a new random set of
    neurons and temporarily removes those, repeating the process for each epoch. [Figure
    15-41](#figure15-41) shows the idea graphically.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![F15041](Images/F15041.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-41: Dropout. (a) 50 percent of the four neurons in the middle layer
    (in gray) are chosen to be disconnected before the batch is evaluated. (b) Our
    schematic for a single dropout layer is a diagonal slash. To the right, we indicate
    the proportion of neurons that are selected for disconnection. Since dropout applies
    to its preceding layer, in this example, we apply it to the middle of the three
    fully connected layers.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Dropout delays overfitting by preventing any neurons from overspecializing and
    dominating. Suppose that one neuron in a photo classification system gets highly
    specialized to detect the eyes of cats. That’s useful for recognizing picture
    of cats’ faces, but useless for all the other photographs the system might be
    asked to classify. If all the neurons in a network also specialize at finding
    just one or two features in the training data, then they can perform beautifully
    on that data because they spot the idiosyncratic details that they’re trained
    to locate. But the system as a whole will then perform badly when presented with
    new data that’s missing the precise cues those neurons became specialized for.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Dropout helps us avoid this kind of specialization. When a neuron is disconnected,
    the remaining neurons must adjust to pick up the slack. Thus, the specialized
    neuron is freed up to perform a more generally useful task, and we’ve delayed
    the onset of overfitting. Dropout helps us put off overfitting by spreading around
    the learning among all the neurons.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Batchnorm
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another regularization technique is called *batch normalization*, often referred
    to simply as *batchnorm* (Ioffe and Szegedy 2015). Like dropout, batchnorm can
    be implemented as a layer without neurons. Unlike dropout, batchnorm actually
    does perform some computation, though there are no parameters for us to specify.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Batchnorm modifies the values that come out of a layer. This might seem strange,
    since the whole purpose of training is to get our neurons to produce output values
    that lead to good results. Why would we want to modify those outputs?
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Recall that many of our activation functions, such as leaky ReLU and tanh, have
    their greatest effect near 0\. To get the most benefit from those functions, we
    need the numbers flowing into them to be in a small range centered around 0\.
    That’s what batchnorm does by scaling and shifting all the outputs of a layer
    together. Because batchnorm moves the neuron outputs into a small range near 0,
    we’re less prone to seeing any neuron learning one specific detail and producing
    a huge output that swamps all the other neurons, and thus we are able to delay
    the onset of overfitting. Batchnorm scales and shifts all the values coming out
    of the previous layer over the course of an entire mini-batch in just this way.
    It learns the parameters for this scaling and shifting along with the weights
    in the network so they take on the most useful values.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: We apply batchnorm before the activation function so that the modified values
    will fall in the region of the activation function where they are affected the
    most. In practice, this means we place no activation function on the neurons going
    into batchnorm (or if we must specify a function, it’s the linear activation function,
    which has no effect). Those values go into batchnorm, and then they’re fed into
    the activation function we want to apply.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: The process is illustrated in [Figure 15-42](#figure15-42). Our icon for a regularization
    step like batchnorm is a black disc inside a circle, suggesting that the values
    in the circle are transformed into a smaller region. In later chapters, we’ll
    see other, similar regularization steps for which we’ll use the same icon. The
    text (or a nearby label) identifies which variety of regularization is applied.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![F15042](Images/F15042.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-42: Applying a batchnorm layer. Top: A neuron followed by a leaky
    ReLU activation function. Bottom: The same neuron with batchnorm. The activation
    function is replaced with the linear function, followed by batchnorm (represented
    by a circle with a black disc inside) and then the leaky ReLU.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Like dropout, batchnorm defers the onset of overfitting, allowing us to train
    longer.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Optimization is the process of adjusting the weights so that our network learns.
    The core idea begins with the gradient for every weight. We follow that gradient
    to direct us to a lower point on the error surface, hence the name gradient descent.
    The most important value in this process is the learning rate. A common technique
    is to reduce the learning rate over time, according to a decay schedule.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: We covered several efficient optimization techniques. We can adjust the weights
    after every epoch (batch gradient descent), after every sample (stochastic gradient
    descent, or SGD), or after mini-batches of samples (mini-batch gradient descent
    or mini-batch SGD). Mini-batch gradient descent is by far the most common technique,
    and the convention in the field is to refer to it simply as SGD. We can improve
    the efficiency of every type of gradient descent by using momentum. We can also
    improve learning by computing a custom, adaptive learning rate for every weight
    over time with an algorithm such as Adam. Lastly, to prevent overfitting, we can
    use a regularization technique such as dropout or batchnorm.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Deep networks that are made up of fully connected layers can do some amazing
    things. But if we create our layers by structuring the neurons in different ways
    and add a little bit of supporting computation, their power increases significantly.
    In the next few chapters, we’ll look at these new layers and how they can be used
    to classify, predict, and even generate images, sounds, and more.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
