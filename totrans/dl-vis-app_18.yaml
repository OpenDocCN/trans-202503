- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Optimizers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: Training neural networks is frequently a time-consuming process. Anything that
    speeds it up is a welcome addition to our toolkit. This chapter is about a family
    of tools that are designed to speed up learning by improving the efficiency of
    gradient descent. The goals are to make gradient descent run faster and avoid
    some of the problems that can cause it to get stuck. These tools also automate
    some of the work of finding the best learning rate, including algorithms that
    can adjust that rate automatically over time. Collectively, these algorithms are
    called *optimizers*. Each optimizer has its strengths and weaknesses, so it’s
    worth becoming familiar with them so we can make good choices when training a
    neural network.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络通常是一个耗时的过程。任何能够加速这一过程的方法，都是我们工具箱中的一个受欢迎的补充。本章介绍了一类旨在通过提高梯度下降效率来加速学习的工具。其目标是让梯度下降运行得更快，并避免一些可能导致其陷入困境的问题。这些工具还自动化了寻找最佳学习率的部分工作，包括能够随着时间推移自动调整学习率的算法。这些算法统称为*优化器*。每个优化器都有其优缺点，因此熟悉它们是值得的，这样我们在训练神经网络时才能做出明智的选择。
- en: Let’s begin by drawing some pictures that enable us to visualize error and how
    it changes as we learn. These pictures will help us build some intuition for the
    algorithms yet to come.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先画一些图，帮助我们可视化误差以及它在学习过程中如何变化。这些图将帮助我们为接下来要介绍的算法建立一些直觉。
- en: Error as a 2D Curve
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 误差作为二维曲线
- en: It’s often helpful to think of the errors in our systems in terms of geometrical
    ideas. We frequently plot error as a 2D curve.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 将系统中的误差从几何学的角度进行思考，通常会非常有帮助。我们经常将误差绘制为二维曲线。
- en: To get familiar with this 2D error, let’s consider the task of splitting two
    classes of samples represented as dots arranged on a line. Dots at negative values
    are in one class, and dots at zero and above are in the other, as shown in [Figure
    15-1](#figure15-1).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了熟悉这种二维误差，我们考虑将表示为线上的点的两个类别样本进行划分的任务。负值处的点属于一个类别，零及以上的点属于另一个类别，如[图 15-1](#figure15-1)所示。
- en: '![F15001](Images/F15001.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![F15001](Images/F15001.png)'
- en: 'Figure 15-1: Two classes of dots on a line. Dots to the left of 0 are in class
    0, shown in blue, and the others are in class 1, shown in beige.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-1：线上的两个类别的点。位于 0 左侧的点属于类别 0，显示为蓝色，其他点属于类别 1，显示为米色。
- en: Let’s build a classifier for these samples. In this example, the boundary consists
    of just a single number. All samples to the left of that number are assigned to
    class 0, and all those to the right are assigned to class 1\. If we imagine moving
    this dividing point along the line, we can count up the number of samples that
    are misclassified and call that our error. We can summarize the results as a plot,
    where the X axis shows us each potential splitting point, and the error associated
    with that point is plotted as a dot above it. [Figure 15-2](#figure15-2) shows
    the result.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为这些样本构建一个分类器。在这个例子中，边界只由一个数字构成。所有位于该数字左侧的样本都被分配到类别 0，所有位于右侧的样本都被分配到类别 1。如果我们想象将这个划分点沿着线移动，我们可以统计被误分类的样本数量，并将其作为我们的误差。我们可以将结果总结为一个图表，其中
    X 轴展示了每个潜在的分割点，而与该点相关的误差则作为一个点绘制在其上。[图 15-2](#figure15-2)展示了这一结果。
- en: '![F15002](Images/F15002.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![F15002](Images/F15002.png)'
- en: 'Figure 15-2: Plotting the error function for a simple classifier'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-2：绘制简单分类器的误差函数
- en: We can smooth out the error curve of [Figure 15-2](#figure15-2) as shown in
    [Figure 15-3](#figure15-3).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将[图 15-2](#figure15-2)中的误差曲线平滑化，如[图 15-3](#figure15-3)所示。
- en: '![F15003](Images/F15003.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![F15003](Images/F15003.png)'
- en: 'Figure 15-3: A smoothed version of [Figure 15-2](#figure15-2)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-3：平滑后的[图 15-2](#figure15-2)版本
- en: For this particular set of random data, we see that the error is 0 when we’re
    at 0, or just a little to the left of it. This tells us that regardless of where
    we start, we want to end up with our divider just to the left of 0.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这组特定的随机数据，我们看到当我们处于 0 或者稍微偏左时，误差为 0。这告诉我们，无论从哪里开始，我们都希望最后将分隔线放在 0 的左侧。
- en: Our goal is to find a way to locate the smallest value of any error curve. When
    we can do that, we can apply the technique to all the weights of a neural network
    and thus reduce the whole network’s error.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到一种方法，定位任何误差曲线的最小值。当我们能够做到这一点时，我们就可以将这一技术应用于神经网络的所有权重，从而减少整个网络的误差。
- en: Adjusting the Learning Rate
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整学习率
- en: When we teach a system using gradient descent, the critical parameter is the
    learning rate, usually written with the lowercase Greek letter *η* (eta). This
    is often a value in the range 0.01 to 0.0001\. Larger values lead to faster learning,
    but they can lead us to miss valleys by jumping right over them. Smaller values
    of *η* (nearing 0, but always positive) lead to slower learning and can find narrow
    valleys, but they can also get stuck in gentle valleys even when there are much
    deeper ones nearby. [Figure 15-4](#figure15-4) recaps these phenomena graphically.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用梯度下降法来训练系统时，关键参数是学习率，通常用希腊字母 *η*（eta）表示。它通常的取值范围是0.01到0.0001。较大的值会导致更快的学习，但也可能会直接跳过深谷而错过它们。较小的
    *η* 值（接近0，但始终为正数）会导致学习较慢，并能够找到较窄的谷底，但也可能会停滞在较浅的谷底，即使附近存在更深的谷底。[图 15-4](#figure15-4)通过图形方式回顾了这些现象。
- en: An important idea shared by many optimizers is that we can improve learning
    by changing the learning rate as we go. The general thinking is analogous to hunting
    for buried metal on a beach using a metal detector. We start by taking big steps
    as we walk across the beach, but when the detector goes off, we take smaller and
    smaller steps to pinpoint the metal object’s location. In the same way, we usually
    take big steps along the error curve early in the learning process while we’re
    hunting for a valley. As time goes on, we hope that we found that valley, and
    we can now take smaller and smaller steps as we approach its lowest point.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 许多优化器共享一个重要的理念，即通过在学习过程中逐渐调整学习率来提高学习效果。这个思路类似于使用金属探测器在海滩上寻找埋藏的金属物品。我们开始时大步走过海滩，但当探测器响起时，我们会逐步缩小步伐，以精确定位金属物品的位置。以此类推，在学习过程的早期，我们通常会在误差曲线中大步前进，寻找谷底。随着时间的推移，我们希望找到了这个谷底，此时我们可以逐渐减小步伐，朝着最低点前进。
- en: '![F15004](Images/F15004.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![F15004](Images/F15004.png)'
- en: 'Figure 15-4: The influence of the learning rate, *η*. (a) When *η* is too large,
    we can jump right over a deep valley and miss it. (b) When *η* is too small, we
    can slowly descend into a local minimum, and miss the deeper valley.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-4：学习率 *η* 的影响。 (a) 当 *η* 太大时，我们可能会跳过一个深谷而错过它。 (b) 当 *η* 太小时，我们可能会慢慢下降到局部最小值，从而错过更深的谷底。
- en: We can illustrate our optimizers with a simple error curve containing a single
    isolated valley with the shape of a negative Gaussian, shown in [Figure 15-5](#figure15-5).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用一个简单的误差曲线来说明我们的优化器，该曲线包含一个孤立的谷底，其形状为负高斯分布，如[图 15-5](#figure15-5)所示。
- en: '![F15005](Images/F15005.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![F15005](Images/F15005.png)'
- en: 'Figure 15-5: Our error curve for looking at optimizers'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-5：我们查看优化器时的误差曲线
- en: Some gradients for this error curve are shown in [Figure 15-6](#figure15-6)
    (we’re actually showing the negative gradients).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该误差曲线的一些梯度在[图 15-6](#figure15-6)中展示（实际上我们展示的是负梯度）。
- en: '![F15006](Images/F15006.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![F15006](Images/F15006.png)'
- en: 'Figure 15-6: Our error curve and its negative gradients (scaled down by a factor
    of 0.25) at some locations'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-6：我们的误差曲线及其在某些位置的负梯度（缩小了0.25倍）
- en: The gradients in [Figure 15-6](#figure15-6) have been scaled down to 25 percent
    of their actual length for clarity. We can see that for this curve, the gradient
    is negative for input values that are less than 0 and positive for input values
    that are greater than 0\. When the input is 0, we’re at the very bottom of the
    bowl, so the gradient there is 0, drawn as just a single dot.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-6](#figure15-6)中的梯度已被缩小至实际长度的25%，以便清晰显示。我们可以看到，对于这条曲线，输入值小于0时梯度为负，输入值大于0时梯度为正。当输入为0时，我们处在碗底，所以此时梯度为0，表示为一个点。'
- en: Constant-Sized Updates
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常量大小的更新
- en: Let’s start our investigation of the effect of the learning rate by seeing what
    happens when we use a constant learning rate. In other words, we always scale
    the gradient by a value of *η* that stays fixed, or constant, during the whole
    training process.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过观察使用常量学习率时会发生什么，来开始研究学习率的影响。换句话说，我们始终使用一个固定的 *η* 值来调整梯度，该值在整个训练过程中保持不变。
- en: '[Figure 15-7](#figure15-7) shows the basic steps of updating with a fixed *η*.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-7](#figure15-7)展示了使用固定 *η* 进行更新的基本步骤。'
- en: '![F15007](Images/F15007.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![F15007](Images/F15007.png)'
- en: 'Figure 15-7: Finding the step for basic gradient descent'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-7：寻找基本梯度下降的步长
- en: Suppose we’re looking at a particular weight in a neural network. Let’s pretend
    that the weight begins with a value of w1, and we updated it once, so it now has
    the value w2, shown in [Figure 15-7](#figure15-7)(a). Its corresponding error
    is the point on the error curve directly above it, marked B. We want to update
    the weight again to a new and better value that we’ll call w3.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在查看神经网络中的一个特定权重。假设该权重初始值为w1，我们更新了一次它，现在它的值变为w2，如[图15-7](#figure15-7)(a)所示。其对应的误差是误差曲线上直接位于其上方的点，标记为B。我们希望再次更新该权重，得到一个新的、更好的值，称为w3。
- en: To update the weight, we find its gradient on the error surface at the point
    B, shown as the arrow labeled *g*. We scale the gradient by the learning rate
    *η* to get a new arrow labeled *ηg*. Because *η* is between 0 and 1, *ηg* is a
    new arrow that points in the same direction as *g* but is either the same size
    as *g* or smaller.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新权重，我们找到误差面上B点的梯度，表示为箭头*g*。我们通过学习率*η*来缩放该梯度，得到一个新箭头，标记为*ηg*。因为*η*介于0和1之间，*ηg*是一个与*g*方向相同但大小要么相同要么更小的新箭头。
- en: In [Figure 15-7](#figure15-7), the arrow we show for the gradient *g* is actually
    the *opposite*, or negative, of the gradient. The positive and negative gradients
    point in opposite directions along the same line, so people tend to refer to simply
    *the gradient* when the choice of positive or negative can be understood from
    context. We’ll follow that convention in this chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图15-7](#figure15-7)中，我们显示的梯度箭头*g*实际上是梯度的*相反*方向，或负梯度。正梯度和负梯度沿同一条线指向相反的方向，因此当正负梯度的选择能够从上下文中理解时，人们通常直接称之为*梯度*。我们将在本章中遵循这一约定。
- en: To find w3, the new value of the weight, we add the scaled gradient to w2\.
    In pictures, this means we place the tail of the arrow *ηg* at B, as in [Figure
    15-7](#figure15-7)(b). The horizontal position of the tip of that arrow is the
    new value of the weight, w3, and its value, directly above it on the error surface,
    is marked C. In this case, we stepped a bit too far and increased our error by
    a little.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到w3，即权重的新值，我们将缩放后的梯度加到w2上。用图示表示，这意味着我们将箭头*ηg*的尾部放置在B点，如[图15-7](#figure15-7)(b)所示。该箭头尖端的水平位置就是权重的新值w3，而它的值，直接位于其上方的误差面上，标记为C。在这种情况下，我们走得有点太远，导致误差略微增加。
- en: Let’s look at this technique in practice using an error curve with a single
    valley. [Figure 15-8](#figure15-8) shows a starting point in the upper left. The
    gradient here is small, so we move to the right a small amount. The error at that
    new point is a little less than the error we started with.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个具有单一谷底的误差曲线来看这个技巧的实际应用。[图15-8](#figure15-8)展示了左上角的起始点。这里的梯度很小，所以我们向右移动了一小步。此时新点的误差比我们开始时的误差稍微小一些。
- en: For these figures, we’ve chosen *η* = 1/8, or 0.125\. This is an unusually large
    value of *η* for constant-sized gradient descent, where we often use a value of
    1/100 or less. We chose this large value because it makes for clearer pictures.
    Smaller values work in similar ways, just more slowly. We aren’t showing values
    on the axes for these graphs to avoid visual clutter, since we’re more interested
    in the nature of what happens rather than the numbers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些图示，我们选择了*η* = 1/8，即0.125。这是一个非常大的*η*值，通常在恒定步长的梯度下降法中，我们常使用1/100或更小的值。我们选择这个较大的值是因为它能使图示更清晰。较小的值也能以类似的方式工作，只是速度更慢。我们没有在这些图表的坐标轴上显示数值，以避免视觉上的混乱，因为我们更关注发生的现象，而不是具体的数字。
- en: '![F15008](Images/F15008.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![F15008](Images/F15008.png)'
- en: 'Figure 15-8: Learning with a constant learning rate'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图15-8：使用恒定学习率的学习过程
- en: Rather than move from our first point by the entire gradient, we’re moving only
    1/8 of its length. This move takes us to a steeper part of the curve, where the
    gradient is larger, so the next update moves a little farther. Each step of learning
    is shown with a new color, which we use to draw the gradient from the previous
    location and then the new point.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不是通过整个梯度移动到第一个点，而是只移动了梯度长度的1/8。这一移动使我们到达了曲线的一个更陡峭部分，那里梯度较大，因此下一个更新会移动得更远。每一步学习都会用一种新的颜色显示，我们用这种颜色绘制上一个位置的梯度，然后是新点。
- en: We show a close-up of six steps in [Figure 15-9](#figure15-9), starting after
    the first step in [Figure 15-8](#figure15-8). We also show the error for each
    point.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[图15-9](#figure15-9)中展示了六步的特写，从[图15-8](#figure15-8)中的第一步开始。我们还显示了每个点的误差。
- en: '![F15009](Images/F15009.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![F15009](Images/F15009.png)'
- en: 'Figure 15-9: Left: A close-up of the final image in [Figure 15-8](#figure15-8).
    Right: The error associated with each of these six points.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-9：左：接近[图 15-8](#figure15-8)中的最终图像。右：与这六个点相关的误差。
- en: Will this process ever reach the bottom of the bowl and get down to 0 error?
    [Figure 15-10](#figure15-10) shows the first 15 steps in this process.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会最终到达碗底并将误差降至0吗？[图 15-10](#figure15-10)展示了该过程的前15步。
- en: '![F15010](Images/F15010.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![F15010](Images/F15010.png)'
- en: 'Figure 15-10: Left: The first 15 steps of learning with a constant learning
    rate. Right: The errors of these 15 points.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-10：左：使用恒定学习率的前15步。右：这些15个点的误差。
- en: We get near the bottom and then head up the hill on the right side. But that’s
    okay because the gradient here points down and to the left, so we head back down
    the valley until we overshoot the bottom again, and end up somewhere on the left
    side, then we turn around and overshoot again and end up on the right side, and
    so on, back and forth. We’re *bouncing around* the bottom of the bowl.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接近底部，然后沿右侧的山坡向上走。但这没关系，因为这里的梯度指向下方和左侧，所以我们又往下走进谷底，直到再次超过底部，最终出现在左侧，然后我们掉头再度超过底部，最终出现在右侧，如此反复。我们正在*在碗底反弹*。
- en: It doesn’t look like we’ll ever get to 0\. The problem is particularly bad in
    this symmetrical valley, as the error jumps back and forth between the left and
    right sides of the minimum. But this type of behavior happens a lot when we use
    a constant learning rate. The bouncing around is happening because when we’re
    near the bottom of a valley, we want to take small steps, but because our learning
    rate is a constant, we’re taking steps that are too big.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们永远也无法到达0。这个问题在对称的谷底特别严重，因为误差在最小值的左右两侧反复跳动。但是，当我们使用恒定学习率时，这种行为很常见。反弹现象的发生是因为我们靠近谷底时希望采取小步伐，但由于学习率是恒定的，我们采取的步伐过大。
- en: We might wonder if the bouncing problem of [Figure 15-10](#figure15-10) was
    caused by too large a learning rate. [Figure 15-11](#figure15-11) shows how things
    go for the first 15 steps of some smaller values of *η.*
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会想，[图 15-10](#figure15-10)中的反弹问题是否是由学习率过大引起的。[图 15-11](#figure15-11)显示了对于某些较小的*η*值，前15步的情况。
- en: '![F15011](Images/F15011.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![F15011](Images/F15011.png)'
- en: 'Figure 15-11: Taking 15 steps with our small learning rates. Top row: Learning
    rates of 0.025 (left column), 0.05 (middle column), and 0.1 (right column). Bottom
    row: Errors for the points in the top row.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-11：用小学习率进行15步。上排：学习率分别为0.025（左列）、0.05（中列）和0.1（右列）。下排：上排对应点的误差。
- en: As we can see from [Figure 15-11](#figure15-11), taking smaller steps doesn’t
    solve the bouncing problem, though the bounces are smaller. On the other hand,
    increasing the learning rate makes the bouncing problem worse, as shown in [Figure
    15-12](#figure15-12).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图 15-11](#figure15-11)中我们可以看到，采取较小的步伐并不能解决反弹问题，尽管反弹幅度变小了。另一方面，增加学习率会使反弹问题更加严重，正如[图
    15-12](#figure15-12)所示。
- en: '![f15012](Images/f15012.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![f15012](Images/f15012.png)'
- en: 'Figure 15-12: Top row: Learning rates of 0.5 (left column), 0.75 (middle column),
    and 1.0 (right column). Bottom row: Errors for the points in the top row.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-12：上排：学习率分别为0.5（左列）、0.75（中列）和1.0（右列）。下排：上排对应点的误差。
- en: Larger learning rates can also cause us to jump out of a nice valley with a
    low minimum. In [Figure 15-13](#figure15-13), starting at the green dot, we jump
    right over the rest of the valley we’re in (and would like to stay in) and into
    a new valley with a much larger minimum.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的学习率也可能导致我们跳出一个具有较低最小值的好谷底。在[图 15-13](#figure15-13)中，从绿色点出发，我们直接跳过了当前所在的谷底（并希望停留其中），进入了一个具有较大最小值的新谷底。
- en: '![F15013](Images/F15013.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![F15013](Images/F15013.png)'
- en: 'Figure 15-13: A large step overshoots the valley and ends up in a different
    valley with a higher minimum.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-13：大步伐超过谷底，最终进入一个具有更高最小值的不同谷底。
- en: Sometimes a big jump like this can help us move from a shallow valley to a deeper
    one, but for such a large learning rate, we’ll probably jump around valleys a
    lot, never finding a minimum. It seems like a challenge to find just one learning
    rate that moves at a reasonable speed but won’t overshoot valleys or get trapped
    bouncing around in the bottom. A nice alternative is to change the learning rate
    as we go.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有时像这样的较大跳跃可以帮助我们从一个较浅的山谷移动到更深的山谷，但对于如此大的学习率，我们可能会在山谷之间反复跳动，永远找不到最小值。找到一个既能以合理速度移动，又不会过度跳跃或困在底部反弹的学习率似乎是一个挑战。一个不错的替代方法是随着训练的进行逐步调整学习率。
- en: Changing the Learning Rate over Time
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随着时间变化调整学习率
- en: We can use a large value of *η* near the start of our learning so we don’t crawl
    along, and a small value near the end so we don’t end up bouncing around the bottom
    of a bowl.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在学习初期使用较大的 *η*，以避免进展缓慢，而在学习后期使用较小的 *η*，以避免在碗底部反复跳动。
- en: An easy way to start big and gradually get smaller is to multiply the learning
    rate by some number that’s almost 1 after every update step. Let’s use 0.99 as
    a multiplier and suppose that the starting learning rate is 0.1\. Then after the
    first step, it will be 0.1 × 0.99 = 0.099\. On the next step, it would be 0.099
    × 0.99 = 0.09801\. [Figure 15-14](#figure15-14) shows what happens to *η* when
    we do this for many steps using a few different values for the multiplier.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的开始时较大并逐渐变小的方法是每次更新后将学习率乘以一个接近 1 的数值。我们可以使用 0.99 作为乘数，假设初始学习率为 0.1。然后在第一步之后，它将变为
    0.1 × 0.99 = 0.099。在下一步，它将变为 0.099 × 0.99 = 0.09801。[图 15-14](#figure15-14)展示了当我们在多个步骤中使用不同的乘数时，*η*
    会发生什么变化。
- en: The easiest way to write the equation of these curves involves using exponents,
    so this kind of curve is called an *exponential decay* curve. The value by which
    we multiply *η* on every step is called the *decay parameter*. This is usually
    a number very close to 1\.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 编写这些曲线方程最简单的方法是使用指数，因此这种曲线被称为 *指数衰减* 曲线。我们在每一步乘以的值称为 *衰减参数*。这个值通常是一个接近 1 的数值。
- en: '![F15014](Images/F15014.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![F15014](Images/F15014.png)'
- en: 'Figure 15-14: Starting with a learning rate of *η* = 1, the various curves
    show how the learning rate drops after multiplying it by a given value after each
    update.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-14：从学习率 *η* = 1 开始，各种曲线显示了在每次更新后，学习率如何在乘以给定的值后下降。
- en: Let’s apply this gradual reduction of the learning rate to gradient descent
    on our error curve. Once again, we start with a learning rate of 1/8\. To make
    the effect of the decay parameter easily visible, let’s set it to the unusually
    low value of 0.8\. This means each step will only be 80 percent as long as the
    step before it. [Figure 15-15](#figure15-15) shows the result for the first 15
    steps.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这种学习率逐渐减少的方法应用于梯度下降中的误差曲线。我们再次从学习率 1/8 开始。为了使衰减参数的效果更加明显，我们将它设置为不寻常的低值 0.8。这意味着每一步将只有前一步的
    80%。[图 15-15](#figure15-15)展示了前 15 步的结果。
- en: '![F15015](Images/F15015.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![F15015](Images/F15015.png)'
- en: 'Figure 15-15: The first 15 steps using a shrinking learning rate'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-15：使用递减学习率的前 15 步
- en: Let’s compare this with our “bouncing” result from using a constant step size.
    [Figure 15-16](#figure15-16) shows the results for the constant and shrinking
    step sizes together for 15 steps.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个与使用常数步长时的“反弹”结果进行比较。[图 15-16](#figure15-16)展示了常数步长和递减步长的结果对比，进行 15 步更新。
- en: '![F15016](Images/F15016.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![F15016](Images/F15016.png)'
- en: 'Figure 15-16: On the left is the constant step size from [Figure 15-10](#figure15-10),
    and on the right is the decaying step size from [Figure 15-15](#figure15-15).
    Notice how the shrinking learning rate helps us efficiently settle into the minimum
    of the valley.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-16：左侧是[图 15-10](#figure15-10)中的常数步长，右侧是[图 15-15](#figure15-15)中的递减步长。注意到递减学习率帮助我们有效地在山谷的最小值处稳定下来。
- en: The shrinking step size does a beautiful job of landing us in the bottom of
    the bowl and keeping us there.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 递减步长成功地帮助我们稳定地达到碗底并保持在那里。
- en: Decay Schedules
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 衰减计划
- en: The decay technique is attractive, but it comes with some new challenges. First,
    we have to choose a value for the decay parameter. Second, we might not want to
    apply the decay after every update. To address these issues, we can try some other
    strategies for reducing the learning rate.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 衰减技术很有吸引力，但它带来了一些新挑战。首先，我们需要选择一个衰减参数的值。其次，我们可能不想在每次更新后都应用衰减。为了解决这些问题，我们可以尝试其他一些减少学习率的策略。
- en: Any given approach to changing the learning rate over time is called a *decay
    schedule* (Bengio 2012).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 改变学习率随时间的任何方法称为*衰减计划*（Bengio 2012）。
- en: Decay schedules are usually expressed in epochs, rather than samples. We train
    on all the samples in our training set, and only then consider changing the learning
    rate before we train on all the samples again.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 衰减计划通常以时期来表达，而不是样本。我们在训练集中的所有样本上进行训练，然后考虑在再次训练所有样本之前改变学习率。
- en: The simplest decay schedule is to always apply decay to the learning rate after
    every epoch, as we just saw. [Figure 15-17](#figure15-17)(a) shows this schedule.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的衰减计划是在每个时期后始终应用学习率衰减，正如我们刚刚看到的那样。 [图15-17](#figure15-17)(a) 显示了这个计划。
- en: Another common scheduling method is to put off any decay at all for a while
    so our weights have a chance to get away from their starting random values and
    into something that might be close to finding a minimum. Then we apply whatever
    schedule we’ve picked. [Figure 15-17](#figure15-17)(b) shows this *delayed exponential
    decay* approach, putting off the exponential decay schedule of [Figure 15-17](#figure15-17)(a)
    for a few epochs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的调度方法是暂时不施加任何衰减，这样我们的权重有机会摆脱它们的起始随机值，并进入可能接近找到最小值的某种状态。然后我们应用我们选择的任何计划。
    [图15-17](#figure15-17)(b) 显示了这种*延迟指数衰减*方法，将[图15-17](#figure15-17)(a)的指数衰减调度推迟了几个时期。
- en: Another option is to apply the decay only every once in a while. The *interval
    decay* approach shown in [Figure 15-17](#figure15-17)(c), also called *fixed-step
    decay*, reduces the learning rate after every fixed number of epochs, say every
    4th or 10th. This way we don’t risk getting too small too fast.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是仅偶尔应用衰减。 [图15-17](#figure15-17)(c) 中显示的*间隔衰减*方法，也称为*固定步长衰减*，在每隔固定数量的时期后减少学习率，例如每4个或10个时期。
    这样我们就不会过快地变得太小。
- en: '![F15017](Images/F15017.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![F15017](Images/F15017.png)'
- en: 'Figure 15-17: Decay schedules for reducing the size of the learning rate over
    time. (a) Exponential decay, where the learning rate is reduced after every epoch.
    (b) Delayed exponential decay. (c) Interval decay, where the learning rate is
    reduced after every fixed number of epochs (here, 4). (d) Error-based decay, where
    the learning rate is reduced when the error stops dropping.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图15-17：随时间减小学习率的衰减计划。（a）指数衰减，在每个时期后减少学习率。（b）延迟指数衰减。（c）间隔衰减，在每个固定数量的时期后减少学习率（这里是4个）。
    （d）基于错误的衰减，在误差停止下降时减少学习率。
- en: Yet another option is to monitor the error of our network. As long as the error
    is going down, we stick with whatever learning rate we have now. When the network
    stops learning, we apply the decay so it can take smaller steps and hopefully
    work its way into a deeper part of the error landscape. This *error-based decay*
    is shown in [Figure 15-17](#figure15-17)(d).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是监视网络的误差。只要误差在下降，我们就坚持使用现有的学习率。当网络停止学习时，我们应用衰减，以便它可以采取较小的步骤，希望能够进入错误景观的更深处。这种*基于误差的衰减*显示在[图15-17](#figure15-17)(d)中。
- en: We can easily cook up a lot of alternatives, such as applying decay only when
    the error decreases by a certain amount or certain percentage, or perhaps updating
    the learning rate by just subtracting a small value from it rather than multiplying
    it by a number close to 1 (as long as we stop at some positive value—if the learning
    rate went to 0, the system would stop learning, and if the learning rate went
    negative, the system would increase the error, rather than decrease it).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以很容易地设计出许多替代方案，例如仅在误差减少一定量或百分比时应用衰减，或者仅通过减去一个小值而不是乘以接近1的数来更新学习率（只要我们在某个正值停止—如果学习率降至0，系统将停止学习，如果学习率变为负数，系统将增加错误，而不是减少错误）。
- en: We can even increase the learning rate over time if we want. The *bold driver*
    method looks at how the total loss is changing after each epoch (Orr 1999a; Orr
    1999b). If the error is going down, then we *increase* the learning rate a little,
    say 1 percent to 5 percent. The thinking is that if things are going well, and
    the error is dropping, we can take big steps. But if the error has gone up by
    more than just a little, then we slash the learning rate, cutting it by half.
    This way we can stop any increases immediately, before they can carry us too far
    away from the decreasing error we were previously enjoying.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意，甚至可以随着时间的推移增加学习率。*强力驱动*方法会查看每个训练周期后总损失的变化（Orr 1999a; Orr 1999b）。如果误差在下降，我们就*增加*学习率一点，比如从1%增加到5%。其思路是，如果一切进展顺利，且误差在下降，我们就可以采取大步伐。但如果误差上升得超过一点，我们就大幅降低学习率，将其减半。这样，我们可以立即停止任何增幅，以免它们把我们带离之前下降的误差。
- en: Learning rate schedules have the drawback that we have to pick their parameters
    in advance (Darken, Chang, and Moody 1992). We think of these parameters as *hyperparameters*,
    just like the learning rate itself. Most deep learning libraries offer routines
    that automatically search ranges of values for us to help us find the best values
    of one or more hyperparameters.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率调度的缺点是我们必须提前选择它们的参数（Darken, Chang, 和 Moody 1992）。我们将这些参数视为*超参数*，就像学习率本身一样。大多数深度学习库提供了自动搜索值范围的例程，帮助我们找到一个或多个超参数的最佳值。
- en: Generally speaking, simple strategies for adjusting the learning rate usually
    work well, and most machine-learning libraries let us pick one of them with little
    fuss (Karpathy 2016).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，调整学习率的简单策略通常效果很好，而且大多数机器学习库让我们可以轻松选择其中之一（Karpathy 2016）。
- en: Some kind of learning rate reduction is a common feature in most machine learning
    systems. We want to learn quickly in the early stages, moving in big steps over
    the landscape, looking for the lowest minimum we can find. Then we reduce the
    learning rate, enabling us to gradually take smaller steps and land in the very
    lowest part of whatever valley we’ve found.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习系统都具备某种形式的学习率衰减。我们希望在早期阶段快速学习，大步走过整个数据景观，寻找能找到的最低点。然后我们减少学习率，使得我们能够逐渐采取更小的步伐，最终停留在我们找到的山谷的最低处。
- en: It’s natural to wonder if there’s a way to control the learning rate that doesn’t
    depend on a schedule that we set up before we start training. Surely, we can somehow
    detect when we’re near a minimum, or in a bowl, or bouncing around, and automatically
    adjust the learning rate in response.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 自然会有人想知道是否有一种方法，可以控制学习率，而不依赖于我们在训练开始前设置的调度。我们一定能以某种方式检测到我们接近最小值、处于一个盆地中，或在四处跳动，并自动调整学习率以作出响应。
- en: An even more interesting question is to consider that maybe we don’t want to
    apply the same learning rate adjustments to all of our weights. It would be nice
    to be able to tune our updates so that each weight is learning at a rate that
    works best for it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 更有趣的问题是，或许我们并不希望对所有权重应用相同的学习率调整。能够调整更新，让每个权重以最适合它的学习速率进行学习，应该是件不错的事。
- en: Let’s look at some variations on gradient descent that address those ideas.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些针对梯度下降的变体，来解决这些问题。
- en: Updating Strategies
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新策略
- en: In the following sections, we compare the performance of three different ways
    to enhance gradient descent. In these examples, we use a small, but real, two-class
    classification problem.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将比较三种不同方式来增强梯度下降的表现。在这些例子中，我们使用一个小的，但真实的二分类问题。
- en: '[Figure 15-18](#figure15-18) shows our familiar dataset of two fuzzy crescent
    moons. The classes for these points are shown by color. These 300 samples are
    our reference data for the rest of this chapter.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15-18](#figure15-18)展示了我们熟悉的两个模糊的新月形数据集。这些点的类别通过颜色显示。300个样本是我们本章其余部分的参考数据。'
- en: In order to compare different networks, we need to train them until the error
    has reached a minimum, or seems to have stopped improving. We can show the results
    of our training with plots that graph the error after each epoch. Because of the
    wide variation in algorithms, the number of epochs in these graphs vary over a
    large range.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较不同的网络，我们需要训练它们直到误差达到最小值，或者似乎不再改善。我们可以通过绘制每个训练周期后的误差图表来展示训练结果。由于算法之间的差异，这些图表中的周期数变化范围较大。
- en: '![F15018](Images/F15018.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![F15018](Images/F15018.png)'
- en: 'Figure 15-18: The data we use for the rest of this chapter. The 300 points
    form two classes of 150 points each.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-18：我们在本章其余部分使用的数据。这300个点分为两个类别，每个类别150个点。
- en: To classify our points, we’ll use a neural network with three fully connected
    hidden layers (of 12, 13, and 13 nodes), and an output layer of 2 nodes, giving
    us the probability for each of the two classes. We’ll use ReLU on each hidden
    layer, and softmax at the end. Whichever class has the larger probability at the
    output is taken as our network’s prediction. For consistency, when we need a constant
    learning rate, we use a value of *η*= 0.01\. The network is shown in [Figure 15-19](#figure15-19).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对我们的数据点进行分类，我们将使用一个具有三个全连接隐藏层（分别有12、13和13个节点）以及一个包含2个节点的输出层的神经网络，这样可以给出每个类别的概率。我们将在每个隐藏层使用ReLU激活函数，并在最后使用softmax函数。输出层概率较大的类别将作为我们的网络预测结果。为了保持一致性，当我们需要一个固定的学习率时，我们使用*η*
    = 0.01。该网络在[图 15-19](#figure15-19)中展示。
- en: '![F15019](Images/F15019.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![F15019](Images/F15019.png)'
- en: 'Figure 15-19: Our network of four fully connected layers'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-19：我们包含四个全连接层的网络
- en: Batch Gradient Descent
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量梯度下降
- en: Let’s begin by updating the weights just once per epoch, after we’ve evaluated
    all the samples. This is *batch gradient descent* (also called *epoch gradient
    descent*). In this approach, we run the entire training set through our system,
    accumulating the errors. Then we update all of the weights once using the combined
    information from all the samples.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先更新每个周期（epoch）中的权重一次，在评估完所有样本之后。这就是*批量梯度下降*（也叫做*周期梯度下降*）。在这种方法中，我们将整个训练集输入到系统中，累积所有的误差。然后我们使用所有样本的合成信息，更新所有权重一次。
- en: '[Figure 15-20](#figure15-20) shows the error from a typical training run using
    batch gradient descent.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-20](#figure15-20)展示了使用批量梯度下降进行典型训练时的误差。'
- en: '![F15020](Images/F15020.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![F15020](Images/F15020.png)'
- en: 'Figure 15-20: The error for a training run using batch gradient descent'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-20：使用批量梯度下降的训练运行误差
- en: The broad features are reassuring. The error drops quite a bit at the beginning,
    suggesting that the network is starting on a steep section of the error surface.
    Then the curve becomes much shallower. The error surface here might be a nearly
    flat region of a shallow saddle or a region that’s nearly a plateau but has just
    a bit of slope to it, because the error does continue to drop slowly. Eventually
    the algorithm finds another steep region and follows it all the way down to 0.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 整体特征令人放心。一开始，误差大幅下降，表明网络正在开始进入误差面上的一个陡峭区域。随后曲线变得平缓。此时的误差面可能是一个几乎平坦的浅鞍点区域，或者是一个几乎是平台的区域，只是稍微有点倾斜，因为误差虽然逐渐变小，但仍在继续下降。最终，算法找到另一个陡峭的区域，并将误差一直降到0。
- en: Batch gradient descent looks very smooth, but to get down to near 0 error for
    this network and data requires about 20,000 epochs, which can take a long time.
    Let’s get a closer look at what happens from one epoch to the next by zooming
    in on the first 400 epochs, shown in [Figure 15-21](#figure15-21).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降看起来非常平滑，但要将这个网络和数据的误差降到接近0，需要大约20,000个周期，这可能需要很长时间。让我们通过放大前400个周期，仔细看看从一个周期到下一个周期发生了什么，具体内容见[图
    15-21](#figure15-21)。
- en: It seems that batch gradient descent really is moving smoothly. That makes sense,
    because it’s using the error from all the samples on each update.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来批量梯度下降的过程确实非常平滑。这是有道理的，因为它在每次更新时都使用了所有样本的误差。
- en: Batch gradient descent usually produces a smooth error curve, but it has some
    issues in practice. If we have more samples than can fit in our computer’s memory,
    then the costs of *paging*, or retrieving data from slower storage media, can
    become substantial enough to make training impractically slow. This can be a problem
    in some real situations when we work with enormous datasets of millions of samples.
    It can take a great deal of time to read samples from slower memory (or even a
    hard drive) over and over. There are solutions to this problem, but they can involve
    a lot of work.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降通常会产生平滑的误差曲线，但在实际应用中存在一些问题。如果我们的样本超过了计算机内存的容量，那么*分页*（即从较慢的存储介质中读取数据）成本会变得相当高，从而使训练过程变得非常缓慢。在处理包含数百万样本的庞大数据集时，这可能是一个问题。因为从较慢的内存（甚至硬盘）中一次次读取样本可能会耗费大量时间。解决这个问题有办法，但通常需要做很多工作。
- en: '![F15021](Images/F15021.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![F15021](Images/F15021.png)'
- en: 'Figure 15-21: A close-up of the first 400 epochs of batch gradient descent
    shown in [Figure 15-20](#figure15-20)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-21：在[图 15-20](#figure15-20)中展示的批量梯度下降前400个周期的特写
- en: Closely related to this memory issue is that we must keep all the samples around
    and available so that we can run through them over and over, once per epoch. We
    sometimes say that batch gradient descent is an *offline algorithm*, meaning that
    it works strictly from information that it has stored and has access to. We can
    imagine disconnecting the computer from all networks, and it could continue to
    learn from all of our training data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相关的内存问题是，我们必须保留所有样本并使其随时可用，以便每个周期都能循环使用它们。我们有时会说批量梯度下降是一个*离线算法*，意味着它严格依赖于其存储和可访问的信息。我们可以想象将计算机与所有网络断开连接，它仍然可以从所有训练数据中继续学习。
- en: Stochastic Gradient Descent
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: Let’s go to the other extreme and update our weights after every sample. This
    is called *stochastic gradient descent*, or, more commonly, just *SGD*. Recall
    that the word *stochastic* is roughly a synonym for *random*. This word is used
    because we present the network with the training samples in a random order, so
    we can’t predict how the weights are going to change from one sample to the next.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们转向另一个极端，每个样本处理后就更新一次权重。这就是所谓的*随机梯度下降*，或更常见的简称*SGD*。回想一下，*随机*一词大致相当于*随机*，它的使用是因为我们将训练样本以随机顺序呈现给网络，因此无法预测权重如何从一个样本变到下一个样本。
- en: Since we update after every sample, our dataset of 300 samples requires us to
    update the weights 300 times over the course of each epoch. This is going to cause
    the error to jump around a lot as each sample pulls the weights one way and then
    another. Since we’re only plotting the error on an epoch-by-epoch basis, we don’t
    see this small-scale wiggling. But we still see a lot of variation epoch by epoch.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们每处理一个样本就更新一次权重，我们的数据集有300个样本，这就意味着每个周期内我们需要更新300次权重。这会导致误差剧烈波动，因为每个样本都会推动权重发生一次变化，接着又会发生反向变化。由于我们仅按每个周期绘制误差，我们看不到这种小范围的波动。但我们仍然能看到每个周期之间存在较大的变化。
- en: '[Figure 15-22](#figure15-22) shows the error of our network learning from this
    data using SGD.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15-22](#figure15-22)展示了我们使用SGD从该数据中学习的网络误差。'
- en: '![F15022](Images/F15022.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![F15022](Images/F15022.png)'
- en: 'Figure 15-22: Stochastic gradient descent, or SGD'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图15-22：随机梯度下降，或SGD
- en: The graph has the same general shape as the one for batch gradient descent in
    [Figure 15-20](#figure15-20), which makes sense since both training runs use the
    same network and data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图与[图15-20](#figure15-20)中批量梯度下降的图形大致相同，这是有道理的，因为两个训练过程使用了相同的网络和数据。
- en: The huge spike at around epoch 225 shows just how unpredictable SGD can be.
    Something in the sequencing of the samples and the way the network’s weights were
    updated caused the error to soar from nearly 0 to nearly 1\. In other words, it
    went from finding the right class for almost every sample to being dead wrong
    on almost every sample, and then back to being right again (though this recovery
    took a few epochs, as shown by the small curve to the right of the spike). If
    we were watching the errors as learning progresses, we might be inclined to stop
    the training session at the spike. If we use an automatic algorithm to watch the
    error, it may also stop it there. Yet just a few epochs after that spike, the
    system has recovered and we’re back to nearly 0\. The algorithm has definitely
    earned the word *stochastic* in its name.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在大约第225个周期时，出现了一个巨大的峰值，显示了SGD的不确定性。样本的顺序以及网络权重更新的方式导致了误差从接近0飙升至接近1。换句话说，网络从几乎正确分类每个样本，到几乎错误地分类每个样本，再到恢复正确（尽管这个恢复花了几个周期，正如峰值右侧的小曲线所示）。如果我们在学习过程中观察误差，可能会在这个峰值时倾向于停止训练。如果我们使用自动算法来观察误差，它也可能在这里停止。然而，就在这个峰值之后的几个周期内，系统已经恢复过来，我们又回到了接近0的误差。这个算法的确展现了“*随机*”这一特性。
- en: We can see from the plot that SGD got down to about 0 error in just 400 epochs.
    We cut off [Figure 15-22](#figure15-22) after that, since the curve stayed at
    0 from then on. Compare this to the roughly 20,000 epochs required by batch gradient
    descent in [Figure 15-20](#figure15-20). This increase in efficiency over batch
    gradient descent is typical (Ruder 2017).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中我们可以看到，SGD在仅仅400个周期内就将误差降到了接近0。之后我们切掉了[图15-22](#figure15-22)，因为从那时起曲线一直保持在0。与此相比，[图15-20](#figure15-20)中的批量梯度下降大约需要20,000个周期。这种相较于批量梯度下降的效率提升是典型的（Ruder
    2017）。
- en: But let’s compare apples to apples. How many times did each algorithm update
    the weights? Batch gradient descent updates the weights after each batch, so the
    20,000 epochs means it did 20,000 updates. SGD does an update after every one
    of our 300 samples. So in 400 epochs it performed 300 × 400 = 120,000 updates,
    six times more than batch gradient descent. The moral is that the amount of time
    we actually spend waiting for results isn’t completely predicted by the number
    of epochs, since the time per epoch can vary considerably.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 但是让我们做一个公平的比较。每个算法更新权重的次数是多少？批量梯度下降在每个批次之后更新权重，所以 20,000 个时期意味着它进行了 20,000 次更新。SGD
    在每一个样本后都进行更新。因此，在 400 个时期中，它执行了 300 × 400 = 120,000 次更新，是批量梯度下降的六倍。关键在于，实际等待结果的时间并不完全由时期数决定，因为每个时期所需的时间可能会有很大差异。
- en: We call SGD an *online algorithm*, because it doesn’t require the samples to
    be stored or even to be consistent from one epoch to the next. It just handles
    each sample as it arrives and updates the network immediately.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称 SGD 为*在线算法*，因为它不需要样本存储，也不需要样本在每个时期之间保持一致。它只处理每个到达的样本，并立即更新网络。
- en: SGD produces noisy results, as we can see in [Figure 15-22](#figure15-22). This
    is both good and bad. The upside of this is that SGD can jump from one region
    of the error surface to another as it searches for minima. But the downside is
    that SGD can overshoot a deep minimum and spend its time searching around inside
    of some valley with a larger error. Reducing the learning rate over time definitely
    helps with the jumping problem, but the progress is still typically noisy.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 产生的结果噪声较大，正如我们在[图 15-22](#figure15-22)中看到的那样。这既是优点也是缺点。其优点是 SGD 可以在寻找最小值的过程中，从误差面的一一区域跳跃到另一一区域。但缺点是，SGD
    可能会越过一个深度最小值，然后花费时间在一个误差更大的谷底中徘徊。随着时间的推移，降低学习率确实有助于解决跳跃问题，但进展通常仍然是噪声较大的。
- en: Noise in the error curve can be a problem because it makes it hard for us to
    know when the system is learning and when it starts overfitting. We can look at
    a sliding window of many epochs, but we may only know that we’ve overshot the
    minimum error long after it happened.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 误差曲线中的噪声可能是一个问题，因为它使得我们很难判断系统何时正在学习，何时开始过拟合。我们可以观察多个时期的滑动窗口，但我们可能直到很久以后才知道自己已经越过了最小误差。
- en: Mini-Batch Gradient Descent
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小批量梯度下降
- en: We can find a nice middle ground between the extremes of batch gradient descent,
    which updates once per epoch, and stochastic gradient descent, which updates after
    every sample. This compromise is called *mini-batch gradient descent*, or sometimes
    *mini-batch SGD*. Here, we update the weights after some fixed number of samples
    has been evaluated. This number is almost always considerably smaller than the
    batch size (the number of samples in the training set). We call this smaller number
    the *mini-batch size*, and a set of that many samples drawn from the training
    set is a *mini-batch*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在批量梯度下降（每个时期更新一次）和随机梯度下降（每个样本后更新一次）这两种极端之间找到一个不错的折衷。这个折衷方法称为*小批量梯度下降*，有时也叫*小批量
    SGD*。在这里，我们在评估了一定数量的样本后更新权重。这个数量通常远小于批量大小（即训练集中的样本数）。我们称这个较小的数量为*小批量大小*，从训练集中抽取的这些样本组成了一个*小批量*。
- en: The mini-batch size is frequently a power of 2 between about 32 and 256, and
    often it is chosen to fully use the parallel capabilities of our GPU, if we have
    one. But that’s just for efficiency purposes. We can use any size of mini-batch
    that we like.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量的大小通常是 2 的幂，介于大约 32 到 256 之间，通常选择这个大小是为了充分利用我们 GPU 的并行计算能力（如果我们有 GPU 的话）。但这只是出于效率考虑。实际上，我们可以选择任何我们喜欢的小批量大小。
- en: '[Figure 15-23](#figure15-23) shows the results of using a mini-batch of 32
    samples.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-23](#figure15-23) 显示了使用 32 个样本的小批量的结果。'
- en: This is indeed a nice blend of the two algorithms. The curve is smooth, like
    batch gradient descent, but not perfectly so. It drops down to 0 in about 5,000
    epochs, between the 400 needed by SGD and the 20,000 of batch gradient descent.
    [Figure 15-24](#figure15-24) shows a close-up of the first 400 steps.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实是两种算法的一个很好的结合。曲线平滑，像批量梯度下降一样，但并不是完美的。它大约在 5,000 个时期后下降到 0，介于 SGD 需要的 400
    个时期和批量梯度下降的 20,000 之间。[图 15-24](#figure15-24) 展示了前 400 步的详细情况。
- en: '![F15023](Images/F15023.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![F15023](Images/F15023.png)'
- en: 'Figure 15-23: Mini-batch gradient descent'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-23：小批量梯度下降
- en: '![F15024](Images/F15024.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![F15024](Images/F15024.png)'
- en: 'Figure 15-24: A close-up of the first 400 epochs of [Figure 15-23](#figure15-23),
    showing the deep plunge at the very beginning of training'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-24：显示[图 15-23](#figure15-23)的前 400 次训练周期的特写，展示了训练初期的深度下降
- en: How many updates did mini-batch SGD perform? We have 300 samples, and we used
    a mini-batch size of 32, so there are 10 mini-batches per epoch. (Ideally, we’d
    like the mini-batches to precisely divide the size of the input, but in practice,
    we can’t control the size of our datasets. This often leaves us with a partial
    mini-batch at the end.) So 10 updates per epoch, times 5,000 epochs, gives us
    50,000 updates. This is also nicely between the 20,000 updates of batch gradient
    descent and the 120,000 updates of SGD.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: mini-batch SGD 进行了多少次更新？我们有 300 个样本，且使用了 32 的 mini-batch 大小，因此每个 epoch 有 10
    个 mini-batch。（理想情况下，我们希望 mini-batches 能精确地划分输入数据的大小，但在实际中，我们无法控制数据集的大小，这通常导致最后一个
    mini-batch 是不完整的。）所以，每个 epoch 更新 10 次，乘以 5,000 次 epoch，总共是 50,000 次更新。这也恰好位于批量梯度下降的
    20,000 次更新和 SGD 的 120,000 次更新之间。
- en: Mini-batch gradient descent is less noisy than SGD, which makes it attractive
    for tracking the error. The algorithm can take advantage of huge efficiency gains
    by using the GPU for calculations, evaluating all the samples in a mini-batch
    in parallel. It’s faster than batch gradient descent, and more attractive in practice
    than SGD.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Mini-batch 梯度下降比 SGD 更少噪声，这使它在跟踪误差时具有吸引力。该算法通过使用 GPU 并行计算来利用巨大的效率提升，能够在 mini-batch
    中评估所有样本。它比批量梯度下降更快，并且在实践中比 SGD 更具吸引力。
- en: For all these reasons, mini-batch SGD is popular in practice, with “plain” SGD
    and batch gradient descent being used relatively infrequently. In fact, most of
    the time when the term *SGD* is used in the literature, or even just *gradient
    descent*, it’s understood that the authors mean mini-batch SGD (Ruder 2017). To
    make things a little more confusing, the term *batch* is often used instead of
    *mini-batch*. Because epoch-based gradient descent is used so rarely these days,
    references to batch gradient descent and batches almost always refer to mini-batch
    gradient descent and mini-batches.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这些原因，mini-batch SGD 在实践中非常流行，而“普通”SGD 和批量梯度下降则相对较少使用。事实上，在文献中，当提到 *SGD* 这个术语，甚至是
    *梯度下降* 时，通常理解为作者指的是 mini-batch SGD（Ruder 2017）。为了让事情变得更复杂，*batch* 这个术语通常用来代替 *mini-batch*。由于基于
    epoch 的梯度下降如今使用得非常少，关于批量梯度下降和批次的引用几乎总是指 mini-batch 梯度下降和 mini-batches。
- en: Gradient Descent Variations
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降变体
- en: Mini-batch gradient descent is an important algorithm, but it’s not perfect.
    Let’s review some of the challenges of mini-batch gradient descent, and a few
    ways to address them. Following convention, from here on we refer to mini-batch
    gradient descent as SGD. (The organization of this section is inspired by Ruder
    2017.)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Mini-batch 梯度下降是一个重要的算法，但它并不完美。让我们回顾一下 mini-batch 梯度下降的一些挑战，以及解决这些问题的一些方法。按照惯例，从这里开始我们将
    mini-batch 梯度下降称为 SGD。（本节的结构灵感来自 Ruder 2017。）
- en: Our first challenge is to specify what value of the learning rate *η* we want
    to use, which is notoriously hard to pick ahead of time. As we’ve seen, a value
    that’s too small can result in long learning times and getting stuck in shallow
    local minima, but a value that’s too big can cause us to overshoot deep local
    minima and then get stuck bouncing around inside a minimum when we do find it.
    If we try to avoid the problem by using a decay schedule to change *η* over time,
    we still have to pick the starting value of *η* and then the schedule’s hyperparameters
    as well.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们面临的第一个挑战是指定我们希望使用的学习率 *η* 的值，这通常是一个难以提前确定的问题。如我们所见，过小的学习率可能导致学习时间过长，且陷入浅层局部最小值，而过大的学习率则可能导致我们越过深层局部最小值，然后在找到最小值后在其中反复跳动。如果我们通过使用衰减计划来改变
    *η* 来避免这个问题，我们仍然需要选择 *η* 的初始值，以及计划的超参数。
- en: We also have to pick the size of the mini-batch. This is rarely an issue, because
    we usually choose whatever value produces calculations that are most closely matched
    to the structure of our GPU or other hardware.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要选择 mini-batch 的大小。这通常不是问题，因为我们通常选择与我们的 GPU 或其他硬件结构最匹配的计算值。
- en: Let’s consider some improvements. Right now, we’re updating all the weights
    with a one-update-rate-fits-all approach. Instead, we can find a unique learning
    rate for each weight in the system so we’re not just moving it in the best direction,
    but we’re moving it by the best amount. We’ll see examples of this in the following
    pages.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一些改进。现在，我们是通过一个统一的更新速率来更新所有权重。我们可以为系统中的每个权重找到一个独特的学习速率，这样我们不仅是在朝最佳方向移动它，还在以最佳的幅度进行移动。我们将在接下来的页面中看到这方面的例子。
- en: 'Another improvement begins with the recognition that sometimes when the error
    surface forms a saddle, the surface can be shallow in all directions, so locally,
    it’s almost (but not quite) a plateau. This can slow our progress to an excruciating
    crawl. Research has shown that deep learning systems often have plenty of saddles
    in their error landscapes (Dauphin et al. 2014). It would be nice if there was
    a way to get unstuck in these situations, or better yet, to avoid getting stuck
    in them in the first place. The same thing goes for plateaus: we’d like to avoid
    getting stuck in the flat regions where the gradient drops to 0\. To do so, we
    want to avoid the regions where the gradient drops to 0, except of course for
    the minima we’re seeking.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个改进是基于这样一个认识：有时当误差面形成鞍点时，表面在所有方向上可能都很浅，所以在局部范围内，它几乎是一个（但不完全是）平原。这会让我们的进展变得非常缓慢。研究表明，深度学习系统的误差面中往往有很多鞍点（Dauphin
    等，2014）。如果有办法在这些情况下不再被卡住，或者更好的是，避免在一开始就陷入这种困境，那就太好了。同样，平原也应该避免：我们希望避免进入那些梯度下降到
    0 的平坦区域，当然，除非是我们要寻找的最小值。
- en: Let’s look at some variations of gradient descent that address these issues.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看一些解决这些问题的梯度下降变种。
- en: Momentum
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动量
- en: Let’s consider two weights at the same time. We can plot their values on an
    XY plane, and above them show the error that results from training the system
    with those values for those weights. Let’s think of our error surface as a landscape.
    Now we can picture our task of minimizing error as following a drop of water that’s
    looking for the lowest point.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们同时考虑两个权重。我们可以在 XY 平面上绘制它们的值，并在它们上方展示通过这些权重值训练系统所产生的误差。让我们把误差面看作是一片地形。现在我们可以将最小化误差的任务想象成跟随一滴水，寻找最低点的过程。
- en: '[Figure 15-25](#figure15-25) repeats a figure from Chapter 5 that shows an
    example of this way of thinking about the training process.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-25](#figure15-25)重复了第 5 章中的一张图，展示了这种思考训练过程的方式。'
- en: '![F15025](Images/F15025.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![F15025](Images/F15025.png)'
- en: 'Figure 15-25: A drop of water rolling down an error surface. This is a repeat
    of a figure from Chapter 5.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-25：一滴水沿误差面滚动。这是第 5 章中的一张重复图。
- en: Instead of water, let’s think of this as a little ball rolling down the error
    surface. We know from the physical world that a real ball rolling down a hill
    in this way has some *inertia*, which describes its resistance to a change in
    its motion. If it’s rolling along in a given direction at a certain speed, it
    will continue to move that way unless something interferes with it.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将水替换为一个小球，想象它沿着误差面滚动。我们知道从物理世界的角度来看，一个真正的球沿着山坡滚动时，会有一些*惯性*，它描述了物体对其运动变化的抵抗力。如果它沿某个方向以一定速度滚动，除非有外力干扰，否则它会继续保持该方向的运动。
- en: A related idea is the ball’s *momentum*, which is a bit more abstract from a
    physical point of view. Although they’re distinct ideas, sometimes deep learning
    discussions casually refer to inertia as momentum, and the algorithm we’re about
    to look at uses that language.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相关的概念是小球的*动量*，从物理角度来看，这个概念稍微抽象一些。虽然它们是不同的概念，但有时深度学习的讨论中会随意地将惯性称为动量，而我们接下来要看的算法也使用了这一术语。
- en: This idea is what keeps the ball in [Figure 15-25](#figure15-25) moving across
    the plateau after it has come down from the peak and passed into the saddle near
    the middle of the figure. If the ball’s motion was determined strictly by the
    gradient, when it hit the plateau near the middle of the figure, it would stop
    (or if it was a near-plateau, the ball would slow to a crawl). But the ball’s
    momentum (or more properly, its inertia) keeps it rolling onward.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念使得[图 15-25](#figure15-25)中的小球在从山顶滚下来并经过图中间的鞍点后，能够继续沿着平台移动。如果小球的运动仅仅由梯度决定，那么当它到达图中间的平原时，它会停下（或者如果它接近平原，小球会慢慢停止）。但小球的动量（或更准确地说，是惯性）让它继续滚动。
- en: Suppose we’re near the left side of [Figure 15-26](#figure15-26). As we roll
    down the hill, we reach the plateau starting at around −0.5.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们靠近 [图 15-26](#figure15-26) 左侧。随着我们沿着山坡向下滚动，我们在约 -0.5 处到达平台期。
- en: '![F15026](Images/F15026.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![F15026](Images/F15026.png)'
- en: 'Figure 15-26: An error curve with a plateau between a hill and valley'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-26：带有平台期的错误曲线，介于山峰和山谷之间
- en: With regular gradient descent, we stop on the plateau since the gradient is
    zero, as shown in the left of [Figure 15-27](#figure15-27). But if we include
    some momentum, as shown on the right, the ball keeps going for a while. It does
    slow down, but if we’re lucky, it continues to roll far enough to find the next
    valley.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用常规梯度下降时，我们会在平台期停止，因为梯度为零，如 [图 15-27](#figure15-27) 左图所示。但如果我们加入一些动量，如右图所示，球体会继续滚动一段时间。虽然它会逐渐减慢，但如果幸运的话，它会继续滚动足够远，找到下一个山谷。
- en: The technique of *momentum gradient descent* (Qian 1999) is based on this idea.
    For each step, once we calculate how much we want each weight to change, we add
    in a small amount of its change from the previousstep. If the change on a given
    step is 0, or nearly 0, but we had some larger change on the last step, we use
    some of that prior motion now, which pushes us along over the plateau.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*动量梯度下降*（Qian 1999）技术基于这个思想。对于每一步，一旦我们计算出每个权重的变化量，我们会在其中加入上一阶段的少量变化。如果在某一步的变化为0，或者接近0，但在上一阶段我们有较大的变化，我们就会利用之前的动量，帮助我们继续前进，跨越平台期。'
- en: '![F15027](Images/F15027.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![F15027](Images/F15027.png)'
- en: 'Figure 15-27: Gradient descent on the error curve of [Figure 15-26](#figure15-26).
    Left: Gradient descent with decay. Right: Gradient descent with decay and momentum.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-27：在 [图 15-26](#figure15-26) 错误曲线上的梯度下降。左：带衰减的梯度下降。右：带衰减和动量的梯度下降。
- en: '[Figure 15-28](#figure15-28) shows the idea visually.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-28](#figure15-28) 直观地展示了这一概念。'
- en: '![F15028](Images/F15028.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![F15028](Images/F15028.png)'
- en: 'Figure 15-28: Finding the step for gradient descent with momentum'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-28：带动量的梯度下降步长的寻找
- en: We suppose that some weight had an error A. We updated that weight to value
    w2, with an error B. We now want to find the next value for the weight, w3, which
    will have error C. To find C, we find the change that we applied to point A. That
    is, we find the previous motion applied to A. This is the momentum, labeled *m*,
    shown in [Figure 15-28](#figure15-28)(a).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 假设某个权重的误差为 A。我们将该权重更新为值 w2，并且误差为 B。现在，我们想找到下一个权重值 w3，它将有误差 C。为了找到 C，我们需要找出对点
    A 所做的变化。也就是说，我们需要找到施加在 A 上的先前变化。这就是动量，用 *m* 表示，如 [图 15-28](#figure15-28)(a) 所示。
- en: We multiply the momentum, *m*, by a scaling factor usually referred to with
    the lowercase Greek letter *γ* (gamma). Sometimes this is called the *momentum
    scaling factor*, and it’s a value from 0 to 1\. Multiplying *m* by this value
    gives us a new arrow *γm* that points in the same direction as *m* but is the
    same length or shorter. We then find the scaled gradient, *ηg*, at B, as we did
    before, shown in [Figure 15-28](#figure15-28)(b). Now we have all we need. We
    add together the scaled momentum, *γm*, and the scaled gradient, *ηg*, to B, which
    we do graphically by placing the tail of *γm* at the head of *ηg*, as in [Figure
    15-28](#figure15-28)(c).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将动量 *m* 乘以一个缩放因子，通常用小写希腊字母 *γ*（gamma）表示。有时这被称为 *动量缩放因子*，其值介于 0 到 1 之间。将 *m*
    乘以这个值后，我们得到一个新的箭头 *γm*，它的方向与 *m* 相同，但长度相同或更短。然后，我们像之前一样，在 B 点找到缩放后的梯度 *ηg*，如 [图
    15-28](#figure15-28)(b) 所示。现在，我们得到了所有需要的信息。我们将缩放后的动量 *γm* 和缩放后的梯度 *ηg* 相加到 B 上，图示化地表现为将
    *γm* 的尾部放置在 *ηg* 的头部，如 [图 15-28](#figure15-28)(c) 所示。
- en: Let’s apply this rule and see how the weight and error change over time. [Figure
    15-29](#figure15-29) shows our symmetrical valley from before, and sequential
    steps of training. In this figure, we use both an exponential decay schedule and
    momentum. This is just like our sequence from [Figure 15-15](#figure15-15), but
    now the change applied to each step also includes momentum, or a scaled version
    of the change from the previous step. We can see this by looking at the two lines
    that emerge from each point (one for the gradient, the other for the momentum).
    This total then becomes the new change.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用这个规则，看看权重和误差如何随时间变化。[图 15-29](#figure15-29) 展示了我们之前的对称山谷，以及训练的连续步骤。在这个图中，我们同时使用了指数衰减计划和动量。这就像我们在
    [图 15-15](#figure15-15) 中的序列一样，但现在每个步骤施加的变化还包括动量，或者说是上一阶段变化的缩放版本。我们可以通过观察从每个点出发的两条线（一个是梯度，另一个是动量）来看到这一点。然后，这个总和就成为了新的变化。
- en: '![F15029](Images/F15029.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![F15029](Images/F15029.png)'
- en: 'Figure 15-29: Learning with both an exponential decay schedule and momentum'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: On each step, we first find the gradient and multiply it by the current value
    of the learning rate *η*, as before. Then we find the previous change, scale it
    by *γ*, and add both of those changes to the current position of the weight. That
    combination gives us the change in this step.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-30](#figure15-30) shows a close-up of the sixth step in the grid,
    along with the error at each point along the way.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'An interesting thing happened here: when the ball reached the right side of
    the valley, it continued to roll up, even though the gradients pointed down. That’s
    just what we’d expect of a real ball. We can see it slowing down, and then eventually
    it comes back down the slope, overshooting the bottom, but by less than before,
    then slowing and coming back down again, and so on.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![F15030](Images/F15030.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-30: The final step in [Figure 15-29](#figure15-29), along with the
    error for each point'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: If we use too much momentum, our ball can fly right up the other side and out
    of the bowl altogether, but if we use too little momentum, our ball may not get
    across the plateaus it encounters along the way. [Figure 15-31](#figure15-31)
    shows our error curve from [Figure 15-26](#figure15-26). Here we used trial and
    error to find a value of *γ* to scale the momentum so that our ball gets through
    the plateau but can still settle into the minimum at the bottom of the bowl.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![F15031](Images/F15031.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-31: Using enough momentum to cross a plateau, but not so much that
    the ball is unable to settle nicely into the bottom of the minimum'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Finding the right amount of momentum to use is another task where we need to
    use our experience and intuition, along with trial and error, to help us understand
    the behavior of our specific network and the data we’re working with. We can also
    search for it using hyperparameter searching algorithms.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: To put this all together, we find the gradient, scale it by the current learning
    rate *η*, add in the previous change scaled by *γ*, and that gives us our new
    position. If we set *γ* to 0, then we add in none of the last step, and we have
    “normal” (or “vanilla”) gradient descent. If *γ* is set to 1, then we add in the
    entirety of the last change. Often we use a value of around 0.9\. In Figures 15-29
    and [Figure 15-31](#figure15-31), we set gamma to 0.7 to better illustrate the
    process.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-32](#figure15-32) shows the result of 15 steps of learning with
    both learning rate decay and momentum. The ball starts on the left, rolls down
    and then far up the right side, then it rolls down again and rolls up the left
    side, and so on, climbing a little less each time.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![F15032](Images/F15032.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-32: Learning with momentum and a decaying learning rate for 15 points'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Momentum helps us get over flat plateaus and out of shallow places in saddles.
    It has the additional benefit of helping us zip down steep slopes, so even with
    a small learning rate, we can pick up some efficiency.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 动量帮助我们跨越平坦的平原并逃离鞍点的浅处。它还有一个额外的好处：帮助我们迅速下坡，即使学习率较小，我们也能提高效率。
- en: '[Figure 15-33](#figure15-33) shows the error for a training run on our dataset
    of [Figure 15-18](#figure15-18) consisting of two crescent moons.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15-33](#figure15-33)显示了在我们由[图15-18](#figure15-18)构成的两弯月数据集上训练时的误差。'
- en: '![F15033](Images/F15033.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![F15033](Images/F15033.png)'
- en: 'Figure 15-33: Error curve for training with our two-crescent data using mini-batch
    gradient descent with momentum. We got to zero error in a little more than 600
    epochs.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图15-33：使用带动量的迷你批量梯度下降训练我们的两弯月数据集的误差曲线。我们在600多个周期内就达到了零误差。
- en: Here we’re using mini-batch gradient descent (or SGD) with momentum. It’s noisier
    than the mini-batch curve of [Figure 15-23](#figure15-23) because the momentum
    sometimes carries us past where we want to be, causing a spike in the error. The
    error when using mini-batch SGD alone in [Figure 15-23](#figure15-23) for our
    data took about 5,000 epochs to reach about 0 error. With momentum, we get there
    in a little over 600 epochs. Not bad!
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用带有动量的迷你批量梯度下降（或SGD）。与[图15-23](#figure15-23)中的迷你批量曲线相比，它的噪声更大，因为动量有时会将我们推过目标位置，导致误差出现尖峰。使用仅迷你批量SGD时，[图15-23](#figure15-23)中的误差需要大约5,000个周期才能达到接近零的误差。而使用动量，我们在600多个周期内就能达到目标。不错吧！
- en: 'Momentum clearly helps us learn more quickly, which is a great thing. But momentum
    brings us a new problem: choosing the momentum value *γ*. As we mentioned, we
    can pick this value using experience and intuition or use a hyperparametersearch
    for the value that gives us the best results.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 动量显然帮助我们更快地学习，这是一个好事。但是动量带来了一个新问题：选择动量值*γ*。正如我们所提到的，我们可以通过经验和直觉来选择这个值，或者使用超参数搜索来寻找给我们最佳结果的值。
- en: Nesterov Momentum
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Nesterov 动量
- en: Momentum let us reach into the past for information to help us train. Now let’s
    reach into the future. The key idea is that instead of using only the gradient
    at the location where we currently are, we also use the gradient at the location
    where we expect that we’re *going to be*. Then we can use some of that “gradient
    from the future” to help us now.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 动量让我们能够从过去获取信息来帮助训练。现在让我们从未来获取信息。关键思想是，不仅仅使用我们当前所在位置的梯度，还使用我们预计将要到达位置的梯度。然后，我们可以利用一些“来自未来的梯度”来帮助我们。
- en: Because we can’t really predict the future, we estimatewhere we’re going to
    be on the next step and use the gradient there. The thinking is that if the error
    surface is relatively smooth, and our estimate is pretty good, then the gradient
    we find at our estimated next position is close to the gradient where we’d actually
    end up if we just moved using standard gradient descent, with or without momentum.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们无法真正预测未来，我们估算下一步的位置，并在那里使用梯度。这样做的思路是，如果误差面比较平滑，并且我们的估计相当准确，那么在我们估计的下一位置上找到的梯度与如果我们仅使用标准梯度下降（无论是否有动量）移动时实际到达的位置的梯度是接近的。
- en: Why is it useful to use the gradient from the future? Suppose we’re rolling
    down one side of a valley and approaching the bottom. On the next step, we overshoot
    the bottom and end up somewhere on the other wall. As we saw before, momentum
    carries us up that wall for a few steps, slowing as we lose momentum, until we
    turn around and come back down. But if we can predictthat we’ll be on the far
    side, we can include some of the gradient at that point in our calculations now.
    So instead of moving so far to the right and up the hill, that leftward push from
    the future causes us to move by a little less distance, so we don’t overshoot
    so far and end up closer to the bottom of the valley.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用来自未来的梯度有用呢？假设我们正从山谷的一侧滚下来，接近底部。在下一步中，我们越过了底部，最终到达了另一侧的墙上。正如我们之前看到的，动量会带着我们沿着墙上几步，随着动量的减弱逐渐减慢，直到我们转过身来重新下降。但如果我们能够预测到自己将会在远侧，我们就可以在现在的计算中包含那时的梯度。因此，来自未来的推力让我们不再像之前那样向右上方移动过远，而是使我们移动的距离稍微小一点，这样我们就不会过度越过底部，最终接近山谷的底部。
- en: In other words, if the next move we’re going to make is in the same direction
    as the last one, we take a larger step now. If the next move is going to move
    us backward, we take a smaller step.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果我们下一步的移动方向与上一步相同，我们现在就采取更大的步伐。如果下一步的移动会使我们后退，我们则采取更小的步伐。
- en: Let’s break this down into steps so we don’t get mixed up between estimates
    and realities. [Figure 15-34](#figure15-34) shows the process.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: As before, we imagine that we started with our weight at position A, and after
    the most recent update, we ended up at B, as shown in [Figure 15-34](#figure15-34)(a).
    As with momentum, we find the change applied at point A to bring us to B (the
    arrow *m*) and we scale that by *γ*.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Now comes the new part, starting in [Figure 15-34](#figure15-34)(b). Rather
    than finding the gradient at B, we first add the scaled momentum to B to get the
    “predicted” error P. This is our guess for where we will end up on the error surface
    after the next step. As shown [Figure 15-34](#figure15-34)(c), we find the gradient
    *g* at point P and scale it as usual to get *ηg*. Now we find the new point C
    in [Figure 15-34](#figure15-34)(d) by adding the scaled momentum *γm* and the
    scaled gradient *ηg* to B.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![F15034](Images/F15034.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-34: Gradient descent with Nesterov momentum'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Notice we’re not using the gradient at point B at all. We just combine a scaled
    version of the momentum that got us to B and a scaled version of the gradient
    at our predicted point P.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Notice too that the point C in [Figure 15-34](#figure15-34)(d) is closer to
    the bottom of the bowl than point P, where we’d have ended up with normal momentum.
    By looking into the future and seeing that we’d be on the other side of the valley,
    we are able to use that left-pointing gradient to prevent rolling far up the far
    side.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: In honor of the researcher who developed this method, it’s called *Nesterov
    momentum*, or the *Nesterov accelerated gradient* (Nesterov 1983). It’s basically
    a souped-up version of the momentum technique we saw earlier. Though we still
    have to pick a value for *γ,* we don’t have to pick any new parameters. This is
    a nice example of an algorithm that gives us increased performance without requiring
    more work on our end.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-35](#figure15-35) shows the result of Nesterov momentum for 15 steps.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![F15035](Images/F15035.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-35: Running Nesterov momentum for 15 steps. It finds the bottom of
    the valley in about seven steps and then stays there.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-36](#figure15-36) shows the error curve for our standard test case
    using Nesterov momentum. This uses the exact same model and parameters as the
    momentum-only results in [Figure 15-33](#figure15-33), but it’s both less noisy
    and more efficient, getting down to about 0 error at roughly epoch 425, rather
    than the roughly 600 required by regular momentum alone.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![f15036](Images/f15036.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-36: Error for mini-batch SGD with Nesterov momentum. The system reaches
    zero error around epoch 600\. The graph shows 1,000 epochs.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Any time we use momentum it’s worth considering Nesterov momentum instead. It
    requires no additional parameters from us, but it usually learns more quickly
    and with less noise.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Adagrad
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve seen two types of momentum that help push us through plateaus and reduce
    overshooting. We’ve been using the same learning rate when we update all the weights
    in our network. Earlier in this chapter, we mentioned the idea of using a learning
    rate *η* that’s tailored individually for each weight.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到两种动量方法，它们帮助我们突破平台期并减少过度波动。我们在更新神经网络中所有权重时使用相同的学习率。在本章早些时候，我们提到过使用一个针对每个权重量身定制的学习率
    *η* 的想法。
- en: Several related algorithms use this idea. Their names all begin with *Ada*,
    standing for “adaptive.”
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一些相关算法使用了这个思想。它们的名字都以 *Ada* 开头，代表“自适应（adaptive）”。
- en: Let’s start with an algorithm called *Adagrad*, which is short for *adaptive
    gradient learning* (Duchi, Hazan, and Singer 2011). As the name implies, the algorithm
    adapts (or changes) the size of the gradient for each weight. In other words,
    Adagrad gives us a way to perform learning-rate decay on a weight-by-weight basis.
    For each weight, Adagrad takes the gradient that we use in that update step, squares
    it, and adds that into a running sum for that weight. Then the gradient is divided
    by a value derived from this sum, giving us the value that’s then used for the
    update.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个名为 *Adagrad* 的算法开始，它是 *自适应梯度学习*（Duchi、Hazan 和 Singer 2011）的缩写。顾名思义，该算法会根据每个权重自适应地调整梯度的大小。换句话说，Adagrad
    给了我们一种按权重逐个进行学习率衰减的方法。对于每个权重，Adagrad 会使用该更新步骤中的梯度，将其平方并加入该权重的累加和中。然后，梯度会被这个累加和衍生出的值除，以得到用于更新的值。
- en: Because each step’s gradient is squared before it’s added in, the value that’s
    added into the sum is always positive. As a result, this running sum gets larger
    and larger over time. To keep it from growing out of control, we divide each change
    by that growing sum, so the changes to each weight get smaller and smaller over
    time.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个步骤的梯度在加入之前都会被平方，因此加入累加和的值始终是正数。因此，这个运行累加和会随着时间的推移变得越来越大。为了防止它失控，我们将每次变化除以这个逐渐增大的累加和，从而使每个权重的变化随着时间推移变得越来越小。
- en: This sounds a lot like learning rate decay. As time goes on, the changes to
    the weights get smaller. The difference here is that the slowdown in learning
    is being computed uniquely for each weight based on its history.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来很像学习率衰减。随着时间的推移，权重的变化变得越来越小。这里的不同之处在于，学习的减速是基于每个权重的历史独特地计算的。
- en: Because Adagrad is effectively automatically computing a learning rate for every
    weight on the fly, the learning rate we use to kick things off isn’t as critical
    as it was for earlier algorithms. This is a huge benefit, since it frees us from
    the task of fine-tuning that error rate. We often set the learning rate *η* to
    a small value like 0.01 and let Adagrad handle things from there.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Adagrad 实际上是自动为每个权重计算学习率，因此我们用来启动的学习率不像早期算法那样重要。这是一个巨大的好处，因为它解放了我们调节错误率的任务。我们通常将学习率
    *η* 设置为像 0.01 这样的小值，然后让 Adagrad 从那里开始处理。
- en: '[Figure 15-37](#figure15-37) shows the performance of Adagrad on our test data.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-37](#figure15-37) 显示了 Adagrad 在我们测试数据上的表现。'
- en: This has the same general shape as most of our other curves, but it takes a
    very long time to get to 0\. Because the sum of the gradients gets larger over
    time, eventually we’ll find that dividing each new gradient by a value related
    to that sum gives us gradients that approach 0\. The increasingly small updates
    are why the error curve for Adagrad descends so very slowly as it tries to get
    rid of that last remaining error.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 它与我们其他曲线的总体形状相同，但需要很长时间才能接近 0。由于梯度的累加和随时间增大，最终我们会发现将每个新梯度除以与该累加和相关的值，会得到接近 0
    的梯度。越来越小的更新是 Adagrad 错误曲线下降如此缓慢的原因，因为它试图消除最后剩余的误差。
- en: We can fix that without too much work.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过不太复杂的工作来解决这个问题。
- en: '![F15037](Images/F15037.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![F15037](Images/F15037.png)'
- en: 'Figure 15-37: The performance of Adagrad on our test setup'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-37：Adagrad 在我们测试设置中的表现
- en: Adadelta and RMSprop
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Adadelta 和 RMSprop
- en: The problem with Adagrad is that the gradient we apply to each weight for its
    update step just keeps getting smaller and smaller. That’s because the running
    sum just gets larger and larger.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Adagrad 的问题在于我们应用于每个权重的梯度在更新步骤中不断变得越来越小。这是因为运行的累加和一直在增大。
- en: Instead of summing up all the squared gradients since the beginning of training,
    suppose we keep a *decaying sum* of these gradients. We can think of this as a
    running list of the most recent gradients for each weight. Each time we update
    the weights, we tack the new gradient onto the end of the list and drop the oldest
    one off the start. To find the value we use to divide the new gradient, we add
    up all the values in the list, but we first multiply them all by a number based
    on their position in the list. Recent values get multiplied by a large value,
    while the oldest ones get multiplied by a very small value. This way our running
    sum is most heavily determined by recent gradients, though it is influenced to
    a lesser degree by the older gradients (Ruder 2017).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于从训练开始时就将所有平方梯度求和，我们假设保持这些梯度的*衰减和*。我们可以将其视为每个权重的最新梯度的一个运行列表。每次更新权重时，我们会将新梯度添加到列表的末尾，并删除列表开头的最旧的一个。为了找到用来除以新梯度的值，我们将列表中的所有值相加，但首先将它们乘以一个与其在列表中的位置相关的数值。最近的值会乘以较大的数值，而最旧的值会乘以非常小的数值。通过这种方式，我们的累加和主要由最近的梯度决定，尽管它也会受到较老梯度的较小影响（Ruder
    2017）。
- en: In this way, the running sum of the gradients (and thus the value we divided
    new gradients by) can go up and down based on the gradients we’ve applied recently.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，梯度的累加和（以及我们用来除以新梯度的值）可以根据我们最近应用的梯度而上下波动。
- en: This algorithm is called *Adadelta* (Zeiler 2012). The name comes from “adaptive,”
    like Adagrad, and the *delta* refers to the Greek letter *δ* (delta), which mathematicians
    often use to refer to change. This algorithm adaptively changes how much the weights
    are updated on each step using each one’s weighted running sum.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法被称为*Adadelta*（Zeiler 2012）。这个名字来源于“adaptive”（自适应），类似于Adagrad，而*delta*指的是希腊字母*δ*（delta），数学家通常用它来表示变化。这个算法自适应地根据每个梯度的加权累加和来改变每一步更新权重的量。
- en: Since Adadelta adjusts the learning rates on the weights individually, any weight
    that’s been on a steep slope for a while will slow down so it doesn’t go flying
    off, but when that weight is on a flatter section, it’s allowed to take bigger
    steps.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Adadelta会单独调整每个权重的学习率，任何在陡峭斜坡上的权重会减慢更新速度，避免过快变化，而当该权重处于较平坦的部分时，它将允许较大的步长更新。
- en: Like Adagrad, we often start the learning rate at a value around 0.01, and then
    let the algorithm adjust it from then on.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 与Adagrad一样，我们通常将学习率设置为大约0.01的值，然后让算法从那时起进行调整。
- en: '[Figure 15-38](#figure15-38) shows the results of Adadelta on our test setup.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-38](#figure15-38)展示了在我们的测试设置上使用Adadelta的结果。'
- en: '![F15038](Images/F15038.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![F15038](Images/F15038.png)'
- en: 'Figure 15-38: The results of training with Adadelta on our test data'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-38：在我们的测试数据上使用Adadelta训练的结果
- en: This compares favorably to Adagrad’s performance in [Figure 15-37](#figure15-37).
    It’s nice and smooth and reaches 0 at around epoch 2,500, much sooner than Adagrad’s
    8,000 epochs.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与[图 15-37](#figure15-37)中Adagrad的表现相比，这种方法表现更好。它平滑且在大约2500个epoch时达到0，远远早于Adagrad的8000个epoch。
- en: Adadelta has the downside of requiring another parameter, which is unfortunately
    also called gamma (*γ*). It’s roughly related to the parameter *γ* used by the
    momentum algorithms, but they’re sufficiently different that it’s best to consider
    them distinct ideas that happen to have been given the same name. The value of
    *γ* here tells us how much we scale down the gradients in our history list over
    time. A large value of *γ* “remembers” values from farther back than smaller values
    and will let them contribute to the sum. A smaller value of *γ* just focuses on
    recent gradients. Often we set this *γ* to around 0.9.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Adadelta的缺点是需要另一个参数，这个参数也叫做gamma（*γ*）。它大致与动量算法中使用的*γ*参数相关，但它们足够不同，因此最好将它们视为两个不同的概念，恰好有相同的名称。这里的*γ*值告诉我们如何随着时间的推移缩小历史梯度列表中的梯度。较大的*γ*值“记住”较远的历史值，而较小的*γ*值则只关注最近的梯度。通常我们将这个*γ*设置为大约0.9。
- en: There’s actually another parameter in Adadelta, named with the Greek letter
    *ε* (epsilon). This is a detail that’s used to keep the calculations numerically
    stable. Most libraries will set this to a default value that’s carefully selected
    by the programmers to make things work as well as possible, so it should never
    be changed unless there’s a specific need.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，Adadelta中还有另一个参数，希腊字母*ε*（epsilon）表示。这个细节用于保持计算的数值稳定性。大多数库会将其设置为一个由程序员精心选择的默认值，以使系统尽可能地正常工作，因此除非有特定需求，否则不应更改此值。
- en: An algorithm that’s very similar to Adadelta, but that uses slightly different
    mathematics, is called *RMSprop* (Hinton, Srivastava, and Swersky 2015). The name
    comes from the fact that it uses a root-mean-squared operation, often abbreviated
    RMS, to determine the adjustment that is added (or *propagated*, hence the “prop”
    in the name) to the gradients.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 与Adadelta非常相似的算法，但使用了稍微不同的数学方法，叫做*RMSprop*（Hinton、Srivastava和Swersky 2015）。这个名字来源于它使用了均方根操作，通常缩写为RMS，用来确定加入（或*传播*，因此名称中有“prop”）到梯度中的调整量。
- en: RMSprop and Adadelta were invented around the same time, and work in similar
    ways. RMSprop also uses a parameter to control how much it “remembers,” and this
    parameter, too, is named *γ*. Again, a good starting value is around 0.9.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: RMSprop和Adadelta大约在同一时间被发明，并且工作方式相似。RMSprop也使用一个参数来控制它“记住”多少信息，这个参数也被命名为*γ*。同样，好的起始值大约是0.9。
- en: Adam
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Adam
- en: The previous algorithms share the idea of saving a list of squared gradients
    with each weight. They then create a scaling factor by adding up the values in
    this list, perhaps after scaling them. The gradient at each update step is divided
    by this total. Adagrad gives all the elements in the list equal weight when it
    builds its scaling factor, while Adadelta and RMSprop treat older elements as
    less important, and thus they contribute less to the overall total.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的算法共享一个思想，即为每个权重保存一个平方梯度的列表。然后它们通过将这个列表中的值加起来（可能在缩放之后）来创建一个缩放因子。每次更新步骤中的梯度都被这个总数除以。Adagrad在构建缩放因子时给列表中的所有元素相等的权重，而Adadelta和RMSprop则认为较旧的元素不那么重要，因此它们对总体总数的贡献较小。
- en: Squaring the gradient before putting it into the list is useful mathematically,
    but when we square a number, the result is always positive. This means that we
    lose track of whether that gradient in our list was positive or negative, which
    is useful information to have. So, to avoid losing this information, we can keep
    a second list of the gradients without squaring them. Then we can use both lists
    to derive our scaling factor.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在将梯度放入列表之前对其平方在数学上是有用的，但当我们对一个数字进行平方时，结果总是正数。这意味着我们无法知道列表中的梯度是正还是负，而这是一个很有用的信息。为了避免丢失这些信息，我们可以保持一个不对梯度进行平方的第二个列表。然后我们可以利用这两个列表来推导出我们的缩放因子。
- en: This is the approach of an algorithm called *adaptive moment estimation*, or
    more commonly *Adam* (Kingma and Ba 2015).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是一种名为*自适应矩估计*的算法方法，或更常见的叫法是*Adam*（Kingma和Ba 2015）。
- en: '[Figure 15-39](#figure15-39) shows how Adam performs.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-39](#figure15-39)展示了Adam的表现。'
- en: '![F15039](Images/F15039.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![F15039](Images/F15039.png)'
- en: 'Figure 15-39: The Adam algorithm on our test set'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-39：Adam算法在我们的测试集上的表现
- en: The output is great. It’s only slightly noisy and hits about 0 error at around
    epoch 900, much sooner than Adagrad or Adadelta. The downside is that Adam has
    two parameters, which we must set at the start of learning. The parameters are
    named for the Greek letter *β* (beta) and are called “beta 1” and “beta 2,” written
    *β*1 and *β*2\. The authors of the paper on Adam suggest setting *β*1 to 0.9,
    and *β*2 to 0.999, and these values indeed often work well.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 输出表现很好，只有略微的噪声，在大约900个训练周期时达到了接近0的误差，远远快于Adagrad或Adadelta。缺点是Adam有两个参数，我们必须在学习开始时设置。参数是以希腊字母*β*（贝塔）命名的，分别称为“beta
    1”和“beta 2”，写作*β*1和*β*2。Adam论文的作者建议将*β*1设置为0.9，将*β*2设置为0.999，这些值确实通常表现良好。
- en: Choosing an Optimizer
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择优化器
- en: This has not been a complete list of all the optimizers that have been proposed
    and studied. There are many others, with more coming all the time, and each has
    its own strengths and weaknesses. Our goal was to give an overview of some of
    the most popular techniques and to understand how they achieve their speedups.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是所有已提出并研究过的优化器的完整列表。还有许多其他优化器，并且新优化器不断出现，每个优化器都有其优缺点。我们的目标是提供一些最流行技术的概述，并理解它们是如何加速的。
- en: '[Figure 15-40](#figure15-40) summarizes our two-moon results for SGD with Nesterov
    momentum and the three adaptive algorithms of Adagrad, Adadelta, and Adam.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-40](#figure15-40)总结了我们关于使用Nesterov动量的SGD和三个自适应算法（Adagrad、Adadelta和Adam）的两个月亮实验结果。'
- en: '![F15040](Images/F15040.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![F15040](Images/F15040.png)'
- en: 'Figure 15-40: The loss, or error, over time for four of the algorithms just
    covered. This graph shows only the first 4,000 epochs.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-40：四种刚才介绍的算法在时间轴上的损失或误差。这张图只显示了前4,000个训练周期。
- en: In this simple test case, mini-batch SGD with Nesterov momentum is the clear
    winner, with Adam coming in a close second. In more complicated situations, the
    adaptive algorithms typically perform better.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的测试案例中，带有 Nesterov 动量的迷你批量 SGD 显然是最优选择，Adam 紧随其后。在更复杂的情况下，自适应算法通常表现更好。
- en: Across a wide variety of datasets and networks, the final three adaptive algorithms
    that we discussed (Adadelta, RMSprop, and Adam) often perform very similarly (Ruder
    2017). Studies have found that Adam does a slightly better job than the others
    in some circumstances, so that’s usually a good place to start (Kingma and Ba
    2015).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种数据集和网络上，我们讨论的最后三种自适应算法（Adadelta、RMSprop 和 Adam）通常表现非常相似（Ruder 2017）。研究发现，在某些情况下，Adam
    的表现略优于其他算法，因此通常是一个不错的起点（Kingma 和 Ba 2015）。
- en: Why are there so many optimizers? Wouldn’t it be wise to find the best one and
    stick with that? It turns out that not only do we not know of a “best” optimizer,
    but there can’t be a best optimizer for all situations*.* No matter what optimizer
    we put forth as the “best,” we can prove that it’s always possible to find some
    situation in which another optimizer would be better. This result is famously
    known by its colorful name, the *No Free Lunch Theorem* (Wolpert 1996; Wolpert
    and Macready 1997). This guarantees us that no optimizer will always perform better
    than any other.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会有这么多优化器？难道不应该找到最佳的一个并一直使用它吗？事实证明，不仅我们不知道什么是“最佳”优化器，而且也不可能有一个适用于所有情况的最佳优化器*。*
    无论我们提出哪个优化器作为“最佳”，我们都可以证明，总是有可能找到某种情况，其中另一个优化器会表现更好。这个结果以其充满色彩的名字著称，即*无免费午餐定理*（Wolpert
    1996；Wolpert 和 Macready 1997）。这保证了没有任何优化器会永远优于其他优化器。
- en: Note that the No Free Lunch Theorem doesn’t say that all optimizers are equal.
    As we’ve seen in our tests in this chapter, different optimizers do perform differently.
    The theorem only tells us that no one optimizer will *always* beat the others.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，“无免费午餐定理”并不意味着所有优化器是相同的。正如我们在本章的测试中所看到的，不同的优化器确实有不同的表现。该定理只告诉我们，没有一个优化器会*永远*优于其他优化器。
- en: Though no one optimizer is the best choice for all possible training situations,
    we can find the best optimizer for any specific combination of network and data.
    Most deep learning libraries offer routines that carry out an automated search
    that can try out multiple optimizers and run through multiple parameter choices
    for each one. Whether we choose our optimizer and its values by ourselves or as
    the result of a search, we need to keep in mind that the best choices can vary
    from one network and set of data to the next. As soon as we make a big change
    to either, we should consider checking to see if a better optimizer would give
    us more efficient training. As a practical guide, many people start out with Adam,
    using its default parameters.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管没有一个优化器适用于所有可能的训练情况，但我们可以为任何特定的网络和数据组合找到最佳的优化器。大多数深度学习库提供了自动化搜索的例程，可以尝试多个优化器，并为每个优化器运行多个参数选择。无论我们是自己选择优化器及其参数，还是通过搜索的结果得到它们，我们都需要记住，最佳选择可能会因网络和数据集的不同而有所不同。一旦我们对网络或数据集做出重大更改，就应考虑检查是否有更好的优化器能提供更高效的训练。作为一个实际指南，许多人开始时使用
    Adam，并采用其默认参数。
- en: Regularization
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化
- en: No matter what optimizer we choose, our network can suffer from overfitting.
    As we discussed in Chapter 9, overfitting is a natural result of training for
    too long. The problem is that the network learns the training data so well that
    it becomes tuned to just that data and performs poorly on new data once it’s released.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们选择什么优化器，我们的网络都可能会遭遇过拟合。正如我们在第九章中讨论的，过拟合是训练过长时间的自然结果。问题在于，网络学得太好，以至于它只适应训练数据，并且在新数据发布后表现不佳。
- en: Techniques that delay the onset of overfitting are called *regularization* methods.
    They allow us to train for more epochs before overfitting has too great an impact,
    which means our networks have more training time in which to improve their performance.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟过拟合出现的技术称为*正则化*方法。它们允许我们在过拟合对性能影响过大之前进行更多轮次的训练，这意味着我们的网络有更多的训练时间来提高其性能。
- en: Dropout
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dropout
- en: A popular regularization method is called *dropout*. It is usually applied in
    a deep network in the form of a *dropout layer* (Srivastava et al. 2014). The
    dropout layer is called an *accessory layer* or a *supplemental layer*, because
    it doesn’t do any computation of its own. We call it a layer, and draw it as one,
    because it’s convenient conceptually, and lets us include dropout in drawings
    of networks. But we don’t consider it a real layer (hidden or otherwise), and
    we don’t count it when we describe how many layers make up a particular network.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的正则化方法叫做 *dropout*。它通常以 *dropout 层* 的形式应用于深度网络（Srivastava 等人，2014）。dropout
    层被称为 *附加层* 或 *补充层*，因为它本身不进行任何计算。我们称之为层，并将其绘制为层，因为在概念上这样方便，并且让我们可以在网络图中包括 dropout。但我们并不认为它是一个真正的层（无论是隐藏层还是其他层），在描述一个特定网络由多少层组成时，我们也不将其计算在内。
- en: Dropout is a placeholder that tells the network to run an algorithm on the previous
    layer. It’s also only active during training. When the network is deployed, dropout
    layers are disabled or removed.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 是一个占位符，用来告诉网络在前一层运行一个算法。它仅在训练期间启用。当网络部署时，dropout 层会被禁用或移除。
- en: The job of the dropout layer is to temporarily disconnect some of the neurons
    on the previous layer. We give it a parameter that describes the percentageof
    neurons that should be affected, and at the start of each batch, it randomly chooses
    that percentage of neurons on the preceding layer and temporarily disconnects
    their inputs and outputs from the network. Since they’re disconnected, these neurons
    don’t participate in any forward calculations, they’re not included in backprop,
    and the weights coming into them are not updated by the optimizer. When the batch
    is done and the rest of the weights have been updated, the chosen neurons and
    all of their connections are restored.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: dropout 层的作用是暂时断开前一层中一些神经元的连接。我们给它一个参数，描述应该影响的神经元的百分比，在每个批次开始时，它会随机选择前一层中该百分比的神经元，并暂时断开它们的输入和输出连接。由于这些神经元被断开，它们不会参与任何前向计算，也不包含在反向传播中，并且进入它们的权重不会被优化器更新。当批次完成并且其他权重已经更新时，选择的神经元及其所有连接会被恢复。
- en: At the start of the next batch, the layer again chooses a new random set of
    neurons and temporarily removes those, repeating the process for each epoch. [Figure
    15-41](#figure15-41) shows the idea graphically.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个批次开始时，该层再次选择一组新的随机神经元并暂时移除它们，重复这个过程直到每个训练周期结束。[图 15-41](#figure15-41) 以图形方式展示了这个概念。
- en: '![F15041](Images/F15041.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![F15041](Images/F15041.png)'
- en: 'Figure 15-41: Dropout. (a) 50 percent of the four neurons in the middle layer
    (in gray) are chosen to be disconnected before the batch is evaluated. (b) Our
    schematic for a single dropout layer is a diagonal slash. To the right, we indicate
    the proportion of neurons that are selected for disconnection. Since dropout applies
    to its preceding layer, in this example, we apply it to the middle of the three
    fully connected layers.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-41：Dropout。 (a) 在评估批次之前，中间层（灰色）中的 50% 的四个神经元被选中断开连接。 (b) 我们对单个 dropout
    层的示意图是一个对角斜线。右侧表示选择断开连接的神经元比例。由于 dropout 应用在前一层，因此在这个例子中，我们将它应用于三个全连接层中的中间层。
- en: Dropout delays overfitting by preventing any neurons from overspecializing and
    dominating. Suppose that one neuron in a photo classification system gets highly
    specialized to detect the eyes of cats. That’s useful for recognizing picture
    of cats’ faces, but useless for all the other photographs the system might be
    asked to classify. If all the neurons in a network also specialize at finding
    just one or two features in the training data, then they can perform beautifully
    on that data because they spot the idiosyncratic details that they’re trained
    to locate. But the system as a whole will then perform badly when presented with
    new data that’s missing the precise cues those neurons became specialized for.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 通过防止神经元过度专注和主导，延缓了过拟合的发生。假设在一个图像分类系统中，一个神经元高度专门化于检测猫眼。这对于识别猫脸的图片很有用，但对于系统可能需要分类的其他所有照片来说就没什么用。如果网络中的所有神经元都专注于训练数据中仅仅一两种特征，那么它们在该数据上可能表现得非常好，因为它们能够发现它们训练时专门定位的特征。但整体系统在遇到缺少这些特征的新数据时，表现就会很差。
- en: Dropout helps us avoid this kind of specialization. When a neuron is disconnected,
    the remaining neurons must adjust to pick up the slack. Thus, the specialized
    neuron is freed up to perform a more generally useful task, and we’ve delayed
    the onset of overfitting. Dropout helps us put off overfitting by spreading around
    the learning among all the neurons.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 帮助我们避免这种专门化。当一个神经元被断开时，其余的神经元必须进行调整以弥补空缺。因此，被断开的神经元可以自由执行更具普遍性任务，我们也就延迟了过拟合的发生。Dropout
    通过在所有神经元中分散学习，帮助我们推迟过拟合。
- en: Batchnorm
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Batchnorm
- en: Another regularization technique is called *batch normalization*, often referred
    to simply as *batchnorm* (Ioffe and Szegedy 2015). Like dropout, batchnorm can
    be implemented as a layer without neurons. Unlike dropout, batchnorm actually
    does perform some computation, though there are no parameters for us to specify.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种正则化技术叫做*批量归一化*，通常简称为*batchnorm*（Ioffe 和 Szegedy 2015）。与 dropout 类似，batchnorm
    可以作为没有神经元的层来实现。不同于 dropout，batchnorm 实际上会执行一些计算，尽管我们不需要指定任何参数。
- en: Batchnorm modifies the values that come out of a layer. This might seem strange,
    since the whole purpose of training is to get our neurons to produce output values
    that lead to good results. Why would we want to modify those outputs?
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Batchnorm 修改了从某一层输出的值。这个可能看起来有些奇怪，因为训练的主要目的是让神经元产生输出值，从而取得好的结果。我们为什么要修改这些输出呢？
- en: Recall that many of our activation functions, such as leaky ReLU and tanh, have
    their greatest effect near 0\. To get the most benefit from those functions, we
    need the numbers flowing into them to be in a small range centered around 0\.
    That’s what batchnorm does by scaling and shifting all the outputs of a layer
    together. Because batchnorm moves the neuron outputs into a small range near 0,
    we’re less prone to seeing any neuron learning one specific detail and producing
    a huge output that swamps all the other neurons, and thus we are able to delay
    the onset of overfitting. Batchnorm scales and shifts all the values coming out
    of the previous layer over the course of an entire mini-batch in just this way.
    It learns the parameters for this scaling and shifting along with the weights
    in the network so they take on the most useful values.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们的许多激活函数，如 leaky ReLU 和 tanh，在接近 0 时效果最强。为了从这些函数中获得最大的收益，我们需要输入到它们的数字处于一个围绕
    0 的小范围内。这正是 batchnorm 通过对所有层输出进行缩放和平移所做的。因为 batchnorm 将神经元输出移到接近 0 的小范围内，所以我们更不容易看到某个神经元学习到一个特定的细节并产生一个巨大的输出，从而压倒了其他神经元，进而延迟了过拟合的出现。Batchnorm
    会在整个小批量的过程中，按这种方式对从前一层输出的所有值进行缩放和平移。它与网络中的权重一起学习这些缩放和平移的参数，使其取到最有用的值。
- en: We apply batchnorm before the activation function so that the modified values
    will fall in the region of the activation function where they are affected the
    most. In practice, this means we place no activation function on the neurons going
    into batchnorm (or if we must specify a function, it’s the linear activation function,
    which has no effect). Those values go into batchnorm, and then they’re fed into
    the activation function we want to apply.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在激活函数之前应用 batchnorm，以便修改后的值能落在激活函数最受影响的区域。在实践中，这意味着我们不在输入 batchnorm 的神经元上使用激活函数（或者如果必须指定一个函数，那就是线性激活函数，它没有任何效果）。这些值进入
    batchnorm 后，再被送入我们想要应用的激活函数。
- en: The process is illustrated in [Figure 15-42](#figure15-42). Our icon for a regularization
    step like batchnorm is a black disc inside a circle, suggesting that the values
    in the circle are transformed into a smaller region. In later chapters, we’ll
    see other, similar regularization steps for which we’ll use the same icon. The
    text (or a nearby label) identifies which variety of regularization is applied.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程在[图 15-42](#figure15-42)中进行了说明。我们用来表示像 batchnorm 这样的正则化步骤的图标是一个黑色圆盘位于圆形内部，表示圆内的值被转换到一个更小的区域。在后续章节中，我们将看到其他类似的正则化步骤，我们会使用相同的图标。文本（或附近的标签）会标明应用的是哪种正则化方法。
- en: '![F15042](Images/F15042.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![F15042](Images/F15042.png)'
- en: 'Figure 15-42: Applying a batchnorm layer. Top: A neuron followed by a leaky
    ReLU activation function. Bottom: The same neuron with batchnorm. The activation
    function is replaced with the linear function, followed by batchnorm (represented
    by a circle with a black disc inside) and then the leaky ReLU.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-42：应用一个 batchnorm 层。顶部：一个神经元后面接着 leaky ReLU 激活函数。底部：同一个神经元加上 batchnorm。激活函数被替换成线性函数，接着是
    batchnorm（用一个黑色圆盘表示）和 leaky ReLU。
- en: Like dropout, batchnorm defers the onset of overfitting, allowing us to train
    longer.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 像dropout一样，batchnorm推迟了过拟合的发生，使我们能够训练更长时间。
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Optimization is the process of adjusting the weights so that our network learns.
    The core idea begins with the gradient for every weight. We follow that gradient
    to direct us to a lower point on the error surface, hence the name gradient descent.
    The most important value in this process is the learning rate. A common technique
    is to reduce the learning rate over time, according to a decay schedule.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 优化是调整权重的过程，以便让我们的网络进行学习。核心思想从每个权重的梯度开始。我们沿着这个梯度前进，引导我们到达误差表面上的低点，因此称为梯度下降。这个过程中的最重要值是学习率。一种常见的技术是根据衰减计划随着时间的推移减少学习率。
- en: We covered several efficient optimization techniques. We can adjust the weights
    after every epoch (batch gradient descent), after every sample (stochastic gradient
    descent, or SGD), or after mini-batches of samples (mini-batch gradient descent
    or mini-batch SGD). Mini-batch gradient descent is by far the most common technique,
    and the convention in the field is to refer to it simply as SGD. We can improve
    the efficiency of every type of gradient descent by using momentum. We can also
    improve learning by computing a custom, adaptive learning rate for every weight
    over time with an algorithm such as Adam. Lastly, to prevent overfitting, we can
    use a regularization technique such as dropout or batchnorm.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了几种高效的优化技术。我们可以在每个周期后调整权重（批量梯度下降），在每个样本后调整权重（随机梯度下降，或SGD），或者在小批量样本后调整权重（小批量梯度下降或小批量SGD）。目前，小批量梯度下降是最常用的技术，业内的惯例是简单地将其称为SGD。我们可以通过使用动量来提高每种梯度下降的效率。我们还可以通过使用像Adam这样的算法，根据时间为每个权重计算一个自适应学习率，从而改善学习。最后，为了防止过拟合，我们可以使用像dropout或batchnorm这样的正则化技术。
- en: Deep networks that are made up of fully connected layers can do some amazing
    things. But if we create our layers by structuring the neurons in different ways
    and add a little bit of supporting computation, their power increases significantly.
    In the next few chapters, we’ll look at these new layers and how they can be used
    to classify, predict, and even generate images, sounds, and more.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 由全连接层构成的深度网络能够做一些令人惊叹的事情。但如果我们通过不同的方式构建神经元层并添加一些支持计算，它们的能力会显著提升。在接下来的几章中，我们将探讨这些新层以及它们如何用于分类、预测，甚至生成图像、声音等。
