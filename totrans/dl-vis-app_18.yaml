- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optimizers
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: Training neural networks is frequently a time-consuming process. Anything that
    speeds it up is a welcome addition to our toolkit. This chapter is about a family
    of tools that are designed to speed up learning by improving the efficiency of
    gradient descent. The goals are to make gradient descent run faster and avoid
    some of the problems that can cause it to get stuck. These tools also automate
    some of the work of finding the best learning rate, including algorithms that
    can adjust that rate automatically over time. Collectively, these algorithms are
    called *optimizers*. Each optimizer has its strengths and weaknesses, so it’s
    worth becoming familiar with them so we can make good choices when training a
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by drawing some pictures that enable us to visualize error and how
    it changes as we learn. These pictures will help us build some intuition for the
    algorithms yet to come.
  prefs: []
  type: TYPE_NORMAL
- en: Error as a 2D Curve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s often helpful to think of the errors in our systems in terms of geometrical
    ideas. We frequently plot error as a 2D curve.
  prefs: []
  type: TYPE_NORMAL
- en: To get familiar with this 2D error, let’s consider the task of splitting two
    classes of samples represented as dots arranged on a line. Dots at negative values
    are in one class, and dots at zero and above are in the other, as shown in [Figure
    15-1](#figure15-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![F15001](Images/F15001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-1: Two classes of dots on a line. Dots to the left of 0 are in class
    0, shown in blue, and the others are in class 1, shown in beige.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s build a classifier for these samples. In this example, the boundary consists
    of just a single number. All samples to the left of that number are assigned to
    class 0, and all those to the right are assigned to class 1\. If we imagine moving
    this dividing point along the line, we can count up the number of samples that
    are misclassified and call that our error. We can summarize the results as a plot,
    where the X axis shows us each potential splitting point, and the error associated
    with that point is plotted as a dot above it. [Figure 15-2](#figure15-2) shows
    the result.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15002](Images/F15002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-2: Plotting the error function for a simple classifier'
  prefs: []
  type: TYPE_NORMAL
- en: We can smooth out the error curve of [Figure 15-2](#figure15-2) as shown in
    [Figure 15-3](#figure15-3).
  prefs: []
  type: TYPE_NORMAL
- en: '![F15003](Images/F15003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-3: A smoothed version of [Figure 15-2](#figure15-2)'
  prefs: []
  type: TYPE_NORMAL
- en: For this particular set of random data, we see that the error is 0 when we’re
    at 0, or just a little to the left of it. This tells us that regardless of where
    we start, we want to end up with our divider just to the left of 0.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to find a way to locate the smallest value of any error curve. When
    we can do that, we can apply the technique to all the weights of a neural network
    and thus reduce the whole network’s error.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting the Learning Rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we teach a system using gradient descent, the critical parameter is the
    learning rate, usually written with the lowercase Greek letter *η* (eta). This
    is often a value in the range 0.01 to 0.0001\. Larger values lead to faster learning,
    but they can lead us to miss valleys by jumping right over them. Smaller values
    of *η* (nearing 0, but always positive) lead to slower learning and can find narrow
    valleys, but they can also get stuck in gentle valleys even when there are much
    deeper ones nearby. [Figure 15-4](#figure15-4) recaps these phenomena graphically.
  prefs: []
  type: TYPE_NORMAL
- en: An important idea shared by many optimizers is that we can improve learning
    by changing the learning rate as we go. The general thinking is analogous to hunting
    for buried metal on a beach using a metal detector. We start by taking big steps
    as we walk across the beach, but when the detector goes off, we take smaller and
    smaller steps to pinpoint the metal object’s location. In the same way, we usually
    take big steps along the error curve early in the learning process while we’re
    hunting for a valley. As time goes on, we hope that we found that valley, and
    we can now take smaller and smaller steps as we approach its lowest point.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15004](Images/F15004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-4: The influence of the learning rate, *η*. (a) When *η* is too large,
    we can jump right over a deep valley and miss it. (b) When *η* is too small, we
    can slowly descend into a local minimum, and miss the deeper valley.'
  prefs: []
  type: TYPE_NORMAL
- en: We can illustrate our optimizers with a simple error curve containing a single
    isolated valley with the shape of a negative Gaussian, shown in [Figure 15-5](#figure15-5).
  prefs: []
  type: TYPE_NORMAL
- en: '![F15005](Images/F15005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-5: Our error curve for looking at optimizers'
  prefs: []
  type: TYPE_NORMAL
- en: Some gradients for this error curve are shown in [Figure 15-6](#figure15-6)
    (we’re actually showing the negative gradients).
  prefs: []
  type: TYPE_NORMAL
- en: '![F15006](Images/F15006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-6: Our error curve and its negative gradients (scaled down by a factor
    of 0.25) at some locations'
  prefs: []
  type: TYPE_NORMAL
- en: The gradients in [Figure 15-6](#figure15-6) have been scaled down to 25 percent
    of their actual length for clarity. We can see that for this curve, the gradient
    is negative for input values that are less than 0 and positive for input values
    that are greater than 0\. When the input is 0, we’re at the very bottom of the
    bowl, so the gradient there is 0, drawn as just a single dot.
  prefs: []
  type: TYPE_NORMAL
- en: Constant-Sized Updates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s start our investigation of the effect of the learning rate by seeing what
    happens when we use a constant learning rate. In other words, we always scale
    the gradient by a value of *η* that stays fixed, or constant, during the whole
    training process.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-7](#figure15-7) shows the basic steps of updating with a fixed *η*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F15007](Images/F15007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-7: Finding the step for basic gradient descent'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we’re looking at a particular weight in a neural network. Let’s pretend
    that the weight begins with a value of w1, and we updated it once, so it now has
    the value w2, shown in [Figure 15-7](#figure15-7)(a). Its corresponding error
    is the point on the error curve directly above it, marked B. We want to update
    the weight again to a new and better value that we’ll call w3.
  prefs: []
  type: TYPE_NORMAL
- en: To update the weight, we find its gradient on the error surface at the point
    B, shown as the arrow labeled *g*. We scale the gradient by the learning rate
    *η* to get a new arrow labeled *ηg*. Because *η* is between 0 and 1, *ηg* is a
    new arrow that points in the same direction as *g* but is either the same size
    as *g* or smaller.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 15-7](#figure15-7), the arrow we show for the gradient *g* is actually
    the *opposite*, or negative, of the gradient. The positive and negative gradients
    point in opposite directions along the same line, so people tend to refer to simply
    *the gradient* when the choice of positive or negative can be understood from
    context. We’ll follow that convention in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: To find w3, the new value of the weight, we add the scaled gradient to w2\.
    In pictures, this means we place the tail of the arrow *ηg* at B, as in [Figure
    15-7](#figure15-7)(b). The horizontal position of the tip of that arrow is the
    new value of the weight, w3, and its value, directly above it on the error surface,
    is marked C. In this case, we stepped a bit too far and increased our error by
    a little.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at this technique in practice using an error curve with a single
    valley. [Figure 15-8](#figure15-8) shows a starting point in the upper left. The
    gradient here is small, so we move to the right a small amount. The error at that
    new point is a little less than the error we started with.
  prefs: []
  type: TYPE_NORMAL
- en: For these figures, we’ve chosen *η* = 1/8, or 0.125\. This is an unusually large
    value of *η* for constant-sized gradient descent, where we often use a value of
    1/100 or less. We chose this large value because it makes for clearer pictures.
    Smaller values work in similar ways, just more slowly. We aren’t showing values
    on the axes for these graphs to avoid visual clutter, since we’re more interested
    in the nature of what happens rather than the numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15008](Images/F15008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-8: Learning with a constant learning rate'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than move from our first point by the entire gradient, we’re moving only
    1/8 of its length. This move takes us to a steeper part of the curve, where the
    gradient is larger, so the next update moves a little farther. Each step of learning
    is shown with a new color, which we use to draw the gradient from the previous
    location and then the new point.
  prefs: []
  type: TYPE_NORMAL
- en: We show a close-up of six steps in [Figure 15-9](#figure15-9), starting after
    the first step in [Figure 15-8](#figure15-8). We also show the error for each
    point.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15009](Images/F15009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-9: Left: A close-up of the final image in [Figure 15-8](#figure15-8).
    Right: The error associated with each of these six points.'
  prefs: []
  type: TYPE_NORMAL
- en: Will this process ever reach the bottom of the bowl and get down to 0 error?
    [Figure 15-10](#figure15-10) shows the first 15 steps in this process.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15010](Images/F15010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-10: Left: The first 15 steps of learning with a constant learning
    rate. Right: The errors of these 15 points.'
  prefs: []
  type: TYPE_NORMAL
- en: We get near the bottom and then head up the hill on the right side. But that’s
    okay because the gradient here points down and to the left, so we head back down
    the valley until we overshoot the bottom again, and end up somewhere on the left
    side, then we turn around and overshoot again and end up on the right side, and
    so on, back and forth. We’re *bouncing around* the bottom of the bowl.
  prefs: []
  type: TYPE_NORMAL
- en: It doesn’t look like we’ll ever get to 0\. The problem is particularly bad in
    this symmetrical valley, as the error jumps back and forth between the left and
    right sides of the minimum. But this type of behavior happens a lot when we use
    a constant learning rate. The bouncing around is happening because when we’re
    near the bottom of a valley, we want to take small steps, but because our learning
    rate is a constant, we’re taking steps that are too big.
  prefs: []
  type: TYPE_NORMAL
- en: We might wonder if the bouncing problem of [Figure 15-10](#figure15-10) was
    caused by too large a learning rate. [Figure 15-11](#figure15-11) shows how things
    go for the first 15 steps of some smaller values of *η.*
  prefs: []
  type: TYPE_NORMAL
- en: '![F15011](Images/F15011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-11: Taking 15 steps with our small learning rates. Top row: Learning
    rates of 0.025 (left column), 0.05 (middle column), and 0.1 (right column). Bottom
    row: Errors for the points in the top row.'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from [Figure 15-11](#figure15-11), taking smaller steps doesn’t
    solve the bouncing problem, though the bounces are smaller. On the other hand,
    increasing the learning rate makes the bouncing problem worse, as shown in [Figure
    15-12](#figure15-12).
  prefs: []
  type: TYPE_NORMAL
- en: '![f15012](Images/f15012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-12: Top row: Learning rates of 0.5 (left column), 0.75 (middle column),
    and 1.0 (right column). Bottom row: Errors for the points in the top row.'
  prefs: []
  type: TYPE_NORMAL
- en: Larger learning rates can also cause us to jump out of a nice valley with a
    low minimum. In [Figure 15-13](#figure15-13), starting at the green dot, we jump
    right over the rest of the valley we’re in (and would like to stay in) and into
    a new valley with a much larger minimum.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15013](Images/F15013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-13: A large step overshoots the valley and ends up in a different
    valley with a higher minimum.'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes a big jump like this can help us move from a shallow valley to a deeper
    one, but for such a large learning rate, we’ll probably jump around valleys a
    lot, never finding a minimum. It seems like a challenge to find just one learning
    rate that moves at a reasonable speed but won’t overshoot valleys or get trapped
    bouncing around in the bottom. A nice alternative is to change the learning rate
    as we go.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the Learning Rate over Time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can use a large value of *η* near the start of our learning so we don’t crawl
    along, and a small value near the end so we don’t end up bouncing around the bottom
    of a bowl.
  prefs: []
  type: TYPE_NORMAL
- en: An easy way to start big and gradually get smaller is to multiply the learning
    rate by some number that’s almost 1 after every update step. Let’s use 0.99 as
    a multiplier and suppose that the starting learning rate is 0.1\. Then after the
    first step, it will be 0.1 × 0.99 = 0.099\. On the next step, it would be 0.099
    × 0.99 = 0.09801\. [Figure 15-14](#figure15-14) shows what happens to *η* when
    we do this for many steps using a few different values for the multiplier.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to write the equation of these curves involves using exponents,
    so this kind of curve is called an *exponential decay* curve. The value by which
    we multiply *η* on every step is called the *decay parameter*. This is usually
    a number very close to 1\.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15014](Images/F15014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-14: Starting with a learning rate of *η* = 1, the various curves
    show how the learning rate drops after multiplying it by a given value after each
    update.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s apply this gradual reduction of the learning rate to gradient descent
    on our error curve. Once again, we start with a learning rate of 1/8\. To make
    the effect of the decay parameter easily visible, let’s set it to the unusually
    low value of 0.8\. This means each step will only be 80 percent as long as the
    step before it. [Figure 15-15](#figure15-15) shows the result for the first 15
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15015](Images/F15015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-15: The first 15 steps using a shrinking learning rate'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s compare this with our “bouncing” result from using a constant step size.
    [Figure 15-16](#figure15-16) shows the results for the constant and shrinking
    step sizes together for 15 steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15016](Images/F15016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-16: On the left is the constant step size from [Figure 15-10](#figure15-10),
    and on the right is the decaying step size from [Figure 15-15](#figure15-15).
    Notice how the shrinking learning rate helps us efficiently settle into the minimum
    of the valley.'
  prefs: []
  type: TYPE_NORMAL
- en: The shrinking step size does a beautiful job of landing us in the bottom of
    the bowl and keeping us there.
  prefs: []
  type: TYPE_NORMAL
- en: Decay Schedules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The decay technique is attractive, but it comes with some new challenges. First,
    we have to choose a value for the decay parameter. Second, we might not want to
    apply the decay after every update. To address these issues, we can try some other
    strategies for reducing the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: Any given approach to changing the learning rate over time is called a *decay
    schedule* (Bengio 2012).
  prefs: []
  type: TYPE_NORMAL
- en: Decay schedules are usually expressed in epochs, rather than samples. We train
    on all the samples in our training set, and only then consider changing the learning
    rate before we train on all the samples again.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest decay schedule is to always apply decay to the learning rate after
    every epoch, as we just saw. [Figure 15-17](#figure15-17)(a) shows this schedule.
  prefs: []
  type: TYPE_NORMAL
- en: Another common scheduling method is to put off any decay at all for a while
    so our weights have a chance to get away from their starting random values and
    into something that might be close to finding a minimum. Then we apply whatever
    schedule we’ve picked. [Figure 15-17](#figure15-17)(b) shows this *delayed exponential
    decay* approach, putting off the exponential decay schedule of [Figure 15-17](#figure15-17)(a)
    for a few epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to apply the decay only every once in a while. The *interval
    decay* approach shown in [Figure 15-17](#figure15-17)(c), also called *fixed-step
    decay*, reduces the learning rate after every fixed number of epochs, say every
    4th or 10th. This way we don’t risk getting too small too fast.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15017](Images/F15017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-17: Decay schedules for reducing the size of the learning rate over
    time. (a) Exponential decay, where the learning rate is reduced after every epoch.
    (b) Delayed exponential decay. (c) Interval decay, where the learning rate is
    reduced after every fixed number of epochs (here, 4). (d) Error-based decay, where
    the learning rate is reduced when the error stops dropping.'
  prefs: []
  type: TYPE_NORMAL
- en: Yet another option is to monitor the error of our network. As long as the error
    is going down, we stick with whatever learning rate we have now. When the network
    stops learning, we apply the decay so it can take smaller steps and hopefully
    work its way into a deeper part of the error landscape. This *error-based decay*
    is shown in [Figure 15-17](#figure15-17)(d).
  prefs: []
  type: TYPE_NORMAL
- en: We can easily cook up a lot of alternatives, such as applying decay only when
    the error decreases by a certain amount or certain percentage, or perhaps updating
    the learning rate by just subtracting a small value from it rather than multiplying
    it by a number close to 1 (as long as we stop at some positive value—if the learning
    rate went to 0, the system would stop learning, and if the learning rate went
    negative, the system would increase the error, rather than decrease it).
  prefs: []
  type: TYPE_NORMAL
- en: We can even increase the learning rate over time if we want. The *bold driver*
    method looks at how the total loss is changing after each epoch (Orr 1999a; Orr
    1999b). If the error is going down, then we *increase* the learning rate a little,
    say 1 percent to 5 percent. The thinking is that if things are going well, and
    the error is dropping, we can take big steps. But if the error has gone up by
    more than just a little, then we slash the learning rate, cutting it by half.
    This way we can stop any increases immediately, before they can carry us too far
    away from the decreasing error we were previously enjoying.
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate schedules have the drawback that we have to pick their parameters
    in advance (Darken, Chang, and Moody 1992). We think of these parameters as *hyperparameters*,
    just like the learning rate itself. Most deep learning libraries offer routines
    that automatically search ranges of values for us to help us find the best values
    of one or more hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, simple strategies for adjusting the learning rate usually
    work well, and most machine-learning libraries let us pick one of them with little
    fuss (Karpathy 2016).
  prefs: []
  type: TYPE_NORMAL
- en: Some kind of learning rate reduction is a common feature in most machine learning
    systems. We want to learn quickly in the early stages, moving in big steps over
    the landscape, looking for the lowest minimum we can find. Then we reduce the
    learning rate, enabling us to gradually take smaller steps and land in the very
    lowest part of whatever valley we’ve found.
  prefs: []
  type: TYPE_NORMAL
- en: It’s natural to wonder if there’s a way to control the learning rate that doesn’t
    depend on a schedule that we set up before we start training. Surely, we can somehow
    detect when we’re near a minimum, or in a bowl, or bouncing around, and automatically
    adjust the learning rate in response.
  prefs: []
  type: TYPE_NORMAL
- en: An even more interesting question is to consider that maybe we don’t want to
    apply the same learning rate adjustments to all of our weights. It would be nice
    to be able to tune our updates so that each weight is learning at a rate that
    works best for it.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some variations on gradient descent that address those ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Updating Strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following sections, we compare the performance of three different ways
    to enhance gradient descent. In these examples, we use a small, but real, two-class
    classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-18](#figure15-18) shows our familiar dataset of two fuzzy crescent
    moons. The classes for these points are shown by color. These 300 samples are
    our reference data for the rest of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to compare different networks, we need to train them until the error
    has reached a minimum, or seems to have stopped improving. We can show the results
    of our training with plots that graph the error after each epoch. Because of the
    wide variation in algorithms, the number of epochs in these graphs vary over a
    large range.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15018](Images/F15018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-18: The data we use for the rest of this chapter. The 300 points
    form two classes of 150 points each.'
  prefs: []
  type: TYPE_NORMAL
- en: To classify our points, we’ll use a neural network with three fully connected
    hidden layers (of 12, 13, and 13 nodes), and an output layer of 2 nodes, giving
    us the probability for each of the two classes. We’ll use ReLU on each hidden
    layer, and softmax at the end. Whichever class has the larger probability at the
    output is taken as our network’s prediction. For consistency, when we need a constant
    learning rate, we use a value of *η*= 0.01\. The network is shown in [Figure 15-19](#figure15-19).
  prefs: []
  type: TYPE_NORMAL
- en: '![F15019](Images/F15019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-19: Our network of four fully connected layers'
  prefs: []
  type: TYPE_NORMAL
- en: Batch Gradient Descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s begin by updating the weights just once per epoch, after we’ve evaluated
    all the samples. This is *batch gradient descent* (also called *epoch gradient
    descent*). In this approach, we run the entire training set through our system,
    accumulating the errors. Then we update all of the weights once using the combined
    information from all the samples.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-20](#figure15-20) shows the error from a typical training run using
    batch gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F15020](Images/F15020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-20: The error for a training run using batch gradient descent'
  prefs: []
  type: TYPE_NORMAL
- en: The broad features are reassuring. The error drops quite a bit at the beginning,
    suggesting that the network is starting on a steep section of the error surface.
    Then the curve becomes much shallower. The error surface here might be a nearly
    flat region of a shallow saddle or a region that’s nearly a plateau but has just
    a bit of slope to it, because the error does continue to drop slowly. Eventually
    the algorithm finds another steep region and follows it all the way down to 0.
  prefs: []
  type: TYPE_NORMAL
- en: Batch gradient descent looks very smooth, but to get down to near 0 error for
    this network and data requires about 20,000 epochs, which can take a long time.
    Let’s get a closer look at what happens from one epoch to the next by zooming
    in on the first 400 epochs, shown in [Figure 15-21](#figure15-21).
  prefs: []
  type: TYPE_NORMAL
- en: It seems that batch gradient descent really is moving smoothly. That makes sense,
    because it’s using the error from all the samples on each update.
  prefs: []
  type: TYPE_NORMAL
- en: Batch gradient descent usually produces a smooth error curve, but it has some
    issues in practice. If we have more samples than can fit in our computer’s memory,
    then the costs of *paging*, or retrieving data from slower storage media, can
    become substantial enough to make training impractically slow. This can be a problem
    in some real situations when we work with enormous datasets of millions of samples.
    It can take a great deal of time to read samples from slower memory (or even a
    hard drive) over and over. There are solutions to this problem, but they can involve
    a lot of work.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15021](Images/F15021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-21: A close-up of the first 400 epochs of batch gradient descent
    shown in [Figure 15-20](#figure15-20)'
  prefs: []
  type: TYPE_NORMAL
- en: Closely related to this memory issue is that we must keep all the samples around
    and available so that we can run through them over and over, once per epoch. We
    sometimes say that batch gradient descent is an *offline algorithm*, meaning that
    it works strictly from information that it has stored and has access to. We can
    imagine disconnecting the computer from all networks, and it could continue to
    learn from all of our training data.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s go to the other extreme and update our weights after every sample. This
    is called *stochastic gradient descent*, or, more commonly, just *SGD*. Recall
    that the word *stochastic* is roughly a synonym for *random*. This word is used
    because we present the network with the training samples in a random order, so
    we can’t predict how the weights are going to change from one sample to the next.
  prefs: []
  type: TYPE_NORMAL
- en: Since we update after every sample, our dataset of 300 samples requires us to
    update the weights 300 times over the course of each epoch. This is going to cause
    the error to jump around a lot as each sample pulls the weights one way and then
    another. Since we’re only plotting the error on an epoch-by-epoch basis, we don’t
    see this small-scale wiggling. But we still see a lot of variation epoch by epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-22](#figure15-22) shows the error of our network learning from this
    data using SGD.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F15022](Images/F15022.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-22: Stochastic gradient descent, or SGD'
  prefs: []
  type: TYPE_NORMAL
- en: The graph has the same general shape as the one for batch gradient descent in
    [Figure 15-20](#figure15-20), which makes sense since both training runs use the
    same network and data.
  prefs: []
  type: TYPE_NORMAL
- en: The huge spike at around epoch 225 shows just how unpredictable SGD can be.
    Something in the sequencing of the samples and the way the network’s weights were
    updated caused the error to soar from nearly 0 to nearly 1\. In other words, it
    went from finding the right class for almost every sample to being dead wrong
    on almost every sample, and then back to being right again (though this recovery
    took a few epochs, as shown by the small curve to the right of the spike). If
    we were watching the errors as learning progresses, we might be inclined to stop
    the training session at the spike. If we use an automatic algorithm to watch the
    error, it may also stop it there. Yet just a few epochs after that spike, the
    system has recovered and we’re back to nearly 0\. The algorithm has definitely
    earned the word *stochastic* in its name.
  prefs: []
  type: TYPE_NORMAL
- en: We can see from the plot that SGD got down to about 0 error in just 400 epochs.
    We cut off [Figure 15-22](#figure15-22) after that, since the curve stayed at
    0 from then on. Compare this to the roughly 20,000 epochs required by batch gradient
    descent in [Figure 15-20](#figure15-20). This increase in efficiency over batch
    gradient descent is typical (Ruder 2017).
  prefs: []
  type: TYPE_NORMAL
- en: But let’s compare apples to apples. How many times did each algorithm update
    the weights? Batch gradient descent updates the weights after each batch, so the
    20,000 epochs means it did 20,000 updates. SGD does an update after every one
    of our 300 samples. So in 400 epochs it performed 300 × 400 = 120,000 updates,
    six times more than batch gradient descent. The moral is that the amount of time
    we actually spend waiting for results isn’t completely predicted by the number
    of epochs, since the time per epoch can vary considerably.
  prefs: []
  type: TYPE_NORMAL
- en: We call SGD an *online algorithm*, because it doesn’t require the samples to
    be stored or even to be consistent from one epoch to the next. It just handles
    each sample as it arrives and updates the network immediately.
  prefs: []
  type: TYPE_NORMAL
- en: SGD produces noisy results, as we can see in [Figure 15-22](#figure15-22). This
    is both good and bad. The upside of this is that SGD can jump from one region
    of the error surface to another as it searches for minima. But the downside is
    that SGD can overshoot a deep minimum and spend its time searching around inside
    of some valley with a larger error. Reducing the learning rate over time definitely
    helps with the jumping problem, but the progress is still typically noisy.
  prefs: []
  type: TYPE_NORMAL
- en: Noise in the error curve can be a problem because it makes it hard for us to
    know when the system is learning and when it starts overfitting. We can look at
    a sliding window of many epochs, but we may only know that we’ve overshot the
    minimum error long after it happened.
  prefs: []
  type: TYPE_NORMAL
- en: Mini-Batch Gradient Descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can find a nice middle ground between the extremes of batch gradient descent,
    which updates once per epoch, and stochastic gradient descent, which updates after
    every sample. This compromise is called *mini-batch gradient descent*, or sometimes
    *mini-batch SGD*. Here, we update the weights after some fixed number of samples
    has been evaluated. This number is almost always considerably smaller than the
    batch size (the number of samples in the training set). We call this smaller number
    the *mini-batch size*, and a set of that many samples drawn from the training
    set is a *mini-batch*.
  prefs: []
  type: TYPE_NORMAL
- en: The mini-batch size is frequently a power of 2 between about 32 and 256, and
    often it is chosen to fully use the parallel capabilities of our GPU, if we have
    one. But that’s just for efficiency purposes. We can use any size of mini-batch
    that we like.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-23](#figure15-23) shows the results of using a mini-batch of 32
    samples.'
  prefs: []
  type: TYPE_NORMAL
- en: This is indeed a nice blend of the two algorithms. The curve is smooth, like
    batch gradient descent, but not perfectly so. It drops down to 0 in about 5,000
    epochs, between the 400 needed by SGD and the 20,000 of batch gradient descent.
    [Figure 15-24](#figure15-24) shows a close-up of the first 400 steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15023](Images/F15023.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-23: Mini-batch gradient descent'
  prefs: []
  type: TYPE_NORMAL
- en: '![F15024](Images/F15024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-24: A close-up of the first 400 epochs of [Figure 15-23](#figure15-23),
    showing the deep plunge at the very beginning of training'
  prefs: []
  type: TYPE_NORMAL
- en: How many updates did mini-batch SGD perform? We have 300 samples, and we used
    a mini-batch size of 32, so there are 10 mini-batches per epoch. (Ideally, we’d
    like the mini-batches to precisely divide the size of the input, but in practice,
    we can’t control the size of our datasets. This often leaves us with a partial
    mini-batch at the end.) So 10 updates per epoch, times 5,000 epochs, gives us
    50,000 updates. This is also nicely between the 20,000 updates of batch gradient
    descent and the 120,000 updates of SGD.
  prefs: []
  type: TYPE_NORMAL
- en: Mini-batch gradient descent is less noisy than SGD, which makes it attractive
    for tracking the error. The algorithm can take advantage of huge efficiency gains
    by using the GPU for calculations, evaluating all the samples in a mini-batch
    in parallel. It’s faster than batch gradient descent, and more attractive in practice
    than SGD.
  prefs: []
  type: TYPE_NORMAL
- en: For all these reasons, mini-batch SGD is popular in practice, with “plain” SGD
    and batch gradient descent being used relatively infrequently. In fact, most of
    the time when the term *SGD* is used in the literature, or even just *gradient
    descent*, it’s understood that the authors mean mini-batch SGD (Ruder 2017). To
    make things a little more confusing, the term *batch* is often used instead of
    *mini-batch*. Because epoch-based gradient descent is used so rarely these days,
    references to batch gradient descent and batches almost always refer to mini-batch
    gradient descent and mini-batches.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent Variations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mini-batch gradient descent is an important algorithm, but it’s not perfect.
    Let’s review some of the challenges of mini-batch gradient descent, and a few
    ways to address them. Following convention, from here on we refer to mini-batch
    gradient descent as SGD. (The organization of this section is inspired by Ruder
    2017.)
  prefs: []
  type: TYPE_NORMAL
- en: Our first challenge is to specify what value of the learning rate *η* we want
    to use, which is notoriously hard to pick ahead of time. As we’ve seen, a value
    that’s too small can result in long learning times and getting stuck in shallow
    local minima, but a value that’s too big can cause us to overshoot deep local
    minima and then get stuck bouncing around inside a minimum when we do find it.
    If we try to avoid the problem by using a decay schedule to change *η* over time,
    we still have to pick the starting value of *η* and then the schedule’s hyperparameters
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: We also have to pick the size of the mini-batch. This is rarely an issue, because
    we usually choose whatever value produces calculations that are most closely matched
    to the structure of our GPU or other hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider some improvements. Right now, we’re updating all the weights
    with a one-update-rate-fits-all approach. Instead, we can find a unique learning
    rate for each weight in the system so we’re not just moving it in the best direction,
    but we’re moving it by the best amount. We’ll see examples of this in the following
    pages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another improvement begins with the recognition that sometimes when the error
    surface forms a saddle, the surface can be shallow in all directions, so locally,
    it’s almost (but not quite) a plateau. This can slow our progress to an excruciating
    crawl. Research has shown that deep learning systems often have plenty of saddles
    in their error landscapes (Dauphin et al. 2014). It would be nice if there was
    a way to get unstuck in these situations, or better yet, to avoid getting stuck
    in them in the first place. The same thing goes for plateaus: we’d like to avoid
    getting stuck in the flat regions where the gradient drops to 0\. To do so, we
    want to avoid the regions where the gradient drops to 0, except of course for
    the minima we’re seeking.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some variations of gradient descent that address these issues.
  prefs: []
  type: TYPE_NORMAL
- en: Momentum
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s consider two weights at the same time. We can plot their values on an
    XY plane, and above them show the error that results from training the system
    with those values for those weights. Let’s think of our error surface as a landscape.
    Now we can picture our task of minimizing error as following a drop of water that’s
    looking for the lowest point.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-25](#figure15-25) repeats a figure from Chapter 5 that shows an
    example of this way of thinking about the training process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F15025](Images/F15025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-25: A drop of water rolling down an error surface. This is a repeat
    of a figure from Chapter 5.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of water, let’s think of this as a little ball rolling down the error
    surface. We know from the physical world that a real ball rolling down a hill
    in this way has some *inertia*, which describes its resistance to a change in
    its motion. If it’s rolling along in a given direction at a certain speed, it
    will continue to move that way unless something interferes with it.
  prefs: []
  type: TYPE_NORMAL
- en: A related idea is the ball’s *momentum*, which is a bit more abstract from a
    physical point of view. Although they’re distinct ideas, sometimes deep learning
    discussions casually refer to inertia as momentum, and the algorithm we’re about
    to look at uses that language.
  prefs: []
  type: TYPE_NORMAL
- en: This idea is what keeps the ball in [Figure 15-25](#figure15-25) moving across
    the plateau after it has come down from the peak and passed into the saddle near
    the middle of the figure. If the ball’s motion was determined strictly by the
    gradient, when it hit the plateau near the middle of the figure, it would stop
    (or if it was a near-plateau, the ball would slow to a crawl). But the ball’s
    momentum (or more properly, its inertia) keeps it rolling onward.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we’re near the left side of [Figure 15-26](#figure15-26). As we roll
    down the hill, we reach the plateau starting at around −0.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15026](Images/F15026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-26: An error curve with a plateau between a hill and valley'
  prefs: []
  type: TYPE_NORMAL
- en: With regular gradient descent, we stop on the plateau since the gradient is
    zero, as shown in the left of [Figure 15-27](#figure15-27). But if we include
    some momentum, as shown on the right, the ball keeps going for a while. It does
    slow down, but if we’re lucky, it continues to roll far enough to find the next
    valley.
  prefs: []
  type: TYPE_NORMAL
- en: The technique of *momentum gradient descent* (Qian 1999) is based on this idea.
    For each step, once we calculate how much we want each weight to change, we add
    in a small amount of its change from the previousstep. If the change on a given
    step is 0, or nearly 0, but we had some larger change on the last step, we use
    some of that prior motion now, which pushes us along over the plateau.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15027](Images/F15027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-27: Gradient descent on the error curve of [Figure 15-26](#figure15-26).
    Left: Gradient descent with decay. Right: Gradient descent with decay and momentum.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-28](#figure15-28) shows the idea visually.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F15028](Images/F15028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-28: Finding the step for gradient descent with momentum'
  prefs: []
  type: TYPE_NORMAL
- en: We suppose that some weight had an error A. We updated that weight to value
    w2, with an error B. We now want to find the next value for the weight, w3, which
    will have error C. To find C, we find the change that we applied to point A. That
    is, we find the previous motion applied to A. This is the momentum, labeled *m*,
    shown in [Figure 15-28](#figure15-28)(a).
  prefs: []
  type: TYPE_NORMAL
- en: We multiply the momentum, *m*, by a scaling factor usually referred to with
    the lowercase Greek letter *γ* (gamma). Sometimes this is called the *momentum
    scaling factor*, and it’s a value from 0 to 1\. Multiplying *m* by this value
    gives us a new arrow *γm* that points in the same direction as *m* but is the
    same length or shorter. We then find the scaled gradient, *ηg*, at B, as we did
    before, shown in [Figure 15-28](#figure15-28)(b). Now we have all we need. We
    add together the scaled momentum, *γm*, and the scaled gradient, *ηg*, to B, which
    we do graphically by placing the tail of *γm* at the head of *ηg*, as in [Figure
    15-28](#figure15-28)(c).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s apply this rule and see how the weight and error change over time. [Figure
    15-29](#figure15-29) shows our symmetrical valley from before, and sequential
    steps of training. In this figure, we use both an exponential decay schedule and
    momentum. This is just like our sequence from [Figure 15-15](#figure15-15), but
    now the change applied to each step also includes momentum, or a scaled version
    of the change from the previous step. We can see this by looking at the two lines
    that emerge from each point (one for the gradient, the other for the momentum).
    This total then becomes the new change.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15029](Images/F15029.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-29: Learning with both an exponential decay schedule and momentum'
  prefs: []
  type: TYPE_NORMAL
- en: On each step, we first find the gradient and multiply it by the current value
    of the learning rate *η*, as before. Then we find the previous change, scale it
    by *γ*, and add both of those changes to the current position of the weight. That
    combination gives us the change in this step.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-30](#figure15-30) shows a close-up of the sixth step in the grid,
    along with the error at each point along the way.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An interesting thing happened here: when the ball reached the right side of
    the valley, it continued to roll up, even though the gradients pointed down. That’s
    just what we’d expect of a real ball. We can see it slowing down, and then eventually
    it comes back down the slope, overshooting the bottom, but by less than before,
    then slowing and coming back down again, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F15030](Images/F15030.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-30: The final step in [Figure 15-29](#figure15-29), along with the
    error for each point'
  prefs: []
  type: TYPE_NORMAL
- en: If we use too much momentum, our ball can fly right up the other side and out
    of the bowl altogether, but if we use too little momentum, our ball may not get
    across the plateaus it encounters along the way. [Figure 15-31](#figure15-31)
    shows our error curve from [Figure 15-26](#figure15-26). Here we used trial and
    error to find a value of *γ* to scale the momentum so that our ball gets through
    the plateau but can still settle into the minimum at the bottom of the bowl.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15031](Images/F15031.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-31: Using enough momentum to cross a plateau, but not so much that
    the ball is unable to settle nicely into the bottom of the minimum'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the right amount of momentum to use is another task where we need to
    use our experience and intuition, along with trial and error, to help us understand
    the behavior of our specific network and the data we’re working with. We can also
    search for it using hyperparameter searching algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: To put this all together, we find the gradient, scale it by the current learning
    rate *η*, add in the previous change scaled by *γ*, and that gives us our new
    position. If we set *γ* to 0, then we add in none of the last step, and we have
    “normal” (or “vanilla”) gradient descent. If *γ* is set to 1, then we add in the
    entirety of the last change. Often we use a value of around 0.9\. In Figures 15-29
    and [Figure 15-31](#figure15-31), we set gamma to 0.7 to better illustrate the
    process.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-32](#figure15-32) shows the result of 15 steps of learning with
    both learning rate decay and momentum. The ball starts on the left, rolls down
    and then far up the right side, then it rolls down again and rolls up the left
    side, and so on, climbing a little less each time.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F15032](Images/F15032.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-32: Learning with momentum and a decaying learning rate for 15 points'
  prefs: []
  type: TYPE_NORMAL
- en: Momentum helps us get over flat plateaus and out of shallow places in saddles.
    It has the additional benefit of helping us zip down steep slopes, so even with
    a small learning rate, we can pick up some efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-33](#figure15-33) shows the error for a training run on our dataset
    of [Figure 15-18](#figure15-18) consisting of two crescent moons.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F15033](Images/F15033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-33: Error curve for training with our two-crescent data using mini-batch
    gradient descent with momentum. We got to zero error in a little more than 600
    epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: Here we’re using mini-batch gradient descent (or SGD) with momentum. It’s noisier
    than the mini-batch curve of [Figure 15-23](#figure15-23) because the momentum
    sometimes carries us past where we want to be, causing a spike in the error. The
    error when using mini-batch SGD alone in [Figure 15-23](#figure15-23) for our
    data took about 5,000 epochs to reach about 0 error. With momentum, we get there
    in a little over 600 epochs. Not bad!
  prefs: []
  type: TYPE_NORMAL
- en: 'Momentum clearly helps us learn more quickly, which is a great thing. But momentum
    brings us a new problem: choosing the momentum value *γ*. As we mentioned, we
    can pick this value using experience and intuition or use a hyperparametersearch
    for the value that gives us the best results.'
  prefs: []
  type: TYPE_NORMAL
- en: Nesterov Momentum
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Momentum let us reach into the past for information to help us train. Now let’s
    reach into the future. The key idea is that instead of using only the gradient
    at the location where we currently are, we also use the gradient at the location
    where we expect that we’re *going to be*. Then we can use some of that “gradient
    from the future” to help us now.
  prefs: []
  type: TYPE_NORMAL
- en: Because we can’t really predict the future, we estimatewhere we’re going to
    be on the next step and use the gradient there. The thinking is that if the error
    surface is relatively smooth, and our estimate is pretty good, then the gradient
    we find at our estimated next position is close to the gradient where we’d actually
    end up if we just moved using standard gradient descent, with or without momentum.
  prefs: []
  type: TYPE_NORMAL
- en: Why is it useful to use the gradient from the future? Suppose we’re rolling
    down one side of a valley and approaching the bottom. On the next step, we overshoot
    the bottom and end up somewhere on the other wall. As we saw before, momentum
    carries us up that wall for a few steps, slowing as we lose momentum, until we
    turn around and come back down. But if we can predictthat we’ll be on the far
    side, we can include some of the gradient at that point in our calculations now.
    So instead of moving so far to the right and up the hill, that leftward push from
    the future causes us to move by a little less distance, so we don’t overshoot
    so far and end up closer to the bottom of the valley.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, if the next move we’re going to make is in the same direction
    as the last one, we take a larger step now. If the next move is going to move
    us backward, we take a smaller step.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s break this down into steps so we don’t get mixed up between estimates
    and realities. [Figure 15-34](#figure15-34) shows the process.
  prefs: []
  type: TYPE_NORMAL
- en: As before, we imagine that we started with our weight at position A, and after
    the most recent update, we ended up at B, as shown in [Figure 15-34](#figure15-34)(a).
    As with momentum, we find the change applied at point A to bring us to B (the
    arrow *m*) and we scale that by *γ*.
  prefs: []
  type: TYPE_NORMAL
- en: Now comes the new part, starting in [Figure 15-34](#figure15-34)(b). Rather
    than finding the gradient at B, we first add the scaled momentum to B to get the
    “predicted” error P. This is our guess for where we will end up on the error surface
    after the next step. As shown [Figure 15-34](#figure15-34)(c), we find the gradient
    *g* at point P and scale it as usual to get *ηg*. Now we find the new point C
    in [Figure 15-34](#figure15-34)(d) by adding the scaled momentum *γm* and the
    scaled gradient *ηg* to B.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15034](Images/F15034.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-34: Gradient descent with Nesterov momentum'
  prefs: []
  type: TYPE_NORMAL
- en: Notice we’re not using the gradient at point B at all. We just combine a scaled
    version of the momentum that got us to B and a scaled version of the gradient
    at our predicted point P.
  prefs: []
  type: TYPE_NORMAL
- en: Notice too that the point C in [Figure 15-34](#figure15-34)(d) is closer to
    the bottom of the bowl than point P, where we’d have ended up with normal momentum.
    By looking into the future and seeing that we’d be on the other side of the valley,
    we are able to use that left-pointing gradient to prevent rolling far up the far
    side.
  prefs: []
  type: TYPE_NORMAL
- en: In honor of the researcher who developed this method, it’s called *Nesterov
    momentum*, or the *Nesterov accelerated gradient* (Nesterov 1983). It’s basically
    a souped-up version of the momentum technique we saw earlier. Though we still
    have to pick a value for *γ,* we don’t have to pick any new parameters. This is
    a nice example of an algorithm that gives us increased performance without requiring
    more work on our end.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-35](#figure15-35) shows the result of Nesterov momentum for 15 steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F15035](Images/F15035.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-35: Running Nesterov momentum for 15 steps. It finds the bottom of
    the valley in about seven steps and then stays there.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-36](#figure15-36) shows the error curve for our standard test case
    using Nesterov momentum. This uses the exact same model and parameters as the
    momentum-only results in [Figure 15-33](#figure15-33), but it’s both less noisy
    and more efficient, getting down to about 0 error at roughly epoch 425, rather
    than the roughly 600 required by regular momentum alone.'
  prefs: []
  type: TYPE_NORMAL
- en: '![f15036](Images/f15036.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-36: Error for mini-batch SGD with Nesterov momentum. The system reaches
    zero error around epoch 600\. The graph shows 1,000 epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: Any time we use momentum it’s worth considering Nesterov momentum instead. It
    requires no additional parameters from us, but it usually learns more quickly
    and with less noise.
  prefs: []
  type: TYPE_NORMAL
- en: Adagrad
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve seen two types of momentum that help push us through plateaus and reduce
    overshooting. We’ve been using the same learning rate when we update all the weights
    in our network. Earlier in this chapter, we mentioned the idea of using a learning
    rate *η* that’s tailored individually for each weight.
  prefs: []
  type: TYPE_NORMAL
- en: Several related algorithms use this idea. Their names all begin with *Ada*,
    standing for “adaptive.”
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with an algorithm called *Adagrad*, which is short for *adaptive
    gradient learning* (Duchi, Hazan, and Singer 2011). As the name implies, the algorithm
    adapts (or changes) the size of the gradient for each weight. In other words,
    Adagrad gives us a way to perform learning-rate decay on a weight-by-weight basis.
    For each weight, Adagrad takes the gradient that we use in that update step, squares
    it, and adds that into a running sum for that weight. Then the gradient is divided
    by a value derived from this sum, giving us the value that’s then used for the
    update.
  prefs: []
  type: TYPE_NORMAL
- en: Because each step’s gradient is squared before it’s added in, the value that’s
    added into the sum is always positive. As a result, this running sum gets larger
    and larger over time. To keep it from growing out of control, we divide each change
    by that growing sum, so the changes to each weight get smaller and smaller over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: This sounds a lot like learning rate decay. As time goes on, the changes to
    the weights get smaller. The difference here is that the slowdown in learning
    is being computed uniquely for each weight based on its history.
  prefs: []
  type: TYPE_NORMAL
- en: Because Adagrad is effectively automatically computing a learning rate for every
    weight on the fly, the learning rate we use to kick things off isn’t as critical
    as it was for earlier algorithms. This is a huge benefit, since it frees us from
    the task of fine-tuning that error rate. We often set the learning rate *η* to
    a small value like 0.01 and let Adagrad handle things from there.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-37](#figure15-37) shows the performance of Adagrad on our test data.'
  prefs: []
  type: TYPE_NORMAL
- en: This has the same general shape as most of our other curves, but it takes a
    very long time to get to 0\. Because the sum of the gradients gets larger over
    time, eventually we’ll find that dividing each new gradient by a value related
    to that sum gives us gradients that approach 0\. The increasingly small updates
    are why the error curve for Adagrad descends so very slowly as it tries to get
    rid of that last remaining error.
  prefs: []
  type: TYPE_NORMAL
- en: We can fix that without too much work.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15037](Images/F15037.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-37: The performance of Adagrad on our test setup'
  prefs: []
  type: TYPE_NORMAL
- en: Adadelta and RMSprop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The problem with Adagrad is that the gradient we apply to each weight for its
    update step just keeps getting smaller and smaller. That’s because the running
    sum just gets larger and larger.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of summing up all the squared gradients since the beginning of training,
    suppose we keep a *decaying sum* of these gradients. We can think of this as a
    running list of the most recent gradients for each weight. Each time we update
    the weights, we tack the new gradient onto the end of the list and drop the oldest
    one off the start. To find the value we use to divide the new gradient, we add
    up all the values in the list, but we first multiply them all by a number based
    on their position in the list. Recent values get multiplied by a large value,
    while the oldest ones get multiplied by a very small value. This way our running
    sum is most heavily determined by recent gradients, though it is influenced to
    a lesser degree by the older gradients (Ruder 2017).
  prefs: []
  type: TYPE_NORMAL
- en: In this way, the running sum of the gradients (and thus the value we divided
    new gradients by) can go up and down based on the gradients we’ve applied recently.
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm is called *Adadelta* (Zeiler 2012). The name comes from “adaptive,”
    like Adagrad, and the *delta* refers to the Greek letter *δ* (delta), which mathematicians
    often use to refer to change. This algorithm adaptively changes how much the weights
    are updated on each step using each one’s weighted running sum.
  prefs: []
  type: TYPE_NORMAL
- en: Since Adadelta adjusts the learning rates on the weights individually, any weight
    that’s been on a steep slope for a while will slow down so it doesn’t go flying
    off, but when that weight is on a flatter section, it’s allowed to take bigger
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: Like Adagrad, we often start the learning rate at a value around 0.01, and then
    let the algorithm adjust it from then on.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-38](#figure15-38) shows the results of Adadelta on our test setup.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F15038](Images/F15038.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-38: The results of training with Adadelta on our test data'
  prefs: []
  type: TYPE_NORMAL
- en: This compares favorably to Adagrad’s performance in [Figure 15-37](#figure15-37).
    It’s nice and smooth and reaches 0 at around epoch 2,500, much sooner than Adagrad’s
    8,000 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Adadelta has the downside of requiring another parameter, which is unfortunately
    also called gamma (*γ*). It’s roughly related to the parameter *γ* used by the
    momentum algorithms, but they’re sufficiently different that it’s best to consider
    them distinct ideas that happen to have been given the same name. The value of
    *γ* here tells us how much we scale down the gradients in our history list over
    time. A large value of *γ* “remembers” values from farther back than smaller values
    and will let them contribute to the sum. A smaller value of *γ* just focuses on
    recent gradients. Often we set this *γ* to around 0.9.
  prefs: []
  type: TYPE_NORMAL
- en: There’s actually another parameter in Adadelta, named with the Greek letter
    *ε* (epsilon). This is a detail that’s used to keep the calculations numerically
    stable. Most libraries will set this to a default value that’s carefully selected
    by the programmers to make things work as well as possible, so it should never
    be changed unless there’s a specific need.
  prefs: []
  type: TYPE_NORMAL
- en: An algorithm that’s very similar to Adadelta, but that uses slightly different
    mathematics, is called *RMSprop* (Hinton, Srivastava, and Swersky 2015). The name
    comes from the fact that it uses a root-mean-squared operation, often abbreviated
    RMS, to determine the adjustment that is added (or *propagated*, hence the “prop”
    in the name) to the gradients.
  prefs: []
  type: TYPE_NORMAL
- en: RMSprop and Adadelta were invented around the same time, and work in similar
    ways. RMSprop also uses a parameter to control how much it “remembers,” and this
    parameter, too, is named *γ*. Again, a good starting value is around 0.9.
  prefs: []
  type: TYPE_NORMAL
- en: Adam
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous algorithms share the idea of saving a list of squared gradients
    with each weight. They then create a scaling factor by adding up the values in
    this list, perhaps after scaling them. The gradient at each update step is divided
    by this total. Adagrad gives all the elements in the list equal weight when it
    builds its scaling factor, while Adadelta and RMSprop treat older elements as
    less important, and thus they contribute less to the overall total.
  prefs: []
  type: TYPE_NORMAL
- en: Squaring the gradient before putting it into the list is useful mathematically,
    but when we square a number, the result is always positive. This means that we
    lose track of whether that gradient in our list was positive or negative, which
    is useful information to have. So, to avoid losing this information, we can keep
    a second list of the gradients without squaring them. Then we can use both lists
    to derive our scaling factor.
  prefs: []
  type: TYPE_NORMAL
- en: This is the approach of an algorithm called *adaptive moment estimation*, or
    more commonly *Adam* (Kingma and Ba 2015).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-39](#figure15-39) shows how Adam performs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F15039](Images/F15039.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-39: The Adam algorithm on our test set'
  prefs: []
  type: TYPE_NORMAL
- en: The output is great. It’s only slightly noisy and hits about 0 error at around
    epoch 900, much sooner than Adagrad or Adadelta. The downside is that Adam has
    two parameters, which we must set at the start of learning. The parameters are
    named for the Greek letter *β* (beta) and are called “beta 1” and “beta 2,” written
    *β*1 and *β*2\. The authors of the paper on Adam suggest setting *β*1 to 0.9,
    and *β*2 to 0.999, and these values indeed often work well.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an Optimizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This has not been a complete list of all the optimizers that have been proposed
    and studied. There are many others, with more coming all the time, and each has
    its own strengths and weaknesses. Our goal was to give an overview of some of
    the most popular techniques and to understand how they achieve their speedups.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-40](#figure15-40) summarizes our two-moon results for SGD with Nesterov
    momentum and the three adaptive algorithms of Adagrad, Adadelta, and Adam.'
  prefs: []
  type: TYPE_NORMAL
- en: '![F15040](Images/F15040.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-40: The loss, or error, over time for four of the algorithms just
    covered. This graph shows only the first 4,000 epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: In this simple test case, mini-batch SGD with Nesterov momentum is the clear
    winner, with Adam coming in a close second. In more complicated situations, the
    adaptive algorithms typically perform better.
  prefs: []
  type: TYPE_NORMAL
- en: Across a wide variety of datasets and networks, the final three adaptive algorithms
    that we discussed (Adadelta, RMSprop, and Adam) often perform very similarly (Ruder
    2017). Studies have found that Adam does a slightly better job than the others
    in some circumstances, so that’s usually a good place to start (Kingma and Ba
    2015).
  prefs: []
  type: TYPE_NORMAL
- en: Why are there so many optimizers? Wouldn’t it be wise to find the best one and
    stick with that? It turns out that not only do we not know of a “best” optimizer,
    but there can’t be a best optimizer for all situations*.* No matter what optimizer
    we put forth as the “best,” we can prove that it’s always possible to find some
    situation in which another optimizer would be better. This result is famously
    known by its colorful name, the *No Free Lunch Theorem* (Wolpert 1996; Wolpert
    and Macready 1997). This guarantees us that no optimizer will always perform better
    than any other.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the No Free Lunch Theorem doesn’t say that all optimizers are equal.
    As we’ve seen in our tests in this chapter, different optimizers do perform differently.
    The theorem only tells us that no one optimizer will *always* beat the others.
  prefs: []
  type: TYPE_NORMAL
- en: Though no one optimizer is the best choice for all possible training situations,
    we can find the best optimizer for any specific combination of network and data.
    Most deep learning libraries offer routines that carry out an automated search
    that can try out multiple optimizers and run through multiple parameter choices
    for each one. Whether we choose our optimizer and its values by ourselves or as
    the result of a search, we need to keep in mind that the best choices can vary
    from one network and set of data to the next. As soon as we make a big change
    to either, we should consider checking to see if a better optimizer would give
    us more efficient training. As a practical guide, many people start out with Adam,
    using its default parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: No matter what optimizer we choose, our network can suffer from overfitting.
    As we discussed in Chapter 9, overfitting is a natural result of training for
    too long. The problem is that the network learns the training data so well that
    it becomes tuned to just that data and performs poorly on new data once it’s released.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques that delay the onset of overfitting are called *regularization* methods.
    They allow us to train for more epochs before overfitting has too great an impact,
    which means our networks have more training time in which to improve their performance.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A popular regularization method is called *dropout*. It is usually applied in
    a deep network in the form of a *dropout layer* (Srivastava et al. 2014). The
    dropout layer is called an *accessory layer* or a *supplemental layer*, because
    it doesn’t do any computation of its own. We call it a layer, and draw it as one,
    because it’s convenient conceptually, and lets us include dropout in drawings
    of networks. But we don’t consider it a real layer (hidden or otherwise), and
    we don’t count it when we describe how many layers make up a particular network.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout is a placeholder that tells the network to run an algorithm on the previous
    layer. It’s also only active during training. When the network is deployed, dropout
    layers are disabled or removed.
  prefs: []
  type: TYPE_NORMAL
- en: The job of the dropout layer is to temporarily disconnect some of the neurons
    on the previous layer. We give it a parameter that describes the percentageof
    neurons that should be affected, and at the start of each batch, it randomly chooses
    that percentage of neurons on the preceding layer and temporarily disconnects
    their inputs and outputs from the network. Since they’re disconnected, these neurons
    don’t participate in any forward calculations, they’re not included in backprop,
    and the weights coming into them are not updated by the optimizer. When the batch
    is done and the rest of the weights have been updated, the chosen neurons and
    all of their connections are restored.
  prefs: []
  type: TYPE_NORMAL
- en: At the start of the next batch, the layer again chooses a new random set of
    neurons and temporarily removes those, repeating the process for each epoch. [Figure
    15-41](#figure15-41) shows the idea graphically.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15041](Images/F15041.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-41: Dropout. (a) 50 percent of the four neurons in the middle layer
    (in gray) are chosen to be disconnected before the batch is evaluated. (b) Our
    schematic for a single dropout layer is a diagonal slash. To the right, we indicate
    the proportion of neurons that are selected for disconnection. Since dropout applies
    to its preceding layer, in this example, we apply it to the middle of the three
    fully connected layers.'
  prefs: []
  type: TYPE_NORMAL
- en: Dropout delays overfitting by preventing any neurons from overspecializing and
    dominating. Suppose that one neuron in a photo classification system gets highly
    specialized to detect the eyes of cats. That’s useful for recognizing picture
    of cats’ faces, but useless for all the other photographs the system might be
    asked to classify. If all the neurons in a network also specialize at finding
    just one or two features in the training data, then they can perform beautifully
    on that data because they spot the idiosyncratic details that they’re trained
    to locate. But the system as a whole will then perform badly when presented with
    new data that’s missing the precise cues those neurons became specialized for.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout helps us avoid this kind of specialization. When a neuron is disconnected,
    the remaining neurons must adjust to pick up the slack. Thus, the specialized
    neuron is freed up to perform a more generally useful task, and we’ve delayed
    the onset of overfitting. Dropout helps us put off overfitting by spreading around
    the learning among all the neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Batchnorm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another regularization technique is called *batch normalization*, often referred
    to simply as *batchnorm* (Ioffe and Szegedy 2015). Like dropout, batchnorm can
    be implemented as a layer without neurons. Unlike dropout, batchnorm actually
    does perform some computation, though there are no parameters for us to specify.
  prefs: []
  type: TYPE_NORMAL
- en: Batchnorm modifies the values that come out of a layer. This might seem strange,
    since the whole purpose of training is to get our neurons to produce output values
    that lead to good results. Why would we want to modify those outputs?
  prefs: []
  type: TYPE_NORMAL
- en: Recall that many of our activation functions, such as leaky ReLU and tanh, have
    their greatest effect near 0\. To get the most benefit from those functions, we
    need the numbers flowing into them to be in a small range centered around 0\.
    That’s what batchnorm does by scaling and shifting all the outputs of a layer
    together. Because batchnorm moves the neuron outputs into a small range near 0,
    we’re less prone to seeing any neuron learning one specific detail and producing
    a huge output that swamps all the other neurons, and thus we are able to delay
    the onset of overfitting. Batchnorm scales and shifts all the values coming out
    of the previous layer over the course of an entire mini-batch in just this way.
    It learns the parameters for this scaling and shifting along with the weights
    in the network so they take on the most useful values.
  prefs: []
  type: TYPE_NORMAL
- en: We apply batchnorm before the activation function so that the modified values
    will fall in the region of the activation function where they are affected the
    most. In practice, this means we place no activation function on the neurons going
    into batchnorm (or if we must specify a function, it’s the linear activation function,
    which has no effect). Those values go into batchnorm, and then they’re fed into
    the activation function we want to apply.
  prefs: []
  type: TYPE_NORMAL
- en: The process is illustrated in [Figure 15-42](#figure15-42). Our icon for a regularization
    step like batchnorm is a black disc inside a circle, suggesting that the values
    in the circle are transformed into a smaller region. In later chapters, we’ll
    see other, similar regularization steps for which we’ll use the same icon. The
    text (or a nearby label) identifies which variety of regularization is applied.
  prefs: []
  type: TYPE_NORMAL
- en: '![F15042](Images/F15042.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-42: Applying a batchnorm layer. Top: A neuron followed by a leaky
    ReLU activation function. Bottom: The same neuron with batchnorm. The activation
    function is replaced with the linear function, followed by batchnorm (represented
    by a circle with a black disc inside) and then the leaky ReLU.'
  prefs: []
  type: TYPE_NORMAL
- en: Like dropout, batchnorm defers the onset of overfitting, allowing us to train
    longer.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Optimization is the process of adjusting the weights so that our network learns.
    The core idea begins with the gradient for every weight. We follow that gradient
    to direct us to a lower point on the error surface, hence the name gradient descent.
    The most important value in this process is the learning rate. A common technique
    is to reduce the learning rate over time, according to a decay schedule.
  prefs: []
  type: TYPE_NORMAL
- en: We covered several efficient optimization techniques. We can adjust the weights
    after every epoch (batch gradient descent), after every sample (stochastic gradient
    descent, or SGD), or after mini-batches of samples (mini-batch gradient descent
    or mini-batch SGD). Mini-batch gradient descent is by far the most common technique,
    and the convention in the field is to refer to it simply as SGD. We can improve
    the efficiency of every type of gradient descent by using momentum. We can also
    improve learning by computing a custom, adaptive learning rate for every weight
    over time with an algorithm such as Adam. Lastly, to prevent overfitting, we can
    use a regularization technique such as dropout or batchnorm.
  prefs: []
  type: TYPE_NORMAL
- en: Deep networks that are made up of fully connected layers can do some amazing
    things. But if we create our layers by structuring the neurons in different ways
    and add a little bit of supporting computation, their power increases significantly.
    In the next few chapters, we’ll look at these new layers and how they can be used
    to classify, predict, and even generate images, sounds, and more.
  prefs: []
  type: TYPE_NORMAL
