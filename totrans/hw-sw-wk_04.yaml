- en: '**4**'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4**'
- en: '**Movie CGI**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**电影CGI**'
- en: '![image](graphics/common-01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/common-01.jpg)'
- en: Some of software’s most impressive work can be seen in movie theaters. Images
    that in earlier eras were painstakingly produced with models, matte paintings,
    elaborate costumes, and trick photography are now created by computers. More than
    merely simplifying the filmmaking process, *computer-generated imagery (CGI)*
    produces images that would have been impossible before. For many filmgoers, movies
    changed forever when they saw *Jurassic Park*. When Steven Spielberg was developing
    the movie, he expected to create his dinosaurs using old-school effects like automated
    puppets and animated miniatures, but once he saw some computer-animated test footage,
    he decided to use CGI for many of the dinosaur shots. The result left viewers
    astounded by images like the panorama shown in [Figure 4-1](ch04.html#ch4fig1).
    For comparison, the old way to put a dinosaur in a movie is shown in [Figure 4-2](ch04.html#ch4fig2).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一些软件最令人印象深刻的工作可以在电影院中看到。那些在早期时代通过模型、底片画、复杂的服装和特技摄影制作的图像，现在都可以通过计算机来实现。*计算机生成图像（CGI）*不仅简化了电影制作过程，还创造出了以前无法实现的图像。对许多影迷来说，当他们看到*《侏罗纪公园》*时，电影的历史永远改变了。当斯蒂文·斯皮尔伯格在开发这部电影时，他本打算使用像自动化木偶和动画微型模型这样的传统特效来制作恐龙，但在看到一些计算机动画的测试镜头后，他决定使用CGI来制作许多恐龙镜头。结果让观众对[图4-1](ch04.html#ch4fig1)中展示的全景画面感到震惊。为对比，用传统方式将恐龙放入电影的效果可以参见[图4-2](ch04.html#ch4fig2)。
- en: '![image](graphics/f04-01.jpg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-01.jpg)'
- en: '*Figure 4-1: CGI dinosaurs visit the watering hole in* Jurassic Park *(Universal
    Pictures/Amblin Entertainment, 1993).*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-1：CGI恐龙在《侏罗纪公园》中造访水源地*（环球影业/安布林娱乐，1993年）。'
- en: '![image](graphics/f04-02.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-02.jpg)'
- en: '*Figure 4-2:* The Beast from 20,000 Fathoms *(Jack Dietz Productions, 1953)
    munches on Coney Island.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-2：*《海底两万里》中的怪兽（杰克·迪茨制片公司，1953年）在康尼岛大吃大喝。'
- en: Amazing as they were, films like *Jurassic Park* were just the beginning of
    the CGI revolution. Now movies like *Avatar* create whole worlds using CGI, so
    that viewers are never sure what parts of a shot are physically real, if any.
    With enough time and money, it seems like filmmakers can produce anything imaginable.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像*《侏罗纪公园》*这样的电影令人惊叹，但它们只是CGI革命的开始。现在，像*《阿凡达》*这样的电影利用CGI创造出完整的世界，以至于观众无法确定镜头中的哪些部分是物理上真实的。如果有的话。只要有足够的时间和资金，似乎电影制作人可以创作出任何他们能想象的东西。
- en: Before computers blew our minds with dinosaurs and lush alien planets, though,
    they were transforming the world of traditionally animated movies. Using computers
    not only radically altered the process of traditional animation, but as you’ll
    discover, the concepts and techniques employed are the foundation for almost everything
    in computer graphics. This is where the story of CGI begins.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在计算机通过恐龙和郁郁葱葱的外星行星震撼我们之前，它们已经开始改变传统动画电影的世界。计算机不仅彻底改变了传统动画的制作过程，而且正如你将要发现的那样，所使用的概念和技术构成了计算机图形学几乎所有内容的基础。CGI的故事从这里开始。
- en: '**Software for Traditional Animation**'
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**传统动画软件**'
- en: A movie is a series of still images, or *frames*, presented to the eye in rapid
    succession, like a high-speed slideshow. Each frame lingers on the retina for
    a moment after it disappears from the screen, effectively blending with the next
    frame to provide the illusion of continuous motion—a phenomenon known as *persistence
    of vision*. Traditionally, movies are shown at a rate of 24 frames per second
    (fps). Making a movie means producing 24 images for every second of the film.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 电影是一系列静态图像或*帧*，以快速连续的方式呈现给观众，就像一个高速幻灯片。每一帧图像在消失后会在视网膜上停留片刻，实际上与下一帧图像融合在一起，从而产生连续运动的错觉——这一现象被称为*视觉延续*。传统上，电影以每秒24帧（fps）的速度播放。制作电影意味着每秒钟需要制作24张图像。
- en: 'A live-action movie uses a camera to collect images in real time. A traditionally
    animated film like *Lady and the Tramp*, though, is created a bit differently:
    each frame of the movie is an individually photographed, hand-crafted work of
    art.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现实电影通过相机实时收集影像。不过，像*《忠犬历险记》*这样的传统动画电影则有些不同：电影的每一帧都是经过单独拍摄的、手工制作的艺术作品。
- en: 'Traditional animation is a huge undertaking requiring a large team of artists.
    Typically, each character in an animated film is assigned a lead animator, but
    the lead animator does not draw the character on every frame in which he or she
    appears, because that’s too much work for one person. Instead, the lead animator
    draws only as many *keyframes* as are needed to suggest the action—perhaps one
    out of every few dozen frames of a finished animation sequence. Other animators
    draw the in-between frames to complete the sequence, a process known as *tweening*.
    At this stage, the animation is still just a series of pencil drawings on paper.
    The drawings must be transferred to transparent cellulose sheets, which is why
    this style of animation is also known as *cel animation*. Then comes what animators
    call “ink and paint”: the faint pencil lines are traced over with black ink, and
    the cel is colored. Then the sheets are placed in front of a separately painted
    background and photographed.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 传统动画是一项庞大的工作，需要一个庞大的艺术团队。通常，每个动画电影中的角色都会指定一个首席动画师，但首席动画师并不会在每一帧中都画出该角色，因为对于一个人来说那样工作量太大了。相反，首席动画师只会绘制足够的*关键帧*来提示动作——也许只是每隔几帧才会绘制一次。这时，其他动画师会绘制中间帧以完成整个动画序列，这一过程称为*tweening*（过渡绘制）。在此阶段，动画仍然只是纸上的一系列铅笔画。这些画必须被转移到透明的纤维素片上，这也是这种风格的动画被称为*cel动画*的原因。接下来是动画师所称的“墨水和上色”：淡淡的铅笔线会被黑色墨水描摹一遍，然后将纤维素片上色。最后，这些片段被放置在单独绘制的背景前并进行拍摄。
- en: As you might expect, tweening, inking, and painting are tedious, time-intensive
    jobs. Beginning around 1990, computer imagery has been used to mimic the cel animation
    style with far less manual labor.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所预料的那样，过渡绘制、描线和上色都是繁琐且耗时的工作。大约从1990年开始，计算机图像被用来模仿cel动画风格，而且所需的人工劳动要少得多。
- en: '***How Digital Images Work***'
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***数字图像是如何工作的***'
- en: In a traditional animated film, each frame is a photograph of physical art,
    but computer animation works with *digital images*—pictures defined by numerical
    data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统动画电影中，每一帧都是一张物理艺术作品的照片，但计算机动画则使用*数字图像*——由数字数据定义的图像。
- en: When you look at a video display such as a television, a smartphone screen,
    or a digitally projected theater screen, the image that reaches your eyes is made
    up of dots of varying colors, known as *pixels*. [Figure 4-3](ch04.html#ch4fig3)
    depicts a tree against a blue sky as a grid of pixels. Each of the 100 pixels
    in this 10×10 grid is assigned a color, here specified by name.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当你观看电视、智能手机屏幕或数字投影影院屏幕等视频显示时，进入你眼睛的图像是由不同颜色的点组成的，这些点被称为*像素*。[图4-3](ch04.html#ch4fig3)展示了一个树木和蓝天的图像，这个图像是由像素网格组成的。这个10×10网格中的每个100个像素都有一个指定的颜色，颜色通过名称来指定。
- en: '![image](graphics/f04-03.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-03.jpg)'
- en: '*Figure 4-3: A tree made of pixels*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-3：由像素构成的树*'
- en: Although we can think of each pixel as a solid color, the underlying reality
    is a bit different. For example, at home you might watch a movie on a common *liquid
    crystal display (LCD)* television in which pixel colors are determined by electrically
    controlled crystals. On the back of an LCD screen is a light source, either a
    fluorescent lamp or a series of *light-emitting diodes (LEDs)*. The light source
    itself is white. In front of the light is a translucent panel with bars in the
    three primary colors—red, green, and blue—as shown in [Figure 4-4](ch04.html#ch4fig4).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以将每个像素看作是一个纯色，但实际情况稍有不同。例如，在家里，你可能会在一台普通的*液晶显示器（LCD）*电视上观看电影，在这种电视中，像素颜色由电控晶体决定。在LCD屏幕的背面有一个光源，要么是荧光灯，要么是一系列*发光二极管（LED）*。光源本身是白色的。在光源前面是一个半透明的面板，上面有三种原色条——红色、绿色和蓝色，如[图4-4](ch04.html#ch4fig4)所示。
- en: '![image](graphics/f04-04.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-04.jpg)'
- en: '*Figure 4-4: Three bars of pure primary colors create one LCD pixel.*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-4：三种纯原色条形图组成一个LCD像素。*'
- en: A layer of liquid crystals lying between the light source and the color panel
    puts an individually controlled crystal behind each of the translucent bars. You
    can think of these crystals as electrically operated doors, and the degree to
    which each crystal door is open determines how much light gets through. By varying
    the amount of red, green, or blue, any one of millions of colors can be produced
    by each pixel. This is *additive color mixing*, in which adding more color makes
    the result brighter. If we want a particular pixel to come across as bright yellow,
    for example, we would set the levels of red and green high, and the level of blue
    low. If we wanted a dark gray, we would set each of the color bars to the same
    low intensity. All three colors at maximum intensity produce pure white. Later
    in this chapter, we’ll see an example of *subtractive color mixing*, which is
    what you might remember from art class, where adding more color makes the result
    darker.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一层液晶位于光源和颜色面板之间，每个半透明条的后面都有一个单独控制的液晶晶体。你可以把这些晶体看作是电控门，每个晶体门的开度决定了光线透过的多少。通过调节红色、绿色或蓝色的量，每个像素可以产生数百万种颜色。这是*加色混合*，即增加颜色会使结果更亮。例如，如果我们希望某个像素呈现明亮的黄色，我们会将红色和绿色的级别设置得较高，而蓝色的级别设置得较低。如果我们想要深灰色，我们会将每种颜色条的强度设置为相同的低值。所有三种颜色在最大强度下会产生纯白色。本章后面我们将看到*减色混合*的例子，这可能是你从美术课上记得的内容，其中添加更多颜色会使结果更暗。
- en: '***How Colors Are Defined***'
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***颜色是如何定义的***'
- en: The most common way to define a pixel’s color is with the *RGB* system, which
    uses three numbers to represent the intensity of red, green, and blue in the pixel.
    The numbers typically range from 0 to 255 to match the range of an eight-bit byte.
    This means that each RGB pixel is specified by three bytes of data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 定义像素颜色最常用的方法是*RGB*系统，该系统通过三个数字表示像素中红色、绿色和蓝色的强度。这些数字通常在0到255之间，以匹配8位字节的范围。这意味着每个RGB像素由三个字节的数据指定。
- en: As far as software is concerned, a digital image such as that shown in [Figure
    4-3](ch04.html#ch4fig3) is just a list of bytes of color data, three bytes for
    each pixel. This block of bytes is known as the image’s *bitmap*. The first three
    bytes in the bitmap are the red, green, and blue levels of the pixel in the upper-left
    corner of the image, and so on. The width and height of an image or bitmap in
    pixels is known as its *resolution*; for instance, [Figure 4-3](ch04.html#ch4fig3)’s
    resolution is 10×10\. A bitmap called a *display buffer* stores the colors of
    each pixel of a digital display like an LCD television; ultimately, computer graphics
    methods are about setting the numbers in a display buffer.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 就软件而言，数字图像（如[图4-3](ch04.html#ch4fig3)所示）只是一组颜色数据的字节，每个像素占用三个字节。这组字节称为图像的*位图*。位图中的前三个字节是图像左上角像素的红色、绿色和蓝色级别，依此类推。图像或位图的宽度和高度（以像素为单位）被称为其*分辨率*；例如，[图4-3](ch04.html#ch4fig3)的分辨率是10×10。一个称为*显示缓冲区*的位图存储数字显示器（如液晶电视）每个像素的颜色；最终，计算机图形方法就是设置显示缓冲区中的数字。
- en: The location of a particular pixel in a bitmap is specified by two *coordinates*,
    an *x*-coordinate for horizontal position and a *y*-coordinate for vertical position.
    The (0,0) coordinate, known as the *origin*, can be located in a corner or in
    the center; it varies among different coordinate systems. When positioning pixels
    on a physical display, we refer to coordinates as *screen coordinates*. Screen
    coordinate systems commonly set the origin at the upper-left pixel, so a 1920×1080
    screen would locate pixels as shown in [Figure 4-5](ch04.html#ch4fig5). Here,
    the y-axis increases moving down the image, the x-axis increases moving right
    across the image, and the center location is (960, 540).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 位图中某个特定像素的位置通过两个*坐标*来指定，一个是水平位置的*x*-坐标，另一个是垂直位置的*y*-坐标。坐标(0,0)，称为*原点*，可以位于角落或中心；它在不同的坐标系统中有所不同。在物理显示器上定位像素时，我们称坐标为*屏幕坐标*。屏幕坐标系统通常将原点设置在左上角像素，因此1920×1080屏幕上像素的位置如[图4-5](ch04.html#ch4fig5)所示。在这里，y轴向下移动时增大，x轴向右移动时增大，中心位置是(960,
    540)。
- en: '![image](graphics/f04-05.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-05.jpg)'
- en: '*Figure 4-5: Locating pixels on a 1920×1080 screen*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-5：在1920×1080屏幕上定位像素*'
- en: Coordinate systems are a ubiquitous part of computer graphics and, as you’ll
    see in this chapter and the next, much of the work of producing graphics involves
    converting coordinates from one system to another.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 坐标系统是计算机图形学中无处不在的一部分，正如你将在本章和下一章看到的那样，制作图形的大部分工作涉及将坐标从一个系统转换到另一个系统。
- en: '***How Software Makes Cel Animations***'
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***软件如何制作原子动画***'
- en: Now that you understand what’s inside a digital image, you’re ready to see how
    software can make digital images that look like traditional cels. The first step
    is getting the artist’s work inside the computer.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了数字图像的构成，你准备好了解软件如何制作看起来像传统原子画的数字图像了。第一步是将艺术家的作品导入计算机。
- en: '**Transforming Drawings into Models**'
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**将图形转化为模型**'
- en: 'Software-generated cel animation starts the same way as traditional animation:
    with an artist sketching a character. Instead of drawing on paper, though, the
    artist draws with a mouse or an electronic stylus and the drawings are recorded
    by software. In order to ultimately produce a bitmapped image, we need a system
    that defines the artist’s strokes numerically, producing a *model* of the drawing.
    Locations within a model are called *local coordinates*. [Figure 4-6](ch04.html#ch4fig6)
    shows a drawing of a bug-man within a box that defines the local coordinate space.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 软件生成的原子动画的起始方式与传统动画相同：艺术家先画出角色。不过，艺术家不是在纸上绘制，而是使用鼠标或电子笔绘制，绘制过程由软件记录。为了最终生成位图图像，我们需要一个系统来数值化艺术家的笔触，生成一个*模型*。模型中的位置被称为*局部坐标*。[图
    4-6](ch04.html#ch4fig6)展示了一个虫人图形，它被放置在一个定义局部坐标空间的框内。
- en: '![image](graphics/f04-06.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-06.jpg)'
- en: '*Figure 4-6: A bug-man drawing inside a box defining coordinate limits*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-6：在定义坐标限制的框内绘制的虫人图形*'
- en: Each line and curve in this model is defined in terms of these local coordinates.
    Straight line segments, like the antennae and legs of our character, can be defined
    by the coordinates of the points at either end of the line, as shown in [Figure
    4-7](ch04.html#ch4fig7). Note that the coordinates here have fractional parts
    to increase precision.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型中的每一条线和曲线都通过这些局部坐标来定义。像我们角色的天线和腿这样的直线段，可以通过线段两端的点的坐标来定义，如[图 4-7](ch04.html#ch4fig7)所示。请注意，这里的坐标包含小数部分，以提高精度。
- en: '![image](graphics/f04-07.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-07.jpg)'
- en: '*Figure 4-7: Defining straight line segments using the coordinates of the end
    points*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-7：通过端点坐标定义直线段*'
- en: For curves, *control points* are needed in addition to end points to define
    the direction and amount of curvature. Imagine that the control point is attached
    to the curve so that moving it controls the degree of curvature, as illustrated
    by the simple curves in [Figure 4-8](ch04.html#ch4fig8). If you’ve ever worked
    with a vector graphics application, you’ve likely worked with curves like this.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于曲线，除了端点外，还需要*控制点*来定义曲线的方向和弯曲程度。可以想象控制点是附加在曲线上的，移动它就能控制弯曲程度，如[图 4-8](ch04.html#ch4fig8)中的简单曲线所示。如果你曾经使用过矢量图形软件，那么你很可能也接触过这种曲线。
- en: '![image](graphics/f04-08.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-08.jpg)'
- en: '*Figure 4-8: Curves defined by two end points and one control point*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-8：由两个端点和一个控制点定义的曲线*'
- en: Simple curves can be represented by just two end points and one control point,
    but longer, more complicated curves are made up of sequences of simple curves,
    as shown with the bug-man’s shoe in [Figure 4-9](ch04.html#ch4fig9).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的曲线可以仅通过两个端点和一个控制点来表示，但较长、较复杂的曲线则由一系列简单的曲线组成，如[图 4-9](ch04.html#ch4fig9)中虫人鞋子的示例所示。
- en: The lines and curves define just the outline of a character or other drawing;
    the colors inside the outline are defined using a system such as RGB. The character
    model, then, is a numerical representation of all the lines, curves, and color
    data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 线条和曲线仅定义了角色或其他图形的轮廓；轮廓内部的颜色则使用像RGB这样的系统来定义。因此，角色模型是所有线条、曲线和颜色数据的数值表示。
- en: '![image](graphics/f04-09.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-09.jpg)'
- en: '*Figure 4-9: A complicated curve made of simple curves*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-9：由简单曲线构成的复杂曲线*'
- en: '**Automatic Tweening**'
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**自动过渡**'
- en: Numerically defining drawings allows for automatic tweening. The animator draws
    one frame of a character’s animation sequence, then creates succeeding keyframes
    by moving the control points of the curves in the previous frames. The animation
    software can then generate the other frames through *interpolation*. The concept
    is demonstrated in [Figure 4-10](ch04.html#ch4fig10). Here, the coordinates of
    the middle point are calculated as the average of the coordinates of the other
    points. The x-coordinate of the interpolated point, 20, is halfway between 10
    and 30; the y-coordinate, 120, is halfway between 100 and 140\. In this example,
    all the points lie on a line, but the interpolation path can be a curve as well.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数值定义绘图允许自动过渡。动画师首先绘制角色动画序列中的一帧，然后通过移动前一帧曲线的控制点来创建后续的关键帧。动画软件接着可以通过*插值*生成其他帧。[图
    4-10](ch04.html#ch4fig10)演示了这个概念。这里，中间点的坐标是其他点坐标的平均值。插值点的x坐标20介于10和30之间；y坐标120介于100和140之间。在这个例子中，所有点都位于一条直线上，但插值路径也可以是曲线。
- en: '![image](graphics/f04-10.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-10.jpg)'
- en: '*Figure 4-10: Computing a middle point between two keyframe points via interpolation*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-10：通过插值计算两个关键帧点之间的中间点*'
- en: '[Figure 4-11](ch04.html#ch4fig11) shows how interpolation creates new frames
    of animation. The leftmost face is the original model; the second face shows some
    of the control points; and the third has a wide mouth created by repositioning
    two of the control points downward. The rightmost face was created through linear
    interpolation, placing each control point halfway between the two keyframe positions.
    Animation software can create as many in-between positions as necessary to fill
    the gap between keyframes.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-11](ch04.html#ch4fig11)展示了插值如何创建新的动画帧。最左边的面是原始模型；第二个面显示了一些控制点；第三个面则通过将两个控制点向下移动，形成了一个大嘴巴。最右边的面通过线性插值创建，线性插值将每个控制点放置在两个关键帧位置的中间。动画软件可以根据需要创建任意数量的中间位置，以填补关键帧之间的空隙。'
- en: '![image](graphics/f04-11.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-11.jpg)'
- en: '*Figure 4-11: From left: a model, the model with selected control points, the
    model with two of the control points moved, and a tweened model created by interpolation
    between the positions of the previous two models*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-11：从左到右：一个模型，选定控制点的模型，移动了两个控制点的模型，以及通过插值生成的过渡模型*'
- en: Although basic interpolation tweening can be a huge time-saver, adjusting the
    positions of lots of little points remains tedious. More advanced animation software
    can treat a character drawing as a complete, interconnected body, in which rigid
    connections and joints are specified. This means that an animator need only position
    the feet for each keyframe to make our bug-man walk, and the software positions
    the rest of the legs accordingly. The software might even handle real-world physics,
    so that a sequence of images of our bug-man falling over a log could be animated
    entirely by the software.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基本的插值过渡可以节省大量时间，但调整大量小点的位置仍然是繁琐的。更先进的动画软件可以将角色绘图视为一个完整的、相互连接的身体，其中定义了刚性连接和关节。这意味着，动画师只需要为每个关键帧定位角色的脚，就能使虫人行走，软件会根据需要自动定位其他部位的腿部。软件甚至可能处理现实世界的物理效果，这样我们虫人跨越树干的动画序列就可以完全由软件自动生成。
- en: '**Positioning and Scaling**'
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**定位与缩放**'
- en: Numerical modeling also allows the drawings to be placed anywhere in a frame
    at any size. Changing the size of a model is called *scaling*, and is accomplished
    by multiplying or dividing the coordinates for each of the points. [Figure 4-12](ch04.html#ch4fig12)
    shows the bug-man model of [Figure 4-6](ch04.html#ch4fig6) scaled down to a quarter
    of its original area by dividing each of the coordinates in half. One point on
    his antenna is highlighted to show the idea.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数值建模还允许将图形放置在框架中的任何位置，并可以调整其大小。改变模型的大小称为*缩放*，通过对每个点的坐标进行乘法或除法来完成。[图 4-12](ch04.html#ch4fig12)展示了[图
    4-6](ch04.html#ch4fig6)中的虫人模型，通过将每个坐标除以二，将其缩小为原始面积的四分之一。模型上的一个天线点被突出显示，以便展示这一思想。
- en: Placing a model in a particular location on the screen is called *translation*,
    and is accomplished by increasing or decreasing coordinates by fixed amounts.
    In [Figure 4-13](ch04.html#ch4fig13), the shrunken bug-man from [Figure 4-12](ch04.html#ch4fig12)
    is translated to the middle of the screen by adding 700 to each x-coordinate and
    200 to each y-coordinate.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型放置在屏幕的特定位置称为*平移*，这一过程通过增加或减少坐标的固定值来实现。在[图 4-13](ch04.html#ch4fig13)中，来自[图
    4-12](ch04.html#ch4fig12)的缩小版虫人被平移到屏幕中央，方法是将每个 x 坐标增加 700，y 坐标增加 200。
- en: '![image](graphics/f04-12.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-12.jpg)'
- en: '*Figure 4-12: Scaling a model means multiplying or dividing each of the coordinates.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-12：缩放模型意味着对每个坐标进行乘法或除法操作。*'
- en: '![image](graphics/f04-13.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-13.jpg)'
- en: '*Figure 4-13: Translating a model means adding to or subtracting from coordinates.*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-13：平移模型意味着在坐标上加减数值。*'
- en: '**“Ink and Paint” for Digital Images**'
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**“数字图像的墨水与上色”**'
- en: Now that the points on the models are mapped to screen coordinates, it’s time
    to transform each frame into a bitmap. This is the software version of cel animation’s
    “ink and paint.” To keep things simple, let’s look at how just the right arm of
    our bug-man model would be converted to a bitmap, or *rasterized*, when displayed
    over a solid white background. [Figure 4-14](ch04.html#ch4fig14) shows the arm
    over a pixel grid, with circles marking the pixel centers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型上的点已映射到屏幕坐标，接下来是将每一帧转换成位图。这就像是传统手绘动画中的“墨水与上色”过程。为了简单起见，让我们看看当我们的虫人模型的右臂显示在纯白背景上时，如何将其转换为位图，或者说*光栅化*。[图
    4-14](ch04.html#ch4fig14)展示了右臂覆盖在像素网格上，圆圈标记了像素的中心。
- en: 'With the model mathematically defined, the software can place the arm at any
    position on the bitmap and then apply the indicated color—in this case, black—to
    the appropriate pixels. Right away we see there’s a problem, though: the contours
    of the arm don’t match the borders of pixels, so how do we determine which pixels
    to color? A simple rule is to color pixels when their centers are covered. [Figure
    4-15](ch04.html#ch4fig15) shows the result of pixel-center coloring.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上定义了模型后，软件可以将手臂放置在位图的任何位置，然后将指定的颜色——在此案例中为黑色——应用于适当的像素。然而，我们很快就会发现一个问题：手臂的轮廓与像素的边界并不匹配，那么我们该如何确定哪些像素需要上色呢？一个简单的规则是，当像素的中心被覆盖时就进行上色。[图
    4-15](ch04.html#ch4fig15)展示了基于像素中心上色的结果。
- en: '![image](graphics/f04-14.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-14.jpg)'
- en: '*Figure 4-14: The right arm of the bug-man superimposed over a pixel grid*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-14：虫人右臂叠加在像素网格上*'
- en: '![image](graphics/f04-15.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-15.jpg)'
- en: '*Figure 4-15: Coloring pixels solid black based on pixel centers*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-15：基于像素中心对像素进行纯黑上色*'
- en: As you can see, though, this result is rather ugly. Because the pixels are square,
    this coloring rule replaces the gracefully curving border of the model with a
    jagged edge, which is why this problem is known as *the jaggies*. The general
    problem is that the model is smooth and continuous, while the bitmap is made with
    square black-and-white pixels. The bitmap is just an approximation of the model.
    The discrepancy between continuous models and their bitmap approximations is known
    as *aliasing*, and is the source of many visual anomalies in computer graphics.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如你所看到的，这个结果相当难看。由于像素是方形的，这个上色规则将模型优雅弯曲的边界替换为锯齿状的边缘，这也是这个问题被称为*锯齿效应*的原因。这个问题的根本在于，模型是平滑连续的，而位图是由方形的黑白像素组成的。位图仅仅是对模型的近似。连续模型与其位图近似之间的差异被称为*别名效应*，它是计算机图形中许多视觉异常的根源。
- en: To avoid the jaggies, we need to color pixels using an *anti-aliasing* technique.
    In our example, instead of coloring the pixels black and white, we’ll use a range
    of grays to produce a better approximation of the model. Each pixel will be colored
    based on how much of it is covered by the arm.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免锯齿效应，我们需要使用*抗锯齿*技术来上色像素。在我们的例子中，我们将使用一系列灰度值，而不是单纯的黑白，来更好地近似模型。每个像素的颜色将根据其被手臂覆盖的程度来决定。
- en: In order to put this idea into action, instead of checking only the center of
    each pixel, let’s test several points in each pixel to see how many of them lie
    within the model. In [Figure 4-16](ch04.html#ch4fig16), 7 of the 10 testing points
    scattered around the pixel area are covered by the shape, meaning this is 70 percent
    coverage.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这个想法付诸实践，我们不再只检查每个像素的中心，而是测试每个像素内的多个点，看看有多少个点位于模型内。在[图 4-16](ch04.html#ch4fig16)中，10个散布在像素区域内的测试点中有7个被形状覆盖，意味着覆盖率为70%。
- en: The percentage of each pixel covered by the model determines the gray level.
    The result for our bug-man’s arm is shown in [Figure 4-17](ch04.html#ch4fig17).
    Although this example might not look like much, if you hold the page at arm’s
    length and squint, the edges should appear to smoothly blend into the white background,
    producing the illusion of a graceful curve.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 每个像素被模型覆盖的百分比决定了其灰度级别。虫人手臂的结果显示在[图4-17](ch04.html#ch4fig17)中。尽管这个例子看起来可能不算特别显眼，但如果你把页面拉远一点并眯起眼睛，边缘应当会平滑地融入白色背景，产生优雅曲线的错觉。
- en: '![image](graphics/f04-16.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-16.jpg)'
- en: '*Figure 4-16: A close-up of one pixel at the end of the bug-man’s arm, with
    a scattering of 10 points to estimate the area covered by the model*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-16：虫人手臂末端一个像素的特写，散布着 10 个点以估算模型覆盖的区域*'
- en: '![image](graphics/f04-17.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-17.jpg)'
- en: '*Figure 4-17: Using grayscale to anti-alias, shown with and without the pixel
    grid.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-17：使用灰度进行抗锯齿，展示了带像素网格和不带像素网格的效果。*'
- en: '**Blending into Any Background**'
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**与任何背景的融合**'
- en: 'We need to generalize the technique just described in order for it to work
    with a background other than solid white. Consider [Figure 4-18](ch04.html#ch4fig18).
    On the left is the bug-man model, and in the middle is the background for the
    shot in which he’ll appear: a close-up of a setting sun over a rocky terrain.
    On the right is the complete image with the model superimposed over the background.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将刚才描述的技术进行概括，使其能够在其他背景上也能使用。考虑[图4-18](ch04.html#ch4fig18)。左侧是虫人模型，中间是他将出现的拍摄背景：日落时分的岩石地貌特写。右侧是将模型叠加在背景上的完整图像。
- en: '![image](graphics/f04-18.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-18.jpg)'
- en: '*Figure 4-18: The bug-man model, a background, and the model superimposed over
    the background*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-18：虫人模型、背景及模型叠加在背景上*'
- en: This book is printed in black and white, but in this image the sun would be
    shades of reddish-orange and the ground would be shades of brown. As before, pixels
    along the model’s edge will appear jagged unless we use an anti-aliasing technique.
    But using the previous technique to color pixels in gray tones won’t help the
    black edge blend into a background of red-orange and brown pixels.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 本书是黑白印刷的，但在此图像中，太阳的颜色应为红橙色调，地面的颜色应为棕色调。和之前一样，模型边缘的像素会显得锯齿状，除非我们使用抗锯齿技术。然而，使用之前的技术将像素涂成灰色并不能帮助黑色边缘与红橙色和棕色像素的背景融合。
- en: A more general anti-aliasing technique calculates an *alpha level* for each
    pixel based on the percentage of the pixel that’s covered by the model. You can
    think of an alpha level as a measure of opacity. Like the color levels, an alpha
    level is typically defined in the range of 0–255\. In [Figure 4-19](ch04.html#ch4fig19),
    a black bar is superimposed over a tree at different alpha levels. At an alpha
    level of 255, the bar is entirely opaque, while at 25 the bar is barely visible.
    An alpha level of 0 would make the bar completely invisible.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更为通用的抗锯齿技术会根据模型覆盖的像素百分比为每个像素计算一个*alpha 水平*。你可以将 alpha 水平看作是透明度的度量。像颜色级别一样，alpha
    水平通常在 0–255 范围内定义。在[图4-19](ch04.html#ch4fig19)中，一条黑色条形叠加在树木上，具有不同的 alpha 水平。在
    alpha 水平为 255 时，条形完全不透明，而在 25 时条形几乎不可见。alpha 水平为 0 时，条形将完全不可见。
- en: The alpha levels of all the pixels in a bitmap are collectively referred to
    as its *alpha channel*. The process of making an alpha channel for a model is
    similar to how we anti-aliased the black arm against the white background, only
    rather than assigning a shade of gray based on the pixel’s coverage percentage,
    we assign an alpha value for the pixel instead. Each model is thus conceptually
    transformed into both a bitmap, showing the color of each pixel covered by the
    model, and an alpha channel, showing the opacity of each pixel. [Figure 4-20](ch04.html#ch4fig20)
    shows the color bitmap (here, just black pixels) and the alpha channel of the
    bug-man arm separately.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 位图中所有像素的 alpha 值统称为*alpha 通道*。为模型制作 alpha 通道的过程类似于我们如何将黑色手臂与白色背景抗锯齿，只不过这次我们不再根据像素覆盖百分比来赋予灰度值，而是为每个像素分配一个
    alpha 值。因此，每个模型从概念上来说被转化为一张位图，显示模型覆盖的每个像素的颜色，以及一个 alpha 通道，显示每个像素的不透明度。[图4-20](ch04.html#ch4fig20)展示了虫人手臂的颜色位图（这里只有黑色像素）和
    alpha 通道。
- en: '![image](graphics/f04-19.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-19.jpg)'
- en: '*Figure 4-19: A tree covered by five black bars of varying alpha level*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-19：一棵树上覆盖了五条具有不同 alpha 水平的黑色条形*'
- en: '![image](graphics/f04-20.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-20.jpg)'
- en: '*Figure 4-20: The arm of the bug-man model with its corresponding color bitmap
    and alpha channel*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-20：虫人模型的手臂及其对应的颜色位图和透明通道*'
- en: Now the model can be applied to any background. The final color of each pixel
    is a blend of the color in the background and the model’s color bitmap, with the
    alpha level determining how much of each color goes into the mix. In the bug-man
    scene of [Figure 4-18](ch04.html#ch4fig18), if a black bug-man pixel with 30 percent
    alpha were placed on top of a red-orange sunset background pixel, the result would
    be a darker red-orange, as shown in [Figure 4-21](ch04.html#ch4fig21). The resulting
    amount of each color component lies somewhere between the two mixed colors, but
    because the black pixel is only 30 percent alpha, the red-orange background color
    dominates. For pixels completely covered by the model, the alpha level is 100
    percent and the color in the final image is the same as in the model’s color bitmap.
    In this way, a bitmap with an alpha channel can be smoothly blended into any background.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型可以应用于任何背景。每个像素的最终颜色是背景颜色和模型颜色位图的混合，透明度水平决定了每种颜色在混合中所占的比例。在[图4-18](ch04.html#ch4fig18)中的虫人场景中，如果一个黑色的虫人像素具有30%的透明度，并覆盖在一个红橙色的日落背景像素上，结果将是一个更暗的红橙色，如[图4-21](ch04.html#ch4fig21)所示。每个颜色成分的最终比例会在两种混合颜色之间，但由于黑色像素的透明度只有30%，因此红橙色的背景颜色占主导地位。对于完全被模型覆盖的像素，透明度为100%，最终图像中的颜色与模型的颜色位图相同。通过这种方式，带有透明通道的位图可以平滑地与任何背景混合。
- en: '![image](graphics/f04-21.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![image](graphics/f04-21.jpg)'
- en: '*Figure 4-21: The red, green, and blue components of three colors: the black
    of the model, the red-orange of the background pixel, and the result of mixing
    these two colors if the black has 30% alpha*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-21：三种颜色的红、绿、蓝组成部分：模型的黑色，背景像素的红橙色，以及这两种颜色混合后的结果，如果黑色的透明度为30%*'
- en: '***From Cel Animation Software to Rendered 2D Graphics***'
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***从卡通动画软件到渲染的2D图形***'
- en: These techniques are now the default way to produce cel-style animation, and
    software is as common a tool for animation studios as brushes and paper were in
    earlier generations. While some animation studios use programs they developed
    themselves, most direct-to-video or television animation and some feature films
    are made with off-the-shelf software. One such program, Toon Boom, has been used
    for television shows such as *The Simpsons* and *Phineas and Ferb*, while the
    artists at Studio Ghibli use a program called Toonz to animate such movies as
    *Spirited Away*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术现在已经成为制作卡通风格动画的默认方式，而软件在动画工作室中的地位与早期世代中的画笔和纸张一样普及。虽然一些动画工作室使用自己开发的程序，但大多数直接面向视频或电视的动画以及一些电影是使用现成的软件制作的。其中一个这样的程序，Toon
    Boom，曾被用于《辛普森一家》和《费尼亚斯与费布》这样的电视节目，而吉卜力工作室的艺术家们则使用一款名为Toonz的程序制作了《千与千寻》等电影。
- en: The usefulness of these techniques is not limited to filmmaking, though. More
    generally, the software techniques used to mimic traditional cel-style animation
    are called two-dimensional graphics, or *2D graphics*, because the control points
    for models are located with two coordinates, x and y. The general task of transforming
    models into final images is called *rendering*, and the software that performs
    the task is the *renderer*. Rendered 2D graphics are used throughout computing.
    Many video games, such as *Angry Birds*, use the cel-animation look. These rendering
    techniques are also used to display fonts and icons in applications such as browsers
    and word processors.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术的应用并不限于电影制作。更广泛地说，用于模拟传统卡通风格动画的软件技术被称为二维图形，或*2D图形*，因为模型的控制点是通过两个坐标x和y来定位的。将模型转换为最终图像的任务称为*渲染*，执行这一任务的软件被称为*渲染器*。渲染的2D图形在计算机中广泛应用。许多视频游戏，如*愤怒的小鸟*，都使用卡通动画风格。这些渲染技术也被用于显示浏览器和文字处理器等应用中的字体和图标。
- en: Although rendered 2D graphics are ubiquitous in computing and can make great
    cel-style animations, creating the mind-blowing visuals of films like *Avatar*
    requires extending these ideas to three dimensions.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管渲染的2D图形在计算机中无处不在，并且可以制作出很棒的卡通风格动画，但像*阿凡达*这样的电影中的震撼视觉效果，要求将这些理念扩展到三维。
- en: '**Software for 3D CGI**'
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**3D CGI软件**'
- en: 'Breathtaking CGI in films like *Avatar* use *3D graphics*. The “3D” here doesn’t
    refer to simulated depth perception, like in a 3D movie, but rather to the three
    coordinates of each control point in the animation models: x- and y-coordinates
    for horizontal and vertical positioning and a *z*-coordinate to indicate depth.
    [Figure 4-22](ch04.html#ch4fig22) shows a three-dimensional model of a box with
    a highlighted point defined by x-, y-, and z-coordinates.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/f04-22.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-22: A box in three-dimensional space*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: As with 2D graphics, 3D graphics are all about rendering models into bitmaps.
    The rendering methods that produce the most realistic results require the most
    processing time. Movie CGI is impressive largely because the renderer can process
    each frame for a very long time, resulting in the high-quality result that I’ll
    call *movie-quality rendering*. We’ll discuss the keys to movie-quality rendering
    in this chapter. Then, in [Chapter 5](ch05.html#ch05), we’ll talk about graphics
    for video games, and see how many of the techniques shown here have to be modified,
    faked, or scrapped altogether when images must be produced in real time in response
    to user interaction.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '***How 3D Scenes Are Described***'
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 3D models are built out of lines and curves just like 2D models, but these lines
    and curves stretch across three dimensions instead of two. The box in [Figure
    4-22](ch04.html#ch4fig22) is a very simple model defined by eight points; the
    models used in movie CGI tend to be complex, defined by hundreds, thousands, or
    even tens of thousands of points. As with 2D rendering, models in 3D rendering
    are defined by local coordinates. The points at the corners of the box in [Figure
    4-22](ch04.html#ch4fig22), for example, are defined relative to the local origin
    at the bottom of the box.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: While 2D rendering can directly map from local coordinates to screen coordinates,
    3D models are first placed into scenes in a virtual world that has its own coordinate
    space called *world coordinates*. Designing a 3D scene is the CGI equivalent of
    building a movie set. We can place as many models as we want in the virtual world,
    of any size and at any location, and the renderer can figure out the world coordinates
    for all the locations on the models.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Introducing another coordinate system might seem like an unnecessary complication,
    but world coordinates actually make 3D graphics much easier in the long run. For
    example, an artist can model a dining room chair independently of the other models
    for the scene in which it will be used. Then the artist can copy the single chair
    model to make as many seats as needed for the dining room scene. Also, a scene,
    like a movie set, isn’t built to produce a single image but to create a space
    that will be shown in many images from many different angles, as we’ll see in
    the next section.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '***The Virtual Camera***'
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With the scenery in place, a *viewpoint* is needed. On a movie set, a cinematographer
    determines what image is captured by placing the camera and choosing a lens. For
    CGI, the viewpoint determines how the three-dimensional scene is transformed into
    a two-dimensional rendered image.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Transformation from three dimensions to two is known as *projection*. To better
    understand projection, consider [Figure 4-23](ch04.html#ch4fig23), in which an
    imaginary pyramid originates from the eye of a viewer looking at a table. A translucent
    grid lies in the pyramid between the viewer and the scene. Looking through the
    grid, the viewer can map each visible location on the three-dimensional table
    to a particular square on the two-dimensional grid. That’s projection, but instead
    of a grid of squares, it’s a grid of pixels in a bitmap.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/f04-23.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-23: Projecting a three-dimensional scene onto a flat display is like
    viewing a real-world scene through a translucent grid.*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '***Direct Lighting***'
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are many different methods of projection, but projection methods in movie-quality
    rendering are part of the larger issue of lighting. Although we don’t often realize
    it, our perception of an object’s color is determined not only by the object itself
    but also by the lighting under which we view the object. Knowing this, filmmakers
    carefully light their scenes for dramatic effect, but the problem of lighting
    in CGI is more fundamental. Without an accurate model of scene lighting, the resulting
    images won’t look realistic at all.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: To understand why this is true, let’s take a simple scene of a yellow metal
    table in a green room, as shown in [Figure 4-24](ch04.html#ch4fig24).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/f04-24.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-24: A 3D scene*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: From this viewpoint, some of the pixels will be “table” pixels and the others
    will be “wall” or “floor” pixels. A simple renderer might color every table pixel
    the same shade of yellow, while coloring all the other pixels an identical green.
    But because this coloring ignores the effect of lighting, the resulting image
    would be flat and unrealistic. (The blocks of solid color would make the image
    resemble an animation cel—an interesting effect, but not realistic.) A movie-quality
    renderer needs a *lighting model* so that the colors in our scenes are influenced
    by virtual light sources.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: The essential real-world lighting effects modeled by CGI renderers include distance,
    diffuse reflection, and specular reflection.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '**The Distance Effect**'
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To understand the distance effect, imagine a lamp emitting pure white light
    hanging directly over the middle of the table, as in [Figure 4-25](ch04.html#ch4fig25).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The closer this light is to the table, the brighter the table appears. In the
    physical world, this effect is caused by the beam of light widening as it gets
    farther from its source. The more narrowly focused a light source is, the less
    the light diminishes with distance—which explains why the highly focused light
    of a laser hardly diminishes at all.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/f04-25.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-25: The closer a light is to a surface, the brighter the surface
    appears.*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Renderers can model the distance effect realistically, but they also allow unrealistic
    distance effects in order to create a particular look or mood. For example, in
    a scene where a character carries a torch through a cave, a lighting designer
    will decide whether the torchlight extends a long way or barely penetrates the
    gloom.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: All of the lighting effects we’ll discuss allow these kinds of adjustments.
    Although it may seem strange to intentionally create unrealistic light when the
    whole point of the lighting model is to make a realistic scene, there’s a subtle
    but important distinction between reality and viewers’ expectations of reality.
    Using light in unrealistic ways is an old cinematic trick. For example, when a
    character in a darkened bedroom turns on a lamp, a stage light in the ceiling
    of the set also turns on, so that the entire scene is softly lit. Without the
    extra, unrealistic light, the scene won’t look right—it will appear too dark.
    In the same way, CGI lighting models allow their controls to be tweaked to produce
    results that are a little wrong, but feel right.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '**The Diffuse Reflection Effect**'
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Light that strikes a surface head-on appears brighter than light that strikes
    a surface at a sharp angle. In [Figure 4-26](ch04.html#ch4fig26), the center of
    the table seems brighter, or yellower, than the corners.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/f04-26.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-26: Diffuse lighting depends on the angle at which light strikes
    a surface.*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: This is due in part to the distance effect—the center is closer to the lamp
    than the corners—but is mostly due to the *diffuse reflection* effect, a change
    in brightness caused by variation in the light’s *angle of incidence*. In [Figure
    4-27](ch04.html#ch4fig27), the solid lines show the incident light rays, while
    the dashed lines are reflections. As you can see, the light strikes point B at
    a much larger angle than at point A, and therefore point B appears brighter than
    point A. But note that the *viewing angle*, or *angle of reflectance*, makes no
    difference in the diffuse reflection effect. Therefore, point A will look the
    same to both viewers, and so will point B.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/f04-27.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-27: Diffuse lighting varies based on the angle at which the light
    strikes the surface, but is the same for all viewpoints.*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '**The Specular Reflection Effect**'
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Because the metal tabletop is highly reflective, it partially acts as a mirror.
    As with any mirror, what you see in it depends on what lies on the opposite angle
    to your point of view. [Figure 4-28](ch04.html#ch4fig28) shows a shiny spot on
    the table where the hanging light is at the opposite angle from our viewpoint,
    approximately midway between the center of the table and the closest edge. Because
    this spot is a mirror-like reflection of the white light bulb, the spot will be
    white.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/f04-28.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-28: Specular lighting depends on both the angle at which the light
    strikes the surface and the view angle.*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: These shiny spots are known as *specular reflections*, and appear where the
    light’s angle of incidence matches the angle of reflectance. [Figure 4-29](ch04.html#ch4fig29)
    shows the location of specular reflections for two different viewpoints; notice
    that each ray rebounds at the same angle that it struck the table. Both viewers
    see a shiny spot on the table, but they see the spot in different places.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, some materials reflect differently than others. A shiny material
    like plastic has a high level of specular reflection, while a dull material like
    cotton cloth has more diffuse reflection. CGI lighting models allow artists to
    set different reflection properties for each surface on a model to match the appearance
    of real-world materials.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/f04-29.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-29: The specular light on the table appears in different places for
    different viewpoints.*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '***Global Illumination***'
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So far we’ve been discussing *direct lighting*, the result of light flowing
    directly from a source to a surface. In reality, the color of every object in
    the physical world is influenced by the color of every other object nearby. A
    light-brown sofa in a room with white walls looks very different than it does
    in a room with blue walls, because the sofa gains a subtle tint from the reflected
    light of the walls. This is *indirect lighting*, and for a computer-generated
    image to look realistic, it must account for this effect. A lighting model that
    accounts for all of the light in the scene, both direct and indirect, is known
    as a *global illumination model*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: An example of indirect lighting is shown in [Figure 4-30](ch04.html#ch4fig30).
    Let’s assume the light bulb emits pure white light. The beam first hits a wall
    that is painted cyan (a light blue). The light reflecting from the wall is likewise
    cyan, and when the reflected cyan light strikes the yellow rug, the resulting
    reflected light is green. The bouncing colors therefore result in a subtle greenish
    tint in the yellow rug. This sequence of color changes is caused by *subtractive
    color*, where mixing colors results in a darker shade, the way a color inkjet
    makes different shades by mixing cyan, yellow, and magenta ink. Subtractive color
    is the opposite of the additive RGB system we discussed early in the chapter,
    in which mixing results in a brighter color.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/f04-30.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-30: Light bouncing off multiple surfaces influences apparent color.*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '***How Light Is Traced***'
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A global illumination model seems to require following the paths of light beams
    as they bounce around the scene. A naive renderer, then, would use three-dimensional
    coordinate math to trace the path of every beam of light from each light source
    as it bounces from surface to surface. This would waste a lot effort, though,
    because it would deduce the color of every surface in the scene—including surfaces
    the viewer can’t actually see because they lie outside of the viewpoint’s field
    of view, are obscured by other objects, or are facing away from the viewpoint.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '**Why Light Is Traced Backward**'
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Renderers avoid this inefficiency by tracing beams backward from the viewpoint
    into the scene, a technique known as *ray tracing*. In ray tracing, an imaginary
    line is traced from the viewpoint through the center of each square in a pixel
    grid, as shown in [Figure 4-31](ch04.html#ch4fig31). The geometry of each model
    in the scene is compared with the imaginary line to see if the two intersect.
    The closest point of intersection to the viewpoint indicates the visible surface
    that will color the pixel. Note that this method of projection closely follows
    the explanation of [Figure 4-23](ch04.html#ch4fig23).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Next, more lines are traced outward from this known visible point. The goal
    is to discover which lines end at light sources, either directly or after bouncing
    off other objects. As shown in [Figure 4-31](ch04.html#ch4fig31), specular reflections
    trace only the rebound at the same angle of each impact, but diffuse reflections
    trace a number of lines in random directions. As the diffuse beams strike other
    objects, they will spawn more diffuse reflections, which means the number of paths
    to trace keeps multiplying the more the process continues. Renderers apply a cut-off
    to limit the number of bounces for each beam.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/f04-31.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-31: Tracing a beam of light from a viewpoint, through the center
    of the shaded pixel, until it reaches a model in the scene. To determine specular
    lighting, the tracing rebounds at the same angle as impact; for diffuse lighting,
    it rebounds at several random angles.*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '**How Ray Tracing Models Real-World Effects**'
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Although ray tracing is a lot of work for even a network of computers, the method
    can accurately model many real-world visual effects.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: One such effect is translucency. Although a bitmap can be made translucent by
    assigning low alpha values to pixels, that’s not the whole story for transparent
    materials like glass. A glass tumbler, for example, doesn’t merely allow light
    to pass through it, but also distorts whatever is behind it, as shown in [Figure
    4-32](ch04.html#ch4fig32).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/f04-32.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-32: The distortion of curved glass*'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: A ray tracing renderer can refract light beams according to the laws of optics
    as they pass through translucent materials. This will not only allow the renderer
    to model glass in CGI, but will also help to reproduce the distorting effects
    of transparent materials and liquids like water.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Ray tracing can also be extended to simulate camera lenses. Normally, all objects
    in a computer-generated image are perfectly in focus. In images shot by a movie
    camera, though, only objects at a certain distance from the camera are in focus,
    leaving other objects less focused the farther they are from that distance. While
    one might consider having everything in focus an *advantage* of computer-generated
    imagery, skilled cinematographers use selective focus to help tell their stories.
    In [Figure 4-33](ch04.html#ch4fig33), Jimmy Stewart and Grace Kelly are in focus
    in the foreground, while the apartments in the background are blurry; the viewer’s
    attention is drawn to the actors, but the distant, open background is a subtle
    reminder of how visible the apartments in this courtyard are from each other—an
    important detail in the film. Because movie viewers have grown accustomed to receiving
    depth information about scenes through the use of focus, computer-generated images
    and movies often must simulate the use of photography lenses to match viewer expectations.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/f04-33.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-33: Focus depth in* Rear Window *(Paramount Pictures/Patron Inc.,
    1954)*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Shadows are another key component of a realistic computer-generated image. Ray
    tracing produces shadows naturally, as shown in [Figure 4-34](ch04.html#ch4fig34).
    Because no beam of light can reach the shadowed area, no beam traced back from
    the viewpoint can reach the light, so the area will remain dark.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/f04-34.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-34: Tracing beams of light renders shadows naturally.*'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Ray tracing can also model highly reflective surfaces simply by setting a very
    high specular reflection property on the material. For example, when you’re standing
    inside a well-lit room when it’s dark outside, the room in which you stand is
    clearly reflected in the window.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: So although ray tracing is computationally intense, adding these real-world
    effects doesn’t add much extra work, and the effects add greatly to the realism
    of the final image. In the next chapter, you’ll see the tricks video games use
    to render reflective surfaces and shadowing in real time, when ray tracing isn’t
    an option. Some effects, like glass distortion, are usually not even attempted
    in real-time rendering; there’s simply not enough time.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '***Full-Scene Anti-Aliasing***'
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While the images rendered by ray tracing can be stunning, they can suffer from
    the same aliasing problems we saw with 2D graphics. Whenever one object is in
    front of another, each projected light beam will either hit the foreground object
    or miss and hit what lies behind the object. [Figure 4-35](ch04.html#ch4fig35)
    shows a chair on a rug as seen from a particular viewpoint. Beams traced from
    this viewpoint near the edge of the chair seat hit either the chair or the rug,
    which assigns the associated pixel the color of one surface or the other. This
    causes a jagged edge like those we saw for 2D images.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The renderer can avoid the jaggies by applying anti-aliasing to the whole image.
    There are many methods for *full-screen anti-aliasing*, but with ray tracing,
    a direct way to anti-alias the entire scene is to project more beams from the
    viewpoint than necessary. For example, rather than just sending out a beam at
    the center of every pixel, the renderer might also send out beams into the spaces
    between the pixel centers. After the color for every beam is determined, the final
    color of each pixel is blended from the colors of the center beam and the beams
    at the neighboring corners. Pixels that lie along an edge in the image are thereby
    assigned intermediate colors, avoiding the jagged “staircase” effect.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/f04-35.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-35: In the highlighted area, each light beam trace ends on the chair
    or the rug, resulting in jaggies.*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-36](ch04.html#ch4fig36) demonstrates this idea. Each circle represents
    a beam projected into a scene. The pixels are colored based on the average of
    colors in the center and corners of each pixel, which results in the anti-aliased
    edge shown on the right. More beams can be traced for even better results, at
    the expense of more processing time.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![image](graphics/f04-36.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-36: Each pixel’s final color is a blend of five beams traced into
    the scene, one at the center of the pixel, and four at the corners.*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '**Combining the Real and the Fake**'
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a completely computer-animated film, rendering is the final step in producing
    each frame, but when CGI is integrated into live-action films, there’s more work
    to be done. Imagine, for example, a scene in which a computer-generated *Tyrannosaurus
    rex* stalks through a real field of grass.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: To make this happen, we first need two sequences of digital images. One sequence
    shows the grass field, and has either been shot on a digital camera or on a traditional
    film camera and then subsequently scanned. Either way, the movements of the camera
    are computer controlled, which allows the camera movement to match up precisely
    with the movement of the virtual camera in the other sequence, the computer-generated
    animation of the dinosaur.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Next, the two sequences are combined, frame-by-frame, in a process called *digital
    composition*. Although the dinosaur sequence was produced from 3D models, at this
    point both sequences are simply two-dimensional bitmaps and are combined using
    the same method used to place our bugman on top of the sunset back in [Figure
    4-18](ch04.html#ch4fig18). Through the use of alpha blending, the edges of the
    dinosaur in each frame are smoothly blended with the field-of-grass background.
    Without this blending, the dinosaur will have a shimmering edge like that of a
    weatherman standing in front of the five-day forecast.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Digital composition is used throughout modern moviemaking, even when no computer-generated
    imagery is involved, such as for *dissolves* (a transition where one scene smoothly
    fades into the next). Formerly, dissolves were produced by a device known as an
    *optical printer*, which pointed a camera at a screen onto which several projectors
    were aimed. The camera would make a new film that combined the images of the projected
    films. A dissolve was accomplished by turning down the light in one projector
    while turning up the light on another. The results were acceptable, but you could
    always spot an optical printer sequence in a movie because the second-generation
    images would be blurry compared to the rest of the film. Now, dissolves, superimposed
    titles, and all sorts of other movie effects that you might not really think of
    as “effects” are performed with digital composition.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '**The Ideal of Movie-Quality Rendering**'
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When all the advanced rendering techniques described in this chapter come together,
    the results can be stunningly realistic, highly stylized, or anything in between.
    The only real limitation on CGI is time, but that’s a big limitation. The truth
    is, what I’ve been calling movie-quality rendering can be an unattainable ideal
    even for Hollywood. Although films can be in production for several years, there’s
    only so much time that can be allotted for each frame. Consider the computer-animated
    Pixar film *WALL-E*. With a running time of 98 minutes, the film required the
    rendering of over 140,000 high-resolution computer images. If Pixar wanted to
    produce all of the images for *WALL-E* in two years, it would have to render images,
    on average, every eight minutes.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Even on a networked “render farm,” eight minutes is not sufficient to use ray
    tracing, global illumination, glass refraction, and all the other high-end techniques
    for every single image. Faced with these practical constraints, filmmakers pick
    and choose which techniques to use on each sequence to maximize visual impact.
    When ideal rendering is required, the time is spent, but when the best effects
    won’t be missed or the budget won’t allow it, they aren’t used. The renderer used
    at Pixar—a program called RenderMan that was originally developed at Lucasfilm—can
    forgo ray tracing and its massive associated computational effort, but that means
    many of the realism-enhancing effects have to be produced some other way.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: But how is that done? What kinds of tricks are needed to render images without
    ray tracing—images that may not be perfectly realistic but are still amazing?
    To answer this question, we’ll turn from Hollywood to the world of video games,
    where rendering is under an extreme time limitation. How extreme? If eight minutes
    isn’t enough time to produce an ideal render, imagine trying to render an image
    in under 20 *milliseconds*. In the next chapter, we’ll see how video games produce
    great graphics in a hurry.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
