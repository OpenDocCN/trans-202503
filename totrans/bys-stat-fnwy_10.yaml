- en: '**8'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: THE PRIOR, LIKELIHOOD, AND POSTERIOR OF BAYES’ THEOREM**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now that we’ve covered how to derive Bayes’ theorem using spatial reasoning,
    let’s examine how we can use Bayes’ theorem as a probability tool to logically
    reason about uncertainty. In this chapter, we’ll use it to calculate and quantify
    how likely our belief is, given our data. To do so, we’ll use the three parts
    of the theorem—the posterior probability, likelihood, and prior probability—all
    of which will come up frequently in your adventures with Bayesian statistics and
    probability.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Three Parts**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Bayes’ theorem allows us to quantify exactly how much our observed data changes
    our beliefs. In this case, what we want to know is: *P*(belief | data). In plain
    English, we want to quantify how strongly we hold our beliefs given the data we’ve
    observed. The technical term for this part of the formula is the *posterior probability*,
    and it’s what we’ll use Bayes’ theorem to solve for.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve for the posterior, we need the next part: the probability of the data
    given our beliefs about the data, or *P*(data | belief). This is known as the
    *likelihood*, because it tells us how likely the data is given our belief.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we want to quantify how likely our initial belief is in the first place,
    or *P*(belief). This part of Bayes’ theorem is called the *prior probability*,
    or simply “the prior,” because it represents the strength of our belief before
    we see the data. The likelihood and the prior combine to produce a posterior.
    Typically we need to use the probability of the data, *P*(data), in order to normalize
    our posterior so it accurately reflects a probability from 0 to 1\. However, in
    practice, we don’t always need *P*(data), so this value doesn’t have a special
    name.
  prefs: []
  type: TYPE_NORMAL
- en: As you know by now, we refer to our belief as a hypothesis, *H*, and we represent
    our data with the variable *D*. [Figure 8-1](ch08.xhtml#ch08fig01) shows each
    part of Bayes’ theorem.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/08fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-1: The parts of Bayes’ theorem*'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll investigate a crime, combining these pieces to reason
    about the situation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Investigating the Scene of a Crime**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s suppose you come home from work one day and find your window broken, your
    front door open, and your laptop missing. Your first thought is probably “I’ve
    been robbed!” But how did you come to this conclusion, and more importantly, how
    can you quantify this belief?
  prefs: []
  type: TYPE_NORMAL
- en: 'Your immediate hypothesis is that you have been robbed, so *H* = I’ve been
    robbed. We want a probability that describes how likely it is that you’ve been
    robbed, so the posterior we want to solve for given our data is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(robbed | broken window, open front door, missing laptop)'
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, we’ll fill in the missing pieces from Bayes’ theorem.
  prefs: []
  type: TYPE_NORMAL
- en: '***Solving for the Likelihood***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First, we need to solve for the likelihood, which in this case is the probability
    that the same evidence would have been observed if you were in fact robbed—in
    other words, how closely the evidence lines up with the hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(broken window, open front door, missing laptop | robbed)'
  prefs: []
  type: TYPE_NORMAL
- en: What we’re asking is, “If you were robbed, how likely is it that you would see
    the evidence you saw here?” You can imagine a wide range of scenarios where not
    all of this evidence was present at a robbery. For example, a clever thief might
    have picked the lock on your door, stolen your laptop, then locked the door behind
    them and not needed to break a window. Or they might have just smashed the window,
    taken the laptop, and then climbed right back out the window. The evidence we’ve
    seen seems intuitively like it would be pretty common at the scene of a robbery,
    so we’ll say there’s a 3/10 probability that if you were robbed, you would come
    home and find this evidence.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that, even though we’re making a guess in this example,
    we could also do some research to get a better estimate. We could go to the local
    police department and ask for statistics about evidence at crime scenes involving
    robbery, or read through news reports of recent robberies. This would give us
    a more accurate estimate for the likelihood that if you were robbed you would
    see this evidence.
  prefs: []
  type: TYPE_NORMAL
- en: The incredible thing about Bayes’ theorem is that we can use it both for organizing
    our casual beliefs and for working with large data sets of very exact probabilities.
    Even if you don’t think 3/10 is a good estimate, you can always go back to the
    calculations—as we will do—and see how the value changes given a different assumption.
    For example, if you think that the probability of seeing this evidence given a
    robbery is just 3/100, you can easily go back and plug in those numbers instead.
    Bayesian statistics lets people disagree about beliefs in a measurable way. Because
    we are dealing with our beliefs in a quantitative way, you can recalculate everything
    we do in this chapter to see if this different probability has a substantial impact
    on any of the final outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: '***Calculating the Prior***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Next, we need to determine the probability that you would get robbed at all.
    This is our prior. Priors are extremely important, because they allow us to use
    background information to adjust a likelihood. For example, suppose the scene
    described earlier happened on a deserted island where you are the only inhabitant.
    In this case, it would be nearly impossible for you to get robbed (by a human,
    at least). In another example, if you owned a home in a neighborhood with a high
    crime rate, robberies might be a frequent occurrence. For simplicity, let’s set
    our prior for being robbed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0075-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Remember, we can always adjust these figures later given different or additional
    evidence.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have nearly everything we need to calculate the posterior; we just need
    to normalize the data. Before moving on, then, let’s look at the unnormalized
    posterior:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0076-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This value is incredibly small, which is surprising since intuition tells us
    that the probability of your house being robbed given the evidence you observed
    seems very, very high. But we haven’t yet looked at the probability of observing
    our evidence.
  prefs: []
  type: TYPE_NORMAL
- en: '***Normalizing the Data***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'What’s missing from our equation is the probability of the data you observed
    whether or not you were robbed. In our example, this is the probability that you
    observe that your window is broken, the door is open, and your laptop is missing
    *all at once*, regardless of the cause. As of now, our equation looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0076-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The reason the probability in the numerator is so low is that we haven’t normalized
    it with the probability that you would find this strange evidence.
  prefs: []
  type: TYPE_NORMAL
- en: We can see how our posterior changes as we change our *P*(*D*) in [Table 8-1](ch08.xhtml#ch08tab01).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 8-1:** How the *P*(*D*) Affects the Posterior'
  prefs: []
  type: TYPE_NORMAL
- en: '| ***P*(*D*)** | **Posterior** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.050 | 0.006 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.010 | 0.030 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.005 | 0.060 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.001 | 0.300 |'
  prefs: []
  type: TYPE_TB
- en: As the probability of our data decreases, our posterior probability increases.
    This is because as the data we observe becomes increasingly unlikely, a typically
    unlikely explanation does a better job of explaining the event (see [Figure 8-2](ch08.xhtml#ch08fig02)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/08fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-2: As the probability of the data decreases, the posterior probability
    increases.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this extreme example: the only way your friend could become a millionaire
    is if they won the lottery or inherited money from some family member they didn’t
    know existed. Your friend becoming a millionaire is therefore shockingly unlikely.
    However, you find out that your friend *did* become a millionaire. The possibility
    that your friend won the lottery then becomes much more likely, because it is
    one of the only two ways they could have become a millionaire.'
  prefs: []
  type: TYPE_NORMAL
- en: Being robbed is, of course, only one possible explanation for what you observed,
    and there are many more explanations. However, if we don’t know the probability
    of the evidence, we can’t figure out how to normalize all these other possibilities.
    So what is our *P*(*D*)? That’s the tricky part.
  prefs: []
  type: TYPE_NORMAL
- en: The common problem with *P*(*D*) is that it’s very difficult to accurately calculate
    in many real-world cases. With every other part of the formula—even though we
    just guessed at a value for this exercise—we can collect real data to provide
    a more concrete probability. For our prior, *P*(robbed), we might simply look
    at historical crime data and pin down a probability that a given house on your
    street would be robbed any given day. Likewise, we could, theoretically, investigate
    past robberies and come up with a more accurate likelihood for observing the evidence
    you did given a robbery. But how could we ever really even guess at *P*(broken
    window,open front door,missing laptop)?
  prefs: []
  type: TYPE_NORMAL
- en: Instead of researching the probability of the data you observed, we could try
    to calculate the probabilities of all other possible events that could explain
    your observations. Since they must sum to 1, we could work backward and find *P*(*D*).
    But for the case of this particular evidence, there’s a virtually limitless number
    of possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: We’re a bit stuck without *P*(*D*). In [Chapters 6](ch06.xhtml#ch06) and [7](ch07.xhtml#ch07),
    where we calculated the probability that a customer service rep was male and the
    probability of choosing different colored LEGO studs, respectively, we had plenty
    of information about *P*(*D*). This allowed us to come up with an exact probability
    of our belief in our hypothesis given what we observed. Without *P*(*D*) we cannot
    come up with a value for *P*(robbed | broken window,open front door,missing laptop).
    However, we’re not completely lost.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that in some cases we don’t need to explicitly know *P*(*D*),
    because we often just want to *compare* hypotheses. In this example, we’ll compare
    how likely it is that you were robbed with another possible explanation. We can
    do this by looking at the ratio of our unnormalized posterior distributions. Because
    the *P*(*D*) would be a constant, we can safely remove it without changing our
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: So, instead of calculating the *P*(*D*), for the remainder of this chapter we’ll
    develop an alternative hypothesis, calculate its posterior, and then compare it
    to the posterior from our original hypothesis. While this means we can’t come
    up with an exact probability of being robbed as the only possible explanation
    for the evidence you observed, we can still use Bayes’ theorem to play detective
    and investigate other possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: '**Considering Alternative Hypotheses**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s come up with another hypothesis to compare with our original one. Our
    new hypothesis consists of three events:'
  prefs: []
  type: TYPE_NORMAL
- en: A neighborhood kid hit a baseball through the front window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You left your door unlocked.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You forgot that you brought your laptop to work and it’s still there.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’ll refer to each of these explanations simply by its number in our list,
    and refer to them collectively as *H*[2] so that *P*(*H*[2]) = *P*(1,2,3). Now
    we need to solve for the likelihood and prior of this data.
  prefs: []
  type: TYPE_NORMAL
- en: '***The Likelihood for Our Alternative Hypothesis***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recall that, for our likelihood, we want to calculate the probability of what
    you observed given our hypothesis, or *P*(*D* | *H*[2]). Interestingly—and logically,
    as you’ll see—the likelihood for this explanation turns out to be 1: *P*(*D* |
    *H*[2]) = 1'
  prefs: []
  type: TYPE_NORMAL
- en: If all the events in our hypothesis did happen, then your observations of a
    broken window, unlocked door, and missing laptop would be certain.
  prefs: []
  type: TYPE_NORMAL
- en: '***The Prior for Our Alternative Hypothesis***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our prior represents the possibility of all three events happening. This means
    we need to first work out the probability of each of these events and then use
    the product rule to determine the prior. For this example, we’ll assume that each
    of these possible outcomes is conditionally independent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of our hypothesis is that a neighborhood kid hit a baseball
    through the front window. While this is common in movies, I’ve personally never
    heard of it happening. I have known far more people who have been robbed, though,
    so let’s say that a baseball being hit through the window is half as likely as
    the probability of getting robbed we used earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0079-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The second part of our hypothesis is that you left the door unlocked. This
    is fairly common; let’s say this happens about once a month, so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0079-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, let’s look at leaving your laptop at work. While bringing a laptop
    to work and leaving it there might be common, completely forgetting you took it
    in the first place is less common. Maybe this happens about once a year:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0079-03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we’ve given each of these pieces of *H*[2] a probability, we can now
    calculate our prior probability by applying the product rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0079-04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the prior probability of all three events happening is extremely
    low. Now we need a posterior for each of our hypotheses to compare.
  prefs: []
  type: TYPE_NORMAL
- en: '***The Posterior for Our Alternative Hypothesis***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We know that our likelihood, *P*(*D* | *H*[2]), equals 1, so if our second
    hypothesis were to be true, we would be certain to see our evidence. Without a
    prior probability in our second hypothesis, it looks like the posterior probability
    for our new hypothesis will be much stronger than it is for our original hypothesis
    that you were robbed (since we aren’t as likely to see the data even if we were
    robbed). We can now see how the prior radically alters our unnormalized posterior
    probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0079-05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now we want to compare our posterior beliefs as well as the strength of our
    hypotheses with a ratio. You’ll see that we don’t need a *P*(*D*) to do this.
  prefs: []
  type: TYPE_NORMAL
- en: '**Comparing Our Unnormalized Posteriors**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we want to compare the ratio of the two posteriors. A ratio tells us
    how many times more likely one hypothesis is than the other. We’ll define our
    original hypothesis as *H*[1], and the ratio looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0080-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next let’s expand this using Bayes’ theorem for each of these. We’ll write
    Bayes’ theorem as *P*(*H*) × *P*(*D* | *H*) × 1/*P*(*D*) to make the formula easier
    to read in this context:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0080-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that both the numerator and denominator contain 1/*P*(*D*), which means
    we can remove that and maintain the ratio. This is why *P*(*D*) doesn’t matter
    when we compare hypotheses. Now we have a ratio of the unnormalized posteriors.
    Because the posterior tells us how strong our belief is, this ratio of posteriors
    tells us how many times better *H*[1] explains our data than *H*[2] without knowing
    *P*(*D*). Let’s cancel out the *P*(*D*) and plug in our numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0080-03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: What this means is that *H*[1] explains what we observed 6,570 times better
    than *H*[2]. In other words, our analysis shows that our original hypothesis (*H*[1])
    explains our data much, much better than our alternate hypothesis (*H*[2]). This
    also aligns well with our intuition—given the scene you observed, a robbery certainly
    sounds like a more likely assessment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’d like to express this property of the unnormalized posterior mathematically
    to be able to use it for comparison. For that, we use the following version of
    Bayes’ theorem, where the symbol ∝ means “proportional to”:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*H* | *D*) ∝ *P*(*H*) × *P*(*D* | *H*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can read this as: “The posterior—that is, the probability of the hypothesis
    given the data—is *proportional to* the prior probability of *H* multiplied by
    the probability of the data given *H*.”'
  prefs: []
  type: TYPE_NORMAL
- en: This form of Bayes’ theorem is extremely useful whenever we want to compare
    the probability of two ideas but can’t easily calculate *P*(*D*). We cannot come
    up with a meaningful value for the probability of our hypothesis in isolation,
    but we’re still using a version of Bayes’ theorem to compare hypotheses. Comparing
    hypotheses means that we can always see exactly how much stronger one explanation
    of what we’ve observed is than another.
  prefs: []
  type: TYPE_NORMAL
- en: '**Wrapping Up**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This chapter explored how Bayes’ theorem provides a framework for modeling
    our beliefs about the world, given data that we have observed. For Bayesian analysis,
    Bayes’ theorem consists of three major parts: the posterior probability, *P*(*H*
    | *D*); the prior probability, *P*(*H*); and the likelihood, *P*(*D* | *H*).'
  prefs: []
  type: TYPE_NORMAL
- en: The data itself, or *P*(*D*), is notably absent from this list, because we often
    won’t need it to perform our analysis if all we’re worried about is comparing
    beliefs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Try answering the following questions to see if you have a solid understanding
    of the different parts of Bayes’ Theorem. The solutions can be found at *[https://nostarch.com/learnbayes/](https://nostarch.com/learnbayes/)*.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, you might disagree with the original probability assigned to the
    likelihood:![Image](../images/f0081-01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How much does this change our strength in believing *H*[1] over *H*[2]?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How unlikely would you have to believe being robbed is—our prior for *H*[1]—in
    order for the ratio of *H*[1] to *H*[2] to be even?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
