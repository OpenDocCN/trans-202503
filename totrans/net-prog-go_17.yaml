- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Logging and Metrics
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录和指标
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: In an ideal world, our code would be free of bugs from the outset. Our network
    services would exceed our expectations for performance and capacity, and they
    would be robust enough to adapt to unexpected input without our intervention.
    But in the real world, we need to worry about unexpected and potentially malicious
    input, hardware degradation, network outages, and outright bugs in our code.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的世界里，我们的代码从一开始就没有缺陷。我们的网络服务将超出我们对性能和容量的预期，并且足够强大，可以在没有我们干预的情况下适应意外的输入。但在现实世界中，我们需要担心意外的和潜在的恶意输入、硬件退化、网络中断以及代码中的明显bug。
- en: Monitoring our applications, no matter whether they are on premises or in the
    cloud, is vital to providing resilient, functional services to our users. Comprehensive
    logging allows us to receive timely details about errors, anomalies, or other
    actionable events, and metrics give us insight into the current state of our services,
    as well as help us identify bottlenecks. Taken together, logging and metrics allow
    us to manage service issues and focus our development efforts to avoid future
    failures.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们的应用程序是在本地还是在云端，监控它们对提供强健、功能完善的服务至关重要。全面的日志记录让我们能够及时获得关于错误、异常或其他可操作事件的详细信息，而指标则帮助我们了解服务的当前状态，并识别瓶颈。综合来看，日志和指标帮助我们管理服务问题，并集中开发力量避免未来的故障。
- en: You’ve used Go’s `log` and `fmt` packages to give you feedback in previous chapters,
    but this chapter will take a deeper dive into logging and instrumenting your services.
    You will learn how to use log levels to increase or decrease the verbosity of
    your logs and when to use each log level. You’ll learn how to add structure to
    your log entries so software can help you make better sense of log entries and
    zero in on relevant logs. I’ll introduce you to the concept of wide event logging,
    which will help you maintain a handle on the amount of data you log as your services
    scale. You’ll learn techniques for dynamically enabling debug logging and managing
    log file rotation from your code.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，你使用了Go的`log`和`fmt`包来获取反馈，但本章将深入探讨日志记录和为服务添加监控。你将学习如何使用日志级别来增加或减少日志的详细程度，以及何时使用每个日志级别。你将学会如何为日志条目添加结构，这样软件可以帮助你更好地理解日志条目，并聚焦于相关日志。我将向你介绍广泛事件日志的概念，这将帮助你在服务规模扩大时控制日志数据量。你将学习如何从代码中动态启用调试日志记录和管理日志文件的轮换。
- en: This chapter will also introduce you to Go kit’s `metrics` package. Per Go kit’s
    documentation, the `metrics` package “provides a set of uniform interfaces for
    service instrumentation.” You’ll learn how to instrument your services by using
    counters, gauges, and histograms.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还将介绍Go kit的`metrics`包。根据Go kit文档，`metrics`包“提供了一组统一的接口，用于服务的监控”。你将学习如何通过使用计数器、仪表和直方图来为你的服务添加监控。
- en: By the end of this chapter, you should have a handle on how to approach logging,
    how to manage log files to prevent them from consuming too much hard drive space,
    and how to instrument your services to gain insight into their current state.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你应该掌握如何处理日志记录，如何管理日志文件以防止它们占用过多硬盘空间，以及如何为你的服务添加监控，以便深入了解其当前状态。
- en: Event Logging
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事件日志
- en: Logging is hard. Even experienced developers struggle to get it right. It’s
    tough to anticipate what questions you’ll need your logs to answer in the future,
    when your service fails—yet you should resist the urge to log everything just
    in case. You need to strike a balance in order to log the right information to
    answer those questions without overwhelming yourself with irrelevant log lines.
    Overzealous logging may suit you fine in development, where you control the scale
    of testing and overall entropy of your service, but it will quickly degrade your
    ability to find the needle in the haystack when you need to diagnose production
    failures.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录很难。即使是经验丰富的开发者也常常难以做到完美。很难预见到在服务发生故障时，你的日志需要回答哪些问题——然而，你应该抵制记录一切日志以防万一的冲动。你需要找到一个平衡点，既能记录正确的信息来回答这些问题，又不会被无关的日志信息淹没。过度的日志记录在开发阶段可能对你来说没问题，因为你控制了测试的规模和服务的整体熵，但它会在你需要诊断生产环境故障时迅速降低你在信息海洋中找到关键线索的能力。
- en: In addition to figuring out what to log, you need to consider that logging isn’t
    free. It consumes CPU and I/O time your application could otherwise use. A log
    entry added to a busy `for` loop while in development may help you understand
    what your service is doing. But it may become a bottleneck in production, insidiously
    adding latency to your service.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 除了搞清楚要记录什么，你还需要考虑日志记录并非无代价。它会消耗CPU和I/O时间，而这些时间本可以被应用程序用来做其他事情。向一个繁忙的`for`循环中添加日志条目，在开发过程中可能有助于你理解你的服务在做什么。但在生产环境中，它可能成为瓶颈，悄悄地为你的服务增加延迟。
- en: Instead, sampling these log entries, or logging on demand, may provide suitable
    compromises between log output and overhead. You might find it helpful to use
    *wide**event* log entries, which summarize a transaction. For example, a service
    in development may log half a dozen entries about a request, any intermediate
    steps, and a response. In production, a single wide event log entry providing
    these details scales better. You’ll learn more about wide event log entries in
    “Scaling Up with Wide Event Logging” on page 312.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，采样这些日志条目，或按需记录日志，可能提供日志输出和开销之间的适当折衷。你可能会发现使用*宽事件*日志条目很有帮助，它们总结了一个事务。例如，一个正在开发中的服务可能会记录关于请求的半打条目、任何中间步骤和响应。而在生产环境中，单个宽事件日志条目提供这些详细信息的方式更具扩展性。你将在第312页的《通过宽事件日志扩展》一节中了解更多宽事件日志条目的内容。
- en: Lastly, logging is subjective. An anomaly may be inconsequential in my application
    but indicative of a larger issue in your application. Whereas I could ignore the
    anomaly, you’d likely want to know about it. For this reason, it’s best if we
    discuss logging in terms of best practices. These practices are a good baseline
    approach, but you should tailor them to each application.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，日志记录是主观的。一个异常在我的应用程序中可能是微不足道的，但在你的应用程序中却可能表明一个更大的问题。虽然我可以忽略这个异常，但你可能希望知道它。出于这个原因，最好我们在最佳实践的框架下讨论日志记录。这些实践是一个良好的基准方法，但你应该根据每个应用程序量身定制它们。
- en: The log Package
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: log 包
- en: 'You have superficial experience using Go’s `log` package, in earlier chapters,
    for basic logging needs, like timestamping log entries and optionally exiting
    your application with `log.Fatal`. But it has a few more features we have yet
    to explore. These require us to go beyond the package-level logger and instantiate
    our own `*log.Logger` instance. You can do this using the `log.New` function:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你已经有了使用Go的`log`包的基本经验，用于基本的日志记录需求，比如给日志条目加上时间戳，或者选择性地使用`log.Fatal`退出应用程序。但它还有一些我们尚未探讨的功能。这些功能要求我们超越包级日志记录器，实例化我们自己的`*log.Logger`实例。你可以使用`log.New`函数来做到这一点：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `log.New` function accepts an `io.Writer`, a string prefix to use on each
    log line, and a set of flags that modify the logger’s output. Accepting an `io.Writer`
    means the logger can write to anything that satisfies that interface, including
    an in-memory buffer or a network socket.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`log.New`函数接受一个`io.Writer`、一个用于每条日志行的字符串前缀，以及一组修改日志记录器输出的标志。接受`io.Writer`意味着日志记录器可以将日志写入任何满足该接口的对象，包括内存缓冲区或网络套接字。'
- en: The default logger writes its output to `os.Stderr`, standard error. Let’s look
    at an example logger in [Listing 13-1](#listing13-1) that writes to `os.Stdout`,
    standard output.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的日志记录器将输出写入`os.Stderr`，即标准错误。让我们看一个示例日志记录器，位于[Listing 13-1](#listing13-1)，它将输出写入`os.Stdout`，即标准输出。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Listing 13-1: Writing a log entry to standard output (*log_test.go*)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 13-1：将日志条目写入标准输出（*log_test.go*）
- en: You create a new `*log.Logger` instance that writes to standard output 1. The
    logger prefixes each line with the string *example:*2. The flags of the default
    logger are `log.Ldate` and `log.Ltime`, collectively `log.LstdFlags`, which print
    the timestamp of each log entry. Since you want to simplify the output for testing
    purposes when you run the example on the command line, you omit the timestamp
    and configure the logger to write the source code filename and line of each log
    entry 3. The `l.Print` function on line 12 of the *log_test.go* file results in
    the output of those values 4. This behavior can help with development and debugging,
    allowing you to zero in on the exact file and line of an interesting log entry.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你创建了一个新的`*log.Logger`实例，将其写入标准输出1。日志记录器将每一行前缀加上字符串*example:*2。默认日志记录器的标志是`log.Ldate`和`log.Ltime`，合起来是`log.LstdFlags`，它们打印每条日志条目的时间戳。由于你希望简化输出以便在命令行运行示例时进行测试，你省略了时间戳，并配置日志记录器以写入每条日志条目的源代码文件名和行号3。*log_test.go*文件中第12行的`l.Print`函数输出了这些值4。这种行为有助于开发和调试，使你能够精确定位到感兴趣的日志条目的文件和行号。
- en: Recognizing that the logger accepts an `io.Writer`, you may realize this allows
    you to use multiple writers, such as a log file and standard output or an in-memory
    ring buffer and a centralized logging server over a network. Unfortunately, the
    `io.MultiWriter` is not ideal for use in logging. An `io.MultiWriter` writes to
    each of its writers in sequence, aborting if it receives an error from any `Write`
    call. This means that if you configure your `io.MultiWriter` to write to a log
    file and standard output in that order, standard output will never receive the
    log entry if an error occurred when writing to the log file.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会意识到，日志记录器接受`io.Writer`，这意味着你可以使用多个写入器，比如日志文件和标准输出，或者内存环形缓冲区和网络上的集中式日志服务器。不幸的是，`io.MultiWriter`并不适合日志记录中使用。`io.MultiWriter`按顺序写入每个写入器，如果它从任何`Write`调用收到错误，就会中止。这意味着，如果你配置`io.MultiWriter`按这个顺序写入日志文件和标准输出，那么如果在写入日志文件时发生错误，标准输出将永远不会收到日志条目。
- en: Fear not. It’s an easy problem to solve. Let’s create our own `io.MultiWriter`
    implementation, starting in [Listing 13-2](#listing13-2), that sustains writes
    across its writers and accumulates any errors it encounters.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心，这是一个容易解决的问题。让我们从[示例 13-2](#listing13-2)开始，创建我们自己的`io.MultiWriter`实现，使其能够跨多个写入器维持写入并累积遇到的任何错误。
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Listing 13-2: A multiwriter that sustains writing even after receiving an error
    (*writer.go*)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 13-2：一个即使遇到错误后仍能持续写入的多写入器 (*writer.go*)
- en: As with `io.MultiWriter`, you’ll use a struct that contains a slice of `io.Writer`
    instances for your sustained multiwriter. Your multiwriter implements the `io.Writer`
    interface 1, so you can pass it into your logger. It calls each writer’s `Write`
    method 2, accumulating any errors with the help of Uber’s `multierr` package 3,
    before ultimately returning the total written bytes and cumulative error.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 和`io.MultiWriter`一样，你将使用一个包含`io.Writer`实例切片的结构体作为持续多写入器。你的多写入器实现了`io.Writer`接口1，因此可以将其传递给你的日志记录器。它调用每个写入器的`Write`方法2，在Uber的`multierr`包3的帮助下累积任何错误，最终返回总共写入的字节数和累积的错误。
- en: '[Listing 13-3](#listing13-3) adds a function to initialize a new sustained
    multiwriter from one or more writers.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 13-3](#listing13-3) 添加了一个函数，用于从一个或多个写入器初始化一个新的持续多写入器。'
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Listing 13-3: Creating a sustained multiwriter (*writer.go*)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 13-3：创建一个持续多写入器 (*writer.go*)
- en: First, you instantiate a new `*sustainedMultiWriter`, initialize its writers
    slice 1, and cap it to the expected length of writers. You then loop through the
    given writers and append them to the slice 4. If a given writer is itself a `*sustainedMultiWriter`2,
    you instead append its writers 3. Finally, you return the pointer to the initialized
    `sustainedMultiWriter`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你实例化一个新的`*sustainedMultiWriter`，初始化其写入器切片1，并将其限制为写入器的预期长度。然后，你遍历给定的写入器并将它们追加到切片中4。如果某个写入器本身是`*sustainedMultiWriter`2，你将改为追加它的写入器3。最后，你返回初始化的`sustainedMultiWriter`的指针。
- en: You can now put your sustained multiwriter to good use in [Listing 13-4](#listing13-4).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以在[示例 13-4](#listing13-4)中充分利用你的持续多写入器了。
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Listing 13-4: Logging simultaneously to a log file and standard output (*log_test.go*)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 13-4：同时写入日志文件和标准输出 (*log_test.go*)
- en: You create a new sustained multiwriter 1, writing to standard output, and a
    `bytes.Buffer` meant to act as a log file in this example. Next, you create a
    new logger using your sustained multiwriter, the prefix *`example`:*, and two
    flags 2 to modify the logger’s behavior. The addition of the `log.Lmsgprefix`
    flag (first available in Go 1.14) tells the logger to locate the prefix just before
    the log message. You can see the effect this has on the log entries in the example
    output. When you run this example, you see that the logger writes the log entry
    to the sustained multiwriter, which in turn writes the log entry to both standard
    output and the log file.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这个示例中创建了一个新的持续多写入器1，写入标准输出，并使用一个`bytes.Buffer`作为模拟日志文件。接着，你使用持续多写入器、前缀*`example`:*和两个标志2来创建一个新的日志记录器，以修改日志记录器的行为。`log.Lmsgprefix`标志的添加（在Go
    1.14中首次引入）告诉日志记录器在日志消息之前查找前缀。你可以看到这对示例输出中日志条目的影响。当你运行这个示例时，你会看到日志记录器将日志条目写入持续多写入器，而持续多写入器又将日志条目写入标准输出和日志文件。
- en: Leveled Log Entries
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分层日志条目
- en: I wrote earlier in the chapter that verbose logging may be inefficient in production
    and can overwhelm you with the sheer number of log entries as your service scales
    up. One way to avoid this is by instituting *logging levels*, which assign a priority
    to each kind of event, enabling you to always log high-priority errors but conditionally
    log low-priority entries more suited for debugging and development purposes. For
    example, you’d always want to know if your service is unable to connect to its
    database, but you may care to log only details about individual connections while
    in development or when diagnosing a failure.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本章早些时候提到过，冗长的日志记录在生产环境中可能效率低下，并且随着服务的扩展，日志条目的数量可能会让你不堪重负。避免这种情况的一种方法是实施*日志级别*，为每种事件分配优先级，使你能够始终记录高优先级的错误日志，但根据条件记录更适合调试和开发的低优先级条目。例如，你总是希望知道服务无法连接到数据库，但在开发或诊断故障时，你可能只关心记录关于单个连接的详细信息。
- en: I recommend you create just a few log levels to begin with. In my experience,
    you can address most use cases with just an *error* level and a *debug* level,
    maybe even an *info* level on occasion. Error log entries should accompany some
    sort of alert, since these entries indicate a condition that needs your attention.
    Info log entries typically log non-error information. For example, it may be appropriate
    for your use case to log a successful database connection or to add a log entry
    when a listener is ready for incoming connections on a network socket. Debug log
    entries should be verbose and serve to help you diagnose failures, as well as
    aid development by helping you reason about the workflow.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你开始时只创建几个日志级别。根据我的经验，你可以通过只设置一个*错误*级别和一个*调试*级别来处理大多数用例，偶尔也可以设置一个*信息*级别。错误日志条目应该伴随某种警报，因为这些条目表示需要你注意的条件。信息日志条目通常记录非错误信息。例如，记录成功的数据库连接，或在网络套接字上监听器准备好接受连接时添加日志条目，可能适合你的用例。调试日志条目应该详细，并帮助你诊断故障，同时通过帮助你理解工作流程来支持开发。
- en: 'Go’s ecosystem offers several logging packages, most of which support numerous
    log levels. Although Go’s `log` package does not have inherent support for leveled
    log entries, you can add similar functionality by creating separate loggers for
    each log level you need. [Listing 13-5](#listing13-5) does this: it writes log
    entries to a log file, but it also writes debug logs to standard output.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Go 的生态系统提供了几种日志包，其中大多数支持多个日志级别。尽管 Go 的`log`包本身不支持分级日志条目，但你可以通过为每个所需的日志级别创建单独的日志记录器来添加类似的功能。[列表13-5](#listing13-5)就实现了这一点：它将日志条目写入日志文件，但也将调试日志写入标准输出。
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Listing 13-5: Writing debug entries to standard output and errors to both the
    log file and standard output (*log_test.go*)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13-5：将调试条目写入标准输出，并将错误同时写入日志文件和标准输出（*log_test.go*）
- en: First, you create a debug logger that writes to standard output and uses the
    `DEBUG:` prefix 1. Next, you create a `*bytes.Buffer` to masquerade as a log file
    for this example and instantiate a sustained multiwriter. The sustained multiwriter
    writes to both the log file and the debug logger’s `io.Writer`2. Then, you create
    an error logger that writes to the sustained multiwriter by using the prefix `ERROR:`3
    to differentiate its log entries from the debug logger. Finally, you use each
    logger and verify that they output what you expect. Standard output should display
    log entries from both loggers, whereas the log file should contain only error
    log entries.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个调试日志记录器，它写入标准输出，并使用`DEBUG:`前缀1。接下来，创建一个`*bytes.Buffer`，作为日志文件的替代，实例化一个持续的多重写入器。持续的多重写入器同时写入日志文件和调试日志记录器的`io.Writer`2。然后，创建一个错误日志记录器，它通过使用`ERROR:`前缀3写入持续的多重写入器，从而将其日志条目与调试日志区分开来。最后，使用每个日志记录器并验证它们的输出，确保它们的输出符合预期。标准输出应显示两个日志记录器的日志条目，而日志文件应仅包含错误日志条目。
- en: As an exercise, figure out how to make the debug logger conditional without
    wrapping its `Print` call in a conditional. If you need a hint, you’ll find a
    suitable writer in the `io/ioutil` package that will let you discard its output.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，找出如何使调试日志记录器具有条件性，而不是将其`Print`调用包装在条件语句中。如果你需要提示，可以在`io/ioutil`包中找到一个合适的写入器，它允许你丢弃输出。
- en: This section is meant to demonstrate additional uses of the `log` package beyond
    what you’ve used so far in this book. Although it’s possible to use this technique
    to log at different levels, you’d be better served by a logger with inherent support
    for log levels, like the Zap logger described in the next section.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本节旨在展示 `log` 包的额外用途，超出了本书中到目前为止所使用的内容。虽然可以使用此技术在不同级别进行日志记录，但使用本节后面描述的具有内置日志级别支持的日志记录器（如
    Zap 日志记录器）会更有帮助。
- en: Structured Logging
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构化日志记录
- en: The log entries made by the code you’ve written so far are meant for human consumption.
    They are easy for you to read, since each log entry is little more than a message.
    This means that finding log lines relevant to an issue involves liberal use of
    the `grep` command or, at worst, manually skimming log entries. But this could
    become more challenging if the number of log entries increases. You may find yourself
    looking for a needle in a haystack. Remember, logging is useful only if you can
    quickly find the information you need.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你目前所编写的代码生成的日志条目是供人类阅读的。它们对你来说很容易阅读，因为每个日志条目不过是一个消息。这意味着，找到与某个问题相关的日志行通常需要广泛使用
    `grep` 命令，或者在最坏的情况下，手动浏览日志条目。但如果日志条目的数量增加，这可能会变得更加困难。你可能会发现自己在大海捞针。记住，日志记录只有在你能快速找到所需信息时才有用。
- en: A common approach to solving this problem is to add metadata to your log entries
    and then parse the metadata with software to help you organize them. This type
    of logging is called *structured logging*. Creating structured log entries involves
    adding key-value pairs to each log entry. In these, you may include the time at
    which you logged the entry, the part of your application that made the log entry,
    the log level, the hostname or IP address of the node that created the log entry,
    and other bits of metadata that you can use for indexing and filtering. Most structured
    loggers encode log entries as JSON before writing them to log files or shipping
    them to centralized logging servers. Structured logging makes the whole process
    of collecting logs in a centralized server easy, since the additional metadata
    associated with each log entry allows the server to organize and collate log entries
    across services. Once they’re indexed, you can query the log server for specific
    log entries to better find timely answers to your questions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的常见方法是向日志条目中添加元数据，然后通过软件解析这些元数据，以帮助你组织它们。这种类型的日志记录称为*结构化日志记录*。创建结构化日志条目需要向每个日志条目添加键值对。在这些条目中，你可以包括日志条目记录的时间、生成日志条目的应用程序部分、日志级别、生成日志条目的节点的主机名或
    IP 地址，以及其他你可以用于索引和筛选的元数据。大多数结构化日志记录器会在将日志条目写入日志文件或发送到集中式日志服务器之前，将其编码为 JSON。结构化日志记录使得将日志收集到集中式服务器的整个过程变得简单，因为与每个日志条目相关的附加元数据使得服务器能够跨服务组织和整理日志条目。一旦它们被索引，你就可以查询日志服务器以找到特定的日志条目，从而更好地找到问题的及时答案。
- en: Using the Zap Logger
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 Zap 日志记录器
- en: Discussing specific centralized logging solutions is beyond the scope of this
    book. If you’re interested in learning more, I suggest you initially investigate
    Elasticsearch or Apache Solr. Instead, this section focuses on implementing the
    logger itself. You’ll use the Zap logger from Uber, found at [https://pkg.go.dev/go.uber.org/zap/](https://pkg.go.dev/go.uber.org/zap/),
    which allows you to integrate log file rotation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论具体的集中式日志解决方案超出了本书的范围。如果你有兴趣了解更多，我建议你首先研究 Elasticsearch 或 Apache Solr。不过，本节将重点介绍如何实现日志记录器本身。你将使用来自
    Uber 的 Zap 日志记录器，网址是[https://pkg.go.dev/go.uber.org/zap/](https://pkg.go.dev/go.uber.org/zap/)，它允许你集成日志文件轮转功能。
- en: '*Log file rotation* is the process of closing the current log file, renaming
    it, and then opening a new log file after the current log file reaches a specific
    age or size threshold. Rotating log files is a good practice to prevent them from
    filling up your available hard drive space. Plus, searching through smaller, date-delimited
    log files is more efficient than searching through a single, monolithic log file.
    For example, you may want to rotate your log files every week and keep only eight
    weeks’ worth of rotated log files. If you wanted to look at log entries for an
    event that occurred last week, you could limit your search to a single log file.
    Also, you can compress the rotated log files to further save hard drive space.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*日志文件轮换*是指在当前日志文件达到特定的年龄或大小阈值后，关闭当前日志文件、重命名它，并打开一个新的日志文件。轮换日志文件是一个好习惯，可以防止它们填满你的硬盘空间。而且，搜索较小的、按日期分隔的日志文件，比搜索一个庞大的日志文件更高效。例如，你可能希望每周轮换一次日志文件，并只保留八周的轮换日志文件。如果你想查看上周发生的事件的日志条目，你可以将搜索范围限制在一个日志文件中。此外，你还可以压缩轮换的日志文件，进一步节省硬盘空间。'
- en: I’ve used other structured loggers on large projects, and in my experience,
    Zap causes the least overhead; I can use it in busy bits of code without a noticeable
    performance hit, unlike other heavyweight structured loggers. But your mileage
    may vary, so I encourage you to find what works best for you. You can apply the
    structured logging principles and log file management techniques described here
    to other structured loggers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我在大型项目中使用过其他结构化日志记录器，根据我的经验，Zap造成的开销最小；我可以在繁忙的代码部分使用它而不会显著影响性能，和其他重量级的结构化日志记录器不同。但是你的使用情况可能会有所不同，因此我鼓励你找到最适合自己的工具。你可以将这里描述的结构化日志记录原理和日志文件管理技术应用到其他结构化日志记录器上。
- en: 'The Zap logger includes `zap.Core` and its options. The `zap.Core` has three
    components: a log-level threshold, an output, and an encoder. The *log-level threshold*
    sets the minimum log level that Zap will log; Zap will simply ignore any log entry
    below that level, allowing you to leave debug logging in your code and configure
    Zap to conditionally ignore it. Zap’s *output* is a `zapcore.WriteSyncer`, which
    is an `io.Writer` with an additional `Sync` method. Zap can write log entries
    to any object that implements this interface. And the *encoder* can encode the
    log entry before writing it to the output.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Zap日志记录器包含`zap.Core`及其选项。`zap.Core`有三个组成部分：日志级别阈值、输出和编码器。*日志级别阈值*设置Zap记录日志的最低级别；Zap会忽略低于该级别的日志条目，这样你可以在代码中保留调试日志，并配置Zap有条件地忽略它。Zap的*输出*是一个`zapcore.WriteSyncer`，它是一个带有额外`Sync`方法的`io.Writer`。Zap可以将日志条目写入任何实现该接口的对象。而*编码器*则可以在将日志条目写入输出之前对其进行编码。
- en: Writing the Encoder
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 编写编码器
- en: Although Zap provides a few helper functions, such as `zap.NewProduction` or
    `zap.NewDevelopment`, to quickly create production and development loggers, you’ll
    create one from scratch, starting with the encoder configuration in [Listing 13-6](#listing13-6).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Zap提供了一些辅助函数，比如`zap.NewProduction`或`zap.NewDevelopment`，可以快速创建生产和开发环境的日志记录器，但你将从头开始创建一个日志记录器，首先从[清单13-6](#listing13-6)中的编码器配置开始。
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Listing 13-6: The encoder configuration for your Zap logger (*zap_test.go*)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 清单13-6：你的Zap日志记录器的编码器配置（*zap_test.go*）
- en: The encoder configuration is independent of the encoder itself in that you can
    use the same encoder configuration no matter whether you’re passing it to a JSON
    encoder or a console encoder. The encoder will use your configuration to dictate
    its output format. Here, your encoder configuration dictates that the encoder
    use the key `msg` 1 for the log message and the key `name`2 for the logger’s name
    in the log entry. Likewise, the encoder configuration tells the encoder to use
    the key `level` for the logging level and encode the level name using all lowercase
    characters 3. If the logger is configured to add caller details, you want the
    encoder to associate these details with the `caller` key and encode the details
    in an abbreviated format 4.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器配置与编码器本身是独立的，你可以使用相同的编码器配置，无论是将它传递给JSON编码器还是控制台编码器。编码器将使用你的配置来决定其输出格式。在这里，你的编码器配置决定了编码器使用`msg`1作为日志消息的键，使用`name`2作为日志条目中的日志记录器名称的键。同样，编码器配置要求编码器使用`level`作为日志级别的键，并使用全小写字符3来编码级别名称。如果日志记录器被配置为添加调用者详细信息，你希望编码器将这些详细信息与`caller`键关联，并以简化格式4对其进行编码。
- en: Since you need to keep the output of the following examples consistent, you’ll
    omit the `time` key 5 so it won’t show up in the output. In practice, you’d want
    to uncomment these two fields.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您需要保持以下示例的输出一致，因此将省略 `time` 键 5，以避免它出现在输出中。实际使用时，您应取消注释这两个字段。
- en: Creating the Logger and Its Options
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建日志记录器及其选项
- en: Now that you’ve defined the encoder configuration, let’s use it in [Listing
    13-7](#listing13-7) by instantiating a Zap logger.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经定义了编码器配置，让我们通过实例化一个 Zap 日志记录器在 [示例 13-7](#listing13-7) 中使用它。
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Listing 13-7: Instantiating a new logger from the encoder configuration and
    logging to JSON (*zap_test.go*)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 13-7：根据编码器配置实例化一个新的日志记录器并记录到 JSON 文件中（*zap_test.go*）
- en: The `zap.New` function accepts a `zap.Core`1 and zero or more `zap.Options`.
    In this example, you’re passing the `zap.AddCaller` option 5, which instructs
    the logger to include the caller information in each log entry, and a field 6
    named `version` that inserts the runtime version in each log entry.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`zap.New` 函数接受一个 `zap.Core`1 和零个或多个 `zap.Options`。在此示例中，您传递了 `zap.AddCaller`
    选项 5，指示日志记录器在每个日志条目中包含调用者信息，并且包含一个名为 `version` 的字段 6，该字段在每个日志条目中插入运行时版本。'
- en: The `zap.Core` consists of a JSON encoder using your encoder configuration 2,
    a `zapcore.WriteSyncer`3, and the logging threshold 4. If the `zapcore.WriteSyncer`
    isn’t safe for concurrent use, you can use `zapcore.Lock` to make it concurrency
    safe, as in this example.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`zap.Core` 包含一个使用您编码器配置的 JSON 编码器 2，一个 `zapcore.WriteSyncer`3 和日志记录阈值 4。如果
    `zapcore.WriteSyncer` 不支持并发使用，您可以使用 `zapcore.Lock` 来使其支持并发，如此示例所示。'
- en: 'The Zap logger includes seven log levels, in increasing severity: `DebugLevel`,
    `InfoLevel`, `WarnLevel`, `ErrorLevel`, `DPanicLevel`, `PanicLevel`, and `FatalLevel`.
    The `InfoLevel` is the default. `DPanicLevel` and `PanicLevel` entries will cause
    Zap to log the entry and then panic. An entry logged at the `FatalLevel` will
    cause Zap to call `os.Exit(1)` after writing the log entry. Since your logger
    is using `DebugLevel`, it will log all entries.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Zap 日志记录器包括七个日志级别，按严重程度递增：`DebugLevel`、`InfoLevel`、`WarnLevel`、`ErrorLevel`、`DPanicLevel`、`PanicLevel`
    和 `FatalLevel`。默认级别是 `InfoLevel`。`DPanicLevel` 和 `PanicLevel` 级别的条目会导致 Zap 记录该条目并引发
    panic。在 `FatalLevel` 级别记录的条目会导致 Zap 在写入日志条目后调用 `os.Exit(1)`。由于您的日志记录器使用的是 `DebugLevel`，它将记录所有条目。
- en: I recommend you restrict the use of `DPanicLevel` and `PanicLevel` to development
    and `FatalLevel` to production, and only then for catastrophic startup errors,
    such as a failure to connect to the database. Otherwise, you’re asking for trouble.
    As mentioned earlier, you can get a lot of mileage out of `DebugLevel`, `ErrorLevel`,
    and on occasion, `InfoLevel`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议您将 `DPanicLevel` 和 `PanicLevel` 限制在开发环境中使用，将 `FatalLevel` 限制在生产环境中，仅用于灾难性启动错误，例如无法连接到数据库。否则，您可能会遇到麻烦。如前所述，您可以充分利用
    `DebugLevel`、`ErrorLevel`，偶尔使用 `InfoLevel`。
- en: Before you start using the logger, you want to make sure you defer a call to
    its `Sync` method 7 to ensure all buffered data is written to the output.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用日志记录器之前，您需要确保调用其 `Sync` 方法 7 来确保所有缓存的数据都被写入输出。
- en: You can also assign the logger a name by calling its `Named` method 8 and using
    the returned logger. By default, a logger has no name. A named logger will include
    a name key in the log entry, provided you defined one in the encoder configuration.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过调用其 `Named` 方法 8 并使用返回的日志记录器来为日志记录器指定名称。默认情况下，日志记录器没有名称。命名日志记录器将会在日志条目中包含一个名称键，前提是您在编码器配置中定义了一个名称。
- en: The log entries 9 now include metadata around the log message, so much so that
    the log line output exceeds the width of this book. It’s also important to mention
    that the Go version a in the example output is dependent on the version of Go
    you’re using to test this example. Although you’re encoding each log entry in
    JSON, you can still read the additional metadata you’re including in the logs.
    You could ingest this JSON into something like Elasticsearch and run queries on
    it, letting Elasticsearch do the heavy lifting of returning only those log lines
    that are relevant to your query.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，日志条目 9 包含了关于日志消息的元数据，以至于日志行的输出超出了本书的宽度。还需要提到的是，示例输出中的 Go 版本 a 取决于您用来测试此示例的
    Go 版本。尽管您正在以 JSON 格式编码每个日志条目，您仍然可以读取您在日志中包含的附加元数据。您可以将这些 JSON 导入到像 Elasticsearch
    这样的工具中，并在其上执行查询，让 Elasticsearch 处理返回仅与查询相关的日志行的重担。
- en: Using the Console Encoder
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用控制台编码器
- en: The preceding example included a bunch of functionality in relatively little
    code. Let’s instead assume you want to log something a bit more human-readable,
    yet that has structure. Zap includes a console encoder that’s essentially a drop-in
    replacement for its JSON encoder. [Listing 13-8](#listing13-8) uses the console
    encoder to write structured log entries to standard output.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的例子在相对较少的代码中包含了许多功能。现在假设你想记录一些更易于人类阅读的内容，但又具有结构化的内容。Zap 包含一个控制台编码器，基本上是其 JSON
    编码器的直接替代品。[列表 13-8](#listing13-8) 使用控制台编码器将结构化日志条目写入标准输出。
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Listing 13-8: Writing structured logs using console encoding (*zap_test.go*)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13-8：使用控制台编码器编写结构化日志（*zap_test.go*）
- en: The console encoder 1 uses tabs to separate fields. It takes instruction from
    your encoder configuration to determine which fields to include and how to format
    each.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台编码器 1 使用制表符分隔字段。它根据你的编码器配置获取指令，确定要包含哪些字段，以及如何格式化每个字段。
- en: Notice you don’t pass the `zap.AddCaller` and `zap.Fields` options to the logger
    in this example. As a result, the log entries won’t have `caller` and `version`
    fields. Log entries will include the `caller` field only if the logger has the
    `zap.AddCaller` option and the encoder configuration defines its `CallerKey`,
    as in [Listing 13-6](#listing13-6).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这个例子中，你没有将 `zap.AddCaller` 和 `zap.Fields` 选项传递给日志记录器。因此，日志条目将没有 `caller`
    和 `version` 字段。只有当日志记录器具有 `zap.AddCaller` 选项，并且编码器配置定义了 `CallerKey` 时，日志条目才会包含
    `caller` 字段，正如[列表 13-6](#listing13-6)所示。
- en: You name the logger 3 and write three log entries, each with a different log
    level. Since the logger’s threshold is the `info` level 2, the debug log entry
    4 does not appear in the output because `debug` is below the `info` threshold.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你命名日志记录器为 3，并写入三个日志条目，每个条目具有不同的日志级别。由于日志记录器的阈值是`info`级别 2，因此调试日志条目 4 不会出现在输出中，因为
    `debug` 低于 `info` 阈值。
- en: The output 5 lacks key names but includes the field values delimited by a tab
    character. Although not obvious in print, there’s a tab character between the
    log level, the log name, and the log message. If you type this into your editor,
    be mindful to add tab characters between those elements lest the example fail
    when you run it.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 5 缺少键名，但包括由制表符字符分隔的字段值。虽然在打印时不明显，但日志级别、日志名称和日志消息之间有一个制表符字符。如果你在编辑器中输入这个，记得在这些元素之间添加制表符字符，否则运行时示例可能会失败。
- en: Logging with Different Outputs and Encodings
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用不同的输出和编码进行日志记录
- en: Zap includes useful functions that allow you to concurrently log to different
    outputs, using different encodings, at different log levels. [Listing 13-9](#listing13-9)
    creates a logger that writes JSON to a log file and console encoding to standard
    output. The logger writes only the debug log entries to the console.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Zap 包含有用的函数，允许你并行地记录到不同的输出，使用不同的编码，在不同的日志级别上。[列表 13-9](#listing13-9) 创建一个日志记录器，将
    JSON 写入日志文件，并将控制台编码写入标准输出。该日志记录器仅将调试日志条目写入控制台。
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Listing 13-9: Using `*bytes.Buffer` as the log output and logging JSON to it
    (*zap_test.go*)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13-9：使用 `*bytes.Buffer` 作为日志输出并将 JSON 写入其中（*zap_test.go*）
- en: You’re using `*bytes.Buffer`1 to act as a mock log file. The only problem with
    this is that `*bytes.Buffer` does not have a `Sync` method and does not implement
    the `zapcore.WriteSyncer` interface. Thankfully, Zap includes a helper function
    named `zapcore.AddSync`2 that intelligently adds a no-op `Sync` method to an `io.Writer`.
    Aside from the use of this function, the rest of the logger implementation should
    be familiar to you. It’s logging JSON to the log file and excluding any log entries
    below the `info` level. As a result, the first log entry 3 should not appear in
    the log file at all.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在使用 `*bytes.Buffer` 1 来充当模拟日志文件。唯一的问题是 `*bytes.Buffer` 没有 `Sync` 方法，并且不实现
    `zapcore.WriteSyncer` 接口。幸运的是，Zap 包含一个名为 `zapcore.AddSync` 2 的辅助函数，它会智能地向 `io.Writer`
    添加一个无操作的 `Sync` 方法。除了使用这个函数之外，日志记录器的其余实现应该对你来说是熟悉的。它将 JSON 日志记录到日志文件，并排除任何低于 `info`
    级别的日志条目。因此，第一个日志条目 3 应该根本不出现在日志文件中。
- en: Now that you have a logger writing JSON to a log file, let’s experiment with
    Zap and create a new logger in [Listing 13-10](#listing13-10) that can simultaneously
    write JSON log entries to a log file and console log entries to standard output.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有了一个将 JSON 写入日志文件的日志记录器，让我们使用 Zap 进行实验，并在[列表 13-10](#listing13-10)中创建一个新的日志记录器，可以同时将
    JSON 日志条目写入日志文件，并将控制台日志条目写入标准输出。
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Listing 13-10: Extending the logger to log to multiple outputs (*zap_test.go*)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13-10：扩展日志记录器以记录到多个输出（*zap_test.go*）
- en: Zap’s `WithOptions` method 1 clones the existing logger and configures the clone
    with the given options. You can use the `zap.WrapCore` function 2 to modify the
    underlying `zap.Core` of the cloned logger. To mix things up, you make a copy
    of the encoder configuration and tweak it to instruct the encoder to output the
    level using all capital letters 3. Lastly, you use the `zapcore.NewTee` function,
    which is like the `io.MultiWriter` function, to return a `zap.Core` that writes
    to multiple cores 4. In this example, you’re passing in the existing core and
    a new core 5 that writes `debug`-level log entries to standard output.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Zap 的 `WithOptions` 方法 1 克隆现有的日志系统，并使用给定的选项配置该克隆。你可以使用 `zap.WrapCore` 函数 2 来修改克隆日志系统的底层
    `zap.Core`。为了有所不同，你会复制编码器配置并调整它，指示编码器以全大写字母输出日志级别 3。最后，你使用 `zapcore.NewTee` 函数，它类似于
    `io.MultiWriter` 函数，用于返回一个将日志写入多个核心的 `zap.Core` 4。在这个示例中，你传入了现有的核心和一个新的核心 5，后者将
    `debug` 级别的日志条目写入标准输出。
- en: When you use the cloned logger, both the log file and standard output receive
    any log entry at the `info` level or above, whereas only standard output receives
    debugging log entries 6.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用克隆的日志系统时，日志文件和标准输出都会接收 `info` 级别及以上的日志条目，而只有标准输出接收调试日志条目 6。
- en: Sampling Log Entries
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 抽样日志条目
- en: One of my warnings to you with regard to logging is to consider how it impacts
    your application from a CPU and I/O perspective. You don’t want logging to become
    your application’s bottleneck. This normally means taking special care when logging
    in the busy parts of your application.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 关于日志记录，我给你的一个警告是要考虑它如何影响你的应用程序的 CPU 和 I/O 性能。你不希望日志记录成为你应用程序的瓶颈。这通常意味着在应用程序繁忙部分进行日志记录时要特别小心。
- en: One method to mitigate the logging overhead in critical code paths, such as
    a loop, is to sample log entries. It may not be necessary to log each entry, especially
    if your logger is outputting many duplicate log entries. Instead, try logging
    every *n*th occurrence of a duplicate entry.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在关键代码路径（如循环）中，减少日志开销的一种方法是对日志条目进行抽样。可能并不需要记录每个条目，尤其是当你的日志系统输出了许多重复的日志条目时。相反，尝试记录每个重复条目的第*n*次出现。
- en: Conveniently, Zap has a logger that does just that. [Listing 13-11](#listing13-11)
    creates a logger that will constrain its CPU and I/O overhead by logging a subset
    of log entries.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 方便的是，Zap 提供了一个可以做到这一点的日志系统。[示例 13-11](#listing13-11) 创建了一个日志系统，通过记录一部分日志条目来限制其
    CPU 和 I/O 开销。
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Listing 13-11: Logging a subset of log entries to limit CPU and I/O overhead
    (*zap_test.go*)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 13-11：记录部分日志条目以限制 CPU 和 I/O 开销 (*zap_test.go*)
- en: 'The `NewSamplerWithOptions` function 1 wraps `zap.Core` with sampling functionality.
    It requires three additional arguments: a sampling interval 2, the number of initial
    duplicate log entries to record 3, and an integer 4 representing the *n*th duplicate
    log entry to record after that point. In this example, you are logging the first
    log entry, and then every third duplicate log entry that the logger receives in
    a one-second interval. Once the interval elapses, the logger starts over and logs
    the first entry, then every third duplicate for the remainder of the one-second
    interval.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`NewSamplerWithOptions` 函数 1 用抽样功能包装了 `zap.Core`。它需要三个额外的参数：一个抽样间隔 2，记录的初始重复日志条目数量
    3，以及一个整数 4，表示在此之后要记录的第*n*个重复日志条目。在这个示例中，你首先记录第一个日志条目，然后记录在一秒钟内，日志系统接收到的每第三个重复日志条目。间隔时间结束后，日志系统重新开始，记录第一个条目，然后记录余下时间内的每第三个重复条目。'
- en: Let’s look at this in action. You make 10 iterations around a loop. Each iteration
    logs both the counter 6 and a generic debug message 7, which stays the same for
    each iteration. On the sixth iteration, the example sleeps for one second 5 to
    ensure that the sample logger starts logging anew during the next one-second interval.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下实际操作。你在循环中执行 10 次迭代。每次迭代都会记录计数器 6 和一个通用的调试信息 7，后者在每次迭代中保持不变。在第六次迭代时，示例会休眠一秒钟
    5，以确保样本日志系统在接下来的一个秒钟间隔中重新开始记录。
- en: Examining the output 8, you see that the debug message prints during the first
    iteration and not again until the logger encounters the third duplicate debug
    message during the fourth loop iteration. But on the sixth iteration, the example
    sleeps, and the sample logger ticks over to the next one-second interval, starting
    the logging over. It logs the first debug message of the interval in the sixth
    loop iteration and the third duplicate debug message in the ninth iteration of
    the loop.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 检查输出 8，您会看到调试消息在第一次迭代时打印出来，直到日志记录器在第四次循环迭代中遇到第三个重复的调试消息才会再次打印。但在第六次迭代时，示例进入休眠，示例日志记录器切换到下一个一秒钟的时间间隔，重新开始记录。在第六次循环迭代中，它记录了该时间间隔的第一个调试消息，而在第九次循环迭代中记录了第三个重复的调试消息。
- en: Granted, this is a contrived example, but one that illustrates how to use this
    log-sampling technique as a compromise in CPU- and I/O-sensitive portions of your
    code. One place this technique may be applicable is when sending work to worker
    goroutines. Although you may send work as fast as the workers can handle it, you
    might want periodic updates on each worker’s progress without having to incur
    too much logging overhead. The sample logger allows you to temper the log output
    and strike a balance between timely updates and minimal overhead.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 诚然，这是一个人为的示例，但它展示了如何使用这种日志采样技术作为一种折衷方法，适用于CPU和I/O敏感的代码部分。这个技术可以应用于将工作分配给工作goroutine的场景。尽管您可以将工作尽可能快速地发送给工作goroutine处理，但您可能希望定期更新每个工作goroutine的进度，而不需要付出过多的日志记录开销。示例日志记录器可以让您调节日志输出，找到及时更新与最小开销之间的平衡。
- en: Performing On-Demand Debug Logging
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 执行按需调试日志记录
- en: If debug logging introduces an unacceptable burden on your application under
    normal operation, or if the sheer amount of debug log data overwhelms your available
    storage space, you might want the ability to enable debug logging on demand. One
    technique is to use a semaphore file to enable debug logging. A *semaphore file*
    is an empty file whose existence is meant as a signal to the logger to change
    its behavior. If the semaphore file is present, the logger outputs `debug`-level
    logs. Once you remove the semaphore file, the logger reverts to its previous log
    level.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果调试日志记录在正常操作下对您的应用程序造成了不可接受的负担，或者调试日志数据的数量过多以至于超过了可用存储空间，您可能需要按需启用调试日志记录。一种方法是使用信号量文件来启用调试日志记录。*信号量文件*是一个空文件，其存在作为日志记录器改变行为的信号。如果信号量文件存在，日志记录器将输出`debug`级别的日志。一旦您移除信号量文件，日志记录器将恢复到先前的日志级别。
- en: 'Let’s use the `fsnotify` package to allow your application to watch for filesystem
    notifications. In addition to the standard library, the `fsnotify` package uses
    the `x/sys` package. Before you start writing code, let’s make sure our `x/sys`
    package is current:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`fsnotify`包来允许您的应用程序监视文件系统通知。除了标准库外，`fsnotify`包还使用了`x/sys`包。在开始编写代码之前，让我们确保我们的`x/sys`包是最新的：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Not all logging packages provide safe methods to asynchronously modify log levels.
    Be aware that you may introduce a race condition if you attempt to modify a logger’s
    level at the same time that the logger is reading the log level. The Zap logger
    allows you to retrieve a `sync/atomic`-based leveler to dynamically modify a logger’s
    level while avoiding race conditions. You’ll pass the atomic leveler to the `zapcore.NewCore`
    function in place of a log level, as you’ve previously done.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有日志记录包都提供安全的异步修改日志级别的方法。请注意，如果您尝试在日志记录器读取日志级别的同时修改其级别，可能会引发竞争条件。Zap日志记录器允许您检索一个基于`sync/atomic`的级别器，以便在避免竞争条件的情况下动态修改日志记录器的级别。您将把原子级别器传递给`zapcore.NewCore`函数，以替代日志级别，就像您之前做的那样。
- en: The `zap.AtomicLevel` struct implements the `http.Handler` interface. You can
    integrate it into an API and dynamically change the log level over HTTP instead
    of using a semaphore.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`zap.AtomicLevel`结构实现了`http.Handler`接口。您可以将它集成到API中，并通过HTTP动态更改日志级别，而不需要使用信号量。'
- en: '[Listing 13-12](#listing13-12) begins an example of dynamic logging using a
    semaphore file. You’ll implement this example over the next few listings.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13-12](#listing13-12)展示了一个使用信号量文件的动态日志记录示例。您将在接下来的几个列表中实现这个示例。'
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Listing 13-12: Creating a new logger using an atomic leveler (*zap_test.go*)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13-12：使用原子级别的日志记录器创建新的日志记录器（*zap_test.go*）
- en: Your code will watch for the *level.debug* file 1 in the temporary directory.
    When the file is present, you’ll dynamically change the logger’s level to `debug`.
    To do that, you need a new atomic leveler 2. By default, the atomic leveler uses
    the `info` level, which suits this example just fine. You pass in the atomic leveler
    3 when creating the core as opposed to specifying a log level itself.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你的代码将监视临时目录中是否存在*level.debug*文件1。当文件存在时，你将动态地将记录器的级别更改为`debug`。为此，你需要一个新的原子级别器2。默认情况下，原子级别器使用`info`级别，这对于这个例子来说完全合适。你在创建核心时传入原子级别器3，而不是直接指定日志级别。
- en: Now that you have an atomic leveler and a place to store your semaphore file,
    let’s write the code that will watch for semaphore file changes in [Listing 13-13](#listing13-13).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有了一个原子级别器和一个存储信号量文件的位置，让我们编写代码来监视信号量文件的变化，见[清单13-13](#listing13-13)。
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Listing 13-13: Watching for any changes to the semaphore file (*zap_test.go*)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 清单13-13：监视信号量文件的任何变化（*zap_test.go*）
- en: First, you create a filesystem watcher 1, which you’ll use to watch the temporary
    directory 2. The watcher will notify you of any changes to or within that directory.
    You also want to capture the current log level 3 so that you can revert to it
    when you remove the semaphore file.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你创建一个文件系统监视器1，用于监视临时目录2。监视器将通知你该目录中的任何变化。你还需要捕获当前的日志级别3，以便在删除信号量文件时恢复到该级别。
- en: Next, you listen for events from the watcher 4. Since you’re watching a directory,
    you filter out any event unrelated to the semaphore file 5 itself. Even then,
    you’re interested in only the creation of the semaphore file or its removal. If
    the event indicates the creation of the semaphore file 6, you change the atomic
    leveler’s level to `debug`. If you receive a semaphore file removal event 7, you
    set the atomic leveler’s level back to its original level.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你监听监视器4的事件。由于你监视的是一个目录，你需要过滤掉与信号量文件5本身无关的任何事件。即使如此，你只对信号量文件的创建或删除事件感兴趣。如果事件表示信号量文件的创建6，你将原子级别器的级别更改为`debug`。如果你收到信号量文件删除事件7，你将原子级别器的级别恢复为原始级别。
- en: If you receive an error from the watcher 8 at any point, you log it at the `error`
    level.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在任何时候从监视器8接收到错误，你应该以`error`级别记录该错误。
- en: Let’s see how this works in practice. [Listing 13-14](#listing13-14) tests the
    logger with and without the semaphore file present.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它在实践中的表现。[清单13-14](#listing13-14)测试了记录器在有信号量文件和没有信号量文件时的行为。
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Listing 13-14: Testing the logger’s use of the semaphore file (*zap_test.go*)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 清单13-14：测试记录器使用信号量文件（*zap_test.go*）
- en: The logger’s current log level via the atomic leveler is `info`. Therefore,
    the logger does not write the initial debug log entry 1 to standard output. But
    if you create the semaphore file 2, the code in [Listing 13-13](#listing13-13)
    should dynamically change the logger’s level to `debug`. If you add another debug
    log entry 3, the logger should write it to standard output. You then remove the
    semaphore file 4 and write both a debug log entry 5 and an info log entry 6. Since
    the semaphore file no longer exists, the logger should write only the info log
    entry to standard output.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 记录器当前通过原子级别器的日志级别是`info`。因此，记录器不会将初始调试日志条目1写入标准输出。但是，如果你创建信号量文件2，位于[清单13-13](#listing13-13)中的代码应该会动态地将记录器的级别更改为`debug`。如果你再添加另一条调试日志条目3，记录器应该将其写入标准输出。然后你删除信号量文件4，并同时写入调试日志条目5和信息日志条目6。由于信号量文件不再存在，记录器应该只将信息日志条目写入标准输出。
- en: Scaling Up with Wide Event Logging
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用宽事件日志记录进行扩展
- en: '*Wide event logging* is a technique that creates a single, structured log entry
    per event to summarize the transaction, instead of logging numerous entries as
    the transaction progresses. This technique is most applicable to request-response
    loops, such as API calls, but it can be adapted to other use cases. When you summarize
    transactions in a structured log entry, you reduce the logging overhead while
    conserving the ability to index and search for transaction details.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*宽事件日志记录*是一种技术，它为每个事件创建一个单一的、结构化的日志条目，以总结事务，而不是随着事务的进行记录大量的条目。这种技术最适用于请求-响应循环，例如API调用，但它也可以适应其他用例。当你在结构化的日志条目中总结事务时，你可以减少日志记录的开销，同时保持索引和搜索事务细节的能力。'
- en: One approach to wide event logging is to wrap an API handler in middleware.
    But first, since the `http.ResponseWriter` is a bit stingy with its output, you
    need to create your own response writer type ([Listing 13-15](#listing13-15))
    to collect and log the response code and length.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛事件日志的一种方法是将API处理器包装在中间件中。但首先，由于`http.ResponseWriter`对其输出比较“吝啬”，你需要创建自己的响应写入器类型（[列表
    13-15](#listing13-15)），以便收集并记录响应码和长度。
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Listing 13-15: Creating a `ResponseWriter` to capture the response status code
    and length (*wide_test.go*)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13-15：创建一个`ResponseWriter`以捕获响应状态码和长度（*wide_test.go*）
- en: The new type embeds an object that implements the `http.ResponseWriter` interface
    1. In addition, you add `length` and `status` fields, since those values are ultimately
    what you want to log from the response. You override the `WriteHeader` method
    2 to easily capture the status code. Likewise, you override the `Write` method
    3 to keep an accurate accounting of the number of written bytes and optionally
    set the status code 4 should the caller execute `Write` before `WriteHeader`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 新类型嵌入了实现`http.ResponseWriter`接口的对象1。此外，你添加了`length`和`status`字段，因为这些值最终是你想从响应中记录的内容。你重写了`WriteHeader`方法2，以便轻松捕获状态码。同样，你重写了`Write`方法3，以准确记录写入字节的数量，并在调用者在`WriteHeader`之前执行`Write`时，可选择设置状态码4。
- en: '[Listing 13-16](#listing13-16) uses your new type in wide event logging middleware.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13-16](#listing13-16) 在广泛事件日志中间件中使用了你新的类型。'
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Listing 13-16: Implementing wide event logging middleware (*wide_test.go*)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13-16：实现广泛事件日志中间件（*wide_test.go*）
- en: The wide event logging middleware accepts both a `*zap.Logger` and an `http.Handler`
    and returns an `http.Handler`. If this pattern is unfamiliar to you, please read
    “Handlers” on page 193.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛事件日志中间件同时接受`*zap.Logger`和`http.Handler`，并返回一个`http.Handler`。如果这个模式对你来说不熟悉，请阅读第193页的“处理器”部分。
- en: First, you embed the `http.ResponseWriter` in a new instance of your wide event
    logging–aware response writer 1. Then, you call the `ServeHTTP` method of the
    next `http.Handler`2, passing in your response writer. Finally, you make a single
    log entry 3 with various bits of data about the request and response.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将`http.ResponseWriter`嵌入到一个新的广泛事件日志感知响应写入器实例中1。然后，你调用下一个`http.Handler`的`ServeHTTP`方法2，传入你的响应写入器。最后，你创建一个日志条目3，记录关于请求和响应的各种数据。
- en: Keep in mind that I’m taking care here to omit values that would change with
    each execution and break the example output, like call duration. You would likely
    have to write code to deal with these in a real implementation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我在这里特意忽略了那些每次执行时会变化并打破示例输出的值，例如调用时长。在实际实现中，你可能需要编写代码来处理这些值。
- en: '[Listing 13-17](#listing13-17) puts the middleware into action and demonstrates
    the expected output.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13-17](#listing13-17) 启动中间件并展示预期的输出。'
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Listing 13-17: Using the wide event logging middleware to log the details of
    a GET call (*wide_test.go*)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13-17：使用广泛事件日志中间件记录GET调用的详细信息（*wide_test.go*）
- en: As in Chapter 9, you use the `httptest` server with your `WideEventLog` middleware
    1. You pass `*zap.Logger` into the middleware as the first argument and `http.Handler`
    as the second argument. The handler writes a simple *Hello!* to the response 2
    so the response length is nonzero. That way, you can prove that your response
    writer works. The logger writes the log entry immediately before you receive the
    response to your GET request 3. As before, I must wrap the JSON output 5 for printing
    in this book, but it consumes a single line otherwise.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如第9章所述，你使用`httptest`服务器与`WideEventLog`中间件1一起工作。你将`*zap.Logger`作为第一个参数传递给中间件，`http.Handler`作为第二个参数。处理器将简单的*Hello!*写入响应2，因此响应长度是非零的。通过这种方式，你可以证明你的响应写入器正常工作。日志记录器会在接收到GET请求的响应之前立即写入日志条目3。如之前所述，我必须将JSON输出5包装起来以便在本书中打印，但它通常只占用一行。
- en: Since this is just an example, I elected to use the logger’s `Fatal` method
    4, which writes the error message to the log file and calls `os.Exit(1)` to terminate
    the application. You shouldn’t use this in code that is supposed to keep running
    in the event of an error.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这只是一个示例，我选择使用记录器的`Fatal`方法4，该方法将错误信息写入日志文件并调用`os.Exit(1)`终止应用程序。如果代码需要在发生错误时继续运行，你不应该在代码中使用这个方法。
- en: Log Rotation with Lumberjack
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Lumberjack进行日志轮换
- en: If you elect to output log entries to a file, you could leverage an application
    like *logrotate* to keep them from consuming all available hard drive space. The
    downside to using a third-party application to manage log files is that the third-party
    application will need to signal to your application to reopen its log file handle
    lest your application keep writing to the rotated log file.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择将日志条目输出到文件，你可以使用像*logrotate*这样的应用程序，以防它们占用所有可用的硬盘空间。使用第三方应用程序管理日志文件的缺点是，第三方应用程序需要向你的应用程序发出信号，提示它重新打开日志文件句柄，否则你的应用程序将继续写入已轮换的日志文件。
- en: A less invasive and more reliable option is to add log file management directly
    to your logger by using a library like *Lumberjack*. Lumberjack handles log rotation
    in a way that is transparent to the logger, because your logger treats Lumberjack
    as any other `io.Writer`. Meanwhile, Lumberjack keeps track of the log entry accounting
    and file rotation for you.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不那么侵入性且更可靠的选择是直接将日志文件管理添加到你的日志记录器中，使用像*Lumberjack*这样的库。Lumberjack以透明的方式处理日志轮换，因为你的日志记录器将Lumberjack视为任何其他`io.Writer`。与此同时，Lumberjack为你跟踪日志条目的记账和文件轮换。
- en: '[Listing 13-18](#listing13-18) adds log rotation to a typical Zap logger implementation.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13-18](#listing13-18)为典型的Zap日志记录实现添加了日志轮换功能。'
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Listing 13-18: Adding log rotation to the Zap logger using Lumberjack (*zap_test.go*)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13-18：使用Lumberjack为Zap日志记录器添加日志轮换功能（*zap_test.go*）
- en: Like the `*bytes.Buffer` in [Listing 13-9](#listing13-9), `*lumberjack.Logger`2
    does not implement the `zapcore.WriteSyncer`. It, too, lacks a `Sync` method.
    Therefore, you need to wrap it in a call to `zapcore.AddSync`1.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 与[列表 13-9](#listing13-9)中的`*bytes.Buffer`一样，`*lumberjack.Logger`2没有实现`zapcore.WriteSyncer`。它同样缺少`Sync`方法。因此，你需要将其封装在对`zapcore.AddSync`1的调用中。
- en: Lumberjack includes several fields to configure its behavior, though its defaults
    are sensible. It uses a log filename in the format *<processname>-lumberjack.log*,
    saved in the temporary directory, unless you explicitly give it a log filename
    3. You can also elect to save hard drive space and have Lumberjack compress 4
    rotated log files. Each rotated log file is timestamped using UTC by default,
    but you can instruct Lumberjack to use local time 5 instead. Finally, you can
    configure the maximum log file age before it should be rotated 6, the maximum
    number of rotated log files to keep 7, and the maximum size in megabytes 8 of
    a log file before it should be rotated.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Lumberjack包含多个字段来配置其行为，尽管其默认值是合理的。它使用格式为*<processname>-lumberjack.log*的日志文件名，并保存在临时目录中，除非你明确指定日志文件名3。你还可以选择节省硬盘空间，并让Lumberjack压缩4已轮换的日志文件。每个已轮换的日志文件默认使用UTC时间戳，但你可以指示Lumberjack改为使用本地时间5。最后，你可以配置日志文件在轮换前的最大年龄6、要保留的最大轮换日志文件数量7，以及日志文件达到的最大大小（以MB为单位）8。
- en: You can continue using the logger as if it were writing directly to standard
    output or `*os.File`. The difference is that Lumberjack will intelligently handle
    the log file management for you.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以继续像直接写入标准输出或`*os.File`一样使用日志记录器。不同之处在于，Lumberjack会智能地为你处理日志文件的管理。
- en: Instrumenting Your Code
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 给代码添加日志
- en: '*Instrumenting* your code is the process of collecting metrics for the purpose
    of making inferences about the current state of your service—such as the duration
    of each request-response loop, the size of each response, the number of connected
    clients, the latency between your service and a third-party API, and so on. Whereas
    logs provide a record of how your service got into a certain state, metrics give
    you insight into that state itself.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*为代码添加指标*是收集指标的过程，目的是推断你服务的当前状态——例如每个请求响应循环的持续时间、每个响应的大小、连接的客户端数量、你的服务和第三方API之间的延迟等等。日志提供了记录服务如何进入某种状态的过程，而指标则让你深入了解该状态本身。'
- en: 'Instrumentation is easy, so much so that I’m going to give you the opposite
    advice I did for logging: instrument everything (initially). Fine-grained instrumentation
    involves hardly any overhead, it’s efficient to ship, and it’s inexpensive to
    store. Plus, instrumentation can solve one of the challenges of logging I mentioned
    earlier: that you won’t initially know all the questions you’ll want to ask, particularly
    for complex systems. An insidious problem may be ready to ruin your weekend because
    you lack critical metrics to give you an early warning that something is wrong.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表化很简单，甚至我要给你与我给日志记录相反的建议：最初，仪表化所有内容。细粒度的仪表化几乎没有任何开销，它高效且易于传输，存储成本低廉。而且，仪表化能够解决我之前提到的日志记录中的一个挑战：你最初并不知道你将要问的所有问题，尤其是在复杂的系统中。一个隐蔽的问题可能会因你缺乏关键的指标而破坏你的周末，它会给你一个早期的警告，提醒你出现了问题。
- en: This section will introduce you to metric types and show you the basics for
    using those types in your services. You will learn about Go kit’s `metrics` package,
    which is an abstraction layer that provides useful interfaces for popular metrics
    platforms. You’ll round out the instrumentation by using Prometheus as your target
    metrics platform and set up an endpoint for Prometheus to scrape. If you elect
    to use a different platform in the future, you will need to swap out only the
    Prometheus bits of this code; you could leave the Go kit code as is. If you’re
    just getting started with instrumentation, one option is to use Grafana Cloud
    at [https://grafana.com/products/cloud/](https://grafana.com/products/cloud/)
    to scrape and visualize your metrics. Its free tier is adequate for experimenting
    with instrumentation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍指标类型，并展示如何在你的服务中使用这些类型的基础知识。你将了解Go kit的`metrics`包，它是一个抽象层，提供了流行指标平台的有用接口。你将通过使用Prometheus作为目标指标平台并设置Prometheus抓取的端点来完成仪表化。如果将来选择使用不同的平台，你只需要替换代码中的Prometheus相关部分；你可以保留Go
    kit代码不变。如果你刚开始进行仪表化，使用[https://grafana.com/products/cloud/](https://grafana.com/products/cloud/)来抓取并可视化你的指标是一个不错的选择。它的免费层足以进行仪表化实验。
- en: Setup
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置
- en: To abstract the implementation of your metrics and the packages they depend
    on, let’s begin by putting them in their own package ([Listing 13-19](#listing13-19)).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了抽象化你的指标实现及其依赖的包，让我们首先将它们放在自己的包中（[清单13-19](#listing13-19)）。
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Listing 13-19: Imports and command line flags for the metrics example (*instrumentation/metrics/metrics.go*)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 清单13-19：指标示例的导入和命令行标志 (*instrumentation/metrics/metrics.go*)
- en: You import Go kit’s `metrics` package 1, which provides the interfaces your
    code will use, its `prometheus` adapter 2 so you can use Prometheus as your metrics
    platform, and Go’s Prometheus client package 3 itself. All Prometheus-related
    imports reside in this package. The rest of your code will use Go kit’s interfaces.
    This allows you to swap out the underlying metrics platform without the need to
    change your code’s instrumentation.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你导入Go kit的`metrics`包 1，它提供你的代码将使用的接口，`prometheus`适配器 2，使你能够将Prometheus作为你的指标平台，以及Go的Prometheus客户端包
    3 本身。所有与Prometheus相关的导入都位于此包中。你的其余代码将使用Go kit的接口。这样，你可以在不需要更改代码中的仪表化部分的情况下，交换底层的指标平台。
- en: Prometheus prefixes its metrics with a namespace and a subsystem. You could
    use the service name for the namespace and the node or hostname for the subsystem,
    for example. In this example, you’ll use `web` for the namespace 4 and `server1`
    for the subsystem 5 by default. As a result, your metrics will use the `web_server1_`
    prefix. You’ll see this prefix in [Listing 13-30](#listing13-30)’s command line
    output.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus会在其指标前加上命名空间和子系统的前缀。你可以使用服务名称作为命名空间，节点或主机名作为子系统，例如。在此示例中，默认情况下，你将使用`web`作为命名空间4，`server1`作为子系统5。因此，你的指标将使用`web_server1_`前缀。你将在[清单13-30](#listing13-30)的命令行输出中看到这个前缀。
- en: Now let’s explore the various metric types, starting with counters.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索各种指标类型，从计数器开始。
- en: Counters
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计数器
- en: '*Counters* are used for tracking values that only increase, such as request
    counts, error counts, or completed task counts. You can use a counter to calculate
    the rate of increase for a given interval, such as the number of connections per
    minute.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*计数器*用于跟踪仅增加的值，例如请求计数、错误计数或完成任务计数。你可以使用计数器来计算给定时间间隔内的增长速率，例如每分钟连接数。'
- en: '[Listing 13-20](#listing13-20) defines two counters: one to track the number
    of requests and another to account for the number of write errors.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 13-20](#listing13-20) 定义了两个计数器：一个用于跟踪请求数量，另一个用于记录写入错误的数量。'
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Listing 13-20: Creating counters as Go kit interfaces (*instrumentation/metrics/metrics.go*)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 13-20：作为 Go kit 接口创建计数器 (*instrumentation/metrics/metrics.go*)
- en: Each counter implements Go kit’s `metrics.Counter` interface 1. The concrete
    type for each counter comes from Go kit’s `prometheus` adapter 2 and relies on
    a `CounterOpts` struct 3 from the Prometheus client package for configuration.
    Aside from the namespace and subsystem values we covered, the other important
    values you set are the metric name 4 and its help string 5, which describes the
    metric.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 每个计数器都实现了 Go kit 的 `metrics.Counter` 接口 1。每个计数器的具体类型来自 Go kit 的 `prometheus`
    适配器 2，并依赖于 Prometheus 客户端包中的 `CounterOpts` 结构体 3 进行配置。除了我们已经讨论过的命名空间和子系统值外，另一个重要的值是你设置的度量名称
    4 及其帮助字符串 5，后者描述了该度量。
- en: Gauges
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 仪表
- en: '*Gauges* allow you to track values that increase or decrease, such as the current
    memory usage, in-flight requests, queue sizes, fan speed, or the number of ThinkPads
    on my desk. Gauges do not support rate calculations, such as the number of connections
    per minute or megabits transferred per second, while counters do.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*仪表* 允许你跟踪增减的值，例如当前内存使用情况、在途请求、队列大小、风扇转速或我桌面上 ThinkPad 的数量。仪表不支持速率计算，例如每分钟的连接数或每秒传输的兆比特，而计数器则支持。'
- en: '[Listing 13-21](#listing13-21) creates a gauge to track open connections.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 13-21](#listing13-21) 创建了一个仪表来跟踪开放的连接数。'
- en: '[PRE22]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Listing 13-21: Creating a gauge as a Go kit interface (*instrumentation/metrics/metrics.go*)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 13-21：作为 Go kit 接口创建仪表 (*instrumentation/metrics/metrics.go*)
- en: Creating a gauge is much like creating a counter. You create a new variable
    of Go kit’s `metrics.Gauge` interface 1 and use the `NewGaugeFrom` function 2
    from Go kit’s `prometheus` adapter to create the underlying type. The Prometheus
    client’s `GaugeOpts` struct 3 provides the settings for your new gauge.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 创建仪表与创建计数器类似。你需要创建一个新的 Go kit 的 `metrics.Gauge` 接口变量，并使用 Go kit 的 `prometheus`
    适配器中的 `NewGaugeFrom` 函数 2 来创建底层类型。Prometheus 客户端的 `GaugeOpts` 结构体 3 提供了新仪表的设置。
- en: Histograms and Summaries
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直方图和摘要
- en: A *histogram* places values into predefined buckets. Each bucket is associated
    with a range of values and named after its maximum one. When a value is observed,
    the histogram increments the maximum value of the smallest bucket into which the
    value fits. In this way, a histogram tracks the frequency of observed values for
    each bucket.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*直方图* 将值放入预定义的桶中。每个桶都与一个值的范围相关联，并以其最大值命名。当观察到一个值时，直方图会增加最小桶中可以容纳该值的最大值。通过这种方式，直方图跟踪每个桶中观察到的值的频率。'
- en: Let’s look at a quick example. Assuming you have three buckets valued at 0.5,
    1.0, and 1.5, if a histogram observes the value 0.42, it will increment the counter
    associated with bucket 0.5, because 0.5 is the smallest bucket that can contain
    0.42\. It covers the range of 0.5 and smaller values. If the histogram observes
    the value 1.23, it will increment the counter associated with the bucket 1.5,
    which covers values in the range of above 1.0 through 1.5\. Naturally, the 1.0
    bucket covers the range of above 0.5 through 1.0.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的例子。假设你有三个桶，值分别为 0.5、1.0 和 1.5，如果直方图观察到值为 0.42，它将增加与桶 0.5 相关联的计数器，因为
    0.5 是可以容纳 0.42 的最小桶。它覆盖了 0.5 及以下的范围。如果直方图观察到值为 1.23，它将增加与桶 1.5 相关联的计数器，桶 1.5 覆盖了
    1.0 到 1.5 之间的值。自然地，桶 1.0 覆盖了 0.5 到 1.0 之间的范围。
- en: You can use a histogram’s distribution of observed values to estimate a percentage
    or an average of all values. For example, you could use a histogram to calculate
    the average request sizes or response sizes observed by your service.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用直方图的观察值分布来估算所有值的百分比或平均值。例如，你可以使用直方图来计算服务中观察到的平均请求大小或响应大小。
- en: A *summary* is a histogram with a few differences. First, a histogram requires
    predefined buckets, whereas a summary calculates its own buckets. Second, the
    metrics server calculates averages or percentages from histograms, whereas your
    service calculates the averages or percentages from summaries. As a result, you
    can aggregate histograms across services on the metrics server, but you cannot
    do the same for summaries.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*摘要* 是一个直方图，有几个不同之处。首先，直方图需要预定义的桶，而摘要会计算自己的桶。其次，度量服务器从直方图计算平均值或百分比，而你的服务从摘要中计算平均值或百分比。因此，你可以在度量服务器上跨服务聚合直方图，但无法对摘要执行相同的操作。'
- en: The general advice is to use summaries when you don’t know the range of expected
    values, but I’d advise you to use histograms whenever possible so that you can
    aggregate histograms on the metrics server. Let’s use a histogram to observe request
    duration (see [Listing 13-22](#listing13-22)).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一般建议是当你不知道预期值的范围时使用总结性指标，但我建议尽可能使用直方图，这样你可以在指标服务器上聚合直方图。让我们使用直方图来观察请求时长（参见[列表13-22](#listing13-22)）。
- en: '[PRE23]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Listing 13-22: Creating a histogram metric (*instrumentation/metrics/metrics.go*)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13-22：创建直方图指标（*instrumentation/metrics/metrics.go*）
- en: Both the summary and histogram metric types implement Go kit’s `metrics.Histogram`
    interface 1 from its `prometheus` adapter. Here, you’re using a histogram metric
    type 2, using the Prometheus client’s `HistogramOpts` struct 3 for configuration.
    Since Prometheus’s default bucket sizes are too large for the expected request
    duration range when communicating over localhost, you define custom bucket sizes
    4. I encourage you to experiment with the number of buckets and bucket sizes.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 总结性指标和直方图指标类型都实现了Go kit的`metrics.Histogram`接口1，这是通过其`prometheus`适配器提供的。在这里，你使用的是直方图指标类型2，并通过Prometheus客户端的`HistogramOpts`结构体3进行配置。由于Prometheus的默认桶大小对于通过localhost进行通信时预期的请求时长范围来说太大，你定义了自定义的桶大小4。我鼓励你尝试不同数量的桶和桶大小。
- en: If you’d rather implement `RequestDuration` as a summary metric, you can substitute
    the code in [Listing 13-22](#listing13-22) for the code in [Listing 13-23](#listing13-23).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更愿意将`RequestDuration`实现为总结性指标，可以将[列表13-22](#listing13-22)中的代码替换为[列表13-23](#listing13-23)中的代码。
- en: '[PRE24]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Listing 13-23: Optionally creating a summary metric'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13-23：可选地创建一个总结性指标
- en: As you can see, this looks a lot like a histogram, minus the `Bucket` method.
    Notice that you still use the `metrics.Histogram` interface 1 with a Prometheus
    summary metric. This is because Go kit does not distinguish between histograms
    and summaries; only your implementation of the interface does.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这看起来很像一个直方图，除了`Bucket`方法。请注意，你仍然使用`metrics.Histogram`接口1来处理Prometheus的总结性指标。这是因为Go
    kit并不区分直方图和总结性指标，只有你的接口实现才会区分。
- en: Instrumenting a Basic HTTP Server
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仪表化基础HTTP服务器
- en: 'Let’s combine these metric types in a practical example: instrumenting a Go
    HTTP server. The biggest challenges here are determining what you want to instrument,
    where best to instrument it, and what metric type is most appropriate for each
    value you want to track. If you use Prometheus for your metrics platform, as you’ll
    do here, you’ll also need to add an HTTP endpoint for the Prometheus server to
    scrape.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一个实际示例中结合这些指标类型：对一个Go HTTP服务器进行仪表化。这里最大的挑战是确定你要监控什么，最合适的监控位置在哪里，以及每个你想追踪的值最适合使用什么类型的指标。如果你使用Prometheus作为你的指标平台，就像在这里一样，你还需要为Prometheus服务器添加一个HTTP端点来抓取数据。
- en: '[Listing 13-24](#listing13-24) details the initial code needed for an application
    that comprises an HTTP server to serve the metrics endpoint and another HTTP server
    to pass all requests to an instrumented handler.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表13-24](#listing13-24)详细介绍了应用程序所需的初始代码，该应用程序包括一个HTTP服务器来提供指标端点，另一个HTTP服务器将所有请求传递给一个仪表化的处理程序。'
- en: '[PRE25]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Listing 13-24: Imports and command line flags for the metrics example (*instrumentation/main.go*)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13-24：指标示例的导入和命令行标志（*instrumentation/main.go*）
- en: The only imports your code needs are the `promhttp` package for the metrics
    endpoint and your `metrics` package to instrument your code. The `promhttp` package
    1 includes an `http.Handler` that a Prometheus server can use to scrape metrics
    from your application. This handler serves not only your metrics but also metrics
    related to the runtime, such as the Go version, number of cores, and so on. At
    a minimum, you can use the metrics provided by the Prometheus handler to gain
    insight into your service’s memory utilization, open file descriptors, heap and
    stack details, and more.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 你的代码所需的唯一导入是`promhttp`包（用于指标端点）和你的`metrics`包（用于仪表化你的代码）。`promhttp`包1包含一个`http.Handler`，Prometheus服务器可以使用它从你的应用程序抓取指标。这个处理程序不仅提供你的指标，还提供与运行时相关的指标，比如Go版本、核心数量等等。至少，你可以使用Prometheus处理程序提供的指标来洞察服务的内存利用率、打开的文件描述符、堆栈详细信息等。
- en: All variables exported by your `metrics` package 2 are Go kit interfaces. Your
    code doesn’t need to concern itself with the underlying metrics platform or its
    implementation, only how these metrics are made available to the metrics server.
    In a real-world application, you could further abstract the Prometheus handler
    to fully remove any dependency other than your metrics package from the rest of
    your code. But in the interest of keeping this example succinct, I’ve included
    the Prometheus handler in the `main` package.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你`metrics`包中导出的所有变量2都是Go kit接口。你的代码无需关心底层的度量平台或其实现，只需关注这些度量如何提供给度量服务器。在实际应用中，你可以进一步抽象Prometheus处理程序，以便完全消除除了你的度量包以外的其他依赖。但为了保持本示例简洁，我将Prometheus处理程序包含在了`main`包中。
- en: Now, onto the code you want to instrument. [Listing 13-25](#listing13-25) adds
    the function your web server will use to handle all incoming requests.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是你要进行仪表化的代码。[列表13-25](#listing13-25)添加了你的Web服务器将用于处理所有传入请求的函数。
- en: '[PRE26]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Listing 13-25: An instrumented handler that responds with random latency (*instrumentation/main.go*)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13-25：一个带有随机延迟的仪表化处理程序（*instrumentation/main.go*）
- en: Even in such a simple handler, you’re able to make three meaningful measurements.
    You increment the requests counter upon entering the handler 1 since it’s the
    most logical place to account for it. You also immediately defer a function that
    calculates the request duration and uses the request duration summary metric to
    observe it 2. Lastly, you account for any errors writing the response 3.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 即便是这样一个简单的处理程序，你也能进行三项有意义的度量。你在进入处理程序1时增加请求计数器，因为这是记录它的最合适位置。你还立即延迟一个函数来计算请求持续时间，并使用请求持续时间的汇总度量进行观察2。最后，你会记录写响应时出现的任何错误3。
- en: 'Now, you need to put the handler to use. But first, you need a helper function
    that will allow you to spin up a couple of HTTP servers: one to serve the metrics
    endpoint and one to serve this handler. [Listing 13-26](#listing13-26) details
    such a function.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你需要使用这个处理程序。但首先，你需要一个帮助函数，允许你启动几个HTTP服务器：一个用于提供metrics端点，另一个用于提供这个处理程序。[列表13-26](#listing13-26)详细描述了这样的一个函数。
- en: '[PRE27]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Listing 13-26: Functions to create an HTTP server and instrument connection
    states (*instrumentation/main.go*)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13-26：创建HTTP服务器并监控连接状态的函数（*instrumentation/main.go*）
- en: This HTTP server code resembles that of Chapter 9\. The exception here is you’re
    defining the server’s `ConnState` field, accepting it as an argument 1 to the
    `newHTTPServer` function.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这个HTTP服务器代码类似于第9章的代码。唯一的不同是你在此定义了服务器的`ConnState`字段，并将其作为参数1传递给`newHTTPServer`函数。
- en: The HTTP server calls its `ConnState` field anytime a network connection changes.
    You can leverage this functionality to instrument the number of open connections
    the server has at any one time. You can pass the `connStateMetrics` function 2
    to the `newHTTPServer` function anytime you want to initialize a new HTTP server
    and track its open connections. If the server establishes a new connection, you
    increment the open connections gauge 3 by 1\. If a connection closes, you decrement
    the gauge 4 by 1\. Go kit’s gauge interface provides an `Add` method, so decrementing
    a value involves adding a negative number.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP服务器每当网络连接发生变化时，都会调用其`ConnState`字段。你可以利用这个功能来监控服务器在任何时刻的开放连接数量。每当你想初始化一个新的HTTP服务器并跟踪其开放连接时，可以将`connStateMetrics`函数2传递给`newHTTPServer`函数。如果服务器建立了新连接，你会将开放连接计量器3增加1。如果连接关闭，你会将计量器4减少1。Go
    kit的计量器接口提供了`Add`方法，因此减少一个值实际上是加上一个负数。
- en: Let’s create an example that puts all these pieces together. [Listing 13-27](#listing13-27)
    creates an HTTP server to serve up the Prometheus endpoint and another HTTP server
    to serve your instrumented handler.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个例子，将所有这些部分结合在一起。[列表13-27](#listing13-27)创建了一个HTTP服务器来提供Prometheus端点，并创建了另一个HTTP服务器来提供你的仪表化处理程序。
- en: '[PRE28]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Listing 13-27: Starting two HTTP servers to serve `metrics` and the `helloHandler`
    (*instrumentation/main.go*)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13-27：启动两个HTTP服务器来提供`metrics`和`helloHandler`（*instrumentation/main.go*）
- en: First, you spawn an HTTP server with the sole purpose of serving the Prometheus
    handler 1 at the `/metrics/` endpoint where Prometheus scrapes metrics from by
    default. Since you do not pass in a function for the third argument 2, this HTTP
    server won’t have a function assigned to its `ConnState` field to call on each
    connection state change. Then, you spin up another HTTP server to handle each
    request with the `helloHandler`3. But this time, you pass in the `connStateMetrics`
    function 4. As a result, this HTTP server will gauge open connections.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你启动一个HTTP服务器，唯一的目的是在`/metrics/`端点提供Prometheus处理程序1，默认情况下Prometheus会从这个端点抓取指标。由于你没有为第三个参数2传入函数，这个HTTP服务器不会为它的`ConnState`字段分配一个在每次连接状态变化时调用的函数。然后，你启动另一个HTTP服务器，使用`helloHandler`3来处理每个请求。但这次，你传入了`connStateMetrics`函数4。结果，这个HTTP服务器将衡量开放连接。
- en: Now, you can spin up many HTTP clients to make a bunch of requests to affect
    your metrics (see [Listing 13-28](#listing13-28)).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以启动多个HTTP客户端，发送大量请求来影响你的指标（参见[Listing 13-28](#listing13-28)）。
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Listing 13-28: Instructing 500 HTTP clients to each make 100 GET calls (*instrumentation/main.go*)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 'Listing 13-28: 指示500个HTTP客户端每个执行100次GET请求（*instrumentation/main.go*）'
- en: You start by spawning 500 HTTP clients 1 to each make 100 GET calls 2. But first,
    you need to address a problem. The `http.Client` uses the `http.DefaultTransport`
    if its `Transport` method is nil. The `http.DefaultTransport` does an outstanding
    job of caching TCP connections. If all 500 HTTP clients use the same transport,
    they’ll all make calls over about two TCP sockets. Our open connections gauge
    would reflect the two idle connections when you’re done with this example, which
    isn’t really the goal.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你会启动500个HTTP客户端，每个客户端进行100次GET请求。但是在此之前，你需要解决一个问题。`http.Client`会使用`http.DefaultTransport`，如果它的`Transport`方法为nil。`http.DefaultTransport`在缓存TCP连接方面表现出色。如果所有500个HTTP客户端使用相同的传输，它们将通过大约两个TCP套接字进行通信。当你完成这个示例时，我们的开放连接指标将反映出这两个空闲连接，而这并不是我们的目标。
- en: Instead, you must make sure to give each HTTP client its own transport. Cloning
    the default transport 3 is good enough for our purposes.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 但你必须确保每个HTTP客户端都有自己的传输。克隆默认传输3对于我们的目的已经足够。
- en: Now that each client has its own transport and you’re assured each client will
    make its own TCP connection, you iterate through a GET call 4 100 times with each
    client. You must also be diligent about draining 5 and closing 6 the response
    body so each client can reuse its TCP connection.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每个客户端都有自己的传输，并且你可以确保每个客户端都会建立自己的TCP连接，你可以让每个客户端通过GET请求4进行100次迭代。你还必须小心地清理5和关闭6响应体，以便每个客户端可以重用其TCP连接。
- en: Once all 500 HTTP clients complete their 100 calls 7, you can move on to [Listing
    13-29](#listing13-29) and check the current state of the metrics.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有500个HTTP客户端完成它们的100次请求7，你就可以继续查看[Listing 13-29](#listing13-29)，检查当前的指标状态。
- en: '[PRE30]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Listing 13-29: Displaying the current metrics matching your namespace and subsystem
    (*instrumentation/main.go*)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 'Listing 13-29: 显示匹配你命名空间和子系统的当前指标（*instrumentation/main.go*）'
- en: You retrieve all the metrics from the metrics endpoint 1. This will cause the
    metrics web server to return all metrics stored by the Prometheus client, in addition
    to details about each metric it tracks, which includes the metrics you added.
    Since you’re interested in only your metrics, you can check each line starting
    with your namespace, an underscore, and your subsystem 2. If the line matches
    this prefix 3, you print it to standard output. Otherwise, you ignore the line
    and move on.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你从指标端点1检索所有的指标。这将导致指标Web服务器返回Prometheus客户端存储的所有指标，除了它跟踪的每个指标的详细信息外，还包括你添加的指标。由于你只对自己的指标感兴趣，你可以检查每一行，以你的命名空间、下划线和子系统2开头的行。如果该行匹配这个前缀3，你就将其打印到标准输出。否则，你忽略该行并继续。
- en: Let’s run this example on the command line and examine the resulting metrics
    in [Listing 13-30](#listing13-30).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在命令行上运行这个示例，并检查在[Listing 13-30](#listing13-30)中生成的指标。
- en: '[PRE31]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Listing 13-30: Web server output and resulting metrics'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 'Listing 13-30: Web服务器输出和生成的指标'
- en: As expected, 500 connections were open 1 at the time you queried the metrics.
    These connections are idle. You can experiment with the HTTP client by invoking
    its `CloseIdleConnections` method after it’s done making 100 GET calls; see how
    that change affects the open connections gauge. Likewise, see what happens to
    the open connections when you don’t define their `Transport` field.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，在你查询指标时，500 个连接已打开 1。这些连接处于空闲状态。你可以通过在完成 100 次 GET 请求后调用 HTTP 客户端的 `CloseIdleConnections`
    方法来进行实验；看看这个变化如何影响打开连接的仪表。同样，看看当你没有定义它们的 `Transport` 字段时，打开连接会发生什么。
- en: The request count is 50,000 2, so all requests succeeded.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 请求计数是 50,000 2，因此所有请求都成功。
- en: Do you notice what’s missing? The write errors counter. Since no write errors
    occur, the write errors counter never increments. As a result, it doesn’t show
    up in the metrics output. You could make a call to `metrics.WriteErrors.Add(0)`
    to make the metric show up without changing its value, but its absence probably
    bothers you more than it bothers Prometheus. Just be aware that the metrics output
    may not include all instrumented metrics, just the ones that have changed since
    initialization.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到缺少了什么吗？写入错误计数器。由于没有发生写入错误，写入错误计数器从未增加。因此，它不会出现在指标输出中。你可以调用 `metrics.WriteErrors.Add(0)`
    使该指标出现，但其缺失可能会让你比 Prometheus 更困扰。只要意识到，指标输出可能不会包括所有已加测的指标，而只包括自初始化以来发生变化的指标。
- en: 'The underlying Prometheus histogram is a *cumulative* histogram: any value
    that increments a bucket’s counter also increments the counters for all buckets
    less than the value. Therefore, you see increasing values in each bucket until
    you reach the 0.01 bucket 4. Even though you define a range of buckets, Prometheus
    adds an infinite bucket for you. In this example, you defined a bucket smaller
    than all observed values 3, so its counter is still zero.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 基础的 Prometheus 直方图是一个 *累积* 直方图：任何增加某个桶计数器的值都会同时增加所有小于该值的桶计数器。因此，你会看到每个桶的值逐渐增大，直到你到达
    0.01 桶 4。即使你定义了一个桶范围，Prometheus 也会为你添加一个无限大的桶。在这个例子中，你定义了一个比所有观察到的值 3 更小的桶，因此它的计数器仍然为零。
- en: 'A histogram and a summary maintain two additional counters: the sum of all
    observed values 5 and the total number of observed values 6. If you use a summary,
    the Prometheus endpoint will present only these two counters. It will not detail
    the summary’s buckets as it does with a histogram. Therefore, the Prometheus server
    can aggregate histogram buckets but cannot do the same for summaries.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 一个直方图和一个汇总会维护两个额外的计数器：所有观察到的值之和 5 和观察到的值的总数 6。如果使用汇总，Prometheus 端点将只显示这两个计数器。它不会像直方图那样详细展示汇总的桶。因此，Prometheus
    服务器可以聚合直方图桶，但无法对汇总做同样的操作。
- en: What You’ve Learned
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你学到了什么
- en: Logging is hard. Instrumentation, not so much. Be frugal with your logging and
    generous with your instrumentation. Logging isn’t free and can quickly add latency
    if you aren’t mindful of where and how much you log. You cannot go wrong by logging
    actionable items, particularly ones that should trigger an alert. On the other
    hand, instrumentation is very efficient. You should instrument everything, at
    least initially. Metrics detail the current state of your service and provide
    insight into potential problems, whereas logs provide an immutable audit trail
    of sorts that explains the current state of your service and helps you diagnose
    failures.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录很难，仪表化则不然。尽量节俭于日志记录，慷慨于仪表化。日志记录不是免费的，如果你不小心控制日志的位置和数量，很快就会增加延迟。记录可以作为有效的行动项，尤其是那些应该触发警报的项，绝对不会出错。另一方面，仪表化非常高效。你应该对一切进行仪表化，至少在初期是如此。指标详细说明了服务的当前状态，并提供了潜在问题的洞察，而日志则提供了一种不可变的审计跟踪，解释了服务的当前状态并帮助你诊断故障。
- en: Go’s `log` package provides enough functionality to satisfy basic log requirements.
    But it becomes cumbersome when you need to log to more than one output or at varying
    levels of verbosity. At that point, you’re better off with a comprehensive solution
    such as Uber’s Zap logger. No matter what logger you use, consider adding structure
    to your log entries by including additional metadata. Structured logging allows
    you to leverage software to quickly filter and search log entries, particularly
    if you centralize logs across your infrastructure.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Go的`log`包提供了足够的功能来满足基本的日志需求。但当你需要将日志输出到多个目标或在不同的详细级别下记录日志时，它变得繁琐。在这种情况下，使用像Uber的Zap日志记录器这样的综合解决方案会更好。无论你使用什么日志记录器，都要考虑通过添加额外的元数据来给日志条目添加结构。结构化日志记录可以让你利用软件快速过滤和搜索日志条目，特别是当你将日志集中到整个基础设施时。
- en: On-demand debug logging and wide event logging are two methods you can use to
    collect important information while minimizing logging’s impact on the performance
    of your service. You can use the creation of a semaphore file to signal your logger
    to enable debug logging. When you remove the semaphore file, the logger immediately
    disables debug logging. Wide event logs summarize events in a request-response
    loop. You can replace numerous log entries with a single wide event log without
    hindering your ability to diagnose failures.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 按需调试日志记录和广泛事件日志记录是你可以用来收集重要信息的方法，同时最小化日志记录对服务性能的影响。你可以通过创建信号量文件来指示日志记录器启用调试日志记录。当你删除信号量文件时，日志记录器会立即禁用调试日志记录。广泛事件日志汇总了请求-响应循环中的事件。你可以用一个广泛的事件日志替代多个日志条目，而不会影响你诊断故障的能力。
- en: One approach to instrumentation is to use Go kit’s `metrics` package, which
    provides interfaces for common metric types and adapters for popular metrics platforms.
    It allows you to abstract the details of each metrics platform away from your
    instrumented code.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一种仪表化的方法是使用Go kit的`metrics`包，它提供了常见指标类型的接口和流行的指标平台适配器。它允许你将每个指标平台的细节抽象化，从而将它们与仪表化代码分离开。
- en: The `metrics` package supports counters, gauges, histograms, and summaries.
    Counters monotonically increase and can be used to calculate rates of change.
    Use counters to track values like request counts, error counts, or completed tasks.
    Gauges track values that can increase and decrease, such as current memory usage,
    in-flight requests, and queue sizes. Histograms and summaries place observed values
    in buckets and allow you to estimate averages or percentages of all values. You
    could use a histogram or summary to approximate the average request duration or
    response size.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`metrics`包支持计数器、仪表、历史记录和摘要。计数器单调递增，可用于计算变化率。使用计数器来跟踪诸如请求计数、错误计数或已完成任务等值。仪表跟踪那些可以增加和减少的值，如当前内存使用情况、进行中的请求和队列大小。历史记录和摘要将观察到的值放入桶中，并允许你估算所有值的平均值或百分比。你可以使用历史记录或摘要来近似请求持续时间或响应大小的平均值。'
- en: Taken together, logging and metrics give you necessary insight into your service,
    allowing you to proactively address potential problems and recover from failures.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，日志记录和指标提供了对你的服务的必要洞察，帮助你主动解决潜在问题并从故障中恢复。
