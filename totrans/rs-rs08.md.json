["```\nenum Poll<T> {\n    Ready(T),\n    Pending\n}\n```", "```\ntrait Future {\n    type Output;\n    fn poll(&mut self) -> Poll<Self::Output>;\n}\n```", "```\nasync fn forward<T>(rx: Receiver<T>, tx: Sender<T>) {\n    while let Some(t) = rx.next().await {\n        tx.send(t).await;\n    }\n}\n```", "```\nenum Forward<T> { 1 \n    WaitingForReceive(ReceiveFuture<T>, Option<Sender<T>>),\n    WaitingForSend(SendFuture<T>, Option<Receiver<T>>),\n}\n\nimpl<T> Future for Forward<T> {\n    type Output = (); 2 \n    fn poll(&mut self) -> Poll<Self::Output> {\n        match self { 3 \n            Forward::WaitingForReceive(recv, tx) => {\n                if let Poll::Ready((rx, v)) = recv.poll() {\n                    if let Some(v) = v {\n                        let tx = tx.take().unwrap(); 4 \n                        *self = Forward::WaitingForSend(tx.send(v), Some(rx)); 5 \n                        // Try to make progress on sending.\n                        return self.poll(); 6 \n                    } else {\n                        // No more items.\n                        Poll::Ready(())\n                    }\n                } else {\n                    Poll::Pending\n                }\n            }\n            Forward::WaitingForSend(send, rx) => {\n                if let Poll::Ready(tx) = send.poll() {\n                    let rx = rx.take().unwrap();\n                    *self = Forward::WaitingForReceive(rx.receive(), Some(tx));\n                    // Try to make progress on receiving.\n                    return self.poll();\n                } else {\n                    Poll::Pending\n                }\n            }\n        }\n    }\n}\n```", "```\nasync fn forward<T>(rx: Receiver<T>, tx: Sender<T>) {\n    while let Some(t) = rx.next().await {\n        tx.send(t).await;\n    }\n}\n```", "```\ngenerator fn forward<T>(rx: Receiver<T>, tx: Sender<T>) {\n    loop {\n        let mut f = rx.next();\n        let r = if let Poll::Ready(r) = f.poll() { r } else { yield };\n        if let Some(t) = r {\n            let mut f = tx.send(t);\n            let _ = if let Poll::Ready(r) = f.poll() { r } else { yield };\n        } else { break Poll::Ready(()); }\n    }\n}\n```", "```\nasync fn try_forward<T>(rx: Receiver<T>, tx: Sender<T>) -> Option<impl Future> {\n    let mut f = forward(rx, tx);\n    if f.poll().is_pending() { Some(f) } else { None }\n}\n```", "```\ntrait Future {\n    type Output;\n    fn poll(self: Pin<&mut Self>) -> Poll<Self::Output>;\n}\n```", "```\nstruct Pin<P> { pointer: P }\nimpl<P> Pin<P> where P: Deref {\n    pub unsafe fn new_unchecked(pointer: P) -> Self;\n}\nimpl<'a, T> Pin<&'a mut T> {\n    pub unsafe fn get_unchecked_mut(self) -> &'a mut T;\n}\nimpl<P> Deref for Pin<P> where P: Deref {\n    type Target = P::Target;\n    fn deref(&self) -> &Self::Target;\n}\n```", "```\nimpl<P> Pin<P> where P: Deref, P::Target: Unpin {\n    pub fn new(pointer: P) -> Self;\n}\nimpl<P> DerefMut for Pin<P> where P: DerefMut, P::Target: Unpin {\n    fn deref_mut(&mut self) -> &mut Self::Target;\n}\n```", "```\nmacro_rules! pin_mut {\n    ($var:ident) => {\n        let mut $var = $var;\n        let mut $var = unsafe { Pin::new_unchecked(&mut $var) };\n    }\n}\n```", "```\nlet foo = /* */; { pin_mut!(foo); foo.poll() }; foo.mut_self_method();\n```", "```\nloop { if let Poll::Ready(r) = expr.poll() { break r } else { yield } }\n```", "```\n1 match expr {\n      mut pinned => loop {\n        2 match unsafe { Pin::new_unchecked(&mut pinned) }.poll() {\n              Poll::Ready(r) => break r,\n              Poll::Pending => yield,\n          }\n    }\n}\n```", "```\ntrait Future {\n    type Output;\n    fn poll(self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Self::Output>;\n}\n```", "````### Waking Is a Misnomer    You may already have realized that `Waker::wake` doesn’t necessarily seem to *wake* anything. For example, for external events (as described in the previous section), the executor is already awake, and it might seem silly for it to then call `wake` on a `Waker` that belongs to that executor anyway! The reality is that `Waker::wake` is a bit of a misnomer—in reality, it signals that a particular future is *runnable*. That is, it tells the executor that it should make sure to poll this particular future when it gets around to it rather than go to sleep again, since this future can make progress. This might wake the executor if it is currently sleeping so it will go poll that future, but that’s more of a side effect than its primary purpose.    It is important for the executor to know which futures are runnable for two reasons. First, it needs to know when it can stop polling a future and go to sleep; it’s not sufficient to just poll each future until it returns `Poll::Pending`, since polling a later future might make it possible to progress an earlier future. Consider the case where two futures bounce messages back and forth on channels to one another. When you poll one, the other becomes ready, and vice versa. In this case, the executor should never go to sleep, as there is always more work to do.    Second, knowing which futures are runnable lets the executor avoid polling futures unnecessarily. If an executor manages thousands of pending futures, it shouldn’t poll all of them just because an event made one of them runnable. If it did, executing asynchronous code would get very slow indeed.    ### Tasks and Subexecutors    The futures in an asynchronous program form a tree: a future may contain any number of other futures, which in turn may contain other futures, all the way down to the leaf futures that interact with wakers. The root of each tree is the future you give to whatever the executor’s main “run” function is. These root futures are called *tasks*, and they are the only point of contact between the executor and the futures tree. The executor calls `poll` on the task, and from that point forward the code of each contained future must figure out which inner future(s) to poll in response, all the way down to the relevant leaf.    Executors generally construct a separate `Waker` for each task they poll so that when `wake` is later called, they know which task was just made runnable and can mark it as such. That is what the raw pointer in `RawWaker` is for—to differentiate between tasks while sharing the code for the various `Waker` methods.    When the executor eventually polls a task, that task starts running from the top of its implementation of `Future::poll` and must decide from there how to get to the future deeper down that can now make progress. Since each future knows only about its own fields, and nothing about the whole tree, this all happens through calls to `poll` that each traverse one edge in the tree.    The choice of which inner future to poll is often obvious, but not always. In the case of `async`/`await`, the future to poll is the one we’re blocked waiting for. But in a future that waits for the first of several futures to make progress (often called a *select*), or for all of a set of futures (often called a *join*), there are many options. A future that has to make such a choice is basically a subexecutor. It could poll all of its inner futures, but doing so could be quite wasteful. Instead, these subexecutors often wrap the `Waker` they receive in `poll`’s `Context` with their own `Waker` type before they invoke `poll` on any inner future. In the wrapping code, they mark the future they just polled as runnable in their own state before they call `wake` on the original `Waker`. That way, when the executor eventually polls the subexecutor future again, the subexecutor can consult its own internal state to figure out which of its inner futures caused the current call to `poll`, and then only poll those.    ## Tying It All Together with spawn    When working with asynchronous executors, you may come across an operation that spawns a future. We’re now in a position to explore what that means! Let’s do so by way of example. First, consider the simple server implementation in [Listing 8-14](#listing8-14).    ``` async fn handle_client(socket: TcpStream) -> Result<()> {     // Interact with the client over the given socket. }  async fn server(socket: TcpListener) -> Result<()> {     while let Some(stream) = socket.accept().await? {         handle_client(stream).await?;     } } ```    Listing 8-14: Handling connections sequentially    The top-level `server` function is essentially one big future that listens for new connections and does something when a new connection arrives. You hand that future to the executor and say “run this,” and since you don’t want your program to then exit immediately, you’ll probably have the executor block on that future. That is, the call to the executor to run the server future will not return until the server future resolves, which may be never (another client could always arrive later).    Now, every time a new client connection comes in, the code in [Listing 8-14](#listing8-14) makes a new future (by calling `handle_client`) to handle that connection. Since the handling is itself a future, we `await` it and then move on to the next client connection.    The downside of this approach is that we only ever handle one connection at a time—there is no concurrency. Once the server accepts a connection, the `handle_client` function is called, and since we `await` it, we don’t go around the loop again until `handle_client`’s return future resolves (presumably when that client has left).    We could improve on this by keeping a set of all the client futures and having the loop in which the server accepts new connections also check all the client futures to see if any can make progress. [Listing 8-15](#listing8-15) shows what that might look like.    ``` async fn server(socket: TcpListener) -> Result<()> {     let mut clients = Vec::new();     loop {         poll_client_futures(&mut clients)?;         if let Some(stream) = socket.try_accept()? {             clients.push(handle_client(stream));         }     } } ```    Listing 8-15: Handling connections with a manual executor    This at least handles many connections concurrently, but it’s quite convoluted. It’s also not very efficient because the code now busy-loops, switching between handling the connections we already have and accepting new ones. And it has to check each connection each time, since it won’t know which ones can make progress (if any). It also can’t `await` at any point, since that would prevent the other futures from making progress. You could implement your own wakers to ensure that the code polls only the futures that can make progress, but ultimately this is going down the path of developing your own mini-executor.    Another downside of sticking with just the one task for the server that internally contains the futures for all of the client connections is that the server ends up being single-threaded. There is just the one task and to poll it the code must hold an exclusive reference to the task’s future (`poll` takes `Pin<&mut Self>`), which only one thread can hold at a time.    The solution is to make each client future its own task and leave it to the executor to multiplex among all the tasks. Which, you guessed it, you do by spawning the future. The executor will continue to block on the server future, but if it cannot make progress on that future, it will use its execution machinery to make progress on the other tasks in the meantime behind the scenes. And best of all, if the executor is multithreaded and your client futures are `Send`, it can run them in parallel since it can hold `&mut`s to the separate tasks concurrently. [Listing 8-16](#listing8-16) gives an example of what this might look like.    ``` async fn server(socket: TcpListener) -> Result<()> {     while let Some(stream) = socket.accept().await? {         // Spawn a new task with the Future that represents this client.         // The current task will continue to just poll for more connections         // and will run concurrently (and possibly in parallel) with handle_client.         spawn(handle_client(stream));     } } ```    Listing 8-16: Spawning futures to create more tasks that can be polled concurrently    When you spawn a future and thus make it a task, it’s sort of like spawning a thread. The future continues running in the background and is multiplexed concurrently with any other tasks given to the executor. However, unlike a spawned thread, spawned tasks still depend on being polled by the executor. If the executor stops running, either because you drop it or because your code no longer runs the executor’s code, those spawned tasks will stop making progress. In the server example, imagine what will happen if the main server future resolves for some reason. Since the executor has returned control back to your code, it cannot continue doing, well, anything. Multi-threaded executors often spawn background threads that continue to poll tasks even if the executor yields control back to the user’s code, but not all executors do this, so check your executor before you rely on that behavior!    ## Summary    In this chapter, we’ve taken a look behind the scenes of the asynchronous constructs available in Rust. We’ve seen how the compiler implements generators and self-referential types, and why that work was necessary to support what we now know as `async`/`await`. We’ve also explored how futures are executed, and how wakers allow executors to multiplex among tasks when only some of them can make progress at any given moment. In the next chapter, we’ll tackle what is perhaps the deepest and most discussed area of Rust: unsafe code. Take a deep breath, and then turn the page.````"]