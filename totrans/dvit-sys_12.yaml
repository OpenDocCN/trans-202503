- en: '12'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CODE OPTIMIZATION
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Code optimization* is the process by which a program is improved by reducing
    its code size, complexity, memory use, or runtime (or some combination thereof)
    without changing the program’s inherent function. Many compilation systems include
    a code optimizer as an intermediate step. Specifically, an *optimizing compiler*
    applies code-improving transformations as part of the compilation process. Virtually
    all modern compilers (including GCC) are optimizing compilers. The GCC C compiler
    implements a wide variety of *optimization flags* that give programmers direct
    access to a subset of the implemented optimizations. Compiler optimization flags
    optimize code at the expense of compile time and ease of debugging. For simplicity,
    GCC wraps up a subset of these optimization flags into different *optimization
    levels* that the programmer can directly invoke. For example, the following command
    compiles a sample program with level 1 optimizations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The level 1 (`-O1` or `-O`) optimizations in GCC perform basic optimizations
    to reduce code size and execution time while attempting to keep compile time to
    a minimum. Level 2 (`-O2`) optimizations include most of GCC’s implemented optimizations
    that do not involve a space–performance trade-off. Lastly, level 3 (`-O3`) performs
    additional optimizations (such as function inlining, discussed later in this chapter),
    and may cause the program to take significantly longer to compile. The GCC documentation^([1](ch12.xhtml#fn12_1))
    describes the implemented optimization flags in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'A detailed discussion of optimizing compilers and their construction and operation
    is beyond the scope of this textbook; we encourage interested readers to check
    out the seminal text, *Compilers: Principles, Techniques, and Tools*, by Aho,
    Sethi, and Ulman. Rather, the purpose of the chapter is to highlight some things
    that most compilers can (and cannot) do, and how programmers can partner with
    their compilers and profiling tools to help improve their code.'
  prefs: []
  type: TYPE_NORMAL
- en: What Compilers Already Do
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Several of the common optimizations performed by virtually every compiler are
    described briefly in the upcoming sections. Students should *never* manually implement
    these optimizations, because they are already implemented by the compiler.
  prefs: []
  type: TYPE_NORMAL
- en: Constant Folding
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Constants in the code are evaluated at compile time to reduce the number of
    resulting instructions. For example, in the code snippet that follows, *macro
    expansion* replaces the statement `int debug = N-5` with `int debug = 5-5`. *Constant
    folding* then updates this statement to `int debug = 0`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Constant Propagation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Constant propagation* replaces variables with a constant value if such a value
    is known at compile time. Consider the following code segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A compiler employing constant propagation will change `if (debug)` to `if (0)`.
  prefs: []
  type: TYPE_NORMAL
- en: Dead Code Elimination
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'It is not uncommon for a program to be littered with unused variables, assignments,
    or statements. Even though these unneeded statements are rarely introduced intentionally,
    they are often a natural by-product of the constant iteration and refinement of
    the software development cycle. If left undetected, these so-called *dead code*
    sequences can cause compilers to output unnecessary assembly instructions that
    in turn waste processing time. Most compilers employ techniques such as dataflow
    analysis to identify unreachable code segments and thereby remove them. *Dead
    code elimination* often makes a program faster by shrinking code size and the
    associated set of instructions. As an example, let’s revisit the `doubleSum` function
    in which the compiler employed constant propagation to replace `debug` with `0`
    in the `if` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: A compiler employing dataflow analysis recognizes that the `if` statement always
    evaluates to false and that the `printf` statement never executes. The compiler
    therefore eliminates the `if` statement and the call to `printf` in the compiled
    executable. Another pass also eliminates the statement `debug = 0`.
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying expressions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Some instructions are more expensive than others. For example, the `imul` and
    `idiv` arithmetic instructions in assembly take a long time to execute. Compilers
    commonly attempt to reduce the number of expensive instructions by simplifying
    mathematical operations whenever possible. For example, in the `doubleSum` function,
    the compiler may replace the expression `2 * total` with `total + total` because
    the addition instruction is less expensive than multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Likewise, the compiler will transform code sequences with bit-shifting and other
    bitwise operators to simplify expressions. For example, the compiler may replace
    the expression `total * 8` with `total << 3`, or the expression `total % 8` with
    `total & 7` given that bitwise operations are performed with a single fast instruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'What Compilers Cannot Always Do: Benefits of Learning Code Optimization'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given the benefits of optimizing compilers, it may not be immediately obvious
    why learning code optimization is useful. It may be tempting to think of the compiler
    as a magical black box that is “smart.” At the end of the day, the compiler is
    a piece of software that performs a series of code transformations in an effort
    to speed up code. Compilers are also limited in the types of optimizations they
    can perform.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic Strength Reduction Is Impossible
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The top reason for poor code performance is bad choices of data structures and
    algorithms. Compilers cannot magically fix these bad decisions. For example, a
    compiler will never optimize a program implementing bubble sort into one that
    implements quick sort. While the sophistication of compilers and their optimizations
    continues to improve, the *quality* of any individual compiler’s optimizations
    varies between platforms. The onus is therefore on the programmer to ensure that
    their code leverages the best algorithms and data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Compiler Optimization Flags Are Not Guaranteed to Make Code “Optimal” (or Consistent)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Increasing the level of compiler optimizations (e.g., from `-O2` to `-O3`) may
    not always decrease the runtime of a program. Sometimes, the programmer may discover
    that updating the optimization flags from `-O2` to `-O3` *slows down* a program
    or yields no performance increase at all. In other cases, a programmer may discover
    that a program compiled without the optimization flags seemingly yields no errors,
    whereas compiling it with `-O2` or `-O3` results in segmentation faults or other
    errors. These types of programming errors are especially difficult to debug, because
    gcc’s debug (`-g`) flag is incompatible with its optimization (`-O`) flags, as
    the transformations performed by compiler optimizations at the `-O` levels interfere
    with the debugger’s ability to analyze the underlying code. The `-g` flag is required
    by many common profiling tools, such as GDB and Valgrind.
  prefs: []
  type: TYPE_NORMAL
- en: One large reason for inconsistent behavior is that the C/C++ standard does not
    provide clear guidance for resolving undefined behavior. As a result, it is often
    up to the compiler to decide how to resolve ambiguity. Inconsistencies on how
    different optimization levels handle undefined behavior can cause answers to *change*.
    Consider the following example from John Regehr:^([2](ch12.xhtml#fn12_2))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Suppose that `silly` was run with `a = INT_MAX`. In this case, the computation
    `a + 1` results in integer overflow. However, the C/C++ standard does not define
    *how* integer overflow should be handled by the compiler. In fact, compiling the
    program with no optimizations causes the function to return 0, while compiling
    it with `-O3` optimizations results in the function returning 1.
  prefs: []
  type: TYPE_NORMAL
- en: In short, optimization flags should be used with caution, thoughtfully, and
    when necessary. Learning which optimization flags to employ can also help the
    programmer work with their compiler instead of against it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note THE COMPILER IS NOT REQUIRED TO HANDLE UNDEFINED BEHAVIOR**'
  prefs: []
  type: TYPE_NORMAL
- en: The `silly` function when run with `a = INT_MAX` is an example of undefined
    behavior. Note that the inconsistent output produced by the compiler is not a
    flaw in the compiler’s design or a consequence of using optimization flags. Compilers
    are specifically designed to follow a language’s specification. The C Language
    standard does not specify what a compiler should do when it encounters undefined
    behavior; the program may crash, fail to compile, or generate inconsistent or
    incorrect results. Ultimately, the programmer is responsible for identifying and
    eliminating undefined behavior in code. Whether `silly` should return 0, 1, or
    some other value is ultimately a decision the programmer must make. To learn more
    about undefined behavior and related issues in C programs, visit the C FAQ^([3](ch12.xhtml#fn12_3))
    or John Regehr’s Guide to Undefined Behavior.^([4](ch12.xhtml#fn12_4))
  prefs: []
  type: TYPE_NORMAL
- en: Pointers Can Prove Problematic
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Recall that the compiler makes transformations that leave the fundamental behavior
    of the source program unchanged. If a transformation risks changing the behavior
    of the program, the compiler will not make the transformation. This is especially
    true in the case of *memory aliasing* where two different pointers point to the
    same address in memory. As an example, consider the function `shiftAdd`, which
    takes two integer pointers as its two parameters. The function multiplies the
    first number by 10 and adds the second number to it. So, if the `shiftAdd` function
    were passed the integers 5 and 6, the result will be 56.
  prefs: []
  type: TYPE_NORMAL
- en: Unoptimized version
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Optimized version
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `shiftAddOpt` function optimizes the `shiftAdd` function by removing an
    additional memory reference to `a`, resulting in a smaller set of instructions
    in the compiled assembly. However, the compiler will never make this optimization
    due to the risk of memory aliasing. To understand why, consider the following
    `main` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this program gives the expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose, instead, that the program were modified so that `shiftAdd` now takes
    a pointer to `x` as its two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is 55\. However, recompiling and rerunning the updated
    code gives two different outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Retracing through the `shiftAdd` functions with the assumption that `a` and
    `b` are pointing to the same memory location reveals the issue. The multiplication
    of `a` by 10 in `shiftAdd` updates `x` to 50\. Next, adding `a` to `b` in `shiftAdd`
    results in `x` doubling to 100\. The risk of memory aliasing reveals that `shiftAdd`
    and `shiftAddOpt` are not in fact equivalent, though the programmer may have intended
    them to be. To fix this issue, recognize that the second parameter of `shiftAdd`
    does not need to be passed in as a pointer. Replacing the second parameter with
    an integer eliminates the risk of aliasing and allows the compiler to optimize
    one function into the other:'
  prefs: []
  type: TYPE_NORMAL
- en: Unoptimized version (fixed)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Optimized version (fixed)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Removing the unneeded memory reference allows the programmer to maintain the
    readability of the original `shiftAdd` function while enabling the compiler to
    optimize the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Partnering with Your Compiler: A Sample Program'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the following sections, we concentrate on learning more about popular types
    of optimizations and discuss programming and profiling strategies to help make
    it easier for compilers to optimize our code. To illustrate our discussion, we
    will work to optimize the following (suboptimally written) program that attempts
    to find all the prime numbers between 2 and *n*:^([5](ch12.xhtml#fn12_5))
  prefs: []
  type: TYPE_NORMAL
- en: optExample.c
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[Table 12-1](ch12.xhtml#ch12tab1) shows the timing results for producing the
    primes between 2 and 5,000,000 with the different optimization level flags using
    the following basic compilation command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Table 12-1:** Time in Seconds to Produce Prime Numbers Between 2 and 5,000,000'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Unoptimized** | -O1 | -O2 | -O3 |'
  prefs: []
  type: TYPE_TB
- en: '| 3.86 | 2.32 | 2.14 | 2.15 |'
  prefs: []
  type: TYPE_TB
- en: The fastest observed time with optimization flags is approximately 2.14 seconds.
    Although using optimization flags does shave off more than a second from the runtime
    of this program, upping the optimization flags provides minimal improvement. In
    the next sections, we will discuss how we can modify our program to make it easier
    for the compiler to optimize.
  prefs: []
  type: TYPE_NORMAL
- en: '12.1 Code Optimization First Steps: Code Profiling'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*The real problem is that programmers have spent far too much time worrying
    about efficiency in the wrong places and at the wrong times; premature optimization
    is the root of all evil (or at least most of it) in programming.*'
  prefs: []
  type: TYPE_NORMAL
- en: —Don Knuth, *The Art of Computer Programming*
  prefs: []
  type: TYPE_NORMAL
- en: One of the biggest dangers in code optimization is the concept of *premature
    optimization*. Premature optimization occurs when a programmer attempts to optimize
    based on “gut feelings” of where performance inefficiencies occur, and not on
    data. Whenever possible, it is important to measure the runtime of different portions
    of code on different inputs *prior* to starting optimization to identify *hot
    spots* or areas in the program in which the most instructions occur.
  prefs: []
  type: TYPE_NORMAL
- en: 'To figure out how to optimize `optExample.c`, let’s start by taking a closer
    look at the `main` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main` function contains calls to two functions: `allocateArray`, which
    initializes an array of a user-specified length (or limit), and `genPrimeSequence`,
    which generates a sequence of primes within the specified limit (note that for
    any sequence between 2 and *n*, there cannot be more than *n* primes, and frequently
    there are significantly less). The `main` function contains code that times each
    of the two functions in the preceding example. Compiling and running the code
    with `limit` set to 5,000,000 reveals the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `optExample` program takes approximately 3.86 seconds to complete, with
    nearly all of the time in the `genPrimeSequence` function. There is no point in
    spending time optimizing `allocateArray`, because any improvements will be negligible
    to the runtime of the overall program. In the examples that follow, we focus more
    closely on the `genPrimeSequence` function and its associated functions. The functions
    are reproduced here for convenience:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: To find hot spots in a program, focus on the areas with the most loops. Manual
    inspection of code can assist in locating hot spots, though it should always be
    verified with benchmarking tools prior to attempting optimization. A manual inspection
    of the `optExample` program yields the following observations.
  prefs: []
  type: TYPE_NORMAL
- en: The `genPrimeSequence` function attempts to generate all the prime numbers between
    2 and some integer *n*. Since the number of primes between 2 and *n* cannot exceed
    *n*, the `for` loop in `genPrimeSequence` runs no more than *n* times. Every iteration
    of the `for` loop calls the `getNextPrime` function once. Thus, `getNextPrime`
    runs no more than *n* times.
  prefs: []
  type: TYPE_NORMAL
- en: The `while` loop in the `getNextPrime` function will continue running until
    a prime is discovered. Although it is difficult to determine the number of times
    the `while` loop in the `getNextPrime` function will execute ahead of time as
    a function of *n* (the gap between consecutive prime numbers can be arbitrarily
    large), it is certain that `isPrime` executes on every iteration of the `while`
    loop.
  prefs: []
  type: TYPE_NORMAL
- en: The `isPrime` function contains exactly one `for` loop. Suppose that the loop
    runs for a total of *k* iterations. Then, the code in the loop body runs *k* times
    in total. Recall from “Loops in C” on [page 33](ch01.xhtml#lev2_7) that the structure
    of a `for` loop consists of an *initialization statement* (which initializes the
    loop variable to a particular value), a *Boolean expression* (that determines
    when to terminate the loop), and a *step expression* (that updates the loop variable
    every iteration). [Table 12-2](ch12.xhtml#ch12tab2) depicts the number of times
    each loop component executes in a `for` loop that runs for *k* iterations. In
    every `for` loop, initialization happens exactly once. The Boolean expression
    executes *k* + 1 times for *k* iterations, since it must perform one final check
    to terminate the loop. The loop body and the step expression execute *k* times
    each.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 12-2:** Loop Execution Components (Assuming k Iterations)'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Initialization statement** | **Boolean expression** | **Step expression**
    | **Loop body** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | *k* + 1 | *k* | *k* |'
  prefs: []
  type: TYPE_TB
- en: Our manual inspection of the code suggests that the program spends most of its
    time in the `isPrime` function, and that the `sqrt` function executes the most
    often. Let’s next use code profiling to verify this hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.1 Using Callgrind to Profile
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In our small program, it was relatively straightforward to use manual inspection
    to form the hypothesis that the `sqrt` function occurs in a “hot spot” in the
    code. However, identifying hot spots can become more complex in larger programs.
    Regardless, it is a good idea to use profiling to verify our hypothesis. Code
    profiling tools like Valgrind^([6](ch12.xhtml#fn12_6)) provide a lot of information
    about program execution. In this section, we use the `callgrind` tool to inspect
    the `OptExample` program’s call graph.
  prefs: []
  type: TYPE_NORMAL
- en: To use `callgrind`, let’s start by recompiling the `optExample` program with
    the `-g` flag and running `callgrind` on a smaller range (2 to 100,000). Like
    other Valgrind applications, `callgrind` runs as a wrapper around a program, adding
    annotations such as the number of times functions execute and the total number
    of instructions that are executed as a result. Consequently, the `optExample`
    program will take longer to execute when run in conjunction with `callgrind`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Typing `ls` at the terminal reveals a new file called `callgrind.out.xxxxx`,
    where `xxxxx` is a unique id. In this case, the file is `callgrind.out.32590`
    (i.e., the number shown along the left-hand column in the preceding output). Running
    `callgrind_annotate` on this file yields additional information on the three functions
    of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The numbers along the left-hand column represent the number of total executed
    instructions associated with each line. The numbers in parentheses indicate the
    number of times a particular function was run. Using the numbers along the left-hand
    column, we are able to verify the results of our manual inspection. In the `genPrimeSequence`
    function, the `getNextPrime` function resulted in the most number of executed
    instructions at 67.8 million instructions, corresponding to 9,592 function calls
    (to generate the primes between 2 and 100,000). Inspecting `getNextPrime` reveals
    that the majority of those instructions (67.1 million, or 99%) result from the
    call to `isPrime`, which is called a total of 100,001 times. Lastly, inspecting
    `isPrime` reveals that 13 million of the total instructions (20.5%) result from
    the `sqrt` function, which executes a total of 2.7 million times.
  prefs: []
  type: TYPE_NORMAL
- en: These results verify our original hypothesis that the program spends most of
    its time in the `isPrime` function, with the `sqrt` function executing the most
    frequently of all the functions. Reducing the total number of executed instructions
    results in a faster program; the above analysis suggests that our initial efforts
    should concentrate on improving the `isPrime` function, and potentially reducing
    the number of times `sqrt` executes.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.2 Loop-Invariant Code Motion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Loop-invariant code motion is an optimization technique that moves static computations
    that occur inside a loop to outside the loop without affecting the loop’s behavior.
    Optimizing compilers are capable of making most loop-invariant code optimizations
    automatically. Specifically, the `-fmove-loop -invariants` compiler flag in GCC
    (enabled at level `-O1`) attempts to identify examples of loop-invariant code
    motion and move them outside their respective loops.
  prefs: []
  type: TYPE_NORMAL
- en: However, the compiler cannot always identify cases of loop-invariant code motion,
    especially in the case of function calls. Since function calls can inadvertently
    cause *side effects* (unintended behavior), most compilers will avoid trying to
    determine whether a function call consistently returns the same result. Thus,
    even though the programmer knows that `sqrt(x)` always returns the square root
    of some input `x`, GCC will not always make that assumption. Consider the case
    where the `sqrt` function updates a secret global variable, `g`. In that case,
    calling `sqrt` once outside of the function (*one* update to `g`) is not the same
    as calling it every iteration of the loop (*n* updates to `g`). If a compiler
    cannot determine that a function always returns the same result, it will not automatically
    move the `sqrt` function outside the loop.
  prefs: []
  type: TYPE_NORMAL
- en: However, the programmer knows that moving the computation `sqrt(x) + 1` outside
    the `for` loop does not effect the loop’s behavior. The updated function is shown
    here and is available online:^([7](ch12.xhtml#fn12_7))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[Table 12-3](ch12.xhtml#ch12tab3) shows that this simple change shaves off
    a full two seconds (47%) of the runtime of `optExample2`, even before using compiler
    flags. Furthermore, the compiler seems to have a slightly easier time optimizing
    `optExample2`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 12-3:** Time in Seconds to Produce the Prime Numbers Between 2 and
    5,000,000'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Version** | **Unoptimized** | -O1 | -O2 | -O3 |'
  prefs: []
  type: TYPE_TB
- en: '| Original | 3.86 | 2.32 | 2.14 | 2.15 |'
  prefs: []
  type: TYPE_TB
- en: '| With loop-invariant code motion | 1.83 | 1.63 | 1.71 | 1.63 |'
  prefs: []
  type: TYPE_TB
- en: 'Rerunning `callgrind` on the `optExample2` executable reveals why such a large
    improvement in runtime was observed. The following code snippet assumes that the
    file `callgrind.out.30086` contains the annotations of running `callgrind` on
    the `optExample2` executable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Moving the call to `sqrt` outside of the `for` loop reduces the number of times
    the `sqrt` function is called in the program from 2.7 million to 100,000 (96%
    reduction). This number corresponds to the number of times the `isPrime` function
    is called, confirming that the `sqrt` function executes only once with every invocation
    of the `isPrime` function.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the compiler was able to perform significant levels of optimization
    when optimization flags were specified, even if the programmer does not manually
    perform code motion. In this case, the reason is due to a special instruction
    called `fsqrt` that is specified by the x86 ISA. When optimization flags are turned
    on, the compiler replaces all instances of the `sqrt` function with the `fsqrt`
    instruction. This process is known as *inlining*, and we cover it greater detail
    in the following section. Since `fsqrt` is no longer a function, it is easier
    for the compiler to identify its loop-invariant nature and move it outside the
    body of the loop.
  prefs: []
  type: TYPE_NORMAL
- en: '12.2 Other Compiler Optimizations: Loop Unrolling and Function Inlining'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The loop-invariant code motion optimization described in the previous section
    was a simple change that resulted in a massive reduction in execution time. However,
    such optimizations are situationally dependent, and may not always result in improvements
    to performance. In most cases, loop-invariant code motion is taken care of by
    the compiler.
  prefs: []
  type: TYPE_NORMAL
- en: Code today is more often read than it is written. In most cases, fractional
    performance gains are not worth the hit to code readability. In general, a programmer
    should let the compiler optimize whenever possible. In this section, we cover
    some optimization techniques that were previously manually implemented by programmers
    but are today commonly implemented by compilers.
  prefs: []
  type: TYPE_NORMAL
- en: There are several sources online that advocate for the manual implementation
    of the techniques we describe in the following sections. However, we encourage
    readers to check whether their compilers support the following optimizations before
    attempting to manually implement them in their code. All the optimizations described
    in this section are implemented in GCC, but may not be available in older compilers.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.1 Function Inlining
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One optimization step that compilers attempt to perform is *function inlining*,
    which replaces calls to a function with the body of the function. For example,
    in the `main` function, a compiler inlining the `allocateArray` function will
    replace the call to `allocateArray` with a direct call to `malloc`:'
  prefs: []
  type: TYPE_NORMAL
- en: Original version
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: allocateArray inlined
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Inlining functions can result in some runtime savings for a program. Recall
    that every time a program calls a function, many instructions associated with
    function creation and destruction are necessarily generated. Inlining functions
    enables the compiler to eliminate these excessive calls, and makes it easier for
    the compiler to identify other potential improvements, including constant propagation,
    constant folding, and dead code elimination. In the case of the `optExample` program,
    inlining likely allows the compiler to replace the call to `sqrt` with the `fsqrt`
    instruction and subsequently move it outside the loop.
  prefs: []
  type: TYPE_NORMAL
- en: The `-finline-functions` flag suggests to GCC that functions should be inlined.
    This optimization is turned on at level 3\. Even though `-finline-functions` can
    be used independently of the `-O3` flag, it is a *suggestion* to the compiler
    to look for functions to inline. Likewise, the `static inline` keyword can be
    used to suggest to the compiler that a particular function should be inlined.
    Keep in mind that the compiler will not inline all functions, and that function
    inlining is not guaranteed to make code faster.
  prefs: []
  type: TYPE_NORMAL
- en: Programmers should generally avoid inlining functions manually. Inlining functions
    carries a high risk of significantly reducing the readability of code, increasing
    the likelihood of errors, and making it harder to update and maintain functions.
    For example, trying to inline the `isPrime` function in the `getNextPrime` function
    will greatly reduce the readability of `getNextPrime`.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.2 Loop Unrolling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The last compiler optimization strategy we discuss in this section is loop
    unrolling. Let’s revisit the `isPrime` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `for` loop executes a total of `max` times, where `max` is one more than
    the square root of integer `x`. At the assembly level, every execution of the
    loop checks to see whether `i` is less than `max`. If so, the instruction pointer
    jumps to the body of the loop, which computes the modulo operation. If the modulo
    operation results in 0, the program immediately exits the loop and returns 0\.
    Otherwise, the loop continues execution. While branch predictors are fairly good
    at predicting what a conditional expression evaluates to (especially inside loops),
    wrong guesses can result in a hit to performance, due to disruptions in the instruction
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '*Loop unrolling* is an optimization that compilers perform to reduce the impact
    of wrong guesses. In loop unrolling, the goal is to reduce the number of iterations
    of a loop by a factor of *n* by increasing the workload that each iteration performs
    by a factor of *n*. When a loop is unrolled by a factor of 2, the number of iterations
    in the loop is cut by *half* , whereas the amount work performed per iteration
    is *doubled*.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s manually apply 2-factor loop unrolling to our `isPrime` function:^([8](ch12.xhtml#fn12_8))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Notice that even though we have halved the number of iterations that the `for`
    loop takes, each iteration of the loop now performs two modulo checks, doubling
    the amount of work per iteration. Recompiling and rerunning the program results
    in marginally improved times (see [Table 12-4](ch12.xhtml#ch12tab4)).
  prefs: []
  type: TYPE_NORMAL
- en: The readability of the code is also reduced. A better way to utilize loop unrolling
    is to invoke the `-funroll-loops` compiler optimization flag, which tells the
    compiler to unroll loops whose iterations can be determined at compile time. The
    `-funroll-all-loops` compiler flag is a more aggressive option that unrolls all
    loops regardless of whether the compiler is certain of the number of iterations.
    [Table 12-4](ch12.xhtml#ch12tab4) shows the runtimes of the manual 2-factor loop
    unrolling^([9](ch12.xhtml#fn12_9)) compared to adding the `-funroll-loops` and
    `-funroll -all-loops` compiler optimization flags to the previous program.⁷
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 12-4:** Time in Seconds to Produce 5,000,000 Prime Numbers'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Version** | **File** | **Unoptimized** | -O1 | -O2 | -O3 |'
  prefs: []
  type: TYPE_TB
- en: '| Original | `optExample.c` | 3.86 | 2.32 | 2.14 | 2.15 |'
  prefs: []
  type: TYPE_TB
- en: '| Loop-invariant code motion | `optExample2.c` | 1.83 | 1.63 | 1.71 | 1.63
    |'
  prefs: []
  type: TYPE_TB
- en: '| Manual factor-of-two loop | `optExample3.c` | 1.65 | 1.53 | 1.45 | 1.45 |'
  prefs: []
  type: TYPE_TB
- en: '| unrolling |'
  prefs: []
  type: TYPE_TB
- en: '| `-funroll-loops` | `optExample2.c` | 1.82 | 1.48 | 1.46 | 1.46 |'
  prefs: []
  type: TYPE_TB
- en: '| `-funroll-all-loops` | `optExample2.c` | 1.81 | 1.47 | 1.47 | 1.46 |'
  prefs: []
  type: TYPE_TB
- en: Manual loop unrolling does result in some performance improvement; however the
    compiler’s built-in loop unrolling flags when combined with the other optimization
    flags yield comparable performance. If a programmer wants to incorporate loop
    unrolling optimizations into their code, they should default to using the appropriate
    compiler flags, and *not* manually unroll loops themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 Memory Considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Programmers should pay special attention to memory use, especially when employing
    memory-intensive data structures such as matrices and arrays. Although compilers
    offer powerful optimization features, the compiler cannot always make optimizations
    that improve a program’s memory use. In this section, we use an implementation
    of a matrix-vector program `matrixVector.c`^([10](ch12.xhtml#fn12_10)) to guide
    discussion of techniques and tools for improving memory use.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `main` function of the program performs two steps. First, it allocates
    and initializes the input matrix, the input vector, and the output matrix. Next,
    it performs matrix-vector multiplication. Running the code on matrix-vector dimensions
    of 10,000 × 10,000 reveals that the `matrixVectorMultiply` function takes up the
    majority of the time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Our discussion will thus focus on the `matrixVectorMultiply` function.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.1 Loop Interchange
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Loop interchange optimizations switch the order of inner and outer loops in
    nested loops in order to maximize cache locality. Automatically performing this
    task is difficult for compilers to do. In GCC, the `-floop-interchange` compiler
    flag exists but is currently not available by default. Therefore, it is a good
    idea for programmers to pay attention to how their code is accessing memory-composite
    data structures like arrays and matrices. As an example, let’s take a closer look
    at the `matrixVectorMultiply` function in `matrixVector.c`:'
  prefs: []
  type: TYPE_NORMAL
- en: Original version
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Loop interchange version
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The input and output matrices are dynamically allocated (see “Method 2: The
    Programmer-Friendly Way” on [page 90](ch02.xhtml#lev3_17)). As a result, the rows
    in the matrices are not contiguous to one another, whereas the elements in each
    row are contiguous. The current ordering of the loops causes the program to cycle
    through each column instead of every row. Recall that data is loaded into cache
    in blocks, not elements (see “Direct-Mapped Caches” on [page 558](ch11.xhtml#lev2_193)).
    As a result, when an element *x* in an array in either `res` or `m` is accessed,
    the *elements adjacent to* x are also loaded into cache. Cycling through every
    “column” of the matrix causes more cache misses, as the cache is forced to load
    new blocks with every access. [Table 12-5](ch12.xhtml#ch12tab5) shows that adding
    optimization flags does not decrease the runtime of the function. However, simply
    switching the order of the loops (as shown in the previous code examples) makes
    the function nearly eight times faster and allows the compiler to perform additional
    optimizations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 12-5:** Time in Seconds to Perform Matrix Multiplication on 10,000
    × 10,000 Elements'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Version** | **Program** | **Unoptimized** | -O1 | -O2 | -O3 |'
  prefs: []
  type: TYPE_TB
- en: '| Original | `matrixVector` | 2.01 | 2.05 | 2.07 | 2.08 |'
  prefs: []
  type: TYPE_TB
- en: '| Loop interchange | `matrixVector2` | 0.27 | 0.08 | 0.06 | 0.06 |'
  prefs: []
  type: TYPE_TB
- en: The Valgrind tool `cachegrind` (discussed in “Cache Analysis and Valgrind” on
    [page 575](ch11.xhtml#lev1_90)) is a great way to identify data locality issues,
    and reveals the cache access differences in the two versions of the `matrixVectorMultiply`
    function shown in the previous example.
  prefs: []
  type: TYPE_NORMAL
- en: '12.3.2 Some Other Compiler Optimizations for Improving Locality: Fission and
    Fusion'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Rerunning the improved program on 10,000 × 10,000 elements yields the following
    runtime numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, matrix allocation and filling takes the most time. Additional timing reveals
    that it is the filling of the matrices that in fact takes the most time. Let’s
    take a closer look at that code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'To fill the input and output matrices, a `for` loop cycles through all the
    rows, and calls the `fillArrayRandom` and `fillArrayZeros` functions on each matrix.
    In some scenarios, it may be advantageous for the compiler to split the single
    loop into two separate loops (known as *loop fission*), as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: Original version
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: With loop fission
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The process of taking two loops that operate over the same range and combining
    their contents into a single loop (i.e., the opposite of loop fission) is called
    *loop fusion*. Loop fission and fusion are examples of optimizations a compiler
    might perform to try to improve data locality. Compilers for multicore processors
    may also use loop fission or fusion to enable loops to execute efficiently on
    multiple cores. For example, a compiler may use loop fission to assign two loops
    to different cores. Likewise, a compiler may use loop fusion to combine together
    dependent operations into the body of the loop and distribute to each core a subset
    of the loop iterations (assuming data between iterations are independent).
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, applying loop fission manually does not directly improve program
    performance; there is virtually no change in the amount of time required to fill
    the array. However, it may reveal a more subtle optimization: the loop containing
    `fillArrayZeros` is not necessary. The `matrixVectorMultiply` function assigns
    values to each element in the `result` array; a prior initialization to all zeros
    is unnecessary.'
  prefs: []
  type: TYPE_NORMAL
- en: Previous version matrixVector2.c
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Updated version matrixVector3.c
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 12.3.3 Memory Profiling with Massif
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Making the previous change results in only a slight decrease in runtime. Although
    it eliminates the step of filling in all elements in the result matrix with zeros,
    a significant amount of time is still required to fill the input matrix with random
    numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Even though each array is stored noncontiguously in memory, each one takes up
    10,000 × `sizeof(int)` bytes, or 40,000 bytes. Since there is a total of 20,000
    (10,000 each for the initial matrix and the result matrix) arrays allocated, this
    corresponds to 800 million bytes, or roughly 762 MB of space. Filling 762 MB with
    random numbers understandably takes a lot of time. With matrices, memory use increases
    quadratically with the input size, and can play a large role in performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Valgrind’s `massif` tool can help you profile memory use. Like the other Valgrind
    tools we covered in this book (see “Debugging Memory with Valgrind” on [page 168](ch03.xhtml#lev1_22),
    “Cache Analysis and Valgrind” on [page 575](ch11.xhtml#lev1_90), and “Using Callgrind
    to Profile” on [page 600](ch12.xhtml#lev2_205)), `massif` runs as a wrapper around
    a program’s executable. Specifically, `massif` takes snapshots of program memory
    use throughout the program, and profiles how memory usage fluctuates. Programmers
    may find the `massif` tool useful for tracking how their programs use heap memory,
    and for identifying opportunities to improve memory use. Let’s run the `massif`
    tool on the `matrixVector3` executable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Running `massif` produces a `massif.out.xxxx` file, where `xxxx` is a unique
    id number. If you are typing along, type ls to reveal your corresponding massif
    file. In the example that follows, the corresponding file is `massif.out.7030`.
    Use the ms_print command to view the `massif` output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: At the top of the output is the memory use graph. The *x*-axis shows the number
    of instructions executed. The *y*-axis shows memory use. The graph above indicates
    that a total of 9.778 billion (Gi) instructions executed during our run of `matrixVector3`.
    During execution, `massif` took a total of 80 snapshots to measure use on the
    heap. Memory use peaked in the last snapshot (79). Peak memory use for the program
    was 763.3 MB, and stayed relatively constant throughout the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summaries of all the snapshots occur after the graph. For example, the following
    table corresponds to the snapshots around snapshot 79:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Each row corresponds to a particular snapshot, the time it was taken, the total
    heap memory consumption (in bytes) at that point, the number of bytes requested
    by the program ("useful-heap") at that point, the number of bytes allocated in
    excess of what the program asked for, and the size of the stack. By default, stack
    profiling is off (it slows `massif` down significantly). To enable stack profiling,
    use the `--stacks=yes` option when running `massif`.
  prefs: []
  type: TYPE_NORMAL
- en: The `massif` tool reveals that 99.96% of the program’s heap memory use occurred
    in the `allocateArray` function and that a total of 800 million bytes were allocated,
    consistent with the back-of-the-envelope calculation we performed earlier. Readers
    will likely find `massif` a useful tool for identifying areas of high heap memory
    use in their programs, which often slows a program down. For example, *memory
    leaks* can occur in programs when programmers frequently call `malloc` without
    calling `free` at the first correct opportunity. The `massif` tool is incredibly
    useful for detecting such leaks.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4 Key Takeaways and Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our short (and perhaps frustrating) journey into code optimization should convey
    one very important message to the reader: if you are thinking about manually optimizing
    your code, think carefully about what is worth spending your time on and what
    should be left to the compiler. Next are some important tips to consider when
    looking to improve code performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Choose Good Data Structures and Algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There is no substitute for using proper algorithms and data structures; failure
    to do so is often the top reason for poor performance in code. For example, the
    famous Sieve of Eratosthenes algorithm is a much more efficient way to generate
    prime numbers than our custom algorithm in `optExample`, and yields a significant
    improvement in performance. The following listing shows the time needed to generate
    all prime numbers between 2 and 5 million using an implementation of the sieve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The sieve algorithm requires only 0.12 seconds to find all the prime numbers
    between 2 and 5 million, compared to the 1.46 seconds it takes `optExample2` to
    generate the same set of primes with the `-O3` optimization flags turned on (12×
    improvement). The implementation of the sieve algorithm is left as an exercise
    for the reader; however, it should be clear that choosing a better algorithm up
    front would have saved hours of tedious optimization effort. Our example demonstrates
    why a knowledge of data structures and algorithms is foundational for computer
    scientists.
  prefs: []
  type: TYPE_NORMAL
- en: Use Standard Library Functions Whenever Possible
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Don’t reinvent the wheel. If in the course of programming you need a function
    that should do something very standard (e.g., find the absolute value, or find
    the maximum or minimum of a list of numbers), stop and check to see whether the
    function already exists as part of the higher-level language’s standard library.
    Functions in the standard libraries are well tested and tend to be optimized for
    performance. For example, if a reader manually implements their own version of
    the `sqrt` function, the compiler may not know to automatically replace the function
    call with the `fsqrt` instruction.
  prefs: []
  type: TYPE_NORMAL
- en: Optimize Based on Data and Not on Feelings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If after choosing the best data structures and algorithms *and* employing standard
    library functions, additional improvements in performance are required, enlist
    the help of a good code profiler like Valgrind. Optimization should *never* be
    based on gut feelings. Concentrating too much on what one *feels* should be optimized
    (without the data to back up the thought) often leads to wasted time.
  prefs: []
  type: TYPE_NORMAL
- en: Split Complex Code into Multiple Functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Manually inlining code usually does not result in a sizable performance gain
    over what modern compilers can achieve. Instead, make it easier for your compiler
    to help optimize for you. Compilers have an easier time optimizing shorter code
    segments. Splitting complex operations into multiple functions simultaneously
    increases code readability and makes it easier for a compiler to optimize. Check
    to see whether your compiler attempts inlining by default or has a separate flag
    to attempt inlining code. It is better to let your compiler perform inlining rather
    than manually doing it yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Prioritize Code Readability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In many applications today, readability is king. The truth is that code is read
    more often than it is written. Many companies spend considerable time training
    their software engineers to write code in a very particular way to maximize readability.
    If optimizing your code results in a noticeable hit to code readability, it is
    important to check if the performance improvement obtained is worth the hit. For
    example, many compilers today have optimization flags that enable loop unrolling.
    Programmers should always use available optimization flags for loop unrolling
    instead of trying to manually unroll loops, which can lead to a significant hit
    in code readability. Reducing code readability often increases the likelihood
    that bugs are inadvertently introduced into code, which can lead to security vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Pay Attention to Memory Use
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A program’s memory usage often has a bigger impact on the program’s execution
    time than the number of instructions that it executes. The loop interchange example
    exemplifies this point. In both cases, the loop executes the same number of instructions.
    However, the ordering of the loops has a significant impact on memory access and
    locality. Remember to also explore memory profiling tools like `massif` and `cachegrind`
    when attempting to optimize a program.
  prefs: []
  type: TYPE_NORMAL
- en: Compilers Are Constantly Improving
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compiler writers continually update compilers to perform more sophisticated
    optimizations safely. For example, GCC switched to the static single assignment
    (SSA) form^([11](ch12.xhtml#fn12_11)) starting in version 4.0, which significantly
    improved the effects of some of its optimizations. The `GRAPHITE` branch of the
    GCC code base implements the polyhedral model,^([12](ch12.xhtml#fn12_12)) which
    allows the compiler to perform more complex types of loop transformations. As
    compilers become more sophisticated, the benefits of manual optimization significantly
    decrease.
  prefs: []
  type: TYPE_NORMAL
- en: Notes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[1.](ch12.xhtml#rfn12_1) *[https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html](https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[2.](ch12.xhtml#rfn12_2) John Regehr, “A Guide to Undefined Behavior in C and
    C++, Part 1,” *[https://blog.regehr.org/archives/213](https://blog.regehr.org/archives/213)*,
    2010.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3.](ch12.xhtml#rfn12_3) C FAQ, “comp.lang.c FAQ list: Question 11.33,” *[http://c-faq.com/ansi/undef.html](http://c-faq.com/ansi/undef.html)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[4.](ch12.xhtml#rfn12_4) John Regehr, “A Guide to Undefined Behavior in C and
    C++, Part 1,” *[https://blog.regehr.org/archives/213](https://blog.regehr.org/archives/213)*,
    2010.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5.](ch12.xhtml#rfn12_5) Source code available at *[https://diveintosystems.org/book/C12-CodeOpt/_attachments/optExample.c](https://diveintosystems.org/book/C12-CodeOpt/_attachments/optExample.c)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[6.](ch12.xhtml#rfn12_6) *[http://valgrind.org/](http://valgrind.org/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[7.](ch12.xhtml#rfn12_7) *[https://diveintosystems.org/book/C12-CodeOpt/_attachments/optExample2.c](https://diveintosystems.org/book/C12-CodeOpt/_attachments/optExample2.c)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[8.](ch12.xhtml#rfn12_8) *[https://diveintosystems.org/book/C12-CodeOpt/_attachments/optExample3.c](https://diveintosystems.org/book/C12-CodeOpt/_attachments/optExample3.c)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[9.](ch12.xhtml#rfn12_9) *[https://diveintosystems.org/book/C12-CodeOpt/_attachments/optExample3.c](https://diveintosystems.org/book/C12-CodeOpt/_attachments/optExample3.c)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[10.](ch12.xhtml#rfn12_10) *[https://diveintosystems.org/book/C12-CodeOpt/_attachments/matrixVector.c](https://diveintosystems.org/book/C12-CodeOpt/_attachments/matrixVector.c)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[11.](ch12.xhtml#rfn12_11) *[https://gcc.gnu.org/onlinedocs/gccint/SSA.html](https://gcc.gnu.org/onlinedocs/gccint/SSA.html)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[12.](ch12.xhtml#rfn12_12) *[https://polyhedral.info/](https://polyhedral.info/)*'
  prefs: []
  type: TYPE_NORMAL
