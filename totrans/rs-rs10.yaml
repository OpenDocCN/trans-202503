- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Concurrency (and Parallelism)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 并发（和并行）
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: With this chapter I hope to provide you with all the information and tools you’ll
    need to take effective advantage of concurrency in your Rust programs, to implement
    support for concurrent use in your libraries, and to use Rust’s concurrency primitives
    correctly. I won’t directly teach you how to implement a concurrent data structure
    or write a high-performance concurrent application. Instead, my goal is to give
    you sufficient understanding of the underlying mechanisms that you’re equipped
    to wield them yourself for whatever you may need them for.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是为你提供所有必要的信息和工具，使你能够在Rust程序中有效地利用并发，实现在库中对并发的支持，并正确使用Rust的并发原语。我不会直接教你如何实现并发数据结构或编写高性能的并发应用程序。我的目标是让你充分理解底层机制，从而具备使用它们的能力，无论你将来需要它们做什么。
- en: 'Concurrency comes in three flavors: single-thread concurrency (like with `async`/`await`,
    as we discussed in Chapter 8), single-core multithreaded concurrency, and multicore
    concurrency, which yields true parallelism. Each flavor allows the execution of
    concurrent tasks in your program to be interleaved in different ways. There are
    even more subflavors if you take the details of operating system scheduling and
    preemption into account, but we won’t get too deep into that.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 并发有三种形式：单线程并发（就像我们在第8章讨论的`async`/`await`），单核多线程并发，以及多核并发，后者实现了真正的并行。每种形式都允许在程序中以不同的方式交替执行并发任务。如果考虑到操作系统调度和抢占的细节，甚至还有更多的子形式，但我们不会深入探讨这些。
- en: 'At the type level, Rust represents only one aspect of concurrency: multithreading.
    Either a type is safe for use by more than one thread, or it is not. Even if your
    program has multiple threads (and so is concurrent) but only one core (and so
    is not parallel), Rust must assume that if there are multiple threads, there may
    be parallelism. Most of the types and techniques we’ll be talking about apply
    equally whether two threads actually execute in parallel or not, so to keep the
    language simple, I’ll be using the word *concurrency* in the informal sense of
    “things running more or less at the same time” throughout this chapter. When the
    distinction is important, I’ll call that out.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在类型层面上，Rust 只表示并发的一个方面：多线程。类型要么对多个线程是安全的，要么不是。如果程序有多个线程（因此是并发的），但只有一个核心（因此不是并行的），Rust
    必须假设，如果有多个线程，就可能存在并行性。我们接下来要讨论的大多数类型和技术，无论两个线程是否实际并行执行，都同样适用，因此为了保持语言简洁，我会在本章中用*并发*这个词来表示“事物大致同时运行”的非正式意义。当这种区别很重要时，我会特别指出。
- en: What’s particularly neat about Rust’s approach to type-based safe multithreading
    is that it is not a feature of the compiler, but rather a library feature that
    developers can extend to develop sophisticated concurrency contracts. Since thread
    safety is expressed in the type system through `Send` and `Sync` implementations
    and bounds, which propagate all the way out to application code, the thread safety
    of the entire program is checked through type checking alone.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Rust 对基于类型的安全多线程的特别之处在于，这不是编译器的特性，而是一个库特性，开发人员可以扩展它来开发复杂的并发契约。由于线程安全通过`Send`和`Sync`实现以及约束在类型系统中表示，并且这些约束会一直传递到应用代码中，整个程序的线程安全性仅通过类型检查就能得到验证。
- en: '*The Rust Programming Language* already covers most of the basics when it comes
    to concurrency, including the `Send` and `Sync` traits, `Arc` and `Mutex`, and
    channels. I therefore won’t reiterate much of that here, except where it’s worth
    repeating something specifically in the context of some other topic. Instead,
    we’ll look at what makes concurrency difficult and some common concurrency patterns
    intended to deal with those difficulties. We’ll also explore how concurrency and
    asynchrony interact (and how they don’t) before diving into how to use atomic
    operations to implement lower-level concurrent operations. Finally, I’ll close
    out the chapter with some advice for how to retain your sanity when working with
    concurrent code.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 《Rust 编程语言》已经涵盖了并发的基础知识，包括`Send`和`Sync`特性、`Arc`和`Mutex`，以及通道。因此，我不会在这里重复这些内容，除非在某些其他话题的上下文中特别值得提及。相反，我们将探讨使并发变得困难的原因，以及一些常见的并发模式来应对这些困难。我们还将探索并发和异步的相互作用（以及它们不相互作用的方式），然后深入研究如何使用原子操作实现更低级别的并发操作。最后，我将以一些建议结束本章，帮助你在处理并发代码时保持理智。
- en: The Trouble with Concurrency
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并发的困境
- en: Before we dive into good patterns for concurrent programming and the details
    of Rust’s concurrency mechanisms, it’s worth taking some time to understand why
    concurrency is challenging in the first place. That is, why do we need special
    patterns and mechanisms for concurrent code?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨并发编程的良好模式以及Rust的并发机制细节之前，花些时间理解并发为何具有挑战性是值得的。换句话说，为什么我们需要并发代码的特殊模式和机制？
- en: Correctness
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正确性
- en: 'The primary difficulty in concurrency is coordinating access—in particular,
    write access—to a resource that is shared among multiple threads. If lots of threads
    want to share a resource solely for the purposes of reading it, then that’s usually
    easy: you stick it in an `Arc` or place it in something you can get a `&''static`
    to, and you’re all done. But once any thread wants to write, all sorts of problems
    arise, usually in the form of *data races*. Briefly, a data race occurs when one
    thread updates shared state while a second thread is also accessing that state,
    either to read it or to update it. Without additional safeguards in place, the
    second thread may read partially overwritten state, clobber parts of what the
    first thread wrote, or fail to see the first thread’s write at all! In general,
    all data races are considered undefined behavior.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 并发中的主要难点在于协调对共享资源的访问——特别是写访问。如果很多线程只是为了读取共享资源而共享它，那么通常很容易：你将它放入一个`Arc`中，或者放入你能获得`&'static`引用的地方，这样就完成了。但一旦有线程想要写入，问题就会层出不穷，通常表现为*数据竞争*。简而言之，数据竞争发生在一个线程更新共享状态时，而第二个线程也在访问该状态，可能是在读取它或更新它。如果没有额外的安全措施，第二个线程可能读取到部分被覆盖的状态，破坏第一个线程写入的部分内容，或根本无法看到第一个线程的写入！一般来说，所有的数据竞争都被认为是未定义行为。
- en: 'Data races are a part of a broader class of problems that primarily, though
    not exclusively, occur in a concurrent setting: *race conditions*. A race condition
    occurs whenever multiple outcomes are possible from a sequence of instructions,
    depending on the relative timing of other events in the system. These events can
    be threads executing a particular piece of code, a timer going off, a network
    packet coming in, or any other time-variable occurrence. Race conditions, unlike
    data races, are not inherently bad, and are not considered undefined behavior.
    However, they are a breeding ground for bugs when particularly peculiar races
    occur, as you’ll see throughout this chapter.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据竞争是更广泛问题类别的一部分，这类问题主要（但不限于）发生在并发环境中：*竞态条件*。竞态条件发生在多个结果可能由一系列指令产生时，这取决于系统中其他事件的相对时机。这些事件可以是线程执行特定代码、定时器触发、网络数据包到达，或任何其他与时间相关的事件。与数据竞争不同，竞态条件并不固有地是坏的，也不被认为是未定义行为。然而，当出现特别奇怪的竞态时，它们会成为漏洞的温床，正如本章中将要展示的那样。
- en: Performance
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能
- en: Often, developers introduce concurrency into their programs in the hope of increasing
    performance. Or, to be more precise, they hope that concurrency will enable them
    to perform more operations per second in aggregate by taking advantage of more
    hardware resources. This can be done on a single core by having one thread run
    while another is waiting, or across multiple cores by having threads do work simultaneously,
    one on each core, that would otherwise happen serially on one core. Most developers
    are referring to the latter kind of performance gain when they talk about concurrency,
    which is often framed in terms of scalability. Scalability in this context means
    “the performance of this program scales with the number of cores,” implying that
    if you give your program more cores, its performance improves.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者通常在程序中引入并发，是希望提高性能。更准确地说，他们希望并发能够通过利用更多的硬件资源来提高每秒操作的总数。这可以在单个核心上通过让一个线程在另一个线程等待时运行，或者在多个核心上通过让线程同时工作（每个核心上一个线程）来完成，这样本来在单个核心上串行执行的操作得以并行化。当开发者谈到并发时，通常指的是后者这种性能提升，常常以“可扩展性”的形式来讨论。在这个语境中，所谓的可扩展性是指“该程序的性能随着核心数的增加而提升”，意味着如果你给程序更多的核心，它的性能会有所提高。
- en: While achieving such a speedup is possible, it’s harder than it seems. The ultimate
    goal in scalability is linear scalability, where doubling the number of cores
    doubles the amount of work your program completes per unit of time. Linear scalability
    is also often called perfect scalability. However, in reality, few concurrent
    programs achieve such speedups. Sublinear scaling is more common, where the throughput
    increases nearly linearly as you go from one core to two, but adding more cores
    yields diminishing returns. Some programs even experience negative scaling, where
    giving the program access to more cores *reduces* throughput, usually because
    the many threads are all contending for some shared resource.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然实现这样的加速是可能的，但比看起来更困难。可扩展性的最终目标是线性扩展性，即当核心数翻倍时，程序每单位时间完成的工作量也翻倍。线性扩展性也常被称为完美扩展性。然而，现实中，很少有并发程序能实现这种加速。亚线性扩展性更为常见，即当从一个核心扩展到两个核心时，吞吐量几乎呈线性增长，但增加更多的核心会带来收益递减。有些程序甚至会经历负扩展性，即让程序使用更多核心反而*减少*吞吐量，通常是因为许多线程都在争用某些共享资源。
- en: It might help to think of a group of people trying to pop all the bubbles on
    a piece of bubble wrap—adding more people helps initially, but at some point you
    get diminishing returns as the crowding makes any one person’s job harder. If
    the humans involved are particularly ineffective, your group may end up standing
    around discussing who should pop next and pop no bubbles at all! This kind of
    interference among tasks that are supposed to execute in parallel is called *contention*
    and is the archnemesis of scaling well. Contention can arise in a number of ways,
    but the primary offenders are mutual exclusion, shared resource exhaustion, and
    false sharing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 可以把它想象成一群人试图将气泡膜上的所有气泡戳破——最初，增加更多的人是有帮助的，但到了某个阶段，由于人太多，每个人的工作反而变得更难。 如果参与的人特别低效，你的团队可能最终会站在那讨论接下来谁该戳破气泡，结果一个气泡也没戳破！这种本应并行执行的任务之间的干扰被称为*争用*，它是扩展性差的死敌。争用可能有多种表现方式，但主要的罪魁祸首是互斥、共享资源耗尽和虚假共享。
- en: Mutual Exclusion
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 互斥
- en: When only a single concurrent task is allowed to execute a particular piece
    of code at any one time, we say that execution of that segment of code is mutually
    exclusive—if one thread executes it, no other thread can do so at the same time.
    The archetypal example of this is a mutual exclusion lock, or *mutex*, which explicitly
    enforces that only one thread gets to enter a particular critical section of your
    program code at any one time. Mutual exclusion can also happen implicitly, however.
    For example, if you spin up a thread to manage a shared resource and send jobs
    to it over an `mpsc` channel, that thread effectively implements mutual exclusion,
    since only one such job gets to execute at a time.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当任何时刻只有一个并发任务可以执行特定代码段时，我们称该段代码的执行是互斥的——如果一个线程执行它，其他线程不能同时执行它。这个概念的典型例子是互斥锁，或称为*mutex*，它明确规定每次只有一个线程可以进入程序代码中的某个关键区段。然而，互斥也可能隐式发生。例如，如果你启动一个线程来管理共享资源，并通过`mpsc`通道发送任务给它，那么这个线程实际上实现了互斥，因为每次只有一个任务可以执行。
- en: Mutual exclusion can also occur when invoking operating system or library calls
    that internally enforce single-threaded access to a critical section. For example,
    for many years, the standard memory allocator required mutual exclusion for some
    allocations, which made memory allocation an operation that incurred significant
    contention in otherwise highly parallel programs. Similarly, many operating system
    operations that may seem like they should be independent, such as creating two
    files with different names in the same directory, may end up having to happen
    sequentially inside the kernel.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 互斥也可能在调用操作系统或库函数时发生，这些函数在内部强制执行对关键区段的单线程访问。例如，多年来，标准的内存分配器在某些分配时需要互斥，这使得内存分配成为一种在其他高度并行的程序中造成显著争用的操作。同样，许多看起来应该是独立的操作系统操作，比如在同一目录下创建两个不同名字的文件，可能最终必须在内核中顺序执行。
- en: Mutual exclusion is the most obvious barrier to parallel speedup since, by definition,
    it forces serial execution of some portion of your program. Even if you make the
    remainder of your program scale with the number of cores perfectly, the total
    speedup you can achieve is limited by the length of the mutually exclusive, serial
    section. Be mindful of your mutually exclusive sections, and seek to restrict
    them to only where strictly necessary.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 互斥是并行加速中最明显的障碍，因为根据定义，它强制执行程序某些部分的串行执行。即使你让程序的其他部分完美地随着核心数扩展，你能达到的总加速仍然受到互斥、串行部分长度的限制。要注意你的互斥部分，并尽量将其限制在严格必要的地方。
- en: Shared Resource Exhaustion
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 共享资源耗尽
- en: Unfortunately, even if you achieve perfect concurrency within your tasks, the
    environment those tasks need to interact with may itself not be perfectly scalable.
    The kernel can handle only so many sends on a given TCP socket per second, the
    memory bus can do only so many reads at once, and your GPU has a limited capacity
    for concurrency. There’s no cure for this. The environment is usually where perfect
    scalability falls apart in practice, and fixes for such cases tend to require
    substantial re-engineering (or even new hardware!), so we won’t talk much more
    about this topic in this chapter. Just remember that scalability is rarely something
    you can “achieve,” and more something you just strive for.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，即使你在任务内部实现了完美的并发，任务需要交互的环境本身可能并不具备完美的可扩展性。内核每秒钟只能处理一定数量的TCP套接字发送，内存总线也只能同时进行有限数量的读取，而你的GPU在并发处理方面也有其容量限制。对此没有解决办法。通常，环境就是在实践中完美可扩展性崩溃的地方，针对这种情况的修复通常需要大规模的重新设计（甚至是新的硬件！），因此我们在本章中不会再多谈这个话题。只要记住，可扩展性很少是你能够“实现”的，而更多的是你需要持续追求的目标。
- en: False Sharing
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 假共享
- en: False sharing occurs when two operations that shouldn’t contend with one another
    contend anyway, preventing efficient simultaneous execution. This usually happens
    because the two operations happen to intersect on some shared resource even though
    they use unrelated parts of that resource.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假共享发生在两个不应该相互竞争的操作，尽管它们不相关，仍然相互竞争，从而阻碍了高效的并行执行。这通常是因为这两个操作恰好在某个共享资源上发生冲突，即便它们使用的是该资源的不同部分。
- en: The simplest example of this is lock oversharing, where a lock guards some composite
    state, and two operations that are otherwise independent both need to take the
    lock to update their particular parts of the state. This in turn means the operations
    must execute serially instead of in parallel. In some cases it’s possible to split
    the single lock into two, one for each of the disjoint parts, which enables the
    operations to proceed in parallel. However, it’s not always straightforward to
    split a lock like this—the state may share a single lock because some third operation
    needs to lock over all the parts of the state. Usually you can still split the
    lock, but you have to be careful about the order in which different threads take
    the split locks to avoid deadlocks that can occur when two operations attempt
    to take them in different orders (look up the “dining philosophers problem,” if
    you’re curious). Alternatively, for some problems, you may be able to avoid the
    critical section entirely by using a lock-free version of the underlying algorithm,
    though those are also tricky to get right. Ultimately, false sharing is a hard
    problem to solve, and there isn’t a single catchall solution—but identifying the
    problem is a good start.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题最简单的例子是锁的过度共享，其中一个锁保护某个复合状态，而两个原本独立的操作都需要获取锁来更新它们各自的状态部分。这就意味着这些操作必须串行执行，而无法并行执行。在某些情况下，可以将一个锁分成两个，每个操作负责独立的部分，这样操作就可以并行进行。然而，这种分锁并不总是直接可行——状态可能因为某个第三操作需要对所有状态部分加锁，所以使用了单一锁。通常情况下，你仍然可以分锁，但必须小心不同线程获取分锁的顺序，以避免死锁的发生——死锁发生在两个操作尝试按不同的顺序获取锁时（如果你感兴趣，可以查阅“哲学家就餐问题”）。另外，对于某些问题，你也许能够完全避免临界区，通过使用底层算法的无锁版本，尽管这些也很难做到完美。归根结底，假共享是一个难以解决的问题，没有一个通用的解决方案，但识别出问题本身就是一个良好的开始。
- en: A more subtle example of false sharing occurs on the CPU level, as we discussed
    briefly in Chapter 2. The CPU internally operates on memory in terms of cache
    lines—longer sequences of consecutive bytes in memory—rather than individual bytes,
    to amortize the cost of memory accesses. For example, on most Intel processors,
    the cache line size is 64 bytes. This means that every memory operation really
    ends up reading or writing some multiple of 64 bytes. The false sharing comes
    into play when two cores want to update the value of two different bytes that
    happen to fall on the same cache line; those updates must execute sequentially
    even though the updates are logically disjoint.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 伪共享的一个更微妙的例子出现在 CPU 层面，正如我们在第二章中简要讨论的那样。CPU 在内部按缓存行操作内存——即内存中连续字节的较长序列——而不是按单个字节操作，以摊销内存访问的成本。例如，在大多数英特尔处理器上，缓存行的大小是
    64 字节。这意味着每个内存操作实际上最终读取或写入的是 64 字节的整数倍。伪共享的发生是在两个核心希望更新两个不同字节的值，而这两个字节恰好位于同一个缓存行中时；即使这些更新在逻辑上是分离的，这些更新也必须顺序执行。
- en: This might seem too low-level to matter, but in practice this kind of false
    sharing can decimate the parallel speedup of an application. Imagine that you
    allocate an array of integer values to indicate how many operations each thread
    has completed, but the integers all fall within the same cache line—now, all your
    otherwise parallel threads will contend on that one cache line for every operation
    they perform. If the operations are relatively quick, *most* of your execution
    time may end up being spent contending on those counters!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来可能太低级，不值得关注，但实际上，这种伪共享会严重影响应用程序的并行加速。想象一下，你为每个线程分配了一个整数数组来表示它完成了多少个操作，但这些整数都位于同一个缓存行内——现在，所有原本并行的线程将在每次执行操作时争用那一行缓存。如果这些操作比较快速，*大部分*执行时间可能最终都花费在争用这些计数器上！
- en: The trick to avoiding false cache line sharing is to pad your values so that
    they are the size of a cache line. That way, two adjacent values always fall on
    different cache lines. But of course, this also inflates the size of your data
    structures, so use this approach only when benchmarks indicate a problem.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 避免伪缓存行共享的技巧是通过填充你的值，使其大小与缓存行相等。这样，两个相邻的值总是位于不同的缓存行上。当然，这也会增加数据结构的大小，所以只有当基准测试表明存在问题时，才使用这种方法。
- en: Concurrency Models
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并发模型
- en: 'Rust has three patterns for adding concurrency to your programs that you’ll
    come across fairly often: shared memory concurrency, worker pools, and actors.
    Going through every way you could add concurrency in detail would take a book
    of its own, so here I’ll focus on just these three patterns.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Rust 有三种常见的并发模式，你很可能会遇到：共享内存并发、工作池和演员模型。要详细介绍每种实现并发的方法本身就足以写一本书，因此在这里，我将专注于这三种模式。
- en: Shared Memory
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 共享内存
- en: 'Shared memory concurrency is, conceptually, very straightforward: the threads
    cooperate by operating on regions of memory shared between them. This might take
    the form of state guarded by a mutex or stored in a hash map with support for
    concurrent access from many threads. The many threads may be doing the same task
    on disjoint pieces of data, such as if many threads perform some function over
    disjoint subranges of a `Vec`, or they may be performing different tasks that
    require some shared state, such as in a database where one thread handles user
    queries to a table while another optimizes the data structures used to store that
    table in the background.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存并发，从概念上讲，非常简单：线程通过在它们之间共享的内存区域上进行操作来协作。这可能表现为由互斥锁保护的状态，或者存储在支持多个线程并发访问的哈希映射中。多个线程可能在不重叠的数据片段上执行相同的任务，例如多个线程对`Vec`的不同子范围执行某些功能，或者它们可能执行需要一些共享状态的不同任务，例如在数据库中，一个线程处理用户对表的查询，而另一个线程在后台优化用于存储该表的数据结构。
- en: When you use shared memory concurrency, your choice of data structures is significant,
    especially if the threads involved need to cooperate very closely. A regular mutex
    might prevent scaling beyond a very small number of cores, a reader/writer lock
    might allow many more concurrent reads at the cost of slower writes, and a sharded
    reader/writer lock might allow perfectly scalable reads at the cost of making
    writes highly disruptive. Similarly, some concurrent hash maps aim for good all-round
    performance while others specifically target, say, concurrent reads where writes
    are rare. In general, in shared memory concurrency, you want to use data structures
    that are specifically designed for something as close to your target use case
    as possible, so that you can take advantage of optimizations that trade off performance
    aspects your application does not care about for those it does.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用共享内存并发时，选择合适的数据结构非常重要，尤其是当涉及的线程需要紧密配合时。一个常规的互斥锁可能会限制核心数的扩展，而一个读写锁可能以牺牲写操作速度为代价，允许更多的并发读取，而一个分片的读写锁可能允许完美可扩展的读取，但会使写操作变得高度破坏性。类似地，一些并发哈希映射旨在提供良好的全方位性能，而其他则特别针对例如并发读取，在写操作较为稀少的场景下表现更好。通常，在共享内存并发中，你希望使用那些专门为你的目标用例设计的数据结构，这样你就可以利用那些针对你应用程序不关心的性能方面做出的优化，换取那些你关注的性能优化。
- en: Shared memory concurrency is a good fit for use cases where threads need to
    jointly update some shared state in a way that does not commute. That is, if one
    thread has to update the state `s` with some function `f`, and another has to
    update the state with some function `g`, and `f(g(s)) != g(f(s))`, then shared
    memory concurrency is likely necessary. If that is not the case, the other two
    patterns are likely better fits, as they tend to lead to simpler and more performant
    designs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存并发非常适合那些线程需要以一种不交换顺序的方式共同更新一些共享状态的用例。也就是说，如果一个线程必须使用某个函数`f`更新状态`s`，而另一个线程必须使用另一个函数`g`更新状态，并且`f(g(s))
    != g(f(s))`，那么共享内存并发可能是必需的。如果情况并非如此，其他两种模式可能会更适合，因为它们通常能带来更简单和更高性能的设计。
- en: Worker Pools
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作池
- en: In the worker pool model, many identical threads receive jobs from a shared
    job queue, which they then execute entirely independently. Web servers, for example,
    often have a worker pool handling incoming connections, and multithreaded runtimes
    for asynchronous code tend to use a worker pool to collectively execute all of
    an application’s futures (or, more accurately, its top-level tasks).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作池模型中，许多相同的线程从共享的任务队列中获取任务，然后完全独立地执行这些任务。例如，Web 服务器通常会有一个工作池来处理传入的连接，而异步代码的多线程运行时通常使用工作池来共同执行应用程序的所有未来任务（或者更准确地说，是其顶层任务）。
- en: The lines between shared memory concurrency and worker pools are often blurry,
    as worker pools tend to use shared memory concurrency to coordinate how they take
    jobs from the queue and how they return incomplete jobs back to the queue. For
    example, say you’re using the data parallelism library `rayon` to perform some
    function over every element of a vector in parallel. Behind the scenes `rayon`
    spins up a worker pool, splits the vector into subranges, and then hands out subranges
    to the threads in the pool. When a thread in the pool finishes a range, `rayon`
    arranges for it to start working on the next unprocessed subrange. The vector
    is shared among all the worker threads, and the threads coordinate through a shared
    memory queue–like data structure that supports work stealing.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存并发和工作池之间的界限通常是模糊的，因为工作池通常使用共享内存并发来协调它们如何从队列中获取任务，以及如何将未完成的任务返回队列。例如，假设你正在使用数据并行库`rayon`来并行处理一个向量的每个元素。在幕后，`rayon`会启动一个工作池，将向量分割成子区间，然后将子区间分配给池中的线程。当池中的某个线程完成一个子区间时，`rayon`会安排它开始处理下一个未处理的子区间。向量在所有工作线程之间共享，线程通过一个支持工作窃取的共享内存队列样式的数据结构进行协调。
- en: Work stealing is a key feature of most worker pools. The basic premise is that
    if one thread finishes its work early, and there’s no more unassigned work available,
    that thread can steal jobs that have already been assigned to a different worker
    thread but haven’t been started yet. Not all jobs take the same amount of time
    to complete, so even if every worker is given the same *number* of jobs, some
    workers may end up finishing their jobs more quickly than others. Rather than
    sit around and wait for the threads that drew longer-running jobs to complete,
    those threads that finish early should help the stragglers so the overall operation
    is completed sooner.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 工作窃取是大多数工作池的一个关键特性。其基本前提是，如果某个线程提前完成了工作，并且没有更多未分配的工作可用，那么该线程可以窃取已经分配给其他工作线程但尚未开始的工作。并非所有的工作都需要相同的时间来完成，因此即使每个工作线程被分配了相同的*工作数量*，一些线程可能比其他线程更快完成它们的任务。与其坐着等那些执行较长任务的线程完成，不如让那些提前完成的线程去帮助落后的线程，从而使整体操作能够更快完成。
- en: It’s quite a task to implement a data structure that supports this kind of work
    stealing without incurring significant overhead from threads constantly trying
    to steal work from one another, but this feature is vital to a high-performance
    worker pool. If you find yourself in need of a worker pool, your best bet is usually
    to use one that has already seen a lot of work go into it, or at least reuse data
    structures from an existing one, rather than to write one yourself from scratch.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 实现一个支持这种工作窃取的数据结构是相当困难的，尤其是在不引入线程间不断相互窃取工作的显著开销的情况下，但这个特性对高性能的工作池至关重要。如果你需要一个工作池，通常最好的选择是使用已经经过大量工作验证的工作池，或者至少重用现有工作池的数据结构，而不是从头开始自己编写。
- en: Worker pools are a good fit when the work that each thread performs is the same,
    but the data it performs it *on* varies. In a `rayon` parallel map operation,
    every thread performs the same map computation; they just perform it on different
    subsets of the underlying data. In a multithreaded asynchronous runtime, each
    thread simply calls `Future::poll`; they just call it on different futures. If
    you start having to distinguish between the threads in your thread pool, a different
    design is probably more appropriate.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当每个线程执行的工作相同，但它所处理的数据*不同*时，工作池是一个不错的选择。在`rayon`并行映射操作中，每个线程执行相同的映射计算，只是它们在不同的底层数据子集上执行这个计算。在一个多线程异步运行时，每个线程简单地调用`Future::poll`，它们只是针对不同的future进行调用。如果你开始需要区分线程池中的线程，那么可能需要考虑采用不同的设计。
- en: Actors
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Actor
- en: The actor concurrency model is, in many ways, the opposite of the worker pool
    model. Whereas the worker pool has many identical threads that share a job queue,
    the actor model has many separate job queues, one for each job “topic.” Each job
    queue feeds into a particular actor, which handles all jobs that pertain to a
    subset of the application’s state. That state might be a database connection,
    a file, a metrics collection data structure, or any other structure that you can
    imagine many threads may need to be able to access. Whatever it is, a single actor
    owns that state, and if some task wants to interact with that state, it needs
    to send a message to the owning actor summarizing the operation it wishes to perform.
    When the owning actor receives that message, it performs the indicated action
    and responds to the inquiring task with the result of the operation, if relevant.
    Since the actor has exclusive access to its inner resource, no locks or other
    synchronization mechanisms are required beyond what’s needed for the messaging.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Actor并发模型在许多方面与工作池模型正好相反。工作池有许多相同的线程共享一个工作队列，而actor模型则有许多独立的工作队列，每个队列对应一个“话题”。每个工作队列输入到一个特定的actor，由它处理所有与应用程序状态子集相关的任务。这个状态可能是数据库连接、文件、度量数据结构，或者任何你能想象的，可能需要多个线程访问的结构。不管是什么，单个actor拥有该状态，如果某个任务想与这个状态交互，它需要向拥有者actor发送一条消息，概述它希望执行的操作。当拥有者actor接收到消息后，它执行指定的操作，并在相关的情况下将操作结果反馈给询问任务。由于actor对其内部资源拥有独占访问权限，因此除了进行消息传递所需的同步机制外，不需要其他锁或同步机制。
- en: A key point in the actor pattern is that actors all talk to one another. If,
    say, an actor that is responsible for logging needs to write to a file and a database
    table, it might send off messages to the actors responsible for each of those,
    asking them to perform the respective actions, and then proceed to the next log
    event. In this way, the actor model more closely resembles a web than spokes on
    a wheel—a user request to a web server might start as a single request to the
    actor responsible for that connection but might transitively spawn tens, hundreds,
    or even thousands of messages to actors deeper in the system before the user’s
    request is satisfied.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Actor 模式的一个关键点是，所有的 actor 都会相互通信。例如，负责日志记录的 actor 如果需要写入文件和数据库表，它可能会向负责这两项操作的
    actor 发送消息，要求它们分别执行相应的动作，然后继续处理下一个日志事件。通过这种方式，actor 模型更像是一个网络，而不是轮子上的辐条——一个用户请求到网络服务器可能开始时只是向负责该连接的
    actor 发出的单一请求，但可能会传递出成百上千条消息，最终到达系统深处的多个 actor，才满足用户的请求。
- en: Nothing in the actor model requires that each actor is its own thread. To the
    contrary, most actor systems suggest that there should be a large number of actors,
    and so each actor should map to a task rather than a thread. After all, actors
    require exclusive access to their wrapped resources only when they execute, and
    do not care whether they are on a thread of their own or not. In fact, very frequently,
    the actor model is used in conjunction with the worker pool model—for example,
    an application that uses the multithreaded asynchronous runtime Tokio can spawn
    an asynchronous task for each actor, and Tokio will then make the execution of
    each actor a job in its worker pool. Thus, the execution of a given actor may
    move from thread to thread in the worker pool as the actor yields and resumes,
    but every time the actor executes it maintains exclusive access to its wrapped
    resource.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在 actor 模式中，没有任何要求每个 actor 都必须是一个独立的线程。相反，大多数 actor 系统建议应该有大量的 actor，因此每个 actor
    应该映射为一个任务，而不是一个线程。毕竟，actor 只在执行时需要独占其封装的资源，并不关心它是否运行在自己的线程上。实际上，actor 模型通常与工作池模型一起使用——例如，使用多线程异步运行时
    Tokio 的应用程序可以为每个 actor 启动一个异步任务，Tokio 会将每个 actor 的执行转变为工作池中的一项工作。因此，给定 actor 的执行可能会在工作池中的线程之间移动，因为
    actor 会在执行时交出控制权并在稍后恢复执行，但每次执行时，actor 都会保持对其封装资源的独占访问。
- en: The actor concurrency model is well suited for when you have many resources
    that can operate relatively independently, and where there is little or no opportunity
    for concurrency within each resource. For example, an operating system might have
    an actor responsible for each hardware device, and a web server might have an
    actor for each backend database connection. The actor model does not work so well
    if you need only a few actors, if work is skewed significantly among the actors,
    or if some actors grow large—in all of those cases, your application may end up
    being bottlenecked on the execution speed of a single actor in the system. And
    since actors each expect to have exclusive access to their little slice of the
    world, you can’t easily parallelize the execution of that one bottleneck actor.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Actor 并发模型非常适合于资源可以相对独立操作，且每个资源内部几乎没有或没有并发机会的情况。例如，操作系统可能会为每个硬件设备分配一个 actor，网络服务器可能会为每个后端数据库连接分配一个
    actor。如果你只需要少数几个 actor，或者工作负载在各个 actor 之间差异很大，或者某些 actor 变得很大，那么 actor 模型可能就不太适用了——在这些情况下，你的应用程序可能会因为系统中某个单一
    actor 的执行速度成为瓶颈。而且，由于每个 actor 都期望独占它所负责的资源，因此你无法轻易地将这个瓶颈 actor 的执行过程并行化。
- en: Asynchrony and Parallelism
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异步与并行
- en: As we discussed in Chapter 8, asynchrony in Rust enables concurrency without
    parallelism—we can use constructs like selects and joins to have a single thread
    poll multiple futures and continue when one, some, or all of them complete. Because
    there is no parallelism involved, concurrency with futures does not fundamentally
    require those futures to be `Send`. Even spawning a future to run as an additional
    top-level task does not fundamentally require `Send`, since a single executor
    thread can manage the polling of many futures at once.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第 8 章中讨论的，Rust 中的异步性实现了没有并行的并发——我们可以使用诸如 select 和 join 这样的结构，让单个线程轮询多个
    future，并在其中一个、一些或所有 future 完成时继续执行。由于没有涉及并行，因此并发使用 futures 并不要求这些 futures 必须是
    `Send`。即使是将一个 future 作为附加的顶层任务进行启动，也不需要 `Send`，因为单个执行线程可以同时管理多个 future 的轮询。
- en: 'However, in *most* cases, applications want both concurrency and parallelism.
    For example, if a web application constructs a future for each incoming connection
    and so has many active connections at once, it probably wants the asynchronous
    executor to be able to take advantage of more than one core on the host computer.
    That won’t happen naturally: your code has to explicitly tell the executor which
    futures can run in parallel and which cannot.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在*大多数*情况下，应用程序既需要并发也需要并行。例如，如果一个Web应用程序为每个传入的连接构建一个`Future`，因此会有多个活跃连接同时存在，它可能希望异步执行器能够利用主机计算机的多个核心。这不会自然而然地发生：你的代码必须明确告诉执行器哪些`Future`可以并行执行，哪些不能。
- en: In particular, two pieces of information must be given to the executor to let
    it know that it can spread the work in the futures across a worker pool of threads.
    The first is that the futures in question are `Send`—if they aren’t, the executor
    is not allowed to send the futures to other threads for processing, and no parallelism
    is possible; only the thread that constructed such futures can poll them.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，必须向执行器提供两条信息，以便让它知道可以将`Future`中的工作分配到线程池中。第一条是这些`Future`是`Send`类型——如果不是，执行器将不允许将这些`Future`发送到其他线程进行处理，也就无法实现并行；只有构建这些`Future`的线程才能调用它们的`poll`方法。
- en: 'The second piece of information is how to split the futures into tasks that
    can operate independently. This ties back to the discussion of tasks versus futures
    from Chapter 8: if one giant `Future` contains a number of `Future` instances
    that themselves correspond to tasks that can run in parallel, the executor must
    still call `poll` on the top-level `Future`, and it must do so from a single thread,
    since `poll` requires `&mut self`. Thus, to achieve parallelism with futures,
    you have to explicitly spawn the futures you want to be able to run in parallel.
    Also, because of the first requirement, the executor function you use to do so
    will require that the passed-in `Future` is `Send`.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个信息是如何将`Future`分割成可以独立操作的任务。这与第8章讨论的任务与`Future`的关系有关：如果一个巨大的`Future`包含多个`Future`实例，而这些`Future`实例本身对应的是可以并行运行的任务，那么执行器仍然必须在顶层`Future`上调用`poll`，并且必须由单个线程来调用，因为`poll`需要`&mut
    self`。因此，为了实现并行，必须显式地生成你希望能够并行运行的`Future`。此外，由于第一个要求，执行器函数需要保证传入的`Future`是`Send`类型。
- en: Lower-Level Concurrency
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低级并发
- en: The standard library provides the `std::sync::atomic` module, which provides
    access to the underlying CPU primitives, higher-level constructs like channels
    and mutexes are built with. These primitives come in the form of atomic types
    with names starting with `Atomic`—`AtomicUsize`, `AtomicI32`, `AtomicBool`, `AtomicPtr`,
    and so on—the `Ordering` type, and two functions called `fence` and `compiler_fence`.
    We’ll look at each of these over the next few sections.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 标准库提供了`std::sync::atomic`模块，它提供了对底层CPU原语的访问，像通道和互斥锁这样的高级构造正是基于这些原语构建的。这些原语以以`Atomic`开头的原子类型的形式出现——`AtomicUsize`、`AtomicI32`、`AtomicBool`、`AtomicPtr`等等，还有`Ordering`类型，以及两个名为`fence`和`compiler_fence`的函数。我们将在接下来的几个部分详细讲解这些内容。
- en: These types are the blocks used to build any code that has to communicate between
    threads. Mutexes, channels, barriers, concurrent hash tables, lock-free stacks,
    and all other synchronization constructs ultimately rely on these few primitives
    to do their jobs. They also come in handy on their own for lightweight cooperation
    between threads where heavyweight synchronization like a mutex is excessive—for
    example, to increment a shared counter or set a shared Boolean to `true`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类型是构建任何需要在线程之间通信的代码的基础块。互斥锁、通道、屏障、并发哈希表、无锁栈以及所有其他同步构造最终都依赖于这些原语来完成它们的工作。它们本身也非常有用，特别是在需要线程间轻量级协作的场景中，这时像互斥锁这样的重型同步机制显得过于繁重——例如，用来递增共享计数器或将共享布尔值设置为`true`。
- en: 'The atomic types are special in that they have defined semantics for what happens
    when multiple threads try to access them concurrently. These types all support
    (mostly) the same API: `load`, `store`, `fetch_*`, and `compare_exchange`. In
    the rest of this section, we’ll look at what those do, how to use them correctly,
    and what they’re useful for. But first, we have to talk about low-level memory
    operations and memory ordering.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 原子类型之所以特别，是因为它们定义了多个线程尝试并发访问时应该发生的语义。这些类型都支持（大致上）相同的API：`load`、`store`、`fetch_*`和`compare_exchange`。在本节的其余部分，我们将探讨这些函数的作用，如何正确使用它们，以及它们的应用场景。但首先，我们必须讨论低级内存操作和内存排序。
- en: Memory Operations
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存操作
- en: Informally, we often refer to accessing variables as “reading from” or “writing
    to” memory. In reality, a lot of machinery between code uses a variable and the
    actual CPU instructions that access your memory hardware. It’s important to understand
    that machinery, at least at a high level, in order to understand how concurrent
    memory accesses behave.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 非正式地，我们通常将访问变量称为“从内存读取”或“写入内存”。实际上，代码使用变量与实际的CPU指令访问内存硬件之间有很多中间机制。理解这些机制，至少从高层次理解它们，对于理解并发内存访问的行为至关重要。
- en: The compiler decides what instructions to emit when your program reads the value
    of a variable or assigns a new value to it. It is permitted to perform all sorts
    of transformations and optimizations on your code and may end up reordering your
    program statements, eliminating operations it deems redundant, or using CPU registers
    rather than actual memory to store intermediate computations. The compiler is
    subject to a number of restrictions on these transformations, but ultimately only
    a subset of your variable accesses actually end up as memory access instructions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器决定在程序读取变量的值或给变量赋新值时发出哪些指令。它可以对代码进行各种转换和优化，并可能最终重新排序程序语句、消除其认为多余的操作，或使用CPU寄存器而不是实际内存来存储中间计算结果。编译器在这些转换上受到一定的限制，但最终只有部分变量访问会实际转化为内存访问指令。
- en: 'At the CPU level, memory instructions come in two main shapes: loads and stores.
    A load pulls bytes from a location in memory into a CPU register, and a store
    stores bytes from a CPU register into a location in memory. Loads and stores operate
    on small chunks of memory at a time: usually 8 bytes or less on modern CPUs. If
    a variable access spans more bytes than can be accessed with a single load or
    store, the compiler automatically turns it into multiple load or store instructions,
    as appropriate. The CPU also has some leeway in how it executes a program’s instructions
    to make better use of the hardware and improve program performance. For example,
    modern CPUs often execute instructions in parallel, or even out of order, when
    they don’t have dependencies on each other. There are also several layers of caches
    between each CPU and your computer’s DRAM, which means that a load of a given
    memory location may not necessarily see the latest store to that memory location,
    going by wall-clock time.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU级别，内存指令有两种主要形式：加载和存储。加载指令将内存位置的字节拉入CPU寄存器，存储指令则将CPU寄存器中的字节存储到内存位置。加载和存储操作一次处理较小的内存块：在现代CPU上通常是8个字节或更少。如果一个变量访问跨越的字节数超过了单次加载或存储可以访问的范围，编译器会自动将其转化为多个加载或存储指令，视情况而定。CPU在执行程序指令时也有一定的灵活性，以更好地利用硬件并提高程序性能。例如，现代CPU常常并行执行指令，甚至在指令之间没有依赖关系时会乱序执行。此外，CPU与计算机的DRAM之间还有几层缓存，这意味着对给定内存位置的加载可能不会看到最新的存储操作，按时钟时间来计算。
- en: In most code, the compiler and CPU are permitted to transform the code only
    in ways that don’t affect the semantics of the resulting program, so these transformations
    are invisible to the programmer. However, in the context of parallel execution,
    these transformations can have a significant impact on application behavior. Therefore,
    CPUs typically provide multiple different variations of the load and store instructions,
    each with different guarantees about how the CPU may reorder them and how they
    may be interleaved with parallel operations on other CPUs. Similarly, compilers
    (or rather, the language the compiler compiles) provide different annotations
    you can use to force particular execution constraints for some subset of their
    memory accesses. In Rust, those annotations come in the form of the atomic types
    and their methods, which we’ll spend the rest of this section picking apart.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数代码中，编译器和CPU只能以不影响程序语义的方式转换代码，因此这些转换对程序员是不可见的。然而，在并行执行的上下文中，这些转换可能对应用程序的行为产生重大影响。因此，CPU通常提供多种不同的加载和存储指令变体，每种变体都有不同的保证，说明CPU如何重新排序它们以及它们如何与其他CPU上的并行操作交错执行。类似地，编译器（或者更准确地说，编译器编译的语言）提供了不同的注解，您可以使用这些注解为其某些内存访问的子集强制执行特定的执行约束。在Rust中，这些注解以原子类型及其方法的形式出现，我们将在本节的剩余部分对其进行详细分析。
- en: Atomic Types
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原子类型
- en: Rust’s atomic types are so called because they can be accessed atomically—that
    is, the value of an atomic-type variable is written all at once and will never
    be written using multiple stores, guaranteeing that a load of that variable cannot
    observe that only some of the bytes composing the value have changed while others
    have not (yet). This is easiest understood by way of contrast with non-atomic
    types. For example, reassigning a new value to a tuple of type `(i64, i64)` typically
    requires two CPU store instructions, one for each 8-byte value. If one thread
    were to perform both of those stores, another thread could (if we ignore the borrow
    checker for a second) read the tuple’s value after the first store but before
    the second, and thus end up with an inconsistent view of the tuple’s value. It
    would end up reading the new value for the first element and the old value for
    the second element, a value that was never actually stored by any thread.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Rust 的原子类型之所以被称为原子类型，是因为它们可以原子地访问——也就是说，原子类型变量的值是一次性写入的，绝不会通过多个存储操作进行写入，从而保证了对该变量的加载操作无法观察到只有部分字节发生了变化，而其他字节尚未发生变化（或还没有发生变化）。通过与非原子类型的对比，最容易理解这一点。例如，将新值重新赋给类型为
    `(i64, i64)` 的元组通常需要两条 CPU 存储指令，每条指令对应一个 8 字节的值。如果一个线程执行这两条存储指令，另一个线程则可以（如果暂时忽略借用检查器）在第一次存储之后、第二次存储之前读取该元组的值，从而得到该元组值的不一致视图。它最终会读取到第一个元素的新值和第二个元素的旧值，而第二个元素的值根本没有被任何线程存储过。
- en: The CPU can atomically access values only of certain sizes, so there are only
    a few atomic types, all of which live in the `atomic` module. Each atomic type
    is of one of the sizes the CPU supports atomic access to, with multiple variations
    for things like whether the value is signed and to differentiate between an atomic
    `usize` and a pointer (which is of the same size as `usize`). Furthermore, the
    atomic types have explicit methods for loading and storing the values they hold,
    and a handful of more complex methods we’ll get back to later, so that the mapping
    between the code the programmer writes and the resulting CPU instructions is clearer.
    For example, `AtomicI32::load` performs a single load of a signed 32-bit value,
    and `AtomicPtr::store` performs a single store of a pointer-sized (64 bits on
    a 64-bit platform) value.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 只能原子地访问某些大小的值，因此只有少数几种原子类型，所有这些类型都位于 `atomic` 模块中。每种原子类型的大小都是 CPU 支持原子访问的大小，并且针对诸如值是否为有符号类型、区分原子
    `usize` 和指针（其大小与 `usize` 相同）等情况提供了多个变体。此外，原子类型还具有显式的方法，用于加载和存储它们所持有的值，并且还有一些更复杂的方法，我们稍后会回到这些方法，这样可以使程序员编写的代码与最终生成的
    CPU 指令之间的映射更加清晰。例如，`AtomicI32::load` 执行对一个有符号 32 位值的单次加载操作，而 `AtomicPtr::store`
    执行对一个指针大小的值（在 64 位平台上为 64 位）的单次存储操作。
- en: Memory Ordering
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存排序
- en: Most of the methods on the atomic types take an argument of type `Ordering`,
    which dictates the memory ordering restrictions the atomic operation is subject
    to. Across different threads, loads and stores of an atomic value may be sequenced
    by the compiler and CPU only in interleavings that are compatible with the requested
    memory ordering of each of the atomic operations on that atomic value. Over the
    next few sections, we’ll see some examples of why control over the ordering is
    important and necessary to get the expected semantics out of the compiler and
    CPU.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数原子类型的方法都接受一个 `Ordering` 类型的参数，该参数决定了原子操作所受的内存排序限制。在不同线程之间，原子值的加载和存储可能仅由编译器和
    CPU 以与每个原子操作请求的内存排序兼容的交错方式来排序。在接下来的几个部分中，我们将看到一些示例，说明为什么对排序的控制对于从编译器和 CPU 获得预期的语义是重要且必要的。
- en: Memory ordering often comes across as counterintuitive, because we humans like
    to read programs from top to bottom and imagine that they execute line by line—but
    that’s not how the code actually executes when it hits the hardware. Memory accesses
    can be reordered, or even entirely elided, and writes on one thread may not immediately
    be visible to other threads, even if later writes in program order have already
    been observed.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 内存排序常常显得直觉上反常，因为我们人类习惯从上到下阅读程序，并想象它们是逐行执行的——但实际上，当代码运行到硬件时，并不是按这个方式执行的。内存访问可能会被重新排序，甚至可能完全省略，一个线程的写操作可能不会立即被其他线程看到，即使程序顺序中稍后的写操作已经被观察到。
- en: 'Think of it like this: each memory location sees a sequence of modifications
    coming from different threads, and the sequences of modifications for different
    memory locations are independent. If two threads T1 and T2 both write to memory
    location M, then even if T1 executed first as measured by a user with a stopwatch,
    T2’s write to M may still appear to have happened first for M absent any other
    constraints between the two threads’ execution. Essentially, *the computer does
    not take wall-clock time into account* when it determines the value of a given
    memory location—all that matter are the execution constraints the programmer puts
    on what constitutes a valid execution. For example, if T1 writes to M and then
    spawns thread T2, which then writes to M, the computer must recognize T1’s write
    as having happened first because T2’s existence depends on T1.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 可以这样理解：每个内存位置看到的是来自不同线程的一系列修改，并且不同内存位置的修改序列是相互独立的。如果两个线程 T1 和 T2 都写入内存位置 M，那么即使
    T1 按照用户用秒表测量的顺序先执行，T2 对 M 的写操作仍然可能看起来先于 T1 执行，前提是两者执行之间没有其他约束。实际上，*计算机在确定给定内存位置的值时并不考虑墙钟时间*——唯一重要的是程序员对什么构成有效执行所施加的执行约束。例如，如果
    T1 向 M 写入数据，然后启动线程 T2，T2 然后写入 M，计算机必须识别 T1 的写操作发生在前，因为 T2 的存在依赖于 T1。
- en: 'If that’s hard to follow, don’t fret—memory ordering can be mind-bending, and
    language specifications tend to use very precise but not very intuitive wording
    to describe it. We can construct a mental model that’s easier to grasp, if a little
    simplified, by instead focusing on the underlying hardware architecture. Very
    basically, your computer memory is structured as a treelike hierarchy of storage
    where the leaves are CPU registers and the roots are the storage on your physical
    memory chips, often called main memory. Between the two are several layers of
    caches, and different layers of the hierarchy can reside on different pieces of
    hardware. When a thread performs a store to a memory location, what really happens
    is that the CPU starts a write request for the value in a given CPU register that
    then has to make its way up the memory hierarchy toward main memory. When a thread
    performs a load, the request flows up the hierarchy until it hits a layer that
    has the value available, and returns from there. Herein lies the problem: writes
    aren’t visible everywhere until all caches of the written memory location have
    been updated, but other CPUs can execute instructions against the same memory
    location at the same time, and weirdness ensues. Memory ordering, then, is a way
    to request precise semantics for what happens when multiple CPUs access a particular
    memory location for a particular operation.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这很难理解，不用担心——内存顺序是个令人费解的话题，而且语言规范通常使用非常精确但不太直观的措辞来描述它。我们可以通过关注底层硬件架构构建一个更易理解的心理模型，尽管它稍微简化一些。基本来说，你的计算机内存是作为一个树状层级结构来组织的，其中叶子节点是
    CPU 寄存器，根节点是存储在物理内存芯片上的数据，通常称为主内存。两者之间有多个层级的缓存，并且不同层级的缓存可能位于不同的硬件上。当一个线程向内存位置写入时，实际上发生的是
    CPU 启动一个写请求，该请求会从给定的 CPU 寄存器开始，最终向上进入内存层级直到主内存。当一个线程执行加载操作时，请求会沿层级流动，直到找到有该值的缓存层级并从那里返回。问题在于：写操作并非在所有地方都可见，直到所有缓存都更新了被写入的内存位置，但其他
    CPU 可以同时对同一内存位置执行指令，这时就会出现奇怪的情况。因此，内存顺序是一种请求精确语义的方法，描述在多个 CPU 访问特定内存位置时发生了什么操作。
- en: With this in mind, let’s take a look at the `Ordering` type, which is the primary
    mechanism by which we, as programmers, can dictate additional constraints on what
    concurrent executions are valid.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这一点后，我们来看看 `Ordering` 类型，它是我们作为程序员用来对并发执行施加额外约束的主要机制。
- en: '`Ordering` is defined as an `enum` with the variants shown in [Listing 10-1](#listing10-1).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`Ordering` 被定义为一个 `enum`，其变体如 [列表 10-1](#listing10-1) 所示。'
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Listing 10-1: The definition of `Ordering`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10-1：`Ordering` 的定义
- en: Each of these places different restrictions on the mapping from source code
    to execution semantics, and we’ll explore each one in turn in the remainder of
    this section.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些地方对源代码到执行语义的映射施加不同的限制，接下来我们将依次探讨每一个。
- en: Relaxed Ordering
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 放松顺序
- en: Relaxed ordering essentially guarantees nothing about concurrent access to the
    value beyond the fact that the access is atomic. In particular, relaxed ordering
    gives no guarantees about the relative ordering of memory accesses across different
    threads. This is the weakest form of memory ordering. [Listing 10-2](#listing10-2)
    shows a simple program in which two threads access two atomic variables using
    `Ordering::Relaxed`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 放松排序本质上并不对并发访问该值提供任何保证，除了访问是原子的这一事实。特别地，放松排序并不保证不同线程间内存访问的相对顺序。这是最弱的内存排序形式。[示例
    10-2](#listing10-2) 展示了一个简单的程序，其中两个线程使用 `Ordering::Relaxed` 访问两个原子变量。
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Listing 10-2: Two racing threads with `Ordering::Relaxed`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 10-2：两个竞态线程与 `Ordering::Relaxed`
- en: Looking at the thread spawned as `t2`, you might expect that `r2` can never
    be `true`, since all values are `false` until the same thread assigns `true` to
    `Y` on the line *after* reading `X`. However, with a relaxed memory ordering,
    that outcome is completely possible. The reason is that the CPU is allowed to
    reorder the loads and stores involved. Let’s walk through exactly what happens
    here to make `r2 = true` possible.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 查看作为 `t2` 生成的线程，你可能会认为 `r2` 永远不会是 `true`，因为所有的值在同一线程将 `true` 赋值给 `Y` 之后，直到读取
    `X` 这一行*才*变成 `true`，之前都是 `false`。然而，在放宽的内存排序下，这个结果完全是可能的。原因在于，CPU 允许重新排序涉及的加载和存储操作。让我们逐步分析一下，是什么让
    `r2 = true` 成为可能。
- en: First, the CPU notices that 4 doesn’t have to happen after 3, since 4 doesn’t
    use any output or side effect of 3. That is, 4 has no execution dependency on
    3. So, the CPU decides to reorder them for *waves hands* reasons that’ll make
    your program go faster. The CPU thus goes ahead and executes 4 first, setting
    `Y = true`, even though 3 hasn’t run yet. Then, `t2` is put to sleep by the operating
    system and thread `t1` executes a few instructions, or `t1` simply executes on
    another core. In `t1`, the compiler must indeed run 1 first and then 2, since
    2 depends on the value read in 1. Therefore, `t1` reads `true` from ``Y (written
    by 4) into `r1` and then writes that back to `X`. Finally, `t2` executes 3, which
    reads `X` and gets `true`, as was written by 2.``
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，CPU 注意到 4 不一定要在 3 之后发生，因为 4 不依赖于 3 的任何输出或副作用。也就是说，4 不依赖于 3 的执行顺序。因此，CPU 决定为了*某些原因*重新排序它们，以使程序运行得更快。于是，CPU
    首先执行 4，设置 `Y = true`，即使 3 还没有执行。然后，`t2` 被操作系统挂起，`t1` 执行了几条指令，或者 `t1` 在另一个核心上执行。在
    `t1` 中，编译器确实必须首先执行 1，然后执行 2，因为 2 依赖于 1 中读取的值。因此，`t1` 从 `Y`（由 4 写入）读取 `true` 并将其写回
    `X`。最后，`t2` 执行 3，读取 `X` 并获得 `true`，正如 2 所写的那样。
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
