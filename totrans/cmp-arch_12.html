<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><h2 class="chn"><span epub:type="pagebreak" id="page_215"/><strong>10</strong></h2>&#13;
<h2 class="cht"><strong>MEMORY</strong></h2>&#13;
<div class="image1"><img src="../images/f0215-01.jpg" alt="Image" width="252" height="252"/></div>&#13;
<p class="chq">So far we’ve constructed registers and a small, Baby-sized RAM to use as memory. We made these from flip-flops. Larger memories can’t usually afford to use flip-flops, however, so they’re typically made using other technologies, like DRAM and hard disks. These other technologies are slower, creating a trade-off between speed and size. In this chapter, we’ll look at the details of larger memories. We’ll discuss primary memory, caches, and secondary and offline memory, and begin by looking at the memory hierarchy.</p>&#13;
<h3 class="h3" id="lev192">The Memory Hierarchy</h3>&#13;
<p class="noindent">At any point in time, usually only some of our data is important and in frequent, current use. Other data is used occasionally, and some is out of use entirely. We usually want to arrange our data so that the parts in working use are kept in fast, easily available memory, while the other parts are kept in slower, cheaper memories. This arrangement is known as a <em>memory hierarchy</em>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_216"/>Memory hierarchies played out in pre-digital life, too. For example, people used to carry around shopping lists and important phone numbers written on scraps of paper for immediate, regular use. On their desks would be larger paper documents used only when at work. Beyond the desk were shelves and cabinets containing books and files with data used less often. Still further removed were storage boxes in attics, then local and national libraries and archives that required increasing time to visit. Data could be promoted and demoted between these different stores at different times. For example, a book might sit in the library unused for years, then be promoted to your desk for a few weeks when you needed it. Unused documents on your desk could be demoted to a filing cabinet then to the attic.</p>&#13;
<p class="indent">The same concepts apply to computer memory. When fast and slow versions of the same technology are available, the fast one is better, so it can command a higher price, meaning you can buy less of it compared to the slower one. Given a budget, you can thus trade off speed for capacity. Since most people want some data to be more readily accessible than other data, it makes economic sense to buy and use a mixture of memory types, ranging from small and fast for working data to large and slow for rarely used data. <a href="ch10.xhtml#ch10fig1">Figure 10-1</a> shows the approximate speeds and capacities for each of the levels of memory hierarchy that we’ll discuss in this chapter.</p>&#13;
<div class="image"><img id="ch10fig1" src="../images/f0216-01.jpg" alt="Image" width="549" height="399"/></div>&#13;
<p class="figcap"><em>Figure 10-1: The memory hierarchy</em></p>&#13;
<p class="indent">These levels can be defined as follows:</p>&#13;
<p class="block2"><strong>Registers</strong> Memory inside the CPU, as described in <a href="ch07.xhtml">Chapter 7</a>.</p>&#13;
<p class="block2"><strong>Cache</strong> Memory outside but close to the CPU, which contains fast copies of primary memory.</p>&#13;
<p class="block2"><strong>Primary memory</strong> Memory stored in an address space that is directly accessible by the CPU’s load and store instructions.</p>&#13;
<p class="block2"><strong>Secondary memory</strong> Memory not directly accessible to the CPU via its registers and address space, but that can be moved into primary memory by I/O to enable such access.</p>&#13;
<p class="block2"><span epub:type="pagebreak" id="page_217"/><strong>Tertiary memory</strong> Memory that isn’t directly connected to the address space or to I/O, but that can be mechanically connected to I/O without human intervention.</p>&#13;
<p class="block2"><strong>Offline memory</strong> Memory that can be connected only to the computer with human intervention.</p>&#13;
<p class="indenta">According to Church’s definition of a computer, any machine that relies on fixed-length addresses—such as the Manchester Baby we built in <a href="ch07.xhtml">Chapter 7</a>—isn’t quite a computer. A Church computer needs to be able to simulate any other machine, and to do this it needs to be able to ask for and get more storage as needed. Machines based on a CPU and bus with fixed-sized addresses can’t easily extend their memory beyond that fixed size, however. To get around this problem, and to allow for unlimited memory, we need to use memory levels below primary memory, such as the secondary and tertiary levels shown in <a href="ch10.xhtml#ch10fig1">Figure 10-1</a>. These lower levels aren’t addressed directly from the CPU, but instead are devices that connect to it through I/O modules.</p>&#13;
<h3 class="h3" id="lev193">Primary Memory</h3>&#13;
<p class="noindent"><em>Primary memory</em> (aka <em>system memory</em>) is memory stored in an address space that’s directly accessible by the CPU’s load and store instructions. This includes RAM and ROM. Most modern machines use von Neumann architectures; remember, this means that the program and data are stored together in the same primary memory.</p>&#13;
<p class="indent">In primary memory, each memory location is given a unique address. For example a 16-bit address space has 2<sup>16</sup> = 65,536<sub>10</sub> unique addresses, numbered from 0000<sub>16</sub> to FFFF<sub>16</sub>. Each address stores a fixed-size array of bits called a <em>word</em>. Often, but not always, the word length is chosen to be the same as the address length, such as storing 64-bit words in a 64-bit address space on a modern laptop. You saw a simple way to implement this structure using flip-flops in <a href="ch06.xhtml">Chapter 6</a>; you saw how to attach it to a CPU directly in <a href="ch07.xhtml">Chapter 7</a> and indirectly via a bus in <a href="ch09.xhtml">Chapter 9</a>.</p>&#13;
<h4 class="h4" id="lev194"><em>Bytes and Endianness</em></h4>&#13;
<p class="noindent">Related to the SI versus binary prefix debate is the question of whether to measure memory in bits (b), bytes (B), or words (W). Bits are the most basic unit, and they work well with SI units.</p>&#13;
<p class="indent">In modern use, a byte means 8 bits, and the term comes from the 8-bit era, when what is now known as a word was by definition 8 bits. One byte was what was stored at one memory address, and what was brought into one register of the CPU for processing. The term <em>byte</em> is supposed to suggest the CPU taking the smallest “bite” of memory to process. It was deliberately misspelled to avoid confusion with the term <em>bit</em>. “Byte” originally meant <em>any</em> such natural CPU size, ranging between 1 and 6 bits in early processors of the 1950s. It only later came to be standardized to mean 8 bits.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_218"/>In the 8-bit era, it was very natural to measure primary memory in bytes and what are now called kibibytes. You would compute the number of addresses, such as 2<sup>16</sup> for addresses that are 16 bits long, then append the word <em>bytes</em> to this number to get the total addressable memory size. For example, a “64 kibibyte” machine such as the Commodore 64 had 2<sup>16</sup> addresses containing 1 byte each.</p>&#13;
<p class="indent">The byte really should have little or no relevance in the modern 64-bit age, in which words are 64 bits rather than 8 bits. If we were to store 64-bit words at each of 2<sup>32</sup> = 4 gibi addresses, we would talk about having primary memory sizes such as “4 gibiwords.”</p>&#13;
<p class="indent">However, most actual current machines <em>don’t</em> address memory per word. For historical reasons, they usually continue to address memory per byte, just as they did in the 8-bit era. This is called <em>byte addressing</em> and it means that a word on, say, a 32-bit architecture is stored across 4 bytes with separate addresses. Suppose we want to store a 32-bit word such as 12B4A85C<sub>16</sub>. We do this using 4 bytes containing 12<sub>16</sub>, B4<sub>16</sub>, A8<sub>16</sub>, and 5C<sub>16</sub>.</p>&#13;
<p class="indent">A standards war raged for decades over the order in which these bytes should be stored in memory addresses. The ordering is referred to as <em>endianness</em>. <em>Big endians</em> believe the bytes should be stored in the order (12<sub>16</sub>, B4<sub>16</sub>, A8<sub>16</sub>, 5C<sub>16</sub>) because this looks like the human-readable number 12B4A85C<sub>16</sub>. Big endians say this makes life easier and nicer for the humans who see architecture, including architects themselves and assembly programmers.</p>&#13;
<p class="indent"><em>Little endians</em>, on the other hand, believe the number should be stored as (5C<sub>16</sub>, A8<sub>16</sub>, B4<sub>16</sub>, 12<sub>16</sub>). This initially seems crazy to most Western people. In particular, if you string the bytes together in this order, you have the nonsensical number 5CA8B412<sub>16</sub> rather than the desired 12B4A85C<sub>16</sub>. However, little endians point out that such stringing is based on certain cultural prejudices.</p>&#13;
<p class="indent">The West uses the Arabic decimal number system, which writes numbers with the highest power on the left and the lowest on the right. It imported this system unchanged from the original Arabic. But Arabic <em>text</em> is written and read from right to left, the opposite of Western text. In Arabic, a number string such as “24” is written the same, and has the same value, 24, as in the West, but it’s <em>read</em> from right to left as “four and twenty.” The zeroth column is the units, and the first column is the tens. This makes sense when arithmetic is performed using the number, because almost all arithmetic algorithms begin by operating on the zeroth column and move progressively up the higher-numbered columns. The numbers of these columns match the powers that the base is raised to—for example, the zeroth column is the units, or zeroth power.</p>&#13;
<p class="indent">The little-endian system assigns numerical addresses so that the zeroth byte is at zero offset from the address of the word, and the <em>n</em>th byte is at an <em>n</em> byte offset. This can make arithmetic easier and faster for the machine in some cases. For example, if the machine is adding two words of different byte lengths (say, a short int plus a long int), it’s easy and quick to find the <span epub:type="pagebreak" id="page_219"/><em>n</em>th byte of each. Similar issues can also arise for words containing instructions of variable lengths: with little endianness, you can always be sure that the opcode is at zero offset rather than having to look for it. Little endianness is now dominant in commercial architectures, so it has effectively won the war.</p>&#13;
<h4 class="h4" id="lev195"><em>Memory Modules</em></h4>&#13;
<p class="noindent">RAM and ROM often come in discrete modules that can be added and removed to change the amount of available memory. With a bus architecture, these modules can easily be attached and detached. For example, <a href="ch10.xhtml#ch10fig2">Figure 10-2</a> shows one ROM module and two RAM modules on the same bus as a CPU.</p>&#13;
<div class="image"><img id="ch10fig2" src="../images/f0219-01.jpg" alt="Image" width="855" height="129"/></div>&#13;
<p class="figcap"><em>Figure 10-2: A bus architecture including a CPU, two RAM modules, and a ROM module</em></p>&#13;
<p class="indent">In general, there could be many modules of both RAM and ROM. All the RAM modules can see the same signals passing along the bus, but each module is configured with a different part of the address space, so only the single module that hosts the specified address will actually respond.</p>&#13;
<p class="indent">All bus modules—including memory and I/O modules—are usually manufactured to respond to some default address space, such as starting at address 0. However, when they’re mounted onto a bus, these addresses need to be remapped to be unique when compared to the other modules. This remapping is done by digital logic components called <em>memory controllers</em>, which listen to the bus for global addresses and route them to the appropriate module, converting to the module’s own local addresses.</p>&#13;
<h4 class="h4" id="lev196"><em>Random-Access Memory</em></h4>&#13;
<p class="noindent"><em>Random access</em> means that any random location in memory can be chosen and accessed quickly, without some regions being faster to access than others. By contrast, something like a cassette tape or punch-card deck isn’t random access because it’s faster to access data in sequence than to fast-forward or rewind to a far-away location. While RAM stands for “random-access memory,” it’s a historical misnomer that doesn’t paint a full picture. By modern convention, RAM refers to memory that’s not only random access but also both readable and writable, as well as <em>volatile</em>, meaning its data is lost when the machine is powered off. Many ROMs are also random access, but they aren’t considered RAM under the conventional use of the term because they don’t fit the other parts of the definition.</p>&#13;
<div class="sidebar">&#13;
<p class="stitle"><span epub:type="pagebreak" id="page_220"/><strong>HISTORICAL RAMS</strong></p>&#13;
<p class="stext">We’ve already discussed Babbage’s Analytical Engine RAM, which is still the foundation for RAM architecture today, in <a href="ch03.xhtml">Chapter 3</a>. In the Analytical Engine, each memory address corresponds to a stack of gears whose rotations represent a word. One address at a time can be physically connected to the bus. Once connected, any rotation of the gears will be transferred first to the linear motion of the bus, and then to rotation of a register in the CPU, and vice versa. Now let’s consider a few other historical examples of RAM.</p>&#13;
<h4 class="h4a"><strong>Acoustic Mercury Delay Line RAM</strong></h4>&#13;
<p class="stext">In “From Combinatorial to Sequential Logic” on <a href="ch06.xhtml#lev129">page 144</a>, we discussed how the presence and absence of the audio feedback created by an electric guitar and amplifier feedback loop could be used to store 1 bit of information. This was, in fact, exactly how computer memory was implemented in the UNIVAC era, using mercury delay lines, as shown in the following figure.</p>&#13;
<div class="imagec"><img src="../images/f0220-01.jpg" alt="Image" width="316" height="557"/></div>&#13;
<p class="stext">A delay line was literally a microphone and speaker placed some distance apart and used to store a bit of information through feedback. By placing them at two ends of a tube and filling the tube with mercury, the speed of sound is delayed, so the tube can be made shorter than earlier versions using air.</p>&#13;
<p class="stext">In machines of this era, delay lines could be organized into an address space, as in the Analytical Engine. When the CPU executed a load or store, this would be implemented by making and breaking the electric circuits to connect the required delay line to the bus, disconnecting the others and placing a copy of the data onto the bus for transmission.</p>&#13;
<h4 class="h4a"><span epub:type="pagebreak" id="page_221"/><strong>Williams Tube RAM</strong></h4>&#13;
<p class="stext">The Manchester Baby was built to research a new type of RAM, known as the Williams tube. The technology, shown below, was conceived in 1946, based on the cathode ray tube (CRT), as found in old TV screens.</p>&#13;
<div class="imagec"><img src="../images/f0221-01.jpg" alt="Image" width="394" height="263"/></div>&#13;
<p class="stext">As with CRT screens, the Williams tube fires a stream of electrons in a beam, and uses adjustable magnets to deflect the beam to land on one pixel at a time, in a scanning pattern covering a screen. The screen is made from a fluorescent material, meaning that each pixel glows when absorbing the electron beam. Unlike CRT televisions and monitors, the Williams tube’s purpose was not as a human-readable display but as actual RAM storage. Pixels retain their charge and color for a short period of time after they’re hit by the beam. This means they can be used in a feedback system: we write a screen-full of pixels using the scanning beam, quickly read the screen’s state, and pass the data read off the screen back to the scanning beam to be written to the screen again. This refreshes the data on the screen, keeping it alive for as long as we like, rather than allowing the pixels to fade away.</p>&#13;
<p class="stext">The original Williams tube’s screen contained 32 words of 32 bits each, with each row of the screen being one word and each column of the screen being a bit within a word. Thus, the whole system stored 32×32 = 1,024 bits. Phosphor was used as the fluorescent material, which glows green when stuck by the electron beam.</p>&#13;
</div>&#13;
<h4 class="h4a"><strong>Static RAM</strong></h4>&#13;
<p class="noindent">The kind of RAM we saw previously in <a href="ch06.xhtml#ch06fig22">Figure 6-22</a>, made from flip-flops, is known as <em>static RAM</em> or <em>SRAM</em> (pronounced “es-ram”). Because SRAM is made from flip-flops (the same structures that are used to make CPU registers), it’s fast and expensive. The flip-flops are typically built from around four to six transistors each (depending on the flip-flop type and on how the logic gates are implemented). They have stable memory states, meaning they don’t have to be actively refreshed. They’re available for reading almost immediately after being written to. What sets SRAM apart from CPU registers is that SRAM is addressed, and CPU registers aren’t.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_222"/>SRAM is typically used to implement caches, as we’ll discuss later in the chapter. It isn’t usually used for main memory, except in some specialized and expensive machines, such as high-end routers, where main memory access speed is critical. <a href="ch10.xhtml#ch10fig3">Figure 10-3</a> shows an SRAM chip.</p>&#13;
<div class="image"><img id="ch10fig3" src="../images/f0222-01.jpg" alt="Image" width="236" height="174"/></div>&#13;
<p class="figcap"><em>Figure 10-3: An SRAM chip</em></p>&#13;
<p class="indent">Cache chips like this may be placed between the CPU and RAM. Alternatively, a similar SRAM cache might be found on the same silicon as the CPU.</p>&#13;
<h4 class="h4a"><strong>Dynamic RAM</strong></h4>&#13;
<p class="noindent"><em>Dynamic RAM (DRAM)</em> is cheaper and more compact than SRAM, but slower. Instead of being made from flip-flops, it’s made using cheaper and slower capacitors. A <em>capacitor</em> is a component for storing electric charge. It consists of two metal plates separated by an insulator. Current can’t flow across the plates, but placing a current on them causes them to accumulate charge until they’re full of it. Capacitors don’t usually appear in CPU design; they’re a different kind of electronic component. One bit of DRAM storage is made from just one transistor plus one capacitor. Capacitors can be manufactured on silicon using similar masking processes to transistor manufacture.</p>&#13;
<p class="indent">As RAM, DRAM features the same addressing system as SRAM, and its circuit diagram has the same overall structure as SRAM, based on words stored at addresses. The difference is that the words are implemented with capacitors instead of flip-flops (<a href="ch10.xhtml#ch10fig4">Figure 10-4</a>).</p>&#13;
<p class="indent">DRAM is structured as a 2D array of words or bytes, with each located at a “row” and “column.” The requested address is converted (by a memory controller chip) into two smaller addresses per row and per column, which are AND gated together using a single transistor at the combined address. This saves a huge amount of digital logic, but the work needed to split the address into two parts makes DRAM addressing slower than SRAM addressing.</p>&#13;
<p class="indent">Due to the nature of capacitors, reading the DRAM discharges it and destroys the stored information (as in the Analytical Engine’s RAM). Reading and writing the capacitor state is an analog process, which takes time to complete. The charge can also leak away over time, as capacitors are analog devices. To handle these related problems, DRAM must be periodically refreshed, for example, around every 64 milliseconds on a 2018 DRAM. (The need to constantly refresh is the source of the “dynamic” in DRAM.) Like mercury lines and Williams tubes, a refresh reads the current state and then rewrites it a short time later. Refreshing must be timed carefully and may sometimes conflict with and stall a CPU read or write, which then has to wait until the refresh completes before trying again.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_223"/><img id="ch10fig4" src="../images/f0223-01.jpg" alt="Image" width="634" height="910"/></div>&#13;
<p class="figcap"><em>Figure 10-4: A DRAM circuit, showing capacitors and addressing</em></p>&#13;
<p class="indent">DRAM benefits from <em>pre-charging</em>, roughly a way to “warm it up” just before it’s used; this avoids recharging conflicts with access. Hence, modern CPUs and memory controllers work together to try to predict—several instructions in advance—which memory should be “warmed up” before use.</p>&#13;
<p class="indent">Modern DRAM chips are usually packaged together on printed circuit board modules of around eight chips, each sharing part of an address space, as shown in <a href="ch10.xhtml#ch10fig5">Figure 10-5</a>. These modules attach to a motherboard via a standard interface, as seen previously in the introduction (<a href="fm03.xhtml#fig2">Figure 2</a>). Extra memory can be added to a desktop PC by adding more DRAM modules to its memory slots.</p>&#13;
<div class="image"><img id="ch10fig5" src="../images/f0223-02.jpg" alt="Image" width="509" height="131"/></div>&#13;
<p class="figcap"><em>Figure 10-5: A DRAM module</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_224"/><em>Single in-line memory modules (SIMMs)</em> have a 32-bit bus width, and they were standard in 1990s PCs. Double in-line memory modules (DIMMs) replaced SIMMs in the 2000s. They have a 64-bit bus width, and each stores many gigabytes. Double data rate (DDR) DRAM doubled the speed of DRAM through technology that enables data to transfer on both the rising and falling edges of the clock. This doubles the bandwidth (as <em>bandwidth = bus width × clock speed</em> × <em>data rate</em>). SIMMs and DIMMs have gone through several improved standards that can be visually distinguished by the different notch positions, designed so they can be inserted only into the right type of sockets.</p>&#13;
<h4 class="h4a"><strong>Error Correction Code RAM</strong></h4>&#13;
<p class="noindent">RAM, like other chips, has become so miniaturized that the component size is getting close to atomic scales. At these scales, quantum effects and particle physics come into play. Quantum effects can include various types of inherent noise and uncertainty about the location of particles used in memory. Cosmic rays are random particles most commonly including electrons, alpha particles, and muons, hurtling at high speed through space from either the sun or elsewhere in the galaxy. If a cosmic ray collides with a sensitive component of RAM, then it can corrupt it and flip its Boolean state.</p>&#13;
<p class="indent"><em>Error correction code RAM (ECC-RAM)</em> has extra chips on the DIMM that store extra copies or checksums of the data and use them to automatically correct such flips at the hardware level. ECC-RAM is primarily used in space applications where computers are located outside the protection of Earth’s atmosphere and so are more exposed to cosmic rays. As its price falls, it may also be found in other high-value, safety-critical systems on the ground.</p>&#13;
<div class="sidebar">&#13;
<p class="stitle"><strong>THE ROWHAMMER VULNERABILITIES</strong></p>&#13;
<p class="stext"><em>Rowhammer</em> refers to a set of memory hardware vulnerabilities currently affecting computer security. DRAM capacitors are now so small and tightly packed that their electric fields may affect neighboring rows of memory. Security researchers have begun to exploit this effect to read and write memory belonging to target programs. The researchers write new programs and arrange for them to be stored in a region of memory physically next to, for example, the addresses containing your online banking password, owned by the target program. They then load and store data in their own program’s locations, in ways that are likely to trigger physical interactions between the capacitors in their own and the target’s memory. For example, this could include putting their own addresses into states likely to cause cosmic ray–style errors in the target memory. Or they might be able to infer the state of the target memory by observing similar errors or small time delays in their own reads and writes caused by the target’s capacitor states.</p>&#13;
<p class="stext">Research is currently ongoing into defenses against rowhammer attacks. Approaches include use of ECC-RAM to correct any maliciously induced cosmic ray–style errors, use of higher memory refresh rates, and software-level solutions such as operating system code to randomize the locations of programs in memory and prevent deliverable co-location of code next to targets.</p>&#13;
</div>&#13;
<h4 class="h4" id="lev197"><span epub:type="pagebreak" id="page_225"/><em>Read-Only Memory</em></h4>&#13;
<p class="noindent"><em>Read-only memory (ROM)</em> traditionally refers to memory chips that can only be read from, not written to, and that are pre-programmed with permanent collections of subroutines by their manufacturer, then mounted at fixed addresses in primary memory. ROMs have since evolved to include other types of memory that don’t fit this traditional definition or name very well or at all.</p>&#13;
<p class="indent">First, the ROM versus RAM distinction has never been a true partition because, as noted earlier, ROM chips are random access, just like RAM: they’re mounted in the main address space and accessing any address within them takes the same amount of time. The difference between ROM and RAM is that RAM is readable and writable, while ROM is traditionally only readable.</p>&#13;
<p class="indent">Second, ROMs have evolved over time to allow increasing ease of rewriting, with programs stored in ROM that are able to be rewritten in some way now known as <em>firmware</em>. The following sections describe the main steps of this evolution, as illustrated in <a href="ch10.xhtml#ch10fig6">Figure 10-6</a>.</p>&#13;
<div class="image"><img id="ch10fig6" src="../images/f0225-01.jpg" alt="Image" width="886" height="141"/></div>&#13;
<p class="figcap"><em>Figure 10-6: Evolution of ROMs: MROM, PROM, EPROM, EEPROM, and SD card–mounted flash. Note that, unusually, the actual silicon is visible in the EPROM package, through a transparent window, which is needed to expose it to light.</em></p>&#13;
<p class="indent">Let’s go through a few of these types of ROM.</p>&#13;
<h4 class="h4a"><strong>Mask ROM</strong></h4>&#13;
<p class="noindent"><em>Mask ROM (MROM)</em> is ROM whose contents are programmed using photo-lithography by the manufacturer. It remains read-only forever and can’t be overwritten. If you want to update an MROM chip, you have to remove it, throw it away, and insert a brand new chip containing the new content. Photolithography is very expensive, so MROMs are difficult to produce and to upgrade.</p>&#13;
<h4 class="h4a"><strong>Programmable ROM</strong></h4>&#13;
<p class="noindent"><em>Programmable ROM (PROM)</em> was a great advance over MROM. Similar to the programmable logic arrays (PLAs) discussed in <a href="ch05.xhtml">Chapter 5</a>, PROMs are chips manufactured by photolithography to include a generic circuit with many fuses. The programmer can then selectively blow the fuses to create different structures. While PLAs enable arbitrary digital logic networks to be burned in this way, PROMs instead contain a fixed structure of addresses and words, and allow only the bits composing the words to be burned, to make a ROM. Usually each bit contains 1 when its fuse is intact and changes to 0 if its fuse is blown. Like PLAs, PROMs can never be erased once they’re programmed.</p>&#13;
<h4 class="h4a"><span epub:type="pagebreak" id="page_226"/><strong>Erasable Programmable ROM</strong></h4>&#13;
<p class="noindent"><em>Erasable programmable ROM (EPROM)</em> is like PROM, but the chip’s data can be erased using ultraviolet light. Then new data can be burned on. This cycle can be repeated many times. Although the erasing process was quite complex, requiring that you take the chip out of the computer and put it in a light box, it was still something you, a skilled end-user customer, could do without needing the computer manufacturer.</p>&#13;
<h4 class="h4a"><strong>Electrically Erasable Programmable ROM</strong></h4>&#13;
<p class="noindent"><em>Electrically erasable programmable ROM (EEPROM)</em> is like EPROM in that you can wipe the entire chip and rewrite it, but here you only need to use electricity to erase and reprogram. This removes the need to physically manipulate the ROM; it can remain inside the computer. EEPROM is used today in ROMs that allow their firmware to be upgraded. If you’ve ever done a firmware update, you’ll have seen that it can be done entirely in software, without having to physically touch anything. You wouldn’t want to be updating firmware every day, but maybe once per year or whenever a bug fix has been found.</p>&#13;
<h4 class="h4a"><strong>Flash Memory</strong></h4>&#13;
<p class="noindent"><em>Flash memory</em> is EEPROM that can be erased and rewritten block-wise, meaning you can selectively wipe and rewrite just one small part, or block, of the memory at a time. This way you can leave most of the ROM intact, unlike with regular EEPROM, where you have to wipe and rewrite an entire chip of ROM at a time, as in a firmware update. Flash memory makes it much easier to rewrite portions of ROM frequently, while the chip is online, making it more feasible for day-to-day storage, functioning almost like RAM in some cases.</p>&#13;
<h3 class="h3" id="lev198">Caches</h3>&#13;
<p class="indent">A <em>cache</em> is an extra layer in the memory pyramid between the fast registers of the CPU and the slower RAM. It stores copies of the most heavily used memory contents, making them available for quick retrieval. (<em>Cache</em> is an archaic word for a store of items such as food, weapons, or pirate treasure.) Without a cache, RAM would connect straight to the CPU, either directly, as discussed in <a href="ch07.xhtml">Chapter 7</a>, or using a bus with control (C), address (A), and data (D) lines, as discussed in <a href="ch09.xhtml">Chapter 9</a> and summarized in <a href="ch10.xhtml#ch10fig7">Figure 10-7</a>.</p>&#13;
<div class="image"><img id="ch10fig7" src="../images/f0226-01.jpg" alt="Image" width="576" height="94"/></div>&#13;
<p class="figcap"><em>Figure 10-7: A basic CPU, bus, and RAM architecture</em></p>&#13;
<p class="indent">The problem with this kind of cacheless architecture is that most programs need to access RAM frequently, but the capacitors that implement DRAM are slower than the flip-flops that implement the CPU’s registers. <span epub:type="pagebreak" id="page_227"/>RAM thus becomes a major bottleneck for system speed. It’s no use having a fast, gigahertz CPU if the RAM is running orders of magnitude slower and the CPU has to wait around for each load and store to complete. Adding an SRAM-based cache made from flip-flops between the CPU and RAM, as shown in <a href="ch10.xhtml#ch10fig8">Figure 10-8</a>, helps avoid these bottlenecks.</p>&#13;
<div class="image"><img id="ch10fig8" src="../images/f0227-01.jpg" alt="Image" width="890" height="99"/></div>&#13;
<p class="figcap"><em>Figure 10-8: A basic CPU, bus, and RAM architecture with a cache in between</em></p>&#13;
<p class="indent">When the CPU needs to load some data, the cache checks if it has it, and returns it quickly if so. If not, the cache refers to the next memory level down (in <a href="ch10.xhtml#ch10fig8">Figure 10-8</a>, RAM) and fetches the data from that level. Caching can also occur at <em>all</em> levels of the memory hierarchy, from registers to hard disks and jukeboxes (more on the latter in the “Tertiary Memory” section). However, it’s most commonly considered at the primary memory level, as we’re discussing here, between the registers and the main DRAM memory.</p>&#13;
<p class="indent">Initial designs began with a single cache, made from SRAM. More recent machines have made use of Moore’s law for transistor density to fill silicon with larger caches and more levels of cache. It’s common today to have at least three cache levels, called L1, L2, and L3, as in <a href="ch10.xhtml#ch10fig9">Figure 10-9</a>.</p>&#13;
<div class="image"><img id="ch10fig9" src="../images/f0227-02.jpg" alt="Image" width="923" height="78"/></div>&#13;
<p class="figcap"><em>Figure 10-9: A basic CPU, bus, and RAM architecture with L1, L2, and L3 caches</em></p>&#13;
<p class="indent">All these cache layers between CPU and DRAM memory are typically made in SRAM, but they have different operations policies that trade off size and speed in their different digital logic implementations. Historically, caches lived on dedicated chips outside the CPU. While lower levels still do this, a major trend is to move bigger and higher cache levels onto the CPU silicon itself.</p>&#13;
<p class="indent">Understanding the caches of your machines helps you write faster programs. Typically, each level of cache is 10 times faster than the one below it, so when you fill a level you’ll see a sudden slowdown in memory access. If you know the cache sizes, you can redesign your code to keep data in use within known cache-level limits to benefit from their speed.</p>&#13;
<h4 class="h4" id="lev199"><em>Cache Concepts</em></h4>&#13;
<p class="noindent">Caches are based on the <em>principle of locality</em>, which states that only a small amount of memory space is being accessed at any given time, and values in that space are being accessed repeatedly. It’s therefore useful to copy recently accessed values and their neighbors from larger, slower memory to smaller, faster memory. There are several different ways to think about “neighbors” and “locality.” <em>Temporal locality</em> is the property that values tend <span epub:type="pagebreak" id="page_228"/>to be accessed repeatedly at nearby times. <em>Sequential locality</em> is the property that some sequences tend to be re-accessed in the same order multiple times. <em>Spatial locality</em> is the property that values nearby in memory tend to be accessed together. These concepts apply to both instructions and data, often arising due to loops and subroutines.</p>&#13;
<p class="indent">Cache memory is made of many <em>cache lines</em>. Each line contains a <em>block</em> with copies of several contiguous words from memory, as well as a <em>tag</em>, an address or other identifier describing which memory location has been copied into the block. Each line also has a <em>dirty bit</em> that tracks whether the CPU has changed the value in the cache, making it different from the equivalent value in memory. <a href="ch10.xhtml#ch10tab1">Table 10-1</a> shows a few example cache lines.</p>&#13;
<p class="tabcap" id="ch10tab1"><strong>Table 10-1:</strong> Cache Lines</p>&#13;
<table class="allc">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:50%"/>&#13;
<col style="width:30%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<th style="vertical-align: top" class="borderb"><p class="tabtext"><strong>Tag</strong></p></th>&#13;
<th style="vertical-align: top" class="borderb"><p class="tabtext"><strong>Block</strong></p></th>&#13;
<th style="vertical-align: top" class="borderb"><p class="tabtext"><strong>Dirty bit</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tabtext"><code>$08F4</code></p></td>&#13;
<td style="vertical-align: top"><p class="tabtext"><code>01101100 01101100 10011010</code></p></td>&#13;
<td style="vertical-align: top"><p class="tabtext"><code>1</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tabtext"><code>$2AD5</code></p></td>&#13;
<td style="vertical-align: top"><p class="tabtext"><code>10010101 11100110 00110110</code></p></td>&#13;
<td style="vertical-align: top"><p class="tabtext"><code>0</code></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Each cache line shown in the table has a block of three 8-bit words, a tag consisting of the full address from a 16-bit address space, and a dirty bit. The 1 dirty bit for the first line indicates it’s been updated, while the 0 dirty bit for the second line indicates it hasn’t.</p>&#13;
<p class="indent">We don’t cache individual addresses, but rather lines because it’s very cheap to move around larger chunks of memory rather than individual words. By bringing in whole lines around a target word, we exploit spatial locality—data and programs in neighboring locations are likely to be used next. The line prepares for this.</p>&#13;
<p class="indent">Some cache systems use “hash functions” to choose a location in the cache for storing a piece of data, usually based on the data’s address in lower-level memory. A <em>hash function</em> is a many-to-one function that maps a big input number to a smaller output number, the <em>hash value</em>. It’s not usually possible to recover the original value from the hash value. For example, a function that takes the last two hex digits of a hex number is a simple hash function: <em>hash</em>(9A8E<sub>16</sub>) = 8E<sub>16</sub>. The function that performs a Boolean AND of all binary digits in a number is another hash function: <em>hash</em>(01101001<sub>2</sub>) = 0&amp;1&amp;1&amp;0&amp;1&amp;0&amp;0 = 0. A commonly used hash function for caches is to compute the value of an address modulo the number of available lines in the cache.</p>&#13;
<p class="indent">Finding an item in a cache is known as a <em>hit</em>. Not finding an item in a cache is known as a <em>miss</em>. When a miss occurs, we have to go back to the underlying memory and find the item there instead, usually making a new copy in the cache for future use. The <em>hit rate</em> is the ratio of hits to attempts (hits and misses together). This measures the proportion of cache lookups that are successful. The <em>miss rate</em> is the ratio of misses to attempts. This measures the proportion of cache lookups that are unsuccessful. The <em>hit time</em> is the time required to access requested data if a hit has occurred, and the <em>miss penalty</em> is the time required to process a miss.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_229"/>A cache has only a limited number of lines, and they quickly fill up as we store cached copies of everything that we access from the underlying memory. Once the cache is full, we’ll continue to request new addresses. These will initially miss, but temporal locality suggests that these new addresses are more likely to be reused than the older ones in the cache. We should therefore choose lines in the cache to overwrite, discarding their previously cached addresses and replacing them with the new ones. The contents of the overwritten lines are called <em>victims</em>.</p>&#13;
<p class="indent">Once we have a cache structure, we need algorithms, implemented in fast digital logic, to manage it. We need to decide how to best make use of the available lines, and how to create and look up tags. As with most digital logic design, there will be trade-offs between methods that are simple and methods that are fast. The latter tend to require more silicon, making them more complex, error-prone, and expensive. Let’s take a look at a few options for using caches.</p>&#13;
<h4 class="h4" id="lev200"><em>Cache Read Policies</em></h4>&#13;
<p class="noindent">Reading from a cache is a simpler task than writing to it, so we’ll first study some options for cache read algorithms.</p>&#13;
<h4 class="h4a"><strong>Direct Mapped</strong></h4>&#13;
<p class="noindent"><em>Direct mapping</em> is the simplest, easiest, and cheapest cache read policy to implement and understand. It’s sketched out in <a href="ch10.xhtml#ch10fig10">Figure 10-10</a>.</p>&#13;
<div class="image"><img id="ch10fig10" src="../images/f0229-01.jpg" alt="Image" width="925" height="354"/></div>&#13;
<p class="figcap"><em>Figure 10-10: A direct mapping cache read policy (showing lookup and caching)</em></p>&#13;
<p class="indent">In essence, the line where we store or look for a tag is addressed using a fixed hash of the tag. A line with this tag will only ever be stored at a single location. If multiple lines compete for the location, the new one will replace the older one. For example, suppose we load from address 67AB<sub>16</sub>. We might compute <em>hash</em>(67AB<sub>16</sub>) = 4<sub>16</sub>, which means that this address and its contents will be cached in line 4<sub>16</sub>, victimizing anything that was previously on this line.</p>&#13;
<p class="indent">The drawback is that direct mapping can’t keep multiple in-use addresses in cache if they share the same hash. Suppose our program has a tight loop that reads and writes the two alternating addresses 67AB<sub>16</sub> and 12C9<sub>16</sub> <span epub:type="pagebreak" id="page_230"/>many times. The problem here is that <em>hash</em>(67AB<sub>16</sub>) = <em>hash</em>(12C0<sub>16</sub>) = 4<sub>16</sub>. Both addresses will continually fight and victimize one another, overwriting line 4<sub>16</sub>, even if no other addresses or cache lines are being used in the loop at all. In such a case, the cache will give no benefit at all, as every attempt will miss.</p>&#13;
<h4 class="h4a"><strong>Fully Associative</strong></h4>&#13;
<p class="noindent">To fix the problem with direct mapping, we’d like to have addresses use different cache lines depending on how in-use our lines are, so that we victimize lines that are the least used, as sketched out in the <em>fully associative cache</em> of <a href="ch10.xhtml#ch10fig11">Figure 10-11</a>.</p>&#13;
<div class="image"><img id="ch10fig11" src="../images/f0230-01.jpg" alt="Image" width="972" height="602"/></div>&#13;
<p class="figcap"><em>Figure 10-11: A fully associative cache sketch</em></p>&#13;
<p class="indent">Here, each line of cache RAM is given its own digital logic block, including a comparator, multiplexer, and OR arrays. Only three such blocks are shown for illustration purposes, but for a 256-line cache, for example, there would be 256 such blocks, all running in parallel.</p>&#13;
<p class="indent">We want to be able to store a tag, block, and dirty bit on <em>any</em> available line and be able to find it quickly. Caching is the easy part here: we just create some digital logic to count how much use each line is getting and to pick out the line with the lowest count.</p>&#13;
<p class="indent">The cache lookup is the harder part. In direct mapping, we just computed the same hash function as we used for caching, to tell us at which line to find a desired address. Now it could be anywhere in the cache, so we need to add lots of extra digital logic to check each of the lines’ tags for a match with the desired one and activate the matching line if it exists. Doing this in parallel (which is the only realistic way to make this fast enough to be useful) <span epub:type="pagebreak" id="page_231"/>requires <em>N</em> copies of this matching digital logic, one for each of the <em>N</em> lines of cache, making it a much larger and more energy-consuming beast.</p>&#13;
<h4 class="h4a"><strong>Set Associative</strong></h4>&#13;
<p class="noindent"><em>Set associative</em> cache reading is an attempt to get the best of both of the above methods. Here we partition the <em>N</em>-line cache into several smaller sets of lines. We use hashing on addresses to hash to a set number, rather than a line number. During caching we find the set number from this hash, similar to the direct mapping approach, then choose as the victim the line within this set that has the least usage, similar to the fully associative approach. During lookup we again find the set number from the hash, then we use parallel matching checks on all items in just the one set to quickly find the matching line.</p>&#13;
<p class="indent">This approach means we only have to activate the comparators within a single set, rather than the entire cache, but we still avoid the direct-mapped problem of tight loops sharing hash values. In practice, this is often found to be a nice balance.</p>&#13;
<h4 class="h4" id="lev201"><em>Cache Write Policies</em></h4>&#13;
<p class="noindent">Caches become a bit more complicated when we do stores because a store changes the state of the memory. Suppose we’ve recently loaded an integer 17 from address 540A<sub>16</sub> and cached a copy during the load. We want to increment this integer to 18 and store the result back at 540A<sub>16</sub>. Due to the locality principles, it’s likely that we’ll continue to both load and store from 540A<sub>16</sub> in the near future, so rather than store 18 directly in 540A<sub>16</sub>, it may be faster to store it only in the cache line that’s currently caching 540A<sub>16</sub>. This means that all the future loads and stores can just hit the cache and don’t need to go to main memory.</p>&#13;
<p class="indent">The problem is that eventually this line will be victimized and we’ll lose all the changes we’ve made to the value; the main memory still contains the old value of 17. To avoid this, at some point we need to copy the modified value back to main memory. The dirty bit shown earlier in <a href="ch10.xhtml#ch10tab1">Table 10-1</a> tracks whether this needs doing. It’s set to 0 if the value in the line is the same as the value in memory, or to 1 if the value in the line has been updated but the value in memory hasn’t. Algorithms called <em>cache write policies</em> use this dirty bit to manage the copying back to memory. Let’s look at two different approaches: write-back and write-through.</p>&#13;
<h4 class="h4a"><strong>Write-Back</strong></h4>&#13;
<p class="noindent"><em>Write-back</em> is the simpler cache writing method: it copies the contents of the cache block back to RAM only when the line is victimized. This is relatively slow, however, because victimization occurs only when an instruction is in a rush to get executed. We get told to start writing back only once the victimization has been announced, and the victimizing instruction will now have to wait for us to do a slow RAM access before it can overwrite our victim line.</p>&#13;
<h4 class="h4a"><span epub:type="pagebreak" id="page_232"/><strong>Write-Through</strong></h4>&#13;
<p class="noindent"><em>Write-through</em> is a potentially faster alternative to write-back, although it uses more resources. In write-through, we don’t wait until our line is victimized to copy our line’s block back to RAM; rather, we do it multiple times, continually, in the background, using digital logic attached to the cache line and bus. This logic acts similarly to an application like SyncThing or Dropbox, continually looking out for any changes in the cached version and copying them back to the main version in RAM. This doesn’t create extra work for the CPU, as the extra digital logic is located on the cache itself. It does, however, lead to more traffic on the bus, as we’re sending these updates many more times than with the write-back approach.</p>&#13;
<h4 class="h4" id="lev202"><em>Advanced Cache Architectures</em></h4>&#13;
<p class="noindent">Consider how caches should interact with the advanced CPU developments of <a href="ch08.xhtml">Chapter 8</a>. Pipelined CPUs need to care a lot about cache misses, as they form another possible hazard. An efficient pipeline may be timed to assume that memory accesses will be cached, and if there’s a miss they’ll need to stall or otherwise handle this hazard.</p>&#13;
<p class="indent">You saw in <a href="ch08.xhtml">Chapter 8</a> how branch prediction attempts to guess the flow of a program to enable pipelines and out-of-order execution to go more smoothly. This can be used in conjunction with caching to <em>preemptively</em> fetch and store data—that is, before the actual load and store instructions are reached. These instructions take much longer to execute than in-CPU operations, so it’s useful to initiate them early. CPUs can look ahead in the program to try to guess which parts of main memory are likely to be needed many instructions down the line, and start caching them in advance so the CPU fetches will be faster.</p>&#13;
<p class="indent">As mentioned, each layer of the cache—L1, L2, and L3—provides roughly a tenfold speedup over the layer below it, so the potential gain from preemptively moving data higher up in the memory hierarchy isn’t trivial. The caches can always be rolled back and the CPU stalled if preemption gets it wrong. It’s not the end of the world if we bring the wrong data into the cache: the cache is a big place, and it’s okay to change what’s in it.</p>&#13;
<p class="indent">Due to the row-column structure of DRAM addressing, it’s faster to read multiple items in a single DRAM row all at once rather than individually. (Once a row is activated, it’s almost free to read many columns versus a single one.) Hence, modern DRAM controllers will typically work in harmony with the cache to move large DRAM rows into cache lines.</p>&#13;
<p class="indent">Cache writes can unnecessarily slow down a system if we know in advance that the data won’t need to be read again soon. In this case, writing to the cache and then transferring to main memory can be slower than just writing directly to main memory. Modern CPUs may provide special instructions for cacheless writing, which canny programmers and compiler writers can use to make programs faster.</p>&#13;
<p class="indent">It’s been found empirically that L1 caches work more smoothly if they’re split into two separate, parallel caches, one for instructions and one for data. <span epub:type="pagebreak" id="page_233"/>This can occur in Harvard architectures, where instructions and data are already separated in RAM, but also in von Neumann architectures, where instructions and data can be distinguished by which part of the CU is requesting them (instructions are requested during the fetch stage, while data is requested during the execute stage). This separation occurs only at L1, with lower cache levels sharing instructions and data, as in <a href="ch10.xhtml#ch10fig12">Figure 10-12</a>.</p>&#13;
<div class="image"><img id="ch10fig12" src="../images/f0233-01.jpg" alt="Image" width="1112" height="224"/></div>&#13;
<p class="figcap"><em>Figure 10-12: A basic CPU, bus, and RAM architecture with separate L1 caches for instructions and data, and shared L2 and L3 caches</em></p>&#13;
<p class="indent">Separating the instructions and data at the L1 level appears to be effective because both data and programs exhibit spatial locality individually, but with little locality between them. Also, instructions aren’t usually overwritten, while data often is, so separating out the instructions can simplify the cache write process.</p>&#13;
<h3 class="h3" id="lev203">Secondary and Offline Memory</h3>&#13;
<p class="noindent"><em>Secondary memory</em> is memory that can quickly be brought into addressed memory space via I/O. Data items in secondary memory don’t have addresses in the primary memory address space. Rather, they’re accessed via I/O, usually via an I/O module that <em>does</em> sit in the primary address space and relays requests to the secondary storage. Secondary storage is sometimes called <em>online storage</em> to emphasize that it’s powered, active, and available whenever the computer is on.</p>&#13;
<p class="indent"><em>Offline memory</em> is that which can’t automatically be loaded into primary memory without <em>manual</em> human interventions. Often this includes secondary memory media that are physically ejectable and replaceable, such as tapes, discs, and USB devices. These media are secondary memory when connected to the computer, and offline memory when disconnected. Offline memory is typically used for backup and archival purposes, as well as for transportation. The fastest way to move petabytes of data around the world is still to put it on a truck as offline memory and drive it to its destination.</p>&#13;
<p class="indent">Secondary and offline memory should really nowadays be measured in bits and SI units—for example, describing an “8.8 terabit hard disk” instead of a “1 tebibyte hard disk.” This is because they aren’t part of primary memory address space and so aren’t addressed using primary memory’s word or byte addresses. The concept of bytes is even less relevant here than in modern primary memory. However, as primary memory is still often byte-addressed and measured in bytes, most people still have a better feel for sizes <span epub:type="pagebreak" id="page_234"/>in bytes rather than bits, so they choose to measure secondary memory in the same units.</p>&#13;
<p class="indent">Secondary (and offline) memory is usually characterized by requiring some mechanical motion to look up data, rather than being random access. This includes scrolling through tape or spinning discs made from various materials. We’ll look at some details of these technologies next.</p>&#13;
<h4 class="h4" id="lev204"><em>Tapes</em></h4>&#13;
<p class="noindent"><em>Tapes</em> are one-dimensional data stores that must be scrolled left or right to locate a required datum. You can think of human-written paper scrolls, like the Torah, as the original tapes. Tapes aren’t random access because a reading device has a position at one point in the tape, and it takes longer to move the tape (or the reader) to access a far-away location than a nearby location. Fast algorithms using tape storage need to take this structure into account and optimize memory access to reduce large address jumps.</p>&#13;
<h4 class="h4a"><strong>Punch Cards</strong></h4>&#13;
<p class="noindent"><em>Punch cards</em> are the original computational secondary storage, as used in the Jacquard loom and Analytical Engine (seen in <a href="ch01.xhtml#ch01fig11">Figure 1-11</a>). They continued to be used in IBM Hollerith machines, and were used to store and read programs for early electronic machines of the 1960s. Occasional industrial use continued even into the 1980s, and allegedly at least one UK council may still be using them today. In punch cards, binary digits of data are represented by the presence or absence of holes punched or not punched at a series of physical locations on a card or piece of paper. The holes are usually about the size made by the desktop hole punchers you buy to file your paper documents into ring binders.</p>&#13;
<p class="indent">Cards are 2D, having rows and columns. Typically each row stores one word, with their row numbers acting as addresses (in a secondary address space, not primary RAM addresses). Conceptually, and sometimes physically, decks of cards are <em>chained</em> together to make what is really a 2D tape.</p>&#13;
<h4 class="h4a"><strong>Punched Tape</strong></h4>&#13;
<p class="noindent"><em>Punched tape</em> is an alternative to punch cards. Such tapes were used by the British Post Office, formed the inspiration for the Turing Machine, and were also used in the Colossus, as seen in <a href="ch01.xhtml#ch01fig22">Figure 1-22</a>. Depending on your point of view, tape is conceptually simpler than cards because it’s just a single 1D row of bits; or it’s more complex than cards because you have to worry more about aligning and reading words, which on cards are easily presented as rows.</p>&#13;
<h4 class="h4a"><strong>Magnetic Tape</strong></h4>&#13;
<p class="noindent"><em>Magnetic tape</em> was developed in the 1920s for analog audio recording in studios, commercialized for home use as 8-track systems in the 1960s, then used widely in 4-track compact cassettes during the 1980s. Analog magnetic tape was also widely used in the 1980s for home video recordings, following <span epub:type="pagebreak" id="page_235"/>one of the first modern data standards wars between competing VHS and Betamax formats.</p>&#13;
<p class="indent">In these systems, a magnetizable material such as iron oxide is formed into a tape structure, and the level of magnetization at each point along the tape is used to store data. Unlike punched paper, magnetic tape is easy to remagnetize and can be rewritten many times.</p>&#13;
<p class="indent">The same magnetic tapes can be used to store digital information, in various ways. For example, 0s and 1s can be encoded as single cycles of two different audible frequencies—a method that’s resilient to the heavy noise added by most tape devices. Algorithms developed for optimal access of punched tape carried over directly to magnetic tapes, as in the 1980s machine of <a href="ch10.xhtml#ch10fig13">Figure 10-13</a>.</p>&#13;
<div class="image"><img id="ch10fig13" src="../images/f0235-01.jpg" alt="Image" width="619" height="271"/></div>&#13;
<p class="figcap"><em>Figure 10-13: A 1980s compact cassette and player/recorder, used for both analog music and digital file storage</em></p>&#13;
<p class="indent">Magnetic tape is still in use today for offline storage, specifically for weekly or daily backups of company systems. Tape is cheap and cost-effective for large-scale storage, where access time is less important. Tapes are thus useful for the daily backup task because you want to have lots of old backups kept around for as long as possible. In particular, if someone attacks your company in a more subtle way than just deleting everything—for example, by making a series of small changes to your database—it’s useful to have a long series of backups so you can recover the state of the system from different days, weeks, months, years, and even decades. You can buy a new tape for a few dollars every day to get this assurance. Having many tapes around also means they can be kept at many more locations than can hard drives—for example, with a different employee taking one tape home each day so that even if half the staff’s houses burn down on the same day, you still have many recent backups around.</p>&#13;
<p class="indent">The most popular current standard for magnetic tape storage is <em>Linear Tape Open (LTO)</em>, shown in <a href="ch10.xhtml#ch10fig14">Figure 10-14</a>.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_236"/><img id="ch10fig14" src="../images/f0236-01.jpg" alt="Image" width="513" height="384"/></div>&#13;
<p class="figcap"><em>Figure 10-14: An IBM Ultrium Linear Tape Open cartridge and drive</em></p>&#13;
<p class="indent">LTO is an open source standard that, as of 2020, stored around 36TB on about 1 km of tape in one cartridge that fits in your pocket and takes around 12 hours to write. This is a good size and time for most small businesses; they can back up the whole system overnight onto a single cartridge.</p>&#13;
<h4 class="h4" id="lev205"><em>Disks</em></h4>&#13;
<p class="noindent">Audio recording began in the 1870s with wax cylinders, as shown in <a href="ch10.xhtml#ch10fig15">Figure 10-15</a>.</p>&#13;
<div class="image"><img id="ch10fig15" src="../images/f0236-02.jpg" alt="Image" width="314" height="308"/></div>&#13;
<p class="figcap"><em>Figure 10-15: A wax cylinder audio storage device</em></p>&#13;
<p class="indent">Here, sound waves enter the acoustic horn and are concentrated to vibrate a needle, etching the sound wave into a spiral around a hot wax cylinder as it rotates and is slowly moved left to right. When the wax cylinder is cool it can then be spun past the needle again to make it vibrate in the same ways, and have its motions amplified by the horn, replaying the sound.</p>&#13;
<p class="indent">Wax cylinders were used commercially until 1898, when they were replaced by gramophones with discs, rotating at 78 revolutions per minute (<a href="ch10.xhtml#ch10fig16">Figure 10-16</a>, left). These “78” disks used the same idea of etching the analog sound wave directly into their spiral grooves, and their vinyl descendants—now with electrical amplification—are still in use by DJs today (<a href="ch10.xhtml#ch10fig16">Figure 10-16</a>, right).</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_237"/><img id="ch10fig16" src="../images/f0237-01.jpg" alt="Image" width="552" height="305"/></div>&#13;
<p class="figcap"><em>Figure 10-16: A gramophone (left) and a modern Technics SL-1200 turntable (right)</em></p>&#13;
<p class="indent">Unlike audio discs, which have a single track spiraling in from the edge to the center, most data disks are truly 2D, as they have many independent <em>tracks</em>, each at a fixed radius, as shown in <a href="ch10.xhtml#ch10fig17">Figure 10-17</a>.</p>&#13;
<div class="image"><img id="ch10fig17" src="../images/f0237-02.jpg" alt="Image" width="549" height="350"/></div>&#13;
<p class="figcap"><em>Figure 10-17: The single track of an audio disc (left) and the 2D track of a data disk (right). The latter shows a track (A), sector (B), geometric sector (C), and cluster (D).</em></p>&#13;
<p class="indent">Tracks near the edge are larger than those in the center, so they store more data. Tracks are divided into fixed-data-size <em>sectors</em> around their circumference. Each sector has an address composed of its track ID and location within the track. In most systems, sectors store their own location in some of their bits so that we can figure out which part of the disk we’re looking at. They may also store redundant bits, which compensate for physical damage to the disk, using Shannon’s theory of communication. Sectors may be grouped into contiguous <em>clusters</em>, which are the smallest unit that can be read or written together.</p>&#13;
<p class="indent">Data on disks can be accessed in an <em>almost</em> random-access manner: individual sectors can be stored or retrieved in any order, not only sequentially, but reads and writes to nearby sectors and tracks will be faster due to the motion of the disk and head. It’s easy and fast to read from a series of sectors in order around the same track as they spin past the head. If you want data on the same track but at a different angle from the current sector, you have to wait for the disk to spin around to bring that sector under your head. If you want data from a different track, you have to move your head along <span epub:type="pagebreak" id="page_238"/>the radius, which is very slow, as it’s a physical device. I/O modules controlling spinning disks thus need to consider the <em>access time</em>—the time it takes to read or write one sector. Access time is composed of two main factors: <em>seek time</em> is the time it takes for the arm to position itself over the track, and <em>rotational delay</em> is the time it takes for the desired sector to position itself under the head.</p>&#13;
<h4 class="h4a"><strong>Floppy Disks</strong></h4>&#13;
<p class="noindent">Magnetic disks use the same technology as magnetic tape to represent data, but they arrange the magnetizable material into a 2D disk rather than a 1D tape. The disk is read and written by a magnetic head on an arm, like a gramophone needle. <em>Floppy disks</em> (<a href="ch10.xhtml#ch10fig18">Figure 10-18</a>) first appeared in the 1960s. They’re so-called because they physically flex.</p>&#13;
<div class="image"><img id="ch10fig18" src="../images/f0238-01.jpg" alt="Image" width="552" height="252"/></div>&#13;
<p class="figcap"><em>Figure 10-18: Three generations of floppy disks: 8 inch (1970s), 5 1/4 inch (1980s), and 3 1/2 inch (1990s)</em></p>&#13;
<p class="indent">Floppy disks are vulnerable to damage, so they’re usually encased in a plastic sheath, as in the figure.</p>&#13;
<h4 class="h4a"><strong>Hard Disks</strong></h4>&#13;
<p class="noindent"><em>Hard disks</em> are made of nonflexible materials. They can store higher information densities and spin faster than floppies. These devices usually require sealing the head into a package with the disk, as in <a href="ch10.xhtml#ch10fig19">Figure 10-19</a>, rather than allowing removable disks, as with floppies.</p>&#13;
<div class="image"><img id="ch10fig19" src="../images/f0238-02.jpg" alt="Image" width="423" height="333"/></div>&#13;
<p class="figcap"><em>Figure 10-19: The inside of a magnetic hard drive</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_239"/>Hard <em>drives</em> usually contain multiple hard disks packaged together, each with its own head, with a single address space spanning all of them. This can help reduce access times, because the heads can all read and write together. The disks spin at speeds such as 90 to 250 Hz, which causes a layer of air to lift the head off the surface, so the head doesn’t physically contact the platter. This means there’s no physical wear to the head or the disk. Designers have invested heavily in technology to automatically and rapidly park the head if the unit is in physical danger, such as being struck or pushed. Without this, the head would crash into the disk and destroy it during such an incident.</p>&#13;
<h4 class="h4a"><strong>Optical Discs</strong></h4>&#13;
<p class="noindent">Optical discs are modern-day version of the Babylonian clay tablets seen in <a href="ch01.xhtml#ch01fig5">Figure 1-5</a>. Like those tablets, they’re solid objects with small cavities—known as pits—made in them to represent data, as shown in <a href="ch10.xhtml#ch10fig20">Figure 10-20</a>. Like punch cards, they use binary encoding, so each location either contains a pit or doesn’t contain a pit. The pits are read using a laser, and their nanometer scales are comparable with the wavelengths of this laser light.</p>&#13;
<div class="image"><img id="ch10fig20" src="../images/f0239-01.jpg" alt="Image" width="749" height="425"/></div>&#13;
<p class="figcap"><em>Figure 10-20: Four generations of optical storage</em></p>&#13;
<p class="indent"><em>LaserDisc</em> (1978) was the first optical disc, having a 12-inch diameter like a vinyl album and marketed for home video. <em>Compact discs</em>, or <em>CDs</em> (1982), used roughly 800 nm pits, read by a laser head, to store up to 700 MB of audio data. CDs started seeing use for general rather than audio data storage in 1988 with the <em>CD-ROM</em> specification. Like CDs, these became read-only after initially creating the pits on their surfaces. <em>CD-R</em> was a version that simplified the recording process, allowing home users to “burn” their own CD-ROMs, again only once. These were used in the late 1990s for copying audio music collections, first using CD audio representations and then using bulk MP3 storage. They were usually blue on the burnable side and gold on top. Their “burning” was a physical process involving lasers and heat; this is the origin of modern slang “burning” now used for writing to other types of <span epub:type="pagebreak" id="page_240"/>ROM, such as flash or FPGA. <em>CD-RW</em> was an improved CD-ROM that could be rewritten several times.</p>&#13;
<p class="indent"><em>Digital Versatile Disc (DVD)</em> (1995), was an order of magnitude improvement, reducing pit size to 400 nm to achieve disc capacity of up to 4.7GB using the same size physical disc as CDs. DVDs were initially used for video but soon also for general data. As with CDs, write-once DVD-R and rewritable DVD-RW were also developed. <em>Blu-ray</em> (like its short-lived competitor, HD-DVD) reduced the pit size again, this time to 150 nm, allowing storage up to 25GB on the same size disc. As these pits are smaller, they require shorter-wavelength blue rather than infrared or red laser light to read them, hence the name.</p>&#13;
<h4 class="h4" id="lev206"><em>Solid-State Drives</em></h4>&#13;
<p class="noindent">For secondary storage, most current computers have moved from hard drives to <em>solid-state drives (SSDs)</em>. These are manufactured to have the same form factors and I/O interfaces, and similar capacities, as hard drives, but with no moving parts. This makes them faster, more reliable, lower power, quieter, smaller, and less prone to breakage when dropped. As there are no moving parts, they can be truly random access. SSDs are flash memory, as we’ve previously reviewed.</p>&#13;
<p class="indent">The same flash memory technology is also used as offline storage, where SSD drives are easily removable, such as when connected to I/O via USB (known as USB sticks) or SD (known as SD cards).</p>&#13;
<h3 class="h3" id="lev207">Tertiary Memory</h3>&#13;
<p class="noindent"><em>Tertiary memory</em> is a recently proposed level in the memory hierarchy. It lies below secondary memory but above offline memory, and has been created to describe memories that used to be offline—requiring humans to physically load and eject media such as discs and tapes—but is now automated by mechanical processes. For example, automated Blu-ray and LTO tape jukeboxes as in <a href="ch10.xhtml#ch10fig21">Figure 10-21</a> form tertiary memory.</p>&#13;
<div class="image"><img id="ch10fig21" src="../images/f0240-01.jpg" alt="Image" width="631" height="315"/></div>&#13;
<p class="figcap"><em>Figure 10-21: A robotic tape jukebox in a data center</em></p>&#13;
<p class="indent">In the figure, a robot arm is used—as in 1950s vinyl record jukeboxes—to pick up tapes and place them into the reader and storage containers. <span epub:type="pagebreak" id="page_241"/>Similar robotic systems can be built around Blu-ray discs. Mobile robots driving baskets of hard disks around can now also be considered tertiary memory.</p>&#13;
<h3 class="h3" id="lev208">Data Centers</h3>&#13;
<p class="noindent">When you put thousands, or tens or hundreds of thousands, of secondary and tertiary memories together in a warehouse-sized building, you get a <em>data center</em>. Search engines, social networks, online retailers, media streamers, and governments all now need to store and access data at this scale. A typical data center will contain many different layers of the lower levels of the memory hierarchy. For example, tapes take longer to fast-forward and rewind than disks, so these are more likely to be found as long-term backup systems than serving the latest social media posts. Once you access something from a slower backup system, it will then be cached somewhere higher up the memory hierarchy, such as on an SSD drive, making for faster retrieval next time.</p>&#13;
<p class="indent">Data centers may be built with extreme security and resilience in mind. For example, HSBC’s literal “data mine” is widely believed to store backups of all its global financial data in a former UK coal mine. You can tell it’s a data center because there are huge air ducts rising out of the ground to disperse all the heat from the computers. The mine is thought to be robust to nuclear, chemical, and biological attack. In the event of a nuclear war, the rest of humanity may be bombed back to computing with Ishango bones, but the bank will still be able to come after your mortgage repayments.</p>&#13;
<h3 class="h3" id="lev209">Summary</h3>&#13;
<p class="noindent">Memory architecture is driven by economics: you can buy big, slow, cheap memory; small, fast, expensive memory; or some mixture of both. Empirically, most programs show spatial, sequential, and temporal locality, in which different small parts of memory tend to be in heavy, repeated use at different times. Memory architectures are thus designed in hierarchies that fit both the economics and usage patterns, including caches between layers to promote currently in-use memory to higher levels. Primary memory is that which is addressed directly by the CPU, using the bus, while secondary memory is connected via I/O. Secondary memory often takes the form of spinning disks, which can be disconnected and replaced, becoming offline memory if humans are involved or tertiary memory if the process is automated by robotics.</p>&#13;
<h3 class="h3" id="lev210">Exercises</h3>&#13;
<h4 class="h4a"><strong>Your Computer’s Memory</strong></h4>&#13;
<ol class="number">&#13;
<li class="tm">Try to find the sizes and speeds for each type of memory in your own computer, including caches, RAM, and secondary storage. If you can open up your computer, look inside, locate them, and <span epub:type="pagebreak" id="page_242"/>find their makes and model numbers, then look up their datasheets online. Most operating systems have utilities that will display useful information about their memory; for example, Linux will show caches with <code>lscpu</code> <code>or cat /proc/cpuinfo</code>, RAM with <code>free -h</code>, and secondary memory with <code>lsblk</code>.</li>&#13;
</ol>&#13;
<h4 class="h4a"><strong>Building a Static RAM in LogiSim</strong></h4>&#13;
<ol class="number">&#13;
<li class="tm">Build the static random-access memory (SRAM) presented in <a href="ch06.xhtml#ch06fig22">Figure 6-22</a> in LogiSim. It should be able to store and read 2-bit words at the four memory locations.</li>&#13;
<li class="tm">Extend your LogiSim SRAM to have longer words and more addresses.</li>&#13;
</ol>&#13;
<h4 class="h4a"><strong>Challenging</strong></h4>&#13;
<ol class="number">&#13;
<li class="tm">Make four copies of your SRAM, representing multiple RAM chips. Each one will have the same address space, starting from address zero. Design a memory controller module that converts addresses from a larger global address space—having two extra bits—to sections of particular RAM chips and these local addresses within them.</li>&#13;
<li class="tm">Try attaching this system to the Manchester Baby model in place of its previous LogiSim RAM.</li>&#13;
</ol>&#13;
<h4 class="h4a"><strong>More Challenging</strong></h4>&#13;
<ol class="number">&#13;
<li class="tm">Design and build a direct-mapped cache in LogiSim and link it to your LogiSim RAM from the previous task. (This won’t speed up that RAM, as it’s already fast SRAM, but it could then enable that SRAM to be replaced by a larger and cheaper, but slower, DRAM.)</li>&#13;
<li class="tm">Try to build the other types of cache too, if you’re feeling brave. Use the sketches provided in this chapter as starting points.</li>&#13;
</ol>&#13;
<h3 class="h3" id="lev211">Further Reading</h3>&#13;
<p class="noindent">For a definitive recent classic on memory, see U. Drepper, “What Every Programmer Should Know About Memory,” November 21, 2007, <em><a href="https://people.freebsd.org/~lstewart/articles/cpumemory.pdf">https://people.freebsd.org/~lstewart/articles/cpumemory.pdf</a></em>. In fact, this resource contains far more than any normal human should know about memory.</p>&#13;
</div>
</div>
</body></html>