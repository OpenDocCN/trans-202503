<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><h2 class="h2part" id="part04"><span epub:type="pagebreak" id="page_163" class="calibre2"/><strong class="calibre3">PART IV</strong></h2>
<h2 class="h2parta"><strong class="calibre3">METHODS BASED ON SEPARATING LINES AND PLANES</strong></h2>
<p class="noindent">The methods we’ve looked at so far were developed by statisticians or, in the case of boosting, by both statisticians and ML researchers. In this part, the two methods under discussion, support vector machines and neural networks, originated solely in the ML world.</p>
<p class="indent">Both of these methods are used primarily in classification applications. Support vector machines are predicated on there being a line (in two dimensions—that is, two features) or a hyperplane (three or more features) that (mostly) separates the data by class. The coefficients in these lines or hyperplanes will work somewhat like those in linear models, though in a more complex manner. The coefficients are obtained by minimizing a sum similar to that of linear models, though again more complex.</p>
<p class="indent">Neural networks will also share these similarities. Using the most popular <em class="calibre13">activation function</em> <span class="literal">ReLU()</span>, the feature space is also broken up into portions defined by hyperplanes (though the original goal is not expressed in those terms), and again a certain sum is minimized.<span epub:type="pagebreak" id="page_164"/></p>
</div></body></html>