- en: '**1'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1'
- en: REGRESSION MODELS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型**
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/common.jpg)'
- en: In this chapter, we’ll introduce *regression functions*. Such functions give
    the mean of one variable in terms of one or more others—for instance, the mean
    weight of children in terms of their age. All ML methods are *regression methods*
    in some form, meaning that they use the data we provide to estimate regression
    functions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍*回归函数*。这种函数根据一个或多个其他变量来给出某个变量的均值——例如，根据年龄给出儿童的平均体重。所有机器学习方法在某种形式上都是*回归方法*，意味着它们利用我们提供的数据来估计回归函数。
- en: 'We’ll present our first ML method, k-nearest neighbors (k-NN), and apply it
    to real data. We’ll also weave in concepts that will recur throughout the book,
    such as dummy variables, overfitting, p-hacking, “dirty” data, and so on. We’ll
    introduce many of these concepts only briefly for the time being in order to give
    you a bird’s-eye view of what we’ll return to in detail later: ML is intuitive
    and coherent but easier to master if taken in stages. Reader, please be prepared
    for frequent statements like “We’ll cover one aspect for now, with further details
    later.”'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍第一个机器学习方法，k最近邻（k-NN），并将其应用于真实数据。我们还将融入一些贯穿全书的概念，如虚拟变量、过拟合、p-hacking、“脏数据”等。我们暂时只简要介绍这些概念，以便让你对后面会详细讨论的内容有一个宏观的了解：机器学习是直观且连贯的，但如果分阶段学习，掌握起来会更容易。读者，请做好准备，时常会看到类似“我们现在只讨论一个方面，稍后会详细介绍”的表述。
- en: 'Before you begin, make sure you have R and the `qeML` and `regtools` packages,
    version 1.7 or newer for the latter, installed on your computer. (Run packageVersion(''regtools'')
    to check.) All code displays in this book assume that the user has already made
    the calls to load the packages:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，确保你已经在计算机上安装了R和`qeML`以及`regtools`包（后者版本需为1.7或更新）。你可以运行`packageVersion('regtools')`来检查版本。书中的所有代码展示假定用户已经加载了这些包：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: So, let’s look at our first example dataset.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们来看一下第一个示例数据集。
- en: '1.1 Example: The Bike Sharing Dataset'
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 示例：自行车共享数据集
- en: Before we introduce k-NN, we’ll need to have some data to work with. Let’s start
    with this dataset from the UC Irvine Machine Learning Repository, which contains
    the Capital Bikeshare system’s hourly and daily count of bike rentals between
    2011 and 2012, with corresponding information on weather and other quantities.
    A more detailed description of the data is available at the UC Irvine Machine
    Learning Repository.[¹](footnote.xhtml#ch1fn1)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍k-NN之前，我们需要一些数据来进行操作。我们从UC Irvine机器学习库获取这个数据集，它包含2011年至2012年期间资本自行车共享系统的每小时和每天的租车数量，以及与天气和其他因素相关的信息。数据的更详细描述可在UC
    Irvine机器学习库中找到。[¹](footnote.xhtml#ch1fn1)
- en: The dataset is included as the `day` dataset in `regtools` by permission of
    the data curator. Note, though, that we will use a slightly modified version,
    `day1` (also included in `regtools`), in which the numeric weather variables are
    given in their original scale rather than transformed to the interval [0,1].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集由数据管理者许可，作为`regtools`中的`day`数据集提供。然而，请注意，我们将使用稍微修改过的版本`day1`（也包含在`regtools`中），其中数值型天气变量以其原始尺度提供，而不是转换到区间[0,1]。
- en: Our main interest will be in predicting total ridership for a day.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要兴趣是预测一天的总骑行人数。
- en: SOME TERMINOLOGY
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一些术语
- en: Say we wish to predict ridership from temperature and humidity. Standard ML
    parlance refers to the variables used for prediction—in this case, temperature
    and humidity—as *features.*
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们希望根据温度和湿度来预测骑行人数。标准的机器学习术语将用于预测的变量——在这种情况下是温度和湿度——称为*特征*。
- en: If the variable to be predicted is numeric, say, ridership, there is no standard
    ML term for it. We’ll just refer to it as the *outcome* variable. But if the variable
    to be predicted is an R factor—that is, a categorical variable—it is called a
    *label*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要预测的变量是数值型的，比如骑行人数，那么机器学习中没有标准术语来表示它。我们就称其为*结果*变量。但如果要预测的变量是R因子——即分类变量——它就被称为*标签*。
- en: 'For instance, later in this book we will analyze a dataset on diseases of human
    vertebrae. There are three possible outcomes or categories: normal (NO), disk
    hernia (DH), or spondylolisthesis (SL). The column in our dataset showing the
    class of each patient, NO, DH, or SL, would be the labels column.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在本书后面，我们将分析关于人类脊椎疾病的数据集。这个数据集有三种可能的结果或类别：正常（NO）、椎间盘突出（DH）或脊椎滑脱（SL）。我们数据集中显示每个病人类别的那一列，NO、DH
    或 SL，就是标签列。
- en: Our dataset, say, `day1` here, is called the *training set*. We use it to make
    predictions in future cases, in which the features are known but the outcome variable
    is unknown. We are predicting the latter.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集，比如这里的`day1`，被称为*训练集*。我们使用它来预测未来的情况，其中特征已知但结果变量未知。我们预测的正是后者。
- en: 1.1.1 Loading the Data
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.1.1 加载数据
- en: 'The data comes in hourly and daily forms, with the latter being the one in
    the `regtools` package. Load the data:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以每小时和每天两种形式呈现，后者是`regtools`包中的数据格式。加载数据：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With any dataset, it’s always a good idea to first take a look around. What
    variables are included in this data? What types are they, say, numeric or R factor?
    What are their typical values? One way to do this is to use R’s `head()` function
    to view the top of the data:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何数据集，首先查看数据总是个好主意。这个数据包含了哪些变量？它们是什么类型的，比如数值型还是R中的因子类型？它们的典型值是什么？一种做法是使用R的`head()`函数查看数据的前几行：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We see there are 731 rows (that is, 731 different days), with data on the date,
    nature of the date (such as `weekday`), and weather conditions (such as the temperature,
    `temp`, and humidity, `hum`). The last three columns measure ridership from casual
    users, registered users, and the total.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到数据集包含731行数据（即731个不同的日期），包括日期、日期性质（例如`weekday`）、以及天气状况（如温度`temp`和湿度`hum`）。最后三列分别表示来自临时用户、注册用户和总用户的骑行数据。
- en: You can find more information on the dataset with the `?day1` command.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过`?day1`命令获取更多关于数据集的信息。
- en: 1.1.2 A Look Ahead
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.1.2 展望未来
- en: We will get to actual analysis of this data shortly. For now, here is a preview.
    Say we wish to predict total ridership for tomorrow, based on specific weather
    conditions and so on. How will we do that with k-NN?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快就会开始对这些数据进行实际分析。现在，先来个预览。假设我们希望基于特定的天气条件等因素预测明天的总骑行量。我们如何通过k-NN算法来实现这一点？
- en: We will search through our data, looking for data points that match or nearly
    match those same weather conditions and other variables. We will then average
    the ridership values among those data points, and that will be our predicted ridership
    for this new day.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过搜索数据，寻找与这些天气条件及其他变量匹配或几乎匹配的数据点。然后，我们将这些数据点的骑行量取平均值，这将是我们对这一天的预测骑行量。
- en: Too simple to be true? No, not really; the above description is accurate. Of
    course, the old saying “The devil is in the details” applies, but the process
    is indeed simple. But first, let’s address some general issues.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这么简单，真的能行吗？其实不，确实如此；上述描述是准确的。当然，正如老话说的那样，“魔鬼藏在细节中”，但过程确实简单。不过，首先，让我们处理一些一般性的问题。
- en: 1.2 Machine Learning and Prediction
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 机器学习与预测
- en: ML is fundamentally about prediction. Before we get into the details of our
    first ML method, we should be sure we know what “prediction” means.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习本质上是关于预测的。在我们深入了解第一个机器学习方法的细节之前，我们应该确保明白“预测”是什么意思。
- en: Consider the bike sharing dataset. Early in the morning, the manager of the
    bike sharing service might want to predict the total number of riders for the
    day. The manager can do so by analyzing the relations between the features—the
    various weather conditions, the work status of the day (weekday, holiday), and
    so on. Of course, predictions are not perfect, but if they are in the ballpark
    of what turns out to be the actual number, they can be quite helpful. For instance,
    they can help the manager decide how many bikes to make available, with pumped-up
    tires and so on. (An advanced version would be to predict the demand for bikes
    at each station so that bikes could be reallocated accordingly.)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设考虑共享单车数据集。一天的清晨，共享单车服务的经理可能想要预测当天的总骑行人数。经理可以通过分析特征之间的关系——如各种天气条件、当天的工作状态（工作日、节假日）等——来做出预测。当然，预测并不完美，但如果结果接近实际数字，它们就非常有帮助。例如，它们可以帮助经理决定需要提供多少辆自行车，并确保轮胎充气完好等。（更高级的版本是预测每个站点的自行车需求，从而进行相应的重新分配。）
- en: 1.2.1 Predicting Past, Present, and Future
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.1 预测过去、现在与未来
- en: The famous baseball player and malapropist Yogi Berra once said, “Prediction
    is hard, especially about the future.” Amusing as this is, he had a point; in
    ML, prediction can refer not only to the future but also to the present or even
    the past. For example, a researcher may wish to estimate the mean wages workers
    made back in the 1700s. Or a physician may wish to make a diagnosis as to whether
    a patient has a particular disease, based on blood tests, symptoms, and so on,
    guessing their condition in the present, not the future. So when we in the ML
    field talk of “prediction,” don’t take the “pre-” too literally.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 著名棒球运动员和误用词语的笑话大师约吉·贝拉曾说过：“预测是困难的，特别是关于未来的预测。”尽管这句话很有趣，但他确实说到了一点；在机器学习中，预测不仅仅是指未来，也可以指现在甚至过去。例如，一位研究者可能希望估算1700年代工人的平均工资。或者一位医生可能希望根据血液检查、症状等做出诊断，判断患者是否患有某种疾病，这时是猜测患者当前的病情，而不是未来的病情。所以，当我们在机器学习领域谈论“预测”时，不要过于字面地理解“预-”这一部分。
- en: 1.2.2 Statistics vs. Machine Learning in Prediction
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.2 统计学与机器学习在预测中的区别
- en: A common misconception is that ML is concerned with prediction, while statisticians
    do *inference*—that is, confidence intervals and testing for quantities of interest—but
    prediction is definitely an integral part of the field of statistics.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的误解是，机器学习关注的是预测，而统计学家则做*推断*——即置信区间和对感兴趣量的检验——但预测无疑是统计学领域中的一个重要部分。
- en: There is sometimes a friendly rivalry between the statistics and ML communities,
    even down to a separate terminology for each (see [Appendix B](app02.xhtml)).
    Indeed, statisticians sometimes use the term *statistical learning* to refer to
    the same methods known in the ML world as machine learning!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学与机器学习社区之间有时存在一种友好的竞争关系，甚至在术语上也有所不同（参见[附录 B](app02.xhtml)）。事实上，统计学家有时使用“*统计学习*”这一术语来指代在机器学习领域中被称为机器学习的相同方法！
- en: As a former statistics professor who has spent most of his career in a computer
    science department, I have a foot in both camps. I will present ML methods in
    computational terms, but with some insights informed by statistical principles.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名曾在计算机科学系度过大部分职业生涯的前统计学教授，我可以同时站在两个阵营的立场上。我将以计算的术语来介绍机器学习方法，但也会结合统计学原理来提供一些见解。
- en: HISTORICAL NOTE
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 历史性备注
- en: Many of the methods treated in this book, which compose part of the backbone
    of ML, were originally developed in the statistics community. These include k-NN,
    decision trees or random forests, logistic regression, and L1/L2 shrinkage. These
    evolved from the linear models formulated way back in the 19th century, but which
    later statisticians felt were inadequate for some applications. The latter consideration
    sparked interest in methods that had less restrictive assumptions, leading to
    the invention first of k-NN and later of other techniques.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中讨论的许多方法，构成了机器学习的骨干部分，最初都是在统计学界发展起来的。这些方法包括k-NN、决策树或随机森林、逻辑回归，以及L1/L2收缩法。这些方法源自19世纪提出的线性模型，但后来的统计学家认为这些模型在某些应用中并不充分。正是这种考虑促使了对假设较少限制的方法的兴趣，最终促成了k-NN等技术的发明。
- en: On the other hand, two other prominent ML methods, support vector machines (SVMs)
    and neural networks, have been developed almost entirely outside of statistics,
    notably in university computer science departments. (Another method, *boosting*,
    began in computer science but has had major contributions from both factions.)
    Their impetus was not statistical at all. Neural networks, as we often hear in
    the media, were studied originally as a means to understand the workings of the
    human brain. SVMs were viewed simply in computer science algorithmic terms—given
    a set of data points of two classes, how can we compute the best line or plane
    separating them?
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，另外两种著名的机器学习方法，支持向量机（SVM）和神经网络，几乎完全在统计学领域之外发展，尤其是在大学计算机科学系中。（另一种方法，*boosting*，最初起源于计算机科学，但得到了两个领域的重大贡献。）它们的发展动力完全与统计学无关。正如我们在媒体中常听到的，神经网络最初是作为理解人类大脑运作的一种手段进行研究的。SVM则单纯以计算机科学算法的角度来看待——给定一组属于两类的数据点，我们如何计算出最佳的分割线或平面？
- en: 1.3 Introducing the k-Nearest Neighbors Method
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 介绍k-最近邻方法
- en: Our featured method in this chapter will be *k-nearest neighbors*, or *k-NN*.
    It’s arguably the oldest ML method, going back to the early 1950s, but it is still
    widely used today, especially in applications in which the number of features
    is small (for reasons that will become clear later). It’s also simple to explain
    and easy to implement—the perfect choice for this introductory chapter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点方法是*k-最近邻*，简称*k-NN*。它可以说是最古老的机器学习方法，最早可追溯到1950年代初期，但今天依然被广泛使用，尤其是在特征数量较少的应用场景中（原因将在后文解释）。它也很容易解释并且易于实现——是本章入门部分的完美选择。
- en: 1.3.1 Predicting Bike Ridership with k-NN
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.1 使用k-NN预测自行车使用量
- en: 'Let’s first look at using k-NN to predict bike ridership from a single feature:
    temperature. Say the day’s temperature is forecast to be 28 degrees centigrade.
    How should we predict ridership for the day, using the 28 figure and our historical
    ridership dataset (our training set)? A person without a background in ML might
    suggest looking at all the days in our data, culling out those of temperature
    closest to 28 (there may be few or none with a temperature of exactly 28), and
    then finding the average ridership on those days. We would use that number as
    our predicted ridership for this day.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来看一下如何使用k-NN根据单一特征：温度预测自行车使用量。假设当天的温度预报为28摄氏度。我们应该如何使用28这个温度值和我们的历史骑行数据集（训练集）来预测当天的使用量呢？没有机器学习背景的人可能会建议查看数据中的所有日期，筛选出温度最接近28的那些日期（可能没有或很少有完全28度的日期），然后计算这些日期的平均骑行量。我们将用这个平均值作为当天的预测骑行量。
- en: Actually, this intuition is correct! This, in fact, is the basis for many common
    ML methods, as we’ll discuss further in [Section 1.6](ch01.xhtml#ch01lev6) on
    the regression function. For now, just know that k-NN takes the form of simply
    averaging over the similar cases—that is, over the neighboring data points. The
    quantity *k* is the number of neighbors we use. We could, say, take the 5 historical
    days with temperatures closest to 28, average the numbers for those days, and
    use the result to predict ridership on a new 28-degree day.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这个直觉是正确的！实际上，这正是许多常见机器学习方法的基础，正如我们将在[第1.6节](ch01.xhtml#ch01lev6)的回归函数中进一步讨论的那样。目前，只需要知道，k-NN的形式就是简单地对相似的案例进行平均——也就是对相邻的数据点进行平均。*k*是我们使用的邻居数量。例如，我们可以选择温度最接近28的5个历史天数，对这些天的骑行量进行平均，然后用这个结果来预测温度为28度的当天的使用量。
- en: Later in this chapter, we will learn to use the `qe*`-series implementation
    of k-NN, `qeKNN()`. For now, though, let’s perform k-NN “manually,” so as to get
    a better understanding of the method.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面的部分，我们将学习如何使用`qe*`系列实现的k-NN，`qeKNN()`。不过目前，我们先手动执行k-NN操作，以便更好地理解这种方法。
- en: Okay, ready to go! In this section we will make our first predictions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，准备好了！在这一节中，我们将做出我们的第一个预测。
- en: 1.3.1.1 R Subsetting Review
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 1.3.1.1 R子集操作回顾
- en: Before presenting the code, let’s review a few aspects of the R language. Recall
    that in R, the `#` symbol is for comments; that is, it is not part of the code
    itself but is for explanatory purposes. The comments in the code below and throughout
    the book are used as inline explanations of what the code is doing.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示代码之前，让我们回顾一下R语言的一些方面。回想一下，在R中，`#`符号用于注释；也就是说，它不是代码的一部分，而是用于解释的。下面代码中的注释和全书中的注释都用来作为代码执行过程的内联解释。
- en: 'For the upcoming example, you’ll also need to remember how subsetting works
    in R. Take this snippet, for instance:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于即将到来的示例，您还需要记住在R中如何进行子集操作。例如，看看这段代码：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The expression `x[c(2,4,5)]` extracts the 2nd, 4th, and 5th elements from the
    vector `x`. Also recall that here we refer to 2, 4, and 5 as *subscripts* or *indices*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式`x[c(2,4,5)]`提取向量`x`中的第2、第4和第5个元素。同时请记住，这里我们将2、4和5称为*下标*或*索引*。
- en: 1.3.1.2 A First Prediction
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 1.3.1.2 第一个预测
- en: 'Here, then, is our small, manually performed k-NN example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们的小型手动k-NN示例：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The distance between any two numbers is the absolute value of their difference:
    |25 − 32| = 7, so 25 is a distance 7 from 32\. This is why we made the call to
    R’s `abs()` (absolute value) function. R’s `order()` function is like `sort()`,
    except that it shows us the indices of the sorted numbers. Here is an example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 两个数之间的距离是它们差的绝对值：|25 − 32| = 7，因此25与32的距离是7。这就是我们调用R的`abs()`（绝对值）函数的原因。R的`order()`函数类似于`sort()`，不同之处在于它展示了排序数字的索引。以下是一个示例：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The values 2, 3, and so on are saying, “The first-smallest number in `x` was
    `x[2]`, the second-smallest was `x[3]`, and so on.” The line
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数值 2、3 等表示：“`x` 中的最小值是 `x[2]`，第二小的是 `x[3]`，以此类推。”这一行
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: will place in `do5` the indices of rows in `day1` that have the 5-closest temperatures
    to 28\. In this case, the temperatures are quite close to 28; the furthest is
    only 0.08 distant.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将会把`do5`中包含的索引设置为`day1`中与 28 度温度最接近的 5 个行。在这种情况下，温度与 28 度非常接近；最远的差距仅为 0.08。
- en: What were the ridership values on those days?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 那些天的乘车人数是多少？
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We then take the average of those values:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算这些值的平均值：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can now predict that on a day with a temperature of 28 degrees, about 5,200
    riders will use the bike sharing service.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以预测，在气温为 28 度的日子里，大约会有 5,200 名乘客使用共享单车服务。
- en: 'There are some issues left hanging, notably: Why take the 5 days nearest in
    temperature to 28? Is 5 too small a sample, or is it sufficient to make an accurate
    prediction? This is a central issue in ML, to which we’ll return in [Section 1.7](ch01.xhtml#ch01lev7).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些悬而未决的问题，尤其是：为什么选择与 28 度最接近的 5 天？样本量 5 是否太小，或者是否足以做出准确预测？这是机器学习中的一个核心问题，我们将在[第
    1.7 节](ch01.xhtml#ch01lev7)中进一步探讨。
- en: 1.4 Dummy Variables and Categorical Variables
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4 虚拟变量与分类变量
- en: To work with this dataset, and do ML in general, you’ll need to be able to understand
    the several columns in the data that represent *dummy variables*. Dummy variables
    take on values 1 and 0 only, depending on whether they satisfy a particular condition.
    For instance, in the `workingday` column, 0 stands for “No” (the date in question
    is not a working day) and 1 stands for “Yes” (the date is a working day). The
    date 2011-01-05 has a 1 in the `workingday` column, meaning yes, this was a working
    day.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这个数据集并进行机器学习，你需要理解数据中几个表示*虚拟变量*的列。虚拟变量只有 1 和 0 两种取值，取决于它们是否满足特定条件。例如，在`workingday`列中，0表示“否”（所示日期不是工作日），1表示“是”（所示日期是工作日）。2011年1月5日的`workingday`列值为
    1，表示这是一个工作日。
- en: Dummy variables are sometimes more formally called *indicator variables* because
    they *indicate* whether a certain condition holds (code 1) or not (code 0). An
    alternative term popular in ML circles is *one-hot coding*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟变量有时被更正式地称为*指示变量*，因为它们*指示*某个特定条件是否成立（代码 1）或不成立（代码 0）。在机器学习领域，另一种流行的术语是*独热编码*。
- en: Our bike sharing data also includes the categorical variables `mnth` and `weekday`.
    There is also a feature `weathersit` consisting of four categories (1 = clear,
    2 = mist or cloudy, 3 = light snow or light rain, 4 = heavy rain or ice pellets
    or thunderstorm). That variable could be considered categorical as well.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的共享单车数据还包括分类变量`mnth`和`weekday`。还有一个特征`weathersit`，包含四个类别（1 = 晴天，2 = 薄雾或多云，3
    = 小雪或小雨，4 = 大雨、冰雹或雷暴）。这个变量也可以视为分类变量。
- en: One very common usage of dummy variables is coding of categorical data. In a
    marketing study, for instance, a factor of interest might be type of region of
    residence, say, Urban, Suburban, or Rural. Our original data might code these
    as 1, 2, or 3\. However, those are just arbitrary codes, so, for example, there
    is no implication that Rural is 3 times as good as Urban. Yet ML algorithms may
    take it that way, which is not what we want.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟变量的一种常见用法是对分类数据的编码。例如，在市场研究中，一个感兴趣的因素可能是居住地区的类型，比如城市、郊区或乡村。我们原始的数据可能将这些类别编码为
    1、2 或 3。然而，这些只是任意的代码，因此，例如，乡村并不意味着比城市好三倍。但机器学习算法可能会将其理解为如此，这是我们不希望的。
- en: The solution, used throughout this book and throughout the ML field, is to use
    dummy variables. We could have a dummy variable for Urban (1 = yes, 0 = no) and
    one for Suburban. Rural-ness would then be coded by having both Urban and Suburban
    set to 0, so we don’t need a third dummy variable (having one might cause technical
    problems beyond the scope of this book). Of course, there is nothing special about
    using the first two values as dummies; we could have, say, one for Urban and one
    for Rural, without Suburban; the latter would then be indicated by 0 values in
    Urban and Rural.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 本书及整个机器学习领域常用的解决方案是使用虚拟变量。我们可以为城市（1 = 是，0 = 否）和郊区创建虚拟变量。乡村属性则通过将城市和郊区都设置为 0
    来编码，这样我们就不需要第三个虚拟变量（拥有第三个变量可能会引发本书范围之外的技术问题）。当然，使用前两个值作为虚拟变量并没有什么特别之处；我们也可以仅为城市和乡村创建虚拟变量，而不使用郊区；在这种情况下，郊区会通过城市和乡村的
    0 值来表示。
- en: In the current chapter, we focus on applications in which our outcome variable
    is numeric, such as total ridership in the bike sharing data. But in many applications,
    the outcome variable is categorical, such as our earlier example of predicting
    vertebral disease. In such settings, termed *classification applications*, the
    *Y* variable is categorical and must be converted to dummies.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前章节中，我们专注于应用场景，其中我们的结果变量是数值型的，例如共享单车数据中的总骑行量。但在许多应用中，结果变量是类别型的，例如我们之前的预测脊椎疾病的例子。在这种情况下，称为*分类应用*，*Y*变量是类别型的，必须转换为虚拟变量。
- en: Fortunately, most ML packages, including `qeML`, do conversions to dummies automatically,
    as we will see shortly.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，大多数机器学习包，包括`qeML`，会自动将数据转换为虚拟变量，正如我们稍后所看到的那样。
- en: 1.5 Analysis with qeKNN()
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5 使用 qeKNN() 进行分析
- en: Now that we have a better sense of what’s going on under the hood, let’s try
    using the `qeKNN()` function to perform some k-NN analysis. As noted in the introduction,
    this book uses the `qe*`-series wrapper functions. For k-NN, this means `qeKNN()`.
    The latter *wraps*, or provides a simple interface for, `regtools`’s basic k-NN
    function, `kNN()`. In other words, `qeKNN()` calls `kNN()` but in a simpler, more
    convenient manner.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们对底层的运作有了更清晰的了解，让我们尝试使用 `qeKNN()` 函数进行一些k-NN分析。正如在介绍中所提到的，本书使用了 `qe*` 系列的封装函数。对于
    k-NN，这意味着使用 `qeKNN()`。后者*封装*了 `regtools` 的基本 k-NN 函数 `kNN()`，也就是说，`qeKNN()` 调用了
    `kNN()`，但以更简单、方便的方式。
- en: Before we start our analysis, we’ll introduce some “X” and “Y” notation to help
    us keep track of what we’re doing. Take notes—this will be important to remember
    for all subsequent chapters.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始分析之前，我们将引入一些“X”和“Y”符号，以帮助我们跟踪正在进行的工作。做笔记——这对于之后的章节非常重要。
- en: '**FEATURES X AND OUTCOMES Y**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征 X 和结果 Y**'
- en: 'The following informal shorthand used to refer to features and outcomes is
    pretty standard in both the ML and statistics fields:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是机器学习和统计学领域中常用的非正式简写，表示特征和结果的标准符号：
- en: Traditionally, one collectively refers to the features as *X* and the outcome
    to be predicted as *Y*.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统上，特征通常统称为 *X*，而要预测的结果称为 *Y*。
- en: '*X* is a set of columns in a data frame or matrix. If we are predicting ridership
    from temperature and humidity, *X* consists of those latter two columns in our
    data. *Y* here is the ridership column.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*X* 是数据框或矩阵中的一组列。如果我们根据温度和湿度预测骑行量，*X* 就是数据中这两个列。这里的 *Y* 是骑行量列。'
- en: In 2-class classification applications, *Y* will typically be a dummy variable,
    a column of 1s and 0s. However, in multiclass classification applications, *Y*
    is a set of columns, one for each dummy variable. Equivalently, *Y* could be an
    R factor, stored in a single column.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在二分类应用中，*Y* 通常是一个虚拟变量，即一列1和0。然而，在多分类应用中，*Y* 是一组列，每一列对应一个虚拟变量。或者，*Y* 也可以是一个R因子，存储在单列中。
- en: 'One more bit of standard notation:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个标准符号：
- en: The number of rows in *X*—that is, the number of data points—is typically denoted
    by *n*.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*X* 中的行数——即数据点的数量——通常用 *n* 来表示。'
- en: The number of columns in *X*—that is, the number of features—is typically denoted
    by *p*.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*X* 中的列数——即特征的数量——通常用 *p* 来表示。'
- en: This is just a convenient shorthand. It’s easier, for instance, to say “X” rather
    than the more cumbersome “our feature set.” Again, *X*, *Y*, *n*, and *p* will
    appear throughout this book (and elsewhere, as they are standard in the ML field),
    so be sure to commit them to memory.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个方便的简写。例如，说“X”比说更繁琐的“我们的特征集”要容易。再次强调，*X*、*Y*、*n* 和 *p* 将在本书中（以及其他地方，因为它们是机器学习领域的标准符号）反复出现，因此请确保记住它们。
- en: 1.5.1 Predicting Bike Ridership with qeKNN()
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.5.1 使用 qeKNN() 预测共享单车骑行量
- en: For the bike sharing example, we’ll predict total ridership on any given day.
    Let’s start out using as features just the dummy for working day and the numeric
    weather variables.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于共享单车的例子，我们将预测任意一天的总骑行量。我们先从使用工作日的虚拟变量和数值型天气变量作为特征开始。
- en: 'Let’s extract those columns of the `day1` data frame. As we saw in [Section
    1.1.1](ch01.xhtml#ch01lev1sec1), they are in columns 8 and 10 through 13, with
    column 16 containing the outcome variable, the total ridership (`tot`). Thus we
    can obtain them via the following expression:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来提取 `day1` 数据框中的这些列。正如我们在[第1.1.1节](ch01.xhtml#ch01lev1sec1)中看到的，它们位于第8列和第10至13列，而第16列包含结果变量，即总骑行量（`tot`）。因此，我们可以通过以下表达式获得这些列：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This extracts the desired columns.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这提取了所需的列。
- en: '**NOTE**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Alternatively, you may prefer to use column names rather than numbers*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*或者，您可能更喜欢使用列名而非数字*'
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '*in base R, or use the tidyverse or data.table, each of which works fine. Numeric
    data frame indexing is much easier to type, but use of column names may be clearer.*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*在基础R中，或者使用tidyverse或data.table，每种方法都可以正常工作。数字数据框索引更容易输入，但使用列名可能会更清晰。*'
- en: '*As pointed out in the introduction, this is a book about ML that happens to
    use R as its vehicle of instruction rather than a book about R in ML. The way
    that individual readers handle data manipulation in R is not the focus of this
    book, so feel free to use your own preferred way to achieve the same results.*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*正如在介绍中所指出的，这是一本关于机器学习的书，它恰好使用R作为教学工具，而不是一本关于R在机器学习中的书。各个读者如何处理R中的数据操作并不是本书的重点，所以请随意使用你自己喜欢的方式来实现相同的结果。*'
- en: So, we form the sub-data frame, and as usual, take a look.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们构建子数据框，并像往常一样查看一下。
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now let’s try a prediction. Say you are the manager this morning, and the day
    is a working day, with temperature 12.0, atemp 11.8, humidity 23 percent, and
    wind at 5 miles per hour. What is your prediction for the ridership?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来进行一次预测。假设你是今天早上的经理，今天是工作日，温度12.0，atemp为11.8，湿度为23%，风速为每小时5英里。你会如何预测骑行人数？
- en: 'All the `qe*`-series functions have a very simple call form:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所有`qe*`系列函数都有一个非常简单的调用形式：
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The option used in this example will be *k*, the number of nearest neighbors,
    which we’ll take to be 5\. (If we do not specify *k* in the call, the default
    value is 25.)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例中使用的选项是*k*，即最近邻的数量，我们设定为5个。（如果在调用时没有指定*k*，默认值是25。）
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We are applying the k-NN method, taking our “Y” (or “outcome,” as you’ll recall)
    to be the variable `tot` in the data frame `day1`, with 5 neighbors. We’ll discuss
    the holdout set shortly, but put that aside for now.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在应用k-NN方法，将我们的“Y”（或者“结果”，如你所记得）设为数据框`day1`中的变量`tot`，并使用5个邻居。我们稍后会讨论持出集，但现在先不考虑这个。
- en: We saved the return value of `qeKNN()` in `knnout`. (Of course, you can use
    whatever name you wish.) It contains lots of information, but we won’t consider
    it at this point. The function `qeKNN()` did the prep work, enabling us to use
    `knnout` to do future predictions.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`qeKNN()`的返回值保存在`knnout`中。（当然，你可以使用任何你喜欢的名字。）它包含了大量信息，但我们暂时不考虑这些信息。`qeKNN()`函数做了预处理，使我们能够使用`knnout`进行未来的预测。
- en: 'So, how exactly is that done? The general form for the `qe*`-series functions
    is likewise extremely simple:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这到底是怎么做到的呢？`qe*`系列函数的一般形式同样非常简单：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s say, as manager of the bike sharing business, we know that today is a
    workday, and the temperature, atemp, humidity, and wind speed will be 12.8, 11.8,
    0.23, and 5, respectively. Here is our prediction:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 假设作为共享单车业务的经理，我们知道今天是工作日，温度、atemp、湿度和风速分别为12.8、11.8、0.23和5。以下是我们的预测：
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We fed `knnout` into the `predict()` function, along with our specified prediction
    point, `today`, yielding the predicted number of riders: about 6,300\. Observe
    that the argument `today` was a data frame with the same column names as in the
    original dataset `day1`. This is needed, both here and in many R ML packages,
    in order to match up the names in the prediction point with the names of the training
    set.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`knnout`传入`predict()`函数，并指定预测点`today`，得到预测的骑行人数：约6,300人。注意，参数`today`是一个数据框，其列名与原始数据集`day1`中的列名相同。这是必须的，无论在这里还是在许多R机器学习包中，都是为了将预测点的名称与训练集的名称匹配。
- en: '**NOTE**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*If you run the above code yourself, you will likely get different output,
    due to randomness of the holdout set. There is a way to standardize the result
    so that different people obtain the same result; this will be explained in [Section
    1.12.3](ch01.xhtml#ch01lev12sec3).*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你自己运行上述代码，可能会得到不同的输出，因为测试集的随机性。确实有一种方法可以标准化结果，以确保不同的人得到相同的结果；这一点将在[第1.12.3节](ch01.xhtml#ch01lev12sec3)中解释。*'
- en: 'Let’s do another prediction, say, the same as above but with wind speed at
    18:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再做一个预测，假设和上述相同，但风速为18：
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: People don’t seem to want to ride as much in the wind.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 人们似乎在风中不太愿意骑行。
- en: 'The second argument, such as `anotherday` above, can be any data frame with
    the same column names. For instance, we could have asked for both predictions
    together:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数，比如上面的`anotherday`，可以是任何具有相同列名的数据框。例如，我们本可以同时要求两个预测：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In [Section 1.8](#ch01lev8), we will deepen our insight into k-NN by analyzing
    another dataset.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1.8节](#ch01lev8)中，我们将通过分析另一个数据集深入探讨k-NN。
- en: 'TECHNICAL NOTE: PREDICT() AND GENERIC FUNCTIONS'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 技术说明：PREDICT()和通用函数
- en: We saw that `qeKNN()` is paired with a `predict()` function. All functions in
    the `qe*`-series are similarly paired, which is a very common technique in R.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到`qeKNN()`与`predict()`函数配对。所有`qe*`系列的函数也都是如此配对，这在R中是一种非常常见的技巧。
- en: While all these functions appear to share the same `predict()`, each one actually
    has its own separate prediction function, such as `predict.qeKNN()` in the case
    of `qeKNN()`. The function `predict()` itself is called a *generic function* in
    R. It simply *dispatches* calls, meaning it relays the original call to a function
    specific to the type of analysis we are doing. Thus a call to `predict()` on an
    object created by `qeKNN()` will actually be relayed to `predict.qeKNN()`, and
    so on.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所有这些函数看起来都使用相同的`predict()`，每个函数实际上都有自己独立的预测函数，例如在`qeKNN()`的情况下是`predict.qeKNN()`。`predict()`本身在R中被称为*通用函数*。它只是*分派*调用，这意味着它将原始调用转发到一个特定于我们正在进行的分析类型的函数。因此，调用`predict()`在由`qeKNN()`创建的对象上，实际上会被转发到`predict.qeKNN()`，依此类推。
- en: R has various generic functions, some of which you’ve probably already been
    using, perhaps without knowing it. Examples are `print()`, `plot()`, and `summary()`.
    We’ll see an example of `plot()` in [Section 9.6](ch09.xhtml#ch09lev6).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: R有多种通用函数，其中一些你可能已经在使用了，也许你并没有意识到。比如`print()`、`plot()`和`summary()`。我们将在[第9.6节](ch09.xhtml#ch09lev6)中看到`plot()`的例子。
- en: '1.6 The Regression Function: The Basis of ML'
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.6 回归函数：机器学习的基础
- en: To understand machine learning methods, you need to know *what* is being “learned.”
    The answer is something called the *regression function*. Directly or indirectly
    (often the latter), it is the basis for ML methods. It gives the mean value of
    one variable, holding another variable fixed. Let’s make this concrete.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解机器学习方法，你需要知道*学习的是什么*。答案是一个叫做*回归函数*的东西。直接或间接地（通常是后者），它是机器学习方法的基础。它给出了一个变量的均值，同时保持另一个变量不变。让我们具体说明一下。
- en: '**NOTE**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: Regression function *is a general statistical and ML term, much broader than
    the concept of linear regression that some readers may have learned in a statistics
    course.*
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 回归函数*是一个广泛的统计学和机器学习术语，比一些读者在统计学课程中学到的线性回归概念要广泛得多。*
- en: Recall our example above, where we took as our predicted value for a 28-degree
    day the mean ridership of days near that temperature. If we were to predict the
    ridership on a 15-degree day, we would use for our prediction the mean ridership
    of all days with a temperature of 15, or near 15, and so on. Denoting the regression
    function by *r*(), the quantities of interest are *r*(28), *r*(15), and so on.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 回想我们之前的例子，其中我们将28度的日子的预测值定为接近该温度的几天的平均乘客量。如果我们要预测15度的日子的乘客量，我们将使用所有15度或接近15度的日子的平均乘客量作为预测值，依此类推。用*r*()表示回归函数，感兴趣的量是*r*(28)、*r*(15)等。
- en: We say *r*() is the regression function of ridership on temperature. It is indeed
    a function; for every input (a temperature), we get an output (the associated
    mean ridership). And we use the function as our predictions; for instance, to
    predict ridership on a 15-degree day, we use an estimate of *r*(15).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说*r*()是乘客量对温度的回归函数。它确实是一个函数；对于每一个输入（一个温度），我们得到一个输出（相应的平均乘客量）。我们使用这个函数来进行预测；例如，要预测15度日子的乘客量，我们使用*r*(15)的估计值。
- en: But it’s an unknown function, not something familiar like `sqrt()`. So, we need
    to use our training data to infer values of the function. In ML parlance, we say
    that we “learn” this function, using our data, showing you where the L comes from
    in ML. (The M just means we use a computer or algorithm to do the learning.) For
    instance, in the above example, we *learn r*(28) by averaging the ridership values
    over the days with a temperature close to 28\. The word *learn* is thus reflected
    in *train*, in the term *training data*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 但是它是一个未知的函数，而不是像`sqrt()`这样熟悉的东西。因此，我们需要使用我们的训练数据来推断该函数的值。在机器学习术语中，我们说我们“学习”这个函数，使用我们的数据，展示机器学习中L的来源。（M仅表示我们使用计算机或算法来进行学习。）例如，在上面的例子中，我们通过平均温度接近28的几天的乘客量来*学习
    r*(28)。因此，“学习”一词反映在*训练*中，即*训练数据*一词。
- en: It is customary to use the “hat” notation for “estimate of.” Thus we denote
    our estimate of *r*() by ![Images](../images/rcap.jpg).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通常使用“帽子”符号表示“估计值”。因此，我们用![Images](../images/rcap.jpg)表示我们对*r*()的估计值。
- en: The regression function is also known as the *conditional mean*. In predicting
    ridership from temperature, *r*(28) is the mean ridership, subject to the *condition*
    that temperature is 28\. That’s a subpopulation mean, which is quite different
    from the overall population mean ridership.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 回归函数也被称为 *条件均值*。在根据温度预测乘客量时，*r*(28) 是在温度为 28 的条件下的平均乘客量。这是一个子人群的均值，和整个总人群的均值有很大不同。
- en: 'Let’s summarize these important points:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下这些重要的观点：
- en: The regression function *r*() gives the mean of our outcome variable as a function
    of our features.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归函数 *r*() 给出了我们的结果变量作为特征的函数的均值。
- en: We estimate *r*() from our training data. We call the estimate ![Images](../images/rcap.jpg).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从训练数据中估计 *r*()。我们将这个估计值称为 ![Images](../images/rcap.jpg)。
- en: We use ![Images](../images/rcap1.jpg) as the basis of our predictions.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 ![Images](../images/rcap1.jpg) 作为我们预测的基础。
- en: Any function has arguments. A regression function has as many arguments as we
    have features. Let’s take humidity as a second feature, for instance. To predict
    ridership for a day with temperature 28 and humidity 0.51, we would use the mean
    ridership in our dataset among days in which temperature and humidity are approximately
    28 and 0.51\. In regression function notation, that’s *r*(28, 0.51). In the example
    on [page 12](ch01.xhtml#page_12), the value of interest was *r*(1, 12.8, 11.8,
    0.23, 5).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 任何函数都有参数。回归函数的参数数量与我们有多少特征有关。比如，我们将湿度作为第二个特征。为了预测温度为 28 并且湿度为 0.51 的一天的乘客量，我们会使用在温度和湿度大约为
    28 和 0.51 的日子中，我们数据集中的平均乘客量。在回归函数的符号中，就是 *r*(28, 0.51)。在[第 12 页](ch01.xhtml#page_12)的示例中，关注的值是
    *r*(1, 12.8, 11.8, 0.23, 5)。
- en: As previously noted, the regression function forms the basis, directly or indirectly,
    in all predictive ML methods. It will come up repeatedly throughout the book.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，回归函数在所有预测性机器学习方法中，直接或间接，都是基础内容。在本书中，它会反复出现。
- en: 1.7 The Bias-Variance Trade-off
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7 偏差-方差权衡
- en: 'In the introduction, specifically in [Section 0.8](introduction.xhtml#ch00lev8),
    we implored the reader:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在引言部分，特别是在[第 0.8 节](introduction.xhtml#ch00lev8)，我们恳请读者：
- en: A page that is all prose—no math, no graphs, and no code—may be one of the most
    important pages in the book.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一页完全由散文构成——没有数学、没有图表，也没有代码——可能是本书中最重要的一页。
- en: The pages in this current section are prime examples of this, as the *Bias-Variance
    Trade-off* is one of the most famous topics in the field. My Google query yielded
    18,400,000 results! It is an absolutely central issue in ML, which we will treat
    in depth in [Chapter 3](ch03.xhtml). However, you should be aware of it from the
    beginning, so let’s give an overview.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的页面是这一点的典范，因为 *偏差-方差权衡* 是该领域最著名的话题之一。我的谷歌搜索结果显示了 18,400,000 个相关结果！这是机器学习中的一个绝对核心问题，我们将在[第
    3 章](ch03.xhtml)中深入探讨。然而，你应该从一开始就意识到它，所以我们来做一个概述。
- en: The issue is, for example, the choice between greater or smaller values of *k*.
    Larger values of *k* have smaller variance but larger bias, and smaller values
    of *k* have the opposite effect. Let’s see how this works.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，例如，选择较大或较小的 *k* 值。较大的 *k* 值具有较小的方差，但较大的偏差；而较小的 *k* 值则有相反的效果。让我们看看这一点是如何运作的。
- en: 1.7.1 Analogy to Election Polls
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.7.1 与选举民调的类比
- en: First consider an analogy to election surveys. During an election campaign,
    voter polls will be taken to estimate the popularity of the various candidates.
    An analyst takes a random sample of the set of all telephone numbers and solicits
    opinions from those who pick up the phone.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 首先考虑与选举调查的类比。在选举竞选期间，会进行选民民意调查，以估计各候选人的受欢迎程度。分析师会从所有电话号码的集合中随机抽取样本，并向接电话的人征求意见。
- en: Suppose we are interested in *p*, the proportion of the entire population that
    favors Candidate C. Since we just have a sample from that population, we can only
    estimate the value of *p* using the proportion ![Images](../images/pcap.jpg) who
    like Candidate C in our sample. Accordingly, the poll results are accompanied
    by a *margin of error*, to recognize that the reported proportion ![Images](../images/pcap.jpg)
    is only an estimate of *p*. (Those who have studied statistics may know that the
    margin of error is the radius of a 95 percent confidence interval.)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们对 *p* 感兴趣，*p* 是支持候选人 C 的整个选民群体的比例。由于我们只有该人群体的一个样本，我们只能通过我们样本中喜欢候选人 C 的比例
    ![Images](../images/pcap.jpg) 来估计 *p* 的值。因此，民意调查结果会伴随一个 *误差范围*，以表明报告的比例 ![Images](../images/pcap.jpg)
    仅是 *p* 的估计值。（有统计学背景的人可能知道，误差范围是 95% 置信区间的半径。）
- en: The margin of error gives an indication of the accuracy of our estimate. It
    measures sampling variability. A large value means that our estimate of *p* varies
    a lot from one sample to another; if the pollster were to call a new random sample
    of phone numbers, the value of our estimated *p* likely would be rather different,
    possibly quite different if the sample size is small. Of course, the pollster
    is not going to take a second sample, but the amount of sampling variability from
    one sample to the next tells us how reliable ![Images](../images/pcap.jpg) is
    an estimate of *p*. The margin of error reflects that sampling variability, and
    if it is large, then our sample size was too small.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 误差范围给出了我们估计值的准确性。它衡量的是抽样变异性。值越大，说明我们对 *p* 的估计在不同样本之间差异越大；如果调查员再次随机抽取电话号码，估计的
    *p* 值可能会有很大不同，尤其是样本量较小的时候。自然，调查员不会再进行第二次抽样，但从一个样本到下一个样本的抽样变异性告诉我们 *p* 的估计有多可靠。误差范围反映了这种抽样变异性，如果误差范围较大，则说明我们的样本量太小。
- en: The key issue, then, is sampling variability, which is called the *variance*
    of ![Images](../images/pcap.jpg). It can be computed in the polling example from
    the margin of error.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 关键问题是抽样变异性，也就是 *方差*，它可以通过误差范围在投票示例中进行计算。
- en: A bias issue may also arise. Suppose the pollster has a list of landline phones
    but not cell phones. Many people, especially younger ones, don’t have a landline,
    so calling only those with landlines may bias our results.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 也可能会出现偏差问题。假设调查员只有固定电话的号码列表，没有手机号码。许多人，尤其是年轻人，没有固定电话，因此仅拨打固定电话的号码可能会导致结果有偏差。
- en: 1.7.2 Back to ML
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.7.2 回到机器学习
- en: Returning to ML, consider the bike sharing example in [Section 1.5.1](#ch01lev5sec1),
    where we wished to have the value of *r*(1, 12.8, 11.8, 0.23, 5), which we wish
    to use as our predicted value. We will obtain an estimate, ![Images](../images/rcap1.jpg)(1,
    12.8, 11.8, 0.23, 5), as the actual predicted value.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 回到机器学习，考虑[第1.5.1节](#ch01lev5sec1)中的共享单车示例，我们希望得到 *r*(1, 12.8, 11.8, 0.23, 5)
    的值，这将作为我们的预测值。我们将得到一个估计值，![Images](../images/rcap1.jpg)(1, 12.8, 11.8, 0.23, 5)，作为实际预测值。
- en: We treat the data on the number of riders per day as a sample from the (rather
    conceptual) population of all days, past, present, and future. Using the k-NN
    method (or any other ML method), we are only obtaining an estimate ![Images](../images/rcap1.jpg)
    of the true population regression function *r*(). Forming a prediction from just
    the closest *k* = 5 neighbors works from a very small sample. Imagine the pollster
    sampling only 5 voters! In another sample, the closest days to our point to be
    predicted would be different, with different ridership values. *In other words,
    this is a variance issue.*
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每日骑行者数量的数据视为从所有日子（无论是过去、现在还是未来）的（相当抽象的）总体中抽取的样本。使用 k-NN 方法（或任何其他机器学习方法），我们只是在估计真实总体回归函数
    *r*()。仅从最近的 *k* = 5 个邻居中形成预测值，实际上是基于一个非常小的样本。想象一下，调查员只抽取了 5 位选民！在另一个样本中，与我们预测的点最接近的日子会不同，骑行人数的值也会不同。*换句话说，这是一个方差问题。*
- en: On the other hand, using just *k* = 5, we found in [Section 1.3.1.2](ch01.xhtml#ch01lev3sec1sec2)
    that the 5 neighbors were all quite close to the prediction point. Suppose we
    look at the nearest *k* = 50 neighbors. In that case, we risk using data points
    far from the prediction point, which are thus not very similar to it. *This would
    create a bias problem.*
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，使用 *k* = 5，我们在[第1.3.1.2节](ch01.xhtml#ch01lev3sec1sec2)中发现，5个邻居都非常接近预测点。假设我们查看最近的
    *k* = 50 个邻居。在这种情况下，我们可能会使用距离预测点较远的数据点，这些数据点与预测点的相似度较低。*这将导致偏差问题。*
- en: In sum, larger values of *k* reduce variance but at the expense of increasing
    bias. We want to find a “Goldilocks” value for *k*—that is, one not too small
    and not too large.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，较大的 *k* 值降低了方差，但以增加偏差为代价。我们想找到一个“黄金值” *k* ——即既不太小也不太大。
- en: 'However, note that computation time and the amount of memory the method requires
    are also important factors: if the best method takes too long to run or uses too
    much memory, we may end up choosing a different method.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，计算时间和方法所需的内存量也是重要因素：如果最佳方法运行时间过长或占用内存过多，我们可能会选择另一种方法。
- en: '1.8 Example: The mlb Dataset'
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.8 示例：mlb 数据集
- en: To cement your understanding of `qeKNN()` and introduce another example that
    we can refer to throughout the rest of the book, let’s try a similar operation
    on the `mlb` dataset. This dataset, provided courtesy of the UCLA Statistics Department,
    records the heights and weights of Major League Baseball players in inches and
    pounds, respectively. It’s included in `regtools`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加深你对`qeKNN()`的理解，并介绍一个我们可以在本书剩余部分中参考的例子，让我们尝试在`mlb`数据集上进行类似的操作。该数据集由UCLA统计学系提供，记录了大联盟棒球运动员的身高和体重，单位分别是英寸和磅。它包含在`regtools`中。
- en: 'Let’s glance at the data first so we know what we’re working with:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先浏览一下数据，这样我们知道自己在处理什么：
- en: '[PRE18]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Let’s predict the weight of a new player for whom it is only known that height
    and age are 72 and 24, respectively.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们预测一个新球员的体重，我们只知道他的身高和年龄分别为72和24。
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note again that we needed to specify the prediction point (72,24) in the same
    data frame form as `mlb`, the dataset on which we had fit the model.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 请再次注意，我们需要在与`mlb`相同的数据框形式中指定预测点（72,24），即我们拟合模型的那个数据集。
- en: 1.9 k-NN and Categorical Features
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.9 k-NN 和类别特征
- en: In the previous baseball player example, both of the features, height and age,
    were numeric. But what if we were to add a third feature, `Position`, a categorical
    variable? Since k-NN is distance-based, the features need to be numeric in order
    to compute distances between data points. How can we use k-NN with non-numeric
    features?
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的棒球球员示例中，身高和年龄这两个特征都是数值型的。但是，如果我们添加一个第三个特征，`Position`，这是一个类别变量呢？由于k-NN是基于距离的，特征需要是数值型的，以便计算数据点之间的距离。那么我们如何使用k-NN处理非数值型特征呢？
- en: The answer, of course, is that the categorical variables (that is, R factors)
    should be converted to dummy variables. We could do this here via the `regtools`
    function `factorToDummies()`. However, as the `qe*`-series functions do this conversion
    internally when needed, we need not convert `Position` to dummies on our own.
    The `qeKNN()` function will also make a note in its output for later use by `predict()`,
    to make the same conversions when predicting.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 答案当然是，类别变量（也就是R因素）应该转换为虚拟变量。我们可以通过`regtools`函数`factorToDummies()`来完成这个转换。不过，由于`qe*`系列函数在需要时会自动进行这种转换，我们不需要自己将`Position`转换为虚拟变量。`qeKNN()`函数也会在其输出中做出标记，以便`predict()`在预测时进行相同的转换。
- en: 'For example, suppose we want to calculate another new player’s weight, using
    the categorical `Position` feature, in addition to height and age:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想计算一个新球员的体重，除了身高和年龄外，还使用类别变量`Position`：
- en: '[PRE20]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In our first prediction, not using `Position`, our predicted value was about
    183 pounds. But if we know that the new player is a catcher, we see that, at least
    in this case, our predicted value increases to 197\. This makes sense; catchers
    do tend to be heavyset so they can guard home plate.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次预测中，没有使用`Position`时，我们的预测值大约是183磅。但如果我们知道新球员是捕手，我们会看到，在这种情况下，预测值增加到197磅。这是有道理的；捕手通常体型较大，以便守住本垒。
- en: The `qe*`-series functions identify categorical features by their status as
    R factors. In the example here, `Position` is indeed an R factor. As noted, the
    internals of `qeKNN()` and `predict.qeKNN()` will automatically do the conversion
    to dummies for us, which is a major convenience.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`qe*`系列函数通过其作为R因素的状态来识别类别特征。在这里的示例中，`Position`确实是一个R因素。如前所述，`qeKNN()`和`predict.qeKNN()`的内部机制会自动为我们完成虚拟变量的转换，这是一个非常方便的功能。'
- en: Typically, categorical features will already be expressed as factors in data
    frames. In some cases, a feature will be categorical but expressed in terms of
    numeric codes. If so, apply `as.factor()` to the feature to convert it to factor
    form.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，类别特征已经在数据框中作为因素表达。在某些情况下，特征是类别型的，但以数值编码的形式表达。如果是这种情况，可以应用`as.factor()`将其转换为因素形式。
- en: 1.10 Scaling
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.10 数据缩放
- en: A theme encountered in many ML methods is that of *scaling* our data. In your
    future ML projects, it’s good practice to keep scaling in mind. It’s used in many
    ML methods and may produce better results even if a method doesn’t require it.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习方法中都涉及到一个主题，那就是*数据缩放*。在你未来的机器学习项目中，最好记得考虑缩放。许多机器学习方法都使用缩放，即使某些方法不要求缩放，进行缩放也可能产生更好的结果。
- en: 'Let’s go back to the baseball player data. Consider two players, one of height
    70 and age 24 and one of height 72 and age 30\. Taking these two pairs of numbers
    in an algebra context, the distance between them is the distance between the points
    (70,24) and (72,30) in the plane:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到棒球选手的数据。考虑两个选手，一个身高70，年龄24，另一个身高72，年龄30。在代数的上下文中，考虑这两个数值对，它们的距离是平面上(70,24)和(72,30)这两点之间的距离：
- en: '![Image](../images/ch1eq1.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch1eq1.jpg)'
- en: 'Distances like this would be computed in k-NN. But what if height were converted
    to, say, meters, and age to months? The heights would be divided by 39.37, while
    age would be multiplied by 12:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的距离会在k-NN中进行计算。但如果身高被转换为米，年龄转换为月数，会怎样呢？身高会除以39.37，而年龄则会乘以12：
- en: '[PRE21]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: That creates a problem in that age would dominate the computation, with height
    playing only a small role. Since height obviously is a major factor in predicting
    weight, the change in units would probably reduce our prediction ability.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这样会产生一个问题，即年龄会主导计算，身高只起到较小的作用。由于身高显然是预测体重的重要因素，因此单位的变化可能会降低我们的预测能力。
- en: The solution is to do away with units like inches, meters, and so on, which
    is called *scaling*. To scale our data, we first subtract the mean, giving everything
    mean 0, called *centering*. Then we divide each feature by its standard deviation,
    *scaling*. This gives everything a standard deviation of 1\. Now all the features
    are unitless and commensurate. (Usually, when doing scaling, we also do centering,
    and the combined process is simply called scaling.)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方法是去掉像英寸、米这样的单位，这叫做*缩放*。为了缩放我们的数据，首先减去平均值，使每个数据的均值为0，这叫做*中心化*。然后，我们将每个特征除以其标准差，*缩放*。这使得所有特征的标准差为1。现在，所有特征都没有单位且具有可比性。（通常，在进行缩放时，我们也会进行中心化，这个过程通常叫做缩放。）
- en: 'To make this more concrete, as a former or current student, you may recall
    one of your professors converting examination scores in this way: “To get an A,
    you needed to be 1.5 standard deviations above the mean.” That professor was subtracting
    the mean exam score, then dividing by the standard deviation. The R function `scale()`
    performs this operation for us, but to illustrate, here is how we would do it
    on our own:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个概念更具体化，作为一名曾经或现在的学生，你可能会记得其中一位教授这样转换考试分数：“要获得A，你需要比平均分高出1.5个标准差。”这位教授是在减去考试的平均分，然后除以标准差。R函数`scale()`为我们执行了这个操作，但为了说明，下面是我们如何自己操作的：
- en: '[PRE22]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `qeKNN()` function has an argument `scaleX` for this purpose. Its default
    value is `TRUE`, so scaling was done by default in our above k-NN examples. In
    each *X* column (recall this means feature column) of our data, `qeKNN()` will
    transform that column by calling `scale()`. (Actually, we can scale all the *X*
    variables with a single `scale()` call.)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`qeKNN()`函数有一个参数`scaleX`，用于此目的。其默认值为`TRUE`，所以在我们之前的k-NN示例中，默认进行了缩放。在每个*X*列（回想一下这意味着特征列）中，`qeKNN()`会通过调用`scale()`来转换该列。（实际上，我们可以通过一次`scale()`调用来缩放所有*X*变量。）'
- en: Of course, we must remember to do the same scaling—dividing by the same standard
    deviations—in the *X* values of new cases that we predict, such as new days in
    our bike sharing example. The `qe*`-series functions make a note of this, which
    is then used by the paired `predict()` functions.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们必须记得对新的预测案例中的*X*值进行相同的缩放——即除以相同的标准差——比如在共享单车示例中的新一天。`qe*`系列函数会注意到这一点，然后由配套的`predict()`函数使用。
- en: '**NOTE**'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Keep in mind that scale() won’t work on a vector in which all values are identical,
    as the standard deviation would be 0\. We can check for this by calling the regtools
    function constCols(), which will report all constant columns in our data frame.*'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*请记住，scale()不会对所有值相同的向量起作用，因为标准差为0。我们可以通过调用regtools函数constCols()来检查这一点，该函数会报告数据框中的所有常数列。*'
- en: 'In the bike sharing data, `day1` uses unscaled data, for instructional purposes.
    The “official” version of the data, `day`, does scale. It does so in a different
    manner, though: it is scaled such that the values of the variables lie in the
    interval [0,1]. One way to do that is to transform by:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在共享单车数据中，`day1`使用的是未经缩放的数据，主要用于教学目的。数据的“官方”版本`day`是经过缩放的。虽然它以不同的方式进行缩放：它将变量的值缩放到区间[0,1]内。实现这一点的一种方法是通过以下方式进行转换：
- en: '![Image](../images/ch01eq02.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/ch01eq02.jpg)'
- en: This has an advantage over `scale()` by producing *bounded variables*—that is,
    numbers bounded by 0 and 1\. By contrast, `scale()` produces variables in the
    interval (*–∞*, *∞*), and a variable with a small standard deviation will likely
    have very large scaled values, giving that variable undue influence in the analysis.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这比 `scale()` 有优势，因为它生成了*有界变量*——即被限制在 0 和 1 之间的数值。相比之下，`scale()` 生成的变量位于区间 (*–∞*,
    *∞*) 内，而标准差较小的变量可能会有非常大的缩放值，这会在分析中给予该变量过大的影响。
- en: 'The `regtools` function `mmscale()` does the above mapping to [0,1]. Here is
    a small example:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`regtools` 函数 `mmscale()` 执行上述的映射，将数据转换到 [0,1] 范围内。这里是一个小例子：'
- en: '[PRE23]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `u` column had a mean of 4, with a minimum and maximum of 3 and 5, respectively.
    Therefore, the 4 in the second row was replaced by (4 – 3) / (5 – 3) = 1/2, for
    example. As you can see, all the resulting values are in [0,1] and are unitless—that
    is, no inches or months.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`u` 列的均值为 4，最小值和最大值分别为 3 和 5。因此，第二行的 4 被替换为 (4 – 3) / (5 – 3) = 1/2，例如。正如你所看到的，所有结果值都在
    [0,1] 范围内，并且是无量纲的——也就是说，没有英寸或月份的单位。'
- en: 1.11 Choosing Hyperparameters
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.11 选择超参数
- en: In the introduction, we mentioned that the number of nearest neighbors *k* is
    a *hyperparameter* or *tuning parameter*, a value chosen by the user that affects
    the predictive ability of the model. As noted in [Section 1.7](ch01.xhtml#ch01lev7),
    *k* is a “Goldilocks” quantity that needs to be carefully set for best performance—not
    too small and not too large.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍中，我们提到最近邻的数量 *k* 是一个*超参数*或*调优参数*，是由用户选择的一个值，它会影响模型的预测能力。正如在[第 1.7 节](ch01.xhtml#ch01lev7)中所述，*k*
    是一个“金发姑娘”的量值，需要仔细设置以获得最佳性能——既不能太小，也不能太大。
- en: Finding the best *k* can be a challenge. Luckily, k-NN has only one hyperparameter,
    but as you will find later in the book, most ML methods have several; in some
    advanced methods outside the scope of the book, there may even be a dozen or more.
    Choosing the “right” combination of values for several hyperparameters is especially
    difficult, but even choosing a good value for a single hyperparameter is nontrivial.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 找到最佳的 *k* 可能是一个挑战。幸运的是，k-NN 只有一个超参数，但正如你稍后在书中看到的，大多数机器学习方法有多个超参数；在一些超出本书范围的高级方法中，甚至可能有十个或更多。选择多个超参数的“正确”组合尤其困难，但即便是为单个超参数选择一个合适的值也并非易事。
- en: We take our first look below at ways to tackle the challenging problem of picking
    hyperparameters and then go into more details in [Chapter 3](ch03.xhtml). Spend
    a little extra time on this section, since this issue will arise repeatedly in
    this book and throughout your ML career.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面首次探讨解决选择超参数这一挑战性问题的方法，然后将在[第 3 章](ch03.xhtml)中详细讨论。请在这一节多花点时间，因为这个问题将在本书中反复出现，并贯穿你整个机器学习职业生涯。
- en: 1.11.1 Predicting the Training Data
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.11.1 预测训练数据
- en: Almost all methods for choosing hyperparameters involve testing our model by
    predicting new cases. In the most basic form, we go back and predict our original
    training data. This sounds odd—we know the ridership values in the data, so why
    predict them? The idea is to try various values of *k* and see which one predicts
    our known data the best. That then would be the value of *k* that we use for predicting
    new *X* data in the future.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有选择超参数的方法都涉及通过预测新案例来测试我们的模型。在最基本的形式中，我们会回头预测我们原始的训练数据。这听起来有点奇怪——我们已经知道数据中的乘车量，那么为什么还要预测它们呢？其背后的思路是尝试不同的
    *k* 值，看看哪一个能最好地预测我们的已知数据。然后，这个 *k* 值将是我们未来预测新 *X* 数据时所使用的值。
- en: 'This is not ideal, and in practice a slight modification of this approach is
    used. We will use it too, but before presenting it, let’s go through an illustration
    of what can go wrong. Let’s predict the third data point in `day1` using the smallest
    possible value of *k*: 1.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不理想，实际上通常会对这种方法进行轻微修改。我们也会使用这种方法，但在介绍它之前，让我们先通过一个例子看看可能会出错的地方。让我们用 *k* 的最小值：1
    来预测 `day1` 中的第三个数据点。
- en: '[PRE24]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We predicted exactly correctly! But wait a minute . . . that seems suspicious,
    and it is. The closest neighbor to data point 3 (the third row in our data) is
    itself! The distance from that point to itself is 0\. Similarly, row 8 is the
    closest data point to row 8, row 56 is the closest data point to row 56, and so
    on. Of course we were 100 percent correct; we took the average of the 1-closest
    neighbor, thus just duplicating the *Y* value. The same analysis shows that even,
    say, *k* = 5 would give us overly optimistic prediction accuracy. One of the 5
    neighbors would still be the original data point, thus biasing our prediction.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预测完全正确！但是等一下……这似乎有些可疑，的确是。数据点3（我们数据中的第三行）最接近的邻居是它自己！该点到它自己的距离是0。同样地，第8行的数据点与第8行最接近，第56行的数据点与第56行最接近，依此类推。当然我们是100%正确的；我们取的是1个最近邻的平均值，因此只是复制了*Y*值。同样的分析表明，即使是*k*
    = 5也会给出过于乐观的预测准确度。5个邻居中的一个仍然是原始数据点，从而偏向了我们的预测。
- en: The takeaway is that when we evaluate the prediction accuracy of a given value
    of *k*, we should predict on a different dataset than the one to which we fit
    the k-NN method. But, you protest, we only have one dataset. How can we get another
    for properly assessing prediction accuracy? That’s the topic of the following
    section on holdout sets.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是，当我们评估给定*k*值的预测准确性时，我们应该在一个与拟合k-NN方法的数据集不同的数据集上进行预测。但是，你可能会抗议，我们只有一个数据集。那么，我们如何才能得到另一个数据集来正确评估预测准确性呢？这正是下一节关于验证集的内容。
- en: 1.12 Holdout Sets
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.12 验证集
- en: Again, as noted in the previous section, to assess the predictive accuracy of
    our model, we need to try it out on “fresh” data, not the data it was fit on.
    But we don’t have any new data, so what can be done? Key to solving this conundrum
    are the notions of *holdout sets* and *cross-validation* covered in this section.
    They are central to ML and will recur throughout the book.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在前一节中提到的，为了评估模型的预测准确性，我们需要在“新鲜”的数据上进行测试，而不是在模型拟合过的数据上。但是我们没有新的数据，那该怎么办呢？解决这个困境的关键是本节介绍的*验证集*和*交叉验证*的概念。它们是机器学习中的核心内容，并且会在整本书中反复出现。
- en: In the 731 data points in `day1`, we could randomly cull out, say, 100 of them.
    These will serve as our *holdout set*, or *test set*. The remaining 631 will temporarily
    be our training set. We fit the model to this training set, then see how well
    it predicts on the holdout set, which serves as “fresh” data not biased by training.
    (In examples here, the holdout size is 73, as the default size is 10 percent of
    the dataset size.)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在`day1`的731个数据点中，我们可以随机抽取出，比如说100个数据点。这些数据点将作为我们的*验证集*，或者*测试集*。其余的631个数据点将暂时作为我们的训练集。我们将模型拟合到这个训练集上，然后查看它在验证集上的预测效果，验证集作为“新鲜”的数据，不受训练数据的偏倚。（在这里的例子中，验证集大小是73，因为默认大小是数据集的10%）
- en: Technically, our accuracy in future predictions will be best if we fit our model
    on the entire dataset, in which case we set `holdout=NULL`. However, in preliminary
    exploration, it’s important to have some idea as to how well our model works on
    new data. Thus it’s best to have a holdout set during the exploration phase, then
    refit the model on the entire data once we’ve chosen hyperparameters and so on.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，如果我们将模型拟合到整个数据集，未来的预测准确度最好，此时我们将`holdout=NULL`。然而，在初步探索阶段，了解我们的模型在新数据上的表现是很重要的。因此，在探索阶段最好使用验证集，然后在选择超参数等后，重新拟合整个数据集。
- en: Before we select our holdout set, if we are to evaluate quality of prediction,
    we need a criterion for evaluating prediction accuracy.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择验证集之前，如果我们要评估预测的质量，我们需要一个评估预测准确性的标准。
- en: 1.12.1 Loss Functions
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.12.1 损失函数
- en: For a data point in the holdout set, we could take the absolute difference between
    the actual value and predicted value, then average those absolute differences
    over all holdout points to get the Mean Absolute Prediction Error (MAPE). This
    is an example of a *loss function*, which is simply a criterion for goodness of
    prediction. We could take MAPE as that criterion, with smaller values being better.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对于验证集中的一个数据点，我们可以计算实际值和预测值之间的绝对差值，然后将所有验证集点的绝对差值取平均，得到平均绝对预测误差（MAPE）。这是一个*损失函数*的例子，损失函数只是一个预测优度的标准。我们可以将MAPE作为这个标准，值越小越好。
- en: Another popular loss function is Mean Squared Prediction Error, or MSPE, where
    we average the squares of the prediction errors rather than their absolute values.
    MSPE is the more common of the two, but I prefer to use MAPE, as MSPE overly accentuates
    large errors. Say we are predicting weight in the MLB data. Consider two cases
    in which we are in error by 12 and 15 pounds. Those two numbers are fairly similar,
    but their squares, 144 and 225, are quite different.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的损失函数是均方预测误差（MSPE），它是将预测误差的平方进行平均，而不是其绝对值。MSPE是两者中更常用的一个，但我更倾向于使用MAPE，因为MSPE过于强调大误差。假设我们在MLB数据中预测体重，考虑两种情况，其中误差分别为12和15磅。这两个数字相当相似，但它们的平方，144和225，差别就很大了。
- en: For classification applications, the most commonly used loss function is simply
    the overall probability of misclassification. We predict each *Y* in the holdout
    set, tally the number of times we are wrong, and divide by the size of the holdout
    set.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类应用，最常用的损失函数就是简单的误分类总体概率。我们预测保留集中的每个*Y*，统计错误的次数，然后除以保留集的大小。
- en: 1.12.2 Holdout Sets in the qe*-Series
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.12.2 `qe*`系列中的保留集
- en: The `qe*`-series functions will automatically perform the above process of finding
    the MAPE, or overall probability of misclassification, then report the result
    in the output component `testAcc`. These functions will sense whether our application
    is a classification problem according to whether we specify *Y* (second argument
    in any `qe*`-series call) as an R factor. If so, then the misclassification probability
    will be computed instead of MAPE.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`qe*`系列函数将自动执行上述寻找MAPE或整体误分类概率的过程，然后将结果报告在输出组件`testAcc`中。这些函数会根据我们是否将*Y*（任何`qe*`系列调用中的第二个参数）指定为R因子，自动判断我们的应用是分类问题。如果是这样，计算的将是误分类概率，而不是MAPE。'
- en: 'We’ll turn again to the bike sharing dataset for an example of finding `qeKNN`’s
    automatically generated MAPE:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用共享单车数据集，作为寻找`qeKNN`自动生成的MAPE的例子：
- en: '[PRE25]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Using 5 nearest neighbors, we make an average prediction error of about 1,200
    riders. That’s not great, but better than the alternative, as follows.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用5个最近邻，我们的平均预测误差大约是1,200名骑行者。虽然这不是很好，但比起另一种方法要好，如下所示。
- en: 'Suppose we had no access to weather conditions and so on. How could we predict
    ridership? One natural idea would be to just use the overall mean:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们无法获取天气条件等信息，那我们该如何预测骑行量呢？一个自然的想法是直接使用总体平均值：
- en: '[PRE26]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In other words, every day, including the one at hand, we would predict about
    4,500 riders. How well would we fare using this strategy?
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，每天，包括今天，我们将预测大约4,500名骑行者。使用这个策略我们会有怎样的表现呢？
- en: '[PRE27]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If we were to always predict ridership using the overall mean riders per day,
    our average prediction error would be nearly 1,600\. Using the weather variables
    and `workingday` as predictors does help bring MAPE down to 1,200.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们总是使用每日的总体平均骑行人数来预测骑行量，我们的平均预测误差将接近1,600。使用天气变量和`workingday`作为预测因子确实有助于将MAPE降低到1,200。
- en: 'Can we find a better value for *k* than 5? Let’s try, say, 10 and 25:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能找到比5更好的*k*值吗？我们来试试10和25：
- en: '[PRE28]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Of the values we’ve tried, *k* = 10 seems to work best, though we must keep
    in mind the randomness of the holdout sets. Indeed, it is better to try several
    holdout sets for each candidate value of *k*, the topic of our next section.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们尝试的值中，*k* = 10似乎是最有效的，尽管我们必须记住保留集的随机性。事实上，对于每个候选的*k*值，最好尝试多个保留集，这是我们下一节的主题。
- en: 1.12.3 Motivating Cross-Validation
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.12.3 激励交叉验证
- en: When using `qeKNN`’s automatically generated MAPEs, it is important to remember
    that the software is choosing the holdout set at random. The holdout set size
    is only 73, a somewhat small sample—imagine our election pollster above sampling
    only 73 voters. Thus there will be considerable *sampling variation* between MAPE
    in one holdout set and another. We can solve this problem through performing *cross-validation*—that
    is, averaging the values of MAPE over multiple holdout sets.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`qeKNN`自动生成的MAPE时，重要的是要记住，软件是随机选择保留集的。保留集的大小只有73，这是一个相对较小的样本——想象一下我们上面的选举民调员只抽取了73名选民。因此，在不同保留集之间，MAPE会有相当大的*抽样波动*。我们可以通过执行*交叉验证*来解决这个问题——也就是说，计算多个保留集的MAPE值的平均值。
- en: 'To demonstrate sampling variation, let’s try running the same code a couple
    more times:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示抽样波动，让我们再运行几次相同的代码：
- en: '[PRE29]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We can see that we cannot rely too much on that one MAPE value; the sample size
    of 73 is too small. There is a lot more to say on this issue, and as noted, we
    will resume this discussion in [Chapter 3](ch03.xhtml). Suffice it to say now
    that we should look at many holdout sets and then perform cross-validation by
    averaging the resulting `testAcc` values.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们不能过分依赖那个 MAPE 值；样本量为 73 太小了。这个问题还有很多值得讨论的地方，正如前面提到的，我们将在[第 3 章](ch03.xhtml)中继续讨论这个问题。现在可以简单说一下，我们应该查看多个验证集，然后通过计算得到的
    `testAcc` 值的平均值来进行交叉验证。
- en: 'By the way, we can control R’s random number generator by using the `set.seed()`
    function:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，我们可以通过使用 `set.seed()` 函数来控制 R 的随机数生成器：
- en: '[PRE30]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We will often do this in the book. It sets a certain sequence of random numbers,
    in case the reader wishes to run the code and check the results. By using the
    same seed, 9999 here, the same training and holdout sets will be generated as
    what I had here. (I just chose 9999 as a favorite; there is nothing special about
    it.)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们经常这么做。它设置了一定的随机数序列，以防读者希望运行代码并检查结果。通过使用相同的种子，这里是 9999，将生成与我这里的训练集和验证集相同的数据集。（我只是选择了
    9999 作为我喜欢的种子，没什么特别的。）
- en: Clearly, we will obtain more accurate results by generating several holdout
    sets, thus averaging the resulting MAPE values. This is called *cross-validation*,
    which will be discussed in detail in [Chapter 3](ch03.xhtml).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，通过生成多个验证集并计算得到的 MAPE 值的平均值，我们可以获得更准确的结果。这就是所谓的 *交叉验证*，将在[第 3 章](ch03.xhtml)中详细讨论。
- en: 1.12.4 Hyperparameters, Dataset Size, and Number of Features
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.12.4 超参数、数据集大小和特征数量
- en: 'Recall the discussion in [Section 1.7.2](ch01.xhtml#ch01lev7sec2) regarding
    a trade-off involving the choice of the number of nearest neighbors *k*:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下[1.7.2 节](ch01.xhtml#ch01lev7sec2)中关于选择最近邻数 *k* 的权衡问题：
- en: If we set *k* = 5, we will be averaging just 5 data points, which seems too
    few. This is a variance problem; averages of 5 *Y* values at a given *X* will
    vary a lot from one sample to another.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们设置 *k* = 5，那么我们将平均 5 个数据点，这似乎太少了。这是一个方差问题；在给定的 *X* 值下，5 个 *Y* 值的平均值在不同样本间差异会很大。
- en: On the other hand, setting *k* = 50, we would likely have some points in the
    neighborhood that are far away from the point to be predicted and thus unrepresentative.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，设置 *k* = 50 时，我们可能会在邻域中有一些距离被预测点很远的点，这些点可能就不具有代表性。
- en: For example, in the bike data, say we are predicting ridership on a 20-degree
    day, which is rather comfortable. Points in our training set with an uncomfortable
    temperature like 40 are not very relevant to the prediction at hand and would
    tend to make our prediction too low. This is a bias problem.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，在共享单车数据中，假设我们预测的是在 20 度的天气下的骑行人数，这种天气相当舒适。训练集中温度为 40 度的样本与当前预测关系不大，往往会导致我们预测的结果过低。这是一个偏差问题。
- en: 'So, we have a trade-off. We want to make *k* large to achieve low variance,
    but setting a large *k* incurs the risk of substantial bias. But . . . what if
    our bike sharing dataset were to have, say, *n* = 73000 data points rather than
    731? In that case, the 50th-nearest neighbor might actually be pretty close to
    the prediction point, solving our bias problem. Then we could afford to use a
    larger *k* so as to hold down variance. In other words:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们面临一个权衡。我们希望将 *k* 设置得较大以降低方差，但设置较大的 *k* 也可能带来较大的偏差风险。但是……如果我们的共享单车数据集有 *n*
    = 73000 个数据点，而不是 731 个呢？在这种情况下，第 50 个最近邻可能实际上与预测点非常接近，从而解决了偏差问题。这样，我们就可以使用较大的
    *k* 来控制方差。换句话说：
- en: All else being equal, the larger *n* is, the larger we can make *k*.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 其他条件相同的情况下，*n* 越大，我们可以设置的 *k* 也就越大。
- en: This still doesn’t tell us what specific *k* to choose. We’ll return to this
    issue in [Chapters 3](ch03.xhtml) and [4](ch04.xhtml), but it is something to
    at least be aware of at this early stage.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这仍然没有告诉我们应选择哪个特定的 *k* 值。我们将在[第 3 章](ch03.xhtml)和[第 4 章](ch04.xhtml)中回到这个问题，但至少在这个早期阶段，这是一个需要注意的事项。
- en: 'The corresponding statement concerning the number of features *p* is:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 关于特征数量 *p* 的相应说明是：
- en: All else being equal, the larger *p* is, the larger we must make *k*.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 其他条件相同的情况下，*p* 越大，我们必须设置更大的 *k*。
- en: This is less intuitive than the previous statement involving *n*, but roughly
    this is the issue. Having more features means more variability in interpoint distances,
    thus increasing variance in predictions. To counter this, we need a larger *k*.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题比涉及 *n* 的前一个陈述更不直观，但大致上问题是这样的。特征更多意味着点间距离的变化性更大，从而增加了预测的方差。为了解决这个问题，我们需要更大的
    *k*。
- en: '1.13 Pitfall: p-Hacking and Hyperparameter Selection'
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.13 陷阱：p-hacking与超参数选择
- en: In non-ML settings, *p-hacking* refers to the following pitfall in analyses
    of large studies. Say one is studying genetic impacts on some outcome, with very
    large numbers of genes involved. Even if no gene has a real impact, due to sampling
    variation, one of them will likely appear to have a “significant” impact just
    by accident. Though we won’t go into detail on how to solve this just yet, you
    should be aware of it from the beginning.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在非机器学习（ML）环境下，*p-hacking*指的是分析大规模研究时的一个常见陷阱。假设有人正在研究基因对某些结果的影响，并且涉及了大量的基因。即便没有任何基因真正产生影响，由于采样的变化，其中一个基因很可能仅仅因为偶然的原因而显得有“显著”的影响。虽然我们现在不深入探讨如何解决这个问题，但你应该从一开始就意识到它的存在。
- en: p-hacking also has major implications for the setting of hyperparameters in
    ML. Let’s say we have four tuning parameters in an ML method, and we try 10 values
    of each. That’s 10⁴ = 10000 possible combinations. Even if all of them are equally
    effective, the odds are that one of them will accidentally have a much better
    MAPE value. What seems to be the “best” setting for the hyperparameters may be
    illusory.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: p-hacking对机器学习中的超参数设置也有重要影响。假设我们在一个机器学习方法中有四个调优参数，并且每个参数试验10个值。那么就有10⁴ = 10000种可能的组合。即便它们都同样有效，也有可能其中一个组合偶然地拥有一个远优于其他组合的MAPE值。看似“最佳”的超参数设置可能只是一个错觉。
- en: The `regtools` function `fineTuning()` takes steps to counter the possibility
    of p-hacking in searches for the best tuning parameter combination. We’ll cover
    more on this in [Chapter 7](ch07.xhtml).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`regtools`函数`fineTuning()`采取了一些措施来应对在搜索最佳调优参数组合时可能出现的p-hacking问题。我们将在[第7章](ch07.xhtml)中进一步讨论。'
- en: '1.14 Pitfall: Long-Term Time Trends'
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.14 陷阱：长期时间趋势
- en: 'Before we move on to the next chapter, let’s cover three major pitfalls you
    may encounter while working with the methods introduced here that may influence
    the quality of your predictions: dirty data, missing data, and longterm trends
    in the data. We will deal with the last pitfall first.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始进入下一章之前，让我们先讨论在使用这里介绍的方法时可能会遇到的三大陷阱，这些陷阱可能会影响你预测的质量：脏数据、缺失数据和数据中的长期趋势。我们将首先处理最后一个陷阱。
- en: 'In the few experiments we did with the MLB data, we found that the best MAPE
    value might be around 1,100\. This seems rather large, but recall we are not using
    all of our data. Look again at the bike dataset in [Section 1.1.1](ch01.xhtml#ch01lev1sec1).
    There are several variables involving timing of the observation: date, season,
    year, and month. To investigate whether the timing data can improve our MAPE value,
    let’s graph ridership against time. (The data is in chronological order.)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对MLB数据进行的少数几次实验中，我们发现最佳的MAPE值可能约为1,100。这看起来相当大，但请记住，我们并没有使用所有的数据。再看一下[1.1.1节](ch01.xhtml#ch01lev1sec1)中的共享单车数据。数据中有几个与观察时间相关的变量：日期、季节、年份和月份。为了研究这些时间数据是否能改善我们的MAPE值，让我们画出骑行人数与时间的关系图。（数据按时间顺序排列。）
- en: '[PRE31]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The result is in [Figure 1-1](ch01.xhtml#ch01fig01). Clearly, there is both
    a seasonal trend (the dips are about a year apart) and an overall upward trend.
    The bike sharing service seems to have gotten much more popular over time. Statistical
    techniques that analyze data over the course of time are known as *time series
    methods*. They arise a lot in ML in various contexts. We’ll investigate this in
    [Chapter 13](ch13.xhtml), but let’s give it a try here without new tools.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 结果见[图 1-1](ch01.xhtml#ch01fig01)。显然，数据中存在季节性趋势（下跌大约每年一次）和总体的上升趋势。共享单车服务似乎随着时间的推移变得越来越受欢迎。分析随时间变化的数据的统计技术称为*时间序列方法*。在机器学习的不同背景下，这种方法经常被使用。我们将在[第13章](ch13.xhtml)中详细探讨这个问题，但现在就先尝试不使用新工具来进行分析。
- en: '![Image](../images/ch01fig01.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/ch01fig01.jpg)'
- en: '*Figure 1-1: Time trends, bike data*'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1-1：时间趋势，骑行数据*'
- en: Column 1 in the bike sharing data, `instant`, is the day number, with the first
    day of the dataset having value 1, the second day with value 2, and so on, through
    the last day, 731.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 共享单车数据中的第1列，`instant`，是日期编号，数据集的第一天值为1，第二天为2，依此类推，直到最后一天731。
- en: 'Let’s add `instant` to our feature set. Recall our earlier selection of columns
    in [Section 1.5.1](ch01.xhtml#ch01lev5sec1):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把`instant`加入到特征集中。回想一下我们之前在[1.5.1节](ch01.xhtml#ch01lev5sec1)中选择的列：
- en: '[PRE32]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The features we decided to explore at that time were in columns 8, 10 through
    13, and 16\. Now we want to also use `instant`, which is in column 1:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当时决定探索的特征位于第8列、第10到13列以及第16列。现在我们还想使用`instant`，它位于第1列：
- en: '[PRE33]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Ah, much better! Our MAPE is down to about 663.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 啊，好多了！我们的MAPE降到了大约663。
- en: When using k-NN and the other methods covered in this book, keep in mind that
    conditions in the phenomena under study may vary through time, possibly becoming
    a major factor. In some cases, the time variable may not even be explicit but
    implied in the ordering of the records. Failure to explore this may result in
    a substantial deterioration in the quality of prediction, so keep an eye out for
    places where you may run into this pitfall.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '1.15 Pitfall: Dirty Data'
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beyond accounting for long-term time trends, you may also encounter problems
    caused by dirty data. For an example of this, look at the entry in the bike sharing
    data for January 1, 2011.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'As you can see in the `holiday` column, the dataset claims this is not a holiday.
    But of course January 1 is a federal holiday in the United States. Also, although
    the documentation for the dataset states there are 4 values for the categorical
    variable `weathersit`, there actually are just values 1, 2, and 3:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Errors in data are quite common and, of course, an obstacle to good analysis.
    For instance, consider the New York City taxi data that will be discussed in depth
    in [Chapter 5](ch05.xhtml), which contains pickup and dropoff locations, trip
    times, and so on. One of the dropoff locations, if one believes the numbers, is
    in Antarctica! (You can take a look at [*https://data.cityofnewyork.us/Transportation/2018-Yellow-Taxi-Trip-Data/t29m-gskq*](https://data.cityofnewyork.us/Transportation/2018-Yellow-Taxi-Trip-Data/t29m-gskq).)
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Whenever working with a new dataset, the analyst should do quite a bit of exploring—for
    example, with `hist()` and `table()`, as we’ve seen here. You should also be wary
    of *multivariate outliers*, meaning data points that are not extreme in any one
    of their components but, when viewed collectively, are unusual. For instance,
    suppose a person is recorded as having height 74 inches (29.1 cm) and age 6\.
    Neither that height nor that age would be cause for concern individually (assume
    we have people of all ages in our data), but in combination it seems quite suspicious.
    In this case, k-NN is a useful tool! A data point like the 74-inch six-year-old
    described above would be unusually distant from the other points and might be
    exposed that way.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Alas, there is no formulaic way to detect anomalous data points. This mirrors
    the nature of ML in general, as we’ve emphasized. There is no magic “recipe.”
    This is an advanced topic in statistics and beyond the scope of this book.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '1.16 Pitfall: Missing Data'
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In R, the value NA means the data is not available—that is, missing. Datasets
    commonly include NAs, maybe many. How should we deal with this?
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: There are entire books devoted to the study of missing value analysis. We cannot
    cover the topic in any depth here, but we will briefly discuss one common method,
    *listwise deletion*, to at least introduce the issues at stake.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'Say our data consists of people, and we have variables Age, Gender, Years of
    Education, and so on. If, for a particular person, Age is missing but the other
    variables are intact, this method would simply skip over that case. But there
    are two problems with this:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: If we have a large number of features, odds are that many cases will have at
    least one NA. This would mean throwing out a lot of precious data.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skipping over cases with NAs may induce a bias. In survey data, for instance,
    the people who decline to respond to a certain question may be different from
    those who do respond to it, and this may affect the accuracy of our predictions.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when we find missing values are coded numerically rather than as NAs,
    we should change such values to NAs.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: As with dirty data, developing one’s personal approach to handling missing values
    comes with experience. Learn some tools and gradually develop your approach, which
    will largely be different for each person. The CRAN Task View series includes
    one on missing data.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 1.17 Direct Access to the regtools k-NN Code
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned in the introduction, most of the code in this book uses popular
    R packages that are specific to each different ML method. We use the `qe*`-series
    of wrappers to interface those packages as a uniform, simple convenience, but
    of course one can also choose direct access.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: One reason for doing so is that the functions in those standard R packages contain
    many advanced options not available via the wrappers. In the particular case of
    `qeKNN()`, but *not* for the other `qe*`-series functions, direct access may also
    be faster if one is doing just a single prediction.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: In most applications, such specialized usage will not be necessary, but our
    book will show how to use the functions directly if the reader is interested.
    Note that we will not be able to demonstrate how to use those advanced options,
    as they are numerous and frequently involve advanced concepts. Instead, we will
    simply show how to use one of our existing examples with a direct call.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the particulars for our k-NN code. The `qeKNN()` function wraps `regtools::kNN()`,
    which has arguments as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Here `x` and `y` are our *X* and *Y*, `newx` is the *X* at which we wish to
    predict, and `kmax` is *k*.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s retrace our steps from [Section 1.8](#ch01lev8):'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Note that the model fitting and prediction are combined into one step, though
    the latter can be postponed if desired.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'The prediction here is slightly different from the earlier one, as the latter
    had a holdout set. If we suppress formation of a holdout set, we obtain the same
    result:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: More information on `kNN()` is available by typing ?kNN (not `?knn`).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 1.18 Conclusions
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are off to a good start! You should now have a good idea of both the k-NN
    method and several general ML concepts: the regression function, how k-NN estimates
    it, the general concept of the Bias-Variance Trade-off, how hyperparameters affect
    it, and how to use holdout sets to find a good point on that trade-off spectrum.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了一个好的开始！你现在应该对 k-NN 方法以及几个常见的机器学习概念有了清晰的了解：回归函数、k-NN 如何估计它、偏差-方差权衡的一般概念、超参数如何影响它，以及如何使用保留集来找到该权衡范围上的一个合适点。
- en: You are already armed with enough tools to experiment with some data analysis
    of your own. Please do!
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经掌握了足够的工具，可以尝试自己进行一些数据分析了。请务必尝试！
- en: The next chapter will introduce the issues in classification applications.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将介绍分类应用中的问题。
