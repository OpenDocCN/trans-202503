- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Network Analysis
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: In [Chapter 2](c02.xhtml), we considered a few network geometry metrics; in
    this chapter, we’ll use them. First, we’ll explain how vertex metrics allow you
    to do supervised learning within a network, that is, predicting values associated
    to vertices and predicting new edges in the network; we’ll also look at how vertex
    metrics enable you to cluster vertices in a network. We’ll then discuss a few
    clustering algorithms that operate directly within the geometry of the network.
    Next, we’ll explain how to use global network metrics to do machine learning and
    statistical analyses on datasets that consist of collections of networks. We’ll
    then explore a network variant of the susceptible, infected, and recovered (SIR)
    model from epidemiology. With this model, we can see how entities (from diseases
    to misinformation) spread through networks and how network geometry influences
    this spread. Finally, we’ll examine how we can use vertex metrics to devise targeted
    strategies for disrupting this spread.
  prefs: []
  type: TYPE_NORMAL
- en: Using Network Data for Supervised Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network data is often accompanied by a traditional structured dataset where
    the rows are identified with the vertices in the network. For instance, you might
    have a social network dataset that consists of a list of individuals (the vertices),
    the friendships between them (edges in the network), and one or more numerical
    or categorical columns providing additional non-network information about each
    individual—such as age, gender, or salary. We might want to consider this as a
    supervised learning problem, where we train a machine learning algorithm to predict
    one of the data columns. The way to do this is to use vertex metrics as independent
    variables (along with any of the other data columns), which lets the algorithm
    incorporate the network role of each vertex when making predictions. Let’s try
    this.
  prefs: []
  type: TYPE_NORMAL
- en: Making Predictions with Social Media Network Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s return to Farrelly’s social network, which we analyzed in the previous
    chapter to illustrate the different measures of vertex centrality. How might bridges
    between different parts of the network or hubs tightly connecting a few medical
    school friends influence how often Farrelly mentioned those individuals in her
    diary during the first term of medical school? Let’s take a look at how network
    metrics relate to diary mentions in this social network.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll choose a few vertex centrality metrics and attach Farrelly’s diary data
    as a dependent variable in the set. Let’s load the data and examine the distribution
    of our dependent variable with [Listing 3-1](#listing3-1).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-1: A script that imports the relevant *.csv* file for further analysis'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-1](#figure3-1) shows a histogram of the dependent variable’s distribution
    (Poisson).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c03/f03001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-1: A histogram of diary entry data for Farrelly’s social network'
  prefs: []
  type: TYPE_NORMAL
- en: Generally, count variables, such as those conforming to the Poisson distribution,
    include a lot of zero and near-zero values, along with some large values. We can
    see from [Figure 3-1](#figure3-1) that this outcome is Poisson distributed. Poisson-distributed
    variables can pose issues to machine learning algorithms, as they involve a lot
    of zero values and some outliers, such as the diary entries involving V10\. This
    suggests that a generalized linear model (Poisson regression) is probably more
    appropriate of a supervised learning model than other machine learning algorithms.
    It looks like most individuals in the network receive few (if any) mentions over
    the term. However, a few outliers exist, including Farrelly and her closest friends
    within the network (V3, V10, and V14). Let’s dive deeper to see how centrality
    measures predict diary mentions. In [Listing 3-2](#listing3-2), we sample Farrelly’s
    social network metrics of interest and use them as independent variables in our
    Poisson regression model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-2: A script that computes a Poisson regression and analyzes its results'
  prefs: []
  type: TYPE_NORMAL
- en: Your results may vary depending on your R version’s seeding, but in the samples
    we modeled with this dataset and other seeds (9 of 10 random splits), the summary
    functions show that betweenness centrality seems to have large coefficient values
    in the model and be the most consistent predictor of diary entry mentions across
    subsets of Farrelly’s social network data modeled with our regression function.
    The Chi-squared test values in our samples ranged from *p* < 0.01 to *p* = 0.25\.
    When we examine the plots associated with the linear regression, we can see that
    most of our sample fits the regression equation well. [Figure 3-2](#figure3-2)
    shows two of the plots generated by [Listing 3-2](#listing3-2) (including V3,
    V7, and V13 as outliers).
  prefs: []
  type: TYPE_NORMAL
- en: The small sample size likely contributes to the variation between samples, but
    overall, we have a good predictive model. Indeed, this reflects Farrelly’s own
    intuition that the bridges of her network tended to coordinate memorable events
    and activities that brought together various pieces of her network that term.
    We’ll return to the instability of regression models on small sample sizes with
    outliers in [Chapter 6](c06.xhtml) and [Chapter 8](c08.xhtml), along with more
    stable models you can use for these situations to get consistent model results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c03/f03002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-2: Residual and quantile plots of the Poisson regression run in [Listing
    3-2](#listing3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Bigger networks with dependent variables more closely tied to one’s social network
    (such as strength of political views, workout habits, and so on) tend to work
    better in this sort of analysis. Within biological networks, centrality measures
    might be used to predict disease severity, likelihood of response to a drug undergoing
    clinical trials, or disease risk at six months after a social-network-based behavioral
    health intervention. Analyzing the geometry of the network often produces useful
    independent variables for predicting some quality associated with the vertices
    of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting Network Links in Social Media
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another important form of supervised learning in a network is *link prediction*,
    in which potential new edges are inferred from a network’s structure or metadata.
    One way to predict links is to use prior growth patterns of a network to predict
    which edges are most likely to appear next. This has many real-world applications,
    some of the most notable being in social media. Whenever Facebook or another platform
    suggests a person for you to friend, an algorithm has run a link prediction on
    its network of users behind the scenes and given the missing edge between you
    and this potential friend a high score. There are many sophisticated methods for
    performing link prediction, but a common general strategy is to translate the
    problem to a traditional Euclidean supervised learning task. Let’s explore this
    conceptually using Farrelly’s social network as an example.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine this network evolving over time, with a binary indicator denoting edges
    that formed since the last time period. Wouldn’t it be neat if we could predict
    edge formation over a time period based on what the network looked like geometrically
    in the past time period? Or if we could use vertex labels (such as class schedule
    or volunteer days) to predict edge formation in the next time period?
  prefs: []
  type: TYPE_NORMAL
- en: For the independent variables, we can use any collection of features associated
    with the two vertices. These can be intrinsic network-based features or extrinsic
    features such as user demographics in a social network. The network-based predictors
    come in two flavors. We can use vertex metrics by choosing a function to aggregate
    the two vertex scores in each pair to a single number (common choices for this
    include sum, max, mean, and absolute value of difference). For instance, we could
    compute the PageRank score for the two vertices in each vertex pair and then take
    the average of these two scores to assign the vertex pair.
  prefs: []
  type: TYPE_NORMAL
- en: The other flavor of network-based features uses some measure of the network
    relationship between the two vertices; the most natural choice here is simply
    the network distance between the two vertices, though you can try other options
    such as the number of shortest paths between the two vertices or the average time
    a random walk takes to get from one to the other. All these network-based predictors
    should be computed for the current version of the network rather than the earlier
    snapshot, and these network-based predictors can be combined with any collection
    of non-network features. (In practice, most non-network features are attached
    to individual vertices rather than pairs, so once again you’ll have to aggregate
    them to get a single score for each vertex pair.)
  prefs: []
  type: TYPE_NORMAL
- en: Once the independent predictors are computed for a time period of interest and
    an indicator variable exists for that time period, a supervised classifier can
    be used to predict edge formation over the time periods of interest. The higher
    the likelihood score for edge formation, the more likely that relationship will
    exist by the next time period. In Farrelly’s social network, betweenness centrality
    would likely be the main network-based predictor, as well as social activity data
    or diary entries from the first term of medical school. Everyone in her original
    social network was connected to her (and most to each other) by the end of that
    term.
  prefs: []
  type: TYPE_NORMAL
- en: Using Network Data for Unsupervised Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just as we can use the vertex metrics from [Chapter 2](c02.xhtml) as predictors
    in a supervised learning task, we can also use them as features in unsupervised
    learning tasks. In the case of clustering, this will partition the vertices into
    sets of those with similar functions in the network (hubs, bridges, and so on).
    Clustering vertices is known in the network sciences as *community mining*, so
    when using vertex metrics for this purpose, we obtain communities defined by the
    structural role they play in the network.
  prefs: []
  type: TYPE_NORMAL
- en: Applying Clustering to the Social Media Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [Listing 3-3](#listing3-3), we apply k-means clustering to Farrelly’s social
    network dataset that was our main running example in [Chapter 2](c02.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-3: A script that uses k-means on the vertex metrics to cluster the
    vertices from the network in [Figure 3-1](#figure3-1) into *k* = 3 groups'
  prefs: []
  type: TYPE_NORMAL
- en: Using k-means with *k* = 3 and PageRank, degree, hub centrality, betweenness,
    and transitivity as our features (which we first rescale), we get clusters with
    the means and sizes in [Table 3-1](#table3-1), where they were computed from the
    original prescaled values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3-1: Cluster Means and Sizes for k-Means Clustering (with *k* = 3) Run
    on a Handful of Vertex Metrics for Farrelly’s Social Network'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Cluster** | **PageRank** | **Degree** | **Hub score** | **Betweenness**
    | **Transitivity** | **Cluster size** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.05 | 3.14 | 0.54 | 3.21 | 0.80 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.11 | 8.00 | 1.00 | 100.50 | 0.25 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.05 | 2.50 | 0.08 | 25.75 | 0.097 | 12 |'
  prefs: []
  type: TYPE_TB
- en: Since k-means involves a random initiation, you might get different clustering
    results each time you try this. For this particular clustering, we see that one
    vertex has been assigned its own cluster (Farrelly’s vertex, V7); all of this
    vertex’s scores other than transitivity are exceptionally high, so it is an outlier
    in many metrics. All the remaining vertices are split between the other two clusters,
    which seem mostly distinguished by the fact that one cluster has higher hub and
    transitivity scores, while the other cluster has higher betweenness. Let’s plot
    this network with the vertices labeled by cluster (doing so is an easy adaptation
    of the code in [Listing 3-3](#listing3-3)); see [Figure 3-3](#figure3-3).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c03/f03003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-3: Farrelly’s social network colored and labeled by cluster for the
    k-means clusters summarized in [Table 3-1](#table3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We see that Farrelly is indeed her own cluster, and rather remarkably, cluster
    1 is almost precisely the remaining medical school individuals, while cluster
    2 is the veterans group individuals. Interestingly, one medical school person
    has been placed in cluster 3, because they are isolated (have a transitivity score
    of zero) and are not part of the main hub of medical school individuals.
  prefs: []
  type: TYPE_NORMAL
- en: Community Mining in a Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we’ve used the geometry of the network to extract features and then
    run traditional Euclidean machine learning clustering algorithms on them. Another
    approach to clustering vertices in a network (called *community mining*) is to
    rely directly on the geometry of the network. We’ll briefly walk through several
    ways to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Networks with Random Walks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *walktrap algorithm* uses random walks to explore the network and find communities
    in which the random walk gets “trapped.” If a random walk frequently stays within
    a certain set of vertices, then that set is a good candidate for a cluster. For
    instance, in Farrelly’s social network, a random walk starting among the medical
    school individuals has a high likelihood of staying with them for many steps because
    the only way out is across the bridge from Farrelly’s vertex. Indeed, to get out,
    the random walk would have to be at Farrelly’s vertex, and we would then choose
    that bridge as the next step (which happens with only one out of eight probability
    from that vertex because that vertex has eight edges attached to it). Similarly,
    a random walk starting among the veterans group individuals has a high likelihood
    of staying among them. In this way, the walktrap algorithm is good at finding
    communities that are separated by bridges. One downside with this approach is
    that it is computationally intensive to explore a large network this way.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a Cluster’s Quality Outcome
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In traditional Euclidean clustering, a typical way to evaluate the quality
    of a clustering outcome is to compare the distances within each cluster to the
    distances between the different clusters. There is a widely used network variant
    of this idea that applies to vertex clustering: the *modularity* of a vertex clustering
    is the probability that a randomly chosen edge is attached to two vertices in
    the same cluster minus the probability that this would occur if the edges in the
    network were randomly distributed. Intuitively, this compares the number of intracommunity
    edges to the number of intercommunity edges. It is sort of like a “lift” measure,
    because it compares how much better (in the sense of edges staying within clusters)
    the clustering is than a randomized benchmark. One of the important small-world
    properties of the Watts–Strogatz networks introduced in the preceding chapter
    is that they tend to be highly modular (clusters with high modularity scores).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Modularity also enables you to view vertex clustering as an optimization problem:
    we find the cluster division that maximizes the modularity score. It’s not practical
    to try all possible divisions into clusters, so various algorithms have been introduced
    that attempt to search for high-modularity clusterings without being guaranteed
    to find the global optimum. Two of the popular approaches for this are *greedy
    algorithms*, where greedy means that at each step they go in the direction that
    most increases the modularity, rather than taking suboptimal steps in the short
    run in the hopes that they lead to greater values in the long run.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Louvain clustering* is one such greedy algorithm. It starts by treating each
    vertex as its own cluster and then iteratively merges neighboring clusters whenever
    doing so increases the modularity. Once this iterative local optimization process
    terminates, the algorithm creates a new, smaller network by merging all the vertices
    that have been assigned to the same cluster. This yields a network in which each
    vertex is its own cluster, so the same iterative local optimization process can
    be run again on this smaller graph. This algorithm is quite fast in practice and
    tends to perform well, but it often struggles to find smaller communities within
    large networks. There is a faster greedy algorithm that is often used, conveniently
    called *fast greedy clustering*, but it tends not to reach as high modularity
    scores.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Spinglass Clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another approach to clustering doesn’t involve random walks or local optimization.
    Rather, it draws from statistical mechanics, a branch of physics dealing with
    particle interactions. *Spinglass algorithms* are based on magnetic couplings
    within a system of particles (positive and negative charges); they seek to optimize
    how the charges are aligned across the system. This can be applied to vertex clustering,
    called *spinglass clustering*. The basic idea is to define an energy associated
    to clusterings and then try to minimize this energy. This energy minimization
    process is usually done by *simulated annealing*, which is an algorithmic approach
    to optimization that also has roots in statistical mechanics. In simulated annealing,
    rather than always moving in the direction that decreases the energy the most
    (as would be done in a greedy algorithm that runs the risk of getting stuck in
    local optima), there is a temperature parameter that determines the probability
    of moving instead in a “wrong” direction. As the algorithm proceeds, the temperature
    is steadily lowered. This helps the algorithm explore large portions of the energy
    landscape early on before settling down and honing in on a particular solution.
    It mimics the cooling process in metallurgy in which metal purifies.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Clustering Algorithms on a Social Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s try running these four vertex clustering algorithms on Farrelly’s social
    network. [Listing 3-4](#listing3-4) does this and then plots the results and computes
    the modularity scores (to run this, make sure you’ve already loaded this network
    data as in [Listing 2-4](c02.xhtml#listing2-4)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-4: A script that runs the four vertex clustering algorithms discussed
    earlier on Farrelly’s social network, plots the results, and computes the modularity
    score for each'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-4](#figure3-4) shows the resulting plots.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c03/f03004_m.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-4: Clustering on Farrelly’s social network provided by four different
    algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: Note that for all these functions the number of clusters was not specified by
    the user as it is for k-means. These algorithms determine the number of clusters
    as part of their search for optimality. In this example, walktrap and spinglass
    found the same three clusters, which are the medical school individuals (including
    Farrelly’s vertex V7) and a division of the veterans group individuals into two
    parts. This three-way clustering yields the highest modularity score among the
    solutions found by these algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Louvain found the next best score, and the result is similar to the previous
    one except that it splits the veterans group in a slightly different way (reassigning
    two vertices from one cluster to the other). With a modularity score just slightly
    below this, the fast greedy algorithm ended up with only two clusters (the medical
    school community with Farrelly in it and the veterans group community). Evidently,
    the greediness in this algorithm prevented it from finding that a higher modularity
    could be achieved by splitting the veterans group community. That said, this two-cluster
    solution found by the fast greedy algorithm describes the original context of
    the data, in which Farrelly combined her two separate communities.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested, you can explore a few other vertex clustering algorithms
    implemented in igraph. For instance, the function `cluster_edge_betweenness()`
    uses the betweenness metric not as a feature but in a more direct way. Vertices
    with a high betweenness score are considered to be *bridges*, and the communities
    this function uncovers are the ones that are separated by these bridges. Another
    interesting approach is provided by the function `cluster_infomap()`, which uses
    information theory to find communities in which information flows readily; this
    can also be interpreted in terms of the behavior of random walks on the network.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve discussed supervised and unsupervised learning among the vertices
    within a single network. Let’s now consider situations in which we are comparing
    the networks themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes a network isn’t your entire dataset. It’s just a single instance in
    a dataset comprising many networks. For instance, detecting bot accounts on social
    media platforms usually involves supervised classification in which the friend
    or follower networks of real users are compared to those of fake users. In the
    Pennsylvania gerrymandering case mentioned in [Chapter 2](c02.xhtml), districting
    maps were converted to networks, and the old map was shown to be a dubious outlier
    in the distribution of networks. Neuroscience provides another important example
    where one needs to compare networks. Indeed, it’s common to translate functional
    magnetic resonance imaging (fMRI) and positron emission tomography (PET) data
    into a network structure in which the vertices represent different regions of
    the brain and in which edges are based on activity patterns (sequential activation
    of an area, for instance, or coactivation of multiple regions during one task).
    One often needs to compare two different groups of patients—either healthy patients
    against a group of patients with a particular neurological or psychological disorder
    or two different disease groups. Translated to network data science, this means
    we’re looking at a two-class dataset of networks to see if there are statistically
    significant differences between the two classes (to understand structural differences).
    We might also want to train a supervised classifier to predict the class based
    on the network structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate some synthetic data, let’s create 100 networks of each of the types
    described at the end of [Chapter 2](c02.xhtml): Erdös–Renyi, scale-free, and Watts–Strogatz.
    In [Listing 3-5](#listing3-5), we do this and plot a histogram of the network
    diameter to see how it varies within and across the different types of networks.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-5: A script that generates 300 networks, evenly split among three
    different types; computes their network diameter; and then plots the histograms
    for each: Erdös–Renyi, scale-free, and Watts–Strogatz'
  prefs: []
  type: TYPE_NORMAL
- en: The parameters are chosen here so that all random networks have the same number
    of vertices (chosen arbitrarily to be 100) and approximately the same edge density
    (around 2 percent); this ensures that the network structure does differentiate
    the three groups, rather than something simpler like the number of vertices or
    edges. [Figure 3-5](#figure3-5) shows the resulting histogram plot.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c03/f03005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-5: Histograms of network diameter for three different types of random
    networks: Erdös–Renyi (light gray), scale-free (medium gray), and Watts–Strogatz
    (dark gray)'
  prefs: []
  type: TYPE_NORMAL
- en: We see that the three histograms are disjoint. Erdös–Renyi networks have moderate
    diameters. Scale-free networks have small diameter values. Watts–Strogatz networks
    have large diameters. If interested, you might try modifying [Listing 3-5](#listing3-5)
    to compute some of the other global network metrics discussed in [Chapter 2](c02.xhtml)
    (such as efficiency, transitivity, and spectral radius) to see how they behave
    for the different types of random graph structures.
  prefs: []
  type: TYPE_NORMAL
- en: There are many machine learning tasks that you can do now on datasets of networks
    using the tools developed so far. For classification (such as labeling social
    media accounts as bot versus real based on their friend network) or regression
    (such as predicting the journal ranking of academic publications based on their
    citation networks), you can compute a collection of global network metrics to
    then use as features for a traditional supervised learning algorithm. Similarly,
    to cluster a collection of networks into different types (for example, grouping
    individuals according to their fMRI brain network structure), you can compute
    global network metrics and feed them into a traditional clustering algorithm.
    You can also do statistics, such as outlier detection and confidence interval
    estimation. Indeed, by representing each network with its vector of global network
    metric values, you “structure” your network data and open the door to all the
    statistical and machine learning methods that we traditionally rely upon in data
    science.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing Spread Through Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another important topic in network analysis is the spread (or *propagation*)
    of various entities through a network. There are many real-world instances of
    this, including infectious diseases in contact networks and viral content in social
    media networks. Understanding the geometry of a network can help predict the way
    that entities spread on the network, and we can leverage this insight to change
    the network geometry so that we impact spread.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking Disease Spread Between Towns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s return to the weighted network of four towns from the previous chapter’s
    [Figure 2-3](c02.xhtml#figure2-3). We’ll take the adjacency matrix for it from
    [Listing 2-3](c02.xhtml#listing2-3) and use it to create a weighted network whose
    edge weights are the inverses of the original distance; this turns distances into
    proximity scores, where shorter roads have larger edge weights than longer roads.
    [Listing 3-6](#listing3-6) (which relies on first running the script in [Listing
    2-3](c02.xhtml#listing2-3)) does this and plots the result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-6: A script that creates and plots the four towns network from [Chapter
    2](c02.xhtml) but with the edge weights given by the inverses of the road lengths'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-6](#figure3-6) shows the resulting plot.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c03/f03006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-6: Four towns and the proximity scores (inverse distance) of the roads
    between them'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a simple epidemiological model for the spread of a transmissible
    disease across this network in which the probability of the disease spreading
    from an infected town to each of its neighboring towns is given by the edge proximity
    scores. If the disease starts in town V1, then it has a 25 percent chance of spreading
    to V4\. If it does this, it then has a 16.7 percent chance of further spreading
    from V4 to V3\. But multiplying these two probabilities (which yields about 4.2
    percent) does not give the probability that the disease spreads from V1 to V3;
    it gives only the probability that it does so along the transmission route V1→V4→V3\.
    Another potential transmission route is V1→V4→V2→V3, which has a 1 percent chance
    of occurring.
  prefs: []
  type: TYPE_NORMAL
- en: For a larger network, computing all the conditional probabilities based on potential
    transmission routes given by paths in the network will clearly be too cumbersome
    to do by hand, so we need these calculations to be automated. Moreover, this epidemiological
    model is too simple to be of much practical use; we discussed it here just to
    give a sense of how the structure of a weighted network might influence the spread
    of various entities (disease, information, and so on) across its vertices, as
    well as to motivate the more sophisticated epidemiological model that we’ll be
    turning to next.
  prefs: []
  type: TYPE_NORMAL
- en: 'An *SIR model*, or *susceptible-infected-resistant* model (alternatively, a
    *susceptible-infected-recovered* model), is a model that projects the spread of
    a disease among a population by assuming each individual can be in one of three
    disease states: susceptible (can become infected), infected (has the disease and
    can transmit it), or recovered or resistant (immune to the disease). Many variations
    of this model exist, including models that are susceptible-infected-susceptible,
    models that are susceptible-infected-recovered-susceptible, models including partial
    immunity from vaccines, models where individuals are born or die during the epidemic,
    and partitioned or geographic models where populations mix at different rates.
    Underlying all these models are systems of partial differential equations with
    parameters related to population mixing (such as contact rates or times) and disease
    characteristics (such as the number of new infections expected for a single infectious
    individual). Often these differential equations are too difficult to solve explicitly,
    so instead we run computer simulations to quantify the range and likelihood of
    different possible outcomes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To take into account the social interactions between individuals within a population,
    SIR models have been adapted to networks. This provides more detailed predictions
    of how a disease might spread, and it also helps people find ways of mitigating
    this spread: we can run the model to see what impact deleting a vertex or edge,
    or restructuring the network in other ways, will have on the spread of the disease.'
  prefs: []
  type: TYPE_NORMAL
- en: Many of the network geometry concepts from [Chapter 2](c02.xhtml) play a role
    here. Hubs are high transmission zones that might need to be shut down or reduced
    in size, bridges and vertices with high betweenness scores suggest targeted ways
    of cutting off the main transmission routes of the disease, and vertices with
    high centrality scores might indicate the individuals most important to vaccinate
    or quarantine as quickly as possible. Moreover, SIR models on networks and the
    computer simulation techniques used to explore them have applications far beyond
    epidemiology because they give a powerful empirical method for studying the complex
    relationship between network structure and network propagation more generally.
    For instance, the spread of misinformation on social media is a problem that has
    attracted a lot of attention recently and driven a need to better understand how
    network structure influences social media virality—and SIR-type models have proven
    to be valuable tools in this realm.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking Disease Spread Between Windsurfers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s jump right in with an example. [Listing 3-7](#listing3-7) loads a popular
    network dataset, the KONECT Windsurfer Network. This is a weighted network representing
    43 Southern California windsurfers and their level of interactions during the
    fall of 1986\. Almost all the nondiagonal entries of the adjacency matrix are
    nonzero—meaning almost every possible edge exists in this network—so it’s really
    the weights that matter. This makes it difficult to visualize the network, so
    let’s create two less dense versions of the network—one with all the edges whose
    weights are not in the top quartile removed and one with those below the median
    removed. (This is a simple form of filtering weighted networks, a concept we’ll
    return to in more depth in [Chapter 4](c04.xhtml).) [Listing 3-7](#listing3-7)
    does this and plots the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-7: A script that loads the KONECT Windsurfer Network dataset and
    creates two less dense versions of it, by removing edges whose weights are not
    in the top one or two quartiles, and then plots the result'
  prefs: []
  type: TYPE_NORMAL
- en: Using the `edge_density()` function, we find that the original network has a
    density of 99.3 percent, the top-quartile network has a density of 25.8 percent,
    and the above-median network has a density of 51.4 percent. The plots in [Figure
    3-7](#figure3-7) show these two thinned-out versions of the windsurfer network.
    The edge thicknesses represent the edge weights, but to increase the visual distinction
    among them, we set the thickness to the square of the edge weight.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c03/f03007_m.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-7: Two thinned-out versions of the windsurfer network, in which all
    edges whose weights are not in the top quartile (left) or top two quartiles (right)
    have been removed'
  prefs: []
  type: TYPE_NORMAL
- en: Running an SIR simulation in R is easy. Using igraph’s `sir()` function, you
    can just specify the network, the infection rate (called *beta*), and the recovery
    rate (called *gamma*), and then (optionally) specify the number of simulation
    trials to conduct (the default value is 100). The infection rate determines the
    probability at each time step that a susceptible vertex becomes infected by an
    infected neighbor (higher rates for more contagious diseases); having two infected
    neighbors doubles the odds of getting infected. The recovery rate determines the
    probability distribution for the duration of infections; higher recovery rates
    mean higher probability at each time step that infected vertices move on to the
    recovered state. Higher recovery rates indicate shorter-duration infections.
  prefs: []
  type: TYPE_NORMAL
- en: 'When plotting the result of `sir()`, you’ll see the number of actively infected
    individuals in the network together with the median value across the trials and
    estimated confidence intervals as a function of time. Applying the function `median()`
    to the output of `sir()` provides three time series: the median number of susceptible
    individuals, the median number of infected individuals, and the median number
    of recovered individuals. Let’s try this. In [Listing 3-8](#listing3-8) we simulate
    a disease on the full dataset with an infection rate of 3 and a recovery rate
    of 2.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-8: A script that runs 100 simulation trials of an SIR model on the
    KONECT Windsurfer Network dataset with an infection rate of `beta=3` and a recovery
    rate of `gamma=2` and then plots the results and displays the median number of
    infected individuals across time'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-8](#figure3-8) shows the resulting plot.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c03/f03008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-8: Plot of SIR simulations (100 trials) on the original KONECT Windsurfer
    Network dataset, showing the number of infected individuals over time (with mean
    and confidence intervals) for a disease with infection rate 3 and recovery rate
    2'
  prefs: []
  type: TYPE_NORMAL
- en: At its peak, the median is 28 actively infected individuals—which is 65 percent
    of the entire network. This is a highly infectious disease spreading through a
    densely connected network. Running the same code as in [Listing 3-4](#listing3-4)
    but lowering the infection rate to `beta=1` and raising the recovery rate to `gamma=10`
    yields the plot in [Figure 3-9](#figure3-9).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c03/f03009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-9: SIR simulations on the original KONECT Windsurfer Network dataset
    (with confidence levels), now with infection rate 1 and recovery rate 10'
  prefs: []
  type: TYPE_NORMAL
- en: As expected, this new epidemic simulation shows fewer infected individuals over
    a smaller period of time. Now the median number of active infections peaks at
    17, and the time period of this epidemic is only one-quarter what it was for the
    previous parameters. When using SIR models to study real-world epidemics, epidemiologists
    look up the infection and recovery rate parameters in the scientific literature
    if they are already known, and if they’re not already known, they can be estimated
    from data on how the disease has spread so far. Usually, such estimates will involve
    some degree of uncertainty, so we can run SIR simulations on a range of parameters
    to see the range of possible outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Disrupting Communication and Disease Spread
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the interesting proposed uses of vertex Forman–Ricci curvature is to
    rank vertices for removal to disrupt communication and disease spread on a network.
    In a communications network, disrupting communication may involve targeting a
    specific cell tower or isolating an individual import to the network. In 2020,
    we saw how isolating COVID-infected or COVID-exposed individuals by social distancing
    and quarantines helped stop the spread of COVID in large cities. Recall that vertex
    7 in the author’s network had a large Forman–Ricci curvature. Let’s run an SIR
    model on the author’s network with and without vertex 7 included to compare the
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This script runs epidemics on the original network and the modified network,
    with vertex 7 removed, to compare the severity of the simulated epidemic. This
    should yield an initial plot similar to [Figure 3-10](#figure3-10), with the epidemic
    propagating through the whole network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c03/f03010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-10: An SIR epidemic on the author’s full network'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-10](#figure3-10) shows an SIR epidemic resulting in five time periods
    of infection spread, with a median number of infected at 5\. Some simulations
    suggest the possibility of up to 12 infections at once within the first 2 time
    periods. This is a pretty severe epidemic, forecast to impact more than 25 percent
    of the population at a time before the infection takes out the whole susceptible
    population.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine what happens when vertex 7 is removed, shown in [Figure 3-11](#figure3-11).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/503083c03/f03011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-11: An SIR epidemic on the author’s edited network (with vertex 7
    removed)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-11](#figure3-11) shows a less severe epidemic over a shorter time
    frame. There are only four periods in which infection occurs, and the median number
    infected at the height of the epidemic is only 2, with a maximum estimate of 8\.
    While the epidemic still impacts the population, it is confined to fewer individuals
    and quickly over. By the end of the second time period, most models suggest the
    epidemic has ended.'
  prefs: []
  type: TYPE_NORMAL
- en: Many applications that let us target vertices to disrupt a network exist. Not
    only can we mitigate potential epidemics by removing pieces of a network, but
    we can also disrupt enemy communication within a terrorist cell or hostile government
    by taking out targets with highly negative Forman–Ricci curvature or disrupt disease
    processes by taking out proteins or genes within the backbone of the biological
    network. Changes in Forman–Ricci curvature as a network evolves also contribute
    to network-related analytics capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '*Forman–Ricci flow* is a geometric flow (differential equation) related to
    changes in curvature over time on a network, analogous to heat dissipating across
    a network from a defined starting point. Tracking changes in curvature can identify
    areas of change within a network. Forman–Ricci flow provides a way to quantify
    regions of growth or shrinkage of connections within networks, such as the rapid
    expansion of terrorist cell membership, accumulation of mutations in a cancer
    gene network, or increased disease spread risk in an epidemic. For instance, the
    increased large party or event activity in some regions during COVID quarantines
    resulted in an increased spread of COVID within those parts of an area’s social
    network. Forman–Ricci flow on image datasets also provides a way to map medical
    image data from a raw source file onto a standard surface, such as a plane or
    a sphere or even a frying pan, such that results can be pooled and compared within
    and across patient groups.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we first saw how the vertex metrics covered in [Chapter 2](c02.xhtml)
    can serve as predictors for supervised learning within a network (including link
    prediction) and as features for vertex clustering (that is, community mining).
    This approach to machine learning translates a network back to structured datasets
    and applies Euclidean machine learning algorithms. We then looked into a handful
    of community mining algorithms that operate directly within the network. Next,
    we moved from analyzing data within a network to analyzing datasets where each
    data point is itself a network. Similar to our use of vertex metrics in the previous
    setting, here we used global network metrics as predictors or features to do machine
    learning and statistical analyses on this kind of network data. Finally, in the
    last section of this chapter, we explored the SIR disease spread model from epidemiology
    as it applies to networks. The emphasis here is on the intersection of network
    geometry and network spread; in particular, we discuss some targeted strategies
    for disrupting epidemic spread that are rooted in network geometry.
  prefs: []
  type: TYPE_NORMAL
