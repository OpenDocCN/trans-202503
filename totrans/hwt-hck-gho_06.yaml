- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Healthy Stalking
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our bouncing servers are silently humming in a datacenter somewhere in Europe.
    Our attacking infrastructure is eagerly awaiting our first order. Before we unleash
    the plethora of attack tools that routinely flood the InfoSec Twitter timeline,
    let’s take a couple of minutes to understand how our target, political consulting
    firm Gretsch Politico, actually works. What is their business model? Which products
    and services do they provide? This kind of information will give us a direction
    to go in and help us narrow down our attack targets. Drawing tangible goals may
    very well be our first challenge. Their main website (www.gretschpolitico.com)
    does not exactly help: it is a boiling, bubbling soup of fuzzy marketing keywords
    that only make sense to the initiated. We’ll start, then, with benign public-facing
    information.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Gretsch Politico
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In an effort to better understand this company, let’s dig up every PowerPoint
    deck and PDF presentation that bears a reference to “Gretsch Politico” (GP). SlideShare
    ([https://www.slideshare.net/](https://www.slideshare.net/)) proves to be an invaluable
    ally in this quest. Many people simply forget to delete their presentations after
    a talk, or default them to “public access,” giving us a plethora of information
    to begin our quest for understanding (see [Figure 4-1](#figure4-1)).
  prefs: []
  type: TYPE_NORMAL
- en: '![f04001](image_fi/501263c04/f04001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-1: Some Gretsch Politico slides'
  prefs: []
  type: TYPE_NORMAL
- en: 'SlideShare is but one example of services hosting documents, so we next scour
    the web looking for resources uploaded to the most popular sharing platforms:
    Scribd, Google Drive, DocumentCloud, you name it. The following search terms will
    narrow down your results in most search engines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Google may be your default search engine, but you may find you achieve better
    results in others, like Yandex, Baidu, Bing, and so on, since Google tends to
    observe copyright infringement laws and moderates its search output.
  prefs: []
  type: TYPE_NORMAL
- en: Another great source of information about a company’s business is metasearch
    engines. Websites like Yippy and Biznar aggregate information from a variety of
    general and specialized search engines, giving a nice overview of the company’s
    recent activity.
  prefs: []
  type: TYPE_NORMAL
- en: From my initial search, many interesting documents pop out, from campaign fund
    reports mentioning GP to marketing pitches for campaign directors. Manually skimming
    through this data makes it clear that GP’s core service is building voter profiles
    based on multiple data inputs. These voter profiles are then studied and fed into
    an algorithm that decides which pitch is most suitable to lock in a voter.
  prefs: []
  type: TYPE_NORMAL
- en: Finding Hidden Relationships
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GP’s algorithms mash the data, that much is clear, but where does the data
    come from? To understand GP, we need to understand its closest partners. Whatever
    company or medium is delivering all this data must be working closely with GP.
    Multiple documents hint at the existence of at least two main channels:'
  prefs: []
  type: TYPE_NORMAL
- en: Data brokers or data management platforms Companies that sell data gathered
    from telecom companies, credit card issuers, online stores, local businesses,
    and many more sources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Research studies and surveys It seems that GP reaches out to the population
    somehow to send out questionnaires and collect opinions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Although GP’s main website barely mentions advertising as a way to reach the
    public, PDF documents abound with references to a particular advertising platform
    with tremendous reach, both on social and traditional media websites. There’s
    no straight link to this advertising platform, but thanks to these selfsame social
    media websites they are so fond of, we dig out the retweet shown in [Figure 4-2](#figure4-2)
    from Jenny, VP of marketing at GP according to her Twitter profile.
  prefs: []
  type: TYPE_NORMAL
- en: '![f04002](image_fi/501263c04/f04002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-2: A revealing GP retweet'
  prefs: []
  type: TYPE_NORMAL
- en: 'The link in the tweet innocuously points to an online advertising agency: MXR
    Ads. They deliver ads on all kinds of websites, charge per thousand impressions
    (CPM), and go quietly about their business of increasing the internet’s load time.'
  prefs: []
  type: TYPE_NORMAL
- en: Short of this excited tweet by Jenny of GP, there is not a single visible link
    between the two companies; there’s barely even a backlink on Google. So what’s
    the connection? We quickly solve this mystery by consulting the legal records
    of the two companies on [https://opencorporates.com/](https://opencorporates.com/),
    a database of companies worldwide and an excellent resource for digging out old
    company filings, shareholder lists, related entities, and so on. It turns out
    that MXR Ads and Gretsch Politico share most of the same directors and officers—hell,
    they even shared the same address a couple of years back.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of intertwined connection can be very profitable for both companies.
    MXR Ads gathers raw data about people’s engagement with a type of product or brand.
    They know, for example, that the person bearing the cookie 83bdfd57a5e likes guns
    and hunting. They transfer this raw data to Gretsch Politico, who analyze it and
    group it into a data segment of similar profiles labeled “people who like guns.”
    GP can then design creatives and videos to convince the population labeled “people
    who like guns” that their right to gun ownership is threatened unless they vote
    for the right candidate. GP’s client, who is running for office in some capacity,
    is pleased and starts dreaming about champagne bubble baths at the Capitol, while
    GP pushes these ads on every media platform with a functioning website. Of course,
    MXR Ads receives its share of creatives to distribute on its network as well,
    thus completing the self-feeding ouroboros of profit and desperation. Chilling.
  prefs: []
  type: TYPE_NORMAL
- en: From this close connection we can reasonably suspect that pwning either MXR
    Ads or GP could prove fatal to *both* companies. Their sharing of data implies
    some link or connection that we can exploit to bounce from one to the other. Our
    potential attack surface just expanded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a first, though very speculative, knowledge of the company’s
    modus operandi, we can set out to answer some interesting questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How precise are these data segments? Are they casting a large net targeting,
    say, all 18- to 50-year-olds, or can they drill down to a person’s most intimate
    habits?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who are GP’s clients? Not the pretty ponies they advertise on their slides,
    like health organizations trying to spread vaccines, but the ugly toads they bury
    in their databases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And finally, what do these creatives and ads look like? It might seem like a
    trivial question, but since they’re supposedly customized to each target population,
    it is hard to have any level of transparency and accountability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next few chapters we’ll attempt to answer these questions. The agenda
    is pretty ambitious, so I hope you are as excited as I am to dive into this strange
    world of data harvesting and deceit.
  prefs: []
  type: TYPE_NORMAL
- en: Scouring GitHub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A recurrent leitmotif in almost every presentation of Gretsch Politico and MXR
    Ads’ methodology is their investment in research and design and their proprietary
    machine learning algorithms. Such technology-oriented companies will likely have
    some source code published in public repositories for various purposes, such as
    minor contributions to the open source world used as bait to fish for talent,
    partial documentation of some API, code samples, and so on. We might just find
    some material that contains an overlooked password or sensitive link to their
    management platform. Fingers crossed!
  prefs: []
  type: TYPE_NORMAL
- en: Searching public repositories on GitHub is rather easy; you don’t even need
    to register for a free account. Simply proceed to look for keywords like “Gretsch
    Politico” and “MXR Ads.” [Figure 4-3](#figure4-3) shows the results when we search
    for MXR Ads’ repository.
  prefs: []
  type: TYPE_NORMAL
- en: '![f04003](image_fi/501263c04/f04003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-3: The MXR Ads GitHub repository'
  prefs: []
  type: TYPE_NORMAL
- en: A single company with 159 public repositories? That seems like a lot. After
    a cursory inspection, it’s clear only half a dozen of these repos actually belong
    to either MXR Ads or one of their employees. The rest are simply forks (copied
    repositories) that happen to mention MXR Ads—for instance, in ad-blocking lists.
    These forked repositories provide little to no value, so we’ll focus on those
    half a dozen original repos. Luckily, GitHub offers some patterns to weed out
    unwanted output. Using the two search prefixes `org:` and `repo:`, we can limit
    the scope of the results to the handful of accounts and repositories we decide
    are relevant.
  prefs: []
  type: TYPE_NORMAL
- en: We start looking for hardcoded secrets, like SQL passwords, AWS access keys,
    Google Cloud private keys, API tokens, and test accounts on the company’s advertising
    platform. Basically, we want anything that might grant us our first beloved access.
  prefs: []
  type: TYPE_NORMAL
- en: 'We enter these queries in the GitHub search and see what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The annoying limitation of GitHub’s search API is that it filters out special
    characters. When we search for “aws_secret_access_key,” GitHub will return any
    piece of code matching any of the four individual words (aws, secret, access,
    or key). This is probably the only time I sincerely miss regular expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that this phase of the recon is not only about blindly grabbing
    dangling passwords; it’s also about discovering URL and API endpoints, and acquainting
    ourselves with the technological preferences of the two companies. Every team
    has some dogma about which framework to use and which language to work with. This
    information might later help us adjust our payloads.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, preliminary GitHub search queries did not return anything worthy,
    so we bring out the big guns and bypass GitHub limitations altogether. Since we’re
    only targeting a handful of repositories, we’ll download the entire repositories
    to disk to unleash the full wrath of good ol’ grep!
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with the interesting list of hundreds of regular expression (regex)
    patterns defined in `shhgit`, a tool specifically designed to look for secrets
    in GitHub, from regular passwords to API tokens ([https://github.com/eth0izzle/shhgit/](https://github.com/eth0izzle/shhgit/)).
    The tool itself is also very useful for defenders, as it flags sensitive data
    pushed to GitHub by listening for webhook events—a *webhook* is a call to a URL
    following a given event. In this case, GitHub sends a POST request to a predefined
    web page every time a regex matches a string in the code submitted.
  prefs: []
  type: TYPE_NORMAL
- en: 'We rework the list of patterns, which you can find at [https://www.hacklikeapornstar.com/secret_regex_patterns.txt](https://www.hacklikeapornstar.com/secret_regex_patterns.txt),
    to make it grep-friendly. Then we download all the repos:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And start the search party:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This quick-and-dirty command will search through each file in the downloaded
    repositories. However, since we are dealing with Git repositories, `egrep` will
    omit previous versions of the code that are compressed and hidden away in Git’s
    internal filesystem structure (the *.git* folder). These old file versions are
    of course the most valuable assets! Think about all the credentials pushed by
    mistake or hardcoded in the early phases of a project. The famous line “It’s just
    a temporary fix” has never been more fatal than in a versioned repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `git` command provides the necessary tools we’ll use to walk down the commit
    memory lane: `git rev-list`, `git log`, `git revert`, and the most relevant to
    us, `git grep`. Unlike the regular `grep`, `git grep` expects a commit ID, which
    we provide using `git` `rev-list`. Chaining the two commands using `xargs` (extended
    arguments), we can retrieve all the commit IDs (all changes ever made to the repo)
    and search each one for interesting patterns using `git grep`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We could also have automated this search using a bash loop or completely relied
    on a tool like Gitleaks ([https://github.com/zricethezav/gitleaks/](https://github.com/zricethezav/gitleaks/))
    or truffleHog ([https://github.com/dxa4481/truffleHog/](https://github.com/dxa4481/truffleHog/))
    that takes care of sifting through all the commit files.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a couple of hours of twisting that public source code in every fashion
    possible, one thing becomes clear: there seems to be no hardcoded credentials
    anywhere. Not even a fake dummy test or test account to boost our enthusiasm.
    Either MXR Ads and GP are good at concealment or we are just not that lucky. No
    matter, we’ll move on!'
  prefs: []
  type: TYPE_NORMAL
- en: 'One feature of GitHub that most people tend to overlook is the ability to share
    snippets of code on [https://gist.github.co](https://gist.github.co), a service
    also provided by [https://pastebin.com/](https://pastebin.com/)*.* These two websites,
    and others such as [https://codepen.io/](https://codepen.io/), often contain pieces
    of code, database extracts, buckets, configuration files, and anything that developers
    want to exchange in a hurry. We’ll scrape some results from these sites using
    some search engine commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: One search yields the result shown in [Figure 4-4](#figure4-4).
  prefs: []
  type: TYPE_NORMAL
- en: '![f04004](image_fi/501263c04/f04004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-4: A snippet of an MXR Ads logfile'
  prefs: []
  type: TYPE_NORMAL
- en: 'This seems to be an extract of a logfile just hanging in a public Gist, available
    for everyone to see. Isn’t that just lovely? Sadly, no critical information is
    immediately available, but we do get these unique URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: format-true-v1.qa.euw1.mxrads.com
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: dash-v3-beta.gretschpolitico.com
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: www.surveysandstats.com/9df6c8db758b35fa0f1d73\. . .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We test these in a browser. The first link times out, and the second one redirects
    to a Google authentication page (see [Figure 4-5](#figure4-5)).
  prefs: []
  type: TYPE_NORMAL
- en: '![f04005](image_fi/501263c04/f04005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-5: Gretsch Politico sign-in link found in the logfile snippet'
  prefs: []
  type: TYPE_NORMAL
- en: Gretsch Politico evidently subscribes to Google Workspace (formerly G Suite)
    apps to manage its corporate emails and likely its user directory and internal
    documents. We’ll keep that in mind for later when we start scavenging for data.
  prefs: []
  type: TYPE_NORMAL
- en: The third URL, pointing to [Figure 4-6](#figure4-6), is promising.
  prefs: []
  type: TYPE_NORMAL
- en: '![f04006](image_fi/501263c04/f04006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-6: Link to an MXR Ad survey found in the logfile snippet'
  prefs: []
  type: TYPE_NORMAL
- en: This must be one of these surveys MXR Ads uses to gather seemingly harmless
    information about people. Attempting to pwn MXR Ads or Gretsch Politico through
    one of their pernicious forms is quite tempting, but we are still in the midst
    of our reconnaissance work, so let’s just note this for a later attempt.
  prefs: []
  type: TYPE_NORMAL
- en: Pulling Web Domains
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Passive reconnaissance has not yielded us many entry points so far. I believe
    it’s time we seriously start digging up all the domains and subdomains related
    to MXR Ads and Gretsch Politico. I’m sure we can find so much more than the three
    measly websites in a forgotten Gist paste. Hopefully we’ll land on a forlorn website
    with a sneaky vulnerability welcoming us inside.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll begin our search by first checking certificate logs for subdomains.
  prefs: []
  type: TYPE_NORMAL
- en: From Certificates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Censys ([https://censys.io/](https://censys.io/)) is a tool that routinely scans
    certificate logs to ingest all newly issued TLS certificates, and it’s number
    one on any pentester’s domain discovery tool list. Upon their issuance by a certificate
    authority, certificates are pushed to a central repository called a *certificate
    log*. This repository keeps a binary tree of all certificates, where each node
    is the hash of its child nodes, thus guaranteeing the integrity of the entire
    chain. It’s roughly the same principle followed by the Bitcoin blockchain. In
    theory, all issued TLS certificates should be publicly published to detect domain
    spoofing, typo-squatting, homograph attacks, and other mischievous ways to deceive
    and redirect users.
  prefs: []
  type: TYPE_NORMAL
- en: We can search these certificate logs to eke out any new registrations matching
    certain criteria, like “mxr ads.” The ugly side of this beautiful canvas is that
    all domains and subdomain names are openly accessible online. Secret applications
    with little security hiding behind obscure domains are therefore easily exposed.
    Tools like Censys and *crt.sh* explore these certificate logs and help speed up
    subdomain enumeration by at least an order of magnitude—a cruel reminder that
    even the sweetest grapes can hide the most bitter seeds. In [Figure 4-7](#figure4-7)
    we use Censys to search for subdomains of gretschpolitico.com.
  prefs: []
  type: TYPE_NORMAL
- en: '![f04007](image_fi/501263c04/f04007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-7: Looking for subdomains with Censys'
  prefs: []
  type: TYPE_NORMAL
- en: 'So much for transparency. It seems that GP did not bother registering subdomain
    certificates and has instead opted for a *wildcard certificate*: a generic certificate
    that matches any subdomain. One certificate to rule them all. Whether this is
    a brilliant security move or pure laziness, the fact is, we’re no further than
    the top domain. We try other top-level domains in Censys—gretschpolitico.io, mxrads.tech,
    mxrads.com, gretschpolitico.news, and so forth—but come up equally empty-handed.
    Our list of domains grew by a whopping big fat zero . . . but do not despair!
    We have other tricks up our collective sleeves.'
  prefs: []
  type: TYPE_NORMAL
- en: By Harvesting the Internet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If certificates are not the way to gather subdomains, then maybe the internet
    can lend us a helping hand. Sublist3r is a great and easy-to-use tool that harvests
    subdomains from various sources: search engines, PassiveDNS, even VirusTotal.
    First, we fetch the tool from the official repository and install requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Then we proceed to search for subdomains, as shown in [Listing 4-1](#listing4-1).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 4-1: Enumerating domains with sublist3r'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve found 12 subdomains, so that’s encouraging. I bet we’d have even more
    luck with mxrads.com. They are, after all, a media company. However, it can get
    boring to use the same tools and methods repeatedly. For the mrxads.com domain,
    let’s use a different tool to perform a classic brute-force attack using well-known
    subdomain keywords like staging.mxrads.com, help.mxrads.com, dev.mxrads.com, and
    so on. There are a few tools we can choose from for the job.
  prefs: []
  type: TYPE_NORMAL
- en: Amass ([https://github.com/OWASP/Amass/](https://github.com/OWASP/Amass/)) from
    the Open Web Application Security Project (OWASP) is written in Golang and cleverly
    uses goroutines to parallelize the load of DNS queries. Whereas most other Python
    tools rely on the system’s DNS resolver to retrieve domains by calling functions
    like `socket.gethostname`, Amass crafts DNS queries from scratch and sends them
    to various DNS servers, thus avoiding the bottleneck caused by using the same
    local resolver. However, Amass is bloated with so many other colorful features,
    like visualizations and 3D graphs, that it may feel like wielding a 10-pound hammer
    to scratch an itch on your back. Tempting, but there are lighter alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'A less mediatized yet very powerful tool that I highly recommend is Fernmelder([https://github.com/stealth/fernmelder/](https://github.com/stealth/fernmelder/)).
    It’s written in C, is barely a few hundred lines of code, and is probably the
    most efficient DNS bruteforcer I have tried lately. Fernmelder takes two inputs:
    a list of candidate DNS names and the IPs of DNS resolvers to use. This is what
    we’ll use.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we create our list of possible DNS names using some `awk` magic applied
    to a public subdomain wordlist, as shown in [Listing 4-2](#listing4-2). Daniel
    Miessler’s SecLists is a good start, for instance:[https://github.com/danielmiessler/SecLists/](https://github.com/danielmiessler/SecLists/)*.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 4-2: Creating a list of potential MXR Ads subdomains'
  prefs: []
  type: TYPE_NORMAL
- en: This gives us a few thousand potential subdomain candidates to try. As for the
    second input, you can borrow the DNS resolvers found at the Fernmelder repo, as
    shown in [Listing 4-3](#listing4-3).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 4-3: Resolving our subdomain candidates to see which are real'
  prefs: []
  type: TYPE_NORMAL
- en: Be careful adding new resolvers, as some servers tend to play dirty and will
    return a default IP when resolving a nonexistent domain rather than the standard
    `NXDOMAIN` reply. The `-A` option at the end of the command hides any unsuccessful
    domain resolution attempts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Results from [Listing 4-3](#listing4-3) start pouring in impressively fast.
    Of the thousand subdomains we tried resolving, a few dozen responded with valid
    IP addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Watching these IP addresses roll by on the screen is mesmerizing. Each entry
    is a door waiting to be subtly engineered or forcefully raided to grant us access.
    This is why this reconnaissance phase is so important: it affords us the luxury
    of choice, with over 100 domains belonging to both organizations!'
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the Web Infrastructure Used
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The traditional approach to examining these sites would be to run WHOIS queries
    on these newly found domains, from which we can figure out the IP segment belonging
    to the company. With that we can scan for open ports in that range using Nmap
    or Masscan, hoping to land on an unauthenticated database or poorly protected
    Windows box. We try WHOIS queries on a few subdomains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: However, looking carefully at this list of IP addresses, we quickly realize
    that they have nothing to do with Gretsch Politico or MXR Ads. It turns out that
    most of the subdomains we collected are running on AWS infrastructure. This is
    an important conclusion. Most internet resources on AWS, like load balancers,
    content distribution networks, S3 buckets, and so on, regularly rotate their IP
    addresses.
  prefs: []
  type: TYPE_NORMAL
- en: That means that if we feed this list of IPs to Nmap and the port scan drags
    on longer than a couple of hours, the addresses will have already been assigned
    to another customer and the results will no longer be relevant. Of course, companies
    can always attach a fixed IP to a server and directly expose their application,
    but that’s like intentionally dropping an iron ball right on your little toe.
    Nobody is that masochistic.
  prefs: []
  type: TYPE_NORMAL
- en: Over the last decade, we hackers have gotten into the habit of only scanning
    IP addresses and skipping DNS resolution in order to gain a few seconds, but when
    dealing with a cloud provider, this could prove fatal. Instead, we should scan
    domain names; that way, the name resolution will be performed closer to the actual
    scan to guarantee its integrity.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s what we will do next. We launch a fast Nmap scan on all the domain names
    we’ve gathered so far to look for open ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We focus on the most common ports using `-F`, grab the component’s version using
    `-sV`, and save the results in XML, RAW, and text formats with `-oA`. This scan
    may take a few minutes, so while waiting for it to finish, we’ll turn our attention
    to the actual content of the hundreds of domains and websites we found belonging
    to MXR Ads and Gretsch Politico.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Find an example of leaked credentials by searching for a bug report of a researcher
    finding API tokens in a Starbucks-owned repo: [https://hackerone.com/reports/716292/](https://hackerone.com/reports/716292/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search for Juri Strumpflohner’s tutorial at [https://juristr.com/](https://juristr.com/)
    if you’re not familiar with Git’s internals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
