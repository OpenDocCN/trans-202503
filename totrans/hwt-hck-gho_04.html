<html><head></head><body>
<section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" title="21" id="Page_21"/>3</span><br/>
<span class="ChapterTitle">Let There Be Infrastructure</span></h1>
</header>
<figure class="opener">
<img src="image_fi/book_art/chapterart.png" alt=""/>
</figure>
<p class="ChapterIntro">In this chapter we’ll set up the backend attacking infrastructure as well as the tooling necessary to faithfully reproduce and automate almost every painful aspect of the manual setup. We’ll stick with two frameworks: Metasploit for Linux targets and SILENTTRINITY for Windows boxes.</p>
<h2 id="h1-501263c03-0001">Legacy Method</h2>
<p class="BodyFirst">The old way to set up an attacking infrastructure would be to install each of your frameworks on a machine and place a web server in front of them to receive and route traffic according to simple pattern-matching rules. As illustrated in <a href="#figure3-1" id="figureanchor3-1">Figure 3-1</a>, requests to <em>/secretPage</em> get forwarded to the C2 backend, while the rest of the pages return seemingly innocuous content.</p>
<span epub:type="pagebreak" title="22" id="Page_22"/><figure>
<img src="image_fi/501263c03/f03001.png" alt="f03001.png"/>
<figcaption><p><a id="figure3-1">Figure 3-1</a>: Illustration of the C2 backend</p></figcaption>
</figure>
<p>The Nginx web server is a popular choice to proxy web traffic and can be tuned relatively quickly. First, we install it using a classic package manager (<code>apt</code> in this case):</p>
<pre><code>root@Lab:~/# <b>apt install -y nginx</b>
root@Lab:~/# <b>vi /etc/nginx/conf.d/reverse.conf</b></code></pre>
<p>Then we create a config file that describes our routing policies, as shown in <a href="#listing3-1" id="listinganchor3-1">Listing 3-1</a>.</p>
<pre><code>#/etc/nginx/conf.d/reverse.conf

server {
  # basic web server configuration
  listen 80;

  # normal requests are served from /var/www/html
  root /var/www/html;
  index index.html;
  server_name <var>www.mydomain.com</var>;

  # return 404 if no file or directory match
  location / {
     try_files $uri $uri/ =404;
  }

  # /msf URL gets redirected to our backend C2 framework
  location /msf {
     proxy_pass https://192.168.1.29:8443;
     proxy_ssl_verify off;
     proxy_set_header Host $host;
     proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
  }
  # repeat previous block for other C2 backends
}</code></pre>
<p class="CodeListingCaption"><a id="listing3-1">Listing 3-1</a>: Standard Nginx configuration file with HTTP redirectors</p>
<p>The first few directives define the root directory containing web pages served for normal queries. Next, we instruct Nginx to forward the URLs we want to redirect, starting with <code>/msf</code>, straight to our C2 backend, as is evident by the <code>proxy_pass</code> directive.</p>
<p><span epub:type="pagebreak" title="23" id="Page_23"/>We can then quickly set up Secure Shell (SSL) certificates using Let’s Encrypt via EFF’s Certbot and have a fully functional web server with HTTPS redirection:</p>
<pre><code>root@Lab:~/# <b>add-apt-repository ppa:certbot/certbot</b>
root@Lab:~/# <b>apt update &amp;&amp; apt install python-certbot-nginx</b>
root@Lab:~/# <b>certbot --nginx -d </b><var class="bold">mydomain.com </var><b>-d </b><var class="bold">www.mydomain.com</var>

Congratulations! Your certificate and chain have been saved at...</code></pre>
<p>This method is completely fine, except that tuning an Nginx or Apache server can quickly get boring and cumbersome, especially since this machine will be facing the target, thus dramatically increasing its volatility. The server is always one IP ban away from being restarted or even terminated.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	Some cloud providers, like Amazon Web Services (AWS), automatically renew the public IP of a host upon restart. Other cloud providers, like DigitalOcean, however, attach a fixed IP to a machine.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>Configuring the C2 backends is no fun either. No hosting provider will give you a shiny Kali distro with all the dependencies pre-installed. That’s on you, and you’d better get that Ruby version of Metasploit just right; otherwise, it will spill out errors that will make you question your own sanity. The same can be said for almost any application that relies on specific advanced features of a given environment. </p>
<h2 id="h1-501263c03-0002">Containers and Virtualization</h2>
<p class="BodyFirst">The solution is to package all your applications with all their dependencies properly installed and tuned to the right version. When you spin up a new machine, you need not install anything. You just download the entire bundle and run it as an ensemble. That’s basically the essence of the container technology that took the industry by storm and changed the way software is managed and run. Since we’ll be dealing with some containers later on, let’s take the time to deconstruct their internals while preparing our own little environment.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	Another solution would be to automate the deployment of these components using a tool like Ansible or Chef.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>There are many players in the container world, each working at different abstraction levels or providing different isolation features, including containerd, runC, LXC, rkt, OpenVZ, and Kata Containers. I’ll be using the flagship product Docker because we’ll run into it later in the book.</p>
<p>In an effort to oversimplify the concept of containerization, most experts liken it to virtualization: “Containers are lightweight virtual machines, except that they share the kernel of their host” is a sentence usually found under the familiar image in <a href="#figure3-2" id="figureanchor3-2">Figure 3-2</a>.</p>
<span epub:type="pagebreak" title="24" id="Page_24"/><figure>
<img src="image_fi/501263c03/f03002.png" alt="f03002.png"/>
<figcaption><p><a id="figure3-2">Figure 3-2</a>: An oversimplified depiction of containers</p></figcaption>
</figure>
<p>This statement may suffice for most programmers who are just looking to deploy an app as quickly as possible, but hackers need more, crave more detail. It’s our duty to know enough about a technology to bend its rules. Comparing virtualization to containerization is like comparing an airplane to a bus. Sure, we can all agree that their purpose is to transport people, but the logistics are not the same. Hell, even the physics involved are different.</p>
<p><em>Virtualization</em> spawns a fully functioning operating system on top of an existing one. It proceeds with its own boot sequence and loads the filesystem, scheduler, kernel structures, the whole nine yards. The guest system believes it is running on real hardware, but secretly, behind every system call, the virtualization service (say, VirtualBox) translates all low-level operations, like reading a file or firing an interrupt, into the host’s own language, and vice versa. That’s how you can have a Linux guest running on a Windows machine.</p>
<p><em>Containerization</em><em> </em>is a different paradigm, where system resources are compartmentalized and protected by a clever combination of three powerful features of the Linux kernel: namespaces, a union filesystem, and cgroups.</p>
<h3 id="h2-501263c03-0001">Namespaces</h3>
<p class="BodyFirst"><em>Namespaces</em> are tags that can be assigned to Linux resources like processes, networks, users, mounted filesystems, and so on. By default, all resources in a given system share the same default namespace, so any regular Linux user can list all processes, see the entire filesystem, list all users, and so on.</p>
<p>However, when we spin up a container, all these new resources created by the container environment—processes, network interfaces, filesystem, and so on—get assigned a different tag. They become <em>contained</em> in their own namespace and ignore the existence of resources outside that namespace.</p>
<p><span epub:type="pagebreak" title="25" id="Page_25"/>A perfect illustration of this concept is the way Linux organizes its processes. Upon booting up, Linux starts the systemd process, which gets assigned process ID (PID) number 1. This process then launches subsequent services and daemons, like NetworkManager, crond, and sshd, that get assigned increasing PID numbers, as shown here:</p>
<pre><code>root@Lab:~/# <b>pstree -p</b>
systemd(1)─┬─accounts-daemon(777)─┬─{gdbus}(841)
           │                      └─{gmain}(826)
           ├─acpid(800)
           ├─agetty(1121)</code></pre>
<p>All processes are linked to the same tree structure headed by systemd, and all processes belong to the same namespace. They can therefore see and interact with each other—provided they have permission to do so, of course.</p>
<p>When Docker (or more accurately runC, the low-level component in charge of spinning up containers) spawns a new container, it first executes itself in the default namespace (with PID 5 in <a href="#figure3-3" id="figureanchor3-3">Figure 3-3</a>) and then spins up child processes in a new namespace. The first child process gets a local PID 1 in this new namespace, along with a different PID in the default namespace (say, 6, as in <a href="#figure3-3">Figure 3-3</a>).</p>
<figure>
<img src="image_fi/501263c03/f03003.png" alt="f03003.png"/>
<figcaption><p><a id="figure3-3">Figure 3-3</a>: Linux process tree with two processes contained in a new namespace</p></figcaption>
</figure>
<p>Processes in the new namespace are not aware of what is happening outside their environment, yet older processes in the default namespace maintain complete visibility over the whole process tree. That’s why the main challenge when hacking a containerized environment is breaking this namespace isolation. If we can somehow run a process in the default namespace, we can effectively snoop on all containers on the host.</p>
<p>Every resource inside a container continues to interact with the kernel without going through any kind of middleman. The containerized processes are just restricted to resources bearing the same tag. With containers, we are in a flat but compartmentalized system, whereas virtualization resembles a set of nesting Russian dolls.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	If you want to learn more about container namespaces, check out the detailed article on namespaces by Mahmud Ridwan at <a href="https://www.toptal.com/" class="LinkURL">https://www.toptal.com/</a>.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h4 id="h3-501263c03-0001"><span epub:type="pagebreak" title="26" id="Page_26"/>A Metasploit Container</h4>
<p class="BodyFirst">Let’s dive into a practical example by launching a Metasploit container. Luckily, a hacker named phocean has already created a ready-to-use image we can do this exercise on, found at <a href="https://github.com/phocean/dockerfile-msf/" class="LinkURL">https://github.com/phocean/dockerfile-msf/</a>. We first have to install Docker, of course:</p>
<pre><code>root@Lab:~/# <b>curl -fsSL https://download.docker.com/linux/ubuntu/gpg   <code class="bold">| apt-key add -</code></b>

root@Lab:~/# <b>add-apt-repository \</b>
   <b>"deb [arch=amd64] https://download.docker.com/linux/ubuntu \</b>
   <b>$(lsb_release -cs) \</b>
   <b>stable"</b>

root@Lab:~/# <b>apt update</b>
root@Lab:~/# <b>apt install -y docker-ce</b></code></pre>
<p>We then download the Docker bundle or image, which contains Metasploit files, binaries, and dependencies that are already compiled and ready to go, with the <code>docker pull</code> command:</p>
<pre><code>root@Lab:~/# <b>docker pull phocean/msf</b>
root@Lab:~/# <b>docker run --rm -it phocean/msf</b>
* Starting PostgreSQL 10 database server
[ OK ]
root@46459ecdc0c4:/opt/metasploit-framework#</code></pre>
<p>The <code>docker run</code> command spins up this container’s binaries in a new namespace. The <code>--rm</code> option deletes the container upon termination to clean up resources. This is a useful option when testing multiple images. The <code>-it</code> double option allocates a pseudoterminal and links to the container’s stdin device to mimic an interactive shell.</p>
<p>We can then start Metasploit using the <code>msfconsole</code> command:</p>
<pre><code>root@46459ecdc0c4:/opt/metasploit-framework#<b> ./msfconsole</b>

       =[ metasploit v5.0.54-dev                          ]
+ -- --=[ 1931 exploits - 1078 auxiliary - 332 post       ]
+ -- --=[ 556 payloads - 45 encoders - 10 nops            ]
+ -- --=[ 7 evasion                                       ]

msf5 &gt; <b>exit</b></code></pre>
<p>Compare that to installing Metasploit from scratch and you will hopefully understand how much blood and sweat were spared by these two commands.</p>
<p>Of course, you may wonder, “How, in this new isolated environment, can we reach a listener from a remote Nginx web server?” Excellent question.</p>
<p>When starting a container, Docker automatically creates a pair of virtual Ethernet (<code>veth</code> in Linux). Think of these devices as the two connectors at the end of a physical cable. One end is assigned the new namespace, <span epub:type="pagebreak" title="27" id="Page_27"/>where it can be used by the container to send and receive network packets. This <code>veth</code> usually bears the familiar <code>eth0</code> name inside the container. The other connector is assigned the default namespace and is plugged into a network switch that carries traffic to and from the external world. Linux calls this virtual switch a <em>network bridge</em>.</p>
<p>A quick <code>ip addr</code> on the machine shows the default <code>docker0</code> bridge with the allocated 172.17.0.0/16 IP range ready to be distributed across new containers:</p>
<pre><code>root@Lab:~/# <b>ip addr</b>
3: <b>docker0</b>: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 state group default
link/ether 03:12:27:8f:b9:42 brd ff:ff:ff:ff:ff:ff
inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
<var>--snip--</var></code></pre>
<p>Every container gets its dedicated <code>veth</code> pair, and therefore IP address, from the <code>docker0</code> bridge IP range.</p>
<p>Going back to our original issue, routing traffic from the external world to a container simply involves forwarding traffic to the Docker network bridge, which will automatically carry it to the right <code>veth</code> pair. Instead of toying with iptables, we can call on Docker to create a firewall rule that does just that. In the following command, ports 8400 to 8500 on the host will map to ports 8400 to 8500 in the container:</p>
<pre><code>root@Lab:~/# <b>sudo docker run --rm \</b>
<b>-it -p 8400-8500:8400-8500 \</b>
<b>-v ~/.msf4:/root/.msf4 \</b>
<b>-v /tmp/msf:/tmp/data \</b>
<b>phocean/msf</b></code></pre>
<p>Now we can reach a handler listening on any port between 8400 and 8500 inside the container by sending packets to the host’s IP address on that same port range.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	If you don’t want to bother with port mapping, just attach the containers to the host’s network interface using the <var>--net=host</var> flag on <code>docker run</code> instead of running <var>-p xxx:xxxx</var>.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>In the previous command we also mapped the directories <em>~/.msf4</em> and <em>/tmp/msf</em> on the host to directories in the container, <em>/root/.msf4</em> and <em>/tmp/data</em>, respectively—a useful trick for persisting data across multiple runs of the same Metasploit container.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	To send the container to the background, simply press <span class="KeyCaps">CTRL</span>-P followed by <span class="KeyCaps">CTRL</span>-Q. You can also send it to the background from the start by adding the <code>-d</code> flag. To get inside once more, execute a <code>docker ps</code>, get the Docker ID, and run <code>docker attach </code><var>&lt;ID&gt;</var>. Or you can run the <code>docker exec -it</code> <var>&lt;ID&gt;</var> <code>sh</code> command. For other useful commands, check out the Docker cheat sheet at <a href="http://dockerlabs.collabnix.com/" class="LinkURL">http://dockerlabs.collabnix.com/</a><em>.</em></p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2-501263c03-0002"><span epub:type="pagebreak" title="28" id="Page_28"/>Union Filesystem</h3>
<p class="BodyFirst">This brings us neatly to the next concept of containerization, the <em>union filesystem</em> <em>(UFS)</em>, which enables a technique of merging files from multiple filesystems to present a single and coherent filesystem layout. Let’s explore it through a practical example: we’ll build a Docker image for SILENTTRINITY.</p>
<p>A Docker image is defined in a <em>Dockerfile</em>. This is a text file containing instructions to build the image by defining which files to download, which environment variables to create, and all the rest. The commands are fairly intuitive, as you can see in <a href="#listing3-2" id="listinganchor3-2">Listing 3-2</a>.</p>
<pre><code># file: ~/SILENTTRINITY/Dockerfile
# The base Docker image containing binaries to run Python 3.7
FROM python:stretch-slim-3.7

# We install the git, make, and gcc tools
RUN apt-get update &amp;&amp; apt-get install -y git make gcc

# We download SILENTTRINITY and change directories
RUN git clone https://github.com/byt3bl33d3r/SILENTTRINITY/ /root/st/
WORKDIR /root/st/

# We install the Python requirements
RUN python3 -m pip install -r requirements.txt

# We inform future Docker users that they need to bind port 5000
EXPOSE 5000

# ENTRYPOINT is the first command the container runs when it starts
ENTRYPOINT ["python3", "teamserver.py", "0.0.0.0", "stringpassword"]</code></pre>
<p class="CodeListingCaption"><a id="listing3-2">Listing 3-2</a>: Dockerfile to start the SILENTTRINITY team server</p>
<p>We start by building a base image of Python 3.7, which is a set of files and dependencies for running Python 3.7 that is already prepared and available in the official Docker repository, Docker Hub. We then install some common utilities like <code>git</code>, <code>make</code>, and <code>gcc</code> that we will later use to download the repository and run the team server. The <code>EXPOSE</code> instruction is purely for documentation purposes. To actually expose a given port, we’ll still need to use the <code>-p</code> argument when executing <code>docker run</code>.</p>
<p>Next, we use a single instruction to pull the base image, populate it with the tools and files we mentioned, and name the resulting image <code>silent</code>:</p>
<pre><code>
root@Lab:~/# <b>docker build -t silent .</b>
Step 1/7 : FROM python:3.7-slim-stretch
 ---&gt; fad2b9f06d3b
Step 2/7 : RUN apt-get update &amp;&amp; apt-get install -y git make gcc
 ---&gt; Using cache
 ---&gt; 94f5fc21a5c4
<var>--snip--</var>
<span epub:type="pagebreak" title="29" id="Page_29"/>Successfully built f5658cf8e13c
Successfully tagged silent:latest</code></pre>
<p>Each instruction generates a new set of files that are grouped together. These folders are usually stored in <em>/var/lib/docker/overlay2/</em> and named after the random ID generated by each step, which will look something like <em>fad2b9f06d3b</em>, <em>94f5fc21a5c4</em>, and so on. When the image is built, the files in each folder are combined under a single new directory called the <em>image layer</em>. Higher directories shadow lower ones. For instance, a file altered in step 3 during the build process will shadow the same file created in step 1.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	The directory changes according to the storage driver used: <em>/var/lib/docker/aufs/diff/</em>, <em>/var/lib/docker/overlay/diff/</em>, or <em>/var/lib/docker/overlay2/diff/</em>. More information about storage drivers is available at <a href="https://dockr.ly/2N7kPsB" class="LinkURL">https://dockr.ly/2N7kPsB</a>.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>When we run this image, Docker mounts the image layer inside the container as a single read-only and chrooted filesystem. To allow users to alter files during runtime, Docker further adds a writable layer, called the <em>container layer</em> or <em>upperdir</em>, on top, as illustrated in <a href="#figure3-4" id="figureanchor3-4">Figure 3-4</a>.</p>
<figure>
<img src="image_fi/501263c03/f03004.png" alt="f03004.png"/>
<figcaption><p><a id="figure3-4">Figure 3-4</a>: Writable layer for a Docker image. Source: <a href="https://dockr.ly/39ToIeq" class="LinkURL">https://dockr.ly/39ToIeq</a>.</p></figcaption>
</figure>
<p>This is what gives containers their immutability. Even though you overwrite the whole <em>/bin</em> directory at runtime, you actually only ever alter the ephemeral writable layer at the top that masks the original <em>/bin</em> folder. The writable layer is tossed away when the container is deleted (recall the <code>--rm</code> option). The underlying files and folders prepared during the image build remain untouched.</p>
<p>We can start the newly built image in the background using the <code>-d</code> switch:</p>
<pre><code>root@Lab:~/# <b>docker run -d \</b>
<b>-v /opt/st:/root/st/data \</b>
<b>-p5000:5000 \</b>
<b>silent</b>

3adf0cfdaf374f9c049d40a0eb3401629da05abc48c

# Connect to the team server running on the container
root@Lab:~st/# <b>python3.7 st.py \wss://</b><var class="bold">username</var><var class="bold">:</var><var class="bold">strongPasswordCantGuess</var><b>@192.168.1.29:5000</b>

[1] ST &gt;&gt;</code></pre>
<p><span epub:type="pagebreak" title="30" id="Page_30"/>Perfect. We have a working SILENTTRINITY Docker image. To be able to download it from any workstation, we need to push it to a Docker repository. To do so, we create an account on <a href="https://hub.docker.com" class="LinkURL">https://hub.docker.com</a> as well as our first public repository, called <em>silent</em>. Following Docker Hub’s convention, we rename the Docker image to <var>username</var>/<var>repo-name</var> using <code>docker tag</code> and then push it to the remote registry, like so:</p>
<pre><code>root@Lab:~/# <b>docker login</b>
Username: <b>sparcflow</b>
Password:

Login Succeeded

root@Lab:~/# <b>docker tag silent sparcflow/silent</b>
root@Lab:~/# <b>docker push sparcflow/silent</b></code></pre>
<p>Now our SILENTTRINITY Docker image is one <code>docker pull</code> away from running on any Linux machine we spawn in the future.</p>
<h3 id="h2-501263c03-0003">Cgroups</h3>
<p class="BodyFirst">The last vital component of containers is <em>control groups (cgroups)</em>, which add some constraints that namespaces cannot address, like CPU limits, memory, network priority, and the devices available to the container. Just as their name implies, cgroups offer a way of grouping and bounding processes by the same limitation on a given resource; for example, processes that are part of the /system.slice/accounts-daemon.service cgroup can only use 30 percent of the CPU and 20 percent of the total bandwidth, and cannot query the external hard drive.</p>
<p>Here is the output of the command <code>systemd-cgtop</code>, which tracks cgroup usage across the system:</p>
<pre><code>root@Lab:~/# <b>systemd-cgtop</b>
Control Group                            Tasks   %CPU   Memory  Input/s
/                                          188    1.1     1.9G        -
/docker                                      2      -     2.2M        -
/docker/08d210aa5c63a81a761130fa6ec76f9      1      -   660.0K        -
/docker/24ef188842154f0b892506bfff5d6fa      1      -   472.0K        -</code></pre>
<p>We will circle back to cgroups later on when we talk about the privileged mode in Docker, so let’s leave it at that for now.</p>
<p>To recap then: whichever cloud provider we choose and whatever Linux distribution they host, as long as there is Docker support, we can spawn our fully configured C2 backends using a couple of command lines. The following will run our Metasploit container:</p>
<pre><code>root@Lab:~/# <b>docker run -dit \</b>
<b>-p 9990-9999:9990-9999 \</b>
<b>-v $HOME/.msf4:/root/.msf4 \</b>
<b>-v /tmp/msf:/tmp/data phocean/msf</b></code></pre>
<p><span epub:type="pagebreak" title="31" id="Page_31"/>And this will run the SILENTTRINITY container:</p>
<pre><code>root@Lab:~/# <b>docker run -d \</b>
<b>-v /opt/st:/root/st/data \</b>
<b>-p5000-5050:5000-5050 \</b>
<b>sparcflow/silent</b></code></pre>
<p>In these examples we used vanilla versions of Metasploit and SILENTTRINITY, but we could have just as easily added custom Boo-Lang payloads, Metasploit resource files, and much more. The best part? We can duplicate our C2 backends as many times as we want, easily maintain different versions, replace them at will, and so forth. Pretty neat, right?</p>
<p>The last step is to “dockerize” the Nginx server that routes calls to either Metasploit or SILENTTRINITY according to the URL’s path.</p>
<p>Fortunately, in this case, most of the heavy lifting has already been done by @staticfloat, who did a great job automating the Nginx setup with SSL certificates generated by Let’s Encrypt with <a href="https://github.com/staticfloat/docker-nginx-certbot" class="LinkURL">https://github.com/staticfloat/docker-nginx-certbot</a>. As shown in <a href="#listing3-3" id="listinganchor3-3">Listing 3-3</a>, we just need to make a couple of adjustments to the Dockerfile in the repo to fit our needs, like accepting a variable domain name and a C2 IP to forward traffic to.</p>
<pre><code># file: ~/nginx/Dockerfile
# The base image with scripts to configure Nginx and Let's Encrypt
FROM staticfloat/nginx-certbot

# Copy a template Nginx configuration
COPY *.conf /etc/nginx/conf.d/

# Copy phony HTML web pages
COPY --chown=www-data:www-data html/* /var/www/html/

# Small script that replaces __DOMAIN__ with the ENV domain value, same for IP
COPY init.sh /scripts/

ENV DOMAIN=<var>"www.customdomain.com"</var>
ENV C2IP="192.168.1.29"
ENV CERTBOT_EMAIL="sparc.flow@protonmail.com"

CMD ["/bin/bash", "/scripts/init.sh"]</code></pre>
<p class="CodeListingCaption"><a id="listing3-3">Listing 3-3</a>: Dockerfile to set up an Nginx server with a Let’s Encrypt certificate</p>
<p>The <em>init.sh</em> script is simply a couple of <code>sed</code> commands we use to replace the string <code>"__DOMAIN__"</code> in Nginx’s configuration file with the environment variable <code>$DOMAIN</code>, which we can override at runtime using the <code>-e</code> switch. This means that whatever domain name we choose, we can easily start an Nginx container that will automatically register the proper TLS certificates.</p>
<p>The Nginx configuration file is almost exactly the same as the one in <a href="#listing3-3">Listing 3-3</a>, so I will not go through it again. You can check out all the files involved in the building of this image in the book’s GitHub repo at <a href="http://www.nostarch.com/how-hack-ghost/" class="LinkURL">www.nostarch.com/how-hack-ghost</a>.</p>
<p><span epub:type="pagebreak" title="32" id="Page_32"/>Launching a fully functioning Nginx server that redirects traffic to our C2 endpoints is now a one-line job:</p>
<pre><code>root@Lab:~/# <b>docker run -d \</b></code></pre>
<code class="bold">-p80:80 -p443:443 \</code>
<pre><code><b>-e DOMAIN=</b><var>"www.customdomain.com"</var><b> \</b></code></pre>
<code class="bold">-e C2IP="192.168.1.29" \</code>
<code class="bold">-v /opt/letsencrypt:/etc/letsencrypt \</code>
<code class="bold">sparcflow/nginx</code>
<p>The DNS record of <em>www.&lt;customdomain&gt;.com</em> should obviously already point to the server’s public IP for this maneuver to work. While Metasploit and SILENTTRINITY containers can run on the same host, the Nginx container should run separately. Consider it as sort of a technological fuse: it’s the first one to burst into flames at the slightest issue. If, for example, our IP or domain gets flagged, we simply respawn a new host and run a <code>docker run</code> command. Twenty seconds later, we have a new domain with a new IP routing to the same backends.</p>
<h2 id="h1-501263c03-0003">IP Masquerading</h2>
<p class="BodyFirst">Speaking of domains, let’s buy a couple of legit ones to masquerade our IPs. I usually like to purchase two types of domains: one for workstation reverse shells and another one for machines. The distinction is important. Users tend to visit normal-looking websites, so maybe buy a domain that implies it’s a blog about sports or cooking. Something like <em>experienceyourfood.com</em> should do the trick.</p>
<p>It would be weird for a server to initiate a connection toward this domain, however, so the second type of domain to purchase should be something like <em>linux-packets.org</em>, which we can masquerade as a legit package distribution point by hosting a number of Linux binaries and source code files. After all, a server initiating a connection to the World Wide Web to download packages is the accepted pattern. I cannot count the number of false positives that threat intelligence analysts have had to discard because a server deep in the network ran an <code>apt update</code> that downloaded hundreds of packages from an unknown host. We can be that false positive!</p>
<p>I will not dwell much more on domain registration because our goal is not to break into the company using phishing, so we’ll avoid most of the scrutiny around domain history, classification, domain authentication through DomainKeys Identified Mail (DKIM), and so on. This is all explored in detail in my book <em>How to Hack Like a Legend.</em></p>
<p>Our infrastructure is almost ready now. We still need to tune our C2 frameworks a bit, prepare stagers, and launch listeners, but we will get there further down the road.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	Both SILENTTRINITY and Metasploit support runtime files or scripts to automate the setup of a listener/stager.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h2 id="h1-501263c03-0004"><span epub:type="pagebreak" title="33" id="Page_33"/>Automating the Server Setup</h2>
<p class="BodyFirst">The last painful experience we need to automate is the setup of the actual servers on the cloud provider. No matter what each provider falsely claims, one still needs to go through a tedious number of menus and tabs to have a working infrastructure: firewall rules, hard drive, machine configuration, SSH keys, passwords, and more.</p>
<p>This step is tightly linked to the cloud provider itself. Giants like AWS, Microsoft Azure, Alibaba, and Google Cloud Platform fully embrace automation through a plethora of powerful APIs, but other cloud providers do not seem to care even one iota. Thankfully, this may not be such a big deal for us since we’re managing just three or four servers at any given time. We can easily set them up or clone them from an existing image, and in three <code>docker run</code> commands have a working C2 infrastructure. But if we can acquire a credit card that we do not mind sharing with AWS, we can automate this last tedious setup as well, and in doing so touch upon something that is or should be fundamental to any modern technical environment: infrastructure as code.</p>
<p><em>Infrastructure as code</em> rests upon the idea of having a full declarative description of the components that should be running at any given time, from the name of the machine to the last package installed on it. A tool then parses this description file and corrects any discrepancies observed, such as updating a firewall rule, changing an IP address, attaching more disk, or whatever is needed. If the resource disappears, it’s brought back to life to match the desired state. Sounds magical, right?</p>
<p>Multiple tools will allow you to achieve this level of automation (both at the infrastructure level and the OS level), but the one we will go with is called Terraform from HashiCorp.</p>
<p><em>Terraform</em> is open source and supports a number of cloud providers (listed in the documentation at <a href="https://registry.terraform.io" class="LinkURL">https://registry.terraform.io</a>), which makes it your best shot should you opt for an obscure provider that accepts Zcash. The rest of the chapter will focus on AWS, so you can easily replicate the code and learn to play with Terraform.</p>
<p>I would like to stress that this step is purely optional to begin with. Automating the setup of two or three servers may take more effort than it saves since we already have such a great container setup, but the automating process helps us to explore current DevOps methodology to better understand what to look for once we are in a similar environment.</p>
<p>Terraform, as is the case with all Golang tools, is a statically compiled binary, so we do not need to bother with wicked dependencies. We SSH into our bouncing servers and promptly download the tool, like so:</p>
<pre><code>root@Bouncer:~/# <b>wget\</b>
<b>https://releases.hashicorp.com/terraform/0.12.12/terraform_0.12.12_linux_amd64.zip</b>

root@Bouncer:~/# <b>unzip terraform_0.12.12_linux_amd64.zip</b>
root@Bouncer:~/# <b>chmod +x terraform</b></code></pre>
<p><span epub:type="pagebreak" title="34" id="Page_34"/>Terraform will interact with the AWS Cloud using valid credentials that we provide. Head to AWS IAM—the user management service—to create a programmatic account and grant it full access to all EC2 operations. <em>EC2</em> is the AWS service managing machines, networks, load balancers, and more. You can follow a step-by-step tutorial to create an account on IAM if it’s your first time dealing with AWS by searching at: <a href="https://serverless-stack.com/chapters/" class="LinkURL">https://serverless-stack.com/chapters/</a>.</p>
<p>In the IAM user creation panel, give your newly created user programmatic access, as shown in <a href="#figure3-5" id="figureanchor3-5">Figure 3-5</a>.</p>
<figure>
<img src="image_fi/501263c03/f03005.png" alt="f03005.png"/>
<figcaption><p><a id="figure3-5">Figure 3-5</a>: Creating a user called <em>terraform</em> with access to the AWS API</p></figcaption>
</figure>
<p>Allow the user full control over EC2 to administer machines by attaching the AmazonEC2FullAccess policy, as shown in <a href="#figure3-6" id="figureanchor3-6">Figure 3-6</a>.</p>
<figure>
<img src="image_fi/501263c03/f03006.png" alt="f03006.png"/>
<figcaption><p><a id="figure3-6">Figure 3-6</a>: Attaching the policy AmazonEC2FullAccess to the <em>terraform</em> user</p></figcaption>
</figure>
<p>Download the credentials as a .<em>csv</em> file. Note the access key ID and secret access key, as shown in <a href="#figure3-7" id="figureanchor3-7">Figure 3-7</a>. We’ll need these next.</p>
<figure>
<img src="image_fi/501263c03/f03007.png" alt="f03007.png"/>
<figcaption><p><a id="figure3-7">Figure 3-7</a>: API credentials to query the AWS API</p></figcaption>
</figure>
<p>Once in possession of an AWS access key and secret access key, download the AWS command line tool and save your credentials:</p>
<pre><code>root@Bouncer:~/# <b>apt install awscli</b>

root@Bouncer:~/# <b>aws configure</b>
<span epub:type="pagebreak" title="35" id="Page_35"/>AWS Access Key ID [None]: <b>AKIA44ESW0EAASQDF5A0</b>
AWS Secret Access Key [None]: <b>DEqg5dDxDA4uSQ6xXdhvu7Tzi53</b>...
Default region name [None]: <b>eu-west-1</b></code></pre>
<p>We then set up a folder to host the infrastructure’s configuration:</p>
<pre><code>root@Bouncer:~/# <b>mkdir infra &amp;&amp; cd infra</b></code></pre>
<p>Next, we create two files: <em>provider.tf</em> and <em>main.tf</em>. In the former, we initialize the AWS connector, load the credentials, and assign a default region to the resources we intend to create, such as <code>eu-west-1</code> (Ireland), like so:</p>
<pre><code># provider.tf
provider "aws" {
  region  = "eu-west-1"
  version = "~&gt; 2.28"
}</code></pre>
<p>In <em>main.tf</em> we’ll place the bulk of the definition of our architecture. One of the primordial structures in Terraform is a <em>resource</em><em>—</em>an element describing a discrete unit of a cloud provider’s service, such as a server, an SSH key, a firewall rule, and so on. The level of granularity depends on the cloud service and can quickly grow to an absurd level of complexity, but that’s the price of flexibility.</p>
<p>To ask Terraform to spawn a server, we simply define the <code>aws_instance</code> resource, as shown here:</p>
<pre><code># main.tf
resource "aws_instance" "basic_ec2" {
  ami           = "ami-0039c41a10b230acb"
  instance_type = "t2.micro"
}</code></pre>
<p>Our <code>basic_ec2</code> resource is a server that will launch the Amazon Machine Image (AMI) identified by <code>ami-0039c41a10b230acb</code>, which happens to be an Ubuntu 18.04 image. You can check all the prepared Ubuntu images at <a href="https://cloud-images.ubuntu.com/locator/ec2/" class="LinkURL">https://cloud-images.ubuntu.com/locator/ec2/</a>. The server (or instance) is of type <code>t2.micro</code>, which gives it 1GB of memory and one vCPU.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	The Terraform documentation is very didactic and helpful, so do not hesitate to go through it when building your resources: <a href="https://www.terraform.io/docs/" class="LinkURL">https://www.terraform.io/docs/</a>.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>We save <em>main.tf</em> and initialize Terraform so it can download the AWS provider:</p>
<pre><code>root@Bounce:~/infra# <b>terraform init</b>
Initializing the backend...
Initializing provider plugins...
- Downloading plugin for provider "aws"

Terraform has been successfully initialized!</code></pre>
<p><span epub:type="pagebreak" title="36" id="Page_36"/>Next, we execute the <code>terraform fmt</code> command to format <em>main.tf</em> followed by the <code>plan</code> instruction to build a list of changes about to happen to the infrastructure, as shown next. You can see our server scheduled to come to life with the attributes we defined earlier. Pretty neat.</p>
<pre><code>root@Bounce:~/infra# <b>terraform fmt &amp;&amp; terraform plan</b>
Terraform will perform the following actions:

  # aws_instance.basic_ec2 will be created
  + resource "aws_instance" "basic_ec2" {
      + ami                          = "ami-0039c41a10b230acb"
      + arn                          = (known after apply)
      + associate_public_ip_address  = (known after apply)
      + instance_type                = "t2.micro"
<var>--snip--</var>

Plan: 1 to add, 0 to change, 0 to destroy.</code></pre>
<p>Once we validate these attributes, we call <code>terraform apply</code> to deploy the server on AWS. This operation also locally creates a state file describing the current resource—a single server—we just created.</p>
<p>If we terminate the server manually on AWS and relaunch a <code>terraform apply</code>, it will detect a discrepancy between the local state file and the current state of our EC2 instances. It will resolve such a discrepancy by re-creating the server. If we want to launch nine more servers bearing the same configuration, we set the <code>count</code> property to <code>10</code> and run an <code>apply</code> once more.</p>
<p>Try manually launching and managing 10 or 20 servers on AWS (or any cloud provider for that matter), and you will soon dye your hair green, paint your face white, and start dancing in the streets of NYC. The rest of us using Terraform will update a single number, as shown in <a href="#listing3-4" id="listinganchor3-4">Listing 3-4</a>, and go on with our lives in sanity.</p>
<pre><code># main.tf launching 10 EC2 servers
resource "aws_instance" "basic_ec2" {
  ami           = "ami-0039c41a10b230acb"
  count         = 10
  instance_type = "t2.micro"
}</code></pre>
<p class="CodeListingCaption"><a id="listing3-4">Listing 3-4</a>: Minimal code to create 10 EC2 instances using Terraform</p>
<h3 id="h2-501263c03-0004">Tuning the Server</h3>
<p class="BodyFirst">Our server so far is pretty basic. Let’s fine-tune it by setting the following properties:</p>
<ul>
<li>An SSH key so we can administer it remotely, which translates to a Terraform resource called <code>aws_key_pair</code>.</li>
<li>A set of firewall rules—known as <em>security groups</em> in AWS terminology—to control which servers are allowed to talk to each other and how. This is <span epub:type="pagebreak" title="37" id="Page_37"/>defined by the Terraform resource <code>aws_security_group</code>. Security groups need to be attached to a <em>virtual private cloud (VPC</em><em>)</em>, a sort of virtualized network. We just use the default one created by AWS.</li>
<li>A public IP assigned to each server.</li>
</ul>
<p><a href="#listing3-5" id="listinganchor3-5">Listing 3-5</a> shows <em>main.tf</em> with those properties set.</p>
<pre><code># main.tf – compatible with Terraform 0.12 only

# We copy-paste our SSH public key
<span class="CodeAnnotationHang" aria-label="annotation1">1</span> resource "aws_key_pair" "ssh_key" {
  key_name   = "mykey"
  public_key = "ssh-rsa AAAAB3NzaC1yc2EAAA..."
}

# Empty resource, since the default AWS VPC (network) already exists
resource "aws_default_vpc" "default" {
}

# Firewall rule to allow SSH from our bouncing server IP only
# All outgoing traffic is allowed
<span class="CodeAnnotationHang" aria-label="annotation2">2</span> resource "aws_security_group" "SSHAdmin" {
  name        = "SSHAdmin"
  description = "SSH traffic"
  vpc_id      = aws_default_vpc.default.id
  ingress {
    from_port   = 0
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["123.123.123.123/32"]
  }
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# We link the SSH key and security group to our basic_ec2 server

resource "aws_instance" "basic_ec2" {
  ami           = "ami-0039c41a10b230acb"
  instance_type = "t2.micro"

  vpc_security_group_ids     = aws_security_group.SSHAdmin.id
<span class="CodeAnnotationHang" aria-label="annotation3">3</span> key_name                   = aws.ssh_key.id
  associate_public_ip_address= "true"
  root_block_device {
    volume_size = "25"
  }
}

<span epub:type="pagebreak" title="38" id="Page_38"/># We print the server's public IP
output "public_ip " {
  value = aws_instance.basic_ec2.public_ip
}</code></pre>
<p class="CodeListingCaption"><a id="listing3-5">Listing 3-5</a>: Adding some properties to <em>main.tf</em></p>
<p>As stated previously, the <code>aws_key_pair</code> registers an SSH key on AWS <span class="CodeAnnotation" aria-label="annotation1">1</span>, which gets injected into the server on the first boot. Every resource on Terraform can later be referenced through its ID variable, which is populated at runtime—in this case, <code>aws.ssh_key.id</code> <span class="CodeAnnotation" aria-label="annotation3">3</span>. The structure of these special variables is always the same: <var>resourceType.resourceName.internalVariable</var>.</p>
<p>The <code>aws_security_group</code> presents no novelty <span class="CodeAnnotation" aria-label="annotation2">2</span>, except perhaps for the reference to the default VPC, which is the default virtual network segment created by AWS (akin to a router’s interface, if you will). The firewall rules allow incoming SSH traffic from our bouncing server only.</p>
<p>We launch another <code>plan</code> command so we can make sure all properties and resources match our intended outcome, as shown in <a href="#listing3-6" id="listinganchor3-6">Listing 3-6</a>.</p>
<pre><code>root@Bounce:~/infra# <b>terraform fmt &amp;&amp; terraform plan</b>
Terraform will perform the following actions:

  # aws_instance.basic_ec2 will be created
  + resource "aws_key_pair" "ssh_key2" {
      + id          = (known after apply)
      + key_name    = "mykey2"
      + public_key  = "ssh-rsa AAAAB3NzaC1yc2..."
    }

  + resource "aws_security_group" "SSHAdmin" {
      + arn                    = (known after apply)
      + description            = "SSH admin from bouncer"
      + id                     = (known after apply)
--<var>snip</var>--
   }

  + resource "aws_instance" "basic_ec2" {
      + ami                          = "ami-0039c41a10b230acb"
      + arn                          = (known after apply)
      + associate_public_ip_address  = true
      + id                           = (known after apply)
      + instance_type                = "t2.micro"
<var>--snip--</var>

Plan: 3 to add, 0 to change, 0 to destroy.</code></pre>
<p class="CodeListingCaption"><a id="listing3-6">Listing 3-6</a>: Checking that the properties are well defined</p>
<p>Terraform will create three resources. Great.</p>
<p>As one last detail, we need to instruct AWS to install Docker and launch our container, Nginx, when the machine is up and running. AWS leverages the <code>cloud-init</code> package installed on most Linux distributions to execute a <span epub:type="pagebreak" title="39" id="Page_39"/>script when the machine first boots. This is in fact how AWS injects the public key we defined earlier. This script is referred to as “user data.”</p>
<p>Alter <em>main.tf</em> to add bash commands to install Docker and execute the container, as shown in <a href="#listing3-7" id="listinganchor3-7">Listing 3-7</a>.</p>
<pre><code>resource "aws_instance" "basic_ec2" {
<var>--snip--</var>
<span class="CodeAnnotationHang" aria-label="annotation1">1</span> user_data = &lt;&lt;EOF

#!/bin/bash
DOMAIN="www.linux-update-packets.org";
C2IP="172.31.31.13";

sleep 10
sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"
apt update
apt install -y docker-ce
docker run -dti -p80:80 -p443:443 \
-e DOMAIN="www.customdomain.com" \
-e C2IP="$C2IP" \
-v /opt/letsencrypt:/etc/letsencrypt \
sparcflow/nginx

EOF
}</code></pre>
<p class="CodeListingCaption"><a id="listing3-7">Listing 3-7</a>: Launching the container from <em>main.tf</em></p>
<p>The EOF block <span class="CodeAnnotation" aria-label="annotation1">1</span> holds a multiline string that makes it easy to inject environment variables whose values are produced by other Terraform resources. In this example we hardcode the C2’s IP and domain name, but in real life these will be the output of other Terraform resources in charge of spinning up backend C2 servers.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	Instead of hardcoding the domain name in <a href="#listing3-7">Listing 3-7</a>, we could further extend Terraform to automatically create and manage DNS records using the Namecheap provider, for instance: <a href="https://github.com/adamdecaf/terraform-provider-namecheap" class="LinkURL">https://github.com/adamdecaf/terraform-provider-namecheap</a>.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<h3 id="h2-501263c03-0005">Pushing to Production</h3>
<p class="BodyFirst">We’re now ready to push this into production with a simple <code>terraform apply</code>, which will spill out the plan once more and request manual confirmation before contacting AWS to create the requested resources:</p>
<pre><code>root@Bounce:~/infra# <b>terraform fmt &amp;&amp; terraform apply</b>

aws_key_pair.ssh_key: Creation complete after 0s [id=mykey2]
aws_default_vpc.default: Modifications complete after 1s [id=vpc-b95e4bdf]
--<var>snip</var>--
aws_instance.basic_ec2: Creating...
<span epub:type="pagebreak" title="40" id="Page_40"/>aws_instance.basic_ec2: Creation complete after 32s [id=i-089f2eff84373da3d]

Apply complete! Resources: 3 added, 0 changed, 0 destroyed.
Outputs:

public_ip = 63.xx.xx.105</code></pre>
<p>Awesome. We can SSH into the instance using the default <code>ubuntu</code> username and the private SSH key to make sure everything is running smoothly:</p>
<pre><code>root@Bounce:~/infra# <b>ssh -i .ssh/id_rsa ubuntu@63.xx.xx.105</b>

Welcome to Ubuntu 18.04.2 LTS (GNU/Linux 4.15.0-1044-aws x86_64)

ubuntu@ip-172-31-30-190:~$ <b>docker ps</b>
CONTAINER ID        IMAGE            COMMAND
5923186ffda5        sparcflow/ngi...   "/bin/bash /sc..."</code></pre>
<p>Perfect. Now that we’ve completely automated the creation, setup, and tuning of a server, we can unleash our inner wildling and duplicate this piece of code to spawn as many servers as necessary, with different firewall rules, user data scripts, and any other settings. A more civilized approach, of course, would be to wrap the code we have just written in a Terraform module and pass it different parameters according to our needs. For details, look in <em>infra/ec2_module</em> in the book’s repository at <a href="http://www.nostarch.com/how-hack-ghost/" class="LinkURL">www.nostarch.com/how-hack-ghost</a>.</p>
<p>I will not go through the refactoring process step-by-step in this already dense chapter. Refactoring would be mostly cosmetic, like defining variables in a separate file, creating multiple security groups, passing private IPs as variables in user data scripts, and so on. I trust that by now you have enough working knowledge to pull the final refactored version from the GitHub repository and play with it to your heart’s content.</p>
<p>The main goal of this chapter was to show you how we can spring up a fully functioning attacking infrastructure in exactly 60 seconds, for that is the power of this whole maneuver: automated reproducibility, which no amount of point-and-click actions can give you.</p>
<p>We deploy our attacking servers with just a few commands:</p>
<pre><code>root@Bounce:~# <b>git clone <var class="bold">your_repo</var></b>
root@Bounce:~# <b>cd infra &amp;&amp; terraform init</b>
#update a few variables
root@Bounce:~# <b>terraform apply</b>
<var>--snip--</var>

Apply complete! Resources: 7 added, 0 changed, 0 destroyed.
Outputs:

nginx_ip_address = 63.xx.xx.105
c2_ip_address = 63.xx.xx.108</code></pre>
<p>Our infrastructure is finally ready!</p>
<h2 id="h1-501263c03-0005"><span epub:type="pagebreak" title="41" id="Page_41"/>Resources</h2>
<ul>
<li>Check out Taylor Brown’s article “Bringing Docker to Windows Developers with Windows Server Containers” at <a href="http://bit.ly/2FoW0nI" class="LinkURL">http://bit.ly/2FoW0nI</a>.</li>
<li>Find a great post about the proliferation of container runtimes at <em/><a href="http://bit.ly/2ZVRGpy" class="LinkURL">http://bit.ly/2ZVRGpy</a><em>.</em></li>
<li>Liz Rice demystifies runtimes by coding one in real time in her talk, “Building a Container from Scratch in Go,” available on YouTube.</li>
<li>Scott Lowe offers a short practical introduction to network namespaces at <a href="https://blog.scottlowe.org/" class="LinkURL">https://blog.scottlowe.org/</a>.</li>
<li>Jérôme Petazzoni provides lots more information about namespaces, cgroups, and UFS: available on YouTube.</li>
</ul>
</section>
</body></html>