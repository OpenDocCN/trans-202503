- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Importing and Exporting Data
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的导入与导出
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: So far, you’ve learned how to add a handful of rows to a table using SQL `INSERT`
    statements. A row-by-row insert is useful for making quick test tables or adding
    a few rows to an existing table. But it’s more likely you’ll need to load hundreds,
    thousands, or even millions of rows, and no one wants to write separate `INSERT`
    statements in those situations. Fortunately, you don’t have to.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学会了如何通过 SQL `INSERT` 语句向表中添加几行数据。逐行插入适用于快速创建测试表或向现有表中添加少量数据。但更常见的是，你可能需要加载数百、数千甚至数百万行数据，而在这种情况下，没人愿意写出一条条的
    `INSERT` 语句。幸运的是，你不需要这样做。
- en: If your data exists in a *delimited* text file, with one table row per line
    of text and each column value separated by a comma or other character, PostgreSQL
    can import the data in bulk via its `COPY` command. This command is a PostgreSQL-specific
    implementation with options for including or excluding columns and handling various
    delimited text types.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据存在于一个*分隔*文本文件中，每行文本表示一个表的行，每列值由逗号或其他字符分隔，PostgreSQL 可以通过 `COPY` 命令批量导入数据。这个命令是
    PostgreSQL 特有的实现，具有包括或排除列以及处理不同分隔文本类型的选项。
- en: In the opposite direction, `COPY` will also *export* data from PostgreSQL tables
    or from the result of a query to a delimited text file. This technique is handy
    when you want to share data with colleagues or move it into another format, such
    as an Excel file.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在相反的方向上，`COPY` 也可以从 PostgreSQL 表中或查询结果中*导出*数据到分隔文本文件。当你想与同事共享数据或将数据转移到其他格式（如
    Excel 文件）时，这个技术非常方便。
- en: 'I briefly touched on `COPY` for export in the “Understanding Characters” section
    of Chapter 4, but in this chapter, I’ll discuss import and export in more depth.
    For importing, I’ll start by introducing you to one of my favorite datasets: annual
    US Census population estimates by county.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我在第4章的“理解字符”部分简要提到了 `COPY` 导出，但在这一章，我将更深入地讨论导入和导出。对于导入部分，我将以我最喜欢的数据集之一为例：美国按县划分的年度人口普查估算数据。
- en: 'Three steps form the outline of most of the imports you’ll do:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 三个步骤构成了你将执行的大多数导入操作的概述：
- en: Obtain the source data in the form of a delimited text file.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取源数据，格式为分隔文本文件。
- en: Create a table to store the data.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个表来存储数据。
- en: Write a `COPY` statement to perform the import.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个 `COPY` 语句来执行导入。
- en: After the import is done, we’ll check the data and look at additional options
    for importing and exporting.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 导入完成后，我们将检查数据并探讨更多的导入和导出选项。
- en: A delimited text file is the most common file format that’s portable across
    proprietary and open source systems, so we’ll focus on that file type. If you
    want to transfer data from another database program’s proprietary format directly
    to PostgreSQL—for example, from Microsoft Access or MySQL—you’ll need to use a
    third-party tool. Check the PostgreSQL wiki at [https://wiki.postgresql.org/wiki/](https://wiki.postgresql.org/wiki/)
    and search for “Converting from other databases to PostgreSQL” for a list of tools
    and options.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 分隔文本文件是最常见的文件格式，具有良好的跨专有系统和开源系统的可移植性，因此我们将重点关注这种文件类型。如果你想将其他数据库程序的专有格式的数据直接传输到
    PostgreSQL——例如，从 Microsoft Access 或 MySQL——你将需要使用第三方工具。请访问 PostgreSQL 的维基页面 [https://wiki.postgresql.org/wiki/](https://wiki.postgresql.org/wiki/)，搜索“从其他数据库转换到
    PostgreSQL”，获取工具和选项的列表。
- en: If you’re using SQL with another database manager, check the other database’s
    documentation for how it handles bulk imports. The MySQL database, for example,
    has a `LOAD DATA INFILE` statement, and Microsoft’s SQL Server has its own `BULK
    INSERT` command.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在使用 SQL 与其他数据库管理系统，查看该数据库的文档，了解它如何处理批量导入。例如，MySQL 数据库有 `LOAD DATA INFILE`
    语句，微软的 SQL Server 也有自己的 `BULK INSERT` 命令。
- en: Working with Delimited Text Files
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用分隔文本文件
- en: Many software applications store data in a unique format, and translating one
    data format to another is about as easy as trying to read the Cyrillic alphabet
    when one understands only English. Fortunately, most software can import from
    and export to a delimited text file, which is a common data format that serves
    as a middle ground.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 许多软件应用程序以独特的格式存储数据，将一种数据格式转换为另一种格式就像是尝试在只懂英语的情况下阅读西里尔字母一样困难。幸运的是，大多数软件都可以导入和导出分隔文本文件，这是一种常见的数据格式，起到了中介的作用。
- en: A delimited text file contains rows of data, each of which represents one row
    in a table. In each row, each data column is separated, or delimited, by a particular
    character. I’ve seen all kinds of characters used as delimiters, from ampersands
    to pipes, but the comma is most commonly used; hence the name of a file type you’ll
    see often is *comma-separated values (CSV)*. The terms *CSV* and *comma-delimited*
    are interchangeable.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 分隔文本文件包含多行数据，每一行表示表中的一行数据。在每一行中，每个数据列由特定字符分隔或界定。我见过许多不同的字符作为分隔符，从&符号到管道符号都有，但逗号是最常用的；因此，您常常会看到的文件类型名称是*逗号分隔值（CSV）*。术语*CSV*和*逗号分隔*可以互换使用。
- en: 'Here’s a typical data row you might see in a comma-delimited file:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是您可能在逗号分隔文件中看到的典型数据行：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Notice that a comma separates each piece of data—first name, last name, street,
    town, state, and phone—without any spaces. The commas tell the software to treat
    each item as a separate column, upon either import or export. Simple enough.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每一项数据—名字、姓氏、街道、城市、州和电话—之间都用逗号分隔，没有空格。逗号告诉软件将每一项作为单独的列处理，无论是导入还是导出。很简单。
- en: Handling Header Rows
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理标题行
- en: 'A feature you’ll often find inside a delimited text file is a *header row*.
    As the name implies, it’s a single row at the top, or *head*, of the file that
    lists the name of each data column. Often, a header is added when data is exported
    from a database or a spreadsheet. Here’s an example with the delimited row I’ve
    been using. Each item in a header row corresponds to its respective column:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你常常会在分隔文本文件中找到一个*标题行*。顾名思义，它是文件顶部，或者说*头部*的单行，用于列出每个数据列的名称。通常，在数据从数据库或电子表格导出时，会添加标题行。以下是我一直在使用的带有分隔符行的示例。标题行中的每一项都对应其相应的列：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Header rows serve a few purposes. For one, the values in the header row identify
    the data in each column, which is particularly useful when you’re deciphering
    a file’s contents. Second, some database managers (although not PostgreSQL) use
    the header row to map columns in the delimited file to the correct columns in
    the import table. PostgreSQL doesn’t use the header row, so we don’t want to import
    that row to a table. We use the `HEADER` option in the `COPY` command to exclude
    it. I’ll cover this with all `COPY` options in the next section.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 标题行有几个作用。首先，标题行中的值标识了每列的数据，这在解读文件内容时特别有用。其次，一些数据库管理系统（虽然PostgreSQL不使用）会用标题行将分隔文件中的列映射到导入表中的正确列。PostgreSQL不使用标题行，因此我们不希望将该行导入到表中。我们使用`HEADER`选项在`COPY`命令中排除它。在下一节中，我将介绍所有`COPY`选项。
- en: Quoting Columns That Contain Delimiters
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引用包含分隔符的列
- en: 'Using commas as a column delimiter leads to a potential dilemma: what if the
    value in a column includes a comma? For example, sometimes people combine an apartment
    number with a street address, as in 123 Main St., Apartment 200\. Unless the system
    for delimiting accounts for that extra comma, during import the line will appear
    to have an extra column and cause the import to fail.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用逗号作为列分隔符会导致一个潜在的难题：如果列中的值包含逗号怎么办？例如，有时人们会将公寓号与街道地址结合在一起，如123 Main St., Apartment
    200\。除非分隔符系统能够考虑到这个额外的逗号，否则在导入时，行会被认为有额外的列，导致导入失败。
- en: 'To handle such cases, delimited files use an arbitrary character called a *text
    qualifier* to enclose a column that includes the delimiter character. This acts
    as a signal to ignore that delimiter and treat everything between the text qualifiers
    as a single column. Most of the time in comma-delimited files the text qualifier
    used is the double quote. Here’s the example data again, but with the street name
    column surrounded by double quotes:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这种情况，分隔符文件使用一种称为*文本限定符*的任意字符，将包含分隔符字符的列括起来。这就像一个信号，告诉程序忽略该分隔符，并将文本限定符之间的内容视为单一列。在逗号分隔文件中，常用的文本限定符是双引号。以下是再次提供的示例数据，但街道名称列被双引号括起来：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: On import, the database will recognize that double quotes signify one column
    regardless of whether it finds a delimiter within the quotes. When importing CSV
    files, PostgreSQL by default ignores delimiters inside double-quoted columns,
    but you can specify a different text qualifier if your import requires it. (And,
    given the sometimes-odd choices made by IT professionals, you may indeed need
    to employ a different character.)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入时，数据库将识别双引号无论是否包含分隔符，都会表示为一列。当导入CSV文件时，PostgreSQL默认会忽略双引号列中的分隔符，但如果导入需要，您可以指定不同的文本限定符。（并且鉴于IT专业人员有时会做出奇怪的选择，您可能确实需要使用不同的字符。）
- en: 'Finally, in CSV mode, if PostgreSQL finds two consecutive text qualifiers inside
    a double-quoted column, it will remove one. For example, let’s say PostgreSQL
    finds this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在 CSV 模式下，如果 PostgreSQL 在双引号列中发现两个连续的文本限定符，它会去掉一个。例如，假设 PostgreSQL 发现了如下内容：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If so, it will treat that text as a single column upon import, leaving just
    one of the qualifiers:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是这样，它将在导入时将该文本视为单一列，只保留一个限定符：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A situation like that could indicate an error in the formatting of your CSV
    file, which is why, as you’ll see later, it’s always a good idea to review your
    data after importing.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的情况可能表明你的 CSV 文件格式存在错误，这也是为什么，正如你稍后会看到的，导入后检查数据总是一个好主意。
- en: Using COPY to Import Data
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 COPY 导入数据
- en: To import data from an external file into our database, we first create a table
    in our database that matches the columns and data types in our source file. Once
    that’s done, the `COPY` statement for the import is just the three lines of code
    in [Listing 5-1](#listing5-1).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要将数据从外部文件导入到我们的数据库中，首先需要在数据库中创建一个与源文件中的列和数据类型匹配的表。完成后，导入的 `COPY` 语句就是 [清单 5-1](#listing5-1)
    中的三行代码。
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Listing 5-1: Using `COPY` for data import'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 5-1：使用 `COPY` 进行数据导入
- en: We start the block of code with the `COPY` keyword 1 followed by the name of
    the target table, which must already exist in your database. Think of this syntax
    as meaning, “Copy data to my table called `table_name`.”
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用 `COPY` 关键字 1 开始代码块，后跟目标表的名称，该表必须已经在数据库中存在。可以把这个语法理解为“将数据复制到名为 `table_name`
    的表中”。
- en: 'The `FROM` keyword 2 identifies the full path to the source file, and we enclose
    the path in single quotes. The way you designate the path depends on your operating
    system. For Windows, begin with the drive letter, colon, backslash, and directory
    names. For example, to import a file located on my Windows desktop, the `FROM`
    line would read as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`FROM` 关键字 2 确定源文件的完整路径，我们用单引号将路径括起来。如何指定路径取决于你的操作系统。对于 Windows，从驱动器字母、冒号、反斜杠和目录名称开始。例如，要导入位于
    Windows 桌面的文件，`FROM` 行会如下所示：'
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'On macOS or Linux, start at the system root directory with a forward slash
    and proceed from there. Here’s what the `FROM` line might look like when importing
    a file located on my macOS desktop:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 macOS 或 Linux 上，从系统根目录开始并使用正斜杠，然后从那里继续。以下是当我从 macOS 桌面导入文件时，`FROM` 行可能的样子：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For the examples in the book, I use the Windows-style path `C:\YourDirectory\`
    as a placeholder. Replace that with the path where you stored the CSV file you
    downloaded from GitHub.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在书中的示例中，我使用 Windows 风格的路径 `C:\YourDirectory\` 作为占位符。请将其替换为你从 GitHub 下载的 CSV
    文件存储的路径。
- en: 'The `WITH` keyword 3 lets you specify options, surrounded by parentheses, that
    you use to tailor your input or output file. Here we specify that the external
    file should be comma-delimited and that we should exclude the file’s header row
    in the import. It’s worth examining all the options in the official PostgreSQL
    documentation at [https://www.postgresql.org/docs/current/sql-copy.html](https://www.postgresql.org/docs/current/sql-copy.html),
    but here is a list of the options you’ll commonly use:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`WITH` 关键字 3 让你指定选项，这些选项被括在圆括号中，用于定制你的输入或输出文件。在这里，我们指定外部文件应使用逗号分隔，并且在导入时应排除文件的表头行。值得查看官方
    PostgreSQL 文档中的所有选项，链接是 [https://www.postgresql.org/docs/current/sql-copy.html](https://www.postgresql.org/docs/current/sql-copy.html)，但这里列出了你常用的一些选项：'
- en: '**Input and output file format**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入和输出文件格式**'
- en: Use the `FORMAT` `format_name` option to specify the type of file you’re reading
    or writing. Format names are `CSV`, `TEXT`, or `BINARY`. Unless you’re deep into
    building technical systems, you’ll rarely encounter a need to work with `BINARY`,
    where data is stored as a sequence of bytes. More often, you’ll work with standard
    CSV files. In the `TEXT` format, a *tab* character is the delimiter by default
    (although you can specify another character), and backslash characters such as
    `\r` are recognized as their ASCII equivalents—in this case, a carriage return.
    The `TEXT` format is used mainly by PostgreSQL’s built-in backup programs.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `FORMAT` `format_name` 选项来指定你正在读取或写入的文件类型。格式名称有 `CSV`、`TEXT` 或 `BINARY`。除非你深入构建技术系统，否则很少会遇到需要处理
    `BINARY` 格式的情况，因为数据是以字节序列存储的。更常见的是，你将使用标准的 CSV 文件。在 `TEXT` 格式中，默认的分隔符是 *制表符*（尽管你可以指定其他字符），而像
    `\r` 这样的反斜杠字符会被识别为其 ASCII 等效值——在这种情况下是回车符。`TEXT` 格式主要由 PostgreSQL 内置的备份程序使用。
- en: '**Presence of a header row**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**表头行的存在**'
- en: On import, use `HEADER` to specify that the source file has a header row that
    you want to exclude. The database will start importing with the second line of
    the file so that the column names in the header don’t become part of the data
    in the table. (Be sure to check your source CSV to make sure this is what you
    want; not every CSV comes with a header row!) On export, using `HEADER` tells
    the database to include the column names as a header row in the output file, which
    helps a user understand the file’s contents.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在导入时，使用`HEADER`来指定源文件包含一个你希望排除的标题行。数据库将从文件的第二行开始导入，这样标题行中的列名就不会成为表格中的数据部分。（务必检查源CSV文件，确保这是你想要的；并非每个CSV文件都有标题行！）在导出时，使用`HEADER`告诉数据库将列名作为输出文件中的标题行包含进去，这有助于用户理解文件的内容。
- en: '**Delimiter**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**分隔符**'
- en: 'The `DELIMITER` `''``character``''` option lets you specify which character
    your import or export file uses as a delimiter. The delimiter must be a single
    character and cannot be a carriage return. If you use `FORMAT CSV`, the assumed
    delimiter is a comma. I include `DELIMITER` here to show that you have the option
    to specify a different delimiter if that’s how your data arrived. For example,
    if you received pipe-delimited data, you would treat the option this way: `DELIMITER
    ''|''`.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DELIMITER` `''``character``''`选项允许你指定导入或导出文件使用的分隔符字符。分隔符必须是单一字符，不能是回车符。如果你使用`FORMAT
    CSV`，默认的分隔符是逗号。这里包括`DELIMITER`是为了说明，如果数据采用了其他分隔符，你也可以选择不同的分隔符。例如，如果你收到了管道符分隔的数据，可以按如下方式设置：`DELIMITER
    ''|''`。'
- en: '**Quote character**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**引号字符**'
- en: Earlier, you learned that in a CSV file, commas inside a single column value
    will mess up your import unless the column value is surrounded by a character
    that serves as a text qualifier, telling the database to handle the value within
    as one column. By default, PostgreSQL uses the double quote, but if the CSV you’re
    importing uses a different character for the text qualifier, you can specify it
    with the `QUOTE` `'``quote_character``'` option.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之前，你学习过在CSV文件中，如果一个单独列的值中包含逗号，会导致导入混乱，除非该列的值被某个字符包围，这个字符作为文本限定符，告诉数据库将该值作为一个完整的列处理。默认情况下，PostgreSQL使用双引号，但如果你导入的CSV文件使用了其他字符作为文本限定符，可以通过`QUOTE`
    `'``quote_character``'`选项指定该字符。
- en: Now that you better understand delimited files, you’re ready to import one.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你对分隔文件有了更好的理解，可以开始导入了。
- en: Importing Census Data Describing Counties
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入描述县的数据
- en: 'The dataset you’ll work with in this import exercise is considerably larger
    than the `teachers` table you made in Chapter 2. It contains census population
    estimates for every county in the United States and is 3,142 rows deep and 16
    columns wide. (Census counties include some geographies with other names: parishes
    in Louisiana, boroughs and census areas in Alaska, and cities, particularly in
    Virginia.)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个导入练习中，你将使用的数据集比第二章中创建的`teachers`表要大得多。它包含美国每个县的普查人口估算，共有3142行，16列宽。（普查中的县包括一些其他名称的地理区域：路易斯安那州的教区、阿拉斯加州的区和普查区，以及特别是在弗吉尼亚州的城市。）
- en: To understand the data, it helps to know a little about the US Census Bureau,
    a federal agency that tracks the nation’s demographics. Its best-known program
    is a full count of the population it undertakes every 10 years, most recently
    in 2020\. That data, which enumerates the age, gender, race, and ethnicity of
    each person in the country, is used to determine how many members from each state
    make up the 435-member US House of Representatives. In recent decades, faster-growing
    states such as Texas and Florida have gained seats, while slower-growing states
    such as New York and Ohio have lost representatives in the House.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解数据，了解一点美国普查局的情况会有所帮助。普查局是一个联邦机构，负责追踪全国人口统计数据。它最著名的项目是每十年进行一次的全国人口普查，最近一次是在2020年进行的。这些数据列出了每个国家的人口年龄、性别、种族和民族信息，用于确定每个州在435个席位的美国众议院中应有多少代表。近年来，像德克萨斯州和佛罗里达州这样的快速增长州获得了更多席位，而像纽约州和俄亥俄州这样增长缓慢的州则失去了代表席位。
- en: The data we’ll work with are the census’ annual population estimates. These
    use the most recent 10-year census count as a base, and they factor in births,
    deaths, and domestic and international migration to produce population estimates
    each year for the nation, states, counties, and other geographies. In lieu of
    an annual physical count, it’s the best way to get an updated measure on how many
    people live where in the United States. For this exercise, I compiled select columns
    from the 2019 US Census county-level population estimates (plus a few descriptive
    columns from census geographic data) into a file named *us_counties_pop_est_2019.csv*.
    You should have this file on your computer if you followed the directions in the
    section “Downloading Code and Data from GitHub” in Chapter 1. If not, go back
    and do that now.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据是人口普查的年度人口估算。这些估算使用最近一次的十年人口普查数据作为基础，并且考虑了出生、死亡以及国内外迁移，以每年为国家、州、县和其他地理区域估算人口。由于缺乏年度的实地人口统计数据，它是获取美国各地人口更新数据的最佳方式。对于这个练习，我将从2019年美国人口普查的县级人口估算数据中选取了几列（以及来自人口普查地理数据的几列描述性列），并将它们编译成一个名为*us_counties_pop_est_2019.csv*的文件。如果你按照第一章中“从GitHub下载代码和数据”部分的说明操作，你应该在电脑上有这个文件。如果没有，现在去下载它。
- en: 'Open the file with a text editor. You should see a header row that begins with
    these columns:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本编辑器打开文件。你应该会看到一行标题，包含以下列：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let’s explore the columns by examining the code for creating the import table.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过检查创建导入表的代码来探索这些列。
- en: Creating the us_counties_pop_est_2019 Table
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建`us_counties_pop_est_2019`表
- en: The code in [Listing 5-2](#listing5-2) shows the `CREATE TABLE` script. In pgAdmin
    click the `analysis` database that you created in Chapter 2. (It’s best to store
    the data in this book in `analysis` because we’ll reuse some of it in later chapters.)
    From the pgAdmin menu bar, select **Tools**▶**Query Tool**. You can type the code
    into the tool or copy and paste it from the files you downloaded from GitHub.
    Once you have the script in the window, run it.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表5-2](#listing5-2)中的代码展示了`CREATE TABLE`脚本。在pgAdmin中点击你在第二章创建的`analysis`数据库。（最好将本书中的数据存储在`analysis`中，因为我们将在后续章节中重用其中的一部分。）在pgAdmin的菜单栏中，选择**工具**▶**查询工具**。你可以将代码输入到工具中，或者从你从GitHub下载的文件中复制并粘贴。将脚本放入窗口后，运行它。'
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Listing 5-2: `CREATE TABLE` statement for census county population estimates'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5-2：人口普查县级人口估算的`CREATE TABLE`语句
- en: 'Return to the main pgAdmin window, and in the object browser, right-click and
    refresh the `analysis` database. Choose **Schemas**▶**public**▶**Tables** to see
    the new table. Although it’s empty, you can see the structure by running a basic
    `SELECT` query in pgAdmin’s Query Tool:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到pgAdmin的主窗口，在对象浏览器中，右键点击并刷新`analysis`数据库。选择**架构**▶**public**▶**表**以查看新创建的表。虽然它为空，但你可以通过在pgAdmin查询工具中运行基本的`SELECT`查询来查看其结构：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: When you run the `SELECT` query, you’ll see the columns in the table you created
    appear in the pgAdmin Data Output pane. No data rows exist yet. We need to import
    them.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行`SELECT`查询时，你会看到你创建的表格的列出现在pgAdmin的数据输出窗格中。目前还没有数据行，我们需要导入它们。
- en: Understanding Census Columns and Data Types
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解人口普查的列和数据类型
- en: 'Before we import the CSV file into the table, let’s walk through several of
    the columns and the data types I chose in [Listing 5-2](#listing5-2). As my guide,
    I used two official census data dictionaries: one for the estimates found at [https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2010-2019/co-est2019-alldata.pdf](https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2010-2019/co-est2019-alldata.pdf)
    and one for the decennial count that includes the geographic columns at [http://www.census.gov/prod/cen2010/doc/pl94-171.pdf](http://www.census.gov/prod/cen2010/doc/pl94-171.pdf).
    I’ve given some columns more readable names in the table definition. Relying on
    a data dictionary when possible is good practice, because it helps you avoid misconfiguring
    columns or potentially losing data. Always ask if one is available, or do an online
    search if the data is public.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在将CSV文件导入到表格之前，让我们先通过[清单5-2](#listing5-2)中的几个列和我所选择的数据类型进行说明。作为指南，我使用了两个官方的人口普查数据字典：一个是用于估算数据的，链接为[https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2010-2019/co-est2019-alldata.pdf](https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2010-2019/co-est2019-alldata.pdf)，另一个是包含地理列的十年一次的普查计数，链接为[http://www.census.gov/prod/cen2010/doc/pl94-171.pdf](http://www.census.gov/prod/cen2010/doc/pl94-171.pdf)。在表格定义中，我给一些列起了更易读的名称。尽可能依赖数据字典是一个好习惯，因为它可以帮助你避免错误配置列或可能丢失数据。始终询问是否有数据字典可用，或者如果数据是公开的，进行在线搜索。
- en: In this set of census data, and thus the table you just made, each row displays
    the population estimates and components of annual change (births, deaths, and
    migration) for one county. The first two columns are the county’s `state_fips`
    1 and `county_fips`, which are the standard federal codes for those entities.
    We use `text` for both because those codes can contain leading zeros that would
    be lost if we stored the values as integers. For example, Alaska’s `state_fips`
    is `02`. If we used an integer type, that leading `0` would be stripped on import,
    leaving `2`, which is the wrong code for the state. Also, we won’t be doing any
    math with this value, so don’t need integers. It’s always important to distinguish
    codes from numbers; these state and county values are actually labels as opposed
    to numbers used for math.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一组人口普查数据中，以及你刚刚创建的表格中，每一行都显示了某个县的年度变化（出生、死亡和迁移）的估算人口和组成部分。前两列是县的`state_fips`
    1 和 `county_fips`，这是这些实体的标准联邦代码。我们使用`text`类型来存储这两个字段，因为这些代码可能包含前导零，如果我们将其存储为整数类型，前导零会丢失。例如，阿拉斯加的`state_fips`是`02`。如果我们使用整数类型，这个前导的`0`在导入时会被去除，留下`2`，这就不是阿拉斯加的正确代码了。此外，我们不会对这个值进行任何数学运算，所以不需要使用整数类型。区分代码和数字非常重要；这些州和县的值实际上是标签，而不是用于计算的数字。
- en: 'Numbers from 1 to 4 in `region` 2 represent the general location of a county
    in the United States: the Northeast, Midwest, South, or West. No number is higher
    than 4, so we define the columns with type `smallint`. The `state_name` 3 and
    `county_name` columns contain the complete name of both the state and county,
    stored as `text`.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`region` 2中的数字从1到4代表了美国县的大致位置：东北、中西部、南部或西部。没有数字大于4，因此我们定义这些列的类型为`smallint`。`state_name`
    3 和 `county_name`列包含州和县的完整名称，存储为`text`类型。'
- en: The number of square meters for land and water in the county are recorded in
    `area_land` 4 and `area_water`, respectively. The two, combined, comprise a county’s
    total area. In certain places—such as Alaska, where there’s lots of land to go
    with all that snow—some values easily surpass the `integer` type’s maximum of
    2,147,483,647\. For that reason, we’re using `bigint`, which will handle the 377,038,836,685
    square meters of land in the Yukon-Koyukuk census area with room to spare.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 县的土地和水域的平方米数分别记录在`area_land` 4 和 `area_water`中。两者合起来构成县的总面积。在某些地方——比如阿拉斯加，那里有大量的土地与积雪——一些数值很容易超出`integer`类型的最大值2,147,483,647。因此，我们使用了`bigint`类型，它能够轻松处理如育空-科尤库克人口普查区的377,038,836,685平方米的土地面积。
- en: The latitude and longitude of a point near the center of the county, called
    an *internal point*, are specified in `internal_point_lat` and `internal_point_lon`
    5, respectively. The Census Bureau—along with many mapping systems—expresses latitude
    and longitude coordinates using a *decimal degrees* system. *Latitude* represents
    positions north and south on the globe, with the equator at 0 degrees, the North
    Pole at 90 degrees, and the South Pole at −90 degrees.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一个接近县中心的点的纬度和经度，被称为*内部点*，分别在`internal_point_lat`和`internal_point_lon`中指定。人口普查局—以及许多映射系统—使用*十进制度*系统表示纬度和经度坐标。*纬度*表示地球上的南北位置，赤道为0度，北极为90度，南极为-90度。
- en: '*Longitude* represents locations east and west, with the *Prime Meridian* that
    passes through Greenwich in London at 0 degrees longitude. From there, longitude
    increases both east and west (positive numbers to the east and negative to the
    west) until they meet at 180 degrees on the opposite side of the globe. The location
    there, known as the *antimeridian*, is used as the basis for the *International
    Date Line*.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*经度*表示东西位置，经过伦敦格林威治的*本初子午线*为0度经度。从那里开始，东经和西经都逐渐增大（东经为正数，西经为负数），直到它们在地球的另一侧相遇于180度。那里的位置，被称为*反子午线*，是*国际日期变更线*的基础。'
- en: When reporting interior points, the Census Bureau uses up to seven decimal places.
    With a value up to 180 to the left of the decimal, we need to account for a maximum
    of 10 digits total. So, we’re using `numeric` with a precision of `10` and a scale
    of `7`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在报告内部点时，人口普查局使用最多七位小数。由于小数点左边最多可达180，我们需要最多10个数字。因此，我们使用了`numeric`类型，精度为`10`，小数位为`7`。
- en: Next, we reach a series of columns 6 that contain the county’s population estimates
    and components of change. [Table 5-1](#table5-1) lists their definitions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进入了一系列包含县人口估算和变化成分的列6。[表5-1](#table5-1)列出了它们的定义。
- en: 'Table 5-1: Census Population Estimate Columns'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表5-1：人口普查人口估算列
- en: '| **Column name** | **Description** |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| **列名** | **描述** |'
- en: '| --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `pop_est_2018` | Estimated population on July 1, 2018 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| `pop_est_2018` | 2018年7月1日的估算人口 |'
- en: '| `pop_est_2019` | Estimated population on July 1, 2019 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `pop_est_2019` | 2019年7月1日的估算人口 |'
- en: '| `births_2019` | Number of births from July 1, 2018, to June 30, 2019 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| `births_2019` | 2018年7月1日至2019年6月30日的出生人数 |'
- en: '| `deaths_2019` | Number of deaths from July 1, 2018, to June 30, 2019 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `deaths_2019` | 2018年7月1日至2019年6月30日的死亡人数 |'
- en: '| `international_migr_2019` | Net international migration from July 1, 2018,
    to June 30, 2019 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `international_migr_2019` | 2018年7月1日至2019年6月30日的国际迁移净值 |'
- en: '| `domestic_migr_2019` | Net domestic migration from July 1, 2018, to June
    30, 2019 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| `domestic_migr_2019` | 2018年7月1日至2019年6月30日的国内迁移净值 |'
- en: '| `residual_2019` | Number used to adjust estimates for consistency |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| `residual_2019` | 用于调整估算值的一项数值，确保一致性 |'
- en: Finally, the `CREATE TABLE` statement ends with a `CONSTRAINT` clause 7 specifying
    that the columns `state_fips` and `county_fips` will serve as the table’s primary
    key. This means that the combination of those columns is unique for every row
    in the table, a concept we’ll cover extensively in Chapter 8. For now, let’s run
    the import.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`CREATE TABLE`语句以`CONSTRAINT`子句7结束，指定`state_fips`和`county_fips`列将作为表的主键。这意味着这两列的组合在表中的每一行都是唯一的，这是我们将在第8章详细讨论的概念。现在，运行导入操作吧。
- en: Performing the Census Import with COPY
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用COPY执行人口普查导入
- en: Now you’re ready to bring the census data into the table. Run the code in [Listing
    5-3](#listing5-3), remembering to change the path to the file to match the location
    of the data on your computer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你准备好将人口普查数据导入表格了。运行[列表 5-3](#listing5-3)中的代码，记得更改文件路径以匹配计算机上数据的位置。
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Listing 5-3: Importing census data using `COPY`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5-3：使用`COPY`导入人口普查数据
- en: 'When the code executes, you should see the following message in pgAdmin:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当代码执行时，你应该在pgAdmin中看到以下消息：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'That’s good news: the import CSV has the same number of rows. If you have an
    issue with the source CSV or your import statement, the database will throw an
    error. For example, if one of the rows in the CSV had more columns than in the
    target table, you’d see an error message in the Data Output pane of pgAdmin that
    provides a hint as to how to fix it:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是个好消息：导入的CSV文件行数相同。如果源CSV文件或导入语句有问题，数据库会抛出错误。例如，如果CSV中的某一行列数多于目标表中的列数，你将在pgAdmin的“数据输出”窗格中看到错误信息，并提供修复的提示：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Even if no errors are reported, it’s always a good idea to visually scan the
    data you just imported to ensure everything looks as expected.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有报告任何错误，最好还是视觉检查一下你刚刚导入的数据，确保一切看起来如预期。
- en: Inspecting the Import
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查导入数据
- en: 'Start with a `SELECT` query of all columns and rows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个 `SELECT` 查询开始，查询所有列和行：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'There should be 3,142 rows displayed in pgAdmin, and as you scroll left and
    right through the result set, each column should have the expected values. Let’s
    review some columns that we took particular care to define with the appropriate
    data types. For example, run the following query to show the counties with the
    largest `area_land` values. We’ll use a `LIMIT` clause, which will cause the query
    to return only the number of rows we want; here, we’ll ask for three:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在 pgAdmin 中应该显示 3,142 行数据，当你在结果集中左右滚动时，每一列应该都有预期的值。让我们回顾一下我们特别注意用适当数据类型定义的一些列。例如，运行以下查询，显示
    `area_land` 值最大的县。我们将使用 `LIMIT` 子句，这将导致查询仅返回我们想要的行数；在这里，我们要求返回三行：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This query ranks county-level geographies from largest land area to smallest
    in square meters. We defined `area_land` as `bigint` because the largest values
    in the field are bigger than the upper range provided by regular `integer`. As
    you might expect, big Alaskan geographies are at the top:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询将县级地理数据从最大土地面积排序到最小，单位为平方米。我们将 `area_land` 定义为 `bigint`，因为该字段中的最大值超过了普通
    `integer` 类型的上限。正如你所料，阿拉斯加的大型地理区域排在最前面：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, let’s check the latitude and longitude columns of `internal_point_lat`
    and `internal_point_lon`, which we defined with `numeric(10,7)`. This code sorts
    the counties by longitude from the greatest to smallest value. This time, we’ll
    use `LIMIT` to retrieve five rows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们检查一下 `internal_point_lat` 和 `internal_point_lon` 列，我们已经用 `numeric(10,7)`
    定义了它们。此代码根据经度将县按从大到小排序。这次，我们将使用 `LIMIT` 来获取五行数据：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Longitude measures locations from east to west, with locations west of the
    Prime Meridian in England represented as negative numbers starting with −1, −2,
    −3, and so on, the farther west you go. We sorted in descending order, so we’d
    expect the easternmost counties of the United States to show at the top of the
    query result. Instead—surprise!—there’s a lone Alaska geography at the top:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 经度测量的是东西方向的位置，位于英格兰本初子午线以西的位置表示为负数，从 -1、-2、-3 开始，越往西数值越小。我们按降序排序，因此我们期望美国东部的县会出现在查询结果的顶部。但结果却让人惊讶！——最上面的是一条孤立的阿拉斯加地理数据：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here’s why: the Alaskan Aleutian Islands extend so far west (farther west than
    Hawaii) that they cross the antimeridian at 180 degrees longitude. Once past the
    antimeridian, longitude turns positive, counting back down to 0\. Fortunately,
    it’s not a mistake in the data; however, it’s a fact you can tuck away for your
    next trivia team competition.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 原因如下：阿拉斯加阿留申群岛延伸得如此遥远（比夏威夷还要远），以至于它们穿越了 180 度经线（反经线）。一旦越过反经线，经度就会变为正值，回到 0。幸运的是，这并不是数据的错误；不过，这是一个你可以记住，用于下一次竞赛的趣味知识。
- en: Congratulations! You have a legitimate set of government demographic data in
    your database. I’ll use it to demonstrate exporting data with `COPY` later in
    this chapter, and then you’ll use it to learn math functions in Chapter 6. Before
    we move on to exporting data, let’s examine a few additional importing techniques.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！你已经在数据库中拥有了一套有效的政府人口统计数据。我将在本章稍后演示如何使用 `COPY` 导出数据，然后你将使用它来学习第 6 章中的数学函数。在继续导出数据之前，让我们再看看几种额外的导入技巧。
- en: Importing a Subset of Columns with COPY
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 `COPY` 导入部分列
- en: 'If a CSV file doesn’t have data for all the columns in your target database
    table, you can still import the data you have by specifying which columns are
    present in the data. Consider this scenario: you’re researching the salaries of
    all town supervisors in your state so you can analyze government spending trends
    by geography. To get started, you create a table called `supervisor_salaries`
    with the code in [Listing 5-4](#listing5-4).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 CSV 文件没有包含目标数据库表中所有列的数据，你仍然可以通过指定数据中存在的列来导入已有的数据。考虑这种情况：你正在研究你所在州所有镇镇长的薪资，以便分析按地理区域划分的政府支出趋势。首先，你创建一个名为
    `supervisor_salaries` 的表，使用 [Listing 5-4](#listing5-4) 中的代码。
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Listing 5-4: Creating a table to track supervisor salaries'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 5-4：创建一个表来跟踪镇长薪资
- en: You want columns for the town and county, the supervisor’s name, the date they
    started, and salary and benefits (assuming you just care about current levels).
    You’re also adding an auto-incrementing `id` column as a primary key. However,
    the first county clerk you contact says, “Sorry, we only have town, supervisor,
    and salary. You’ll need to get the rest from elsewhere.” You tell them to send
    a CSV anyway. You’ll import what you can.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要包含城镇和县的列、主管的名字、他们开始的日期、以及薪资和福利（假设你只关心当前的水平）。你还将添加一个自增的`id`列作为主键。然而，你联系的第一个县书记员说：“抱歉，我们只有城镇、主管和薪资这几个数据。其他信息你需要从别处获取。”你告诉他们还是发送CSV文件过来，你会导入能获取的数据。
- en: 'I’ve included such a sample CSV you can download via the book’s resources at
    [https://www.nostarch.com/practical-sql-2nd-edition/](https://www.nostarch.com/practical-sql-2nd-edition/),
    called *supervisor_salaries.csv*. If you view the file with a text editor, you
    should see these two lines at the top:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经包含了一个示例CSV文件，你可以通过书中的资源下载，网址是[https://www.nostarch.com/practical-sql-2nd-edition/](https://www.nostarch.com/practical-sql-2nd-edition/)，文件名为*supervisor_salaries.csv*。如果你用文本编辑器查看该文件，你应该能看到文件顶部的这两行：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You could try to import it using this basic `COPY` syntax:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试使用这个基本的`COPY`语法来导入数据：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'But if you do, PostgreSQL will return an error:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你这样做，PostgreSQL会返回一个错误：
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The problem is that your table’s first column is the auto-incrementing `id`,
    but your CSV file begins with the text column `town`. Even if your CSV file had
    an integer present in its first column, the `GENERATED ALWAYS AS IDENTITY` keywords
    would prevent you from adding a value to `id`. The workaround for this situation
    is to tell the database which columns in the table are present in the CSV, as
    shown in [Listing 5-5](#listing5-5).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于你的表的第一列是自增的`id`，而你的CSV文件从文本列`town`开始。即使你的CSV文件在第一列中包含整数，`GENERATED ALWAYS
    AS IDENTITY`关键字也会阻止你向`id`列添加值。解决这个问题的方法是告诉数据库表中哪些列在CSV中存在，如[示例 5-5](#listing5-5)所示。
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Listing 5-5: Importing salaries data from CSV to three table columns'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 5-5：从CSV导入薪资数据到三个表列
- en: 'By noting in parentheses 1 the three present columns after the table name,
    we tell PostgreSQL to only look for data to fill those columns when it reads the
    CSV. Now, if you select the first couple of rows from the table, you’ll see those
    columns filled with the appropriate values:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在表名后面括号中标注1，我们告诉PostgreSQL只查找那些数据来填充这些列，尤其是当它读取CSV时。现在，如果你从表中选择前几行，你会看到这些列已经填充了相应的值：
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Importing a Subset of Rows with COPY
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用COPY导入部分行
- en: Starting with PostgreSQL version 12, you can add a `WHERE` clause to a `COPY`
    statement to filter which rows from the source CSV you import into a table. You
    can see how this works using the supervisor salaries data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 从PostgreSQL版本12开始，你可以在`COPY`语句中添加`WHERE`子句，以过滤从源CSV中导入的行。你可以通过主管薪资数据查看这个功能是如何工作的。
- en: Start by clearing all the data you already imported into `supervisor_salaries`
    using a `DELETE` query.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 从清空你已导入的所有`supervisor_salaries`数据开始，使用`DELETE`查询。
- en: '[PRE25]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This will remove data from the table, but it will not reset the `id` column’s
    `IDENTITY` column sequence. We’ll cover how to do that when we discuss table design
    in Chapter 8. When that query finishes, run the `COPY` statement in [Listing 5-6](#listing5-6),
    which adds a `WHERE` clause that filters the import to include only rows in which
    the `town` column in the CSV input matches New Brillig.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从表中删除数据，但不会重置`id`列的`IDENTITY`序列。我们将在第8章讨论表设计时讲解如何做这个操作。当这个查询完成后，运行[示例 5-6](#listing5-6)中的`COPY`语句，加入一个`WHERE`子句，过滤导入的数据，只包括那些`town`列值为New
    Brillig的行。
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Listing 5-6: Importing a subset of rows with `WHERE`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 5-6：使用`WHERE`导入部分行
- en: 'Next, run `SELECT * FROM supervisor_salaries;` to view the contents of the
    table. You should see just one row:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，运行`SELECT * FROM supervisor_salaries;`来查看表中的内容。你应该只看到一行数据：
- en: '[PRE27]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This is a handy shortcut. Now, let’s see how to use a temporary table to do
    even more data wrangling during an import.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个便捷的快捷方式。现在，让我们看看如何使用临时表在导入过程中进行更多的数据处理。
- en: Adding a Value to a Column During Import
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在导入过程中为列添加值
- en: What if you know that “Mills” is the name that should be added to the `county`
    column during the import, even though that value is missing from the CSV file?
    One way to modify your import to include the name is by loading your CSV into
    a *temporary table* before adding it to `supervisors_salary`. Temporary tables
    exist only until you end your database session. When you reopen the database (or
    lose your connection), those tables disappear. They’re handy for performing intermediary
    operations on data as part of your processing pipeline; we’ll use one to add the
    county name to the `supervisor_salaries` table as we import the CSV.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道“米尔斯（Mills）”是应该在导入时添加到`county`列中的名称，尽管该值在CSV文件中缺失呢？一种修改导入以包含该名称的方法是将CSV加载到*临时表*中，然后再添加到`supervisors_salary`。临时表只会存在到数据库会话结束为止。当你重新打开数据库（或失去连接）时，这些表会消失。它们在数据处理管道中执行中间操作时非常有用；我们将用一个临时表来在导入CSV时为`supervisor_salaries`表添加县名。
- en: Again, clear the data you’ve imported into `supervisor_salaries` using a `DELETE`
    query. When it completes, run the code in [Listing 5-7](#listing5-7), which will
    make a temporary table and import your CSV. Then, we will query data from that
    table and include the county name for an insert into the `supervisor_salaries`
    table.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用`DELETE`查询清除你导入到`supervisor_salaries`中的数据。完成后，运行[Listing 5-7](#listing5-7)中的代码，它将创建一个临时表并导入你的CSV。然后，我们将从该表中查询数据，并在插入`supervisor_salaries`表时包含县名。
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Listing 5-7: Using a temporary table to add a default value to a column during
    import'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 5-7：使用临时表在导入时为列添加默认值
- en: This script performs four tasks. First, we create a temporary table called `supervisor_salaries_temp`
    1 based on the original `supervisor_salaries` table by passing as an argument
    the `LIKE` keyword followed by the source table name. The keywords `INCLUDING
    ALL` tell PostgreSQL to not only copy the table rows and columns but also components
    such as indexes and the `IDENTITY` settings. Then we import the *supervisor_salaries.csv*
    file 2 into the temporary table using the now-familiar `COPY` syntax.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本执行四个任务。首先，我们通过传递`LIKE`关键字和源表名作为参数，基于原始的`supervisor_salaries`表创建一个名为`supervisor_salaries_temp`
    1 的临时表。`INCLUDING ALL`关键字告诉PostgreSQL不仅要复制表的行和列，还要复制如索引和`IDENTITY`设置等组件。接着，我们使用熟悉的`COPY`语法将*supervisor_salaries.csv*文件
    2 导入到临时表中。
- en: Next, we use an `INSERT` statement 3 to fill the salaries table. Instead of
    specifying values, we employ a `SELECT` statement to query the temporary table.
    That query specifies `Mills` as the value for the second column, not as a column
    name, but as a string inside single quotes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`INSERT`语句 3 填充工资表。我们不直接指定值，而是使用`SELECT`语句查询临时表。该查询将`Mills`指定为第二列的值，而不是列名，而是作为单引号中的字符串。
- en: Finally, we use `DROP TABLE` to erase the temporary table 4 since we’re done
    using it for this import. The temporary table will automatically disappear when
    you disconnect from the PostgreSQL session, but this removes it now in case we
    want to do another import and use a fresh temporary table for another CSV.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`DROP TABLE`删除临时表 4，因为我们已经完成了这次导入。当你断开与PostgreSQL会话的连接时，临时表会自动消失，但现在先删除它，以防我们想做另一次导入并使用一个新的临时表来处理另一个CSV。
- en: 'After you run the query, run a `SELECT` statement on the first couple of rows
    to see the effect:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行查询后，执行一个`SELECT`语句查看前几行的效果：
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You’ve filled the `county` field with a value even though your source CSV didn’t
    have one. The path to this import might seem laborious, but it’s instructive to
    see how data processing can require multiple steps to get the desired results.
    The good news is that this temporary table demo is an apt indicator of the flexibility
    SQL offers to control data handling.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你的源CSV文件没有该值，你也已经在`county`字段中填入了一个值。这个导入过程看起来可能有点繁琐，但通过这一过程可以看到，数据处理有时需要多个步骤才能获得期望的结果。好消息是，这个临时表的示范恰好展示了SQL在数据处理中的灵活性。
- en: Using COPY to Export Data
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用COPY导出数据
- en: When exporting data with `COPY`, rather than using `FROM` to identify the source
    data, you use `TO` for the path and name of the output file. You control how much
    data to export—an entire table, just a few columns, or the results of a query.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`COPY`导出数据时，与其使用`FROM`来标识源数据，不如使用`TO`来指定输出文件的路径和名称。你可以控制导出多少数据——整个表、仅部分列或查询结果。
- en: Let’s look at three quick examples.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看三个快速示例。
- en: Exporting All Data
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导出所有数据
- en: The simplest export sends everything in a table to a file. Earlier, you created
    the table `us_counties_pop_est_2019` with 16 columns and 3,142 rows of census
    data. The SQL statement in [Listing 5-8](#listing5-8) exports all the data to
    a text file named *us_counties_export.txt*. To demonstrate the flexibility you
    have in choosing output options, the `WITH` keyword tells PostgreSQL to include
    a header row and use the pipe symbol instead of a comma for a delimiter. I’ve
    used the *.txt* file extension here for two reasons. First, it demonstrates that
    you can name your file with an extension other than *.csv*; second, we’re using
    a pipe for a delimiter, not a comma, so I want to avoid calling the file *.csv*
    unless it truly has commas as a separator.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的导出将表中的所有数据发送到文件。之前，你创建了包含16列和3,142行人口普查数据的`us_counties_pop_est_2019`表。[清单
    5-8](#listing5-8)中的SQL语句将所有数据导出到名为*us_counties_export.txt*的文本文件中。为了演示你在选择输出选项时的灵活性，`WITH`关键字告诉PostgreSQL包括一个标题行，并使用管道符号而不是逗号作为分隔符。我在这里使用了*.txt*文件扩展名，原因有两个。首先，它演示了你可以将文件命名为*.csv*以外的扩展名；其次，我们使用的是管道符作为分隔符，而不是逗号，因此除非文件确实使用逗号分隔，否则我不想将文件命名为*.csv*。
- en: Remember to change the output directory to your preferred save location.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 记得将输出目录更改为你首选的保存位置。
- en: '[PRE30]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Listing 5-8:Exporting an entire table with `COPY`
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 5-8：使用`COPY`导出整个表
- en: 'View the export file with a text editor to see the data in this format (I’ve
    truncated the results):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本编辑器查看导出文件，查看数据以此格式（我已截断结果）：
- en: '[PRE31]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The file includes a header row with column names, and all columns are separated
    by the pipe delimiter.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 文件包含一个带有列名的标题行，所有列由管道分隔符分开。
- en: Exporting Particular Columns
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导出特定列
- en: 'You don’t always need (or want) to export all your data: you might have sensitive
    information, such as Social Security numbers or birthdates, that need to remain
    private. Or, in the case of the census county data, maybe you’re working with
    a mapping program and only need the county name and its geographic coordinates
    to plot the locations. We can export only these three columns by listing them
    in parentheses after the table name, as shown in [Listing 5-9](#listing5-9). Of
    course, you must enter these column names precisely as they’re listed in the data
    for PostgreSQL to recognize them.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你不总是需要（或想要）导出所有数据：你可能有敏感信息，例如社会保障号码或出生日期，需要保密。或者，在人口普查县数据的情况下，也许你正在使用一个映射程序，只需要县名和其地理坐标来绘制位置。我们可以通过在表名后列出这三列来只导出它们，如[清单
    5-9](#listing5-9)所示。当然，必须精确输入这些列名，以便PostgreSQL能识别它们。
- en: '[PRE32]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Listing 5-9: Exporting selected columns from a table with `COPY`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 5-9：使用`COPY`导出选择的列
- en: Exporting Query Results
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导出查询结果
- en: Additionally, you can add a query to `COPY` to fine-tune your output. In [Listing
    5-10](#listing5-10) we export the name and state of only those counties whose
    names contain the letters `mill`, catching it in either uppercase or lowercase
    by using the case-insensitive `ILIKE` and the `%` wildcard character we covered
    in “Using LIKE and ILIKE with WHERE” in Chapter 3. Also note that for this example,
    I’ve removed the `DELIMITER` keyword from the `WITH` clause. As a result, the
    output will default to comma-separated values.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以向`COPY`添加查询来微调输出。在[清单 5-10](#listing5-10)中，我们只导出那些名称包含字母`mill`的县的名称和州信息，使用不区分大小写的`ILIKE`和我们在第三章“在WHERE中使用LIKE和ILIKE”中介绍的`%`通配符字符，无论是大写还是小写。此外，请注意，在这个例子中，我已从`WITH`子句中删除了`DELIMITER`关键字。因此，输出将默认为逗号分隔的值。
- en: '[PRE33]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Listing 5-10:Exporting query results with `COPY`
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 5-10：使用`COPY`导出查询结果
- en: 'After running the code, your output file should have nine rows with county
    names including Miller, Roger Mills, and Vermillion:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，你的输出文件应该包含九行县名称，包括Miller、Roger Mills和Vermillion：
- en: '[PRE34]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Importing and Exporting Through pgAdmin
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过pgAdmin进行导入和导出
- en: At times, the SQL `COPY` command won’t be able to handle certain imports and
    exports. This typically happens when you’re connected to a PostgreSQL instance
    running on a computer other than yours. A machine in a cloud computing environment
    such as Amazon Web Services is a good example. In that scenario, PostgreSQL’s
    `COPY` command will look for files and file paths that exist on that remote machine;
    it can’t find files on your local computer. To use `COPY`, you’d need to transfer
    your data to the remote server, but you might not always have the rights to do
    that.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，SQL 的 `COPY` 命令无法处理某些导入和导出操作。这通常发生在你连接到运行在其他计算机上的 PostgreSQL 实例时。像亚马逊云服务（AWS）中的机器就是一个很好的例子。在这种情况下，PostgreSQL
    的 `COPY` 命令会查找该远程机器上的文件和文件路径；它无法找到你本地计算机上的文件。要使用 `COPY`，你需要将数据传输到远程服务器，但你可能并不总是有权限这么做。
- en: One workaround is to use pgAdmin’s built-in import/export wizard. In pgAdmin’s
    object browser (the left vertical pane), locate the list of tables in your `analysis`
    database by choosing **Databases**▶**analysis**▶**Schemas**▶**public**▶**Tables**.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一个解决方法是使用 pgAdmin 内置的导入/导出向导。在 pgAdmin 的对象浏览器（左侧垂直窗格）中，选择 **Databases**▶**analysis**▶**Schemas**▶**public**▶**Tables**，定位到你
    `analysis` 数据库中的表列表。
- en: Next, right-click the table you want to import to or export from, and select
    **Import/Export**. A dialog appears that lets you choose to either import or export
    from that table, as shown in [Figure 5-1](#figure5-1).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，右键单击你想要导入或导出的表，然后选择 **Import/Export**。会弹出一个对话框，让你选择是从该表导入还是导出，如 [图 5-1](#figure5-1)
    所示。
- en: '![f05001](Images/f05001.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![f05001](Images/f05001.png)'
- en: 'Figure 5-1: The pgAdmin Import/Export dialog'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-1：pgAdmin 导入/导出对话框
- en: To import, move the Import/Export slider to **Import**. Then click the three
    dots to the right of the **Filename** box to locate your CSV file. From the Format
    drop-down list, choose **csv**. Then adjust the header, delimiter, quoting, and
    other options as needed. Click **OK** to import the data.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 要导入，将导入/导出滑块移动到 **Import**。然后点击 **Filename** 框右侧的三个点，定位到你的 CSV 文件。在格式下拉列表中，选择
    **csv**。然后根据需要调整标题、分隔符、引号和其他选项。点击 **OK** 以导入数据。
- en: To export, use the same dialog and follow similar steps.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 若要导出，使用相同的对话框并遵循类似的步骤。
- en: In Chapter 18, when we discuss using PostgreSQL from your computer’s command
    line, we’ll explore another way to accomplish this using a utility called `psql`
    and its `\copy` command. pgAdmin’s import/export wizard actually uses `\copy`
    in the background but gives it a friendlier face.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 18 章中，当我们讨论如何从计算机的命令行使用 PostgreSQL 时，我们将探索另一种方法，通过名为`psql`的工具及其`\copy`命令来实现。pgAdmin
    的导入/导出向导实际上在后台使用`\copy`，但它提供了一个更友好的界面。
- en: Wrapping Up
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Now that you’ve learned how to bring external data into your database, you can
    start digging into a myriad of datasets, whether you want to explore one of the
    thousands of publicly available datasets, or data related to your own career or
    studies. Plenty of data is available in CSV format or a format easily convertible
    to CSV. Look for data dictionaries to help you understand the data and choose
    the right data type for each field.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学会了如何将外部数据导入到数据库中，你可以开始深入研究各种数据集，无论是探索成千上万的公开数据集，还是与自己的职业或学习相关的数据。大量数据以
    CSV 格式或易于转换为 CSV 格式的形式提供。查找数据字典以帮助你理解数据，并为每个字段选择正确的数据类型。
- en: The census data you imported as part of this chapter’s exercises will play a
    starring role in the next chapter, in which we explore math functions with SQL.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 你在本章练习中导入的人口普查数据将在下一章发挥重要作用，在那一章我们将探索使用 SQL 的数学函数。
