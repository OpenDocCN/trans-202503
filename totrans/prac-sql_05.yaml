- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Importing and Exporting Data
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: So far, you’ve learned how to add a handful of rows to a table using SQL `INSERT`
    statements. A row-by-row insert is useful for making quick test tables or adding
    a few rows to an existing table. But it’s more likely you’ll need to load hundreds,
    thousands, or even millions of rows, and no one wants to write separate `INSERT`
    statements in those situations. Fortunately, you don’t have to.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: If your data exists in a *delimited* text file, with one table row per line
    of text and each column value separated by a comma or other character, PostgreSQL
    can import the data in bulk via its `COPY` command. This command is a PostgreSQL-specific
    implementation with options for including or excluding columns and handling various
    delimited text types.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: In the opposite direction, `COPY` will also *export* data from PostgreSQL tables
    or from the result of a query to a delimited text file. This technique is handy
    when you want to share data with colleagues or move it into another format, such
    as an Excel file.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'I briefly touched on `COPY` for export in the “Understanding Characters” section
    of Chapter 4, but in this chapter, I’ll discuss import and export in more depth.
    For importing, I’ll start by introducing you to one of my favorite datasets: annual
    US Census population estimates by county.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'Three steps form the outline of most of the imports you’ll do:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Obtain the source data in the form of a delimited text file.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a table to store the data.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a `COPY` statement to perform the import.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the import is done, we’ll check the data and look at additional options
    for importing and exporting.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: A delimited text file is the most common file format that’s portable across
    proprietary and open source systems, so we’ll focus on that file type. If you
    want to transfer data from another database program’s proprietary format directly
    to PostgreSQL—for example, from Microsoft Access or MySQL—you’ll need to use a
    third-party tool. Check the PostgreSQL wiki at [https://wiki.postgresql.org/wiki/](https://wiki.postgresql.org/wiki/)
    and search for “Converting from other databases to PostgreSQL” for a list of tools
    and options.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using SQL with another database manager, check the other database’s
    documentation for how it handles bulk imports. The MySQL database, for example,
    has a `LOAD DATA INFILE` statement, and Microsoft’s SQL Server has its own `BULK
    INSERT` command.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Working with Delimited Text Files
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many software applications store data in a unique format, and translating one
    data format to another is about as easy as trying to read the Cyrillic alphabet
    when one understands only English. Fortunately, most software can import from
    and export to a delimited text file, which is a common data format that serves
    as a middle ground.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: A delimited text file contains rows of data, each of which represents one row
    in a table. In each row, each data column is separated, or delimited, by a particular
    character. I’ve seen all kinds of characters used as delimiters, from ampersands
    to pipes, but the comma is most commonly used; hence the name of a file type you’ll
    see often is *comma-separated values (CSV)*. The terms *CSV* and *comma-delimited*
    are interchangeable.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a typical data row you might see in a comma-delimited file:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Notice that a comma separates each piece of data—first name, last name, street,
    town, state, and phone—without any spaces. The commas tell the software to treat
    each item as a separate column, upon either import or export. Simple enough.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Handling Header Rows
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A feature you’ll often find inside a delimited text file is a *header row*.
    As the name implies, it’s a single row at the top, or *head*, of the file that
    lists the name of each data column. Often, a header is added when data is exported
    from a database or a spreadsheet. Here’s an example with the delimited row I’ve
    been using. Each item in a header row corresponds to its respective column:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Header rows serve a few purposes. For one, the values in the header row identify
    the data in each column, which is particularly useful when you’re deciphering
    a file’s contents. Second, some database managers (although not PostgreSQL) use
    the header row to map columns in the delimited file to the correct columns in
    the import table. PostgreSQL doesn’t use the header row, so we don’t want to import
    that row to a table. We use the `HEADER` option in the `COPY` command to exclude
    it. I’ll cover this with all `COPY` options in the next section.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Quoting Columns That Contain Delimiters
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using commas as a column delimiter leads to a potential dilemma: what if the
    value in a column includes a comma? For example, sometimes people combine an apartment
    number with a street address, as in 123 Main St., Apartment 200\. Unless the system
    for delimiting accounts for that extra comma, during import the line will appear
    to have an extra column and cause the import to fail.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'To handle such cases, delimited files use an arbitrary character called a *text
    qualifier* to enclose a column that includes the delimiter character. This acts
    as a signal to ignore that delimiter and treat everything between the text qualifiers
    as a single column. Most of the time in comma-delimited files the text qualifier
    used is the double quote. Here’s the example data again, but with the street name
    column surrounded by double quotes:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: On import, the database will recognize that double quotes signify one column
    regardless of whether it finds a delimiter within the quotes. When importing CSV
    files, PostgreSQL by default ignores delimiters inside double-quoted columns,
    but you can specify a different text qualifier if your import requires it. (And,
    given the sometimes-odd choices made by IT professionals, you may indeed need
    to employ a different character.)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in CSV mode, if PostgreSQL finds two consecutive text qualifiers inside
    a double-quoted column, it will remove one. For example, let’s say PostgreSQL
    finds this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If so, it will treat that text as a single column upon import, leaving just
    one of the qualifiers:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A situation like that could indicate an error in the formatting of your CSV
    file, which is why, as you’ll see later, it’s always a good idea to review your
    data after importing.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Using COPY to Import Data
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To import data from an external file into our database, we first create a table
    in our database that matches the columns and data types in our source file. Once
    that’s done, the `COPY` statement for the import is just the three lines of code
    in [Listing 5-1](#listing5-1).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Listing 5-1: Using `COPY` for data import'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: We start the block of code with the `COPY` keyword 1 followed by the name of
    the target table, which must already exist in your database. Think of this syntax
    as meaning, “Copy data to my table called `table_name`.”
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'The `FROM` keyword 2 identifies the full path to the source file, and we enclose
    the path in single quotes. The way you designate the path depends on your operating
    system. For Windows, begin with the drive letter, colon, backslash, and directory
    names. For example, to import a file located on my Windows desktop, the `FROM`
    line would read as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'On macOS or Linux, start at the system root directory with a forward slash
    and proceed from there. Here’s what the `FROM` line might look like when importing
    a file located on my macOS desktop:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For the examples in the book, I use the Windows-style path `C:\YourDirectory\`
    as a placeholder. Replace that with the path where you stored the CSV file you
    downloaded from GitHub.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'The `WITH` keyword 3 lets you specify options, surrounded by parentheses, that
    you use to tailor your input or output file. Here we specify that the external
    file should be comma-delimited and that we should exclude the file’s header row
    in the import. It’s worth examining all the options in the official PostgreSQL
    documentation at [https://www.postgresql.org/docs/current/sql-copy.html](https://www.postgresql.org/docs/current/sql-copy.html),
    but here is a list of the options you’ll commonly use:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '**Input and output file format**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Use the `FORMAT` `format_name` option to specify the type of file you’re reading
    or writing. Format names are `CSV`, `TEXT`, or `BINARY`. Unless you’re deep into
    building technical systems, you’ll rarely encounter a need to work with `BINARY`,
    where data is stored as a sequence of bytes. More often, you’ll work with standard
    CSV files. In the `TEXT` format, a *tab* character is the delimiter by default
    (although you can specify another character), and backslash characters such as
    `\r` are recognized as their ASCII equivalents—in this case, a carriage return.
    The `TEXT` format is used mainly by PostgreSQL’s built-in backup programs.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Presence of a header row**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: On import, use `HEADER` to specify that the source file has a header row that
    you want to exclude. The database will start importing with the second line of
    the file so that the column names in the header don’t become part of the data
    in the table. (Be sure to check your source CSV to make sure this is what you
    want; not every CSV comes with a header row!) On export, using `HEADER` tells
    the database to include the column names as a header row in the output file, which
    helps a user understand the file’s contents.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Delimiter**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DELIMITER` `''``character``''` option lets you specify which character
    your import or export file uses as a delimiter. The delimiter must be a single
    character and cannot be a carriage return. If you use `FORMAT CSV`, the assumed
    delimiter is a comma. I include `DELIMITER` here to show that you have the option
    to specify a different delimiter if that’s how your data arrived. For example,
    if you received pipe-delimited data, you would treat the option this way: `DELIMITER
    ''|''`.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Quote character**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, you learned that in a CSV file, commas inside a single column value
    will mess up your import unless the column value is surrounded by a character
    that serves as a text qualifier, telling the database to handle the value within
    as one column. By default, PostgreSQL uses the double quote, but if the CSV you’re
    importing uses a different character for the text qualifier, you can specify it
    with the `QUOTE` `'``quote_character``'` option.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that you better understand delimited files, you’re ready to import one.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Importing Census Data Describing Counties
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dataset you’ll work with in this import exercise is considerably larger
    than the `teachers` table you made in Chapter 2. It contains census population
    estimates for every county in the United States and is 3,142 rows deep and 16
    columns wide. (Census counties include some geographies with other names: parishes
    in Louisiana, boroughs and census areas in Alaska, and cities, particularly in
    Virginia.)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: To understand the data, it helps to know a little about the US Census Bureau,
    a federal agency that tracks the nation’s demographics. Its best-known program
    is a full count of the population it undertakes every 10 years, most recently
    in 2020\. That data, which enumerates the age, gender, race, and ethnicity of
    each person in the country, is used to determine how many members from each state
    make up the 435-member US House of Representatives. In recent decades, faster-growing
    states such as Texas and Florida have gained seats, while slower-growing states
    such as New York and Ohio have lost representatives in the House.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: The data we’ll work with are the census’ annual population estimates. These
    use the most recent 10-year census count as a base, and they factor in births,
    deaths, and domestic and international migration to produce population estimates
    each year for the nation, states, counties, and other geographies. In lieu of
    an annual physical count, it’s the best way to get an updated measure on how many
    people live where in the United States. For this exercise, I compiled select columns
    from the 2019 US Census county-level population estimates (plus a few descriptive
    columns from census geographic data) into a file named *us_counties_pop_est_2019.csv*.
    You should have this file on your computer if you followed the directions in the
    section “Downloading Code and Data from GitHub” in Chapter 1. If not, go back
    and do that now.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据是人口普查的年度人口估算。这些估算使用最近一次的十年人口普查数据作为基础，并且考虑了出生、死亡以及国内外迁移，以每年为国家、州、县和其他地理区域估算人口。由于缺乏年度的实地人口统计数据，它是获取美国各地人口更新数据的最佳方式。对于这个练习，我将从2019年美国人口普查的县级人口估算数据中选取了几列（以及来自人口普查地理数据的几列描述性列），并将它们编译成一个名为*us_counties_pop_est_2019.csv*的文件。如果你按照第一章中“从GitHub下载代码和数据”部分的说明操作，你应该在电脑上有这个文件。如果没有，现在去下载它。
- en: 'Open the file with a text editor. You should see a header row that begins with
    these columns:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本编辑器打开文件。你应该会看到一行标题，包含以下列：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let’s explore the columns by examining the code for creating the import table.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过检查创建导入表的代码来探索这些列。
- en: Creating the us_counties_pop_est_2019 Table
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建`us_counties_pop_est_2019`表
- en: The code in [Listing 5-2](#listing5-2) shows the `CREATE TABLE` script. In pgAdmin
    click the `analysis` database that you created in Chapter 2. (It’s best to store
    the data in this book in `analysis` because we’ll reuse some of it in later chapters.)
    From the pgAdmin menu bar, select **Tools**▶**Query Tool**. You can type the code
    into the tool or copy and paste it from the files you downloaded from GitHub.
    Once you have the script in the window, run it.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表5-2](#listing5-2)中的代码展示了`CREATE TABLE`脚本。在pgAdmin中点击你在第二章创建的`analysis`数据库。（最好将本书中的数据存储在`analysis`中，因为我们将在后续章节中重用其中的一部分。）在pgAdmin的菜单栏中，选择**工具**▶**查询工具**。你可以将代码输入到工具中，或者从你从GitHub下载的文件中复制并粘贴。将脚本放入窗口后，运行它。'
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Listing 5-2: `CREATE TABLE` statement for census county population estimates'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5-2：人口普查县级人口估算的`CREATE TABLE`语句
- en: 'Return to the main pgAdmin window, and in the object browser, right-click and
    refresh the `analysis` database. Choose **Schemas**▶**public**▶**Tables** to see
    the new table. Although it’s empty, you can see the structure by running a basic
    `SELECT` query in pgAdmin’s Query Tool:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到pgAdmin的主窗口，在对象浏览器中，右键点击并刷新`analysis`数据库。选择**架构**▶**public**▶**表**以查看新创建的表。虽然它为空，但你可以通过在pgAdmin查询工具中运行基本的`SELECT`查询来查看其结构：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: When you run the `SELECT` query, you’ll see the columns in the table you created
    appear in the pgAdmin Data Output pane. No data rows exist yet. We need to import
    them.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行`SELECT`查询时，你会看到你创建的表格的列出现在pgAdmin的数据输出窗格中。目前还没有数据行，我们需要导入它们。
- en: Understanding Census Columns and Data Types
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解人口普查的列和数据类型
- en: 'Before we import the CSV file into the table, let’s walk through several of
    the columns and the data types I chose in [Listing 5-2](#listing5-2). As my guide,
    I used two official census data dictionaries: one for the estimates found at [https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2010-2019/co-est2019-alldata.pdf](https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2010-2019/co-est2019-alldata.pdf)
    and one for the decennial count that includes the geographic columns at [http://www.census.gov/prod/cen2010/doc/pl94-171.pdf](http://www.census.gov/prod/cen2010/doc/pl94-171.pdf).
    I’ve given some columns more readable names in the table definition. Relying on
    a data dictionary when possible is good practice, because it helps you avoid misconfiguring
    columns or potentially losing data. Always ask if one is available, or do an online
    search if the data is public.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: In this set of census data, and thus the table you just made, each row displays
    the population estimates and components of annual change (births, deaths, and
    migration) for one county. The first two columns are the county’s `state_fips`
    1 and `county_fips`, which are the standard federal codes for those entities.
    We use `text` for both because those codes can contain leading zeros that would
    be lost if we stored the values as integers. For example, Alaska’s `state_fips`
    is `02`. If we used an integer type, that leading `0` would be stripped on import,
    leaving `2`, which is the wrong code for the state. Also, we won’t be doing any
    math with this value, so don’t need integers. It’s always important to distinguish
    codes from numbers; these state and county values are actually labels as opposed
    to numbers used for math.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Numbers from 1 to 4 in `region` 2 represent the general location of a county
    in the United States: the Northeast, Midwest, South, or West. No number is higher
    than 4, so we define the columns with type `smallint`. The `state_name` 3 and
    `county_name` columns contain the complete name of both the state and county,
    stored as `text`.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: The number of square meters for land and water in the county are recorded in
    `area_land` 4 and `area_water`, respectively. The two, combined, comprise a county’s
    total area. In certain places—such as Alaska, where there’s lots of land to go
    with all that snow—some values easily surpass the `integer` type’s maximum of
    2,147,483,647\. For that reason, we’re using `bigint`, which will handle the 377,038,836,685
    square meters of land in the Yukon-Koyukuk census area with room to spare.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: The latitude and longitude of a point near the center of the county, called
    an *internal point*, are specified in `internal_point_lat` and `internal_point_lon`
    5, respectively. The Census Bureau—along with many mapping systems—expresses latitude
    and longitude coordinates using a *decimal degrees* system. *Latitude* represents
    positions north and south on the globe, with the equator at 0 degrees, the North
    Pole at 90 degrees, and the South Pole at −90 degrees.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '*Longitude* represents locations east and west, with the *Prime Meridian* that
    passes through Greenwich in London at 0 degrees longitude. From there, longitude
    increases both east and west (positive numbers to the east and negative to the
    west) until they meet at 180 degrees on the opposite side of the globe. The location
    there, known as the *antimeridian*, is used as the basis for the *International
    Date Line*.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: When reporting interior points, the Census Bureau uses up to seven decimal places.
    With a value up to 180 to the left of the decimal, we need to account for a maximum
    of 10 digits total. So, we’re using `numeric` with a precision of `10` and a scale
    of `7`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Next, we reach a series of columns 6 that contain the county’s population estimates
    and components of change. [Table 5-1](#table5-1) lists their definitions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5-1: Census Population Estimate Columns'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '| **Column name** | **Description** |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: '| `pop_est_2018` | Estimated population on July 1, 2018 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: '| `pop_est_2019` | Estimated population on July 1, 2019 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: '| `births_2019` | Number of births from July 1, 2018, to June 30, 2019 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: '| `deaths_2019` | Number of deaths from July 1, 2018, to June 30, 2019 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '| `international_migr_2019` | Net international migration from July 1, 2018,
    to June 30, 2019 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '| `domestic_migr_2019` | Net domestic migration from July 1, 2018, to June
    30, 2019 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: '| `residual_2019` | Number used to adjust estimates for consistency |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: Finally, the `CREATE TABLE` statement ends with a `CONSTRAINT` clause 7 specifying
    that the columns `state_fips` and `county_fips` will serve as the table’s primary
    key. This means that the combination of those columns is unique for every row
    in the table, a concept we’ll cover extensively in Chapter 8. For now, let’s run
    the import.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Performing the Census Import with COPY
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now you’re ready to bring the census data into the table. Run the code in [Listing
    5-3](#listing5-3), remembering to change the path to the file to match the location
    of the data on your computer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Listing 5-3: Importing census data using `COPY`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'When the code executes, you should see the following message in pgAdmin:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'That’s good news: the import CSV has the same number of rows. If you have an
    issue with the source CSV or your import statement, the database will throw an
    error. For example, if one of the rows in the CSV had more columns than in the
    target table, you’d see an error message in the Data Output pane of pgAdmin that
    provides a hint as to how to fix it:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Even if no errors are reported, it’s always a good idea to visually scan the
    data you just imported to ensure everything looks as expected.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting the Import
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Start with a `SELECT` query of all columns and rows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'There should be 3,142 rows displayed in pgAdmin, and as you scroll left and
    right through the result set, each column should have the expected values. Let’s
    review some columns that we took particular care to define with the appropriate
    data types. For example, run the following query to show the counties with the
    largest `area_land` values. We’ll use a `LIMIT` clause, which will cause the query
    to return only the number of rows we want; here, we’ll ask for three:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This query ranks county-level geographies from largest land area to smallest
    in square meters. We defined `area_land` as `bigint` because the largest values
    in the field are bigger than the upper range provided by regular `integer`. As
    you might expect, big Alaskan geographies are at the top:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, let’s check the latitude and longitude columns of `internal_point_lat`
    and `internal_point_lon`, which we defined with `numeric(10,7)`. This code sorts
    the counties by longitude from the greatest to smallest value. This time, we’ll
    use `LIMIT` to retrieve five rows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Longitude measures locations from east to west, with locations west of the
    Prime Meridian in England represented as negative numbers starting with −1, −2,
    −3, and so on, the farther west you go. We sorted in descending order, so we’d
    expect the easternmost counties of the United States to show at the top of the
    query result. Instead—surprise!—there’s a lone Alaska geography at the top:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here’s why: the Alaskan Aleutian Islands extend so far west (farther west than
    Hawaii) that they cross the antimeridian at 180 degrees longitude. Once past the
    antimeridian, longitude turns positive, counting back down to 0\. Fortunately,
    it’s not a mistake in the data; however, it’s a fact you can tuck away for your
    next trivia team competition.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have a legitimate set of government demographic data in
    your database. I’ll use it to demonstrate exporting data with `COPY` later in
    this chapter, and then you’ll use it to learn math functions in Chapter 6. Before
    we move on to exporting data, let’s examine a few additional importing techniques.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Importing a Subset of Columns with COPY
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If a CSV file doesn’t have data for all the columns in your target database
    table, you can still import the data you have by specifying which columns are
    present in the data. Consider this scenario: you’re researching the salaries of
    all town supervisors in your state so you can analyze government spending trends
    by geography. To get started, you create a table called `supervisor_salaries`
    with the code in [Listing 5-4](#listing5-4).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Listing 5-4: Creating a table to track supervisor salaries'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: You want columns for the town and county, the supervisor’s name, the date they
    started, and salary and benefits (assuming you just care about current levels).
    You’re also adding an auto-incrementing `id` column as a primary key. However,
    the first county clerk you contact says, “Sorry, we only have town, supervisor,
    and salary. You’ll need to get the rest from elsewhere.” You tell them to send
    a CSV anyway. You’ll import what you can.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve included such a sample CSV you can download via the book’s resources at
    [https://www.nostarch.com/practical-sql-2nd-edition/](https://www.nostarch.com/practical-sql-2nd-edition/),
    called *supervisor_salaries.csv*. If you view the file with a text editor, you
    should see these two lines at the top:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You could try to import it using this basic `COPY` syntax:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'But if you do, PostgreSQL will return an error:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The problem is that your table’s first column is the auto-incrementing `id`,
    but your CSV file begins with the text column `town`. Even if your CSV file had
    an integer present in its first column, the `GENERATED ALWAYS AS IDENTITY` keywords
    would prevent you from adding a value to `id`. The workaround for this situation
    is to tell the database which columns in the table are present in the CSV, as
    shown in [Listing 5-5](#listing5-5).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Listing 5-5: Importing salaries data from CSV to three table columns'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'By noting in parentheses 1 the three present columns after the table name,
    we tell PostgreSQL to only look for data to fill those columns when it reads the
    CSV. Now, if you select the first couple of rows from the table, you’ll see those
    columns filled with the appropriate values:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Importing a Subset of Rows with COPY
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Starting with PostgreSQL version 12, you can add a `WHERE` clause to a `COPY`
    statement to filter which rows from the source CSV you import into a table. You
    can see how this works using the supervisor salaries data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Start by clearing all the data you already imported into `supervisor_salaries`
    using a `DELETE` query.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This will remove data from the table, but it will not reset the `id` column’s
    `IDENTITY` column sequence. We’ll cover how to do that when we discuss table design
    in Chapter 8. When that query finishes, run the `COPY` statement in [Listing 5-6](#listing5-6),
    which adds a `WHERE` clause that filters the import to include only rows in which
    the `town` column in the CSV input matches New Brillig.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Listing 5-6: Importing a subset of rows with `WHERE`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, run `SELECT * FROM supervisor_salaries;` to view the contents of the
    table. You should see just one row:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This is a handy shortcut. Now, let’s see how to use a temporary table to do
    even more data wrangling during an import.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Adding a Value to a Column During Import
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What if you know that “Mills” is the name that should be added to the `county`
    column during the import, even though that value is missing from the CSV file?
    One way to modify your import to include the name is by loading your CSV into
    a *temporary table* before adding it to `supervisors_salary`. Temporary tables
    exist only until you end your database session. When you reopen the database (or
    lose your connection), those tables disappear. They’re handy for performing intermediary
    operations on data as part of your processing pipeline; we’ll use one to add the
    county name to the `supervisor_salaries` table as we import the CSV.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Again, clear the data you’ve imported into `supervisor_salaries` using a `DELETE`
    query. When it completes, run the code in [Listing 5-7](#listing5-7), which will
    make a temporary table and import your CSV. Then, we will query data from that
    table and include the county name for an insert into the `supervisor_salaries`
    table.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Listing 5-7: Using a temporary table to add a default value to a column during
    import'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: This script performs four tasks. First, we create a temporary table called `supervisor_salaries_temp`
    1 based on the original `supervisor_salaries` table by passing as an argument
    the `LIKE` keyword followed by the source table name. The keywords `INCLUDING
    ALL` tell PostgreSQL to not only copy the table rows and columns but also components
    such as indexes and the `IDENTITY` settings. Then we import the *supervisor_salaries.csv*
    file 2 into the temporary table using the now-familiar `COPY` syntax.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use an `INSERT` statement 3 to fill the salaries table. Instead of
    specifying values, we employ a `SELECT` statement to query the temporary table.
    That query specifies `Mills` as the value for the second column, not as a column
    name, but as a string inside single quotes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we use `DROP TABLE` to erase the temporary table 4 since we’re done
    using it for this import. The temporary table will automatically disappear when
    you disconnect from the PostgreSQL session, but this removes it now in case we
    want to do another import and use a fresh temporary table for another CSV.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'After you run the query, run a `SELECT` statement on the first couple of rows
    to see the effect:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You’ve filled the `county` field with a value even though your source CSV didn’t
    have one. The path to this import might seem laborious, but it’s instructive to
    see how data processing can require multiple steps to get the desired results.
    The good news is that this temporary table demo is an apt indicator of the flexibility
    SQL offers to control data handling.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Using COPY to Export Data
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When exporting data with `COPY`, rather than using `FROM` to identify the source
    data, you use `TO` for the path and name of the output file. You control how much
    data to export—an entire table, just a few columns, or the results of a query.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at three quick examples.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Exporting All Data
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest export sends everything in a table to a file. Earlier, you created
    the table `us_counties_pop_est_2019` with 16 columns and 3,142 rows of census
    data. The SQL statement in [Listing 5-8](#listing5-8) exports all the data to
    a text file named *us_counties_export.txt*. To demonstrate the flexibility you
    have in choosing output options, the `WITH` keyword tells PostgreSQL to include
    a header row and use the pipe symbol instead of a comma for a delimiter. I’ve
    used the *.txt* file extension here for two reasons. First, it demonstrates that
    you can name your file with an extension other than *.csv*; second, we’re using
    a pipe for a delimiter, not a comma, so I want to avoid calling the file *.csv*
    unless it truly has commas as a separator.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Remember to change the output directory to your preferred save location.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Listing 5-8:Exporting an entire table with `COPY`
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'View the export file with a text editor to see the data in this format (I’ve
    truncated the results):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The file includes a header row with column names, and all columns are separated
    by the pipe delimiter.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Exporting Particular Columns
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You don’t always need (or want) to export all your data: you might have sensitive
    information, such as Social Security numbers or birthdates, that need to remain
    private. Or, in the case of the census county data, maybe you’re working with
    a mapping program and only need the county name and its geographic coordinates
    to plot the locations. We can export only these three columns by listing them
    in parentheses after the table name, as shown in [Listing 5-9](#listing5-9). Of
    course, you must enter these column names precisely as they’re listed in the data
    for PostgreSQL to recognize them.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Listing 5-9: Exporting selected columns from a table with `COPY`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Exporting Query Results
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Additionally, you can add a query to `COPY` to fine-tune your output. In [Listing
    5-10](#listing5-10) we export the name and state of only those counties whose
    names contain the letters `mill`, catching it in either uppercase or lowercase
    by using the case-insensitive `ILIKE` and the `%` wildcard character we covered
    in “Using LIKE and ILIKE with WHERE” in Chapter 3. Also note that for this example,
    I’ve removed the `DELIMITER` keyword from the `WITH` clause. As a result, the
    output will default to comma-separated values.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Listing 5-10:Exporting query results with `COPY`
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the code, your output file should have nine rows with county
    names including Miller, Roger Mills, and Vermillion:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Importing and Exporting Through pgAdmin
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At times, the SQL `COPY` command won’t be able to handle certain imports and
    exports. This typically happens when you’re connected to a PostgreSQL instance
    running on a computer other than yours. A machine in a cloud computing environment
    such as Amazon Web Services is a good example. In that scenario, PostgreSQL’s
    `COPY` command will look for files and file paths that exist on that remote machine;
    it can’t find files on your local computer. To use `COPY`, you’d need to transfer
    your data to the remote server, but you might not always have the rights to do
    that.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: One workaround is to use pgAdmin’s built-in import/export wizard. In pgAdmin’s
    object browser (the left vertical pane), locate the list of tables in your `analysis`
    database by choosing **Databases**▶**analysis**▶**Schemas**▶**public**▶**Tables**.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Next, right-click the table you want to import to or export from, and select
    **Import/Export**. A dialog appears that lets you choose to either import or export
    from that table, as shown in [Figure 5-1](#figure5-1).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![f05001](Images/f05001.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-1: The pgAdmin Import/Export dialog'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: To import, move the Import/Export slider to **Import**. Then click the three
    dots to the right of the **Filename** box to locate your CSV file. From the Format
    drop-down list, choose **csv**. Then adjust the header, delimiter, quoting, and
    other options as needed. Click **OK** to import the data.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: To export, use the same dialog and follow similar steps.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 18, when we discuss using PostgreSQL from your computer’s command
    line, we’ll explore another way to accomplish this using a utility called `psql`
    and its `\copy` command. pgAdmin’s import/export wizard actually uses `\copy`
    in the background but gives it a friendlier face.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you’ve learned how to bring external data into your database, you can
    start digging into a myriad of datasets, whether you want to explore one of the
    thousands of publicly available datasets, or data related to your own career or
    studies. Plenty of data is available in CSV format or a format easily convertible
    to CSV. Look for data dictionaries to help you understand the data and choose
    the right data type for each field.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The census data you imported as part of this chapter’s exercises will play a
    starring role in the next chapter, in which we explore math functions with SQL.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
