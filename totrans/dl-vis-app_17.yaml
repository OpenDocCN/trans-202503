- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Backpropagation
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播
- en: '![](Images/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/chapterart.png)'
- en: As we’ve seen, a neural network is just a collection of neurons, each doing
    its own little calculation and then passing on its results to other neurons. How
    can we train such a thing to produce the results we want? And how can we do it
    efficiently?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，神经网络只是由一组神经元组成，每个神经元进行自己的小计算，然后将结果传递给其他神经元。我们如何训练这样一个系统，让它产生我们想要的结果呢？我们又如何高效地完成这项任务？
- en: The answer is *backpropagation*, or simply *backprop*. Without backprop, we
    wouldn’t have today’s widespread use of deep learning because we wouldn’t be able
    to train big networks in reasonable amounts of time. Every modern deep learning
    library provides a stable and efficient implementation of backprop. Even though
    most people will never implement backprop, it’s important to understand the algorithm
    because so much of deep learning depends on it.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是*反向传播*，简称*backprop*。没有反向传播，今天深度学习的广泛应用是不可能的，因为我们无法在合理的时间内训练大型网络。每个现代深度学习库都提供了稳定高效的反向传播实现。尽管大多数人永远不会实现反向传播，但理解这个算法很重要，因为深度学习中有很多内容都依赖于它。
- en: Most introductions to backprop are presented mathematically, as a collection
    of equations with associated discussion (Fullér 2010). As usual, we skip the mathematics
    here and focus instead on the concepts. The middle of this chapter, where we discuss
    the core of backprop, is the most detailed part of this book. You might want to
    read it lightly the first time to get the big picture for what’s going on and
    how the pieces fit together. Then, if you like, you can come back and take it
    more slowly, following the individual steps.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数反向传播的介绍都以数学形式呈现，作为一系列方程和相关讨论（Fullér 2010）。和往常一样，我们这里跳过数学内容，专注于概念。章节的中间部分是本书中讨论反向传播核心的最详细部分。第一次阅读时，你可能想轻松读一下，以便大致了解发生了什么以及各部分是如何结合的。然后，如果你愿意，可以返回来慢慢阅读，跟随每一个步骤。
- en: A High-Level Overview of Training
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练的高级概述
- en: Networks learn by minimizing their mistakes. The process begins with a number
    called a *cost*, *loss*, or *penalty*, for each mistake. During training, the
    network reduces the cost, resulting in outputs closer to what we want.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通过最小化错误来学习。这个过程从一个被称为*成本*、*损失*或*惩罚*的数值开始，用来衡量每一个错误。在训练过程中，网络减少成本，输出结果会越来越接近我们想要的目标。
- en: Punishing Error
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 惩罚错误
- en: Suppose we have a classifier that identifies each input as one of five classes,
    numbered 1 to 5\. The class that has the largest value is the network’s prediction
    for each input’s class. Our classifier is brand-new and has had no training, so
    all of the weights have small random values. [Figure 14-1](#figure14-1) shows
    the network classifying its first input sample.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个分类器，可以将每个输入识别为五个类别中的一个，类别编号为1到5。具有最大值的类别是网络对每个输入类别的预测。我们的分类器是全新的，尚未训练，因此所有的权重都有小的随机值。[图14-1](#figure14-1)展示了网络对其第一个输入样本的分类。
- en: '![F14001](Images/F14001.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![F14001](Images/F14001.png)'
- en: 'Figure 14-1: A neural network processing a sample and assigning it to class
    1\. We want it to be assigned to class 3.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-1：一个神经网络处理一个样本并将其分配到类别1。我们希望它被分配到类别3。
- en: In this example, the network has decided that the sample belongs to class 1
    because the largest output, 0.35, is from output 1 (we’re assuming we have a softmax
    layer at the end of the network, so the outputs add up to 1). Unfortunately, the
    sample was labeled as belonging to class 3\. We shouldn’t have expected the right
    answer. The network can easily have thousands, or even millions, of weights, and
    they currently all have their initial, random values. Therefore, the outputs are
    just random values as well. Even if the network had predicted class 3 for this
    sample, it would have been pure luck.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，网络认为该样本属于类别1，因为最大输出0.35来自输出1（假设我们在网络的最后有一个softmax层，所以输出加起来等于1）。不幸的是，样本被标记为属于类别3。我们本不应期待正确答案。网络可能有成千上万，甚至数百万个权重，而它们目前都只有初始的随机值。因此，输出也只是随机值。如果网络预测了该样本为类别3，那也只是纯粹的运气。
- en: When a prediction doesn’t match that sample’s label, we can come up with a single
    number to tell us just how wrong this answer is. For example, if class 3 were
    to get almost the largest score, we say the network would be more correct (or
    less wrong) relative to assigning class 3 the smallest score. We call this number
    describing the mismatch between the label and the prediction the *error score*,
    or *error*, or sometimes the *penalty*, or *loss* (if the word *loss seems like
    a strange synonym for “error,” it may help to think of it as describing how much
    information is “lost” because we have a wrong answer).*
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当预测与该样本的标签不匹配时，我们可以得出一个单一的数字，告诉我们这个答案错得有多远。例如，如果类别3几乎得到了最大得分，我们就说网络相较于将类别3赋予最小得分时，会更正确（或者更不错误）。我们称这个描述标签与预测之间不匹配的数字为*误差分数*，或*误差*，有时也叫*惩罚*，或*损失*（如果*损失*这个词似乎是“误差”的一个奇怪同义词，可以考虑将其看作描述由于错误答案“丧失”的信息量）。
- en: '*The error (or loss) is a floating-point number that can take on any value,
    though often we set things up so that it’s always positive. The larger the error,
    the more “wrong” our network’s prediction is for the label of this input. An error
    of zero means that the network predicted the sample’s label correctly. In a perfect
    world, we’d get the error down to zero for every sample in the training set. In
    practice, we usually settle for getting as close as we can.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*误差（或损失）是一个浮动的数值，它可以取任何值，尽管我们通常会设置使其始终为正数。误差越大，说明网络对于该输入的标签预测越“错误”。零误差意味着网络正确预测了样本的标签。在一个理想的世界里，我们希望将每个训练样本的误差降到零。实际上，我们通常会尽可能接近零。*'
- en: Although in this chapter we focus on reducing the error on specific samples
    (or groups of samples), our overall goal is to minimize the total error for the
    entire training set, which is usually just the sum of the individual errors.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在本章中我们主要关注减少特定样本（或样本组）的误差，但我们的总体目标是最小化整个训练集的总误差，通常这只是各个个体误差的总和。
- en: 'The way we choose to determine the error gives us tremendous flexibility in
    guiding the network’s learning process. The thinking can seem a little backward,
    however, because the error tells the network what *not* to do. It’s like the apocryphal
    quote about sculpting: to carve an elephant, you simply start with a block of
    stone and chip away everything that doesn’t look like an elephant (Quote Investigator
    2020).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择确定误差的方式赋予了我们极大的灵活性来引导网络的学习过程。然而，这种思维方式看起来可能有些逆向，因为误差告诉网络应该*不做*什么。这就像雕刻的大名鼎鼎的引语：要雕刻一头大象，你只需从一块石头开始，剔除所有不看起来像大象的部分（引言调查员，2020年）。
- en: 'In our case, we start with an initialized network and then use the error term
    to chip away all the behavior we don’t want. In other words, we don’t really teach
    the network to find the correct answers. Instead, we penalize incorrect answers
    by assigning them a positive amount of error. The only way the network can reduce
    the overall error is to avoid incorrect answers, so that’s what it learns to do.
    This is a powerful idea: to get the behavior we want, we penalize the behavior
    we don’t want.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们从一个初始化的网络开始，然后利用误差项去消除我们不希望的行为。换句话说，我们并不是在教网络找出正确的答案。相反，我们通过赋予错误答案一个正的误差值来惩罚它们。网络减少总体误差的唯一方法就是避免错误答案，因此它学会了这样做。这是一个强大的理念：为了获得我们想要的行为，我们惩罚我们不希望的行为。
- en: If we want to penalize several things at once, we compute a *value*, or *term*,
    for each one and add them up to get the total error. For instance, we might want
    our classifier to predict the correct class *and* assign it a score that is at
    least twice as large as the score for the next-closest class. We can compute numbers
    representing both desires and use their sum as our error term. The only way for
    the network to drive the error down to zero (or as close to zero as it can get)
    is to change its weights to achieve both goals.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要同时惩罚多个因素，我们会为每个因素计算一个*值*，或者*项*，然后将它们加起来得到总误差。例如，我们可能希望我们的分类器预测正确的类别*并*赋予它一个得分，且这个得分至少是下一个最接近类别得分的两倍。我们可以计算代表这两个愿望的数值，并将它们的和作为我们的误差项。网络唯一降低误差到零（或者尽可能接近零）的方式是改变其权重，以实现这两个目标。
- en: A popular error term comes from the observation that learning is often the most
    efficient when the weights in the network are all in a small range, such as [–1,
    1]. To enforce this, we can include an error term that has a large value when
    the weights get too far from this range. This is called *regularization*. In order
    to minimize the error, the network learns to keep the weights small.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的误差项来源于观察到，当网络中的权重都处于一个小范围内时，学习效率通常是最高的，比如[–1, 1]。为了强制实现这一点，我们可以加入一个误差项，当权重偏离这个范围太远时，它会产生较大的值。这叫做*正则化*。为了最小化误差，网络会学会保持权重较小。
- en: All of this raises the natural question of how on earth the network is able
    to accomplish the goal of minimizing the error. That’s the point of this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都引出了一个自然的问题，那就是网络是如何实现最小化误差的目标的。这就是本章的重点。
- en: To keep things simple, we’ll use an error measure with just one term that punishes
    a mismatch between the network’s prediction and the label. Everything we see in
    the rest of this chapter works identically when there are more terms in the error.
    Our first algorithm for teaching the network is just a thought experiment since
    it would be absurdly slow on today’s computers. But the ideas resulting from this
    experiment form the conceptual basis for the more efficient techniques we discuss
    later in this chapter.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，我们将使用一个仅包含一个项的误差度量，惩罚网络的预测与标签之间的不匹配。在本章的其余部分，所有的内容在误差项更多时的表现是相同的。我们为网络教学设计的第一个算法只是一个思想实验，因为在今天的计算机上它会非常缓慢。但这个实验产生的思想为我们在本章后面讨论的更高效的技术奠定了概念基础。
- en: A Slow Way to Learn
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一种缓慢的学习方式
- en: Let’s stick with our running example of a classifier trained with supervised
    learning. We’ll give the network a sample and compare the system’s prediction
    with the sample’s label. If the network gets it right and predicts the correct
    label, we won’t change anything and we’ll move on to the next sample (as the proverb
    goes, “If it ain’t broke, don’t fix it” [Seung 2005]). But if the result for a
    particular sample is incorrect, we’ll try to improve things.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用一个通过监督学习训练的分类器作为例子。我们将给网络一个样本，并将系统的预测与样本的标签进行比较。如果网络预测正确并且预测了正确的标签，我们就不做任何改变，直接进入下一个样本（正如俗话所说：“如果它没坏，就不要修理”[Seung
    2005]）。但如果某个特定样本的结果是错误的，我们将尝试改进。
- en: Let’s make this improvement in a simple way. We’ll pick one weight at random
    from the whole network and *freeze* all the other values so they can’t change.
    We already know the error associated with the weight’s current value, so we create
    a small random value centered around zero, which we’ll call *m,* add that to that
    weight, and reevaluate the same sample again. This change to one weight causes
    a ripple effect through the rest of the network, as every neuron that depends
    on a computation involving that neuron’s output also changes. The result is a
    new set of predictions, and thus a new error for that sample.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一种简单的方法来进行改进。我们将从整个网络中随机选择一个权重，并*冻结*其他所有值，使它们无法改变。我们已经知道与该权重当前值相关的误差，所以我们创建一个以零为中心的小随机值，称之为*m*，将其加到该权重上，并重新评估相同的样本。这对一个权重的改变会通过整个网络产生连锁反应，因为每个依赖于该神经元输出进行计算的神经元也会发生变化。结果是一个新的预测集，因此该样本的误差也会发生变化。
- en: If the new error is less than the previous error, then we’ve made things better
    and we keep this change. If the results didn’t get better, then we need to undo
    the change. Now we pick another weight at random, modify it by another random
    amount, reevaluate the network to see if we want to keep that change, pick another
    weight, modify it, and so on, again and again.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果新的误差小于之前的误差，那么我们就改进了，保持这个变化。如果结果没有变好，那么我们需要撤销这个变化。现在我们随机选择另一个权重，按另一个随机值修改它，重新评估网络，看看是否要保持这个变化，再选择另一个权重，修改它，依此类推，一遍又一遍。
- en: We can continue nudging weights until the results improve by a certain amount,
    we decide we’ve tried enough times, or we decide to stop for any other reason.
    At this point, we select the next sample and tune lots of weights again. When
    we’ve used all the samples in our training set, we just go through them all again
    (maybe in a different order), over and over. The idea is that each little improvement
    brings us closer to a network that accurately predicts the label for every sample.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续微调权重，直到结果改进到某个程度，或者我们决定已经尝试了足够多的次数，或者我们出于其他任何原因决定停止。此时，我们选择下一个样本，再次调整许多权重。当我们用完训练集中的所有样本后，我们就再次遍历这些样本（可能顺序不同），一遍又一遍。这个想法是，每一次小的改进都会让我们更接近一个能够准确预测每个样本标签的网络。
- en: With this technique, we expect the network to slowly improve, though there may
    be setbacks along the way. For example, later samples may cause changes that ruin
    the improvements we just made for earlier samples.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个技术，我们预计网络会慢慢改进，尽管过程中可能会有挫折。例如，后来的样本可能会导致某些变化，破坏我们刚刚为早期样本所做的改进。
- en: Given enough time and resources, we expect that the network will eventually
    improve to the point where it’s predicting every sample as well as it can. The
    important word in that last sentence is *eventually*. As in, “The water will boil,
    eventually,” or “The Andromeda galaxy will collide with our Milky Way galaxy,
    eventually” (NASA 2012). Although the concepts are right, this technique is definitely
    not practical. Modern networks can have millions of weights. Trying to find the
    best values for all those weights with this algorithm is just not realistic.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有足够的时间和资源，我们预计网络最终会改进到能够尽可能好地预测每个样本的程度。那句话中的关键词是*最终*。就像“水最终会沸腾”或“仙女座星系最终会与我们的银河系碰撞”一样（NASA
    2012）。虽然这些概念是对的，但这个技术显然不切实际。现代网络可能有数百万个权重。用这个算法去寻找所有权重的最佳值根本不现实。
- en: Our goal in the rest of this chapter is to take this rough idea and restructure
    it into a vastly more practical algorithm.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 本章其余部分的目标是将这个粗略的想法重新结构化为一个更为实用的算法。
- en: Before we move on, it’s worth noting that because we’re focusing on weights,
    we’re automatically adjusting the influence of each neuron’s bias, thanks to the
    bias trickwe saw in Chapter 13\. That means we don’t have to think about the bias
    terms, which makes everything simpler.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，值得注意的是，由于我们专注于权重调整，我们实际上通过第13章中看到的偏置技巧自动调整了每个神经元偏置的影响。这意味着我们不需要考虑偏置项，这使得一切变得更简单。
- en: Let’s now consider how we might improve our incredibly slow weight-changing
    algorithm.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑如何改进我们极其缓慢的权重变化算法。
- en: Gradient Descent
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'The algorithm of the last section improved our network, but at a glacial pace.
    One big source of inefficiency was that half of our adjustments to the weights
    were in the wrong direction: we added a value when we should have subtracted it,
    and vice versa. That’s why we had to undo our changes when the error went up.
    Another problem is that we tuned each weight one by one, which required us to
    evaluate an immense number of samples. Let’s solve these problems.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节的算法改进了我们的网络，但速度极其缓慢。一个效率低下的主要原因是我们对权重的调整有一半是错误的方向：我们应该减去的值却加了上去，反之亦然。这就是为什么当误差增大时，我们不得不撤销更改的原因。另一个问题是我们一个个地调整每个权重，这需要评估大量的样本。让我们解决这些问题。
- en: We can double our training speed if we know beforehand whether we want to nudge
    each weight in a positive or negative direction. We can get exactly that information
    from the gradientof the error with respect to that weight. Recall that we met
    the gradient in Chapter 5, where it told us how the height of a surface changes
    as each of its parameters changes. Let’s narrow that down for the present case.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们事先知道每个权重应朝正方向还是负方向微调，我们就可以将训练速度加倍。我们可以通过该权重的误差梯度精确获得这些信息。回想一下我们在第5章中接触到的梯度，它告诉我们随着每个参数的变化，表面高度如何变化。让我们将这个概念缩小到当前的情况。
- en: As before, we’re going to freeze the whole network except for one weight. If
    we plot the value of that weight on a horizontal axis, we can plot the network’s
    error for that weight vertically. The errors, taken together, form a curve called
    the *error curve*. In this situation, we can find the gradient (or derivative)
    of the error at any particular value of the weight by finding the slope of the
    error curve above that weight.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们将冻结整个网络，除了一个权重。如果我们将该权重的值绘制在横轴上，我们可以将该权重的网络误差在纵轴上绘制出来。这些误差合在一起，形成一条曲线，称为*误差曲线*。在这种情况下，我们可以通过找到该权重上方误差曲线的斜率来找到该权重处误差的梯度（或导数）。
- en: If the gradient directly above the weight is positive (that is, the line goes
    up as we move to the right), then increasing the value of the weight (moving it
    to the right) causes the error to go up. Similarly, and more usefully for us,
    decreasing the value of the weight (moving it to the left) causes the error to
    go down. If the slope of the error is negative, the situations are reversed.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果该权重上方的梯度是正的（即，线条向右移动时上升），那么增大该权重的值（向右移动）会导致误差增加。类似地，对我们更有用的是，减小该权重的值（向左移动）会导致误差减少。如果误差的斜率是负的，情况则相反。
- en: '[Figure 14-2](#figure14-2) shows two examples.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-2](#figure14-2)展示了两个示例。'
- en: '![F14002](Images/F14002.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![F14002](Images/F14002.png)'
- en: 'Figure 14-2: The gradient tells us what happens to the error (the black curves)
    if we make a weight smaller or larger, for two different error curves. Each figure
    shows the gradient at two weights.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-2：梯度告诉我们，如果我们增大或减小某个权重，误差（黑色曲线）会发生什么变化，对于两条不同的误差曲线。每个图展示了两个权重处的梯度。
- en: The error curve for every weight in the network is different because every weight
    has a different effect on the final error. But if we can find the gradient for
    a specific weight, we’ve solved the problem of guessing whether it needs to increase
    or decrease in order to reduce the error. If we can find the gradients for all
    the weights, we can adjust them all at once, rather than one by one. If we can
    adjust every weight simultaneously, using its own specific gradient to tell us
    whether to make it bigger or smaller, we have an efficient way to improve our
    network.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每个权重在网络中的误差曲线都是不同的，因为每个权重对最终误差的影响不同。但是，如果我们能找到某个特定权重的梯度，我们就解决了是否需要增加或减少该权重以减少误差的问题。如果我们能找到所有权重的梯度，我们可以一次性调整它们，而不是一个一个地调整。如果我们能同时调整每个权重，使用它自己特定的梯度来告诉我们是增大还是减小，那么我们就有了一种有效的改进网络的方法。
- en: This is just what we do. Because we use the gradient to move each weight to
    produce a lower value on the error curve, we call the algorithm *gradient descent*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们所做的。因为我们使用梯度来移动每个权重，以产生误差曲线上的更低值，所以我们称该算法为*梯度下降*。
- en: Before we dig into gradient descent, notice that this algorithm makes the assumption
    that tweaking all the weights independently and simultaneously after we evaluate
    an incorrect sample leads to a reduction in the error, not just for that sample,
    but for the entire training set, and by extension, all data that we see after
    the network is released. This is a bold assumption because we’ve already noted
    how changing one weight can cause ripple effects through the rest of the network.
    Changing the output of one neuron changes the inputs, and thus the outputs, of
    all neurons that use that value, which in turn changes their gradients. If we’re
    unlucky, some weights that had a positive gradient might now have a negative gradient,
    or vice versa. That means if we stick with the gradients we computed, changing
    those weights makes the error bigger, not smaller. To control this problem, we
    usually make small changes to every weight in the hopes that any such mistakes
    won’t drown out our improvements.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论梯度下降之前，注意到该算法假设，在我们评估一个错误的样本后，独立且同时调整所有权重会导致误差减少，不仅是对该样本，而且是对整个训练集，并且在网络发布后，所有看到的数据也会受益。这是一个大胆的假设，因为我们已经注意到，改变一个权重可能会通过网络的其余部分引起涟漪效应。改变一个神经元的输出会改变所有使用该值的神经元的输入和输出，从而改变它们的梯度。如果我们运气不好，一些原本有正梯度的权重可能会变成负梯度，反之亦然。这意味着如果我们坚持使用我们计算出来的梯度，改变这些权重会让误差变大，而不是变小。为了控制这个问题，我们通常会对每个权重进行小幅调整，希望这些错误不会淹没我们的改进。
- en: Getting Started
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 入门
- en: Let’s reduce the overall error by adjusting the network’s weights in two steps.
    In the first step, called *backpropagation* or *backprop,* we visit each neuron
    where we calculate and store a number that is related to the network’s error.
    Once we have this value for every neuron, we use it to update every weight coming
    into that neuron. This second step is called the *update* *step*, or the *optimization
    step*. It’s not typically considered part of backpropagation, but sometimes people
    casually roll the two steps together and call the whole thing backpropagation.
    This chapter focuses on just the first step. Chapter 15 focuses on optimization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过两步调整网络的权重来减少整体误差。在第一步中，称为*反向传播*或*反传*，我们访问每个神经元，在那里计算并存储一个与网络误差相关的数值。一旦我们为每个神经元获得这个值，我们就用它来更新进入该神经元的每个权重。第二步称为*更新*
    *步骤*，或*优化步骤*。它通常不被视为反向传播的一部分，但有时人们会随便将这两个步骤合并在一起，称整个过程为反向传播。本章专注于第一步，第15章专注于优化。
- en: In this discussion, we’re going to ignore activation functions. Their nonlinear
    nature is essential to making neural networks work, but that same nature introduces
    a lot of detail that isn’t relevant to understanding the essence of backprop.
    Despite this simplification for the point of a clearer discussion, activation
    functions are definitely accounted for in any implementation of backprop.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本讨论中，我们将忽略激活函数。激活函数的非线性特性对神经网络的正常工作至关重要，但这种特性引入了许多与理解反向传播本质无关的细节。尽管为了更清晰的讨论而进行简化，但在任何反向传播的实现中，激活函数肯定是被考虑在内的。
- en: 'With this simplification in place, we can make an important observation: When
    any neuron output in our network changes, the final output error changes by a
    proportional amount.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 进行此简化后，我们可以得出一个重要的观察：当我们网络中的任何神经元输出发生变化时，最终的输出误差也会按比例发生变化。
- en: 'Let’s unpack that. We only care about two types of values in a neural network:
    weights (which we can set and change as we please), and neuron outputs (which
    are computed automatically and are beyond our direct control). Except for the
    very first layer, a neuron’s input values are each the output of a previous neuron
    times the weight of the edge that output travels on. Each neuron’s output is just
    the sum of all of these weighted inputs. Without an activation function, every
    change in a neuron’s output is proportional to the changes in its inputs, or the
    weights on those inputs. If the inputs themselves are constant, the only way for
    a neuron’s output to change (and thus affect the final error) is if the weights
    on its inputs change.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分析一下。我们在神经网络中只关心两种类型的值：权重（我们可以随意设置和更改）和神经元输出（这些是自动计算的，超出了我们直接控制的范围）。除了第一层之外，每个神经元的输入值都是前一个神经元的输出与该输出经过的边的权重的乘积。每个神经元的输出只是所有这些加权输入的总和。如果没有激活函数，每个神经元输出的变化与其输入或这些输入的权重的变化成比例。如果输入本身保持不变，神经元输出变化（从而影响最终误差）唯一的方式是输入上的权重发生变化。
- en: Imagine we’re looking at a neuron whose output has just changed. What happens
    to the network’s error as a result? Without activation functions, the only operations
    in our network are multiplication and addition. If we write down the math (which
    we won’t do), it turns out that the change in the final error is always proportional
    to the change in the neuron’s output.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们正在查看一个刚刚发生输出变化的神经元。网络的误差会因此发生什么变化呢？没有激活函数时，网络中的唯一操作是乘法和加法。如果我们写下数学公式（我们这里不写），结果表明，最终误差的变化总是与神经元输出的变化成比例。
- en: The connection between any changein the neuron’s output and the resulting changein
    the final error is just the neuron’s change multiplied by some number. This number
    goes by various names, but the most popular is the lowercase Greek letter *δ*
    (delta), though sometimes the uppercase version, Δ, is used. Mathematicians often
    use the delta character to mean “change” of some sort, so this was a natural (if
    terse) choice of name.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 任何神经元输出变化与最终误差变化之间的关系就是神经元的变化乘以一个数值。这个数值有各种名称，但最流行的是小写希腊字母*δ*（德尔塔），有时也使用大写版本Δ。数学家们常用德尔塔字符表示某种“变化”，因此这是一个自然（虽然简洁）的名称选择。
- en: Every neuron has a delta, or *δ*, associated with it as a result of evaluating
    the current network with the current sample. This is a real number that can be
    big or small, positive or negative. Assuming the network’s input doesn’t change
    and the rest of the network is frozen, if a neuron’s output changes by a particular
    amount, we can multiply that change by the neuron’s delta to see how the entire
    network’s output will change.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元都有一个与之相关的delta，或称*δ*，这是通过使用当前样本评估当前网络得到的结果。这是一个实数，可能很大或很小，正数或负数。假设网络的输入没有变化，且其余网络被冻结，如果一个神经元的输出发生了特定的变化，我们可以将这个变化乘以神经元的delta，看看整个网络的输出将如何变化。
- en: To illustrate the idea, let’s focus just on one neuron’s output for a moment.
    Let’s add some arbitrary number to its output just before that value emerges.
    [Figure 14-3](#figure14-3) shows the idea graphically, where we use the letter
    *m* (for “modification”) for this extra value.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个概念，我们暂时只关注一个神经元的输出。我们在其输出值产生之前，添加一些任意的数字。[图 14-3](#figure14-3)图示了这一概念，其中我们使用字母*m*（代表“修改”）表示这个额外的值。
- en: '![F14003](Images/F14003.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![F14003](Images/F14003.png)'
- en: 'Figure 14-3: Computing the change in the network’s final error due to a change
    in a neuron’s output'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-3：计算由于神经元输出变化导致的网络最终误差变化
- en: Because the output will change by *m*, we know the change in the final error
    is *m* times the neuron’s *δ*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因为输出会发生*m*的变化，我们知道最终误差的变化是*m*乘以神经元的*δ*。
- en: In [Figure 14-3](#figure14-3), we changed the output directly by placing the
    value *m* inside the neuron. Alternatively, we can cause a change in the output
    by changing one of the inputs. Let’s change the value that’s coming in from any
    other neuron. The same logic holds as for [Figure 14-3](#figure14-3) and is shown
    in [Figure 14-4](#figure14-4). We can instead add *m* to the value coming in from
    neurons A or C if we prefer; all that matters is that the output of D changes
    by *m*. Since we’re still just changing the output by *m*, we find the change
    in the final error by multiplying it by the same value of *δ* as in [Figure 14-3](#figure14-3).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 14-3](#figure14-3)中，我们通过将值*m*直接放入神经元来改变输出。或者，我们可以通过改变输入中的一个值来引起输出的变化。让我们改变来自其他神经元的输入值。与[图
    14-3](#figure14-3)中的逻辑相同，这在[图 14-4](#figure14-4)中也有展示。如果我们愿意，也可以将*m*添加到来自神经元A或C的输入值；关键是D的输出发生了*m*的变化。由于我们仍然只是通过*m*改变输出，我们通过将其与[图
    14-3](#figure14-3)中相同的*δ*值相乘来找到最终误差的变化。
- en: '![F14004](Images/F14004.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![F14004](Images/F14004.png)'
- en: 'Figure 14-4: A variation of [Figure 14-3](#figure14-3), where we add *m* to
    the output of B (after it has been multiplied by the weight BD)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-4：是[图 14-3](#figure14-3)的变体，我们在B的输出中添加了*m*（在乘以权重BD之后）。
- en: '[Figure 14-3](#figure14-3) and [Figure 14-4](#figure14-4) illustrate that the
    change in the network’s final output can be predicted from either a change to
    any neuron’s output or any weight in the network.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-3](#figure14-3)和[图 14-4](#figure14-4)说明了网络最终输出的变化可以通过任何神经元输出或网络中的任何权重的变化来预测。'
- en: We can use the delta associated with each neuron to tell us whether each of
    its incoming weights should be nudged in the positive or negative direction.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用与每个神经元相关的delta，来告诉我们每个输入权重应该朝着正向还是负向调整。
- en: Let’s walk through an example.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来说明。
- en: This is where things start to get detailed. The basic idea is that the error
    will give us a gradient for every weight, and then we can use this gradient to
    adjust each weight by a little bit so that the overall error decreases. The mechanics
    for this aren’t super complicated, but there are some new ideas, some new names,
    and a bunch of details to keep straight. If it feels like too much to take in,
    you might want to skim this part of the chapter on first reading (up to, say,
    the section “Backprop on a Larger Network”), and return here later for a more
    complete understanding of the process.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候进入细节了。基本的概念是，误差会为每个权重提供一个梯度，然后我们可以使用这个梯度稍微调整每个权重，以减少整体误差。这个过程的机制并不复杂，但有一些新的概念、新的术语和很多细节需要理清。如果这部分内容让你觉得信息量太大，你可以在第一次阅读时略过这一部分（例如，跳过“在更大网络中的反向传播”部分），之后再回来深入理解这个过程。
- en: Backprop on a Tiny Neural Network
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小型神经网络的反向传播
- en: To get a handle on backprop, we’ll use a tiny network that classifies 2D points
    into two categories, which we’ll call class 1 and class 2\. If the points can
    be separated by a straight line, then we can do this job with just one neuron,
    but let’s use a little network because it lets us see the general principles.
    Let’s begin by looking at the network and giving a label to everything we care
    about. This will make later discussions simpler and easier to follow. [Figure
    14-5](#figure14-5) shows our little network, along with a name for each of its
    eight weights. For simplicity, we’ll leave out the usual softmax step after neurons
    C and D.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解反向传播，我们将使用一个小型网络，将2D点分类为两类，我们称之为类1和类2。如果这些点可以通过一条直线分开，那么我们只需一个神经元就能完成这个任务，但我们还是使用一个小网络，因为它让我们能看到一般的原理。让我们从查看网络并给每个我们关心的部分加上标签开始。这将使后续的讨论更加简洁易懂。[图14-5](#figure14-5)显示了我们的这个小网络，并为它的八个权重命名。为了简化，我们省略了神经元C和D之后的常规softmax步骤。
- en: '![F14005](Images/F14005.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![F14005](Images/F14005.png)'
- en: 'Figure 14-5: A simple neural network with four neurons'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-5：一个简单的四神经元神经网络
- en: Finally, we want to refer to the output and delta for every neuron. For this,
    let’s make little two-letter names by combining the neuron’s name with the value
    we want to refer to. So *Ao* and *Bo* are the names of the outputs of neurons
    A and B, and *Aδ* and *Bδ* are the delta values for those two neurons.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们想要引用每个神经元的输出和delta值。为此，我们通过将神经元的名称与我们想要引用的值结合，来创建简短的两字母名称。所以*Ao*和*Bo*是神经元A和B的输出名称，而*Aδ*和*Bδ*是这两个神经元的delta值。
- en: '[Figure 14-6](#figure14-6) shows these values stored with their neurons.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14-6](#figure14-6)显示了这些值与它们的神经元一起存储。'
- en: '![F14006](Images/F14006.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![F14006](Images/F14006.png)'
- en: 'Figure 14-6: Our simple network with the output and delta values for each neuron'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-6：我们简单的网络，每个神经元的输出和delta值
- en: We can watch what happens when neuron outputs change, causing changes to the
    error. Let’s label the change in the output of neuron A due to a change by an
    amount *m* as *Am*, the network’s final error as *E*, and the resulting change
    to the error as *Em*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察当神经元输出发生变化时，如何导致误差的变化。假设神经元A的输出因变化而变化的量为*m*，则将其标记为*Am*，网络的最终误差为*E*，而导致的误差变化为*Em*。
- en: Now we can be more precise about what happens to the error when a neuron’s output
    changes. If we have a change *Am* in the output of neuron A, then multiplying
    that change by *Aδ* gives us the change in the error. That is, the change *Em*
    is given by *Am* × *Aδ*. We think of the action of *Aδ* as multiplying, or scaling,
    the change in the output of neuron A, giving us the corresponding change in the
    error. [Figure 14-7](#figure14-7) shows the schematic setup we use in this chapter
    for visualizing the way changes in a neuron’s output are scaled by its delta to
    produce changes to the error.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以更精确地描述当神经元输出发生变化时，误差会发生什么变化。如果神经元A的输出发生变化*Am*，那么将该变化与*Aδ*相乘得到误差的变化。即，变化*Em*由*Am*
    × *Aδ*给出。我们认为*Aδ*的作用是将神经元A输出的变化进行乘法操作或缩放，从而得到对应的误差变化。[图14-7](#figure14-7)展示了我们在本章中用于可视化神经元输出的变化如何通过其delta值进行缩放，以产生对误差的变化的示意图。
- en: '![F14007](Images/F14007.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![F14007](Images/F14007.png)'
- en: 'Figure 14-7: Our schematic for visualizing how changes in a neuron’s output
    can change the network’s error'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-7：我们的示意图，用于可视化神经元输出的变化如何改变网络的误差
- en: At the left of [Figure 14-7](#figure14-7) we start withneuron A. We see the
    starting output of A, or *Ao*, a change in output *Am*, and its new output *Ao*
    + *Am*. The arrow inside the box for *Am* shows that this change is positive.
    This change is multiplied by *Aδ* to give us *Em*, the change in the error. We
    show this operation as a wedge, illustrating the amplification of *Am*. Adding
    *Em* to the previous value of the error, *E*, gives us the new error *E* + *Em*.
    In this case, both *Am* and *Aδ* are positive, so the change in the error *Am*
    × *Aδ* is also positive, increasing the error. When either (but not both) of *Am*
    or *Aδ* is negative, the error decreases.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图14-7](#figure14-7)的左侧，我们从神经元A开始。我们看到A的初始输出，或称*Ao*，输出变化*Am*，以及其新的输出*Ao* +
    *Am*。框内的箭头表示*Am*的变化是正向的。这个变化与*Aδ*相乘得到*Em*，即误差的变化。我们用楔形图示来表示这个操作，说明*Am*的放大过程。将*Em*加到之前的误差值*E*上，就得到了新的误差值*E*
    + *Em*。在这种情况下，*Am*和*Aδ*都是正的，因此误差变化*Am* × *Aδ*也是正的，导致误差增加。当*Am*或*Aδ*中的任意一个（但不是两个）为负时，误差会减小。
- en: Now that we’ve labeled everything, we’re finally ready to look at the backpropagation
    algorithm.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经标记了所有内容，终于可以开始查看反向传播算法了。
- en: Finding Deltas for the Output Neurons
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 找到输出神经元的增量
- en: 'Backpropagation is all about finding the delta value for each neuron. To do
    that, we find gradients of the error at the end of the network and then propagate,
    or move, those gradients backward to the start. So, we begin at the end: the output
    layer.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的核心就是找到每个神经元的增量值。为此，我们首先找到网络末端的误差梯度，然后将这些梯度反向传播，即向网络的起始位置移动。因此，我们从末端开始：输出层。
- en: Calculating the Network Error
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算网络误差
- en: The outputs of neuron C and D in our tiny network give us the probabilities
    that the input is in class 1 or class 2, respectively. In a perfect world, a sample
    that belongs to class 1 would produce a value of 1.0 for *P1* and 0.0 for *P2*,
    meaning that the system is certain that it belongs to class 1 and simultaneously
    certain that it does notbelong to class 2\. If the system’s a little less certain,
    we might get *P1* = 0.8 and *P2* = 0.2, telling us that it’s much more likely
    that the sample is in class 1.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们小型网络中的神经元C和D的输出给出了输入属于类1或类2的概率。在理想情况下，一个属于类1的样本将产生*P1* = 1.0和*P2* = 0.0的值，这意味着系统确信它属于类1，同时确信它不属于类2。如果系统的不确定性稍大，我们可能会得到*P1*
    = 0.8和*P2* = 0.2，告诉我们样本更可能属于类1。
- en: We want to come up with a single number to represent the network’s error. To
    do that, we compare the values of *P1* and *P2* with the label for this sample.
    The easiest way to make that comparison is if the label is one-hot encoded, as
    we saw in Chapter 10\. Recall that one-hot encoding makes a list of zeros as long
    as the number of classes, except for a 1 in the entry corresponding to the correct
    class. In our case, we have only two classes, so our labels are (1, 0) for a sample
    in class 1, and (0, 1) for a sample in class 2\. Sometimes this form of label
    is also called a *target*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要得到一个单一的数字来表示网络的误差。为此，我们将*P1*和*P2*的值与该样本的标签进行比较。最简单的比较方式是，如果标签采用了独热编码，如我们在第10章中所看到的那样。回想一下，独热编码将零列表示为类别数的长度，只有对应正确类别的位置上是1。在我们的案例中，我们只有两个类别，因此类1的样本标签为(1,
    0)，类2的样本标签为(0, 1)。有时这种标签形式也称为*目标*。
- en: 'Let’s put the predictions *P1* and *P2* into a list as well: (*P1*, *P2*).
    Now we can just compare the lists. We almost always use cross entropy for this,
    as discussed in Chapter 6\. [Figure 14-8](#figure14-8) shows the idea.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将预测值*P1*和*P2*也放入一个列表中：(*P1*, *P2*)。现在我们可以直接比较这两个列表。我们几乎总是使用交叉熵来进行此比较，如第6章中所讨论的那样。[图14-8](#figure14-8)展示了这一思想。
- en: '![F14008](Images/F14008.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![F14008](Images/F14008.png)'
- en: 'Figure 14-8: Finding the error from a sample'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-8：从样本中找出误差
- en: Every deep learning library provides a built-in cross entropy function to help
    us find the error in a classifier such as this one. In addition to computing the
    network’s error, the function also provides a gradient to tell us how the error
    will change if we increase any one of its four inputs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 每个深度学习库都提供了一个内置的交叉熵函数，帮助我们计算分类器中的误差，如本例所示。除了计算网络的误差外，函数还提供了梯度，告诉我们如果增加四个输入中的任何一个，误差将如何变化。
- en: Using the error gradient, we can look at the value coming out of every neuron
    in the output layer, and determine if we’d like that value to become more positive
    or more negative. We will later nudge each neuron in the direction that causes
    the error to decrease.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用误差梯度，我们可以查看输出层中每个神经元的值，并确定是否希望该值变得更正或更负。稍后，我们会将每个神经元朝着使误差减少的方向进行调整。
- en: Drawing Our Error
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 绘制我们的误差
- en: Let’s look at an error curve. We’ll also draw the gradient with respect to one
    particular output or weight in the network. Remember that this is just the slope
    of the error at that point.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一条误差曲线。我们还将绘制与网络中某一特定输出或权重相关的梯度。记住，这只是该点的误差的斜率。
- en: Let’s look at how the error varies with changes in the prediction *P1*, shown
    in [Figure 14-9](#figure14-9). Suppose *P1* has the value –1.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看误差如何随着预测值*P1*的变化而变化，参见[图14-9](#figure14-9)。假设*P1*的值为-1。
- en: In [Figure 14-9](#figure14-9) we’ve marked the value *P1* = −1 with an orange
    dot, and we’ve drawn the derivative at the location on the curve directly above
    this value of *P1* with a green line. That derivative (or gradient) tells us that
    if we make *P1* more positive (that is, we move right from −1), the error in the
    network will decrease.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图14-9](#figure14-9)中，我们用橙色圆点标记了*P1* = −1的值，并且用绿色线条在该*P1*值上方的曲线上绘制了导数。这个导数（或梯度）告诉我们，如果我们使*P1*变得更正（即从−1向右移动），网络中的误差将减少。
- en: '![F14009](Images/F14009.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![F14009](Images/F14009.png)'
- en: 'Figure 14-9: How the error depends on different values of *P1*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-9：误差如何依赖于*P1*的不同值
- en: If we knew the black curve representing the error, we wouldn’t need the gradient,
    since we’d just find the curve’s minimum. Unfortunately, the math doesn’t give
    us the black curve (we’re drawing it here just for reference). But the day is
    saved because the math gives us enough information to find the curve’s derivative
    at any location.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道代表误差的黑色曲线，我们就不需要梯度了，因为我们只需找到曲线的最小值。不幸的是，数学并没有给我们黑色曲线（我们在这里仅为参考绘制它）。但幸运的是，数学提供了足够的信息，使我们能够在任何位置找到曲线的导数。
- en: The derivative in [Figure 14-9](#figure14-9) tells us what happens to the error
    if we increase or decrease *P1* by a little bit. After we’ve changed *P1*, we
    can find the derivative at its new location and repeat. The derivative, or gradient,
    accurately predicts the new error after each change to *P1* as long as we keep
    that change small. The bigger the change, the less accurate the prediction is.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-9](#figure14-9)中的导数告诉我们，如果我们稍微增加或减少*P1*，误差会发生什么变化。在我们改变*P1*之后，我们可以在新的位置找到导数并重复操作。导数或梯度能够准确预测每次改变*P1*后的新误差，只要我们保持每次变化较小。变化越大，预测的准确性就越差。'
- en: We can see this characteristic in [Figure 14-9](#figure14-9). Suppose we move
    *P1* by one unit to the right from −1\. According to the derivative, we now expect
    an error of 0\. But at *P1* = 0, the error (the value of the black curve) is really
    about 1\. We moved *P1* too far. In the interests of clear figures that are easy
    to read, we’ll sometimes make large moves, but in practice, we change our weights
    by small amounts.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[图 14-9](#figure14-9)中看到这一特点。假设我们将*P1*从−1向右移动一个单位。根据导数，我们现在预计误差为0。然而在*P1*
    = 0时，误差（黑色曲线的值）实际上约为1。我们把*P1*移动得太远了。为了确保图形清晰且易于阅读，我们有时会做出较大的移动，但实际上，我们改变权重时通常是小幅度的。
- en: Let’s use the derivative to predict the numerical change in the error due to
    a change in *P1*. What’s the slope of the green line in [Figure 14-9](#figure14-9)?
    The left end is at about (−2, 8), and the right end is at about (0, 0). Thus,
    the line descends about four units for every one unit we move to the right, for
    a slope of −4/1 or −4\. If *P1* changed by 0.5 (that is, it changed from −1 to
    −0.5), we’d predict that the error would go down by 0.5 × −4 = −2.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用导数来预测由于*P1*变化而导致的误差数值变化。[图 14-9](#figure14-9)中绿色线的斜率是多少？左端大约在(−2, 8)，右端大约在(0,
    0)。因此，这条线每向右移动一个单位，下降大约四个单位，斜率为−4/1，即−4。如果*P1*变化了0.5（即从−1变化到−0.5），我们预测误差将下降0.5
    × −4 = −2。
- en: Remember that our goal is to find *Cδ*. We’ve just done it! *P1* in this discussion
    is just another name for *Co,* the output of neuron C. We’ve found that when P1
    = –1, a change of 1 in *Co* (or *P1*) would result in a change of −4 in the error.
    As we discussed, we shouldn’t have too much confidence in this prediction after
    such a big change in *P1*. But for small moves, the proportion is right. For instance,
    if we increase *P1* by 0.01, then we expect the error to change by −4 × 0.01 =
    −0.04, and for such a small change in *P1*, the predicted change in the error
    should be pretty accurate. If we increase *P1* by 0.02, then we expect the error
    to change by −4 × 0.02 = −0.08\.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们的目标是找到*Cδ*。我们刚刚做到了！在这个讨论中，*P1*只是*Co*，神经元C输出的另一种名称。我们发现当P1 = –1时，*Co*（或*P1*）变化1会导致误差变化−4。正如我们所讨论的，在*P1*发生如此大的变化后，我们不应该对这个预测过于自信。但对于小幅度的移动，比例是正确的。例如，如果我们将*P1*增加0.01，那么我们预计误差会变化−4
    × 0.01 = −0.04，对于如此小的*P1*变化，预测的误差变化应该非常准确。如果我们将*P1*增加0.02，那么我们预计误差会变化−4 × 0.02
    = −0.08。
- en: The same thinking holds if we decrease the value of *P1*, or move it to the
    left. If *P1* changes from −1 to, say, −1.1, we expect the error to change by
    −0.1 × −4 = 0.4, so the error would increase by 0.4.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的思路也适用于减少*P1*的值，或者将其向左移动。如果*P1*从−1变化到−1.1，我们预计误差会变化−0.1 × −4 = 0.4，因此误差将增加0.4。
- en: We’ve found that for any amount of change in *Co*, we can predict the change
    in the error by multiplying *Co* by −4\. That’s exactly what we’ve been looking
    for! The value of *Cδ* is −4\. Note that as soon as the value of *P1* changes,
    for any reason, the error curve changes and the value of *Cδ* has to be computed
    all over again.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，对于*Co*的任何变化量，我们可以通过将*Co*乘以−4来预测误差的变化。这正是我们一直在寻找的！*Cδ*的值是−4。请注意，一旦*P1*的值因任何原因发生变化，误差曲线也会发生变化，*Cδ*的值必须重新计算。
- en: We’ve just found our first delta value, which tells us how much the error will
    change if there’s a change to the output of C. It’s just the derivative of the
    error function measured at *P1* (or *Co*). [Figure 14-10](#figure14-10) shows
    all of this visually using our error diagram.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚找到了第一个增量值，它告诉我们如果C的输出发生变化，误差将变化多少。这只是误差函数在*P1*（或*Co*）处的导数。[图14-10](#figure14-10)通过我们的误差图形象地展示了这一切。
- en: '![F14010](Images/F14010.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![F14010](Images/F14010.png)'
- en: 'Figure 14-10: Our error diagram illustrating the change in the error from a
    change in the output of neuron C due to a small increase, Cm'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-10：我们的误差图，展示了神经元C输出的小幅增加Cm引起的误差变化。
- en: The original output is the green bar at the far left of [Figure 14-10](#figure14-10).
    We imagine that due to a change in one of the input weights, the output of C increases
    by an amount *Cm*. This is amplified by multiplying it by *Cδ*, which gives us
    the change in the error, *Em*. That is, *Em* = *Cm* × *Cδ*. Here the value of
    *Cm* is about 1/4 (the upward arrow in the box for *Cm* tells us that the change
    is positive), and the value of *Cδ* is −4 (the arrow in that box tells us the
    value is negative). So *Em* = −4 × 1/4 = −1\. The new error, at the far right,
    is the previous error plus *Em*, or 4 + (−1) = 3.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 原始输出是[图14-10](#figure14-10)最左边的绿色条形。我们假设由于其中一个输入权重的变化，C的输出增加了一个量*Cm*。这通过乘以*Cδ*来放大，得到误差的变化量*Em*。即，*Em*
    = *Cm* × *Cδ*。这里*Cm*的值大约是1/4（*Cm*框中的上箭头告诉我们变化是正的），而*Cδ*的值是−4（该框中的箭头告诉我们值是负的）。因此，*Em*
    = −4 × 1/4 = −1。新的误差值在最右侧，是前一个误差加上*Em*，即4 + (−1) = 3。
- en: Remember that at this point, we’re not yet doing anything with this delta value.
    Our goal right now is just to find the deltas for our neurons. We’ll use them
    later to change the weights.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在这个阶段，我们还没有对这个增量值做任何处理。我们现在的目标只是找出我们神经元的增量。稍后我们将使用它们来改变权重。
- en: Finding Dδ
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 寻找Dδ
- en: Let’s repeat this whole process for *P2*, to get the value of *Dδ*, or the delta
    for neuron D.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对*P2*重复整个过程，以得到*Dδ*的值，或者神经元D的增量。
- en: Let’s start with a recap of *Cδ*. On the left of [Figure 14-11](#figure14-11)
    we show the error curve for *P1*. As a result of also moving all of the other
    weights to better values, the error curve for *P1* now has a minimum of around
    2.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从* Cδ*的回顾开始。在[图14-11](#figure14-11)的左侧，我们展示了*P1*的误差曲线。由于将所有其他权重调整到更好的值，*P1*的误差曲线现在有一个约为2的最小值。
- en: '![F14011](Images/F14011.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![F14011](Images/F14011.png)'
- en: 'Figure 14-11: Left: The error for different values of *P1*. Right: The error
    for different values of *P2*.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-11：左侧：不同*P1*值的误差；右侧：不同*P2*值的误差。
- en: If we use the new value and error curve for *P1*, it looks like a change of
    about 0.5 in *P1* will result in a change of about −1.5 in the error, so *Cδ*
    is about −1.5 / 0.5 = −3\. Instead of changing *P1*, what if we change *P2*?
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用新的*P1*值和误差曲线，似乎*P1*的约0.5的变化将导致误差变化约−1.5，因此*Cδ*大约是−1.5 / 0.5 = −3。如果不改变*P1*，如果我们改变*P2*呢？
- en: Take a look at the graph on the right of [Figure 14-11](#figure14-11). A change
    of about −0.5 (moving left this time, toward the minimum of the bowl) results
    in a change of about −1.25 in the error, so *Dδ* is about 1.25 / −0.5 = 2.5\.
    The positive result here tells us that moving *P2* to the right causes the error
    to go up, so we want to move *P2* to the left.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下[图14-11](#figure14-11)右侧的图表。约−0.5的变化（这次向左移动，朝着碗的最小值）会导致误差变化约−1.25，因此*Dδ*大约是1.25
    / −0.5 = 2.5。这里的正值告诉我们，移动*P2*到右侧会导致误差上升，所以我们希望将*P2*向左移动。
- en: There are some interesting things to observe here. First, although both curves
    are bowl shaped, the bottoms of the bowls are at different weight values. Second,
    because the current values of *P1* and *P2* are on opposite sides of the bottom
    of their respective bowls, their derivatives have opposite signs (one is positive,
    the other is negative).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些有趣的事情值得观察。首先，尽管这两条曲线都是碗状的，但碗底的位置在不同的权重值上。其次，因为*P1*和*P2*的当前值位于各自碗底的对立面，它们的导数符号相反（一个为正，另一个为负）。
- en: The most important observation is that we cannot currently get the error down
    to 0\. In this example, the curves never get lower than about 2\. That’s because
    each curve looks at changing just one value, while the other is fixed. So even
    if *P1* got to a value of 1, where its curve is a minimum, there would still be
    error in the result because *P2* is not at its ideal value of 0, and vice versa.
    This means that if we change just one of these two values, we can’t get down to
    the minimum error of 0\. Getting an error of 0 is ideal, but, more generally,
    our goal is to move each weight, a little bit at a time, until we’ve pushed the
    error to as small a value as possible. For some networks, we may never be able
    to get to 0.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的观察是，我们目前无法将误差降至0。在这个例子中，曲线的最低点约为2。这是因为每条曲线仅考虑变化一个值，而另一个值保持不变。因此，即使*P1*达到了1的值，它的曲线达到了最小值，结果仍然会有误差，因为*P2*没有达到理想值0，反之亦然。这意味着，如果我们只改变这两个值中的一个，我们无法将误差降至0。得到0的误差是理想的，但更一般地说，我们的目标是逐步调整每个权重，直到将误差降到尽可能小的值。对于某些网络来说，我们可能永远无法达到0的误差。
- en: Note that we may not want to get the error to 0, even if we could. As we saw
    in Chapter 9, when a network is overfitting, its training error continues to decrease,
    but its ability to handle new data gets worse. We really want to minimize the
    error as much as possible without overfitting. In casual discussions, we usually
    say that we want to get down to zero error, with the understanding that it’s better
    to stop with some error than to keep training and overfit.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，即使我们能够将误差降至0，我们也可能不希望这样做。正如我们在第9章中看到的，当网络过拟合时，训练误差会继续下降，但它处理新数据的能力却变得更差。我们真正希望的是尽可能地将误差最小化，而不发生过拟合。在日常讨论中，我们通常说我们希望将误差降至零，理解是最好在有些误差的情况下停止训练，而不是继续训练并发生过拟合。
- en: We’ll see later that we canimprove all the weights in the network at the same
    time, as long as we take very small steps. Then we have to evaluate the errors
    again to find new curves and then new derivatives and deltas before we can make
    another adjustment. Rather than take many steps after each sample, we usually
    adjust the weights only once, and then evaluate another sample, adjust the weights
    once again, and so on.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后将看到，只要我们采取非常小的步伐，就可以同时改进网络中的所有权重。然后，我们需要重新评估误差，找到新的曲线，再计算新的导数和delta值，然后再进行调整。与每次样本后采取多次步骤不同，我们通常只调整一次权重，然后评估另一个样本，再次调整权重，如此循环。
- en: Measuring Error
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测量误差
- en: We mentioned earlier that we often compute the error in a classifier using cross
    entropy. For this discussion, let’s use a simpler formula that makes it easy to
    find the delta for each output neuron. This error measure is called the *quadratic
    cost function*, or the *mean squared error (MSE)* (Nielsen 2015). As usual, we
    won’t get into the mathematics of this equation. We chose it because it lets us
    find the delta for an output neuron as the difference between the neuron’s value
    and the corresponding label entry (Seung 2005). [Figure 14-12](#figure14-12) shows
    the idea graphically.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，我们通常通过交叉熵计算分类器的误差。为了本次讨论，我们使用一个更简单的公式，使得每个输出神经元的delta值容易计算。这个误差度量被称为*二次代价函数*，或者*均方误差
    (MSE)*（Nielsen 2015）。像往常一样，我们不会深入探讨这个方程的数学原理。我们选择它是因为它让我们能够通过神经元的值与相应标签值之间的差来计算输出神经元的delta值（Seung
    2005）。[图 14-12](#figure14-12)直观地展示了这一思想。
- en: '![F14012](Images/F14012.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![F14012](Images/F14012.png)'
- en: 'Figure 14-12: When we use the quadratic cost function, the delta for any output
    neuron is just the value in the label minus the output of that neuron. As shown
    in red, we save that delta value with its neuron.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-12：当我们使用二次代价函数时，任何输出神经元的delta值仅为标签中的值减去该神经元的输出值。如图中红色部分所示，我们将该delta值与其神经元一起保存。
- en: Remember that *Co* and *P1* are two names for the same value, as are *Do* and
    *P2*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，*Co*和*P1*是同一个值的两个名字，*Do*和*P2*也是如此。
- en: Let’s consider *Co* (or *P1*) when the first label is 1\. If *Co* = 1, then
    the value of *Cδ* = 1 – *Co* = 0, so any change in *Co* gets multiplied by 0,
    resulting in no change to the output error.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑*Co*（或*P1*），当第一个标签为1时。如果*Co* = 1，那么*Cδ*的值为1 – *Co* = 0，因此*Co*的任何变化都会乘以0，导致输出误差不发生变化。
- en: Now suppose that *Co* = 2\. Then the difference is *Cδ* = 1 – *Co* = −1, telling
    us that a change to *Co* changes the error by the same amount, but with the opposite
    sign. If *Co* is much larger, say *Co* = 5, then 1 – *Co* = −4, which tells us
    that any change to *Co* is amplified by a factor of −4 in the change to the error.
    We’ve been using large numbers for convenience, but remember that the derivative
    only accurately predicts what happens if we take a very small step.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设*Co* = 2。那么它们的差值是*Cδ* = 1 – *Co* = −1，告诉我们改变*Co*会导致误差变化相同的量，但符号相反。如果*Co*更大，比如*Co*
    = 5，那么1 – *Co* = −4，这意味着任何对*Co*的变化都会使误差的变化被放大一个-4倍。我们使用了较大的数字是为了方便，但请记住，导数只准确预测在我们进行非常小的步长时会发生什么。
- en: The same thought process holds for neuron D, and its output *Do* (or *P2*).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的思路也适用于神经元D及其输出*Do*（或*P2*）。
- en: 'We’ve now completed the first step in backpropagation: we found the delta values
    for all the neurons in the output layer. We know from [Figure 14-12](#figure14-12)
    that the delta for an output neuron depends on the value in the label and the
    neuron’s output. When we change the values of the weights going into that neuron,
    its delta changes as well. The delta is a temporary value that changes with every
    change to the network or its inputs. This is another reason we adjust the weights
    only once per sample. Since we have to recompute all the deltas after each update,
    we might as well evaluate a new sample first, and make use of the extra information
    it provides us.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了反向传播的第一步：我们找到了输出层所有神经元的δ值。从[图14-12](#figure14-12)中我们知道，输出神经元的δ值取决于标签中的值和神经元的输出。当我们改变输入该神经元的权重值时，它的δ值也会发生变化。δ值是一个临时值，它会随着每次网络或其输入的变化而变化。这也是我们每次只对每个样本调整一次权重的原因。由于每次更新后我们必须重新计算所有的δ值，不如先评估一个新的样本，并利用它提供的额外信息。
- en: Remember that our big goal is to find changes for the weights. When we know
    the deltas for all the neurons in a layer, we can update all the weights feeding
    into that layer. Let’s see how that’s done.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们的大目标是找到权重的变化。当我们知道一层中所有神经元的δ值时，就可以更新所有输入该层的权重。让我们来看一下如何操作。
- en: Using Deltas to Change Weights
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用δ值来改变权重
- en: We’ve seen how to find a delta value for every neuron in the output layer. We
    know that a change to the neuron’s output must come from a change in an input,
    which in turn can come from either a change in a previous neuron’s output or the
    weight connecting that output to this neuron. Let’s look at these cases.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道如何为输出层的每个神经元找到δ值。我们知道，神经元输出的变化必须来自输入的变化，而输入的变化又可以来自前一个神经元输出的变化，或者是连接该输出和当前神经元的权重变化。让我们来看看这些情况。
- en: For convenience, let’s say that a neuron’s output or a weight is changed by
    a value of 1\. [Figure 14-13](#figure14-13) shows that every change of 1 in the
    weight AC, which multiplies the output of neuron A before it’s received by neuron
    C, leads to a corresponding change of *Ao* × *Cδ* in the network error in our
    network. Subtracting that value leads to a change of *–Ao* × *Cδ* in the error.
    So, if we want to reduce the network’s error by subtracting *Ao* × *Cδ* from it,
    we can change the value of weight AC by –1 to accomplish this.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们假设一个神经元的输出或一个权重的变化值为1。[图14-13](#figure14-13)显示了权重AC每变化1，神经元A的输出与神经元C的输出相乘后，网络中的误差会发生相应的变化，变化值为*Ao*
    × *Cδ*。将该值减去后，误差会发生变化，变化值为*–Ao* × *Cδ*。因此，如果我们想通过从网络误差中减去*Ao* × *Cδ*来减少网络的误差，我们可以通过将权重AC的值减少1来实现这一点。
- en: '![F14013](Images/F14013.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![F14013](Images/F14013.png)'
- en: 'Figure 14-13: When *AC* changes by 1, the network error changes by *Ao*× *C**δ*.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-13：当*AC*变化1时，网络误差变化*Ao* × *Cδ*。
- en: We can summarize this process visually with an additional convention for our
    diagrams. We’ve been drawing the outputs of neurons as arrows coming out of a
    circle to the right. Let’s draw deltas using arrows coming out of the circles
    to the left, as in [Figure 14-14](#figure14-14).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过为我们的图示添加一个额外的约定来形象化地总结这一过程。我们一直将神经元的输出画成从圆形右侧出来的箭头。现在，让我们将δ值画成从圆形左侧出来的箭头，如[图14-14](#figure14-14)所示。
- en: '![F14014](Images/F14014.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![F14014](Images/F14014.png)'
- en: 'Figure 14-14: Neuron C has an output *Co*, drawn with an arrow pointing right,
    and a delta *Cδ*, drawn with an arrow pointing left.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-14：神经元C有一个输出*Co*，画成指向右的箭头，并且有一个δ值*Cδ*，画成指向左的箭头。
- en: With this convention, the whole process for finding the updated value for weight
    *AC*, or *AC – (Ao* × *Cδ),* is summarized in [Figure 14-15](#figure14-15). Showing
    subtraction in a diagram like this is hard, because if we have a “minus” node
    with two incoming arrows, it’s not clear which value is being subtracted from
    the other (that is, if the inputs are *x* and *y*, are we computing *x* − *y*
    or *y* − *x*?). To sidestep that problem, we compute *AC* − (*Ao* × *Cδ*) by finding
    *Ao* × *Cδ*, multiplying that by −1, and then adding that result to *AC*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个约定，找到更新后的权重*AC*值或*AC – (Ao* × *Cδ*)*的整个过程在[图14-15](#figure14-15)中做了总结。像这样在图中显示减法是困难的，因为如果我们有一个“减号”节点，并且有两条输入箭头，那么不清楚哪个值从另一个值中被减去（也就是说，如果输入是*x*和*y*，我们是计算*x*
    − *y* 还是 *y* − *x*?）。为了避免这个问题，我们通过先计算*Ao* × *Cδ*，将其乘以−1，然后将结果加到*AC*上，来计算*AC* −
    (*Ao* × *Cδ*)。
- en: '![F14015](Images/F14015.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![F14015](Images/F14015.png)'
- en: 'Figure 14-15: Updating the value of weight AC to the new value *AC –* (*Ao*
    *×* *Cδ**)*'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-15：将权重AC的值更新为新值*AC –*（*Ao* *×* *Cδ**）*
- en: Let’s walk through this figure. We start with the output *Ao* from neuron A
    and the delta *Cδ* from output neuron C, and multiply them together (at the top
    of the figure). We want to subtract this from the current value of *AC*. To show
    this clearly in the diagram, we multiply the product by −1 and then add it to
    the weight *AC*. The green arrow is the update step, where this result becomes
    the new value of *AC*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起分析一下这个图。我们从神经元A的输出*Ao*和输出神经元C的增量*Cδ*开始，然后将它们相乘（在图的顶部）。我们想从当前的*AC*值中减去这个结果。为了在图中清楚地表示这一点，我们将乘积乘以−1，然后将其加到权重*AC*上。绿色箭头表示更新步骤，在这一过程中，结果成为*AC*的新值。
- en: '[Figure 14-15](#figure14-15) is big news! We’ve found out how to change the
    weights coming into the output neurons in order to reduce the network’s error.
    We can apply this to all four weights going into the output neurons (that is,
    *AC*, *BC*, *AD*, and *BD*). We’ve just trained our neural network a little bit
    by improving four of its weights.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14-15](#figure14-15)是一个大新闻！我们已经找到了如何改变进入输出神经元的权重，以减少网络的误差。我们可以将这一方法应用于进入输出神经元的所有四个权重（即*AC*，*BC*，*AD*和*BD*）。通过改进它的四个权重，我们已经稍微训练了一下我们的神经网络。'
- en: Sticking with the output layer, if we change the weights for both output neurons
    C and D to reduce the error by 1 from each neuron, we’d expect the error to go
    down by −2\. We can predict this because the neurons sharing the same layer don’t
    rely on each other’s outputs. Since C and D are both in the output layer, C doesn’t
    depend on *Do* and D doesn’t depend on *Co*. They do depend on the outputs of
    neurons on previous layers, but right now we’re just focusing on the effect of
    changing weights for C and D.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 继续关注输出层，如果我们将输出神经元C和D的权重都改变，从每个神经元那里减少1的误差，我们预计误差会下降−2。我们可以预测到这一点，因为同一层的神经元不依赖彼此的输出。由于C和D都位于输出层，C不依赖*Do*，D也不依赖*Co*。它们确实依赖于前一层神经元的输出，但现在我们只关注改变C和D的权重所带来的影响。
- en: It’s wonderful that we know how to adjust the weights on edges going into the
    output layer, but how about all the other weights? Our next goal is to figure
    out the deltas for all the neurons in all the preceding layers. Once we have a
    delta for every neuron in the network, we can use [Figure 14-15](#figure14-15)
    to adjust every weight in the network to reduce the error.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道如何调整进入输出层的边缘权重，这很棒，但其他所有权重怎么办呢？我们接下来的目标是计算所有前一层神经元的增量。一旦我们为网络中的每个神经元都计算出增量，就可以使用[图14-15](#figure14-15)来调整网络中的每个权重，以减少误差。
- en: 'And this brings us to the remarkable trick of backpropagation: we can use the
    neuron deltas at one layer to find the neuron deltas for its preceding layer.
    Let’s see how.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这将引出反向传播的神奇技巧：我们可以利用一层的神经元增量来找到其前一层的神经元增量。让我们看看是怎么做到的。
- en: Other Neuron Deltas
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他神经元增量
- en: Now that we have the delta values for the output neurons, we can use them to
    compute the deltas for neurons on the layer just before the output layer. In our
    simple model, that layer is the hidden layer containing neurons A and B. Let’s
    focus for the moment just on neuron A and its connection to neuron C.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了输出神经元的增量值，我们可以用它们来计算输出层前一层神经元的增量。在我们的简单模型中，这一层是包含神经元A和B的隐藏层。现在我们暂时专注于神经元A及其与神经元C的连接。
- en: What happens if *Ao*, the output of A, changes for some reason? Let’s say it
    goes up by *Am*. [Figure 14-16](#figure14-16) follows the chain of actions using
    arbitrary values for *AC* and *Cδ*.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*Ao*，即A的输出，因某种原因发生变化，会发生什么呢？假设它增加了*Am*。[图14-16](#figure14-16)通过使用*AC*和*Cδ*的任意值，跟踪这一系列操作。
- en: '![F14016](Images/F14016.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![F14016](Images/F14016.png)'
- en: 'Figure 14-16: Following the results if we change the output of neuron A'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-16：如果我们改变神经元A的输出，结果如何
- en: If we read the diagram in [Figure 14-16](#figure14-16) from left to right, the
    change to A, shown as *Am*, is multiplied by the weight *AC* and then added to
    the values accumulated by neuron C. This raises the output of C by *Cm*. As we
    know, this change in C can be multiplied by *Cδ* to find the change in the network
    error.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从左到右读取[图14-16](#figure14-16)中的图表，A的变化（表示为*Am*）乘以权重*AC*，然后加到神经元C累积的值上。这使得C的输出增加了*Cm*。正如我们所知道的，C的这一变化可以乘以*Cδ*来找到网络误差的变化。
- en: So now we have a chain of operations from neuron A to neuron C and then to the
    error. The first step of the chain says that if we multiply the change in *Ao*
    (that is, *Am*) by the weight *AC*, giving us *Am* × *AC*, we get *Cm*, the change
    in the output of C. And we know from earlier that if we multiply this value of
    *Cm* by *Cδ*, forming *Cm* × *Cδ*, we get the change in the error.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们有了从神经元A到神经元C，再到误差的操作链。链条的第一步表明，如果我们将*Ao*（即*Am*）的变化乘以权重*AC*，得到*Am* × *AC*，我们就得到了*Cm*，C的输出变化。我们之前知道，如果将*Cm*乘以*Cδ*，得到*Cm*
    × *Cδ*，就能得到误差的变化。
- en: So, mushing this all together, we find that the change in the error due to a
    change *Am* in the output of A is *Am* × *AC* × *Cδ*. We just found the delta
    for A! It’s just *Aδ* = *AC* × *Cδ.*
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，将这些全部合并，我们发现由于A的输出变化*Am*而引起的误差变化为*Am* × *AC* × *Cδ*。我们刚刚找到了A的delta！它就是*Aδ*
    = *AC* × *Cδ*。
- en: '[Figure 14-17](#figure14-17) shows this visually.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14-17](#figure14-17)直观地展示了这一点。'
- en: '![F14017](Images/F14017.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![F14017](Images/F14017.png)'
- en: 'Figure 14-17: We can mush together the operations in [Figure 14-16](#figure14-16)
    into a more succinct diagram.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-17：我们可以将[图14-16](#figure14-16)中的操作合并成一个更简洁的图。
- en: This is kind of amazing. Neuron C has disappeared. It’s literally out of the
    picture in [Figure 14-17](#figure14-17). All we needed was its delta, *Cδ,* and
    from that we could find *Aδ*, the delta for A. And now that we know *Aδ*, we can
    update all of the weights that feed into neuron A, and then . . . no, wait a second.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是令人惊讶。神经元C消失了。它在[图14-17](#figure14-17)中简直不见了。我们所需要的只是它的delta，*Cδ*，通过它我们可以找到*Aδ*，A的delta。现在我们知道了*Aδ*，就可以更新所有输入到神经元A的权重，然后……等等，稍等一下。
- en: We don’t really have *Aδ* yet. We just have one piece of it.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上还没有得到*Aδ*。我们只得到了它的一部分。
- en: At the start of this discussion we said we’d focus on neurons A and C, and that
    was fine. But if we now remember the rest of the network in [Figure 14-8](#figure14-8),
    we can see that neuron D also uses the output of A. If *Ao* changes due to *Am*,
    then the output of D changes as well, and that also affects the error.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本讨论开始时，我们说过我们将关注神经元A和C，这没问题。但如果现在我们回想一下[图14-8](#figure14-8)中的其余网络，我们可以看到神经元D也使用了A的输出。如果*Ao*由于*Am*的变化而发生变化，那么D的输出也会变化，这也会影响误差。
- en: To find the change in the error due to neuron D caused by a change in the output
    of neuron A, we can repeat the analysis we just went through by just replacing
    neuron C with neuron D. If *Ao* changes by *Am*, and nothing else changes, the
    change in the error due to the change in D is given by *AD* × *Dδ*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到由于神经元D的变化引起的误差变化，我们可以重复我们刚才的分析，只需要将神经元C替换为神经元D。如果*Ao*发生了*Am*的变化，其他不变，D引起的误差变化由*AD*
    × *Dδ*给出。
- en: '[Figure 14-18](#figure14-18) shows these two outputs from A at the same time.
    This figure is set up slightly differently from previous figures of this type
    that we’ve seen earlier. Here, the effect of a change in A on the error due to
    a change in C is shown by the path from the center of the diagram moving to the
    right. The effect of a change in A on the error due to a change in D is shown
    by the path from the center of the diagram and moving left.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14-18](#figure14-18)同时展示了A的这两个输出。这个图的设置与我们之前看到的同类图稍有不同。在这里，A的变化对由于C变化引起的误差的影响通过从图表中心向右的路径展示。A的变化对由于D变化引起的误差的影响则通过从图表中心向左的路径展示。'
- en: '![F14018](Images/F14018.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![F14018](Images/F14018.png)'
- en: 'Figure 14-18: The output of neuron A is used by both neuron C and neuron D.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-18：神经元A的输出被神经元C和神经元D共同使用。
- en: '[Figure 14-18](#figure14-18) shows two separate changes to the error. Since
    neurons C and D don’t influence each other, their effects on the error are independent.
    To find the total change to the error, we just add up the two changes. [Figure
    14-19](#figure14-19) shows the result of adding the change in error via neuron
    C and the change via neuron D.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14-18](#figure14-18)展示了对误差的两个独立变化。由于神经元C和D互不影响，它们对误差的作用是独立的。为了找到误差的总变化，我们只需将这两者的变化相加。[图14-19](#figure14-19)展示了通过神经元C和神经元D的误差变化结果。'
- en: '![F14019](Images/F14019.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![F14019](Images/F14019.png)'
- en: 'Figure 14-19: When the output of neuron A is used by both neuron C and neuron
    D, the resulting changes to the error add together.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-19：当神经元A的输出同时被神经元C和神经元D使用时，误差的变化相加。
- en: Now that we’ve handled all the paths from A to the outputs, we can finally write
    the value for *Aδ*. Since the errors add together, as in [Figure 14-19](#figure14-19),
    we can just add up the factors that scale *Am*. If we write it out, this is *Aδ*
    = (*AC* × *Cδ*) + (*AD* × *Dδ*).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经处理了从A到输出的所有路径，我们终于可以写出*Aδ*的值了。由于误差是累加的，就像在[图14-19](#figure14-19)中一样，我们只需加总那些影响*Am*的因子。如果我们将其写出来，就是*Aδ*
    = (*AC* × *Cδ*) + (*AD* × *Dδ*)。
- en: Now that we’ve found the value of delta for neuron A, we can repeat the process
    for neuron B to find its delta.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经找到了神经元A的增量值，我们可以对神经元B重复这个过程，找到它的增量。
- en: What we’ve just done is actually far better than finding the delta for just
    neurons A and B. We’ve found out how to get the value of delta for everyneuron
    in *any* network, no matter how many layers it has or how many neurons there are!
    That’s because everything we’ve done involves nothing more than a neuron, the
    deltas of all the neurons in the next layer that use its value as an input, and
    the weights that join them. With nothing more than these values, we can find the
    effect of a neuron’s change on the network’s error, even if the output layer is
    dozens of layers away.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚做的其实远比仅仅找到神经元A和B的增量要好得多。我们已经学会了如何获取*任何*神经网络中每个神经元的增量值，无论它有多少层或多少个神经元！因为我们所做的所有工作仅仅涉及一个神经元、所有在下一层中使用它的值作为输入的神经元的增量值，以及连接它们的权重。仅凭这些值，我们就能找到神经元变化对网络误差的影响，即使输出层可能相隔数十层之远。
- en: To summarize this visually, let’s expand on our convention for drawing outputs
    and deltas as right-pointing and left-pointing arrows to include the weights,
    as in [Figure 14-20](#figure14-20). Let’s say that the weight on a connection
    multiplies either the output moving to the right, or the delta moving to the left,
    depending on which step we’re thinking about.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更直观地总结这一点，让我们扩展一下我们画输出和增量箭头的惯例，加入权重，如[图14-20](#figure14-20)所示。假设一个连接上的权重乘以向右移动的输出值，或者乘以向左移动的增量值，这取决于我们考虑的步骤。
- en: '![F14020](Images/F14020.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![F14020](Images/F14020.png)'
- en: 'Figure 14-20: Drawing the values associated with neuron A. (a) The output *Ao*
    is an arrow coming out of the right of the neuron, and the delta *Aδ* as an arrow
    coming out of the left. (b) *Ao* is multiplied by *AC* on its way to being used
    by C. (c) *Cδ* is multiplied by *AC* on its way to being used by A.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-20：绘制与神经元A相关的值。(a) 输出*Ao*是从神经元右侧出来的箭头，而增量*Aδ*是从左侧出来的箭头。(b) *Ao*在传递到C的过程中被*AC*乘以。(c)
    *Cδ*在传递到A的过程中被*AC*乘以。
- en: Here’s a way to think about [Figure 14-20](#figure14-20). There is one connection
    with one weight joining neurons A and C. If the arrow points to the right, then
    the weight multiplies *Ao*, the output of A, as it heads into neuron C. If the
    arrow points to the left, the weight multiplies *Cδ*, the delta of C, as it heads
    into neuron A.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一种理解[图14-20](#figure14-20)的方式。神经元A和C之间有一条连接，连接上有一个权重。如果箭头指向右边，那么权重会乘以*A0*，也就是A的输出值，传递到神经元C。如果箭头指向左边，那么权重会乘以*Cδ*，也就是C的增量值，传递到神经元A。
- en: When we evaluate a sample, we use the feed-forward, left-to-right style of flow,
    where the output value from neuron A to neuron C travels over a connection with
    weight *AC*. The result is that the value *Ao* × *AC* arrives at neuron C where
    it’s added to other incoming values, as in [Figure 14-20](#figure14-20)(b).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们评估一个样本时，我们使用从左到右的前馈流动方式，其中来自神经元A到神经元C的输出值通过一个权重为*AC*的连接传递。结果是，值*Ao* × *AC*到达神经元C，并与其他传入值相加，如[图14-20](#figure14-20)(b)所示。
- en: When we later want to compute *Aδ*, we follow the flow from right to left. Then
    the delta leaving neuron C travels over a connection with weight *AC*. The result
    is that the value *Cδ* × *AC* arrives at neuron A where it’s added to other incoming
    values, as in [Figure 14-20](#figure14-20)(c).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们稍后想计算 *Aδ* 时，我们从右向左跟踪流动。然后，离开神经元C的增量通过一个带有权重 *AC* 的连接传递。结果是，值 *Cδ* × *AC*
    到达神经元A，并与其他输入值一起加总，如[图14-20](#figure14-20)(c)所示。
- en: Now we can summarize both the processing of a sample input and the computation
    of the deltas for some arbitrary neuron named H (remember, we’re ignoring the
    activation function), as in [Figure 14-21](#figure14-21).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以总结一下对一个样本输入的处理过程，以及计算一个任意神经元（命名为H）增量的过程（记住，我们忽略了激活函数），如[图14-21](#figure14-21)所示。
- en: '![F14021](Images/F14021.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![F14021](Images/F14021.png)'
- en: 'Figure 14-21: Left: To calculate *Ho*, we scale the output of each preceding
    neuron by the weight of its connection and add the results together. Right: To
    calculate *Hδ*, we scale the delta of each following neuron by the connection’s
    weight and add the results together. As usual, we’re ignoring activation functions.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-21：左图：为了计算 *Ho*，我们将每个前一神经元的输出按连接的权重进行缩放，并将结果加总。右图：为了计算 *Hδ*，我们将每个后续神经元的增量按连接的权重进行缩放，并将结果加总。和往常一样，我们忽略了激活函数。
- en: 'This is pleasingly symmetrical. It also reveals an important practical result:
    calculating deltas is often as efficient as calculating output values. Even when
    the number of incoming connections is different from the number of outgoing connections,
    the amount of work involved is still close in both directions.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点令人感到对称和愉悦。它还揭示了一个重要的实际结果：计算增量（delta）通常和计算输出值一样高效。即使输入连接的数量和输出连接的数量不同，涉及的工作量在两个方向上仍然接近。
- en: Note that [Figure 14-21](#figure14-21) doesn’t require anything of neuron H
    except that it has inputs from a neighbor layer that travel on connections with
    weights and deltas. We can apply the left half of [Figure 14-21](#figure14-21)
    and calculate the output of neuron H as soon as the outputs from the previous
    layer are available. We can apply the right half of [Figure 14-21](#figure14-21)
    and calculate the delta of neuron H as soon as the deltas from the following layer
    are available.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[图14-21](#figure14-21)并不要求神经元H做任何特殊的事情，除了它需要从邻接层接收输入，这些输入通过带有权重和增量的连接传递过来。我们可以应用[图14-21](#figure14-21)的左半部分，并在前一层的输出可用时立即计算神经元H的输出。我们可以应用[图14-21](#figure14-21)的右半部分，并在下一层的增量可用时立即计算神经元H的增量。
- en: 'The dependence of *Hδ* on the deltas of the following neurons shows why we
    had to treat the output layer neurons as special cases: there are no “next layer”
    deltas to be used.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*Hδ* 对后续神经元增量的依赖，解释了为何我们必须将输出层的神经元视为特殊情况：因为没有“下一层”的增量可供使用。'
- en: Throughout this discussion we’ve left out activation functions. It turns out
    that we can fit them into [Figure 14-21](#figure14-21) without changing the basic
    approach. Though the process is conceptually straightforward, the mechanics involve
    a lot of details, so we won’t go into them here.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个讨论中，我们省略了激活函数。事实上，我们可以将它们纳入到[图14-21](#figure14-21)中，而不改变基本方法。尽管这个过程在概念上非常直接，但其机制涉及很多细节，所以我们在这里不会深入探讨。
- en: This process of finding the delta for every neuron in the network isthe heart
    of the backpropagation algorithm. Let’s get a feeling for how backprop works in
    a larger network.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程是为网络中每个神经元寻找增量的核心部分，它是反向传播算法的关键。让我们来了解一下反向传播在一个更大网络中的工作方式。
- en: Backprop on a Larger Network
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更大的网络中的反向传播
- en: In the last section we saw the backpropagation algorithm, which lets us compute
    the delta for every neuron in a network. Because that calculation depended on
    the deltas in the following neurons and the output neurons don’t have any of those,
    and because the changes to the output neurons are driven directly by the loss
    function, we treat the output neurons as a special case. Once all the neuron deltas
    for any layer (including the output layer) have been found, we can then step backward
    one layer (toward the inputs), and find the deltas for all the neurons on that
    layer. Then we step backward again, compute all the deltas, step back again, and
    so on, until we reach the input layer. Once we have the delta for every neuron,
    we can adjust the values of the weights going into that neuron, thereby training
    our network.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了反向传播算法，它允许我们计算出网络中每个神经元的增量。由于这个计算依赖于后续神经元的增量，并且输出神经元没有这些增量，且输出神经元的变化直接由损失函数驱动，因此我们将输出神经元视为特殊情况。一旦找到了任何一层（包括输出层）中所有神经元的增量，我们就可以向后逐层（朝着输入层方向）推进，找到该层所有神经元的增量。然后我们再次向后推进，计算所有增量，再次向后推进，依此类推，直到我们到达输入层。一旦我们为每个神经元找到了增量，就可以调整进入该神经元的权重值，从而训练我们的网络。
- en: Let’s walk through the process of using backprop to find the deltas for all
    the neurons in a slightly larger network.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过反向传播的过程，逐步找到稍大网络中所有神经元的增量。
- en: In [Figure 14-22](#figure14-22) we show a network with four layers. There are
    still two inputs and outputs, but now we have three hidden layers of two, four,
    and three neurons.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图14-22](#figure14-22)中，我们展示了一个具有四层的网络。仍然有两个输入和两个输出，但现在我们有三个隐藏层，分别包含两个、四个和三个神经元。
- en: '![F14022](Images/F14022.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![F14022](Images/F14022.png)'
- en: 'Figure 14-22: A new classifier network with two inputs, two outputs, and three
    hidden layers'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-22：一个新的分类器网络，具有两个输入、两个输出和三个隐藏层
- en: We start things off by evaluating a sample. We provide the values of its X and
    Y features to the inputs, and eventually the network produces the output predictions
    *P1* and *P2*.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过评估一个样本来开始。我们将其X和Y特征的值提供给输入，最终网络会产生输出预测值*P1*和*P2*。
- en: Now we can start backpropagation by finding the error in the first of the output
    neurons, as shown in the upper part of [Figure 14-23](#figure14-23).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过找到第一个输出神经元的误差，开始进行反向传播，如[图14-23](#figure14-23)的上部所示。
- en: We’ve begun arbitrarily with the upper neuron, which gives us the prediction
    we’ve labeled *P1* (the probability that the sample is in class 1). From the values
    of *P1* and *P2* and the label, we can compute the error in the network’s output.
    Let’s suppose the network didn’t predict this sample perfectly, so the error is
    greater than zero.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们任意从上面的神经元开始，它给出了我们标记为*P1*的预测值（即样本属于类别1的概率）。根据*P1*和*P2*的值以及标签，我们可以计算出网络输出中的误差。假设网络没有完美预测这个样本，那么误差值就大于零。
- en: Using the error, the label, and the values of *P1* and *P2*, we can compute
    the value of delta for this neuron. If we’re using the quadratic cost function,
    this delta is just the value of the label minus the value of the neuron, as we
    saw in [Figure 14-12](#figure14-12). But if we’re using some other function, it
    might be more complicated, so we’ll discuss the general case.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 利用误差、标签和*P1*与*P2*的值，我们可以计算出该神经元的增量值。如果我们使用的是二次代价函数，这个增量只是标签值减去神经元的值，就像我们在[图14-12](#figure14-12)中看到的那样。但如果我们使用其他函数，计算可能会更复杂，所以我们将讨论一般情况。
- en: We save this delta with its neuron, and then repeat this process for all the
    other neurons in the output layer (here we have only one more), as shown in the
    lower part of [Figure 14-23](#figure14-23). That finishes up the output layer,
    since we now have a delta for every neuron in the output layer.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们保存这个增量与其对应的神经元，然后对输出层中的所有其他神经元重复此过程（此处只有一个神经元），如[图14-23](#figure14-23)的下部所示。这样就完成了输出层，因为我们现在为输出层中的每个神经元都找到了增量。
- en: At this point, we could start adjusting the weights coming into the output layer,
    but we usually first find all the neuron deltas first, and then adjust all the
    weights. Let’s follow that typical sequence here.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们可以开始调整输入层到输出层的权重，但通常我们会先找到所有神经元的增量，然后再调整所有的权重。让我们按照这个典型的顺序来操作。
- en: '![F14023](Images/F14023.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![F14023](Images/F14023.png)'
- en: 'Figure 14-23: Summarizing the steps for finding the delta for both output neurons'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-23：总结为两个输出神经元计算增量的步骤
- en: We move backward one step to the third hidden layer (the one with three neurons).
    Let’s consider finding the value of delta for the topmost of these three, as in
    the left image of [Figure 14-24](#figure14-24).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们退后一步到第三隐藏层（有三个神经元）。让我们考虑如何计算这三个神经元中最上面一个的增量，如[图 14-24](#figure14-24)的左侧图所示。
- en: '![F14024](Images/F14024.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![F14024](Images/F14024.png)'
- en: 'Figure 14-24: Using backpropagation to find the deltas for the next-to-last
    layer of neurons'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-24：使用反向传播计算倒数第二层神经元的增量
- en: To find the delta for this neuron, we follow the recipe of [Figure 14-18](#figure14-18)
    to get the individual contributions, and then the recipe of [Figure 14-19](#figure14-19)
    to add them together to get the delta for this neuron.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到这个神经元的增量，我们按照[图 14-18](#figure14-18)中的步骤计算个别贡献，然后按照[图 14-19](#figure14-19)中的步骤将它们加在一起，得到该神经元的增量。
- en: Now we just work our way through the layer, applying the same process to each
    neuron. When we’ve completed all the neurons in this three-neuron layer, we take
    a step backward and start on the preceding hidden layer with four neurons. This
    is where things really become beautiful. To find the deltas for each neuron in
    this layer, we need only the weights to each neuron that uses that neuron’s output
    and the deltas for those neurons, which we just computed.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们逐层处理，应用相同的过程到每个神经元。当我们完成这一层的所有神经元后，我们退后一步，开始处理前一层的四个神经元。这就是过程真正变得美妙的地方。为了找到这一层中每个神经元的增量，我们只需要该神经元的输出所连接的每个神经元的权重，以及这些神经元的增量，这些我们刚刚计算出来。
- en: The other layers are irrelevant. We don’t care about the output layer anymore
    now.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 其他层就不相关了。我们现在不再关心输出层。
- en: '[Figure 14-25](#figure14-25) shows how we compute the deltas for the four neurons
    in the second hidden layer.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-25](#figure14-25)展示了如何计算第二隐藏层四个神经元的增量。'
- en: '![F14025](Images/F14025.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![F14025](Images/F14025.png)'
- en: 'Figure 14-25: Using backprop to find the delta values for the second hidden
    layer'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-25：使用反向传播计算第二隐藏层的增量值
- en: When all four neurons have had deltas assigned to them, that layer is finished,
    and we take another step backward. Now we’re at the first hidden layer with two
    neurons. Each of these connects to the four neurons on the next layer. Once again,
    all we care about are the deltas in that next layer and the weights that connect
    the two layers. For each neuron, we find the deltas for all the neurons that consume
    that neuron’s output, multiply those by the weights, and add up the results, as
    shown in [Figure 14-26](#figure14-26).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 当四个神经元的增量都被分配完后，该层就完成了，我们再退后一步。现在我们来到第一隐藏层，这一层有两个神经元。每个神经元都与下一层的四个神经元相连。我们关心的只是下一层的增量和连接两层的权重。对于每个神经元，我们找到所有消费该神经元输出的神经元的增量，将这些增量与权重相乘，并将结果加总，如[图
    14-26](#figure14-26)所示。
- en: '![F14026](Images/F14026.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![F14026](Images/F14026.png)'
- en: 'Figure 14-26: Using backprop to find the deltas for the first hidden layer'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-26：使用反向传播计算第一隐藏层的增量（delta）
- en: When [Figure 14-26](#figure14-26) is complete, we’ve found the delta for every
    neuron in the network.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 当[图 14-26](#figure14-26)完成后，我们就找到了网络中每个神经元的增量。
- en: Now let’s adjust the weights. We can run through the connections between neurons
    and use the technique we saw in [Figure 14-15](#figure14-15) to update every weight
    to a new and improved value.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们调整权重。我们可以遍历神经元之间的连接，并使用我们在[图 14-15](#figure14-15)中看到的技术来更新每个权重，使其达到新的、更好的值。
- en: '[Figure 14-23](#figure14-23) through [Figure 14-26](#figure14-26) show why
    the algorithm is called *backward propagation*. We’re taking the deltas from any
    layer and *propagating*, or moving, their delta (or gradient) information *backward*
    one layer at a time, modifying it as we go. As we’ve seen, computing each of these
    delta values is fast. Even when we put the activation function steps in, that
    doesn’t add much to the computational cost.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-23](#figure14-23)到[图 14-26](#figure14-26)展示了为什么这个算法叫做*反向传播*。我们把任何一层的增量信息*传播*，即一层一层地将其*向后*移动，并在这个过程中进行修改。正如我们所见，计算每个增量值的速度非常快。即使我们将激活函数步骤加进去，计算成本也不会增加太多。'
- en: Backprop becomes highly efficient when we use parallel hardware like a GPU,
    because we can use a GPU to multiply all the deltas and weights for an entire
    layersimultaneously. The tremendous efficiency boost that comes from this parallelism
    is a key reason why backprop has made learning practical for huge neural networks.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用并行硬件如GPU时，反向传播变得非常高效，因为我们可以利用GPU同时为整个层计算所有的delta和权重。这种并行化带来的巨大效率提升是反向传播使得巨大的神经网络学习变得可行的关键原因。
- en: Now we have all of the deltas, and we can update the weights. That’s the core
    process of training a neural network.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了所有的delta，并且可以更新权重了。这就是训练神经网络的核心过程。
- en: Before we leave the discussion, though, let’s return to the issue of how much
    we should move each weight.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们结束讨论之前，让我们回到该如何调整每个权重的问题。
- en: The Learning Rate
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率
- en: As we’ve mentioned, changing a weight by a lot in a single step is often a recipe
    for trouble. The derivative is only an accurate predictor of the shape of a curve
    for very tiny changes in the input value. If we change a weight by too much, we
    can jump right over the smallest value of the error and even find ourselves increasing
    the error.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，在单步中改变权重过多通常是麻烦的根源。导数只在输入值的变化非常小的情况下准确预测曲线的形状。如果我们改变权重过多，我们可能会跳过错误的最小值，甚至可能会导致错误增加。
- en: On the other hand, if we change a weight by too little, we might see only the
    tiniest bit of learning, requiring us to spend more time learning than we should
    actually require. Still, that inefficiency is usually better than a system that’s
    constantly overreacting to errors.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们将权重的调整幅度设置得太小，我们可能只能看到极少的学习进展，这会让我们花费比实际需要更多的时间来学习。然而，这种低效率通常比一个总是过度反应的系统要好。
- en: In practice, we control the amount of change to the weights during every update
    with a hyperparameter called the *learning rate*, usually symbolized by the lowercase
    Greek letter *η* (eta). This is a number between 0 and 1, and it tells the weights
    how much of each neuron’s newly computed change to use when it updates.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们通过一个超参数叫做*学习率*来控制每次更新时权重的变化量，通常用小写希腊字母*η*（eta）表示。这个数字介于0和1之间，它告诉权重在更新时应使用每个神经元新计算出的变化量的多少。
- en: When we set the learning rate to 0, the weights don’t change at all. Our system
    never changes and never learns. If we set the learning rate to 1, the system applies
    big changes to the weights and may cause them to increase the error, not decrease
    it. If this happens a lot, the network can spend its time constantly overshooting
    and then compensating, with the weights bouncing around and never settling into
    their best values. Therefore, we usually set the learning rate somewhere between
    these extremes. In practice, we usually set it to be only slightly larger than
    0\.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将学习率设置为0时，权重完全不会发生变化。我们的系统永远不会改变，也永远不会学习。如果我们将学习率设置为1，系统会对权重进行大幅度的变化，可能会导致错误增加，而不是减少。如果这种情况频繁发生，网络可能会不断地过度调整，然后进行补偿，导致权重一直在波动，无法稳定在最佳值。因此，我们通常将学习率设置在这两个极端值之间。实际上，我们通常将其设置为稍微大于0。
- en: '[Figure 14-27](#figure14-27) shows how the learning rate is applied. Starting
    with [Figure 14-15](#figure14-15), we insert an extra step to scale the value
    of −(*Ao* × *Cδ*) by *η* before adding it back in to *AC*.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14-27](#figure14-27)展示了学习率的应用。我们从[图14-15](#figure14-15)开始，插入一个额外的步骤，在将−(*Ao*
    × *Cδ*)加回*AC*之前，先通过*η*缩放它的值。'
- en: '![F14027](Images/F14027.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![F14027](Images/F14027.png)'
- en: 'Figure 14-27: The learning rate helps us control how fast the network learns
    by controlling the amount by which weights change on each update.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-27：学习率帮助我们控制网络学习的速度，通过控制每次更新时权重变化的幅度来实现。
- en: The best value to use for the learning rate is dependent on the specific network
    we’ve built and the data we’re training on. Finding a good choice of learning
    rate can be essential to getting the network to learn at all. Once the system
    is learning, changing this value can affect whether that process goes quickly
    or slowly. Usually we have to hunt for the best value of *η* using trial and error.
    Happily, some algorithms automate the search for a good starting value for the
    learning rate and others fine-tune the learning rate as learning progresses. As
    a general rule of thumb, and if none of our other choices direct us to a particular
    learning rate, we often start with a value around 0.001 and then train the network
    for a while, watching how well it learns. Then we raise or lower it from that
    value and train again, over and over, hunting for the value that learns most efficiently.
    We’ll look at techniques for controlling the learning rate more closely in Chapter
    15.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率的最佳值取决于我们构建的特定网络和我们正在训练的数据。找到一个合适的学习率对网络的学习至关重要。一旦系统开始学习，改变学习率的值可能会影响学习的快慢。通常我们需要通过反复试验来寻找最合适的*η*值。幸运的是，一些算法可以自动搜索适合的学习率初始值，而其他算法则在学习过程中对学习率进行微调。作为一个通用的经验法则，如果其他选择没有特别指示学习率，我们通常会从0.001左右的值开始训练网络，观察其学习效果。然后我们根据情况调整这个值，再次训练，反复进行，寻找学习最有效的学习率。我们将在第15章更详细地讨论如何控制学习率的技术。
- en: Let’s see how the choice of learning rate affects the performance of backprop,
    and thus learning.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看学习率的选择如何影响反向传播的性能，从而影响学习效果。
- en: Building a Binary Classifier
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建二分类器
- en: Let’s build a classifier to find the boundary between two crescent moons. We
    will use about 1,500 points of training data, shown in [Figure 14-28](#figure14-28).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个分类器，找出两个月牙形区域之间的边界。我们将使用大约1,500个训练数据点，如[图14-28](#figure14-28)所示。
- en: '![F14028](Images/F14028.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![F14028](Images/F14028.png)'
- en: 'Figure 14-28: About 1,500 points assigned to two classes'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-28：约1,500个点被分配到两个类别
- en: Because we have only two classes, we only need a binary classifier. This lets
    us skip the whole one-hot encoding of labels and dealing with multiple outputs
    and instead lets us use just one output neuron. If the value is near 0, the input
    is in one class. If the output is near 1, the input is in the other class.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们只有两个类别，所以只需要一个二分类器。这使我们可以跳过标签的独热编码和处理多个输出，转而只使用一个输出神经元。如果输出接近0，说明输入属于一个类别；如果输出接近1，说明输入属于另一个类别。
- en: Our classifier will have just two hidden layers, each with four neurons. These
    are essentially arbitrary choices we’ve made that give us a network that’s just
    complex enough for our discussion. As shown in [Figure 14-29](#figure14-29), both
    layers are fully connected.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分类器只有两层隐藏层，每层包含四个神经元。这些都是我们为了讨论而做出的基本任意选择，足够复杂以满足我们的讨论需求。如[图14-29](#figure14-29)所示，两层之间是全连接的。
- en: This network uses ReLU activation functions for the neurons in the hidden layers
    and a sigmoid activation function on the output neuron.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络对隐藏层的神经元使用ReLU激活函数，对输出神经元使用sigmoid激活函数。
- en: '![F14029](Images/F14029.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![F14029](Images/F14029.png)'
- en: 'Figure 14-29: Our binary classifier with two inputs, four neurons in each of
    the two hidden layers, and a single output neuron'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-29：我们的二分类器，具有两个输入、每个隐藏层四个神经元和一个输出神经元
- en: How many weights are in our network? There are four coming out of each of the
    two inputs, then four times four between the layers, and then four going into
    the output neuron. That gives us (2 × 4) + (4 × 4) + 4 = 28\. Each of the nine
    neurons also has a bias term, so our network has a total of 28 + 9 = 37 weights.
    They are all initialized as small random numbers. Our goal is to use backprop
    to adjust those 37 weights so that the number that comes out of the final neuron
    always matches the label for that sample.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的网络中有多少个权重？每个输入有四个权重，然后在两层之间有四乘四个权重，再加上四个连接到输出神经元的权重。总数是(2 × 4) + (4 × 4)
    + 4 = 28。每个神经元还具有一个偏置项，所以我们的网络共有28 + 9 = 37个权重。所有权重都初始化为小的随机数。我们的目标是通过反向传播调整这37个权重，使最终神经元的输出始终与该样本的标签匹配。
- en: As we discussed earlier, we evaluate one sample, calculate the error, and if
    the error is not zero, we compute the deltas with backprop and then update the
    weights using the learning rate. Then we move on to the next sample. Note that
    if the error is 0, then we don’t change anything, since the network gave us the
    answer we wanted. Each time we process all the samples in the training set, we
    say we’ve completed one *epoch* of training.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的那样，我们评估一个样本，计算误差，如果误差不为零，我们通过反向传播计算增量，然后使用学习率更新权重。然后我们继续处理下一个样本。请注意，如果误差为0，我们就不做任何更改，因为网络已经给出了我们想要的答案。每次处理完训练集中的所有样本后，我们就说完成了一个*训练周期*。
- en: Running backprop successfully relies on making small changes to the weights.
    There are two reasons for this. The first, which we’ve discussed, is because the
    gradient is only accurate very near the point we’re evaluating. If we move too
    far, we may find ourselves increasing the error rather than decreasing it.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 成功运行反向传播依赖于对权重做出小的变化。这有两个原因。第一个，我们已经讨论过，是因为梯度仅在我们评估的点附近是准确的。如果我们走得太远，可能会发现自己在增加误差，而不是减少误差。
- en: The second reason for taking small steps is that changes in weights near the
    start of the network cause changes in the outputs of neurons in later layers,
    which change their deltas. To prevent everything from turning into a terrible
    snarl of conflicting changes, we adjust the weights only by small amounts.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个小步前进的原因是，网络开始部分的权重变化会导致后续层中神经元的输出发生变化，从而改变它们的增量。为了防止一切变成相互冲突的混乱，我们只对权重做小的调整。
- en: But what is “small”? For every network and dataset, we have to experiment to
    find out. As we saw earlier, the size of our step is controlled by the learning
    rate, or eta (*η*). The bigger this value, the more each weight moves toward its
    new value.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，“小”是什么意思？对于每个网络和数据集，我们必须进行实验来找出答案。正如我们之前看到的，步长的大小由学习率或eta（*η*）控制。这个值越大，每个权重朝着新值移动的步伐就越大。
- en: Picking a Learning Rate
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择学习率
- en: Let’s start with an unusually large learning rate of 0.5\. [Figure 14-30](#figure14-30)
    shows the boundaries computed by our network for our test data, using a different
    background color for each class.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个不寻常的大学习率0.5开始。[图14-30](#figure14-30)展示了我们网络为测试数据计算的边界，每个类别使用不同的背景颜色。
- en: '![F14030](Images/F14030.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![F14030](Images/F14030.png)'
- en: 'Figure 14-30: The boundaries computed by our network using a learning rate
    of 0.5'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-30：学习率为0.5时，我们的网络计算的边界
- en: 'This is terrible: there don’t seem to be any boundaries at all! Everything
    is being assigned to a single class, shown by the light orange background. If
    we look at the accuracy and error (or loss) after each epoch, we get the graphs
    of [Figure 14-31](#figure14-31).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这太糟糕了：似乎根本没有任何边界！所有的点都被分配到一个类别，显示为浅橙色背景。如果我们查看每个训练周期后的准确率和误差（或损失），我们会得到[图14-31](#figure14-31)的图形。
- en: '![F14031](Images/F14031.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![F14031](Images/F14031.png)'
- en: 'Figure 14-31: Accuracy and loss for our half-moons data with a learning rate
    of 0.5'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-31：学习率为0.5时，我们的半月数据的准确率和损失
- en: Things are looking bad. As we’d expect, the accuracy is just about 0.5, meaning
    that half the points are being misclassified. This makes sense, since the red
    and blue points are roughly evenly divided. If we assign them all to one class,
    as we’re doing here, half of those assignments will be wrong. The loss, or error,
    starts high and doesn’t fall. If we let the network run for hundreds of epochs,
    it continues on in this way, never improving.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 情况看起来很糟糕。正如我们预期的那样，准确率大约为0.5，这意味着有一半的点被错误分类了。这是有道理的，因为红色和蓝色点大致平均分布。如果我们将它们都分配到一个类别，就像我们现在做的那样，那么一半的分配将是错误的。损失或误差开始时很高，并且没有下降。如果我们让网络运行几百个周期，它将继续这样运行，永远不会有所改善。
- en: What are the weights doing? [Figure 14-32](#figure14-32) shows the values of
    all 37 weights during training.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 权重在做什么？[图14-32](#figure14-32)展示了训练过程中所有37个权重的值。
- en: '![F14032](Images/F14032.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![F14032](Images/F14032.png)'
- en: 'Figure 14-32: The weights of our network when using a learning rate of 0.5\.
    One weight is constantly changing and overshooting its goal, while the others
    are making changes too small to show on this graph.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-32：使用学习率为0.5时，我们网络的权重。一个权重不断变化并超出目标，而其他权重的变化太小，无法在图表上显示出来。
- en: The graph is dominated by one weight that’s jumping all over. That weight is
    one of those going into the output neuron, trying to move its output around to
    match the label. That weight goes up, then down, then up, jumping too far almost
    every time, then overcorrecting by too much, then overcorrecting for that, and
    so on. The other neurons are changing too, but at too small a scale to see in
    this graph.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图表中由一个权重主导，它跳跃得很厉害。这个权重是连接到输出神经元的权重之一，试图调整输出以匹配标签。这个权重先上升，然后下降，然后再次上升，几乎每次都跳得太远，然后过度修正，再过度修正，如此反复。其他神经元也在变化，但变化幅度太小，在图中看不出来。
- en: These results are disappointing, but they’re not shocking, because a learning
    rate of 0.5 is *big*. That’s what’s causing all the erratic bouncing around in
    [Figure 14-32](#figure14-32).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果令人失望，但并不令人震惊，因为学习率0.5是*较大的*。这就是为什么[图 14-32](#figure14-32)中的权重会发生剧烈波动的原因。
- en: Let’s reduce the training rate by a factor of 10 to a more reasonable (though
    still big) value of 0.05\. We’ll change absolutely nothing else about the network
    or the data, and we’ll even reuse the same sequence of pseudorandom numbers to
    initialize the weights. The new boundaries are shown in [Figure 14-33](#figure14-33).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将训练率减少一个数量级，设为一个更合理（尽管仍然较大）的值0.05。我们将保持网络和数据的其他一切不变，甚至重新使用相同的伪随机数序列来初始化权重。新的边界如[图
    14-33](#figure14-33)所示。
- en: '![F14033](Images/F14033.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![F14033](Images/F14033.png)'
- en: 'Figure 14-33: The decision boundaries when we use a learning rate of 0.05'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-33：使用学习率为0.05时的决策边界
- en: This is muchbetter! Looking at the graphs in [Figure 14-34](#figure14-34) reveals
    that we’ve reached 100 percent accuracy on both the training and test sets after
    about 16 epochs. Using a smaller learning rate gave us a huge improvement.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这要好得多！查看[图 14-34](#figure14-34)中的图表可以发现，在大约16个周期后，我们已经在训练集和测试集上都达到了100%的准确率。使用更小的学习率给我们带来了巨大的改进。
- en: '![F14034](Images/F14034.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![F14034](Images/F14034.png)'
- en: 'Figure 14-34: Accuracy and loss for our network when using a learning rate
    of 0.05'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-34：使用学习率为0.05时网络的准确率和损失
- en: This shows us the importance of tuning the learning rate for every new combination
    of network and data. If a network refuses to learn, we can sometimes make things
    better by simply reducing the learning rate.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这向我们展示了为每个新的网络和数据组合调整学习率的重要性。如果一个网络无法学习，我们有时可以通过简单地降低学习率来改善情况。
- en: What are the weights doing now? [Figure 14-35](#figure14-35) shows us their
    history.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在权重在做什么？[图 14-35](#figure14-35)展示了它们的变化历史。
- en: '![F14035](Images/F14035.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![F14035](Images/F14035.png)'
- en: 'Figure 14-35: The weights in our network over time, using a learning rate of
    0.05'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-35：使用学习率为0.05时网络权重的变化
- en: Overall, this is way better, because lots of weights are changing. They’re getting
    pretty large, which can itself inhibit or slow down learning. We usually want
    our weights to be in a small range, typically [–1, 1]. We’ll see some ways to
    control weight values when we discuss regularization in Chapter 15.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这要好得多，因为许多权重正在变化。它们变得相当大，而这本身可能会抑制或减缓学习。我们通常希望权重在一个较小的范围内，通常是[–1, 1]。我们将在第15章讨论正则化时看到一些控制权重值的方法。
- en: '[Figure 14-33](#figure14-33) and [Figure 14-34](#figure14-34) are pictures
    of success. Our network has learned to perfectly classify the data, and it did
    it in only 16 epochs, which is nice and fast (in fact, the graphs show us that
    it really took only 10 epochs). On a late 2014 iMac without GPU support, the whole
    training process for 16 epochs took less than 10 seconds.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-33](#figure14-33)和[图 14-34](#figure14-34)是成功的象征。我们的网络已经学会了完美地分类数据，并且只用了16个周期，速度很快（事实上，图表显示它只用了10个周期）。在一台没有GPU支持的2014年末款iMac上，整个16个周期的训练过程不到10秒钟。'
- en: An Even Smaller Learning Rate
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个更小的学习率
- en: What if we lower the learning rate down to 0.01? Now the weights change even
    more slowly. Does this produce better results?
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将学习率降低到0.01会怎样？现在权重变化得更慢了。这会带来更好的结果吗？
- en: '[Figure 14-36](#figure14-36) shows the decision boundary resulting from these
    tiny steps. The boundary seems simpler than the boundary in [Figure 14-33](#figure14-33),
    but both boundaries separate the sets perfectly.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-36](#figure14-36)展示了这些微小步骤所产生的决策边界。这个边界看起来比[图 14-33](#figure14-33)中的边界更简单，但两个边界都完美地分开了各自的集合。'
- en: '![F14036](Images/F14036.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![F14036](Images/F14036.png)'
- en: 'Figure 14-36: The decision boundaries for a learning rate of 0.01'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-36：学习率为0.01时的决策边界
- en: '[Figure 14-37](#figure14-37) shows our accuracy and loss graphs. Because our
    learning rate is so much slower, our network takes around 170 epochs to get to
    100 percent accuracy, rather than the 16 in [Figure 14-35](#figure14-35).'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-37](#figure14-37)展示了我们的准确度和损失图。由于我们的学习率非常慢，网络大约需要 170 个周期才能达到 100% 的准确度，而不是像在[图
    14-35](#figure14-35)中那样仅需 16 个周期。'
- en: '![F14037](Images/F14037.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![F14037](Images/F14037.png)'
- en: 'Figure 14-37: The accuracy and learning rate for our network using a learning
    rate of 0.01'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-37：使用学习率 0.01 时我们网络的准确度和学习率
- en: These graphs show an interesting learning behavior. After an initial jump, both
    the training and test accuracies reach about 90 percent and plateau there. At
    the same time, the losses plateau as well. Then around epoch 170, things improve
    rapidly again, with the accuracy climbing to 100 percent and the errors dropping
    to zero.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图展示了一种有趣的学习行为。经过最初的跃升后，训练和测试准确度都达到了约 90%，并在那里形成了平台。同时，损失也达到了平台状态。然后，大约在第 170
    个周期左右，情况再次迅速改善，准确度攀升至 100%，错误降至零。
- en: This pattern of alternating improvement and plateaus is not unusual, and we
    can even see a hint of a plateau in [Figure 14-34](#figure14-34) between epochs
    3 and 8\. These plateaus come from the weights finding themselves on nearly flat
    regions of the error surface, resulting in near-zero gradients, and thus their
    updates are very small.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这种交替改善和平台区的模式并不罕见，我们甚至可以在[图 14-34](#figure14-34)中看到在第 3 到第 8 个周期之间的一个平台。这个平台是因为权重处于误差面上几乎平坦的区域，导致梯度接近零，因此它们的更新非常小。
- en: Though our weights might be getting stuck in a local minimum, it’s more common
    for them to get caught in a flat region of a saddle, like those we saw in Chapter
    5 (Dauphin et al. 2014). Sometimes it takes a long time for one of the weights
    to move into a region where the gradient is large enough to give it a good push.
    When one weight gets moving, it’s common to see the others kick in as well, thanks
    to the cascading effect of that weight’s changes on the rest of the network.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的权重可能陷入了局部最小值，但它们更常见的是停滞在鞍点的平坦区域，就像我们在第 5 章中看到的那样（Dauphin 等，2014）。有时，某些权重需要很长时间才能进入梯度足够大的区域，从而获得足够的推动力。当一个权重开始变化时，通常可以看到其他权重也开始变化，这是因为该权重的变化对网络其他部分产生了级联效应。
- en: The values of the weights follow almost the same pattern over time, as shown
    in [Figure 14-38](#figure14-38). The interesting thing is that at least some of
    the weights are not flat or on a plateau near the middle of our training process.
    They’re changing, but slowly. The system is getting better, but in tiny steps
    that don’t show up in the performance graphs until the changes become bigger around
    epoch 170.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 权重的值随时间变化几乎遵循相同的模式，如[图 14-38](#figure14-38)所示。值得注意的是，在我们的训练过程中，至少部分权重并非处于平坦或平台区，而是有变化的，尽管这些变化较慢。系统在不断改进，但进展非常缓慢，直到大约第
    170 个训练周期，性能图中的变化才变得明显。
- en: '![F14038](Images/F14038.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![F14038](Images/F14038.png)'
- en: 'Figure 14-38: The history of our weights using a learning rate of 0.01'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-38：使用学习率 0.01 时我们权重的变化历史
- en: So, was there any benefit to lowering the learning rate down to 0.01? In this
    case, not really. Even at 0.05, the classification was already perfect on both
    the training and test data. For this network and this data, the smaller learning
    rate just meant the network took longer to learn. This investigation has shown
    us how sensitive the network is to our choice of learning rate. We want to find
    a value that’s not too big, or too small, but just right (Pyle 1918).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，降低学习率到 0.01 是否有好处呢？在这种情况下，并没有太大帮助。即使是 0.05，分类结果在训练数据和测试数据上已经非常完美。对于这个网络和这个数据，较小的学习率只是意味着网络学习的时间更长。这项调查向我们展示了网络对学习率选择的敏感性。我们需要找到一个既不太大也不太小，而是恰到好处的值（Pyle
    1918）。
- en: We usually do this kind of experimenting with the learning rate as part of developing
    nearly every deep learning network. We need to find a value that does the best
    job on each specific network and data. Happily, in Chapter 15 we’ll see algorithms
    that can automatically adjust the learning rate for us in sophisticated ways.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常在开发几乎每个深度学习网络时都会进行这类学习率的实验。我们需要找到一个在特定网络和数据上表现最好的值。幸运的是，在第 15 章中，我们将看到一些算法，可以通过复杂的方式自动调整学习率。
- en: Summary
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter was all about backpropagation. We saw that we can predict how the
    error of a network will change in response to a change in each weight. If we can
    determine whether each weight should increase or decrease in value, we can reduce
    the error.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讲解了反向传播。我们看到，我们可以预测网络的误差如何随着每个权重的变化而变化。如果我们能确定每个权重应该增加还是减少，我们就能减少误差。
- en: To find how we should change each weight, we started by assigning a delta value
    to each neuron. This value tells us the relationship between a change in a weight’s
    value and a change in the final error. This enabled us to determine how to change
    each weight in order to reduce the error.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出如何调整每个权重，我们从给每个神经元分配一个delta值开始。这个值告诉我们权重值变化和最终误差变化之间的关系。这使我们能够确定如何调整每个权重，以减少误差。
- en: The computation of these deltas proceeds from the final layer backward to the
    first. Because the gradient information needed to compute the delta for each neuron
    is propagated backward one layer at a time, we get the name *backpropagation*.
    Backprop can be implemented on a GPU, where we can carry out the calculations
    for many neurons simultaneously.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这些delta的计算从最后一层开始，逐层向前传播。因为计算每个神经元的delta所需的梯度信息是逐层向后传播的，所以我们得名*反向传播*。反向传播可以在GPU上实现，在那里我们可以同时为多个神经元进行计算。
- en: It’s important to keep in mind that backprop is propagating the gradient of
    the error, which is the information that tells us how the error changes when the
    weight changes. Some authors casually speak of backprop as propagating the error,
    but that’s a misleading simplification. We’re propagating the gradient, which
    tells us how to manipulate the weights to improve the network’s output.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的是，反向传播是传播误差的梯度，梯度是告诉我们当权重变化时，误差如何变化的信息。一些作者随意地将反向传播称为传播误差，但这是一种误导性的简化。我们传播的是梯度，它告诉我们如何调整权重，以改善网络的输出。
- en: Now that we know whether each weight should be adjusted to be larger or smaller,
    we need to decide how big a change to actually make. That’s exactly what we’ll
    figure out in the next chapter.*
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道每个权重是应该增加还是减少，我们需要决定实际的调整幅度。这正是我们将在下一章中解决的问题。
