- en: '5'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'WHAT VON NEUMANN KNEW: COMPUTER ARCHITECTURE'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The term *computer architecture* may refer to the entire hardware level of the
    computer. However, it is often used to refer to the design and implementation
    of the digital processor part of the computer hardware, and we focus on the computer
    processor architecture in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The *central processing unit* (CPU, or processor) is the part of the computer
    that executes program instructions on program data. Program instructions and data
    are stored in the computer’s random access memory (RAM). A particular digital
    processor implements a specific *instruction set architecture* (ISA), which defines
    the set of instructions and their binary encoding, the set of CPU registers, and
    the effects of executing instructions on the state of the processor. There are
    many different ISAs, including SPARC, IA32, MIPS, ARM, ARC, PowerPC, and x86 (the
    latter including IA32 and x86-64). A *microarchitecture* defines the circuitry
    of an implementation of a specific ISA. Microarchitecture implementations of the
    same ISA can differ as long as they implement the ISA definition. For example,
    Intel and AMD produce different microprocessor implementations of IA32 ISA.
  prefs: []
  type: TYPE_NORMAL
- en: Some ISAs define a *reduced instruction set computer* (RISC), and others define
    a *complex instruction set computer* (CISC). RISC ISAs have a small set of basic
    instructions that each execute quickly; each instruction executes in about a single
    processor clock cycle, and compilers combine sequences of several basic RISC instructions
    to implement higher-level functionality. In contrast, a CISC ISA’s instructions
    provide higher-level functionality than RISC instructions. CISC architectures
    also define a larger set of instructions than RISC, support more complicated addressing
    modes (ways to express the memory locations of program data), and support variable-length
    instructions. A single CISC instruction may perform a sequence of low-level functionality
    and may take several processor clock cycles to execute. This same functionality
    would require multiple instructions on a RISC architecture.
  prefs: []
  type: TYPE_NORMAL
- en: THE HISTORY OF RISC VERSUS CISC
  prefs: []
  type: TYPE_NORMAL
- en: In the early 1980s, researchers at Berkeley and Stanford universities developed
    RISC through the Berkeley RISC project and the Stanford MIPS project. David Paterson
    of Berkeley and John Hennessy of Stanford won the 2017 Turing Award^([1](ch05.xhtml#fn5_1))
    (the highest award in computing) for their work developing RISC architectures.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of its development, the RISC architecture was a radical departure
    from the commonly held view that ISAs needed to be increasingly complex to achieve
    high performance. “The RISC approach differed from the prevailing complex instruction
    set computer (CISC) computers of the time in that it required a small set of simple
    and general instructions (functions a computer must perform), requiring fewer
    transistors than complex instruction sets and reducing the amount of work a computer
    must perform.”^([2](ch05.xhtml#fn5_2))
  prefs: []
  type: TYPE_NORMAL
- en: CISC ISAs express programs in fewer instructions than RISC, often resulting
    in smaller program executables. On systems with small main memory, the size of
    the program executable is an important factor in the program’s performance, since
    a large executable leaves less RAM space available for other parts of a running
    program’s memory space. Microarchitectures based on CISC are also typically specialized
    to efficiently execute the CISC variable-length and higher- functionality instructions.
    Specialized circuitry for executing more complex instructions may result in more
    efficient execution of specific higher-level functionality, but at the cost of
    requiring more complexity for all instruction execution.
  prefs: []
  type: TYPE_NORMAL
- en: In comparing RISC to CISC, RISC programs contain more total instructions to
    execute, but each instruction executes much more efficiently than most CISC instructions,
    and RISC allows for simpler microarchitecture designs than CISC. CISC programs
    contain fewer instructions, and CISC microarchitectures are designed to execute
    more complicated instructions efficiently, but they require more complex microarchitecture
    designs and faster clock rates. In general, RISC processors result in more efficient
    design and better performance. As computer memory sizes have increased over time,
    the size of the program executable is less important to a program’s performance.
    CISC, however, has been the dominant ISA due in large part to it being implemented
    by and supported by industry.
  prefs: []
  type: TYPE_NORMAL
- en: Today, CISC remains the dominant ISA for desktop and many server-class computers.
    For example, Intel’s x86 ISAs are CISC-based. RISC ISAs are more commonly seen
    in high-end servers (e.g., SPARC) and in mobile devices (e.g., ARM) due to their
    low power requirements. A particular microarchitecture implementation of a RISC
    or CISC ISA may incorporate both RISC and CISC design under the covers. For example,
    most CISC processors use microcode to encode some CISC instructions in a more
    RISC-like instruction set that the underlying processor executes, and some modern
    RISC instruction sets contain a few more complex instructions or addressing modes
    than the initial MIPS and Berkeley RISC instruction sets.
  prefs: []
  type: TYPE_NORMAL
- en: All modern processors, regardless of their ISA, adhere to the von Neumann architecture
    model. The general-purpose design of the von Neumann architecture allows it to
    execute any type of program. It uses a stored-program model, meaning that the
    program instructions reside in computer memory along with program data, and both
    are inputs to the processor.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces the von Neumann architecture and the ancestry and components
    that underpin modern computer architecture. We build an example digital processor
    (CPU) based on the von Neumann architecture model, design a CPU from digital circuits
    that are constructed from logic gate building blocks, and demonstrate how the
    CPU executes program instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 The Origin of Modern Computing Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When tracing the ancestry of modern computing architecture, it is tempting to
    consider that modern computers are part of a linear chain of successive transmutations,
    with each machine simply an improvement of the one that previously existed. While
    this view of inherited improvements in computer design may hold true for certain
    classes of architecture (consider the iterative improvements of the iPhone X from
    the original iPhone), the root of the architectural tree is much less defined.
  prefs: []
  type: TYPE_NORMAL
- en: From the 1700s until the early 1900s, mathematicians served as the first *human*
    computers for calculations related to applications of science and engineering.^([3](ch05.xhtml#fn5_3))
    The word “computer” originally referred to “one who computes.” Women mathematicians
    often served in the role of computer. In fact, the use of women as human computers
    was so pervasive that computational complexity was measured in “kilo-girls,” or
    the amount of work a thousand human computers could complete in one hour.^([4](ch05.xhtml#fn5_4))
    Women were widely considered to be better at doing mathematical calculations than
    men, as they tended to be more methodical. Women were not allowed to hold the
    position of engineer. As such, they were relegated to more “menial” work, such
    as computing complex calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first general-purpose digital computer, the *Analytical Engine*, was designed
    by British mathematician Charles Babbage, who is credited by some as the father
    of the computer. The Analytical Engine was an extension of his original invention,
    the Difference Engine, a mechanical calculator that was capable of calculating
    polynomial functions. Ada Lovelace, who perhaps should be known as the mother
    of computing, was the very first person to develop a computer program and the
    first to publish an algorithm that could be computed using Charles Babbage’s Analytical
    Engine. In her notes is included her recognition of the general-purpose nature
    of the Analytical Engine: “[t]he Analytical Engine has no pretensions whatever
    to originate anything. It can do whatever we know how to order it to perform.”^([5](ch05.xhtml#fn5_5))
    However, unlike modern computers, the Analytical Engine was a mechanical device
    and was only partially built. Most of the designers of what became the direct
    forerunners to the modern computer were unaware of the work of Babbage and Lovelace
    when they developed their own machines.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, it is perhaps more accurate to think about modern computer architecture
    rising out of a primordial soup of ideas and innovations that arose in the 1930s
    and 1940s. For example, in 1937, Claude Shannon, a student at MIT, wrote what
    would go on to be perhaps the most influential masters thesis of all time. Drawing
    upon the work of George Boole (the mathematician who developed Boolean algebra),
    Shannon showed that Boolean logic could be applied to circuits and could be used
    to develop electrical switches. This would lead to the development of the binary
    computing system, and much of future digital circuit design. While men would design
    many early electronic computers, women (who were not allowed to be engineers)
    became programming pioneers, leading the design and development of many early
    software innovations, such as programming languages, compilers, algorithms, and
    operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: A comprehensive discussion of the rise of computer architecture is not possible
    in this book (see elsewhere^([6](ch05.xhtml#fn5_6),[7](ch05.xhtml#fn5_7)) for
    details); however, we briefly enumerate several significant innovations that occurred
    in the 1930s and 1940s that were instrumental in the rise of modern computer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 The Turing Machine
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 1937, British mathematician Alan Turing proposed^([8](ch05.xhtml#fn5_8))
    the “Logical Computing Machine,” a theoretical computer. Turing used this machine
    to prove that there exists no solution to the decision problem (in German, the
    *Entscheidungsproblem*), posed by the mathematicians David Hilbert and Wilhelm
    Ackermann in 1928\. The decision problem is an algorithm that takes a statement
    as input and determines whether the statement is universally valid. Turing proved
    that no such algorithm exists by showing that the *halting problem* (will machine
    *X* halt on input *y*?) was undecidable for Turing’s machine. As part of this
    proof, Turing described a universal machine that is capable of performing the
    tasks of any other computing machine. Alonzo Church, Turing’s dissertation advisor
    at Princeton University, was the first to refer to the *logical computing machine*
    as the *Turing machine*, and its universal form as the *universal Turing machine*.
  prefs: []
  type: TYPE_NORMAL
- en: Turing later returned to England and served his country as part of the code
    breaking unit in Bletchley Park during World War II. He was instrumental in the
    design and construction of the *Bombe*, an electromechanical device that helped
    break the cipher produced by the Enigma machine, which was commonly used by Nazi
    Germany to protect sensitive communication during World War II.
  prefs: []
  type: TYPE_NORMAL
- en: After the war, Turing designed the *automatic computing engine* (ACE). The ACE
    was a stored-program computer, meaning that both the program instructions and
    its data are loaded into the computer memory and run by the general-purpose computer.
    His paper, published in 1946, is perhaps the most detailed description of such
    a computer.^([9](ch05.xhtml#fn5_9))
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Early Electronic Computers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: World War II accelerated much of the development of early computers. However,
    due to the classified nature of military operations in World War II, many of the
    details of innovations that occurred as a result of the frenetic activity during
    the war was not publicly acknowledged until years later. A good example of this
    is Colossus, a machine designed by British engineer Tommy Flowers to help break
    the Lorenz cipher, which was used by Nazi Germany to encode high-level intelligence
    communication. Some of Alan Turing’s work aided in its design. Built in 1943,
    Colossus is arguably the first programmable, digital, and fully electronic computer.
    However, it was a special-purpose computer, designed specifically for code breaking.
    The Women’s Royal Naval Service (WRNS, known as the “Wrens”) served as operators
    of Colossus. In spite of the *General Report on Tunny*^([10](ch05.xhtml#fn5_10))
    noting that several of the Wrens showed ability in cryptographic work, none of
    them were given the position of cryptographer, and instead were delegated to more
    menial Colossus operation tasks.^([11](ch05.xhtml#fn5_11),[12](ch05.xhtml#fn5_12))
  prefs: []
  type: TYPE_NORMAL
- en: On the other side of the Atlantic, American scientists and engineers were hard
    at work creating computers of their own. Harvard professor Howard Aiken (who was
    also a Naval Commander in the US Navy Reserves) designed the Mark I, an electromechanical,
    general-purpose programmable computer. Built in 1944, it aided in the design of
    the atomic bomb. Aiken built his computer largely unaware of Turing’s work and
    was motivated by the goal of bringing Charles Babbage’s analytical engine to life.^([13](ch05.xhtml#fn5_13))
    A key feature of the Mark I was that it was fully automatic and able to run for
    days without human intervention. This would be a foundational feature in future
    computer design.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, American engineers John Mauchly and Presper Eckert of the University
    of Pennsylvania designed and built the *Electronic Numerical Integrator and Computer*
    (ENIAC) in 1945\. ENIAC is arguably the forerunner of modern computers. It was
    digital (though it used decimal rather than binary), fully electronic, programmable,
    and general purpose. While the original version of ENIAC did not have stored-program
    capabilities, this feature was built into it before the end of the decade. ENIAC
    was financed and built for the US Army’s Ballistic Research Laboratory and was
    designed primarily to calculate ballistic trajectories. Later, it would be used
    to aid in the design of the hydrogen bomb.
  prefs: []
  type: TYPE_NORMAL
- en: As men were drafted into the armed forces during World War II, women were hired
    to help in the war effort as human computers. With the arrival of the first electronic
    computers, women became the first programmers, as programming was considered secretarial
    work. It should come as no surprise that many of the early innovations in programming,
    such as the first compiler, the notion of modularizing programs, debugging, and
    assembly language, are credited to women inventors. Grace Hopper, for example,
    developed the first high-level and machine-independent programming language (COBOL)
    and its compiler. Hopper was also a programmer for the Mark I and wrote the book
    that described its operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ENIAC programmers were six women: Jean Jennings Bartik, Betty Snyder Holberton,
    Kay McNulty Mauchly, Frances Bilas Spence, Marlyn Wescoff Meltzer, and Ruth Lichterman
    Teitelbaum. Unlike the Wrens, the ENIAC women were given a great deal of autonomy
    in their task; given just the wiring diagrams of ENIAC, they were told to figure
    out how it worked and how to program it. In addition to their innovation in solving
    how to program (and debug) one of the world’s first electronic general-purpose
    computers, the ENIAC programmers also developed the idea of algorithmic flow charts,
    and developed important programming concepts such as subroutines and nesting.
    Like Grace Hopper, Jean Jennings Bartik and Betty Snyder Holberton would go on
    to have long careers in computing, and are some of the early computing pioneers.
    Unfortunately, the full extent of women’s contributions in early computing is
    not known. Unable to advance, many women left the field after World War II. We
    encourage readers to learn more about early women programmers.^([14](ch05.xhtml#fn5_14),[15](ch05.xhtml#fn5_15),[16](ch05.xhtml#fn5_16))'
  prefs: []
  type: TYPE_NORMAL
- en: The British and the Americans were not the only ones interested in the potential
    of computers. In Germany, Konrad Zuse developed the first electromechanical general-purpose
    digital programmable computer, the Z3, which was completed in 1941\. Zuse came
    up with his design independently of the work of Turing and others. Notably, Zuse’s
    design used binary (rather than decimal), the first computer of its kind to use
    the binary system. However, the Z3 was destroyed during aerial bombing of Berlin,
    and Zuse was unable to continue his work until 1950\. His work largely went unrecognized
    until years later. He is widely considered the father of computing in Germany.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 So What Did von Neumann Know?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: From our discussion of the origin of modern computer architecture, it is apparent
    that in the 1930s and 1940s there were several innovations that led to the rise
    of the computer as we know it today. In 1945, John von Neumann published a paper,
    “First draft of a report on the EDVAC,”^([17](ch05.xhtml#fn5_17)) which describes
    an architecture on which modern computers are based. EDVAC was the successor of
    ENIAC. It differed from ENIAC in that it was a binary computer instead of decimal,
    and it was a stored-program computer. Today, this description of EDVAC’s architectural
    design is known as the von Neumann architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The *von Neumann architecture* describes a general-purpose computer, one that
    is designed to run any program. It also uses a stored-program model, meaning that
    program instructions and data are both loaded onto the computer to run. In the
    von Neumann model, there is no distinction between instructions and data; both
    are loaded into the computer’s internal memory, and program instructions are fetched
    from memory and executed by the computer’s functional units that execute program
    instructions on program data.
  prefs: []
  type: TYPE_NORMAL
- en: 'John von Neumann’s contributions weave in and out of several of the previous
    stories in computing. A Hungarian mathematician, he was a professor at both the
    Institute of Advanced Study and Princeton University, and he served as an early
    mentor to Alan Turing. Later, von Neumann became a research scientist on the Manhattan
    Project, which led him to Howard Aiken and the Mark I; he would later serve as
    a consultant on the ENIAC project, and correspond regularly with Eckert and Mauchly.
    His famous paper describing EDVAC came from his work on the Electronic Discrete
    Variable Automatic Computer (EDVAC), proposed to the US Army by Eckert and Mauchly,
    and built at the University of Pennsylvania. EDVAC included several architectural
    design innovations that form the foundation of almost all modern computers: it
    was general purpose, used the binary numeric system, had internal memory, and
    was fully electric. In large part because von Neumann was the sole author of the
    paper,^([18](ch05.xhtml#fn5_18)) the architectural design the paper describes
    is primarily credited to von Neumann and has become known as the von Neumann architecture.
    It should be noted that Turing described in great detail the design of a similar
    machine in 1946\. However, since von Neumann’s paper was published before Turing’s,
    von Neumann received the chief credit for these innovations.'
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of who “really” invented the von Neumann architecture, von Neumann’s
    own contributions should not be diminished. He was a brilliant mathematician and
    scientist. His contributions to mathematics range from set theory to quantum mechanics
    and game theory. In computing, he is also regarded as the inventor of the *merge
    sort* algorithm. Walter Isaacson argued that one of von Neumann’s greatest strengths
    lay in his ability to collaborate widely and to intuitively see the importance
    of novel concepts.^([19](ch05.xhtml#fn5_19)) A lot of the early designers of the
    computer worked in isolation from one another. Isaacson argues that by witnessing
    the slowness of the Mark I computer, von Neumann was able to intuitively realize
    the value of a truly electronic computer, and the need to store and modify programs
    in memory. It could therefore be argued that von Neumann, even more than Eckert
    and Mauchly, grasped and fully appreciated the power of a fully electronic stored-program
    computer.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 The von Neumann Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The von Neumann architecture serves as the foundation for most modern computers.
    In this section, we briefly characterize the architecture’s major components.
  prefs: []
  type: TYPE_NORMAL
- en: The von Neumann architecture (depicted in [Figure 5-1](ch05.xhtml#ch5fig1))
    consists of five main components.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The *processing unit* executes program instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The *control unit* drives program instruction execution on the processing
    unit. Together, the processing and control units make up the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. The *memory unit* stores program data and instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. The *input unit(s)* load program data and instructions on the computer and
    initiate program execution.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. The *output unit(s)* store or receive program results.
  prefs: []
  type: TYPE_NORMAL
- en: Buses connect the units, and are used by the units to send control and data
    information to one another. A *bus* is a communication channel that transfers
    binary values between communication endpoints (the senders and receivers of the
    values). For example, a data bus that connects the memory unit and the CPU could
    be implemented as 32 parallel wires that together transfer a four-byte value,
    one bit transferred on each wire. Typically, architectures have separate buses
    for sending data, memory addresses, and control between units. The units use the
    control bus to send control signals that request or notify other units of actions,
    the address bus to send the memory address of a read or write request to the memory
    unit, and the data bus to transfer data between units.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-1: The von Neumann architecture consists of the processing, control,
    memory, input, and output units. The control and processing units make up the
    CPU, which contains the ALU, the general-purpose CPU registers, and some special-purpose
    registers (IR and PC). The units are connected by buses used for data transfer
    and communication between the units.*'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 The CPU
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The control and processing units together implement the CPU, which is the part
    of the computer that executes program instructions on program data.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 The Processing Unit
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *processing unit* of the von Neumann machine consists of two parts. The
    first is the *arithmetic/logic unit* (ALU), which performs mathematical operations
    such as addition, subtraction, and logical or, to name a few. Modern ALUs typically
    perform a large set of arithmetic operations. The second part of the processing
    unit is a set of registers. A *register* is a small, fast unit of storage used
    to hold program data and the instructions that are being executed by the ALU.
    Crucially, there is no distinction between instructions and data in the von Neumann
    architecture. For all intents and purposes, instructions *are* data. Each register
    is therefore capable of holding one data word.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 The Control Unit
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The *control unit* drives the execution of program instructions by loading
    them from memory and feeding instruction operands and operations through the processing
    unit. The control unit also includes some storage to keep track of execution state
    and to determine its next action to take: the *program counter* (PC) keeps the
    memory address of the next instruction to execute, and the *instruction register*
    (IR) stores the instruction, loaded from memory, that is currently being executed.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4 The Memory Unit
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Internal memory is a key innovation of the von Neumann architecture. It provides
    program data storage that is close to the processing unit, significantly reducing
    the amount of time to perform calculations. The *memory unit* stores both program
    data and program instructions—storing program instructions is a key part of the
    stored-program model of the von Neumann architecture
  prefs: []
  type: TYPE_NORMAL
- en: The size of memory varies from system to system. However, a system’s ISA limits
    the range of addresses that it can express. In modern systems, the smallest addressable
    unit of memory is one byte (8 bits), and thus each address corresponds to a unique
    memory location for one byte of storage. As a result, 32-bit architectures typically
    support a maximum address space size of 2^(32), which corresponds to 4 gigabytes
    (GiB) of addressable memory.
  prefs: []
  type: TYPE_NORMAL
- en: The term *memory* sometimes refers to an entire hierarchy of storage in the
    system. It can include registers in the processing unit as well as secondary storage
    devices like hard disk drives (HDD) or solid-state drives (SSD). In [Chapter 11](ch11.xhtml#ch11),
    we discuss the memory hierarchy in detail. For now, we use the term “memory” interchangeably
    with internal *random access memory* (RAM)—memory that can be accessed by the
    central processing unit. RAM storage is random access because all RAM storage
    locations (addresses) can be accessed directly. It is useful to think of RAM as
    a linear array of addresses, where each address corresponds to one byte of memory.
  prefs: []
  type: TYPE_NORMAL
- en: WORD SIZES THROUGH HISTORY
  prefs: []
  type: TYPE_NORMAL
- en: '*Word size*, which is defined by an ISA, is the number of bits of the standard
    data size that a processor handles as a single unit. The standard word size has
    fluctuated over the years. For EDVAC, the word size was proposed at 30 bits. In
    the 1950s, 36-bit word sizes were common. With the innovation of the IBM 360 in
    the 1960s, word sizes became more or less standardized, and started to expand
    from 16 bits, to 32 bits, to today’s 64 bits. If you examine the Intel architecture
    in more detail, you may notice the remnants of some of these old decisions, as
    32-bit and 64-bit architectures were added as extensions of the original 16-bit
    architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.5 The Input and Output (I/O) Units
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While the control, processing, and memory units form the foundation of the computer,
    the input and output units enable it to interact with the outside world. In particular,
    they provide mechanisms for loading a program’s instructions and data into memory,
    storing its data outside of memory, and displaying its results to users.
  prefs: []
  type: TYPE_NORMAL
- en: The *input unit* consists of the set of devices that enable a user or program
    to get data from the outside world into the computer. The most common forms of
    input devices today are the keyboard and mouse. Cameras and microphones are other
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: The *output unit* consists of the set of devices that relay results of computation
    from the computer back to the outside world or that store results outside internal
    memory. For example, the monitor is a common output device. Other output devices
    include speakers and haptics.
  prefs: []
  type: TYPE_NORMAL
- en: Some modern devices, such as the touchscreen, act as both input and output,
    enabling users to both input and receive data from a single unified device.
  prefs: []
  type: TYPE_NORMAL
- en: Solid-state and hard drives are another example of devices that act as both
    input and output devices. These storage devices act as input devices when they
    store program executable files that the operating system loads into computer memory
    to run, and they act as output devices when they store files to which program
    results are written.
  prefs: []
  type: TYPE_NORMAL
- en: '5.2.6 The von Neumann Machine in Action: Executing a Program'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The five units that make up the von Neumann architecture work together to implement
    a *fetch–decode–execute–store* cycle of actions that together execute program
    instructions. This cycle starts with a program’s first instruction, and is repeated
    until the program exits:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The control unit *fetches* the next instruction from memory. The control
    unit has a special register, the program counter (PC), that contains the address
    of the next instruction to fetch. It places that address on the *address bus*
    and places a *read* command on the *control bus* to the memory unit. The memory
    unit then reads the bytes stored at the specified address and sends them to the
    control unit on the *data bus*. The instruction register (IR) stores the bytes
    of the instruction received from the memory unit. The control unit also increments
    the PC’s value to store the address of the new next instruction to fetch.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The control unit *decodes* the instruction stored in the IR. It decodes
    the instruction bits that encode which operation to perform and the bits that
    encode where the operands are located. The instruction bits are decoded based
    on the ISA’s definition of the encoding of its instructions. The control unit
    also fetches the data operand values from their locations (from CPU registers,
    memory, or encoded in the instruction bits), as input to the processing unit.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. The processing unit *executes* the instruction. The ALU performs the instruction
    operation on instruction data operands.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. The control unit *stores* the result to memory. The result of the processing
    unit’s execution of the instruction is stored to memory. The control unit writes
    the result to memory by placing the result value on the *data bus*, placing the
    address of the storage location on the *address bus*, and placing a *write* command
    on the *control bus*. When received, the memory unit writes the value to memory
    at the specified address.
  prefs: []
  type: TYPE_NORMAL
- en: The input and output units are not directly involved in the execution of program
    instructions. Instead, they participate in the program’s execution by loading
    a program’s instructions and data and by storing or displaying the results of
    the program’s computation.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figures 5-2](ch05.xhtml#ch5fig2) and [5-3](ch05.xhtml#ch5fig3) show the four
    phases of instruction execution by the von Neumann architecture for an example
    addition instruction whose operands are stored in CPU registers. In the *fetch*
    phase, the control unit reads the instruction at the memory address stored in
    the PC (1234). It sends the address on the address bus, and a READ command on
    the control bus. The memory unit receives the request, reads the value at address
    1234, and sends it to the control unit on the data bus. The control unit places
    the instruction bytes in the IR register and updates the PC with the address of
    the next instruction (1238 in this example). In the *decode* phase, the control
    unit feeds bits from the instruction that specify which operation to perform to
    the processing unit’s ALU, and uses instruction bits that specify which registers
    store operands to read operand values from the processing unit’s registers into
    the ALU (the operand values are 3 and 4 in this example). In the *execute* phase,
    the ALU part of the processing unit executes the operation on the operands to
    produce the result (3 + 4 is 7). Finally, in the *store* phase the control unit
    writes the result (7) from the processing unit to the memory unit. The memory
    address (5678) is sent on the address bus, a WRITE command is sent on the control
    bus, and the data value to store (7) is sent on the data bus. The memory unit
    receives this request and stores 7 at memory address 5678\. In this example, we
    assume that the memory address to store the result is encoded in the instruction
    bits.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-2: The *fetch* and *decode* stages of execution of the von Neumann
    architecture for an example addition instruction. Operand, result, and memory
    addresses are shown as decimal values, and memory contents are shown as binary
    values.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-3: The *execute* and *store* stages of execution of the von Neumann
    architecture for an example addition instruction. Operand, result, and memory
    addresses are shown as decimal values, and memory contents are shown as binary
    values.*'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Logic Gates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Logic gates* are the building blocks of the digital circuitry that implements
    arithmetic, control, and storage functionality in a digital computer. Designing
    complicated digital circuits involves employing a high degree of abstraction:
    a designer creates simple circuits that implement basic functionality from a small
    set of basic logic gates; these simple circuits, abstracted from their implementation,
    are used as the building blocks for creating more complicated circuits (simple
    circuits are combined together to create new circuits with more complicated functionality);
    these more complicated circuits may be further abstracted and used as a building
    block for creating even more complicated functionality; and so on to build complete
    processing, storage, and control components of a processor.'
  prefs: []
  type: TYPE_NORMAL
- en: TRANSISTORS
  prefs: []
  type: TYPE_NORMAL
- en: Logic gates are created from transistors that are etched into a semiconductor
    material (e.g., silicon chips). Transistors act as switches that control electrical
    flow through the chip. A transistor can switch its state between on or off (between
    a high or low voltage output). Its output state depends on its current state plus
    its input state (high or low voltage). Binary values are encoded with these high
    (1) and low (0) voltages, and logic gates are implemented by arrangements of a
    few transistors that perform switching actions on the inputs to produce the logic
    gate’s output. The number of transistors that can fit on an integrated circuit
    (a chip) is a rough measure of its power; with more transistors per chip, there
    are more building blocks to implement more functionality or storage.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Basic Logic Gates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At the lowest level, all circuits are built from linking logic gates together.
    Logic gates implement Boolean operations on Boolean operands (0 or 1). *AND*,
    *OR*, and *NOT* form a complete set of logic gates from which any circuit can
    be constructed. A logic gate has one (NOT) or two (AND and OR) binary input values
    and produces a binary output value that is the bitwise logical operation on its
    input. For example, an input value of 0 to a NOT gate outputs 1 (1 is NOT(0)).
    A *truth table* for a logical operation lists the operation’s value for each permutation
    of inputs. [Table 5-1](ch05.xhtml#ch5tab1) shows the truth tables for the AND,
    OR, and NOT logic gates.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-1:** Truth Table for AND, OR, and NOT'
  prefs: []
  type: TYPE_NORMAL
- en: '| **A** | **B** | **A AND B** | **A OR B** | **NOT A** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '[Figure 5-4](ch05.xhtml#ch5fig4) shows how computer architects represent these
    gates in circuit drawings.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-4: The AND, OR, and NOT logic gates for single-bit inputs produce
    a single-bit output*'
  prefs: []
  type: TYPE_NORMAL
- en: A multibit version of a logic gate (for *M*-bit input and output) is a very
    simple circuit constructed using *M* one-bit logic gates. Individual bits of the
    *M*-bit input value are each input into a different one-bit gate that produces
    the corresponding output bit of the *M*-bit result. For example, [Figure 5-5](ch05.xhtml#ch5fig5)
    shows a four-bit AND circuit built from four 1-bit AND gates.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-5: A four-bit AND circuit built from four 1-bit AND gates*'
  prefs: []
  type: TYPE_NORMAL
- en: This type of very simple circuit, one that just expands input and output bit
    width for a logic gate, is often referred to as an *M*-bit gate for a particular
    value of *M* specifying the input and output bit width (number of bits).
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Other Logic Gates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Even though the set of logic gates consisting of AND, OR, and NOT is sufficient
    for implementing any circuit, there are other basic logic gates that are often
    used to construct digital circuits. These additional logic gates include NAND
    (the negation of A AND B), NOR (the negation of A OR B), and XOR (exclusive OR).
    Their truth tables are shown in [Table 5-2](ch05.xhtml#ch5tab2).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-2:** Truth Table for NAND, NOR, and XOR'
  prefs: []
  type: TYPE_NORMAL
- en: '| **A** | **B** | **A NAND B** | **A NOR B** | **A XOR B** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: The NAND, NOR, and XOR gates appear in circuit drawings, as shown in [Figure
    5-6](ch05.xhtml#ch5fig6).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-6: The NAND, NOR, and XOR logic gates*'
  prefs: []
  type: TYPE_NORMAL
- en: The circle on the end of the NAND and NOR gates represents negation or NOT.
    For example, the NOR gate looks like an OR gate with a circle on the end, representing
    the fact that NOR is the negation of OR.
  prefs: []
  type: TYPE_NORMAL
- en: MINIMAL SUBSETS OF LOGIC GATES
  prefs: []
  type: TYPE_NORMAL
- en: NAND, NOR, and XOR are not necessary for building circuits, but they are additional
    gates added to the set {AND, OR, NOT} that are commonly used in circuit design.
    Of the larger set {AND, OR, NOT, NAND, NOR, XOR}, there exist other minimal subsets
    of logic gates that alone are sufficient for building any circuit (the subset
    {AND, OR, NOT} is not the only one, but it is the easiest set to understand).
    Because NAND, NOR, and XOR are not necessary, their functionality can be implemented
    by combining AND, OR, and NOT gates into circuits that implement NAND, NOR, and
    XOR functions. For example, NOR can be built using a NOT combined with an OR gate,
    `(A NOR B)` ≡ `NOT(A OR B)`), as shown in [Figure 5-7](ch05.xhtml#ch5fig7).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-7: The NOR gate can be implemented using an OR and a NOT gate. The
    inputs, A and B, are first fed through an OR gate, and the OR gate’s output is
    input to a NOT gate (NOR is the NOT of OR).*'
  prefs: []
  type: TYPE_NORMAL
- en: Today’s integrated circuits chips are built using CMOS technology, which uses
    NAND as the basic building block of circuits on the chip. The NAND gate by itself
    makes up another minimal subset of complete logic gates.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Circuits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Digital circuits implement core functionality of the architecture. They implement
    the *Instruction Set Architecture* (ISA) in hardware, and also implement storage
    and control functionality throughout the system. Designing digital circuits involves
    applying multiple levels of abstraction: circuits implementing complex functionality
    are built from smaller circuits that implement partial functionality, which are
    built from even simpler circuits, and so on down to the basic logic gate building
    blocks of all digital circuits. [Figure 5-8](ch05.xhtml#ch5fig8) illustrates a
    circuit abstracted from its implementation. The circuit is represented as a *black
    box* labeled with its functionality or name and with only its input and output
    shown, hiding the details of its internal implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-8: A circuit is implemented by linking together subcircuits and logic
    gates. Its functionality is abstracted from the details of its implementation
    and can be used as a building block for creating other circuits.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main categories of circuit building blocks: arithmetic/logic,
    control, and storage circuits. A processor integrated circuit, for example, contains
    all three types of subcircuits: its register set uses storage circuits; its core
    functionality for implementing arithmetic and logic functions uses arithmetic
    and logic circuits; and control circuits are used throughout the processor to
    drive the execution of instructions and to control loading and storing values
    in its registers.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discuss these three types of circuit, showing how to design
    a basic circuit from logic gates, and then how to build larger circuits from basic
    circuits and logic gates.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 Arithmetic and Logic Circuits
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Arithmetic and logic circuits implement the arithmetic and logic instructions
    of an ISA that together make up the *arithmetic logic unit* (ALU) of the processor.
    Arithmetic and logic circuits also implement parts of other functionality in the
    CPU. For example, arithmetic circuits are used to increment the program counter
    (PC) as part of the first step of instruction execution, and they are used to
    calculate memory addresses by combining instruction operand bits and register
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Circuit design often starts with implementing a 1-bit version of a simple circuit
    from logic gates. This 1-bit circuit is then used as a building block for implementing
    *M*-bit versions of the circuit. The steps for designing a 1-bit circuit from
    basic logic gates are:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Design the truth table for the circuit: determine the number of inputs
    and outputs, and add a table entry for every permutation of input bit(s) that
    specifies the value of the output bit(s).'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Using the truth table, write an expression for when each circuit output
    is 1 in terms of its input values combined with AND, OR, NOT.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Translate the expression into a sequence of logic gates, where each gate
    gets its inputs from either an input to the circuit or from the output of a preceding
    logic gate.
  prefs: []
  type: TYPE_NORMAL
- en: 'We follow these steps to implement a single-bit *equals* circuit: bitwise equals
    (`A` `== B`) outputs 1 when the values of `A` and `B` are the same, and it outputs
    0 otherwise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, design the truth table for the circuit:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-3:** Truth Table for a Simple Equality Circuit'
  prefs: []
  type: TYPE_NORMAL
- en: '| **A** | **B** | **A == B Output** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Next, write expressions for when `A == B` is 1 in terms of `A` and `B` combined
    with AND, OR, and NOT. First, consider each row whose output is 1 separately,
    starting with the first row in the truth table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **A** | **B** | **A == B** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'For the input values in this row, construct a *conjunction* of expressions
    of its inputs that evaluate to 1\. A conjunction combines subexpressions that
    evaluate to 0 or 1 with AND, and is itself 1 only when both of its subexpressions
    evaluate to 1\. Start by expressing when each input evaluates to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, create their conjunction (combine them with AND) to yield an expression
    for when this row of the truth table evaluates to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We do the same thing for the last row in the truth table, whose output is also
    1:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **A** | **B** | **A == B** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, create a *disjunction* (an OR) of each conjunction corresponding to
    a row in the truth table that evaluates to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: At this point we have an expression for `A == B` that can be translated to a
    circuit. At this step, circuit designers employ techniques to simplify the expression
    to create a minimal equivalent expression (one that corresponds to the fewest
    operators and/or shortest path length of gates through the circuit). Designers
    must take great care when minimizing a circuit design to ensure the equivalence
    of the translated expression. There are formal methods for circuit minimization
    that are beyond the scope of our coverage, but we will employ a few heuristics
    as we develop circuits.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, we directly translate the preceding expression to a circuit.
    We may be tempted to replace (NOT(A) AND NOT(B)) with (A NAND B), but note that
    these two expressions *are not* equivalent: they do not evaluate the same for
    all permutations of A and B. For example, when A is 1 and B is 0, (A == B) is
    0 and (A NAND B) is 1.'
  prefs: []
  type: TYPE_NORMAL
- en: To translate the expression to a circuit, start from the innermost expression
    and work outward (the innermost will be the first gates, whose outputs will be
    inputs to subsequent gates). The first set of gates correspond to any negation
    of input values (NOT gates of inputs A and B). Next, for each conjunction, create
    parts of the circuit feeding input values into an AND gate. The AND gate outputs
    are then fed into OR gate(s) representing the disjunction. The resulting circuit
    is shown in [Figure 5-9](ch05.xhtml#ch5fig9).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-9: The one-bit equality circuit (A == B) constructed from AND, OR,
    and NOT logic gates*'
  prefs: []
  type: TYPE_NORMAL
- en: To verify the correctness of this circuit, simulate all possible permutations
    of input values A and B through the circuit and verify that the output of the
    circuit matches its corresponding row in the truth table for (A == B). For example,
    if A is 0 and B is 0, the two NOT gates negate their values before being fed through
    the top AND gate, so the input to this AND gate is (1, 1), resulting in an output
    of 1, which is the top input value to the OR gate. The values of A and B (0, 0)
    are fed directly though the bottom AND gate, resulting in output of 0 from the
    bottom AND gate, which is the lower input to the OR gate. The OR gate thus receives
    input values (1, 0) and outputs the value 1\. So, when A and B are both 0, the
    circuit correctly outputs 1\. [Figure 5-10](ch05.xhtml#ch5fig10) illustrates this
    example.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-10: An example showing how the one-bit equality circuit computes
    (A == B). Starting with input values 0 for A and 0 for B, the values propagate
    through the gates making up the circuit to compute the correct output value of
    1 for A == B.*'
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the implementation of a one-bit equality circuit as a unit allows it
    to be abstracted from its implementation, and thus it can be more easily used
    as a building block for other circuits. We represent this abstraction of the one-bit
    equality circuit (shown in [Figure 5-11](ch05.xhtml#ch5fig11)) as a box with its
    two inputs labeled *A* and *B* and its single output labeled *A == B*. The internal
    gates that implement the one-bit equality circuit are hidden in this abstracted
    view of the circuit.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-11: The one-bit equality circuit abstraction. This circuit can be
    used as a building block in other circuits.*'
  prefs: []
  type: TYPE_NORMAL
- en: Single-bit versions of NAND, NOR, and XOR circuits can be constructed similarly,
    using only AND, OR, and NOT gates, starting with their truth tables ([Table 5-4](ch05.xhtml#ch5tab4))
    and applying the same steps as the one-bit equality circuit.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-4:** Truth Table for the NAND, NOR, and XOR Circuits'
  prefs: []
  type: TYPE_NORMAL
- en: '| **A** | **B** | **A NAND B** | **A NOR B** | **A XOR B** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Multibit versions of these circuits are constructed from multiple single-bit
    versions of the circuits in a similar way to how the four-bit AND gate was constructed
    from four 1-bit AND gates in “Basic Logic Gates” on [page 243](ch05.xhtml#lev2_94).
  prefs: []
  type: TYPE_NORMAL
- en: Arithmetic Circuits
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Arithmetic circuits are constructed using exactly the same method as we used
    for constructing the logic circuits. For example, to construct a 1-bit adder circuit,
    start with the truth table for single-bit addition, which has two input values,
    A and B, and two output values, one for the SUM of A and B, and another output
    for overflow or CARRY OUT. [Table 5-5](ch05.xhtml#ch5tab5) shows the resulting
    truth table for one-bit add.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-5:** Truth Table for a One-Bit Adder Circuit'
  prefs: []
  type: TYPE_NORMAL
- en: '| **A** | **B** | **SUM** | **CARRY OUT** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'In the next step, for each output, SUM and CARRY OUT, create logical expressions
    of when the output value is 1\. These expressions are expressed as disjunctions
    of per-row conjunctions of input values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The expression for CARRY OUT cannot be simplified. However, the expression for
    SUM is more complicated and can be simplified, leading to a simpler circuit design.
    The first thing to note is that the SUM output can also be expressed as (A XOR
    B). If we have an XOR gate or circuit, expressing SUM as (A XOR B) results in
    a simpler adder circuit design. If not, then the expression using AND, OR, and
    NOT is used and implemented using AND, OR, and NOT gates.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume that we have an XOR gate that we can use for implementing the 1-bit
    adder circuit. The resulting circuit is shown in [Figure 5-12](ch05.xhtml#ch5fig12).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-12: The one-bit adder circuit has two inputs, A and B, and two outputs,
    SUM and CARRY OUT.*'
  prefs: []
  type: TYPE_NORMAL
- en: The one-bit adder circuit can be used as a building block for more complicated
    circuits. For example, we may want to create *N*-bit adder circuits for performing
    addition on values of different sizes (e.g. one-byte, two-byte, or four-byte adder
    circuits). However, creating an *N*-bit adder circuit from *N* one-bit adder circuits
    requires more care than creating an *N*-bit logic circuits from *N* 1-bit logic
    circuits.
  prefs: []
  type: TYPE_NORMAL
- en: When performing a multibit addition (or subtraction), individual bits are summed
    in order from the least significant bit to the most significant bit. As this bitwise
    addition proceeds, if the sum of the *i*th bits results in a carry out value of
    1, then an additional 1 is added with the two (*i* + 1)st bits. In other words,
    the carry out of the *i*th bit adder circuit is an input value to the (*i* + 1)st
    bit adder circuit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, to implement a multibit adder circuit, we need a new one-bit adder circuit
    that has three inputs: A, B, and CARRY IN. To do this, follow the steps described
    earlier for creating a one-bit adder circuit, with three inputs (A, B, CARRY IN)
    and two outputs (SUM and CARRY OUT), starting with the truth table for all possible
    permutations of its three inputs. We leave the design of this circuit as an exercise
    for the reader, but we show its abstraction as a one-bit adder circuit in [Figure
    5-13](ch05.xhtml#ch5fig13).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-13: The one-bit adder circuit with three inputs (A, B, and CARRY
    IN) and two outputs (SUM and CARRY OUT).*'
  prefs: []
  type: TYPE_NORMAL
- en: Using this version of a one-bit adder circuit as a building block, we can construct
    an *N*-bit adder circuit by feeding corresponding operand bits through individual
    one-bit adder circuits, feeding the CARRY OUT value from the *i*th one-bit adder
    circuit into the CARRY IN value of the (*i* + 1)st one-bit adder circuit. The
    one-bit adder circuit for the 0th bits receives a value of 0 for its CARRY IN
    from another part of the CPU circuitry that decodes the ADD instruction.
  prefs: []
  type: TYPE_NORMAL
- en: This type of *N*-bit adder circuit, built from *N* one-bit adder circuits, is
    called a *ripple carry adder*, shown in [Figure 5-14](ch05.xhtml#ch5fig14). The
    SUM result *ripples* or propagates through the circuit from the low-order to the
    high-order bits. Only after bit 0 of the SUM and CARRY OUT values are computed
    will bit 1 of the SUM and CARRY OUT be correctly computed. This is because the
    1st bit’s CARRY IN gets its value from the 0th bit’s CARRY OUT, and so on for
    subsequent higher-order bits of the result.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-14: A four-bit ripple adder circuit created from four 1-bit adder
    circuits*'
  prefs: []
  type: TYPE_NORMAL
- en: Circuits for other arithmetic and logic functions are constructed in similar
    ways by combining circuits and logic gates. For example, a subtraction circuit
    that computes (A − B) can be built from adder and negation circuits that compute
    subtraction as (A + (−B)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2 Control Circuits
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Control circuits are used throughout a system. On the processor, they drive
    the execution of program instructions on program data. They also control loading
    and storing values to different levels of storage (between registers, cache, and
    RAM), and control hardware devices in the system. Just like arithmetic and logic
    circuits, control circuits that implement complicated functionality are built
    by combining simpler circuits and logic gates.
  prefs: []
  type: TYPE_NORMAL
- en: A *multiplexer* (MUX) is an example of a control circuit that selects, or chooses,
    one of several values. The CPU may use a multiplexer circuit to select from which
    CPU register to read an instruction operand value.
  prefs: []
  type: TYPE_NORMAL
- en: An *N*-way multiplexer has a set of *N* input values and a single output value
    selected from one of its inputs. An additional input value, *Select* (S), encodes
    which of its *N* inputs is chosen for its output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most basic two-way MUX selects between two 1-bit inputs, A and B. The Select
    input for a two-way multiplexer is a single bit: if the S input is 1, it will
    select A for output; if it is 0 it will select B for output. The table that follows
    shows the truth table for a two-way one-bit multiplexer. The value of the selection
    bit (S) chooses either the value of A or B as the MUX output value.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **A** | **B** | **S** | **Out** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 0 (B’s value) |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 0 | 1 (B’s value) |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 | 0 (B’s value) |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 | 1 (B’s value) |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 1 | 0 (A’s value) |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 | 0 (A’s value) |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 | 1 (A’s value) |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 | 1 (A’s value) |'
  prefs: []
  type: TYPE_TB
- en: '[Figure 5-15](ch05.xhtml#ch5fig15) shows the two-way multiplexer circuit for
    single-bit input.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-15: A two-way 1-bit multiplexer circuit. The value of the signal
    input (S) is used to pick which of its two inputs (A or B) will be the circuit’s
    output value: when S is 1, A is chosen; when S is 0, B is chosen.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-16](ch05.xhtml#ch5fig16) shows how the multiplexer chooses A’s output
    with an S input value of 1\. For example, suppose that the input values are 1
    for A, 0 for B, and 1 for S. S is negated before being sent to the top AND gate
    with B (0 AND B), resulting in a 0 output value from the top AND gate. S feeds
    into the bottom AND gate with A, resulting in (1 AND A), which evaluates to the
    value of A being output from the bottom AND gate. The value of A (1 in our example)
    and 0 from the top AND gate feed as input to the OR gate, resulting in (0 OR A)
    being output. In other words, when S is 1, the MUX chooses the value of A as its
    output (A’s value being 1 in our example). The value of B does not affect the
    final output of the MUX, because 0 will always be the output of the top AND gate
    when S is 1.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-16: A two-way 1-bit multiplexer circuit chooses (outputs) A when
    S is 1.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-17](ch05.xhtml#ch5fig17) shows the path through the multiplexer when
    the S input value 0 chooses B’s output. If we consider the same input for A and
    B as the previous example, but change S to 0, then the negation of 0 is input
    to the top AND gate resulting in (1 AND B), or B’s value, output from the top
    AND gate. The input to the bottom AND gate is (0 AND A), resulting in 0 from the
    bottom AND gate. Thus, the input values to the OR gate are (B OR 0), which evaluates
    to B’s value as the MUX’s output (B’s value being 0 in our example).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-17: A two-way 1-bit multiplexer circuit chooses (outputs) B when
    S is 0.*'
  prefs: []
  type: TYPE_NORMAL
- en: A two-way 1-bit MUX circuit is a building block for constructing two-way *N*-bit
    MUX circuits. For example, [Figure 5-18](ch05.xhtml#ch5fig18) shows a two-way
    four-bit MUX built from four 1-bit two-way MUX circuits.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-18: A two-way four-bit multiplexer circuit built from four two-way
    1-bit multiplexer circuits. A single signal bit, S, chooses either A or B as output.*'
  prefs: []
  type: TYPE_NORMAL
- en: An *N*-way multiplexer chooses one of *N* inputs as output. It requires a slightly
    different MUX circuit than a two-way MUX, and needs log[2](*N*) bits for its Select
    input. The additional selection bits are needed because with log[2](*N*) bits,
    *N* distinct values can be encoded, one for selecting each of the *N* inputs.
    Each distinct permutation of the log[2](*N*) Select bits is input with one of
    the *N* input values to an AND gate, resulting in exactly one MUX input value
    selected as the MUX output. [Figure 5-19](ch05.xhtml#ch5fig19) shows an example
    of a one-bit four-way MUX circuit.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-19: A four-way multiplexer circuit has four inputs and two* (log[2](4))
    *select bits that encode which of the four inputs should be output by the multiplexer.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The four-way MUX circuit uses four three-input AND gates and one four-input
    OR gate. Multi-input versions of gates can be built by chaining together multiple
    two-input AND (and OR) gates. For example, a three-input AND gate is built from
    two two-input AND gates, where the first AND gate takes two of the input values
    and the second AND gate takes the third input value and the output from the first
    AND gate: (x AND y AND z) is equivalent to ((x AND y) AND z).'
  prefs: []
  type: TYPE_NORMAL
- en: To see how the four-way MUX circuit works, consider an S input value of 2 (0b10
    in binary), as shown in [Figure 5-20](ch05.xhtml#ch5fig20). The top AND gate gets
    as input (NOT(S⁰) AND NOT(S¹) AND A), or (1 AND 0 AND A), resulting in 0 output
    from the top AND gate. The second AND gate gets input values (0 AND 0 AND B),
    resulting in 0 output. The third AND gate gets input values (1 AND 1 AND C), resulting
    in the value of C output. The last AND gate gets (0 AND 1 AND D), resulting in
    0 output. The OR gate has inputs (0 OR 0 OR C OR 0), resulting in the value of
    C output by the MUX (an S value of 2 chooses C).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-20: A four-way multiplexer circuit chooses C as output when the Select
    input, S, is 2 (0b10).*'
  prefs: []
  type: TYPE_NORMAL
- en: Demultiplexers and decoders are two other examples of control circuits. A *demultiplexer*
    (DMUX) is the inverse of a multiplexer. Whereas a multiplexer chooses one of *N*
    inputs, a demultiplexer chooses one of *N* outputs. A DMUX takes a single input
    value and a selection input, and has *N* outputs. Based on the value of S, it
    sends the input value to exactly one of its *N* outputs (the value of the input
    is routed on to one of *N* output lines). A DMUX circuit is often used to select
    one of *N* circuits to pass a value. A *decoder* circuit takes an encoded input
    and enables one of several outputs based on the input value. For example, a decoder
    circuit that has an *N*-bit input value uses that value to enable (to set to 1)
    exactly one of its 2^(*N*) output lines (the one corresponding to the encoding
    of the *N*-bit value). [Figure 5-21](ch05.xhtml#ch5fig21) shows an example of
    a two-way one-bit DMUX circuit, whose selection input value (s) chooses which
    of its two outputs gets the input value A. It also shows an example of a two-bit
    decoder circuit, whose input bits determine which of four outputs get set to 1\.
    The truth tables for both circuits are also shown.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-21: A two-way one-bit demultiplexer and a two-bit decoder, along
    with their truth tables*'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.3 Storage Circuits
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Storage circuits* are used to construct computer memory for storing binary
    values. The type of computer memory built from storage circuits is called *static
    RAM* (SRAM). It is used to build CPU register storage and on-chip cache memory.
    Systems typically use *dynamic RAM* (DRAM) for main memory (RAM) storage. The
    capacitor-based design of DRAM requires that it be periodically refreshed with
    the value it stores, hence the “dynamic” moniker. SRAM is circuit-based storage
    that does not need to have its values refreshed, thus it is referred to as static
    RAM. Circuit-based memory is faster but more expensive than capacitor-based memory.
    As a result, SRAM tends to be used for storage at the top of the memory hierarchy
    (CPU registers and on-chip cache memory), and DRAM for main memory (RAM) storage.
    In this chapter, we focus on circuit-based memory like SRAM.'
  prefs: []
  type: TYPE_NORMAL
- en: To store a value, a circuit must contain a feedback loop so that the value is
    retained by the circuit. In other words, a storage circuit’s value depends on
    its input values and also its currently stored value. When the circuit stores
    a value, its currently stored value and its inputs together produce an output
    that matches the currently stored value (i.e., the circuit continues to store
    the same value). When a new value is written into a storage circuit, the circuit’s
    input values change momentarily to modify the behavior of the circuit, which results
    in a new value being written into and stored in the circuit. Once written, the
    circuit resumes a steady state of storing the newly written value until the next
    write to the circuit occurs.
  prefs: []
  type: TYPE_NORMAL
- en: RS Latch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A latch is a digital circuit that stores (or remembers) a one-bit value. One
    example is a *reset–set latch* (or RS latch). An RS latch has two input values,
    R and S, and one output value, Q, which is also the value stored in the latch.
    An RS latch may additionally output NOT(Q), the negation of the stored value.
    [Figure 5-22](ch05.xhtml#ch5fig22) shows an RS latch circuit for storing a single
    bit.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-22: An RS latch circuit stores a one-bit value.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to note about the RS latch is the feedback loop from its outputs
    to its inputs: the output of the top NAND gate (Q) is input (a) to the bottom
    NAND gate, and the output of the bottom NAND gate (~Q) is input (b) to the top
    NAND gate. When inputs S and R are both 1, the RS latch stores the value Q. In
    other words, when S and R are both 1, the RS latch output value Q is stable. To
    see this behavior, consider [Figure 5-23](ch05.xhtml#ch5fig23); this shows an
    RS latch that stores the value 1 (Q is 1). When R and S are both 1, the feedback
    input value (a) to the bottom NAND gate is the value of Q, which is 1, so the
    output of the bottom NAND gate is 0 (1 NAND 1 is 0). The feedback input value
    (b) to the top NAND gate is the output of the bottom NAND gate, which is 0\. The
    other input to the top NAND gate is 1, the value of S. The output of the top gate
    is 1 (1 NAND 0 is 1). Thus, when S and R are both 1, this circuit continuously
    stores the value of Q (1 in this example).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-23: An RS latch that stores a one-bit value. R and S are both 1 when
    the latch stores a value. The stored value is output Q.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To change the value stored in an RS latch, the value of exactly one of R or
    S is set to 0\. When the latch stores the new value, R and S are set back to 1\.
    Control circuitry around the RS latch ensures that R and S can never simultaneously
    be 0: at most one of them will have a value 0, and a value of 0 for one of R or
    S means that a value is being written into the RS latch. To store the value 0
    in an RS latch, input R is set to 0 (and the value of S stays at 1). To store
    the value 1 in an RS latch, input S is set to 0 (and the value of R stays at 1).
    For example, assume that the RS latch currently stores 1\. To write 0 into the
    latch, R’s value is set to 0\. This means that the values 0 and 1 are input to
    the lower NAND gate which computes the result of (0 NAND 1), or is 1\. This output
    value of 1 is also input b to the top NAND gate (shown in [Figure 5-24](ch05.xhtml#ch5fig24)
    B). With a new b input value of 1 and the S input value 1, the upper NAND gate
    computes a new output value 0 for Q, which is also fed as input a into the lower
    NAND gate (shown in [Figure 5-24](ch05.xhtml#ch5fig24) C). With a’s value 0 and
    b’s value 1, the latch now stores 0\. When R is eventually set back to 1 the RS
    latch continues to store the value 0 (shown in [Figure 5-24](ch05.xhtml#ch5fig24)
    D).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-24: To write 0 to an RS latch, momentarily set R to 0.*'
  prefs: []
  type: TYPE_NORMAL
- en: Gated D Latch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A *gated D latch* adds circuitry to an RS latch to ensure that it never receives
    an input of 0 to both R and S simultaneously. [Figure 5-25](ch05.xhtml#ch5fig25)
    shows the construction of a gated D latch.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-25: A gated D latch stores a one-bit value. Its first set of NAND
    gates control writes to the RS latch and ensure that the values of R and S are
    never both simultaneously 0.*'
  prefs: []
  type: TYPE_NORMAL
- en: The data input (D) to the gated D latch is the value to store into the circuit
    (either 0 or 1). The Write Enable (WE) input controls writing a value into the
    RS latch. When WE is 0, the output from both NAND gates is 1, resulting in R and
    S input values of 1 to the RS latch (the RS latch stores a value). The gated D
    latch writes the value of D into the RS latch only when WE is 1\. Because the
    data input (D) value is inverted before it is sent to the bottom NAND gate, the
    input of only one of the top or bottom NAND gates is 1\. This means that when
    the WE bit is 1, exactly one of R or S is 0\. For example, when D is 1 and WE
    is 1, the top NAND computes (1 NAND 1) and the bottom NAND gate computes (O NAND
    1). As a result, the input to S from the top NAND gate is 0 and the input to R
    from the bottom NAND gate is 1, resulting in writing the value 1 into the RS latch.
    When the WE input is 0, both NAND gates output 1, keeping R and S at 1\. In other
    words, when WE is 0, the value of D has no effect on the value stored in the RS
    latch; only when WE is 1 is the value of D written into the latch. To write another
    value into the gated D latch, set D to the value to store and WE to 0.
  prefs: []
  type: TYPE_NORMAL
- en: CPU Register
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Multibit storage circuits are built by linking several one-bit storage circuits
    together. For example, combining 32 one-bit D latches together yields a 32-bit
    storage circuit that could be used as a 32-bit CPU register, as shown in [Figure
    5-26](ch05.xhtml#ch5fig26). The register circuit has two input values: a 32-bit
    data value and a one-bit Write Enable signal. Internally, each one-bit D latch
    takes as its D input one bit of the register’s 32-bit *Data in* input, and each
    one-bit D latch takes the register’s WE input as its WE input. The register’s
    output is the 32-bit value stored across the 32 one-bit D latches that make up
    the register circuit.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-26: A CPU register is built from multiple gated D latches (32 of
    them for a 32-bit register). When its WE input is 1, the Data input is written
    into the register. Its Data output is the stored value.*'
  prefs: []
  type: TYPE_NORMAL
- en: '5.5 Building a Processor: Putting It All Together'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *central processing unit* (CPU) implements the processing and control units
    of the von Neumann architecture, the parts that drive the execution of program
    instructions on program data (see [Figure 5-27](ch05.xhtml#ch5fig27)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-27: The CPU implements the processing and control unit parts of the
    von Neumann architecture.*'
  prefs: []
  type: TYPE_NORMAL
- en: The CPU is constructed from basic arithmetic/logic, storage, and control circuit
    building blocks. Its main functional components are the *arithmetic logic unit*
    (ALU), which performs arithmetic and logic operations; a set of general-purpose
    *registers* for storing program data; some control circuitry and special-purpose
    registers that are used in the implementation of instruction execution; and a
    *clock* that drives the circuitry of the CPU to execute program instructions.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we present the main parts of the CPU, including the ALU and
    register file, and show how they are combined to implement a CPU. In the next
    section, we discuss how the CPU executes program instructions and how the clock
    is used to drive the execution of program instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.1 The ALU
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ALU is a complex circuit that implements all arithmetic and logic operations
    on signed and unsigned integers. A separate floating-point unit performs arithmetic
    operations on floating-point values. The ALU takes integer operand values and
    an *opcode* value that specifies the operation to perform (e.g., addition). The
    ALU outputs the resulting value of performing the specified operation on the operand
    inputs and *condition code* values that encode information about the result of
    the operation. Common condition codes specify whether the ALU result is negative,
    zero, or if there is a carry-out bit from the operation. For example, given the
    C statement
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'the CPU begins executing the addition by feeding the operand values (6 and
    8) and the bits that encode an ADD operation to the ALU circuit. The ALU computes
    the result and outputs it along with condition codes to indicate that the result
    is nonnegative, is nonzero, and causes no carry-out. Each condition code is encoded
    in a single bit. A bit value of 1 indicates that the condition holds, and a bit
    value of 0 indicates that it does not hold for the ALU result. In our example,
    the bit pattern 000 specifies the set of three conditions associated with executing
    6 + 8: the result is not negative (0), is not zero (0), and the carry-out value
    is zero (0).'
  prefs: []
  type: TYPE_NORMAL
- en: Condition codes, set by the ALU as part of its execution of an operation, are
    sometimes used by subsequent instructions that choose an action based on a particular
    condition. For example, an ADD instruction can compute the (x + 8) part of the
    following `if` statement.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The ALU’s execution of the ADD instruction sets condition codes based on the
    result of adding `(x + 8)`. A conditional jump instruction executed after the
    ADD instruction tests the condition code bits set by the ADD instruction and either
    jumps (skips over executing the instructions in the `if` body) or not based on
    their value. For example, if the ADD instruction sets the zero condition code
    to 0, the conditional jump instruction will not jump past the instructions associated
    with the `if` body (0 for the zero condition code means that the result of the
    ADD was not zero). If the zero condition code is 1, it will jump past the `if`
    body instructions. To implement a jump past a set of instructions, the CPU writes
    the memory address of the first instruction after the `if` body instructions into
    the *program counter* (PC), which contains the address of the next instruction
    to execute.
  prefs: []
  type: TYPE_NORMAL
- en: An ALU circuit combines several arithmetic and logic circuits (for implementing
    its set of operations) with a multiplexer circuit to pick the ALU’s output. Rather
    than trying to selectively activate only the arithmetic circuit associated with
    the specific operation, a simple ALU sends its operand input values to all of
    its internal arithmetic and logic circuits. The output from all of the ALU’s internal
    arithmetic and logic circuits are input to its multiplexer circuit, which chooses
    the ALU’s output. The opcode input to the ALU is used as the signal input to the
    multiplexer to select which arithmetic/logic operation to select as the ALU’s
    output. Condition code output is based on the MUX output combined with circuitry
    to test the output’s value to determine each condition code bit.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-28](ch05.xhtml#ch5fig28) shows an example ALU circuit that performs
    four different operations (ADD, OR, AND, and EQUALS) on two 32-bit operands. It
    also produces a single condition code output that indicates whether the result
    of the operation is zero. Notice that the ALU directs the opcode to a multiplexer
    that selects which of the ALU’s four arithmetic results it outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-28: A four-function ALU that performs ADD, OR, AND, and EQUALS on
    two 32-bit operands. It has one condition code output bit that specifies whether
    the result is 0.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The opcode input to the ALU comes from bits in the instruction that the CPU
    is executing. For example, the binary encoding for an ADD instruction might consist
    of four parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Depending on the CPU architecture, operand source bits might encode a CPU register,
    the memory address storing the operand value, or literal operand values. For example,
    in an instruction to perform 6 + 8, the literal values 6 and 8 could be encoded
    directly into the operand specifier bits of the instruction.
  prefs: []
  type: TYPE_NORMAL
- en: For our ALU, the opcode requires two bits because the ALU supports four operations,
    and two bits can encode four distinct values (00, 01, 10, 11), one for each operation.
    In general, an ALU that performs *N* distinct operations needs log[2](*N*) opcode
    bits to specify which operation result to output from the ALU.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-29](ch05.xhtml#ch5fig29) shows an example of how the opcode and operand
    bits of an ADD instruction are used as input into our ALU.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-29: Opcode bits from an instruction are used by the ALU to choose
    which operation to output. In this example, different bits from an ADD instruction
    are fed into the ALU operand and opcode inputs to perform addition of 6 and 8.*'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.2 The Register File
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At the top of the memory hierarchy, the CPU’s set of general-purpose registers
    store temporary values. CPUs provide a very small number of registers, commonly
    8–32 (e.g., the IA32 architecture provides 8, MIPS provides 16, and ARM provides
    13). Instructions often get their operand values from, or store their results
    to, general-purpose registers. For example, an ADD instruction may be encoded
    as “add the value from Register 1 to the value from Register 2 and store the result
    in Register 3.”
  prefs: []
  type: TYPE_NORMAL
- en: The CPU’s set of general-purpose registers is organized into a *register file*
    circuit. A register file consists of a set of register circuits (see “CPU Register”
    on [page 260](ch05.xhtml#lev3_51)) for storing data values and some control circuits
    (see “Control Circuits” on [page 252](ch05.xhtml#lev2_98)) for controlling reads
    and writes to its registers. The circuit typically has a single data input line
    for the value to write into one of its registers, and two data output lines for
    simultaneously reading two values from its registers.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-30](ch05.xhtml#ch5fig30) shows an example of a register file circuit
    with four registers. Its two output values (Data out[**0**] and Data out[**1**])
    are controlled by two multiplexer circuits. Each of its read selection inputs
    (Sr[**0**] and Sr[**1**]) is fed into one of the MUXs to pick the register value
    for the corresponding output. The data input to the register file (the Data in
    line) is sent to every register circuit, and its write enable (WE) input is fed
    through a demultiplexer (DMUX) circuit first before being sent to each register
    circuit. A DMUX circuit takes one input value and chooses which of *N* outputs
    to send the value to, sending the remaining *N –* 1 outputs 0\. The write selection
    input (S[**w**]) to the register file is sent to the DMUX circuit to choose the
    WE value’s destination register. When the register file’s WE input value is 0,
    no value is written into a register because each register’s WE bit also gets 0
    (thus, Data in has no effect on the values stored in the registers). When the
    WE bit is 1, the DMUX outputs a WE bit value of 1 to only the register specified
    by the write selection input (S[**w**]), resulting in the Data in value being
    written to the selected register only.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-30: The register file: the set of CPU general-purpose registers used
    to store instruction operand and result values*'
  prefs: []
  type: TYPE_NORMAL
- en: Special-Purpose Registers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In addition to the set of general-purpose registers in the register file, a
    CPU contains special-purpose registers that store the address and content of instructions.
    The *program counter* (PC) stores the memory address of the next instruction to
    execute, and the *instruction register* (IR) stores the bits of the current instruction
    being executed by the CPU. The bits of the instruction stored in the IR are used
    as input into different parts of the CPU during the instruction’s execution. We
    discuss these registers in more detail in “The Processor’s Execution of Program
    Instructions” on [page 266](ch05.xhtml#lev1_43).
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.3 The CPU
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With the ALU and register file circuits, we can build the main parts of the
    CPU, as shown in [Figure 5-31](ch05.xhtml#ch5fig31). Because instruction operands
    often come from values stored in general-purpose registers, the register file’s
    outputs send data to the ALU’s inputs. Similarly, because instruction results
    are often stored in registers, the ALU’s result output is sent as input to the
    register file. The CPU has additional circuitry to move data between the ALU,
    register file, and other components (e.g., main memory).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-31: The ALU and register file make up the main parts of the CPU.
    The ALU performs operations, and the register file stores operand and result values.
    Additional special-purpose registers store instruction addresses (PC) and contents
    (IR). Note that instructions might retrieve operands from or store results to
    locations other than the register file (e.g., main memory).*'
  prefs: []
  type: TYPE_NORMAL
- en: These main parts of the CPU make up its *data path*. The data path consists
    of the parts of the CPU that perform arithmetic and logic operations (the ALU)
    and store data (registers), and the buses that connect these parts. The CPU also
    implements a *control path* that drives the execution of program instructions
    by the ALU on operands stored in the register file. Additionally, the control
    path issues commands to I/O devices and coordinates memory accesses as needed
    by instructions. For example, some instructions may get their operand values directly
    from (or store their results directly to) memory locations rather than general-purpose
    registers. In the next section, we focus our discussion of CPU instruction execution
    on instructions that get operand values and store results to the register file.
    The CPU requires additional control circuitry to read operand values or to write
    instruction results to other locations, but the main instruction execution steps
    behave the same regardless of the source and destination locations.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 The Processor’s Execution of Program Instructions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instruction execution is performed in several stages. Different architectures
    implement different numbers of stages, but most implement the Fetch, Decode, Execute,
    and WriteBack phases of instruction execution in four or more discrete stages.
    In discussing instruction execution, we focus on these four stages of execution,
    and we use an ADD instruction as our example. Our ADD instruction example is encoded
    as shown in [Figure 5-32](ch05.xhtml#ch5fig32).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-32: An example instruction format for a three-register operation.
    The instruction is encoded in binary with subsets of its bits corresponding to
    encodings of different parts of the instruction: the operation (opcode), the two
    source registers (the operands), and the destination register for storing the
    result of the operation. The example shows the encoding of an ADD instruction
    in this format.*'
  prefs: []
  type: TYPE_NORMAL
- en: To execute an instruction, the CPU first *fetches* the next instruction from
    memory into a special-purpose register, the instruction register (IR). The memory
    address of the instruction to fetch is stored in another special-purpose register,
    the program counter (PC). The PC keeps track of the memory address of the next
    instruction to fetch and is incremented as part of executing the fetch stage so
    that it stores the value of the very next instruction’s memory address. For example,
    if all instructions are 32 bits long, the PC’s value is incremented by four (each
    byte, eight bits, has a unique address) to store the memory address of the instruction
    immediately following the one being fetched. Arithmetic circuits that are separate
    from the ALU increment the PC’s value. The PC’s value may also change during the
    WriteBack stage. For example, some instructions jump to specific addresses, such
    as those associated with the execution of loops, `if`–`else` blocks, or function
    calls. [Figure 5-33](ch05.xhtml#ch5fig33) shows the fetch stage of execution.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-33: The fetch stage of instruction execution: the instruction at
    the memory address value stored in the PC register is read from memory and stored
    into the IR. The PC’s value is also incremented at the end of this stage (if instructions
    are four bytes, the next address is 1238; the actual instruction size varies by
    architecture and instruction type).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'After fetching the instruction, the CPU *decodes* the instruction bits stored
    in the IR register into four parts: the high-order bits of an instruction encode
    the opcode, which specifies the operation to perform (e.g., ADD, SUB, OR . . .
    ), and the remaining bits are divided into three subsets that specify the two
    operand sources and the result destination. In our example, we use registers for
    both sources and the result destination. The opcode is sent on wires that are
    input to the ALU and the source bits are sent on wires that are inputs to the
    register file. The source bits are sent to the two read selection inputs (Sr[0]
    and Sr[1]) that specify which register values are read from the register file.
    The Decode stage is shown in [Figure 5-34](ch05.xhtml#ch5fig34).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-34: The Decode stage of instruction execution: separate the instruction
    bits in the IR into components and send them as input to the ALU and register
    file. The opcode bits in the IR are sent to the ALU selection input to choose
    which operation to perform. The two sets of operand bits in the IR are sent to
    the selection inputs of the register file to pick the registers from which to
    read the operand values. The destination bits in the IR are sent to the register
    file in the WriteBack stage. They specify the register to which to write the ALU
    result.*'
  prefs: []
  type: TYPE_NORMAL
- en: After the Decode stage determines the operation to perform and the operand sources,
    the ALU performs the operation in the next stage, the *Execution* stage. The ALU’s
    data inputs come from the two outputs of the register file, and its selection
    input comes from the opcode bits of the instruction. These inputs propagate through
    the ALU to produce a result that combines the operand values with the operation.
    In our example, the ALU outputs the result of adding the value stored in Reg1
    to the value stored in Reg3, and outputs the condition code values associated
    with the result value. The Execution stage is shown in [Figure 5-35](ch05.xhtml#ch5fig35).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-35: The Execution stage of instruction execution: the ALU performs
    the specified operation (from the instruction opcode bits) on its input values
    (from the register file outputs).*'
  prefs: []
  type: TYPE_NORMAL
- en: In the *WriteBack* stage, the ALU result is stored in the destination register.
    The register file receives the ALU’s result output on its Data in input, the destination
    register (from instructions bits in the IR) on its write-select (S[w]) input,
    and 1 on its WE input. For example, if the destination register is Reg0, then
    the bits encoding Reg0 in the IR are sent as the S[w] input to the register file
    to pick the destination register. The output from the ALU is sent as the Data
    in input to the register file, and the WE bit is set to 1 to enable writing the
    ALU result into Reg0\. The WriteBack stage is shown in [Figure 5-36](ch05.xhtml#ch5fig36).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-36: The WriteBack stage of instruction execution: the result of the
    execution stage (the output from the ALU) is written to the destination register
    in the register file. The ALU output is the register file’s Data in input, the
    destination bits of the instruction go to the register file’s write-selection
    input (S[w]), and the WE input is set to 1 to enable writing the Data in value
    to the specified destination register.*'
  prefs: []
  type: TYPE_NORMAL
- en: 5.6.1 Clock-Driven Execution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A clock drives the CPU’s execution of instructions, triggering the start of
    each stage. In other words, the clock is used by the CPU to determine when inputs
    to circuits associated with each stage are ready to be used by the circuit, and
    it controls when outputs from circuits represent valid results from one stage
    and can be used as inputs to other circuits executing the next stage.
  prefs: []
  type: TYPE_NORMAL
- en: A CPU clock measures discrete time as opposed to continuous time. In other words,
    there exists a time 0, followed by a time 1, followed by a time 2, and so on for
    each subsequent clock tick. A processor’s *clock cycle time* measures the time
    between each clock tick. A processor’s *clock speed* (or *clock rate*) is `1/(clock`
    `cycle time)`. It is typically measured in megahertz (MHz) or gigahertz (GHz).
    A 1-MHz clock rate has one million clock ticks per second, and 1GHz has one billion
    clock ticks per second. The clock rate is a measure of how fast the CPU can run,
    and is an estimate of the maximum number of instructions per second a CPU can
    execute. For example, on simple scalar processors like our example CPU, a 2-GHz
    processor might achieve a maximum instruction execution rate of two billion instructions
    per second (or two instructions every nanosecond).
  prefs: []
  type: TYPE_NORMAL
- en: Although increasing the clock rate on a single machine will improve its performance,
    clock rate alone is not a meaningful metric for comparing the performance of different
    processors. For example, some architectures (such as RISC) require fewer stages
    to execute instructions than others (such as CISC). In architectures with fewer
    execution stages a slower clock may yield the same number of instructions completed
    per second as on another architecture with a faster clock rate but more execution
    stages. For a specific microprocessor, however, doubling its clock speed will
    roughly double its instruction execution speed.
  prefs: []
  type: TYPE_NORMAL
- en: CLOCK RATES AND PROCESSOR PERFORMANCE
  prefs: []
  type: TYPE_NORMAL
- en: Historically, increasing the clock rate (along with designing more complicated
    and powerful microarchitectures that a faster clock can drive) has been a very
    effective way for computer architects to improve processor performance. For example,
    in 1974, the Intel 8080 CPU ran at 2 MHz (a clock rate of two million cycles per
    second). The clock rate of the Intel Pentium Pro, introduced in 1995, was 150
    MHz (150 million cycles per second), and the clock rate of the Intel Pentium 4,
    introduced in 2000, was 1.3 GHz or (1.3 *billion* cycles per second). Clock rates
    peaked in the mid to late 2000s with processors like the IBM z10, which had a
    clock rate of 4.4 GHz.
  prefs: []
  type: TYPE_NORMAL
- en: Today, however, CPU clock rates have reached their limit due to problems associated
    with handling heat dissipation of faster clocks. This limit is known as the *power
    wall*. The power wall resulted in the development of multicore processors starting
    in the mid 2000s. Multicore processors have multiple “simple” CPU cores per chip,
    each core driven by a clock whose rate has not increased from the previous-generation
    core. Multicore processor design is a way to improve CPU performance without having
    to increase the CPU clock rate.
  prefs: []
  type: TYPE_NORMAL
- en: The Clock Circuit
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A clock circuit uses an oscillator circuit to generate a very precise and regular
    pulse pattern. Typically, a crystal oscillator generates the base frequency of
    the oscillator circuit, and the pulse pattern of the oscillator is used by the
    clock circuit to output a pattern of alternating high and low voltages that correspond
    to an alternating pattern of 1 and 0 binary values. [Figure 5-37](ch05.xhtml#ch5fig37)
    shows an example clock circuit generating a regular output pattern of 1 and 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-37: The regular output pattern of 1 and 0 of a clock circuit. Each
    sequence of 1 and 0 makes up a clock cycle.*'
  prefs: []
  type: TYPE_NORMAL
- en: A *clock cycle* (or tick) is a 1 and 0 subsequence from the clock circuit pattern.
    The transition from a 1 to a 0 or a 0 to a 1 is called a *clock edge*. Clock edges
    trigger state changes in CPU circuits, driving the execution of instructions.
    The rising clock edge (the transition from 0 to 1 at the beginning of a new clock
    cycle) indicates a state in which input values are ready for a stage of instruction
    execution. For example, the rising edge transition signals that input values to
    the ALU circuit are ready. While the clock’s value is 1, these inputs propagate
    through the circuit until the output of the circuit is ready. This is called the
    *propagation delay* through the circuit. For example, while the clock signal is
    1 the input values to the ALU propagate through the ALU operation circuits and
    then through the multiplexer to produce the correct output from the ALU for the
    operation combining the input values. On the falling edge (the transition from
    1 to 0), the outputs of the stage are stable and ready to be propagated to the
    next location (shown as “output ready” in [Figure 5-38](ch05.xhtml#ch5fig38)).
    For example, the output from the ALU is ready on the falling edge. For the duration
    of the clock value 0, the ALU’s output propagates to register file inputs. On
    the next clock cycle the rising edge indicates that the register file input value
    is ready to write into a register (shown as “new input” in [Figure 5-38](ch05.xhtml#ch5fig38)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-38: The rising edge of a new clock cycle triggers changes in the
    inputs to the circuits it controls. The falling edge triggers when the outputs
    are valid from the circuits it controls.*'
  prefs: []
  type: TYPE_NORMAL
- en: The length of the clock cycle (or the clock rate) is bounded by the longest
    propagation delay through any stage of instruction execution. The execution stage
    and propagation through the ALU is usually the longest stage. Thus, half of the
    clock cycle time must be no faster than the time it takes for the ALU input values
    to propagate through the slowest operation circuit to the ALU outputs (i.e., the
    outputs reflect the results of the operation on the inputs). For example, in our
    four-operation ALU (OR, ADD, AND, and EQUALS), the ripple carry adder circuit
    has the longest propagation delay and determines the minimum length of the clock
    cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Because it takes one clock cycle to complete one stage of CPU instruction execution,
    a processor with a four-stage instruction execution sequence (Fetch, Decode, Execute,
    WriteBack; see [Figure 5-39](ch05.xhtml#ch5fig39)) completes at most one instruction
    every four clock cycles.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-39: Four-stage instruction execution takes four clock cycles to complete.*'
  prefs: []
  type: TYPE_NORMAL
- en: If, for example, the clock rate is 1 GHz, one instruction takes four nanoseconds
    to complete (each of the four stages taking one nanosecond). With a 2-GHz clock
    rate, one instruction takes only two nanoseconds to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Although clock rate is a factor in a processor’s performance, clock rate alone
    is not a meaningful measure of its performance. Instead, the average number of
    *cycles per instruction* (CPI) measured over a program’s full execution is a better
    measure of a CPU’s performance. Typically, a processor cannot maintain its maximum
    CPI for an entire program’s execution. A submaximum CPI is the result of many
    factors, including the execution of common program constructs that change control
    flow such as loops, `if`–`else` branching, and function calls. The average CPI
    for running a set of standard benchmark programs is used to compare different
    architectures. CPI is a more accurate measure of the CPU’s performance as it measures
    its speed executing a program versus a measure of one aspect of an individual
    instruction’s execution. See a computer architecture textbook^([20](ch05.xhtml#fn5_20))
    for more details about processor performance and designing processors to improve
    their performance.
  prefs: []
  type: TYPE_NORMAL
- en: '5.6.2 Putting It All Together: The CPU in a Full Computer'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The data path (ALU, register file, and the buses that connect them) and the
    control path (instruction execution circuitry) make up the CPU. Together they
    implement the processing and control parts of the von Neumann architecture. Today’s
    processors are implemented as digital circuits etched into silicon chips. The
    processor chip also includes some fast on-chip cache memory (implemented with
    latch storage circuits), used to store copies of recently used program data and
    instructions close to the processor. See [Chapter 11](ch11.xhtml#ch11) for more
    information about on-chip cache memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-40](ch05.xhtml#ch5fig40) shows an example of a processor in the context
    of a complete modern computer, whose components together implement the von Neumann
    architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-40: The CPU in a full modern computer. Buses connect the processor
    chip, main memor, and input and output devices.*'
  prefs: []
  type: TYPE_NORMAL
- en: '5.7 Pipelining: Making the CPU Faster'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our four-stage CPU takes four cycles to execute one instruction: the first
    cycle is used to fetch the instruction from memory; the second to decode the instruction
    and read operands from the register file; the third for the ALU to execute the
    operation; and the fourth to write back the ALU result to a register in the register
    file. To execute a sequence of *N* instructions takes 4*N* clock cycles, as each
    is executed one at a time, in order, by the CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-41: Executing three instructions takes 12 total cycles.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-41](ch05.xhtml#ch5fig41) shows three instructions taking a total
    of 12 cycles to execute, four cycles per instruction, resulting in a CPI of 4
    (CPI is the average number of cycles to execute an instruction). However, the
    control circuitry of the CPU can be improved to achieve a better (lower) CPI value.'
  prefs: []
  type: TYPE_NORMAL
- en: In considering the pattern of execution in which each instruction takes four
    cycles to execute, followed by the next instruction taking four cycles, and so
    on, the CPU circuitry associated with implementing each stage is only actively
    involved in instruction execution once every four cycles. For example, after the
    Fetch stage, the fetch circuitry in the CPU is not used to perform any useful
    action related to executing an instruction for the next three clock cycles. If,
    however, the fetch circuitry could continue to actively execute the Fetch parts
    of subsequent instructions in the next three cycles, the CPU could complete the
    execution of more than a single instruction every four cycles.
  prefs: []
  type: TYPE_NORMAL
- en: CPU *pipelining* is this idea of starting the execution of the next instruction
    before the current instruction has fully completed its execution. CPU pipelining
    executes instructions in order, but it allows the execution of a sequence of instructions
    to overlap. For example, in the first cycle, the first instruction enters its
    Fetch stage of execution. In the second cycle, the first instruction moves to
    its Decode stage, and the second instruction simultaneously enters its Fetch stage.
    In the third cycle, the first instruction moves to its Execution stage, the second
    instruction to its Decode stage, and the third instruction is fetched from memory.
    In the fourth cycle, the first instruction moves to its WriteBack stage and completes,
    the second instruction moves to its Execution stage, the third to its Decode,
    and the fourth instruction enters its Fetch stage. At this point, the CPU pipeline
    of instructions is full—every CPU stage is actively executing program instructions
    where each subsequent instruction is one stage behind its predecessor. When the
    pipeline is full, the CPU completes the execution of one instruction every clock
    cycle!
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-42: Pipelining: overlapping instruction execution to achieve one
    instruction completed per cycle. The circle indicates that the CPU has reached
    the steady state of completing one instruction every cycle.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-42](ch05.xhtml#ch5fig42) shows an example of pipelined instruction
    execution through our CPU. Starting with the fourth clock cycle the pipeline fills,
    meaning that the CPU completes the execution of one instruction every cycle, achieving
    a CPI of 1 (shown in the circle in [Figure 5-42](ch05.xhtml#ch5fig42)). Notice
    that the total number of cycles required to execute a single instruction (the
    instruction *latency*) has not decreased in pipelined execution—it still takes
    four cycles for each instruction to execute. Instead, pipelining increases instruction
    *throughput*, or the number of instructions that the CPU can execute in a given
    period of time, by overlapping the execution of sequential instructions in a staggered
    manner, through the different stages of the pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: Since the 1970s, computer architects have used pipelining as a way to drastically
    improve the performance of microprocessors. However, pipelining comes at the cost
    of a more complicated CPU design than one that does not support pipelined execution.
    Additional storage and control circuitry is needed to support pipelining. For
    example, multiple instruction registers may be required to store the multiple
    instructions currently in the pipeline. This added complexity is almost always
    worth the large improvements in CPI that pipelining provides. As a result, most
    modern microprocessors implement pipelined execution.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of pipelining is also used in other contexts in computer science to
    speed up execution, and the idea applies to many non-CS applications as well.
    Consider, for example, the task of doing multiple loads of laundry using a single
    washing machine. If completing one laundry consists of four steps (washing, drying,
    folding, and putting away clothes), then after washing the first load, the second
    load can go in the washing machine while the first load is in the dryer, overlapping
    the washing of individual laundry loads to speed up the total time it takes to
    wash four loads. Factory assembly lines are another example of pipelining.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our discussion of how a CPU executes program instructions and CPU pipelining,
    we used a simple four-stage pipeline and an example ADD instruction. To execute
    instructions that load and store values between memory and registers, a five-stage
    pipeline is used. A five-stage pipeline includes a Memory stage for memory access:
    Fetch–Decode–Execute–Memory– WriteBack. Different processors may have fewer or
    more pipeline stages than a typical five-stage pipeline. For example, the initial
    ARM architecture had three stages (Fetch, Decode, and Execute, wherein the Execute
    stage performed both the ALU execution and the register file WriteBack functionality).
    More recent ARM architectures have more than five stages in their pipelines. The
    initial Intel Pentium architectures had a five-stage pipeline, but later architectures
    had significantly more pipeline stages. For example, the Intel Core i7 has a 14-stage
    pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.8 Advanced Pipelined Instruction Considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recall that pipelining improves the performance of a processor by overlapping
    the execution of multiple instructions. In our earlier discussion on pipelining,
    we described a simple four-stage pipeline with the basic stages of Fetch (F),
    Decode (D), Execute (E) and WriteBack (W). In our discussion that follows, we
    also consider a fifth stage, Memory (M), which represents an access to data memory.
    Our five-stage pipeline therefore comprises the following stages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fetch (F): reads an instruction from memory (pointed to by the program counter).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Decode (D): reads source registers and sets control logic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Execute (E): executes the instruction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Memory (M): reads from or writes to data memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WriteBack (W): stores a result in a destination register.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recall that the compiler transforms lines of code into a series of machine
    code instructions for the CPU to execute. Assembly code is a human-readable version
    of machine code. The snippet below displays a series of made-up assembly instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Don’t worry if you are having trouble parsing the snippet—we cover assembly
    in greater detail in [Chapter 7](ch07.xhtml#ch07). For now, it suffices to focus
    on the following set of facts:'
  prefs: []
  type: TYPE_NORMAL
- en: Every ISA defines a set of instructions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each instruction operates on one or more operands (that is, registers, memory,
    or constant values).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not all instructions require the same number of pipeline stages to execute.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our previous discussion, it was assumed that every instruction takes the
    same number of cycles to execute; however, this is usually not the case. For example,
    the first `MOV` instruction requires all five stages, as it requires the movement
    of data from memory to a register. In contrast, the next three instructions require
    only four stages (F, D, E, W) to execute given that the operations involve only
    registers, and not memory. The last instruction (`JMP`) is a type of *branch*
    or *conditional* instruction. Its purpose is to transfer the flow of control to
    another part of the code. Specifically, addresses in the code region of memory
    reference different *instructions* in an executable. Since the `JMP` instruction
    does not update a general-purpose register, the WriteBack stage is omitted, resulting
    in only three stages (F, D, E) being required. We cover conditional instructions
    in greater detail in “Conditional Control and Loops” on [page 310](ch07.xhtml#lev1_54).
  prefs: []
  type: TYPE_NORMAL
- en: A *pipeline stall* results when any instruction is forced to wait for another
    to finish executing before it can continue. Compilers and processors do whatever
    they can to avoid pipeline stalls in order to maximize performance.
  prefs: []
  type: TYPE_NORMAL
- en: '5.8.1 Pipelining Consideration: Data Hazards'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A *data hazard* occurs when two instructions attempt to access common data
    in an instruction pipeline. As an example, consider the first pair of instructions
    from the previous code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/05fig43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-43: An example of a pipeline hazard arising from two instructions
    simultaneously reaching the same pipeline stage*'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that this `MOV` instruction requires five stages (as it involves an access
    to memory), whereas the `ADD` instruction requires only four. In this scenario,
    both instructions will attempt to write to register `Reg1` at the same time (see
    [Figure 5-43](ch05.xhtml#ch5fig43)).
  prefs: []
  type: TYPE_NORMAL
- en: The processor prevents the aforementioned scenario by first forcing every instruction
    to take five pipeline stages to execute. For instructions that normally take fewer
    than five stages, the CPU adds a “no-operation” (`NOP`) instruction (also called
    a pipeline “bubble”) to substitute for that phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the problem is still not fully resolved. Since the goal of the second
    instruction is to add `2` to the value stored in register `Reg1`, the `MOV` instruction
    needs to finish *writing* to register `Reg1` before the `ADD` instruction can
    execute correctly. A similar problem exists in the next two instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![image](../images/05fig44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-44: The processor can reduce the damage caused by pipeline hazards
    by forwarding operands between instructions.*'
  prefs: []
  type: TYPE_NORMAL
- en: These two instructions load the value `4` into register `Reg2` and then multiply
    it by 2 (by adding to itself). Once again, bubbles are added to enforce that each
    instruction takes five pipeline stages. In this case, regardless of the bubbles,
    the second instruction’s execute phase occurs *before* the first instruction finishes
    writing the required value (`4`) to register `Reg2`.
  prefs: []
  type: TYPE_NORMAL
- en: Adding more bubbles is a suboptimal solution because it stalls the pipeline.
    Instead, processors employ a technique called *operand forwarding*, in which the
    pipeline reads the result from the previous operation. Looking at [Figure 5-44](ch05.xhtml#ch5fig44),
    while the instruction `MOV 4, Reg2` executes, it forwards its results to the instruction
    `ADD Reg2, Reg2, Reg2`. So, while the `MOV` instruction is writing to register
    `Reg2`, the `ADD` instruction can use the updated value of `Reg2` that it received
    from the `MOV` instruction.
  prefs: []
  type: TYPE_NORMAL
- en: '5.8.2 Pipelining Hazards: Control Hazards'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The pipeline is optimized for instructions that occur one after another. Control
    changes in a program arising from conditionals such as `if` statements or loops
    can seriously affect the pipeline performance. Let’s take a look at a different
    example code snippet, first in C:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This snippet simply reads integer data from two different pointers, compares
    the values, and then does different arithmetic based on the result. Here is how
    the preceding code snippet may translate into assembly instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This sequence of instructions loads data from memory into two separate registers,
    compares the values, and then does different arithmetic based on whether the value
    in the first register is less than the value in the second. The `if` statement
    is represented in this example with two instructions: the compare (`CMP`) instruction
    and a conditional jump less than (`JLE`) instruction. We cover conditional instructions
    in greater detail in “Conditional Control and Loops” on [page 310](ch07.xhtml#lev1_54);
    for now, it is sufficient to understand that the `CMP` instruction *compares*
    two registers, while the `JLE` instruction is a special type of branch instruction
    that switches code execution to another part of the program *if and only if* the
    condition (i.e., less than or equal, in this case) is true.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Warning DON’T GET OVERWHELMED BY THE DETAILS!**'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at assembly for the first time can be understandably intimidating. If
    this is how you feel, try not to worry! We cover assembly in much greater detail
    in [Chapter 7](ch07.xhtml#ch07). The key takeaway is that code containing conditional
    statements translates to a series of assembly instructions just like any other
    code snippet. However, unlike other code snippets, conditional statements are
    *not* guaranteed to execute in a particular way. The uncertainty surrounding how
    a conditional statement executes has large ramifications for the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-45: An example of a control hazard resulting from a conditional branch*'
  prefs: []
  type: TYPE_NORMAL
- en: A *control hazard* occurs when the pipeline encounters a branch (or conditional)
    instruction. When this happens, the pipeline has to “guess” whether the branch
    will be taken. If the branch is not taken, the process continues to execute the
    next instructions in sequence. Consider the example in [Figure 5-45](ch05.xhtml#ch5fig45).
    If the branch is taken, the next instruction that executes should be the `SUB`
    instruction. However, it is impossible to know whether the branch is taken until
    the `JLE` instruction finishes executing. At that point, the `ADD` and `JMP` instructions
    have already been loaded into the pipeline. If the branch *is* taken, these “junk”
    instructions in the pipeline need to be removed, or *flushed*, before the pipeline
    can be reloaded with new instructions. Flushing the pipeline is expensive.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few options that hardware engineers can choose to implement to
    help the processor deal with control hazards:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stall the pipeline**: As a simple solution, whenever there is a branch, add
    lots of `NOP` bubbles and stall the pipeline until the processor is sure that
    the branch is taken. Although stalling the pipeline will fix the issue, it will
    also lead to a performance hit (see [Figure 5-46](ch05.xhtml#ch5fig46)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Branch prediction**: The most common solution is to use a *branch predictor*,
    which will predict which way a branch will go, based on previous executions. Modern
    branch predictors are really good and accurate. However, this approach has recently
    caused some security vulnerabilities (e.g. Spectre^([21](ch05.xhtml#fn5_21))).
    [Figure 5-46](ch05.xhtml#ch5fig46) depicts how a branch predictor may deal with
    the control hazard discussed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Eager execution**: In eager execution, the CPU executes both sides of the
    branch and performs a conditional transfer of data rather than control (implemented
    through the `cmov` and `csel` instructions in x86 and ARMv8-A, respectively).
    A conditional transfer of data enables the processor to continue execution without
    disrupting the pipeline. However, not all code is capable of taking advantage
    of eager execution, which can be dangerous in the case of pointer dereferences
    and side effects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![image](../images/05fig46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-46: Potential solutions for handling control hazards*'
  prefs: []
  type: TYPE_NORMAL
- en: '5.9 Looking Ahead: CPUs Today'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CPU pipelining is one example of *instruction-level parallelism* (ILP), in which
    the CPU simultaneously executes multiple instructions in parallel. In a pipelined
    execution, the CPU simultaneously executes multiple instructions by overlapping
    their execution in the pipeline. A simple pipelined CPU can achieve a CPI of 1,
    completing the execution of one instruction every clock cycle. Modern microprocessors
    typically employ pipelining along with other ILP techniques and include multiple
    CPU cores to achieve processor CPI values of less than 1\. For these microarchitectures,
    the average number of *instructions per cycle* (IPC) is the metric commonly used
    to describe their performance. A large IPC value indicates that a processor achieves
    a high sustained degree of simultaneous instruction execution.
  prefs: []
  type: TYPE_NORMAL
- en: Transistors are the building blocks of all circuitry on an integrated circuit
    (a chip). The processing and control units of modern CPUs are constructed from
    circuits, which are built from subcircuits and basic logic gates that are implemented
    with transistors. Transistors also implement the storage circuits used in CPU
    registers and in fast on-chip cache memory that stores copies of recently accessed
    data and instructions (we discuss cache memory in detail in [Chapter 11](ch11.xhtml#ch11).
  prefs: []
  type: TYPE_NORMAL
- en: The number of transistors that can fit on a chip is a rough measure of its performance.
    *Moore’s Law* is the observation, made by Gordon Moore in 1975, that the number
    of transistors per integrated circuit doubles about every two years.^([22](ch05.xhtml#fn5_22))
    A doubling in the number of transistors per chip every two years means that computer
    architects can design a new chip with twice as much space for storage and computation
    circuitry, roughly doubling its power. Historically, computer architects used
    the extra transistors to design more complex single processors using ILP techniques
    to improve overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: 5.9.1 Instruction-Level Parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instruction level parallelism (ILP) is a term for a set of design techniques
    used to support parallel execution of a single program’s instructions on a single
    processor. ILP techniques are transparent to the programmer, meaning that a programmer
    writes a sequential C program but the processor executes several of its instructions
    simultaneously, in parallel, on one or more execution units. Pipelining is one
    example of ILP, where a sequence of program instructions execute simultaneously,
    each in a different pipeline stage. A pipelined processor can execute one instruction
    per cycle (can achieve an IPC of 1). Other types of microprocessor ILP designs
    can execute more than a single instruction per clock cycle and achieve IPC values
    higher than 1.
  prefs: []
  type: TYPE_NORMAL
- en: A *vector processor* is an architecture that implements ILP through special
    vector instructions that take one-dimensional arrays (vectors) of data as their
    operands. Vector instructions are executed in parallel by a vector processor on
    multiple execution units, each unit performing an arithmetic operation on single
    elements of its vector operands. In the past, vector processors were often used
    in large parallel computers. The 1976 Cray-1 was the first vector processor–based
    supercomputer, and Cray continued to design its supercomputers with vector processors
    throughout the 1990s. However, eventually this design could not compete with other
    parallel supercomputer designs, and today vector processors appear primarily in
    accelerator devices such as graphics processing units (GPUs) that are particularly
    optimized for performing computation on image data stored in 1D arrays.
  prefs: []
  type: TYPE_NORMAL
- en: '*Superscalar* is another example of an ILP processor design. A superscalar
    processor is a single processor with multiple execution units and multiple execution
    pipelines. A superscalar processor fetches a set of instructions from a sequential
    program’s instruction stream, and breaks them up into multiple independent streams
    of instructions that are executed in parallel by its execution units. A superscalar
    processor is an *out-of-order processor*, or one that executes instructions out
    of the order in which they appear in a sequential instruction stream. Out-of-order
    execution requires identifying sequences of instructions without dependencies
    that can safely execute in parallel. A superscalar processor contains functionality
    to dynamically create the multiple streams of independent instructions to feed
    through its multiple execution units. This functionality must perform dependency
    analysis to ensure the correct ordering of any instruction whose execution depends
    on the result of a previous instruction in these sequential streams. As an example,
    a superscalar processor with five pipelined execution units can execute five instructions
    from a sequential program in a single cycle (can achieve an IPC of 5). However,
    due to instruction dependencies, it is not always the case that a superscalar
    processor can keep all of its pipelines full.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Very long instruction word* (VLIW) is another ILP microarchitecture design
    that is similar to superscalar. In VLIW architectures, however, the compiler is
    responsible for constructing the multiple independent instruction streams executed
    in parallel by the processor. A compiler for a VLIW architecture analyzes the
    program instructions to statically construct a VLIW instruction that consists
    of multiple instructions, one from each independent stream. VLIW leads to simpler
    processor design than superscalar because the VLIW processor does not need to
    perform dependency analysis to construct the multiple independent instruction
    streams as part of its execution of program instructions. Instead, a VLIW processor
    just needs added circuitry to fetch the next VLIW instruction and break it up
    into its multiple instructions that it feeds into each of its execution pipelines.
    However, by pushing dependency analysis to the compiler, VLIW architectures require
    specialized compilers to achieve good performance.'
  prefs: []
  type: TYPE_NORMAL
- en: One problem with both superscalar and VLIW is that the degree of parallel performance
    is often significantly limited by the sequential application programs they execute.
    Dependencies between instructions in the program limit the ability to keep all
    of the pipelines full.
  prefs: []
  type: TYPE_NORMAL
- en: 5.9.2 Multicore and Hardware Multithreading
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: By designing single processors that employed increasingly complicated ILP techniques
    and increasing the CPU clock speed to drive this increasingly complicated functionality,
    computer architects designed processors whose performance kept pace with Moore’s
    Law until the early 2000s. After this time, CPU clock speeds could no longer increase
    without greatly increasing a processor’s power consumption.^([23](ch05.xhtml#fn5_23))
    This led to the current era of multicore and multithreaded microarchitectures,
    both of which require *explicit parallel programming* by a programmer to speed
    up the execution of a single program.
  prefs: []
  type: TYPE_NORMAL
- en: '*Hardware multithreading* is a single-processor design that supports executing
    multiple hardware threads. A *thread* is an independent stream of execution. For
    example, two running programs each have their own thread of independent execution.
    These two programs’ threads of execution could then be scheduled by the operating
    system to run “at the same time” on a multithreaded processor. Hardware multithreading
    may be implemented by a processor alternating between executing instructions from
    each of its threads’ instruction streams each cycle. In this case, the instructions
    of different hardware threads are not all executed simultaneously each cycle.
    Instead, the processor is designed to quickly switch between executing instructions
    from different threads’ execution streams. This usually results in a speed-up
    of their execution as a whole as compared to their execution on a singly threaded
    processor.'
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading can be implemented in hardware on either scalar- or superscalar-type
    microprocessors. At a minimum, the hardware needs to support fetching instructions
    from multiple separate instruction streams (one for each thread of execution),
    and have separate register sets for each thread’s execution stream. These architectures
    are *explicitly multithreaded*^([24](ch05.xhtml#fn5_24)) because, unlike superscalar
    architectures, each of the execution streams is independently scheduled by the
    operating system to run a separate logical sequence of program instructions. The
    multiple execution streams could come from multiple sequential programs or from
    multiple software threads from a single multithreaded parallel program (we discuss
    multithreaded parallel programming in section 14.1).
  prefs: []
  type: TYPE_NORMAL
- en: Hardware multithreaded microarchitectures that are based on superscalar processors
    have multiple pipelines and multiple execution units, and thus they can execute
    instructions from several hardware threads simultaneously, in parallel, resulting
    in an IPC value greater than 1\. Multithreaded architectures based on simple scalar
    processors implement *interleaved multithreading*. These microarchitectures typically
    share a pipeline and always share the processor’s single ALU (the CPU switches
    between executing different threads on the ALU). This type of multithreading cannot
    achieve IPC values greater than 1\. Hardware threading supported by superscalar-based
    microarchitectures is often called *simultaneous multithreading* (SMT).^([25](ch05.xhtml#fn5_25))
    Unfortunately, SMT is often used to refer to both types of hardware multithreading,
    and the term alone is not always sufficient to determine whether a multithreaded
    microarchitecture implements true simultaneous or interleaved multithreading.
  prefs: []
  type: TYPE_NORMAL
- en: '*Multicore processors* contain multiple complete CPU cores. Like multithreaded
    processors, each core is independently scheduled by the OS. However, each core
    of a multicore processor is a full CPU core, one that contains its own complete
    and separate functionality to execute program instructions. A multicore processor
    contains replicas of these CPU cores with some additional hardware support for
    the cores to share cached data. Each core of a multicore processor could be scalar,
    superscalar, or hardware multithreaded. [Figure 5-47](ch05.xhtml#ch5fig47) shows
    an example of a multicore computer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/05fig47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-47: A computer with a multicore processor. The processor contains
    multiple complete CPU cores, each with its own private cache memory. The cores
    communicate with each and share a larger shared cached memory via on-chip buses.*'
  prefs: []
  type: TYPE_NORMAL
- en: Multicore microprocessor design is the primary way in which the performance
    of processor architectures can continue to keep pace with Moore’s Law without
    increasing the processor clock rate. A multicore computer can simultaneously run
    several sequential programs, the OS scheduling each core with a different program’s
    instruction stream. It can speed up execution of a single program if the program
    is written as an explicitly multithreaded (software-level threads) parallel program.
    For example, the OS can schedule the threads of an individual program to run simultaneously
    on individual cores of the multicore processor, speeding up the execution of the
    program compared to its execution of a sequential version of the same program.
    In [Chapter 14](ch14.xhtml#ch14), we discuss explicit multithreaded parallel programming
    for multicore and other types of parallel systems with shared main memory.
  prefs: []
  type: TYPE_NORMAL
- en: 5.9.3 Some Example Processors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Today, processors are built using a mix of ILP, hardware multithreading, and
    multicore technologies. In fact, it is difficult to find a processor that is not
    multicore. Desktop-class processors typically have two to eight cores, many of
    which also support a low level of per-core multithreading. For example, AMD Zen
    multicore processors^([26](ch05.xhtml#fn5_26)) and Intel’s hyperthreaded multicore
    Xeon and Core processors^([27](ch05.xhtml#fn5_27)) both support two hardware threads
    per core. Intel’s hyperthreaded cores implement interleaved multithreading. Thus,
    each of its cores can only achieve an IPC of 1, but with multiple CPU cores per
    chip, the processor can achieve higher IPC levels.
  prefs: []
  type: TYPE_NORMAL
- en: Processors designed for high-end systems, such as those used in servers and
    supercomputers, contain many cores, where each core has a high degree of multithreading.
    For example, Oracle’s SPARC M7 processor,^([28](ch05.xhtml#fn5_28)) used in high-end
    servers, has 32 cores. Each of its cores has eight hardware threads, two of which
    can execute simultaneously, resulting in a maximum IPC value of 64 for the processor.
    The two fastest supercomputers in the world (as of June 2019)^([29](ch05.xhtml#fn5_29))
    use IBM’s Power 9 processors.^([30](ch05.xhtml#fn5_30)) Power 9 processors have
    up to 24 cores per chip, and each core supports up to eight-way simultaneous multithreading.
    A 24-core version of the Power 9 processor can achieve an IPC of 192.
  prefs: []
  type: TYPE_NORMAL
- en: 5.10 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we presented the computer’s architecture, focusing on its processor
    (CPU) design and implementation in order to understand how it runs a program.
    Today’s modern processors are based on the von Neumann architecture, which defines
    a stored-program, universal computer. The general-purpose design of the von Neumann
    architecture allows it to execute any type of program.
  prefs: []
  type: TYPE_NORMAL
- en: To understand how the CPU executes program instructions, we built an example
    CPU, starting with basic logic-gate building blocks to create circuits that together
    implement a digital processor. A digital processor’s functionality is built by
    combining control, storage, and arithmetic/logic circuits, and is run by a clock
    circuit that drives the Fetch, Decode, Execute, and WriteBack phases of its execution
    of program instructions.
  prefs: []
  type: TYPE_NORMAL
- en: All processor architectures implement an instruction set architecture (ISA)
    that defines the set of CPU instructions, the set of CPU registers, and the effects
    of executing instructions on the state of the processor. There are many different
    ISAs, and there are often different microprocessor implementations of a given
    ISA. Today’s microprocessors also use a variety of techniques to improve processor
    performance, including pipelined execution, instruction-level parallelism, and
    multicore design.
  prefs: []
  type: TYPE_NORMAL
- en: For more breadth and depth of coverage on computer architecture, we recommend
    reading a computer architecture textbook.^([31](ch05.xhtml#fn5_31))
  prefs: []
  type: TYPE_NORMAL
- en: Notes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[1.](ch05.xhtml#rfn5_1) “ACM A. M. Turing award winners,” *[https://amturing.acm.org/](https://amturing.acm.org/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[2.](ch05.xhtml#rfn5_2) “Pioneers of modern computer architecture receive ACM
    A. M. Turing award,” ACM Media Center Notice, March 2018, *[https://www.acm.org/media-center/2018/march/turing-award-2017](https://www.acm.org/media-center/2018/march/turing-award-2017)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[3.](ch05.xhtml#rfn5_3) David Alan Grier, *When Computers Were Human*, Princeton
    University Press, 2005.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4.](ch05.xhtml#rfn5_4) Megan Garber, “Computing power used to be measured
    in *kilo-girls*,” *The Atlantic*, October 16, 2013\. *[https://www.theatlantic.com/technology/archive/2013/10/computing-power-used-to-be-measured-in-kilo-girls/280633/](https://www.theatlantic.com/technology/archive/2013/10/computing-power-used-to-be-measured-in-kilo-girls/280633/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[5.](ch05.xhtml#rfn5_5) Betty Alexandra Toole, *Ada, The Enchantress of Numbers*,
    Strawberry Press, 1998.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6.](ch05.xhtml#rfn5_6) George Dyson, *Turing’s Cathedral: The Origins of the
    Digital Universe*, Pantheon, 2012.'
  prefs: []
  type: TYPE_NORMAL
- en: '[7.](ch05.xhtml#rfn5_7) Walter Isaacson, *The Innovators: How a Group of Inventors,
    Hackers, Genius and Geeks Created the Digital Revolution*, Simon and Schuster,
    2014.'
  prefs: []
  type: TYPE_NORMAL
- en: '[8.](ch05.xhtml#rfn5_8) Alan M. Turing, “On computable numbers, with an application
    to the *Entscheidungsproblem*,” *Proceedings of the London Mathematical Society*
    2(1), pp. 230–265, 1937.'
  prefs: []
  type: TYPE_NORMAL
- en: '[9.](ch05.xhtml#rfn5_9) Brian Carpenter and Robert Doran, “The other Turing
    machine,” *The Computer Journal* 20(3), pp. 269–279, 1977.'
  prefs: []
  type: TYPE_NORMAL
- en: '[10.](ch05.xhtml#rfn5_10) James A. Reeds, Whitfield Diffie, and J. V. Field
    (eds), *Breaking Teleprinter Ciphers at Bletchley Park: General Report on Tunny
    with Emphasis on Statistical Methods (1945)*, Wiley, 2015.'
  prefs: []
  type: TYPE_NORMAL
- en: '[11.](ch05.xhtml#rfn5_11) Jack Copeland et al., *Colossus: The Secrets of Bletchley
    Park’s Code-Breaking Computers*, OUP, 2010.'
  prefs: []
  type: TYPE_NORMAL
- en: '[12.](ch05.xhtml#rfn5_12) Janet Abbate, *Recoding Gender*, MIT Press, 2012.'
  prefs: []
  type: TYPE_NORMAL
- en: '[13.](ch05.xhtml#rfn5_13) Walter Isaacson, *The Innovators: How a Group of
    Inventors, Hackers, Genius and Geeks Created the Digital Revolution*, Simon and
    Schuster, 2014.'
  prefs: []
  type: TYPE_NORMAL
- en: '[14.](ch05.xhtml#rfn5_14) Janet Abbate, *Recoding Gender*, MIT Press, 2012.'
  prefs: []
  type: TYPE_NORMAL
- en: '[15.](ch05.xhtml#rfn5_15) LeAnn Erickson, *Top Secret Rosies: The Female Computers
    of World War II*, Public Broadcasting System, 2010.'
  prefs: []
  type: TYPE_NORMAL
- en: '[16.](ch05.xhtml#rfn5_16) Kathy Kleiman, The Computers, *[http://eniacprogrammers.org/](http://eniacprogrammers.org/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[17.](ch05.xhtml#rfn5_17) John von Neumann, “First draft of a report on the
    EDVAC (1945).” Reprinted in *IEEE Annals of the History of Computing* 4, pp. 27–75,
    1993.'
  prefs: []
  type: TYPE_NORMAL
- en: '[18.](ch05.xhtml#rfn5_18) John von Neumann, “First draft of a report on the
    EDVAC (1945).” Reprinted in *IEEE Annals of the History of Computing* 4, pp. 27–75,
    1993.'
  prefs: []
  type: TYPE_NORMAL
- en: '[19.](ch05.xhtml#rfn5_19) Walter Isaacson, *The Innovators: How a Group of
    Inventors, Hackers, Genius and Geeks Created the Digital Revolution*, Simon and
    Schuster, 2014.'
  prefs: []
  type: TYPE_NORMAL
- en: '[20.](ch05.xhtml#rfn5_20) One suggestion is John Hennessy and David Patterson,
    *Computer Architecture: A Quantitative Approach*, Morgan Kaufmann, 2011.'
  prefs: []
  type: TYPE_NORMAL
- en: '[21.](ch05.xhtml#rfn5_21) Peter Bright, “Google: Software is never going to
    be able to fix Spectre-type bugs,” *Ars Technica*, 2019.'
  prefs: []
  type: TYPE_NORMAL
- en: '[22.](ch05.xhtml#rfn5_22) Moore first observed a doubling every year in 1965;
    he then updated this in 1975 to every > 2 years, which became known as Moore’s
    Law. Moore’s Law held until around 2012 when improvements in transistor density
    began to slow. Moore predicted the end of Moore’s Law in the mid 2020s.'
  prefs: []
  type: TYPE_NORMAL
- en: '[23.](ch05.xhtml#rfn5_23) Adrian McMenamin, “The end of Dennard scaling,” *[https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/](https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[24.](ch05.xhtml#rfn5_24) T. Ungerer, B. Robic, and J. Silc, “A survey of processors
    with explicit multi-threading,” *ACM Computing Surveys* 35(1), pp. 29–63, 2003.'
  prefs: []
  type: TYPE_NORMAL
- en: '[25.](ch05.xhtml#rfn5_25) T. Ungerer, B. Robic, and J. Silc, “A survey of processors
    with explicit multi-threading,” *ACM Computing Surveys* 35(1), pp. 29–63, 2003.'
  prefs: []
  type: TYPE_NORMAL
- en: '[26.](ch05.xhtml#rfn5_26) *[https://www.amd.com/en/technologies/zen-core](https://www.amd.com/en/technologies/zen-core)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[27.](ch05.xhtml#rfn5_27) *[https://www.intel.com/content/www/us/en/architecture-and-technology/hyper-threading-technology.html](https://www.intel.com/content/www/us/en/architecture-and-technology/hyper-threading-technology.html)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[28.](ch05.xhtml#rfn5_28) *[https://web.archive.org/web/20190819165804/http://www.oracle.com/us/products/servers-storage/sparc-m7-processor-ds-2687041.pdf](https://web.archive.org/web/20190819165804/http://www.oracle.com/us/products/servers-storage/sparc-m7-processor-ds-2687041.pdf)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[29.](ch05.xhtml#rfn5_29) *[https://www.top500.org/lists/top500/](https://www.top500.org/lists/top500/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[30.](ch05.xhtml#rfn5_30) *[https://www.ibm.com/it-infrastructure/power/power9](https://www.ibm.com/it-infrastructure/power/power9)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[31.](ch05.xhtml#rfn5_31) One suggestion is David A. Patterson and John L.
    Hennessy, *Computer Organization and Design: The Hardware and Software Interface*,
    Morgan Kaufmann, 2010.'
  prefs: []
  type: TYPE_NORMAL
